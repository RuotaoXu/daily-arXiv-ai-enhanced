<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 106]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments](https://arxiv.org/abs/2512.23786)
*Ankan Aich,Yangming Lee*

Main category: cs.CV

TL;DR: 提出一种将Depth Anything V2的高保真合成深度先验迁移到内窥镜单目深度估计的方案，通过DV-LORA高效适配，并在SCARED上引入物理分层评测；在强高光环境下显著优于现有方法，<1.25达98.1%，SqRel降17%+。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景存在镜面反射、液体和透明表面对自监督MDE造成伪标签噪声与薄结构边界塌陷问题，现有依赖真实世界伪标签的基础模型易失稳，难以在手术强高光下鲁棒工作。需要借助能准确刻画细薄结构的高保真先验并缩小合成到真实的域差。

Method: 以Depth Anything V2为骨干，利用合成数据中对细结构的几何先验；通过动态向量低秩适配（DV-LORA）在小参数开销下进行领域自适应，缓解合成-真实域差；提出基于物理属性分层（如高镜面区）的评测协议，在SCARED数据集上更精细地度量性能，避免被汇总指标掩盖。

Result: 在SCARED上建立SOTA：准确率（<1.25）达98.1%，相比主流基线的平方相对误差（SqRel）降低17%以上；在高镜面/强光条件下的分层指标显著领先，边界与薄器械、透明表面上更稳健。

Conclusion: 高保真合成先验与DV-LORA的高效适配可有效缓解自监督MDE在外科内窥镜高光环境的失效，配合物理分层评测能更真实反映鲁棒性；方法以极低参数增量达到SOTA，适用于对薄结构和透明表面敏感的手术场景。

Abstract: Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.

</details>


### [2] [Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments](https://arxiv.org/abs/2512.23819)
*Surya Rayala,Marcos Quinones-Grueiro,Naveeduddin Mohammed,Ashwin T S,Benjamin Goldberg,Randall Spain,Paige Lawton,Gautam Biswas*

Main category: cs.CV

TL;DR: 论文提出一种基于视频的城市巷战“入室清剿（ECR）”训练评估管线，仅依赖训练视频，通过计算机视觉提取骨架、视线与轨迹，构建心理运动、态势感知与团队协同指标，并在扩展CTA层级中加权汇总为认知与团队总体得分，案例验证可用于AAR与GIFT/GM仪表板；讨论跟踪、标注与泛化等限制，展望3D与STE规模化应用。


<details>
  <summary>Details</summary>
Motivation: 传统ECR训练评估依赖昂贵/侵入式传感器或人工主观评分，难以客观、可扩展地衡量认知、动作与团队协作能力。需要一种低成本、非侵入、可量化且可集成于合成训练环境的自动化评估方案。

Method: 构建视频分析管线：1) 从训练视频使用计算机视觉提取2D骨架、注视向量、移动轨迹；2) 设计面向任务的指标以衡量心理运动流畅度（如路径平滑、动作时序、房间占领速度）、态势感知（视线与威胁/门窗/队友的对准与覆盖）、团队协调（进出顺序、扇区覆盖互补、相对位姿与间距）；3) 将指标映射到扩展的CTA层级，按权重汇总为“认知”和“团队协作”分数；4) 在真实ECR演练案例中演示，并将结果对接Gamemaster与GIFT的交互式AAR仪表板。

Result: 系统在真实ECR训练中产出可操作的个体与团队指标与总体评分，能揭示战术动作流畅度、视域覆盖不足、队形协同等问题，并以可视化与仪表板呈现，为AAR提供具体反馈。

Conclusion: 基于视频的自动评估在不增加硬件负担的前提下，为ECR训练提供客观、细粒度与可扩展的绩效度量；但仍受限于视觉跟踪鲁棒性、缺乏强地面真实标注与跨场景泛化。未来将扩展到3D视频/多视角融合，并在STE中实现大规模评估。

Abstract: Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.

</details>


### [3] [Pretraining Frame Preservation in Autoregressive Video Memory Compression](https://arxiv.org/abs/2512.23851)
*Lvmin Zhang,Shengqu Cai,Muyang Li,Chong Zeng,Beijia Lu,Anyi Rao,Song Han,Gordon Wetzstein,Maneesh Agrawala*

Main category: cs.CV

TL;DR: 提出PFP，一种将长视频压缩为短上下文的神经结构，通过显式预训练目标在任意时间位置保留单帧的高频细节；可将20秒视频压到约5k上下文长度，并能较好回忆随机帧外观；预训练模型可直接微调用作自回归视频模型的记忆编码器，实现低上下文开销的长历史记忆，文中做消融并讨论架构权衡。


<details>
  <summary>Details</summary>
Motivation: 长视频建模常受限于上下文长度与计算成本；现有压缩/记忆机制易丢失细粒度视觉细节，导致回放或检索质量差。作者希望在极低上下文成本下仍能保留任意时刻单帧的高频细节，以服务于长时视频生成/理解任务。

Method: 设计PFP神经网络作为视频到紧凑表征的压缩器，显式预训练目标针对“任意时间位置的单帧重建的高频保真度”（如感知与高频损失），使编码后的短上下文可支持随机帧的高保真重建/检索；随后将该预训练压缩器微调用作自回归视频模型的记忆编码器，为生成模型提供长历史记忆；并通过消融比较不同神经架构权衡。

Result: 基线可把约20秒视频压缩为约5k长度的上下文，同时在随机帧检索上保持感知一致的外观；作为记忆编码器微调后，实现低上下文成本下的长历史条件建模，且相对较低的保真度损失；消融显示不同架构在压缩率、重建保真度与计算之间存在权衡。

Conclusion: PFP能在显著压缩上下文的同时保留关键高频细节，适合作为长视频建模的记忆模块；在实际自回归视频模型中带来更长记忆与较低保真损失，但需要在压缩率与重建质量间做架构层面的权衡。

Abstract: We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.

</details>


### [4] [Lifelong Domain Adaptive 3D Human Pose Estimation](https://arxiv.org/abs/2512.23860)
*Qucheng Peng,Hongfei Xue,Pu Wang,Chen Chen*

Main category: cs.CV

TL;DR: 提出首个终身域自适应3D人体姿态估计框架：以GAN为核心，结合3D生成器、2D判别器与3D估计器，实现跨连续目标域适应与抗遗忘，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D HPE多依赖受控环境的标注3D数据，难以泛化到野外场景；已有域自适应/源自由自适应忽视目标域随时间变化（非平稳）的问题，容易在连续适应中灾难性遗忘。需要一种能在无法访问源域与历史目标域数据情况下，持续适应新域并保留旧知识的方案。

Method: 提出“终身域自适应3D HPE”任务与GAN框架：包含3D姿态生成器、2D姿态判别器与3D姿态估计器。通过对抗训练缓解域偏移并对齐原始与增强姿态；设计新的3D生成器范式，注入姿态感知、时序感知与域感知知识，用于当前域适应，同时通过生成与对齐机制缓解对既往域知识的遗忘。

Result: 在多个3D HPE域自适应数据集上进行广泛实验，所提方法在性能上优于现有基线与对比方法（论文称“显著/优越”）。

Conclusion: 终身域自适应视角下，结合GAN与多维知识注入的生成器能同时提升当前域适应与抗遗忘能力；该框架为3D HPE在真实、持续变化场景中的应用提供有效路径，并首开该任务研究。

Abstract: 3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.

</details>


### [5] [MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework](https://arxiv.org/abs/2512.23894)
*Krithika Iyer,Austin Tapp,Athelia Paulli,Gabrielle Dickerson,Syed Muhammad Anwar,Natasha Lepore,Marius George Linguraru*

Main category: cs.CV

TL;DR: 提出从婴幼儿T1加权MRI合成“无辐射”的颅骨sCT，并由此完成颅骨与颅缝分割；结果在结构相似度、感知质量与分割精度上接近真实CT，统计学等效成立。


<details>
  <summary>Details</summary>
Motivation: 儿童颅颌发育评估常依赖CT来观察骨与颅缝，但CT电离辐射对无明显异常的儿童不宜；MRI无辐射但难以显示骨/缝。需要一种把常规儿科MRI转化为具备CT骨信息的影像，以在无辐射前提下实现颅骨与颅缝的定量分析。

Method: 构建深度学习流水线：将0.2–2岁儿童的T1 MRI经域特化变分自编码器等模型生成合成CT（sCT）；在sCT上进行精细颅骨分割；基于模型输出的颅缝概率热图进一步得到直接颅缝分割。采用内部儿科数据训练与评估，并进行TOST等效性检验。

Result: sCT相对真实CT的结构相似度SSIM达99%，FID为1.01；7块颅骨的平均Dice为85%，颅缝Dice为80%；sCT与真实CT在颅骨与颅缝分割性能上经TOST检验等效（p<0.05）。

Conclusion: 该工作首次在儿科场景下从MRI合成可用于颅缝分割的sCT，尽管MRI对骨/缝显影有限。通过稳健、领域特化的VAE等模块，可从常规儿科MRI生成感知上与CT难以区分的颅骨sCT，为无创、无辐射的颅颌评估提供可行路径。

Abstract: Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.

</details>


### [6] [Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale](https://arxiv.org/abs/2512.23903)
*Charith Wickrema,Eliza Mace,Hunter Brown,Heidys Cabrera,Nick Krall,Matthew O'Neill,Shivangi Sarkar,Lowell Weissman,Eric Hughes,Guido Zarrella*

Main category: cs.CV

TL;DR: 论文探讨在极高分辨率电光（EO）遥感数据上训练基础模型的可扩展性与实践技巧，基于超过千万亿像素的数据与大型算力平台，系统评估大尺度ViT的训练成败模式与跨模态域迁移影响，结论指向当前仍处于数据受限而非参数受限的 regime，并给出对数据采集、算力预算与优化日程的指导。


<details>
  <summary>Details</summary>
Motivation: 多模态GenAI在图像字幕、检索、推理等任务中依赖强健的非文本编码器。自然图像领域因互联网级数据而形成了成熟的尺度定律，但在高价值且数据稀缺的遥感领域，这些规律并不清晰。作者希望建立可操作的扩展法则与训练实践，缩小遥感与自然图像领域在可扩展性理解上的差距。

Method: 利用MITRE Federal AI Sandbox与商业卫星EO数据（累计像素达千万亿级），按规模逐步训练更大的Vision Transformer（ViT）骨干；在“拍级”训练中记录成功与失败模式，量化模型、数据与算力的共同扩展关系，并分析跨遥感模态的域间差距与可迁移性。

Result: 即使在如此大规模训练下，性能表现更符合“数据受限”而非“参数受限”的特征；报告了在超大规模训练中的稳定性、优化与泛化的经验性发现，并指出跨模态与跨域迁移的影响因素。

Conclusion: 在高分辨率EO遥感上构建前沿级基础模型的关键瓶颈是高质量数据规模而非单纯增大模型参数。应优先优化数据采集/过滤策略，合理配置算力预算与训练调度，以更高效地推动RS基础模型的可扩展训练与跨模态普适能力。

Abstract: We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.

</details>


### [7] [Learning to learn skill assessment for fetal ultrasound scanning](https://arxiv.org/abs/2512.23920)
*Yipei Wang,Qianye Yang,Lior Drukker,Aris T. Papageorghiou,Yipeng Hu,J. Alison Noble*

Main category: cs.CV

TL;DR: 提出一种无需人工评分的双层优化框架，通过任务完成质量来自动评估胎儿超声操作技能，并在真实胎头扫描视频上验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有超声技能评估依赖专家主观打分且耗时；自动评估多为监督学习，受限于预设的技能要素与标签，难以全面、客观地反映操作水平。

Method: 构建双层优化（bi-level）框架：包含临床任务预测器与技能预测器两个网络，联合训练、相互细化。以临床任务在所得超声图像上的完成表现作为技能信号，无需手工定义技能评分。

Result: 在真实临床胎头超声视频数据上实验，证明该框架能够根据优化后的任务表现预测操作技能，显示出可行性与有效性。

Conclusion: 以任务完成度量化技能是可行的；联合优化的双网络框架为客观自动化超声技能评估提供了新途径，摆脱了人工预设评分与因素的限制。

Abstract: Traditionally, ultrasound skill assessment has relied on expert supervision and feedback, a process known for its subjectivity and time-intensive nature. Previous works on quantitative and automated skill assessment have predominantly employed supervised learning methods, often limiting the analysis to predetermined or assumed factors considered influential in determining skill levels. In this work, we propose a novel bi-level optimisation framework that assesses fetal ultrasound skills by how well a task is performed on the acquired fetal ultrasound images, without using manually predefined skill ratings. The framework consists of a clinical task predictor and a skill predictor, which are optimised jointly by refining the two networks simultaneously. We validate the proposed method on real-world clinical ultrasound videos of scanning the fetal head. The results demonstrate the feasibility of predicting ultrasound skills by the proposed framework, which quantifies optimised task performance as a skill indicator.

</details>


### [8] [MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation](https://arxiv.org/abs/2512.23936)
*Yulong Zou,Bo Liu,Cun-Jing Zheng,Yuan-ming Geng,Siyue Li,Qiankun Zuo,Shuihua Wang,Yudong Zhang,Jin Hong*

Main category: cs.CV

TL;DR: 提出MGML框架，通过元参数化自适应模态融合与一致性正则，在多模态MRI缺失场景下提升脑肿瘤分割；无需改动骨干架构，端到端训练，在BraTS2020/2023上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 临床中多模态MRI常缺失，现有多模态分割方法在模态不完整时性能显著下降，如何最大化利用不完整多模态信息并保持鲁棒与泛化是关键问题。

Method: 提出Meta-Guided Multi-Modal Learning（MGML）：1）Meta-AMF（元参数化自适应模态融合）根据当前可用模态生成自适应软标签监督，显式引导更一致的多模态融合，并适配不同输入组合；2）一致性正则模块，在不同模态可用性或扰动下促进预测一致，从而提升鲁棒与泛化；整体不改变原模型骨干，作为训练管线插件式集成，端到端优化。

Result: 在BraTS2020与BraTS2023上广泛实验，对比历年多种SOTA方法取得更好性能；BraTS2020在15种缺失模态组合的平均Dice上，相比基线分别达到WT 87.55、TC 79.36、ET 62.67；代码开源。

Conclusion: MGML能在模态缺失条件下有效融合信息并通过一致性正则提升鲁棒性与泛化，且无需改动架构、易集成，实验显著优于现有方法。

Abstract: Leveraging multimodal information from Magnetic Resonance Imaging (MRI) plays a vital role in lesion segmentation, especially for brain tumors. However, in clinical practice, multimodal MRI data are often incomplete, making it challenging to fully utilize the available information. Therefore, maximizing the utilization of this incomplete multimodal information presents a crucial research challenge. We present a novel meta-guided multi-modal learning (MGML) framework that comprises two components: meta-parameterized adaptive modality fusion and consistency regularization module. The meta-parameterized adaptive modality fusion (Meta-AMF) enables the model to effectively integrate information from multiple modalities under varying input conditions. By generating adaptive soft-label supervision signals based on the available modalities, Meta-AMF explicitly promotes more coherent multimodal fusion. In addition, the consistency regularization module enhances segmentation performance and implicitly reinforces the robustness and generalization of the overall framework. Notably, our approach does not alter the original model architecture and can be conveniently integrated into the training pipeline for end-to-end model optimization. We conducted extensive experiments on the public BraTS2020 and BraTS2023 datasets. Compared to multiple state-of-the-art methods from previous years, our method achieved superior performance. On BraTS2020, for the average Dice scores across fifteen missing modality combinations, building upon the baseline, our method obtained scores of 87.55, 79.36, and 62.67 for the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET), respectively. We have made our source code publicly available at https://github.com/worldlikerr/MGML.

</details>


### [9] [Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation](https://arxiv.org/abs/2512.23938)
*Hualin Ye,Bingxi Liu,Jixiang Du,Yu Qin,Ziyi Chen,Hong Zhang*

Main category: cs.CV

TL;DR: 提出一种改进的跨视角地理定位（CVGL）系统，通过DINOv2适配、通道重分配与MoE聚合提升在大视角差异下的特征对齐与检索效果，并以更少可训练参数在University-1652与SUES-200上取得竞争性能。


<details>
  <summary>Details</summary>
Motivation: 跨视角检索面临极大视角差异，导致特征聚合与对齐困难；现有方法在适应异构域、保持空间表征多样性与稳定性、以及高效聚合上仍存在不足。

Method: 1) 采用DINOv2主干并通过卷积适配器进行微调，增强对跨视域变化的适应性；2) 设计多尺度通道重分配模块，提升空间表征的多样性与稳定性；3) 在特征聚合中引入带MoE路由的改进聚合模块，于跨注意力中对keys/values动态选择专家子空间，实现对异构输入的自适应处理。

Result: 在University-1652与SUES-200数据集上进行大量实验，方法以更少可训练参数达到具有竞争力的检索/定位性能。

Conclusion: 结合DINOv2适配、通道重分配与MoE路由的聚合机制，可在显著视角差异条件下有效提升CVGL的特征对齐与检索性能，并兼具参数效率。

Abstract: Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.

</details>


### [10] [Kinematic-Based Assessment of Surgical Actions in Microanastomosis](https://arxiv.org/abs/2512.23942)
*Yan Meng,Daniel Donoho,Marcelle Altshuler,Omar Arnaout*

Main category: cs.CV

TL;DR: 提出一个在边缘设备上运行的AI框架，用于微血管吻合术中的动作分割与技能评估，结合YOLO+DeepSORT跟踪、基于自相似矩阵的无监督动作边界检测与聚类、以及监督分类评估手术手势熟练度；在58段专家标注视频上达成92.4%帧级分割准确率和85.5%技能分类准确率，支持客观、实时反馈与标准化培训。


<details>
  <summary>Details</summary>
Motivation: 微显微操作要求极高的注意力与精细运动，传统评估依赖专家评分与视频回放，主观、耗时且一致性差；需要自动化、可扩展、可在资源受限环境（边缘计算）运行的客观评估方案。

Method: 三模块架构：1) 目标器械尖端跟踪定位：YOLO检测+DeepSORT多目标跟踪；2) 动作分割：利用自相似矩阵（SSM）进行动作边界检测并做无监督聚类得到动作段；3) 监督分类：基于分割结果与轨迹特征训练分类器，输出手术手势熟练度/技能等级。系统设计为在边缘设备上高效运行。

Result: 在包含58段专家评分的微血管吻合术视频数据集上验证：帧级动作分割准确率92.4%；整体技能分类准确率85.5%，与专家评价较高一致。

Conclusion: 该方法能在显微外科训练中提供客观、实时的动作分割与技能评估，促进标准化、数据驱动的教学与胜任力评估，适用于高风险外科环境与边缘计算部署。

Abstract: Proficiency in microanastomosis is a critical surgical skill in neurosurgery, where the ability to precisely manipulate fine instruments is crucial to successful outcomes. These procedures require sustained attention, coordinated hand movements, and highly refined motor skills, underscoring the need for objective and systematic methods to evaluate and enhance microsurgical training. Conventional assessment approaches typically rely on expert raters supervising the procedures or reviewing surgical videos, which is an inherently subjective process prone to inter-rater variability, inconsistency, and significant time investment. These limitations highlight the necessity for automated and scalable solutions. To address this challenge, we introduce a novel AI-driven framework for automated action segmentation and performance assessment in microanastomosis procedures, designed to operate efficiently on edge computing platforms. The proposed system comprises three main components: (1) an object tip tracking and localization module based on YOLO and DeepSORT; (2) an action segmentation module leveraging self-similarity matrix for action boundary detection and unsupervised clustering; and (3) a supervised classification module designed to evaluate surgical gesture proficiency. Experimental validation on a dataset of 58 expert-rated microanastomosis videos demonstrates the effectiveness of our approach, achieving a frame-level action segmentation accuracy of 92.4% and an overall skill classification accuracy of 85.5% in replicating expert evaluations. These findings demonstrate the potential of the proposed method to provide objective, real-time feedback in microsurgical education, thereby enabling more standardized, data-driven training protocols and advancing competency assessment in high-stakes surgical environments.

</details>


### [11] [U-Net-Like Spiking Neural Networks for Single Image Dehazing](https://arxiv.org/abs/2512.23950)
*Huibin Li,Haoran Liu,Mingzhe Liu,Yulong Xiao,Peng Li,Guibin Zan*

Main category: cs.CV

TL;DR: 提出DehazeSNN：结合U-Net结构与脉冲神经网络（SNN），通过新OLIFBlock提升跨通道交互，在更小模型与更少MAC下实现与SOTA相当/更优的去雾效果。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法：基于物理模型方法受限；CNN难以建模长程依赖；Transformer虽能捕获全局信息但计算开销大。需要一种既能处理多尺度与长程依赖、又具高效低算力成本的架构。

Method: 构建U-Net式编码-解码框架，引入脉冲神经网络以事件/时序脉冲方式高效建模特征；提出Orthogonal Leaky-Integrate-and-Fire Block（OLIFBlock）加强跨通道信息交流与稳定性；整体在多尺度上同时处理局部与长程依赖，减少MAC与参数量。

Result: 在标准去雾基准上进行大量实验，性能与SOTA竞争甚至优于部分方法；同时模型更小、MAC更低，生成更清晰的无雾图像；提供开源实现。

Conclusion: DehazeSNN通过SNN+U-Net及OLIFBlock实现低计算成本下的高质量去雾，为在受资源限制场景中部署提供可行方案，并验证SNN在低层视觉任务中的潜力。

Abstract: Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.

</details>


### [12] [T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models](https://arxiv.org/abs/2512.23953)
*Changzhen Li,Yuecong Min,Jie Zhang,Zheng Yuan,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出T2VAttack，系统性研究文本到视频扩散模型在语义与时间维度的对抗脆弱性；通过替换/插入极少词即可显著破坏生成视频的语义一致性与时间连贯性。


<details>
  <summary>Details</summary>
Motivation: T2V扩散模型快速发展但其对抗鲁棒性几乎未被系统评估；视频具有动态属性，攻击不仅应影响语义对齐，还应破坏时间动态，因此需要从语义与时间两方面设计攻击与度量。

Method: 设计两类攻击目标：语义目标（评估文-视对齐）与时间目标（评估时序动态）。提出两种黑盒式/轻量提示级攻击：1) T2VAttack-S：通过贪心搜索识别语义或时间关键词并用同义词替换；2) T2VAttack-I：迭代插入优化词，尽量最小扰动。将目标与策略组合，在多款SOTA模型（ModelScope、CogVideoX、Open-Sora、HunyuanVideo）上评测。

Result: 实验证明即便仅替换或插入一个词，也能大幅降低生成视频的语义保真与时间连贯，表明现有T2V扩散模型对提示级微小扰动极为敏感。

Conclusion: 当前T2V模型在对抗鲁棒性方面存在关键缺陷；提示级微扰能有效攻击其语义与时间一致性，提示未来需要更稳健的训练、鲁棒解码与防御机制。

Abstract: The rapid evolution of Text-to-Video (T2V) diffusion models has driven remarkable advancements in generating high-quality, temporally coherent videos from natural language descriptions. Despite these achievements, their vulnerability to adversarial attacks remains largely unexplored. In this paper, we introduce T2VAttack, a comprehensive study of adversarial attacks on T2V diffusion models from both semantic and temporal perspectives. Considering the inherently dynamic nature of video data, we propose two distinct attack objectives: a semantic objective to evaluate video-text alignment and a temporal objective to assess the temporal dynamics. To achieve an effective and efficient attack process, we propose two adversarial attack methods: (i) T2VAttack-S, which identifies semantically or temporally critical words in prompts and replaces them with synonyms via greedy search, and (ii) T2VAttack-I, which iteratively inserts optimized words with minimal perturbation to the prompt. By combining these objectives and strategies, we conduct a comprehensive evaluation on the adversarial robustness of several state-of-the-art T2V models, including ModelScope, CogVideoX, Open-Sora, and HunyuanVideo. Our experiments reveal that even minor prompt modifications, such as the substitution or insertion of a single word, can cause substantial degradation in semantic fidelity and temporal dynamics, highlighting critical vulnerabilities in current T2V diffusion models.

</details>


### [13] [DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation](https://arxiv.org/abs/2512.23983)
*Yuang Jia,Jinlong Wang,Jiayi Zhao,Chunlam Li,Shunzhou Wang,Wei Gao*

Main category: cs.CV

TL;DR: 提出一种仅依赖图像（可选相机位姿）的自动驾驶场景视角外推方法：先估计静态/动态点云并融合，用可形变4D高斯重建场景，再用其退化渲染训练视频扩散模型，交替用扩散模型细化逐步偏移视角的高斯渲染并回灌训练，直至到达目标视角；在无昂贵先验下优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视角外推常依赖LiDAR、3D框、车道线等昂贵/费标注的先验，限制实用性。需要一种仅用图像即可在自动驾驶中生成高质量偏移视角图像的方案。

Method: 输入仅为图像（与可选相机位姿）。1）估计全局静态点云与逐帧动态点云并融合；2）用可形变4D Gaussian（4DGS）重建场景；3）用初始4DGS渲染得到质量较差的伪视频帧以训练视频扩散模型；4）逐步增加视角偏移，扩散模型对4DGS渲染进行修复/细化；5）将增强结果回灌以继续训练/优化4DGS；6）迭代进行直至到达目标外推视角。

Result: 在无需LiDAR/3D框/车道线等外部先验的条件下，于自动驾驶数据集上在外推视角生成的图像质量优于基线方法。

Conclusion: 仅用图像与可选位姿即可实现高质量视角外推；4DGS与视频扩散模型的循环强化能逐步稳定扩展视角范围，具有更强部署可行性。

Abstract: This paper presents an effective solution for view extrapolation in autonomous driving scenarios. Recent approaches focus on generating shifted novel view images from given viewpoints using diffusion models. However, these methods heavily rely on priors such as LiDAR point clouds, 3D bounding boxes, and lane annotations, which demand expensive sensors or labor-intensive labeling, limiting applicability in real-world deployment. In this work, with only images and optional camera poses, we first estimate a global static point cloud and per-frame dynamic point clouds, fusing them into a unified representation. We then employ a deformable 4D Gaussian framework to reconstruct the scene. The initially trained 4D Gaussian model renders degraded and pseudo-images to train a video diffusion model. Subsequently, progressively shifted Gaussian renderings are iteratively refined by the diffusion model,and the enhanced results are incorporated back as training data for 4DGS. This process continues until extrapolation reaches the target viewpoints. Compared with baselines, our method produces higher-quality images at novel extrapolated viewpoints.

</details>


### [14] [Anomaly detection in satellite imagery through temporal inpainting](https://arxiv.org/abs/2512.23986)
*Bertrand Rouet-Leduc,Claudia Hulbert*

Main category: cs.CV

TL;DR: 论文提出一种利用卫星时间序列“预测最后一帧并与观测差分”的深度学习异常检测方法，相比传统方法灵敏度和特异性显著提升，能以约三分之一阈值检测到地表突变（如地震裂缝）。


<details>
  <summary>Details</summary>
Motivation: 传统变化检测受大气噪声、季节性变化、传感器伪影影响，易产生误报/漏报。卫星多时序具有时间冗余，若能学到“无变化情况下应有的外观”，即可通过偏差揭示异常，提高灾害响应与环境监测效率与可靠性。

Method: 基于SATLAS基础模型构建时序图像修复（inpainting/预测）网络：用Sentinel-2前序影像预测最后一帧；在全球多气候与多地表类型样本上训练。推理时，将模型预测与真实最后一帧做差，作为异常显著图；与传统时间中值法、RX（Reed-Xiaoli）进行对比评估。

Result: 在2023土耳其-叙利亚地震案例（Tepehan裂谷）上验证：相较时间中值与RX，方法对地表裂缝的检测灵敏度和特异性更高，达到约3倍更低的检测阈值，能发现传统方法遗漏的异常。

Conclusion: 学习“无变化”预测并用偏差指示异常的思路，在多光谱、全球尺度上有效，显著提高突发地表变化检测能力，为自动化、全球化监测提供可行路径。

Abstract: Detecting surface changes from satellite imagery is critical for rapid disaster response and environmental monitoring, yet remains challenging due to the complex interplay between atmospheric noise, seasonal variations, and sensor artifacts. Here we show that deep learning can leverage the temporal redundancy of satellite time series to detect anomalies at unprecedented sensitivity, by learning to predict what the surface should look like in the absence of change. We train an inpainting model built upon the SATLAS foundation model to reconstruct the last frame of a Sentinel-2 time series from preceding acquisitions, using globally distributed training data spanning diverse climate zones and land cover types. When applied to regions affected by sudden surface changes, the discrepancy between prediction and observation reveals anomalies that traditional change detection methods miss. We validate our approach on earthquake-triggered surface ruptures from the 2023 Turkey-Syria earthquake sequence, demonstrating detection of a rift feature in Tepehan with higher sensitivity and specificity than temporal median or Reed-Xiaoli anomaly detectors. Our method reaches detection thresholds approximately three times lower than baseline approaches, providing a path towards automated, global-scale monitoring of surface changes from freely available multi-spectral satellite data.

</details>


### [15] [GCA-ResUNet: Medical Image Segmentation Using Grouped Coordinate Attention](https://arxiv.org/abs/2512.23990)
*Jun Ding,Shang Gao*

Main category: cs.CV

TL;DR: 提出GCA-ResUNet，通过轻量分组坐标注意力模块提升医学图像分割精度与效率，在Synapse与ACDC上优于Swin-UNet、TransUNet等，尤其擅长小结构边界分割。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net类CNN受限于局部感受野与同质化注意机制，难以建模长程依赖与多器官/低对比场景；Transformer具全局注意但算力与数据需求高，不利于资源受限临床部署。需要一种兼顾全局表征能力与计算效率的分割方法。

Method: 在ResUNet框架中引入轻量可插拔的分组坐标注意力（GCA）模块：将通道上下文按组解耦以刻画跨通道语义异质性，并引入方向感知的坐标编码（横/纵）以建模结构化空间依赖，从而在保持CNN高效性的同时增强全局表征。

Result: 在Synapse与ACDC基准上分别取得86.11%与92.64%的Dice，优于多种代表性CNN与Transformer方法（含Swin-UNet、TransUNet），对小型、边界复杂的解剖结构分割有稳定提升。

Conclusion: GCA-ResUNet在准确性与效率间取得良好权衡，能提升多器官与低对比区域的分割表现，具备临床应用的可行性与可扩展性。

Abstract: Accurate segmentation of heterogeneous anatomical structures is pivotal for computer-aided diagnosis and subsequent clinical decision-making. Although U-Net based convolutional neural networks have achieved remarkable progress, their intrinsic locality and largely homogeneous attention formulations often limit the modeling of long-range contextual dependencies, especially in multi-organ scenarios and low-contrast regions. Transformer-based architectures mitigate this issue by leveraging global self-attention, but they usually require higher computational resources and larger training data, which may impede deployment in resource-constrained clinical environments.In this paper, we propose GCA-ResUNet, an efficient medical image segmentation framework equipped with a lightweight and plug-and-play Grouped Coordinate Attention (GCA) module. The proposed GCA decouples channel-wise context modeling into multiple groups to explicitly account for semantic heterogeneity across channels, and integrates direction-aware coordinate encoding to capture structured spatial dependencies along horizontal and vertical axes. This design enhances global representation capability while preserving the efficiency advantages of CNN backbones. Extensive experiments on two widely used benchmarks, Synapse and ACDC, demonstrate that GCA-ResUNet achieves Dice scores of 86.11% and 92.64%, respectively, outperforming a range of representative CNN and Transformer-based methods, including Swin-UNet and TransUNet. In particular, GCA-ResUNet yields consistent improvements in delineating small anatomical structures with complex boundaries. These results indicate that the proposed approach provides a favorable trade-off between segmentation accuracy and computational efficiency, offering a practical and scalable solution for clinical deployment.

</details>


### [16] [Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation](https://arxiv.org/abs/2512.23997)
*Haotang Li,Zhenyu Qi,Hao Qin,Huanrui Yang,Sen He,Kebin Peng*

Main category: cs.CV

TL;DR: 提出GASeg：通过拓扑信息桥接几何与外观，实现更稳健的自监督语义分割；核心包括可微分盒计数(DBC)、拓扑增强(TopoAug)与跨模态对齐损失(GALoss)，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 自监督分割对阴影、眩光、纹理等外观噪声敏感，出现外观歧义时容易失败；需要更稳定、不依赖易变外观的结构性表征。拓扑与几何信息在多尺度上更稳定，可用于规避外观不确定性。

Method: 1) 两条并行分支：几何特征分支与外观特征分支；2) 可微盒计数(DBC)从多尺度上提取拓扑统计(如连通性/复杂度指标)；3) 拓扑增强(TopoAug)用形态学操作对输入制造结构性扰动，模拟真实歧义以对抗训练；4) 多目标损失GALoss，显式约束几何与外观分支在拓扑统计上的对齐与一致。

Result: 在COCO-Stuff、Cityscapes、PASCAL等四个基准上取得SOTA表现，显示方法在存在外观歧义的场景下更稳健。

Conclusion: 通过引入可微拓扑统计并用对抗性拓扑增强进行训练，GASeg能在几何与外观之间建立稳定对齐，从而提升自监督语义分割的鲁棒性与精度，并优于现有方法。

Abstract: Self-supervised semantic segmentation methods often fail when faced with appearance ambiguities. We argue that this is due to an over-reliance on unstable, appearance-based features such as shadows, glare, and local textures. We propose \textbf{GASeg}, a novel framework that bridges appearance and geometry by leveraging stable topological information. The core of our method is Differentiable Box-Counting (\textbf{DBC}) module, which quantifies multi-scale topological statistics from two parallel streams: geometric-based features and appearance-based features. To force the model to learn these stable structural representations, we introduce Topological Augmentation (\textbf{TopoAug}), an adversarial strategy that simulates real-world ambiguities by applying morphological operators to the input images. A multi-objective loss, \textbf{GALoss}, then explicitly enforces cross-modal alignment between geometric-based and appearance-based features. Extensive experiments demonstrate that GASeg achieves state-of-the-art performance on four benchmarks, including COCO-Stuff, Cityscapes, and PASCAL, validating our approach of bridging geometry and appearance via topological information.

</details>


### [17] [Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge](https://arxiv.org/abs/2512.23998)
*Tae Ha Park,Simone D'Amico*

Main category: cs.CV

TL;DR: 提出一种在空间交会近程操作中，从图像序列重建未知目标航天器3D结构的管线：以3D Gaussian Splatting表示几何与外观，并将太阳位置先验融入训练以应对快速变化光照，从而提升光度一致性并利于后续位姿估计。实验显示方法能学习全局阴影与自遮挡效应。


<details>
  <summary>Details</summary>
Motivation: 3DGS在静态场景假设下训练，难以处理空间任务中剧烈变化的光照；而用于位姿估计的光度优化要求渲染具有高光度准确性。因此需要一种能在动态照明下仍获得几何与光度可靠模型的方法。

Method: 以3DGS表示目标航天器几何与外观；在训练过程中将由服务星估计/维护的太阳方向先验注入渲染—优化管线，约束或调制辐照/着色以适配快速变化光照，从而提升渲染的光度一致性；训练完成的3DGS还用于通过光度误差进行相机位姿优化。

Result: 在包含剧烈照明变化的图像序列上，所提方案训练得到的3DGS能更好适应空间环境的动态光照，能够体现全局阴影与自遮挡，并提升与光度相关的渲染质量（从而有利于位姿估计）。

Conclusion: 融入太阳位置先验能显著增强3DGS在RPO场景下的光度建模能力与鲁棒性，兼顾几何精度与光度准确性，为后续基于渲染的相机位姿估计提供可靠基础。

Abstract: This work presents a novel pipeline to recover the 3D structure of an unknown target spacecraft from a sequence of images captured during Rendezvous and Proximity Operations (RPO) in space. The target's geometry and appearance are represented as a 3D Gaussian Splatting (3DGS) model. However, learning 3DGS requires static scenes, an assumption in contrast to dynamic lighting conditions encountered in spaceborne imagery. The trained 3DGS model can also be used for camera pose estimation through photometric optimization. Therefore, in addition to recovering a geometrically accurate 3DGS model, the photometric accuracy of the rendered images is imperative to downstream pose estimation tasks during the RPO process. This work proposes to incorporate the prior knowledge of the Sun's position, estimated and maintained by the servicer spacecraft, into the training pipeline for improved photometric quality of 3DGS rasterization. Experimental studies demonstrate the effectiveness of the proposed solution, as 3DGS models trained on a sequence of images learn to adapt to rapidly changing illumination conditions in space and reflect global shadowing and self-occlusion.

</details>


### [18] [Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis](https://arxiv.org/abs/2512.24013)
*Hao Wu,Hui Li,Yiyun Su*

Main category: cs.CV

TL;DR: 提出Hilbert‑VLM：以两阶段融合框架把3D医学分割与VLM分类联动，通过Hilbert空间填充曲线与改进的SAM2/SSM扫描、HMCA与尺度感知解码器，生成多模态增强提示引导VLM，提升分割与诊断表现（BraTS2021 Dice 82.35%、ACC 78.85%）。


<details>
  <summary>Details</summary>
Motivation: 3D多模态医学影像复杂，VLM常无法有效整合互补信息且易忽略细微病灶。需要结构化利用分割与语义线索，并在时空/空间序列建模中保持局部性，以便更精准诊断。

Method: 两阶段：1) HilbertMed-SAM用于精确病灶分割。对SAM2进行系统性重构：将Hilbert空间填充曲线引入Mamba SSM扫描以最大化3D空间局部性保持；提出Hilbert‑Mamba Cross‑Attention（HMCA）与尺度感知解码器捕获细粒度特征。2) 提示增强模块将分割掩膜与相应文本属性融合成信息密集的提示，引导VLM做疾病分类。

Result: 在BraTS2021分割基准上Dice=82.35%，同时诊断分类ACC=78.85%，多项实验表明Hilbert‑VLM在分割与分类上有效。

Conclusion: 通过Hilbert曲线驱动的SSM扫描、HMCA与尺度感知解码器，并将分割与文本融合为增强提示，Hilbert‑VLM提升了3D医学影像的分割与诊断准确性，展示了VLM在医疗分析中更高的可靠性与应用潜力。

Abstract: Recent studies suggest that Visual Language Models (VLMs) hold great potential for tasks such as automated medical diagnosis. However, processing complex three-dimensional (3D) multimodal medical images poses significant challenges - specifically, the effective integration of complementary information and the occasional oversight of subtle yet critical pathological features. To address these issues, we present a novel two-stage fusion framework termed Hilbert-VLM. This framework leverages the HilbertMed-SAM module for precise lesion segmentation, with the generated multimodal enhanced prompts then guiding the VLM toward accurate disease classification. Our key innovation lies in the systematic redesign of the Segment Anything Model 2 (SAM2) architecture: we incorporate Hilbert space-filling curves into the scanning mechanism of the Mamba State Space Model (SSM) to maximize the preservation of spatial locality in 3D data, a property critical for medical image analysis. We also introduce a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. Meanwhile, the prompt enhancement module unifies segmentation masks and their corresponding textual attributes into an information-dense prompt to support VLM inference. Extensive experiments were conducted to validate the effectiveness of the Hilbert-VLM model. On the BraTS2021 segmentation benchmark, it achieves a Dice score of 82.35 percent, with a diagnostic classification accuracy (ACC) of 78.85 percent. These results demonstrate that the proposed model offers substantial potential to improve the accuracy and reliability of medical VLM-based analysis.

</details>


### [19] [On Exact Editing of Flow-Based Diffusion Models](https://arxiv.org/abs/2512.24015)
*Zixiang Li,Yue Song,Jianing Peng,Ting Liu,Jun Huang,Xiaochao Qu,Luoqi Liu,Wei Wang,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出CVC框架，通过条件速度校正改进基于flow的扩散编辑，分解潜在轨迹为结构保持与语义引导两支路，并用经验贝叶斯/Tweedie校正补偿绝对速度误差，实现稳定、保真且可控的编辑。


<details>
  <summary>Details</summary>
Motivation: 现有flow-based diffusion编辑无需显式反演即可在源与目标分布间转换，但潜在轨迹会累积速度误差，导致语义不一致与结构失真；缺乏对速度场误差的建模与校正。

Method: 将编辑视作由已知源先验驱动的分布变换：1) 提出双视角速度转换，显式将潜在演化分解为结构保持分支（贴合源轨迹）与语义引导分支（朝向目标分布受控偏移）；2) 指出条件速度场相对真实轨迹存在绝对速度误差；3) 基于经验贝叶斯与Tweedie校正，对条件速度进行“后验一致”的时间递进式修正，抵消可量化偏差，稳定潜在动力学。

Result: 得到稳定、可解释的潜在动态，实现高保真重建与平滑的局部语义转换；在多种任务上较现有方法在保真度、语义对齐与编辑可靠性上更优。

Conclusion: CVC通过对条件速度的结构化分解与贝叶斯/Tweedie校正，有效抑制速度误差与轨迹漂移，兼顾结构保真与语义可控，提供可靠的flow-based扩散编辑方案。

Abstract: Recent methods in flow-based diffusion editing have enabled direct transformations between source and target image distribution without explicit inversion. However, the latent trajectories in these methods often exhibit accumulated velocity errors, leading to semantic inconsistency and loss of structural fidelity. We propose Conditioned Velocity Correction (CVC), a principled framework that reformulates flow-based editing as a distribution transformation problem driven by a known source prior. CVC rethinks the role of velocity in inter-distribution transformation by introducing a dual-perspective velocity conversion mechanism. This mechanism explicitly decomposes the latent evolution into two components: a structure-preserving branch that remains consistent with the source trajectory, and a semantically-guided branch that drives a controlled deviation toward the target distribution. The conditional velocity field exhibits an absolute velocity error relative to the true underlying distribution trajectory, which inherently introduces potential instability and trajectory drift in the latent space. To address this quantifiable deviation and maintain fidelity to the true flow, we apply a posterior-consistent update to the resulting conditional velocity field. This update is derived from Empirical Bayes Inference and Tweedie correction, which ensures a mathematically grounded error compensation over time. Our method yields stable and interpretable latent dynamics, achieving faithful reconstruction alongside smooth local semantic conversion. Comprehensive experiments demonstrate that CVC consistently achieves superior fidelity, better semantic alignment, and more reliable editing behavior across diverse tasks.

</details>


### [20] [FitControler: Toward Fit-Aware Virtual Try-On](https://arxiv.org/abs/2512.24016)
*Lu Yang,Yicheng Liu,Yanan Li,Xiang Bai,Hao Lu*

Main category: cs.CV

TL;DR: 提出FitControler，实现“版型贴合度(garment fit)”可控的虚拟试衣；包含布局生成与多尺度注入，并提供Fit4Men数据集与一致性指标，兼容多种VTON模型，能准确控制贴合度。


<details>
  <summary>Details</summary>
Motivation: 现有VTON多关注衣物细节保真，忽略整体风格关键因素——衣物与身体的贴合度。缺乏对不同贴合版型的显式建模与可控生成能力，也缺少数据与评测指标。

Method: 提出可插拔模块FitControler：1) 贴合感知布局生成器，基于服装无关表示重绘人体-服装布局以指定不同fit；2) 多尺度贴合注入器，将布局线索注入到VTON主干以实现布局驱动渲染。并构建Fit4Men数据集(1.3万对，上下装、多姿态、多机位)，以及两种贴合一致性度量。

Result: 与多种VTON模型结合均能实现准确的贴合度控制与真实渲染，在新提出的贴合一致性指标上表现优异，并在大量实验中验证兼容性与有效性。

Conclusion: 将“贴合度”纳入VTON生成流程，可通过FitControler实现可控的版型布局与渲染，提升整体风格协调与可控性；数据集与指标为后续研究提供基准。

Abstract: Realistic virtual try-on (VTON) concerns not only faithful rendering of garment details but also coordination of the style. Prior art typically pursues the former, but neglects a key factor that shapes the holistic style -- garment fit. Garment fit delineates how a garment aligns with the body of a wearer and is a fundamental element in fashion design. In this work, we introduce fit-aware VTON and present FitControler, a learnable plug-in that can seamlessly integrate into modern VTON models to enable customized fit control. To achieve this, we highlight two challenges: i) how to delineate layouts of different fits and ii) how to render the garment that matches the layout. FitControler first features a fit-aware layout generator to redraw the body-garment layout conditioned on a set of delicately processed garment-agnostic representations, and a multi-scale fit injector is then used to deliver layout cues to enable layout-driven VTON. In particular, we build a fit-aware VTON dataset termed Fit4Men, including 13,000 body-garment pairs of different fits, covering both tops and bottoms, and featuring varying camera distances and body poses. Two fit consistency metrics are also introduced to assess the fitness of generations. Extensive experiments show that FitControler can work with various VTON models and achieve accurate fit control. Code and data will be released.

</details>


### [21] [Structure-Guided Allocation of 2D Gaussians for Image Representation and Compression](https://arxiv.org/abs/2512.24018)
*Huanxiong Liang,Yunuo Chen,Yicheng Pan,Sixian Wang,Jincheng Dai,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 论文提出一种“结构引导”的2D 高斯喷溅图像表示与量化方法，在不牺牲毫秒级解码速度的前提下显著提升低码率下的率失真性能（Kodak 与 DIV2K 上相对基线BD-rate分别降43.44%与29.91%）。


<details>
  <summary>Details</summary>
Motivation: 现有2DGS在分配高斯数量/参数精度时与图像结构脱钩，导致低码率RD效率差、边缘/细节易受损。需要一种利用图像结构先验来同时指导表示容量和量化精度分配的方法。

Method: 1) 结构引导初始化：依据自然图像的空间/结构先验分配2D高斯，使其分布更局部且具语义。2) 量化感知微调：对协方差参数采用自适应位宽量化，小尺度且位于复杂区域的高斯给予更高精度，其余区域使用更低精度；端到端RD感知优化以减少冗余且不损伤边缘。3) 几何一致性正则：将高斯主轴方向与局部梯度方向对齐，强化结构细节保真。整个流程保持原生超高速解码（>1000 FPS）。

Result: 在Kodak与DIV2K数据集上相对GSImage基线显著提升RD性能：BD-rate分别降低43.44%与29.91%；同时维持>1000 FPS解码速度，并提高表示能力与边缘/结构保真度。

Conclusion: 将图像结构显式耦合到2DGS的容量分配与量化精度，可在不牺牲解码速度的情况下显著提升低码率RD效率与细节保真。方法通用且可作为2DGS图像压缩的有效改进方向。

Abstract: Recent advances in 2D Gaussian Splatting (2DGS) have demonstrated its potential as a compact image representation with millisecond-level decoding. However, existing 2DGS-based pipelines allocate representation capacity and parameter precision largely oblivious to image structure, limiting their rate-distortion (RD) efficiency at low bitrates. To address this, we propose a structure-guided allocation principle for 2DGS, which explicitly couples image structure with both representation capacity and quantization precision, while preserving native decoding speed. First, we introduce a structure-guided initialization that assigns 2D Gaussians according to spatial structural priors inherent in natural images, yielding a localized and semantically meaningful distribution. Second, during quantization-aware fine-tuning, we propose adaptive bitwidth quantization of covariance parameters, which grants higher precision to small-scale Gaussians in complex regions and lower precision elsewhere, enabling RD-aware optimization, thereby reducing redundancy without degrading edge quality. Third, we impose a geometry-consistent regularization that aligns Gaussian orientations with local gradient directions to better preserve structural details. Extensive experiments demonstrate that our approach substantially improves both the representational power and the RD performance of 2DGS while maintaining over 1000 FPS decoding. Compared with the baseline GSImage, we reduce BD-rate by 43.44% on Kodak and 29.91% on DIV2K.

</details>


### [22] [FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing](https://arxiv.org/abs/2512.24022)
*Yunkai Dang,Donghao Wang,Jiacheng Yang,Yifan Jiang,Meiyi Zhu,Yuekun Yang,Cong Wang,Qi Fan,Wenbin Li,Yang Gao*

Main category: cs.CV

TL;DR: 提出MF-RSVLM，通过多特征融合与循环视觉注入，提升遥感领域VLM在分类、图像描述与VQA上的表现。


<details>
  <summary>Details</summary>
Motivation: 通用VLM在遥感领域表现受限：遥感图像与自然图像差异大，现有方法难以抽取细粒度特征，且在深度语言生成过程中易出现“视觉遗忘”。

Method: 1) 多尺度视觉表示学习：联合全局上下文与局部细节，强调小目标与复杂结构捕获；2) 多特征融合：将不同层级/尺度的视觉特征有效融合用于语言对齐；3) 循环视觉特征注入：在生成过程中反复向语言模型注入视觉证据，缓解视觉遗忘并保持语义对齐。

Result: 在多种遥感基准上（分类、图像描述、VQA）达到SOTA或高度竞争的成绩，验证了方法的有效性与泛化性。

Conclusion: 多尺度+多特征融合配合循环注入能显著提升遥感VLM的细粒度理解与稳健生成，为遥感多模态建模提供有效范式。

Abstract: Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.

</details>


### [23] [RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations](https://arxiv.org/abs/2512.24023)
*Xingqi He,Yujie Zhang,Shuyong Gao,Wenjie Li,Lingyi Hong,Mingxi Chen,Kaixun Jiang,Jiyuan Fu,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 提出RSAgent：一种通过多轮推理与工具调用进行文本引导目标分割的代理式多模态大模型，能反复定位与细化掩码，在ReasonSeg与RefCOCOg上达SOTA零样本与总体性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为“一次性”定位后由外部分割器生成掩码，缺乏在初次定位错误时的验证、再聚焦和迭代改进能力，限制了跨模态推理与像素级落地的闭环。

Method: 设计代理式MLLM——RSAgent：在多轮交互中调用分割工具箱，基于视觉反馈与历史观察更新空间假设，不断重定位并细化掩码；构建多轮推理分割轨迹的数据生成流程；两阶段训练——先冷启动SFT，再进行带细粒度任务奖励的代理式强化学习。

Result: 在ReasonSeg测试集实现零样本gIoU 66.5%，较Seg-Zero-7B提升9%；在RefCOCOg上达到cIoU 81.5%，在域内与跨域基准上皆达SOTA。

Conclusion: 多轮推理+工具交互的代理化范式显著提升文本引导分割的鲁棒性与精度，可在错误初定位下自我修正并迭代优化，证明了代理式训练与细粒度奖励对跨模态像素级任务的有效性。

Abstract: Text-guided object segmentation requires both cross-modal reasoning and pixel grounding abilities. Most recent methods treat text-guided segmentation as one-shot grounding, where the model predicts pixel prompts in a single forward pass to drive an external segmentor, which limits verification, refocusing and refinement when initial localization is wrong. To address this limitation, we propose RSAgent, an agentic Multimodal Large Language Model (MLLM) which interleaves reasoning and action for segmentation via multi-turn tool invocations. RSAgent queries a segmentation toolbox, observes visual feedback, and revises its spatial hypothesis using historical observations to re-localize targets and iteratively refine masks. We further build a data pipeline to synthesize multi-turn reasoning segmentation trajectories, and train RSAgent with a two-stage framework: cold-start supervised fine-tuning followed by agentic reinforcement learning with fine-grained, task-specific rewards. Extensive experiments show that RSAgent achieves a zero-shot performance of 66.5% gIoU on ReasonSeg test, improving over Seg-Zero-7B by 9%, and reaches 81.5% cIoU on RefCOCOg, demonstrating state-of-the-art performance on both in-domain and out-of-domain benchmarks.

</details>


### [24] [PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing](https://arxiv.org/abs/2512.24026)
*Mustafa Munir,Md Mostafijur Rahman,Kartikeya Bhardwaj,Paul Whatmough,Radu Marculescu*

Main category: cs.CV

TL;DR: PipeFlow是一种可扩展的长视频编辑流水线：通过运动感知的帧跳过、段内并行的DDIM反演与联合编辑调度、以及神经插帧/边界平滑，实现编辑时间随视频长度线性增长，在保持质量的同时大幅提速（相较TokenFlow最高9.6倍、DMT最高31.7倍）。


<details>
  <summary>Details</summary>
Motivation: 长视频编辑需要对大量帧做DDIM反演与联合编辑，计算量随长度急剧膨胀，现有方法在显存与时间上难以扩展，且跨段一致性与低运动帧的冗余计算未被充分利用。

Method: 1) 运动分析：用SSIM与光流识别低运动帧并跳过其编辑；2) 流水线调度：将视频切分为多段，依据可用GPU显存并行执行DDIM反演与联合编辑；3) 神经插值：对被跳过的帧进行插值重建，并对相邻段的边界帧进行平滑，确保时空一致性。

Result: 在不牺牲编辑一致性的前提下，PipeFlow的总时延随视频长度近似线性增长，实现对超长视频的可扩展编辑；实验报告相较TokenFlow加速最高9.6×，相较DMT加速最高31.7×。

Conclusion: 通过运动感知的帧选择、分段并行流水线与神经插值，PipeFlow将长视频编辑的计算复杂度从随长度超线性增长转为线性扩展，使“无限长”视频编辑在工程上可行，并在现有基线之上显著提速。

Abstract: Long-form video editing poses unique challenges due to the exponential increase in the computational cost from joint editing and Denoising Diffusion Implicit Models (DDIM) inversion across extended sequences. To address these limitations, we propose PipeFlow, a scalable, pipelined video editing method that introduces three key innovations: First, based on a motion analysis using Structural Similarity Index Measure (SSIM) and Optical Flow, we identify and propose to skip editing of frames with low motion. Second, we propose a pipelined task scheduling algorithm that splits a video into multiple segments and performs DDIM inversion and joint editing in parallel based on available GPU memory. Lastly, we leverage a neural network-based interpolation technique to smooth out the border frames between segments and interpolate the previously skipped frames. Our method uniquely scales to longer videos by dividing them into smaller segments, allowing PipeFlow's editing time to increase linearly with video length. In principle, this enables editing of infinitely long videos without the growing per-frame computational overhead encountered by other methods. PipeFlow achieves up to a 9.6X speedup compared to TokenFlow and a 31.7X speedup over Diffusion Motion Transfer (DMT).

</details>


### [25] [Reinforced Diffusion: Learning to Push the Limits of Anisotropic Diffusion for Image Denoising](https://arxiv.org/abs/2512.24035)
*Xinran Qin,Yuhui Quan,Ruotao Xu,Hui Ji*

Main category: cs.CV

TL;DR: 提出一种基于深度Q学习的可训练各向异性扩散去噪框架，以学习扩散动作序列来提升对复杂结构的适应性，在三类常见噪声上优于传统扩散方法，并与代表性CNN方法竞争。


<details>
  <summary>Details</summary>
Motivation: 传统各向异性扩散采用手工设计/显式扩散算子，难以适应复杂图像结构，性能落后于学习型方法；需要一种既保留扩散物理直觉又具有数据驱动自适应性的去噪方案。

Method: 将去噪过程视为一系列“朴素扩散动作”的序列决策问题，使用深度Q学习学习在每次迭代中选择何种扩散动作及其顺序，从而形成一个随机的、随结构自适应的各向异性扩散过程。

Result: 在三种常见噪声类型的实验中，所提方法优于现有扩散类方法，并与代表性深度CNN去噪器达到可比性能。

Conclusion: 通过引入强化学习来调度扩散动作，可获得对图像结构更敏感的各向异性扩散去噪器，兼具传统扩散的可解释性与学习方法的性能，具有推广潜力。

Abstract: Image denoising is an important problem in low-level vision and serves as a critical module for many image recovery tasks. Anisotropic diffusion is a wide family of image denoising approaches with promising performance. However, traditional anisotropic diffusion approaches use explicit diffusion operators which are not well adapted to complex image structures. As a result, their performance is limited compared to recent learning-based approaches. In this work, we describe a trainable anisotropic diffusion framework based on reinforcement learning. By modeling the denoising process as a series of naive diffusion actions with order learned by deep Q-learning, we propose an effective diffusion-based image denoiser. The diffusion actions selected by deep Q-learning at different iterations indeed composite a stochastic anisotropic diffusion process with strong adaptivity to different image structures, which enjoys improvement over the traditional ones. The proposed denoiser is applied to removing three types of often-seen noise. The experiments show that it outperforms existing diffusion-based methods and competes with the representative deep CNN-based methods.

</details>


### [26] [Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval](https://arxiv.org/abs/2512.24064)
*Yizhi Liu,Ruitao Pu,Shilin Xu,Yingke Chen,Quan-Hui Liu,Yuan Sun*

Main category: cs.CV

TL;DR: 提出NIRNL框架，应对跨模态检索中的噪声标注：用CMP拉开正负对间距、用NIR按邻域共识细粒度划分样本并分别优化，在三数据集达到SOTA，噪声下鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 多模态大规模高质量标注难且易含噪，导致跨模态检索性能下降；现有鲁棒方法难以兼顾性能上限、标注校准可靠性与数据利用率。

Method: 1) 交叉模态边距保持（CMP）：直接调节正负样本对的相对距离，提升判别性；2) 基于邻域共识的实例精炼（NIR）：通过跨模态邻域一致性将样本划分为纯净子集、困难子集与噪声子集；3) 针对三类子集定制不同优化策略，既抑制噪声误导又最大化数据利用。

Result: 在三个基准数据集上取得SOTA表现；在高噪声率场景下展现出显著鲁棒性。

Conclusion: 通过CMP与NIR的协同，NIRNL在不牺牲数据利用率的前提下提升了跨模态检索的鲁棒性与上限表现，且在高噪声下仍稳定有效。

Abstract: In recent years, Cross-Modal Retrieval (CMR) has made significant progress in the field of multi-modal analysis. However, since it is time-consuming and labor-intensive to collect large-scale and well-annotated data, the annotation of multi-modal data inevitably contains some noise. This will degrade the retrieval performance of the model. To tackle the problem, numerous robust CMR methods have been developed, including robust learning paradigms, label calibration strategies, and instance selection mechanisms. Unfortunately, they often fail to simultaneously satisfy model performance ceilings, calibration reliability, and data utilization rate. To overcome the limitations, we propose a novel robust cross-modal learning framework, namely Neighbor-aware Instance Refining with Noisy Labels (NIRNL). Specifically, we first propose Cross-modal Margin Preserving (CMP) to adjust the relative distance between positive and negative pairs, thereby enhancing the discrimination between sample pairs. Then, we propose Neighbor-aware Instance Refining (NIR) to identify pure subset, hard subset, and noisy subset through cross-modal neighborhood consensus. Afterward, we construct different tailored optimization strategies for this fine-grained partitioning, thereby maximizing the utilization of all available data while mitigating error propagation. Extensive experiments on three benchmark datasets demonstrate that NIRNL achieves state-of-the-art performance, exhibiting remarkable robustness, especially under high noise rates.

</details>


### [27] [Pathology Context Recalibration Network for Ocular Disease Recognition](https://arxiv.org/abs/2512.24066)
*Zunjie Xiao,Xiaoqing Zhang,Risa Higashita,Jiang Liu*

Main category: cs.CV

TL;DR: 提出PCRNet：在DNN中引入病理上下文重标定(PRM)与专家先验引导(EPGA)，并配合集成损失(IL)，在多眼科数据集上优于现有注意力模型与损失方法且解释性更强。


<details>
  <summary>Details</summary>
Motivation: 现有眼科疾病自动识别的DNN虽性能不错，但缺乏对临床病理上下文与专家经验先验的显式建模，导致判别能力与可解释性受限。

Method: 1) 设计PRM：通过像素级上下文压缩与病理分布集中算子，显式利用病理上下文先验进行特征重标定；2) 设计EPGA：挖掘专家经验先验，突出关键像素级区域；3) 构建PCRNet：将PRM与EPGA插入现代DNN骨干；4) 提出IL集成损失：综合样本级损失分布与训练标签频率以缓解难样本与类不平衡问题。

Result: 在三个眼科数据集上，PCRNet+IL在识别性能上超过SOTA的注意力网络与先进损失函数。可视化显示PRM与EPGA能聚焦临床重要区域并影响决策路径。

Conclusion: 融合病理上下文与专家先验的PCRNet配合IL能显著提升眼科疾病识别精度与可解释性，验证了先验引入对临床AI的价值。

Abstract: Pathology context and expert experience play significant roles in clinical ocular disease diagnosis. Although deep neural networks (DNNs) have good ocular disease recognition results, they often ignore exploring the clinical pathology context and expert experience priors to improve ocular disease recognition performance and decision-making interpretability. To this end, we first develop a novel Pathology Recalibration Module (PRM) to leverage the potential of pathology context prior via the combination of the well-designed pixel-wise context compression operator and pathology distribution concentration operator; then this paper applies a novel expert prior Guidance Adapter (EPGA) to further highlight significant pixel-wise representation regions by fully mining the expert experience prior. By incorporating PRM and EPGA into the modern DNN, the PCRNet is constructed for automated ocular disease recognition. Additionally, we introduce an Integrated Loss (IL) to boost the ocular disease recognition performance of PCRNet by considering the effects of sample-wise loss distributions and training label frequencies. The extensive experiments on three ocular disease datasets demonstrate the superiority of PCRNet with IL over state-of-the-art attention-based networks and advanced loss methods. Further visualization analysis explains the inherent behavior of PRM and EPGA that affects the decision-making process of DNNs.

</details>


### [28] [Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images](https://arxiv.org/abs/2512.24074)
*Jingzhou Chen,Dexin Chen,Fengchao Xiong,Yuntao Qian,Liang Xiao*

Main category: cs.CV

TL;DR: 提出一种在DETR中结合“均衡层级对比损失”和“解耦学习策略”的方法，用于层级标注的细粒度遥感检测，缓解类不平衡并避免语义关系学习干扰定位，实验在三套数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 细粒度遥感数据常有层级标签，但如何把层级语义有效注入表示学习、同时提升细粒度检测仍困难：一是层级中的类分布极度不均，高频类主导学习；二是层级语义建模往往与类无关的定位目标冲突，影响检测质量。

Method: 在DETR框架内提出两点：1) 均衡层级对比损失（Balanced Hierarchical Contrastive Loss）：为每个层级引入可学习的类别原型，并在小批量内按层级对不同类别的梯度进行均衡，使每个层级的每个类对损失贡献相等；通过监督对比把同父类聚合、区分同级子类。2) 解耦学习策略：将DETR的object queries拆分为“分类查询”和“定位查询”，分别进行任务特化的特征提取与优化，减少语义关系学习对定位的干扰。

Result: 在三个具有层级注释的细粒度遥感数据集上取得了优于现有最先进方法的检测性能（总体mAP等指标提升，细粒度类别识别与定位更稳健）。

Conclusion: 通过在对比学习中均衡层级与类别贡献、并在DETR中解耦分类与定位，能够有效缓解数据不均衡与任务冲突，显著提升层级细粒度遥感目标检测效果。

Abstract: Fine-grained remote sensing datasets often use hierarchical label structures to differentiate objects in a coarse-to-fine manner, with each object annotated across multiple levels. However, embedding this semantic hierarchy into the representation learning space to improve fine-grained detection performance remains challenging. Previous studies have applied supervised contrastive learning at different hierarchical levels to group objects under the same parent class while distinguishing sibling subcategories. Nevertheless, they overlook two critical issues: (1) imbalanced data distribution across the label hierarchy causes high-frequency classes to dominate the learning process, and (2) learning semantic relationships among categories interferes with class-agnostic localization. To address these issues, we propose a balanced hierarchical contrastive loss combined with a decoupled learning strategy within the detection transformer (DETR) framework. The proposed loss introduces learnable class prototypes and equilibrates gradients contributed by different classes at each hierarchical level, ensuring that each hierarchical class contributes equally to the loss computation in every mini-batch. The decoupled strategy separates DETR's object queries into classification and localization sets, enabling task-specific feature extraction and optimization. Experiments on three fine-grained datasets with hierarchical annotations demonstrate that our method outperforms state-of-the-art approaches.

</details>


### [29] [RainFusion2.0: Temporal-Spatial Awareness and Hardware-Efficient Block-wise Sparse Attention](https://arxiv.org/abs/2512.24086)
*Aiyue Chen,Yaofu Liu,Junjian Huang,Guang Lian,Yiwu Yao,Wangli Lan,Jing Lin,Zhixin Ma,Tingting Zhou,Harry Yang*

Main category: cs.CV

TL;DR: 提出RainFusion2.0，一种在线自适应、硬件友好的稀疏注意力机制，通过块均值代理、时空感知重排和首帧下沉，在图像/视频生成中实现约80%稀疏度与1.5~1.8倍端到端加速且质量无损，并具备跨模型与跨硬件泛化。


<details>
  <summary>Details</summary>
Motivation: DiT在图像/视频生成中的注意力计算代价极高，限制部署；现有稀疏注意力方法多为GPU定制且需要额外稀疏模式预测开销，难以在ASIC等多样硬件上高效泛化。

Method: 设计RainFusion2.0在线稀疏注意力框架：1) 用块级均值作为代表token预测稀疏mask，降低预测成本；2) 进行时空感知token重排，强化重要性对齐与局部性；3) 在视频生成中引入首帧sink机制，稳定关键参考信息传播；整体面向多硬件实现低开销。

Result: 在多种生成模型与多硬件平台上验证：可达到约80%稀疏度，端到端加速1.5~1.8×，视频质量基本不下降，显示良好通用性与鲁棒性。

Conclusion: RainFusion2.0在不牺牲生成质量的前提下显著降低DiT推理成本，并以低预测开销与硬件通用设计克服现有稀疏注意力的两大瓶颈，适用于图像与视频生成的实际部署。

Abstract: In video and image generation tasks, Diffusion Transformer (DiT) models incur extremely high computational costs due to attention mechanisms, which limits their practical applications. Furthermore, with hardware advancements, a wide range of devices besides graphics processing unit (GPU), such as application-specific integrated circuit (ASIC), have been increasingly adopted for model inference. Sparse attention, which leverages the inherent sparsity of attention by skipping computations for insignificant tokens, is an effective approach to mitigate computational costs. However, existing sparse attention methods have two critical limitations: the overhead of sparse pattern prediction and the lack of hardware generality, as most of these methods are designed for GPU. To address these challenges, this study proposes RainFusion2.0, which aims to develop an online adaptive, hardware-efficient, and low-overhead sparse attention mechanism to accelerate both video and image generative models, with robust performance across diverse hardware platforms. Key technical insights include: (1) leveraging block-wise mean values as representative tokens for sparse mask prediction; (2) implementing spatiotemporal-aware token permutation; and (3) introducing a first-frame sink mechanism specifically designed for video generation scenarios. Experimental results demonstrate that RainFusion2.0 can achieve 80% sparsity while achieving an end-to-end speedup of 1.5~1.8x without compromising video quality. Moreover, RainFusion2.0 demonstrates effectiveness across various generative models and validates its generalization across diverse hardware platforms.

</details>


### [30] [Factorized Learning for Temporally Grounded Video-Language Models](https://arxiv.org/abs/2512.24097)
*Wenzheng Zeng,Difei Gao,Mike Zheng Shou,Hwee Tou Ng*

Main category: cs.CV

TL;DR: 他们提出D^2VLM，通过“先定位证据再基于证据回答”来提升事件级时间定位与文本回答的可靠性，并配套提出因子化偏好优化（FPO）与合成数据集，实验证明在多任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频-语言模型在事件级时间定位不准，且将时间定位与文本生成耦合训练，缺乏清晰逻辑结构，导致目标函数与学习过程次优；需要把“先找到时间证据、再生成回答”的层级关系显式化。

Method: 提出D^2VLM：1) 任务解耦但相互依赖的范式——先做时间证据定位（引入evidence tokens捕获事件级视觉语义，不仅是时间戳），再进行基于证据引用的回答；2) 提出因子化偏好优化FPO，将概率化的时间定位建模显式纳入偏好优化目标，使偏好学习同时覆盖定位与回答；3) 构建带显式时间标注的合成数据集以支撑因子化偏好学习。

Result: 在多种视频理解与时序定位相关任务上，D^2VLM显著优于现有方法（论文称“clear advantage”），展示了更准确的时间证据定位与更可靠的文本回答。

Conclusion: 把时间定位与文本回答因子化并以“先定位、后回答”的范式训练，再结合FPO能够提升事件级时序感知与回答质量；该思路与数据构建为视频语言模型提供了更合理的优化目标与训练流程。

Abstract: Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a "grounding then answering with evidence referencing" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.

</details>


### [31] [Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation](https://arxiv.org/abs/2512.24100)
*Yijie Qian,Juncheng Wang,Yuxiang Feng,Chao Xu,Wang Lu,Yang Liu,Baigui Sun,Yiqiang Chen,Yong Liu,Shujun Wang*

Main category: cs.CV

TL;DR: 论文提出将文本到动作生成从“一步到位”的直接翻译，转为“先思考再行动”的两阶段推理：先在语义对齐的低频潜空间规划，再在高频执行潜空间合成细节，从而缓解语言语义与运动学细节的不匹配，显著提升语义一致性与物理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有T2M多把离散语言直接映射到连续高频动作，简单动作有效，但在复杂语义下受制于“语义—运动学阻抗不匹配”：语言的稀疏离散、语义密度高，而动作数据连续致密、频率高；单次映射难以同时兼顾全局语义与物理细节。

Method: 提出Latent Motion Reasoning (LMR)：受层级运动控制启发，设计“两阶段Think-then-Act”架构与双粒度分词器。- 推理潜空间（Reasoning Latent）：压缩且语义丰富，用于自回归规划全局拓扑与粗轨迹。- 执行潜空间（Execution Latent）：高频细粒度，负责物理细节与帧级保真。在生成时先在推理潜空间规划，再条件化到执行潜空间实例化帧序列。LMR可无缝适配离散（T2M-GPT）与连续（MotionStreamer）基线。

Result: 在多项实验上，相比原基线，LMR在语义对齐与物理可实现性均取得显著提升，表明以学习到的、与运动对齐的概念空间作为规划载体优于直接以自然语言进行规划。

Conclusion: T2M生成更适合在分层潜空间内进行“先规划后执行”的系统2式推理。双粒度潜表示有效桥接语言与物理的鸿沟，提升可控性与真实性，验证了运动对齐的概念空间是更优的规划基质。

Abstract: Current state-of-the-art paradigms predominantly treat Text-to-Motion (T2M) generation as a direct translation problem, mapping symbolic language directly to continuous poses. While effective for simple actions, this System 1 approach faces a fundamental theoretical bottleneck we identify as the Semantic-Kinematic Impedance Mismatch: the inherent difficulty of grounding semantically dense, discrete linguistic intent into kinematically dense, high-frequency motion data in a single shot. In this paper, we argue that the solution lies in an architectural shift towards Latent System 2 Reasoning. Drawing inspiration from Hierarchical Motor Control in cognitive science, we propose Latent Motion Reasoning (LMR) that reformulates generation as a two-stage Think-then-Act decision process. Central to LMR is a novel Dual-Granularity Tokenizer that disentangles motion into two distinct manifolds: a compressed, semantically rich Reasoning Latent for planning global topology, and a high-frequency Execution Latent for preserving physical fidelity. By forcing the model to autoregressively reason (plan the coarse trajectory) before it moves (instantiates the frames), we effectively bridge the ineffability gap between language and physics. We demonstrate LMR's versatility by implementing it for two representative baselines: T2M-GPT (discrete) and MotionStreamer (continuous). Extensive experiments show that LMR yields non-trivial improvements in both semantic alignment and physical plausibility, validating that the optimal substrate for motion planning is not natural language, but a learned, motion-aligned concept space. Codes and demos can be found in \hyperlink{https://chenhaoqcdyq.github.io/LMR/}{https://chenhaoqcdyq.github.io/LMR/}

</details>


### [32] [Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks](https://arxiv.org/abs/2512.24111)
*Yongtao Chen,Yanbo Wang,Wentao Zhao,Guole Shen,Tianchen Deng,Jingchuan Wang*

Main category: cs.CV

TL;DR: 提出一种无需训练的生成式对抗攻击框架，用扩散模型生成与场景一致的自然物体，针对单目深度估计在自动驾驶中的脆弱性，显著提升攻击效果、隐蔽性与物理可部署性。


<details>
  <summary>Details</summary>
Motivation: 现有物理对抗攻击大多依赖纹理贴片，位置受限、真实感不足，难以在复杂道路场景中稳定生效；而MDE错误会影响下游决策，带来安全风险，亟需更隐蔽、可部署的攻击来评估与提升系统鲁棒性。

Method: 提出训练免疫的扩散式条件生成攻击：1) Salient Region Selection自动选取对MDE最敏感的区域作为插入位置；2) 采用Jacobian Vector Product Guidance，用目标MDE模型的对抗梯度与扩散模型的生成方向对齐，确保扰动落在扩散先验支持的“自然”流形上；生成自然、场景一致的对抗物体并插入图像/物理场景。

Result: 在数字与物理实验中，相比现有贴片类方法，该方法产生更大的深度偏移，同时更隐蔽、更稳定，且更易于在真实世界部署。

Conclusion: 利用扩散模型先验与梯度引导，可在无需训练的条件下生成物理合理且隐蔽的对抗物体，有效攻破MDE并提供更贴近实际的自动驾驶安全评测基准。

Abstract: Monocular Depth Estimation (MDE) serves as a core perception module in autonomous driving systems, but it remains highly susceptible to adversarial attacks. Errors in depth estimation may propagate through downstream decision making and influence overall traffic safety. Existing physical attacks primarily rely on texture-based patches, which impose strict placement constraints and exhibit limited realism, thereby reducing their effectiveness in complex driving environments. To overcome these limitations, this work introduces a training-free generative adversarial attack framework that generates naturalistic, scene-consistent adversarial objects via a diffusion-based conditional generation process. The framework incorporates a Salient Region Selection module that identifies regions most influential to MDE and a Jacobian Vector Product Guidance mechanism that steers adversarial gradients toward update directions supported by the pre-trained diffusion model. This formulation enables the generation of physically plausible adversarial objects capable of inducing substantial adversarial depth shifts. Extensive digital and physical experiments demonstrate that our method significantly outperforms existing attacks in effectiveness, stealthiness, and physical deployability, underscoring its strong practical implications for autonomous driving safety assessment.

</details>


### [33] [GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation](https://arxiv.org/abs/2512.24119)
*Yuan Feng,Yue Yang,Xiaohan He,Jiatong Zhao,Jianlong Chen,Zijun Chen,Daocheng Fu,Qi Liu,Renqiu Xia,Bo Zhang,Junchi Yan*

Main category: cs.CV

TL;DR: GeoBench提出一个分层几何推理基准（四个层级），用形式化生成与验证的六类任务评估VLM的几何解题过程，发现随复杂度上升性能明显下降，子目标分解与无关前提过滤至关重要，某些任务中CoT反而拖累表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM几何评测存在三大问题：可能被教材数据污染、只看最终答案忽视过程、缺乏细粒度诊断。因此需要一个可分层、过程导向、可控且可验证的基准来精准刻画几何推理能力缺口。

Method: 构建GeoBench：以四层推理阶梯（视觉感知、目标导向规划、严格定理应用、自我反思回溯），使用TrustGeoGen生成并形式验证六种任务，覆盖从属性抽取到逻辑纠错；系统对比推理型模型与通用MLLM，分析子目标分解、无关前提过滤、以及CoT提示等策略的影响。

Result: 推理型模型（如OpenAI-o3）整体优于通用MLLM，但任务难度提升时性能显著下降；子目标分解与无关前提过滤显著提升最终正确率；在部分任务中，链式思维提示反而降低表现。

Conclusion: GeoBench提供全面、可诊断的几何推理评测框架，揭示当前VLM在高复杂度几何推理上的瓶颈，并给出改进方向：强化子目标规划、增强无关信息过滤与过程严谨性，谨慎使用CoT。

Abstract: Geometric problem solving constitutes a critical branch of mathematical reasoning, requiring precise analysis of shapes and spatial relationships. Current evaluations of geometric reasoning in vision-language models (VLMs) face limitations, including the risk of test data contamination from textbook-based benchmarks, overemphasis on final answers over reasoning processes, and insufficient diagnostic granularity. To address these issues, we present GeoBench, a hierarchical benchmark featuring four reasoning levels in geometric problem-solving: Visual Perception, Goal-Oriented Planning, Rigorous Theorem Application, and Self-Reflective Backtracking. Through six formally verified tasks generated via TrustGeoGen, we systematically assess capabilities ranging from attribute extraction to logical error correction. Experiments reveal that while reasoning models like OpenAI-o3 outperform general MLLMs, performance declines significantly with increasing task complexity. Key findings demonstrate that sub-goal decomposition and irrelevant premise filtering critically influence final problem-solving accuracy, whereas Chain-of-Thought prompting unexpectedly degrades performance in some tasks. These findings establish GeoBench as a comprehensive benchmark while offering actionable guidelines for developing geometric problem-solving systems.

</details>


### [34] [Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design](https://arxiv.org/abs/2512.24120)
*Chandini Vysyaraju,Raghuvir Duvvuri,Avi Goyal,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 论文探讨用大语言模型（LLM）自动生成计算机视觉网络结构，提出少样本架构提示（FSAP）与空白归一化哈希校验两项方法，在7个视觉数据集上大规模验证，给出可复现实用的评估与去重流程。


<details>
  <summary>Details</summary>
Motivation: NAS代价高、任务多样且算力受限，亟需低成本但有效的自动化架构设计手段。现有将LLM用于架构生成的研究缺乏系统性的提示工程探索与轻量级去重/校验策略，尤其在计算机视觉领域。

Method: 1) Few-Shot Architecture Prompting（FSAP）：系统比较支持示例数n=1..6对LLM生成视觉模型架构的影响；2) Whitespace-Normalized Hash Validation：将生成代码做空白规范化后哈希，毫秒级检测重复，替代耗时的AST解析；3) 在MNIST、CIFAR-10/100、CelebA、ImageNette、SVHN、Places365七个基准上大规模生成并训练模型；4) 引入“数据集均衡”的跨任务评测方法，公平比较异构任务下的架构表现。

Result: 发现n=3的少样本提示在“多样性与上下文聚焦”间最优；哈希去重较AST解析加速约100倍(<1ms)，避免重复训练；共得到约1900个独特架构，并在多基准上完成系统评测。

Conclusion: LLM可作为低成本的架构搜索替代方案；FSAP为提示示例数提供实证指南（推荐n=3），哈希去重显著提升流水线效率；提出的数据集均衡评测提升了跨任务比较的严谨性，为算力受限研究者提供可操作的自动化设计流程。

Abstract: Automated neural network architecture design remains a significant challenge in computer vision. Task diversity and computational constraints require both effective architectures and efficient search methods. Large Language Models (LLMs) present a promising alternative to computationally intensive Neural Architecture Search (NAS), but their application to architecture generation in computer vision has not been systematically studied, particularly regarding prompt engineering and validation strategies. Building on the task-agnostic NNGPT/LEMUR framework, this work introduces and validates two key contributions for computer vision. First, we present Few-Shot Architecture Prompting (FSAP), the first systematic study of the number of supporting examples (n = 1, 2, 3, 4, 5, 6) for LLM-based architecture generation. We find that using n = 3 examples best balances architectural diversity and context focus for vision tasks. Second, we introduce Whitespace-Normalized Hash Validation, a lightweight deduplication method (less than 1 ms) that provides a 100x speedup over AST parsing and prevents redundant training of duplicate computer vision architectures. In large-scale experiments across seven computer vision benchmarks (MNIST, CIFAR-10, CIFAR-100, CelebA, ImageNette, SVHN, Places365), we generated 1,900 unique architectures. We also introduce a dataset-balanced evaluation methodology to address the challenge of comparing architectures across heterogeneous vision tasks. These contributions provide actionable guidelines for LLM-based architecture search in computer vision and establish rigorous evaluation practices, making automated design more accessible to researchers with limited computational resources.

</details>


### [35] [Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning](https://arxiv.org/abs/2512.24146)
*Chubin Chen,Sujie Hu,Jiashu Zhu,Meiqi Wu,Jintao Chen,Yanxun Li,Nisha Huang,Chengyu Fang,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: 论文发现并命名了偏好模式坍缩（PMC）：基于人类反馈强化学习对文生图扩散模型对齐时，模型为追求奖励而收敛到少数高分但单一风格/过曝等狭窄输出，牺牲多样性。作者提出用于量化PMC的基准DivGenBench，并提出方向解耦对齐（D^2-Align）：在冻结的奖励模型嵌入空间中学习一个方向性校正向量，用以在优化时修正奖励信号，抑制过度沿偏置方向优化。实验证明在兼顾质量与多样性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF对齐方法虽提升自动化偏好分数，但出现“奖励黑客”式的多样性崩塌（单一风格、过度曝光等），说明仅用奖励最大化会放大奖励模型偏置，亟需既保质量又保多样性的对齐方案与可测度基准。

Method: 1) 定义并形式化PMC；2) 提出DivGenBench用于评测PMC程度（质量与多样性维度的定量指标与任务）；3) 提出D^2-Align：冻结奖励模型，基于其嵌入学习一个方向性校正（directional correction），在训练生成模型时对奖励进行方向加权/减敏，避免沿偏置方向过优化，从而维持多样性；4) 结合定性与定量评测。

Result: 在多个文生图设置中，D^2-Align相较现有RLHF对齐方法显著降低PMC，提高生成多样性，同时保持或提升人类偏好与自动化奖励分数；在DivGenBench上取得更优的质量-多样性兼顾表现。

Conclusion: PMC是RLHF对齐文生图模型中的系统性问题；通过在奖励嵌入空间进行方向性校正，可抑制奖励黑客导致的坍缩。D^2-Align在不牺牲质量的前提下提升多样性，并为未来对齐方法与评测提供新范式与基准。

Abstract: Recent studies have demonstrated significant progress in aligning text-to-image diffusion models with human preference via Reinforcement Learning from Human Feedback. However, while existing methods achieve high scores on automated reward metrics, they often lead to Preference Mode Collapse (PMC)-a specific form of reward hacking where models converge on narrow, high-scoring outputs (e.g., images with monolithic styles or pervasive overexposure), severely degrading generative diversity. In this work, we introduce and quantify this phenomenon, proposing DivGenBench, a novel benchmark designed to measure the extent of PMC. We posit that this collapse is driven by over-optimization along the reward model's inherent biases. Building on this analysis, we propose Directional Decoupling Alignment (D$^2$-Align), a novel framework that mitigates PMC by directionally correcting the reward signal. Specifically, our method first learns a directional correction within the reward model's embedding space while keeping the model frozen. This correction is then applied to the reward signal during the optimization process, preventing the model from collapsing into specific modes and thereby maintaining diversity. Our comprehensive evaluation, combining qualitative analysis with quantitative metrics for both quality and diversity, reveals that D$^2$-Align achieves superior alignment with human preference.

</details>


### [36] [Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset](https://arxiv.org/abs/2512.24160)
*TsaiChing Ni,ZhenQi Chen,YuanFu Yang*

Main category: cs.CV

TL;DR: IMDD-1M 提供首个百万级工业多模态缺陷数据集及其上训练的扩散式视觉-语言基础模型，支持多任务并以极少下游数据达到与专家模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 工业质检场景多样、缺陷类型繁多且数据昂贵，现有多模态数据规模与通用模型在工业上的适配不足，限制了跨任务、跨材料的泛化与数据高效迁移。

Method: 构建包含100万对图像-文本的高分辨率真实工业缺陷数据（60+材料、400+缺陷类型），配以专家校验与细粒度文本（位置、严重度、上下文）。在此之上从零训练面向工业的扩散式视觉-语言基础模型，并支持通过轻量微调适配特定任务/领域。

Result: 该基础模型在仅使用不到5%任务特定数据的情况下，可达到与专用专家模型相当的性能，覆盖分类、分割、检索、字幕与生成等任务。

Conclusion: 大规模、细粒度的工业多模态数据结合扩散式VL基础模型，可实现数据高效、可扩展、可迁移的工业质检与生成智能，为面向制造的可泛化与知识驱动方法铺路。

Abstract: We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.

</details>


### [37] [Bayesian Self-Distillation for Image Classification](https://arxiv.org/abs/2512.24162)
*Anton Adelöw,Matteo Gamba,Atsuto Maki*

Main category: cs.CV

TL;DR: 提出BSD：用模型自身预测做贝叶斯推断，生成样本自适应软目标，替代硬标签；在多架构/数据集上提升准确率、显著降低校准误差，并增强对腐蚀、扰动与噪声标签的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 硬标签训练导致过度自信、较差的校准与泛化；现有自蒸馏虽利用模型预测信息，但仍依赖硬标签，限制效果。需要一种不依赖硬标签、能为每个样本构造更合适目标分布的方法。

Method: 提出Bayesian Self-Distillation：以模型自身预测为观测，进行贝叶斯推断得到样本特定的后验目标分布，训练期间持续用该软目标替代硬标签；并可与对比损失结合以增强对噪声的鲁棒性。

Result: 在CIFAR-100上ResNet-50测试准确率提升约+1.4%；ECE显著下降（如ResNet-50 -40%）；在多种网络与数据集上稳定优于保持架构不变的自蒸馏基线。

Conclusion: BSD提供了一个不再依赖硬标签的自蒸馏框架，兼顾更好的准确率、校准与鲁棒性；与对比学习结合可在噪声标签场景下达到单阶段单网络SOTA鲁棒性。

Abstract: Supervised training of deep neural networks for classification typically relies on hard targets, which promote overconfidence and can limit calibration, generalization, and robustness. Self-distillation methods aim to mitigate this by leveraging inter-class and sample-specific information present in the model's own predictions, but often remain dependent on hard targets, reducing their effectiveness. With this in mind, we propose Bayesian Self-Distillation (BSD), a principled method for constructing sample-specific target distributions via Bayesian inference using the model's own predictions. Unlike existing approaches, BSD does not rely on hard targets after initialization. BSD consistently yields higher test accuracy (e.g. +1.4% for ResNet-50 on CIFAR-100) and significantly lower Expected Calibration Error (ECE) (-40% ResNet-50, CIFAR-100) than existing architecture-preserving self-distillation methods for a range of deep architectures and datasets. Additional benefits include improved robustness against data corruptions, perturbations, and label noise. When combined with a contrastive loss, BSD achieves state-of-the-art robustness under label noise for single-stage, single-network methods.

</details>


### [38] [DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models](https://arxiv.org/abs/2512.24165)
*Zefeng He,Xiaoye Qu,Yafu Li,Tong Zhu,Siyuan Huang,Yu Cheng*

Main category: cs.CV

TL;DR: 提出DiffThinker：把多模态推理重塑为原生“图到图”的生成过程，用扩散模型进行推理，在视觉中心、长链条任务上优于MLLMs。四大特性：高效、可控、原生并行、可协作。实验覆盖规划、组合优化、约束满足、空间配置，显著超越多个闭源与开源强基线。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM推理以文本为主，难以在强视觉、长时序、多约束的任务中保持空间精度与逻辑一致性，导致性能受限。需要一种更贴近视觉本质的推理范式。

Method: 提出“生成式多模态推理”范式与DiffThinker框架：将推理表述为图像到图像的生成问题，采用扩散模型在视觉空间中迭代“思考”。系统比较该范式与MLLM，提炼出效率、可控性、原生并行性和协作性四个内在属性。

Result: 在四类任务（顺序规划、组合优化、约束满足、空间配置）上做广泛实验。相对领先闭源模型（如GPT-5、Gemini-3-Flash）与微调的Qwen3-VL-32B大幅提升，报告的平均改进达+314.2%、+111.6%、+39.0%。

Conclusion: 将推理转化为原生生成的图到图过程，能强化视觉中心任务的逻辑与空间一致性；生成式多模态推理是有前景的方向，DiffThinker展示了效率与可控并行的优势，并在多领域显著超越主流MLLM基线。

Abstract: While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.

</details>


### [39] [Deep Global Clustering for Hyperspectral Image Segmentation: Concepts, Applications, and Open Challenges](https://arxiv.org/abs/2512.24172)
*Yu-Tang Chang,Pin-Wei Chen,Shih-Fang Chen*

Main category: cs.CV

TL;DR: 提出DGC框架：在内存受限下，通过局部补丁学习全局聚类结构，实现HSI分割与无监督病害检测，训练快但易因多目标损失失衡导致聚类过合并与不稳定。


<details>
  <summary>Details</summary>
Motivation: 现有HSI分析受大数据量内存瓶颈限制；通用遥感预训练模型对近距农业场景迁移性差（光谱/尺度/语义差异）。需要无需预训练、能在消费级硬件上训练、且能把局部观测汇聚为全局语义的方案。

Method: Deep Global Clustering（DGC）：将大图划分为小重叠补丁，利用重叠区域一致性约束与多目标损失学习全局聚类结构；常量内存占用、<30分钟训练；通过语义粒度可导航地进行无监督分割/检测。

Result: 在叶片病害数据集上，实现背景-组织分离（mIoU=0.925），并展示无监督病害检测能力；但训练存在不稳定性，表现为特征空间聚类逐步过度合并，性能退化。

Conclusion: DGC概念具有价值，可在资源受限环境下快速训练并取得良好起点，但当前实现受多目标损失权重动态平衡问题制约，需更有原则的自适应/动态损失平衡策略以达稳定、可复现的性能；代码与数据已开源。

Abstract: Hyperspectral imaging (HSI) analysis faces computational bottlenecks due to massive data volumes that exceed available memory. While foundation models pre-trained on large remote sensing datasets show promise, their learned representations often fail to transfer to domain-specific applications like close-range agricultural monitoring where spectral signatures, spatial scales, and semantic targets differ fundamentally. This report presents Deep Global Clustering (DGC), a conceptual framework for memory-efficient HSI segmentation that learns global clustering structure from local patch observations without pre-training. DGC operates on small patches with overlapping regions to enforce consistency, enabling training in under 30 minutes on consumer hardware while maintaining constant memory usage. On a leaf disease dataset, DGC achieves background-tissue separation (mean IoU 0.925) and demonstrates unsupervised disease detection through navigable semantic granularity. However, the framework suffers from optimization instability rooted in multi-objective loss balancing: meaningful representations emerge rapidly but degrade due to cluster over-merging in feature space. We position this work as intellectual scaffolding - the design philosophy has merit, but stable implementation requires principled approaches to dynamic loss balancing. Code and data are available at https://github.com/b05611038/HSI_global_clustering.

</details>


### [40] [Guiding a Diffusion Transformer with the Internal Dynamics of Itself](https://arxiv.org/abs/2512.24176)
*Xingyu Zhou,Qifan Li,Xiaobin Hu,Hai Chen,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出一种用于扩散模型的新型“内部引导”(Internal Guidance, IG)策略：训练时在中间层加入辅助监督，采样时外推中深层输出，从而在无需额外训练/退化设计/额外采样步的前提下提升质量与效率。于ImageNet 256×256上，SiT‑XL/2+IG在80/800轮取得FID 5.31/1.75；LightningDiT‑XL/1+IG达FID 1.34，结合CFG达SOTA FID 1.19。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能覆盖整体（条件）数据分布，但在低概率区域因训练与数据不足难以学习，导致生成这类样本质量差。现有采样阶段的CFG可将采样引向高概率区，但常致过度简化或失真；用“坏模型”引导的方法又依赖精心退化策略、额外训练与采样成本。亟需一种既能提升质量又不引入高额额外成本的引导方式。

Method: 提出Internal Guidance（IG）：1）训练阶段在模型的中间层引入辅助监督（auxiliary supervision），鼓励中间表示更可控、更语义一致；2）采样阶段对中深层输出进行外推（extrapolation），将其作为生成过程的引导信号，从而在不增加或极少增加采样步骤的情况下提高样本质量与稳定性；3）该策略与现有基线/架构（如SiT、LightningDiT）和CFG兼容。

Result: 在ImageNet 256×256：SiT‑XL/2+IG于80与800轮训练分别达到FID 5.31与1.75；LightningDiT‑XL/1+IG取得FID 1.34；与CFG结合进一步达到当前最优的FID 1.19。除质量外，作者声称训练效率也显著提升。

Conclusion: IG通过在中间层加监督并在采样时外推中深层输出，提供了一种简单、通用且高效的引导策略，缓解CFG导致的过简化/失真与“坏模型引导”的高成本问题，在标准基准上取得SOTA或接近SOTA的FID，同时具备良好的兼容性与效率提升潜力。

Abstract: The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.

</details>


### [41] [PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds](https://arxiv.org/abs/2512.24193)
*Pieter M. Blok,Haozhou Wang,Hyun Kwon Suh,Peicheng Wang,James Burridge,Wei Guo*

Main category: cs.CV

TL;DR: 提出PointRAFT，一种从部分点云直接回归马铃薯块茎重量的高吞吐3D网络；通过高度嵌入缓解自遮挡导致的体量低估，精度高、速度快，优于线性回归与PointNet++。


<details>
  <summary>Details</summary>
Motivation: 收获机上的RGB-D点云因自遮挡不完整，传统做法重建完整几何困难且会系统性低估块茎重量；亟需一种在实用工况下，能在不重建完整形状的前提下，直接从部分点云稳健估计重量的方法，满足商业化收获的高吞吐需求。

Method: 提出PointRAFT：以部分点云为输入的端到端回归网络，直接输出连续3D形状属性（如重量），而非重建几何。核心创新是“对象高度嵌入”，将块茎高度作为额外几何线索融合以提升估计。以四品种、三季节、在日本运营收获机上采集的26,688个部分点云（859个块茎）进行训练评估，并与线性回归和PointNet++回归基线比较。

Result: 在5,254个点云（172个块茎）的测试集上，MAE=12.0 g，RMSE=17.2 g，显著优于线性回归与PointNet++；平均推理6.3 ms/点云，支持约150块茎/秒，满足商业高吞吐需求。

Conclusion: PointRAFT能在实际收获条件下从不完整点云准确、快速地估计马铃薯重量，减少自遮挡带来的系统性偏差；方法通用于更广泛的3D表型与机器人感知回归任务。源码与部分数据公开。

Abstract: Potato yield is a key indicator for optimizing cultivation practices in agriculture. Potato yield can be estimated on harvesters using RGB-D cameras, which capture three-dimensional (3D) information of individual tubers moving along the conveyor belt. However, point clouds reconstructed from RGB-D images are incomplete due to self-occlusion, leading to systematic underestimation of tuber weight. To address this, we introduce PointRAFT, a high-throughput point cloud regression network that directly predicts continuous 3D shape properties, such as tuber weight, from partial point clouds. Rather than reconstructing full 3D geometry, PointRAFT infers target values directly from raw 3D data. Its key architectural novelty is an object height embedding that incorporates tuber height as an additional geometric cue, improving weight prediction under practical harvesting conditions. PointRAFT was trained and evaluated on 26,688 partial point clouds collected from 859 potato tubers across four cultivars and three growing seasons on an operational harvester in Japan. On a test set of 5,254 point clouds from 172 tubers, PointRAFT achieved a mean absolute error of 12.0 g and a root mean squared error of 17.2 g, substantially outperforming a linear regression baseline and a standard PointNet++ regression network. With an average inference time of 6.3 ms per point cloud, PointRAFT supports processing rates of up to 150 tubers per second, meeting the high-throughput requirements of commercial potato harvesters. Beyond potato weight estimation, PointRAFT provides a versatile regression network applicable to a wide range of 3D phenotyping and robotic perception tasks. The code, network weights, and a subset of the dataset are publicly available at https://github.com/pieterblok/pointraft.git.

</details>


### [42] [CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers](https://arxiv.org/abs/2512.24195)
*Yonglak Son,Suhyeok Kim,Seungryong Kim,Young Geun Kim*

Main category: cs.CV

TL;DR: 提出CorGi/CorGi+：在不改训练的情况下，通过按块间隔缓存与部分注意力更新，加速DiT推理达约2倍且基本不降质。


<details>
  <summary>Details</summary>
Motivation: DiT迭代去噪步骤间存在高度冗余计算，尤其是大容量模型导致推理代价高；需要在不牺牲生成质量的前提下降低冗余、提升推理效率。

Method: 提出训练无关的推理加速框架CorGi：按块评估“贡献度”，在每个时间区间内对低贡献transformer块进行输出缓存并跨步骤复用，从而减少重复计算；对文本到图像，提出CorGi+：利用每块的跨注意力图识别显著token，仅对关键token执行部分注意力更新，以保护重要物体细节，其余继续缓存复用。

Result: 在SOTA DiT上评测，CorGi/CorGi+在保持高生成质量的同时，平均可获得最高约2.0×加速。

Conclusion: 通过贡献度引导的块级间隔缓存与基于注意力的选择性更新，可显著削减DiT推理冗余，实现近2倍加速而质量基本不受影响。

Abstract: Diffusion transformer (DiT) achieves remarkable performance in visual generation, but its iterative denoising process combined with larger capacity leads to a high inference cost. Recent works have demonstrated that the iterative denoising process of DiT models involves substantial redundant computation across steps. To effectively reduce the redundant computation in DiT, we propose CorGi (Contribution-Guided Block-Wise Interval Caching), training-free DiT inference acceleration framework that selectively reuses the outputs of transformer blocks in DiT across denoising steps. CorGi caches low-contribution blocks and reuses them in later steps within each interval to reduce redundant computation while preserving generation quality. For text-to-image tasks, we further propose CorGi+, which leverages per-block cross-attention maps to identify salient tokens and applies partial attention updates to protect important object details. Evaluation on the state-of-the-art DiT models demonstrates that CorGi and CorGi+ achieve up to 2.0x speedup on average, while preserving high generation quality.

</details>


### [43] [Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19](https://arxiv.org/abs/2512.24214)
*Sina Jahromi,Farshid Hajati,Alireza Rezaee,Javaher Nourian*

Main category: cs.CV

TL;DR: 提出一种渐进式GAN生成合成胸部X光数据，并与真实数据加权融合，结合多目标群体智能优化分类器超参数，在COVID-19不平衡数据上显著提升分类性能（4类95.5%，2类98.5%）。


<details>
  <summary>Details</summary>
Motivation: 医疗影像分类常遭遇类别不平衡（尤其疫情期间阳性样本稀缺），导致模型偏向多数类、召回率低，限制AI在快速准确筛查中的应用。需要在数据不足且分布不均时提高分类鲁棒性与泛化。

Method: 1) 设计“渐进式”生成对抗网络（逐步增广/细化生成器与判别器）以合成逼真的少数类样本；2) 提出对合成与真实样本的加权融合策略，再输入深度分类器；3) 采用多目标、群体式元启发式算法联合优化分类器超参数（在准确率、稳定性等目标间权衡）；4) 进行交叉验证评估。

Result: 在大规模且不平衡的COVID-19胸片数据集上，所提模型优于现有方法；在4类任务达到95.5%准确率，在2类任务达到98.5%准确率，并在交叉验证指标上表现更佳。

Conclusion: 通过渐进式GAN数据增强与加权融合，再配合多目标超参优化，可有效缓解严重类别不平衡下的医疗影像分类难题，对疫情场景下的快速筛查具有实用价值。

Abstract: The challenge of imbalanced data is prominent in medical image classification. This challenge arises when there is a significant disparity in the number of images belonging to a particular class, such as the presence or absence of a specific disease, as compared to the number of images belonging to other classes. This issue is especially notable during pandemics, which may result in an even more significant imbalance in the dataset. Researchers have employed various approaches in recent years to detect COVID-19 infected individuals accurately and quickly, with artificial intelligence and machine learning algorithms at the forefront. However, the lack of sufficient and balanced data remains a significant obstacle to these methods. This study addresses the challenge by proposing a progressive generative adversarial network to generate synthetic data to supplement the real ones. The proposed method suggests a weighted approach to combine synthetic data with real ones before inputting it into a deep network classifier. A multi-objective meta-heuristic population-based optimization algorithm is employed to optimize the hyper-parameters of the classifier. The proposed model exhibits superior cross-validated metrics compared to existing methods when applied to a large and imbalanced chest X-ray image dataset of COVID-19. The proposed model achieves 95.5% and 98.5% accuracy for 4-class and 2-class imbalanced classification problems, respectively. The successful experimental outcomes demonstrate the effectiveness of the proposed model in classifying medical images using imbalanced data during pandemics.

</details>


### [44] [ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.24224)
*Ziquan Liu,Zhewei Zhu,Xuyang Shi*

Main category: cs.CV

TL;DR: 提出一种轻量可学习的注意力精炼模块（ARM），在无需额外大型模型与训练的框架中，解锁并细化 CLIP 的层级特征，从而显著提升开放词汇语义分割（OVSS）性能，且几乎无推理开销。


<details>
  <summary>Details</summary>
Motivation: CLIP 仅具图像级语义，缺乏像素级细节，导致训练免费 OVSS 表现受限；现有方法要么依赖昂贵外部基础模型（SAM/DINO），要么用静态手工启发式融合特征，分别代价高或效果不佳，亟需一种高效且可泛化的细粒度特征利用方式。

Method: 提出 Attention Refinement Module（ARM）：在 CLIP 内部进行自适应层级特征融合。首先用语义引导的交叉注意力，以深层稳健语义特征作为 K、V，引导对浅层细节特征（Q）的选择与精炼；随后通过自注意力进一步整合信息。ARM 为轻量可学习模块，采用“训练一次、到处可用”的范式，在通用数据集（如 COCO-Stuff）上训练后，可作为即插即用后处理器接入不同训练免费框架。

Result: 在多项基准与多种训练免费基线中，ARM 一致性提升性能，同时推理开销可忽略；验证了其高效且有效。

Conclusion: ARM 能有效挖掘并细化 CLIP 的层级表示，以极低成本提升训练免费 OVSS，确立了“训练一次、到处可用”的高效范式，并作为通用可插拔后处理器具有广泛适用性。

Abstract: Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP's internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP's internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere" paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.

</details>


### [45] [Mirage: One-Step Video Diffusion for Photorealistic and Coherent Asset Editing in Driving Scenes](https://arxiv.org/abs/2512.24227)
*Shuyun Wang,Haiyang Sun,Bing Wang,Hangjun Ye,Xin Yu*

Main category: cs.CV

TL;DR: 提出Mirage：一种一步式视频扩散模型，用于自动驾驶场景中的高保真、时序一致的资产编辑，并通过2D-3D混合潜变量注入与两阶段对齐策略解决细节与姿态错配问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要多样且可扩展的视频数据；现有视频对象编辑方法难以同时兼顾时空一致性与逼真度，尤其3D因果VAE压缩导致细节损失、直接传递3D特征破坏因果性，以及场景与插入资产优化目标差异引起的高斯分布不匹配与姿态偏差。

Method: 构建基于文本到视频扩散先验的一步式视频扩散模型Mirage：1) 以T2V扩散确保跨帧时序一致；2) 在3D解码器中注入来自预训练2D编码器的“时间无关”潜变量，恢复空间细节并保持因果结构；3) 提出两阶段数据对齐：粗粒度3D对齐+精细2D细化，缓解场景与资产高斯分布不匹配并提升姿态对齐与监督质量。

Result: 在多种编辑场景中实现高写实度与强时序一致性；方法还可泛化至其他视频到视频翻译任务，表现为强基线。

Conclusion: Mirage有效统筹视频编辑的空间细节与时间一致性，通过2D/3D潜变量融合与两阶段对齐消除分布与姿态差异，为自动驾驶数据增强与更广泛的视频转换提供可靠方案，并公开代码以促进研究。

Abstract: Vision-centric autonomous driving systems rely on diverse and scalable training data to achieve robust performance. While video object editing offers a promising path for data augmentation, existing methods often struggle to maintain both high visual fidelity and temporal coherence. In this work, we propose \textbf{Mirage}, a one-step video diffusion model for photorealistic and coherent asset editing in driving scenes. Mirage builds upon a text-to-video diffusion prior to ensure temporal consistency across frames. However, 3D causal variational autoencoders often suffer from degraded spatial fidelity due to compression, and directly passing 3D encoder features to decoder layers breaks temporal causality. To address this, we inject temporally agnostic latents from a pretrained 2D encoder into the 3D decoder to restore detail while preserving causal structures. Furthermore, because scene objects and inserted assets are optimized under different objectives, their Gaussians exhibit a distribution mismatch that leads to pose misalignment. To mitigate this, we introduce a two-stage data alignment strategy combining coarse 3D alignment and fine 2D refinement, thereby improving alignment and providing cleaner supervision. Extensive experiments demonstrate that Mirage achieves high realism and temporal consistency across diverse editing scenarios. Beyond asset editing, Mirage can also generalize to other video-to-video translation tasks, serving as a reliable baseline for future research. Our code is available at https://github.com/wm-research/mirage.

</details>


### [46] [MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model](https://arxiv.org/abs/2512.24231)
*Rahul Medicharla,Alper Yilmaz*

Main category: cs.CV

TL;DR: 提出MotivNet，一种无需跨域训练即可在多数据集上具备强泛化能力的人脸表情识别模型，基于大规模自监督预训练的人类视觉基础模型Sapiens（Masked Autoencoder预训练），并以基准表现、模型相似度与数据相似度三项标准验证其作为Sapiens下游任务的可行性，实验显示跨域鲁棒、与SOTA可比。


<details>
  <summary>Details</summary>
Motivation: 现有FER在跨数据集/真实场景中泛化弱，常依赖复杂架构与跨域训练，现实部署矛盾且成本高；需要一种不依赖跨域训练、可直接在野外应用的通用模型。

Method: 以Sapiens/Meta-Sapiens为骨干（人类视觉基础模型，MAE大规模预训练）构建MotivNet，将FER定义为其新增下游任务；提出三项可行性评估准则：基准性能、模型相似性与数据相似性；描述了MotivNet组件与训练流程，并在多数据集上评测且不进行跨域训练。

Result: 在无跨域训练条件下，MotivNet在多数据集上获得与现有SOTA可比的结果，展现良好的跨域泛化能力，并满足提出的三项评估标准。

Conclusion: MotivNet作为Sapiens下游任务是可行的，能够提升FER在真实场景中的实用性与吸引力；代码已开源，验证其在野外应用的潜力。

Abstract: In this paper, we introduce MotivNet, a generalizable facial emotion recognition model for robust real-world application. Current state-of-the-art FER models tend to have weak generalization when tested on diverse data, leading to deteriorated performance in the real world and hindering FER as a research domain. Though researchers have proposed complex architectures to address this generalization issue, they require training cross-domain to obtain generalizable results, which is inherently contradictory for real-world application. Our model, MotivNet, achieves competitive performance across datasets without cross-domain training by using Meta-Sapiens as a backbone. Sapiens is a human vision foundational model with state-of-the-art generalization in the real world through large-scale pretraining of a Masked Autoencoder. We propose MotivNet as an additional downstream task for Sapiens and define three criteria to evaluate MotivNet's viability as a Sapiens task: benchmark performance, model similarity, and data similarity. Throughout this paper, we describe the components of MotivNet, our training approach, and our results showing MotivNet is generalizable across domains. We demonstrate that MotivNet can be benchmarked against existing SOTA models and meets the listed criteria, validating MotivNet as a Sapiens downstream task, and making FER more incentivizing for in-the-wild application. The code is available at https://github.com/OSUPCVLab/EmotionFromFaceImages.

</details>


### [47] [MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation](https://arxiv.org/abs/2512.24243)
*Fuqiang Gu,Yuanke Li,Xianlei Long,Kangping Ji,Chao Chen,Qingyi Gu,Zhenliang Ni*

Main category: cs.CV

TL;DR: 提出MambaSeg：用并行Mamba编码器融合RGB与事件相机数据，结合跨空间与跨时间交互模块（DDIM=CSIM+CTIM）实现细粒度双维度融合，在DDD17与DSEC上达SOTA且计算更高效。


<details>
  <summary>Details</summary>
Motivation: 单一RGB在高速运动、弱光/高动态范围下性能下降；事件相机具高时域分辨率与低延迟但缺乏颜色纹理。现有多模态融合多偏重空间静态融合、计算开销大、忽略事件数据的时序特性，需一种既高效又能同时建模空间与时间的融合方法。

Method: 构建双分支语义分割框架：并行Mamba编码器分别建模RGB与事件流；提出双维交互模块DDIM，其中CSIM进行跨模态细粒度空间交互，CTIM进行跨模态时间交互，实现空间与时间上的对齐与信息补充，从而降低跨模态歧义。

Result: 在DDD17与DSEC数据集上取得SOTA分割精度，同时显著降低计算成本（推断更高效、可扩展）。

Conclusion: 并行Mamba与DDIM（CSIM+CTIM）的空间-时间双维融合能有效对齐并互补RGB与事件模态，实现高精度且高效的多模态语义分割，适用于可扩展、鲁棒的感知场景。

Abstract: Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and Transformers, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored multimodal fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust multimodal perception.

</details>


### [48] [Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT](https://arxiv.org/abs/2512.24260)
*Zhi Li,Yaqi Wang,Bingtao Ma,Yifan Zhang,Huiyu Zhou,Shuai Wang*

Main category: cs.CV

TL;DR: PGMP提出以物理为本的单次前向金属伪影消除方案：用AAPS合成高保真训练对，DMP-Former把还原视作确定性流形投影，SSA用医学大模型先验约束语义与结构，实测在多中心数据上更稳更快、更准。


<details>
  <summary>Details</summary>
Motivation: 牙科CBCT中的金属伪影严重遮挡解剖结构；现有监督法易均值回归导致光谱/细节模糊，非监督法易产生结构幻觉；扩散模型虽逼真但采样慢且随机，不适合临床时效与可控性。

Method: 1) AAPS：基于蒙特卡洛光谱建模与患者数字孪生，按解剖自适应地合成高保真成对数据，缩小合成-真实域差距；2) DMP-Former：采用direct x-prediction思想，将重建表述为确定性“流形投影”，单次前向恢复无伪影解剖，避免扩散多步/随机性；3) SSA：引入MedDINOv3等医学基础模型的语义-结构先验，对重建结果进行语义与形态对齐，保证临床可信。

Result: 在合成与多中心临床数据上，对未见解剖泛化良好；相较SOTA在效率（单次前向）、稳定性与诊断可靠性上均显著提升，刷新多项指标基准；代码与数据已开源。

Conclusion: PGMP通过物理仿真数据生成、确定性流形投影重建与医学语义先验约束，兼顾速度、真实性与结构可信度，为牙科CBCT金属伪影消除提供更实用的临床方案。

Abstract: Metal artifacts in Dental CBCT severely obscure anatomical structures, hindering diagnosis. Current deep learning for Metal Artifact Reduction (MAR) faces limitations: supervised methods suffer from spectral blurring due to "regression-to-the-mean", while unsupervised ones risk structural hallucinations. Denoising Diffusion Models (DDPMs) offer realism but rely on slow, stochastic iterative sampling, unsuitable for clinical use. To resolve this, we propose the Physically-Grounded Manifold Projection (PGMP) framework. First, our Anatomically-Adaptive Physics Simulation (AAPS) pipeline synthesizes high-fidelity training pairs via Monte Carlo spectral modeling and patient-specific digital twins, bridging the synthetic-to-real gap. Second, our DMP-Former adapts the Direct x-Prediction paradigm, reformulating restoration as a deterministic manifold projection to recover clean anatomy in a single forward pass, eliminating stochastic sampling. Finally, a Semantic-Structural Alignment (SSA) module anchors the solution using priors from medical foundation models (MedDINOv3), ensuring clinical plausibility. Experiments on synthetic and multi-center clinical datasets show PGMP outperforms state-of-the-art methods on unseen anatomy, setting new benchmarks in efficiency and diagnostic reliability. Code and data: https://github.com/ricoleehduu/PGMP

</details>


### [49] [Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation](https://arxiv.org/abs/2512.24271)
*Zhe Huang,Hao Wen,Aiming Hao,Bingze Song,Meiqi Wu,Jiahong Wu,Xiangxiang Chu,Sheng Lu,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出DualityForge框架与DNA-Train训练范式，用可控扩散视频编辑合成反事实视频与配对QA，结合对比式SFT+RL，显著降低MLLM在反事实视频上的幻觉，并提升通用表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视频理解中过度依赖语言先验，面对违背常识的反事实视频易产生视觉未落地的幻觉；收集标注反事实数据昂贵且稀缺，需一种可扩展的数据生成与训练策略以缓解数据失衡并抑制幻觉。

Method: 1) DualityForge：基于可控扩散视频编辑，将真实视频编辑为反事实版本；在编辑与QA生成中嵌入结构化上下文，自动生成原始-编辑视频对与高质量QA对，用于对比训练。2) 构建DualityVidQA大规模数据集。3) DNA-Train：两阶段SFT-RL，RL阶段进行配对样本的L1优势归一化，提升策略优化稳定性与效率。

Result: 在DualityVidQA-Test上相对Qwen2.5-VL-7B基线降低反事实幻觉、相对提升24.0%；在幻觉与通用基准上均有显著增益，显示良好泛化。

Conclusion: 通过可控合成反事实视频与对比式SFT-RL（优势归一化），可有效缓解MLLM语言先验导致的视觉幻觉，提升反事实与通用视频理解性能；数据集与代码将开源。

Abstract: Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.

</details>


### [50] [LiftProj: Space Lifting and Projection-Based Panorama Stitching](https://arxiv.org/abs/2512.24276)
*Yuan Jia,Ruimin Wu,Rui Song,Jiaojiao Li,Bin Song*

Main category: cs.CV

TL;DR: 该论文提出一种“空间提升”的全景拼接框架：先把多张图片提升为统一坐标系下的稠密3D点，再全局融合并设定统一投影中心，用等距圆柱投影生成几何一致的360°全景，最后在画布域进行缺失区域填补，从而显著减少视差导致的重影和形变。


<details>
  <summary>Details</summary>
Motivation: 传统基于2D单应/网格形变的拼接在多深度层、遮挡和大视差的真实3D场景中容易出现重影、结构弯曲和拉伸，尤其在多视角累积与闭环360°场景更严重，需要一种能在三维一致性上处理对齐与投影的方案。

Method: 1) 将每张输入图像提升为统一坐标系下的稠密3D点云/体素，并引入置信度进行跨视角全局融合；2) 在三维空间中确定统一投影中心；3) 采用等距圆柱投影将融合后的3D数据映射到单一全景流形，形成几何一致的360°版式；4) 在画布域进行孔洞填充，恢复连续纹理与语义。框架可灵活替换3D提升与补全模块。

Result: 在存在显著视差与复杂遮挡的场景中，该方法明显减少几何畸变与重影，获得更自然、更一致的全景效果；在多视角累积与闭环360°拼接场景中表现尤为突出。

Conclusion: 将拼接范式由2D形变转向3D一致性，通过统一投影中心与圆柱投影实现几何一致的360°布局，并配合孔洞填补有效应对视差与遮挡问题；框架通用、可模块化扩展，实验验证其优于传统方法的稳健性与视觉质量。

Abstract: Traditional image stitching techniques have predominantly utilized two-dimensional homography transformations and mesh warping to achieve alignment on a planar surface. While effective for scenes that are approximately coplanar or exhibit minimal parallax, these approaches often result in ghosting, structural bending, and stretching distortions in non-overlapping regions when applied to real three-dimensional scenes characterized by multiple depth layers and occlusions. Such challenges are exacerbated in multi-view accumulations and 360° closed-loop stitching scenarios. In response, this study introduces a spatially lifted panoramic stitching framework that initially elevates each input image into a dense three-dimensional point representation within a unified coordinate system, facilitating global cross-view fusion augmented by confidence metrics. Subsequently, a unified projection center is established in three-dimensional space, and an equidistant cylindrical projection is employed to map the fused data onto a single panoramic manifold, thereby producing a geometrically consistent 360° panoramic layout. Finally, hole filling is conducted within the canvas domain to address unknown regions revealed by viewpoint transitions, restoring continuous texture and semantic coherence. This framework reconceptualizes stitching from a two-dimensional warping paradigm to a three-dimensional consistency paradigm and is designed to flexibly incorporate various three-dimensional lifting and completion modules. Experimental evaluations demonstrate that the proposed method substantially mitigates geometric distortions and ghosting artifacts in scenarios involving significant parallax and complex occlusions, yielding panoramic results that are more natural and consistent.

</details>


### [51] [One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training](https://arxiv.org/abs/2512.24278)
*Jia Yu,Yan Zhu,Peiyao Fu,Tianyi Chen,Zhihua Wang,Fei Wu,Quanlin Li,Pinghong Zhou,Shuo Wang,Xian Yang*

Main category: cs.CV

TL;DR: 提出EndoRare：从单张参考内镜图像一键生成多样高保真罕见病灶图像，无需再训练；能分离病灶关键特征与非诊断属性，用于数据增强与教学，显著提升分类器与新手医生表现。


<details>
  <summary>Details</summary>
Motivation: 罕见胃肠道病变在常规内镜中少见，导致AI训练样本匮乏、模型泛化差，亦限制了初学者的识别训练；需要一种能在极少样本条件下合成可信、可用于诊断训练的图像。

Method: 提出语言引导的概念解耦生成框架EndoRare：从单张参考图像中分离病灶的致病学特征（编码为可学习原型嵌入）与非诊断属性（视角、光照、纹理等），固定前者、扰动后者以生成多样且保持病灶本质的合成图；一键式（one-shot）、无需对大模型再训练。

Result: 在四类罕见病变（CFT、JPS、FAP、PJS）上验证；专家评估认为合成图像临床上可信；用于数据增广显著提升下游分类器在低假阳性率下的真阳性率；盲法读片研究中，经EndoRare案例训练的新手内镜医师召回率+0.400、精确率+0.267。

Conclusion: EndoRare在极少样本场景下可生成高保真、多样的罕见病灶图像，为计算机辅助诊断与临床教育提供数据高效的实用路径，缩小罕见病数据鸿沟。

Abstract: Rare gastrointestinal lesions are infrequently encountered in routine endoscopy, restricting the data available for developing reliable artificial intelligence (AI) models and training novice clinicians. Here we present EndoRare, a one-shot, retraining-free generative framework that synthesizes diverse, high-fidelity lesion exemplars from a single reference image. By leveraging language-guided concept disentanglement, EndoRare separates pathognomonic lesion features from non-diagnostic attributes, encoding the former into a learnable prototype embedding while varying the latter to ensure diversity. We validated the framework across four rare pathologies (calcifying fibrous tumor, juvenile polyposis syndrome, familial adenomatous polyposis, and Peutz-Jeghers syndrome). Synthetic images were judged clinically plausible by experts and, when used for data augmentation, significantly enhanced downstream AI classifiers, improving the true positive rate at low false-positive rates. Crucially, a blinded reader study demonstrated that novice endoscopists exposed to EndoRare-generated cases achieved a 0.400 increase in recall and a 0.267 increase in precision. These results establish a practical, data-efficient pathway to bridge the rare-disease gap in both computer-aided diagnostics and clinical education.

</details>


### [52] [Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction](https://arxiv.org/abs/2512.24294)
*Md. Enamul Hoq,Linda Larson-Prior,Fred Prior*

Main category: cs.CV

TL;DR: 论文提出并验证了一个名为 Virtual-Eyes 的16位CT质量控制预处理管线，在LDCT肺癌筛查中系统量化其对不同模型的影响。它通过严格的分辨率、序列筛除与肺区块提取来稳定输入，显著提升通用基础模型（如RAD-DINO）的AUC与校准，但对依赖原始上下文的专科模型（如Sybil、ResNet-18）反而可能造成性能下降。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT筛查流程中，预处理质量控制常被忽视且鲜少量化评估；然而输入分辨率、无诊断价值片段、以及非肺部区域的干扰会导致模型性能不稳定、过拟合或捷径学习。作者旨在提供可临床落地、可量化影响的QC管线，并比较其对通用基础模型与专科模型的差异化作用。

Method: 提出 Virtual-Eyes：保持原生16位网格，强制512×512面内分辨率；剔除过短或无诊断序列；通过HU滤波与双侧肺覆盖评分提取连续肺体块。数据：NLST 765名患者（182癌/583非癌）。用RAD-DINO与Merlin的冻结编码器提取切片嵌入，训练无泄漏的患者级MLP头；同时评估Sybil与2D ResNet-18在Raw vs Virtual-Eyes输入下的表现（不重训主干）。

Result: 在RAD-DINO上：切片级AUC 0.576→0.610；患者级AUC均值池化0.646→0.683，最大池化0.619→0.735；校准Brier 0.188→0.112。Sybil与ResNet-18在Virtual-Eyes下性能下降（Sybil AUC 0.886→0.837；ResNet-18 0.571→0.596小幅变化），显示上下文依赖/捷径学习；Merlin迁移性有限（AUC约0.507→0.567），与预处理无强相关。

Conclusion: 解剖学靶向的QC可稳定并提升通用基础模型的工作流与校准，但可能破坏适应原始临床上下文的专科模型。预处理策略应与模型类型匹配：基础模型受益于严格QC，专科模型需要谨慎调整以避免丢失其依赖的上下文特征。

Abstract: Robust preprocessing is rarely quantified in deep-learning pipelines for low-dose CT (LDCT) lung cancer screening. We develop and validate Virtual-Eyes, a clinically motivated 16-bit CT quality-control pipeline, and measure its differential impact on generalist foundation models versus specialist models. Virtual-Eyes enforces strict 512x512 in-plane resolution, rejects short or non-diagnostic series, and extracts a contiguous lung block using Hounsfield-unit filtering and bilateral lung-coverage scoring while preserving the native 16-bit grid. Using 765 NLST patients (182 cancer, 583 non-cancer), we compute slice-level embeddings from RAD-DINO and Merlin with frozen encoders and train leakage-free patient-level MLP heads; we also evaluate Sybil and a 2D ResNet-18 baseline under Raw versus Virtual-Eyes inputs without backbone retraining. Virtual-Eyes improves RAD-DINO slice-level AUC from 0.576 to 0.610 and patient-level AUC from 0.646 to 0.683 (mean pooling) and from 0.619 to 0.735 (max pooling), with improved calibration (Brier score 0.188 to 0.112). In contrast, Sybil and ResNet-18 degrade under Virtual-Eyes (Sybil AUC 0.886 to 0.837; ResNet-18 AUC 0.571 to 0.596) with evidence of context dependence and shortcut learning, and Merlin shows limited transferability (AUC approximately 0.507 to 0.567) regardless of preprocessing. These results demonstrate that anatomically targeted QC can stabilize and improve generalist foundation-model workflows but may disrupt specialist models adapted to raw clinical context.

</details>


### [53] [UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots](https://arxiv.org/abs/2512.24321)
*Nan Jiang,Zimo He,Wanhe Yu,Lexi Pang,Yunhao Li,Hongjie Li,Jieming Cui,Yuhan Li,Yizhou Wang,Yixin Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: UniAct提出一个两阶段、多模态-控制统一的框架，用共享离散码本（FSQ）把语言/音乐/轨迹等指令对齐到动作空间，并用因果流式管线实现<500ms低时延执行；在20小时UniMoCap基准上展现强泛化，零样本跟踪不完美参考动作成功率提升19%。


<details>
  <summary>Details</summary>
Motivation: 现有类人机器人虽能控制肢体，但难把高层多模态感知（语言、音乐、轨迹等）稳定、实时地映射为全身动作；多模态异构性与动作物理约束使实时性与鲁棒性成为瓶颈。

Method: 提出UniAct：两阶段框架。阶段一：微调的多模态大模型（MLLM）将不同模态指令统一编码到共享离散码本（通过FSQ）以实现跨模态对齐；阶段二：因果流式（causal streaming）控制管线，把离散代码实时解码为受物理约束的全身动作轨迹，保证低延迟与稳定性。并构建20小时的UniMoCap人形动作数据集作训练/评测。

Result: 在<500毫秒端到端时延下，能稳定执行语言、音乐、轨迹等多模态指令；在零样本跟踪不完美参考动作任务上，成功率提升19%；在UniMoCap上表现出对多种真实场景的稳健泛化。

Conclusion: 统一的离散码本与流式因果控制有效桥接感知与全身控制，实现低时延、多模态、稳健的人形机器人指令执行；这是迈向通用、可交互的人形助理的重要一步。

Abstract: A long-standing objective in humanoid robotics is the realization of versatile agents capable of following diverse multimodal instructions with human-level flexibility. Despite advances in humanoid control, bridging high-level multimodal perception with whole-body execution remains a significant bottleneck. Existing methods often struggle to translate heterogeneous instructions -- such as language, music, and trajectories -- into stable, real-time actions. Here we show that UniAct, a two-stage framework integrating a fine-tuned MLLM with a causal streaming pipeline, enables humanoid robots to execute multimodal instructions with sub-500 ms latency. By unifying inputs through a shared discrete codebook via FSQ, UniAct ensures cross-modal alignment while constraining motions to a physically grounded manifold. This approach yields a 19% improvement in the success rate of zero-shot tracking of imperfect reference motions. We validate UniAct on UniMoCap, our 20-hour humanoid motion benchmark, demonstrating robust generalization across diverse real-world scenarios. Our results mark a critical step toward responsive, general-purpose humanoid assistants capable of seamless interaction through unified perception and control.

</details>


### [54] [Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention](https://arxiv.org/abs/2512.24323)
*Haijing Liu,Zhiyuan Song,Hefeng Wu,Tao Pu,Keze Wang,Liang Lin*

Main category: cs.CV

TL;DR: 提出CERES，一个用于自我中心指代视频目标分割的因果框架，通过对语言与视觉通路进行因果干预，缓解数据偏置与视角混淆，在基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Ego-RVOS方法易受两类偏差影响：1) 数据集中对象-动作配对不均导致模型从语言统计中学到伪相关；2) 自我中心视角的快速运动、遮挡等视觉混淆使分割不稳健。亟需一种能系统抵御语言与视觉偏置的机制。

Method: 在强预训练RVOS骨干上，设计可插拔的因果模块，进行双模态干预：1) 语言通路采用后门调整，削弱数据统计导致的语言表示偏置；2) 视觉通路采用前门调整，将语义视觉特征与几何深度信息在因果指导下融合，隔离和抑制自我中心视角中的混淆因素，得到更稳健表征。

Result: 在多个Ego-RVOS基准上达到最新最优性能，显著优于现有方法，验证了因果干预对提升鲁棒性的有效性。

Conclusion: 通过将后门与前门调整引入Ego-RVOS，CERES能系统性缓解语言与视觉偏置，提升对自我中心视频中被指代目标的稳健分割，显示因果推理在更广泛的自我中心视频理解任务中的潜力。

Abstract: Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.

</details>


### [55] [SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning](https://arxiv.org/abs/2512.24330)
*Yong Xien Chng,Tao Hu,Wenwen Tong,Xueheng Li,Jiandong Chen,Haojia Yu,Jiefan Lu,Hewei Guo,Hanming Deng,Chengjun Xie,Gao Huang,Dahua Lin,Lewei Lu*

Main category: cs.CV

TL;DR: 提出SenseNova-MARS，一个通过强化学习让VLM实现交错式视觉推理与工具调用（图搜、文搜、裁剪）的框架，并配套BN-GSPO算法与HR-MMSearch基准；在多项检索与细粒度视觉理解任务上达SOTA，开源代码模型数据。


<details>
  <summary>Details</summary>
Motivation: 现有VLM的“智能体式”能力多停留在文本链式思维或单次工具调用，难以在人类式场景中连续推理并动态协调外部工具，尤其在高分辨率、知识密集且需检索/裁剪的任务上表现不足。

Method: 构建SenseNova-MARS框架：将图像搜索、文本搜索、图像裁剪三类工具与VLM推理过程动态交织；在RL训练阶段提出BN-GSPO以提升策略优化稳定性与工具调用能力；同时设计HR-MMSearch基准以评测高分辨率、检索驱动的复杂视觉任务。

Result: 在开放源检索与细粒度视觉理解基准上取得SOTA；在检索型基准上，8B模型MMSearch得分67.84、HR-MMSearch得分41.64，超过Gemini-3-Flash与GPT-5等商用模型。

Conclusion: SenseNova-MARS显著提升VLM的交错式工具使用与视觉推理能力，特别适用于知识密集与高分辨率任务；BN-GSPO增强训练稳定与策略质量；发布代码、模型与数据以促进后续研究。

Abstract: While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.

</details>


### [56] [Spatial-aware Vision Language Model for Autonomous Driving](https://arxiv.org/abs/2512.24331)
*Weijie Wei,Zhipeng Luo,Ling Feng,Venice Erin Liong*

Main category: cs.CV

TL;DR: LVLDrive将LiDAR引入VLM，以渐进式融合策略提升3D度量理解，从而改进自动驾驶的场景理解与决策可靠性。


<details>
  <summary>Details</summary>
Motivation: 纯视觉VLM在自动驾驶中受限于2D线索，难以进行准确的度量空间推理与几何推断，导致决策不稳定且不安全。需要显式的3D度量信息来增强VLM的空间理解与可靠性。

Method: 提出LVLDrive（LiDAR-Vision-Language）：在现有VLM上引入LiDAR点云作为额外模态；设计Gradual Fusion Q-Former，逐步注入LiDAR特征以减轻对预训练VLM知识的破坏；构建空间感知问答（SA-QA）数据集，显式教授3D感知与推理能力。

Result: 在多项自动驾驶基准上，LVLDrive在场景理解、度量空间感知与可靠驾驶决策方面均优于仅用视觉的基线方法。

Conclusion: 显式3D度量数据（如LiDAR）对构建可信赖的基于VLM的自动驾驶系统至关重要；渐进式多模态融合与专门数据集能稳健提升VLM的3D理解与决策性能。

Abstract: While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM's existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.

</details>


### [57] [The Mechanics of CNN Filtering with Rectification](https://arxiv.org/abs/2512.24338)
*Liam Frija-Altrac,Matthew Toews*

Main category: cs.CV

TL;DR: 提出“初等信息力学”，用物理中能量-动量的视角解释带整流的卷积滤波；偶核扩散不移动质心，奇核推动信息质心位移；小核在DCT域由DC与梯度基主导。


<details>
  <summary>Details</summary>
Motivation: 当前对CNN中卷积+非线性为何与如何传播“信息”的机理缺乏统一直观的物理化描述；希望用物理学（相对论、量子力学）中的能量—动量关系提供可解释框架，区分不同核成分对信息传播的作用。

Method: 将卷积核分解为正交的偶、奇分量；分析整流下图像“信息质心”的扩散与位移行为；定义奇/总能量比作为“信息位移速度”；在频域用DCT刻画小卷积核结构，强调DC(Σ)与一阶梯度(∇)基的主导性；建立与能量-动量关系的类比。

Result: 得到：偶核导致各向同性扩散并保持信息质心不动（类比静能/势能，零动量）；奇核导致方向性位移（类比动能，非零动量）；位移速度与奇能量占比线性相关；在3×3等小核中，低频的DC与梯度模态主导信息传播。

Conclusion: 信息传播可被分解为“扩散”(偶)与“位移”(奇)两种基本模式，并与相对论能量-动量关系形成映射，为CNN提供物理启发的可解释性框架，尤其提示设计小核时应关注DC与梯度成分。

Abstract: This paper proposes elementary information mechanics as a new model for understanding the mechanical properties of convolutional filtering with rectification, inspired by physical theories of special relativity and quantum mechanics. We consider kernels decomposed into orthogonal even and odd components. Even components cause image content to diffuse isotropically while preserving the center of mass, analogously to rest or potential energy with zero net momentum. Odd kernels cause directional displacement of the center of mass, analogously to kinetic energy with non-zero momentum. The speed of information displacement is linearly related to the ratio of odd vs total kernel energy. Even-Odd properties are analyzed in the spectral domain via the discrete cosine transform (DCT), where the structure of small convolutional filters (e.g. $3 \times 3$ pixels) is dominated by low-frequency bases, specifically the DC $Σ$ and gradient components $\nabla$, which define the fundamental modes of information propagation. To our knowledge, this is the first work demonstrating the link between information processing in generic CNNs and the energy-momentum relation, a cornerstone of modern relativistic physics.

</details>


### [58] [DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images](https://arxiv.org/abs/2512.24340)
*Wen-wai Yim,Yujuan Fu,Asma Ben Abacha,Meliha Yetisgen,Noel Codella,Roberto Andres Novoa,Josep Malvehy*

Main category: cs.CV

TL;DR: 提出DermaVQA-DAS：在DermaVQA基础上扩展出同时支持封闭式问答与皮损分割的新基准，并以专家制定的Dermatology Assessment Schema（DAS）为核心。基于DAS构建中英双语多选题与标注，评测多模态模型与不同分割提示策略，公开数据、架构与评测协议。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤科影像数据多为皮镜图，缺乏患者提问与临床语境，难以支撑以患者为中心的应用；同时缺少统一的、可操作的临床特征结构化框架与跨任务基准。

Method: 1) 设计DAS：包含36个高层与27个细粒度评估问题，提供中英文多选选项；2) 基于DAS扩展DermaVQA，构建封闭式QA与病灶分割数据集并由专家标注；3) 基准评测：在分割任务上探索不同提示策略与多种聚合评估方案（Mean-of-Max、Mean-of-Mean、基于多数投票的microscore）；在QA任务上评测多种SOTA多模态模型。

Result: 分割：提示词设计显著影响表现；默认提示在Mean-of-Max与Mean-of-Mean下最佳，而“标题+内容”的增强提示在多数投票microscore下最好；以BiomedParse获得Jaccard 0.395、Dice 0.566。QA：整体表现较强，平均准确率约0.729–0.798；o3最高0.798，GPT‑4.1为0.796，Gemini‑1.5‑Pro为0.783。

Conclusion: DermaVQA-DAS以DAS为核心，提供统一的结构化评估框架和跨任务数据与协议，验证了提示设计对分割评测的敏感性，并证实多模态模型在封闭式QA上已具备较强能力。数据与协议公开以推动面向患者的皮肤科视觉-语言研究。

Abstract: Recent advances in dermatological image analysis have been driven by large-scale annotated datasets; however, most existing benchmarks focus on dermatoscopic images and lack patient-authored queries and clinical context, limiting their applicability to patient-centered care. To address this gap, we introduce DermaVQA-DAS, an extension of the DermaVQA dataset that supports two complementary tasks: closed-ended question answering (QA) and dermatological lesion segmentation. Central to this work is the Dermatology Assessment Schema (DAS), a novel expert-developed framework that systematically captures clinically meaningful dermatological features in a structured and standardized form. DAS comprises 36 high-level and 27 fine-grained assessment questions, with multiple-choice options in English and Chinese. Leveraging DAS, we provide expert-annotated datasets for both closed QA and segmentation and benchmark state-of-the-art multimodal models. For segmentation, we evaluate multiple prompting strategies and show that prompt design impacts performance: the default prompt achieves the best results under Mean-of-Max and Mean-of-Mean evaluation aggregation schemes, while an augmented prompt incorporating both patient query title and content yields the highest performance under majority-vote-based microscore evaluation, achieving a Jaccard index of 0.395 and a Dice score of 0.566 with BiomedParse. For closed-ended QA, overall performance is strong across models, with average accuracies ranging from 0.729 to 0.798; o3 achieves the best overall accuracy (0.798), closely followed by GPT-4.1 (0.796), while Gemini-1.5-Pro shows competitive performance within the Gemini family (0.783). We publicly release DermaVQA-DAS, the DAS schema, and evaluation protocols to support and accelerate future research in patient-centered dermatological vision-language modeling (https://osf.io/72rp3).

</details>


### [59] [Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems](https://arxiv.org/abs/2512.24385)
*Song Wang,Lingdong Kong,Xiaolu Liu,Hao Shi,Wentong Li,Jianke Zhu,Steven C. H. Hoi*

Main category: cs.CV

TL;DR: 论文综述多模态预训练以实现空间智能，提出统一分类法、分析传感器与学习策略关系，评估平台数据集作用，并探讨文本与占据表示融入，指出效率与可扩展性瓶颈并给出路线图。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶与无人机等自主系统需要从相机、激光雷达等多传感器数据中构建“真空间智能”。现有基础模型多偏单模态，难以跨传感器统一理解，缺乏系统化方法与共识。

Method: 提出一个多模态预训练的综合框架与统一分类法：从单模态基线到学习整体表征的统一范式；剖析传感器特性与学习策略的耦合；评估平台特定数据集的作用；探讨将文本输入与占据表示融合以支持开放世界感知与规划。

Result: 形成一套系统的分类与对比视角，阐明何种预训练策略有助于3D目标检测与语义占据预测等任务；总结多模态集成的关键技术路径与数据需求。

Conclusion: 实现通用多模态基础模型的关键在于高效、可扩展的预训练与统一表征学习；需解决计算效率、模型规模等瓶颈，融合文本与占据表示可推进开放世界空间智能，论文给出未来研究与工程路线图。

Abstract: The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.

</details>


### [60] [RedunCut: Measurement-Driven Sampling and Accuracy Performance Modeling for Low-Cost Live Video Analytics](https://arxiv.org/abs/2512.24386)
*Gur-Eyal Sela,Kumar Krishna Agrawal,Bharathan Balaji,Joseph Gonzalez,Ion Stoica*

Main category: cs.CV

TL;DR: RedunCut 是一套用于实时视频分析的动态模型规模选择（DMSS）系统，通过更聪明的抽样规划与轻量数据驱动的性能模型，在不改动模型的前提下降低推理成本，同时维持既定精度，跨多类视频与任务节省14–62%算力。


<details>
  <summary>Details</summary>
Motivation: LVA在大规模相机上持续运行，现代视觉模型推理昂贵。现有DMSS虽可内容自适应、把模型当黑盒，但在缺少实时标注的情况下需要先抽样再选择，常因抽样开销大于收益、以及每段精度预测不准而在移动视频、较低精度目标等场景失灵。

Method: 提出RedunCut：1) 以测量驱动的规划器估计抽样的成本-收益并决定是否/如何抽样；2) 采用轻量、数据驱动的性能模型改进对每段视频在不同模型规模下的精度预测；整体在无标注运行时做黑盒模型选择。

Result: 在道路车辆、无人机、监控等视频、多个模型族与任务上，在固定精度下降低计算成本14–62%；对有限历史数据与分布漂移保持鲁棒。

Conclusion: 通过更高效的抽样策略与更准确的段级精度预测，DMSS可在不改动模型的情况下显著压降LVA成本并具备跨场景泛化能力。

Abstract: Live video analytics (LVA) runs continuously across massive camera fleets, but inference cost with modern vision models remains high. To address this, dynamic model size selection (DMSS) is an attractive approach: it is content-aware but treats models as black boxes, and could potentially reduce cost by up to 10x without model retraining or modification. Without ground truth labels at runtime, we observe that DMSS methods use two stages per segment: (i) sampling a few models to calculate prediction statistics (e.g., confidences), then (ii) selection of the model size from those statistics. Prior systems fail to generalize to diverse workloads, particularly to mobile videos and lower accuracy targets. We identify that the failure modes stem from inefficient sampling whose cost exceeds its benefit, and inaccurate per-segment accuracy prediction.
  In this work, we present RedunCut, a new DMSS system that addresses both: It uses a measurement-driven planner that estimates the cost-benefit tradeoff of sampling, and a lightweight, data-driven performance model to improve accuracy prediction. Across road-vehicle, drone, and surveillance videos and multiple model families and tasks, RedunCut reduces compute cost by 14-62% at fixed accuracy and remains robust to limited historical data and to drift.

</details>


### [61] [DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model](https://arxiv.org/abs/2512.24408)
*Bohong Chen,Haiyang Liu*

Main category: cs.CV

TL;DR: DyStream提出一种超低延迟的双人（说话者-聆听者）对话头像视频生成模型，通过流匹配的自回归框架与轻量前瞻（约60ms）因果编码器，实现实时（<100ms端到端）高质量唇形同步与反应。


<details>
  <summary>Details</summary>
Motivation: 现有基于分块的说话头像生成需要非因果的整窗上下文，导致显著时延，不适合需要即时非语言反馈的逼真听者场景。需在保持高质量的同时，将系统延迟压到实时范围内。

Method: 1) 采用面向流式的自回归框架，并以flow-matching头进行概率建模，稳定高效地逐帧生成；2) 设计因果编码器并加入短时前瞻模块（如60ms），在不牺牲低延迟的前提下引入有限未来信息；并与蒸馏、生成式编码器等替代的因果策略进行对比分析。

Result: 实现34ms/帧的视频生成速度，使整体系统延迟<100ms；在HDTF上达成SOTA唇形同步：离线与在线LipSync Confidence分别为8.13与7.61；优于蒸馏与生成式编码器等因果方案。

Conclusion: DyStream在双人对话头像场景中，以流匹配自回归+短前瞻因果编码器实现实时、低延迟且高质量的唇形同步生成，提供实用的工程权衡与开源实现。

Abstract: Generating realistic, dyadic talking head video requires ultra-low latency. Existing chunk-based methods require full non-causal context windows, introducing significant delays. This high latency critically prevents the immediate, non-verbal feedback required for a realistic listener. To address this, we present DyStream, a flow matching-based autoregressive model that could generate video in real-time from both speaker and listener audio. Our method contains two key designs: (1) we adopt a stream-friendly autoregressive framework with flow-matching heads for probabilistic modeling, and (2) We propose a causal encoder enhanced by a lookahead module to incorporate short future context (e.g., 60 ms) to improve quality while maintaining low latency. Our analysis shows this simple-and-effective method significantly surpass alternative causal strategies, including distillation and generative encoder. Extensive experiments show that DyStream could generate video within 34 ms per frame, guaranteeing the entire system latency remains under 100 ms. Besides, it achieves state-of-the-art lip-sync quality, with offline and online LipSync Confidence scores of 8.13 and 7.61 on HDTF, respectively. The model, weights and codes are available.

</details>


### [62] [AI-Driven Evaluation of Surgical Skill via Action Recognition](https://arxiv.org/abs/2512.24411)
*Yan Meng,Daniel A. Donoho,Marcelle Altshuler,Omar Arnaout*

Main category: cs.CV

TL;DR: 提出一套基于视频Transformer与目标检测/跟踪的自动化显微血管吻合术技能评估系统，在58段专家标注视频上实现高精度动作分割与多维度技能评分复现。


<details>
  <summary>Details</summary>
Motivation: 传统手术技能评估依赖专家观摩或离线回看，主观性强、评分不一致、耗时高，尤其在中低收入国家难以规模化；需要客观、可扩展、可解释的自动评估方案。

Method: 构建AI框架：以改进的TimeSformer为骨干，引入分层时间注意力与加权空间注意力进行手术视频动作识别；结合基于YOLO的器械检测与跟踪提取细粒度运动学特征；从整体执行、关键步骤动作质量、器械操作等五个方面进行技能度量，并与专家评分对齐。

Result: 在58段专家标注数据上验证：动作分割帧级准确率87.7%，经后处理提升至93.62%；在五个技能维度上复现实验专家评估的平均分类准确率达76%。

Conclusion: 该系统能以客观、稳定、可解释的方式提供显微吻合训练反馈，有望标准化并数据驱动地提升外科教育评估，特别适用于资源受限环境的规模化培训。

Abstract: The development of effective training and evaluation strategies is critical. Conventional methods for assessing surgical proficiency typically rely on expert supervision, either through onsite observation or retrospective analysis of recorded procedures. However, these approaches are inherently subjective, susceptible to inter-rater variability, and require substantial time and effort from expert surgeons. These demands are often impractical in low- and middle-income countries, thereby limiting the scalability and consistency of such methods across training programs. To address these limitations, we propose a novel AI-driven framework for the automated assessment of microanastomosis performance. The system integrates a video transformer architecture based on TimeSformer, improved with hierarchical temporal attention and weighted spatial attention mechanisms, to achieve accurate action recognition within surgical videos. Fine-grained motion features are then extracted using a YOLO-based object detection and tracking method, allowing for detailed analysis of instrument kinematics. Performance is evaluated along five aspects of microanastomosis skill, including overall action execution, motion quality during procedure-critical actions, and general instrument handling. Experimental validation using a dataset of 58 expert-annotated videos demonstrates the effectiveness of the system, achieving 87.7% frame-level accuracy in action segmentation that increased to 93.62% with post-processing, and an average classification accuracy of 76% in replicating expert assessments across all skill aspects. These findings highlight the system's potential to provide objective, consistent, and interpretable feedback, thereby enabling more standardized, data-driven training and evaluation in surgical education.

</details>


### [63] [Exploring Compositionality in Vision Transformers using Wavelet Representations](https://arxiv.org/abs/2512.24438)
*Akshad Shyam Purushottamdas,Pranav K Nayak,Divya Mehul Rajparia,Deekshith Patel,Yashmitha Gogineni,Konda Reddy Mopuri,Sumohana S. Channappayya*

Main category: cs.CV

TL;DR: 论文用可离散小波变换（DWT）定义“视觉原语”，检验ViT编码表征是否在潜空间可组合，发现一层DWT原语的编码基本可近似线性组合重构原始表征。


<details>
  <summary>Details</summary>
Motivation: 语言模型可组合性研究丰富，但视觉Transformer（ViT）内部表征如何以“部件→整体”的方式组织尚不清楚；需要一个在视觉域可操作的“原语/组合”框架来检验ViT表征的可组合性。

Method: 提出与表征可组合性评估类似的框架：以图像的离散小波变换（DWT）得到输入依赖的原语（子带/尺度-方向分量），分别送入ViT编码器得到子表示；再将这些子表示在潜空间进行组合（如加权叠加），衡量其重构原始整图表示的能力，从而评估可组合性。

Result: 实验显示：用一层DWT分解得到的原语，其对应的ViT编码表征在潜空间中可近似组成，组合后的表示能较好重现整图表示，说明ViT对信息的组织具有近似可组合结构。

Conclusion: ViT编码器对图像的潜在表征呈现近似可组合性；DWT提供了简单有效的视觉原语定义与评测工具，为理解ViT信息结构与归纳偏置提供了新视角。

Abstract: While insights into the workings of the transformer model have largely emerged by analysing their behaviour on language tasks, this work investigates the representations learnt by the Vision Transformer (ViT) encoder through the lens of compositionality. We introduce a framework, analogous to prior work on measuring compositionality in representation learning, to test for compositionality in the ViT encoder. Crucial to drawing this analogy is the Discrete Wavelet Transform (DWT), which is a simple yet effective tool for obtaining input-dependent primitives in the vision setting. By examining the ability of composed representations to reproduce original image representations, we empirically test the extent to which compositionality is respected in the representation space. Our findings show that primitives from a one-level DWT decomposition produce encoder representations that approximately compose in latent space, offering a new perspective on how ViTs structure information.

</details>


### [64] [Spectral and Spatial Graph Learning for Multispectral Solar Image Compression](https://arxiv.org/abs/2512.24463)
*Prasiddha Siwakoti,Atefeh Khoshkhahtinat,Piyush M. Mehta,Barbara J. Thompson,Michael S. F. Kirk,Daniel da Silva*

Main category: cs.CV

TL;DR: 提出一种面向多光谱太阳影像的学习式压缩框架，通过频谱间与空间两类图注意模块，提升在有限码率下的光谱与细节保真度；在SDOML六个EUV通道上显著优于强基线（更低MSID、更高PSNR与MS-SSIM）。


<details>
  <summary>Details</summary>
Motivation: 空间任务带宽受限，但太阳多光谱成像需要同时保留精细的光谱与空间结构。现有通用或单一分支的学习式压缩在跨谱段相关建模与细粒度结构保留上不足，需要专门设计以兼顾码率与跨通道保真。

Method: 构建两大模块的端到端压缩网络：1) iSWGE：将各谱段视作图节点，学习边特征以显式建模谱间关系，并采用窗口化处理以提高效率与局部一致性；2) WSGA-C：在空间域引入窗口化稀疏图注意力并结合卷积式通道/空间注意，减少空间冗余、突出细小结构；整体作为可训练编解码器以率失真联合优化。

Result: 在SDOML六个EUV通道上，相比强学习基线，MSID降低20.15%，PSNR最高提升1.09%，对数变换MS-SSIM提升1.62%，在相近bpp下得到更锐利且光谱更忠实的重建。

Conclusion: 通过将谱间图嵌入与空间图注意/卷积注意结合的专用压缩框架，可在低码率下保持太阳多光谱影像的光谱一致性与细节清晰度，优于现有学习式方法；代码已开源，具备复现与扩展潜力。

Abstract: High-fidelity compression of multispectral solar imagery remains challenging for space missions, where limited bandwidth must be balanced against preserving fine spectral and spatial details. We present a learned image compression framework tailored to solar observations, leveraging two complementary modules: (1) the Inter-Spectral Windowed Graph Embedding (iSWGE), which explicitly models inter-band relationships by representing spectral channels as graph nodes with learned edge features; and (2) the Windowed Spatial Graph Attention and Convolutional Block Attention (WSGA-C), which combines sparse graph attention with convolutional attention to reduce spatial redundancy and emphasize fine-scale structures. Evaluations on the SDOML dataset across six extreme ultraviolet (EUV) channels show that our approach achieves a 20.15%reduction in Mean Spectral Information Divergence (MSID), up to 1.09% PSNR improvement, and a 1.62% log transformed MS-SSIM gain over strong learned baselines, delivering sharper and spectrally faithful reconstructions at comparable bits-per-pixel rates. The code is publicly available at https://github.com/agyat4/sgraph .

</details>


### [65] [F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model](https://arxiv.org/abs/2512.24473)
*Devendra K. Jangid,Ripon K. Saha,Dilshan Godaliyadda,Jing Li,Seok-Jun Lee,Hamid R. Sheikh*

Main category: cs.CV

TL;DR: 论文提出用更低层次的视觉特征而非文本特征来条件扩散模型，实现更少幻觉、适配手机相机场景的单张超分辨率。


<details>
  <summary>Details</summary>
Motivation: 消费级摄影中的LR图像质量较高，仅需微弱、无幻觉的生成；而基于文本的T2I扩散模型在SISR中易产生幻觉且不适合高分辨率、分块推理（>12MP）的手机图像。文本特征过于高层，难描述小块纹理；现有模型面向<1MP图像，需patch化，进一步削弱文本条件有效性。

Method: 提出以较低层视觉表征（DINOv2特征）作为条件的Feature-to-Image Diffusion（F2IDiff）基础模型，用更严格且丰富的局部特征约束扩散过程，适配大幅分辨率、分块推理场景，实现SISR。

Result: 在抽象中未给出定量结果，声称通过低层特征条件可在小块级别提供更严格且丰富的描述，从而降低幻觉、提升实际手机SISR质量。

Conclusion: 以低层视觉特征（如DINOv2）替代文本特征作为扩散模型条件，可在高保真LR的手机相机SISR中提供更可控、少幻觉的生成，并适配高分辨率分块推理需求。

Abstract: With the advent of Generative AI, Single Image Super-Resolution (SISR) quality has seen substantial improvement, as the strong priors learned by Text-2-Image Diffusion (T2IDiff) Foundation Models (FM) can bridge the gap between High-Resolution (HR) and Low-Resolution (LR) images. However, flagship smartphone cameras have been slow to adopt generative models because strong generation can lead to undesirable hallucinations. For substantially degraded LR images, as seen in academia, strong generation is required and hallucinations are more tolerable because of the wide gap between LR and HR images. In contrast, in consumer photography, the LR image has substantially higher fidelity, requiring only minimal hallucination-free generation. We hypothesize that generation in SISR is controlled by the stringency and richness of the FM's conditioning feature. First, text features are high level features, which often cannot describe subtle textures in an image. Additionally, Smartphone LR images are at least $12MP$, whereas SISR networks built on T2IDiff FM are designed to perform inference on much smaller images ($<1MP$). As a result, SISR inference has to be performed on small patches, which often cannot be accurately described by text feature. To address these shortcomings, we introduce an SISR network built on a FM with lower-level feature conditioning, specifically DINOv2 features, which we call a Feature-to-Image Diffusion (F2IDiff) Foundation Model (FM). Lower level features provide stricter conditioning while being rich descriptors of even small patches.

</details>


### [66] [Using Large Language Models To Translate Machine Results To Human Results](https://arxiv.org/abs/2512.24518)
*Trishna Niraula,Jonathan Stubblefield*

Main category: cs.CV

TL;DR: 将YOLOv5/YOLOv8的胸片异常检测结果（框与类别）输入LLM（如GPT-4），由其生成放射学报告；较强的语义一致性与临床准确性，但文本风格仍可被区分为“非放射科撰写”。


<details>
  <summary>Details</summary>
Motivation: CV系统在医学影像上能做分类/检测，但输出是结构化结果，仍需放射科医生写叙述式报告；LLM具备从结构化信息生成自然语言的能力，有望自动化报告撰写、缩短工作流程并弥合“检测→叙述”的鸿沟。

Method: 构建端到端流水线：用YOLOv5与YOLOv8在胸片上检测异常（输出边界框和类别），将这些结构化发现输入LLM（采用GPT-4）以生成描述性所见与临床摘要；比较两代YOLO在检测精度、推理延迟，以及生成文本与真实报告的余弦相似度；另进行人工评估（清晰度与行文流畅度）。

Result: AI报告与人工报告在语义上具有较高相似度；GPT-4生成报告清晰度评分高（4.88/5），但自然行文流畅度较低（2.81/5）；同时比较了YOLOv5/YOLOv8在精度与延迟方面的差异（未给出具体数值）。

Conclusion: 基于YOLO的检测+LLM叙述的方案在临床正确性方面可行，但当前生成文本在风格层面仍与放射科医生有所差距；未来需改进写作风格与可读性，同时继续权衡检测性能与推理效率。

Abstract: Artificial intelligence (AI) has transformed medical imaging, with computer vision (CV) systems achieving state-of-the-art performance in classification and detection tasks. However, these systems typically output structured predictions, leaving radiologists responsible for translating results into full narrative reports. Recent advances in large language models (LLMs), such as GPT-4, offer new opportunities to bridge this gap by generating diagnostic narratives from structured findings. This study introduces a pipeline that integrates YOLOv5 and YOLOv8 for anomaly detection in chest X-ray images with a large language model (LLM) to generate natural-language radiology reports. The YOLO models produce bounding-box predictions and class labels, which are then passed to the LLM to generate descriptive findings and clinical summaries. YOLOv5 and YOLOv8 are compared in terms of detection accuracy, inference latency, and the quality of generated text, as measured by cosine similarity to ground-truth reports. Results show strong semantic similarity between AI and human reports, while human evaluation reveals GPT-4 excels in clarity (4.88/5) but exhibits lower scores for natural writing flow (2.81/5), indicating that current systems achieve clinical accuracy but remain stylistically distinguishable from radiologist-authored text.

</details>


### [67] [Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression](https://arxiv.org/abs/2512.24547)
*Manikanta Kotthapalli,Banafsheh Rekabdar*

Main category: cs.CV

TL;DR: 提出一种轻量级多尺度VQ-VAE用于64×64低分辨率视频的紧凑潜表示与重建，基于3D残差卷积与两级层次码本，在UCF101上取得25.96 dB PSNR、0.8375 SSIM，并优于单尺度基线。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器（H.264/HEVC）面向像素域重建，难以与深度学习潜表示原生融合，不利于在CDN、边缘端进行高效存储/传输及在下游ML任务中复用。需要一种既具高压缩比又与ML友好的潜表示学习方法。

Method: 扩展VQ-VAE-2到时空域：采用3D残差卷积构建两级（多尺度）层次潜变量结构；码本向量量化；面向64×64、32帧（16 FPS，2秒）片段；引入基于预训练VGG16的感知损失以提升主观质量；模型总参约18.5M，适配边缘设备。

Result: 在UCF101测试集上达到25.96 dB PSNR与0.8375 SSIM；在验证集上较单尺度基线提升1.41 dB PSNR和0.0248 SSIM。

Conclusion: MS-VQ-VAE能生成紧凑高保真视频潜表示，兼顾压缩与ML集成，适合带宽敏感场景（实时流媒体、移动视频分析、CDN存储优化）与边缘端解码部署。

Abstract: The exponential growth of video traffic has placed increasing demands on bandwidth and storage infrastructure, particularly for content delivery networks (CDNs) and edge devices. While traditional video codecs like H.264 and HEVC achieve high compression ratios, they are designed primarily for pixel-domain reconstruction and lack native support for machine learning-centric latent representations, limiting their integration into deep learning pipelines. In this work, we present a Multi-Scale Vector Quantized Variational Autoencoder (MS-VQ-VAE) designed to generate compact, high-fidelity latent representations of low-resolution video, suitable for efficient storage, transmission, and client-side decoding. Our architecture extends the VQ-VAE-2 framework to a spatiotemporal setting, introducing a two-level hierarchical latent structure built with 3D residual convolutions. The model is lightweight (approximately 18.5M parameters) and optimized for 64x64 resolution video clips, making it appropriate for deployment on edge devices with constrained compute and memory resources. To improve perceptual reconstruction quality, we incorporate a perceptual loss derived from a pre-trained VGG16 network. Trained on the UCF101 dataset using 2-second video clips (32 frames at 16 FPS), on the test set we achieve 25.96 dB PSNR and 0.8375 SSIM. On validation, our model improves over the single-scale baseline by 1.41 dB PSNR and 0.0248 SSIM. The proposed framework is well-suited for scalable video compression in bandwidth-sensitive scenarios, including real-time streaming, mobile video analytics, and CDN-level storage optimization.

</details>


### [68] [PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.24551)
*Yuanhao Cai,Kunpeng Li,Menglin Jia,Jialiang Wang,Junzhe Sun,Feng Liang,Weifeng Chen,Felix Juefei-Xu,Chu Wang,Ali Thabet,Xiaoliang Dai,Xuan Ju,Alan Yuille,Ji Hou*

Main category: cs.CV

TL;DR: 提出一套从数据到训练的物理一致性文本生成视频方法：用VLM链式推理构建物理丰富数据集PhyVidGen-135K，并以物理感知的组式DPO（PhyGDPO）结合VLM物理奖励与LoRA切换式参考高效训练，显著提升在物理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有T2V虽画质好，但难以遵循物理规律，且缺乏含丰富物理交互的数据；基于图形学或提示工程的方案泛化差，无法学到隐式物理推理。

Method: 1) 数据：提出PhyAugPipe，利用带链式思维的VLM自动收集与增强含物理现象/交互的视频数据，构建PhyVidGen-135K；2) 训练：提出PhyGDPO，将组式Plackett-Luce偏好建模用于超越成对比较的整体偏好学习；3) 奖励：设计PGR，将VLM物理一致性评分嵌入优化过程；4) 效率：提出LoRA-SR，避免重复参考权重带来的显存开销，实现高效训练。

Result: 在PhyGenBench与VideoPhy2两个物理一致性评测基准上，显著优于当前开源SOTA。

Conclusion: 结合物理增强数据、组式偏好优化与VLM引导的物理奖励，可系统性提升T2V的物理一致性与泛化；代码、模型和数据将开源，项目页提供更多可视化结果。

Abstract: Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO

</details>


### [69] [OCP-LS: An Efficient Algorithm for Visual Localization](https://arxiv.org/abs/2512.24552)
*Jindi Zhong,Hongxia Wang,Huanshui Zhang*

Main category: cs.CV

TL;DR: 提出一种结合OCP与Hessian对角近似的二阶优化器，在大规模深度学习定位任务上更快更稳、更抗噪且精度具竞争力。


<details>
  <summary>Details</summary>
Motivation: 一阶方法在大规模深度学习、尤其是视觉定位任务中常面临收敛慢、训练不稳、对噪声敏感等问题；全量二阶方法计算/存储代价高。作者希望在不牺牲可扩展性的前提下，利用二阶曲率信息提高收敛速度和稳健性。

Method: 构建一种二阶优化框架：引入OCP（优化控制/最优控制范式，具体未明）并对Hessian矩阵的对角元素进行合适近似，以低成本捕获二阶信息；据称可在大规模设置中高效实现。

Result: 在多个标准视觉定位基准上进行大量实验，所提方法相较传统优化算法表现出更快收敛、更稳定训练、对噪声更鲁棒，同时达到具有竞争力的定位精度。

Conclusion: 该二阶优化器在实际大规模视觉定位训练中优于常规方法，兼顾速度、稳定性与鲁棒性，显示出在深度学习优化中的潜在实用价值。

Abstract: This paper proposes a novel second-order optimization algorithm. It aims to address large-scale optimization problems in deep learning because it incorporates the OCP method and appropriately approximating the diagonal elements of the Hessian matrix. Extensive experiments on multiple standard visual localization benchmarks demonstrate the significant superiority of the proposed method. Compared with conventional optimiza tion algorithms, our framework achieves competitive localization accuracy while exhibiting faster convergence, enhanced training stability, and improved robustness to noise interference.

</details>


### [70] [RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios](https://arxiv.org/abs/2512.24561)
*Tianyi Zhao,Jiawen Xi,Linhui Xiao,Junnan Li,Xue Yang,Maoxun Yuan,Xingxing Wei*

Main category: cs.CV

TL;DR: 提出RGBT-Ground，一个面向复杂真实场景的可视化指代（视觉指向/grounding）新基准，并给出统一框架与基线RGBT-VGNet，在夜间与远距等困难条件显著优于改造的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉指代数据多源自COCO等干净环境，场景多样性不足，无法反映真实世界中光照、天气等变化带来的复杂性，难以评估在安全关键场景中的鲁棒性与泛化能力。

Method: 1) 构建RGBT-Ground：同位RGB与热红外(TIR)成对图像，配高质量指代表达、目标框，以及场景/环境/目标多粒度标注；2) 提供统一视觉指代框架，支持单模态(RGB或TIR)与多模态(RGB+TIR)输入；3) 基于该框架提出融合互补模态的简单有效基线RGBT-VGNet；4) 对现有方法进行适配并在新基准上系统评测。

Result: 在RGBT-Ground上，RGBT-VGNet显著优于经适配的现有方法，尤其在夜间与长距离场景中表现突出。

Conclusion: RGBT-Ground填补复杂真实环境下视觉指代评测空白，统一框架与RGBT-VGNet为多模态鲁棒指代提供强基线；数据与代码将公开，促进该方向研究。

Abstract: Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.

</details>


### [71] [Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning](https://arxiv.org/abs/2512.24591)
*Fuyu Dong,Ke Li,Di Wang,Nan Luo,Yiming Zhang,Kaiyu Li,Jianfei Yang,Quan Wang*

Main category: cs.CV

TL;DR: 提出DARFT：在CDVQA中通过挖掘决策歧义样本并用群相对策略优化进行强化微调，压制强干扰项，显著优于仅SFT，尤在小样本下。


<details>
  <summary>Details</summary>
Motivation: SFT提升CDVQA有效但常因决策歧义导致错误：模型对正确答案与强干扰项置信度接近。需要显式优化这些歧义样本以提高判别性与鲁棒性。

Method: 1) 定义决策歧义样本(DAS)：正确答案与最具竞争替代项的概率差距小；2) 用SFT训练的参考策略挖掘DAS；3) 在挖掘到的子集上进行决策歧义引导的强化微调DARFT：采用多样本解码生成候选，利用组内相对优势(Group-relative Policy Optimization)对比奖惩，压制强干扰、拉大决策边界，无需额外监督。

Result: 在多组实验中，相比SFT基线取得一致增益，尤其在小样本（few-shot）场景下提升更明显；显示出更强的判别能力与鲁棒性。

Conclusion: 针对CDVQA中的决策歧义，DARFT通过识别并专门优化DAS，利用组相对优势的强化微调有效削弱干扰、锐化边界，为提升CDVQA性能提供通用且数据高效的途径。

Abstract: Change detection visual question answering (CDVQA) requires answering text queries by reasoning about semantic changes in bi-temporal remote sensing images. A straightforward approach is to boost CDVQA performance with generic vision-language models via supervised fine-tuning (SFT). Despite recent progress, we observe that a significant portion of failures do not stem from clearly incorrect predictions, but from decision ambiguity, where the model assigns similar confidence to the correct answer and strong distractors. To formalize this challenge, we define Decision-Ambiguous Samples (DAS) as instances with a small probability margin between the ground-truth answer and the most competitive alternative. We argue that explicitly optimizing DAS is crucial for improving the discriminability and robustness of CDVQA models. To this end, we propose DARFT, a Decision-Ambiguity-guided Reinforcement Fine-Tuning framework that first mines DAS using an SFT-trained reference policy and then applies group-relative policy optimization on the mined subset. By leveraging multi-sample decoding and intra-group relative advantages, DARFT suppresses strong distractors and sharpens decision boundaries without additional supervision. Extensive experiments demonstrate consistent gains over SFT baselines, particularly under few-shot settings.

</details>


### [72] [SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks](https://arxiv.org/abs/2512.24592)
*Wei Zhang,Chaoqun Wang,Zixuan Guan,Sam Kao,Pengfei Zhao,Peng Wu,Sifeng He*

Main category: cs.CV

TL;DR: 提出SliceLens与FeSD：用LLM/VLM进行有根据视觉推理的假设生成与验证，发现多实例视觉任务中的细粒度错误切片，并提供首个针对该问题的基准；在FeSD上显著提升Precision@10（0.73 vs 0.31），并能指导模型修复。


<details>
  <summary>Details</summary>
Motivation: 现有切片发现方法多聚焦分类任务，难以处理检测/分割/姿态等多实例场景；实际失败常源于复杂关系与角落案例，缺乏细粒度推理导致洞察不足；现有基准偏算法/任务、标注不贴近真实失败。

Method: 提出SliceLens：以假设为驱动，利用LLM与VLM生成多样化“失败假设”，并通过有锚定（grounded）的视觉推理进行验证，从而识别细粒度、可解释的错误切片；同时构建FeSD基准，提供专家标注、精细定位到局部错误区域的真值切片，用于评测细粒度切片发现能力。

Result: 在现有基准与新FeSD上取得SOTA：FeSD的Precision@10从0.31提升到0.73（+0.42）；发现的切片具备可解释性，并在模型修复实验中带来实证改进。

Conclusion: SliceLens通过LLM/VLM的有根据推理显著提升多实例视觉任务的细粒度错误切片发现能力；FeSD为该方向提供了首个严格评测基准；方法不仅评测更可靠，也能指导实际模型改进。

Abstract: Systematic failures of computer vision models on subsets with coherent visual patterns, known as error slices, pose a critical challenge for robust model evaluation. Existing slice discovery methods are primarily developed for image classification, limiting their applicability to multi-instance tasks such as detection, segmentation, and pose estimation. In real-world scenarios, error slices often arise from corner cases involving complex visual relationships, where existing instance-level approaches lacking fine-grained reasoning struggle to yield meaningful insights. Moreover, current benchmarks are typically tailored to specific algorithms or biased toward image classification, with artificial ground truth that fails to reflect real model failures. To address these limitations, we propose SliceLens, a hypothesis-driven framework that leverages LLMs and VLMs to generate and verify diverse failure hypotheses through grounded visual reasoning, enabling reliable identification of fine-grained and interpretable error slices. We further introduce FeSD (Fine-grained Slice Discovery), the first benchmark specifically designed for evaluating fine-grained error slice discovery across instance-level vision tasks, featuring expert-annotated and carefully refined ground-truth slices with precise grounding to local error regions. Extensive experiments on both existing benchmarks and FeSD demonstrate that SliceLens achieves state-of-the-art performance, improving Precision@10 by 0.42 (0.73 vs. 0.31) on FeSD, and identifies interpretable slices that facilitate actionable model improvements, as validated through model repair experiments.

</details>


### [73] [3D Semantic Segmentation for Post-Disaster Assessment](https://arxiv.org/abs/2512.24593)
*Nhut Le,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出利用UAV采集的飓风Ian灾后航拍视频，通过SfM与MVS重建3D点云，构建专门的灾后3D语义分割数据集，并用FPT、PTv3、OA-CNN等SOTA模型评测，发现性能不足，呼吁改进算法与基准。


<details>
  <summary>Details</summary>
Motivation: 灾害频发造成生命与经济损失；灾后评估需要3D语义分割，但现有模型/数据集不针对灾后场景，存在域差异与泛化弱的问题。

Method: 用无人机采集飓风Ian受灾区航拍数据；采用SfM与MVS重建高密度3D点云；标注并构建灾后专题3D数据集；在其上评测SOTA模型（FPT、PTv3、OA-CNN），系统比较其表现。

Result: 在该灾后数据集上，现有SOTA模型表现显著受限，暴露出对灾后复杂破坏场景的鲁棒性不足与泛化缺陷。

Conclusion: 应开发面向灾后的3D语义分割新方法，并建立专门的3D基准数据集，以提升灾后场景理解与应急响应能力。

Abstract: The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses. While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments. To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds. We evaluated the state-of-the-art (SOTA) 3D semantic segmentation models, Fast Point Transformer (FPT), Point Transformer v3 (PTv3), and OA-CNNs on this dataset, exposing significant limitations in existing methods for disaster-stricken regions. These findings underscore the urgent need for advancements in 3D segmentation techniques and the development of specialized 3D benchmark datasets to improve post-disaster scene understanding and response.

</details>


### [74] [Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers](https://arxiv.org/abs/2512.24603)
*Zheng Liu,Jinchao Zhu,Gao Huang*

Main category: cs.CV

TL;DR: 提出CLoRA：通过共享低秩投影基空间与样本无关的多样性增强，在保证参数效率下提升微调ViT性能，并在图像与点云任务上以更少GFLOPs达SOTA平衡。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA改进要么追求更少参数导致性能下降，要么提升表示但引入过多可训练参数，难以在“性能-参数效率”之间取得兼顾。需要一种既扩展低秩模块表达力又保持高效的调优方法。

Method: 设计协同低秩适配（CLoRA），包含两部分：1）基空间共享（base-space sharing）：所有低秩模块共享一组下/上投影的基空间，不同层的LRM由这些共享空间协同构造，从而在固定参数预算下扩大表示容量；2）样本无关的多样性增强（SADE）：对由共享空间生成的多个低秩表示施加相似度正则，鼓励相互去冗余、提升多样性，且正则与样本无关、训练期可通用。

Result: 在常用图像与点云数据集上进行广泛实验，相比SOTA方法，CLoRA在相同或更低参数量下取得更优性能，并且在点云分析中需要的GFLOPs最少。

Conclusion: CLoRA通过共享基空间与多样性正则协同提升LoRA的表达力与鲁棒性，在不增加大量参数的前提下实现更佳的性能-效率权衡，并在图像与点云任务上验证了其有效性。

Abstract: Low-rank adaptation (LoRA) has achieved remarkable success in fine-tuning pre-trained vision transformers for various downstream tasks. Existing studies mainly focus on exploring more parameter-efficient strategies or more effective representation learning schemes. However, these methods either sacrifice fine-tuning performance or introduce excessive trainable parameters, failing to strike a balance between learning performance and parameter efficiency. To address this problem, we propose a novel tuning method named collaborative low-rank adaptation (CLoRA) in this paper. CLoRA consists of base-space sharing and sample-agnostic diversity enhancement (SADE) components. To maintain parameter efficiency while expanding the learning capacity of low-rank modules (LRMs), base-space sharing allows all LRMs to share a set of down/up-projection spaces. In CLoRA, the low-rank matrices obtained from the shared spaces collaboratively construct each LRM. Since the representations extracted by these matrices may contain redundant information, SADE is employed to regularize the similarities among them to encourage diverse representations in the training process. We conduct extensive experiments on widely used image and point cloud datasets to evaluate the performance of CLoRA. Experimental results demonstrate that CLoRA strikes a better balance between learning performance and parameter efficiency, while requiring the fewest GFLOPs for point cloud analysis, compared with the state-of-the-art methods.

</details>


### [75] [MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding](https://arxiv.org/abs/2512.24605)
*Panquan Yang,Junfei Huang,Zongzhangbao Yin,Yingsong Hu,Anni Xu,Xinyi Luo,Xueqi Sun,Hai Wu,Sheng Ao,Zhaoxing Zhu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: 提出面向道路监控场景的3D视觉指向新任务与数据集MoniRefer，并给出多模态方法Moni3DVG，实验证明有效，将开源。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉指向主要集中在室内或车载视角的户外驾驶场景，缺乏来自路侧基础设施（监控）场景的点云-文本配对数据，导致基础设施层面理解复杂交通环境的需求无法满足。

Method: 1) 构建MoniRefer数据集：从多个真实复杂路口采集路侧传感器数据，标注136,018个目标与411,128条自然语言表达，并对语言与3D标签进行人工核验；2) 提出端到端方法Moni3DVG：融合图像外观信息与点云几何/光学信息，进行多模态特征学习与3D目标定位；3) 在所提基准上进行大量实验与消融。

Result: 在MoniRefer基准上，Moni3DVG取得优于现有方法的性能（抽象中未给出具体指标），消融验证了各模态与设计对性能的贡献。

Conclusion: 路侧监控场景的3D视觉指向是重要且此前未被充分探索的课题；MoniRefer填补数据空白，Moni3DVG提供有效技术方案；实验显示优越性，数据与代码将开源以推动研究。

Abstract: 3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.

</details>


### [76] [LLHA-Net: A Hierarchical Attention Network for Two-View Correspondence Learning](https://arxiv.org/abs/2512.24620)
*Shuyuan Lin,Yu Guo,Xiao Chen,Yanjie Liang,Guobao Xiao,Feiran Huang*

Main category: cs.CV

TL;DR: 提出分层层次注意网络（LL-HAN），通过分阶段融合、分层注意与两种特征提取架构，提升含大量离群点场景下的特征点匹配精度；在YFCC100M与SUN3D上优于多种SOTA于离群点剔除与位姿估计。


<details>
  <summary>Details</summary>
Motivation: 特征点匹配常受大量离群点干扰，导致鲁棒性与精度下降；高比例负样本时如何保留高质量信息并降低误匹配成为难题。

Method: (1) 分阶段特征语义保留与整体融合：提出逐层通道融合模块，保留各阶段的语义并实现全局融合以增强表征。
(2) 分层注意模块：自适应融合全局感知与结构语义信息，强调关键特征、抑制离群点。
(3) 两种特征提取与集成架构：用于不同场景的特征提炼与融合，提高网络适配性。

Result: 在YFCC100M与SUN3D数据集上，所提方法在离群点剔除和相机位姿估计上优于多种现有方法（定性与定量均有提升）。

Conclusion: 通过分阶段融合与分层注意机制，显著提升在高离群点环境下的匹配鲁棒性与精度；方法通用且在公开数据集验证有效，源代码可用。

Abstract: Establishing the correct correspondence of feature points is a fundamental task in computer vision. However, the presence of numerous outliers among the feature points can significantly affect the matching results, reducing the accuracy and robustness of the process. Furthermore, a challenge arises when dealing with a large proportion of outliers: how to ensure the extraction of high-quality information while reducing errors caused by negative samples. To address these issues, in this paper, we propose a novel method called Layer-by-Layer Hierarchical Attention Network, which enhances the precision of feature point matching in computer vision by addressing the issue of outliers. Our method incorporates stage fusion, hierarchical extraction, and an attention mechanism to improve the network's representation capability by emphasizing the rich semantic information of feature points. Specifically, we introduce a layer-by-layer channel fusion module, which preserves the feature semantic information from each stage and achieves overall fusion, thereby enhancing the representation capability of the feature points. Additionally, we design a hierarchical attention module that adaptively captures and fuses global perception and structural semantic information using an attention mechanism. Finally, we propose two architectures to extract and integrate features, thereby improving the adaptability of our network. We conduct experiments on two public datasets, namely YFCC100M and SUN3D, and the results demonstrate that our proposed method outperforms several state-of-the-art techniques in both outlier removal and camera pose estimation. Source code is available at http://www.linshuyuan.com.

</details>


### [77] [FireRescue: A UAV-Based Dataset and Enhanced YOLO Model for Object Detection in Fire Rescue Scenes](https://arxiv.org/abs/2512.24622)
*Qingyu Xu,Runtong Zhang,Zihuan Qiu,Fanman Meng*

Main category: cs.CV

TL;DR: 提出FireRescue数据集与FRS-YOLO模型，面向城市等多场景消防救援目标检测，缓解类别混淆与小目标漏检，显著提升YOLO系在该场景的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有消防场景目标检测多集中于山地/森林，忽视更复杂且更常见的城市救援；类别覆盖单一（多为火焰、烟），缺乏对指挥决策关键目标（消防车、消防员等）的系统性检测，且混乱环境与远距离拍摄导致类别混淆与小目标漏检。

Method: 1) 构建FireRescue数据集：涵盖城市、山地、森林、水域等多场景，8个关键类别，共15,980张图、32,000标注框；2) 提出FRS-YOLO：- 引入可插拔的多维协同增强注意力模块，通过跨通道/空间等维度的特征交互，提升易混类别的判别性；- 集成动态特征采样器，强化高响应前景特征，缓解烟雾遮挡与背景干扰。

Result: 实验表明消防救援场景目标检测具有高挑战性；在该数据上，FRS‑YOLO相较YOLO系列基线取得更优检测性能（摘要未给出具体数值）。

Conclusion: FireRescue提供了更全面的救援指挥数据基础；FRS‑YOLO有效缓解类别混淆与小目标漏检，提升YOLO在消防救援场景的检测效果，适用于复杂城市等多场景。

Abstract: Object detection in fire rescue scenarios is importance for command and decision-making in firefighting operations. However, existing research still suffers from two main limitations. First, current work predominantly focuses on environments such as mountainous or forest areas, while paying insufficient attention to urban rescue scenes, which are more frequent and structurally complex. Second, existing detection systems include a limited number of classes, such as flames and smoke, and lack a comprehensive system covering key targets crucial for command decisions, such as fire trucks and firefighters. To address the above issues, this paper first constructs a new dataset named "FireRescue" for rescue command, which covers multiple rescue scenarios, including urban, mountainous, forest, and water areas, and contains eight key categories such as fire trucks and firefighters, with a total of 15,980 images and 32,000 bounding boxes. Secondly, to tackle the problems of inter-class confusion and missed detection of small targets caused by chaotic scenes, diverse targets, and long-distance shooting, this paper proposes an improved model named FRS-YOLO. On the one hand, the model introduces a plug-and-play multidi-mensional collaborative enhancement attention module, which enhances the discriminative representation of easily confused categories (e.g., fire trucks vs. ordinary trucks) through cross-dimensional feature interaction. On the other hand, it integrates a dynamic feature sampler to strengthen high-response foreground features, thereby mitigating the effects of smoke occlusion and background interference. Experimental results demonstrate that object detection in fire rescue scenarios is highly challenging, and the proposed method effectively improves the detection performance of YOLO series models in this context.

</details>


### [78] [From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation](https://arxiv.org/abs/2512.24639)
*Siyang Wang,Hanting Li,Wei Li,Jie Hu,Xinghao Chen,Feng Zhao*

Main category: cs.CV

TL;DR: RadAR 提出一种“径向并行”自回归视觉生成：以中心token为起点，按到中心的距离把所有token分成同心环，逐环生成、环内并行，并配合嵌套注意力在前向中动态纠错，显著提升推理效率而保持表示能力。


<details>
  <summary>Details</summary>
Motivation: 传统自回归视觉生成沿栅格逐token解码，强序依赖导致推理慢；而视觉token具有强局部相关性与空间结构性，现有光栅扫描未充分利用这些局部/空间相关，限制并行化与效率。

Method: 1) 径向拓扑：选定起始中心token，按空间距离将其他token划分为多重“同心环”；2) 环级生成：从内到外逐环推进，同一环内token并行预测；3) 嵌套注意力：在并行生成时动态识别并修正不合理输出，缓解因上下文不足带来的不一致与误差累积。

Result: 在不牺牲表达能力的前提下，实现显著的并行度提升与生成效率加速，并通过动态纠错避免模型坍塌与错误蔓延。

Conclusion: 径向并行预测结合动态输出校正可在自回归视觉生成中同时获得高效率与空间一致性，为突破传统顺序解码的效率瓶颈提供了通用框架。

Abstract: Inspired by the remarkable success of autoregressive models in language modeling, this paradigm has been widely adopted in visual generation. However, the sequential token-by-token decoding mechanism inherent in traditional autoregressive models leads to low inference efficiency.In this paper, we propose RadAR, an efficient and parallelizable framework designed to accelerate autoregressive visual generation while preserving its representational capacity. Our approach is motivated by the observation that visual tokens exhibit strong local dependencies and spatial correlations with their neighbors--a property not fully exploited in standard raster-scan decoding orders. Specifically, we organize the generation process around a radial topology: an initial token is selected as the starting point, and all other tokens are systematically grouped into multiple concentric rings according to their spatial distances from this center. Generation then proceeds in a ring-wise manner, from inner to outer regions, enabling the parallel prediction of all tokens within the same ring. This design not only preserves the structural locality and spatial coherence of visual scenes but also substantially increases parallelization. Furthermore, to address the risk of inconsistent predictions arising from simultaneous token generation with limited context, we introduce a nested attention mechanism. This mechanism dynamically refines implausible outputs during the forward pass, thereby mitigating error accumulation and preventing model collapse. By integrating radial parallel prediction with dynamic output correction, RadAR significantly improves generation efficiency.

</details>


### [79] [Renormalization Group Guided Tensor Network Structure Search](https://arxiv.org/abs/2512.24663)
*Maolin Wang,Bowen Yu,Sheng Zhang,Linjie Mi,Wanyu Wang,Yiqi Wang,Pengyue Jia,Xuetao Wei,Zenglin Xu,Ruocheng Guo,Xiangyu Zhao*

Main category: cs.CV

TL;DR: 提出RGTN，一种受重整化群启发的张量网络结构搜索框架，通过多尺度连续结构演化与可学习边门，实现更快更稳的TN结构与秩配置发现，达到SOTA压缩并提速4–600×。


<details>
  <summary>Details</summary>
Motivation: 现有TN结构搜索在三方面受限：仅单尺度优化忽视多尺度结构；离散搜索空间阻碍结构的平滑演化；结构与参数分离优化导致计算低效与鲁棒性差。需要一种可在多尺度上自适应、连续地演化结构且能高效联合优化的方法。

Method: 引入物理学中的重整化群思想：从粗尺度低复杂度网络出发，逐步细化到更细尺度，允许结构在尺度流中连续演化。核心机制包括：1）可学习的edge gates，在优化阶段动态开关/调节边以修改拓扑；2）基于物理量的智能提议：节点张力（度量局部应力）与边信息流（度量连接重要性）指导添加/剪枝与秩调整；3）尺度诱导的扰动帮助跳出局部最优；4）结构与参数的联合优化流程，在多尺度间传递；从而在粗到细的阶段性优化中搜索紧凑高效的TN结构。

Result: 在光场数据、高阶合成张量与视频补全任务上，RGTN取得最先进的压缩率，并相较现有方法快4–600倍，同时能发现更紧凑、泛化更好的TN结构。

Conclusion: RGTN通过多尺度重整化群引导的连续结构演化与可学习拓扑调整，克服了TN-SS在可计算性、结构自适应与优化鲁棒性上的瓶颈，提供了高效、稳健且可扩展的张量网络结构搜索方案。

Abstract: Tensor network structure search (TN-SS) aims to automatically discover optimal network topologies and rank configurations for efficient tensor decomposition in high-dimensional data representation. Despite recent advances, existing TN-SS methods face significant limitations in computational tractability, structure adaptivity, and optimization robustness across diverse tensor characteristics. They struggle with three key challenges: single-scale optimization missing multi-scale structures, discrete search spaces hindering smooth structure evolution, and separated structure-parameter optimization causing computational inefficiency. We propose RGTN (Renormalization Group guided Tensor Network search), a physics-inspired framework transforming TN-SS via multi-scale renormalization group flows. Unlike fixed-scale discrete search methods, RGTN uses dynamic scale-transformation for continuous structure evolution across resolutions. Its core innovation includes learnable edge gates for optimization-stage topology modification and intelligent proposals based on physical quantities like node tension measuring local stress and edge information flow quantifying connectivity importance. Starting from low-complexity coarse scales and refining to finer ones, RGTN finds compact structures while escaping local minima via scale-induced perturbations. Extensive experiments on light field data, high-order synthetic tensors, and video completion tasks show RGTN achieves state-of-the-art compression ratios and runs 4-600$\times$ faster than existing methods, validating the effectiveness of our physics-inspired approach.

</details>


### [80] [Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting](https://arxiv.org/abs/2512.24702)
*Kai Ye,Xiaotong You,Jianghang Lin,Jiayi Ji,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出EVOL-SAM3：把推理分割从一次性“生成-分割”改为推理时的进化搜索，通过生成-评估-进化循环在零样本条件下显著超越静态方法，甚至超过全监督SOTA。


<details>
  <summary>Details</summary>
Motivation: SFT易灾难性遗忘且依赖领域，RL训练不稳且依赖预定义奖励；无训练方法虽省去训练但推理静态、缺乏深度推理与自纠错能力，易产生语言幻觉与空间误判。

Method: 将推理分割表述为推理时的进化搜索：维护一组提示词种群，循环执行“生成-评估-进化”。1) 视觉竞技场(Visual Arena)进行无参考成对对战评估以估计提示适应度；2) 语义变异(Semantic Mutation)引入多样性并纠正语义错误；3) 异构竞技场(Heterogeneous Arena)融合几何先验与语义推理做鲁棒最终选择。整个流程为零样本、无需训练。

Result: 在ReasonSeg基准上，EVOL-SAM3在零样本设定下显著优于所有静态基线，并且性能超过全监督的最新SOTA。

Conclusion: 将推理分割转化为进化式、推理时可自我改进的搜索范式，有效克服SFT/RL与静态推理的局限，带来强鲁棒性与更高精度；框架通用且训练无关，具备良好可扩展性。

Abstract: Reasoning Segmentation requires models to interpret complex, context-dependent linguistic queries to achieve pixel-level localization. Current dominant approaches rely heavily on Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). However, SFT suffers from catastrophic forgetting and domain dependency, while RL is often hindered by training instability and rigid reliance on predefined reward functions. Although recent training-free methods circumvent these training burdens, they are fundamentally limited by a static inference paradigm. These methods typically rely on a single-pass "generate-then-segment" chain, which suffers from insufficient reasoning depth and lacks the capability to self-correct linguistic hallucinations or spatial misinterpretations. In this paper, we challenge these limitations and propose EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. Instead of relying on a fixed prompt, EVOL-SAM3 maintains a population of prompt hypotheses and iteratively refines them through a "Generate-Evaluate-Evolve" loop. We introduce a Visual Arena to assess prompt fitness via reference-free pairwise tournaments, and a Semantic Mutation operator to inject diversity and correct semantic errors. Furthermore, a Heterogeneous Arena module integrates geometric priors with semantic reasoning to ensure robust final selection. Extensive experiments demonstrate that EVOL-SAM3 not only substantially outperforms static baselines but also significantly surpasses fully supervised state-of-the-art methods on the challenging ReasonSeg benchmark in a zero-shot setting. The code is available at https://github.com/AHideoKuzeA/Evol-SAM3.

</details>


### [81] [FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation](https://arxiv.org/abs/2512.24724)
*Jibin Song,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出FlowBlending：在扩散/流式生成采样中，按时间步对“容量敏感”与“不敏感”阶段分治，用大模型负责早晚阶段、小模型负责中间阶段，在保证质量的同时显著提速降FLOPs。


<details>
  <summary>Details</summary>
Motivation: 生成视频/图像模型推理成本高，且不同时间步对模型容量的依赖可能不均衡；若能在不损质量的情况下用小模型承担一部分时间步，可显著加速。

Method: 1) 经验和分析表明：早期与末期时间步对容量敏感，中段不敏感；2) 提出FlowBlending：在容量敏感阶段用大模型，在中间阶段切换小模型；3) 给出简单的阶段边界选择准则，并提出基于速度（流）发散度的分析作为容量敏感区域的代理指标；4) 与现有采样加速技巧可组合。

Result: 在LTX-Video(2B/13B)与WAN 2.1(1.3B/14B)上，推理最高加速1.65×，FLOPs降低57.35%，同时维持大模型的视觉质量、时序一致性与语义对齐；与其他采样加速叠加可再获至多2×的额外加速。

Conclusion: 模型容量需求随时间步异质；利用阶段感知的多模型采样可在几乎不损质量下显著降算与提速，且通用可与其他加速方法兼容。

Abstract: In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.

</details>


### [82] [EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation](https://arxiv.org/abs/2512.24731)
*Bingxuan Li,Yiming Cui,Yicheng He,Yiwei Wang,Shu Zhang,Longyin Wen,Yulei Niu*

Main category: cs.CV

TL;DR: EchoFoley 引入视频绑定的细粒度可控声音生成任务与数据集，并提出基于事件的代理式框架 EchoVidia，在可控性与感知质量上显著优于现有 VT2A 模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本到音频方法存在三大问题：视觉与文本条件失衡（视觉主导）；缺乏对细粒度可控生成的明确定义；指令理解与跟随能力弱（数据多为简短类别标签）。需要一种能在事件层面精准控制并理解层级语义/指令的声音生成范式。

Method: 1) 定义新任务 EchoFoley，实现视频约束下的声音生成，兼顾事件级局部控制与层级语义控制；2) 引入符号化事件表示，明确每个声音在何时、何物、如何发声，支持生成、插入、编辑等精细操作；3) 构建专家标注的 EchoFoley-6k 数据集（视频-指令-注释三元组）；4) 提出 EchoVidia：以声音事件为中心的代理式生成框架，采用“慢-快思考”策略以提升规划与执行能力。

Result: 在实验中，EchoVidia 相比最新 VT2A 基线在可控性提升40.7%，在感知质量提升12.5%。

Conclusion: 通过事件级符号表示、层级控制与代理式慢-快思考框架，EchoFoley/EchoVidia 实现对视频声音生成的精细可控与更高感知质量，为多模态叙事中的声效生成提供了更强的指令跟随与编辑能力。

Abstract: Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.

</details>


### [83] [Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2512.24742)
*Xiang Liu,Yimin Zhou,Jinxiang Wang,Yujun Huang,Shuzhao Xie,Shiyu Qin,Mingyao Hong,Jiawei Li,Yaowei Wang,Zhi Wang,Shu-Tao Xia,Bin Chen*

Main category: cs.CV

TL;DR: Splatwizard 是一个面向 3D Gaussian Splatting（3DGS）压缩模型的统一基准工具包，提供实现与评测的一站式框架与自动化指标管线。


<details>
  <summary>Details</summary>
Motivation: 3DGS 在实时新视角合成上突飞猛进，相关压缩算法激增，但现有基准缺乏针对性与全面性，无法同时衡量渲染速度、率失真权衡、内存效率与几何精度等关键特性，导致方法比较不统一也不公平。

Method: 提出 Splatwizard：1) 统一框架，便于集成与实现新的 3DGS 压缩模型，同时复用既有 SOTA 技术；2) 集成自动化评测管线，计算图像质量指标、重建网格的 Chamfer 距离、渲染帧率以及算力/内存等资源消耗；3) 开源代码与标准化接口，降低对比与复现实验成本。

Result: 提供一个开箱即用的评测工具包，能够在统一设置下对 3DGS 压缩方法进行系统性、可重复的比较，覆盖感知质量、几何精度、速度与资源占用等维度。

Conclusion: Splatwizard 填补了 3DGS 压缩评测的空白，建立了全面而标准的基准流程，促进领域内方法的公平比较与快速迭代；代码已开源以便社区采用与扩展。

Abstract: The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis. However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task. Existing benchmarks often lack the specific metrics necessary to holistically assess the unique characteristics of different methods, such as rendering speed, rate distortion trade-offs memory efficiency, and geometric accuracy. To address this gap, we introduce Splatwizard, a unified benchmark toolkit designed specifically for benchmarking 3DGS compression models. Splatwizard provides an easy-to-use framework to implement new 3DGS compression model and utilize state-of-the-art techniques proposed by previous work. Besides, an integrated pipeline that automates the calculation of key performance indicators, including image-based quality metrics, chamfer distance of reconstruct mesh, rendering frame rates, and computational resource consumption is included in the framework as well. Code is available at https://github.com/splatwizard/splatwizard

</details>


### [84] [UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning](https://arxiv.org/abs/2512.24763)
*Ankit Dhiman,Srinath R,Jaswanth Reddy,Lokesh R Boregowda,Venkatesh Babu Radhakrishnan*

Main category: cs.CV

TL;DR: 提出一个将3D实例/语义分割统一到3DGS的端到端框架：在高斯原语中学习可训练特征嵌入，并通过新颖的Embedding-to-Label解码得到实例标签；引入边界样本挖掘与线性层+三元组损失稳定训练，显著优于现有两阶段方法。


<details>
  <summary>Details</summary>
Motivation: 现有多视图2D到3D分割面临跨视角实例标签不一致，导致3D预测差、训练耗时且依赖对比学习与聚类等超参敏感或繁琐的预处理。需要一种能统一优化、减少步骤并提升鲁棒性的端到端方案。

Method: 在3D高斯点（primitives）中为分割学习可训练的特征嵌入；提出Embedding-to-Label模块，将嵌入高效解码为实例标签，实现训练流程的一体化；观察到物体边界伪影后，设计边界区域的困难样本挖掘；直接在嵌入上做hard mining不稳定，于是对栅格化后的特征嵌入施加线性层，再计算三元组损失以稳定与强化边界判别。

Result: 在ScanNet、Replica3D、Messy-Rooms上，定性与定量均优于多种基线；训练时间降低且性能提升，尤其在边界区域显著改善。

Conclusion: 统一的嵌入学习与标签解码框架解决了跨视角标签不一致与两阶段训练的缺点；通过边界难例挖掘与线性层+三元组损失，稳定并提升了边界分割质量，提供高效而准确的3D实例/语义分割方案。

Abstract: 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel "Embedding-to-Label" process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.

</details>


### [85] [Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation](https://arxiv.org/abs/2512.24792)
*Takeru Kusakabe,Yudai Hirose,Mashiho Mukaida,Satoshi Ono*

Main category: cs.CV

TL;DR: 论文提出一种基于投影的对抗攻击：通过向目标物体投射特定光扰动，使单目深度估计网络产生严重误判，甚至导致场景中物体部分“消失”。方法在真实环境中闭环优化，实验显示攻击有效。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计在自动驾驶/机器人等场景关键，但DNN 对抗脆弱性会导致深度误差、感知缺失，影响安全与可靠。现有大多在数字域模拟，忽视设备与环境噪声，缺乏可在真实世界稳定生效的攻击/评估手段。

Method: 提出投影式物理对抗扰动：用投影仪向目标物体投射光图案。采用“物理在环”(PITL)优化，在真实环境对候选扰动进行评估，显式纳入设备特性与扰动因素。优化器使用分布式 CMA-ES 演化策略搜索有效扰动。

Result: 在真实实验中成功生成对抗样本，使单目深度估计产生显著误判，表现为深度偏差和物体局部在深度图中消失，验证了方法的有效性与MDE模型的脆弱性。

Conclusion: DNN 基于的单目深度估计对物理可实现的投影扰动高度脆弱；引入PITL与分布式CMA-ES可稳定找到在真实世界有效的对抗光扰动。该工作为评估与提升MDE鲁棒性提供了现实基线，并提示需考虑物理层面防御。

Abstract: Deep neural networks (DNNs) remain vulnerable to adversarial attacks that cause misclassification when specific perturbations are added to input images. This vulnerability also threatens the reliability of DNN-based monocular depth estimation (MDE) models, making robustness enhancement a critical need in practical applications. To validate the vulnerability of DNN-based MDE models, this study proposes a projection-based adversarial attack method that projects perturbation light onto a target object. The proposed method employs physics-in-the-loop (PITL) optimization -- evaluating candidate solutions in actual environments to account for device specifications and disturbances -- and utilizes a distributed covariance matrix adaptation evolution strategy. Experiments confirmed that the proposed method successfully created adversarial examples that lead to depth misestimations, resulting in parts of objects disappearing from the target scene.

</details>


### [86] [Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training](https://arxiv.org/abs/2512.24794)
*Andrew Tinits,Stephen Mann*

Main category: cs.CV

TL;DR: 提出在Noise2Noise框架下，某些非线性变换可在几乎不引入偏差的前提下用于噪声目标，从而缓解HDR蒙特卡洛渲染去噪中由极端值导致的训练不稳定问题；通过匹配合适的损失与色调映射组合，仅用噪声数据即可接近以高采样干净参考训练的效果。


<details>
  <summary>Details</summary>
Motivation: Noise2Noise避免干净标签，但对目标上的非线性操作会引入偏差，限制实际预处理（如色调映射）的使用；HDR/蒙特卡洛渲染数据存在离群值，直接训练易被淹没，亟需兼顾稳健性与无偏性的理论与实践方案。

Method: 建立分析非线性对Noise2Noise期望无偏性的理论框架，刻画可用且“最小偏差”的非线性函数类；在HDR去噪中，研究色调映射与损失函数的成对选择，使输出与目标在映射域训练以抑制离群值，同时控制由非线性带来的偏差；将方法集成到现有MC去噪网络，并仅用噪声-噪声对进行训练。

Result: 在HDR蒙特卡洛渲染去噪上，选定的色调映射与损失组合显著降低离群值影响，训练稳定；仅用噪声数据训练的模型效果接近使用高采样干净参考训练的原始实现。

Conclusion: 通过理论与实证证明：在Noise2Noise中，恰当设计的非线性（如特定色调映射）与损失可在不显著引入偏差的同时提升稳健性，使仅用噪声数据也能在HDR场景中获得接近有参考训练的性能。

Abstract: The Noise2Noise method allows for training machine learning-based denoisers with pairs of input and target images where both the input and target can be noisy. This removes the need for training with clean target images, which can be difficult to obtain. However, Noise2Noise training has a major limitation: nonlinear functions applied to the noisy targets will skew the results. This bias occurs because the nonlinearity makes the expected value of the noisy targets different from the clean target image. Since nonlinear functions are common in image processing, avoiding them limits the types of preprocessing that can be performed on the noisy targets. Our main insight is that certain nonlinear functions can be applied to the noisy targets without adding significant bias to the results. We develop a theoretical framework for analyzing the effects of these nonlinearities, and describe a class of nonlinear functions with minimal bias.
  We demonstrate our method on the denoising of high dynamic range (HDR) images produced by Monte Carlo rendering. Noise2Noise training can have trouble with HDR images, where the training process is overwhelmed by outliers and performs poorly. We consider a commonly used method of addressing these training issues: applying a nonlinear tone mapping function to the model output and target images to reduce their dynamic range. This method was previously thought to be incompatible with Noise2Noise training because of the nonlinearities involved. We show that certain combinations of loss functions and tone mapping functions can reduce the effect of outliers while introducing minimal bias. We apply our method to an existing machine learning-based Monte Carlo denoiser, where the original implementation was trained with high-sample count reference images. Our results approach those of the original implementation, but are produced using only noisy training data.

</details>


### [87] [Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control](https://arxiv.org/abs/2512.24826)
*Jason Armitage,Rico Sennnrich*

Main category: cs.CV

TL;DR: 提出一种基于遗憾最小化与无导数优化的算法，使仅在2D视觉上训练的跨模态系统，能在线控制场景内摄像机，在3D多物体场景中适应遮挡并区分特征，从而在无需预训练/微调的情况下提升跨模态任务表现。


<details>
  <summary>Details</summary>
Motivation: 跨模态系统多在2D图像上训练，遇到3D场景时存在维度鸿沟；通过在场景内控制摄像机可弥补，但需稳健的控制策略，同时现有互信息估计在噪声和多变量场景下难以可靠指导控制。

Method: 提出改进多变量互信息估计的框架：以遗憾最小化为目标，采用无导数优化来在噪声的视觉-语言模型输出下进行在线优化；将该“表达力强的度量+基于价值的优化”与场景内可控摄像机制结合，实现自适应视角选择与特征区分。

Result: 在3D多物体场景的跨模态任务上，所提出的流水线通过在线摄像机控制，提升了性能，能适应遮挡并更好地区分目标特征。

Conclusion: 无需额外预训练或微调，仅利用2D训练的现成跨模态模型，结合改进的互信息估计与无导数优化控制，即可在3D场景中实现更强的跨模态表现。

Abstract: Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.

</details>


### [88] [CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture](https://arxiv.org/abs/2512.24838)
*Md Ahmed Al Muzaddid,Jordan A. James,William J. Beksi*

Main category: cs.CV

TL;DR: 提出CropTrack：融合外观与运动信息的农业多目标跟踪框架，通过重排序增强外观关联、一对多关联并用外观冲突消解、以及EMA原型特征库，提高遮挡和外观相似场景下的ID保持，较SOTA显著提升IDF1与AA，减少ID切换。


<details>
  <summary>Details</summary>
Motivation: 农业场景中目标（如作物、果实、农机等）高度相似、纹理重复，光照突变与频繁遮挡常见，导致仅靠运动信息的现有跟踪器在强遮挡后难以维持目标身份；而直接引入外观特征又因外观高度相似而易误匹配，亟需兼顾鲁棒外观表示与运动约束的MOT方案。

Method: 提出CropTrack框架：1）重排序（re-ranking）增强的外观关联，提升相似外观间的判别力；2）一对多关联策略，并通过外观冲突消解机制处理多候选竞争，缓解遮挡与近距离聚集时的歧义；3）指数移动平均（EMA）原型特征库，随时间平滑更新目标外观原型，增强在光照变化与部分遮挡下的外观稳定性；最终结合运动与外观进行联合匹配。

Result: 在公开的农业MOT数据集上，CropTrack在身份保持方面表现稳定，相比传统以运动为主的跟踪器取得显著优势；对比SOTA方法，IDF1与AA（association accuracy）明显提升，且身份切换（ID switches）数量更少。

Conclusion: 将外观与运动信息有效融合，并通过重排序、冲突消解与EMA原型库提升外观关联，可在农业场景复杂遮挡与相似外观条件下显著改善MOT的身份保持与关联精度。

Abstract: Multiple-object tracking (MOT) in agricultural environments presents major challenges due to repetitive patterns, similar object appearances, sudden illumination changes, and frequent occlusions. Contemporary trackers in this domain rely on the motion of objects rather than appearance for association. Nevertheless, they struggle to maintain object identities when targets undergo frequent and strong occlusions. The high similarity of object appearances makes integrating appearance-based association nontrivial for agricultural scenarios. To solve this problem we propose CropTrack, a novel MOT framework based on the combination of appearance and motion information. CropTrack integrates a reranking-enhanced appearance association, a one-to-many association with appearance-based conflict resolution strategy, and an exponential moving average prototype feature bank to improve appearance-based association. Evaluated on publicly available agricultural MOT datasets, CropTrack demonstrates consistent identity preservation, outperforming traditional motion-based tracking methods. Compared to the state of the art, CropTrack achieves significant gains in identification F1 and association accuracy scores with a lower number of identity switches.

</details>


### [89] [VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents](https://arxiv.org/abs/2512.24851)
*Xunyi Zhao,Gengze Zhou,Qi Wu*

Main category: cs.CV

TL;DR: 提出VLN-MME统一评测框架，零样本评估MLLM作为具身导航代理，发现加入CoT与自反思反而降性能，揭示其3D空间与顺序决策薄弱。


<details>
  <summary>Details</summary>
Motivation: MLLM在视觉-语言任务上表现强，但其作为具身代理（需要多轮对话的空间推理与序列动作预测）的能力尚不清楚；缺少统一可扩展的评测框架系统检验其在VLN中的泛化与决策能力。

Method: 构建统一、模块化、可扩展的VLN评测基准VLN-MME：将传统导航数据集标准化为统一接口，便于跨模型、代理设计与任务的对比与消融；以零样本设置探测不同MLLM；在基线代理上加入CoT推理与自反思模块，分析其影响。

Result: 在该框架下，增强基线代理的CoT与自反思却导致导航性能意外下降；显示模型虽能遵循指令并格式化输出，但3D空间推理与上下文感知能力弱。

Conclusion: VLN-MME为系统评估通用MLLM在具身导航中的能力提供基础；当前MLLM在顺序决策与三维空间理解方面存在显著不足，对后续作为具身代理的后训练与方法设计具有指导意义。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.

</details>


### [90] [OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation](https://arxiv.org/abs/2512.24861)
*Meng Lan,Lefei Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出OFL-SAM2：在无需人工提示的情况下，将SAM2适配到医疗影像分割，实现少标注高效分割。方法通过轻量映射网络把通用图像特征映射为目标特征，并在推理时在线更新；再与冻结的SAM2记忆注意特征自适应融合，在三套MIS数据集上以少量训练数据达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 将强大的视频分割模型SAM2扩展到医疗图像分割可带来收益，但面临两大痛点：需要大量带标注医疗数据进行微调，以及推理时依赖高质量人工提示（点/框/掩码），成本高且需专家参与。希望在标注稀缺和无需人工提示的前提下，获得稳定准确的医学目标分割。

Method: 设计无提示(prompt-free)框架OFL-SAM2：1) 轻量映射网络学习医疗先验，将通用图像特征映射为目标判别特征；该网络可在推理阶段进行在线参数更新，提高跨序列泛化与自适应能力。2) 在线少样本学习器，利用少量标注数据训练映射网络以生成目标特征。3) 自适应融合模块，将映射网络产生的目标特征与冻结SAM2产生的记忆-注意特征动态融合，形成更稳健的目标表示。整个框架避免手工提示与大规模微调。

Result: 在三个具有差异性的医疗分割数据集上进行广泛实验，使用有限训练数据即可取得最先进（SOTA）性能，表现出准确性与鲁棒性优势。

Conclusion: 通过少样本在线学习与自适应特征融合，OFL-SAM2在无需人工提示与大规模标注的情况下，有效将SAM2迁移到医疗分割，提升泛化与性能，验证了映射网络与在线更新策略的有效性。

Abstract: The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model's generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.

</details>


### [91] [FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation](https://arxiv.org/abs/2512.24903)
*Zichen Tang,Haihong E,Rongjin Li,Jiacheng Liu,Linwei Jia,Zhuodi Hao,Zhongjun Yang,Yuanze Li,Haolin Tian,Xinyi Hu,Peizhi Zhao,Yuan Liu,Zhengyu Wang,Xianghe Wang,Yiling Huang,Xueyuan Lin,Ruofei Bai,Zijian Xie,Qian Huang,Ruining Cao,Haocheng Gao*

Main category: cs.CV

TL;DR: FinMMDocR 是一个面向真实金融数值推理的中英双语多模态基准，强调情景理解、长文档理解与多步计算，当前最佳 MLLM 仅达 58% 准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准在金融真实场景的隐含情景、长篇复杂文档与跨页证据、多步数值推理方面不足，难以全面评估与推动 MLLM 在金融任务中的能力。

Method: 构建包含1,200道专家标注问题与837份中英双语、9类金融文档（平均50.8页、富含可视化）的数据集；问题覆盖12类隐式金融情景，平均需要11步推理（信息抽取与计算分别约5.3与5.7步），65% 需跨页证据；并系统评测多种MLLM与不同RAG策略。

Result: 在该基准上，最好的多模态大模型仅取得58.0%准确率；不同RAG方法在此任务上表现差异显著，显示任务难度高且检索策略关键。

Conclusion: FinMMDocR 能更真实、全面地刻画金融多模态数值推理难点，有望推动MLLM与结合RAG/推理增强方法在复杂真实世界金融场景中的进步。

Abstract: We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.

</details>


### [92] [Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection](https://arxiv.org/abs/2512.24922)
*Bartłomiej Olber,Jakub Winter,Paweł Wawrzyński,Andrii Gamalii,Daniel Górniak,Marcin Łojek,Robert Nowak,Krystian Radlak*

Main category: cs.CV

TL;DR: 提出一种基于神经元激活模式的激光雷达3D目标检测跨域自适应方法，只需对目标域中少量且多样的样本标注，即可在小标注预算下取得SOTA；并结合持续学习式后训练以抑制权重漂移，优于线性探测和现有DA方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测器在本域表现强，但在跨地域/传感器等域转移时显著退化；全面标注目标域数据成本极高，需要低标注预算、可泛化的迁移方案。

Method: 1) 以神经元激活模式为核心的样本选择策略：从目标域未标注数据中挑选“代表性+多样性”的子集进行标注；2) 进行小样本微调；3) 引入受持续学习启发的后训练/正则（如参数巩固、权重约束或回放）以避免偏离源模型（权重漂移）。

Result: 在跨区域场景的激光雷达3D检测任务上，使用极少标注的目标域样本即可达到或刷新SOTA；相较线性探测与主流域自适应方法，取得更高检测精度与稳定性。

Conclusion: 合理选择并标注少量目标域样本结合持续学习式约束，可高效实现3D激光雷达检测的跨域适应，在极低标注成本下获得SOTA，并缓解灾难性遗忘/权重漂移。

Abstract: 3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.

</details>


### [93] [HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films](https://arxiv.org/abs/2512.24946)
*Rongji Xun,Junjie Yuan,Zhongjie Wang*

Main category: cs.CV

TL;DR: 提出HaineiFRDM电影修复扩散模型，通过补丁化训练/测试、位置感知全局提示与帧融合、全局-局部频域模块以及低分辨率全局残差，引入真实与高仿真数据集，实现在单块24GB显存GPU上对高分辨率影片的更优缺陷修复，优于开源方法。


<details>
  <summary>Details</summary>
Motivation: 开源电影修复方法因训练数据质量低、光流噪声大，且未有效覆盖高分辨率影片，导致性能落后于商业方案。需要一种能更好理解内容、避免光流噪声、并可在有限显存上处理高分辨率影片的框架。

Method: 1) 采用扩散模型作为核心生成/修复器；2) 补丁化训练与测试，使高分辨率修复可在单张24GB显存GPU上运行；3) 位置感知的全局提示（Global Prompt）与帧融合模块（Frame Fusion），显式利用时空与位置信息；4) 全局-局部频域模块，跨补丁重建一致纹理；5) 先生成低分辨率修复结果并作为全局残差指导高分辨率补丁合成，缓解拼接块效应；6) 构建包含真实退化修复样本与真实感合成数据的电影修复数据集。

Result: 在多个实验与综合评测中，HaineiFRDM在缺陷修复能力上显著超过现有开源方法，能在高分辨率场景中稳定运行并减少块状伪影与纹理不一致问题。

Conclusion: HaineiFRDM利用扩散模型与多级全局-局部、空间-频域约束，实现对高分辨率影片的更优缺陷修复，缓解光流噪声与数据质量不足带来的问题；代码与数据集将开放，具备实际应用潜力。

Abstract: Existing open-source film restoration methods show limited performance compared to commercial methods due to training with low-quality synthetic data and employing noisy optical flows. In addition, high-resolution films have not been explored by the open-source methods.We propose HaineiFRDM(Film Restoration Diffusion Model), a film restoration framework, to explore diffusion model's powerful content-understanding ability to help human expert better restore indistinguishable film defects.Specifically, we employ a patch-wise training and testing strategy to make restoring high-resolution films on one 24GB-VRAMR GPU possible and design a position-aware Global Prompt and Frame Fusion Modules.Also, we introduce a global-local frequency module to reconstruct consistent textures among different patches. Besides, we firstly restore a low-resolution result and use it as global residual to mitigate blocky artifacts caused by patching process.Furthermore, we construct a film restoration dataset that contains restored real-degraded films and realistic synthetic data.Comprehensive experimental results conclusively demonstrate the superiority of our model in defect restoration ability over existing open-source methods. Code and the dataset will be released.

</details>


### [94] [CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement](https://arxiv.org/abs/2512.24947)
*Wentao Zhang,Tao Fang,Lina Lu,Lifei Wang,Weihe Zhong*

Main category: cs.CV

TL;DR: 提出CPJ：无需训练的少样本框架，用多角度图像字幕与“判官”迭代校对来增强农业害病VQA，并双重作答（识别+治理）。在CDDMBench上显著提升，提升透明可解释性与跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有作物病虫害诊断依赖昂贵的有监督微调，且在域偏移下表现不佳；农业决策需要准确、可解释且可迁移的方法。

Method: Caption–Prompt–Judge流程：1) 用大规模视觉语言模型生成多角度、结构化图像字幕；2) 通过LLM-as-Judge模块对字幕进行迭代评审与精炼；3) 基于高质量字幕执行双通道VQA，一路进行病害识别，一路给出管理/防治建议；全流程训练免微调、少样本即可。

Result: 在CDDMBench上，使用GPT-5-mini生成字幕时，GPT-5-Nano较无字幕基线在病害分类上提升+22.7个百分点，在QA综合得分上提升+19.5分；提供可追溯证据链与解释。

Conclusion: CPJ在无需微调前提下实现对农业病虫害VQA的显著提升并具备强解释性与跨域鲁棒性；方法通用、可复现（代码数据开源），适合实际农业诊断场景。

Abstract: Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \textbf{+22.7} pp in disease classification and \textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.

</details>


### [95] [ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT](https://arxiv.org/abs/2512.24948)
*Xinran Gong,Gorkem Durak,Halil Ertugrul Aktas,Vedat Cicek,Jinkui Hao,Ulas Bagci,Nilay S. Shah,Bo Zhou*

Main category: cs.CV

TL;DR: 提出ProDM扩散生成框架，从非门控胸部CT中校正心动/呼吸运动伪影，恢复无运动的冠状动脉钙化灶，从而显著提升CAC评分与风险分层的准确性与可读性。


<details>
  <summary>Details</summary>
Motivation: 非门控胸部CT广泛可得且适合机会性CAC评估，但运动伪影严重影响钙化灶描绘与定量；心电门控CT虽能减少伪影，却受制于门控要求与报销限制，难以普及。需要一种在非门控场景下可靠恢复钙化信息的方法，以提高临床风险评估。

Method: 提出ProDM（Property-aware Progressive Correction Diffusion Model）：(1) 构建CAC运动仿真引擎，从门控CT合成具有多样运动轨迹的非门控样本，实现无成对数据的监督训练；(2) 引入属性感知学习，利用可微的“钙一致性”损失将钙化先验纳入训练，保护病灶完整性；(3) 采用渐进式纠正策略，在扩散反演多个步长中逐步去伪影，提升稳定性与钙化保真度。

Result: 在真实患者数据上，相较多种基线显著提升CAC评分准确度、空间病灶保真度与风险分层性能；读者研究显示在真实非门控扫描中能有效压制运动伪影并提高临床可用性。

Conclusion: 渐进式、属性感知的扩散框架可在常规胸部CT中可靠地量化CAC，弥补非门控成像的不足，具有推广至临床筛查与随访的潜力。

Abstract: Coronary artery calcium (CAC) scoring from chest CT is a well-established tool to stratify and refine clinical cardiovascular disease risk estimation. CAC quantification relies on the accurate delineation of calcified lesions, but is oftentimes affected by artifacts introduced by cardiac and respiratory motion. ECG-gated cardiac CTs substantially reduce motion artifacts, but their use in population screening and routine imaging remains limited due to gating requirements and lack of insurance coverage. Although identification of incidental CAC from non-gated chest CT is increasingly considered for it offers an accessible and widely available alternative, this modality is limited by more severe motion artifacts. We present ProDM (Property-aware Progressive Correction Diffusion Model), a generative diffusion framework that restores motion-free calcified lesions from non-gated CTs. ProDM introduces three key components: (1) a CAC motion simulation data engine that synthesizes realistic non-gated acquisitions with diverse motion trajectories directly from cardiac-gated CTs, enabling supervised training without paired data; (2) a property-aware learning strategy incorporating calcium-specific priors through a differentiable calcium consistency loss to preserve lesion integrity; and (3) a progressive correction scheme that reduces artifacts gradually across diffusion steps to enhance stability and calcium fidelity. Experiments on real patient datasets show that ProDM significantly improves CAC scoring accuracy, spatial lesion fidelity, and risk stratification performance compared with several baselines. A reader study on real non-gated scans further confirms that ProDM suppresses motion artifacts and improves clinical usability. These findings highlight the potential of progressive, property-aware frameworks for reliable CAC quantification from routine chest CT imaging.

</details>


### [96] [VIPER: Process-aware Evaluation for Generative Video Reasoning](https://arxiv.org/abs/2512.24952)
*Yifan Li,Yukai Gu,Yingqian Min,Zikang Liu,Yifan Du,Kun Zhou,Min Yang,Wayne Xin Zhao,Minghui Qiu*

Main category: cs.CV

TL;DR: 提出VIPER基准与POC@r指标，用过程感知的方式评估生成式视频推理中的Chain-of-Frames能力，发现现有视频模型过程一致性很低（POC@1.0约20%），存在显著“结果黑客”，并分析测试时扩展与采样鲁棒性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型出现“帧链式”推理迹象，但评估多用单帧结果判定，无法识别错误过程导致的正确答案（结果黑客），因此需要能同时衡量中间过程与最终结论一致性的评测范式。

Method: 1) 构建VIPER：覆盖时间、结构、符号、空间、物理、规划6大类共16项任务的综合基准；2) 设计POC@r指标：基于分层评分细则，用VLM充当评审，对中间步骤有效性与最终答案进行联合判定，并以不同严格度r给出一致性得分；3) 通过实验评测SOTA视频模型，并研究测试时扩展与采样策略对性能与稳健性的影响。

Result: SOTA视频模型在POC@1.0仅约20%，存在显著的结果黑客（中间推理错误但结论偶然正确）；测试时扩展与采样显示性能有一定提升但仍不稳健，暴露出与通用视觉推理能力的明显差距。

Conclusion: 仅用单帧/结果导向的评估不足以衡量生成式视频推理；过程感知评测（VIPER与POC@r）能更准确反映模型真实推理质量。当前模型距离通用化的视觉推理仍有较大差距，基准将公开以推动研究。

Abstract: Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.

</details>


### [97] [ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands](https://arxiv.org/abs/2512.24965)
*Siyuan Hu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出ShowUI-π：首个面向GUI“拖拽” dexterity 的流式生成模型，统一点按与拖拽动作，用增量光标调节实现稳定连续轨迹，并发布20K拖拽数据与ScreenDrag基准；在450M参数下超越多款专有代理。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体多以离散(x,y)点击为核心，难以处理需要连续感知与闭环控制的拖拽操作（如进度条、时间线、滑块等），限制了人机界面中的“灵巧操作”与人样自动化能力。

Method: 1) 统一离散-连续动作空间：同一模型覆盖点击与拖拽；2) 以流式（flow-based）动作生成建模拖拽，基于连续视觉观测预测逐步的光标位移，由轻量动作专家确保轨迹平滑稳定；3) 数据与评测：人工收集与合成五大应用域的2万条拖拽轨迹；提出ScreenDrag，含在线/离线评测协议。

Result: 在ScreenDrag上，现有专有代理表现低（如Operator 13.27，Gemini-2.5-CUA 22.18）；ShowUI-π仅450M参数取得26.98分，显示任务难度高且方法有效。

Conclusion: 将拖拽等连续交互纳入统一动作与流式生成框架，可显著提升GUI智能体的灵巧操控；所发布数据与基准为后续研究提供标准化评测与训练资源。

Abstract: Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-$π$, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-$π$ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.

</details>


### [98] [Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions](https://arxiv.org/abs/2512.24971)
*Itallo Patrick Castro Alves Da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza,Baldoino Fonseca dos Santos Neto,Marcio de Medeiros Ribeiro*

Main category: cs.CV

TL;DR: 评估量化、剪枝、权值聚类等压缩方法（单独与组合）在CNN上对鲁棒性、准确率与压缩比的权衡；发现某些策略能在复杂架构上提升对自然腐化的鲁棒性，并通过多目标评估选出最佳配置。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署视觉模型需压缩，但压缩可能削弱对自然腐化的鲁棒性；缺乏系统性比较不同压缩策略在鲁棒性、精度与压缩比之间的权衡。

Method: 在ResNet-50、VGG-19、MobileNetV2上分别与组合地应用量化、剪枝、权值聚类；使用CIFAR-10-C与CIFAR-100-C测试不同腐化类型与强度下的性能；通过多目标（鲁棒性、干净准确率、压缩比）评估挑选帕累托优配置。

Result: 部分压缩策略不仅保持，还可提升鲁棒性，尤其在更复杂的网络上；不同技术组合能获得更优的多目标折中；给出各模型在腐化集上的最佳配置与相应权衡。

Conclusion: 合理定制并组合压缩技术可在不牺牲甚至提升鲁棒性的同时实现高压缩；多目标方法可指导在受污染真实环境中稳健高效的模型部署。

Abstract: Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices. However, model compression may affect robustness, especially under natural corruption. Therefore, it is important to consider robustness evaluation while validating computer vision systems. This paper presents a comprehensive evaluation of compression techniques - quantization, pruning, and weight clustering applied individually and in combination to convolutional neural networks (ResNet-50, VGG-19, and MobileNetV2). Using the CIFAR-10-C and CIFAR 100-C datasets, we analyze the trade-offs between robustness, accuracy, and compression ratio. Our results show that certain compression strategies not only preserve but can also improve robustness, particularly on networks with more complex architectures. Utilizing multiobjective assessment, we determine the best configurations, showing that customized technique combinations produce beneficial multi-objective results. This study provides insights into selecting compression methods for robust and efficient deployment of models in corrupted real-world environments.

</details>


### [99] [DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments](https://arxiv.org/abs/2512.24985)
*Yohan Park,Hyunwoo Ha,Wonjun Jo,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出DarkEQA基准，用物理真实的低照度退化模拟，评测VLM在暗光下的EQA相关感知与问答鲁棒性；结果显示现有VLM在低光条件下显著受限。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多在理想光照下评测，无法反映真实24/7场景中的暗光、夜间等退化条件下的性能缺口；需要一个能可控、可归因地测试感知瓶颈的基准。

Method: 构建DarkEQA：在自我视角(Egocentric)观测上施加多级低照度退化。退化在线性RAW域进行，物理化地模拟照度下降与传感器噪声，再经类似ISP的渲染流程输出；围绕EQA相关的感知原语设计问答任务，并对多种SOTA VLM与低照度增强(LLIE)模型进行评测，实现可归因的鲁棒性分析。

Result: 系统评测表明：在低光与多级退化下，当前SOTA VLM的问答与感知能力明显下降；结合LLIE也存在不足，暴露出在感知层面的瓶颈。

Conclusion: DarkEQA提供了一个物理逼真、可控的暗光评测平台，能分离并量化VLM在低照度场景中的感知缺陷；研究提示需要在模型与预处理/ISP层面改进以实现稳健的全天候 embodied 推理。

Abstract: Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs' limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.

</details>


### [100] [Bi-C2R: Bidirectional Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification](https://arxiv.org/abs/2512.25000)
*Zhenyu Cui,Jiahuan Zhou,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出RFL-ReID任务：在终身行人重识别中禁止对历史图库重建索引（不重新提取特征），并给出Bi-C2R框架实现新旧模型特征兼容，达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现实中历史图库常因隐私与成本无法保存原图或反复重索引；现有L-ReID需对大规模图库每次更新后重提特征，代价高且不可行，导致更新后查询特征与旧图库特征不兼容、性能大幅下降。

Method: 定义“无重索引”的终身ReID新任务RFL-ReID；提出双向连续兼容表示（Bi-C2R）框架，使新旧模型输出的特征可互相兼容，并可在不访问原始历史图像的情况下持续更新图库特征表示；通过理论分析与多基准实验验证。

Result: 在多个基准上，Bi-C2R在新提出的RFL-ReID任务与传统L-ReID任务上均达到领先表现，证明兼容性与持续学习效果。

Conclusion: 无需重索引即可进行终身ReID是可行的。Bi-C2R实现新旧特征的双向兼容与连续更新，显著缓解遗忘与不兼容问题，并在实际约束下取得SOTA性能。

Abstract: Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as "re-indexing". However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. We verify our proposed Bi-C2R method through theoretical analysis and extensive experiments on multiple benchmarks, which demonstrate that the proposed method can achieve leading performance on both the introduced RFL-ReID task and the traditional L-ReID task.

</details>


### [101] [FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM](https://arxiv.org/abs/2512.25008)
*Yuchen Wu,Jiahe Li,Fabio Tosi,Matteo Poggi,Jin Zheng,Xiao Bai*

Main category: cs.CV

TL;DR: FoundationSLAM 是一种基于学习的单目稠密 SLAM，将光流估计与几何推理通过基础深度模型融合，实现几何一致的跟踪与建图；通过混合光流网络、双一致性 BA 层和可靠性感知的自适应更新，在多数据集上以约18 FPS 实时获得更高轨迹精度与重建质量。


<details>
  <summary>Details</summary>
Motivation: 以往基于光流的单目稠密 SLAM 缺乏几何一致性，导致深度与位姿不稳定、全局一致性差；需要一种既利用深度先验又能在多视角下全局优化的框架，实现稳健、准确、可泛化并可实时运行的跟踪与建图。

Method: 1) 引入基础深度模型提供几何先验；2) 设计 Hybrid Flow Network，输出具几何意识的匹配以在不同关键帧间一致地推断深度与位姿；3) 提出 Bi-Consistent Bundle Adjustment 层，在多视图约束下联合优化关键帧位姿与深度以保持全局一致；4) 可靠性感知的细化机制，区分可靠/不确定区域，动态调整光流更新，形成匹配-优化闭环。

Result: 在多项具有挑战性的数据集上，轨迹精度和稠密重建质量均显著优于现有方法，并能泛化到多种场景；系统以约18 FPS 实时运行。

Conclusion: 将光流与几何通过基础深度模型耦合，并用双一致性 BA 与可靠性感知细化实现全局一致与稳健更新，可在保持实时性的同时提升单目稠密 SLAM 的精度、鲁棒性与泛化能力。

Abstract: We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.

</details>


### [102] [From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing](https://arxiv.org/abs/2512.25066)
*Xu He,Haoxian Zhang,Hejia Chen,Changyuan Zheng,Liyang Chen,Songlin Tang,Jiehui Huang,Xiaoqiang Liu,Pengfei Wan,Zhiyong Wu*

Main category: cs.CV

TL;DR: 提出一种自举式音频驱动视觉配音方法：先用DiT生成“理想成对数据”，再用DiT编辑器在完整对齐的视频条件下只做唇形修改，配合时间步自适应多阶段训练，达成高精度对口、身份保持与强鲁棒性，并发布评测集ContextDubBench。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺少理想训练对（仅唇动不同、其余完全相同），被迫用“遮罩修复”来同时补全内容和对齐唇形，易产生伪影、身份漂移和同步差。需要把问题从病态的补全转为有充分条件的视频到视频编辑。

Method: 自举框架：1) 用Diffusion Transformer先充当数据生成器，为每个真实视频合成一个仅改唇的伴随视频，构成视觉对齐的视频对；2) 训练一个基于DiT的音频驱动编辑器，以完整对齐帧作为条件，只执行精确的唇部修改；3) 提出时间步自适应的多阶段学习策略，分离扩散不同时间步的冲突编辑目标，提升稳定性与效果；4) 发布ContextDubBench用于多样、复杂场景评测。

Result: 在完整对齐的视觉上下文支持下，方法实现更准确的唇形同步、身份保真与对复杂实拍场景的鲁棒性；时间步自适应多阶段训练进一步提升同步与画质。并提供新的基准数据集用于系统评测。

Conclusion: 把视觉配音从遮罩修复转为条件充分的视频编辑，通过DiT自举生成训练对与DiT编辑器联合训练，加之时间步自适应学习，可显著缓解伪影与身份漂移，提升对口精度和鲁棒性，并推动标准化评测。

Abstract: Audio-driven visual dubbing aims to synchronize a video's lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject's lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.

</details>


### [103] [FineTec: Fine-Grained Action Recognition Under Temporal Corruption via Skeleton Decomposition and Sequence Completion](https://arxiv.org/abs/2512.25067)
*Dian Shao,Mingfei Shi,Like Liu*

Main category: cs.CV

TL;DR: FineTec提出一个统一框架，在存在时间缺失/腐蚀的骨架序列上进行细粒度动作识别：先补全时序，再做语义区域与动态/静态分解并生成增强序列，引入拉格朗日物理估计得到关节加速度，最终将位置与加速度融合输入GCN分类。实验在NTU与Gym细粒度数据集的多种腐蚀强度下均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线姿态估计常产生严重的时间缺失，导致细微动作线索丢失；现有方法难以同时恢复时序动态与细粒度空间结构，尤其在相近动作区分上表现欠佳。

Method: 1) 语境感知完成与多样化时间掩码：从腐蚀输入重建基础骨架序列；2) 骨架空间分解：按五个语义区域划分，并依据运动方差分为动态/静态子群，进行针对性扰动以生成两条增强序列；3) 物理驱动估计：基于拉格朗日动力学估计关节加速度；4) 融合与识别：融合位置序列与加速度序列，输入GCN分类头进行动作识别。

Result: 在NTU-60/120（粗粒度）与Gym99/Gym288（细粒度）上，在多级别时间腐蚀下均取得SOTA；在更具挑战的Gym99-severe与Gym288-severe设置下Top-1分别为89.1%与78.1%。

Conclusion: 通过时序补全、空间语义分解与物理先验相结合，FineTec在带有时间缺失的骨架动作识别中实现稳健且泛化良好的细粒度识别性能。

Abstract: Recognizing fine-grained actions from temporally corrupted skeleton sequences remains a significant challenge, particularly in real-world scenarios where online pose estimation often yields substantial missing data. Existing methods often struggle to accurately recover temporal dynamics and fine-grained spatial structures, resulting in the loss of subtle motion cues crucial for distinguishing similar actions. To address this, we propose FineTec, a unified framework for Fine-grained action recognition under Temporal Corruption. FineTec first restores a base skeleton sequence from corrupted input using context-aware completion with diverse temporal masking. Next, a skeleton-based spatial decomposition module partitions the skeleton into five semantic regions, further divides them into dynamic and static subgroups based on motion variance, and generates two augmented skeleton sequences via targeted perturbation. These, along with the base sequence, are then processed by a physics-driven estimation module, which utilizes Lagrangian dynamics to estimate joint accelerations. Finally, both the fused skeleton position sequence and the fused acceleration sequence are jointly fed into a GCN-based action recognition head. Extensive experiments on both coarse-grained (NTU-60, NTU-120) and fine-grained (Gym99, Gym288) benchmarks show that FineTec significantly outperforms previous methods under various levels of temporal corruption. Specifically, FineTec achieves top-1 accuracies of 89.1% and 78.1% on the challenging Gym99-severe and Gym288-severe settings, respectively, demonstrating its robustness and generalizability. Code and datasets could be found at https://smartdianlab.github.io/projects-FineTec/.

</details>


### [104] [Edit3r: Instant 3D Scene Editing from Sparse Unposed Images](https://arxiv.org/abs/2512.25071)
*Jiageng Liu,Weijie Lyu,Xueting Li,Yejie Guo,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: Edit3r 是一个单次前馈的3D重建与编辑框架，可从未配准、跨视角不一致且经指令编辑的图像直接生成语义对齐、3D一致的场景与编辑，无需逐场景优化或位姿估计，速度快且具有高保真渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法常需每个场景进行耗时优化与位姿估计，且缺乏对跨视角一致的编辑监督数据，难以在存在2D编辑导致的视角不一致情况下获得快速、稳定的3D结果。

Method: 提出前馈网络Edit3r：在训练中使用SAM2驱动的重着色策略生成跨视角一致的监督信号；采用不对称输入策略，将“重着色的参考视图”与“原始辅助视图”配对，促使网络融合并对齐不同观察；推理时可直接处理来自InstructPix2Pix等2D编辑输出的图像。

Result: 在新建的DL3DV-Edit-Bench（包含20个场景、4类编辑、共100个编辑）上进行大规模评测，定量与定性结果显示，Edit3r在语义对齐与3D一致性方面优于近期基线，同时推理速度显著更快。

Conclusion: Edit3r实现了无需优化与位姿估计的快速、照片级3D编辑，具备更好的语义一致性与空间一致性，适合实时3D编辑应用。

Abstract: We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.

</details>


### [105] [GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction](https://arxiv.org/abs/2512.25073)
*Yi-Chuan Huang,Hao-Jen Chien,Chin-Yang Lin,Ying-Huan Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出GaMO，通过多视角“外延(outpainting)”扩展现有相机视野，在稀疏视角下实现高质量3D重建；零训练、几何一致、覆盖更广、速度提升约25倍，PSNR/LPIPS优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角重建难：少量输入导致外周覆盖不足、跨视角几何不一致、扩散式新视角合成代价高。需要一种既能扩展可见范围、又保持几何一致、且高效的方案。

Method: 将问题从“新视角合成”转为“同视角FOV扩展”的多视图外延：1) 多视图条件引导；2) 几何感知去噪确保跨视角一致；3) 零训练推理式流程；在扩展后的图像上进行重建。

Result: 在Replica与ScanNet++上，3/6/9视角场景均达到SOTA重建质量，PSNR更高、LPIPS更低；相较最新扩散式方法推理加速约25×，整套流程<10分钟。

Conclusion: 多视图外延比新视角生成更稳定且一致，能在稀疏视角场景中以更低成本提供更广覆盖与更高质量重建，成为高效几何一致的替代范式。

Abstract: Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/

</details>


### [106] [SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time](https://arxiv.org/abs/2512.25075)
*Zhening Huang,Hyeonho Jeong,Xuelin Chen,Yulia Gryaditskaya,Tuanfeng Y. Wang,Joan Lasenby,Chun-Hao Huang*

Main category: cs.CV

TL;DR: SpaceTimePilot 是一种将空间与时间解耦的可控视频扩散生成模型，可在给定单目视频下独立修改相机视角与运动序列，实现跨空间与时间的连续重渲染与探索。其通过动画时间嵌入与时间扭曲训练，并结合改进的相机条件与CamxTime全覆盖合成数据集，获得精确的双重控制与稳健解耦，在真实与合成数据上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成/重渲染模型难以同时、独立地控制相机视角（空间）与物体/场景运动（时间），且缺乏针对同一动态场景的时间变化配对数据来监督这种解耦与控制。

Method: 1) 在扩散模型中引入动画时间嵌入，使生成过程可显式对齐/重排目标视频的运动序列相对于源视频；2) 设计时间扭曲（temporal-warping）训练，把多视角静态/有限动态数据重新利用为具有时间差异的伪配对数据，监督模型学习时间控制与空间-时间解耦；3) 改进相机条件机制，支持从第一帧就改变相机；4) 构建CamxTime合成数据集，提供在同一场景内完全自由的空间-时间视频轨迹；5) 将时间扭曲训练与CamxTime联合训练。

Result: 模型在真实与合成数据上展示清晰的空间-时间解耦能力：能在给定单目视频的前提下独立改变相机与运动序列，并在定量与定性指标上优于现有方法。

Conclusion: 通过时间嵌入、时间扭曲训练与改进的相机条件，再加上CamxTime数据集，SpaceTimePilot实现了精确的空间与时间可控生成与重渲染，填补了缺乏时间配对数据条件下的时空解耦训练难题，并在多数据集上验证了其有效性。

Abstract: We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot

</details>
