<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 97]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration](https://arxiv.org/abs/2601.07845)
*Shree Charran R,Rahul Kumar Dubey*

Main category: cs.CV

TL;DR: 提出一种在Jetson Nano上运行的实时路侧AI感知节点，结合YOLOv8n+DeepSORT+规则引导OCR，以低功耗高帧率检测并跟踪车辆、识别车牌与五类交通违法，并通过V2X发布标准化安全事件，较多种轻量模型有显著mAP与能效优势。


<details>
  <summary>Details</summary>
Motivation: 新兴经济体（如印度）机动化迅速导致执法供需失衡：海量交通违法与稀缺人力警力不匹配，传统抓拍与人工开罚单难以扩展。需要自治、协同、节能的边缘AI感知基础设施，以实现可规模化的违法分析与道路安全事件协同。

Method: 在路侧节点上集成：1) YOLOv8 Nano进行多类别目标检测；2) DeepSORT实现时序一致的车辆跟踪；3) 规则引导的OCR后处理识别退化/多语种车牌，符合MoRTH AIS 159与ISO 7591对比度标准。部署于NVIDIA Jetson Nano（128核Maxwell GPU），经TensorRT FP16量化优化；无需手动ROI标定。系统通过V2X向车辆与ITS后端发布CAM/DENM标准事件。

Result: 在9.6W功耗下维持28–30 FPS推理；五类违法（闯红灯、压斑马线、逆行、违章掉头、超速）检测准确率97.7%，OCR精度84.9%。相较YOLOv4-Tiny、PP-YOLOE-S与NanoDetPlus，mAP提升10.7%，单位功耗准确率提升1.4倍。

Conclusion: 低功耗边缘AI路侧节点可在实时条件下高精度识别多类交通违法并发布标准化安全消息，增强协同感知与主动道路安全管理；验证了在IEEE智能网联车辆生态中，以V2X联动的边缘感知对执法与安全的双重价值。

Abstract: Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.

</details>


### [2] [An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.07855)
*Subeen Lee,Siyeong Lee,Namil Kim,Jaesik Choi*

Main category: cs.CV

TL;DR: ROAD基准：在LiDAR点云上评估同时存在领域与标签演化（拆分/扩展/插入）的鲁棒持续与迁移学习，揭示现有方法不足并给出强基线。


<details>
  <summary>Details</summary>
Motivation: 现实3D感知（自动驾驶、具身AI）需在对象定义与传感器域不断变化下保持性能。但相比2D，3D点云在同时经历领域迁移与标签演化（分布、类别集合变化）的持续/迁移学习研究不足，缺乏系统评测。

Method: 提出ROAD基准：围绕LiDAR目标分类，显式设计并覆盖多种数据集域移（Waymo、NuScenes、Argoverse2）与三类标签演化情形（类别拆分、类别扩展、类别插入）；评测零样本迁移、线性探测与持续学习策略；系统对比不同主干架构、训练目标与CL方法。

Result: 在真实的域移与标签变动组合下，现有零样本、线性探测与多种CL方法表现明显受限；不同主干/目标的稳健性差异被量化，ROAD给出可复现的强基线。

Conclusion: ROAD为3D点云鲁棒感知提供首个全面基准之一，凸显当前方法短板，促进在现实标签与域动态下的更强持续与迁移学习研究。

Abstract: For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.

</details>


### [3] [Moonworks Lunara Aesthetic Dataset](https://arxiv.org/abs/2601.07941)
*Yan Wang,M M Sayeef Abdullah,Partho Hassan,Sabit Hassan*

Main category: cs.CV

TL;DR: 提出并发布Lunara Aesthetic Dataset：由Moonworks Lunara模型生成、经人工打磨的高审美质量、多风格图像数据集，含细粒度标注与人类优化提示，Apache 2.0许可。


<details>
  <summary>Details</summary>
Motivation: 现有大规模网络数据集重广度轻精度，审美质量参差、风格标注稀疏且许可不清；审美导向的数据集亦难兼顾跨地域风格多样性与高质量标注。作者欲提供一个高审美评分、风格多元、可放心商用的基准。

Method: 使用Lunara生成覆盖中东、北欧、东亚、南亚等地域审美及素描、油画等通用风格的图像；每张图像配有人类精修的生成提示与结构化标注（对象、属性、关系、风格线索）；严格控质与许可，形成高一致性的审美数据。

Result: 得到首个以高审美与风格多样为核心、评分显著高于现有审美与通用数据集的数据集；图像与标注质量稳定，并具透明授权。

Conclusion: Lunara Aesthetic Dataset为审美与风格学习提供高质量、开放许可的数据基础，适合研究与商业应用，并可作为对比与微调基准以促进风格控制与美学建模研究。

Abstract: The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.

</details>


### [4] [LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification on Edge Devices](https://arxiv.org/abs/2601.07957)
*Fikadu Weloday,Jianmei Su*

Main category: cs.CV

TL;DR: 提出LWMSCNN-SE轻量级CNN，用多尺度、深度可分离卷积与SE注意力，实现96.63%准确率，仅24.1万参数、0.666 GFLOPs，适合移动/无人机端玉米病害实时检测。


<details>
  <summary>Details</summary>
Motivation: 传统病害检测模型在手机与无人机等资源受限环境计算开销大、难以实时部署，需要在准确率与效率之间取得平衡以支撑精准农业的边缘智能诊断。

Method: 设计LWMSCNN-SE：结合多尺度特征提取、深度可分离卷积以降参/降算，以及SE注意力增强通道重要性；整体网络针对边缘设备优化，兼顾轻量与判别力。

Result: 在玉米病害分类任务上达到96.63%准确率，仅241,348参数与0.666 GFLOPs，显著降低模型规模与计算量。

Conclusion: 该方法在保持高精度的同时显著降低计算与参数开销，适合在智能手机、无人机等边缘设备上实时部署，用于精准农业中的玉米病害诊断。

Abstract: Maize disease classification plays a vital role in mitigating yield losses and ensuring food security. However, the deployment of traditional disease detection models in resource-constrained environments, such as those using smartphones and drones, faces challenges due to high computational costs. To address these challenges, we propose LWMSCNN-SE, a lightweight convolutional neural network (CNN) that integrates multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-Excitation (SE) attention mechanisms. This novel combination enables the model to achieve 96.63% classification accuracy with only 241,348 parameters and 0.666 GFLOPs, making it suitable for real-time deployment in field applications. Our approach addresses the accuracy--efficiency trade-off by delivering high accuracy while maintaining low computational costs, demonstrating its potential for efficient maize disease diagnosis on edge devices in precision farming systems.

</details>


### [5] [3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing](https://arxiv.org/abs/2601.07963)
*Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 3DGS-Drag 提出一种基于点/高斯的3D场景可视化拖拽编辑框架，结合3D高斯变形引导与扩散模型外观修复，实现高质量、几何一致的拖拽式编辑，覆盖运动/形状调整、修复与内容扩展，单卡4090约10–20分钟。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑要么依赖变形（难以保证外观/一致性），要么把问题转为2D编辑（难以保持几何一致和多视角一致），尤其对涉及几何变化的拖拽式直观编辑仍然不足。需要一种既直观又能在3D中保持几何与外观一致性的编辑方法。

Method: 提出3DGS-Drag：以3D Gaussian Splatting作为场景表示与“变形引导”，保证跨视角一致的几何修改；再用扩散模型提供“内容/外观修正与质量增强”的“扩散引导”。采用渐进式编辑策略以支持大幅度拖拽变形。可执行运动更改、形状调整、补洞（inpainting）与内容扩展。

Result: 在多种真实3D场景上展示了可视化拖拽编辑的效果，几何相关编辑达到SOTA质量；编辑效率高，在单张RTX 4090上10–20分钟完成一次编辑。

Conclusion: 3DGS-Drag弥合了基于变形与基于2D编辑的3D编辑方法差距，实现稳定一致的几何拖拽与高质量外观；方法通用、编辑类型广、效率高，适合真实3D场景的交互式编辑。

Abstract: The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.

</details>


### [6] [Sesame Plant Segmentation Dataset: A YOLO Formatted Annotated Dataset](https://arxiv.org/abs/2601.07970)
*Sunusi Ibrahim Muhammad,Ismail Ismail Tijjani,Saadatu Yusuf Jumare,Fatima Isah Jibrin*

Main category: cs.CV

TL;DR: 提出并公开了芝麻植株分割数据集，含YOLO可兼容的像素级标注，基于尼日利亚田间实拍，配套YOLOv8评测显示检测与分割均取得较好性能。


<details>
  <summary>Details</summary>
Motivation: 农业视觉领域缺乏针对芝麻作物、且在真实田间环境下的高质量像素级标注数据，限制了早期生长阶段目标检测、分割与监测等AI模型的发展与落地。

Method: 在尼日利亚Katsina州Daura辖区Jirdede农田用高分辨率手机采集图像；使用SAM v2在农户监督下生成像素级分割掩膜；整理为YOLO兼容的分割格式，划分为训练206、验证43、测试43；使用Ultralytics YOLOv8对检测与分割任务进行评测。

Result: 检测：召回79%、精度79%、mAP@0.50为84%、mAP@0.50:0.95为58%；分割：召回82%、精度77%、mAP@0.50为84%、mAP@0.50:0.95为52%。

Conclusion: 该数据集填补了尼日利亚芝麻作物视觉数据空白，像素级标注优于仅框标注，有助于田间精细识别与分析，支持植株监测、产量估计与研究等应用。

Abstract: This paper presents the Sesame Plant Segmentation Dataset, an open source annotated image dataset designed to support the development of artificial intelligence models for agricultural applications, with a specific focus on sesame plants. The dataset comprises 206 training images, 43 validation images, and 43 test images in YOLO compatible segmentation format, capturing sesame plants at early growth stages under varying environmental conditions. Data were collected using a high resolution mobile camera from farms in Jirdede, Daura Local Government Area, Katsina State, Nigeria, and annotated using the Segment Anything Model version 2 with farmer supervision. Unlike conventional bounding box datasets, this dataset employs pixel level segmentation to enable more precise detection and analysis of sesame plants in real world farm settings. Model evaluation using the Ultralytics YOLOv8 framework demonstrated strong performance for both detection and segmentation tasks. For bounding box detection, the model achieved a recall of 79 percent, precision of 79 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 58 percent. For segmentation, it achieved a recall of 82 percent, precision of 77 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 52 percent. The dataset represents a novel contribution to sesame focused agricultural vision datasets in Nigeria and supports applications such as plant monitoring, yield estimation, and agricultural research.

</details>


### [7] [An Efficient Additive Kolmogorov-Arnold Transformer for Point-Level Maize Localization in Unmanned Aerial Vehicle Imagery](https://arxiv.org/abs/2601.07975)
*Fei Li,Lang Qiao,Jiahao Fan,Yijia Xu,Shawn M. Kaeppler,Zhou Zhang*

Main category: cs.CV

TL;DR: 提出AKT与PML数据集，针对超高分辨率UAV图像中的玉米点级定位难题，在更低计算量下显著提升检测精度与速度，并支持下游计数与株距估计。


<details>
  <summary>Details</summary>
Motivation: UAV遥感在精准农业中需要厘米级与点级定位，但存在：目标极小（<0.1%像素）、超高分辨率导致自注意力二次复杂度过高、农业场景稀疏与环境变化使通用视觉模型失效，迫切需要兼顾小目标表征与高效长程依赖的模型。

Method: 提出Additive Kolmogorov-Arnold Transformer (AKT)：1) 用Pade Kolmogorov-Arnold Network（PKAN）替代MLP，提高函数表达力以更好提取小目标特征；2) 设计PKAN Additive Attention（PAA），通过可加性与多尺度建模降低计算复杂度并捕获空间依赖；3) 构建Point-based Maize Localization（PML）数据集，含1928张超高分辨率UAV图像与约50.1万点标注。

Result: 在PML上，AKT平均F1=62.8%，较SOTA提升4.2%，同时FLOPs下降12.6%、推理吞吐提升20.7%；在下游任务中，群体计数MAE=7.1，株距估计RMSE约1.95–1.97 cm。

Conclusion: 将Kolmogorov-Arnold表征理论与高效注意力相结合，可在超高分辨率农业遥感中以更低计算实现更强小目标点级定位，并有效支撑计数与株距等下游应用。

Abstract: High-resolution UAV photogrammetry has become a key technology for precision agriculture, enabling centimeter-level crop monitoring and point-level plant localization. However, point-level maize localization in UAV imagery remains challenging due to (1) extremely small object-to-pixel ratios, typically less than 0.1%, (2) prohibitive computational costs of quadratic attention on ultra-high-resolution images larger than 3000 x 4000 pixels, and (3) agricultural scene-specific complexities such as sparse object distribution and environmental variability that are poorly handled by general-purpose vision models.
  To address these challenges, we propose the Additive Kolmogorov-Arnold Transformer (AKT), which replaces conventional multilayer perceptrons with Pade Kolmogorov-Arnold Network (PKAN) modules to enhance functional expressivity for small-object feature extraction, and introduces PKAN Additive Attention (PAA) to model multiscale spatial dependencies with reduced computational complexity. In addition, we present the Point-based Maize Localization (PML) dataset, consisting of 1,928 high-resolution UAV images with approximately 501,000 point annotations collected under real field conditions.
  Extensive experiments show that AKT achieves an average F1-score of 62.8%, outperforming state-of-the-art methods by 4.2%, while reducing FLOPs by 12.6% and improving inference throughput by 20.7%. For downstream tasks, AKT attains a mean absolute error of 7.1 in stand counting and a root mean square error of 1.95-1.97 cm in interplant spacing estimation. These results demonstrate that integrating Kolmogorov-Arnold representation theory with efficient attention mechanisms offers an effective framework for high-resolution agricultural remote sensing.

</details>


### [8] [Likelihood ratio for a binary Bayesian classifier under a noise-exclusion model](https://arxiv.org/abs/2601.07982)
*Howard C. Gifford*

Main category: cs.CV

TL;DR: 提出一种新的统计理想观察者模型，通过对可提取的最小图像特征设定阈值来实现整体（gist）视觉搜索，并以更少自由参数达成更精简的系统，用于医学影像感知、计算机视觉、性能基准与特征选择/评估，以及安防目标检测与传感器评估。


<details>
  <summary>Details</summary>
Motivation: 传统理想观察者或视觉搜索模型往往依赖大量参数与局部特征处理，难以刻画人类快速“整体感知”（gist）并导致过拟合、可解释性弱及系统优化成本高。作者希望在保持统计最优决策框架的同时，引入对“最小可提取特征”的阈值约束，既贴近人类整体搜索策略，又降低参数维度，便于跨任务与跨系统比较与优化。

Method: 构建一个统计理想观察者框架：在图像表示层首先定义一组可提取的低/中层图像特征，并对其设定最小可检测阈值；仅当特征超过阈值时纳入后续全局（holistic/gist）判别；在决策层使用理想观察者准则（如似然比或充分统计量）进行目标存在性/类别判断。通过阈值机制减少自由参数与无效特征，形成更紧凑的判别器。

Result: 模型能在不牺牲理想观察者的统计判别力前提下，显著减少自由参数，并提供一套面向整体视觉搜索的特征选择准则。其框架适用于医学影像系统优化、算法对比、计算机视觉基准评测，以及安防领域的目标检测与传感器/探测器评估。

Conclusion: 以“最小可提取特征阈值”为核心的理想观察者模型，为整体视觉搜索提供了统计严谨且参数更少的实现路径，可作为成像系统设计、算法优化与跨领域检测任务的统一评估与特征筛选工具。

Abstract: We develop a new statistical ideal observer model that performs holistic visual search (or gist) processing in part by placing thresholds on minimum extractable image features. In this model, the ideal observer reduces the number of free parameters thereby shrinking down the system. The applications of this novel framework is in medical image perception (for optimizing imaging systems and algorithms), computer vision, benchmarking performance and enabling feature selection/evaluations. Other applications are in target detection and recognition in defense/security as well as evaluating sensors and detectors.

</details>


### [9] [Predicting Region of Interest in Human Visual Search Based on Statistical Texture and Gabor Features](https://arxiv.org/abs/2601.07998)
*Hongwei Lin,Diego Andrade,Mini Das,Howard C. Gifford*

Main category: cs.CV

TL;DR: 研究将Gabor特征与GLCM纹理特征结合，用于预测早期视觉搜索中的注视候选区域；在乳腺断层影像模拟数据与人眼动实验上显示两者结合与阈值型观察者一致且能与早期凝视相符。


<details>
  <summary>Details</summary>
Motivation: 视觉搜索的早期阶段涉及对结构与纹理线索的快速整合，但不同特征家族（如频域的Gabor与统计纹理的GLCM）如何相互关联、以及联合使用是否能更好地预测人类注视仍不清楚。

Method: 提出两条特征组合流水线：提取Gabor与GLCM特征并融合，用于收敛潜在注视区域；在模拟的数字乳腺断层摄影(DBT)图像上评估，并与基于阈值的模型观察者比较；同时采集人类眼动数据检验一致性；分析GLCM均值与Gabor响应的相关性。

Result: 两条流水线预测的注视候选与阈值模型观察者在质性上吻合；发现GLCM均值与Gabor特征响应高度相关，显示二者编码相关图像信息；人类眼动显示早期注视与预测区域一致。

Conclusion: 结构( Gabor)与纹理(GLCM)特征的结合有助于建模早期视觉搜索与注视分配，支持构建感知驱动的观察者模型，并强调不同特征表征之间的互补性与相关性。

Abstract: Understanding human visual search behavior is a fundamental problem in vision science and computer vision, with direct implications for modeling how observers allocate attention in location-unknown search tasks. In this study, we investigate the relationship between Gabor-based features and gray-level co-occurrence matrix (GLCM) based texture features in modeling early-stage visual search behavior. Two feature-combination pipelines are proposed to integrate Gabor and GLCM features for narrowing the region of possible human fixations. The pipelines are evaluated using simulated digital breast tomosynthesis images. Results show qualitative agreement among fixation candidates predicted by the proposed pipelines and a threshold-based model observer. A strong correlation is observed between GLCM mean and Gabor feature responses, indicating that these features encode related image information despite their different formulations. Eye-tracking data from human observers further suggest consistency between predicted fixation regions and early-stage gaze behavior. These findings highlight the value of combining structural and texture-based features for modeling visual search and support the development of perceptually informed observer models.

</details>


### [10] [CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation](https://arxiv.org/abs/2601.08010)
*Chaoyu Li,Deeparghya Dutta Barua,Fei Tao,Pooyan Fazli*

Main category: cs.CV

TL;DR: 提出CASHEW与CASHEW-RL两种在测试阶段稳定多步视觉-语言推理的方法，通过聚合多条推理轨迹并进行视觉证据核验，显著提升多模态任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在多步推理上不稳定，同一输入多次采样会产生分歧的推理路径与不一致答案，影响可靠性与可扩展性。

Method: 1) CASHEW：推理时对同一输入生成多条候选推理轨迹，进行迭代式聚合；引入显式视觉验证筛除幻觉步骤，将推理与可见证据对齐。2) CASHEW-RL：将聚合行为内化到单模型中，采用GSPO强化学习，设计复合奖励：答案正确性、最小但充分的视觉证据支撑、依据任务难度自适应分配推理努力，从而在推理时实现稳健自聚合。

Result: 在13个图像理解、视频理解与视频推理基准上显著提升，如ScienceQA提升最高+23.6个百分点、EgoSchema提升+8.1个百分点。

Conclusion: 通过测试时轨迹聚合与带视觉证据约束的强化学习训练，可稳定和强化VLM的多步推理，减少幻觉并更好地依据难度分配推理资源，带来广泛而显著的性能增益。

Abstract: Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.

</details>


### [11] [TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models](https://arxiv.org/abs/2601.08011)
*Xin Jin,Yichuan Zhong,Yapeng Tian*

Main category: cs.CV

TL;DR: 提出TP-Blend：一个训练免、双提示词的扩散编辑框架，将“物体内容提示词”和“风格提示词”在单一去噪轨迹中融合，实现同时换物体与换风格，生成高分辨率、照片级真实感且高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的扩散编辑器对单一物体替换表现尚可，但难以在同一次编辑中同时引入全新物体与全新风格，常出现内容-风格耦合不稳、细节纹理破坏或计算开销大。需要一种在不重新训练的前提下，既精准控制内容，又稳定注入风格细节的方案。

Method: 提出Twin-Prompt Attention Blend（TP-Blend），使用两条独立文本提示：一条描述要融合的物体（内容），另一条描述目标风格，并在一次去噪过程中同时注入。方法包含两种互补注意力处理器：1) Cross-Attention Object Fusion（CAOF）：先对跨注意力头进行平均以定位对任一提示响应强的空间token，再解熵正则的最优传输，将完整多头特征向量重新分配到这些位置；以全头合并维度（如SD-XL的640维）更新特征，保留跨头相关性且内存开销低。2) Self-Attention Style Fusion（SASF）：在每个自注意层通过“细节敏感实例归一化”注入风格：用轻量1D高斯滤波分离低/高频，仅回融高频残差以刻画笔触级纹理而不破坏全局结构；同时以风格提示导出的K/V矩阵替换原K/V，实现与物体融合解耦的上下文感知纹理调制。

Result: 在大量实验中，TP-Blend可生成高分辨率、照片级真实的编辑结果，对内容与外观均具精确可控性；在定量保真度、感知质量与推理速度上均优于近期基线方法。

Conclusion: TP-Blend在无需训练的前提下，实现双提示的内容-风格解耦融合：CAOF负责定位与融合物体内容，SASF负责稳定注入风格细节，从而在效率、质量和控制性方面全面提升。

Abstract: Current text-conditioned diffusion editors handle single object replacement well but struggle when a new object and a new style must be introduced simultaneously. We present Twin-Prompt Attention Blend (TP-Blend), a lightweight training-free framework that receives two separate textual prompts, one specifying a blend object and the other defining a target style, and injects both into a single denoising trajectory. TP-Blend is driven by two complementary attention processors. Cross-Attention Object Fusion (CAOF) first averages head-wise attention to locate spatial tokens that respond strongly to either prompt, then solves an entropy-regularised optimal transport problem that reassigns complete multi-head feature vectors to those positions. CAOF updates feature vectors at the full combined dimensionality of all heads (e.g., 640 dimensions in SD-XL), preserving rich cross-head correlations while keeping memory low. Self-Attention Style Fusion (SASF) injects style at every self-attention layer through Detail-Sensitive Instance Normalization. A lightweight one-dimensional Gaussian filter separates low- and high-frequency components; only the high-frequency residual is blended back, imprinting brush-stroke-level texture without disrupting global geometry. SASF further swaps the Key and Value matrices with those derived from the style prompt, enforcing context-aware texture modulation that remains independent of object fusion. Extensive experiments show that TP-Blend produces high-resolution, photo-realistic edits with precise control over both content and appearance, surpassing recent baselines in quantitative fidelity, perceptual quality, and inference speed.

</details>


### [12] [Decoder Generates Manufacturable Structures: A Framework for 3D-Printable Object Synthesis](https://arxiv.org/abs/2601.08015)
*Abhishek Kumar*

Main category: cs.CV

TL;DR: 提出一种基于解码器的深度学习方法，将潜在表示直接解码为满足增材制造约束的可打印3D结构，并在多类对象上验证其可制造性提升与实物打印可行性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式3D建模常忽略增材制造约束（如悬垂、壁厚、结构强度），导致输出难以直接打印；需要一种能在生成阶段内生地嵌入制造可行性的框架。

Method: 构建神经解码器，将潜在空间表示映射为三维几何，并在解码过程中显式/隐式施加制造约束（悬垂角、最小壁厚、结构完整性），使输出几何天然满足打印可制造性；在多类对象数据上训练与评估。

Result: 与朴素生成方法相比，所生成零件在可制造性指标上显著提升，几何有效且可打印；在多种类别上均表现稳健，并完成了样件的实际3D打印验证。

Conclusion: 神经解码器能够学习从抽象潜在表示到满足制造约束的有效3D几何的复杂映射，为面向增材制造的可制造性优先生成提供了有效路径，并具备实际落地潜力。

Abstract: This paper presents a novel decoder-based approach for generating manufacturable 3D structures optimized for additive manufacturing. We introduce a deep learning framework that decodes latent representations into geometrically valid, printable objects while respecting manufacturing constraints such as overhang angles, wall thickness, and structural integrity. The methodology demonstrates that neural decoders can learn complex mapping functions from abstract representations to valid 3D geometries, producing parts with significantly improved manufacturability compared to naive generation approaches. We validate the approach on diverse object categories and demonstrate practical 3D printing of decoder-generated structures.

</details>


### [13] [Representations of Text and Images Align From Layer One](https://arxiv.org/abs/2601.08017)
*Evžen Wybitul,Javier Rando,Florian Tramèr,Stanislav Fort*

Main category: cs.CV

TL;DR: 论文提出一种基于合成的可视化方法，显示在适配器式视觉-语言模型中，图像与文本概念在非常靠前的层（甚至第一层）就已出现有意义的对齐，与常见“后期对齐”观点相悖。方法简单、快速、无需外部模型/数据，并能用于可解释性。


<details>
  <summary>Details</summary>
Motivation: 主流认知认为多模态模型的图像-文本对齐主要在后期层出现，但缺乏逐层、逐概念的直接证据；现有度量手段依赖额外数据或模型，复杂且间接。作者希望提供一种无需外部资源、能逐层检验并可视化概念对齐的直接方法。

Method: 受DeepDream启发：对给定文本概念（如“木星”），在指定层提取其概念向量，然后通过优化合成一张使该层表示与该概念向量对齐的图像。将该流程应用于Gemma 3的七个层级、数百个概念，观察与评估合成图像是否呈现目标概念的显著视觉特征。

Result: 在Gemma 3上的实验显示，从第1层起就能生成体现目标文本概念显著视觉要素的图像；在第1层，超过50%的合成图像呈现可辨识的动物、活动或季节等特征。此结果构成逐概念、逐层的直接建设性证据，指向早期层的图文对齐。

Conclusion: 图文对齐并非仅在深层出现；适配器式视觉-语言模型在早期层就存在有意义的对齐。提出的方法简单快速、无需外部资源，为多模态对齐评估提供新工具，并为可解释性打开通过回溯可视化表示空间的新路径。

Abstract: We show that for a variety of concepts in adapter-based vision-language models, the representations of their images and their text descriptions are meaningfully aligned from the very first layer. This contradicts the established view that such image-text alignment only appears in late layers. We show this using a new synthesis-based method inspired by DeepDream: given a textual concept such as "Jupiter", we extract its concept vector at a given layer, and then use optimisation to synthesise an image whose representation aligns with that vector. We apply our approach to hundreds of concepts across seven layers in Gemma 3, and find that the synthesised images often depict salient visual features of the targeted textual concepts: for example, already at layer 1, more than 50 % of images depict recognisable features of animals, activities, or seasons. Our method thus provides direct, constructive evidence of image-text alignment on a concept-by-concept and layer-by-layer basis. Unlike previous methods for measuring multimodal alignment, our approach is simple, fast, and does not require auxiliary models or datasets. It also offers a new path towards model interpretability, by providing a way to visualise a model's representation space by backtracing through its image processing components.

</details>


### [14] [Training Free Zero-Shot Visual Anomaly Localization via Diffusion Inversion](https://arxiv.org/abs/2601.08022)
*Samet Hicsonmez,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一个无需训练、仅视觉的零样本异常检测方法，基于预训练DDIM的反演与中途起点重建，通过输入-重建差异实现像素级定位，在VISA上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有ZSAD要么依赖语言等多模态提示实现定位，要么仅能做图像级判断、缺乏空间精度；希望在不依赖细粒度提示与额外模态的前提下，获得强定位能力并减少提示工程依赖。

Method: 对输入图像配以通用文本描述“an image of an [object class]”，在预训练的DDIM中进行图像反演得到潜变量，从固定的中间t步开始去噪重建图像。由于扩散模型仅在正常数据上训练，重建偏向“正常外观”。以输入与重建的差异（像素/特征层面）作为异常热图，实现无训练的定位与检测。

Result: 在VISA数据集上取得SOTA性能，展现出强像素级定位效果，且无需辅助模态或复杂提示。

Conclusion: 基于DDIM反演与中途去噪的训练免方法可在纯视觉ZSAD中实现高精度定位，减少对细粒度提示的依赖，为零样本异常检测从“提示依赖”向“模型内在先验”转变提供路径。

Abstract: Zero-Shot image Anomaly Detection (ZSAD) aims to detect and localise anomalies without access to any normal training samples of the target data. While recent ZSAD approaches leverage additional modalities such as language to generate fine-grained prompts for localisation, vision-only methods remain limited to image-level classification, lacking spatial precision. In this work, we introduce a simple yet effective training-free vision-only ZSAD framework that circumvents the need for fine-grained prompts by leveraging the inversion of a pretrained Denoising Diffusion Implicit Model (DDIM). Specifically, given an input image and a generic text description (e.g., "an image of an [object class]"), we invert the image to obtain latent representations and initiate the denoising process from a fixed intermediate timestep to reconstruct the image. Since the underlying diffusion model is trained solely on normal data, this process yields a normal-looking reconstruction. The discrepancy between the input image and the reconstructed one highlights potential anomalies. Our method achieves state-of-the-art performance on VISA dataset, demonstrating strong localisation capabilities without auxiliary modalities and facilitating a shift away from prompt dependence for zero-shot anomaly detection research. Code is available at https://github.com/giddyyupp/DIVAD.

</details>


### [15] [A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs](https://arxiv.org/abs/2601.08024)
*Amin Abbasishahkoo,Mahboubeh Dadkhah,Lionel Briand*

Main category: cs.CV

TL;DR: 提出一种利用视觉-语言模型的概念多样性（CBD）度量，显著降低多样性选取的计算成本，同时保持与几何多样性（GD）高度相关；结合简单不确定性指标Margin构成混合选取，在多数据集/模型/预算上优于SOTA基线并具备良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 微调DNN以维持/提升性能需要为新收集输入标注，成本高；需从大量未标注样本中挑选少量但信息量大的子集。现有以多样性为核心的方法有效但计算昂贵、难以扩展到大规模输入。

Method: 提出概念多样性CBD：借助视觉-语言模型对图像的概念空间表示来度量样本间多样性，计算高效且与GD强相关；据此与简单的不确定性指标Margin进行混合选取。系统评估涵盖多种DNN、输入集、预算，并对比五个最有效的SOTA选取基线。

Result: CBD度量与GD呈强相关，但计算时间仅为其很小一部分；基于CBD的（含与Margin的混合）选取在提升DNN方面持续优于所有基线，且在如ImageNet等大规模输入上，选取耗时接近Margin等简单不确定性方法。

Conclusion: CBD在效用和计算效率上兼具优势，尤其优于混合型基线；在重复、规模化的输入选择情境中可扩展性强，适合作为实用的高效多样性选取方案。

Abstract: Maintaining or improving the performance of Deep Neural Networks (DNNs) through fine-tuning requires labeling newly collected inputs, a process that is often costly and time-consuming. To alleviate this problem, input selection approaches have been developed in recent years to identify small, yet highly informative subsets for labeling. Diversity-based selection is one of the most effective approaches for this purpose. However, they are often computationally intensive and lack scalability for large input sets, limiting their practical applicability. To address this challenge, we introduce Concept-Based Diversity (CBD), a highly efficient metric for image inputs that leverages Vision-Language Models (VLM). Our results show that CBD exhibits a strong correlation with Geometric Diversity (GD), an established diversity metric, while requiring only a fraction of its computation time. Building on this finding, we propose a hybrid input selection approach that combines CBD with Margin, a simple uncertainty metric. We conduct a comprehensive evaluation across a diverse set of DNN models, input sets, selection budgets, and five most effective state-of-the-art selection baselines. The results demonstrate that the CBD-based selection consistently outperforms all baselines at guiding input selection to improve the DNN model. Furthermore, the CBD-based selection approach remains highly efficient, requiring selection times close to those of simple uncertainty-based methods such as Margin, even on larger input sets like ImageNet. These results confirm not only the effectiveness and computational advantage of the CBD-based approach, particularly compared to hybrid baselines, but also its scalability in repetitive and extensive input selection scenarios.

</details>


### [16] [FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures](https://arxiv.org/abs/2601.08026)
*Jifeng Song,Arun Das,Pan Wang,Hui Ji,Kun Zhao,Yufei Huang*

Main category: cs.CV

TL;DR: FigEx2 是一个从复合科学图中自动定位面板并生成面板级标题的视觉条件框架，通过降噪融合与分阶段优化实现强一致的多模态对齐，在生物科学基准上取得最优检测与生成表现，并具备跨学科零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现实论文工作流中，复合图往往只有整图级说明或缺失面板级描述，导致细粒度理解与复现困难；现有开集生成受表述多样性与噪声影响，难以稳定地进行面板检测与caption对齐。

Method: 提出 FigEx2：1) 从复合图直接定位面板并生成面板级caption；2) 设计噪声感知门控融合模块，对token级特征进行自适应滤波，稳定检测查询空间；3) 分阶段优化：有监督学习 + 强化学习，利用CLIP对齐和BERTScore语义奖励，强化跨模态一致性；4) 构建BioSci-Fig-Cap高质量面板级标注基准，并提供物理与化学跨学科测试集。

Result: 在检测上达到0.726 mAP@0.5:0.95；在生成上较Qwen3-VL-8B分别提升METEOR 0.51、BERTScore 0.24；在无微调条件下对OOD科学领域表现出显著零样本可迁移性。

Conclusion: FigEx2 能端到端地完成面板定位与面板级caption生成，通过降噪融合与RL对齐实现稳健的多模态一致性，刷新基准并具备良好的跨域泛化能力。

Abstract: Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.

</details>


### [17] [Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling](https://arxiv.org/abs/2601.08040)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: 提出首个面向生物医学图像的视觉-语言引导造假生成与检测一体化框架：用扩散模型+文本提示生成可控伪造，并用Integscan实现精确定位与检测，在新建数据集Rescind上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 生物医学论文中图像造假威胁研究诚信，但与自然图像取证不同：存在领域特有伪影、复杂纹理、非结构化排版，现有方法难以泛化与精确定位。

Method: 1) 生成：将扩散式合成与视觉-语言提示结合，执行复制、拼接、区域移除等可控操纵；引入VLM一致性验证环以过滤不符语义的伪造。2) 数据：构建Rescind基准，含细粒度标注与按模态划分。3) 检测：提出Integscan——结构化状态空间(SSM)框架，融合注意力增强的视觉编码与提示条件的语义对齐，实现精细定位。

Result: 在Rescind及现有基准上，Integscan在检测与定位任务均达成SOTA；生成管线能产生语义一致、逼真且多模态覆盖的伪造样本。

Conclusion: 视觉-语言引导的生成+检测范式有效应对生物医学图像造假难题，Rescind与Integscan为自动化科研诚信分析提供强有力基础与通用框架。

Abstract: Scientific image manipulation in biomedical publications poses a growing threat to research integrity and reproducibility. Unlike natural image forensics, biomedical forgery detection is uniquely challenging due to domain-specific artifacts, complex textures, and unstructured figure layouts. We present the first vision-language guided framework for both generating and detecting biomedical image forgeries. By combining diffusion-based synthesis with vision-language prompting, our method enables realistic and semantically controlled manipulations, including duplication, splicing, and region removal, across diverse biomedical modalities. We introduce Rescind, a large-scale benchmark featuring fine-grained annotations and modality-specific splits, and propose Integscan, a structured state space modeling framework that integrates attention-enhanced visual encoding with prompt-conditioned semantic alignment for precise forgery localization. To ensure semantic fidelity, we incorporate a vision-language model based verification loop that filters generated forgeries based on consistency with intended prompts. Extensive experiments on Rescind and existing benchmarks demonstrate that Integscan achieves state of the art performance in both detection and localization, establishing a strong foundation for automated scientific integrity analysis.

</details>


### [18] [The Role of Noisy Data in Improving CNN Robustness for Image Classification](https://arxiv.org/abs/2601.08043)
*Oscar H. Ramírez-Agudelo,Nicoleta Gorea,Aliza Reif,Lorenzo Bonasera,Michael Karl*

Main category: cs.CV

TL;DR: 在CIFAR-10与ResNet-18上，训练集仅加入约10%的受控噪声（高斯噪声、椒盐噪声、模糊）即可在噪声测试条件下显著降低测试损失、提升准确率，对干净数据性能影响很小，表明噪声暴露可作为简洁有效的正则化手段。


<details>
  <summary>Details</summary>
Motivation: 现实世界输入常含噪声与失真，纯净数据训练的CNN对分布偏移与扰动较脆弱。作者希望验证：在训练阶段有意识地加入受控噪声，能否提高模型对腐化测试的鲁棒性，同时不显著损害干净数据性能。

Method: 以CIFAR-10为基准，选用ResNet-18，系统性地向训练集注入三类常见腐化：高斯噪声、椒盐噪声、高斯模糊；变化噪声强度与污染比例（如10%等），并在干净与完全腐化的测试集上评估测试损失与准确率。

Result: 当训练集中仅包含约10%带噪样本时，在完全腐化测试条件下测试损失显著下降、准确率提升；对干净测试集的性能影响最小。相比纯净训练，鲁棒性收益明显。

Conclusion: 适度、受控的噪声暴露可作为简单有效的正则化策略，在保持干净数据性能的同时，提高模型对常见图像腐化的鲁棒性，为数据洁净度与实际抗扰性的权衡提供实践路径。

Abstract: Data quality plays a central role in the performance and robustness of convolutional neural networks (CNNs) for image classification. While high-quality data is often preferred for training, real-world inputs are frequently affected by noise and other distortions. This paper investigates the effect of deliberately introducing controlled noise into the training data to improve model robustness. Using the CIFAR-10 dataset, we evaluate the impact of three common corruptions, namely Gaussian noise, Salt-and-Pepper noise, and Gaussian blur at varying intensities and training set pollution levels. Experiments using a Resnet-18 model reveal that incorporating just 10\% noisy data during training is sufficient to significantly reduce test loss and enhance accuracy under fully corrupted test conditions, with minimal impact on clean-data performance. These findings suggest that strategic exposure to noise can act as a simple yet effective regularizer, offering a practical trade-off between traditional data cleanliness and real-world resilience.

</details>


### [19] [Exploiting DINOv3-Based Self-Supervised Features for Robust Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2601.08078)
*Guoping Xu,Jayaram K. Udupa,Weiguo Lu,You Zhang*

Main category: cs.CV

TL;DR: 提出DINO-AugSeg，将DINOv3特征与小样本医学分割结合，通过小波域特征增强与上下文引导融合，跨五种模态六个数据集显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 少样本医学图像分割受限于标注稀缺，尽管自监督基础模型（如DINOv3）具备强密集特征能力，但域差异导致直接迁移到医学图像效果受限，需要兼顾语义与细节的鲁棒特征增强与融合策略。

Method: 1) WT-Aug：基于小波的特征级数据增强，对DINOv3提取的特征在频域进行扰动，提升特征多样性与鲁棒性；2) CG-Fuse：上下文信息引导的融合模块，利用跨注意力将语义丰富的低分辨率特征与空间细节丰富的高分辨率特征进行有效整合；整体框架以DINOv3为特征 backbone，服务于少样本分割。

Result: 在MRI、CT、超声、内镜、皮肤镜五种模态、六个公开基准上，在有限样本条件下均优于现有方法，表现出稳定且一致的领先性能。

Conclusion: 将小波域特征增强与上下文引导的跨尺度融合结合到DINOv3特征上，可有效缓解域差异，提升少样本医学分割的泛化与鲁棒性；DINO-AugSeg是推进少样本医学图像分割的有前景方向，代码与数据将开源。

Abstract: Deep learning-based automatic medical image segmentation plays a critical role in clinical diagnosis and treatment planning but remains challenging in few-shot scenarios due to the scarcity of annotated training data. Recently, self-supervised foundation models such as DINOv3, which were trained on large natural image datasets, have shown strong potential for dense feature extraction that can help with the few-shot learning challenge. Yet, their direct application to medical images is hindered by domain differences. In this work, we propose DINO-AugSeg, a novel framework that leverages DINOv3 features to address the few-shot medical image segmentation challenge. Specifically, we introduce WT-Aug, a wavelet-based feature-level augmentation module that enriches the diversity of DINOv3-extracted features by perturbing frequency components, and CG-Fuse, a contextual information-guided fusion module that exploits cross-attention to integrate semantic-rich low-resolution features with spatially detailed high-resolution features. Extensive experiments on six public benchmarks spanning five imaging modalities, including MRI, CT, ultrasound, endoscopy, and dermoscopy, demonstrate that DINO-AugSeg consistently outperforms existing methods under limited-sample conditions. The results highlight the effectiveness of incorporating wavelet-domain augmentation and contextual fusion for robust feature representation, suggesting DINO-AugSeg as a promising direction for advancing few-shot medical image segmentation. Code and data will be made available on https://github.com/apple1986/DINO-AugSeg.

</details>


### [20] [From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models](https://arxiv.org/abs/2601.08095)
*Dongsik Yoon,Jongeun Kim*

Main category: cs.CV

TL;DR: 提出一个三阶段自动化管线，用扩散模型合成领域特定数据，结合多模态自动评估与用户偏好分类器筛选，缓解预训练到部署环境的分布偏移，降低真实数据采集成本。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散/视觉模型在真实部署场景中存在分布偏移，导致检测/识别等下游任务性能下降。采集和标注与目标域高度匹配的大规模数据昂贵且慢，需要一种自动化方式快速构建高质量、可部署的领域数据集。

Method: 三阶段框架：1) 受控图像修复（inpainting）：在领域特定背景中合成目标对象，实现对象与背景的可控组合；2) 多模态质量验证：综合对象检测结果、审美评分与视觉-语言对齐度，对合成样本进行自动筛选；3) 用户偏好分类器：学习主观偏好与任务相关的选择标准，进一步精炼样本集。

Result: 该管线可自动、高效生成高质量领域数据集，提升可部署性，减少对真实世界数据的依赖。

Conclusion: 通过受控合成与多模态/用户偏好筛选的闭环，能构建贴近目标域分布的数据集，以缓解分布偏移并节省数据采集与标注成本。

Abstract: In this paper, we present an automated pipeline for generating domain-specific synthetic datasets with diffusion models, addressing the distribution shift between pre-trained models and real-world deployment environments. Our three-stage framework first synthesizes target objects within domain-specific backgrounds through controlled inpainting. The generated outputs are then validated via a multi-modal assessment that integrates object detection, aesthetic scoring, and vision-language alignment. Finally, a user-preference classifier is employed to capture subjective selection criteria. This pipeline enables the efficient construction of high-quality, deployable datasets while reducing reliance on extensive real-world data collection.

</details>


### [21] [PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images](https://arxiv.org/abs/2601.08127)
*Mohamad Koohi-Moghadam,Mohammad-Ali Nikouei Mahani,Kyongtae Tyler Bae*

Main category: cs.CV

TL;DR: 提出PathoGen：用于病理图像病灶可控高保真补绘的扩散生成模型，显著提升合成质量与下游分割表现，并缓解稀缺标注难题。


<details>
  <summary>Details</summary>
Motivation: 病理AI受限于稀有病灶与亚型的专家标注稀缺；现有数据增强无法生成具有真实形态与复杂组织结构关系的病灶，难以支撑鲁棒模型训练。

Method: 构建基于扩散模型的可控inpainting框架PathoGen：以良性组织为底图，在迭代扩散/去噪过程中合成病灶，保持自然组织边界、细胞结构与染色特征；同时输出像素级真值掩膜。与条件GAN、Stable Diffusion等基线对比评估。

Result: 在肾、皮肤、乳腺、前列腺四个数据集上，PathoGen在图像保真与分布相似度上优于SOTA生成方法；用其合成病灶扩充训练数据，在数据稀缺场景下显著提升下游分割性能，优于传统几何增强。

Conclusion: PathoGen能以高保真、可控方式生成病灶并提供像素级标注，缓解手工标注瓶颈，为在有限标注条件下构建可泛化的医学AI提供可扩展路径。

Abstract: The development of robust artificial intelligence models for histopathology diagnosis is severely constrained by the scarcity of expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. While data augmentation offers a potential solution, existing methods fail to generate sufficiently realistic lesion morphologies that preserve the complex spatial relationships and cellular architectures characteristic of histopathological tissues. Here we present PathoGen, a diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images. Unlike conventional augmentation techniques, PathoGen leverages the iterative refinement process of diffusion models to synthesize lesions with natural tissue boundaries, preserved cellular structures, and authentic staining characteristics. We validate PathoGen across four diverse datasets representing distinct diagnostic challenges: kidney, skin, breast, and prostate pathology. Quantitative assessment confirms that PathoGen outperforms state-of-the-art generative baselines, including conditional GAN and Stable Diffusion, in image fidelity and distributional similarity. Crucially, we show that augmenting training sets with PathoGen-synthesized lesions enhances downstream segmentation performance compared to traditional geometric augmentations, particularly in data-scarce regimes. Besides, by simultaneously generating realistic morphology and pixel-level ground truth, PathoGen effectively overcomes the manual annotation bottleneck. This approach offers a scalable pathway for developing generalizable medical AI systems despite limited expert-labeled data.

</details>


### [22] [How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?](https://arxiv.org/abs/2601.08133)
*Peng Gao,Yujian Lee,Yongqi Xu,Wentao Fan*

Main category: cs.CV

TL;DR: 本文提出用于音频-视觉语义分割（AVSS）的协同框架SSP，结合光流预掩码与文本提示，并通过视觉-文本对齐与后掩码训练，显著优于现有AVS方法。


<details>
  <summary>Details</summary>
Motivation: 传统AVS仅定位发声目标像素，缺乏语义理解；既有AVSS方案将任务拆分但对静止声源与语义一致性处理不足。需要一种能同时利用时序运动信息与文本语义、并实现跨模态对齐的方案，以稳健分割包含移动与静止声源的复杂场景。

Method: SSP采用“踏脚石+”两阶段思路：1）预掩码阶段（pre-mask）：利用光流获取运动动态，生成辅助分割掩码，为后续语义分析提供时序上下文；2）文本提示：引入两类prompt——声源类别提示与场景描述提示，缓解静止声源难以通过运动辨识的问题；3）视觉-文本对齐模块（VTA）：实现跨模态融合，提升语义一致性；4）训练时引入后掩码（post-mask）技术，迫使模型学习光流结构图式，强化对运动信息的内化。

Result: 在实验中，SSP在多个AVS/AVSS基准上取得更高的分割精度与效率，相比现有方法表现更优，能在包含移动与静止声源的场景中提供更准确的语义分割。

Conclusion: 通过光流驱动的预掩码、双文本提示与VTA跨模态对齐，并辅以后掩码训练，SSP有效解决AVSS中运动/静止声源的统一处理与语义一致性问题，达到领先性能。

Abstract: Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \textit{S}tepping \textit{S}tone \textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.

</details>


### [23] [Subspace Alignment for Vision-Language Model Test-time Adaptation](https://arxiv.org/abs/2601.08139)
*Zhichen Zeng,Wenxuan Bao,Xiao Lin,Ruizhong Qiu,Tianxin Wei,Xuying Ning,Yuchen Yan,Chen Luo,Monica Xiao Cheng,Jingrui He,Hanghang Tong*

Main category: cs.CV

TL;DR: 提出SubTTA：在测试时通过对齐视觉与文本的语义子空间并过滤视觉噪声，提升VLM在分布移位下的TTA效果，平均超越SOTA 2.24%。


<details>
  <summary>Details</summary>
Motivation: VLM在零样本下强大但易受分布移位影响。现有TTA依赖零样本伪标签，移位下易不可靠：跨模态关系失真（模态鸿沟）与视觉表征含大量与任务无关的噪声（视觉干扰），导致适应被误导。

Method: SubTTA包含两步：1) 模态对齐：提取视觉与文本的主子空间，最小化两者的弦距离，将视觉流形对齐到文本语义锚点；2) 噪声抑制：将已对齐的视觉特征投影到与任务相关的文本子空间内，过滤无关噪声；随后在该“净化”空间执行标准TTA以细化决策边界。

Result: 在多种数据集与VLM架构上，SubTTA优于现有TTA方法，平均提升2.24%。

Conclusion: 通过子空间对齐与投影净化，SubTTA缓解模态鸿沟与视觉干扰问题，使零样本伪标签更可靠，从而稳定提升VLM在分布移位下的测试时适应性能。

Abstract: Vision-language models (VLMs), despite their extraordinary zero-shot capabilities, are vulnerable to distribution shifts. Test-time adaptation (TTA) emerges as a predominant strategy to adapt VLMs to unlabeled test data on the fly. However, existing TTA methods heavily rely on zero-shot predictions as pseudo-labels for self-training, which can be unreliable under distribution shifts and misguide adaptation due to two fundamental limitations. First (Modality Gap), distribution shifts induce gaps between visual and textual modalities, making cross-modal relations inaccurate. Second (Visual Nuisance), visual embeddings encode rich but task-irrelevant noise that often overwhelms task-specific semantics under distribution shifts. To address these limitations, we propose SubTTA, which aligns the semantic subspaces of both modalities to enhance zero-shot predictions to better guide the TTA process. To bridge the modality gap, SubTTA extracts the principal subspaces of both modalities and aligns the visual manifold to the textual semantic anchor by minimizing their chordal distance. To eliminate visual nuisance, SubTTA projects the aligned visual features onto the task-specific textual subspace, which filters out task-irrelevant noise by constraining visual embeddings within the valid semantic span, and standard TTA is further performed on the purified space to refine the decision boundaries. Extensive experiments on various benchmarks and VLM architectures demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.

</details>


### [24] [Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention](https://arxiv.org/abs/2601.08151)
*Shezheng Song,Shasha Li,Jie Yu*

Main category: cs.CV

TL;DR: 本文通过逐层遮罩与注意力分析揭示MLLM中视觉-文本融合并非均匀发生，而是在若干关键层显著出现，部分模型在末段存在“复查”式视觉信号再激活；据此提出无需训练的对比式注意力框架，提升多模态推理表现。


<details>
  <summary>Details</summary>
Motivation: 虽MLLM在视觉-语言理解上进展迅速，但内部如何融合视觉与文本仍缺乏系统性认识，限制了可解释性与进一步性能优化。

Method: 对多种架构进行系统的逐层遮罩（layer-wise masking）实验，度量各层对融合与输出的贡献；并分析层内/层间注意力演化，观察无关区域的高噪声与对齐区域注意力递增。基于早期融合层与最终层之间的注意力变换关系，提出一个训练-free的对比注意力（contrastive attention）框架，显式强化有意义的注意力迁移以抑制噪声。

Result: 发现融合在若干特定层集中出现，而非全网均匀；部分模型在临近输出阶段对视觉信息进行“晚期复查”再激活。注意力层面存在持续的无关区域高噪声，同时文本对齐区域注意力逐步升高。所提训练-free框架在多种MLLM与基准上带来一致的多模态推理性能提升。

Conclusion: MLLM的视觉-文本融合具有分层阶段性与晚期再激活特征；对比式注意力在无需再训练的前提下能有效突出有意义的注意力变化、抑制噪声，从而提升多模态推理。代码将开源。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage "review" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.

</details>


### [25] [Instance-Aligned Captions for Explainable Video Anomaly Detection](https://arxiv.org/abs/2601.08155)
*Inpyo Song,Minjun Joo,Joonhyung Kwon,Eunji Jeon,Jangwon Lee*

Main category: cs.CV

TL;DR: 提出实例对齐的空间落地说明字幕与新基准VIEW360+，用于让视频异常检测的解释可验证并更可信；实验揭示现有LLM/VLM方案的不足。


<details>
  <summary>Details</summary>
Motivation: 现有可解释视频异常检测缺乏空间落地与实体级关联，尤其在多实体交互中说明常不完整或与视觉不对齐，难以验证、可信度低。

Method: 提出“实例对齐说明字幕”（instance-aligned captions），将每条文本主张与具体目标实例绑定，标注其外观与运动属性，并回答谁引发异常、每个实体在做什么、影响了谁、以及解释在何处落地。框架为8个常用VAD基准进行实例级空间标注，并扩展360度自中心数据集VIEW360，新增868个视频、8个场景与4类异常，形成VIEW360+。

Result: 基于所构建的实例级、空间落地的说明字幕进行实验，显示当前基于LLM/VLM的方法在多实体与空间对齐上的显著缺陷；新基准能可靠评估可解释性与可信度。

Conclusion: 实例级空间对齐解释能让VAD的说明可验证且可操作；VIEW360+为未来可信、可解释异常检测研究提供了稳健测试平台，并暴露了现有方法的关键短板。

Abstract: Explainable video anomaly detection (VAD) is crucial for safety-critical applications, yet even with recent progress, much of the research still lacks spatial grounding, making the explanations unverifiable. This limitation is especially pronounced in multi-entity interactions, where existing explainable VAD methods often produce incomplete or visually misaligned descriptions, reducing their trustworthiness. To address these challenges, we introduce instance-aligned captions that link each textual claim to specific object instances with appearance and motion attributes. Our framework captures who caused the anomaly, what each entity was doing, whom it affected, and where the explanationis grounded, enabling verifiable and actionable reasoning. We annotate eight widely used VAD benchmarks and extend the 360-degree egocentric dataset, VIEW360, with 868 additional videos, eight locations, and four new anomaly types, creating VIEW360+, a comprehensive testbed for explainable VAD. Experiments show that our instance-level spatially grounded captions reveal significant limitations in current LLM- and VLM-based methods while providing a robust benchmark for future research in trustworthy and interpretable anomaly detection.

</details>


### [26] [A Hardware-Algorithm Co-Designed Framework for HDR Imaging and Dehazing in Extreme Rocket Launch Environments](https://arxiv.org/abs/2601.08162)
*Jing Tao,Banglei Guan,Pengju Sun,Taihang Lei,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出硬件—算法协同方案，用SVE多曝光传感器与物理先验去雾算法，在极端火箭发射成像中鲁棒恢复羽流与发动机区域的真实光辐射，支撑速度、频率与振动等关键力学参数的定量提取。


<details>
  <summary>Details</summary>
Motivation: 火箭发射场景下强烈燃烧导致颗粒雾霾与超大亮度动态范围（>120 dB），使传统成像/去雾与后续摄影测量、测速严重失真，需要一种可在单帧中获取可靠多曝光信息并基于物理约束稳健去雾的方案，以恢复可用于力学量化分析的有效图像。

Method: 硬件—算法协同：定制空间可变曝光（SVE）传感器单次采集多曝光数据；基于该数据的物理感知去雾流程，包含动态雾密度估计、区域自适应照度优化与多尺度熵约束融合，以将雾散射与场景辐射分离。

Result: 在真实火箭发射影像与受控实验上验证，相比基线方法更好恢复羽流与发动机区域的物理准确视觉信息。

Conclusion: 该框架在极端航天环境下提供可靠图像基础，可稳健提取粒子速度、流动不稳定频率与结构振动等关键力学参数，为精确定量分析提供支撑。

Abstract: Quantitative optical measurement of critical mechanical parameters -- such as plume flow fields, shock wave structures, and nozzle oscillations -- during rocket launch faces severe challenges due to extreme imaging conditions. Intense combustion creates dense particulate haze and luminance variations exceeding 120 dB, degrading image data and undermining subsequent photogrammetric and velocimetric analyses. To address these issues, we propose a hardware-algorithm co-design framework that combines a custom Spatially Varying Exposure (SVE) sensor with a physics-aware dehazing algorithm. The SVE sensor acquires multi-exposure data in a single shot, enabling robust haze assessment without relying on idealized atmospheric models. Our approach dynamically estimates haze density, performs region-adaptive illumination optimization, and applies multi-scale entropy-constrained fusion to effectively separate haze from scene radiance. Validated on real launch imagery and controlled experiments, the framework demonstrates superior performance in recovering physically accurate visual information of the plume and engine region. This offers a reliable image basis for extracting key mechanical parameters, including particle velocity, flow instability frequency, and structural vibration, thereby supporting precise quantitative analysis in extreme aerospace environments.

</details>


### [27] [Representation Learning with Semantic-aware Instance and Sparse Token Alignments](https://arxiv.org/abs/2601.08165)
*Phuoc-Nguyen Bui,Toan Duc Nguyen,Junghyun Bum,Duc-Tai Le,Hyunseung Choo*

Main category: cs.CV

TL;DR: 提出SISTA多层次对齐框架，通过语义感知的实例级与稀疏词元级对齐改进医学图文预训练，缓解错误负样本并精细对齐补丁-词，提高多下游任务迁移表现，尤其在小样本细粒度任务上显著受益。


<details>
  <summary>Details</summary>
Motivation: 医学图文数据（影像-报告）中不同患者样本常存在高语义相似性。传统对比学习将所有非配对样本视为负样本，易引入“假负样本”，破坏语义结构，导致表征质量下降。需要一种能够识别语义相似对并在更细粒度上对齐图文信息的方法。

Method: 提出SISTA框架，进行两层级语义对齐：1) 图像-报告级：在对比学习中显式建模报告间相似度，过滤或重加权潜在假负样本，从而进行语义感知的实例级对齐；2) 补丁-词级：引入稀疏词元选择与图像补丁对齐策略，将与视觉区域相关的关键词（token）与相应图像patch进行匹配，提升细粒度跨模态一致性。整体在医学VLP预训练阶段集成上述改进。

Result: 在多数据集上迁移到三类下游任务（分类、分割、检测）均优于基线；在标注有限的细粒度任务上提升尤为显著。

Conclusion: 语义感知的多层级对齐能有效缓解医学图文预训练中的假负样本问题，并通过补丁-词级精细对齐提升跨模态表征，带来广泛而稳定的下游性能增益；代码和预训练模型将开源。

Abstract: Medical contrastive vision-language pre-training (VLP) has demonstrated significant potential in improving performance on downstream tasks. Traditional approaches typically employ contrastive learning, treating paired image-report samples as positives and unpaired ones as negatives. However, in medical datasets, there can be substantial similarities between images or reports from different patients. Rigidly treating all unpaired samples as negatives, can disrupt the underlying semantic structure and negatively impact the quality of the learned representations. In this paper, we propose a multi-level alignment framework, Representation Learning with Semantic-aware Instance and Sparse Token Alignments (SISTA) by exploiting the semantic correspondence between medical image and radiology reports at two levels, i.e., image-report and patch-word levels. Specifically, we improve the conventional contrastive learning by incorporating inter-report similarity to eliminate the false negatives and introduce a method to effectively align image patches with relevant word tokens. Experimental results demonstrate the effectiveness of the proposed framework in improving transfer performance across different datasets on three downstream tasks: image classification, image segmentation, and object detection. Notably, our framework achieves significant improvements in fine-grained tasks even with limited labeled data. Codes and pre-trained models will be made available.

</details>


### [28] [Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling](https://arxiv.org/abs/2601.08174)
*Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He*

Main category: cs.CV

TL;DR: 基于PVRCNN++并通过跨域增强与伪标签自训练提升泛化，在RoboSense2025跨平台3D检测赛获季军，目标域取得较高3D AP。


<details>
  <summary>Details</summary>
Motivation: 跨平台（不同雷达/平台）带来域间差异，直接迁移会显著掉点，需要在不依赖大量标注的情况下提升目标域性能与泛化。

Method: 以PVRCNN++为基座融合点/体素特征；设计针对域差异的数据增广；引入自训练：用源域训练的模型在目标域产生伪标签，并迭代精炼以缩小域间差距。

Result: RoboSense2025挑战赛中获第3名；Phase-1目标域Car类3D AP=62.67%；Phase-2目标域Car类=58.76%，Pedestrian类=49.81%。

Conclusion: 在强基线之上，通过面向跨域的数据增广与伪标签自训练，可有效缩小域间差距并提升跨平台3D检测性能，具有实证效果与可拓展性。

Abstract: This technical report represents the award-winning solution to the Cross-platform 3D Object Detection task in the RoboSense2025 Challenge. Our approach is built upon PVRCNN++, an efficient 3D object detection framework that effectively integrates point-based and voxel-based features. On top of this foundation, we improve cross-platform generalization by narrowing domain gaps through tailored data augmentation and a self-training strategy with pseudo-labels. These enhancements enabled our approach to secure the 3rd place in the challenge, achieving a 3D AP of 62.67% for the Car category on the phase-1 target domain, and 58.76% and 49.81% for Car and Pedestrian categories respectively on the phase-2 target domain.

</details>


### [29] [CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval](https://arxiv.org/abs/2601.08175)
*Feiran Wang,Junyi Wu,Dawen Cai,Yuan Hong,Yan Yan*

Main category: cs.CV

TL;DR: CogniMap3D是一种受生物启发的动态3D场景理解与重建框架，结合运动线索检测动态物体、认知式静态场景记忆与检索、以及因子图优化，实现跨长序列与多次回访的连续场景理解并达SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解/重建在长期时序、多次回访与动态场景中往往难以兼顾：动态目标干扰重建，静态结构难以长期记忆与快速检索，摄像机位姿在长序列中易漂移。人类具备将静态环境记忆、在回访时快速定位并更新认知地图的能力，论文意在将这种认知过程引入计算机视觉系统。

Method: 提出CogniMap3D框架，包含三大组件：1) 多阶段运动线索机制，结合深度与位姿先验，识别动态区域；2) 认知式映射系统，维护持久的静态场景记忆库，实现跨访问的存储、召回与更新，并在回访时进行场景匹配与相机重定位；3) 因子图优化用于全局位姿精炼。整体流程：输入图像流→利用运动线索检测动态→将静态部分与记忆库匹配→在回访时检索已存场景并更新记忆。

Result: 在视频深度估计、相机位姿重建与3D建图任务上取得SOTA或领先结果；在长时序与多次回访场景中表现出连续、稳健的场景理解能力。

Conclusion: 通过模拟人类认知的记忆与更新机制，CogniMap3D能有效隔离动态干扰、复用并拓展静态场景知识，并配合因子图优化提升位姿与重建质量，适用于持续与多回访的3D理解/建图任务。

Abstract: We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.

</details>


### [30] [Instruction-Driven 3D Facial Expression Generation and Transition](https://arxiv.org/abs/2601.08179)
*Anh H. Vo,Tae-Seok Kim,Hulin Jin,Soo-Mi Choi,Yong-Guk Kim*

Main category: cs.CV

TL;DR: 提出一个指令驱动的3D人脸表情生成与过渡框架，能从单张人脸图像出发，根据文本指令在两种表情间生成平滑过渡序列，并在CK+与CelebV-HQ上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D头像通常局限于六种基本表情，缺乏细腻的情感变化与可控的表情过渡。需要一种能把文本描述与表情特征对齐、并能在任意两种表情间生成自然过渡的模型，以扩充表情库并提升可控性与真实感。

Method: 1) 提出IFED（指令驱动表情分解器），进行多模态学习，将文本描述与人脸表情特征对齐；2) 提出I2FET，将IFED与顶点重建损失结合，优化潜向量的语义理解，从而根据文本生成表情序列；3) 设计Facial Expression Transition模型，生成在起始与目标表情之间的平滑过渡。

Result: 在CK+与CelebV-HQ数据集上广泛评测，整体性能优于SOTA方法；能够依据文本指令生成符合语义的表情轨迹与平滑过渡序列。

Conclusion: 该框架有效实现了从文本到3D表情及其过渡的可控生成，显著扩展了表情与过渡的表达空间，具有较强的应用潜力；项目页提供更多信息与演示。

Abstract: A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/

</details>


### [31] [Second-order Gaussian directional derivative representations for image high-resolution corner detection](https://arxiv.org/abs/2601.08182)
*Dongbo Xie,Junjie Qiu,Changming Sun,Weichuan Zhang*

Main category: cs.CV

TL;DR: 提出基于二阶高斯方向导数（SOGDD）的高分辨率角点建模与检测方法，克服相邻角点灰度相互干扰的理论缺陷，能更准确定位相邻角点，并在匹配、模糊鲁棒性与三维重建上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于简单角模型（如Zhang等）的角点特征推导忽略了相邻角点之间的灰度耦合，导致在高分辨率、角点相互接近时定位与判别失真，影响后续图像匹配和3D重建。需建立能分辨相邻角点并稳定提取强度变化的信息表征与检测框架。

Method: 1) 用SOGDD滤波器对两类典型高分辨率角模型（END型与L型）进行平滑建模；2) 分别推导两模型在SOGDD下的解析表征，分析强度变化与方向响应；3) 基于表征结果给出高斯滤波尺度选择准则，以分离相邻角点的灰度影响；4) 设计新的高分辨率角点检测流程，利用SOGDD响应与尺度策略实现精准定位。

Result: 理论上揭示了相邻角点灰度互扰的机制与SOGDD表征特性；实验表明新方法在定位误差更低、对图像模糊更鲁棒，并在图像匹配与三维重建任务中整体性能优于SOTA。

Conclusion: 使用SOGDD构建与分析高分辨率角模型可有效分离相邻角点影响，指导合理尺度选择并提升检测精度，所提方法在多项视觉任务上优于现有方法。

Abstract: Corner detection is widely used in various computer vision tasks, such as image matching and 3D reconstruction. Our research indicates that there are theoretical flaws in Zhang et al.'s use of a simple corner model to obtain a series of corner characteristics, as the grayscale information of two adjacent corners can affect each other. In order to address the above issues, a second-order Gaussian directional derivative (SOGDD) filter is used in this work to smooth two typical high-resolution angle models (i.e. END-type and L-type models). Then, the SOGDD representations of these two corner models were derived separately, and many characteristics of high-resolution corners were discovered, which enabled us to demonstrate how to select Gaussian filtering scales to obtain intensity variation information from images, accurately depicting adjacent corners. In addition, a new high-resolution corner detection method for images has been proposed for the first time, which can accurately detect adjacent corner points. The experimental results have verified that the proposed method outperforms state-of-the-art methods in terms of localization error, robustness to image blur transformation, image matching, and 3D reconstruction.

</details>


### [32] [GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards](https://arxiv.org/abs/2601.08183)
*Yan Zhu,Te Luo,Pei-Yao Fu,Zhen Zhang,Zi-Long Wang,Yi-Fan Qu,Zi-Han Geng,Jia-Qi Xu,Lu Yao,Li-Yun Ma,Wei Su,Wei-Feng Chen,Quan-Lin Li,Shuo Wang,Ping-Hong Zhou*

Main category: cs.CV

TL;DR: GI-Bench系统性评估12个MLLM在胃肠内镜全流程（定位-识别-诊断-描述-处置）上的表现。顶尖模型诊断接近初级医师并优于住培，但在空间定位显著落后于人类；模型报告更流畅却更不准确，存在过度解读与幻觉。提供动态排行榜。


<details>
  <summary>Details</summary>
Motivation: MLLM在消化内镜中的潜在价值被广泛讨论，但缺乏覆盖完整临床工作流、可与人类基线比较的系统评测。需要明确模型在不同任务环节的强弱与实际临床可用性边界。

Method: 构建GI-Bench：涵盖20类细分病变与五阶段工作流（解剖定位、病变识别、诊断、描述、处置）。对12个MLLM进行量化评估，采用Macro-F1、mIoU和多维Likert量表，并与3名初级内镜医师与3名住培医师对比；并进行质性分析。

Result: Gemini-3-Pro总体最佳。在诊断推理上，顶尖模型Macro-F1=0.641，优于住培(0.492)，接近初级医师(0.727，差异不显著)。在人类病变定位mIoU>0.506显著优于最佳模型(0.345)。质性上，模型报告语言流畅度高于人类，但事实正确性更低，主因为“过度解读”和视觉幻觉。

Conclusion: MLLM已在诊断推理上接近初级内镜医师，但受制于“空间落地瓶颈”，临床可用性仍有限；应优先提升定位/可视化对齐与事实性控制。GI-Bench提供开放、动态排行榜以持续跟踪进展。

Abstract: Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical "spatial grounding bottleneck" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a "fluency-accuracy paradox": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to "over-interpretation" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.

</details>


### [33] [Human-inspired Global-to-Parallel Multi-scale Encoding for Lightweight Vision Models](https://arxiv.org/abs/2601.08190)
*Wei Xu*

Main category: cs.CV

TL;DR: 提出GPM架构与轻量级H-GPE网络，通过“先全局后细节、局部中仍维持全局感知”的人类视觉机制，兼顾参数量与算力开销，在分类/检测/分割上取得更优准确-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有轻量模型常以降低计算为目标，却引入较多参数（如LSNet、MobileMamba），不利于在资源受限设备部署；同时，受人类视觉启发的建模多过度简化，难以真实反映“先整体后细节、局部中保有全局”的感知过程。

Method: 提出GPM（Global-to-Parallel Multi-scale Encoding）：先用GIG（Global Insight Generator）提取全局线索，再以并行多尺度分支处理特征：LSAE突出中/大尺度语义关系，IRB保留细粒度纹理。两者协同，实现全局与局部的一致表示，据此构建轻量级网络H-GPE。

Result: 在图像分类、目标检测与语义分割多任务上，H-GPE在FLOPs与参数规模均衡的前提下，达到强性能，较近期SOTA轻量模型取得更优准确-效率权衡。

Conclusion: 基于人类视觉协同机制的GPM与H-GPE能在保持低计算与低参数的同时提升多任务性能，提供更实用的轻量化视觉网络设计方案。

Abstract: Lightweight vision networks have witnessed remarkable progress in recent years, yet achieving a satisfactory balance among parameter scale, computational overhead, and task performance remains difficult. Although many existing lightweight models manage to reduce computation considerably, they often do so at the expense of a substantial increase in parameter count (e.g., LSNet, MobileMamba), which still poses obstacles for deployment on resource-limited devices. In parallel, some studies attempt to draw inspiration from human visual perception, but their modeling tends to oversimplify the visual process, making it hard to reflect how perception truly operates. Revisiting the cooperative mechanism of the human visual system, we propose GPM (Global-to-Parallel Multi-scale Encoding). GPM first employs a Global Insight Generator (GIG) to extract holistic cues, and subsequently processes features of different scales through parallel branches: LSAE emphasizes mid-/large-scale semantic relations, while IRB (Inverted Residual Block) preserves fine-grained texture information, jointly enabling coherent representation of global and local features. As such, GPM conforms to two characteristic behaviors of human vision perceiving the whole before focusing on details, and maintaining broad contextual awareness even during local attention. Built upon GPM, we further develop the lightweight H-GPE network. Experiments on image classification, object detection, and semantic segmentation show that H-GPE achieves strong performance while maintaining a balanced footprint in both FLOPs and parameters, delivering a more favorable accuracy-efficiency trade-off compared with recent state-of-the-art lightweight models.

</details>


### [34] [Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging](https://arxiv.org/abs/2601.08192)
*Md. Faiyaz Abdullah Sayeedi,Rashedur Rahman,Siam Tahsin Bhuiyan,Sefatul Wasi,Ashraful Islam,Saadia Binte Alam,AKM Mahbubur Rahman*

Main category: cs.CV

TL;DR: R^4 是一个将医学影像工作流拆分为“路由-检索-反思-修复”四个协同代理的框架，在无需微调的前提下，显著提升胸片报告生成与弱监督检测的可靠性与空间对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有用于医学影像的多模态大模型多为单轮黑盒推理，难以控制推理过程、保障安全与可解释性，且在空间定位与临床一致性上易出现否定/左右侧/证据不足等关键错误。需要一种可组合、可审校、可迭代修正的代理式流程来提升稳健性与可溯源性。

Method: 提出 R^4 框架：1) Router 基于图像、病史与元数据生成任务与专科感知的提示；2) Retriever 结合示例记忆与 pass@k 采样，同时生成自由文本报告与检测框框；3) Reflector 针对临床关键错误模式（否定、左右侧、无依据、矛盾、遗漏、定位错误）逐对评审“报告-框”草案；4) Repairer 在目标化约束下迭代修订文本与空间输出，并将高质量样例回灌至记忆。框架可与多种当代 VLM 背骨结合，无需梯度微调。

Result: 在胸部 X 光任务上，R^4 相比强单模型基线，将 LLM-as-a-Judge 报告评分提升约 +1.7 至 +2.5 分，检测 mAP50 提升约 +2.5 至 +3.5 个百分点，且无需参数微调。

Conclusion: 通过路由、反思与修复的代理化流程，可将强但脆的 VLM 转化为更可靠、可审计、空间落地更好的医学影像解读工具；方法通用、可与多种 VLM 结合，并具备持续自我改进潜力。

Abstract: Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: https://github.com/faiyazabdullah/MultimodalMedAgent

</details>


### [35] [Unified Multi-Site Multi-Sequence Brain MRI Harmonization Enriched by Biomedical Semantic Style](https://arxiv.org/abs/2601.08193)
*Mengqi Wu,Yongheng Sun,Qianqian Wang,Pew-Thian Yap,Mingxia Liu*

Main category: cs.CV

TL;DR: 提出MMH框架，通过扩散式全局统一与目标特定微调两阶段，实现多站点多序列脑MRI的风格对齐，同时保持解剖内容；利用三平面注意力BiomedCLIP编码器显式解耦风格与解剖，无需成对数据，实验在T1/T2上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多中心MRI数据能增强深度学习，但不同站点/序列的非生物学差异（厂商、参数、协议）导致风格偏移，削弱泛化。现有去偏方法要么依赖稀缺的旅行受试者配对数据，要么难以有效解耦风格与解剖，且多聚焦单序列，不符合临床多序列常态。

Method: 提出MMH两阶段统一框架：1) 扩散式全局harmonizer，以“风格无关的梯度条件”将图像映射到各序列的统一域；2) 目标域细化微调器，将全局对齐结果适配到指定目标站点/域。借助三平面注意力BiomedCLIP编码器聚合多视角嵌入，表征体素级体积风格信息，实现无配对数据的“风格-解剖”显式解耦与序列感知对齐。

Result: 在4163例T1/T2 MRI上，MMH在图像特征聚类、体素级比较、组织分割、以及下游年龄与站点分类任务上均优于SOTA，显示更好的和谐化效果与下游泛化。

Conclusion: MMH可在多站点多序列场景中实现序列感知的风格统一与目标域适配，无需配对数据并能更好保持解剖信息，具有实际临床与多中心研究的应用潜力。

Abstract: Aggregating multi-site brain MRI data can enhance deep learning model training, but also introduces non-biological heterogeneity caused by site-specific variations (e.g., differences in scanner vendors, acquisition parameters, and imaging protocols) that can undermine generalizability. Recent retrospective MRI harmonization seeks to reduce such site effects by standardizing image style (e.g., intensity, contrast, noise patterns) while preserving anatomical content. However, existing methods often rely on limited paired traveling-subject data or fail to effectively disentangle style from anatomy. Furthermore, most current approaches address only single-sequence harmonization, restricting their use in real-world settings where multi-sequence MRI is routinely acquired. To this end, we introduce MMH, a unified framework for multi-site multi-sequence brain MRI harmonization that leverages biomedical semantic priors for sequence-aware style alignment. MMH operates in two stages: (1) a diffusion-based global harmonizer that maps MR images to a sequence-specific unified domain using style-agnostic gradient conditioning, and (2) a target-specific fine-tuner that adapts globally aligned images to desired target domains. A tri-planar attention BiomedCLIP encoder aggregates multi-view embeddings to characterize volumetric style information, allowing explicit disentanglement of image styles from anatomy without requiring paired data. Evaluations on 4,163 T1- and T2-weighted MRIs demonstrate MMH's superior harmonization over state-of-the-art methods in image feature clustering, voxel-level comparison, tissue segmentation, and downstream age and site classification.

</details>


### [36] [MobiDiary: Autoregressive Action Captioning with Wearable Devices and Wireless Signals](https://arxiv.org/abs/2601.08204)
*Fei Deng,Yinghui He,Chuntong Chu,Ge Wang,Han Ding,Jinsong Han,Fei Wang*

Main category: cs.CV

TL;DR: 提出MobiDiary：从IMU与Wi‑Fi等异构物理信号直接生成日常活动自然语言描述的框架，基于统一传感器编码与Transformer解码，在多基准上取得SOTA的活动“字幕”式描述与连续行为理解表现。


<details>
  <summary>Details</summary>
Motivation: 现有智能家居HAR多依赖视觉，受隐私与遮挡影响且通常输出有限类别标签；需要能在保护隐私前提下，从非视觉、噪声且异构的物理信号中生成更丰富的人类可读描述。

Method: 设计统一传感器编码器：利用运动诱发信号的共性（惯性与无线共同反映运动学动态），采用patch划分以建模局部时间相关性，并引入异构位置/放置嵌入统一不同传感器的空间语境；将统一信号token送入Transformer自回归解码器，逐词生成连贯动作描述。

Result: 在XRF V2、UWash、WiFiTAD等数据集上，跨模态泛化良好；在BLEU@4、CIDEr、RMC等“字幕式”指标上达SOTA，并在连续动作理解上优于专门化基线。

Conclusion: 非视觉、隐私友好的HAR可通过统一编码+语言解码实现从原始物理信号到自然语言的端到端描述，证明了跨模态共享归纳偏置的有效性与实际可用性。

Abstract: Human Activity Recognition (HAR) in smart homes is critical for health monitoring and assistive living. While vision-based systems are common, they face privacy concerns and environmental limitations (e.g., occlusion). In this work, we present MobiDiary, a framework that generates natural language descriptions of daily activities directly from heterogeneous physical signals (specifically IMU and Wi-Fi). Unlike conventional approaches that restrict outputs to pre-defined labels, MobiDiary produces expressive, human-readable summaries. To bridge the semantic gap between continuous, noisy physical signals and discrete linguistic descriptions, we propose a unified sensor encoder. Instead of relying on modality-specific engineering, we exploit the shared inductive biases of motion-induced signals--where both inertial and wireless data reflect underlying kinematic dynamics. Specifically, our encoder utilizes a patch-based mechanism to capture local temporal correlations and integrates heterogeneous placement embedding to unify spatial contexts across different sensors. These unified signal tokens are then fed into a Transformer-based decoder, which employs an autoregressive mechanism to generate coherent action descriptions word-by-word. We comprehensively evaluate our approach on multiple public benchmarks (XRF V2, UWash, and WiFiTAD). Experimental results demonstrate that MobiDiary effectively generalizes across modalities, achieving state-of-the-art performance on captioning metrics (e.g., BLEU@4, CIDEr, RMC) and outperforming specialized baselines in continuous action understanding.

</details>


### [37] [FUME: Fused Unified Multi-Gas Emission Network for Livestock Rumen Acidosis Detection](https://arxiv.org/abs/2601.08205)
*Taminul Islam,Toqi Tahamid Sarker,Mohamed Embaby,Khaled R Ahmed,Amer AbuGhazaleh*

Main category: cs.CV

TL;DR: 提出FUME深度学习模型，用双气体红外影像（CO2与CH4）在体外条件下检测奶牛瘤胃酸中毒，联合分割与分类，数据集含8967帧，达成80.99%mIoU与98.82%分类准确，参数与算力开销低，证实以气体排放监测健康的可行性。


<details>
  <summary>Details</summary>
Motivation: 瘤胃酸中毒常见且代价高，但现有诊断依赖侵入式pH测量，不便连续、可扩展监测。红外光学气体成像能无创观察代谢气体，但缺乏针对健康状态识别的算法与标注数据。

Method: 构建FUME：轻量双流网络，两个共享权重编码器分别处理CO2与CH4；在各模态内采用自注意力增强判别特征，通过通道注意力进行跨模态融合；联合优化气体羽流分割与健康状态（健康/过渡/酸中毒）分类。发布首个双气体OGI数据集（8967帧、六个pH水平、像素级分割掩膜）。进行消融以分析模态贡献与任务耦合。

Result: 在数据集上，分割mIoU达80.99%，分类准确率98.82%；模型仅1.28M参数、1.97G MACs，分割质量优于SOTA且计算成本低约10倍。消融显示CO2为主要判别信号，联合学习两任务显著提升性能。

Conclusion: 双气体红外排放影像结合轻量多任务网络可有效、低成本识别瘤胃酸中毒，在体外场景验证了基于气体排放的牲畜健康监测可行性，为实用化无创监测系统奠定基础。

Abstract: Ruminal acidosis is a prevalent metabolic disorder in dairy cattle causing significant economic losses and animal welfare concerns. Current diagnostic methods rely on invasive pH measurement, limiting scalability for continuous monitoring. We present FUME (Fused Unified Multi-gas Emission Network), the first deep learning approach for rumen acidosis detection from dual-gas optical imaging under in vitro conditions. Our method leverages complementary carbon dioxide (CO2) and methane (CH4) emission patterns captured by infrared cameras to classify rumen health into Healthy, Transitional, and Acidotic states. FUME employs a lightweight dual-stream architecture with weight-shared encoders, modality-specific self-attention, and channel attention fusion, jointly optimizing gas plume segmentation and classification of dairy cattle health. We introduce the first dual-gas OGI dataset comprising 8,967 annotated frames across six pH levels with pixel-level segmentation masks. Experiments demonstrate that FUME achieves 80.99% mIoU and 98.82% classification accuracy while using only 1.28M parameters and 1.97G MACs--outperforming state-of-the-art methods in segmentation quality with 10x lower computational cost. Ablation studies reveal that CO2 provides the primary discriminative signal and dual-task learning is essential for optimal performance. Our work establishes the feasibility of gas emission-based livestock health monitoring, paving the way for practical, in vitro acidosis detection systems. Codes are available at https://github.com/taminulislam/fume.

</details>


### [38] [Knowledge-based learning in Text-RAG and Image-RAG](https://arxiv.org/abs/2601.08226)
*Alexander Shim,Khalil Saieh,Samuel Clarke*

Main category: cs.CV

TL;DR: 研究将EVA‑ViT图像编码器与LLaMA/ChatGPT类LLM结合，比较基线、文本RAG与图像RAG在胸片疾病检测与幻觉控制上的效果。结果：文本RAG显著抑制幻觉；图像RAG用KNN提升置信度与校准；GPT系LLM优于LLaMA，ECE更低。


<details>
  <summary>Details</summary>
Motivation: 医疗影像诊断中，多模态大模型易产生幻觉与校准不足；需要评估不同RAG策略与不同LLM在胸片任务中的可靠性与性能差异。

Method: 使用NIH Chest X-ray数据，采用EVA‑ViT作为视觉编码器，结合LLM（GPT与LLaMA）。设置三种方案：基线（无RAG）、文本RAG（接入外部知识库）、图像RAG（以KNN检索相似影像与信息），比较幻觉率、预测置信度与Expected Calibration Error等指标。

Result: 文本RAG通过外部知识有效降低幻觉；图像RAG借助KNN提升模型置信度与校准；GPT类LLM整体表现优于LLaMA，幻觉率更低、ECE更好。

Conclusion: 在胸片疾病检测中，引入RAG能缓解幻觉并改善校准：文本RAG更利于降低幻觉，图像RAG提升置信与校准；选择GPT类LLM优于LLaMA。仍面临数据不平衡与多阶段流水线复杂度，需更大规模与更均衡的训练与评测环境。

Abstract: This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.

</details>


### [39] [Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence](https://arxiv.org/abs/2601.08241)
*Michele Fiori,Gabriele Civitarese,Marco Colussi,Claudio Bettini*

Main category: cs.CV

TL;DR: 提出用事件分割+置信度估计改进零样本ADL识别，优于时间分割及部分监督方法，并可有效区分正确/错误预测。


<details>
  <summary>Details</summary>
Motivation: 零样本LLM可免标注，但现有方法用时间窗分割，与LLM的语境推理不匹配，且缺乏预测置信度评估，影响可靠性与部署。

Method: 将传感器数据按事件而非固定时间窗进行分割，形成语义更一致的输入片段；设计一种新颖的置信度估计方法用于区分正确与错误预测；在复杂真实数据集上以中小型LLM（如Gemma 3 27B）进行零样本推理并比较。

Result: 事件分割在多个复杂、真实数据集上稳定优于基于时间分割的LLM方法，并且在一些场景下超过传统监督学习的数据驱动方法；提出的置信度指标能有效鉴别正确与错误预测。

Conclusion: 事件级分割能更好地发挥LLM的上下文推理能力，结合新置信度估计可提升零样本ADL识别性能与可靠性，具备实际应用潜力。

Abstract: Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.

</details>


### [40] [AIMC-Spec: A Benchmark Dataset for Automatic Intrapulse Modulation Classification under Variable Noise Conditions](https://arxiv.org/abs/2601.08265)
*Sebastian L. Cocks,Salvador Dreo,Feras Dayoub*

Main category: cs.CV

TL;DR: 提出AIMC-Spec合成数据集（33类调制、13个SNR），并用统一输入基准评测5种深度模型；FM类在低SNR更稳健，提供可复现实验基线以推进AIMC标准化。


<details>
  <summary>Details</summary>
Motivation: AIMC缺少标准化公开数据集与统一评测流程，导致算法比较不一致、进展缓慢，尤其在噪声/退化条件下难以量化鲁棒性。

Method: 构建面向谱图图像分类的合成数据集AIMC-Spec：覆盖33种调制、13档SNR；统一预处理与输入格式；重实现并对比5类代表性深度学习模型（轻量CNN、去噪架构、Transformer等）；在全类别与仅FM子集上系统评测。

Result: 不同模型与调制类型性能差异显著；频率调制(FM)类在低SNR条件下识别率更高，位相或混合调制更易受噪声影响；FM-only实验表明调制特性与网络结构共同影响鲁棒性。

Conclusion: AIMC-Spec为AIMC提供可复现、可比的基线与标准化评测平台，促进后续算法研究与领域标准化，尤其在低SNR鲁棒性方面的探索。

Abstract: A lack of standardized datasets has long hindered progress in automatic intrapulse modulation classification (AIMC) - a critical task in radar signal analysis for electronic support systems, particularly under noisy or degraded conditions. AIMC seeks to identify the modulation type embedded within a single radar pulse from its complex in-phase and quadrature (I/Q) representation, enabling automated interpretation of intrapulse structure. This paper introduces AIMC-Spec, a comprehensive synthetic dataset for spectrogram-based image classification, encompassing 33 modulation types across 13 signal-to-noise ratio (SNR) levels. To benchmark AIMC-Spec, five representative deep learning algorithms - ranging from lightweight CNNs and denoising architectures to transformer-based networks - were re-implemented and evaluated under a unified input format. The results reveal significant performance variation, with frequency-modulated (FM) signals classified more reliably than phase or hybrid types, particularly at low SNRs. A focused FM-only test further highlights how modulation type and network architecture influence classifier robustness. AIMC-Spec establishes a reproducible baseline and provides a foundation for future research and standardization in the AIMC domain.

</details>


### [41] [HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding](https://arxiv.org/abs/2601.08273)
*Qitan Lv,Tianyu Liu,Wen Wu,Xuenan Xu,Bowen Zhou,Feng Wu,Chao Zhang*

Main category: cs.CV

TL;DR: HIPPO提出一种面向视频-LLM的整体感知并行推测解码框架，通过语义保留的高比例剪枝与并行化草稿-验证流程，实现最高3.51倍加速且保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频-LLM的推测解码主要靠剪枝视觉token以减负，但仍难达到文本LLM的加速：一是剪枝不够语义感知，关键视觉语义遭破坏，导致草稿质量和接受率下降；二是即使激进剪枝后，草稿模型剩余计算仍成为瓶颈，限制整体加速。

Method: 提出HIPPO框架：1) 语义感知token保留，融合全局注意力分数与局部视觉语义，支持高剪枝率下保持关键信息；2) 视频并行推测解码，将草稿生成与目标模型验证解耦并重叠执行，实现阶段并行与流水化。

Result: 在4个视频-LLM与6个基准上，HIPPO较标准自回归推理实现最高3.51×加速，同时维持输出质量；相较现有视频SD方法在高剪枝率下有更高草稿接受率与更稳定性能。

Conclusion: 通过语义保留剪枝与阶段并行化，HIPPO克服了视频-LLM推测解码中语义流失与草稿阶段瓶颈两大限制，提供可泛化的整体并行SD方案，显著提升推理速度而不牺牲质量。

Abstract: Speculative decoding (SD) has emerged as a promising approach to accelerate LLM inference without sacrificing output quality. Existing SD methods tailored for video-LLMs primarily focus on pruning redundant visual tokens to mitigate the computational burden of massive visual inputs. However, existing methods do not achieve inference acceleration comparable to text-only LLMs. We observe from extensive experiments that this phenomenon mainly stems from two limitations: (i) their pruning strategies inadequately preserve visual semantic tokens, degrading draft quality and acceptance rates; (ii) even with aggressive pruning (e.g., 90% visual tokens removed), the draft model's remaining inference cost limits overall speedup. To address these limitations, we propose HIPPO, a general holistic-aware parallel speculative decoding framework. Specifically, HIPPO proposes (i) a semantic-aware token preservation method, which fuses global attention scores with local visual semantics to retain semantic information at high pruning ratios; (ii) a video parallel SD algorithm that decouples and overlaps draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate HIPPO's effectiveness, yielding up to 3.51x speedup compared to vanilla auto-regressive decoding.

</details>


### [42] [One-Shot Identification with Different Neural Network Approaches](https://arxiv.org/abs/2601.08278)
*Janis Mohr,Jörg Frochte*

Main category: cs.CV

TL;DR: 论文提出在小样本（一次性学习）场景中，利用“堆叠图像”输入与孪生胶囊网络（Siamese Capsule Networks）来做类别识别，在工业与人脸识别数据集上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: CNN 在计算机视觉中表现突出，但在数据稀缺的场景（如一次性学习）下训练良好特征既困难又昂贵，亟需能在极少样本条件下仍具泛化能力的模型。

Method: 将待比较的两张图像进行特殊“堆叠”处理作为输入，构建孪生结构的胶囊网络以学习相似度度量或身份判别；与多种基线方法进行对比评测，覆盖工业应用与人脸识别基准。

Result: 所提基于胶囊架构的孪生网络在多个数据集上取得强劲表现，整体超过其他对比技术，训练与调参过程相对容易。

Conclusion: 胶囊网络结合孪生结构与堆叠图像输入可有效应对一次性学习识别任务，在不同领域具有良好通用性与实用性。

Abstract: Convolutional neural networks (CNNs) have been widely used in the computer vision community, significantly improving the state-of-the-art. But learning good features often is computationally expensive in machine learning settings and is especially difficult when there is a lack of data. One-shot learning is one such area where only limited data is available. In one-shot learning, predictions have to be made after seeing only one example from one class, which requires special techniques. In this paper we explore different approaches to one-shot identification tasks in different domains including an industrial application and face recognition. We use a special technique with stacked images and use siamese capsule networks. It is encouraging to see that the approach using capsule architecture achieves strong results and exceeds other techniques on a wide range of datasets from industrial application to face recognition benchmarks while being easy to use and optimise.

</details>


### [43] [KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?](https://arxiv.org/abs/2601.08292)
*Xianfeng Wang,Kaiwei Zhang,Qi Jia,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: KidVis基准检验MLLM是否具有人类直觉般的基础视觉原语。结果：儿童≈95.3分，最强模型GPT-5仅≈67.3，且参数变大不带来线性收益，揭示“尺度定律悖论”。结论：当前MLLM缺乏通用视觉智能所需的生理级感知基元。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在高层次多模态推理上表现亮眼，但未知其是否具备与人类早期视觉发展一致的基础视觉能力。为量化与诊断这一差距，需要一个低语义依赖、贴近儿童生理视觉发展的评测基准。

Method: 基于人类视觉发展理论构建KidVis，将基础视觉智能分解为6类原子能力（专注、追踪、辨别、记忆、空间、闭合），涵盖10类低语义视觉任务；对20个SOTA MLLM与6-7岁儿童生理基线进行对比评测，并分析模型规模与能力的关系。

Result: 儿童平均得分95.32；SOTA模型（GPT-5）仅67.33，显著落后；发现“Scaling Law Paradox”：增加参数未带来在这些基础视觉能力上的线性提升。

Conclusion: 当前MLLM虽然擅长高层推理，但在基础生理感知原语上明显不足，难以支撑通用视觉智能；仅靠扩模型规模难以弥补，需要面向基础视觉的针对性训练或架构改进。

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a "Scaling Law Paradox": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.

</details>


### [44] [M3SR: Multi-Scale Multi-Perceptual Mamba for Efficient Spectral Reconstruction](https://arxiv.org/abs/2601.08293)
*Yuze Zhang,Lingjie Li,Qiuzhen Lin,Zhong Ming,Fei Yu,Victor C. M. Leung*

Main category: cs.CV

TL;DR: 提出M3SR：在Mamba基础上引入多感知融合与U-Net多尺度框架，用更低算力实现更优高光谱重建。


<details>
  <summary>Details</summary>
Motivation: 现有将Mamba用于光谱重建时存在两点不足：仅单一空间感知，难以全面理解高光谱图像；仅单尺度特征，难以刻画复杂结构与细节。

Method: 构建多尺度、多感知的Mamba架构M3SR。核心为多感知融合块（整合全局/中程/局部感知），并将其嵌入U-Net，编码-解码间实现多尺度特征提取与融合，从而在不同尺度上重建光谱信息。

Result: 在定量与定性实验中，M3SR优于当前SOTA方法，且计算成本更低。

Conclusion: 多感知融合+U-Net多尺度的Mamba变体能更全面建模高光谱图像的跨尺度与跨感知信息，实现更准确且高效的光谱重建。

Abstract: The Mamba architecture has been widely applied to various low-level vision tasks due to its exceptional adaptability and strong performance. Although the Mamba architecture has been adopted for spectral reconstruction, it still faces the following two challenges: (1) Single spatial perception limits the ability to fully understand and analyze hyperspectral images; (2) Single-scale feature extraction struggles to capture the complex structures and fine details present in hyperspectral images. To address these issues, we propose a multi-scale, multi-perceptual Mamba architecture for the spectral reconstruction task, called M3SR. Specifically, we design a multi-perceptual fusion block to enhance the ability of the model to comprehensively understand and analyze the input features. By integrating the multi-perceptual fusion block into a U-Net structure, M3SR can effectively extract and fuse global, intermediate, and local features, thereby enabling accurate reconstruction of hyperspectral images at multiple scales. Extensive quantitative and qualitative experiments demonstrate that the proposed M3SR outperforms existing state-of-the-art methods while incurring a lower computational cost.

</details>


### [45] [ReCo-KD: Region- and Context-Aware Knowledge Distillation for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2601.08301)
*Qizhen Lan,Yu-Chun Hsu,Nida Saddaf Khan,Xiaoqian Jiang*

Main category: cs.CV

TL;DR: 提出ReCo-KD，一个仅训练期的知识蒸馏框架，让小模型在3D医学分割中接近大模型精度，同时显著降低参数量与推理延迟。


<details>
  <summary>Details</summary>
Motivation: 临床部署受限于算力与延时，现有SOTA 3D分割模型体量过大；轻量化通常带来明显性能下降。需要在不设计复杂学生网络的前提下，将教师模型的细粒度解剖信息与长程上下文有效迁移给小模型。

Method: 提出Region- and Context-aware Knowledge Distillation（ReCo-KD），含两大组件：1）多尺度结构感知区域蒸馏（MS-SARD）：基于类别感知掩码与尺度归一化权重，强调小且临床关键的区域，实现细粒度结构监督；2）多尺度上下文对齐（MS-CA）：对齐师生在多层特征上的亲和/关系模式，迁移长程上下文。框架以nnU-Net为载体，骨干无关，无需定制学生结构，易于迁移到其它架构。

Result: 在多个公共3D医学分割数据集及一个更具挑战的聚合数据集上，蒸馏后的小模型在精度上接近教师，同时显著减少参数规模与推理时延。

Conclusion: ReCo-KD能在不改学生结构的前提下有效蒸馏区域细节与全局上下文，使轻量模型兼顾准确性与效率，适合资源受限的临床部署。

Abstract: Accurate 3D medical image segmentation is vital for diagnosis and treatment planning, but state-of-the-art models are often too large for clinics with limited computing resources. Lightweight architectures typically suffer significant performance loss. To address these deployment and speed constraints, we propose Region- and Context-aware Knowledge Distillation (ReCo-KD), a training-only framework that transfers both fine-grained anatomical detail and long-range contextual information from a high-capacity teacher to a compact student network. The framework integrates Multi-Scale Structure-Aware Region Distillation (MS-SARD), which applies class-aware masks and scale-normalized weighting to emphasize small but clinically important regions, and Multi-Scale Context Alignment (MS-CA), which aligns teacher-student affinity patterns across feature levels. Implemented on nnU-Net in a backbone-agnostic manner, ReCo-KD requires no custom student design and is easily adapted to other architectures. Experiments on multiple public 3D medical segmentation datasets and a challenging aggregated dataset show that the distilled lightweight model attains accuracy close to the teacher while markedly reducing parameters and inference latency, underscoring its practicality for clinical deployment.

</details>


### [46] [SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices](https://arxiv.org/abs/2601.08303)
*Dongting Hu,Aarush Gupta,Magzhan Gabidolla,Arpit Sahni,Huseyin Coskun,Yanyu Li,Yerlan Idelbayev,Ahsan Mahmood,Aleksei Lebedev,Dishani Lahiri,Anujraaj Goyal,Ju Hu,Mingming Gong,Sergey Tulyakov,Anil Kag*

Main category: cs.CV

TL;DR: 提出一套面向移动/边缘设备的高效Diffusion Transformer框架，在严格资源约束下仍保持接近Transformer级生成质量。核心包括：自适应全局-局部稀疏注意力的紧凑DiT、弹性联合训练的超网络以支持多容量子模型动态推理、以及结合分布匹配与教师知识的分步蒸馏，实现少步（如4步）高保真低时延生成。


<details>
  <summary>Details</summary>
Motivation: 现有DiT在图像生成质量上领先，但算力与内存开销大，难以上端侧。需要在不显著牺牲质量的前提下，将DiT高效化并适配多样硬件、支持低延迟少步推理。

Method: 1) 结构：设计紧凑DiT并引入自适应全局-局部稀疏注意力，兼顾全局语义与局部细节；2) 训练：构建超网络，联合优化不同容量的子DiT（弹性训练），使单模型可按硬件动态切换推理配置；3) 蒸馏：提出“知识引导的分布匹配蒸馏”，将DMD目标与少步教师模型的知识迁移结合，形成分步蒸馏流程，面向4步等实时生成。

Result: 在移动/边缘约束下实现可扩展、高质量且低延迟的扩散生成（如4步推理），显著降低计算与内存，同时保持Transformer级别的生成表现。

Conclusion: 通过架构、训练与蒸馏三方面协同优化，实现可在多样硬件上部署的高效DiT，兼顾可扩展性、效率与生成质量，适合端侧实时应用。

Abstract: Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.

</details>


### [47] [Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.08311)
*Kang Fu,Huiyu Duan,Zicheng Zhang,Yucheng Zhu,Jun Zhao,Xiongkuo Min,Jia Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: IQARAG是一种无需训练的RAG式框架，把与输入图像语义相似但质量不同且带MOS的参考图检索出来，与输入图像一起喂给LMM，通过提示构造提升其图像质量评价零样本能力，在多数据集上显著提升表现，成本低于微调。


<details>
  <summary>Details</summary>
Motivation: LMM在IQA上有零样本潜力，但要达SOTA通常依赖昂贵的微调以对齐质量相关token分布。近期有训练自由法启发，作者希望在不训练的前提下增强LMM的质量感知能力，降低算力与数据成本，同时保持泛化。

Method: 提出IQARAG：1) 检索特征提取——为图像构建用于检索的表征；2) 图像检索——从带MOS的库中找出语义相似但质量有差异的参考图；3) 集成与打分——把输入图与检索到的参考图及其MOS组合成特定提示，作为视觉锚点喂给LMM，由LMM输出质量分数。整个流程为训练自由，依赖RAG思想。

Result: 在KADID、KonIQ、LIVE Challenge、SPAQ等多样IQA数据集上，利用IQARAG能够稳定提升多种LMM在IQA任务上的性能，接近或优于需要微调的方法，同时计算与资源开销更低。

Conclusion: 通过引入带MOS的相似参考图作为视觉锚并采用RAG式提示集成，IQARAG在无需训练的条件下显著增强LMM的IQA能力，提供比微调更高效的替代方案，具备跨数据集的有效性。

Abstract: Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.

</details>


### [48] [YOLOBirDrone: Dataset for Bird vs Drone Detection and Classification and a YOLO based enhanced learning architecture](https://arxiv.org/abs/2601.08319)
*Dapinder Kaur,Neeraj Battish,Arnav Bhavsar,Shashi Poddar*

Main category: cs.CV

TL;DR: 提出YOLOBirDrone，一种改进YOLO的视觉检测架构，结合AELAN、MPDA与RMPDA以保形并融合多尺度空间/通道信息，同时发布包含小目标的BirDrone数据集；在多场景下区分鸟与无人机的检测准确率约85%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人机在商用与国防中广泛应用但也被用于定点攻击，带来安全隐患；视觉检测易将小型鸟类与无人机混淆、准确率受限，需更鲁棒的小目标识别方法与数据集支撑。

Method: 设计YOLOBirDrone：在YOLO骨干/颈部引入自适应扩展层聚合(AELAN)以更好地特征融合；加入多尺度渐进双重注意力(MPDA)与其反向结构(RMPDA)，同时在空间与通道上捕获局部与全局信息并保留目标形状；同时构建包含小且具有挑战样本的大规模BirDrone数据集进行训练与评测。

Result: 与多种SOTA算法相比，YOLOBirDrone在多个场景上实现更高的检测指标，鸟/无人机区分效果显著提升，总体检测准确率约85%，对小目标和复杂场景更稳健。

Conclusion: 特殊的层聚合与双重注意力模块能有效缓解鸟/无人机易混问题，结合新数据集提高小目标检测鲁棒性；该方案在实际多场景中优于现有方法，但仍有进一步提升空间（如更高精度与实时性均衡）。

Abstract: The use of aerial drones for commercial and defense applications has benefited in many ways and is therefore utilized in several different application domains. However, they are also increasingly used for targeted attacks, posing a significant safety challenge and necessitating the development of drone detection systems. Vision-based drone detection systems currently have an accuracy limitation and struggle to distinguish between drones and birds, particularly when the birds are small in size. This research work proposes a novel YOLOBirDrone architecture that improves the detection and classification accuracy of birds and drones. YOLOBirDrone has different components, including an adaptive and extended layer aggregation (AELAN), a multi-scale progressive dual attention module (MPDA), and a reverse MPDA (RMPDA) to preserve shape information and enrich features with local and global spatial and channel information. A large-scale dataset, BirDrone, is also introduced in this article, which includes small and challenging objects for robust aerial object identification. Experimental results demonstrate an improvement in performance metrics through the proposed YOLOBirDrone architecture compared to other state-of-the-art algorithms, with detection accuracy reaching approximately 85% across various scenarios.

</details>


### [49] [UM-Text: A Unified Multimodal Model for Image Understanding](https://arxiv.org/abs/2601.08321)
*Lichen Ma,Xiaolong Fu,Gaojing Zhou,Zipeng Guo,Ting Zhu,Yichun Liu,Yu Shi,Jason Li,Junshi Huang*

Main category: cs.CV

TL;DR: 提出UM-Text：统一多模态模型，通过指令与参考图理解，实现风格一致的视觉文字编辑，含VLM解析上下文、UM-Encoder自适应融合条件、区域一致性损失与三阶段训练，并发布20万规模数据集UM-DATA-200K，在多基准上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多需显式指定文字内容与属性（字号、颜色、排版），忽视与参考图风格一致性，且步骤繁琐、对上下文理解不足，难以生成与场景协调的视觉文字。

Method: 1) 采用VLM联合理解自然语言指令与参考图像，自动设计文本内容与布局；2) 提出UM-Encoder自适应融合多种条件嵌入（由VLM依据指令自动配置）；3) 训练期引入区域一致性损失，在潜空间与RGB空间共同监督字形生成；4) 三阶段训练策略以稳健优化；5) 构建UM-DATA-200K大规模多场景视觉文字数据集。

Result: 在多个公开基准上取得定性与定量的SOTA表现，生成的文本在内容、布局与风格上更准确、和谐。

Conclusion: 统一多模态的上下文理解与自适应条件融合能显著提升视觉文字编辑的风格一致性与质量；区域一致性损失与分阶段训练进一步增强性能，并由大规模数据集支撑。

Abstract: With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.

</details>


### [50] [IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks](https://arxiv.org/abs/2601.08332)
*Ahmed A. Hashim,Ali Al-Shuwaili,Asraa Saeed,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: IGAN 通过引入更深的Inception风格卷积与空洞卷积，并在生成器与判别器中使用dropout与谱归一化，缓解梯度不稳定与模式崩溃，在CUB-200与ImageNet上以更低FID与更高IS实现更稳定高保真图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统GAN在追求高质量图像时常遭遇训练不稳定、模式崩溃与深层网络的梯度消失/爆炸。虽然DCGAN、BigGAN、StyleGAN等提升了视觉质量，但在更深结构和大规模数据上仍易不稳。作者希望在保持或提升生成质量的同时，系统性提高训练稳定性与多样性。

Method: 提出Inception Generative Adversarial Network (IGAN)：在架构中融入更深的Inception式多分支卷积与空洞卷积以扩大感受野与特征复用；在生成器与判别器同时施加dropout与谱归一化以缓解过拟合、控制Lipschitz常数并抑制梯度爆炸；以此降低模式崩溃并维持稳定训练。

Result: 在CUB-200与ImageNet上分别取得FID 13.12与15.08，相比SOTA提升约28–33%；IS分别为9.27与68.25，显示更高多样性与质量。

Conclusion: IGAN在不显著增加计算开销的前提下，兼顾训练稳定性与高保真图像生成，作为可扩展且计算高效的框架，能有效缓解模式崩溃与梯度问题，在标准数据集上取得显著指标优势。

Abstract: Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.

</details>


### [51] [Tissue Classification and Whole-Slide Images Analysis via Modeling of the Tumor Microenvironment and Biological Pathways](https://arxiv.org/abs/2601.08336)
*Junzhuo Liu,Xuemei Du,Daniel Reisenbuchler,Ye Chen,Markus Eckstein,Christian Matek,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 提出BioMorphNet，多模态融合WSI补丁级形态学与空间转录组，提升组织分类并支持差异基因分析。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦单基因或整张切片分类，较少处理空间转录组与补丁级任务，难以刻画肿瘤微环境与形态-分子跨模态关系。

Method: 1) 形态建图：以目标补丁为节点，构建与邻域的图结构；基于形态与分子相似度自适应调节信息传递强度，刻画肿瘤微环境。2) 通路桥接：从空间转录组提取临床通路特征（预定义通路库），作为形态与基因间的语义桥梁。3) 可学习通路模块：模拟生物通路形成的可学习结构，补充预定义通路表示。4) 多模态融合用于组织分类，并据预测置信度在类间执行差异基因分析。

Result: 在前列腺癌、结直肠癌、乳腺癌数据集上，平均分类指标较最新多模态基线分别提升2.67%、5.48%、6.29%；能定位肿瘤相关组织并产出差异基因候选。

Conclusion: BioMorphNet通过图结构与通路级多模态对齐有效融合形态与空间基因信息，提升补丁级组织分类并支持潜在生物标志物发现。

Abstract: Automatic integration of whole slide images (WSIs) and gene expression profiles has demonstrated substantial potential in precision clinical diagnosis and cancer progression studies. However, most existing studies focus on individual gene sequences and slide level classification tasks, with limited attention to spatial transcriptomics and patch level applications. To address this limitation, we propose a multimodal network, BioMorphNet, which automatically integrates tissue morphological features and spatial gene expression to support tissue classification and differential gene analysis. For considering morphological features, BioMorphNet constructs a graph to model the relationships between target patches and their neighbors, and adjusts the response strength based on morphological and molecular level similarity, to better characterize the tumor microenvironment. In terms of multimodal interactions, BioMorphNet derives clinical pathway features from spatial transcriptomic data based on a predefined pathway database, serving as a bridge between tissue morphology and gene expression. In addition, a novel learnable pathway module is designed to automatically simulate the biological pathway formation process, providing a complementary representation to existing clinical pathways. Compared with the latest morphology gene multimodal methods, BioMorphNet's average classification metrics improve by 2.67%, 5.48%, and 6.29% for prostate cancer, colorectal cancer, and breast cancer datasets, respectively. BioMorphNet not only classifies tissue categories within WSIs accurately to support tumor localization, but also analyzes differential gene expression between tissue categories based on prediction confidence, contributing to the discovery of potential tumor biomarkers.

</details>


### [52] [From Local Windows to Adaptive Candidates via Individualized Exploratory: Rethinking Attention for Image Super-Resolution](https://arxiv.org/abs/2601.08341)
*Chunyu Meng,Wei Long,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出个体化探索Transformer（IET），用个体化探索注意力（IEA）让每个token自适应选择注意力候选，兼顾精度与效率，实现SISR的SOTA表现且计算复杂度可比。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的单图像超分方法虽强，但注意力计算代价高。为提效，多数方法将图像划分固定分组并在组内做注意力，忽略了token相似性的非对称性与差异性，限制了灵活、精细的信息聚合。

Method: 设计Individualized Exploratory Transformer（IET），核心为Individualized Exploratory Attention（IEA）：对每个token独立、内容感知地选择其注意力候选集合，实现token级自适应与非对称注意力；在保证计算预算的同时进行更精准的信息聚合。

Result: 在标准超分基准上，IET在可比计算复杂度下取得了最先进性能（SOTA）。

Conclusion: 通过引入token自适应、非对称的IEA机制，IET在维持效率的同时提升了重建质量，优于固定分组的注意力策略，适合高效高精度SISR。

Abstract: Single Image Super-Resolution (SISR) is a fundamental computer vision task that aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) input. Transformer-based methods have achieved remarkable performance by modeling long-range dependencies in degraded images. However, their feature-intensive attention computation incurs high computational cost. To improve efficiency, most existing approaches partition images into fixed groups and restrict attention within each group. Such group-wise attention overlooks the inherent asymmetry in token similarities, thereby failing to enable flexible and token-adaptive attention computation. To address this limitation, we propose the Individualized Exploratory Transformer (IET), which introduces a novel Individualized Exploratory Attention (IEA) mechanism that allows each token to adaptively select its own content-aware and independent attention candidates. This token-adaptive and asymmetric design enables more precise information aggregation while maintaining computational efficiency. Extensive experiments on standard SR benchmarks demonstrate that IET achieves state-of-the-art performance under comparable computational complexity.

</details>


### [53] [Semantic Misalignment in Vision-Language Models under Perceptual Degradation](https://arxiv.org/abs/2601.08355)
*Guo Cheng*

Main category: cs.CV

TL;DR: 研究发现：即便上游语义分割仅遭受轻度、现实感知退化（常规像素级指标只小幅下降），下游视觉-语言模型在语义层面会出现严重失配：幻觉、遗漏关键安全实体、以及安全判断不一致。像素稳健≠语义可靠。


<details>
  <summary>Details</summary>
Motivation: VLM 在自动驾驶与具身智能中被用于理解与决策，需高度可靠。然而现有评测多停留在基准性能与像素级鲁棒性，缺乏对“现实感知退化”下语义层面稳健性的系统理解与度量，尤其在安全关键场景。

Method: 以 Cityscapes 上的语义分割作为上游感知模块，构造“感知真实”的图像腐蚀，使常规分割指标仅中度下降；将其输出馈入多种对比式与生成式 VLM，系统考察语言层面的行为变化。提出新的语言失配指标：幻觉、关键遗漏、安全误判，并分析这些指标与分割质量之间的关系。

Result: 尽管像素级分割性能仅小幅下降，VLM 的语言输出却出现明显恶化：虚构对象、漏报行人/车辆/交通信号等关键实体，以及对安全性的矛盾判断。多种 VLM 都表现出这种断裂，显示分割鲁棒性与多模态语义可靠性之间存在显著脱节。

Conclusion: 当前 VLM 驱动系统存在关键短板：像素稳健并不能保证语义与安全可靠。应建立面向安全的评估框架，显式纳入上游感知不确定性与误差传播，以提升端到端多模态系统在现实退化条件下的可靠性。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.

</details>


### [54] [Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild with an SDF Renderer](https://arxiv.org/abs/2601.08371)
*Anastasios Tsalakopoulos,Angelos Kanlis,Evangelos Chatzis,Antonis Karakottas,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: Geo-NVS-w 是一个面向野外数据的几何感知新视角合成框架，利用SDF几何表示与几何保留损失，在保持高保真与几何一致性的同时，将能耗降至同类方法的1/4–1/5。


<details>
  <summary>Details</summary>
Motivation: 现有野外NVS方法在复杂表面上缺乏几何约束，容易出现结构不一致或伪影；需要一种能显式绑定几何的渲染框架以提升几何一致性与细节保真，同时降低计算/能耗。

Method: 以SDF为底层几何表示，指导渲染并显式约束场景表面；提出Geometry-Preservation Loss以保持细粒度结构；在无结构的野外图像集合上训练与评估，强调几何一致与高保真合成。

Result: 在渲染质量上达到具有竞争力（photorealistic、细节锐利、几何一致），并较相近方法能耗降低4–5倍。

Conclusion: 将显式SDF几何与几何保留损失结合，可在野外NVS中实现高保真且几何连贯的结果，并显著提升能效。

Abstract: We introduce Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. While existing in-the-wild methods already excel at novel view synthesis, they often lack geometric grounding on complex surfaces, sometimes producing results that contain inconsistencies. Geo-NVS-w addresses this limitation by leveraging an underlying geometric representation based on a Signed Distance Function (SDF) to guide the rendering process. This is complemented by a novel Geometry-Preservation Loss which ensures that fine structural details are preserved. Our framework achieves competitive rendering performance, while demonstrating a 4-5x reduction reduction in energy consumption compared to similar methods. We demonstrate that Geo-NVS-w is a robust method for in-the-wild NVS, yielding photorealistic results with sharp, geometrically coherent details.

</details>


### [55] [Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation](https://arxiv.org/abs/2601.08375)
*Yuan Gao,Di Cao,Xiaohuan Xi,Sheng Nie,Shaobo Xia,Cheng Wang*

Main category: cs.CV

TL;DR: 提出LoGo：一种针对地理空间点云的源数据不可用无监督域适应（SFUDA）方法，通过局部原型与全局分布对齐的双一致性机制提升跨域语义分割。


<details>
  <summary>Details</summary>
Motivation: 跨区域与采集策略差异导致显著域移，现有方法常需访问源域数据，但受隐私、政策和传输限制难以满足；需要一种仅用预训练模型与目标无标注数据即可适配的SFUDA方案。

Method: LoGo包含三部分：1）局部层面：类均衡原型估计，抛弃全局阈值过滤，采用类内独立锚点挖掘，稳健获得各类（含长尾小样本）特征原型，缓解特征塌缩；2）全局层面：基于最优传输的全局分布对齐，将伪标签分配建模为全局优化，在全局分布约束下抑制头部类别的过度主导；3）双一致性伪标签筛选：仅保留局部多增强集成预测与全局OT分配一致且高置信的伪标签用于自训练。

Result: 在目标域中，能显著减轻长尾分布与类别偏置问题，提升伪标签质量与跨域分割性能（摘要未给出具体数值，但强调有效纠偏与鲁棒性提升）。

Conclusion: 通过局部原型与全局分布的双重约束与一致性筛选，LoGo在SFUDA场景下实现对点云语义分割的有效适配，减少对源数据依赖并缓解类别不平衡带来的偏置。

Abstract: Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.

</details>


### [56] [Design and Development of a Low-Cost Scalable GSM-IoT Smart Pet Feeder with a Remote Mobile Application](https://arxiv.org/abs/2601.08394)
*Md. Rakibul Hasan Nishat,S. M. Khalid Bin Zahid,Abdul Hasib,T. M. Mehrab Hasan,Mohammad Arman,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 提出一种低成本、可扩展、基于GSM的物联网智能喂宠系统，可远程短信控制与监测，实测短信指令成功率98%，定量出粮误差±2.67%，支持离网使用。


<details>
  <summary>Details</summary>
Motivation: 城市快节奏生活使宠物定时喂食难以保证，现有联网方案依赖Wi‑Fi/互联网、成本较高且在资源受限环境适用性差，亟需一种低成本、可扩展、无需互联网的远程喂宠方案。

Method: 基于Arduino控制器，集成SIM800L GSM模块实现蜂窝通信，超声波传感器实时检测料位，舵机实现定量出粮；配套用MIT App Inventor开发的手机App，通过短信下发喂食指令并接收状态；模块化、节能设计，并进行实验评估。

Result: 实验表明：短信指令成功率98%；出粮份量一致性良好，方差为±2.67%；系统可稳定自主运行，支持实时状态反馈。

Conclusion: 该GSM‑IoT智能喂宠器在成本、可扩展性与可靠性上表现出色，且完全不依赖互联网，适用于资源受限家庭，推动了低成本蜂窝自动化在宠物护理领域的应用，为智能宠物产品设定了新基准。

Abstract: Pet ownership is increasingly common in modern households, yet maintaining a consistent feeding schedule remains challenging for the owners particularly those who live in cities and have busy lifestyles. This paper presents the design, development, and validation of a low-cost, scalable GSM-IoT smart pet feeder that enables remote monitoring and control through cellular communication. The device combines with an Arduino microcontroller, a SIM800L GSM module for communication, an ultrasonic sensor for real-time food-level assessment, and a servo mechanism for accurate portion dispensing. A dedicated mobile application was developed using MIT App Inventor which allows owners to send feeding commands and receive real-time status updates. Experimental results demonstrate a 98\% SMS command success rate, consistent portion dispensing with $\pm 2.67$\% variance, and reliable autonomous operation. Its modular, energy-efficient design makes it easy to use in a wide range of households, including those with limited resources. This work pushes forward the field of accessible pet care technology by providing a practical, scalable, and completely internet-independent solution for personalized pet feeding. In doing so, it sets a new benchmark for low-cost, GSM-powered automation in smart pet products.

</details>


### [57] [An Explainable Two Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50](https://arxiv.org/abs/2601.08401)
*Ajo Babu George,Pranav S,Kunal Agarwal*

Main category: cs.CV

TL;DR: 提出一个用于全景片智辅诊断智齿冠周炎的两阶段AI系统：YOLOv8做智齿定位与Winter位姿分型，ResNet-50判别是否有冠周炎征象，并用Grad-CAM提供可解释性；检测与分类性能较高，解释性与放射科医师一致性良好。


<details>
  <summary>Details</summary>
Motivation: 全景片上冠周炎诊断受解剖结构重叠、图像质量与经验差异影响，易漏诊与误判；需要一个既能精确定位第三磨牙、判断解剖姿态，又能识别冠周炎影像学征象并提供可解释性的自动化系统，以提升临床效率与信任。

Method: 两阶段深度学习流程：1) 使用YOLOv8在全景片中检测第三磨牙并按Winter分类标注解剖位置与倾斜角度；2) 将检测到的ROI输入改进版ResNet-50进行二分类（正常/冠周炎征象）。为增强可解释性，采用Grad-CAM在影像上高亮关键判读区域，供医生核对。

Result: YOLOv8阶段：精确率92%，mAP 92.5%；ResNet-50分类：正常F1=88%，冠周炎F1=86%；Grad-CAM与放射科医师诊断关注点的一致性为84%。

Conclusion: 该可解释的两阶段AI系统在全景片上对智齿及冠周炎征象的检测与分类表现良好，具备临床辅助评估潜力，并通过可解释性输出增强临床信心。

Abstract: Objectives: To overcome challenges in diagnosing pericoronitis on panoramic radiographs, an AI-assisted assessment system integrating anatomical localization, pathological classification, and interpretability. Methods: A two-stage deep learning pipeline was implemented. The first stage used YOLOv8 to detect third molars and classify their anatomical positions and angulations based on Winter's classification. Detected regions were then fed into a second-stage classifier, a modified ResNet-50 architecture, for detecting radiographic features suggestive of pericoronitis. To enhance clinical trust, Grad-CAM was used to highlight key diagnostic regions on the radiographs. Results: The YOLOv8 component achieved 92% precision and 92.5% mean average precision. The ResNet-50 classifier yielded F1-scores of 88% for normal cases and 86% for pericoronitis. Radiologists reported 84% alignment between Grad-CAM and their diagnostic impressions, supporting the radiographic relevance of the interpretability output. Conclusion: The system shows strong potential for AI-assisted panoramic assessment, with explainable AI features that support clinical confidence.

</details>


### [58] [Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2](https://arxiv.org/abs/2601.08408)
*Yizhan Feng,Hichem Snoussi,Jing Teng,Jian Liu,Yuyang Wang,Abel Cherouat,Tian Wang*

Main category: cs.CV

TL;DR: 提出一个基于BLIP-2并深度集成YOLO-World与YOLOv8-Seg的轻量多模态任务平台，使无人机在边缘设备上实现实时视觉理解与交互，无需针对无人机数据微调。


<details>
  <summary>Details</summary>
Motivation: 无人机在复杂场景中的实时视觉理解与交互需求强，但大型视觉语言模型计算代价高，与边缘算力受限矛盾突出，亟需在保持能力的前提下降低计算成本、增强任务泛化。

Method: 1) 将BLIP-2与YOLO-World/YOLOv8-Seg深度融合，利用YOLO的精确检测与分割作为感知先验，提升BLIP-2的视觉注意与推理；2) 设计基于K-Means的内容感知关键帧采样，进行智能帧选取与时序特征拼接，赋予轻量BLIP-2视频级交互能力；3) 提出统一的多任务提示优化方案，将YOLO结构化事件日志注入为上下文并配合输出约束，指导生成准确、相关的任务输出。

Result: 实现了在不对无人机数据进行任务特定微调的前提下，扩展BLIP-2在无人机场景的多任务能力，提升了时空理解与交互效果，并在边缘设备上更高效地运行。

Conclusion: 通过YOLO先验融合、关键帧时序建模与统一提示优化，构建了适用于无人机边缘侧的轻量多模态平台，在保证准确性与上下文相关性的同时降低计算开销，满足复杂场景的实时需求。

Abstract: The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.

</details>


### [59] [SPARK: Scalable Real-Time Point Cloud Aggregation with Multi-View Self-Calibration](https://arxiv.org/abs/2601.08414)
*Chentian Sun*

Main category: cs.CV

TL;DR: SPARK 是一个可自标定、实时的多相机点云重建框架，通过联合外参估计与置信度驱动的点云融合，在动态场景中稳定输出高质量点云，并能线性扩展到大量相机，实验显示在外参精度、几何一致性、时序稳定性和实时性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多相机实时三维重建在三方面受限：多视融合易受噪声与视角差异影响；相机外参不确定与漂移导致跨视图/时间的不一致；当相机数量增多时，计算与误差累积使系统难以扩展。需要一种能边运行边自标定、并能稳健融合多视深度的框架。

Method: 提出 SPARK，包括：1) 几何感知的在线外参估计模块，融合多视先验并施加跨视图与时序一致性约束，实现稳定自标定；2) 置信度驱动的点云融合策略，在像素与点级同时建模深度可靠性与可见性，抑制噪声与视角依赖不一致。采用逐帧、不累积的融合方式，避免动态场景中的漂移与错误积累，并使复杂度随相机数线性扩展。

Result: 在真实多相机系统上，SPARK 在外参精度、几何一致性、时序稳定性与实时性能方面全面优于现有方法；在大规模相机设置下仍保持线性扩展与稳定输出。

Conclusion: 联合自标定与置信度融合的框架可在动态场景中实时产出稳定高质量点云，并具备对大量相机的可扩展性；SPARK 为大规模多相机三维重建提供了有效且实用的解决方案。

Abstract: Real-time multi-camera 3D reconstruction is crucial for 3D perception, immersive interaction, and robotics. Existing methods struggle with multi-view fusion, camera extrinsic uncertainty, and scalability for large camera setups. We propose SPARK, a self-calibrating real-time multi-camera point cloud reconstruction framework that jointly handles point cloud fusion and extrinsic uncertainty. SPARK consists of: (1) a geometry-aware online extrinsic estimation module leveraging multi-view priors and enforcing cross-view and temporal consistency for stable self-calibration, and (2) a confidence-driven point cloud fusion strategy modeling depth reliability and visibility at pixel and point levels to suppress noise and view-dependent inconsistencies. By performing frame-wise fusion without accumulation, SPARK produces stable point clouds in dynamic scenes while scaling linearly with the number of cameras. Extensive experiments on real-world multi-camera systems show that SPARK outperforms existing approaches in extrinsic accuracy, geometric consistency, temporal stability, and real-time performance, demonstrating its effectiveness and scalability for large-scale multi-camera 3D reconstruction.

</details>


### [60] [MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP](https://arxiv.org/abs/2601.08420)
*Aditya Chaudhary,Sneha Barman,Mainak Singha,Ankit Jha,Girish Mishra,Biplab Banerjee*

Main category: cs.CV

TL;DR: 提出MMLGNet：用语言引导对齐遥感多模态（HSI与LiDAR）与文本语义，基于CLIP式双向对比学习，在两个基准上以简单CNN编码器优于多模态视觉-only方法。


<details>
  <summary>Details</summary>
Motivation: 多源遥感数据（光谱、空间、几何）激增，但现有方法多为视觉特征层面的融合，缺乏语义层级理解与跨模态（含语言）对齐能力，难以实现语言可解释的检索、分类与分析。

Method: 为每种遥感模态设计专属编码器（如HSI、LiDAR），提取视觉特征；构造人工设计的文本嵌入（类标签/描述），在共享潜在空间中通过双向对比学习对齐视觉与文本特征，训练范式受CLIP启发；强调即便采用简单的CNN编码器也能受益于语言监督。

Result: 在两个公开基准数据集上，MMLGNet显著超越多种现有仅视觉多模态融合方法，显示语言引导对齐带来的性能增益；代码已开源。

Conclusion: 语言监督与对比对齐能有效桥接高维遥感数据与自然语言语义，提升多模态融合与下游理解效果；即使用轻量CNN编码器也能取得强性能，表明该范式具有实用性与推广潜力。

Abstract: In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.

</details>


### [61] [Deep Learning Based Facial Retargeting Using Local Patches](https://arxiv.org/abs/2601.08429)
*Yeonsoo Choi,Inyup Lee,Sihun Cha,Seonghyeon Kim,Sunjin Jung,Junyong Noh*

Main category: cs.CV

TL;DR: 提出一种基于局部补丁的面部动画重定向方法，将真人视频表情语义稳健地迁移到风格化/夸张的3D角色。三模块：自动补丁提取、再演绎生成目标补丁、权重估计输出逐帧动画参数。实验显示可在面部结构差异大时保持表情语义。


<details>
  <summary>Details</summary>
Motivation: 现有重定向在形状相近的模型间效果好，但对与人脸结构差异显著的风格化3D角色容易失真、丢失语义与动作幅度边界；需要显式考虑目标角色结构与可动范围，保证源表情语义在目标上可解释且自然。

Method: 提出“局部补丁”范式：1) 自动补丁提取模块从源视频帧中分割/选择若干面部局部区域（如眼、眉、口等及更细粒度的肌肉相关块）；2) 再演绎模块对每个源局部补丁生成在目标角色上的对应再演绎补丁（考虑目标几何与材质/形变空间）；3) 权重估计模块融合所有目标补丁，估计目标角色在每帧的动画参数（如blendshape/骨骼/形变权重），拼接成完整动画序列。核心是局部对应与融合，避免全局形变带来的语义偏移。

Result: 在多组风格化角色（面部比例变化显著）上的大量实验表明：该方法能稳定保留源表情的语义与时间动态，并产生自然的目标形变与动作幅度，优于传统全局或形状依赖的重定向方案。

Conclusion: 局部补丁驱动的重定向在处理人脸结构差异大、风格化强的3D角色时有效，能保持表情语义与自然度；为跨风格面部动画提供了通用、稳健的流程。

Abstract: In the era of digital animation, the quest to produce lifelike facial animations for virtual characters has led to the development of various retargeting methods. While the retargeting facial motion between models of similar shapes has been very successful, challenges arise when the retargeting is performed on stylized or exaggerated 3D characters that deviate significantly from human facial structures. In this scenario, it is important to consider the target character's facial structure and possible range of motion to preserve the semantics assumed by the original facial motions after the retargeting. To achieve this, we propose a local patch-based retargeting method that transfers facial animations captured in a source performance video to a target stylized 3D character. Our method consists of three modules. The Automatic Patch Extraction Module extracts local patches from the source video frame. These patches are processed through the Reenactment Module to generate correspondingly re-enacted target local patches. The Weight Estimation Module calculates the animation parameters for the target character at every frame for the creation of a complete facial animation sequence. Extensive experiments demonstrate that our method can successfully transfer the semantic meaning of source facial expressions to stylized characters with considerable variations in facial feature proportion.

</details>


### [62] [Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis](https://arxiv.org/abs/2601.08440)
*Yi Qin,Lehan Wang,Chenxu Zhao,Alex P. W. Lee,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出Cardiac Reasoning Template与CardiacMind，通过模板化步骤与强化学习奖励提升MLLM在超声心动图多视角复杂心脏病诊断与推理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有超声基础模型难以将定量测量与临床表现建立有效联系；医学推理型MLLM需要昂贵的细粒度推理路径标注，且难以把超声先验纳入推理过程。需要一种既能低成本规范化推理、又能把视觉-临床先验融入模型的方案。

Method: 1) 设计Cardiac Reasoning Template（CRT），将复杂心脏病诊断流程拆解为循证、逐步的标准化步骤，减少逐例验证成本。2) 提出CardiacMind强化学习框架，引入三类奖励：PQtR鼓励更完整细致的推理步骤；PQlR奖励跨视角与跨模态证据整合质量；ESR将逐步描述与图像语义对齐。整体用于激励MLLM按CRT进行“类心内科医师”推理。

Result: 在15种复杂心脏病的多视角超声诊断上相对提升48%；在CardiacNet-PAH基准上提升5%。用户研究显示其推理逻辑与心内科医师一致性达93.33%。

Conclusion: 通过CRT的规范化推理框架与CardiacMind的多维奖励，模型能更好地把握超声定量—临床表现关系与多视角证据融合，显著提升诊断与可解释推理表现，具有推广潜力；代码将开源。

Abstract: Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.

</details>


### [63] [Noise-Adaptive Regularization for Robust Multi-Label Remote Sensing Image Classification](https://arxiv.org/abs/2601.08446)
*Tom Burgert,Julia Henkel,Begüm Demir*

Main category: cs.CV

TL;DR: 提出NAR：在半监督框架中区分加性/减性/混合标签噪声，通过置信度驱动的保留/停用/翻转与ELR结合，选择性削弱监督，显著提升遥感多标签分类在各类噪声下的鲁棒性，尤其对减性与混合噪声收益最大。


<details>
  <summary>Details</summary>
Motivation: 遥感多标签分类依赖廉价标注（主题产品/众包），常引入部分错误标签。以往方法未区分加性（多标了错标签）与减性（漏标真标签）噪声，直接把噪声当监督，易过拟合并对不同噪声类型缺乏自适应处理。

Method: 在半监督学习框架中提出噪声自适应正则NAR：1) 置信度驱动的标签处理——高置信保留，居中置信暂时停用（不参与损失），低置信翻转纠正；2) 将选择性削弱的监督与早期学习正则（ELR）联合，抑制网络对噪声的快速记忆并稳定训练；3) 在加性、减性、混合噪声情境下统一适用。

Result: 在多种噪声场景下均优于现有方法；对减性与混合噪声的提升最显著，表明自适应抑制与选择性纠正策略有效增强了鲁棒性。

Conclusion: 显式区分噪声类型并进行置信度驱动的保留/停用/翻转，再配合ELR，可在遥感多标签分类中稳健应对不同噪声，尤其缓解漏标与混合噪声影响。

Abstract: The development of reliable methods for multi-label classification (MLC) has become a prominent research direction in remote sensing (RS). As the scale of RS data continues to expand, annotation procedures increasingly rely on thematic products or crowdsourced procedures to reduce the cost of manual annotation. While cost-effective, these strategies often introduce multi-label noise in the form of partially incorrect annotations. In MLC, label noise arises as additive noise, subtractive noise, or a combination of both in the form of mixed noise. Previous work has largely overlooked this distinction and commonly treats noisy annotations as supervised signals, lacking mechanisms that explicitly adapt learning behavior to different noise types. To address this limitation, we propose NAR, a noise-adaptive regularization method that explicitly distinguishes between additive and subtractive noise within a semi-supervised learning framework. NAR employs a confidence-based label handling mechanism that dynamically retains label entries with high confidence, temporarily deactivates entries with moderate confidence, and corrects low confidence entries via flipping. This selective attenuation of supervision is integrated with early-learning regularization (ELR) to stabilize training and mitigate overfitting to corrupted labels. Experiments across additive, subtractive, and mixed noise scenarios demonstrate that NAR consistently improves robustness compared with existing methods. Performance improvements are most pronounced under subtractive and mixed noise, indicating that adaptive suppression and selective correction of noisy supervision provide an effective strategy for noise robust learning in RS MLC.

</details>


### [64] [Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.08448)
*Kexin Bao,Daichi Zhang,Yong Li,Dan Zeng,Shiming Ge*

Main category: cs.CV

TL;DR: 提出一种用于小样本类增量学习（FSCIL）的两阶段框架SDC，通过静态保持与动态学习协作，兼顾旧知识稳定性与新知识可塑性，取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: FSCIL在连续接收新类且样本有限时面临稳定-可塑性权衡：既要避免灾难性遗忘旧类，又要有效吸收新类。现有方法难以同时兼顾两者，需要一种结构化机制分别处理旧知识保留与新知识适应。

Method: 将FSCIL流程拆分为两阶段：1) 静态保持阶段（SRS）：在基类会话用充足数据训练初始模型，并将关键部分作为“静态记忆”冻结以保留基础旧知识；2) 动态学习阶段（DLS）：在后续增量会话中，引入额外的“动态投影器”（dynamic projector），与先前的静态记忆联合训练，利用新类少样本进行适应；静态与动态模块协同使用，静态负责稳定、动态负责塑形。

Result: 在三个公开基准和一个真实应用数据集上进行广泛实验，方法相较竞争方法实现了最先进（SOTA）性能，表现为更好的旧类保留与新类识别。

Conclusion: 通过静态-动态协作框架，将稳定性与可塑性解耦并联合优化，能在FSCIL中有效缓解遗忘并提升增量学习能力，具有通用性和实用价值。

Abstract: Few-shot class-incremental learning (FSCIL) aims to continuously recognize novel classes under limited data, which suffers from the key stability-plasticity dilemma: balancing the retention of old knowledge with the acquisition of new knowledge. To address this issue, we divide the task into two different stages and propose a framework termed Static-Dynamic Collaboration (SDC) to achieve a better trade-off between stability and plasticity. Specifically, our method divides the normal pipeline of FSCIL into Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS), which harnesses old static and incremental dynamic class information, respectively. During SRS, we train an initial model with sufficient data in the base session and preserve the key part as static memory to retain fundamental old knowledge. During DLS, we introduce an extra dynamic projector jointly trained with the previous static memory. By employing both stages, our method achieves improved retention of old knowledge while continuously adapting to new classes. Extensive experiments on three public benchmarks and a real-world application dataset demonstrate that our method achieves state-of-the-art performance against other competitors.

</details>


### [65] [Developing Predictive and Robust Radiomics Models for Chemotherapy Response in High-Grade Serous Ovarian Carcinoma](https://arxiv.org/abs/2601.08455)
*Sepideh Hatamikia,Geevarghese George,Florian Schwarzhans,Amirreza Mahbod,Marika AV Reinius,Ali Abbasian Ardakani,Mercedes Jimenez-Linan,Satish Viswanath,Mireia Crispin-Ortuzar,Lorena Escudero Sanchez,Evis Sala,James D Brenton,Ramona Woitek*

Main category: cs.CV

TL;DR: 本研究利用CT影像放射组学与机器学习，结合鲁棒性导向的特征筛选，预测HGSOC新辅助化疗（NACT）疗效；全病灶合并预测体积缩小（VolR）表现最佳（AUC 0.83），网膜病灶最利于CRS预测（AUC 0.77），盆腔病灶最利于直径缩小（DiaR）（AUC 0.76）。


<details>
  <summary>Details</summary>
Motivation: HGSOC多在晚期诊断且腹膜广泛转移，NACT常用以降期但约四成患者疗效欠佳。临床亟需在治疗前以无创方式预测疗效，以优化方案与时机。放射组学可从CT影像中挖掘隐含表型特征，但受特征稳定性和观察者差异影响，需要更鲁棒的特征选择策略。

Method: 提出一套自动随机化算法模拟阅片者间差异，筛选兼顾鲁棒性与预测力的放射组学特征；基于NACT前后CT提取特征并训练ML模型。分别以CRS、RECIST、体积缩小（VolR）、直径缩小（DiaR）为终点，按解剖部位（网膜、盆腔等）与“全病灶合并”构建模型；一队列训练，独立外部队列测试。

Result: 全病灶合并用于VolR预测的模型AUC达0.83；网膜病灶对CRS预测最佳（AUC 0.77）；盆腔病灶对DiaR最佳（AUC 0.76）。

Conclusion: 在特征选择中引入鲁棒性可提升模型可靠性，有助于HGSOC放射组学模型的临床转化。建议未来在实时临床流程与更广泛应用场景中验证与扩展该方法。

Abstract: Objectives: High-grade serous ovarian carcinoma (HGSOC) is typically diagnosed at an advanced stage with extensive peritoneal metastases, making treatment challenging. Neoadjuvant chemotherapy (NACT) is often used to reduce tumor burden before surgery, but about 40% of patients show limited response. Radiomics, combined with machine learning (ML), offers a promising non-invasive method for predicting NACT response by analyzing computed tomography (CT) imaging data. This study aimed to improve response prediction in HGSOC patients undergoing NACT by integration different feature selection methods. Materials and methods: A framework for selecting robust radiomics features was introduced by employing an automated randomisation algorithm to mimic inter-observer variability, ensuring a balance between feature robustness and prediction accuracy. Four response metrics were used: chemotherapy response score (CRS), RECIST, volume reduction (VolR), and diameter reduction (DiaR). Lesions in different anatomical sites were studied. Pre- and post-NACT CT scans were used for feature extraction and model training on one cohort, and an independent cohort was used for external testing. Results: The best prediction performance was achieved using all lesions combined for VolR prediction, with an AUC of 0.83. Omental lesions provided the best results for CRS prediction (AUC 0.77), while pelvic lesions performed best for DiaR (AUC 0.76). Conclusion: The integration of robustness into the feature selection processes ensures the development of reliable models and thus facilitates the implementation of the radiomics models in clinical applications for HGSOC patients. Future work should explore further applications of radiomics in ovarian cancer, particularly in real-time clinical settings.

</details>


### [66] [Modality-Decoupled RGB-Thermal Object Detector via Query Fusion](https://arxiv.org/abs/2601.08458)
*Chao Tian,Zikun Zhou,Chao Yang,Guoqing Zhu,Fu'an Zhong,Zhenyu He*

Main category: cs.CV

TL;DR: 提出一种模态解耦并带有查询融合的RGB-热成像目标检测框架MDQF，在极端条件下通过在两个DETR分支间传递高质量查询，兼顾模态互补与分离，无需配对数据也可训练，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RGB与热成像在光照与天气变化下具有互补性，但当某一模态严重退化时，直接融合会引入噪声，反而拖累检测；需要一种既能利用互补信息又能在劣质模态出现时进行分离与抑制的机制。

Method: 构建两个解耦的DETR样式分支分别处理RGB与TIR。每个迭代精炼阶段在分支间进行“查询融合”：先从一支选择高质量queries并做自适应变换，再馈送到另一支以引导其预测；当某模态退化时，可有效排除其影响。解耦设计允许用非配对的RGB或TIR图像分别优化各自分支，降低对配对RGB-T数据的依赖。

Result: 在多个数据集的广泛实验中，MDQF较现有RGB-T检测器取得更高的检测精度与鲁棒性，并体现更强的模态独立性（在单模态或模态受损场景下性能更稳健）。

Conclusion: MDQF通过“解耦分支+跨分支高质查询融合”在利用模态互补与规避劣质模态噪声之间取得平衡，同时支持无配对训练，实证显示其优于主流RGB-T检测器并具备更好的泛化与鲁棒性。

Abstract: The advantage of RGB-Thermal (RGB-T) detection lies in its ability to perform modality fusion and integrate cross-modality complementary information, enabling robust detection under diverse illumination and weather conditions. However, under extreme conditions where one modality exhibits poor quality and disturbs detection, modality separation is necessary to mitigate the impact of noise. To address this problem, we propose a Modality-Decoupled RGB-T detection framework with Query Fusion (MDQF) to balance modality complementation and separation. In this framework, DETR-like detectors are employed as separate branches for the RGB and TIR images, with query fusion interspersed between the two branches in each refinement stage. Herein, query fusion is performed by feeding the high-quality queries from one branch to the other one after query selection and adaptation. This design effectively excludes the degraded modality and corrects the predictions using high-quality queries. Moreover, the decoupled framework allows us to optimize each individual branch with unpaired RGB or TIR images, eliminating the need for paired RGB-T data. Extensive experiments demonstrate that our approach delivers superior performance to existing RGB-T detectors and achieves better modality independence.

</details>


### [67] [CoMa: Contextual Massing Generation with Vision-Language Models](https://arxiv.org/abs/2601.08464)
*Evgenii Maslov,Valentin Khrulkov,Anastasia Volkova,Anton Gusarov,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出CoMa-20K数据集与自动化体量生成框架，用VLM作条件生成基线，证明任务难度与可行性并存，奠定数据驱动建筑体量设计基准。


<details>
  <summary>Details</summary>
Motivation: 概念设计阶段（尤其体量推敲）高度依赖经验与手工，缺乏可支撑数据驱动方法的公开数据集，限制了自动化生成与客观评测。

Method: 1) 构建CoMa-20K：包含精细体量几何、经济与功能计划数据、场地及周边城市环境可视化；2) 将体量生成表述为条件生成任务；3) 以视觉-语言模型为核心，评估微调与大规模零样本VLM，比较其在情境敏感体量生成上的表现。

Result: 实验显示任务本质复杂，现有VLM虽能生成与情境相关的体量选项，但总体性能仍有限；数据集可作为基准用于量化评测。

Conclusion: CoMa-20K与基准实验为数据驱动建筑体量设计提供基础，VLM具潜力但仍需改进，未来研究可在模型、约束融入、多目标优化与更丰富评价指标上深入。

Abstract: The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.

</details>


### [68] [Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling](https://arxiv.org/abs/2601.08467)
*Takamichi Miyata,Sumiko Miyata,Andrew Morris*

Main category: cs.CV

TL;DR: 提出一种“主体解耦+文本正交化”的VLM增强框架，用于分心驾驶零样本检测，显著减小由驾驶员外观差异带来的偏置并提升实测表现。


<details>
  <summary>Details</summary>
Motivation: VLM虽擅长零样本分类，但在分心驾驶检测中容易将人的外观（衣着、年龄、性别等）与行为线索纠缠，导致模型按“谁在开车”而非“在做什么”做出判断，真实环境下鲁棒性不足。

Method: 1) 主体解耦：从图像中提取驾驶员外观嵌入，并在进行零样本分类前，从图像嵌入中去除其影响，凸显与分心相关的视觉证据。2) 文本嵌入正交化：将类别文本嵌入通过度量投影到Stiefel流形，使各类别向量更可分且保持原有语义接近。

Result: 在多项实验与基线对比中，该方法均取得稳定、显著的性能提升，显示更强的泛化与鲁棒性。

Conclusion: 通过对图像侧的主体外观因素解耦并对文本侧进行正交化，可缓解VLM在分心驾驶检测中的外观偏置问题，为实际道路安全应用提供更可靠的零样本检测方案。

Abstract: Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.

</details>


### [69] [Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs](https://arxiv.org/abs/2601.08470)
*Takara Taniguchi,Kuniaki Saito,Atsushi Hashimoto*

Main category: cs.CV

TL;DR: 提出HazardForge管线，用图像编辑与布局/校验生成包含动态与异常物体的危险情景，并构建MovSafeBench（7,254图像、MCQ）评测VLM；结果显示VLM在异常与需细粒度运动理解场景下显著掉分。


<details>
  <summary>Details</summary>
Motivation: VLM正用于自动驾驶与移动系统，安全决策依赖其对复杂、动态危险的理解；现有基准对多样危险，尤其具时空动态的异常情景覆盖不足；直接用图像编辑合成此类场景仍难以生成包含移动、侵入、远距等真实世界特征的规范场景。

Method: 提出HazardForge：可扩展图像编辑生成管线，结合布局决策算法确定物体位置/关系，利用验证模块保证情景合理性；据此构建MovSafeBench，多项选择题格式，含7,254张图与问答，覆盖13类物体（正常与异常）。

Result: 在MovSafeBench上评测多种VLM，发现遇到异常物体以及需要精细运动理解的情境时性能显著下降，降幅最大在细微运动相关题目。

Conclusion: HazardForge可系统生成含复杂时空危险的评测数据；MovSafeBench揭示现有VLM在异常与运动理解上的薄弱环节，提示需针对动态与异常感知能力进行模型与训练改进。

Abstract: Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.

</details>


### [70] [Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models](https://arxiv.org/abs/2601.08476)
*Hao Tang,Yu Liu,Shuanglin Yan,Fei Shen,Shengfeng He,Jing Qin*

Main category: cs.CV

TL;DR: 提出CoEvo：在零样本OOD检测中，于测试时无训练/标注地双向自适应文本与视觉代理，动态校准跨模态相似度，显著提升鲁棒性与SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 零样本OOD检测缺乏负标签，需要依赖代理信号。现有方法使用固定文本负样本：一是仅稀疏覆盖ID之外的语义空间；二是在分布漂移时文本静态而视觉漂移，导致跨模态错配与预测不稳定。因此需要一种能随样本与分布变化而自适应的双模态代理机制。

Method: 提出测试时框架CoEvo，无需训练与额外标注。核心是“代理对齐的共同进化”：维护两个随样本演化的代理缓存。1) 文本侧：由测试图像引导，从大规模语义空间动态挖掘上下文相关的文本负样本，作为动态文本代理；2) 视觉侧：迭代细化视觉代理，使其与选出的文本负样本逐步对齐，扩大局部OOD间隔；3) 最后对双模态代理贡献进行动态重加权，生成对分布漂移更稳健的校准OOD分数。

Result: 在标准基准上达SOTA。在ImageNet-1K零样本OOD检测上，相比强负标签基线，AUROC提升1.33%，FPR95降低45.98%，并在多数据集/多场景下表现稳定。

Conclusion: 通过测试时无监督的文本与视觉代理共同进化，CoEvo缓解跨模态错配，扩大OOD判别边界，并在分布漂移下提供更稳健、校准的OOD评分，取得显著性能提升。

Abstract: Reliable zero-shot detection of out-of-distribution (OOD) inputs is critical for deploying vision-language models in open-world settings. However, the lack of labeled negatives in zero-shot OOD detection necessitates proxy signals that remain effective under distribution shift. Existing negative-label methods rely on a fixed set of textual proxies, which (i) sparsely sample the semantic space beyond in-distribution (ID) classes and (ii) remain static while only visual features drift, leading to cross-modal misalignment and unstable predictions. In this paper, we propose CoEvo, a training- and annotation-free test-time framework that performs bidirectional, sample-conditioned adaptation of both textual and visual proxies. Specifically, CoEvo introduces a proxy-aligned co-evolution mechanism to maintain two evolving proxy caches, which dynamically mines contextual textual negatives guided by test images and iteratively refines visual proxies, progressively realigning cross-modal similarities and enlarging local OOD margins. Finally, we dynamically re-weight the contributions of dual-modal proxies to obtain a calibrated OOD score that is robust to distribution shift. Extensive experiments on standard benchmarks demonstrate that CoEvo achieves state-of-the-art performance, improving AUROC by 1.33% and reducing FPR95 by 45.98% on ImageNet-1K compared to strong negative-label baselines.

</details>


### [71] [An IoT-Enabled Smart Aquarium System for Real-Time Water Quality Monitoring and Automated Feeding](https://arxiv.org/abs/2601.08484)
*MD Fatin Ishraque Ayon,Sabrin Nahar,Ataur Rahman,Md. Taslim Arif,Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 该论文提出一套基于ESP32与多传感器/执行器的物联网智能水族箱，实现对水质的实时监测与自动控制，并在10升环境中验证其高精度与高可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统水族箱维护依赖人工定期检测多项水质指标，效率低、易出错，难以及时应对波动，导致水体条件不稳定，影响水生生物健康，亟需低成本、连续、可靠的自动化方案。

Method: 构建以ESP32为核心的边缘计算与云协同架构，集成pH、TDS、温度、浊度传感器与伺服投喂器、水泵等执行器；通过Blynk物联网平台实现云连接与远程可视化；在本地实现异常检测与告警冷却机制，降低通知疲劳；在10升水族箱中开展实验评估传感准确度、响应时延与执行模块可靠性。

Result: 系统实现平均传感精度96%，异常检测响应时间1.2秒；自动投喂与水循环模块在长时测试中达到97%的运行可靠性，显著减少人工干预并维持稳定水质。

Conclusion: 该低成本IoT方案可有效提升水族箱维护的可达性、可靠性与效率，适用于家用与商用场景，有潜力推广为水生生态管理的实用标准。

Abstract: Maintaining optimal water quality in aquariums is critical for aquatic health but remains challenging due to the need for continuous monitoring of multiple parameters. Traditional manual methods are inefficient, labor-intensive, and prone to human error, often leading to suboptimal aquatic conditions. This paper presents an IoT-based smart aquarium system that addresses these limitations by integrating an ESP32 microcontroller with multiple sensors (pH, TDS, temperature, turbidity) and actuators (servo feeder, water pump) for comprehensive real-time water quality monitoring and automated control. The system architecture incorporates edge processing capabilities, cloud connectivity via Blynk IoT platform, and an intelligent alert mechanism with configurable cooldown periods to prevent notification fatigue. Experimental evaluation in a 10-liter aquarium environment demonstrated the system's effectiveness, achieving 96\% average sensor accuracy and 1.2-second response time for anomaly detection. The automated feeding and water circulation modules maintained 97\% operational reliability throughout extended testing, significantly reducing manual intervention while ensuring stable aquatic conditions. This research demonstrates that cost-effective IoT solutions can revolutionize aquarium maintenance, making aquatic ecosystem management more accessible, reliable, and efficient for both residential and commercial applications.

</details>


### [72] [PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.08493)
*Kexin Baoa,Fanzhao Lin,Zichen Wang,Yong Li,Dan Zeng,Shiming Ge*

Main category: cs.CV

TL;DR: 提出一种面向小样本增量学习（FSCIL）的先验知识注入网络（PKI），通过级联投影器与冻结骨干来缓解遗忘与过拟合，并提供两种压缩变体，在三大基准上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: FSCIL中模型需在极少样本下持续学习新类，易出现对旧类的灾难性遗忘与对新类的过拟合。现有方法多冻结大部分参数并借助记忆库微调少量模块，但如何更高效地利用历史会话的先验知识、且在资源受限下保持性能，仍未解决。

Method: 构建包含骨干、投影器集成（每次会话新增一个投影器并仅微调该投影器与分类器）、分类器和额外记忆的PKI框架。通过级联投影器在各增量会话中累积并注入先验知识，固定其他组件以稳定旧知识、减少过拟合。同时提出两种变体PKIV-1与PKIV-2，减少投影器数量，在资源占用与性能间折中。

Result: 在三个流行基准数据集上，所提方法整体性能超过现有最先进方法；在保持旧类识别准确率的同时，提高对新类的学习效率。

Conclusion: 通过将先验知识显式注入到可增量扩展的投影器集成中，PKI在缓解遗忘与抑制过拟合之间取得更好平衡，并在资源可控的条件下实现更优的FSCIL表现。

Abstract: Few-shot class-incremental learning (FSCIL) aims to continually adapt a model on a limited number of new-class examples, facing two well-known challenges: catastrophic forgetting and overfitting to new classes. Existing methods tend to freeze more parts of network components and finetune others with an extra memory during incremental sessions. These methods emphasize preserving prior knowledge to ensure proficiency in recognizing old classes, thereby mitigating catastrophic forgetting. Meanwhile, constraining fewer parameters can help in overcoming overfitting with the assistance of prior knowledge. Following previous methods, we retain more prior knowledge and propose a prior knowledge-infused neural network (PKI) to facilitate FSCIL. PKI consists of a backbone, an ensemble of projectors, a classifier, and an extra memory. In each incremental session, we build a new projector and add it to the ensemble. Subsequently, we finetune the new projector and the classifier jointly with other frozen network components, ensuring the rich prior knowledge is utilized effectively. By cascading projectors, PKI integrates prior knowledge accumulated from previous sessions and learns new knowledge flexibly, which helps to recognize old classes and efficiently learn new classes. Further, to reduce the resource consumption associated with keeping many projectors, we design two variants of the prior knowledge-infused neural network (PKIV-1 and PKIV-2) to trade off a balance between resource consumption and performance by reducing the number of projectors. Extensive experiments on three popular benchmarks demonstrate that our approach outperforms state-of-the-art methods.

</details>


### [73] [EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers](https://arxiv.org/abs/2601.08499)
*Wenwen Liao,Hang Ruan*

Main category: cs.CV

TL;DR: 提出EfficientFSL：在ViT上进行仅查询端微调的少样本分类框架，以极少可训练参数达成SOTA并显著降低计算/显存成本。


<details>
  <summary>Details</summary>
Motivation: ViT在少样本分类上表现优于小模型，但完整微调大型模型耗费显存与时间，难以在低资源场景落地；需要一种既保留大模型表征能力又低开销的微调方法。

Method: 在冻结预训练ViT的前提下，仅通过轻量可训练模块操作“查询”来完成任务：1) Forward Block：生成任务特定查询，按层从中间特征中以query-only方式抽取信息；2) Combine Block：融合多层输出，提升表征深度与鲁棒性；3) Support-Query Attention Block：通过支持-查询注意力调整原型，使其与查询分布对齐，以缓解分布偏移。整体训练参数极少、显存与计算开销低。

Result: 在四个域内少样本数据集与六个跨域数据集上取得SOTA或竞争性结果，同时显著降低显存占用与训练时间。

Conclusion: EfficientFSL有效利用冻结ViT的知识，仅用极少参数即可在少样本任务中实现高精度与强泛化，适合低资源、真实应用场景。

Abstract: Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.

</details>


### [74] [Closed-Loop LLM Discovery of Non-Standard Channel Priors in Vision Models](https://arxiv.org/abs/2601.08517)
*Tolgay Atinc Uzun,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 用LLM驱动的NAS，通过条件代码生成与性能反馈循环，优化通道配置；用AST变异生成大量形状一致的网络以缓解数据稀缺，使LLM学到通道配置与性能的关系；在CIFAR-100上取得显著精度提升，体现语言驱动设计潜力。


<details>
  <summary>Details</summary>
Motivation: 传统NAS在通道/层宽配置上是高维离散组合优化，且受张量形状与算力预算约束，启发式方法难以捕捉代码级结构与性能的复杂关系。作者认为LLM具备理解与推理架构代码结构的能力，或可突破传统方法局限。

Method: 将通道配置搜索表述为一系列条件代码生成任务：LLM基于性能遥测（训练结果、资源使用等）迭代改写架构规格；为解决训练LLM的数据稀缺，使用AST级别的程序变异自动生成大量形状一致但多样的网络作为结构语料，使LLM学习通道配置与性能的潜在映射与设计模式。

Result: 在CIFAR-100上，所提LLM-NAS方案相较随机搜索与基线取得统计上显著的准确率提升；实验显示LLM能利用学到的结构先验做出更有效的通道配置决策。

Conclusion: LLM能够内化领域特定的架构先验，用语言与代码推理驱动的方式有效优化通道配置，区别于随机搜索；语言驱动的深度学习架构设计具备巨大潜力。

Abstract: Channel configuration search the optimization of layer specifications such as layer widths in deep neural networks presents a complex combinatorial challenge constrained by tensor shape compatibility and computational budgets. We posit that Large Language Models (LLMs) offer a transformative approach to Neural Architecture Search (NAS), capable of reasoning about architectural code structure in ways that traditional heuristics cannot. In this paper, we investigate the application of an LLM-driven NAS framework to the problem of channel configuration. We formulate the search as a sequence of conditional code generation tasks, where an LLM refines architectural specifications based on performance telemetry. Crucially, we address the data scarcity problem by generating a vast corpus of valid, shape-consistent architectures via Abstract Syntax Tree (AST) mutations. While these mutated networks are not necessarily high-performing, they provide the critical volume of structural data required for the LLM to learn the latent relationship between channel configurations and model performance. This allows the LLM to internalize complex design patterns and apply them to optimize feature extraction strategies. Experimental results on CIFAR-100 validate the efficacy of this approach, demonstrating that the model yields statistically significant improvements in accuracy. Our analysis confirms that the LLM successfully acquires domain-specific architectural priors, distinguishing this method from random search and highlighting the immense potential of language-driven design in deep learning.

</details>


### [75] [CD^2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.08519)
*Kexin Bao,Daichi Zhang,Hansong Zhang,Yong Li,Yutao Yue,Shiming Ge*

Main category: cs.CV

TL;DR: 提出CD^2框架，通过合成精炼数据与分布约束，解决FSCIL中遗忘问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FSCIL需要在连续增量类别、且每类样本极少的情况下学习，核心挑战是灾难性遗忘。现有方法多依赖外部记忆并对各增量类一视同仁，导致对先前关键信息的保存不足，亟需更有效的知识保留与传递机制。

Method: 提出CD^2框架，包含两模块：1) 数据集蒸馏模块（DDM）：在分类器引导下合成高度浓缩的代表性样本，使模型从少量增量样本中学习到紧凑的类相关线索；2) 蒸馏约束模块（DCM）：设计损失以约束已学类别的分布，强化对蒸馏知识的保持。整体通过合成数据+分布约束实现更稳健的增量学习。

Result: 在三个公开数据集上进行大量实验，CD^2在FSCIL场景下的性能优于多种SOTA基线，验证了其有效性与优势。

Conclusion: 通过结合数据集蒸馏与分布约束，CD^2能更充分保留旧知识并从少样本中提炼关键信息，有效缓解灾难性遗忘，适用于FSCIL的连续增量分类任务。

Abstract: Few-shot class-incremental learning (FSCIL) receives significant attention from the public to perform classification continuously with a few training samples, which suffers from the key catastrophic forgetting problem. Existing methods usually employ an external memory to store previous knowledge and treat it with incremental classes equally, which cannot properly preserve previous essential knowledge. To solve this problem and inspired by recent distillation works on knowledge transfer, we propose a framework termed \textbf{C}onstrained \textbf{D}ataset \textbf{D}istillation (\textbf{CD$^2$}) to facilitate FSCIL, which includes a dataset distillation module (\textbf{DDM}) and a distillation constraint module~(\textbf{DCM}). Specifically, the DDM synthesizes highly condensed samples guided by the classifier, forcing the model to learn compacted essential class-related clues from a few incremental samples. The DCM introduces a designed loss to constrain the previously learned class distribution, which can preserve distilled knowledge more sufficiently. Extensive experiments on three public datasets show the superiority of our method against other state-of-the-art competitors.

</details>


### [76] [VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations](https://arxiv.org/abs/2601.08557)
*Sushant Gautam,Cise Midoglu,Vajira Thambawita,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: 提出VideoHEDGE框架，用多重扰动与高温采样并结合语义聚类，给出三种置信度/可靠性分数（SE、RadFlag、VASE）来检测视频VQA中的幻觉；在SoccerChat基准三种7B模型上，VASE最稳健、AUC最高，且嵌入聚类成本低、效果与NLI相当。


<details>
  <summary>Details</summary>
Motivation: 视频VLM在视频问答中常产生高置信但错误的“幻觉”，现有不确定性度量与正确性不对齐，且多来自图像域，缺乏对时序与扰动鲁棒性的系统检测方法与可复现实验基准。

Method: 提出VideoHEDGE：对每个视频-问题，先得到基线答案；对原始与经光度和时空扰动后的片段进行高温多次生成；将所有文本输出通过NLI或嵌入相似度进行语义聚类，得到若干语义假设簇；以簇概率质量计算三类可靠性分数：Semantic Entropy (SE)、RadFlag、Vision-Amplified Semantic Entropy (VASE)。在SoccerChat上用LLM judge给二元幻觉标签进行评测。

Result: 在Qwen2-VL、Qwen2.5-VL与SoccerChat微调模型上，VASE在各失真预算下ROC-AUC最高，SE和RadFlag常接近随机；嵌入聚类与NLI聚类检测效果相当但计算更省；领域微调能降低幻觉频率，但对校准改进有限。

Conclusion: VideoHEDGE有效检测视频VQA中的幻觉，VASE是最可靠指标；嵌入聚类提供高性价比；领域微调并不能显著提升校准。提供hedge-bench库与代码以支持复现与扩展。

Abstract: Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .

</details>


### [77] [REVNET: Rotation-Equivariant Point Cloud Completion via Vector Neuron Anchor Transformer](https://arxiv.org/abs/2601.08558)
*Zhifan Ni,Eckehard Steinbach*

Main category: cs.CV

TL;DR: 提出REVNET：一种基于Vector Neuron的旋转等变点云补全框架，通过锚点表示与Transformer预测缺失锚点，结合等变偏置与ZCA归一化，实现任意姿态下更稳健的补全，在MVP上SOTA、KITTI上无需对齐仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有点云补全多在标准姿态训练，遇到任意旋转时性能显著下降；简单随机旋转增强训练成本高且不能保证稳健。需要一个在几何上对旋转具等变/不变性的框架，既保留局部细节又能鲁棒补全。

Method: 1) 以Vector Neuron为基础构建旋转等变网络REVNET；2) 将部分点云表示为等变锚点集合；3) 设计VN Missing Anchor Transformer预测缺失锚点的位置与特征；4) 提出旋转等变偏置与基于ZCA的层归一化，增强特征表达；5) 在等变/不变特征间灵活转换，稳定生成点坐标。

Result: 在MVP合成数据集的等变设置下超过现有SOTA；在KITTI真实数据集上，无需输入姿态对齐也取得与非等变网络相当或更优的结果。

Conclusion: REVNET通过等变锚点与VN扩展模块，实现对任意旋转的鲁棒点云补全，在合成与真实数据上验证有效，减少对姿态对齐和大量旋转增强的依赖，代码将开源。

Abstract: Incomplete point clouds captured by 3D sensors often result in the loss of both geometric and semantic information. Most existing point cloud completion methods are built on rotation-variant frameworks trained with data in canonical poses, limiting their applicability in real-world scenarios. While data augmentation with random rotations can partially mitigate this issue, it significantly increases the learning burden and still fails to guarantee robust performance under arbitrary poses. To address this challenge, we propose the Rotation-Equivariant Anchor Transformer (REVNET), a novel framework built upon the Vector Neuron (VN) network for robust point cloud completion under arbitrary rotations. To preserve local details, we represent partial point clouds as sets of equivariant anchors and design a VN Missing Anchor Transformer to predict the positions and features of missing anchors. Furthermore, we extend VN networks with a rotation-equivariant bias formulation and a ZCA-based layer normalization to improve feature expressiveness. Leveraging the flexible conversion between equivariant and invariant VN features, our model can generate point coordinates with greater stability. Experimental results show that our method outperforms state-of-the-art approaches on the synthetic MVP dataset in the equivariant setting. On the real-world KITTI dataset, REVNET delivers competitive results compared to non-equivariant networks, without requiring input pose alignment. The source code will be released on GitHub under URL: https://github.com/nizhf/REVNET.

</details>


### [78] [End-to-End Video Character Replacement without Structural Guidance](https://arxiv.org/abs/2601.08587)
*Zhengbo Xu,Jie Ma,Ziheng Wang,Zhan Peng,Jun Liang,Jing Li*

Main category: cs.CV

TL;DR: MoCha 提出一种无需逐帧分割与显式结构引导、仅用单帧遮罩即可进行视频角色替换的框架，并通过条件感知RoPE、强化学习后训练与三套合成/增强数据集，显著提升身份保真与时序一致性，优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖重建式范式：需要逐帧分割与骨架/深度等结构先验，导致在遮挡、人与物交互、极端姿态与复杂光照下泛化差、易出现伪影与时序不一致。缺少成对视频数据也限制了训练与评估。

Method: 1) 框架MoCha：仅需单个任意帧的遮罩作为条件进行视频角色替换。2) 条件感知RoPE以适配多模态条件并强化身份特征。3) 引入基于强化学习的后训练阶段以优化人脸身份一致性与质量。4) 数据构建流水线：a) UE5高保真渲染数据集；b) 基于人像驱动技术合成的表情驱动数据集；c) 从现有视频-掩膜对增强得到的扩展数据集。

Result: 在大量实验中，MoCha在视觉质量、身份保真与时序一致性等指标上显著优于现有SOTA方法，适应复杂场景（遮挡、交互、异常姿态、复杂光照），并展示更强泛化。

Conclusion: MoCha以最小标注需求（单帧遮罩）和专门的训练/数据策略，实现更稳健的可控视频角色替换，缓解了传统方法对结构先验与密集标注的依赖；代码将开源以促进后续研究。

Abstract: Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha

</details>


### [79] [WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation](https://arxiv.org/abs/2601.08602)
*Zishan Shu,Juntong Wu,Wei Yan,Xudong Liu,Hongyu Zhang,Chang Liu,Youdong Mao,Jie Chen*

Main category: cs.CV

TL;DR: 提出WaveFormer：以欠阻尼波动方程为基础的全局交互模块WPO，显式建模空间频率与“传播时间”，O(N log N)复杂度，作为ViT/CNN替代，在分类/检测/分割上以更高吞吐与更少FLOPs取得可比精度。


<details>
  <summary>Details</summary>
Motivation: 注意力能捕获长程依赖但缺乏对语义在空间中如何传播的可解释与可控机制；现有“热扩散”式设计偏向低频平滑，难兼顾高频细节与全局一致性，且复杂度高。

Method: 将特征图视作空间信号，随“内部传播时间”（网络深度）按欠阻尼波动方程演化；推导频率-时间解耦的闭式解，并实现为Wave Propagation Operator（WPO），用FFT等实现O(N log N)的全局交互；基于WPO构建WaveFormer，可无缝替换ViT/CNN模块。

Result: 在图像分类、目标检测、语义分割上取得与注意力模型相当或更优的精度；相较注意力最高提高1.6倍吞吐、节省约30% FLOPs；能更好同时捕获全局布局与高频边缘纹理。

Conclusion: 波传播为视觉模型提供了区别于热扩散/注意力的互补偏置，通过显式频率与传播时间建模，以更低复杂度实现全局交互并提升效率与语义质量。

Abstract: Vision modeling has advanced rapidly with Transformers, whose attention mechanisms capture visual dependencies but lack a principled account of how semantic information propagates spatially. We revisit this problem from a wave-based perspective: feature maps are treated as spatial signals whose evolution over an internal propagation time (aligned with network depth) is governed by an underdamped wave equation. In this formulation, spatial frequency-from low-frequency global layout to high-frequency edges and textures-is modeled explicitly, and its interaction with propagation time is controlled rather than implicitly fixed. We derive a closed-form, frequency-time decoupled solution and implement it as the Wave Propagation Operator (WPO), a lightweight module that models global interactions in O(N log N) time-far lower than attention. Building on WPO, we propose a family of WaveFormer models as drop-in replacements for standard ViTs and CNNs, achieving competitive accuracy across image classification, object detection, and semantic segmentation, while delivering up to 1.6x higher throughput and 30% fewer FLOPs than attention-based alternatives. Furthermore, our results demonstrate that wave propagation introduces a complementary modeling bias to heat-based methods, effectively capturing both global coherence and high-frequency details essential for rich visual semantics. Codes are available at: https://github.com/ZishanShu/WaveFormer.

</details>


### [80] [Interpretability and Individuality in Knee MRI: Patient-Specific Radiomic Fingerprint with Reconstructed Healthy Personas](https://arxiv.org/abs/2601.08604)
*Yaxi Chen,Simin Ni,Shuai Li,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出“放射组学指纹”和“健康画像”两种互补策略，使膝关节MRI自动评估兼具个体化与可解释性，并在三项临床任务中达到或优于SOTA深度学习表现。


<details>
  <summary>Details</summary>
Motivation: 临床落地需要既准确又可解释的模型。传统放射组学在群体水平预设特征，难以捕捉个体差异，性能常逊于端到端DL；而端到端DL虽准确但可解释性弱。亟需能体现个体差异、同时保持特征层面解释的方案。

Method: 1) 放射组学指纹：从候选特征池中为每位患者动态选择最相关特征。用图像条件化的预测器估计特征使用概率（潜变量模型），再用全局系数的透明逻辑回归完成分类，兼顾个体化与可解释性。2) 健康画像：训练扩散模型重建“健康膝”MRI，为每位患者合成无病基线。将病灶图像与其健康画像的特征对比，突出异常偏离，实现直观、病例级解释。3) 系统比较单用与组合策略，在三项临床任务上评估。

Result: 无论单独使用还是组合，两种方法在三项任务中均达到或超过当前SOTA DL模型的性能，同时在特征选择、异常定位和病例级解释上提供了多层次可解释性。案例研究显示可用于可解释的生物标志物发现与病灶定位。

Conclusion: 个体化的特征选择（指纹）与患者特定的健康基线（画像）互补，既保留放射组学的可解释性，又接近或优于端到端DL的准确度，促进临床可用的MRI自动评估与可解释发现。

Abstract: For automated assessment of knee MRI scans, both accuracy and interpretability are essential for clinical use and adoption. Traditional radiomics rely on predefined features chosen at the population level; while more interpretable, they are often too restrictive to capture patient-specific variability and can underperform end-to-end deep learning (DL). To address this, we propose two complementary strategies that bring individuality and interpretability: radiomic fingerprints and healthy personas. First, a radiomic fingerprint is a dynamically constructed, patient-specific feature set derived from MRI. Instead of applying a uniform population-level signature, our model predicts feature relevance from a pool of candidate features and selects only those most predictive for each patient, while maintaining feature-level interpretability. This fingerprint can be viewed as a latent-variable model of feature usage, where an image-conditioned predictor estimates usage probabilities and a transparent logistic regression with global coefficients performs classification. Second, a healthy persona synthesises a pathology-free baseline for each patient using a diffusion model trained to reconstruct healthy knee MRIs. Comparing features extracted from pathological images against their personas highlights deviations from normal anatomy, enabling intuitive, case-specific explanations of disease manifestations. We systematically compare fingerprints, personas, and their combination across three clinical tasks. Experimental results show that both approaches yield performance comparable to or surpassing state-of-the-art DL models, while supporting interpretability at multiple levels. Case studies further illustrate how these perspectives facilitate human-explainable biomarker discovery and pathology localisation.

</details>


### [81] [SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling](https://arxiv.org/abs/2601.08608)
*Xi Chen,Hongxun Yao,Sicheng Zhao,Jiankun Zhu,Jing Jiang,Kui Jiang*

Main category: cs.CV

TL;DR: 提出SfMamba用于源不可用域自适应（SFDA），通过通道序列扫描与语义一致洗牌，在线性复杂度下更好建模长程依赖并增强域不变特征，实验在多基准上优于现有方法且参数高效。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA需在不访问源数据的情况下将源预训练模型适配到无标注目标域，但在学习域不变表征时常在感受野与计算效率间权衡；VMamba虽具线性复杂度长程建模能力，但不足以刻画对齐所需的通道频率特性且在大域移位下空间鲁棒性欠佳。

Method: 提出SfMamba框架：1) 设计Channel-wise Visual State-Space（CVSS）模块，对通道维进行序列扫描，捕获通道频率与长程依赖以提取域不变特征；2) 提出Semantic-Consistent Shuffle（SCS），在2D选择性扫描中打乱背景patch序列，同时约束预测一致性，抑制错误积累并提升在大域偏移下的鲁棒性；整体保持线性复杂度与参数效率。

Result: 在多个SFDA基准上进行全面评测，SfMamba在准确率/泛化性能上持续超过现有方法，同时保持较少参数与良好计算效率。

Conclusion: SfMamba通过通道序列状态空间建模与语义一致洗牌，有效提升SFDA的域不变表示学习与鲁棒性，在保证效率的同时取得SOTA或接近SOTA的表现，具备实际应用价值。

Abstract: Source-free domain adaptation (SFDA) tackles the critical challenge of adapting source-pretrained models to unlabeled target domains without access to source data, overcoming data privacy and storage limitations in real-world applications. However, existing SFDA approaches struggle with the trade-off between perception field and computational efficiency in domain-invariant feature learning. Recently, Mamba has offered a promising solution through its selective scan mechanism, which enables long-range dependency modeling with linear complexity. However, the Visual Mamba (i.e., VMamba) remains limited in capturing channel-wise frequency characteristics critical for domain alignment and maintaining spatial robustness under significant domain shifts. To address these, we propose a framework called SfMamba to fully explore the stable dependency in source-free model transfer. SfMamba introduces Channel-wise Visual State-Space block that enables channel-sequence scanning for domain-invariant feature extraction. In addition, SfMamba involves a Semantic-Consistent Shuffle strategy that disrupts background patch sequences in 2D selective scan while preserving prediction consistency to mitigate error accumulation. Comprehensive evaluations across multiple benchmarks show that SfMamba achieves consistently stronger performance than existing methods while maintaining favorable parameter efficiency, offering a practical solution for SFDA. Our code is available at https://github.com/chenxi52/SfMamba.

</details>


### [82] [SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning](https://arxiv.org/abs/2601.08617)
*Leo Fillioux,Omprakash Chakraborty,Ismail Ben Ayed,Paul-Henry Cournède,Stergios Christodoulidis,Maria Vakalopoulou,Jose Dolz*

Main category: cs.CV

TL;DR: 论文指出在VLM测试时提示调优中，强制文本原型“完全正交”会损害语义相近类的关系，导致过度自信；提出基于Huber的语义正交校准SoC，在保持语义相近性的同时实现平滑分离，从而显著改进不确定性校准并维持竞争性的判别性能。


<details>
  <summary>Details</summary>
Motivation: VLM正被用于高风险场景，模型不仅要准，还要有可靠的不确定性估计。然而现有TPT主要追求准确率，近期做法用“全正交”约束增强可分离性以期改善校准，但其对语义相关类别的影响与校准效果缺乏系统理解。

Method: 1) 理论分析：证明全正交约束的梯度会把语义相近的类别原型彼此强烈推远，破坏语义结构并诱发过度自信。2) 提出SoC：以Huber型正则替代硬正交，按距离分段惩罚，促使原型在必要时分离、在语义相近时保留适当接近；适用于VLM测试时提示调优。3) 实验：在多数据集上评估校准（如ECE/NLL）与判别性能（准确率）。

Result: 与正交化的SOTA相比，SoC在多数据集上持续降低校准误差，同时保持或接近最优的分类准确率。

Conclusion: 硬性全正交并非校准的万灵药；保持语义结构的平滑分离更利于可信不确定性。SoC提供了简单有效的正则化方案，在不牺牲判别力的情况下稳健提升VLM的校准。

Abstract: With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.

</details>


### [83] [CtrlFuse: Mask-Prompt Guided Controllable Infrared and Visible Image Fusion](https://arxiv.org/abs/2601.08619)
*Yiming Sun,Yuan Ruan,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出CtrlFuse：可控的红外-可见光图像融合框架，利用掩码提示进行交互式动态融合，在并行分割与融合分支的协同优化下，同时提升融合质量与下游分割性能，达到SOTA，并使任务分支超越原始分割模型。


<details>
  <summary>Details</summary>
Motivation: 现有融合方法要么只做像素级融合，忽视下游任务适配；要么依赖级联检测/分割隐式学习僵化语义，无法交互式满足多样化的语义目标感知需求。需要一种可按任务语义进行可控、交互的融合方法，兼顾感知与视觉质量。

Method: 提出CtrlFuse，包括多模态特征提取器、参考提示编码器（RPE）与提示-语义融合模块（PSFM）。RPE通过对预训练分割模型进行微调，并结合输入掩码提示，动态编码任务特定的语义提示；PSFM将这些显式语义注入到融合特征中。采用并行的分割分支与融合分支协同优化，实现语义引导的可控融合。

Result: 在可控性与分割准确率方面取得SOTA；适配后的任务分支性能甚至超过原始的分割模型，显示语义引导融合对下游任务的促进。

Conclusion: 通过掩码提示驱动的可控融合与并行分支的协同优化，CtrlFuse实现了融合质量与任务性能的双提升，证明显式注入语义与交互式控制是红外-可见光融合在多任务感知中的有效范式。

Abstract: Infrared and visible image fusion generates all-weather perception-capable images by combining complementary modalities, enhancing environmental awareness for intelligent unmanned systems. Existing methods either focus on pixel-level fusion while overlooking downstream task adaptability or implicitly learn rigid semantics through cascaded detection/segmentation models, unable to interactively address diverse semantic target perception needs. We propose CtrlFuse, a controllable image fusion framework that enables interactive dynamic fusion guided by mask prompts. The model integrates a multi-modal feature extractor, a reference prompt encoder (RPE), and a prompt-semantic fusion module (PSFM). The RPE dynamically encodes task-specific semantic prompts by fine-tuning pre-trained segmentation models with input mask guidance, while the PSFM explicitly injects these semantics into fusion features. Through synergistic optimization of parallel segmentation and fusion branches, our method achieves mutual enhancement between task performance and fusion quality. Experiments demonstrate state-of-the-art results in both fusion controllability and segmentation accuracy, with the adapted task branch even outperforming the original segmentation model.

</details>


### [84] [SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models](https://arxiv.org/abs/2601.08623)
*Renyang Liu,Kangjie Chen,Han Qiu,Jie Zhang,Kwok-Yan Lam,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: 提出SafeRedir：一种推理时通过提示嵌入重定向实现稳健“遗忘”的轻量框架，无需改模型即可屏蔽不安全概念并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型容易记忆并复现NSFW与受版权风格等不良概念，带来安全与合规风险；后处理过滤鲁棒性差、粒度粗；模型级遗忘法需重训、伤及良性内容，且易被改写提示与对抗攻击绕过。

Method: 在不改动基础IGM的前提下，于推理阶段对提示词嵌入做“重定向”。核心包含：1) 潜变量感知的多模态安全分类器，在线识别不安全生成轨迹；2) 词元级Δ生成器，针对不安全语义生成嵌入偏移，并辅以词元掩码与自适应缩放预测器，以局部、可控地施加干预，将语义引导至安全区域。

Result: 在多种代表性“遗忘”任务上，SafeRedir实现有效的不安全概念清除，同时较好保留语义与感知质量，图像质量稳健，并显著提升对改写提示与对抗攻击的抵抗力。方法可泛化到多种扩散骨干与现有已做遗忘的模型，具备即插即用兼容性。

Conclusion: SafeRedir以推理时词元级嵌入重定向实现稳健且高保真的安全控制，避免重训开销与副作用，兼具通用性与鲁棒性，适合在实际部署中用于安全与合规治理。

Abstract: Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.

</details>


### [85] [Além do Desempenho: Um Estudo da Confiabilidade de Detectores de Deepfakes](https://arxiv.org/abs/2601.08674)
*Lucas Lopes,Rayson Laroca,André Grégio*

Main category: cs.CV

TL;DR: 论文提出一个评估深度伪造检测方法“可靠性”的四维框架：可迁移性、鲁棒性、可解释性、计算效率；用其分析5种SOTA方法，发现进步与不足并存。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦检测器在单一数据集上的分类精度，忽视跨域迁移、对抗与扰动下稳定性、模型可解释性以及实际部署的效率要求；而深度伪造带来诈骗、虚假信息与隐私风险，迫切需要更全面的评估标准来指导方法选择与落地。

Method: 构建四大支柱的可靠性评估框架：1) 可迁移性（跨数据集/跨生成器/跨模态泛化）；2) 鲁棒性（对压缩、噪声、后期编辑与对抗攻击的耐受）；3) 可解释性（可视化证据、特征归因、一致性）；4) 计算效率（训练/推理时延、参数量、能耗）。基于该框架，对5种当前SOTA检测方法进行系统实验与对比分析。

Result: 五种方法在标准精度上均表现优秀，但在跨域泛化与抗扰动方面差异显著；部分方法可解释性薄弱、推理开销较高；不存在“一招通吃”的方案，各方法在四项维度上呈现权衡。

Conclusion: 单一精度指标不足以反映深度伪造检测的实用可靠性；所提框架能揭示方法间的关键差异并指导部署与研究，应推动社区在迁移、鲁棒、可解释与高效方向上协同优化。

Abstract: Deepfakes are synthetic media generated by artificial intelligence, with positive applications in education and creativity, but also serious negative impacts such as fraud, misinformation, and privacy violations. Although detection techniques have advanced, comprehensive evaluation methods that go beyond classification performance remain lacking. This paper proposes a reliability assessment framework based on four pillars: transferability, robustness, interpretability, and computational efficiency. An analysis of five state-of-the-art methods revealed significant progress as well as critical limitations.

</details>


### [86] [Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation](https://arxiv.org/abs/2601.08728)
*Runfeng Qu,Ole Hall,Pia K Bideau,Julie Ouerfelli-Ethier,Martin Rolfs,Klaus Obermayer,Olaf Hellwich*

Main category: cs.CV

TL;DR: 提出Salience-SGG，通过迭代显著性解码器强调空间显著的三元组，用语义无关的显著性标签指导，在多个数据集上达SOTA并提升空间理解。


<details>
  <summary>Details</summary>
Motivation: SGG存在长尾分布与偏置，去偏方法往往牺牲空间理解、过度依赖语义先验；需要一种兼顾去偏与空间关系建模的方案。

Method: 构建Salience-SGG框架，核心为Iterative Salience Decoder（ISD）：通过迭代选择并强化空间结构显著的主体-谓词-客体三元组；引入“语义无关”的显著性标签作为监督信号，避免语义先验偏置，突出空间线索。与现有Unbiased-SGG可模块化结合。

Result: 在Visual Genome、Open Images V6、GQA-200上取得SOTA；在衡量空间理解的Pairwise Localization Average Precision指标上显著优于现有Unbiased-SGG，并能提升其空间定位表现。

Conclusion: 强调空间显著性的解码与语义无关标签可有效缓解长尾偏置同时增强空间理解，提供与现有去偏方法互补的路径。

Abstract: Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision

</details>


### [87] [ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning](https://arxiv.org/abs/2601.08732)
*Vincent Roca,Martin Bretzner,Hilde Henon,Laurent Puy,Grégory Kuchcinski,Renaud Lopes*

Main category: cs.CV

TL;DR: 提出ISLA深度学习模型，在三大多中心数据集（>1500例AIS）上针对DWI进行急性缺血性脑卒中病灶自动分割，系统优化损失、网络结构、深监督与注意力，并结合无监督域自适应以提升外部泛化，在外部测试集上优于两种SOTA方法，代码与模型将开源。


<details>
  <summary>Details</summary>
Motivation: 现有AIS病灶分割多基于U-Net，但在损失函数、深监督、残差与注意力等组件选择上缺乏一致结论，且实现多未公开，最佳配置不明确；临床外部泛化与跨域适应性不足。

Method: 构建ISLA框架：在三大多中心DWI数据上，对损失函数（如Dice/CE等组合）、卷积/残差结构、深监督、注意力模块进行系统化组合与调优；并研究无监督域自适应策略以提升对外部临床数据的泛化能力；与两种SOTA方法进行外部测试集对比评估。

Result: ISLA在外部测试集上总体分割性能优于两种SOTA基线；无监督域自适应进一步提升跨域泛化性能。

Conclusion: 经系统设计与优化的ISLA在AIS病灶分割上达到SOTA水平，具备良好跨域泛化；代码与模型即将公开，便于复用与可重复研究。

Abstract: Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.

</details>


### [88] [UR-Bench: A Benchmark for Multi-Hop Reasoning over Ultra-High-Resolution Images](https://arxiv.org/abs/2601.08748)
*Siqi Li,Xinyu Cai,Jianbiao Mei,Nianchen Deng,Pinlong Cai,Licheng Wen,Yufan Shen,Xuemeng Yang,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: UR-Bench提出用于评估多模态大模型在超高分辨率图像上的推理能力，并配套一个调用外部视觉工具的智能体框架与语义抽象/检索工具以高效处理百兆至千兆像素图像，实验显示该框架优于端到端MLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准多为中等分辨率、视觉复杂度有限，难以覆盖超高分辨率（百兆-千兆像素）场景中的细粒度定位与跨尺度推理需求，因此需要一个能系统评测MLLM在极端视觉信息条件下推理能力的基准与有效求解框架。

Method: 1) 构建UR-Bench：包含人文场景与自然场景两大类、四个子集，图像分辨率达百兆至千兆像素；为每张图像设计三级难度问题以分层评估推理。2) 提出基于智能体的推理框架：语言模型作为控制器，按需调用外部视觉工具。3) 设计语义抽象与检索工具：对超高分辨率图像进行语义分块、索引与按需检索，提升处理效率。4) 对比评测：端到端MLLMs vs. 智能体框架。

Result: 在UR-Bench上，采用智能体框架并结合语义抽象/检索工具的方案在多项指标上优于直接端到端的MLLMs，表明该策略在超高分辨率推理场景更有效。

Conclusion: UR-Bench为评测MLLM在超高分辨率场景的推理能力提供了系统基准；通过智能体式工具调用与语义抽象/检索，可高效处理超高分辨率图像并显著提升表现，建议未来研究沿工具协同与跨尺度推理方向推进。

Abstract: Recent multimodal large language models (MLLMs) show strong capabilities in visual-language reasoning, yet their performance on ultra-high-resolution imagery remains largely unexplored. Existing visual question answering (VQA) benchmarks typically rely on medium-resolution data, offering limited visual complexity. To bridge this gap, we introduce Ultra-high-resolution Reasoning Benchmark (UR-Bench), a benchmark designed to evaluate the reasoning capabilities of MLLMs under extreme visual information. UR-Bench comprises two major categories, Humanistic Scenes and Natural Scenes, covering four subsets of ultra-high-resolution images with distinct spatial structures and data sources. Each subset contains images ranging from hundreds of megapixels to gigapixels, accompanied by questions organized into three levels, enabling evaluation of models' reasoning capabilities in ultra-high-resolution scenarios. We further propose an agent-based framework in which a language model performs reasoning by invoking external visual tools. In addition, we introduce Semantic Abstraction and Retrieval tools that enable more efficient processing of ultra-high-resolution images. We evaluate state-of-the-art models using both an end-to-end MLLMs and our agent-based framework, demonstrating the effectiveness of our framework.

</details>


### [89] [Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN](https://arxiv.org/abs/2601.08776)
*Yanhua Zhao*

Main category: cs.CV

TL;DR: 用CycleGAN将多通道荧光显微图像无配对地翻译为拟H&E图像，保留结构且呈现H&E色彩，便于病理工作流整合。


<details>
  <summary>Details</summary>
Motivation: H&E染色是病理学标准，但荧光显微能提供互补信息；若能将荧光图像转为H&E样式，可便于病理医生解读并融入现有H&E分析流程。

Method: 采用CycleGAN进行无配对图像到图像翻译：将C01与C02荧光通道组合为RGB输入；生成器为基于ResNet的残差块网络，判别器为PatchGAN；训练损失包括对抗损失、循环一致性损失与identity损失；实现荧光↔H&E双向映射。

Result: 在荧光显微数据上生成逼真的拟H&E图像，既保持形态结构信息，又呈现接近H&E的颜色特征。

Conclusion: 该方法可将荧光数据以病理医师熟悉的H&E格式可视化，并支持与现有H&E分析管线的融合，提升解释性与实用性。

Abstract: Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.

</details>


### [90] [Aggregating Diverse Cue Experts for AI-Generated Image Detection](https://arxiv.org/abs/2601.08790)
*Lei Tan,Shuwei Li,Mohan Kankanhalli,Robby T. Tan*

Main category: cs.CV

TL;DR: 提出MCAN统一聚合多种线索（原图、高频、色度不一致）通过混合编码器适配器动态建模，显著提升AIGC检测跨模型泛化，在多个基准上SOTA，GenImage平均ACC最高提升7.4%。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC图像检测器过度依赖模型特定痕迹，易过拟合、跨生成器泛化差；需要能利用与真实成像机理相关、跨模型稳定的线索。

Method: 构建Multi-Cue Aggregation Network：输入包含三类线索——(1) 原始图像（全局语义/空间结构）；(2) 高频分量（边缘与纹理）；(3) 色度不一致CI线索（强度归一化后捕获真实成像噪声模式，与AI噪声可分）。采用mixture-of-encoders adapter对不同线索进行动态加权与特征路由，统一网络内进行特征融合学习，结合空间、频域及色度信息。

Result: 在GenImage、Chameleon、UniversalFakeDetect上取得SOTA；GenImage跨8个生成器的平均ACC较最佳现有方法最高提升7.4%。

Conclusion: 统一的多线索聚合与动态编码适配能学习到更贴近真实成像机理的特征，显著提升AIGC检测的跨模型泛化和鲁棒性；CI线索与高频信息对区分真实与生成尤为有效。

Abstract: The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.

</details>


### [91] [DentalX: Context-Aware Dental Disease Detection with Radiographs](https://arxiv.org/abs/2601.08797)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出DentalX：通过口腔解剖语义分割提供结构上下文，显著提升牙科X光疾病检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 牙片中的病灶特征细微、对比度低，传统针对自然图像的目标检测器难以稳健识别；需要利用口腔结构先验来缓解视觉模糊与弱证据问题。

Method: 设计DentalX框架，引入结构化上下文提取模块，联合学习辅助任务——口腔解剖语义分割；将分割得到的结构语义特征融合进主任务的疾病检测头，以上下文增强弱目标；端到端联合优化，使两任务的相关性在训练中被捕获。

Result: 在专用基准上，DentalX在疾病检测与解剖分割两任务上均优于现有方法，显示明显性能提升。

Conclusion: 结构上下文与疾病检测互补，联合学习能减轻X光视觉模糊并提升细微病灶检出；方法有效且代码开源（链接提供）。

Abstract: Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.

</details>


### [92] [Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching](https://arxiv.org/abs/2601.08798)
*Maayan Yesharim,R. G. Bina Perl,Uri Roll,Sarig Gafny,Eli Geffen,Yoav Ram*

Main category: cs.CV

TL;DR: 该研究比较用于珍稀两栖动物（胡拉彩绘蛙）照片重识别的计算机视觉方法，发现零样本的深度局部特征匹配在准确率上显著优于全局特征嵌入，并通过两阶段流程实现高准确与高效率兼顾，已部署为实地应用的网络工具。


<details>
  <summary>Details</summary>
Motivation: 濒危物种个体监测需要可靠识别，但侵入式打标对极危物种不适宜。已有照片识别方法在此类物种、小样本与长时间序列下的效果未明，需评估并提供可在野外实用的非侵入式方案。

Method: 基于2013-2020年捕捉-再捕捉调查获取的191只个体、1,233张腹面照片：1) 比较零样本深度局部特征匹配管线与多种深度全局特征嵌入模型（含微调）。2) 提出两阶段流程：先用微调后的全局嵌入检索候选，再以局部特征匹配重排序；评估闭集Top-k识别精度、开集阈值分离与端到端运行时长。3) 将流程部署为Web应用以供常规野外使用。

Result: 零样本局部特征匹配闭集Top-1准确率达98%，优于所有全局嵌入模型；微调最佳全局模型至Top-1为60%（Top-10为91%），仍不及局部匹配。两阶段流程在维持约96% Top-1的同时，将运行时间由6.5-7.8小时降至约38分钟；同/异个体得分分布可良好分离，支持开集阈值化识别。

Conclusion: 对于胡拉彩绘蛙，深度局部特征匹配在零样本条件下最为稳健，应作为照片重识别默认方案；结合全局检索+局部重排的两阶段架构可在不显著牺牲准确率的前提下大幅提升可扩展性与实用性，适合部署到常规保护监测与捕再捕分析中。

Abstract: Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.

</details>


### [93] [S3-CLIP: Video Super Resolution for Person-ReID](https://arxiv.org/abs/2601.08807)
*Tamas Endrei,Gyorgy Cserey*

Main category: cs.CV

TL;DR: 提出S3-CLIP：以视频超分辨率提升轨迹片段质量，增强视频行人ReID在跨视角困难场景的表现，取得与基线相当且在排序指标上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频ReID多在模型结构上做改动，忽视了真实场景中轨迹片段（tracklet）分辨率低、质量差的问题；跨视角（空-地、地-空）尤其严重。作者动机是利用视频超分辨率提升输入质量，从源头改善ReID性能。

Method: 提出S3-CLIP框架：将最新的视频超分辨率（VSR）网络与任务驱动的超分管线结合，先对tracklet做超分增强，再与CLIP式ReID表征/匹配模块对接，面向视频化、跨视角ReID进行系统适配与训练。

Result: 在VReID-XFD挑战（WACV 2026）上，aerial-to-ground获得mAP 37.52%，ground-to-aerial mAP 29.16%。在地-空设定下，Rank-1/5/10分别提升11.24%、13.48%、17.98%，总体与强基线竞争。

Conclusion: 首次系统性探讨“以视频超分提升tracklet质量”用于视频ReID，证明在艰难跨视角场景中能带来有意义的排序精度增益，具备实用价值。

Abstract: Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.

</details>


### [94] [Reasoning Matters for 3D Visual Grounding](https://arxiv.org/abs/2601.08811)
*Hsiang-Wei Huang,Kuang-Ming Chen,Wenhao Chai,Cheng-Yen Yang,Jen-Hao Cheng,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出Reason3DVG-8B：一种通过自动合成“带推理过程”的3D视觉指代数据来微调LLM，从而显著提升3D视觉指代能力的模型。用仅约1.6%的他法数据量，优于现有LLM方法3D-GRAND。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉指代模型推理能力不足、依赖大量人工标注或昂贵的合成数据扩展，性能提升与数据成本不成正比；LLM在推理任务中表现强，但尚未有效用于3D视觉指代中的可解释推理。

Method: 构建自动化数据生成流水线：从3D场景中合成包含多步推理链条的文本指令与对应目标（即“带推理过程”的3D视觉指代样本），再用这些数据对LLM进行微调，形成3D视觉指代LLM——Reason3DVG-8B。

Result: Reason3DVG-8B在3D视觉指代任务上超越此前LLM方法3D-GRAND，且仅使用其约1.6%的训练数据量即可达到更好性能。

Conclusion: 带推理过程的数据对3D视觉指代至关重要；自动化推理数据合成结合LLM微调能以极低数据成本获得显著性能提升，凸显推理能力在3D理解中的核心作用。

Abstract: The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.

</details>


### [95] [Motion Attribution for Video Generation](https://arxiv.org/abs/2601.08828)
*Xindi Wu,Despoina Paschalidou,Jun Gao,Antonio Torralba,Laura Leal-Taixé,Olga Russakovsky,Sanja Fidler,Jonathan Lorraine*

Main category: cs.CV

TL;DR: 提出Motive：一种面向“运动”的梯度归因框架，用于度量哪些微调视频片段会改善或破坏生成视频的时间动态，并据此做数据策划，显著提升文本生成视频的运动平滑与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型进步显著，但“数据如何影响运动质量（时间一致性、动态程度）”缺乏系统理解与可操作工具；既有归因多聚焦静态外观，难以区分运动与外观因素，阻碍高效的数据筛选与微调。

Method: 提出Motive（MOTIon attribution for Video gEneration）：基于梯度的、面向运动的数据影响归因框架。核心是用运动加权的损失掩膜，将时间动态从静态外观中解耦，只对运动相关区域/帧进行影响分配；可扩展到大规模高质量数据与现代文本到视频模型。利用该影响分数识别对运动影响大的微调片段，并据此进行数据策展与微调。

Result: 在文本到视频模型上，Motive能识别显著影响运动的训练片段，指导数据筛选后提升时间一致性与物理合理性；在VBench上同时提升运动平滑度与动态程度，对比预训练基线获得74.1%的人类偏好胜率。

Conclusion: Motive是首个专注“运动”而非外观的归因与数据策展框架，能高效、可扩展地度量微调数据对视频时间动态的影响，并通过挑选高影响数据显著改善生成视频运动质量。

Abstract: Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.

</details>


### [96] [3AM: Segment Anything with Geometric Consistency in Videos](https://arxiv.org/abs/2601.08831)
*Yang-Che Sun,Cheng Sun,Chin-Yang Lin,Fu-En Yang,Min-Hung Chen,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 3AM 将 MUSt3R 的3D几何特征在训练期融合进 SAM2，使记忆型 VOS 在大视角变化下保持几何一致，同时推理仅用RGB，无需位姿/深度/预处理；在ScanNet++等宽基线数据上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 记忆式视频目标分割（如 SAM2）依赖外观特征，遇到大视角变化易失效；而传统3D实例分割虽具视角一致性，却依赖相机位姿、深度与繁琐预处理，不适合通用VOS。需要一种既具几何稳健性又在推理阶段轻量、仅用RGB的方案。

Method: 提出训练期增强框架 3AM：将 MUSt3R 产生的隐式几何对应特征在多层级通过轻量 Feature Merger 融入 SAM2 的记忆结构；并设计视场感知采样策略，保证采样帧在空间上观测对象的一致区域，促进可靠的3D对应学习。推理阶段仅依赖RGB，既保留外观匹配能力又引入几何一致性。

Result: 在含宽基线运动的数据集（ScanNet++、Replica）上显著提升：在 ScanNet++ Selected Subset 上 IoU 90.6%、Positive IoU 71.7%，相较最强 VOS 基线（含 SAM2 及其扩展）分别提升 +15.9 与 +30.4 点。

Conclusion: 通过在训练期引入3D感知特征并与外观特征融合，3AM 实现对大视角变化的稳健视频分割，同时维持推理阶段的轻量与无需外部几何信息，达到了新的SOTA。

Abstract: Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/

</details>


### [97] [RAVEN: Erasing Invisible Watermarks via Novel View Synthesis](https://arxiv.org/abs/2601.08832)
*Fahad Shamshad,Nils Lukas,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 该论文发现：通过将“去水印”转化为“视角合成”问题，利用零样本扩散模型在潜空间施加受控几何变换并用视图引导的对应注意力重建，可在不知水印/无检测器的前提下，大幅削弱多种不可见水印，同时保持高感知质量，优于多数现有攻击。


<details>
  <summary>Details</summary>
Motivation: 不可见水印被广泛用于AI生成图像的溯源与鉴别，但现有评估多集中于像素或频域干扰；若存在在语义保持的前提下仍能可靠去除水印的方法，将动摇其鲁棒性假设并指导更稳健的水印设计。

Method: 把去水印重新表述为“从同一语义内容生成一个感知一致但视角变换的替代视图”。具体做法：在冻结的预训练扩散模型上，零样本地在潜空间施加受控几何变换（视角/位姿/投影变化），并引入视图引导的对应注意力以维持结构一致性，再重建图像。方法不需要水印先验或检测器访问。

Result: 在多数据集上对15种水印方案进行评测，提出的方法在抑制水印强度方面达到SOTA，超过14种基线攻击，同时在感知质量（如保真度/结构一致性等指标）上更优。

Conclusion: 当前对像素与频域鲁棒的不可见水印，仍对语义保持的视角变换脆弱。通过零样本、潜空间几何变换与对应注意力的扩散框架，可有效抑制水印且保持高质量图像，提示未来水印设计需考虑对语义/几何视图变换的鲁棒性。

Abstract: Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.

</details>
