<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification](https://arxiv.org/abs/2512.11015)
*Anoop Krishnan*

Main category: cs.CV

TL;DR: 论文提出用文本引导来提升人脸性别分类的公平性：通过图文匹配指导与图文融合，利用图像字幕的语义信息，在无人口统计标签情况下缓解种族/性别偏差并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有人脸性别分类存在对不同种族/群体的性能差异，且依赖人口统计标签进行去偏困难、可解释性不足。作者期望利用现成的图像字幕语义作为额外监督，提升泛化与公平性，同时避免使用敏感属性标签。

Method: 两条主线：1) 图文匹配（ITM）指导：训练模型学习图像与文本之间细粒度对齐，获得更强的多模态表示；2) 图文融合：将图像与文本嵌入融合形成联合表示，用于分类与公平性提升。整个训练在无显式人口统计标签下进行，以文本语义作为辅助监督信号。

Result: 在基准数据集上，所提方法较现有方法提升总体准确率，并显著降低不同性别/种族组间的性能差距（偏差），显示更好的跨组一致性与鲁棒性。

Conclusion: 文本引导的多模态训练能在不使用人口统计标签的前提下提升人脸性别分类的公平性与可解释性。ITM指导与图文融合均有效缓解偏差、增强泛化，对开发更公平的计算机视觉系统具有应用无关的潜力。

Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.

</details>


### [2] [SoccerMaster: A Vision Foundation Model for Soccer Understanding](https://arxiv.org/abs/2512.11016)
*Haolin Yang,Jiayuan Rao,Haoning Wu,Weidi Xie*

Main category: cs.CV

TL;DR: 提出 SoccerMaster：一个通过监督多任务预训练统一多种足球视觉理解任务的领域特定视觉基础模型；配套构建自动标注与多数据集整合的数据工程管线 SoccerFactory；在多下游任务上优于各类专家模型。


<details>
  <summary>Details</summary>
Motivation: 现有足球视频理解多依赖分散的任务专用模型，难以覆盖从精细感知到语义推理的多样需求，缺乏一个统一、可扩展且可迁移的基础模型与配套的大规模高质量数据资源。

Method: 构建统一框架的视觉基础模型 SoccerMaster，通过监督式多任务预训练同时学习细粒度感知（如球员检测）与高层语义推理（如事件分类）；设计自动化数据整理与空间标注生成流水线，并将其与多个现有足球视频数据集整合，形成预训练数据集 SoccerFactory；在多种下游任务上进行系统评测。

Result: SoccerMaster 在广泛的下游足球视觉任务上稳定超越各类任务特定专家模型，体现出更强的通用性与性能优势。

Conclusion: 统一的多任务预训练与可扩展数据管线能在足球领域带来跨任务的性能提升；所发布的数据、代码与模型为后续研究与应用提供基础设施。

Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.

</details>


### [3] [Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation](https://arxiv.org/abs/2512.11057)
*Marshal Ashif Shawkat,Moidul Hasan,Taufiq Hasan*

Main category: cs.CV

TL;DR: 提出用知识蒸馏训练无框标注的TB定位与分类模型，在TBX11k上取得0.2428 mIOU，学生模型泛化优于教师。


<details>
  <summary>Details</summary>
Motivation: TB仍致死率高，资源受限地区缺乏放射科专家。现有ML虽分类强，但依赖伪相关、泛化差；高质量医学标注昂贵且需多专家一致，难以规模化。需要降低对精细标注与伪相关的依赖，同时提升可解释定位能力。

Method: 采用教师-学生知识蒸馏框架（ResNet50为骨干），在无边界框标注下，通过蒸馏引导学生学习对TB相关异常的判别与定位。训练在TBX11k数据集上进行，使用分类监督并从教师特征/输出蒸馏，使CAM/显著图更聚焦病灶，从而减轻伪相关影响并实现弱监督定位。

Result: 在TBX11k上获得0.2428的mIOU定位成绩；多轮实验显示学生模型在鲁棒性与性能上持续优于教师模型。

Conclusion: 知识蒸馏可在无框标注条件下提升TB病灶定位与泛化，减少伪相关依赖；学生优于教师，显示方法有望在多样化临床场景推广应用。

Abstract: Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.

</details>


### [4] [Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning](https://arxiv.org/abs/2512.11060)
*Chenjun Li,Cheng Wan,Laurin Lux,Alexander Berger,Richard B. Rosen,Martin J. Menten,Johannes C. Paetzold*

Main category: cs.CV

TL;DR: 提出SVR框架生成逼真的视网膜血管及DR病变，并自动生成细粒度推理文本，构建OCTA-100K-SVR，用其微调通用VLM在真实OCTA零样本分类达89.67%，且解释与定位更佳。


<details>
  <summary>Details</summary>
Motivation: 医学VLM需要细致、可溯源的推理文本与图像配对数据，但如OCTA等专科领域缺乏大规模带病灶定位与推理的文本标注，限制了可解释诊断能力。

Method: 提出“合成血管推理”(SVR)：可控生成含DR关键征象（毛细血管灌注缺失、微动脉瘤、新生血管、迂曲度）的逼真血管图像；同时自动产出与病灶对应、可定位的细粒度推理文本。基于此构建10万对图文的OCTA-100K-SVR数据集，并用通用VLM（Qwen3-VL-8b）进行训练/指令微调，评估零样本分类、解释质量与病灶定位。

Result: 在真实OCTA数据上实现零样本平衡分类准确率89.67%，优于监督学习基线；经人类专家评估，解释质量与病理定位显著提升。

Conclusion: 通过大规模可控合成图像与自动推理文本，能有效提升通用VLM在OCTA等专科影像上的诊断与可解释性，减少对稀缺精细标注的依赖。

Abstract: Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.

</details>


### [5] [VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation](https://arxiv.org/abs/2512.11061)
*Felix O'Mahony,Roberto Cipolla,Ayush Tewari*

Main category: cs.CV

TL;DR: 提出VDAWorld：用VLM把图像-文本对蒸馏为可模拟的场景表示，并选取合适物理引擎进行动态预测，克服纯生成视频模型的物理不一致、不可交互与不可查询问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频世界模型常违背物理/逻辑、缺乏交互性、黑箱难以结构化查询，难以支撑可解释、可控的世界建模与推理。需要一种将感知信息转化为可计算、可模拟的抽象表征的新范式。

Method: 提出VDAWorld框架：以VLM为智能调度器，从图像-文本对出发，自动选择视觉工具构建有锚定的2D/3D场景表示（对象、关系、属性），并依据场景选择兼容的物理模拟器（如刚体、流体）施加作用；由静态场景中推断潜在动力学以预测未来状态。核心是“智能抽象+自适应仿真”的管线。

Result: 实验显示该方法能在多种动态场景下生成高质量、可信的仿真与未来预测，表现出较强的通用性与物理一致性，相较纯视频生成具有更好的结构化可查询与可控性。

Conclusion: 通过将视觉-语言感知蒸馏为结构化场景并联动物理引擎，VDAWorld实现可解释、可交互且可查询的世界模型，为替代黑箱视频生成式建模提供有效路径。

Abstract: Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.

</details>


### [6] [E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring](https://arxiv.org/abs/2512.11076)
*Jack Brady,Andrew Dailey,Kristen Schang,Zo Vic Shong*

Main category: cs.CV

TL;DR: 本文综述将“事件相机”引入城市动态监测：它按光强变化而非RGB取样，低光高动态优势明显，兼具隐私友好；并主张与红外、激光雷达、振动等多传感融合以弥补局限、提升城市运动与事件感知。


<details>
  <summary>Details</summary>
Motivation: 传统城市监测从人工观察到RGB相机与各类传感器，仍在低光、快速运动、隐私保护与高效数据获取上存在不足，需要新的感知范式来更稳健地捕捉人群与城市事件。

Method: 以综述方式梳理城市动态研究的演进，聚焦事件相机的原理（基于光强变化的异步事件）、应用场景、优势/挑战以及与机器学习结合的路径，并探讨与红外、LiDAR、振动等传感器的多模态融合。

Result: 总结事件相机在低光、高动态范围、数据稀疏与隐私方面的潜在优势，同时指出其噪声、数据表征与算法适配等挑战；归纳现有和潜在应用，并给出多传感融合可显著提升鲁棒性的证据与论点。

Conclusion: 事件相机是研究城市动态的有前景媒介，兼顾关键信息获取与隐私；建议以多传感融合（事件+红外/事件+LiDAR/事件+振动）缓解其短板，提升城市事件感知与分析能力。

Abstract: Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.

</details>


### [7] [Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description](https://arxiv.org/abs/2512.11098)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: 提出VLM-IRIS：将红外热像转为RGB兼容表示并结合提示集成，使CLIP零样本识别红外工业场景，无需再训练即可高精度检测3D打印床工件存在。


<details>
  <summary>Details</summary>
Motivation: 工业现场常在弱光/封闭设备内，传统可见光视觉受限；红外相机适用但缺乏大规模标注难以训练监督模型。现有VLM基于RGB训练，无法直接理解红外数据，需一种无需标注与再训练即可在红外域工作的零样本方法。

Method: 提出VLM-IRIS框架：1) 将FLIR Boson采集的红外图像经预处理转化为与RGB兼容的“magma”伪彩热图表示；2) 使用CLIP ViT-B/32编码器；3) 采用基于质心的文本提示集成（centroid prompt ensembling）以稳健对齐热像特征与文本类别；4) 在无需微调的前提下执行零样本推断。

Result: 在3D打印床工件存在性检测任务中，利用构建板与工件的温差，方法在红外图像上实现高准确率的零样本检测，且无需对VLM进行再训练。

Conclusion: 通过简单的热像到RGB兼容表示与提示集成，现有VLM可有效迁移至热成像工业监测，实现免标注、免再训练的红外应用；为热成像下的无监督/弱监督工业监控提供可行路径。

Abstract: Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.

</details>


### [8] [VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction](https://arxiv.org/abs/2512.11099)
*Weitai Kang,Jason Kuen,Mengwei Ren,Zijun Wei,Yan Yan,Kangning Liu*

Main category: cs.CV

TL;DR: VGent提出一种模块化编码器-解码器视觉指代框定框架：冻结的MLLM负责高层推理，检测器生成候选框，解码器跨注意选择目标框，避免自回归解码的慢与幻觉，并可独立升级两端；通过RL增强多目标推理、掩码感知标签消歧、全局目标识别，取得显著SOTA并保持快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉指代要么依赖MLLM自回归解码，速度慢且易幻觉；要么强行让LLM学习新视觉对齐/对象标记，削弱原有推理能力。需要一种既保留强推理、又高效稳健定位的设计，并能模块化升级。

Method: 采用“冻结MLLM编码器 + 候选框查询解码器”的二段式：检测器给高质量候选框，解码器以这些框为查询，对编码器隐状态跨注意选择目标框；提出三项增强：1) QuadThinker：基于强化学习训练，提升多目标推理；2) mask-aware label，解决检测/分割的边界歧义；3) global target recognition，提升所有目标的识别以在扩增候选中更好选择。

Result: 在多目标视觉指代基准上达SOTA：F1较前作提升+20.6%；在visual reference挑战中gIoU提升+8.2%、cIoU提升+5.8%；同时推理时延稳定且快速。

Conclusion: 通过解耦高层语言-视觉推理与低层框选，VGent兼得MLLM推理优势与检测器定位精度，避免自回归弊端；其可插拔升级机制（RL推理增强、掩码感知标签、全局目标识别）进一步提升多目标场景表现并保持高效。

Abstract: Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.

</details>


### [9] [Information-driven Fusion of Pathology Foundation Models for Enhanced Disease Characterization](https://arxiv.org/abs/2512.11104)
*Brennan Flannery,Thomas DeSilvio,Jane Nguyen,Satish E. Viswanath*

Main category: cs.CV

TL;DR: 提出一种基于相关性的“智能融合”方法，将多种病理基础模型(FM)的嵌入进行冗余剪枝后融合，较单一模型与朴素融合在肾、前列腺、直肠三种癌的分级/分期二分类上均有稳定提升，并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽多种病理FM在预训练目标上相近，但其嵌入空间的互补性/冗余性与生物学可解释性尚不清楚；如何有效整合多FM信息以提升下游病理任务表现与可解释性仍是开放问题。

Method: 收集三类癌症的诊断H&E全视野图像(肾519、前列腺490、直肠200)，构建低/高等级或分期二分类。选取多种tile级FM(Conch v1.5、MUSK、Virchow2、H-Optimus1、Prov-Gigapath)与slide级FM(TITAN、CHIEF、MADELEINE)提取特征；比较三种融合策略：多数投票集成、特征直连(拼接)、以及相关性引导的冗余特征剪枝“智能融合”。采用按患者分层的交叉验证与独立留出测试评估，并分析全局/局部嵌入相似性与注意力图。

Result: 智能融合在tile级嵌入上相对最佳单一FM与朴素融合在三种癌症中均获得一致的分类性能提升；全局相似性显示不同FM嵌入空间高度对齐，但局部邻域一致性较低，提示存在细粒度互补信息；注意力图显示智能融合更聚焦肿瘤区域并减少对良性区域的误关注。

Conclusion: 相关性引导的智能融合可在保持紧凑表示的同时提升下游病理任务的预测性能与可解释性，说明多FM间存在可挖掘的互补细粒度特征，融合优于单模或朴素拼接/投票。

Abstract: Foundation models (FMs) have demonstrated strong performance across diverse pathology tasks. While there are similarities in the pre-training objectives of FMs, there is still limited understanding of their complementarity, redundancy in embedding spaces, or biological interpretation of features. In this study, we propose an information-driven, intelligent fusion strategy for integrating multiple pathology FMs into a unified representation and systematically evaluate its performance for cancer grading and staging across three distinct diseases. Diagnostic H&E whole-slide images from kidney (519 slides), prostate (490 slides), and rectal (200 slides) cancers were dichotomized into low versus high grade or stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were considered to train downstream classifiers. We then evaluated three FM fusion schemes at both tile and slide levels: majority-vote ensembling, naive feature concatenation, and intelligent fusion based on correlation-guided pruning of redundant features. Under patient-stratified cross-validation with hold-out testing, intelligent fusion of tile-level embeddings yielded consistent gains in classification performance across all three cancers compared with the best single FMs and naive fusion. Global similarity metrics revealed substantial alignment of FM embedding spaces, contrasted by lower local neighborhood agreement, indicating complementary fine-grained information across FMs. Attention maps showed that intelligent fusion yielded concentrated attention on tumor regions while reducing spurious focus on benign regions. Our findings suggest that intelligent, correlation-guided fusion of pathology FMs can yield compact, task-tailored representations that enhance both predictive performance and interpretability in downstream computational pathology tasks.

</details>


### [10] [Learning from a Generative Oracle: Domain Adaptation for Restoration](https://arxiv.org/abs/2512.11121)
*Yuyang Hu,Mojtaba Sahraee-Ardakan,Arpit Bansal,Kangfu Mei,Christian Qi,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: 提出LEGO：一种无需成对数据的三阶段图像恢复域适应框架。用预训练模型生成初始恢复→用冻结的大规模生成式“oracle”精修为伪真值→用混合监督（原分布真值+新伪对）微调，实现跨域适应且不改结构，并在多种真实基准上显著提升。


<details>
  <summary>Details</summary>
Motivation: 预训练图像恢复模型在真实分布外退化上失效，因域间差异大且无标注。现有适应方法常需复杂架构改动或成对数据。需要一种既不改结构又无需真实成对标注、还能保持原有鲁棒性的实用适配方案。

Method: 三阶段：1) 用原预训练恢复器在目标域数据上生成初始恢复结果；2) 引入冻结的大规模生成式“oracle”（如高性能扩散/生成模型）对初始结果进行精修，得到高质量伪真值；3) 以混合监督训练：用原来分布的有标注数据维持原能力，同时用新域数据与对应伪真值进行微调，完成后训练阶段的无配对域适应，无需改动网络结构。

Result: 在多样的真实世界基准上显著提升跨域性能，成功缩小域间差距；同时保持原分布上的鲁棒性与性能，没有因适配而回退。

Conclusion: 通过将无监督域适应转化为“伪监督”问题，LEGO在不改变架构且无成对标注的前提下实现了有效的后训练域适应，兼顾新域提升与原域保持，具备实用性与通用性。

Abstract: Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.

</details>


### [11] [Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching](https://arxiv.org/abs/2512.11130)
*Bowen Wen,Shaurya Dewan,Stan Birchfield*

Main category: cs.CV

TL;DR: 提出Fast-FoundationStereo，在保持零样本泛化的同时实现实时速率，较FoundationStereo快10倍且精度接近。


<details>
  <summary>Details</summary>
Motivation: 现有Stereo foundation模型零样本能力强但计算开销大，难以实时；高效模型实时但鲁棒性差且需逐域微调。需要一种既实时又具强泛化的双目深度方法。

Method: 采用“分而治之”的加速框架：1) 知识蒸馏，将混合骨干压缩为单一高效学生网络；2) 分块式神经架构搜索，在给定时延约束下自动发现最优代价体过滤结构，指数级降低搜索复杂度；3) 结构化剪枝，去除迭代细化模块冗余。另设计自动伪标签流程，整理140万野外双目数据以辅助蒸馏与训练。

Result: 得到的模型在零样本精度上接近FoundationStereo，但速度提升超过10倍，在实时方法中达到新的SOTA。

Conclusion: 通过蒸馏+NAS+剪枝并结合大规模伪标注数据，Fast-FoundationStereo在保证强零样本泛化的同时实现实时推理，弥合了鲁棒性与效率的鸿沟。

Abstract: Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/

</details>


### [12] [Learning complete and explainable visual representations from itemized text supervision](https://arxiv.org/abs/2512.11141)
*Yiwei Lyu,Chenhui Zhao,Soumyanil Banerjee,Shixuan Liu,Akshay Rao,Akhil Kondepudi,Honglak Lee,Todd C. Hollon*

Main category: cs.CV

TL;DR: ItemizedCLIP提出用“条目化”文本监督训练视觉模型，通过跨注意力生成按条目条件的视觉嵌入，并用目标函数同时保证条目独立与表征完整，显著提升多域零样本与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统以语言为监督的视觉训练多基于整句/多caption，其文本往往冗余且围绕单一对象；而医疗影像、遥感等非对象中心领域常见“一图多条目”的独立发现描述。现有方法难以做到逐条语义分离与覆盖全部条目，导致可解释性和零样本泛化受限。

Method: 提出ItemizedCLIP：1) 采用跨注意力模块，用每个文本条目作为查询，生成相应的条件化视觉嵌入；2) 设计联合目标：条目独立性（不同条目对应不同图像区域/特征）与表征完整性（覆盖所有条目）；3) 在多个自然条目化数据域和一个合成数据集上训练/评估。

Result: 在脑MRI、头CT、胸部CT、遥感及合成条目化数据上，ItemizedCLIP在零样本任务和细粒度可解释性上相较基线取得显著提升，学到的表示可区分条目、语义落地且可视化解释更强。

Conclusion: 条目化文本监督可通过跨注意力与专门损失实现“独立且完整”的视觉表示，改善非对象中心领域的泛化与可解释性；ItemizedCLIP验证了该范式的有效性并开源实现。

Abstract: Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.

</details>


### [13] [Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context](https://arxiv.org/abs/2512.11167)
*Anatole Jacquin de Margerie,Alexis Roger,Irina Rish*

Main category: cs.CV

TL;DR: 复现并批判性分析Monkey VLM的图像切片（tiling）策略：确认能恢复局部细节，进一步研究全局上下文作用；结果随任务与切片粒度而波动。


<details>
  <summary>Details</summary>
Motivation: 复杂多模态模型常缺乏透明实现与可获取的训练基础设施，阻碍复现与扩展。Monkey VLM 提出通过高分辨率图像切片兼顾细节与效率，但其细节与可复用性需要验证与澄清。

Method: 使用开放检查点重现Monkey VLM的训练与推理流程，严格复现实验设置；评估图像切片对高分辨率理解的影响，并新增实验分析“全局上下文”信息的引入；比较不同任务类型与切片粒度下的性能差异。

Result: 1) 复现了核心发现：切片能有效恢复局部细节；2) 加入全局上下文可带来实用收益；3) 与原文有偏差，且效果大小强依赖任务类别与切片粒度设置。

Conclusion: 图像切片是高分辨率多模态建模的有效途径，但需结合全局上下文与任务定制的粒度选择；复现性提升了方法透明度，同时揭示了对任务与超参敏感的局限，为后续模型设计提供指导。

Abstract: Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.

</details>


### [14] [Lightweight 3D Gaussian Splatting Compression via Video Codec](https://arxiv.org/abs/2512.11186)
*Qi Yang,Geert Van Der Auwera,Zhu Li*

Main category: cs.CV

TL;DR: 提出LGSCV：以Morton扫描+视频编解码为核心的轻量级3D Gaussian Splatting压缩，结合SH PCA与MiniPLAS，在中低码率显著提升RD，同时极大降低映射生成与编码耗时。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频的GS压缩依赖PLAS将3D高斯按光滑2D映射排序，但PLAS计算昂贵、耗时长，不利于在轻量设备上应用；同时在中低码率下，简单块状映射易出现质量崩塌，需新的结构与特征降维以兼顾效率与率失真。

Method: 1) 提出两阶段Morton扫描：先3D Morton对高斯基元排序，再2D Morton将序列映射为与视频编解码方形CU对齐的块状2D图。2) 观察到中低码率质量下滑，引入两点改进：a) 对球谐系数做PCA降维，降低冗余与码耗；b) 设计MiniPLAS，在给定块内进行轻量级局部排序，兼顾块一致性与局部平滑性，并可反向指导视频编解码的CU大小配置，降低编码复杂度。

Result: 在MPEG数据集上，相比SOTA获得>20% RD增益；2D映射生成时间降至约1秒；视频编码时间减少约50%。在高码率接近PLAS性能，中低码率通过SH PCA+MiniPLAS显著改善。

Conclusion: LGSCV通过两阶段Morton块状映射结合SH PCA与MiniPLAS，在保持与视频编解码器良好匹配的同时，显著提升中低码率率失真并减少计算开销，适合轻量设备与实用场景；代码已开源。

Abstract: Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .

</details>


### [15] [Multi-task Learning with Extended Temporal Shift Module for Temporal Action Localization](https://arxiv.org/abs/2512.11189)
*Anh-Kiet Duong,Petra Gomez-Krämer*

Main category: cs.CV

TL;DR: 基于TSM的多任务与加权集成，用固定窗格的区间分类做时序动作定位，在多视角多模态数据上夺冠。


<details>
  <summary>Details</summary>
Motivation: 应对BinEgo-360挑战：多视角（全景/第三人称/第一人称）与多模态视频下的时序动作定位难题，需要有效利用环境上下文并在资源受限下保证鲁棒和高效。

Method: 以TSM为骨干，扩展为TAL：引入“背景”类，按固定长度、非重叠时间片进行分类；联合优化场景分类与TAL的多任务学习，利用动作-场景上下文；最终用多模型加权集成提升稳定性与一致性。

Result: 在ICCV 2025 BinEgo-360挑战的初赛和加赛均获第一。

Conclusion: 高效骨干（TSM）+多任务学习+加权集成的组合在多视角多模态TAL上效果显著、鲁棒性强。

Abstract: We present our solution to the BinEgo-360 Challenge at ICCV 2025, which focuses on temporal action localization (TAL) in multi-perspective and multi-modal video settings. The challenge provides a dataset containing panoramic, third-person, and egocentric recordings, annotated with fine-grained action classes. Our approach is built on the Temporal Shift Module (TSM), which we extend to handle TAL by introducing a background class and classifying fixed-length non-overlapping intervals. We employ a multi-task learning framework that jointly optimizes for scene classification and TAL, leveraging contextual cues between actions and environments. Finally, we integrate multiple models through a weighted ensemble strategy, which improves robustness and consistency of predictions. Our method is ranked first in both the initial and extended rounds of the competition, demonstrating the effectiveness of combining multi-task learning, an efficient backbone, and ensemble learning for TAL.

</details>


### [16] [CADKnitter: Compositional CAD Generation from Text and Geometry Guidance](https://arxiv.org/abs/2512.11199)
*Tri Le,Khang Nguyen,Baoru Huang,Tung D. Ta,Anh Nguyen*

Main category: cs.CV

TL;DR: 提出CADKnitter：一种面向装配的组合式CAD生成框架，结合几何引导的扩散采样，可在给定CAD部件的几何约束和文本语义约束下生成互补部件；并发布含31万+样本的KnitCAD数据集，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实产品设计通常由多部件装配而成，需同时满足语义与几何（配合、公差、约束）要求；现有方法多聚焦单部件生成，难以支撑实际可装配、可编辑的CAD设计流程。

Method: 提出CADKnitter框架：以组合式（compositional）生成思路，将给定CAD模型的几何约束与文本设计意图作为条件；引入几何引导的扩散采样策略，在采样过程中显式遵循几何约束（如接触/配合关系、装配元数据）并对语义文本对齐；同时构建KnitCAD数据集（31万+样本），包含CAD模型、文本提示和装配元数据以支持训练与评测。

Result: 在多项基准与自建评测上，相比SOTA基线取得显著提升；能生成与已有部件互补且可装配的CAD零件，并更好满足文本语义与几何一致性。

Conclusion: 几何引导的扩散与组合式条件建模可有效推动从单部件到可装配CAD生成；KnitCAD数据集为该方向提供了规模化数据基础，方法在语义—几何双重约束下展现出明显优势。

Abstract: Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.

</details>


### [17] [AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path](https://arxiv.org/abs/2512.11203)
*Zhengyang Yu,Akio Hayakawa,Masato Ishii,Qingtao Yu,Takashi Shibuya,Jing Zhang,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出AutoRefiner：一种面向自回归视频扩散模型（AR-VDM）的前馈噪声精炼器，通过路径式噪声精炼与反射式KV-cache显著提升生成保真度，且几乎不增加推理开销。


<details>
  <summary>Details</summary>
Motivation: AR-VDM具备实时与交互优势，但样本保真度仍待提升。推理时对齐（仅优化噪声不改模型参数）在T2I中有效，但现有优化/搜索方法对AR-VDM计算代价高。T2I的单次前馈噪声精炼器难以直接迁移到AR-VDM，存在失效点，亟需针对AR-VDM特性的专门设计。

Method: 提出AutoRefiner，核心包括：1）路径式（pathwise）噪声精炼：沿随机去噪路径逐步调制各步噪声，使精炼与自回归时间因果一致；2）反射式KV-cache：在自回归生成中复用并“反射”先前时刻的键值缓存，提供跨时间步一致的上下文调制；整体为前馈式插件，对原AR-VDM参数零改动，仅在噪声采样上施加可学习调制。

Result: 在多个AR-VDM与视频基准上，AutoRefiner以极低额外开销显著提升样本保真度和细节一致性，相较天真移植的T2I精炼器表现更稳健；同时维持AR-VDM的实时性与交互性。

Conclusion: 针对AR-VDM特点设计的AutoRefiner能在不修改模型参数的前提下，通过路径一致的噪声精炼与KV缓存反射机制，有效提升视频生成质量，是适用于AR-VDM的高效可插拔推理时对齐方案。

Abstract: Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.

</details>


### [18] [SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection](https://arxiv.org/abs/2512.11215)
*Tianye Qi,Weihao Li,Nick Barnes*

Main category: cs.CV

TL;DR: SmokeBench基准评估多模态大模型在图像中识别与定位野火烟雾的能力，含分类与多粒度定位四任务。结果显示：大面积烟雾可被识别，但早期小体量烟雾定位普遍失败；模型性能与烟雾体量高度相关，与对比度相关性较弱。结论：当前MLLM在安全关键的早期烟雾监测上存在明显短板，需要改进早期定位方法。


<details>
  <summary>Details</summary>
Motivation: 野火早期发现至关重要，但烟雾透明、形态不定且易与云混淆，使得视觉早期检测困难。现有MLLM虽具通用视觉语言能力，但其在烟雾识别与定位（尤其早期阶段）的真实能力缺乏系统评估，因此需要一个专门基准来量化与分析其局限。

Method: 构建SmokeBench，包含四类任务：1) 烟雾二分类；2) 基于小块(tile)的定位；3) 基于网格(grid)的定位；4) 检测任务。对Idefics2、Qwen2.5-VL、InternVL3、Unified-IO 2、Grounding DINO、GPT-4o、Gemini-2.5 Pro等模型进行评测，并分析烟雾体量与对比度等因素对性能的影响。

Result: 多数模型在烟雾覆盖面积较大时能正确分类，但在精准定位方面整体表现不佳，尤其对早期、小体量烟雾定位失败率高。统计分析显示，模型性能与烟雾体量强相关，而与图像对比度仅弱相关。

Conclusion: 当前MLLM不适合单独承担安全关键的早期野火监测；应研发面向小体量、低显著度烟雾的鲁棒定位方法与训练策略（如更细粒度标注、难例挖掘、体积感知训练等），以提升早期阶段的定位能力。

Abstract: Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.

</details>


### [19] [VFMF: World Modeling by Forecasting Vision Foundation Model Features](https://arxiv.org/abs/2512.11225)
*Gabrijel Boduljak,Yushi Lan,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 该论文提出在视觉基础模型(VFM)特征空间中进行自回归生成式预测，通过流匹配/扩散在紧凑潜变量上建模不确定性，较确定性回归获得更锐利、准确的多模态(语义、深度、法线、RGB)预测。


<details>
  <summary>Details</summary>
Motivation: 像素级视频生成虽逼真但计算昂贵且难以直接用于决策；用VFM特征做确定性回归虽高效、易转译为任务信号，但会平均多种可能未来，缺乏不确定性建模，导致预测不准。

Method: 将VFM特征编码到为扩散/流匹配设计的紧凑潜空间，并在此进行自回归的条件生成(流匹配)。与以往PCA压缩不同，使用学习式编码器保持更多信息。预测的潜变量解码回多种输出模态(语义分割、深度、法线、RGB)。与回归在架构与算力匹配条件下比较。

Result: 所学潜空间较PCA在信息保持、预测和图像生成任务上更优；在所有模态上生成的预测更锐利、误差更低，优于确定性回归；可高效从VFM特征生成多模态输出。

Conclusion: 在VFM特征空间进行随机式条件生成(通过潜空间扩散/流匹配)克服了确定性回归的均值化问题，提供可扩展、准确且可解释的世界建模路径。

Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.

</details>


### [20] [FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model](https://arxiv.org/abs/2512.11226)
*Hongbin Lin,Yiming Yang,Yifan Zhang,Chaoda Zheng,Jie Feng,Sheng Wang,Zhennan Wang,Shijia Chen,Boyang Wang,Yu Zhang,Xianming Liu,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: FutureX 是一种将“思维链”(CoT)与世界模型相结合的端到端自动驾驶规划框架：在简单场景下快速前向生成轨迹；在复杂/高动态场景下触发“思考”模式，用潜在世界模型滚动预测未来场景并据此细化轨迹，从而更理性、碰撞更少、效率不降；在 NAVSIM 上为 TransFuser 带来 6.2 的 PDMS 提升。


<details>
  <summary>Details</summary>
Motivation: 纯依赖当前场景的端到端规划在高动态交通中容易次优，因为自车动作会改变未来场景。需要在规划时显式建模未来场景的演化以及自车-环境交互，进行复杂推理，以获得更安全合理的轨迹。

Method: 提出 FutureX：
- Auto-think Switch：判断当前场景是否需要额外推理；
- 若进入 Thinking 模式：Latent World Model 在 CoT 指导下进行多步未来场景潜在滚动预测（world model rollout），Summarizer Module 基于这些未来表示细化并优化轨迹；
- 若无需推理：Instant 模式直接前向生成轨迹；
- 与现有端到端规划器（如 TransFuser）集成以增强其决策。

Result: 在广泛实验（如 NAVSIM 基准）中，相比原方法，FutureX 产生更理性的运动规划、碰撞更少、效率不受影响；以 TransFuser 为例，PDMS 指标提升 6.2。

Conclusion: 将 CoT 驱动的潜在世界模型推理融入端到端规划，有效在复杂动态交通中提升决策质量与安全性，同时保持推理效率；该框架可作为插件增强现有方法，代码将开源。

Abstract: In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.

</details>


### [21] [REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation](https://arxiv.org/abs/2512.11229)
*Haotian Wang,Yuzhe Weng,Xinyi Yu,Jun Du,Haoran Xu,Xiaoyan Wu,Shan He,Bing Yin,Cong Liu,Qingfeng Liu*

Main category: cs.CV

TL;DR: 提出REST：首个扩散式、端到端、实时流式的语音驱动说话人脸生成框架；通过高时空压缩的VAE建立紧凑视频潜空间，引入ID-Context Cache实现自回归流式生成并保持身份与时序一致，提出异步流式蒸馏(ASD)以减轻误差累积；在速度与质量上均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在说话人脸生成中质量高但推理慢，且多为非自回归范式，不适合实时场景（如直播、视频通话）。需要一种既保留扩散模型质量，又能实时、低延迟并长时稳定输出的方案。

Method: 1) 通过高时空压缩VAE学习紧凑视频潜空间，降低实时生成计算量；2) 设计ID-Context Cache机制，将ID-Sink与Context-Cache思想用于KV缓存，以在长序列流式生成中保持身份一致与时序连贯，并实现自回归解码；3) 提出异步流式蒸馏(ASD)：用非流式教师扩散模型、异步噪声日程对流式学生进行监督，缓解自回归误差传播并提升时序一致性。

Result: REST实现端到端实时流式说话人脸生成，显著提升推理速度，同时在视觉质量、同步性与稳定性上达到或超过现有方法；实验显示其在速度与综合指标上优于SOTA。

Conclusion: REST有效弥合自回归与扩散式方法的鸿沟，在紧凑潜空间+缓存机制+异步蒸馏的组合下，实现了实时、长时稳定的语音驱动人脸生成，对需要低延迟的应用具有实际价值。

Abstract: Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.

</details>


### [22] [RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing](https://arxiv.org/abs/2512.11234)
*Wentang Chen,Shougao Zhang,Yiman Zhang,Tianhao Zhou,Ruihui Li*

Main category: cs.CV

TL;DR: RoomPilot提出一个以室内领域专用语言（IDSL）为核心的统一框架，把文本或CAD平面图解析成可控、可交互的室内场景，兼顾多模态理解、物理一致性与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有室内场景生成方法要么仅支持单一/窄模态输入，要么依赖随机过程导致可控性差，且多为“看起来合理但不可交互”。需要一种既能跨模态、又能在功能与交互层面保持一致与可编辑的生成方案。

Method: 设计一个室内领域专用语言（IDSL）作为统一语义表示；将文本描述或CAD平面图解析为IDSL；基于带有交互标注的资产库进行场景合成，从而在生成中编码交互语义与对象行为；相较传统程序化方法，框架强调可控性、功能性与一致性。

Result: 在多模态理解、细粒度可控生成、物理一致性与视觉保真度上优于现有方法；能从任一单一模态输入生成连贯、高质量且含交互语义的室内环境。

Conclusion: IDSL作为共享语义中枢使RoomPilot实现通用、可控、可交互的3D室内场景生成，向通用可控的室内环境合成迈出重要一步。

Abstract: Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.

</details>


### [23] [WildCap: Facial Appearance Capture in the Wild via Hybrid Inverse Rendering](https://arxiv.org/abs/2512.11237)
*Yuxuan Han,Xin Ming,Tianxiao Li,Zhuofan Shen,Qixuan Zhang,Lan Xu,Feng Xu*

Main category: cs.CV

TL;DR: WildCap从普通手机视频中高质量重建人脸外观。它先用数据驱动的SwitchLight把野外复杂光照“标准化”，再用基于物理的逆渲染；为解决网络伪影与尺度歧义，提出“texel网格光照”模型并结合扩散先验联合优化反射与光照，效果显著优于同类方法，将野外与可控光照的质量差距大幅缩小。


<details>
  <summary>Details</summary>
Motivation: 以往高质量人脸外观捕获依赖可控光场，成本高、场景受限；而在自然环境下拍摄的手机视频包含复杂光照与非物理伪影，导致难以准确分离材质与光照，需一种既实用又高保真度的野外逆渲染方案。

Method: 提出混合式逆渲染框架：1）先用数据驱动的SwitchLight将输入帧转换到更受控的成像条件；2）在此基础上进行模型驱动的逆渲染。为处理网络预测的局部非物理伪影（如阴影烘焙），引入“texel grid lighting”局部光照模型，把非物理效应解释为干净反照率受到局部物理光照的结果；3）优化时联合采样扩散先验以正则反射率贴图，同时优化局部光照，解决局部光照与反照率的尺度歧义。

Result: 在与现有同级拍摄配置的比较中，WildCap显著优于以往方法，在材质与光照分离、反照率清洁度、细节与一致性方面大幅提升，显著缩小了野外与可控光照录制之间的质量差距。

Conclusion: 通过数据驱动的预标准化、物理模型逆渲染、texel网格光照与扩散先验联合优化，WildCap能从野外手机视频中获得高质量人脸外观，兼顾易用性与准确性，并为在自然条件下的高保真外观捕获提供可行路径。

Abstract: Existing methods achieve high-quality facial appearance capture under controllable lighting, which increases capture cost and limits usability. We propose WildCap, a novel method for high-quality facial appearance capture from a smartphone video recorded in the wild. To disentangle high-quality reflectance from complex lighting effects in in-the-wild captures, we propose a novel hybrid inverse rendering framework. Specifically, we first apply a data-driven method, i.e., SwitchLight, to convert the captured images into more constrained conditions and then adopt model-based inverse rendering. However, unavoidable local artifacts in network predictions, such as shadow-baking, are non-physical and thus hinder accurate inverse rendering of lighting and material. To address this, we propose a novel texel grid lighting model to explain non-physical effects as clean albedo illuminated by local physical lighting. During optimization, we jointly sample a diffusion prior for reflectance maps and optimize the lighting, effectively resolving scale ambiguity between local lights and albedo. Our method achieves significantly better results than prior arts in the same capture setup, closing the quality gap between in-the-wild and controllable recordings by a large margin. Our code will be released \href{https://yxuhan.github.io/WildCap/index.html}{\textcolor{magenta}{here}}.

</details>


### [24] [Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition](https://arxiv.org/abs/2512.11239)
*Wen-Jue He,Xiaofeng Zhu,Zheng Zhang*

Main category: cs.CV

TL;DR: 提出一种用于不完整多模态情感识别的跨模态提示方法ComP，通过生成一致语义提示、跨模态知识传播与动态重加权，缓解模态性能差异与缺失数据带来的优化不充分问题，并在多数据集多缺失率下优于多种SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实多模态情感数据常存在模态缺失与质量不均，导致“性能鸿沟”和某些模态未充分优化，进而拖累整体识别效果。需要一种机制在缺失和不一致条件下仍能提炼并对齐各模态的关键信息，提升每个模态及整体表现。

Method: 提出ComP：1) 逐步式提示生成模块，配合动态梯度调制器，产生精炼且一致的模态语义提示；2) 跨模态知识传播，用提示选择性放大不同模态间的一致信息，增强模态特征判别性；3) 协调器对各模态输出进行动态重加权，作为平衡策略补充，提升融合效果。

Result: 在4个数据集、7个SOTA基线和不同缺失率设置下进行大量实验，ComP在整体准确率与稳健性上均取得领先，显示出在模态缺失条件下的显著优势。

Conclusion: 跨模态提示与知识传播结合动态重加权，能有效缓解模态性能差距与缺失带来的优化不足，提升不完整多模态情感识别的精度与鲁棒性。

Abstract: Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.

</details>


### [25] [PersonaLive! Expressive Portrait Image Animation for Live Streaming](https://arxiv.org/abs/2512.11253)
*Zhiyuan Li,Chi-Man Pun,Chen Fang,Jue Wang,Xiaodong Cun*

Main category: cs.CV

TL;DR: PersonaLive 是一个面向直播场景的扩散式人像驱动框架，通过多阶段训练在保证表情真实与画质的同时，将时延与推理速度显著降低，实现稳定的实时串流生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式人像动画方法重画质与表情逼真，忽视生成时延与实时性，难以用于直播与长时视频生成。需要一种既可精细控制表情动作、又能低延迟、长时稳定输出的视频生成方案。

Method: 1) 混合隐式信号控制：结合隐式人脸表征与3D隐式关键点，实现图像级的细腻运动/表情驱动；2) 少步外观蒸馏：在扩散去噪中蒸馏外观冗余，减少推理步数以提速；3) 自回归微块串流生成：采用滑动训练策略与历史关键帧机制，逐块生成并保持时序一致与长期稳定，降低端到端延迟。

Result: 在多项实验中达到SOTA，并相较以往扩散式人像动画实现约7–22倍的速度提升，同时保持高视觉质量与稳定性。

Conclusion: 通过混合隐式控制、少步蒸馏与微块自回归串流，PersonaLive在直播场景下实现低时延、长时稳定的人像动画生成，为扩散式视频驱动在实时应用中落地提供了有效方案。

Abstract: Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.

</details>


### [26] [Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers](https://arxiv.org/abs/2512.11260)
*Ali El Bellaj,Mohammed-Amine Cheddadi,Rhassan Berber*

Main category: cs.CV

TL;DR: 论文探讨用 Reformer（LSH 注意力）替代标准 ViT 作为视觉骨干：在小数据集上精度可优于 ViT，但在更大/更高分辨率场景中端到端效率与实测耗时仍不如 ViT，表明 LSH 的理论复杂度优势需要远超常见图像长度的序列才能转化为实际收益。


<details>
  <summary>Details</summary>
Motivation: 标准 ViT 的全局自注意力计算随 token 数量二次增长，导致高分辨率与资源受限场景下代价过高。作者想验证 Reformer（通过 LSH 近似全局注意力、理论上 O(n log n)）是否能在视觉任务中兼顾精度与效率。

Method: - 以图像补丁为 token，采用 Reformer 的 LSH 注意力近似全局注意力，降低理论复杂度。
- 在 CIFAR-10（小规模）、ImageNet-100（更现实的准确率–效率权衡）、以及高分辨率医学影像（长序列）上评测，与 ViT 风格基线比较精度与端到端效率。

Result: - CIFAR-10 上 Reformer 精度高于 ViT 基线。
- 在更大与更高分辨率设置中，ViT 在实际效率与总计算时间上始终优于 Reformer。
- 说明 LSH 的理论优势在这些序列长度下未转化为实际加速。

Conclusion: 尽管理论上 LSH 注意力可将复杂度从 O(n^2) 降到 O(n log n)，但在常见高分辨率图像产生的序列长度范围内，Reformer 的实际效率收益有限；要获得显著计算收益需要更长序列或不同应用场景。

Abstract: Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.

</details>


### [27] [Evaluating the Efficacy of Sentinel-2 versus Aerial Imagery in Serrated Tussock Classification](https://arxiv.org/abs/2512.11267)
*Rezwana Sultana,Manzur Murshed,Kathryn Sheffield,Singarayer Florentine,Tsz-Kwan Lee,Shyh Wei Teng*

Main category: cs.CV

TL;DR: 研究用多时相Sentinel‑2影像结合光谱、纹理、植被指数与季节信息，训练随机森林识别澳大利亚维州入侵草种锯叶针茅；最佳模型在相同数据上略优于高分辨率航拍模型，显示卫星遥感的可扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 入侵物种对生态与农业危害巨大；锯叶针茅扩散快、治理成本高。小尺度地面/航拍监测虽精细但不具可扩展性、成本高。需要验证低成本、可扩展的卫星遥感（尽管空间分辨率低）能否借助更高光谱分辨率与季相信息达到可比精度。

Method: 基于维州研究区，构建11个特征组合模型（光谱波段、纹理特征、植被指数、季节/物候特征的不同拼配），使用Sentinel‑2多时相数据；以随机森林分类器训练与评估，并与基于航拍影像的最佳模型在同一数据集上对比。

Result: 最佳Sentinel‑2模型（M76*）总体精度OA 68%、Kappa 0.55，略优于最佳航拍模型的OA 67%、Kappa 0.52；表明多季节、特征增强的卫星模型能有效识别锯叶针茅。

Conclusion: 尽管空间分辨率较低，多时相Sentinel‑2结合丰富特征与物候信息可在景观尺度实现具成本效益的入侵物种监测，且性能可与昂贵的航拍方案相当或略优；支持推广卫星为大范围监测的可扩展方案。

Abstract: Invasive species pose major global threats to ecosystems and agriculture. Serrated tussock (\textit{Nassella trichotoma}) is a highly competitive invasive grass species that disrupts native grasslands, reduces pasture productivity, and increases land management costs. In Victoria, Australia, it presents a major challenge due to its aggressive spread and ecological impact. While current ground surveys and subsequent management practices are effective at small scales, they are not feasible for landscape-scale monitoring. Although aerial imagery offers high spatial resolution suitable for detailed classification, its high cost limits scalability. Satellite-based remote sensing provides a more cost-effective and scalable alternative, though often with lower spatial resolution. This study evaluates whether multi-temporal Sentinel-2 imagery, despite its lower spatial resolution, can provide a comparable and cost-effective alternative for landscape-scale monitoring of serrated tussock by leveraging its higher spectral resolution and seasonal phenological information. A total of eleven models have been developed using various combinations of spectral bands, texture features, vegetation indices, and seasonal data. Using a random forest classifier, the best-performing Sentinel-2 model (M76*) has achieved an Overall Accuracy (OA) of 68\% and an Overall Kappa (OK) of 0.55, slightly outperforming the best-performing aerial imaging model's OA of 67\% and OK of 0.52 on the same dataset. These findings highlight the potential of multi-seasonal feature-enhanced satellite-based models for scalable invasive species classification.

</details>


### [28] [FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion](https://arxiv.org/abs/2512.11274)
*Xiangyang Luo,Qingyu Li,Xiaokun Liu,Wenyu Qin,Miao Yang,Meng Wang,Pengfei Wan,Di Zhang,Kun Gai,Shao-Lun Huang*

Main category: cs.CV

TL;DR: 提出FilmWeaver：面向任意长度、任意镜头的多镜头视频生成框架，通过自回归扩散+双级缓存（镜头级记忆与时间级记忆）实现跨镜头一致性与镜头内连贯性，同时支持交互式多轮创作、多概念注入与视频扩展，并在一致性和美学质量指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在单镜头合成上表现尚可，但在多镜头场景中很难保持人物与背景的一致性，且难以灵活生成任意长度和镜头数量的视频。需要一种既能扩展长度又能在叙事级别维持一致性的生成方法，以支持更可控、可叙事的内容创作。

Method: 1) 采用自回归扩散范式，逐镜头/逐段扩展，实现任意长度视频生成。2) 一致性问题拆分为跨镜头一致性（inter-shot）与镜头内连贯性（intra-shot）。3) 双级缓存机制：a) 镜头记忆（shot memory）缓存前序镜头关键帧，维持角色与场景身份；b) 时间记忆（temporal memory）保留当前镜头的历史帧，确保平滑连续运动。4) 支持多轮用户交互以逐步构建多镜头视频，并因解耦设计而天然支持多概念注入与视频续写。5) 构建高质量多镜头视频数据集以训练一致性感知模型。

Result: 在多项一致性和美学质量指标上优于现有方法；能够稳定生成多镜头、长时长视频，且在多概念注入与视频扩展等下游任务中表现出较强通用性和鲁棒性。

Conclusion: FilmWeaver通过自回归扩散与双级缓存将多镜头一致性问题解耦并有效解决，实现了任意长度、可交互、可控的多镜头视频生成，在一致性与视觉质量上显著领先，为叙事驱动的视频创作打开新可能。

Abstract: Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io

</details>


### [29] [RcAE: Recursive Reconstruction Framework for Unsupervised Industrial Anomaly Detection](https://arxiv.org/abs/2512.11284)
*Rongcheng Wu,Hao Zhu,Shiying Zhang,Mingzhe Wang,Zhidong Li,Hui Li,Jianlong Zhou,Jiangtao Cui,Fang Chen,Pingyang Sun,Qiyu Liao,Ye Lin*

Main category: cs.CV

TL;DR: 提出递归自编码器RcAE，通过多次重构逐步抑制异常并保细节，配合跨递归检测CRD与细节保真网络DPN，在无监督工业异常检测上优于非扩散方法，接近扩散模型但更轻更快。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器单次解码易残留异常、丢失高频纹理，难以覆盖不同尺度与强度的缺陷；需要一种既能逐步抑制异常又能保留正常结构细节的无监督方案，同时兼顾效率与参数量以适配工业场景。

Method: 设计递归自编码器RcAE：对输入进行多步迭代重构，每步逐渐强化正常模式、压制异常；利用该重构序列，引入跨递归检测CRD，比较相邻/跨步重构不一致性以显著化异常（兼顾细微与大尺度）；再接入细节保留网络DPN，补偿重构时丢失的高频纹理，从而获得更精细的重建与差异图用于检测。

Result: 在广泛实验中，RcAE+CRD+DPN显著优于现有非扩散方法；在精度上与最新扩散模型相当，但仅用其约10%参数，推理更快。

Conclusion: 递归重构结合跨步不一致性与细节补偿，能在无监督工业异常检测中实现高精度、轻量且高效的检测，具有良好的工程实用性。

Abstract: Unsupervised industrial anomaly detection requires accurately identifying defects without labeled data. Traditional autoencoder-based methods often struggle with incomplete anomaly suppression and loss of fine details, as their single-pass decoding fails to effectively handle anomalies with varying severity and scale. We propose a recursive architecture for autoencoder (RcAE), which performs reconstruction iteratively to progressively suppress anomalies while refining normal structures. Unlike traditional single-pass models, this recursive design naturally produces a sequence of reconstructions, progressively exposing suppressed abnormal patterns. To leverage this reconstruction dynamics, we introduce a Cross Recursion Detection (CRD) module that tracks inconsistencies across recursion steps, enhancing detection of both subtle and large-scale anomalies. Additionally, we incorporate a Detail Preservation Network (DPN) to recover high-frequency textures typically lost during reconstruction. Extensive experiments demonstrate that our method significantly outperforms existing non-diffusion methods, and achieves performance on par with recent diffusion models with only 10% of their parameters and offering substantially faster inference. These results highlight the practicality and efficiency of our approach for real-world applications.

</details>


### [30] [Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context](https://arxiv.org/abs/2512.11293)
*Cuifeng Shen,Lumin Xu,Xingguo Zhu,Gengdai Liu*

Main category: cs.CV

TL;DR: 提出ARVAE，一种自回归视频自编码器：逐帧在前一帧条件下压缩/重建，以“流场(运动)”与“空间补偿”解耦表征，配合多阶段训练，在轻量模型与小数据下实现高质量重建，并在视频生成下游显示潜力。


<details>
  <summary>Details</summary>
Motivation: 现有视频自编码器将时空信息纠缠，难以保持时间一致性，影响重建与生成质量与效率；需要一种既高压缩率又保持时序连贯、可处理任意长度视频的表示与编码方式。

Method: 采用自回归框架：每帧在前一帧条件下编码/解码；提出时-空解耦潜变量——下采样光流(或流场)表征时间一致性与运动，空间相对补偿用于新出现内容与细节；编码器将当前与前一帧压缩为“时间运动+空间补充”，解码器以前一帧和潜变量重建当前帧；使用多阶段训练逐步优化。

Result: 在大量实验中，以极轻量模型和小规模训练数据达成更优重建质量与压缩效率；在视频生成评测中表现出较强下游适用性。

Conclusion: 通过自回归、时空解耦表示与多阶段训练，ARVAE在保持时间一致性与信息无损压缩方面优于现有方法，兼具高效与可扩展性，适合长度任意的视频处理与生成应用。

Abstract: Video autoencoders compress videos into compact latent representations for efficient reconstruction, playing a vital role in enhancing the quality and efficiency of video generation. However, existing video autoencoders often entangle spatial and temporal information, limiting their ability to capture temporal consistency and leading to suboptimal performance. To address this, we propose Autoregressive Video Autoencoder (ARVAE), which compresses and reconstructs each frame conditioned on its predecessor in an autoregressive manner, allowing flexible processing of videos with arbitrary lengths. ARVAE introduces a temporal-spatial decoupled representation that combines downsampled flow field for temporal coherence with spatial relative compensation for newly emerged content, achieving high compression efficiency without information loss. Specifically, the encoder compresses the current and previous frames into the temporal motion and spatial supplement, while the decoder reconstructs the original frame from the latent representations given the preceding frame. A multi-stage training strategy is employed to progressively optimize the model. Extensive experiments demonstrate that ARVAE achieves superior reconstruction quality with extremely lightweight models and small-scale training data. Moreover, evaluations on video generation tasks highlight its strong potential for downstream applications.

</details>


### [31] [Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining](https://arxiv.org/abs/2512.11296)
*Yasaman Hashem Pour,Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: 提出一种少样本视觉语言模型（VLM）方法，同时审查G-code文本与CNC机床HMI截图，提升错误与安全状态的检测，用于教学场景下的手工G-code验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的G-code验证主要聚焦代码语法/逻辑错误，忽略了CNC操作中对HMI状态与报错信息的依赖；LLM缺少视觉能力，难以利用HMI知识。因此需要能同时理解文本与图像的模型来更全面地验证与调试。

Method: 构建包含配对数据集：来自15-slant-PRO车床的G-code与对应HMI截图，含正确与含错样本。设计基于启发式先验知识的结构化JSON schema，作为少样本提示的骨架；选取若干含错/无错的G-code与HMI实例作为few-shot示例，引导VLM在统一schema的“槽位”上进行判断。与零样本VLM在多种G-code与HMI错误场景下进行对比评测，度量为per-slot accuracy。

Result: 少样本提示显著提高了VLM对HMI错误及其与G-code不一致之处的检测能力，相较零样本方案在多场景下取得更高的按槽位准确率，实现更全面的调试与验证。

Conclusion: 少样本VLM结合结构化schema与配对G-code/HMI输入，适用于CNC教学中手工G-code的综合验证，能更有效发现HMI相关错误与与代码的偏差，提升安全与调试效率。

Abstract: Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.

</details>


### [32] [MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction](https://arxiv.org/abs/2512.11301)
*Bate Li,Houqiang Zhong,Zhengxue Cheng,Qiang Hu,Qiang Wang,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 提出MultiEgo：首个用于4D动态场景重建的多视角第一人称数据集，涵盖五类社交互动场景，提供五路AR眼镜自带视角视频，硬件同步至亚毫秒并带准确位姿，验证其对自由视点视频有效。


<details>
  <summary>Details</summary>
Motivation: 现有数据集要么是静态多视角，要么是单一第一人称视角，缺乏支持多视角第一人称的动态场景重建的数据，限制了全息社交互动记录与FVV等应用的研究进展。

Method: 构建MultiEgo数据集：搭建硬件采集系统与处理流水线；由参与者佩戴AR眼镜获取五路真实第一人称视频；实现跨视角亚毫秒级时间同步；提供精确的位姿标注；覆盖会议、表演、演讲等五种典型社交互动场景。

Result: 通过实验验证数据集在自由视点视频（FVV）应用中的实用性与有效性，展示MultiEgo能支持多视角第一人称动态场景重建方法的训练与评测。

Conclusion: MultiEgo作为首个多视角第一人称4D动态重建数据集，填补了领域空白，凭借高精度同步与位姿标注，为推进多视角第一人称动态场景重建与FVV研究提供了基础资源。

Abstract: Multi-view egocentric dynamic scene reconstruction holds significant research value for applications in holographic documentation of social interactions. However, existing reconstruction datasets focus on static multi-view or single-egocentric view setups, lacking multi-view egocentric datasets for dynamic scene reconstruction. Therefore, we present MultiEgo, the first multi-view egocentric dataset for 4D dynamic scene reconstruction. The dataset comprises five canonical social interaction scenes: meetings, performances, and a presentation. Each scene provides five authentic egocentric videos captured by participants wearing AR glasses. We design a hardware-based data acquisition system and processing pipeline, achieving sub-millisecond temporal synchronization across views, coupled with accurate pose annotations. Experiment validation demonstrates the practical utility and effectiveness of our dataset for free-viewpoint video (FVV) applications, establishing MultiEgo as a foundational resource for advancing multi-view egocentric dynamic scene reconstruction research.

</details>


### [33] [SATMapTR: Satellite Image Enhanced Online HD Map Construction](https://arxiv.org/abs/2512.11319)
*Bingyuan Huang,Guanyi Zhao,Qian Xu,Yang Lou,Yung-Hui Li,Jianping Wang*

Main category: cs.CV

TL;DR: 提出SATMapTR：利用门控特征精炼与几何感知融合，把受遮挡与阴影影响的卫星图像与车载BEV特征高鲁棒融合，显著提升在线HD地图构建精度与范围。


<details>
  <summary>Details</summary>
Motivation: 在线HD地图需在多遮挡、传感器能力受限条件下构建，车载视角数据易缺失/噪声。卫星图像可补充但受阴影和植被/建筑遮蔽且与BEV存在视角与无关区域干扰，现有简单特征提取与融合方法难以有效利用。

Method: SATMapTR含两大核心模块：1) 门控特征精炼（Gated Feature Refinement）：将高层语义与低层结构线索结合，抑制阴影/遮挡等低质卫星特征，提取高信噪比地图相关表示；2) 几何感知融合（Geometry-aware Fusion）：在网格到网格层面按几何一致性对齐并融合卫星与BEV特征，过滤无关区域与低质量输入，提升一致性与鲁棒性。

Result: 在nuScenes上mAP=73.8，较现有卫星增强模型最高提升14.2 mAP；在恶劣天气与传感器失效时mAP退化更小；在扩展感知范围下mAP约提升近3倍。

Conclusion: 通过门控精炼与几何一致的网格级融合，SATMapTR有效利用受干扰的卫星图像，显著提升在线HD地图构建的精度、鲁棒性与远距性能。

Abstract: High-definition (HD) maps are evolving from pre-annotated to real-time construction to better support autonomous driving in diverse scenarios. However, this process is hindered by low-quality input data caused by onboard sensors limited capability and frequent occlusions, leading to incomplete, noisy, or missing data, and thus reduced mapping accuracy and robustness. Recent efforts have introduced satellite images as auxiliary input, offering a stable, wide-area view to complement the limited ego perspective. However, satellite images in Bird's Eye View are often degraded by shadows and occlusions from vegetation and buildings. Prior methods using basic feature extraction and fusion remain ineffective. To address these challenges, we propose SATMapTR, a novel online map construction model that effectively fuses satellite image through two key components: (1) a gated feature refinement module that adaptively filters satellite image features by integrating high-level semantics with low-level structural cues to extract high signal-to-noise ratio map-relevant representations; and (2) a geometry-aware fusion module that consistently fuse satellite and BEV features at a grid-to-grid level, minimizing interference from irrelevant regions and low-quality inputs. Experimental results on the nuScenes dataset show that SATMapTR achieves the highest mean average precision (mAP) of 73.8, outperforming state-of-the-art satellite-enhanced models by up to 14.2 mAP. It also shows lower mAP degradation under adverse weather and sensor failures, and achieves nearly 3 times higher mAP at extended perception ranges.

</details>


### [34] [KeyframeFace: From Text to Expressive Facial Keyframes](https://arxiv.org/abs/2512.11321)
*Jingchao Wu,Zejian Kang,Haibo Liu,Yuanchen Fei,Xiangru Huang*

Main category: cs.CV

TL;DR: 提出KeyframeFace大规模多模态数据集与首个利用LLM先验的文本到3D面部动画框架，实现可解释、关键帧引导、具上下文意识的高保真表情合成。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦语音驱动或无结构的表情序列，缺乏文本语义锚定与时间结构，难以生成细粒度且可解释的富表现力3D面部动画。

Method: 1) 构建KeyframeFace：包含2100段富表现力脚本、单目视频、逐帧ARKit系数、情景背景、复杂情绪、人工关键帧，以及基于LLM/MLLM的多视角标注（由图像与ARKit系数生成）。2) 提出文本到动画框架：显式利用LLM的语义理解，将文本解析为可解释的关键帧与表情控制信号，并与ARKit系数空间对齐，实现关键帧级监督与语义到运动的映射。

Result: 数据集与方法共同实现高保真、可解释、关键帧驱动的表情动画合成，在语义一致性、时序结构与表现力方面优于现有无结构或语音驱动方法（摘要未给出具体数值）。

Conclusion: KeyframeFace及其LLM驱动框架为文本到面部动画提供了新的基础设施与范式，强调关键帧监督与ARKit可解释空间的结合，支持上下文感知与高质量表达生成，并开源代码与数据。

Abstract: Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.

</details>


### [35] [MLLM Machine Unlearning via Visual Knowledge Distillation](https://arxiv.org/abs/2512.11325)
*Yuhang Wang,Zhenxing Niu,Haoxuan Ji,Guangyu He,Haichang Gao,Gang Hua*

Main category: cs.CV

TL;DR: 提出一种面向多模态大模型（MLLM）的可选择“遗忘”视觉知识的方法：通过视觉知识蒸馏（VKD）在中间表征层进行监督，仅微调视觉组件，高效擦除目标视觉知识，同时保留文本能力，效果和效率均优于现有方法，并首次评测对“再学习”攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘多集中于纯文本LLM，MLLM的遗忘仍初期；输出层监督往往牺牲实用性、效率低且难以精准定位被擦除的视觉知识。需要一种能区分视觉与文本知识、选择性删除视觉目标且保持模型整体能力的高效方案。

Method: 基于对MLLM内部机制的观察，显式解耦视觉与文本知识：仅微调视觉分支/模块；提出视觉知识蒸馏（VKD），使用模型中间视觉表示作为监督信号，针对目标视觉知识进行“反向蒸馏/对比”以去除相关表示，同时对非目标知识保持或对齐；避免依赖输出级别标签，从表示层进行细粒度控制。

Result: 在多项基准与任务上，相比SOTA遗忘方法实现更强的遗忘效果（更彻底、对目标触发更不敏感）与更好的保留性能（文本与通用视觉-语言能力降幅更小），训练开销显著降低（仅调视觉部件）；此外，首次进行对“再学习”攻击评测，表现出更强鲁棒性。

Conclusion: 通过中间表征级的视觉知识蒸馏与视觉-文本知识解耦，可高效且有选择地从MLLM中擦除目标视觉知识，同时保持文本能力与总体实用性；方法在效果、效率及对再学习攻击的鲁棒性上均优于现有方案。

Abstract: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.

</details>


### [36] [Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene](https://arxiv.org/abs/2512.11327)
*Junqiao Wang,Yuanfei Huang,Hua Huang*

Main category: cs.CV

TL;DR: 提出一个物理先验的视频炫光(镜头光斑)合成管线与去除网络，并配套首个视频炫光数据集；通过无对齐的时空建模抑制炫光，提升一致性与还原质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频中的炫光由强光源引起，具有与光源、场景独立的复杂运动，导致现有主要针对图像的方法在视频上出现闪烁与伪影；缺乏面向视频的合成与去除方法和系统数据集，限制了研究与评测。

Method: 1) 物理启发的动态炫光合成：用光流模拟光源运动，并分别建模散射型与反射型炫光的时间行为；2) 视频去炫光网络：空间上用注意力抑制炫光区域；时间上引入基于 Mamba 的长程时空建模组件，构建对运动独立的时空表征，从而无需多帧对齐，减少炫光与场景的时间混叠；3) 数据集：大规模合成成对视频+来自互联网的真实视频用于泛化评估。

Result: 在真实与合成视频上，相比现有视频复原与图像去炫光方法，所提方法在去除动态炫光、保持光源完整性与场景时空一致性方面均取得更优表现（实验细节未给出但称“持续领先”）。

Conclusion: 物理先验的动态合成与无对齐的长程时空建模相结合，可有效解决视频炫光的时空不一致与伪影问题；所建数据集促进评测与研究，方法在多种场景下具备更强泛化与稳定性。

Abstract: Lens flare is a degradation phenomenon caused by strong light sources. Existing researches on flare removal have mainly focused on images, while the spatiotemporal characteristics of video flare remain largely unexplored. Video flare synthesis and removal pose significantly greater challenges than in image, owing to the complex and mutually independent motion of flare, light sources, and scene content. This motion independence further affects restoration performance, often resulting in flicker and artifacts. To address this issue, we propose a physics-informed dynamic flare synthesis pipeline, which simulates light source motion using optical flow and models the temporal behaviors of both scattering and reflective flares. Meanwhile, we design a video flare removal network that employs an attention module to spatially suppress flare regions and incorporates a Mamba-based temporal modeling component to capture long range spatio-temporal dependencies. This motion-independent spatiotemporal representation effectively eliminates the need for multi-frame alignment, alleviating temporal aliasing between flares and scene content and thereby improving video flare removal performance. Building upon this, we construct the first video flare dataset to comprehensively evaluate our method, which includes a large set of synthetic paired videos and additional real-world videos collected from the Internet to assess generalization capability. Extensive experiments demonstrate that our method consistently outperforms existing video-based restoration and image-based flare removal methods on both real and synthetic videos, effectively removing dynamic flares while preserving light source integrity and maintaining spatiotemporal consistency of scene.

</details>


### [37] [FreqDINO: Frequency-Guided Adaptation for Generalized Boundary-Aware Ultrasound Image Segmentation](https://arxiv.org/abs/2512.11335)
*Yixuan Zhang,Qing Xu,Yue Li,Xiangjian He,Qian Zhang,Mainul Haque,Rong Qu,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: 提出FreqDINO：在DINOv3基础上引入频域引导与边界建模，以提升超声图像分割的边界感知与结构一致性，显著优于现有方法并具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: DINOv3虽在医学分割上表现出色，但其在自然图像上预训练，对超声图像特有的斑点噪声与边界退化不敏感，导致边界识别与结构一致性不足。

Method: 1) 多尺度频率提取与对齐（MFEA）：从特征中分离低频结构与多尺度高频边界细节，并通过可学习注意力进行跨频对齐融合；2) 频引导的边界细化（FGBR）：从高频组件学习边界原型，反哺并细化空间特征；3) 多任务边界引导解码器（MBGD）：联合预测语义分割与边界图，建立两者空间一致性约束。框架基于DINOv3表征做频域增强。

Result: 在多组超声分割数据上超过现有SOTA，表现出更强的边界定位、结构保持与跨数据集泛化能力。

Conclusion: 频域引导与边界原型建模能弥补DINOv3对超声特有退化的不敏感性，显著提升超声分割质量与泛化；代码已开源（GitHub: MingLang-FD/FreqDINO）。

Abstract: Ultrasound image segmentation is pivotal for clinical diagnosis, yet challenged by speckle noise and imaging artifacts. Recently, DINOv3 has shown remarkable promise in medical image segmentation with its powerful representation capabilities. However, DINOv3, pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation. To address this limitation, we propose FreqDINO, a frequency-guided segmentation framework that enhances boundary perception and structural consistency. Specifically, we devise a Multi-scale Frequency Extraction and Alignment (MFEA) strategy to separate low-frequency structures and multi-scale high-frequency boundary details, and align them via learnable attention. We also introduce a Frequency-Guided Boundary Refinement (FGBR) module that extracts boundary prototypes from high-frequency components and refines spatial features. Furthermore, we design a Multi-task Boundary-Guided Decoder (MBGD) to ensure spatial coherence between boundary and semantic predictions. Extensive experiments demonstrate that FreqDINO surpasses state-of-the-art methods with superior achieves remarkable generalization capability. The code is at https://github.com/MingLang-FD/FreqDINO.

</details>


### [38] [UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models](https://arxiv.org/abs/2512.11336)
*Hewen Pan,Cong Wei,Dashuang Liang,Zepeng Huang,Pengfei Gao,Ziqi Zhou,Lulu Xue,Pengfei Yan,Xiaoming Wei,Minghui Li,Shengshan Hu*

Main category: cs.CV

TL;DR: UFVideo 提出首个具备“统一多粒度协同理解”的视频多模态大模型，能在同一模型内完成全局问答、时间定位与像素级掩膜等任务，并在自建基准 UFVideo-Bench 与9个公开数据集上优于现有方法（含对比GPT-4o）。


<details>
  <summary>Details</summary>
Motivation: 现有 Video LLM 多聚焦于单一或专门化任务，缺乏在全局、时间、像素等多尺度上统一处理的能力，难以实现全面细致的视频理解与交互式推理评估。

Method: 提出统一的视觉-语言引导对齐框架：对不同任务的视觉与文本输入进行动态编码与对齐，在单一模型中输出三类结果——文本回答（全局理解）、时间片段定位（时间尺度）与像素级掩膜（空间/像素尺度）。并构建包含跨尺度协作任务的UFVideo-Bench，用于评测多粒度协同理解。

Result: 在UFVideo-Bench上展示对多粒度协作任务的灵活性与性能优势，并在9个公共基准的多种视频理解任务上取得领先或显著竞争力；论文声称对比GPT-4o具有优势。

Conclusion: 统一多粒度协同理解是推进视频LLM全面感知与交互能力的关键路径；UFVideo证明单一模型可同时处理全局问答、时间定位与像素级指认，提供了评测基准与实践经验，为后续视频LLM研究提供方向。

Abstract: With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.

</details>


### [39] [Task-Specific Distance Correlation Matching for Few-Shot Action Recognition](https://arxiv.org/abs/2512.11340)
*Fei Long,Yao Zhang,Jiaming Lv,Jiangtao Xie,Peihua Li*

Main category: cs.CV

TL;DR: 提出TS-FSAR用于小样本动作识别，通过更强的匹配度量和高效适配CLIP，在五个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有小样本动作识别两大痛点：1）基于余弦相似的集合匹配只捕捉线性帧间关系，忽略非线性依赖与任务特定线索；2）为高效适配CLIP而引入的侧路/跳连微调层在小数据下难以优化，影响性能与稳定性。

Method: TS-FSAR包含三部分：1）Ladder Side Network（LSN）：一种视觉侧路网络，以“梯形/逐层侧接”方式对CLIP进行参数高效微调；2）Task-Specific Distance Correlation Matching（TS-DCM）：基于α-distance correlation建模线性与非线性帧间依赖，并借助任务原型实现任务特定的集合匹配；3）GLAC模块：用已适配且冻结的CLIP来“引导/正则化”LSN训练，提高在少监督下对α-distance correlation的估计质量与稳定性。

Result: 在五个主流基准上取得SOTA或超越SOTA的性能，表明所提匹配度量与适配策略有效且泛化良好。

Conclusion: 联合高效的CLIP侧路微调（LSN）、任务特定的α-距离相关匹配（TS-DCM）以及由冻结适配CLIP引导的正则（GLAC），能够更好刻画非线性时序依赖并缓解小样本训练难题，从而显著提升FSAR表现。

Abstract: Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $α$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $α$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.

</details>


### [40] [Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture](https://arxiv.org/abs/2512.11350)
*Tanu Singh,Pranamesh Chakraborty,Long T. Truong*

Main category: cs.CV

TL;DR: 论文针对交通事故检测任务，基于Transformer并融合运动线索（光流）提出模型与数据集，最佳方案为RGB特征与光流拼接，准确率88.3%，并与多种VLM进行对比验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统事故检测方法在跨场景泛化与时空建模上不足；现有Transformer方法受限于数据集规模与多样性，且忽视运动线索，难以鲁棒检测多样化交通事故。

Method: 1) 构建一个覆盖多环境、事故类型与上下文变化的平衡大规模数据集；2) 提出基于预提取空间视频特征的Transformer架构：卷积层提取帧内局部相关模式，Transformer捕获跨帧时序依赖；3) 系统评估多种运动信息融合策略，最终采用RGB特征与光流拼接输入；4) 与VLM（GPT、Gemini、LLaVA‑NeXT‑Video）进行对比评测。

Result: 在多种输入策略中，RGB+光流拼接取得最高准确率88.3%；整体性能优于所对比的VLM方案，显示所提方法在事故检测任务上的有效性。

Conclusion: 高质量、多样化数据集结合卷积+Transformer的时空建模，并显式引入运动线索可显著提升事故检测准确率与泛化能力；RGB与光流的融合是当前最有效策略，方法较通用VLM更适合该特定任务。

Abstract: Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.

</details>


### [41] [A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection](https://arxiv.org/abs/2512.11354)
*Qinghan Hu,Haijiang Zhu,Na Sun,Lei Chen,Zhengqiang Fan,Zhiqing Li*

Main category: cs.CV

TL;DR: 论文提出一套用于水下管道检测的多模态结构光3D成像系统，融合视觉与声学传感，包含快速畸变校正、基于因子图的跨传感器外参优化、多模3D成像、融合与自适应EKF位姿估计，以及结合边缘检测与ICP的点云配准（ED-ICP）。实验显示在不同模式、速度、深度下具备高精度、适应性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下管道易腐蚀且人工巡检效率低、风险高。现有水下成像方法在空间细节恢复、跨传感器标定与运动干扰下的鲁棒性方面存在不足，迫切需要一个可实时、高精度、适应复杂环境的成像与检测系统。

Method: 构建多源信息融合的UW-SLD系统：1) 提出快速畸变校正（FDC）以高效矫正水下图像；2) 引入基于因子图的参数优化估计结构光-声学传感器的外参；3) 设计多模式3D成像策略匹配不同管道几何；4) 利用多源融合与自适应EKF实现稳定位姿与高精度测量；5) 提出ED-ICP，将管道边缘检测网络与增强ICP配准结合，提升运动变化下的点云配准与缺陷重建质量。

Result: 在多种作业模式、运行速度与水深条件下进行大量实验，系统在精度、适应性与鲁棒性方面优于对比方案，实现对管道缺陷的高保真重建。

Conclusion: 多模态结构光3D成像结合因子图外参优化、AEKF融合与ED-ICP，可在复杂水下环境中实现稳定高精度的管道检测与缺陷重建，为自主化水下管道巡检奠定基础。

Abstract: Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.

</details>


### [42] [Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video](https://arxiv.org/abs/2512.11356)
*Meng-Li Shih,Ying-Huan Chen,Yu-Lun Liu,Brian Curless*

Main category: cs.CV

TL;DR: 提出一条面向随手拍单目RGB视频的全自动动态场景重建流水线，在不改动表示的前提下强化Dynamic Gaussian Splatting的先验，达到更清晰几何与更一致运动，并在渲染质量上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目动态场景重建在细结构分割/跟踪、深度一致性与运动连贯性上存在不足，易产生“浮点/漂浮物”、细节丢失与运动不稳定。作者希望在不重新设计场景表示的情况下，通过更强的先验与训练目标提升重建质量与可视化效果。

Method: 1) 视频分割+极线几何误差图得到贴合细结构的对象级掩码；用其引导：a) 对象深度损失以锐化并一致化视频深度；b) 基于骨架的采样与掩码引导的重识别，获得可靠全面的2D轨迹。2) 在重建阶段加入两项损失：a) 虚拟视图深度损失，抑制漂浮伪影；b) 脚手架投影损失，将运动节点与轨迹绑定，保留细致几何与连贯运动。整体在Dynamic Gaussian Splatting框架上实现的端到端自动流程。

Result: 在单目动态场景重建基准上超过以往方法，呈现更少浮点、更清晰细节和更连贯的运动，渲染质量显著提升。

Conclusion: 强化先验与设计针对性的损失（而非更换表示）即可显著提升单目动态场景重建的几何精度与运动一致性；该管线在可见质量与定量指标上均优于现有方法。

Abstract: We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings

</details>


### [43] [Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts](https://arxiv.org/abs/2512.11360)
*Mohammad Sadegh Gholizadeh,Amir Arsalan Rezapour,Hamidreza Shayegh,Ehsan Pazouki*

Main category: cs.CV

TL;DR: 利用迁移学习初始化的Faster R-CNN，用无人机高分辨率影像检测稻田幼苗；构建大规模UAV数据集并跨时间测试，证明迁移学习加速收敛且在成像条件变化下保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要可靠的无人机作物检测，但目标尺度小、背景复杂、成像条件多变使检测困难，尤其是稻田幼苗等微小目标。在缺乏标注数据与域偏移存在的情况下，需要一种既高效又具泛化能力的方法。

Method: 采用Faster R-CNN作为检测器，并通过迁移学习进行权重初始化；构建并标注较大规模的稻田幼苗UAV数据集；在不同时间获取的三套测试集上进行严格评测，以考察模型在不同成像条件（时间、环境）下的泛化与鲁棒性。

Result: 实验表明：迁移学习可显著加速模型收敛，并在不同时间采集、存在域偏移的测试集上保持一致、稳定的检测性能。

Conclusion: 在农业场景下，基于迁移学习的Faster R-CNN对微小目标（稻苗）检测有效且鲁棒；构建的大规模UAV数据和跨时间评测验证了其在域变化下的可靠性，可为规模化精准农业提供可行的检测方案。

Abstract: Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.

</details>


### [44] [Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection](https://arxiv.org/abs/2512.11369)
*Kuan Wang,Yanjun Qin,Mengge Lu,Liejun Wang,Xiaoming Tao*

Main category: cs.CV

TL;DR: 提出ARNet-v2用于伪装目标检测，通过通道信息交互与先验引导的协同解码，兼顾区域与边界，取得SOTA并可迁移至显著性检测及多种下游分割任务。


<details>
  <summary>Details</summary>
Motivation: 现有COD方法在解码阶段存在两大痛点：同层特征的跨通道信息交互不足，导致表达力弱；边界与区域难以协同建模，致使语义歧义与边界不精确。

Method: 1) 通道信息交互模块（CIIM）：在通道维度引入横纵一体化机制，重组并交互通道特征以捕获互补信息。2) 先验引导的协同解码：通过边界提取（BE）与区域提取（RE）生成边界先验与定位图，利用混合注意力联合校准解码特征。3) 多尺度增强（MSE）：丰富上下文表征。整体形成可在多尺度上同时关注区域完整性与边界锐度的解码架构。

Result: 在四个COD基准上取得SOTA表现；将模型迁移到SOD并在息肉分割、透明体检测、工业与道路缺陷检测等下游任务上展现良好适应性。

Conclusion: 通道级交互与先验引导的协同解码能有效缓解语义歧义与边界不准问题，提升COD的区域与边界重建质量，并具备良好的跨任务泛化能力。

Abstract: Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.

</details>


### [45] [Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty](https://arxiv.org/abs/2512.11373)
*Arnold Brosch,Abdelrahman Eldesokey,Michael Felsberg,Kira Maag*

Main category: cs.CV

TL;DR: 提出一种用于开放世界语义分割的“证据分割”框架，核心用Wasserstein损失结合KL正则和Dice结构一致性，较基于不确定性的办法在OOD目标分割上更好。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割网络只针对预定义类别训练，遇到未知（OOD）目标会失败；在自动驾驶等安全关键场景需要能识别并分割这些未知目标。

Method: 构建证据分割（evidence segmentation）框架：以符合概率单纯形几何的Wasserstein损失衡量分布距离；再加入KL散度正则化与Dice结构一致性项，联合优化以提升对OOD的区分与分割质量。

Result: 相较于以不确定性为基础的OOD分割方法，该方法在OOD识别与分割性能上取得提升（摘要未给出具体指标）。

Conclusion: 结合Wasserstein损失、KL正则与Dice项的证据分割框架能更有效地在开放世界中分割未知对象，对安全关键应用更为可靠。

Abstract: Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.

</details>


### [46] [The N-Body Problem: Parallel Execution from Single-Person Egocentric Video](https://arxiv.org/abs/2512.11393)
*Zhifan Zhu,Yifei Huang,Yoichi Sato,Dima Damen*

Main category: cs.CV

TL;DR: 提出“多体并行化问题”：从单个第一视角视频推断如何由N个人并行完成同一组任务，并在保证物理可行性的前提下最大化加速比。方法通过结构化提示引导VLM进行3D空间、物体使用与因果约束推理，设计性能与可行性指标，并在EPIC-Kitchens/HD-EPIC上显著提升覆盖率并减少冲突。


<details>
  <summary>Details</summary>
Motivation: 人类能够直觉地将复杂活动并行化，但现有模型从单人示范中学习多人并行执行仍缺乏系统定义与评测，且简单分配常导致物理与因果不一致（空间碰撞、物体冲突、时序违背）。需要形式化问题、建立度量，并开发能在现实约束下生成可行并行计划的方法。

Method: 1) 形式化“N-Body Problem”：给定一段自我中心视频及其任务片段，推断N个体如何并行执行以最大化速度与覆盖，同时满足可行性约束。2) 设计指标：性能（加速比、任务/动作覆盖）与可行性（空间碰撞、物体冲突、因果/时间依赖约束）。3) 提出结构化提示策略：分阶段引导VLM（Gemini 2.5 Pro）对3D环境布局、物体占用与共享、时序/因果依赖进行显式推理，产出可行的并行日程与分配。

Result: 在EPIC-Kitchens与HD-EPIC的100段视频上，当N=2时，相比基线提示，动作覆盖提升约45%；同时空间碰撞、物体冲突、因果冲突分别降低约55%、45%、55%。

Conclusion: 通过将并行化问题形式化并利用结构化提示驱动VLM进行空间-物体-因果联合推理，可从单人视频推导出多人的可行并行执行计划，兼顾速度与物理可行性，显著优于简单提示基线。

Abstract: Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.

</details>


### [47] [FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing](https://arxiv.org/abs/2512.11395)
*Yilei Jiang,Zhen Wang,Yanghao Wang,Jun Yu,Yueting Zhuang,Jun Xiao,Long Chen*

Main category: cs.CV

TL;DR: 提出FlowDC：将复杂文本图像编辑分解为并行的子编辑并在流匹配过程中叠加，同时分解并衰减与编辑位移正交的速度以更好保留源结构；在新构建的Complex-PIE-Bench等基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 简单（单目标）文本编辑已显著进步，但现实需求常涉及多目标复杂编辑。现有方案要么单轮长文本导致跟随困难与语义漂移，要么多轮累积误差与不一致，难以兼顾语义对齐与源图一致性。

Method: 1) 提出FlowDC：将复杂编辑拆分为多个子编辑效果，并在编辑过程中并行叠加（superpose），实现解耦与并行控制；2) 观察到与编辑位移正交的速度分量会破坏源结构，因而对流匹配中的速度进行分解，并对正交分量施加衰减，以提升源一致性；3) 构建复杂编辑评测集Complex-PIE-Bench，用于系统评估。

Result: 在两个基准（包含新提出的Complex-PIE-Bench）上，FlowDC在复杂编辑任务中优于现有单轮与多轮方法；并通过消融实验验证并行叠加与速度正交衰减模块的有效性。

Conclusion: 并行解耦的子编辑叠加与速度正交分量衰减有效缓解了复杂编辑中的语义对齐与源一致性的权衡，FlowDC实现了更稳定、更一致的复杂文本图像编辑，并在新基准上取得SOTA表现。

Abstract: With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.

</details>


### [48] [Collaborative Reconstruction and Repair for Multi-class Industrial Anomaly Detection](https://arxiv.org/abs/2512.11401)
*Qishan Wang,Haofeng Wang,Shuyong Gao,Jia Guo,Li Xiong,Jiaqi Li,Dengxuan Bai,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 提出CRR框架，通过“协同重建与修复”替代传统重建，缓解重建网络的同态映射问题，实现多类别工业异常检测的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多类别工业异常检测需要统一模型而非每类单独训练，但重建式方法在开放集场景易学到恒等映射，无法区分正常与异常，且多类建模受内存和泛化性限制。

Method: 1) 将重建目标改为“正常重建+合成异常修复”：优化解码器使其对正常样本重建、对合成异常进行修复，使解码器输出在异常区域与编码器特征明显不同、在正常区域相似。2) 在特征层进行随机mask，迫使解码器利用局部上下文、避免抄写输入。3) 训练一个由合成异常掩码监督的分割网络，融合编码器与解码器特征，减少二者表征差异带来的检测误差、提升定位。

Result: 在多个工业数据集上，CRR有效缓解恒等映射问题，并在多类别异常检测上达到SOTA表现（文中称“广泛实验”验证）。

Conclusion: 将重建范式转为修复范式并结合特征mask与分割监督，可在统一框架下实现更稳健的多类工业异常检测与更佳定位性能。

Abstract: Industrial anomaly detection is a challenging open-set task that aims to identify unknown anomalous patterns deviating from normal data distribution. To avoid the significant memory consumption and limited generalizability brought by building separate models per class, we focus on developing a unified framework for multi-class anomaly detection. However, under this challenging setting, conventional reconstruction-based networks often suffer from an identity mapping problem, where they directly replicate input features regardless of whether they are normal or anomalous, resulting in detection failures. To address this issue, this study proposes a novel framework termed Collaborative Reconstruction and Repair (CRR), which transforms the reconstruction to repairation. First, we optimize the decoder to reconstruct normal samples while repairing synthesized anomalies. Consequently, it generates distinct representations for anomalous regions and similar representations for normal areas compared to the encoder's output. Second, we implement feature-level random masking to ensure that the representations from decoder contain sufficient local information. Finally, to minimize detection errors arising from the discrepancies between feature representations from the encoder and decoder, we train a segmentation network supervised by synthetic anomaly masks, thereby enhancing localization performance. Extensive experiments on industrial datasets that CRR effectively mitigates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection.

</details>


### [49] [JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion](https://arxiv.org/abs/2512.11423)
*Chaochao Li,Ruikui Wang,Liangbo Zhou,Jinheng Feng,Huaishao Luo,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: JoyAvatar提出一种音频驱动的自回归头像生成模型，通过三项关键技术（PSB、MCI、URCR）在单卡实时（16 FPS）实现高视觉质量、时序一致、可无限时长的视频生成，缓解自回归误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT的音频驱动头像生成在计算开销大、难以生成长视频等方面受限；块级自回归扩散虽能延长时长，但会造成误差累积与画质劣化，阻碍落地与实时应用。

Method: 提出JoyAvatar（约13亿参数的因果模型），含三项模块：1) Progressive Step Bootstrapping（PSB）：对起始帧分配更多去噪步数，稳定初始分布、降低后续误差；2) Motion Condition Injection（MCI）：将带噪的前帧作为运动条件注入，强化跨帧时序一致性；3) Unbounded RoPE via Cache-Resetting（URCR）：通过动态位置编码与缓存重置，解除位置长度限制，实现无限时长生成。

Result: 在单GPU上可达16 FPS，生成视觉质量、时序一致性与唇形同步达到有竞争力的水平；可进行实时推理并支持无限长度视频生成。

Conclusion: JoyAvatar通过PSB、MCI、URCR协同缓解自回归误差累积并提升跨帧一致性，在保持实时性的同时支持无限时长生成，为音频驱动头像的实际应用提供可行方案。

Abstract: Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.

</details>


### [50] [Flowception: Temporally Expansive Flow Matching for Video Generation](https://arxiv.org/abs/2512.11438)
*Tariq Berrada Ifriqi,John Nguyen,Karteek Alahari,Jakob Verbeek,Ricky T. Q. Chen*

Main category: cs.CV

TL;DR: Flowception 是一种非自回归、可变长度的视频生成框架，通过在采样中交替进行离散“插帧”和连续“去噪”来建模视频，既减少长序列漂移，又显著降低训练计算量，并可联合学习视频长度；在FVD与VBench上优于自回归与全序列流模型，并可统一支持图生视频与视频插帧等任务。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成两端受限：自回归方法易累积误差且长程上下文昂贵；全序列流/扩散在长视频上计算量高且难以适配局部注意力，同时固定长度限制灵活性。需要一种既能高效处理长时依赖、又能降低训练FLOPs并支持可变长度的统一生成范式。

Method: 提出Flowception，学习一条在时间上交替进行操作的“概率路径”：1) 离散帧插入——在已有关键帧之间插入新帧，作为一种高效的时间压缩与上下文管理机制；2) 连续帧去噪——对当前帧进行连续化的去噪细化。训练上采用分段的非自回归流程，显著降低FLOPs，并与局部注意力兼容；采样时可动态决定插入步数，从而联合学习视频的长度与内容。

Result: 在定量评测上，FVD与VBench指标均优于自回归与全序列流式基线；训练计算量约降低至三分之一。定性结果显示更少的漂移与更稳定的长程一致性。

Conclusion: Flowception通过“插帧+去噪”的交替流程，在效率、稳定性与灵活性上兼顾，缓解长序列误差累积，并天然兼容多任务（如图生视频、视频插帧），为可变长度视频生成提供了统一且高效的方案。

Abstract: We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.

</details>


### [51] [YawDD+: Frame-level Annotations for Accurate Yawn Prediction](https://arxiv.org/abs/2512.11446)
*Ahmed Mujtaba,Gleb Radchenko,Marc Masana,Radu Prodan*

Main category: cs.CV

TL;DR: 他们提出半自动标注管线修正YawDD数据的时间粗粒度噪声，使打哈欠检测/分类在边缘设备上更准更快：分类99.34%、检测mAP 95.69%，在Jetson Nano上达59.8 FPS。


<details>
  <summary>Details</summary>
Motivation: 打哈欠是疲劳驾驶的早期行为信号，但现有方法依赖视频级标注，时间粒度粗、标签噪声高，导致模型学到错误相关性和性能受限；需要更精确的帧级标注且成本可控。

Method: 构建半自动标注流水线：模型预标注+人类在环校验，给YawDD生成更干净的帧级数据集（YawDD+）。在此数据上训练主流轻量级架构：分类用MNasNet，检测用YOLOv11；对比视频级监督与帧级（清洗后）监督的效果与速度，并在Jetson Nano上基准测试。

Result: 用YawDD+训练使帧级分类准确率最高达99.34%，较视频级监督提升至多6%；检测mAP达95.69%，提升约5%。在NVIDIA Jetson Nano上实现最高59.8 FPS，无需服务端算力。

Conclusion: 仅通过提升数据质量（更精确帧级标注）即可显著提升打哈欠检测/分类的准确性与实时性，满足边缘侧部署需求，验证了数据层面改进对疲劳监测系统的关键作用。

Abstract: Driver fatigue remains a leading cause of road accidents, with 24\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\% and mAP by 5\% over video-level supervision, achieving 99.34\% classification accuracy and 95.69\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.

</details>


### [52] [Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation](https://arxiv.org/abs/2512.11458)
*Jingmin Zhu,Anqi Zhu,Hossein Rahmani,Jun Liu,Mohammed Bennamoun,Qiuhong Ke*

Main category: cs.CV

TL;DR: Skeleton-Cache 是一种无需训练的测试时自适应框架，用于骨架零样本动作识别，通过在推理时检索非参数化缓存中全局+局部骨架描述子，并用LLM生成的语义权重进行融合，从而提升对未见动作的泛化；在多数据集上稳定提升多种基线在ZSL/GZSL下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有骨架零样本动作识别在遇到分布偏移或未见类时泛化受限，传统方法依赖训练或对先验的强假设，测试时难以快速适应且可能需要训练数据或更新参数。需要一种在不额外训练、无训练数据访问的条件下，仍能动态适配未见动作的方案。

Method: 将推理重构为对“骨架缓存”的轻量检索：缓存中存放结构化的骨架表示，含全局与细粒度局部描述子；各描述子产生类概率。再利用大语言模型进行语义推理，为各类别分配描述子重要性权重，指导多粒度预测的加权融合。整个过程非参数化、无梯度更新，不依赖训练数据。

Result: 在 NTU RGB+D 60/120 与 PKU-MMD II 上，对多种 SZAR 骨干在零样本与广义零样本设置下均取得一致增益，显示该框架在不同模型与数据集上的有效性与普适性。

Conclusion: Skeleton-Cache 通过结构化骨架缓存检索与LLM引导的语义加权，实现了无需训练的测试时适配，显著提升对未见动作的识别能力，可作为通用插件增强SZAR模型泛化。

Abstract: We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.

</details>


### [53] [Exploring MLLM-Diffusion Information Transfer with MetaCanvas](https://arxiv.org/abs/2512.11464)
*Han Lin,Xichen Pan,Ziqi Huang,Ji Hou,Jialiang Wang,Weifeng Chen,Zecheng He,Felix Juefei-Xu,Junzhe Sun,Zhipeng Fan,Ali Thabet,Mohit Bansal,Chu Wang*

Main category: cs.CV

TL;DR: 提出MetaCanvas：让多模态大模型直接在空间/时空潜空间中进行推理与规划，并与扩散生成器紧耦合，从而显著提升精确布局与属性绑定的可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有多模态LLM在理解上很强（复杂布局、属性、知识推理），但在视觉生成中通常仅作为全局文本编码器参与扩散模型，导致其推理与规划能力被浪费，难以获得精确、结构化的控制。需要一种方法把LLM的认知能力“注入”到生成过程与潜空间操作中，以缩小理解与生成的能力鸿沟。

Method: 提出轻量框架MetaCanvas：将MLLM作为“潜空间规划器”，直接在空间与时空潜表示上进行推理与规划；与不同扩散骨干紧密接口，实现布局约束、属性绑定与时序一致性等控制。具体在三类扩散骨干上实现，并在多任务中以潜空间指令/草图/掩码等方式驱动生成或编辑。

Result: 在六类任务上（文生图、文/图生视频、图/视频编辑、in-context视频生成等）相较仅全局条件的基线方法稳定取得更好性能，表现为更精确的布局控制、更稳健的属性绑定与更强的推理型可控性。

Conclusion: 将MLLM视作潜空间规划器并与扩散模型深度耦合是缩小多模态理解与生成差距的有效方向；MetaCanvas验证了这一思路的可行性与通用性。

Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.

</details>


### [54] [DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation](https://arxiv.org/abs/2512.11465)
*Mohamed Abdelsamad,Michael Ulrich,Bin Yang,Miao Zhang,Yakov Miron,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出DOS：在可见点上蒸馏语义相关的软映射，并用Zipf-Sinkhorn引入幂律原型先验的3D点云自监督框架，显著提升分割与检测性能。


<details>
  <summary>Details</summary>
Motivation: 3D点云SSL受不规则几何、掩码重建走捷径、语义分布不均衡等问题限制；需要无标注下更稳健、可扩展的表示学习方法。

Method: 1) Distilling Observable Softmaps：仅在未被掩码的可观测点上蒸馏教师生成的语义相关“软映射”（softmap），避免从被掩码区域泄漏信息，相比离散原型分配提供更丰富监督。2) Zipfian prototypes：设定原型使用的幂律分布先验；3) Zipf-Sinkhorn：修改的Sinkhorn-Knopp分配算法，在训练中同时施加幂律先验并控制目标软映射的尖锐度。整体形成无标签的自蒸馏SSL框架。

Result: 在nuScenes、Waymo、SemanticKITTI、ScanNet、ScanNet200等多个基准上的语义分割与3D目标检测均优于SOTA，且无需额外数据或标注。

Conclusion: 在可观测点上进行软映射蒸馏并结合幂律原型先验，是学习稳健3D点云表示的有效且可扩展范式。

Abstract: Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.

</details>


### [55] [CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop](https://arxiv.org/abs/2512.11480)
*Weijian Ma,Shizhao Sun,Ruiyu Wang,Jiang Bian*

Main category: cs.CV

TL;DR: 提出CADMorph：在无三元组数据下，通过计划-生成-验证三阶段协调预训练P2S扩散与MPP掩码参数预测，实现几何驱动的参数化CAD编辑，兼顾结构保持、语义有效性与形状保真，并优于GPT-4o与专业基线。


<details>
  <summary>Details</summary>
Motivation: 几何形状调整需与底层参数化构造序列同步，但存在三难题：保持原序列结构、保证编辑语义有效、最大化目标形状保真；同时缺乏“（初始序列，编辑，目标形状）”的监督三元组数据。

Method: 提出迭代的“计划-生成-验证”框架CADMorph，编排两类预训练先验：1) 参数到形状的P2S潜空间扩散模型；2) MPP掩码参数预测模型。计划阶段利用P2S的跨注意力热图定位需修改的序列片段并生成编辑掩码；生成阶段用MPP在掩码处填充语义有效的参数化编辑；验证阶段再用P2S将候选序列嵌入形状潜空间，度量与目标形状距离并选最接近者。两模型均无需三元组数据训练。

Result: 在几何驱动的参数化编辑任务上，CADMorph在结构保持、语义有效性与形状保真方面均超过GPT-4o与专门CAD基线；还能支持迭代编辑与逆向工程增强等下游应用。

Conclusion: 利用预训练几何与设计先验，通过计划-生成-验证协同可在缺乏三元组数据下实现高质量的参数化CAD编辑，兼顾可编辑性与形状准确性，并具备良好的通用性与应用潜力。

Abstract: A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.

</details>


### [56] [VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing](https://arxiv.org/abs/2512.11490)
*Emanuel Sánchez Aimar,Gulnaz Zhambulova,Fahad Shahbaz Khan,Yonghao Xu,Michael Felsberg*

Main category: cs.CV

TL;DR: VLM2GeoVec 提出一个可指令跟随的单编码器视觉-语言模型，把图像、文本、框与地理坐标交错编码到同一向量空间，用对比学习训练；并发布RSMEB基准，覆盖从场景分类到区域级推理与语义地理检索等任务；在区域级与语义地理检索上大幅超越双编码与专用方法，同时保持常规任务的领先或持平表现。


<details>
  <summary>Details</summary>
Motivation: 遥感图像与自然图像差异显著：俯视视角、超高分辨率、尺度变化大、小目标密集，需要同时具备区域级空间推理与整体场景理解。现有方法割裂：双编码检索可扩展但无法交错模态；生成式助手能做区域级解释但缺乏可扩展检索。需要一种既能交错多模态输入又可扩展检索的统一模型与评测。

Method: 提出VLM2GeoVec：单编码器架构，将图像、文本、边界框与地理坐标按指令交错为单一联合嵌入，采用对比学习训练，消除多阶段与任务特定模块。并构建RSMEB基准，涵盖场景分类、跨模态检索、组合式检索、VQA、视觉指代表达与区域级推理、语义地理检索等，评估模型通用性。

Result: 在RSMEB上，区域-字幕检索P@1=26.6%（较双编码基线+25个百分点），指代表达检索P@1=32.5%（+19个百分点），语义地理定位检索P@1=17.8%（为此前SOTA的3倍以上）；在常规任务如场景分类与跨模态检索上匹配或超过专用基线。

Conclusion: VLM2GeoVec通过单一对比学习嵌入统一了可扩展检索与区域级空间推理，能对交错多模态输入进行连贯分析；RSMEB为遥感嵌入任务提供系统评测。代码、模型与数据将开源。

Abstract: Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.

</details>


### [57] [TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition](https://arxiv.org/abs/2512.11503)
*Yanan Liu,Jun Liu,Hao Zhang,Dan Xu,Hossein Rahmani,Mohammed Bennamoun,Qiuhong Ke*

Main category: cs.CV

TL;DR: 提出TSkel-Mamba：结合空间Transformer与时间Mamba的骨架动作识别框架，并加入TDM与MTI（多尺度循环算子）以建模跨通道时间交互，SOTA且高效。


<details>
  <summary>Details</summary>
Motivation: 骨架动作识别需要同时捕获空间骨架关系与时间动态。Mamba在长序列时间建模上强，但其按通道独立的SSM限制了跨通道依赖的建模，难以充分表达关节间的时序互动。为此，需要一个既能保留Mamba高效长程建模、又能强化跨通道时序交互的方案。

Method: 构建混合Transformer–Mamba框架TSkel-Mamba：空间上使用Spatial Transformer学习骨架的空间特征；时间上采用Mamba进行时序建模。同时提出可插拔的Temporal Dynamic Modeling (TDM) 模块，包含新颖的Multi-scale Temporal Interaction (MTI) 子模块。MTI通过多尺度Cycle算子实现跨通道的时间交互与特征融合，从而弥补Mamba通道独立建模的不足。

Result: 在NTU-RGB+D 60、NTU-RGB+D 120、NW-UCLA、UAV-Human等数据集上实现SOTA性能，并保持较低推理延迟，显示出优越的准确率与效率。

Conclusion: TSkel-Mamba有效融合空间与时间建模能力，TDM/MTI加强跨通道时序交互，克服Mamba原生的通道独立局限，在多基准数据集上以更低的推理成本达到SOTA，适合高效的骨架动作识别应用。

Abstract: Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.

</details>


### [58] [SSA3D: Text-Conditioned Assisted Self-Supervised Framework for Automatic Dental Abutment Design](https://arxiv.org/abs/2512.11507)
*Mianjie Zheng,Xinquan Yang,Along He,Xuguang Li,Feilie Zhong,Xuefen Liu,Kun Tang,Zhicheng Zhang,Linlin Shen*

Main category: cs.CV

TL;DR: 提出SSA^3D：用重建+回归双分支、文本条件提示结合临床信息，实现免预训练的自动基台参数设计，训练时长减半、精度优于SSL与现有方法。


<details>
  <summary>Details</summary>
Motivation: 手工设计种植体基台需要繁琐测量与拟合，自动化研究受限于缺乏大规模标注；传统自监督需先预训练再微调，计算开销与时长高。需要一种既能利用未标注结构信息、又避免两阶段训练成本的方法，并能融入临床先验以提升可靠性。

Method: 提出SSA^3D框架：双分支结构——重建分支对口内扫描进行掩膜重建以学习几何与结构表征，并将该表征传递至回归分支；回归分支在有监督下直接预测基台参数，从而省去单独的预训练-微调流程。引入文本条件提示（TCP）模块，将临床信息（植体位置、系统、系列等）编码为文本条件，引导网络关注相关区域并对参数范围施加约束。

Result: 在自建数据集上，相比传统SSL，训练时间减少约50%，且参数预测精度更高；与其他方法相比达到SOTA，显著提升自动基台设计的准确性与效率。

Conclusion: 通过重建-回归联合学习与文本条件先验融合，SSA^3D在无需两阶段训练的前提下高效学习结构信息并提升参数回归精度，实证表明其在自动基台设计任务上具备更高效、更准确的优势。

Abstract: Abutment design is a critical step in dental implant restoration. However, manual design involves tedious measurement and fitting, and research on automating this process with AI is limited, due to the unavailability of large annotated datasets. Although self-supervised learning (SSL) can alleviate data scarcity, its need for pre-training and fine-tuning results in high computational costs and long training times. In this paper, we propose a Self-supervised assisted automatic abutment design framework (SS$A^3$D), which employs a dual-branch architecture with a reconstruction branch and a regression branch. The reconstruction branch learns to restore masked intraoral scan data and transfers the learned structural information to the regression branch. The regression branch then predicts the abutment parameters under supervised learning, which eliminates the separate pre-training and fine-tuning process. We also design a Text-Conditioned Prompt (TCP) module to incorporate clinical information (such as implant location, system, and series) into SS$A^3$D. This guides the network to focus on relevant regions and constrains the parameter predictions. Extensive experiments on a collected dataset show that SS$A^3$D saves half of the training time and achieves higher accuracy than traditional SSL methods. It also achieves state-of-the-art performance compared to other methods, significantly improving the accuracy and efficiency of automated abutment design.

</details>


### [59] [On Geometric Understanding and Learned Data Priors in VGGT](https://arxiv.org/abs/2512.11508)
*Jelena Bratulić,Sudhanshu Mittal,Thomas Brox,Christian Rupprecht*

Main category: cs.CV

TL;DR: VGGT是一种单次前馈的3D基础模型，虽未显式加入几何约束，但其内部表征自发学得对应匹配与极线几何；同时依赖数据驱动先验，并在遮挡、外观变化与相机配置下展现一定鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 澄清VGGT的能力来源：它是否像传统多视几何那样依赖显式几何原理，还是主要依靠大规模数据学到的外观先验？并理解其内部机制如何实现相机与场景结构推断。

Method: 对VGGT进行表征探针与可解释性分析：1) 探测中间特征；2) 分析全局注意力的注意力模式；3) 通过干预实验验证机制；另外用空间掩蔽与扰动实验衡量其对遮挡、外观变化、相机配置的鲁棒性，并与经典多阶段几何流水线对比。

Result: 发现VGGT的全局注意力层隐式执行跨视图对应匹配并编码极线几何，即使训练中没有显式几何约束。同时模型对数据先验有依赖，在不同遮挡与外观/相机变化下表现出一定稳健性。

Conclusion: VGGT在内部表征中内化了几何结构（如对应与极线约束），同时利用学习到的数据驱动先验工作；二者共同支撑其单步推断能力，与传统多视几何方法形成功能互补。

Abstract: The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.

</details>


### [60] [Reconstruction as a Bridge for Event-Based Visual Question Answering](https://arxiv.org/abs/2512.11510)
*Hanyue Lou,Jiayi Zhou,Yang Zhang,Boyu Li,Yi Wang,Guangnan Ye,Boxin Shi*

Main category: cs.CV

TL;DR: 提出将事件相机与多模态大语言模型结合，通过“重建+标记化”桥接事件数据与帧式模型，并构建首个客观真实场景基准EvQA；两种方法FRT与ART在EvQA上达SOTA，显示事件视觉+MLLM的潜力。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高速、HDR、低延迟等极端条件下优于传统相机，但MLLM生态主要基于帧图像。如何既保留事件数据优势，又与现有帧式MLLM兼容，是当前瓶颈。缺乏统一、客观、真实世界评测也阻碍了进展。

Method: 以“重建”为桥：1) FRT（Frame-based Reconstruction and Tokenization）：将事件流重建为帧图像，再用常规视觉编码器/标记化送入MLLM，简单通用；2) ART（Adaptive Reconstruction and Tokenization）：利用事件稀疏性自适应重建与标记化，提高效率与信息保真。并提出EvQA基准：从22个公开数据集构建1000条事件问答对，用于客观评测事件-MLLM能力。

Result: 在EvQA上，所提方法取得当前最优性能（SOTA），验证了重建桥接策略的有效性；ART在效率与性能上优于简单FRT（从摘要可推断），整体显著优于现有方法。

Conclusion: 通过重建与自适应标记化，事件数据可无缝对接现有MLLM，实现复杂场景理解；EvQA为社群提供了标准化评测。该方向展现出在恶劣视觉条件下应用MLLM的显著潜力。

Abstract: Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.

</details>


### [61] [Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France](https://arxiv.org/abs/2512.11524)
*Ekaterina Kalinicheva,Florian Helen,Stéphane Mermoz,Florian Mouret,Milena Planells*

Main category: cs.CV

TL;DR: 提出THREASURE-Net，一个仅用哨兵‑2时间序列与多分辨率LiDAR高度标注训练的端到端树高回归与超分辨率框架，在法国区尺度每年生产2.5/5/10 m树高图，误差约2.6–2.9 m，优于基于Sentinel的方法，接近超高分辨率图像方法。


<details>
  <summary>Details</summary>
Motivation: 精细尺度森林监测需要准确刻画树冠结构及其时序变化，以支撑碳储量评估、生物多样性与健康监测。现有方法要么依赖高成本的超高分辨率影像/预训练模型，要么难以同时利用光谱‑时序‑空间信息并跨分辨率学习。

Method: 提出THREASURE-Net：在哨兵‑2时间序列上端到端训练，监督信号来自多分辨率LiDAR树高度量；同时学习树高回归与超分辨率三种输出分辨率（2.5/5/10 m），不依赖外部预训练或VHR光学影像，只用LiDAR高度作为教师信号；比较三种变体并年度化生成树高图。

Result: 在法国全区实验，THREASURE-Net较基于Sentinel的SOTA有显著提升，并与基于超高分辨率影像的方法具有竞争力；在2.5/5/10 m分辨率下的年树高图平均绝对误差分别为2.62/2.72/2.88 m。

Conclusion: 仅依赖免费卫星数据与LiDAR标注即可实现可扩展、成本效益高的森林树高精细监测；THREASURE-Net为温带森林年度结构监测提供了准确、可部署的方案，代码开源可复现。

Abstract: Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.

</details>


### [62] [HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning](https://arxiv.org/abs/2512.11534)
*Yiqing Yang,Kin-Man Lam*

Main category: cs.CV

TL;DR: 提出一种端到端、任务自适应的视频关键帧选择框架：用SLM在CoT引导下生成任务隐式查询，与多模态特征融合进行动态打分；通过包含相关性、覆盖度与冗余的连续集合级目标，并用Gumbel-Softmax实现可微分的集合级选帧；再以学生-教师互学让SLM选择器与MLLM推理器对齐，最终在多数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于独立打分的Top-K关键帧选择易选到时间上聚集且视觉冗余的帧，无法在集合层面全局最优；且依赖MLLM离线伪标训练的轻量选择器难以随任务目标自适应，监督信号静态、难以协同优化。

Method: 1) Chain-of-Thought引导小语言模型生成任务特定的隐式查询向量，并与多模态特征融合以动态评分帧重要性；2) 设计连续的集合级目标函数，联合考虑相关性、覆盖度、冗余，配合Gumbel-Softmax进行可微分的组合优化，从而在集合层面选帧；3) 学生-教师互学习：学生为SLM选择器，教师为MLLM推理器，通过KL散度对齐帧重要性分布，并与交叉熵联合端到端训练，摒弃静态伪标。

Result: 在Video-MME、LongVideoBench、MLVU、NExT-QA等基准上，方法显著优于现有方法（文中声称全面超越）。

Conclusion: 集合级、任务自适应、可微的关键帧选择框架能减少冗余、提升覆盖与相关性；通过CoT引导的SLM查询与MLLM-学生教师对齐，实现端到端优化，优于传统独立Top-K与离线伪标方案。

Abstract: Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.

</details>


### [63] [Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models](https://arxiv.org/abs/2512.11542)
*Hossein Shahabadi,Niki Sepasian,Arash Marioriyad,Ali Sharifi-Zarchi,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 论文对6个文本到图像模型在组合对齐（对象、属性、空间关系、数量与多物体复杂场景）上的能力进行系统评测，发现VAR家族的Infinity-8B整体最强，Infinity-2B以更小规模实现与大型扩散模型相当甚至更好的表现，而SDXL与PixArt-α在属性与空间任务上偏弱，建立了VAR与扩散模型在组合对齐上的统一基线。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型常难以同时正确处理对象、属性绑定与空间关系等组合性要求。扩散模型已被大量探讨，但新兴的视觉自回归（VAR）模型在组合对齐方面缺乏系统性评估与对比，阻碍了方法选择与后续研究。

Method: 选取六种代表性T2I系统（SDXL、PixArt-α、Flux-Dev、Flux-Schnell、Infinity-2B、Infinity-8B），在T2I-CompBench++与GenEval两大公开基准上进行全面评测，覆盖颜色与属性绑定、空间关系、数目理解与多目标复杂提示等维度，比较VAR与扩散两类范式在组合对齐上的表现与效率。

Result: 在两套基准上，Infinity-8B综合成绩最佳；Infinity-2B在若干类别上匹配或优于更大的扩散模型，显示出良好的效率-性能权衡；SDXL与PixArt-α在与属性敏感和空间关系相关的任务上持续偏弱。

Conclusion: VAR模型在组合对齐方面展现出强竞争力，尤其是Infinity-8B；小规模VAR（Infinity-2B）亦能取得与大型扩散模型可比或更优的结果。论文首次系统比对VAR与扩散在组合对齐的差异，并给出统一基线，为后续T2I模型的开发与评测提供参考。

Abstract: Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$α$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$α$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.

</details>


### [64] [SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2](https://arxiv.org/abs/2512.11548)
*Zhendi Gong,Xin Chen*

Main category: cs.CV

TL;DR: 提出SSL-MedSAM2：用SAM2做训练免调的少样本伪标注，再用nnUNet全监督迭代精炼伪标注，在CARE-LiSeg肝脏分割挑战上取得SOTA级表现（Dice≈0.97/0.965，HD≈20/22），显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割常依赖大规模精确标注，但标注昂贵且耗时，限制临床落地。需要在有限标注下获得高质量分割的半监督方法，并充分利用大型基础模型（如SAM2）的泛化能力与传统强基线（nnUNet）的稳健训练。

Method: 构建两分支SSL框架：1）TFFS-MedSAM2：基于预训练SAM2，训练免调，进行少样本提示与推理，自动生成未标注数据的伪标注；2）FSL-nnUNet：以少量真标注+来自TFFS的伪标注为起点，用nnUNet执行全监督训练，并采用迭代精炼策略（伪标注→训练→更新伪标注→再训练）逐步提升伪标注质量与模型性能。

Result: 在MICCAI2025 CARE-LiSeg肝脏分割挑战中，测试集上GED4与T1 MRI的平均Dice分别为0.9710与0.9648，Hausdorff距离分别为20.07与21.97，优于同类方法，显示出强泛化与精度。代码开源于GitHub。

Conclusion: 通过将SAM2的零/少样本能力与nnUNet的强监督学习结合，SSL-MedSAM2在极少标注场景下仍能实现SOTA级肝脏分割，显著降低标注成本并具备实用潜力；迭代精炼机制是性能提升关键。

Abstract: Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.

</details>


### [65] [3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation](https://arxiv.org/abs/2512.11557)
*Zhiguo Lu,Jianwen Lou,Mingjun Ma,Hairong Jin,Youyi Zheng,Kun Zhou*

Main category: cs.CV

TL;DR: 提出3DTeethSAM：将SAM2适配到3D牙齿实例与语义分割，通过多视角渲染→2D分割→2D-3D回投，并加入三小型可学习模块与DGAP插件，在3DTeethSeg上达IoU 91.90%，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D牙齿分割因牙列形态复杂、实例定位与语义分类并存而困难。现有方法在泛化与标注成本上受限；通用分割基础模型SAM2虽强，但对提示依赖强、输出粗糙且类无关，直接用于3D牙齿效果不足，因此需要一种能利用SAM2能力、又能适配3D任务与类别识别的方案。

Method: - 将3D牙齿网格从预设视角渲染为多张图像；
- 以SAM2进行2D分割；
- 通过2D-3D投影重建3D掩膜；
- 设计三轻量模块：
  1) Prompt Embedding Generator：由图像特征自适应生成提示嵌入，提升掩膜解码精度；
  2) Mask Refiner：对SAM2初始掩膜进行细化修正；
  3) Mask Classifier：对生成掩膜进行类别判别，实现实例-语义联动；
- 在SAM2图像编码器中引入Deformable Global Attention Plugin（DGAP），以更高效的全局依赖建模与可变形采样提升精度与训练速度。

Result: 在3DTeethSeg基准上对高分辨率3D牙齿网格取得IoU 91.90%，刷新SOTA；同时报告了训练加速与精度提升（文中强调DGAP贡献）。

Conclusion: 通过“渲染到2D→SAM2分割→3D回投”的管线，并以轻量模块缓解SAM2对提示、类无关与初始掩膜不足的问题，结合DGAP增强编码器，方法在3D牙齿分割上实现精度与效率兼顾，展现了通用分割基础模型向特定3D医疗场景迁移的有效范式。

Abstract: 3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.

</details>


### [66] [DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry](https://arxiv.org/abs/2512.11558)
*Zhenyang Cai,Jiaming Zhang,Junjie Zhao,Ziyi Zeng,Yanchao Li,Jingyi Liang,Junying Chen,Yunjin Yang,Jiajun You,Shuzhi Deng,Tongfei Wang,Wanting Chen,Chunxiu Hao,Ruiqi Xie,Zhenwei Wen,Xiangyi Feng,Zou Ting,Jin Zou Lin,Jianquan Li,Guangjun Yu,Liangyi Chen,Junwen Wang,Shan Jiang,Benyou Wang*

Main category: cs.CV

TL;DR: DentalGPT 是一个面向口腔领域的多模态大模型：基于12万+牙科图像与细粒度标注进行预训练，并通过强化学习增强多模态推理，在多种口腔影像与医用VQA基准上以仅7B参数超越多款SOTA。


<details>
  <summary>Details</summary>
Motivation: 通用MLLM在牙科任务上两大痛点：难以捕捉细粒度牙科视觉线索（如龋损边界、根尖影像特征等），以及在复杂诊断推理/问答中的逻辑能力不足。为实现自动化、可靠的口腔诊疗辅助，需要一个具备专业视觉理解与推理能力的领域模型。

Method: 两阶段领域适配：1) 知识注入与数据构建：汇聚>12万张牙科多模态影像（口内/全景等），并配对强调诊断性视觉要点的详细描述，构成目前规模最大的牙科多模态数据集；以此进行多模态指令对齐/预训练，提升细粒度视觉理解。2) 强化学习阶段：在多模态任务上进行RL优化，进一步增强复杂跨模态推理与决策能力。

Result: 在口内照与全景片基准、以及医疗VQA的牙科子集上，DentalGPT在疾病分类与VQA任务显著优于多款SOTA MLLM；尽管参数量仅7B，仍取得领先表现。

Conclusion: 高质量、细粒度的牙科多模态数据结合分阶段适配（预训练/指令对齐+强化学习）是构建强健、专业化牙科MLLM的有效路径；DentalGPT验证了小参数规模也能在专业领域达到或超越SOTA。

Abstract: Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.

</details>


### [67] [Multi-temporal Calving Front Segmentation](https://arxiv.org/abs/2512.11560)
*Marcel Dreier,Nora Gourmelon,Dakota Pyles,Fei Wu,Matthias Braun,Thorsten Seehaus,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 提出在SAR时间序列多帧并行处理并进行时序特征交换，以稳健分割海冰末端（崩解前缘）；集成到Tyrion架构，在CaFFe基准上取得新的SOTA（MDE 184.4 m，mIoU 83.6）。


<details>
  <summary>Details</summary>
Motivation: 现有用于SAR影像的深度学习前缘分割模型在季节性干扰（冰杂、积雪等）下易误判，影响对海洋末端冰川的持续监测与动力学研究，亟需更稳健的自动化方法。

Method: 对同一冰川的卫星时间序列选取多帧并行输入，设计在对应特征图间进行时序信息交换与融合，从而稳定每一帧的预测；将该机制嵌入当前SOTA架构Tyrion中进行端到端训练与推理。

Result: 在CaFFe数据集上取得新的SOTA：Mean Distance Error 184.4 m，mean IoU 83.6，优于现有方法，表明在季节性复杂条件下分割更准确稳定。

Conclusion: 时序多帧并行与特征级信息交换能有效缓解季节性表面条件带来的分类混淆，提升SAR前缘自动化测绘的稳健性；方法与Tyrion兼容且可推广至其他时间序列遥感分割任务。

Abstract: The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.

</details>


### [68] [Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis](https://arxiv.org/abs/2512.11574)
*Valentina Lilova,Toyesh Chakravorty,Julian I. Bibo,Emma Boccaletti,Brandon Li,Lívia Baxová,Cees G. M. Snoek,Mohammadreza Salehi*

Main category: cs.CV

TL;DR: 提出一个无需微调的3D场景理解“在上下文”评测基准，扩展自Hummingbird到MVImgNet，通过键-查询多视角分割测试，比较8个基础模型；结果显示DINO系在大视角变化下依然强，3D感知模型如VGGT需专门多视角适配。


<details>
  <summary>Details</summary>
Motivation: 现有3D评测多依赖在线性头或任务解码器微调，难以隔离预训练编码器的内在3D推理与密集特征质量；需要一个能够直接 Probe 编码器、贴近真实多视角应用（机器人/无人驾驶）的无微调基准。

Method: 基于Hummingbird的“在上下文”评测范式，扩展到3D多视角：使用MVImgNet，给定若干特定视角的图像与其掩码作为键，要求在不微调的前提下，用仅来自键的提示对新视角（查询）进行分割；按键-查询视角差异将难度分为easy/medium/hard/extreme，并在统一协议下评测8个SOTA基础模型（含DINO、3D-aware如VGGT）。

Result: 在大视角变化下，基于DINO的编码器维持较强且稳定的分割与特征对齐能力；而名义上3D感知的模型（如VGGT）若不进行专门的多视角调整，其性能不如预期。给出四个难度分段的分数对比与总体排名（文中）。

Conclusion: 无微调在上下文3D场景理解基准能够更公平地衡量预训练编码器的密集特征与3D推理；DINO系在跨视角泛化上具竞争力，3D-aware模型需要面向多视角的特定适配。代码已开源，便于复现与后续研究。

Abstract: Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .

</details>


### [69] [In-Context Learning for Seismic Data Processing](https://arxiv.org/abs/2512.11575)
*Fabian Fuchs,Mario Ruben Fernandez,Norman Ettrich,Janis Keuper*

Main category: cs.CV

TL;DR: 提出ContextSeisNet：一种用于地震去多次波的“上下文/示例条件”深度模型，通过在推理时输入邻近测点的示例-标签对，获得更好的侧向一致性与用户可控性，并在合成与实测数据上优于U-Net和传统Radon方法，且更省数据。


<details>
  <summary>Details</summary>
Motivation: 传统地震处理与现有深度学习方法存在两大痛点：1）噪声与参数调优导致结果不稳；2）深度模型在相邻道集间空间一致性差，且缺乏用户控制。需要一种既能在空间上保持一致、又可在推理阶段按任务/区域自适应且可由用户引导的方案。

Method: 提出ContextSeisNet，一个基于“in-context learning”的模型。在推理时引入支持集（support set）：来自同一测线的邻近CDP道集及其对应标签（去多次的目标），模型据此学习该区域的特定处理风格，无需再训练。该条件化推理既提供用户可控性（用户可选示例），又通过邻近示例增强侧向一致性。与U-Net对比评估，数据包括合成与实测；任务为去多次波（demultiple）。

Result: 在合成数据上：定量性能优于U-Net，且相邻道集间空间连贯性更强。在实测数据上：侧向一致性优于传统Radon去多次与U-Net；相对于U-Net，在近偏移（near-offset）更好、残余多次波更少。显著的数据效率：用少90%的训练数据即可在实测上取得可比性能。

Conclusion: ContextSeisNet在去多次任务中实现了空间一致性、用户可控与数据效率三者兼顾，具有实际应用价值，并有望推广到其他地震处理任务。

Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.

</details>


### [70] [Using GUI Agent for Electronic Design Automation](https://arxiv.org/abs/2512.11611)
*Chunyi Li,Longfei Li,Zicheng Zhang,Xiaohong Liu,Min Tang,Weisi Lin,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出GUI-EDA数据集与评测基准，系统研究GUI智能体在EDA/CAD中的应用；并提出带反思机制的评测指标EDAgent，在工业级CAD上表现可靠，首次超过电气工程博士生；结果表明EDA任务对现有GUI智能体仍是重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体多在办公软件上评测，缺乏对高价值、专业化的CAD/EDA场景的系统研究；而EDA具有显著经济价值但现有方法表现薄弱，需要数据、基准与专用评测方法推动进展。

Method: 构建GUI-EDA大规模数据集（覆盖5款CAD工具与5个物理领域，收集2000+由工程师在真实设计中产生的截图-答复-动作三元组）；建立综合基准评测30+主流GUI智能体在EDA任务上的表现；设计面向EDA的专用评测指标EDAgent，并引入反思机制以更可靠地度量与提升任务执行。

Result: 实验证明EDA任务对现有GUI智能体仍极具挑战；EDAgent在工业CAD软件上展现出可靠性能，评测结果显示其首次在该类任务上超过电气工程专业博士生。

Conclusion: 将GUI智能体从通用办公自动化扩展到高价值工程领域；提供数据与基准以推动EDA生产力提升，并以EDAgent展示在专业CAD环境中的可行性与优势。

Abstract: Graphical User Interface (GUI) agents adopt an end-to-end paradigm that maps a screenshot to an action sequence, thereby automating repetitive tasks in virtual environments. However, existing GUI agents are evaluated almost exclusively on commodity software such as Microsoft Word and Excel. Professional Computer-Aided Design (CAD) suites promise an order-of-magnitude higher economic return, yet remain the weakest performance domain for existing agents and are still far from replacing expert Electronic-Design-Automation (EDA) engineers. We therefore present the first systematic study that deploys GUI agents for EDA workflows. Our contributions are: (1) a large-scale dataset named GUI-EDA, including 5 CAD tools and 5 physical domains, comprising 2,000+ high-quality screenshot-answer-action pairs recorded by EDA scientists and engineers during real-world component design; (2) a comprehensive benchmark that evaluates 30+ mainstream GUI agents, demonstrating that EDA tasks constitute a major, unsolved challenge; and (3) an EDA-specialized metric named EDAgent, equipped with a reflection mechanism that achieves reliable performance on industrial CAD software and, for the first time, outperforms Ph.D. students majored in Electrical Engineering. This work extends GUI agents from generic office automation to specialized, high-value engineering domains and offers a new avenue for advancing EDA productivity. The dataset will be released at: https://github.com/aiben-ch/GUI-EDA.

</details>


### [71] [Embodied Image Compression](https://arxiv.org/abs/2512.11612)
*Chunyi Li,Rui Qing,Jianbo Zhang,Yuan Tian,Xiangyang Zhu,Zicheng Zhang,Xiaohong Liu,Weisi Lin,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出“具身图像压缩”（Embodied Image Compression）新问题，构建EmbodiedComp基准，在超低码率闭环场景下评测，发现现有VLA在低于“具身码率阈值”时连简单操作都不可靠，呼吁面向具身体代理的专用压缩方法。


<details>
  <summary>Details</summary>
Motivation: 传统“为机器而压”的图像压缩多面向离线、任务特定或虚拟环境的模型；而现实中的多智能体具身系统受通信带宽与时延强约束，需要在超低码率下稳定执行实时操作，现有方法与评测缺乏对这种闭环决策场景的系统性支持。

Method: 1) 提出具身图像压缩科学问题；2) 构建标准化Benchmark——EmbodiedComp，用于闭环、超低码率条件下的系统评测；3) 在仿真与真实环境中进行大量实验，测试现有视觉-语言-行动（VLA）模型在不同码率下的操控任务表现；4) 定义并观察“具身码率阈值”。

Result: 实验显示：当压缩低于某一阈值时，主流VLA在即使是简单的操作任务上也无法保持可靠性能；该阈值在不同环境与任务中稳定存在，揭示了闭环具身决策对感知质量/码率的刚性需求。

Conclusion: EmbodiedComp为研究具身图像压缩提供了统一平台和评测协议；现有通用压缩或ICM方法不足以支持具身闭环执行；需要发展针对具身代理、任务与通信约束定制的压缩技术，以加速真实世界具身AI落地。

Abstract: Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.

</details>


### [72] [Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling](https://arxiv.org/abs/2512.11624)
*Maik Dannecker,Steven Jia,Nil Stolt-Ansó,Nadine Girard,Guillaume Auzias,François Rousseau,Daniel Rueckert*

Main category: cs.CV

TL;DR: 用各向异性高斯原语显式表示3D体数据，利用高斯卷积封闭性给出采集物理的解析前向模型，避免蒙特卡罗近似PSF；在胎/新生儿MRI的自监督切片到体重建中，以相当重建质量实现5–10倍加速，收敛<30秒。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示在自监督SVR中效果好，但必须用昂贵的随机采样来近似点扩散函数，计算开销大、梯度传播不精确且影响临床实时性。需要一种既能忠实建模采集物理又显著加速、稳定可微的方案。

Method: 将高分辨率3D体参数化为各向异性高斯原语场。利用高斯对卷积的封闭性，将采集前向模型从积分近似转为解析解：观测协方差为HR协方差与PSF协方差之和(Σ_obs = Σ_HR + Σ_PSF)，从而获得闭式渲染/投影与精确梯度；在自监督框架下从低分辨、受运动影响的2D切片重建3D。

Result: 在胎儿与新生儿MRI数据上，与最先进自监督SVR重建质量相当，同时实现5–10倍速度提升；多例中30秒内收敛，免除蒙特卡罗采样成本并保持端到端可微。

Conclusion: 高斯显式表示为SVR提供了解析、快速且物理一致的前向模型，兼顾精度与效率，具备将胎儿3D MRI推向临床实时应用的潜力；代码将开源以促进复现与推广。

Abstract: Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\mathbfΣ_{obs} = \mathbfΣ_{HR} + \mathbfΣ_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\times$--10$\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.

</details>


### [73] [FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint](https://arxiv.org/abs/2512.11645)
*Jiapeng Tang,Kai Li,Chengxiang Yin,Liuhao Ge,Fei Jiang,Jiu Xu,Matthias Nießner,Christian Häne,Timur Bagautdinov,Egor Zakharov,Peihong Guo*

Main category: cs.CV

TL;DR: FactorPortrait提出一种可控人像动画的视频扩散方法，可将表情、头部运动与相机视角解耦控制，从单张人像与驱动视频合成逼真、多视角一致的动画，并在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人像驱动方法往往难以同时实现：1) 表情与姿态等控制信号的解耦；2) 从单张图像实现稳定新视角合成；3) 在保持身份一致性的同时传递细腻表情动态。作者旨在统一解决可控性、逼真度、身份与视角一致性之间的权衡。

Method: - 输入：单张人像、驱动视频、相机轨迹。
- 表情控制：用预训练图像编码器从驱动视频提取“表情潜变量”，其与身份/姿态解耦；通过“expression controller”将潜变量高效注入视频扩散Transformer。
- 姿态/相机控制：利用从3D人体网格跟踪渲染得到的Plücker射线图与法线图，分别表征相机与头部几何约束。
- 训练数据：构建大规模合成数据，涵盖多样视角、头姿与表情动态组合。
- 目标：在扩散生成中联合约束表情、头动与相机视角，实现可控、多视角一致的人像动画。

Result: 在真实感、表情表现力、控制精度与跨视角一致性等方面较现有方法显著提升（通过大量实验验证）。

Conclusion: 通过引入解耦的表情潜变量注入机制与基于几何的相机/头姿控制，FactorPortrait可从单张人像与驱动视频生成可控且多视角一致的高逼真人像动画，显示出在可控生成任务中的有效性与泛化潜力。

Abstract: We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plücker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.

</details>


### [74] [Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation](https://arxiv.org/abs/2512.11654)
*Luca Cazzola,Ahed Alboody*

Main category: cs.CV

TL;DR: 提出KineMIC：将通用文本到动作(T2M)扩散模型迁移到少样本HAR动作合成，利用CLIP文本嵌入做“语义-运动”软监督挖掘，实现从少量标注动作生成高保真、可区分的骨架序列，用于增强训练，带来+23.1%准确率提升。


<details>
  <summary>Details</summary>
Motivation: HAR需要大量、精确、类别可判别的骨架动作数据，但收集昂贵；现有T2M模型偏“艺术/通用”运动，目标与数据组织与HAR不匹配，导致域差距大、难直接用于HAR数据增广。

Method: 提出KineMIC迁移框架：以T2M扩散模型为骨干，使用CLIP文本编码空间的语义对应关系进行“动力学挖掘”与软监督蒸馏；通过将稀疏的HAR类标签与T2M源语料对齐，指导微调，把通用T2M转化为少样本的Action-to-Motion生成器；源数据HumanML3D，目标域为NTU RGB+D 120子集，每类仅10样本。

Result: 在仅少样本监督下生成更连贯、符合类语义且运动学准确的骨架动作；用生成样本做数据增广，HAR分类性能提升+23.1个百分点。

Conclusion: 语义文本空间可为运动学知识迁移提供有效软监督；通过KineMIC的上下文挖掘与微调，通用T2M可适配HAR少样本动作合成并显著提升下游识别性能。

Abstract: The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).

</details>


### [75] [Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing](https://arxiv.org/abs/2512.11680)
*Xu Zhang,Jiabin Fang,Zhuoming Ding,Jin Yuan,Xuan Liu,Qianjun Zhang,Zhiyong Li*

Main category: cs.CV

TL;DR: 提出CLV-Net：用简单视觉提示（框选区域）引导遥感多模态理解，结合上下文关系建模与跨模态一致性约束，生成意图对齐的分割与描述，并达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 遥感图像尺度大、目标相似且关系复杂，文本提示往往笼统，现有多模态方法难以聚焦用户关注区域并准确识别相似目标与其关系，需要一种能由轻量提示引导且能利用上下文关系的框架。

Method: 用户提供一个ROI边框作为视觉提示。模型包含：1) 上下文感知掩码解码器（Context-Aware Mask Decoder），显式建模并融合目标间关系以增强目标表示与掩码质量；2) 语义与关系对齐模块（Semantic and Relationship Alignment），包括跨模态语义一致性损失（提升相似外观目标的细粒度区分）与关系一致性损失（对齐文本关系与视觉交互）；输出关联的分割掩码与区域描述。

Result: 在两个基准数据集上显著优于现有方法，刷新SOTA，能够更好地捕捉用户意图并生成精确的、多模态对齐的输出。

Conclusion: 以视觉提示驱动并结合上下文关系建模与跨模态一致性约束的CLV-Net，可在遥感多模态任务中有效聚焦用户指定区域，提升分割与描述质量，具有通用性与实用价值。

Abstract: Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.

</details>


### [76] [Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection](https://arxiv.org/abs/2512.11683)
*Qiushi Guo*

Main category: cs.CV

TL;DR: 提出“深度拷贝粘贴”数据增强：利用文本/图像语义匹配、精细分割与深度引导粘贴，生成更真实的合成样本，显著提升人脸检测在复杂场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统copy-paste增强易出现前景抠图不准、几何不一致、语义不匹配，导致合成样本不真实，难以提升在遮挡、光照变化、复杂背景下的人脸检测鲁棒性。

Method: 多模态与深度感知框架：1) 通过BLIP+CLIP联合评估前景人物与背景的语义与视觉一致性，实现自动背景检索与匹配；2) 使用SAM3进行高质量人物精细分割，并结合Depth-Anything提取仅可见且未被遮挡的人体区域，避免受损人脸纹理；3) 在目标背景的深度图上进行深度引导的滑窗放置，寻找最优粘贴位置以保证深度连续与尺度对齐，从而获得几何真实的合成图像。

Result: 生成的合成数据在深度关系、几何尺度与语义一致性方面更自然多样；在多项实验中，相较传统copy-paste与无深度增强方法，下游人脸检测性能显著提升。

Conclusion: 深度感知与多模态一致性约束能显著提高合成样本的物理与视觉逼真度，从而为人脸检测提供更有效的训练数据；该框架优于深度无关的增强基线。

Abstract: Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.

</details>


### [77] [Text images processing system using artificial intelligence models](https://arxiv.org/abs/2512.11691)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: 提出一种将图像中文本识别并按发票/表单/信件/报告四类分类的设备，含图库与实时两种模式，采用DBNet++检测文本、BART对文本元素分类，PyQt5展示结果；在Total-Text数据集上十小时测试，文本识别率约94.62%，在复杂成像条件下表现良好。


<details>
  <summary>Details</summary>
Motivation: 在非受控环境（光照变化、旋转、弯曲、遮挡、低分辨率、弱可见文本）下，传统OCR与文档分类鲁棒性不足。需要一个端到端设备，能从多源输入中可靠检测文本并进行文档类型分类，满足实际场景（U盘/硬盘/相机）浏览与实时处理需求。

Method: 四阶段流水线：1) 图像采集与预处理（适配图库和相机实时流）；2) 采用DBNet++进行文本区域检测（可微二值化增强边界与曲线文本）；3) 使用BART对检测到的文本元素进行分类并据此将整图归入四类（Invoice/Form/Letter/Report）；4) 以Python+PyQt5开发UI呈现分类结果与工作流。所有阶段无缝集成以形成顺畅处理管线。

Result: 在Total-Text数据集上进行了约10小时的测试，获得约94.62%的文本识别率，数据集包含多样且具有挑战的高分辨率图像，模拟实际复杂条件。

Conclusion: 所提方法在各种非理想成像条件下实现了较高的文本识别与文档分类效果，证明了混合来源文本分类在实践中的可行性与有效性。

Abstract: This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.

</details>


### [78] [EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing](https://arxiv.org/abs/2512.11715)
*Wei Chow,Linfeng Li,Lingdong Kong,Zefeng Li,Qi Xu,Hang Song,Tian Ye,Xian Wang,Jinbin Bai,Shilin Xu,Xiangtai Li,Junting Pan,Shaoteng Liu,Ran Zhou,Tianshu Yang,Songhua Liu*

Main category: cs.CV

TL;DR: 论文提出基于掩码生成式Transformer（MGT）的图像编辑框架EditMGT，以替代扩散模型在全局去噪导致的非目标区域被误改的问题。通过跨注意力定位与多层注意力融合获得精细编辑区域，并用region-hold采样抑制低注意区域的token翻转，从而将修改限制在目标区域。基于CrispEdit-2M数据集，无增参地将预训练T2I MGT适配为编辑模型，在四个基准上实现与<1B参数的相近或更优质量，编辑速度提升6倍，并在风格改变/迁移上分别提升3.6%/17.6%。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的全局去噪机制在局部编辑中会耦合全图上下文，容易对非目标区域造成“溢出编辑”。需要一种能天然局部化解码、精确限定编辑范围且高效的替代方案。

Method: 以MGT为基础：1）利用MGT跨注意力图作为定位信号，提出多层注意力整合（consolidation）以获得细粒度且准确的编辑区域掩码；2）在采样阶段提出region-hold策略，限制低注意区域的token翻转，显式保护非相关区域；3）通过attention injection将预训练文本到图像的MGT适配为图像编辑模型，无需新增参数；4）构建高分辨率、覆盖七类编辑的CrispEdit-2M数据集用于训练与评测。

Result: 在四个标准基准上，模型参数少于1B，编辑速度较对比方法快约6倍；在编辑质量上达到相当或更优表现，在风格改变与风格迁移任务上分别提升3.6%与17.6%。

Conclusion: MGT的局部化预测范式天然适合图像局部编辑。EditMGT通过注意力驱动的区域定位与region-hold采样有效抑制溢出编辑，在保持非目标区域完整性的同时实现高效高质的编辑，为超越扩散模型的编辑范式提供了有力证据。

Abstract: Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.

</details>


### [79] [Referring Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2512.11719)
*Yilmaz Korkmaz,Jay N. Paranjape,Celso M. de Melo,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出一种“基于自然语言指令的遥感变化检测（RCD）”，结合跨模态网络与扩散式数据生成，支持按文本指定变化类别并缓解数据稀缺与类别不均衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统变化检测只找“是否变化”，无法区分变化类型；语义变化检测虽能分类型，但类别通道与模型架构强耦合，难以跨数据集/任务复用，且标注与类不均衡严重。期望让用户用自然语言精确指定关注的变化，并能高效扩展训练数据。

Method: 两阶段框架：1) RCDNet：跨模态融合网络，将文本提示与双时相遥感图像融合，实现“按指令”检测特定类别的变化；2) RCDGen：基于扩散模型的数据生成流水线，仅用t0（变化前）图像与文本类别，就能合成逼真的t1（变化后）图像与相应变化图，无需语义分割掩码，从而可规模化合成带标注数据。

Result: 在多数据集上验证，RCD框架实现可扩展、可定制的变化检测，较传统/语义方法在针对性检测与数据扩展能力上有优势。

Conclusion: 以文本作为开放词汇接口解耦了类别与模型结构，结合扩散合成缓解数据稀缺与不平衡，提升了变化检测的可迁移性与可扩展性。

Abstract: Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.

</details>


### [80] [Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation](https://arxiv.org/abs/2512.11720)
*Yan Zhang,Han Zou,Lincong Feng,Cong Xie,Ruiqi Yu,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 将音乐转舞蹈视为“音乐条件的多通道图像生成”，用图像VAE+DiT建模2D姿态序列，并配合时间同步索引与参考姿态条件，实现高方差场景下节奏对齐、身份保持与长时生成，较现有方法在多项指标与主观偏好上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有pose-to-video已能从2D姿态得到逼真且保身份的视频，瓶颈转移到：如何从音乐生成时间一致、节奏对齐、能适应野外高方差分布的2D姿态序列。传统序列建模难以兼顾节奏同步、结构多样性与鲁棒性，且难充分利用T2I领域的模型与训练进展。

Method: 将2D姿态序列编码为one-hot图像，输入预训练图像VAE压缩为潜变量，再用DiT风格骨干在音乐token条件下进行扩散建模，实现“音乐→姿态潜变量”的生成。关键设计：1）时间共享的temporal indexing，使音乐token与姿态潜变量在时间维显式对齐；2）reference-pose条件，固定主体比例与屏幕尺度，并支持长时“分段-拼接”生成。

Result: 在大规模野外2D舞蹈语料与校准的AIST++2D基准上，相比代表性音乐转舞蹈方法，在姿态与视频空间的客观指标以及人类偏好评测上均取得一致性提升；消融实验证明表示方式、时间索引与参考条件各自的有效性。

Conclusion: 通过把音乐转舞蹈重述为图像生成问题并引入时间索引与参考姿态条件，模型继承T2I前沿架构优势，显著提升复杂场景下的节奏对齐、时间一致性与身份保持，并支持长时稳定生成。

Abstract: Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io

</details>


### [81] [Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images](https://arxiv.org/abs/2512.11722)
*Lin Bai,Xiaoyang Li,Liqiang Huang,Quynh Nguyen,Hien Van Nguyen,Saurabh Prasad,Dragan Maric,John Redell,Pramod Dash,Badrinath Roysam*

Main category: cs.CV

TL;DR: 提出一种从弱监督到强泛化的自动化训练流程，基于带高效通道注意力的多头Mask R-CNN，用于多重循环免疫荧光全片图像中重叠细胞核的可靠分割；无需人工标注即可适配新仪器/新协议，并配套自诊断指标；在与5种主流方法对比中显著优于基线，并开放代码与数据。


<details>
  <summary>Details</summary>
Motivation: 多重IF全片图像中细胞核常重叠且成像域间差异大，传统模型对新仪器/协议迁移差、人工逐片标注成本极高；生产环境无法大规模人工校对，需要自动化质量自评与可扩展训练方法。

Method: 构建带多头输出的Mask R-CNN并加入高效通道注意力；提出“弱到强”泛化流程：利用弱标注/伪标签进行自训练，通过伪标签纠错与覆盖扩展迭代提升；设计生产可用的自动化分割质量自诊断指标；在新域（新设备/协议）上无人工标注进行de novo学习。

Result: 与五种广泛使用的方法对比，在重叠细胞核分割上取得显著性能提升；方法能在无人工标注的新域中实现可靠分割；提供开源代码、样例WSI和高分辨率分割结果。

Conclusion: 弱到强的泛化策略可使多头注意力Mask R-CNN在多域多协议的WSI核分割中实现无需人工标注的适配；伪标签纠错与覆盖扩展是效果提升的关键；自诊断指标有助于在生产环境中保障质量，方法具备社区可复现与推广价值。

Abstract: We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.

</details>


### [82] [SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder](https://arxiv.org/abs/2512.11749)
*Minglei Shi,Haolin Wang,Borui Zhang,Wenzhao Zheng,Bohan Zeng,Ziyang Yuan,Xiaoshi Wu,Yuanxing Zhang,Huan Yang,Xintao Wang,Pengfei Wan,Kun Gai,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 作者提出SVG-T2I：在视觉基础模型（VFM）特征空间内训练的端到端文本生成图像扩散框架，达到接近SOTA的指标，并全面开源。


<details>
  <summary>Details</summary>
Motivation: 现有大规模文本到图像扩散模型几乎都在像素空间或VAE潜空间训练，鲜少直接在VFM表征空间内训练。作者认为VFM具有强泛化与语义结构，若能在其表征域中进行生成，可统一理解与生成，并验证VFM对生成任务的内在表达力。

Method: 扩展SVG框架，构建SVG-T2I：1) 使用自监督学得的VFM特征作为生成目标；2) 采用标准T2I扩散流水线（如噪声调度、UNet/Transformer去噪器、文本条件编码）但将重建域从像素/VAE潜空间替换为VFM特征域；3) 配套自动编码器用于图像↔VFM特征的可逆映射；4) 提供训练、推理与评测完整管线。

Result: 在GenEval得到0.75，在DPG-Bench得到85.78，显示与主流T2I方法具有竞争力，证明在VFM特征空间进行生成的可行性与有效性。

Conclusion: VFM表征具备足够生成表达力，能支撑高质量文本到图像生成。开源全部组件与权重，期望推动“表征驱动”的视觉生成研究。

Abstract: Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.

</details>


### [83] [Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting](https://arxiv.org/abs/2512.11763)
*Mohammad Dehghanmanshadi,Wallapak Tavanapong*

Main category: cs.CV

TL;DR: 将艺术风格迁移的InST框架移植到生物显微图像：用扩散模型的随机反演+潜空间AdaIN，把真实荧光显微“风格”迁到合成图上，弱保持内容；用于细胞计数的预训/微调显示显著降低MAE，并优于仅用真实数据训练。


<details>
  <summary>Details</summary>
Motivation: 在标注稀缺、单图细胞密度高任务中，硬规则的合成图与真实显微图存在显著域差，传统DA难以弥合，因为合成图缺乏真实复杂纹理与视觉模式。需要一种能在保持结构前提下赋予真实显微风格的方法，提升下游计数性能并降低标注成本。

Method: 将Inversion-Based Style Transfer应用到显微领域：在扩散模型中进行随机反演（stochastic inversion），并在潜空间采用自适应实例归一化（AdaIN）实现风格注入，对内容进行弱约束保持。生成含真实显微风格的合成数据集；随后用EfficientNet-B0在多数据源（真实、硬编码合成、Cell200-s、以及InST合成）上进行预训练与微调；并与轻量域适配（如DACS+CutMix）联合评估。

Result: InST合成数据训练的模型：相对硬编码合成数据，MAE最多下降37%；相对Cell200-s从53.70降至25.95（降52%）；优于仅用真实数据训练（25.95 vs 27.74 MAE）。与DACS+CutMix结合还有进一步提升。

Conclusion: InST式风格迁移有效缩小合成与真实显微数据的域差，能以较低标注成本显著提升细胞计数精度；方法可扩展、实用，代码已开源，适合构建高效的合成训练集并与轻量DA结合以获得更佳性能。

Abstract: Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.
  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.

</details>


### [84] [Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints](https://arxiv.org/abs/2512.11771)
*Kai Yao,Marc Juarez*

Main category: cs.CV

TL;DR: 本文对AI图像源模型指纹检测在对抗场景下的鲁棒性进行首次系统安全评估，定义白盒/黑盒与“去除/伪造”两类攻击目标，实作5种攻击并在14种指纹方法、12个生成器上评测，发现干净与对抗性能存在巨大鸿沟：白盒去除>80%成功，黑盒>50%；伪造更难但依目标差异显著；高准确方法常更脆弱，无一方法在所有威胁模型下兼具高准确与高鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有模型指纹技术能归因AI生成图像，但对抗条件下稳健性未被充分研究；产业与治理需要可靠归因以应对滥用与合规，因此需系统性威胁建模与实证评估。

Method: 形式化威胁模型（白盒/黑盒；去除/伪造），实现五类攻击策略；覆盖RGB、频域、与学习特征三大指纹范式；在12个SOTA生成模型和14种指纹方法上统一基准评测，比较干净与对抗下的归因准确与成功率，并分析效用-鲁棒权衡。

Result: 去除攻击在白盒场景成功率常>80%，黑盒受限仍>50%；伪造攻击成功率更不稳定，依目标模型差异较大；高干净集准确度的方法往往在对抗下明显退化；不存在同时在所有威胁下保持高准确与高鲁棒的方法，但个别技术在特定设置下较稳健。

Conclusion: 当前指纹检测在对抗下普遍脆弱，存在显著效用-鲁棒权衡；需设计兼顾鲁棒与准确的新方法，并以本文评测为基准优选方向与实践指南。

Abstract: Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.

</details>


### [85] [MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator](https://arxiv.org/abs/2512.11782)
*Peiqing Yang,Shangchen Zhou,Kai Hao,Qingyi Tao*

Main category: cs.CV

TL;DR: 提出Matting Quality Evaluator（MQE）用于无GT评估alpha抠图的语义与边界质量，并据此做在线训练反馈与离线数据筛选，构建大规模真实视频数据集VMReal，并配合长程参考帧训练的MatAnyone 2在多基准上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频抠图数据集规模小、真实性不足；用分割数据增强语义稳定性会丢细节，缺乏有效的边界监督导致结果“像分割”。需要一种无需GT即可评估与监督matting质量的方法，同时扩展真实数据规模与质量。

Method: 1) 训练一个无监督/自监督的Matting Quality Evaluator（MQE），输出像素级质量图，区分可靠与错误区域；2) 在线：在视频抠图训练中以MQE作为质量反馈，抑制错误区域、提供细粒度监督；3) 离线：结合领先的视频/图像抠图模型结果，用MQE筛选与融合以策划高质标注，构建VMReal（28K片段、2.4M帧）；4) 参考帧训练：在长视频中引入超出局部窗口的长程参考帧以提升外观变化下的学习；5) 提出MatAnyone 2并在合成与真实基准上评估。

Result: 得到VMReal大规模真实世界数据集；MatAnyone 2在合成和真实数据基准上多项指标全面超越以往方法，表现SOTA。

Conclusion: MQE实现了无GT的精细质量评估，既能在线提升训练监督，又能离线提升数据策划质量；结合参考帧策略与大规模VMReal数据，MatAnyone 2实现了视频抠图领域的新SOTA，兼顾语义稳定与精细边界细节。

Abstract: Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.

</details>


### [86] [Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs](https://arxiv.org/abs/2512.11791)
*Wentao Jiang,Vamsi Varra,Caitlin Perez-Stable,Harrison Zhu,Meredith Apicella,Nicole Nyamongo*

Main category: cs.CV

TL;DR: 提出一个用于白癜风临床照片分割的“可信+频率感知”框架，在小样本下仍能稳定高精度，Dice 85.05%，边界误差显著下降，并给出不确定性热图辅助临床复核。


<details>
  <summary>Details</summary>
Motivation: 临床随访需要准确量化白癜风病灶范围，但常规照片背景复杂、边界细碎，现有CNN/Transformer在小数据与噪声环境下易失稳、边界粗糙，且缺乏不确定性输出，不利于可信临床决策。

Method: 三大支柱：1) 数据高效训练：在ISIC 2019皮肤病变数据上进行域自适应预训练，并用ROI约束的双任务损失抑制背景噪声；2) 架构改进：以ConvNeXt V2编码器为骨干，引入高频谱门控（HFSG）模块与stem-skip连接，强化细微纹理与边界；3) 可信机制：K折集成+测试时增强（TTA）生成像素级不确定性/熵图。

Result: 在专家标注临床队列上优于强基线（ResNet-50、UNet++、MiT-B5）：Dice达85.05%；95% Hausdorff距离由44.79px降至29.95px；零灾难性失败；可生成可解释的不确定性熵图标记模糊区域。

Conclusion: 该频率感知与可信分割框架在白癜风自动评估中兼具精度、稳健性与可解释性，可作为临床量化评估的可靠基线/标准方案。

Abstract: Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.

</details>


### [87] [Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation](https://arxiv.org/abs/2512.11792)
*Yang Fei,George Stoica,Jingyuan Liu,Qifeng Chen,Ranjay Krishna,Xiaojuan Wang,Benlin Liu*

Main category: cs.CV

TL;DR: 提出SAM2VideoX：将自回归视频跟踪模型的结构运动先验蒸馏进双向扩散视频生成模型，提升生成视频的结构一致性与物理合理性，在VBench与人评上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散视频生成在保持真实结构与物理一致的运动方面仍不足，尤其对人体/动物等关节与可形变对象。现有方法依赖噪声较大的外部运动表征（光流、骨架），且单纯扩充数据无法解决不合理过渡，因此需要从更强的时序跟踪模型中获取更干净的结构运动先验并注入扩散模型。

Method: 将自回归视频跟踪模型SAM2的时序结构先验蒸馏到双向扩散模型CogVideoX，形成SAM2VideoX。核心包含：1）双向特征融合模块，从循环模型中提取全局结构保持的运动先验并与扩散特征融合；2）Local Gram Flow损失，对齐局部特征的协同运动（局部Gram统计随时间的一致性）。整体训练在无须外部噪声运动标签的蒸馏框架下完成。

Result: 在VBench与人类偏好评测中显著优于基线：VBench得分95.51%，比REPA的92.91%高2.60%；FVD降至360.57，相比REPA微调与LoRA微调分别降低21.20%与22.46%；人类偏好71.4%。

Conclusion: 蒸馏自回归跟踪先验并配合双向融合与Local Gram Flow损失，可显著改善视频扩散模型的结构一致与物理合理运动，优于依赖噪声运动条件或单纯扩数据的方案，推进复杂可形变对象的视频生成质量。

Abstract: Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .

</details>


### [88] [Particulate: Feed-Forward 3D Object Articulation](https://arxiv.org/abs/2512.11798)
*Ruining Li,Yuxin Yao,Chuanxia Zheng,Christian Rupprecht,Joan Lasenby,Shangzhe Wu,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Particulate 是一种前馈方法，从单个静态3D网格直接推断物体的关节结构：部件分割、运动学拓扑与运动约束，并在秒级生成可驱动的关节化模型。核心是处理点云的Transformer（Part Articulation Transformer），支持多关节，训练于多样化公共数据集并能泛化到AI生成3D资产。作者还构建了更贴近人类偏好的新基准与评测协议，方法在定量与定性上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D关节化结构估计多需逐物体优化或多视图/视频输入，效率低且对真实或合成3D资产泛化差；同时评测标准与人类直觉不一致。作者希望从单个静态网格端到端、快速、稳健地恢复部件与运动学，并建立更合理的评测基准。

Method: 提出 Part Articulation Transformer：以点云为输入的可扩展Transformer，端到端同时预测部件分割、关节类型/参数、拓扑（多关节支持）与运动约束；训练于多样公共关节化3D资产。推理阶段将预测“提升”到输入网格，得到可驱动的关节化模型；还能与现成图像到3D生成器组合，从单张图像间接获得关节化模型。

Result: 在作者新提出的高质量关节化基准与改进评测协议上，定量与定性结果均显著优于现有方法；推理速度为秒级，远快于需逐对象优化的方法；对AI生成3D资产也能准确推断结构。

Conclusion: Particulate 实现了从单个静态3D网格到完整关节结构的快速前馈估计，兼顾速度、准确性与多关节支持，并推动了评测基准与协议的改进，显示出在真实与合成3D场景中的广泛适用性。

Abstract: We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.

</details>


### [89] [V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties](https://arxiv.org/abs/2512.11799)
*Ye Fang,Tong Wu,Valentin Deschaintre,Duygu Ceylan,Iliyan Georgiev,Chun-Hao Paul Huang,Yiwei Hu,Xuelin Chen,Tuanfeng Yang Wang*

Main category: cs.CV

TL;DR: V-RGBX提出一个端到端、内在属性感知的视频编辑框架，能从视频中做逆向渲染得到albedo/法线/材质/辐照度等通道，并据此进行高保真视频生成与关键帧驱动的可编辑传播，获得时序一致、物理合理的效果，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成虽能合成逼真外观与光照，但缺乏一个闭环：同时理解场景内在属性、用这些内在表示进行视频合成，并支持对这些内在通道进行直观可控的编辑与在视频时序上的传播。

Method: 提出V-RGBX框架，核心为交错式（interleaved）条件机制：1）视频逆向渲染得到内在通道（albedo、normal、material、irradiance等）；2）从这些内在表示进行照片级视频合成；3）基于用户选定关键帧、以内在通道为条件进行视频编辑，并将编辑在时间上传播。

Result: 在大量定性与定量实验中，V-RGBX生成的结果具有更好的时序一致性与照片真实感；关键帧上的内在编辑可物理合理地传播到整段视频；在物体外观编辑与场景级重光照等应用中超越现有方法。

Conclusion: V-RGBX实现了内在属性理解—条件合成—可编辑传播的闭环，为物理一致、可控的视频编辑与生成提供了统一框架，并在多种任务上达到或超过当前水平。

Abstract: Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.

</details>


### [90] [Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance](https://arxiv.org/abs/2512.11800)
*Jan U. Müller,Robin Tim Landsgesell,Leif Van Holland,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 提出一种基于统计矩的透射率计算方法，把3D高斯光栅化与物理准确的半透明渲染结合，避免排序/射线追踪，实现复杂半透明场景的更高质量重建与实时渲染。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting虽能快速优化并实时渲染，但其依赖顺序相关的Alpha混合与密度积分近似，难以正确处理重叠的半透明物体，影响物理准确性与图像质量。

Method: 借鉴基于矩的顺序无关透明度思想：对每条相机光线，用所有贡献的3D高斯解析地计算一组像素级统计矩，形成沿光线密度分布的紧致连续表示；再由这些矩重建每条光线的连续透射函数，并在每个高斯内独立采样来评估光衰减，无需光线追踪或逐像素样本排序。

Result: 在复杂半透明与重叠场景中，相比传统3DGS的栅格化近似，显著提升透射/衰减建模的准确性与最终重建与渲染质量，同时保持实时性（避免排序与射线追踪的开销）。

Conclusion: 通过像素级矩表示与连续透射重建，方法在栅格化框架下实现更物理的体渲染，弥合了实时渲染效率与半透明物理精度之间的鸿沟，适用于复杂半透明介质的高保真新视角合成。

Abstract: The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.

</details>
