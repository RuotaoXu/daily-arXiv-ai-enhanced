<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 212]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary](https://arxiv.org/abs/2509.00033)
*Tahoshin Alam Ishat*

Main category: cs.CV

TL;DR: 将YOLOv8分割、手部关键点运动序列的LSTM、ASR(whisper-base)与TinyLLaMA串联，自动从做菜视频/音频中提取多模态特征，预测菜谱并生成分步烹饪指引；数据由作者自采集以适应复杂厨房环境。


<details>
  <summary>Details</summary>
Motivation: 日常厨房场景复杂、噪声多、遮挡多，现有单模态或单模型难以稳定获得可执行的烹饪步骤文本。作者希望通过多模态融合（视觉分割+手部运动序列+语音识别）为小型LLM提供更充分的上下文，从而自动生成结构化菜谱步骤，展示计算机视觉在日常任务中的可扩展性。

Method: - 视觉：采用YOLOv8 segmentation对视频中物体/食材/器具进行分割与检测。
- 动作：基于手部关键点/手指轨迹序列训练LSTM，建模手部操作动态。
- 语音：使用Whisper-base进行自动语音识别，提取口述内容。
- 融合：将上述多模态特征整理为LLM输入（TinyLLaMA），让LLM预测菜谱并生成逐步烹饪指引。
- 数据：作者自建并采集厨房场景数据，以增强在复杂环境下的鲁棒性；并对现有模型进行微调以适配任务。

Result: 系统能够从复杂厨房环境中的视频/音频数据中抽取关键信息，并由TinyLLaMA生成可读的分步烹饪指南，展示了多模态流水线在菜谱预测任务上的可行性与有效性。具体量化指标（如准确率、BLEU、步骤一致性、时序对齐等）未给出。

Conclusion: 多模态融合（分割+动作时序+ASR+小型LLM）可在现实厨房场景中实现自动菜谱与步骤生成，体现了计算机视觉在日常任务的可扩展性。工作为其他日常多模态任务提供了思路，但仍需补充标准化评测、可解释性与泛化性验证。

Abstract: This is a research exploring existing models and fine tuning them to combine
a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence
and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to
predict the recipe and generate text creating a step by step guide for the
cooking procedure. All the data were gathered by the author for a robust task
specific system to perform best in complex and challenging environments proving
the extension and endless application of computer vision in daily activities
such as kitchen work. This work extends the field for many more crucial task of
our day to day life.

</details>


### [2] [AMMKD: Adaptive Multimodal Multi-teacher Distillation for Lightweight Vision-Language Models](https://arxiv.org/abs/2509.00039)
*Yuqi Li,Chuanguang Yang,Junhao Dong,Zhengtao Yao,Haoyan Xu,Zeyu Dong,Hansheng Zeng,Zhulin An,Yingli Tian*

Main category: cs.CV

TL;DR: 提出AMMKD框架，通过多模态特征融合、多教师蒸馏与自适应权重优化，显著减小检索模型体量同时保持/提升性能。


<details>
  <summary>Details</summary>
Motivation: VLP模型在图文检索中效果好但体积大、计算重，难以上移动端。需要在不牺牲性能的前提下降低参数与推理成本，并提升蒸馏时师生对齐与多教师冲突问题的处理。

Method: 1) 多模态特征融合：构建融合网络同时抽取并合并图像与文本判别性特征。2) 多教师蒸馏：预训练两个CLIP教师；将文本特征通过教师文本编码器预计算为类向量，解耦模态并加速。3) 输出对齐：使用KL scatter进行概率分布匹配，提升师生对齐质量。4) 自适应动态加权：把多教师蒸馏建模为多目标优化，基于梯度空间多样性自适应调整各教师权重，缓解冲突并引导学生朝更优方向学习。

Result: 在三个基准数据集上取得优于现有方法的检索性能，同时显著降低模型复杂度（参数量与计算量），证明了在轻量化与效果间的良好权衡。

Conclusion: AMMKD在保持甚至提升检索效果的同时实现轻量化部署，可灵活适配移动端等资源受限场景；多教师+自适应优化有效缓解冲突并提升蒸馏效率。

Abstract: The success of large-scale visual language pretraining (VLP) models has
driven widespread adoption of image-text retrieval tasks. However, their
deployment on mobile devices remains limited due to large model sizes and
computational complexity. We propose Adaptive Multi-Modal Multi-Teacher
Knowledge Distillation (AMMKD), a novel framework that integrates multi-modal
feature fusion, multi-teacher distillation, and adaptive optimization to
deliver lightweight yet effective retrieval models. Specifically, our method
begins with a feature fusion network that extracts and merges discriminative
features from both the image and text modalities. To reduce model parameters
and further improve performance, we design a multi-teacher knowledge
distillation framework to pre-train two CLIP teacher models. We decouple
modalities by pre-computing and storing text features as class vectors via the
teacher text encoder to enhance efficiency. To better align teacher and student
outputs, we apply KL scatter for probability distribution matching. Finally, we
design an adaptive dynamic weighting scheme that treats multi-teacher
distillation as a multi-objective optimization problem. By leveraging gradient
space diversity, we dynamically adjust the influence of each teacher, reducing
conflicts and guiding the student toward more optimal learning directions.
Extensive experiments on three benchmark datasets demonstrate that AMMKD
achieves superior performance while significantly reducing model complexity,
validating its effectiveness and flexibility.

</details>


### [3] [ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization](https://arxiv.org/abs/2509.00042)
*Poyraz Baydemir*

Main category: cs.CV

TL;DR: 提出ARTPS：融合单目深度估计（ViT）、多组件异常检测与可学习好奇心评分，用于行星表面自主探索与目标优先级排序；在火星数据集上达SOTA（AUROC 0.94/AUPRC 0.89/F1 0.87），混合融合将误报降23%。


<details>
  <summary>Details</summary>
Motivation: 行星探测中，漫游车带宽与行动时间受限，需要在海量视觉流中自动发现并优先关注有科学价值/异常目标，且需在复杂地形下保持高灵敏度并降低误报。现有方法要么仅做异常检测、要么仅靠几何/深度，难以兼顾鲁棒性与可解释性。

Method: 构建混合式AI框架ARTPS：1) 使用基于Vision Transformer的单目深度估计，获取深度与不确定性；2) 多组件异常检测（可能含外观/语义/重建/对比等信号）；3) 计算可学习的加权好奇心分数，融合已知价值、异常信号、深度方差与表面粗糙度；4) 融合策略进行目标排序与选择；并进行消融分析评估各组件贡献。

Result: 在火星漫游车数据集上取得SOTA：AUROC 0.94、AUPRC 0.89、F1 0.87。混合融合相较基线将误报率降低23%，在多样地形条件下保持高灵敏度；消融显示各组件对性能均有正向贡献。

Conclusion: 混合融合+可学习好奇心评分有效提升自主探索中的目标优先级判定，兼顾准确性与低误报，适用于复杂地形行星场景；未来可扩展至更多传感器与在线自适应。

Abstract: We present ARTPS (Autonomous Rover Target Prioritization System), a novel
hybrid AI system that combines depth estimation, anomaly detection, and
learnable curiosity scoring for autonomous exploration of planetary surfaces.
Our approach integrates monocular depth estimation using Vision Transformers
with multi-component anomaly detection and a weighted curiosity score that
balances known value, anomaly signals, depth variance, and surface roughness.
The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of
0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant
improvements in target prioritization accuracy through ablation studies and
provide comprehensive analysis of component contributions. The hybrid fusion
approach reduces false positives by 23% while maintaining high detection
sensitivity across diverse terrain types.

</details>


### [4] [Performance is not All You Need: Sustainability Considerations for Algorithms](https://arxiv.org/abs/2509.00045)
*Xiang Li,Chong Zhang,Hongpeng Wang,Shreyank Narayana Gowda,Yushi Li,Xiaobo Jin*

Main category: cs.CV

TL;DR: 提出一套面向深度学习训练的“双维度”可持续性评估体系，用FMS与ASC两指标同时度量精度与能耗，以跨任务基准验证其实用性，推动绿色AI从理念到落地。


<details>
  <summary>Details</summary>
Motivation: 深度学习训练碳排与能耗高，现有评估多只看性能（精度）而忽视能效，缺乏统一、可量化、可跨任务比较的绿色指标与基准，阻碍产业建立能效标准与最佳实践。

Method: 构建双指标体系：1) FMS（可持续调和平均）：将累计能耗与性能参数用调和平均融合，衡量单位能耗下的有效性能；2) ASC（可持续曲线下面积）：构建性能—功耗曲线，度量训练全周期能效特征。并在多模态任务（分类、分割、姿态估计、批量/在线学习）上建立基准，公开评估框架代码。

Result: 在多任务、多场景实验中，两指标可稳定、可解释地对不同算法给出能效排序与对比，能跨任务量化比较，揭示仅追求精度可能带来的能耗代价，为算法/配置选型提供数据依据。

Conclusion: 该评估体系可作为绿色AI的通用度量工具，支撑行业制定算法能效标准，促进研究从单一性能导向转向性能—能耗协同优化，并推动绿色AI实践落地。

Abstract: This work focuses on the high carbon emissions generated by deep learning
model training, specifically addressing the core challenge of balancing
algorithm performance and energy consumption. It proposes an innovative
two-dimensional sustainability evaluation system. Different from the
traditional single performance-oriented evaluation paradigm, this study
pioneered two quantitative indicators that integrate energy efficiency ratio
and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy
consumption and performance parameters through the harmonic mean to reveal the
algorithm performance under unit energy consumption; the area under the
sustainability curve (ASC) constructs a performance-power consumption curve to
characterize the energy efficiency characteristics of the algorithm throughout
the cycle. To verify the universality of the indicator system, the study
constructed benchmarks in various multimodal tasks, including image
classification, segmentation, pose estimation, and batch and online learning.
Experiments demonstrate that the system can provide a quantitative basis for
evaluating cross-task algorithms and promote the transition of green AI
research from theory to practice. Our sustainability evaluation framework code
can be found here, providing methodological support for the industry to
establish algorithm energy efficiency standards.

</details>


### [5] [MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition](https://arxiv.org/abs/2509.00056)
*Luu Tu Nguyen,Vu Tram Anh Khuong,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Micro-expression recognition (MER) is a challenging task due to the subtle
and fleeting nature of micro-expressions. Traditional input modalities, such as
Apex Frame, Optical Flow, and Dynamic Image, often fail to adequately capture
these brief facial movements, resulting in suboptimal performance. In this
study, we introduce the Micro-expression Spatio-Temporal Image (MESTI), a novel
dynamic input modality that transforms a video sequence into a single image
while preserving the essential characteristics of micro-movements.
Additionally, we present the Micro-expression Gradient Attention Network
(MEGANet), which incorporates a novel Gradient Attention block to enhance the
extraction of fine-grained motion features from micro-expressions. By combining
MESTI and MEGANet, we aim to establish a more effective approach to MER.
Extensive experiments were conducted to evaluate the effectiveness of MESTI,
comparing it with existing input modalities across three CNN architectures
(VGG19, ResNet50, and EfficientNetB0). Moreover, we demonstrate that replacing
the input of previously published MER networks with MESTI leads to consistent
performance improvements. The performance of MEGANet, both with MESTI and
Dynamic Image, is also evaluated, showing that our proposed network achieves
state-of-the-art results on the CASMEII and SAMM datasets. The combination of
MEGANet and MESTI achieves the highest accuracy reported to date, setting a new
benchmark for micro-expression recognition. These findings underscore the
potential of MESTI as a superior input modality and MEGANet as an advanced
recognition network, paving the way for more effective MER systems in a variety
of applications.

</details>


### [6] [Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion](https://arxiv.org/abs/2509.00062)
*Justin Jung*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Generating realistic sparse multi-category 3D voxel structures is difficult
due to the cubic memory scaling of voxel structures and moreover the
significant class imbalance caused by sparsity. We introduce Scaffold
Diffusion, a generative model designed for sparse multi-category 3D voxel
structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete
diffusion language model to generate 3D voxel structures. We show that discrete
diffusion language models can be extended beyond inherently sequential domains
such as text to generate spatially coherent 3D structures. We evaluate on
Minecraft house structures from the 3D-Craft dataset and demonstrate that,
unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion
produces realistic and coherent structures even when trained on data with over
98% sparsity. We provide an interactive viewer where readers can visualize
generated samples and the generation process. Our results highlight discrete
diffusion as a promising framework for 3D sparse voxel generative modeling.

</details>


### [7] [Dual-Stage Global and Local Feature Framework for Image Dehazing](https://arxiv.org/abs/2509.00108)
*Anas M. Ali,Anis Koubaa,Bilel Benjdira*

Main category: cs.CV

TL;DR: 提出SGLC框架，通过全局与局部特征融合提升高分辨率图像去雾性能，显著提高PSNR且可无缝集成到现有模型（如Uformer）。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法在高分辨率图像上表现退化，常依赖降采样或切片，导致难以兼顾全球上下文与局部细节，亟需能高效融合二者的新方法。

Method: 设计两阶段、两分支的框架SGLC：1) Global Features Generator (GFG) 生成基于全局上下文的初始去雾结果；2) Local Features Enhancer (LFE) 在初始结果上对像素级与局部结构进行细化与增强，实现全局外观与局部结构的交互；并将SGLC以插件式集成进Uformer验证。

Result: 在高分辨率数据集上，集成SGLC的Uformer取得显著更高的PSNR，证明在大尺寸图像去雾上的有效性。

Conclusion: SGLC能有效缓解高分辨率去雾中全局与局部信息融合难题，设计与模型无关，可作为通用模块增强各类去雾网络，提升高分辨率场景下的视觉保真度。

Abstract: Addressing the challenge of removing atmospheric fog or haze from digital
images, known as image dehazing, has recently gained significant traction in
the computer vision community. Although contemporary dehazing models have
demonstrated promising performance, few have thoroughly investigated
high-resolution imagery. In such scenarios, practitioners often resort to
downsampling the input image or processing it in smaller patches, which leads
to a notable performance degradation. This drop is primarily linked to the
difficulty of effectively combining global contextual information with
localized, fine-grained details as the spatial resolution grows. In this
chapter, we propose a novel framework, termed the Streamlined Global and Local
Features Combinator (SGLC), to bridge this gap and enable robust dehazing for
high-resolution inputs. Our approach is composed of two principal components:
the Global Features Generator (GFG) and the Local Features Enhancer (LFE). The
GFG produces an initial dehazed output by focusing on broad contextual
understanding of the scene. Subsequently, the LFE refines this preliminary
output by enhancing localized details and pixel-level features, thereby
capturing the interplay between global appearance and local structure. To
evaluate the effectiveness of SGLC, we integrated it with the Uformer
architecture, a state-of-the-art dehazing model. Experimental results on
high-resolution datasets reveal a considerable improvement in peak
signal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in
addressing haze in large-scale imagery. Moreover, the SGLC design is
model-agnostic, allowing any dehazing network to be augmented with the proposed
global-and-local feature fusion mechanism. Through this strategy, practitioners
can harness both scene-level cues and granular details, significantly improving
visual fidelity in high-resolution environments.

</details>


### [8] [Self-supervised large-scale kidney abnormality detection in drug safety assessment studies](https://arxiv.org/abs/2509.00131)
*Ivan Slootweg,Natalia P. García-De-La-Puente,Geert Litjens,Salma Dammak*

Main category: cs.CV

TL;DR: 提出首个面向药物安全研究（158种化合物）的肾脏毒理病理大规模自监督异常检测模型。直接用UNI基础模型特征+KNN仅达随机水平；在相同特征上叠加自监督学习后，AUC=0.62、NPV=89%，可用于先行排除正常切片，从而节省成本与时间。


<details>
  <summary>Details</summary>
Motivation: 药物非临床阶段需筛查大量肾脏全切片图像，多数为正常，但必须捕捉细微毒性改变。现有流程费时费钱，且基础视觉FM特征能否直接用于异常检测未知，缺乏在大规模药物安全场景的可行方案。

Method: 以158种化合物的毒理病理研究数据为规模，先提取UNI基础模型（Foundation Model）图像特征；基线用KNN进行异常检测（表现接近随机）；随后在同一特征空间上采用自监督方法（未监督标签、利用数据内在结构）学习异常表示，评估AUC与NPV。

Result: KNN+UNI特征仅达机会水平；自监督方法在同特征上取得AUC=0.62、NPV=89%，优于随机且具备较强的排除正常能力。

Conclusion: 基础FM特征不足以直接用于肾脏异常检测，但结合自监督可获得实用的“排除正常”性能。若进一步改进，有望在药物安全评估中自动剔除大量正常切片，降低时间与成本。

Abstract: Kidney abnormality detection is required for all preclinical drug
development. It involves a time-consuming and costly examination of hundreds to
thousands of whole-slide images per drug safety study, most of which are
normal, to detect any subtle changes indicating toxic effects. In this study,
we present the first large-scale self-supervised abnormality detection model
for kidney toxicologic pathology, spanning drug safety assessment studies from
158 compounds. We explore the complexity of kidney abnormality detection on
this scale using features extracted from the UNI foundation model (FM) and show
that a simple k-nearest neighbor classifier on these features performs at
chance, demonstrating that the FM-generated features alone are insufficient for
detecting abnormalities. We then demonstrate that a self-supervised method
applied to the same features can achieve better-than-chance performance, with
an area under the receiver operating characteristic curve of 0.62 and a
negative predictive value of 89%. With further development, such a model can be
used to rule out normal slides in drug safety assessment studies, reducing the
costs and time associated with drug development.

</details>


### [9] [Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments](https://arxiv.org/abs/2509.00176)
*Muhammad Ali,Salman Khan*

Main category: cs.CV

TL;DR: 提出一个面向现实垃圾分类场景（复杂环境、物体形变、遮挡混乱）的新数据集，并用其系统评测视觉大语言模型（VLLM）的鲁棒性与准确性；结果显示现有VLLM在复杂场景下明显不足，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM多在规整、标准的自然图像上验证，缺乏对“杂乱环境、形变物体、遮挡”等真实复杂条件的检验。垃圾分类场景正好具有这些挑战，能更真实地暴露VLLM的泛化与鲁棒性问题。

Method: 1) 构建一个面向废弃物识别的图像数据集，包含复杂背景、物体形变与混杂场景；2) 设计一套细粒度、全面的评测方案，从准确率、鲁棒性等维度考察多种VLLM；3) 对模型在该数据集上的表现进行深入误差分析。

Result: 在该数据集上，主流VLLM的准确率与鲁棒性显著下降，尤其在遮挡、形变、背景复杂度高时错误率上升明显，暴露出多模态理解在复杂场景下的脆弱性。

Conclusion: 现有VLLM对复杂真实世界的适应性不足，需在鲁棒性、场景理解与形变感知方面继续改进。作者将公开数据与代码，为后续研究提供基准与分析工具。

Abstract: Recent advancements in Large Language Models (LLMs) have paved the way for
Vision Large Language Models (VLLMs) capable of performing a wide range of
visual understanding tasks. While LLMs have demonstrated impressive performance
on standard natural images, their capabilities have not been thoroughly
explored in cluttered datasets where there is complex environment having
deformed shaped objects. In this work, we introduce a novel dataset
specifically designed for waste classification in real-world scenarios,
characterized by complex environments and deformed shaped objects. Along with
this dataset, we present an in-depth evaluation approach to rigorously assess
the robustness and accuracy of VLLMs. The introduced dataset and comprehensive
analysis provide valuable insights into the performance of VLLMs under
challenging conditions. Our findings highlight the critical need for further
advancements in VLLM's robustness to perform better in complex environments.
The dataset and code for our experiments will be made publicly available.

</details>


### [10] [Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders](https://arxiv.org/abs/2509.00177)
*Faizan Farooq Khan,Vladan Stojnić,Zakaria Laskar,Mohamed Elhoseiny,Giorgos Tolias*

Main category: cs.CV

TL;DR: 提出将文本查询先用扩散模型生成“视觉查询”，再用图像-图像相似度进行检索，并通过聚合网络融合多张生成图及跨模态得分，显著优于仅用文本查询的VLM检索。


<details>
  <summary>Details</summary>
Motivation: 传统基于VLM（如CLIP）的开放词表检索直接在共同嵌入空间比较文本-图像，但两种模态在表示空间中仍存在较远分布与对齐不足，导致基于类别描述的检索效果受限。作者希望缩小模态鸿沟，充分利用强大的视觉编码器与生成模型以提升召回。

Method: 两步法：1) 文本到图像：用文本到图像扩散模型把文本查询生成若干候选图（视觉查询）。2) 图像到图像：用强视觉编码器计算生成图与库图像的相似度。并设计聚合网络：将多张生成图编码并聚合为单一向量；同时融合文本-图像和图像-图像两路相似度，得到最终排名。

Result: 在多项数据集上的广泛评测中，该方法在基于类别语义描述的检索上稳定超越仅依赖文本查询（如直接用CLIP）的基线。

Conclusion: 通过把文本查询显式“视觉化”并与图像相似度计算结合，再辅以聚合与跨模态得分融合，可有效弥合模态间距，提升文本到图像检索性能；代码已开源。

Abstract: This work explores text-to-image retrieval for queries that specify or
describe a semantic category. While vision-and-language models (VLMs) like CLIP
offer a straightforward open-vocabulary solution, they map text and images to
distant regions in the representation space, limiting retrieval performance. To
bridge this modality gap, we propose a two-step approach. First, we transform
the text query into a visual query using a generative diffusion model. Then, we
estimate image-to-image similarity with a vision model. Additionally, we
introduce an aggregation network that combines multiple generated images into a
single vector representation and fuses similarity scores across both query
modalities. Our approach leverages advancements in vision encoders, VLMs, and
text-to-image generation models. Extensive evaluations show that it
consistently outperforms retrieval methods relying solely on text queries.
Source code is available at: https://github.com/faixan-khan/cletir

</details>


### [11] [Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety](https://arxiv.org/abs/2509.00192)
*Younggun Kim,Sirnam Swetha,Fazil Kagdi,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出PRISM基准与Safe-LLaVA数据集，用于检测与缓解多模态大模型在视觉语言任务中的生物特征泄露，并证明经Safe-LLaVA微调后可显著降低泄露。


<details>
  <summary>Details</summary>
Motivation: MLLM在处理图像与文本时会显式/隐式推断敏感生物特征（如种族、性别、年龄、体重、眼睛颜色），即使未被请求，带来现实应用与社会领域隐私风险。目前缺乏公开数据集/基准系统评估这类泄露并指导缓解。

Method: 1) 构建PRISM基准：双重评测维度——(a) 对生物特征相关请求的拒答能力；(b) 在一般回答中隐式生物特征泄露的检测，同时要求保持语义忠实。2) 系统性审计主流LLaVA预训练与指令数据，量化并示例其生物特征泄露。3) 基于审计结果，制作Safe-LLaVA：从LLaVA中系统移除显式与隐式生物信息，形成隐私保护训练集。4) 以PRISM评测多种MLLM，并在Safe-LLaVA上微调模型作对比评估。

Result: 评测显示多种MLLM在多个属性上存在显著隐私泄露；使用Safe-LLaVA微调的模型在PRISM上泄露显著下降，同时维持语义一致性。

Conclusion: PRISM与Safe-LLaVA为隐私对齐的MLLM开发与评估提供了标准化工具链；经Safe-LLaVA训练可有效降低生物特征泄露。数据集与代码已开源，利于复现和后续研究。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language tasks. However, these models often infer and
reveal sensitive biometric attributes - such as race, gender, age, body weight,
and eye color - even when such information is not explicitly requested. This
raises critical concerns, particularly in real-world applications and
socially-sensitive domains. Despite increasing awareness, no publicly available
dataset or benchmark exists to comprehensively evaluate or mitigate biometric
leakage in MLLMs. To address this gap, we introduce PRISM (Privacy-aware
Evaluation of Responses in Sensitive Modalities), a new benchmark designed to
assess MLLMs on two fronts: (1) refuse biometric-related queries and (2)
implicit biometric leakage in general responses while maintaining semantic
faithfulness. Further, we conduct a detailed audit of the widely used LLaVA
datasets and uncover extensive biometric leakage across pretraining and
instruction data. To address this, we present Safe-LLaVA dataset, the first
privacy-preserving MLLM training dataset constructed by systematically removing
explicit and implicit biometric information from LLaVA dataset. Our evaluations
on PRISM reveal biometric leakages across MLLMs for different attributes,
highlighting the detailed privacy-violations. We also fine-tune a model on
Safe-LLaVA dataset and show that it substantially reduces the biometric
leakages. Together, Safe-LLaVA & PRISM set a new standard for privacy-aligned
development and evaluation of MLLMs. The Safe-LLaVA dataset & PRISM benchmark
are publicly available at https://huggingface.co/datasets/kyh9191/Safe-LLaVA,
and the source code is available at
https://github.com/Kimyounggun99/Safe-LLaVA.git.

</details>


### [12] [Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment](https://arxiv.org/abs/2509.00210)
*Jinzhou Tang,Jusheng zhang,Sidi Liu,Waikit Xiu,Qinhan Lv,Xiying Li*

Main category: cs.CV

TL;DR: 提出VEME：以自我中心、经验驱动的世界表征来强化VLM在动态环境中的时空推理与导航/EQA；通过跨模态对齐、隐式认知地图与指令驱动规划，提升泛化与探索效率，在VSI-Bench与VLN-CE上带来1%-3%的精度与效率增益。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型尽管擅长静态场景理解，但在动态、开放集任务（如任务导向导航、具身问答）中的时空推理与适应性不足，根因在于缺乏对细粒度时空线索与物理世界的建模与记忆机制，因此需要一种能在未知场景中泛化的具身世界模型。

Method: 提出VEME框架，核心包括：1）跨模态对齐：将对象、空间表示与视觉语义通过时空线索统一对齐，增强VLM的in-context学习；2）动态隐式认知地图：由“世界嵌入”激活，可按任务检索几何-语义记忆；3）指令驱动的导航与推理：利用具身先验进行长期规划与高效探索；整体以几何感知的时空“情景经验”作为训练/提示信号，形成以自我为中心、经验为核心的世界模型。

Result: 在VSI-Bench与VLN-CE两个具身基准上，相比传统方法，准确率与探索效率提升约1%-3%。

Conclusion: 通过将几何感知的时空经验嵌入到VLM，并用跨模态对齐与隐式认知地图支撑长期规划，VEME在动态未知场景中增强了推理与规划能力，实现了小幅但稳定的性能提升，显示出更好的泛化与适应性。

Abstract: Achieving human-like reasoning in deep learning models for complex tasks in
unknown environments remains a critical challenge in embodied intelligence.
While advanced vision-language models (VLMs) excel in static scene
understanding, their limitations in spatio-temporal reasoning and adaptation to
dynamic, open-set tasks like task-oriented navigation and embodied question
answering (EQA) persist due to inadequate modeling of fine-grained
spatio-temporal cues and physical world comprehension. To address this, we
propose VEME, a novel cross-modal alignment method that enhances generalization
in unseen scenes by learning an ego-centric, experience-centered world model.
Our framework integrates three key components: (1) a cross-modal alignment
framework bridging objects, spatial representations, and visual semantics with
spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,
implicit cognitive map activated by world embedding to enable task-relevant
geometric-semantic memory recall; and (3) an instruction-based navigation and
reasoning framework leveraging embodied priors for long-term planning and
efficient exploration. By embedding geometry-aware spatio-temporal episodic
experiences, our method significantly improves reasoning and planning in
dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate
1%-3% accuracy and exploration efficiency improvement compared to traditional
approaches.

</details>


### [13] [Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data](https://arxiv.org/abs/2509.00213)
*Farhan Fuad Abir,Abigail Elliott Daly,Kyle Anderman,Tolga Ozmen,Laura J. Brattain*

Main category: cs.CV

TL;DR: 提出一种将乳腺超声(BUS)图像与结构化临床数据融合的多模态深度学习框架，用于术前区分良性与交界/恶性叶状肿瘤；在81例确诊PT数据上，融合模型优于单模态，最佳AUC≈0.94，显示多模态AI有望减少不必要手术并改进决策。


<details>
  <summary>Details</summary>
Motivation: 叶状肿瘤罕见且与良性纤维腺瘤影像学相似，术前难以区分，导致过度活检/手术。需要一种在不增加侵入性的情况下提升诊断准确性的工具。

Method: 基于双分支神经网络：一支提取BUS图像特征，另一支提取患者结构化临床信息特征；后在融合层联合判别。采用81例确诊PT数据，进行类别感知采样缓解类别不平衡，并用按受试者分层的5折交叉验证避免数据泄漏。比较六种图像编码器（含ConvNeXt、ResNet18等），并与单模态基线对照。

Result: 多模态模型在区分良性与交界/恶性PT方面优于单模态基线。最佳图像编码器为ConvNeXt与ResNet18：在多模态设置下AUC-ROC分别为0.9427与0.9349；F1分别为0.6720与0.7294。

Conclusion: 多模态AI将BUS与临床数据融合可显著提升PT术前分级的准确性，具有作为非侵入性辅助诊断工具的潜力，有望减少不必要的活检/切除并优化乳腺肿瘤管理决策。

Abstract: Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are
difficult to classify preoperatively due to their radiological similarity to
benign fibroadenomas. This often leads to unnecessary surgical excisions. To
address this, we propose a multimodal deep learning framework that integrates
breast ultrasound (BUS) images with structured clinical data to improve
diagnostic accuracy. We developed a dual-branch neural network that extracts
and fuses features from ultrasound images and patient metadata from 81 subjects
with confirmed PTs. Class-aware sampling and subject-stratified 5-fold
cross-validation were applied to prevent class imbalance and data leakage. The
results show that our proposed multimodal method outperforms unimodal baselines
in classifying benign versus borderline/malignant PTs. Among six image
encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal
setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and
0.7294, respectively. This study demonstrates the potential of multimodal AI to
serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and
improving clinical decision-making in breast tumor management.

</details>


### [14] [GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery](https://arxiv.org/abs/2509.00226)
*René Parlange,Juan C. Cuevas-Tello,Octavio Valenzuela,Omar de J. Cabrera-Rosas,Tomás Verdugo,Anupreeta More,Anton T. Jaelani*

Main category: cs.CV

TL;DR: 提出GraViT：基于PyTorch的引力透镜自动检测流水线，系统评估大规模预训练的ViT与MLP-Mixer在迁移学习、训练策略与集成上的效果，并在公共数据集上复现实验、对比卷积基线与推理复杂度。


<details>
  <summary>Details</summary>
Motivation: LSST将发现约10^5级引力透镜，人工筛选不可行，需高精度、可扩展的自动分类器；同时需要理解预训练、数据质量与训练策略对透镜可探测性的影响，并与既有卷积方法公平对比。

Method: 构建GraViT流水线：采用ViT与MLP-Mixer等SOTA视觉主干的大规模预训练权重，进行迁移学习；系统变量包含数据源与规模（HOLISMOKES VI、SuGOHI X）、模型架构选择与微调方式、数据增强/归一化/优化策略，以及模型集成；复现先前系统比较的测试集实验，并加入复杂度与推理时间分析，与卷积网络基线对照。

Result: 在公共测试样本上，对10种架构进行了微调并与卷积基线对比；迁移学习与适当训练策略显著提升强引力透镜的检测性能；集成预测进一步提高准确性；并量化了不同模型在复杂度与推理时延上的差异。

Conclusion: 预训练的Transformer类模型在强引力透镜识别任务上具备竞争力或优于卷积基线；合理的微调与训练策略、以及模型集成，对性能提升关键；GraViT为即将到来的LSST规模数据提供了可扩展、可复现实验的检测框架。

Abstract: Gravitational lensing offers a powerful probe into the properties of dark
matter and is crucial to infer cosmological parameters. The Legacy Survey of
Space and Time (LSST) is predicted to find O(10^5) gravitational lenses over
the next decade, demanding automated classifiers. In this work, we introduce
GraViT, a PyTorch pipeline for gravitational lens detection that leverages
extensive pretraining of state-of-the-art Vision Transformer (ViT) models and
MLP-Mixer. We assess the impact of transfer learning on classification
performance by examining data quality (source and sample size), model
architecture (selection and fine-tuning), training strategies (augmentation,
normalization, and optimization), and ensemble predictions. This study
reproduces the experiments in a previous systematic comparison of neural
networks and provides insights into the detectability of strong gravitational
lenses on that common test sample. We fine-tune ten architectures using
datasets from HOLISMOKES VI and SuGOHI X, and benchmark them against
convolutional baselines, discussing complexity and inference-time analysis.

</details>


### [15] [A High-Accuracy Fast Hough Transform with Linear-Log-Cubed Computational Complexity for Arbitrary-Shaped Images](https://arxiv.org/abs/2509.00231)
*Danil Kazimirov,Dmitry Nikolaev*

Main category: cs.CV

TL;DR: 提出FHT2SP：在保持常数级近似误差（可由超像素大小调节）的同时，将霍夫变换的复杂度降至近似最优O(wh ln^3 w)，并适用于任意图像尺寸与形状。


<details>
  <summary>Details</summary>
Motivation: 现有快速霍夫变换（如Brady–Yong与FHT2DT）在复杂度上高效，但后者在任意尺寸时精度随尺度恶化；而高精度算法虽误差有常数上界，但代价接近立方。需要兼顾高精度与近最优复杂度且适用于任意尺寸/形状的算法。

Method: 基于Brady的超像素思想进行推广：将超像素从2的幂方正方形扩展到任意形状，并将其无缝整合进FHT2DT框架。通过选择合适的超像素大小（作为元参数），在分层累积与参数空间划分中实现高效、可控的近似。

Result: 理论与实验表明：对w×h图像，算法复杂度为O(wh ln^3 w)（近最优线性对数级），同时近似误差具常数上界且与图像尺寸无关，可由超像素大小调节。

Conclusion: FHT2SP在不牺牲精度的前提下，将任意尺寸图像的HT计算提升至近最优复杂度，弥合了FHT2DT（高效但降精度）与高精度HT（高成本）之间的鸿沟；适合需要规模化且高精度的HT应用。

Abstract: The Hough transform (HT) is a fundamental tool across various domains, from
classical image analysis to neural networks and tomography. Two key aspects of
the algorithms for computing the HT are their computational complexity and
accuracy - the latter often defined as the error of approximation of continuous
lines by discrete ones within the image region. The fast HT (FHT) algorithms
with optimal linearithmic complexity - such as the Brady-Yong algorithm for
power-of-two-sized images - are well established. Generalizations like $FHT2DT$
extend this efficiency to arbitrary image sizes, but with reduced accuracy that
worsens with scale. Conversely, accurate HT algorithms achieve constant-bounded
error but require near-cubic computational cost. This paper introduces $FHT2SP$
algorithm - a fast and highly accurate HT algorithm. It builds on our
development of Brady's superpixel concept, extending it to arbitrary shapes
beyond the original power-of-two square constraint, and integrates it into the
$FHT2DT$ algorithm. With an appropriate choice of the superpixel's size, for an
image of shape $w \times h$, the $FHT2SP$ algorithm achieves near-optimal
computational complexity $\mathcal{O}(wh \ln^3 w)$, while keeping the
approximation error bounded by a constant independent of image size, and
controllable via a meta-parameter. We provide theoretical and experimental
analyses of the algorithm's complexity and accuracy.

</details>


### [16] [Generative AI for Industrial Contour Detection: A Language-Guided Vision System](https://arxiv.org/abs/2509.00284)
*Liang Gong,Tommy,Wang,Sara Chaker,Yanchen Dong,Fouad Bousetouane,Brenden Morton,Mark Mendez*

Main category: cs.CV

TL;DR: 提出一种语言引导的生成式视觉系统用于制造业中的残余轮廓检测，三阶段流程（预处理→条件GAN生成→VLM多模态精修）在FabTrack数据上显著提升轮廓保真度，且在精修基准中GPT-image-1优于Gemini 2.0 Flash。


<details>
  <summary>Details</summary>
Motivation: 工业视觉在噪声、材料差异、成像不可控等条件下，传统边缘检测与手工规则管线鲁棒性差、精度不足，难以达到CAD级轮廓精度且依赖大量人工描边与后处理。作者希望用生成式与语言引导的多模态方法突破经典管线的瓶颈。

Method: 系统分三阶段：1) 数据获取与预处理；2) 条件GAN进行轮廓生成，生成初始残余轮廓；3) 多模态精修：通过人机协同制定标准化提示词，用视觉-语言模型执行图文引导的轮廓优化与合成。精修阶段对多种VLM（Gemini 2.0 Flash、GPT-image-1、若干开源基线）进行对比。

Result: 在专有FabTrack数据集上，系统提升了轮廓保真度（更好的边缘连续性与几何对齐），减少人工描边工作量。在精修基准中，在相同标准化条件下，GPT-image-1在结构准确性与感知质量上均稳定优于Gemini 2.0 Flash与开源基线。

Conclusion: 语言引导的生成式工作流能将工业计算机视觉推进到超越经典手工管线的水平，实现更接近CAD标准的轮廓检测；VLM（尤其GPT-image-1）在多模态精修中的效果突出，验证了人机协同与图文引导合成在工业场景的可行性与优势。

Abstract: Industrial computer vision systems often struggle with noise, material
variability, and uncontrolled imaging conditions, limiting the effectiveness of
classical edge detectors and handcrafted pipelines. In this work, we present a
language-guided generative vision system for remnant contour detection in
manufacturing, designed to achieve CAD-level precision. The system is organized
into three stages: data acquisition and preprocessing, contour generation using
a conditional GAN, and multimodal contour refinement through vision-language
modeling, where standardized prompts are crafted in a human-in-the-loop process
and applied through image-text guided synthesis. On proprietary FabTrack
datasets, the proposed system improved contour fidelity, enhancing edge
continuity and geometric alignment while reducing manual tracing. For the
refinement stage, we benchmarked several vision-language models, including
Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided
workflow, and open-source baselines. Under standardized conditions, GPT-image-1
consistently outperformed Gemini 2.0 Flash in both structural accuracy and
perceptual quality. These findings demonstrate the promise of VLM-guided
generative workflows for advancing industrial computer vision beyond the
limitations of classical pipelines.

</details>


### [17] [Language-Aware Information Maximization for Transductive Few-Shot CLIP](https://arxiv.org/abs/2509.00305)
*Ghassen Baklouti,Maxime Zanella,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 提出LIMO：一种面向VLM/CLIP的传导式小样本学习方法，利用信息论目标与PEFT微调，仅调少量参数即可在未标注测试集上提升分类性能，显著超过现有传导式与最优归纳式方法。


<details>
  <summary>Details</summary>
Motivation: 现有传导式小样本方法主要针对纯视觉模型；在VLM（如CLIP）语境中研究稀少，且已有工作表明传导式有潜力但缺乏专门设计。需要在利用未标注测试分布的同时，兼顾语言先验与稳定微调，避免灾难性偏移。

Method: 基于CLIP提出LIMO损失，由三部分构成：（i）最大化视觉输入与文本类描述之间的互信息；（ii）通过KL项约束模型输出不偏离文本驱动的零样本预测；（iii）对少量标注样本施加标准交叉熵。训练时采用参数高效微调（PEFT），仅更新模型参数子集（如适配层/LoRA等），并系统性探索传导式设置下的微调策略。

Result: 在多项基准的全面评测中，LIMO显著优于近期传导式few-shot CLIP方法，并相对最强的归纳式方法也有明显增益；实验还显示在传导式场景中采用PEFT能带来“意外的”大幅性能提升。

Conclusion: 在VLM中进行传导式few-shot学习，结合信息最大化与零样本先验的正则化，并配合PEFT局部微调，是有效且稳健的途径；LIMO树立了新的性能标杆并验证了只调少量参数即可充分利用未标注测试数据的价值。

Abstract: Transductive few-shot learning has triggered an abundant literature focusing
on vision-only models, but is still at a nascent stage within the recent
context of foundational vision-language models (VLMs). Only a few recent
methods addressed the problem, pointing to the potential of tranduction in VLMs
and to the need for VLM-tailored methods. Building on this momentum, we
leverage information-theoretic concepts and recent progress in
parameter-efficient fine-tuning (PEFT), developing a highly competitive
transductive few-shot CLIP method. Specifically, we introduce a novel
Language-aware Information MaximizatiOn (LIMO) loss integrating three
complementary terms: (i) the mutual information between the vision inputs and
the textual class descriptions; (ii) a Kullback-Leibler (KL) divergence
penalizing deviation of the network's probabilistic outputs from the
text-driven zero-shot predictions; and (iii) a standard cross-entropy loss
based on the labeled shots. Furthermore, we challenge the commonly followed
fine-tuning practices in the context of transductive few-shot learning, and
explore PEFT strategies, completely overlooked in this context. Surprisingly,
we observe substantial boosts in performances, which points to the potential of
adapting a subset of the model's parameters in the transductive few-shot
setting. We report comprehensive evaluations, which show that LIMO outperforms
the very recent transductive few-shot CLIP methods by a large margin and yields
significant gains over the best-performing inductive methods. Our code is
publicly available at:\[
\href{https://github.com/ghassenbaklouti/LIMO}{\text{here}} \]

</details>


### [18] [MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification](https://arxiv.org/abs/2509.00311)
*Hikmat Khan,Syed Farhan Alam Zaidi,Pir Masoom Shah,Kiruthika Balakrishnan,Rabia Khan,Muhammad Waqas,Jia Wu*

Main category: cs.CV

TL;DR: 提出MorphGen：在监督对比学习中对齐图像与细胞核分割掩膜的表征，优先捕捉核与形态学异常及空间组织，从而提升病理WSI在跨域与干扰下的稳健性；并结合SWA获得更平坦解，表现出OOD泛化、抗腐蚀与抗对抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: WSI跨机构域差异（制备、染色、成像）导致模型依赖非诊断性、易变的域特征而掉点；而病理医师依赖稳健的域不变形态学线索（核异型性、结构异型等）。需要一种显式建模生物学稳健形态信号的方法以提升泛化与安全性。

Method: 提出MorphGen（形态引导泛化）：1) 输入包括组织图像、图像增强视图与核分割掩膜；2) 在监督对比学习框架中，对齐图像与对应核掩膜的潜在表征，使模型聚焦核形态、空间组织等诊断性特征而弱化染色与域特征；3) 引入SWA进行随机权重平均，引导优化到更平坦极小值，提高OOD稳健性；4) 通过注意力图分析验证模型关注区域。

Result: 注意力图显示模型主要依赖核形态、细胞组成与空间组织进行分类；在含染色污染等图像腐蚀与对抗攻击下表现稳健，OOD泛化性能优于对比方法；提供代码、数据与模型。

Conclusion: 显式对齐图像与核级形态学表征并结合SWA，可学习到对域偏移与攻击更鲁棒的癌症表征，缓解数字病理中深度模型的关键脆弱性。

Abstract: Domain generalization in computational histopathology is hindered by
heterogeneity in whole slide images (WSIs), caused by variations in tissue
preparation, staining, and imaging conditions across institutions. Unlike
machine learning systems, pathologists rely on domain-invariant morphological
cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,
chromatin texture, spatial disorganization), structural atypia (abnormal
architecture and gland formation), and overall morphological atypia that remain
diagnostic across diverse settings. Motivated by this, we hypothesize that
explicitly modeling biologically robust nuclear morphology and spatial
organization will enable the learning of cancer representations that are
resilient to domain shifts. We propose MorphGen (Morphology-Guided
Generalization), a method that integrates histopathology images, augmentations,
and nuclear segmentation masks within a supervised contrastive learning
framework. By aligning latent representations of images and nuclear masks,
MorphGen prioritizes diagnostic features such as nuclear and morphological
atypia and spatial organization over staining artifacts and domain-specific
features. To further enhance out-of-distribution robustness, we incorporate
stochastic weight averaging (SWA), steering optimization toward flatter minima.
Attention map analyses revealed that MorphGen primarily relies on nuclear
morphology, cellular composition, and spatial cell organization within tumors
or normal regions for final classification. Finally, we demonstrate resilience
of the learned representations to image corruptions (such as staining
artifacts) and adversarial attacks, showcasing not only OOD generalization but
also addressing critical vulnerabilities in current deep learning systems for
digital pathology. Code, datasets, and trained models are available at:
https://github.com/hikmatkhan/MorphGen

</details>


### [19] [Towards Adaptive Visual Token Pruning for Large Multimodal Models](https://arxiv.org/abs/2509.00320)
*Hao Zhang,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 提出一种仅对视觉token进行剪枝的策略，在保证跨模态对齐和视觉内部信息多样性的前提下，大幅减少推理token数（约-88.9%），在LLaVA系模型上保持性能同时将推理速度提升约56.7%。


<details>
  <summary>Details</summary>
Motivation: 现有LMM将视觉特征转为大量稠密token，与文本token拼接送入LLM，导致推理时计算与显存开销激增。已有token剪枝方法需要额外校准成本或使用欠佳的重要性指标，保留了许多冗余token。作者观察到视觉与文本token冗余度不同，旨在更有针对性地降低视觉token冗余且不牺牲性能。

Method: 仅在视觉token上执行两阶段剪枝：1) 基于互信息的跨模态对齐剪枝：估计视觉token与文本token之间的语义对齐（互信息），移除与文本语义不匹配的视觉token，以保留跨模态一致性。2) 基于多样性的冗余消解：在剩余视觉token中，通过最大化嵌入空间的期望成对距离来选择子集，确保信息多样性；使用高效贪心算法求解以控制开销。最终将保留的视觉token与文本token一并送入LLM。

Result: 在LLaVA-1.5-7B、LLaVA-NEXT-7B等模型上，平均剪掉约88.9%的视觉token，同时基本保持任务性能；因序列显著缩短，实现约56.7%的推理速度提升。

Conclusion: 视觉token存在更高冗余度，专注于视觉侧并兼顾跨模态对齐与内在多样性的剪枝可在不明显损伤性能的前提下大幅降本提速，具有通用性与工程可行性。

Abstract: Large Multimodal Models (LMMs) have achieved significant success across
various tasks. These models usually encode visual inputs into dense token
sequences, which are then concatenated with textual tokens and jointly
processed by a language model. However, the increased token count substantially
raises computational and memory costs during inference. Token pruning has
emerged as a promising approach to address this issue. Existing token pruning
methods often rely on costly calibration or suboptimal importance metrics,
leading to redundant retained tokens. In this paper, we analyze the redundancy
differences between visual and textual tokens and propose pruning exclusively
on visual tokens. Based on this, we propose a visual token pruning strategy
that explicitly preserves both cross-modal alignment and intra-modal
informational diversity. We introduce a mutual information-based token pruning
strategy that removes visual tokens semantically misaligned with textual
tokens, effectively preserving the alignment between the visual and textual
modalities. To further improve the representational quality of the retained
tokens, we additionally prune redundant visual tokens by maximizing the
expected pairwise distances in the embedding space, which is solved efficiently
with a greedy algorithm. Extensive experiments demonstrate that our method
maintains strong performance while reducing tokens by 88.9% on models such as
LLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference
speed.

</details>


### [20] [CryptoFace: End-to-End Encrypted Face Recognition](https://arxiv.org/abs/2509.00332)
*Wei Ao,Vishnu Naresh Boddeti*

Main category: cs.CV

TL;DR: 提出CryptoFace：首个端到端使用同态加密(FHE)的面部识别系统，实现特征提取、存储与匹配全流程在密文中完成；通过浅层补丁卷积网络与并行FHE评估，降低乘法深度、加速推理并提升验证准确度。


<details>
  <summary>Details</summary>
Motivation: 传统人脸识别在认证、安防与个性化等应用中普及，但明文图像/特征在提取、存储、匹配流程中易被窃取，隐私与合规风险高。现有FHE神经网络在高维图像上的延迟与精度受限，难以实用。

Method: 1) 设计混合浅层“补丁卷积”网络：将高维图像分块处理，减少卷积路径的乘法深度；2) 在FHE下全流程运算（特征提取、库中存储与相似度匹配均在密文域进行）；3) 并行化FHE评估与补丁处理，实现近似与分辨率无关的延迟；4) 与现有FHE NN方法对比与基准评测。

Result: 在标准人脸识别基准上，相比将通用FHE网络改造用于人脸的方案，CryptoFace实现更快的推理速度与更高的验证准确率，同时保证端到端密文处理。

Conclusion: CryptoFace证明了端到端FHE在人脸识别中的可行性与优越性，为需要强隐私与可证明安全的实用化人脸系统提供方案与实现代码。

Abstract: Face recognition is central to many authentication, security, and
personalized applications. Yet, it suffers from significant privacy risks,
particularly arising from unauthorized access to sensitive biometric data. This
paper introduces CryptoFace, the first end-to-end encrypted face recognition
system with fully homomorphic encryption (FHE). It enables secure processing of
facial data across all stages of a face-recognition process--feature
extraction, storage, and matching--without exposing raw images or features. We
introduce a mixture of shallow patch convolutional networks to support
higher-dimensional tensors via patch-based processing while reducing the
multiplicative depth and, thus, inference latency. Parallel FHE evaluation of
these networks ensures near-resolution-independent latency. On standard face
recognition benchmarks, CryptoFace significantly accelerates inference and
increases verification accuracy compared to the state-of-the-art FHE neural
networks adapted for face recognition. CryptoFace will facilitate secure face
recognition systems requiring robust and provable security. The code is
available at https://github.com/human-analysis/CryptoFace.

</details>


### [21] [LUT-Fuse: Towards Extremely Fast Infrared and Visible Image Fusion via Distillation to Learnable Look-Up Tables](https://arxiv.org/abs/2509.00346)
*Xunpeng Yi,Yibing Zhang,Xinyu Xiang,Qinglong Yan,Han Xu,Jiayi Ma*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Current advanced research on infrared and visible image fusion primarily
focuses on improving fusion performance, often neglecting the applicability on
real-time fusion devices. In this paper, we propose a novel approach that
towards extremely fast fusion via distillation to learnable lookup tables
specifically designed for image fusion, termed as LUT-Fuse. Firstly, we develop
a look-up table structure that utilizing low-order approximation encoding and
high-level joint contextual scene encoding, which is well-suited for
multi-modal fusion. Moreover, given the lack of ground truth in multi-modal
image fusion, we naturally proposed the efficient LUT distillation strategy
instead of traditional quantization LUT methods. By integrating the performance
of the multi-modal fusion network (MM-Net) into the MM-LUT model, our method
achieves significant breakthroughs in efficiency and performance. It typically
requires less than one-tenth of the time compared to the current lightweight
SOTA fusion algorithms, ensuring high operational speed across various
scenarios, even in low-power mobile devices. Extensive experiments validate the
superiority, reliability, and stability of our fusion approach. The code is
available at https://github.com/zyb5/LUT-Fuse.

</details>


### [22] [Target-Oriented Single Domain Generalization](https://arxiv.org/abs/2509.00351)
*Marzi Heidari,Yuhong Guo*

Main category: cs.CV

TL;DR: 提出TO-SDG新设定：利用目标域的文本描述（无目标数据）引导单源域泛化；并给出STAR模块，通过VLM文本锚定与谱投影等，使模型在分类与检测上显著更稳健。


<details>
  <summary>Details</summary>
Motivation: 单源域泛化常在分布移位下失效，现有方法多做数据增强或学习不变特征，却忽视了实际可得的目标环境文本描述这一信息源。希望在无目标数据但有文本元数据时提升部署鲁棒性。

Method: 1) 定义TO-SDG：只给目标域文本描述，不给目标图像。2) 提出STAR模块：a) 利用VLM（如CLIP）将目标文本嵌入为“目标锚定子空间”，对源图像特征做重心校正，使其靠近目标语义；b) 谱投影：保留与目标线索对齐的特征方向，滤除源域特有噪声；c) 视觉-语言蒸馏：将骨干特征对齐到VLM的语义几何；d) 特征空间Mixup：在源域与目标导向表示间做平滑过渡，提升稳定性。

Result: 在多种图像分类与目标检测基准上，STAR优于现有SDG方法，表现出更好的跨域泛化能力。

Conclusion: 少量、实用的目标域文本元数据可显著提升单源域在看不见目标数据情况下的泛化；STAR为此提供轻量有效的实现，提示将文本描述纳入域泛化是一条可行新途径。

Abstract: Deep models trained on a single source domain often fail catastrophically
under distribution shifts, a critical challenge in Single Domain Generalization
(SDG). While existing methods focus on augmenting source data or learning
invariant features, they neglect a readily available resource: textual
descriptions of the target deployment environment. We propose Target-Oriented
Single Domain Generalization (TO-SDG), a novel problem setup that leverages the
textual description of the target domain, without requiring any target data, to
guide model generalization. To address TO-SDG, we introduce Spectral TARget
Alignment (STAR), a lightweight module that injects target semantics into
source features by exploiting visual-language models (VLMs) such as CLIP. STAR
uses a target-anchored subspace derived from the text embedding of the target
description to recenter image features toward the deployment domain, then
utilizes spectral projection to retain directions aligned with target cues
while discarding source-specific noise. Moreover, we use a vision-language
distillation to align backbone features with VLM's semantic geometry. STAR
further employs feature-space Mixup to ensure smooth transitions between source
and target-oriented representations. Experiments across various image
classification and object detection benchmarks demonstrate STAR's superiority.
This work establishes that minimal textual metadata, which is a practical and
often overlooked resource, significantly enhances generalization under severe
data constraints, opening new avenues for deploying robust models in target
environments with unseen data.

</details>


### [23] [AQFusionNet: Multimodal Deep Learning for Air Quality Index Prediction with Imagery and Sensor Data](https://arxiv.org/abs/2509.00353)
*Koushik Ahmed Kushal,Abdullah Al Mamun*

Main category: cs.CV

TL;DR: 提出AQFusionNet，多模态轻量级深度学习框架，将地面大气图像与污染物传感数据融合，在资源受限地区实现高精度AQI预测；在印度与尼泊尔8k+样本上优于单模态，最高92.02%分类准确、RMSE 7.70，并具备低算力开销与边缘端可部署性。


<details>
  <summary>Details</summary>
Motivation: 资源受限地区监测网络稀疏、基础设施不足，传统依赖密集传感器或单一模态的方法在覆盖、鲁棒性与成本上受限。需要一种既能利用有限传感数据，又能从廉价获取的地面图像提取空气质量线索的框架，以提升AQI预测准确性和可用性。

Method: 构建多模态架构：1) 视觉分支采用轻量CNN骨干（MobileNetV2/ResNet18/EfficientNet-B0）提取图像特征；2) 传感分支处理污染物浓度等结构化数据；3) 通过语义对齐的嵌入空间进行特征融合（跨模态对齐/投影），实现联合表示；4) 融合表示用于AQI分类/回归；5) 以低参数量与计算量设计，面向边缘设备。

Result: 在印度与尼泊尔>8,000样本上，多模态模型相对单模态基线平均提升18.5%；使用EfficientNet-B0时，分类准确率达92.02%，回归RMSE达7.70；在算力开销方面保持轻量，可在边缘端运行；在部分传感器缺失场景仍具鲁棒性。

Conclusion: AQFusionNet通过图像与传感数据的语义对齐融合，有效提升资源受限环境中的AQI预测精度与稳健性；兼顾性能与效率，具备可扩展与边缘部署能力，适用于传感器稀疏、数据部分缺失的实际场景。

Abstract: Air pollution monitoring in resource-constrained regions remains challenging
due to sparse sensor deployment and limited infrastructure. This work
introduces AQFusionNet, a multimodal deep learning framework for robust Air
Quality Index (AQI) prediction. The framework integrates ground-level
atmospheric imagery with pollutant concentration data using lightweight CNN
backbones (MobileNetV2, ResNet18, EfficientNet-B0). Visual and sensor features
are combined through semantically aligned embedding spaces, enabling accurate
and efficient prediction. Experiments on more than 8,000 samples from India and
Nepal demonstrate that AQFusionNet consistently outperforms unimodal baselines,
achieving up to 92.02% classification accuracy and an RMSE of 7.70 with the
EfficientNet-B0 backbone. The model delivers an 18.5% improvement over
single-modality approaches while maintaining low computational overhead, making
it suitable for deployment on edge devices. AQFusionNet provides a scalable and
practical solution for AQI monitoring in infrastructure-limited environments,
offering robust predictive capability even under partial sensor availability.

</details>


### [24] [Iterative Low-rank Network for Hyperspectral Image Denoising](https://arxiv.org/abs/2509.00356)
*Jin Ye,Fengchao Xiong,Jun Zhou,Yuntao Qian*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Hyperspectral image (HSI) denoising is a crucial preprocessing step for
subsequent tasks. The clean HSI usually reside in a low-dimensional subspace,
which can be captured by low-rank and sparse representation, known as the
physical prior of HSI. It is generally challenging to adequately use such
physical properties for effective denoising while preserving image details.
This paper introduces a novel iterative low-rank network (ILRNet) to address
these challenges. ILRNet integrates the strengths of model-driven and
data-driven approaches by embedding a rank minimization module (RMM) within a
U-Net architecture. This module transforms feature maps into the wavelet domain
and applies singular value thresholding (SVT) to the low-frequency components
during the forward pass, leveraging the spectral low-rankness of HSIs in the
feature domain. The parameter, closely related to the hyperparameter of the
singular vector thresholding algorithm, is adaptively learned from the data,
allowing for flexible and effective capture of low-rankness across different
scenarios. Additionally, ILRNet features an iterative refinement process that
adaptively combines intermediate denoised HSIs with noisy inputs. This manner
ensures progressive enhancement and superior preservation of image details.
Experimental results demonstrate that ILRNet achieves state-of-the-art
performance in both synthetic and real-world noise removal tasks.

</details>


### [25] [SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding](https://arxiv.org/abs/2509.00357)
*Zhen Chen,Xingjian Luo,Kun Yuan,Jinlin Wu,Danny T. M. Chan,Nassir Navab,Hongbin Liu,Zhen Lei,Jiebo Luo*

Main category: cs.CV

TL;DR: 提出SurgLLM，一个面向手术视频的多模态大模型，通过空间聚焦与时间建模显著提升手术视频理解（描述、通用VQA、时序VQA），在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有手术视频理解存在两大瓶颈：1) 对关键可视内容（如器械、解剖结构）感知不足；2) 时间关系/长程依赖建模薄弱，限制CAS系统的通用性与可靠性。需要一个既能空间聚焦又具备时序推理的统一多任务框架。

Method: 提出SurgLLM框架，包含三大模块：1) Surg-Pretrain：对视频编码器进行“器械中心”的掩码视频重建（MV-Recon）以强化空间关注，并进行跨模态对齐以连接视觉与语言表征；2) TM-Tuning：通过交错式多模态嵌入提升时序推理能力，实现时间感知的多模态微调；3) Surgical Task Dynamic Ensemble：为不同任务（描述、VQA、时序VQA）动态选择最优可学习参数子集，减少任务冲突、提升通用性。

Result: 在多种手术视频任务上（视频描述、通用VQA、时序VQA）进行广泛实验，均超过现有SOTA方法，显示更强的空间与时间理解能力。

Conclusion: SurgLLM通过器械感知的预训练、时间感知的多模态调优与任务动态集成，有效缓解手术视频空间与时间建模不足的问题，实现通用、可扩展的手术视频理解；代码已开源。

Abstract: Surgical video understanding is crucial for facilitating Computer-Assisted
Surgery (CAS) systems. Despite significant progress in existing studies, two
major limitations persist, including inadequate visual content perception and
insufficient temporal awareness in surgical videos, and hinder the development
of versatile CAS solutions. In this work, we propose the SurgLLM framework, an
effective large multimodal model tailored for versatile surgical video
understanding tasks with enhanced spatial focus and temporal awareness.
Specifically, to empower the spatial focus of surgical videos, we first devise
Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video
encoder of SurgLLM, by performing instrument-centric Masked Video
Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate
surgical temporal knowledge into SurgLLM, we further propose Temporal-aware
Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved
multimodal embeddings. Moreover, to accommodate various understanding tasks of
surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble
to efficiently triage a query with optimal learnable parameters in our SurgLLM.
Extensive experiments performed on diverse surgical video understanding tasks,
including captioning, general VQA, and temporal VQA, demonstrate significant
improvements over the state-of-the-art approaches, validating the effectiveness
of our SurgLLM in versatile surgical video understanding. The source code is
available at https://github.com/franciszchen/SurgLLM.

</details>


### [26] [A Multimodal Head and Neck Cancer Dataset for AI-Driven Precision Oncology](https://arxiv.org/abs/2509.00367)
*Numan Saeed,Salma Hassan,Shahad Hardan,Ahmed Aly,Darya Taratynova,Umair Nawaz,Ufaq Khan,Muhammad Ridzuan,Thomas Eugene,Rapha"el Metz,M'elanie Dore,Gregory Delpon,Vijay Ram Kumar Papineni,Kareem Wahid,Cem Dede,Alaa Mohamed Shawky Ali,Carlos Sjogreen,Mohamed Naser,Clifton D. Fuller,Valentin Oreiller,Mario Jreige,John O. Prior,Catherine Cheze Le Rest,Olena Tankyevych,Pierre Decazes,Su Ruan,Stephanie Tanadini-Lang,Martin Valli`eres,Hesham Elhalawani,Ronan Abgral,Romain Floch,Kevin Kerleguer,Ulrike Schick,Maelle Mauguen,Vincent Andrearczyk,Adrien Depeursinge,Mathieu Hatt,Arman Rahmim,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 公开的头颈部肿瘤FDG-PET/CT多中心数据集（1123例），含标准化专家标注的GTVp/GTVn分割、部分放疗剂量分布与丰富临床随访元数据；并提供三大任务（自动分割、生存预测、HPV分类）的基线结果。


<details>
  <summary>Details</summary>
Motivation: 头颈癌研究需要大规模、标准化且多模态（影像+临床+治疗）的公开数据来推动算法开发与可比性，但现有数据集规模小、标注不足、中心单一、临床与预后信息缺失。

Method: - 多中心（10家机构）收集1123例经病理确诊的头颈癌FDG-PET/CT。
- 所有检查均为配准的PET/CT，保留真实临床采集差异。
- 由资深放疗科/影像科专家依据统一指南手工分割GTVp与GTVn，设质控流程。
- 发布匿名化NifTi影像、对应分割掩膜、部分患者放疗剂量分布，以及包含分期、HPV、人口学、随访结局、生存与治疗信息的临床元数据。
- 以UNet、SegResNet及多模态预后框架在三类任务上给出基线：自动分割、复发无进展生存预测、HPV状态分类。

Result: 形成一个规模大、异质性高且高质量标注的PET/CT+临床数据集，并在三项关键任务上提供可复现的SOTA深度学习基线结果，展示数据集的实用性与挑战性。

Conclusion: 该数据集为头颈癌影像组学与多模态AI研究提供了标准化基准与丰富标签，适用于分割、预后与生物标志预测等任务，可促进跨机构算法泛化研究与临床转化。

Abstract: We describe a publicly available multimodal dataset of annotated Positron
Emission Tomography/Computed Tomography (PET/CT) studies for head and neck
cancer research. The dataset includes 1123 FDG-PET/CT studies from patients
with histologically confirmed head and neck cancer, acquired from 10
international medical centers. All examinations consisted of co-registered
PET/CT scans with varying acquisition protocols, reflecting real-world clinical
diversity across institutions. Primary gross tumor volumes (GTVp) and involved
lymph nodes (GTVn) were manually segmented by experienced radiation oncologists
and radiologists following standardized guidelines and quality control
measures. We provide anonymized NifTi files of all studies, along with
expert-annotated segmentation masks, radiotherapy dose distribution for a
subset of patients, and comprehensive clinical metadata. This metadata includes
TNM staging, HPV status, demographics (age and gender), long-term follow-up
outcomes, survival times, censoring indicators, and treatment information. We
demonstrate how this dataset can be used for three key clinical tasks:
automated tumor segmentation, recurrence-free survival prediction, and HPV
status classification, providing benchmark results using state-of-the-art deep
learning models, including UNet, SegResNet, and multimodal prognostic
frameworks.

</details>


### [27] [Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs](https://arxiv.org/abs/2509.00371)
*Guangzong Si,Hao Yin,Xianfei Li,Qing Ding,Wenlong Liao,Tao He,Pai Peng*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive advances,
yet object hallucination remains a persistent challenge. Existing methods,
based on the flawed assumption that omission and fabrication hallucinations
share a common cause, often reduce omissions only to trigger more fabrications.
In this work, we overturn this view by demonstrating that omission
hallucinations arise from insufficient confidence when mapping perceived visual
features to linguistic expressions, whereas fabrication hallucinations result
from spurious associations within the cross-modal representation space due to
statistical biases in the training corpus. Building on findings from visual
attention intervention experiments, we propose the Visual-Semantic Attention
Potential Field, a conceptual framework that reveals how the model constructs
visual evidence to infer the presence or absence of objects. Leveraging this
insight, we introduce Visual Potential Field Calibration (VPFC), a
plug-and-play hallucination mitigation method that effectively reduces omission
hallucinations without introducing additional fabrication hallucinations. Our
findings reveal a critical oversight in current object hallucination research
and chart new directions for developing more robust and balanced hallucination
mitigation strategies.

</details>


### [28] [Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models](https://arxiv.org/abs/2509.00373)
*Sihao Wu,Gaojie Jin,Wei Huang,Jianhong Wang,Xiaowei Huang*

Main category: cs.CV

TL;DR: 提出SPO-VLM：一个两阶段的VLM安全防御框架，先用自适应分层激活引导抑制有害行为，再以序列级偏好优化结合毒性与视觉一致性奖励精修，从而在不损伤视觉理解的前提下显著提升对抗鲁棒性与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽强，但对对抗攻击脆弱。激活引导（activation steering）是潜力方向，但依赖任务/提示特定的对比方向，泛化差、可能伤害视觉对齐与定位能力。因此需要一种既能广泛抑制有害行为、又不牺牲视觉语义与普通任务性能的防御方法。

Method: 两阶段框架SPO-VLM：
- 阶段I（激活层面干预）：从多样数据源提取自适应、分层的steering vectors，在推理时对中间激活进行方向性调控，广义地压制有害倾向。
- 阶段II（策略层面优化）：对阶段I的向量进行序列级偏好优化（类似RLHF/SFT上的SPO范式），引入自动毒性评估作为安全奖励，同时以图文对齐（caption-image alignment）度量作为视觉一致性奖励，促使生成既安全又语义落地。两阶段兼顾效率（轻量即时干预）与效果（更深层策略修正）。

Result: 广泛实验表明：相比现有基于提示的激活引导方法，SPO-VLM对对抗攻击更稳健，安全性提升明显；同时在良性任务上保持强性能，未牺牲视觉理解/视觉落地能力。

Conclusion: SPO-VLM通过“激活干预+序列偏好优化”的组合，提供可泛化、低开销且有效的VLM安全防御路径，兼顾安全与视觉语义保真；作者将开源代码、权重与评测工具以促进复现与后续研究。

Abstract: Vision Language Models (VLMs) have demonstrated impressive capabilities in
integrating visual and textual information for understanding and reasoning, but
remain highly vulnerable to adversarial attacks. While activation steering has
emerged as a promising defence, existing approaches often rely on task-specific
contrastive prompts to extract harmful directions, which exhibit suboptimal
performance and can degrade visual grounding performance. To address these
limitations, we propose \textit{Sequence-Level Preference Optimization} for VLM
(\textit{SPO-VLM}), a novel two-stage defense framework that combines
activation-level intervention with policy-level optimization to enhance model
robustness. In \textit{Stage I}, we compute adaptive layer-specific steering
vectors from diverse data sources, enabling generalized suppression of harmful
behaviors during inference. In \textit{Stage II}, we refine these steering
vectors through a sequence-level preference optimization process. This stage
integrates automated toxicity assessment, as well as visual-consistency rewards
based on caption-image alignment, to achieve safe and semantically grounded
text generation. The two-stage structure of SPO-VLM balances efficiency and
effectiveness by combining a lightweight mitigation foundation in Stage I with
deeper policy refinement in Stage II. Extensive experiments shown SPO-VLM
enhances safety against attacks via activation steering and preference
optimization, while maintaining strong performance on benign tasks without
compromising visual understanding capabilities. We will release our code, model
weights, and evaluation toolkit to support reproducibility and future research.
\textcolor{red}{Warning: This paper may contain examples of offensive or
harmful text and images.}

</details>


### [29] [Adaptive Point-Prompt Tuning: Fine-Tuning Heterogeneous Foundation Models for 3D Point Cloud Analysis](https://arxiv.org/abs/2509.00374)
*Mengke Li,Lihao Chen,Peng Zhang,Yiu-ming Cheung,Hui Huang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Parameter-efficient fine-tuning strategies for foundation models in 1D
textual and 2D visual analysis have demonstrated remarkable efficacy. However,
due to the scarcity of point cloud data, pre-training large 3D models remains a
challenging task. While many efforts have been made to apply pre-trained visual
models to 3D domains through "high-to-low" mapping, these approaches often lead
to the loss of spatial geometries and lack a generalizable framework for
adapting any modality to 3D. This paper, therefore, attempts to directly
leverage point features to calibrate the heterogeneous foundation model of any
modality for 3D point cloud analysis. Specifically, we propose the Adaptive
Point-Prompt Tuning (APPT) method, which fine-tunes pre-trained models with a
modest number of parameters, enabling direct point cloud processing without
heterogeneous mappings. We convert raw point clouds into point embeddings by
aggregating local geometry to capture spatial features followed by linear
layers to ensure seamless utilization of frozen pre-trained models. Given the
inherent disorder of point clouds, in contrast to the structured nature of
images and language, we employ a permutation-invariant feature to capture the
relative positions of point embeddings, thereby obtaining point tokens enriched
with location information to optimize self-attention mechanisms. To calibrate
self-attention across source domains of any modality to 3D and reduce
computational overhead, we introduce a prompt generator that shares weights
with the point embedding module, dynamically producing point-prompts without
adding additional parameters. These prompts are then concatenated into a frozen
foundation model, providing rich global structural information and compensating
for the lack of structural context in the heterogeneous data.

</details>


### [30] [NoiseCutMix: A Novel Data Augmentation Approach by Mixing Estimated Noise in Diffusion Models](https://arxiv.org/abs/2509.00378)
*Shumpei Takezaki,Ryoma Bise,Shinnosuke Matsuo*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: In this study, we propose a novel data augmentation method that introduces
the concept of CutMix into the generation process of diffusion models, thereby
exploiting both the ability of diffusion models to generate natural and
high-resolution images and the characteristic of CutMix, which combines
features from two classes to create diverse augmented data. Representative data
augmentation methods for combining images from multiple classes include CutMix
and MixUp. However, techniques like CutMix often result in unnatural boundaries
between the two images due to contextual differences. Therefore, in this study,
we propose a method, called NoiseCutMix, to achieve natural, high-resolution
image generation featuring the fused characteristics of two classes by
partially combining the estimated noise corresponding to two different classes
in a diffusion model. In the classification experiments, we verified the
effectiveness of the proposed method by comparing it with conventional data
augmentation techniques that combine multiple classes, random image generation
using Stable Diffusion, and combinations of these methods. Our codes are
available at: https://github.com/shumpei-takezaki/NoiseCutMix

</details>


### [31] [Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation](https://arxiv.org/abs/2509.00379)
*Jialiang Kang,Jiawen Wang,Dingsheng Luo*

Main category: cs.CV

TL;DR: 提出两种跨模态知识蒸馏方法（UDAKD、FSKD），用2D图像模型指导3D LiDAR语义分割，无需3D标注，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D LiDAR语义分割对自动驾驶关键，但3D点云标注昂贵且耗时；相比之下，2D图像数据大量且易得。因此希望利用现成的2D模型与未标注2D数据，将知识迁移到3D，减少或免除3D标注。

Method: 利用自动驾驶中同步的相机-激光雷达数据和已知2D-3D对应关系：1) UDAKD：基于无监督域自适应的跨模态知识蒸馏，将预训练2D模型在未标注图像上的输出对齐到3D网络的点云输出；2) FSKD：从特征与语义层面对齐，强调保留模态无关信息、抑制模态特异信息。为提高对齐稳健性与域适应能力，在3D分支中引入自校准卷积作为域自适应模块的基础。

Result: 在多项严格实验中，两种方法在无需3D标注的设置下，稳定且显著超越现有SOTA方案的性能。

Conclusion: 跨模态蒸馏结合2D-3D对应与自校准卷积，可有效用2D知识指导3D LiDAR语义分割，减少标注成本并提升精度，验证了保留模态通用信息、过滤模态特异信息的有效性。

Abstract: Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous
driving. Traditional approaches rely on extensive annotated data for point
cloud analysis, incurring high costs and time investments. In contrast,
realworld image datasets offer abundant availability and substantial scale. To
mitigate the burden of annotating 3D LiDAR point clouds, we propose two
crossmodal knowledge distillation methods: Unsupervised Domain Adaptation
Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge
Distillation (FSKD). Leveraging readily available spatio-temporally
synchronized data from cameras and LiDARs in autonomous driving scenarios, we
directly apply a pretrained 2D image model to unlabeled 2D data. Through
crossmodal knowledge distillation with known 2D-3D correspondence, we actively
align the output of the 3D network with the corresponding points of the 2D
network, thereby obviating the necessity for 3D annotations. Our focus is on
preserving modality-general information while filtering out modality-specific
details during crossmodal distillation. To achieve this, we deploy
self-calibrated convolution on 3D point clouds as the foundation of our domain
adaptation module. Rigorous experimentation validates the effectiveness of our
proposed methods, consistently surpassing the performance of state-of-the-art
approaches in the field.

</details>


### [32] [Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction](https://arxiv.org/abs/2509.00381)
*Runtong Wu,Jiayao Song,Fei Teng,Xianhao Ren,Yuyan Gao,Kailun Yang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Narrative inquiry has been one of the prominent application domains for the
analysis of human experience, aiming to know more about the complexity of human
society. However, researchers are often required to transform various forms of
data into coherent hand-drafted narratives in storied form throughout narrative
analysis, which brings an immense burden of data analysis. Participants, too,
are expected to engage in member checking and presentation of these narrative
products, which involves reviewing and responding to large volumes of
documents. Given the dual burden and the need for more efficient and
participant-friendly approaches to narrative making and representation, we made
a first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt
to push the field of narrative inquiry. Name is able to transfer research
documents into coherent story images, alleviating the cognitive burden of
interpreting extensive text-based materials during member checking for both
researchers and participants. (ii) We develop an actor location and shape
module to facilitate plausible image generation. (iii) We have designed a set
of robust evaluation metrics comprising three key dimensions to objectively
measure the perceptual quality and narrative consistency of generated
characters. Our approach consistently demonstrates state-of-the-art performance
across different data partitioning schemes. Remarkably, while the baseline
relies on the full 100% of the available data, our method requires only 0.96%
yet still reduces the FID score from 195 to 152. Under identical data volumes,
our method delivers substantial improvements: for the 70:30 split, the FID
score decreases from 175 to 152, and for the 95:5 split, it is nearly halved
from 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the
newly introduced metric, surpassing the baseline score of 2.66.

</details>


### [33] [HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization](https://arxiv.org/abs/2509.00385)
*Joohyun Chang,Soyeon Hong,Hyogun Lee,Seong Jong Ha,Dongho Lee,Seong Tae Kim,Jinwoo Choi*

Main category: cs.CV

TL;DR: 提出HERO-VQL用于自我视角长视频的视觉查询定位，通过顶自下（Top-down）注意力与自我视角增强一致性训练，在VQ2D上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 自我视角视频存在视角频繁突变、遮挡与外观大幅变化，现有方法难以稳定、精确地定位查询物体。需要一种能利用高层语义指导与对极端视角变化具有鲁棒性的定位方法。

Method: 提出HERO-VQL，包括两大核心：1) Top-down Attention Guidance (TAG)：用class token提供高层语义上下文，并结合主成分得分图（由特征的PCA/主成分生成）来细粒度引导注意力，从粗到细提升定位精度。2) Egocentric Augmentation based Consistency Training (EgoACT)：通过两种增强提升鲁棒性：EgoAug-a 以标注中同一对象的其他实例替换原查询，增加查询多样性；EgoAug-b 通过重排视频帧模拟剧烈视角变化。并加入一致性（CT）损失，约束不同增强下的定位结果保持稳定。

Result: 在VQ2D数据集上进行大量实验，HERO-VQL在处理自我视角特有挑战（视角突变、遮挡、外观变化）方面效果显著，性能大幅超过现有基线。

Conclusion: 层次化的顶自下注意引导与面向自我视角的增强+一致性训练能显著提升VQL任务的鲁棒定位能力；HERO-VQL在标准基准上验证了其实用性与先进性。

Abstract: In this work, we tackle the egocentric visual query localization (VQL), where
a model should localize the query object in a long-form egocentric video.
Frequent and abrupt viewpoint changes in egocentric videos cause significant
object appearance variations and partial occlusions, making it difficult for
existing methods to achieve accurate localization. To tackle these challenges,
we introduce Hierarchical, Egocentric and RObust Visual Query Localization
(HERO-VQL), a novel method inspired by human cognitive process in object
recognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric
Augmentation based Consistency Training (EgoACT). Top-down Attention Guidance
refines the attention mechanism by leveraging the class token for high-level
context and principal component score maps for fine-grained localization. To
enhance learning in diverse and challenging matching scenarios, EgoAug enhances
query diversity by replacing the query with a randomly selected corresponding
object from groundtruth annotations and simulates extreme viewpoint changes by
reordering video frames. Additionally, CT loss enforces stable object
localization across different augmentation scenarios. Extensive experiments on
VQ2D dataset validate that HERO-VQL effectively handles egocentric challenges,
significantly outperforming baselines.

</details>


### [34] [Double-Constraint Diffusion Model with Nuclear Regularization for Ultra-low-dose PET Reconstruction](https://arxiv.org/abs/2509.00395)
*Mengxiao Geng,Ran Hong,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Ultra-low-dose positron emission tomography (PET) reconstruction holds
significant potential for reducing patient radiation exposure and shortening
examination times. However, it may also lead to increased noise and reduced
imaging detail, which could decrease the image quality. In this study, we
present a Double-Constraint Diffusion Model (DCDM), which freezes the weights
of a pre-trained diffusion model and injects a trainable double-constraint
controller into the encoding architecture, greatly reducing the number of
trainable parameters for ultra-low-dose PET reconstruction. Unlike full
fine-tuning models, DCDM can adapt to different dose levels without retraining
all model parameters, thereby improving reconstruction flexibility.
Specifically, the two constraint modules, named the Nuclear Transformer
Constraint (NTC) and the Encoding Nexus Constraint (ENC), serve to refine the
pre-trained diffusion model. The NTC leverages the nuclear norm as an
approximation for matrix rank minimization, integrates the low-rank property
into the Transformer architecture, and enables efficient information extraction
from low-dose images and conversion into compressed feature representations in
the latent space. Subsequently, the ENC utilizes these compressed feature
representations to encode and control the pre-trained diffusion model,
ultimately obtaining reconstructed PET images in the pixel space. In clinical
reconstruction, the compressed feature representations from NTC help select the
most suitable ENC for efficient unknown low-dose PET reconstruction.
Experiments conducted on the UDPET public dataset and the Clinical dataset
demonstrated that DCDM outperforms state-of-the-art methods on known dose
reduction factors (DRF) and generalizes well to unknown DRF scenarios, proving
valuable even at ultra-low dose levels, such as 1% of the full dose.

</details>


### [35] [DAOVI: Distortion-Aware Omnidirectional Video Inpainting](https://arxiv.org/abs/2509.00396)
*Ryosuke Seshimo,Mariko Isogawa*

Main category: cs.CV

TL;DR: 提出DAOVI，一种面向全景（equirectangular）视频的失真感知时空补洞方法，通过地理测地线感知的时间运动评估和深度感知的特征传播来缓解投影失真，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 全景视频广角带来不需要目标频繁入镜，需要视频修补；现有方法多针对窄视场普通视频，未考虑全景equirectangular投影的几何失真与时空一致性问题。

Method: 设计DAOVI框架：1）在图像空间引入基于测地线距离的时间运动信息评估模块，避免以像素欧氏距离度量导致的球面到平面投影误差；2）在特征空间加入深度感知的特征传播模块，利用深度或几何先验，按失真与视差信息进行自适应传播/聚合，提高空域结构一致性与边界对齐；总体为深度学习端到端视频补洞网络。

Result: 在多组数据上（未指明数据集）实现定量与定性全面领先，说明对失真处理与时空一致性的改进有效。

Conclusion: 考虑球面几何（测地线）与深度几何的失真感知设计对全景视频补洞至关重要，DAOVI在实际全景应用中更稳健优于传统窄视场方法。

Abstract: Omnidirectional videos that capture the entire surroundings are employed in a
variety of fields such as VR applications and remote sensing. However, their
wide field of view often causes unwanted objects to appear in the videos. This
problem can be addressed by video inpainting, which enables the natural removal
of such objects while preserving both spatial and temporal consistency.
Nevertheless, most existing methods assume processing ordinary videos with a
narrow field of view and do not tackle the distortion in equirectangular
projection of omnidirectional videos. To address this issue, this paper
proposes a novel deep learning model for omnidirectional video inpainting,
called Distortion-Aware Omnidirectional Video Inpainting (DAOVI). DAOVI
introduces a module that evaluates temporal motion information in the image
space considering geodesic distance, as well as a depth-aware feature
propagation module in the feature space that is designed to address the
geometric distortion inherent to omnidirectional videos. The experimental
results demonstrate that our proposed method outperforms existing methods both
quantitatively and qualitatively.

</details>


### [36] [DevilSight: Augmenting Monocular Human Avatar Reconstruction through a Virtual Perspective](https://arxiv.org/abs/2509.00403)
*Yushuo Chen,Ruizhi Shao,Youxin Pang,Hongwen Zhang,Xinyi Wu,Rihui Wu,Yebin Liu*

Main category: cs.CV

TL;DR: 单目视频重建高细节人类化身。核心做法：用强大的视频生成模型Human4DiT从“替代视角”生成额外监督，配合身份微调和补丁式去噪以提升一致性与分辨率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目视频重建易受观测不足与模型表达力有限的双重限制：1) 难以捕获细粒度动态细节；2) 新视角细节虚假或不连贯。亟需一种既能补全未观察区域信息，又能对头像表示进行正则的机制。

Method: 1) 利用Human4DiT从新视角生成同一动作的视频，作为额外监督，丰富不可见区域并正则化化身表示以减少伪影。2) 身份一致性：对Human4DiT进行目标人物视频微调，注入个体物理身份，保证动作与外观在生成视频中的一致重现。3) 分辨率与细节：采用patch-based去噪生成高分辨率视频，捕获更精细的纹理与动态。

Result: 在与近期SOTA的对比实验中取得更高的重建质量（细节、更少伪影、更一致的多视角表现），并通过消融验证替代视角监督、身份微调和补丁去噪各自的有效性。

Conclusion: 通过将视频生成模型作为替代视角监督并辅以身份微调与补丁去噪，可显著提升单目视频人类化身重建的细节与新视角一致性，优于现有方法。

Abstract: We present a novel framework to reconstruct human avatars from monocular
videos. Recent approaches have struggled either to capture the fine-grained
dynamic details from the input or to generate plausible details at novel
viewpoints, which mainly stem from the limited representational capacity of the
avatar model and insufficient observational data. To overcome these challenges,
we propose to leverage the advanced video generative model, Human4DiT, to
generate the human motions from alternative perspective as an additional
supervision signal. This approach not only enriches the details in previously
unseen regions but also effectively regularizes the avatar representation to
mitigate artifacts. Furthermore, we introduce two complementary strategies to
enhance video generation: To ensure consistent reproduction of human motion, we
inject the physical identity into the model through video fine-tuning. For
higher-resolution outputs with finer details, a patch-based denoising algorithm
is employed. Experimental results demonstrate that our method outperforms
recent state-of-the-art approaches and validate the effectiveness of our
proposed strategies.

</details>


### [37] [LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging and KV Cache Compression](https://arxiv.org/abs/2509.00419)
*Lianyu Hu,Fanhua Shang,Wei Feng,Liang Wan*

Main category: cs.CV

TL;DR: LightVLM是一种无需再训练即可部署在现有多模态VLM上的轻量加速方案，同时加速编码与解码：分层合并视觉token与压缩解码KV缓存，在几乎不掉点的前提下显著提升吞吐与降低时延。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在推理时存在两大瓶颈：1) 编码阶段视觉token数量庞大、贯穿多层LLM造成计算冗余；2) 解码阶段长序列生成导致KV缓存爆炸与高时延。需要一种在不改动或再训练模型的情形下，通用且高效的加速方法，便于大模型落地。

Method: - 分阶段加速框架：将推理分为编码与解码两段分别优化。
- 编码：提出金字塔式token合并（pyramid token merging），在不同LLM层分层、分步地合并/筛选视觉token，最终仅保留少量“主导”token，最大化减少跨层传播的视觉冗余。
- 解码：提出KV Cache压缩，去除对后续预测贡献小或冗余的缓存键值对，提高解码阶段吞吐与并行度，降低长序列生成的延迟。
- 全流程训练自由：对现有VLM即插即用，无需微调。

Result: - 在仅保留35%图像token时性能100%保持；仅保留3%图像token时性能约98%。
- 吞吐量提升约2.02×，预填充（prefill）时延降低3.65×。
- 使大型模型（如InternVL2.5 26B）推理速度快于更小模型（如8B）。
- 对长序列（4096 tokens）生成，整体推理时间减少3.21×，优于现有方法。

Conclusion: LightVLM以简单、训练自由的方案在编码与解码两端同步降本增效，几乎不损精度且显著提升吞吐与时延，尤其在长文本生成与大模型部署中具备实际落地价值。

Abstract: In this paper, we introduce LightVLM, a simple but effective method that can
be seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly
accelerate the inference process in a training-free manner. We divide the
inference procedure of VLMs into two stages, i.e., encoding and decoding, and
propose to simultaneously accelerate VLMs in both stages to largely improve
model efficiency. During encoding, we propose pyramid token merging to reduce
tokens of different LLM layers in a hierarchical manner by finally only keeping
a few dominant tokens to achieve high efficiency. During decoding, aimed at
reducing the high latency of outputting long sequences, we propose KV Cache
compression to remove unnecessary caches to increase the network throughput.
Experimental results show that LightVLM successfully retains 100% performance
when only preserving 35% image tokens, and maintains around 98% performance
when keeping only 3% image tokens. LightVLM could 2.02$\times$ the network
throughput and reduce the prefilling time by 3.65$\times$. LightVLM also makes
large VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to
infer faster than significantly smaller models (e.g., InternVL2.5 8B),
hopefully facilitating the real-world deployment. When generating long text
sequences (e.g., 4096 tokens), LightVLM could reduce the inference time by
3.21$\times$, largely outperforming existing methods.

</details>


### [38] [Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation](https://arxiv.org/abs/2509.00428)
*Xuechao Zou,Shun Zhang,Xing Fu,Yue Li,Kai Li,Yushe Cao,Congyan Lang,Pin Tao,Junliang Xing*

Main category: cs.CV

TL;DR: 提出Face-MoGLE：在DiT框架中通过语义解耦潜空间+全局/局部专家混合+时空动态门控，实现高保真、可精细控制的人脸生成，并具备零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有人脸可控生成常在“语义可控性”与“照片级真实感”之间权衡，难以有效解耦语义控制与生成流程，导致编辑泄漏、属性干扰与细粒度控制不足。作者认为Diffusion Transformer具备结构潜力，若结合专家化分工可缓解这些问题。

Method: 1) 语义解耦潜表示：通过基于掩码的空间因子分解，在潜空间中将被编辑的区域/属性与其余内容分离，支持精准属性操控。2) 全局-局部专家混合（MoE）：设置同时建模整体结构（全局专家）与局部区域语义（局部专家）的专家模块，以细粒度控制面部区域（如眼、口等）。3) 动态门控网络：随扩散步数与空间位置变化，生成对各专家的时间依赖系数，实现从粗到细、从全局到局部的自适应特化。实现于DiT架构，支持多模态/单模态条件。

Result: 在多模态（如文本-图像）与单模态（图像编辑）场景中均优于现有方法，兼顾高保真与高可控；在未见过属性/组合上的零样本泛化稳健。消融实验验证了掩码因子化、全局-局部专家和动态门控的必要性。

Conclusion: Face-MoGLE通过在DiT中引入语义解耦与专家混合及动态门控，显著提升人脸生成的可控性与真实感，同时具备良好的泛化，适用于生成与安全相关应用。

Abstract: Controllable face generation poses critical challenges in generative modeling
due to the intricate balance required between semantic controllability and
photorealism. While existing approaches struggle with disentangling semantic
controls from generation pipelines, we revisit the architectural potential of
Diffusion Transformers (DiTs) through the lens of expert specialization. This
paper introduces Face-MoGLE, a novel framework featuring: (1)
Semantic-decoupled latent modeling through mask-conditioned space
factorization, enabling precise attribute manipulation; (2) A mixture of global
and local experts that captures holistic structure and region-level semantics
for fine-grained controllability; (3) A dynamic gating network producing
time-dependent coefficients that evolve with diffusion steps and spatial
locations. Face-MoGLE provides a powerful and flexible solution for
high-quality, controllable face generation, with strong potential in generative
modeling and security applications. Extensive experiments demonstrate its
effectiveness in multimodal and monomodal face generation settings and its
robust zero-shot generalization capability. Project page is available at
https://github.com/XavierJiezou/Face-MoGLE.

</details>


### [39] [SemaMIL: Semantic Reordering with Retrieval-Guided State Space Modeling for Whole Slide Image Classification](https://arxiv.org/abs/2509.00442)
*Lubin Gan,Xiaoman Wu,Jing Zhang,Zhifeng Wang,Linhao Qu,Siying Wu,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SemaMIL：通过“语义重排序+语义引导检索状态空间模块”在WSI多实例学习中兼顾上下文建模、效率与可解释性，达SOTA且更省算。


<details>
  <summary>Details</summary>
Motivation: 现有WSI的MIL方法各有缺陷：注意力MIL能找关键patch但忽视上下文；Transformer能建模交互却计算量二次方且易过拟合；状态空间模型线性高效但对patch顺序敏感，随机顺序破坏组织学语义、降低可解释性。需要一种既高效又能利用语义上下文、同时保持可解释性的MIL框架。

Method: 提出SemaMIL，核心包含两部分：1) 语义重排序（SR）：自适应聚类相似patch，并通过可逆置换将其按语义相近的顺序排列成序列，既保留组织学上下文，又保证可回溯、可解释；2) 语义引导检索状态空间模块（SRSM）：从序列中检索一组具有代表性的查询patch，用于调节SSM的参数，从而在保持线性复杂度的同时加强全局建模能力。整体以MIL框架对WSI进行patch级特征提取与聚合。

Result: 在四个WSI亚型数据集上，相比强基线方法，SemaMIL以更少的FLOPs与更少参数达到或超越SOTA分类准确率。

Conclusion: 语义重排序与检索增强的SSM有效缓解了注意力MIL的上下文缺失、Transformer的开销与过拟合问题，以及SSM对顺序敏感的可解释性不足，提供了兼具准确性、效率与可解释性的WSI MIL方案。

Abstract: Multiple instance learning (MIL) has become the leading approach for
extracting discriminative features from whole slide images (WSIs) in
computational pathology. Attention-based MIL methods can identify key patches
but tend to overlook contextual relationships. Transformer models are able to
model interactions but require quadratic computational cost and are prone to
overfitting. State space models (SSMs) offer linear complexity, yet shuffling
patch order disrupts histological meaning and reduces interpretability. In this
work, we introduce SemaMIL, which integrates Semantic Reordering (SR), an
adaptive method that clusters and arranges semantically similar patches in
sequence through a reversible permutation, with a Semantic-guided Retrieval
State Space Module (SRSM) that chooses a representative subset of queries to
adjust state space parameters for improved global modeling. Evaluation on four
WSI subtype datasets shows that, compared to strong baselines, SemaMIL achieves
state-of-the-art accuracy with fewer FLOPs and parameters.

</details>


### [40] [Stage-wise Adaptive Label Distribution for Facial Age Estimation](https://arxiv.org/abs/2509.00450)
*Bo Wu,Zhiqi Ai,Jun Jiang,Congcong Zhu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出SA-LDL用于年龄估计中的标签歧义，基于“阶段性”模糊模式，自适应建模各年龄阶段的分布方差并配合加权损失，显著提升鲁棒性与精度；在MORPH-II与FG-NET上MAE分别为1.74与2.15。


<details>
  <summary>Details</summary>
Motivation: 年龄估计标签存在模糊（一个人可能被标注为相邻年龄），现有标签分布学习多仅考虑邻近年龄的相关性，忽略不同年龄阶段模糊程度差异（如儿童/老年阶段更难区分）。需要能反映阶段性差异的建模。

Method: 1) 通过分析锚样本与各年龄的嵌入相似度，发现歧义呈“阶段性”模式；2) 设计阶段自适应的标签分布方差：为不同年龄阶段分配不同分布宽度（方差），以匹配实际模糊度；3) 引入加权损失，使训练更关注高模糊或重要阶段；4) 将二者联合优化，形成Stage-wise Adaptive Label Distribution Learning（SA-LDL）。

Result: 在MORPH-II与FG-NET两数据集上取得有竞争力结果：MAE分别为1.74与2.15，显示方法在准确性与鲁棒性上的优势。

Conclusion: 年龄标签的歧义具有阶段性结构。通过阶段自适应方差和加权损失的联合建模，SA-LDL更好地刻画标签分布，提升年龄估计性能，并在主流数据集上验证有效性。

Abstract: Label ambiguity poses a significant challenge in age estimation tasks. Most
existing methods address this issue by modeling correlations between adjacent
age groups through label distribution learning. However, they often overlook
the varying degrees of ambiguity present across different age stages. In this
paper, we propose a Stage-wise Adaptive Label Distribution Learning (SA-LDL)
algorithm, which leverages the observation -- revealed through our analysis of
embedding similarities between an anchor and all other ages -- that label
ambiguity exhibits clear stage-wise patterns. By jointly employing stage-wise
adaptive variance modeling and weighted loss function, SA-LDL effectively
captures the complex and structured nature of label ambiguity, leading to more
accurate and robust age estimation. Extensive experiments demonstrate that
SA-LDL achieves competitive performance, with MAE of 1.74 and 2.15 on the
MORPH-II and FG-NET datasets.

</details>


### [41] [Encoder-Only Image Registration](https://arxiv.org/abs/2509.00451)
*Xiang Chen,Renjiu Hu,Jinwei Zhang,Yuxi Zhang,Xinyao Yue,Min Liu,Yaonan Wang,Hang Zhang*

Main category: cs.CV

TL;DR: 提出EOIR：仅用轻量编码器与分层流估计器的配准框架，在五个数据集上实现更优精度-效率与精度-平滑性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有学习式形变配准虽快准，但在计算复杂度高、处理大形变稳定性与效率权衡上仍有瓶颈。作者从Horn–Schunck光流角度分析ConvNet作用机制，试图用更简洁的结构保留关键能力（局部线性化与全局对比度一致化），以降低开销并提升大形变下的稳健性。

Method: 理论与实证表明ConvNet主要起两作用：1）局部强度线性化；2）全局对比度协调。基于此提出EOIR：- 仅用3层ConvNet做特征提取（编码器），不与流估计解耦的复杂解码器；- 采用多组3层流估计器构建拉普拉斯特征金字塔；- 在大形变模型下逐级组合可微分形变（diffeomorphic）以保证拓扑一致性与光滑性；- 将特征学习与流估计分离，降低参数与计算量。

Result: 在跨模态与多解剖区域的五个数据集上，EOIR在相当精度下提供更高效率与更好形变平滑性；或在相当计算预算/平滑性下取得更高精度。总体上实现优于现有方法的精度-效率与精度-平滑性权衡。

Conclusion: ConvNet在配准中的关键贡献可归纳为局部线性化与全局对比度协调。围绕这两点的轻量化EOIR框架能在保证或提升精度的同时显著提高效率与形变平滑性，适用于大形变情形；代码将开源以促进复现与扩展。

Abstract: Learning-based techniques have significantly improved the accuracy and speed
of deformable image registration. However, challenges such as reducing
computational complexity and handling large deformations persist. To address
these challenges, we analyze how convolutional neural networks (ConvNets)
influence registration performance using the Horn-Schunck optical flow
equation. Supported by prior studies and our empirical experiments, we observe
that ConvNets play two key roles in registration: linearizing local intensities
and harmonizing global contrast variations. Based on these insights, we propose
the Encoder-Only Image Registration (EOIR) framework, designed to achieve a
better accuracy-efficiency trade-off. EOIR separates feature learning from flow
estimation, employing only a 3-layer ConvNet for feature extraction and a set
of 3-layer flow estimators to construct a Laplacian feature pyramid,
progressively composing diffeomorphic deformations under a large-deformation
model. Results on five datasets across different modalities and anatomical
regions demonstrate EOIR's effectiveness, achieving superior
accuracy-efficiency and accuracy-smoothness trade-offs. With comparable
accuracy, EOIR provides better efficiency and smoothness, and vice versa. The
source code of EOIR will be publicly available on
https://github.com/XiangChen1994/EOIR.

</details>


### [42] [Exploring Decision-Making Capabilities of LLM Agents: An Experimental Study on Jump-Jump Game](https://arxiv.org/abs/2509.00483)
*Juwu Li*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The Jump-Jump game, as a simple yet challenging casual game, provides an
ideal testing environment for studying LLM decision-making capabilities. The
game requires players to precisely control jumping force based on current
position and target platform distance, involving multiple cognitive aspects
including spatial reasoning, physical modeling, and strategic planning. It
illustrates the basic gameplay mechanics of the Jump-Jump game, where the
player character (red circle) must jump across platforms with appropriate force
to maximize score.

</details>


### [43] [VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding](https://arxiv.org/abs/2509.00484)
*Zhihong Zhang,Xiaojian Huang,Jin Xu,Zhuodong Luo,Xinzhi Wang,Jiansheng Wei,Xuejin Chen*

Main category: cs.CV

TL;DR: 提出VideoRewardBench，首个覆盖感知、知识、推理与安全四维度的视频多模态奖励模型基准，含1563偏好样本、1482视频、1559问题；评测28种MRMs，最强仅~57%准确率，揭示RL不必带来更强跨模态泛化、除判别式外推理时扩展有效、帧数变化对不同MRMs影响不同。


<details>
  <summary>Details</summary>
Motivation: 现有视频域MRM基准在题目数量/多样性不足、评估维度不全面、模型类型覆盖不全，难以真实反映MRM能力并指导训练与部署。

Method: 构建AI辅助数据流水线，收集并标注偏好数据（三元组：视频-文本提示、优选回答、被拒回答），覆盖四大能力维度；以此形成VideoRewardBench，并对28种不同范式（生成式、判别式、半标量）的MRMs开展系统评测，分析准确率与多种因素（RL训练、推理时扩展、输入帧数）的影响。

Result: 数据集规模显著超越以往（问题数约为此前最多的15倍）；总体准确率不高：GPT-4o约57.0%，Qwen2.5-VL-72B约53.3%；不同训练与推理策略、帧数设置对各类MRMs表现的影响具有差异性。

Conclusion: VideoRewardBench为视频域MRM提供更全面、具挑战性的评测标准，可揭示当前模型在跨模态理解与偏好判别上的显著不足，并为训练范式、推理策略及输入配置的改进提供依据。

Abstract: Multimodal reward models (MRMs) play a crucial role in the training,
inference, and evaluation of Large Vision Language Models (LVLMs) by assessing
response quality. However, existing benchmarks for evaluating MRMs in the video
domain suffer from a limited number and diversity of questions, a lack of
comprehensive evaluation dimensions, and inadequate evaluation of diverse types
of MRMs. To address these gaps, we introduce VideoRewardBench, the first
comprehensive benchmark covering four core aspects of video understanding:
perception, knowledge, reasoning, and safety. Through our AI-assisted data
pipeline, we curate a high-quality preference dataset of 1,563 annotated
samples, including 1,482 unique videos and 1,559 distinct questions--15 times
the number found in the most question-rich prior benchmark. Each sample is a
triplet consisting of a video-text prompt, a chosen response, and a rejected
response. We also conduct a comprehensive evaluation across 28 multimodal
reward models spanning three categories: generative, discriminative, and
semi-scalar. Results show that even the top-performing model GPT-4o achieves
only 57.0% overall accuracy, and the state-of-the-art open-source model
Qwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three key
insights: (i) MRMs trained with reinforcement learning (RL) do not necessarily
exhibit stronger cross-modal generalization than those trained without RL; (ii)
except for discriminative MRMs, other types of MRMs across varying model
capacities can benefit from inference-time scaling; and (iii) variations in
input video frame count have different effects on different types of MRMs. We
believe VideoRewardBench offers a challenging and valuable benchmark for
advancing the evaluation and development of MRMs in the video domain.

</details>


### [44] [Multi-Focused Video Group Activities Hashing](https://arxiv.org/abs/2509.00490)
*Zhongmiao Qi,Yan Jiang,Bolin Zhang,Lijun Guo,Chong Wang,Qiangbo Qian*

Main category: cs.CV

TL;DR: 提出STVH与增强版M-STVH两种视频哈希方法，实现对群体活动与对象层面的快速检索，在多个公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索多以整段视频为单位，难以对“活动粒度”进行检索；真实场景有时需要活动语义，有时需要对象视觉外观，单一特征视角不足。

Method: 1) STVH：在统一框架中交织建模个体对象动态与群体交互，联合捕获群体视觉特征与位置（轨迹/布局）特征的时空演化，并输出紧致哈希码以高效检索。2) M-STVH：在STVH基础上加入多聚焦表示学习与分层特征整合，显式同时关注活动语义与对象视觉特征，从而适配不同检索需求。

Result: 在公开数据集上的对比实验显示，STVH与M-STVH均取得优异性能（相对现有方法更好或具竞争力），证明方法有效。

Conclusion: 通过时空交织建模与多聚焦分层整合，方法实现了面向活动粒度与对象视觉的统一视频哈希检索框架，兼顾效率与准确性，适用于复杂场景下的群体活动检索。

Abstract: With the explosive growth of video data in various complex scenarios, quickly
retrieving group activities has become an urgent problem. However, many tasks
can only retrieve videos focusing on an entire video, not the activity
granularity. To solve this problem, we propose a new STVH (spatiotemporal
interleaved video hashing) technique for the first time. Through a unified
framework, the STVH simultaneously models individual object dynamics and group
interactions, capturing the spatiotemporal evolution on both group visual
features and positional features. Moreover, in real-life video retrieval
scenarios, it may sometimes require activity features, while at other times, it
may require visual features of objects. We then further propose a novel M-STVH
(multi-focused spatiotemporal video hashing) as an enhanced version to handle
this difficult task. The advanced method incorporates hierarchical feature
integration through multi-focused representation learning, allowing the model
to jointly focus on activity semantics features and object visual features. We
conducted comparative experiments on publicly available datasets, and both STVH
and M-STVH can achieve excellent results.

</details>


### [45] [TRUST: Token-dRiven Ultrasound Style Transfer for Cross-Device Adaptation](https://arxiv.org/abs/2509.00508)
*Nhat-Tuong Do-Tran,Ngoc-Hoang-Lam Le,Ian Chiu,Po-Tsun Paul Kuo,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出TRUST：面向超声图像跨设备风格差异的无配对图像到图像翻译框架，通过token驱动的双流结构在不混淆内容的前提下仅迁移目标域通用风格，并以行为镜像损失对下游模型最优风格token进行选择，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 不同超声设备导致图像风格差异，现有下游模型在新设备上性能下降。传统UI2I翻译虽可做风格迁移，但未明确筛选与下游任务最相关的风格特征，易使翻译图像与下游需求不匹配，且内容与风格可能混淆。

Method: 提出TRUST（Token-dRiven dUal-Stream Translation）：
- 双流结构：内容流保持源域内容不变，风格流学习目标域“通用风格”，两者解耦避免内容-风格混杂。
- Token-dRiven模块（TR）从两视角选择目标风格token：
  1) 数据视角：为每个源token选择“合适”的目标域token进行风格对齐；
  2) 模型视角：通过行为镜像损失（behavior mirror loss）选择对冻结下游模型“最优”的目标token，使翻译结果对下游行为一致/友好。
- 辅助提示（prompts）注入源编码器，使内容表征与下游模型的行为匹配。
- 训练在无配对设置下进行，保持源内容、迁移目标通用风格。

Result: 在多组超声数据上，TRUST在视觉质量与下游任务指标（如分割/检测等）均优于现有UI2I方法；能更好地跨设备泛化，同时避免内容被风格污染。

Conclusion: 通过token级选择与双流解耦，TRUST实现了对下游任务友好的风格迁移与内容保持，显著提升超声跨设备适应性，优于传统UI2I。

Abstract: Ultrasound images acquired from different devices exhibit diverse styles,
resulting in decreased performance of downstream tasks. To mitigate the style
gap, unpaired image-to-image (UI2I) translation methods aim to transfer images
from a source domain, corresponding to new device acquisitions, to a target
domain where a frozen task model has been trained for downstream applications.
However, existing UI2I methods have not explicitly considered filtering the
most relevant style features, which may result in translated images misaligned
with the needs of downstream tasks. In this work, we propose TRUST, a
token-driven dual-stream framework that preserves source content while
transferring the common style of the target domain, ensuring that content and
style remain unblended. Given multiple styles in the target domain, we
introduce a Token-dRiven (TR) module that operates from two perspectives: (1) a
data view--selecting "suitable" target tokens corresponding to each source
token, and (2) a model view--identifying ``optimal" target tokens for the
downstream model, guided by a behavior mirror loss. Additionally, we inject
auxiliary prompts into the source encoder to match content representation with
downstream behavior. Experimental results on ultrasound datasets demonstrate
that TRUST outperforms existing UI2I methods in both visual quality and
downstream task performance.

</details>


### [46] [Make me an Expert: Distilling from Generalist Black-Box Models into Specialized Models for Semantic Segmentation](https://arxiv.org/abs/2509.00509)
*Yasser Benigmim,Subhankar Roy,Khalid Oublal,Imad Eddine Marouf,Slim Essid,Vicky Kalogeiton,Stéphane Lathuilière*

Main category: cs.CV

TL;DR: 提出黑盒蒸馏（B2D）设定：仅用AIaaS黑盒API的一热预测、不开权重/数据/logits，使本地模型能适配；发现开放词汇分割模型存在“分辨率诅咒”；用ATGC基于DINOv2注意力与熵选择最优尺度做伪标签，显著提升多数据集表现。


<details>
  <summary>Details</summary>
Motivation: AIaaS普及但只提供黑盒接口，使传统需要logits/源数据/可见参数的域自适应与蒸馏方法难以落地；开放词汇分割模型在不同分辨率对不同类别表现不一，影响伪标签质量。

Method: 定义B2D设定：只能获取API的一热预测；提出ATGC：用DINOv2注意力图，计算熵对不同输入尺度打分，动态选取信息量高的尺度进行API推理与伪标签生成，从而蒸馏到本地模型。

Result: 在多个数据集上（开放词汇分割场景）仅依赖一热预测即取得显著性能提升，相较基线的黑盒监督方法更优。

Conclusion: 在现实AIaaS约束下，ATGC有效缓解“分辨率诅咒”，提升黑盒监督下的蒸馏效果，验证B2D设定的可行性与实用性；代码已开源。

Abstract: The rise of Artificial Intelligence as a Service (AIaaS) democratizes access
to pre-trained models via Application Programming Interfaces (APIs), but also
raises a fundamental question: how can local models be effectively trained
using black-box models that do not expose their weights, training data, or
logits, a constraint in which current domain adaptation paradigms are
impractical ? To address this challenge, we introduce the Black-Box
Distillation (B2D) setting, which enables local model adaptation under
realistic constraints: (1) the API model is open-vocabulary and trained on
large-scale general-purpose data, and (2) access is limited to one-hot
predictions only. We identify that open-vocabulary models exhibit significant
sensitivity to input resolution, with different object classes being segmented
optimally at different scales, a limitation termed the "curse of resolution".
Our method, ATtention-Guided sCaler (ATGC), addresses this challenge by
leveraging DINOv2 attention maps to dynamically select optimal scales for
black-box model inference. ATGC scores the attention maps with entropy to
identify informative scales for pseudo-labelling, enabling effective
distillation. Experiments demonstrate substantial improvements under black-box
supervision across multiple datasets while requiring only one-hot API
predictions. Our code is available at https://github.com/yasserben/ATGC.

</details>


### [47] [Learning Yourself: Class-Incremental Semantic Segmentation with Language-Inspired Bootstrapped Disentanglement](https://arxiv.org/abs/2509.00527)
*Ruitao Wu,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 提出LBD框架，用CLIP语义先验引导原型-特征与前景-背景的解缠，缓解CISS中的“灾难性语义纠缠”，在VOC与ADE20K多步增量上达SOTA。


<details>
  <summary>Details</summary>
Motivation: CISS在持续学习新类时需保持旧类，但现有方法仅依赖视觉特征学习，缺少区分语义的充足线索，导致两类纠缠：1) 原型-特征纠缠（增量过程中语义错配）；2) 背景-增量纠缠（数据分布与背景标签动态变化）。这些纠缠引入噪声与误差，造成遗忘与漂移。

Method: 将主流方法抽象为视觉特征提取与原型匹配两阶段，提出语言启发的自举式解缠框架LBD：1) 语言引导的原型解缠（Language-guided Prototypical Disentanglement）：用手工文本描述从预训练VLM（如CLIP）获得的类语义，作为拓扑模板引导新类原型的构建与对齐；2) 流形互助的背景解缠（Manifold Mutual Background Disentanglement）：通过多可学习原型与基于mask-pooling的监督，对背景-新增类进行解缠；3) 结合soft prompt tuning与编码器适配，缩小CLIP在稀疏（分类）与稠密（分割）任务间的能力差距。

Result: 在Pascal VOC与ADE20K的多步增量设置上取得SOTA，说明对新类学习与旧类保持均有提升，尤其在多阶段（多步）场景中优势明显。

Conclusion: 利用语言先验进行原型级与背景级双重解缠，可显著缓解CISS中的语义纠缠问题。通过轻量适配（软提示与编码器微调），VLM可有效迁移到密集预测，实现更稳健的增量分割。

Abstract: Class-Incremental Semantic Segmentation (CISS) requires continuous learning
of newly introduced classes while retaining knowledge of past classes. By
abstracting mainstream methods into two stages (visual feature extraction and
prototype-feature matching), we identify a more fundamental challenge termed
catastrophic semantic entanglement. This phenomenon involves Prototype-Feature
Entanglement caused by semantic misalignment during the incremental process,
and Background-Increment Entanglement due to dynamic data evolution. Existing
techniques, which rely on visual feature learning without sufficient cues to
distinguish targets, introduce significant noise and errors. To address these
issues, we introduce a Language-inspired Bootstrapped Disentanglement framework
(LBD). We leverage the prior class semantics of pre-trained visual-language
models (e.g., CLIP) to guide the model in autonomously disentangling features
through Language-guided Prototypical Disentanglement and Manifold Mutual
Background Disentanglement. The former guides the disentangling of new
prototypes by treating hand-crafted text features as topological templates,
while the latter employs multiple learnable prototypes and mask-pooling-based
supervision for background-incremental class disentanglement. By incorporating
soft prompt tuning and encoder adaptation modifications, we further bridge the
capability gap of CLIP between dense and sparse tasks, achieving
state-of-the-art performance on both Pascal VOC and ADE20k, particularly in
multi-step scenarios.

</details>


### [48] [A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging](https://arxiv.org/abs/2509.00549)
*Peirong Liu,Oula Puonti,Xiaoling Hu,Karthik Gopinath,Annabel Sorby-Adams,Daniel C. Alexander,W. Taylor Kimberly,Juan E. Iglesias*

Main category: cs.CV

TL;DR: BrainFM 是一个面向人脑影像的“模态无关、多任务”基础模型，通过新颖的数据生成与混合训练策略，显著提升在未校准MR等多样成像条件下的泛化与稳健性，能一模多能直接完成影像合成、分割、距离估计、偏置场估计和配准等五类任务，并在11个公开数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在CT等校准成像上表现突出，但在MR等未校准模态中易受对比度、分辨率、取向与伪影差异影响，泛化差，限制了临床多协议广泛应用。需要一个对成像外观变化鲁棒、可跨任务与跨模态直接应用的通用模型。

Method: 提出 BrainFM：一个多任务、模态无关的人脑影像基础模型。核心训练策略包括：(1) “mild-to-severe” 同受试者生成：对同一受试者图像合成由轻到重的外观与几何变化（对比度、分辨率、形变、伪影等），促使模型学习对外观变化的不变性；(2) “real-synth” mix-up：将真实数据与合成数据混合训练，提高域泛化与稳健性。模型统一处理不同输入（CT、T1w/T2w/FLAIR MRI 等）并以多任务头输出五类任务结果（影像合成、解剖分割、头皮-皮层距离、偏置场估计、配准）。

Result: 在11个公开数据集上评估，BrainFM 在五个任务与多种输入模态上均表现稳健且有效，相比以往方法更好地跨对比度、分辨率、取向与伪影泛化。

Conclusion: 通过“同受试者渐进扰动+真实-合成混合”的训练策略，BrainFM 成为能够跨模态、跨任务直接应用的人脑影像基础模型，缓解了未校准MR场景下的泛化瓶颈，并在多数据集上验证了鲁棒性与有效性。

Abstract: Recent learning-based approaches have made astonishing advances in calibrated
medical imaging like computerized tomography (CT), yet they struggle to
generalize in uncalibrated modalities -- notably magnetic resonance (MR)
imaging, where performance is highly sensitive to the differences in MR
contrast, resolution, and orientation. This prevents broad applicability to
diverse real-world clinical protocols. Here we introduce BrainFM, a
modality-agnostic, multi-task vision foundation model for human brain imaging.
With the proposed "mild-to-severe" intra-subject generation and "real-synth"
mix-up training strategy, BrainFM is resilient to the appearance of acquired
images (e.g., modality, contrast, deformation, resolution, artifacts), and can
be directly applied to five fundamental brain imaging tasks, including image
synthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical
distance, bias field estimation, and registration. We evaluate the efficacy of
BrainFM on eleven public datasets, and demonstrate its robustness and
effectiveness across all tasks and input modalities. Code is available at
https://github.com/jhuldr/BrainFM.

</details>


### [49] [C-DiffDet+: Fusing Global Scene Context with Generative Denoising for High-Fidelity Object Detection](https://arxiv.org/abs/2509.00578)
*Abdellah Zakaria Sellam,Ilyes Benaissa,Salah Eddine Bekhouche,Abdenour Hadid,Vito Renó,Cosimo Distante*

Main category: cs.CV

TL;DR: 提出在DiffusionDet中加入跨注意力的上下文感知融合（CAF），用全局场景编码器+局部proposal特征融合，提升细粒度目标检测（如车辆受损评估），在CarDD上刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: DiffusionDet等生成式检测方法在上下文依赖强的细粒度场景中受限：仅依赖局部特征，难以准确判断需要全局语义的目标（例如车辆损伤的类型/部位）。需要一种能把全局场景信息有效注入到每个候选框特征中的机制。

Method: 在DiffusionDet框架中加入Context-Aware Fusion：1) 通过独立的全局上下文编码器提取整图语义；2) 采用跨注意力，让每个object proposal（局部特征）查询全局上下文键值，从而融合场景级信息；3) 与原有条件去噪过程结合，形成上下文感知的生成式检测。

Result: 在CarDD（车辆损伤检测）基准上相较当前SOTA取得显著提升，建立新的性能基线（具体数值未给出）。

Conclusion: 将全局上下文通过跨注意力与局部proposal直接融合，能弥补生成式检测在上下文依赖场景中的短板，为细粒度目标检测提供更鲁棒的方案，并在实际基准上证明有效。

Abstract: Fine-grained object detection in challenging visual domains, such as vehicle
damage assessment, presents a formidable challenge even for human experts to
resolve reliably. While DiffusionDet has advanced the state-of-the-art through
conditional denoising diffusion, its performance remains limited by local
feature conditioning in context-dependent scenarios. We address this
fundamental limitation by introducing Context-Aware Fusion (CAF), which
leverages cross-attention mechanisms to integrate global scene context with
local proposal features directly. The global context is generated using a
separate dedicated encoder that captures comprehensive environmental
information, enabling each object proposal to attend to scene-level
understanding. Our framework significantly enhances the generative detection
paradigm by enabling each object proposal to attend to comprehensive
environmental information. Experimental results demonstrate an improvement over
state-of-the-art models on the CarDD benchmark, establishing new performance
benchmarks for context-aware object detection in fine-grained domains

</details>


### [50] [DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation](https://arxiv.org/abs/2509.00598)
*Boyi Li,Ce Zhang,Richard M. Timmerman,Wenxuan Bao*

Main category: cs.CV

TL;DR: 提出DGL-RSIS：一个无需训练的远感分割框架，通过全局-局部解耦与双尺度视觉-文本对齐，支持开放词汇语义分割与指代表达分割。


<details>
  <summary>Details</summary>
Motivation: VLM在自然图像上强大，但迁移到遥感分割困难：1) 遥感数据类别多样性不足，2) 自然图像与遥感图像存在显著域间差异，导致对齐与泛化不佳、监督数据稀缺。需要一种无需额外训练、能充分利用VLM语义的跨模态对齐方案。

Method: 提出训练免（training-free）的DGL-RSIS框架，三大组件：1) 全局-局部解耦（GLD）：用NLP将文本拆为“局部类名（nouns）”与“全局修饰（modifiers）”；用无监督mask proposal网络生成类别无关的候选掩膜。2) 局部尺度对齐（用于OVSS）：提出“上下文感知裁剪”获取边界合理的图像patch；向文本注入遥感特定知识增强表征；以掩膜引导提取视觉特征，与增强后的文本特征匹配，实现掩膜分类，实现开放词汇分割。3) 全局尺度对齐（用于RES）：提出Cross-Scale Grad-CAM，利用全局修饰信息在多尺度上细化Grad-CAM热力图；随后通过掩膜选择模块把像素级Grad-CAM激活融合到掩膜层级输出，实现更精确、可解释的全局-局部一致对齐。

Result: 在不额外训练的前提下，同时支持开放词汇语义分割与指代表达分割；通过跨尺度Grad-CAM细化与掩膜级融合，提高掩膜分类准确度与可解释性（文中暗示在RS数据集上表现优于现有训练免或零样本基线）。

Conclusion: 通过文本/图像全局-局部解耦与双尺度对齐，可显著缓解VLM从自然图像迁移到遥感分割的域差与类别受限问题；DGL-RSIS提供了一条无需训练即可在RS场景实现高质量开放词汇与指代表达分割的途径。

Abstract: The emergence of vision language models (VLMs) has bridged vision and
language, enabling joint multimodal understanding beyond traditional
visual-only deep learning models. However, transferring VLMs from the natural
image domain to remote sensing (RS) segmentation remains challenging due to the
limited category diversity in RS datasets and the domain gap between natural
and RS imagery. Here, we propose a training-free framework, DGL-RSIS, that
decouples visual and textual inputs, performing visual-language alignment at
both the local semantic and global contextual levels through tailored
strategies. Specifically, we first introduce a global-local decoupling (GLD)
module, where text inputs are divided into local class nouns and global
modifiers using natural language processing (NLP) techniques; image inputs are
partitioned into a set of class-agnostic mask proposals via unsupervised mask
proposal networks. Second, visual and textual features are aligned at local
scale, through a novel context-aware cropping strategy for extracting image
patches with proper boundaries and introducing RS-specific knowledge to enrich
the text inputs. By matching the enhanced text features with mask-guided visual
features, we enable the mask classification, supporting open-vocabulary
semantic segmentation (OVSS). Third, at the global scale, we propose a
Cross-Scale Grad-CAM module to refine Grad-CAM maps using contextual
information from global modifiers. A subsequent mask selection module
integrates pixel-level Grad-CAM activations into the mask-level segmentation
output, such that accurate and interpretable alignment can be realized across
global and local dimensions for referring expression segmentation (RES).

</details>


### [51] [Towards Methane Detection Onboard Satellites](https://arxiv.org/abs/2509.00626)
*Maggie Chen,Hala Lambdouar,Luca Marini,Laura Martínez-Ferrer,Chris Bridges,Giacomo Acciarini*

Main category: cs.CV

TL;DR: 提出UnorthoDOS：直接用未正射校正的EMIT高光谱数据训练/部署甲烷羽流检测ML模型，性能与正射数据相当；基于正射数据的ML模型也优于传统匹配滤波（mag1c）。同时发布数据集与代码，便于在卫星端快速检测、降低下行成本。


<details>
  <summary>Details</summary>
Motivation: 甲烷快速监测对减排关键；卫星端ML可加速响应并省下带宽。传统流程需正射校正与匹配滤波，计算与数据预处理昂贵且延迟高，难以上星端实时应用。作者希望验证：1) 未正射数据是否足以训练/推理；2) 端到端ML能否超越传统信号处理基线。

Method: 构建两类EMIT高光谱数据集：正射与未正射（UnorthoDOS）；训练多种ML模型（未具体列出，但应为卷积/光谱-空间网络）分别在两数据上；与传统matched filter基线mag1c比较；评估检测性能（如精度/召回/ROC，文摘未给指标）。发布模型检查点、数据与代码以复现。

Result: 在未正射数据上训练的模型性能与在正射数据上训练的模型相当；在正射数据上训练的模型可超过mag1c基线。

Conclusion: 正射校正并非ML甲烷检测的必要前置，可省略以简化管线、降低延迟与算力需求，适合星载端快速检测；端到端ML在正射数据上也能优于传统方法。公开资源有助于社区复现与扩展。

Abstract: Methane is a potent greenhouse gas and a major driver of climate change,
making its timely detection critical for effective mitigation. Machine learning
(ML) deployed onboard satellites can enable rapid detection while reducing
downlink costs, supporting faster response systems. Conventional methane
detection methods often rely on image processing techniques, such as
orthorectification to correct geometric distortions and matched filters to
enhance plume signals. We introduce a novel approach that bypasses these
preprocessing steps by using \textit{unorthorectified} data (UnorthoDOS). We
find that ML models trained on this dataset achieve performance comparable to
those trained on orthorectified data. Moreover, we also train models on an
orthorectified dataset, showing that they can outperform the matched filter
baseline (mag1c). We release model checkpoints and two ML-ready datasets
comprising orthorectified and unorthorectified hyperspectral images from the
Earth Surface Mineral Dust Source Investigation (EMIT) sensor at
https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at
https://github.com/spaceml-org/plume-hunter.

</details>


### [52] [MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation](https://arxiv.org/abs/2509.00649)
*Aviral Chharia,Wenbo Gou,Haoye Dong*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: While significant progress has been made in single-view 3D human pose
estimation, multi-view 3D human pose estimation remains challenging,
particularly in terms of generalizing to new camera configurations. Existing
attention-based transformers often struggle to accurately model the spatial
arrangement of keypoints, especially in occluded scenarios. Additionally, they
tend to overfit specific camera arrangements and visual scenes from training
data, resulting in substantial performance drops in new settings. In this
study, we introduce a novel Multi-View State Space Modeling framework, named
MV-SSM, for robustly estimating 3D human keypoints. We explicitly model the
joint spatial sequence at two distinct levels: the feature level from
multi-view images and the person keypoint level. We propose a Projective State
Space (PSS) block to learn a generalized representation of joint spatial
arrangements using state space modeling. Moreover, we modify Mamba's
traditional scanning into an effective Grid Token-guided Bidirectional Scanning
(GTBS), which is integral to the PSS block. Multiple experiments demonstrate
that MV-SSM achieves strong generalization, outperforming state-of-the-art
methods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU
Panoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP
(+38%) on Campus A1 in cross-dataset evaluations. Project Website:
https://aviralchharia.github.io/MV-SSM

</details>


### [53] [Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains](https://arxiv.org/abs/2509.00658)
*Yumeng Lin,Dong Li,Xintao Wu,Minglai Shao,Xujiang Zhao,Zhong Chen,Chen Zhao*

Main category: cs.CV

TL;DR: 提出Face4FairShifts，一个用于评估公平性与域泛化的面部图像基准；10万张、4个域、14大类39项标注；实验揭示在分布漂移下显著性能与公平性差距，强调现有数据与方法的不足，呼吁更有效的公平域适配；数据已开源。


<details>
  <summary>Details</summary>
Motivation: 现有公平性评估多在单一域或小规模数据上，难以系统检验模型在分布漂移下的公平与鲁棒性；相关数据集维度有限、注释不足，难以支撑对群体差异与域偏移的联合分析。

Method: 构建大规模面部数据集：包含4个视觉差异显著的域、10万张图像、覆盖14类属性下的39项细粒度注释；设计并运行广泛实验，对公平性与性能在跨域设置下进行系统测评，与现有方法与数据集进行对比。

Result: 在跨域测试中，模型在整体性能与群体公平指标上均出现显著退化，且不同域与属性组合下的差距明显；现有相关数据集不足以暴露这些问题。

Conclusion: Face4FairShifts为评估与推动公平感知的域泛化/域适配研究提供了全面测试平台，表明当前方法在分布漂移下对公平性的保障不足，亟需更有效的公平域适配技术；数据集已公开可用。

Abstract: Ensuring fairness and robustness in machine learning models remains a
challenge, particularly under domain shifts. We present Face4FairShifts, a
large-scale facial image benchmark designed to systematically evaluate
fairness-aware learning and domain generalization. The dataset includes 100,000
images across four visually distinct domains with 39 annotations within 14
attributes covering demographic and facial features. Through extensive
experiments, we analyze model performance under distribution shifts and
identify significant gaps. Our findings emphasize the limitations of existing
related datasets and the need for more effective fairness-aware domain
adaptation techniques. Face4FairShifts provides a comprehensive testbed for
advancing equitable and reliable AI systems. The dataset is available online at
https://meviuslab.github.io/Face4FairShifts/.

</details>


### [54] [Automatic Identification and Description of Jewelry Through Computer Vision and Neural Networks for Translators and Interpreters](https://arxiv.org/abs/2509.00661)
*Jose Manuel Alcalde-Llergo,Aurora Ruiz-Mezcua,Rocio Avila-Ramirez,Andrea Zingoni,Juri Taborri,Enrique Yeguas-Bolivar*

Main category: cs.CV

TL;DR: 提出用多层级图像描述模型自动识别并生成珠宝自然语言说明，面向翻译与口译需求，最终模型 caption 准确率>90%。


<details>
  <summary>Details</summary>
Motivation: 珠宝种类与设计繁多，非业内人士（如翻译/口译）难以准确描述与理解；现有精确描述多依赖专家，亟需自动化工具快速提供可靠的结构化与自然语言信息。

Method: 构建包含多类型配饰的大规模图像数据库；采用计算机视觉+图像描述（image captioning）方案，设计三层次（由粗到细）的描述框架；比较多种图像描述架构，重点评估编码器-解码器模型；系统包含珠宝检测与分类并生成不同细粒度的自然语言说明。

Result: 在自建数据集上，多种架构对比后，最终选用的编码器-解码器图像描述模型在生成描述上的“captioning accuracy”超过90%。

Conclusion: 多层级自然语言描述结合视觉识别能有效模拟专家对珠宝的分析，为语言服务人员提供快捷、准确的知识支持；方法在内部数据上表现优异，具备应用潜力。

Abstract: Identifying jewelry pieces presents a significant challenge due to the wide
range of styles and designs. Currently, precise descriptions are typically
limited to industry experts. However, translators and interpreters often
require a comprehensive understanding of these items. In this study, we
introduce an innovative approach to automatically identify and describe jewelry
using neural networks. This method enables translators and interpreters to
quickly access accurate information, aiding in resolving queries and gaining
essential knowledge about jewelry. Our model operates at three distinct levels
of description, employing computer vision techniques and image captioning to
emulate expert analysis of accessories. The key innovation involves generating
natural language descriptions of jewelry across three hierarchical levels,
capturing nuanced details of each piece. Different image captioning
architectures are utilized to detect jewels in images and generate descriptions
with varying levels of detail. To demonstrate the effectiveness of our approach
in recognizing diverse types of jewelry, we assembled a comprehensive database
of accessory images. The evaluation process involved comparing various image
captioning architectures, focusing particularly on the encoder decoder model,
crucial for generating descriptive captions. After thorough evaluation, our
final model achieved a captioning accuracy exceeding 90 per cent.

</details>


### [55] [Fusion to Enhance: Fusion Visual Encoder to Enhance Multimodal Language Model](https://arxiv.org/abs/2509.00664)
*Yifei She,Huangxuan Wu*

Main category: cs.CV

TL;DR: 提出FtZ框架：用语义强的“锚点”编码器+细节感知强的“增强”编码器，通过轻量级多头交叉注意力融合，显著提升MLLM在细粒度视觉任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多用单一视觉编码器，强调高层语义对齐，导致细节感知能力不足；在文本推理强但基础视觉任务（文字识别、小目标、局部属性）表现差。需要一种既保留语义理解又捕捉精细视觉信息的架构。

Method: 提出Fusion to Enhance (FtZ)“视觉塔”框架：1）选用两个异构专家编码器——语义强的anchor encoder与感知丰富的augmenting encoder；2）使用轻量级多头交叉注意力（MHCA）在两者间进行特征互导与融合；3）保持整体训练/推理开销可控（轻量融合层），在LLM前提供更信息充分的视觉表征。

Result: 在TextVQA、POPE、MMMU、MME、MM-Vet等强调细粒度理解的基准上，FtZ显著优于单编码器基线和既有特征融合方案。

Conclusion: 异构视觉编码器的组合加上高效的跨注意力融合能有效缓解MLLM细节感知瓶颈，为下一代具更强感知能力的MLLM提供新范式。

Abstract: Multimodal Large Language Models (MLLMs) have made significant progress in
bridging visual perception with high-level textual reasoning. However, they
face a fundamental contradiction: while excelling at complex semantic
understanding, these models often fail at basic visual tasks that require
precise detail perception. This deficiency primarily stems from the prevalent
architectural reliance on a single vision encoder optimized for high-level
semantic alignment, which inherently sacrifices the ability to capture
fine-grained visual information. To address this issue, we introduce Fusion to
Enhance (FtZ), a novel vision tower framework. FtZ moves beyond the
single-encoder design by innovatively composing a semantically powerful anchor
encoder with a perception-rich augmenting encoder via a lightweight Multi-Head
Cross-Attention mechanism. Experimental results demonstrate that on several
challenging benchmarks demanding fine-grained visual understanding, such as
TextVQA, POPE, MMMU, MME and MM-Vet, our FtZ model significantly outperforms
baselines that use only a single encoder or existing feature fusion methods.
This work proves that composing heterogeneous expert encoders is an efficient
and effective path to overcoming the visual perception bottleneck in current
MLLMs, offering a new design paradigm for building next-generation AI systems
with stronger perceptual capabilities.

</details>


### [56] [ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth Estimation](https://arxiv.org/abs/2509.00665)
*Weilong Yan,Xin Zhang,Robby T. Tan*

Main category: cs.CV

TL;DR: 提出STM策略，实现对视觉基础模型进行参数高效微调，仅用少量正常天气数据即可在多种恶劣天气下进行单目深度估计，显著优于现有PEFT、全量微调与合成不良天气训练方案。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气下缺乏可靠真实深度标注，合成数据存在域间隙，自监督在逆光/积雪/雨雾等场景违背光度一致性假设，导致现有方法效果受限。需要一种既能适应恶办天气、又能保留VFM强泛化能力、且参数高效的微调方法。

Method: 提出Selecting–Tuning–Maintaining (STM) 策略，基于两种有效秩：entropy-rank与stable-rank，对VFM预训练权重进行结构化分解与约束。• Selecting：依据entropy-rank与全量微调得到的权重，选择合适的秩与与任务相关的奇异方向用于初始化；• Tuning：在选定秩/方向子空间内进行参数高效微调；• Maintaining：依据stable-rank施加主方向正则，维持预训练模型的主成分方向，避免灾难性遗忘。整体实现灵活适配与知识保持的平衡。

Result: 在四个真实世界、涵盖雨/雾/雪/夜间等多种天气的基准上，STM优于现有PEFT方法与全量微调；也超过使用不良天气合成数据训练的方案，甚至超过深度领域的基础模型。

Conclusion: 通过对VFM进行基于有效秩的选择–微调–保持（STM）流程，可在仅用少量高能见度数据的条件下实现跨恶劣天气的泛化型单目深度估计，证明了PEFT在几何任务中的潜力，并提供了适配与保知识的有效平衡机制。

Abstract: Monocular depth estimation under adverse weather conditions (e.g.\ rain, fog,
snow, and nighttime) remains highly challenging due to the lack of reliable
ground truth and the difficulty of learning from unlabeled real-world data.
Existing methods often rely on synthetic adverse data with pseudo-labels, which
suffer from domain gaps, or employ self-supervised learning, which violates
photometric assumptions in adverse scenarios. In this work, we propose to
achieve weather--generalized depth estimation by Parameter--Efficient
Fine--Tuning (PEFT) of Vision Foundation Models (VFMs), using only a small
amount of high--visibility (normal) data. While PEFT has shown strong
performance in semantic tasks such as segmentation, it remains underexplored
for geometry--centric tasks like depth estimation -- especially in terms of
balancing effective adaptation with the preservation of pretrained knowledge.
To this end, we introduce the Selecting--Tuning--Maintaining (STM) strategy,
which structurally decomposes the pretrained weights of VFMs based on two kinds
of effective ranks (entropy--rank and stable--rank). In the tuning phase, we
adaptively select the proper rank number as well as the task--aware singular
directions for initialization, based on the entropy--rank and full--tuned
weight; while in the maintaining stage, we enforce a principal direction
regularization based on the stable--rank. This design guarantees flexible task
adaptation while preserving the strong generalization capability of the
pretrained VFM. Extensive experiments on four real--world benchmarks across
diverse weather conditions demonstrate that STM not only outperforms existing
PEFT methods and full fine--tuning but also surpasses methods trained with
adverse synthetic data, and even the depth foundation model

</details>


### [57] [LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model](https://arxiv.org/abs/2509.00676)
*Xiyao Wang,Chunyuan Li,Jianwei Yang,Kai Zhang,Bo Liu,Tianyi Xiong,Furong Huang*

Main category: cs.CV

TL;DR: 把“评审/打分的评论模型”直接用作生成策略：用偏好标注的评论数据构造成可验证信号，对基座多模态生成模型做RL，得到既会评估又会生成的LLaVA-Critic-R1/R1+，在多项视觉推理基准上达SOTA，并可在推理时自我评审带来明显增益。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言系统将“评论者(critic)”和“策略(policy)”分离：评论者只打分或比较，策略负责生成。这样一来，评论数据的价值仅用于训练评估器，没被直接用于提升生成能力；同时需要维护两套模型，难以扩展与自我改进。作者想打破这种分工，让评论数据直接优化生成策略，获得能统一“评估+生成”的模型，提高多模态推理性能与可扩展性。

Method: - 将偏好标注的评论数据重组为可验证的训练信号（如从对比/偏好数据导出奖励）。
- 直接对基座多模态生成模型（Qwen-2.5-VL-7B）进行强化学习，优化其在偏好判断上的表现，同时保留完整的生成能力，得到LLaVA-Critic-R1。
- 将相同流程扩展到已有强推理VLM，构成LLaVA-Critic-R1+。
- 在推理阶段引入“自我批评”(self-critique)：模型先评估/反思再调整答案，无需额外训练。

Result: - LLaVA-Critic-R1作为评论器达到顶尖，同时作为策略模型在26个视觉推理/理解基准上与专门的推理VLM匹敌或超越，相比基座平均+5.7%。
- R1+在不牺牲评论质量的前提下进一步提升策略性能；在7B规模的MMMU上达71.9的SOTA。
- 测试时启用自我批评，在5个代表性推理任务上平均带来+13.8%的提升。

Conclusion: 用评论数据做RL可以得到一个统一的“评估+生成”的多模态模型，既能打分也能高质量生成；该路线简单、可扩展，并通过推理时自我批评进一步提升表现，指向可自我改进的多模态系统。

Abstract: In vision-language modeling, critic models are typically trained to evaluate
outputs -- assigning scalar scores or pairwise preferences -- rather than to
generate responses. This separation from policy models, which produce the
responses, is so entrenched that critics are rarely considered for direct
policy use. In this work, we challenge this convention. We propose to
reorganize preference-labeled critic datasets into verifiable training signals
and perform reinforcement learning directly on a base generative model,
producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference
judgments while retaining full generation ability. Surprisingly,
LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a
competitive policy model -- matching or surpassing specialized reasoning VLMs
trained with in-domain data across 26 visual reasoning and understanding
benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B).
Extending this approach to existing strong reasoning VLMs yields
LLaVA-Critic-R1+, which further advances policy performance without sacrificing
critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale.
Finally, we show that the enhanced critic ability benefits inference: applying
self-critique at test time yields an average +13.8% improvement on five
representative reasoning tasks without additional training. Our results reveal
that RL training on critic data can produce a unified model excelling at both
evaluation and generation, offering a simple path toward scalable,
self-improving multimodal systems.

</details>


### [58] [CSFMamba: Cross State Fusion Mamba Operator for Multimodal Remote Sensing Image Classification](https://arxiv.org/abs/2509.00677)
*Qingyu Wang,Xue Jiang,Guozheng Xu*

Main category: cs.CV

TL;DR: 提出CSFMamba网络，用Mamba的线性复杂度与CNN结合，并设计跨状态融合模块，实现HSI与LiDAR高效深度融合，较Transformer在MUUFL与Houston2018上更优且训练负担更小。


<details>
  <summary>Details</summary>
Motivation: 现有多模态（HSI+LiDAR）融合常依赖CNN/Transformer，长程依赖建模复杂度高（通常二次），导致训练/推理负担重；Mamba基于时变参数的状态空间模型具线性复杂度但缺乏直接的特征融合机制。因此需要一种既能保持线性高效、又能有效融合多模态空间-光谱信息的框架。

Method: 1) 预处理模块：为适配Mamba结构对序列化/通道组织的需求，对遥感数据进行特定重排与规范化，并与CNN结合提取多层特征。2) 主干设计：融合CNN的局部纹理建模与Mamba的长程依赖建模，构成更强的混合骨干。3) 跨状态模块（基于Mamba算子）：创造性地在Mamba状态空间内进行跨模态交互，利用时变参数/选择门实现HSI与LiDAR间的跨状态信息传递与融合，实现全图范围的依赖捕获。4) 端到端训练，在多层级进行融合监督/正则。

Result: 在MUUFL与Houston2018两套HSI+LiDAR数据上，相比Transformer等强基线取得更高分类精度（具体数值未给出），同时显著降低训练计算/显存开销，表明CSFMamba在效率与性能上兼具优势。

Conclusion: CSFMamba通过跨状态Mamba融合模块与CNN-Mamba混合骨干，实现在低计算复杂度下对HSI与LiDAR的充分融合与全局建模，超过Transformer类方法并降低训练负担，展示了SSM在遥感多模态融合中的潜力。

Abstract: Multimodal fusion has made great progress in the field of remote sensing
image classification due to its ability to exploit the complementary
spatial-spectral information. Deep learning methods such as CNN and Transformer
have been widely used in these domains. State Space Models recently highlighted
that prior methods suffer from quadratic computational complexity. As a result,
modeling longer-range dependencies of spatial-spectral features imposes an
overwhelming burden on the network. Mamba solves this problem by incorporating
time-varying parameters into ordinary SSM and performing hardware optimization,
but it cannot perform feature fusion directly. In order to make full use of
Mamba's low computational burden and explore the potential of internal
structure in multimodal feature fusion, we propose Cross State Fusion Mamba
(CSFMamba) Network. Specifically, we first design the preprocessing module of
remote sensing image information for the needs of Mamba structure, and combine
it with CNN to extract multi-layer features. Secondly, a cross-state module
based on Mamba operator is creatively designed to fully fuse the feature of the
two modalities. The advantages of Mamba and CNN are combined by designing a
more powerful backbone. We capture the fusion relationship between HSI and
LiDAR modalities with stronger full-image understanding. The experimental
results on two datasets of MUUFL and Houston2018 show that the proposed method
outperforms the experimental results of Transformer under the premise of
reducing the network training burden.

</details>


### [59] [CascadeFormer: A Family of Two-stage Cascading Transformers for Skeleton-based Human Action Recognition](https://arxiv.org/abs/2509.00692)
*Yusen Peng,Alper Yilmaz*

Main category: cs.CV

TL;DR: 提出CascadeFormer：一个两阶段级联Transformer用于基于骨架的人体动作识别，先做掩码预训练学习通用表示，再级联微调进行判别分类，在多数据集上取得有竞争力结果，并开源代码与权重。


<details>
  <summary>Details</summary>
Motivation: GCN长期主导骨架动作识别，但Transformer与掩码自监督预训练在表征学习上展现优势；希望摆脱对手工图结构与局部归纳偏置的依赖，获得更通用、可迁移的骨架表示，并提升下游分类性能。

Method: 设计CascadeFormer两阶段流程：1) 掩码预训练阶段（类似MAE/MIM）：对骨架关节序列进行部分掩码，通过Transformer重建或预测被掩码信息，学习时空表征；2) 级联微调阶段：将多个Transformer层（或模块）按级联方式组织，用于判别式动作分类，可能逐级聚合时空信息或分辨关节/帧层级特征。

Result: 在Penn Action、N-UCLA、NTU RGB+D 60三大基准上取得“有竞争力”的效果（未给具体数值），显示方法在跨数据集上具备强性能与泛化性。

Conclusion: 基于Transformer的掩码预训练结合级联微调可在骨架动作识别中替代/补充GCN，提供强表征与分类能力；方法可复现并已开放代码与模型。

Abstract: Skeleton-based human action recognition leverages sequences of human joint
coordinates to identify actions performed in videos. Owing to the intrinsic
spatiotemporal structure of skeleton data, Graph Convolutional Networks (GCNs)
have been the dominant architecture in this field. However, recent advances in
transformer models and masked pretraining frameworks open new avenues for
representation learning. In this work, we propose CascadeFormer, a family of
two-stage cascading transformers for skeleton-based human action recognition.
Our framework consists of a masked pretraining stage to learn generalizable
skeleton representations, followed by a cascading fine-tuning stage tailored
for discriminative action classification. We evaluate CascadeFormer across
three benchmark datasets (Penn Action N-UCLA, and NTU RGB+D 60), achieving
competitive performance on all tasks. To promote reproducibility, we release
our code and model checkpoints.

</details>


### [60] [Prompt the Unseen: Evaluating Visual-Language Alignment Beyond Supervision](https://arxiv.org/abs/2509.00700)
*Raehyuk Jung,Seungjun Yu,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出评测VLM投影层（将视觉特征映射到LLM嵌入）对未见概念的泛化能力的基准。将检测数据集改造成提示格式并构造完全不重叠的类划分。发现投影层在未见类上仍保留约79–88%的已见类性能，并从可解释性角度揭示其前馈网络像“键值记忆”般对已见/未见token以相似机制处理。结论：可在有限对齐数据下实现高效VLM训练。


<details>
  <summary>Details</summary>
Motivation: VLM对齐的关键在于投影层，但社区缺少系统性测评其对未见视觉概念的泛化能力。现有基准难以精准控制“见过/未见”概念分隔，难以回答投影层是否在无显式监督下仍能泛化。

Method: 1) 构建基准：将具有细粒度标注的目标检测数据集转为问答/提示格式；2) 设计训练/测试的类别完全不相交划分，严格控制已见与未见概念；3) 在多种设置下评估VLM，比较未见类对已见类的相对性能；4) 进行机制可解释性分析，研究投影层前馈网络的内部表示与运算（视作键值记忆）。

Result: 在不同模型与设置下，未见类的性能可达到已见类的79–88%，显示出显著但非完美的泛化；机制分析发现投影层的前馈网络以类似方式处理已见与未见token，呈现“键值存取”模式。

Conclusion: 提出了衡量VLM投影层对齐泛化的新评测框架，证明投影层即使缺少对某些概念的对齐监督也具备较强泛化；这启示可用有限的对齐数据实现高效训练，并为后续改进投影架构与数据选择提供依据。

Abstract: Vision-Language Models (VLMs) combine a vision encoder and a large language
model (LLM) through alignment training, showing strong performance on
multimodal tasks. A central component in this architecture is the projection
layer, which maps visual features into the LLM's embedding space. Despite its
importance, its ability to generalize to unseen visual concepts has not been
systematically evaluated. To address this, we propose a benchmark for
evaluating projection-layer generalization. We adapt object detection datasets
(rich in fine-grained annotations) into a prompting format and design
train/test splits with disjoint label sets, enabling precise control over seen
and unseen concept separation. Experimental results show that the projection
layer retains about 79 to 88 percent of the performance on unseen classes
compared to seen ones across various settings, suggesting a non-trivial level
of generalization even without explicit alignment supervision on those
concepts. We further analyze this behavior through a mechanistic
interpretability lens. Our findings indicate that the feed-forward network in
the projection layer functions like a key-value memory, processing seen and
unseen tokens in similar ways. This study introduces a new evaluation framework
for alignment generalization and highlights the potential for efficient VLM
training with limited aligned data.

</details>


### [61] [Enhancing Fairness in Skin Lesion Classification for Medical Diagnosis Using Prune Learning](https://arxiv.org/abs/2509.00745)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos,Tanaya Maslekar*

Main category: cs.CV

TL;DR: 提出一种面向皮肤病变分类的公平性算法：通过计算VGG卷积层特征图、ViT补丁与注意力头的偏度，裁剪与肤色相关的冗余通道/头，聚焦病灶区域，从而降低偏差与计算成本并可能缩小模型体积。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽提升皮肤病变分类准确率，但肤色相关偏差会影响临床公平性；现有公平方法依赖难以获取/标准化的肤色标签、统计校正复杂且计算开销大，客观验证困难，限制实际落地。

Method: 在CNN与ViT架构中（以VGG与Vision Transformer为例），对中间表示的通道/补丁/注意力头计算分布偏度（skewness），将高偏度、与肤色特征相关的通道/头判为“非病灶”冗余并进行裁剪或降权，从而抑制肤色信号，保留与病灶相关的表征；不依赖显式肤色标签或传统统计公平性约束。

Result: 方法在不额外引入统计公平约束和高计算成本的前提下，降低对肤色的敏感性与模型偏差，同时减少推理计算量；在维持分类性能的同时提升跨肤色群体的一致性。

Conclusion: 基于偏度驱动的特征/头部裁剪可在CNN与ViT中实现对肤色偏差的缓解与成本下降，具备实际部署价值；可在保证性能的同时提升公平性并减少模型体积。

Abstract: Recent advances in deep learning have significantly improved the accuracy of
skin lesion classification models, supporting medical diagnoses and promoting
equitable healthcare. However, concerns remain about potential biases related
to skin color, which can impact diagnostic outcomes. Ensuring fairness is
challenging due to difficulties in classifying skin tones, high computational
demands, and the complexity of objectively verifying fairness. To address these
challenges, we propose a fairness algorithm for skin lesion classification that
overcomes the challenges associated with achieving diagnostic fairness across
varying skin tones. By calculating the skewness of the feature map in the
convolution layer of the VGG (Visual Geometry Group) network and the patches
and the heads of the Vision Transformer, our method reduces unnecessary
channels related to skin tone, focusing instead on the lesion area. This
approach lowers computational costs and mitigates bias without relying on
conventional statistical methods. It potentially reduces model size while
maintaining fairness, making it more practical for real-world applications.

</details>


### [62] [Causal Interpretation of Sparse Autoencoder Features in Vision](https://arxiv.org/abs/2509.00749)
*Sangyu Han,Yearim Kim,Nojun Kwak*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Understanding what sparse auto-encoder (SAE) features in vision transformers
truly represent is usually done by inspecting the patches where a feature's
activation is highest. However, self-attention mixes information across the
entire image, so an activated patch often co-occurs with-but does not cause-the
feature's firing. We propose Causal Feature Explanation (CaFE), which leverages
Effective Receptive Field (ERF). We consider each activation of an SAE feature
to be a target and apply input-attribution methods to identify the image
patches that causally drive that activation. Across CLIP-ViT features, ERF maps
frequently diverge from naive activation maps, revealing hidden context
dependencies (e.g., a "roaring face" feature that requires the co-occurrence of
eyes and nose, rather than merely an open mouth). Patch insertion tests confirm
that CaFE more effectively recovers or suppresses feature activations than
activation-ranked patches. Our results show that CaFE yields more faithful and
semantically precise explanations of vision-SAE features, highlighting the risk
of misinterpretation when relying solely on activation location.

</details>


### [63] [EVENT-Retriever: Event-Aware Multimodal Image Retrieval for Realistic Captions](https://arxiv.org/abs/2509.00751)
*Dinh-Khoi Vo,Van-Loc Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 提出一套多阶段事件图像检索系统，融合语言检索、事件感知重排与多模态打分，并用RRF集成，最终在EVENTA 2025 Track 2 私测集拿到top-1。


<details>
  <summary>Details</summary>
Motivation: 自由文本描述往往包含抽象事件、隐含因果、时间语境与长叙事，传统视觉-语言检索对这类“事件语义+世界知识”理解不足，导致召回与排序弱。作者希望通过引入更强的语言推理与事件对齐，弥补仅依赖视觉或浅层跨模态对齐的不足。

Method: 多阶段流水线：1) 密集文章检索（Qwen3）把自由文本映射到相关新闻/文章，扩大知识与上下文；2) 事件感知重排（Qwen3-Reranker）进行语境与事件一致性对齐；3) 高效图像集合构建（从候选文章收集相关图像）；4) 文本引导的语义匹配（Qwen2-VL对图像与caption精细打分）；5) Rank-aware选择（利用排序信号筛选最终结果）；6) 多配置结果用Reciprocal Rank Fusion融合，提升稳健性与覆盖率。

Result: 在EVENTA 2025 Grand Challenge的Track 2私有测试集上获得top-1成绩，显示该框架在复杂、真实世界事件图像检索上的有效性。

Conclusion: 语言驱动的事件理解与多模态检索深度结合（检索→重排→多模态打分→排序融合）能显著提升自由文本到事件图像的匹配效果；RRF等集成方法提升鲁棒性。代码已开源，便于复现与扩展。

Abstract: Event-based image retrieval from free-form captions presents a significant
challenge: models must understand not only visual features but also latent
event semantics, context, and real-world knowledge. Conventional
vision-language retrieval approaches often fall short when captions describe
abstract events, implicit causality, temporal context, or contain long, complex
narratives. To tackle these issues, we introduce a multi-stage retrieval
framework combining dense article retrieval, event-aware language model
reranking, and efficient image collection, followed by caption-guided semantic
matching and rank-aware selection. We leverage Qwen3 for article search,
Qwen3-Reranker for contextual alignment, and Qwen2-VL for precise image
scoring. To further enhance performance and robustness, we fuse outputs from
multiple configurations using Reciprocal Rank Fusion (RRF). Our system achieves
the top-1 score on the private test set of Track 2 in the EVENTA 2025 Grand
Challenge, demonstrating the effectiveness of combining language-based
reasoning and multimodal retrieval for complex, real-world image understanding.
The code is available at https://github.com/vdkhoi20/EVENT-Retriever.

</details>


### [64] [Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification](https://arxiv.org/abs/2509.00752)
*Y Hop Nguyen,Doan Anh Phan Huu,Trung Thai Tran,Nhat Nam Mai,Van Toi Giap,Thao Thi Phuong Dao,Trung-Nghia Le*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We present a unified vision-language framework tailored for ENT endoscopy
image analysis that simultaneously tackles three clinically-relevant tasks:
image classification, image-to-image retrieval, and text-to-image retrieval.
Unlike conventional CNN-based pipelines that struggle to capture cross-modal
semantics, our approach leverages the CLIP ViT-B/16 backbone and enhances it
through Low-Rank Adaptation, multi-level CLS token aggregation, and spherical
feature interpolation. These components collectively enable efficient
fine-tuning on limited medical data while improving representation diversity
and semantic alignment across modalities. To bridge the gap between visual
inputs and textual diagnostic context, we introduce class-specific natural
language prompts that guide the image encoder through a joint training
objective combining supervised classification with contrastive learning. We
validated our framework through participation in the ACM MM'25 ENTRep Grand
Challenge, achieving 95% accuracy and F1-score in classification, Recall@1 of
0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, and
MRR scores of 0.97 and 0.96. Ablation studies demonstrated the incremental
benefits of each architectural component, validating the effectiveness of our
design for robust multimodal medical understanding in low-resource clinical
settings.

</details>


### [65] [MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure](https://arxiv.org/abs/2509.00757)
*Xiufeng Huang,Ziyuan Luo,Qi Song,Ruofei Wang,Renjie Wan*

Main category: cs.CV

TL;DR: 提出首个可泛化的3DGS水印框架MarkSplatter，通过一次前向传播即可在Splatter Image表示的3D高斯中嵌入消息，兼顾不可感知性与鲁棒提取。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting在3D内容制作与传播中迅速普及，但现有3DGS水印依赖针对每条消息的昂贵微调，缺乏高效、可泛化、对渲染视图与小目标鲁棒的解决方案。

Method: 1) GaussianBridge：将非结构化3D高斯转换为可被神经网络直接处理的Splatter Image表示，从而实现任意消息的单次前向嵌入；2) 不可感知性：基于高斯不确定性与感知热力图(“Gaussian-Uncertainty-Perceptual”)预测，约束修改区域与幅度以保持视觉质量；3) 鲁棒提取：设计密集分割式解码机制，即使水印目标在渲染视图中占比很小也能稳定恢复消息。

Result: 在无需对每条消息微调的前提下，实现高效嵌入；在多种渲染视角与目标占比极小场景下保持稳定恢复；在视觉质量上接近未水印模型（不可感知）。

Conclusion: MarkSplatter通过结构化表示与感知引导的嵌入/解码策略，实现对3DGS的通用、高效且鲁棒的水印保护，为3D生成与分发中的版权标记提供实用方案。

Abstract: The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the
need for effective copyright protection. Current 3DGS watermarking methods rely
on computationally expensive fine-tuning procedures for each predefined
message. We propose the first generalizable watermarking framework that enables
efficient protection of Splatter Image-based 3DGS models through a single
forward pass. We introduce GaussianBridge that transforms unstructured 3D
Gaussians into Splatter Image format, enabling direct neural processing for
arbitrary message embedding. To ensure imperceptibility, we design a
Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving
visual quality. For robust message recovery, we develop a dense
segmentation-based extraction mechanism that maintains reliable extraction even
when watermarked objects occupy minimal regions in rendered views. Project
page: https://kevinhuangxf.github.io/marksplatter.

</details>


### [66] [No More Sibling Rivalry: Debiasing Human-Object Interaction Detection](https://arxiv.org/abs/2509.00760)
*Bin Yang,Yulin Zhang,Hong-Yu Zhou,Sibei Yang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Detection transformers have been applied to human-object interaction (HOI)
detection, enhancing the localization and recognition of human-action-object
triplets in images. Despite remarkable progress, this study identifies a
critical issue-"Toxic Siblings" bias-which hinders the interaction decoder's
learning, as numerous similar yet distinct HOI triplets interfere with and even
compete against each other both input side and output side to the interaction
decoder. This bias arises from high confusion among sibling
triplets/categories, where increased similarity paradoxically reduces
precision, as one's gain comes at the expense of its toxic sibling's decline.
To address this, we propose two novel debiasing learning
objectives-"contrastive-then-calibration" and "merge-then-split"-targeting the
input and output perspectives, respectively. The former samples sibling-like
incorrect HOI triplets and reconstructs them into correct ones, guided by
strong positional priors. The latter first learns shared features among sibling
categories to distinguish them from other groups, then explicitly refines
intra-group differentiation to preserve uniqueness. Experiments show that we
significantly outperform both the baseline (+9.18% mAP on HICO-Det) and the
state-of-the-art (+3.59% mAP) across various settings.

</details>


### [67] [InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos](https://arxiv.org/abs/2509.00767)
*Yangsong Zhang,Abdul Ahad Butt,Gül Varol,Ivan Laptev*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Human motion generation has shown great advances thanks to the recent
diffusion models trained on large-scale motion capture data. Most of existing
works, however, currently target animation of isolated people in empty scenes.
Meanwhile, synthesizing realistic human-object interactions in complex 3D
scenes remains a critical challenge in computer graphics and robotics. One
obstacle towards generating versatile high-fidelity human-object interactions
is the lack of large-scale datasets with diverse object manipulations. Indeed,
existing motion capture data is typically restricted to single people and
manipulations of limited sets of objects. To address this issue, we propose an
automatic motion extraction pipeline and use it to collect interaction-rich
human motions. Our new dataset InterPose contains 73.8K sequences of 3D human
motions and corresponding text captions automatically obtained from 45.8K
videos with human-object interactions. We perform extensive experiments and
demonstrate InterPose to bring significant improvements to state-of-the-art
methods for human motion generation. Moreover, using InterPose we develop an
LLM-based agent enabling zero-shot animation of people interacting with diverse
objects and scenes.

</details>


### [68] [Secure and Scalable Face Retrieval via Cancelable Product Quantization](https://arxiv.org/abs/2509.00781)
*Haomiao Tang,Wenjie Li,Yixiang Qiu,Genping Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出一种用于人脸检索的可撤销乘积量化框架，在保证隐私的前提下实现高效候选筛选与密文空间精排，兼顾准确性、速度与安全。


<details>
  <summary>Details</summary>
Motivation: 第三方外包的人脸检索阶段易泄露用户肖像隐私；现有同态加密虽安全但计算开销大、不适合实时应用，缺乏在实际系统中可用的高效安全检索方案。

Method: 构建层级化两阶段架构：1) 高吞吐的可撤销PQ（Product Quantization）索引模块，用于快速候选过滤，并设计专门的保护与可撤销（cancelable biometric）机制以在不损失效率的前提下保护索引；2) 在密文空间进行细粒度检索与最终排序的精排模块，避免明文暴露；整体实现在效率、安全与效果之间的折中。

Result: 在多个人脸基准数据集上，方法在检索准确性、检索时延/吞吐、以及对攻击（隐私泄露）防护上取得良好权衡，相较同态加密等纯安全方案更高效，同时保持可接受的检索质量。

Conclusion: 可撤销乘积量化为安全人脸表示检索提供了可落地的工程化方案：用可保护的高速索引做粗筛，再在密文空间精排，实现隐私保护与实用性能的兼顾。

Abstract: Despite the ubiquity of modern face retrieval systems, their retrieval stage
is often outsourced to third-party entities, posing significant risks to user
portrait privacy. Although homomorphic encryption (HE) offers strong security
guarantees by enabling arithmetic computations in the cipher space, its high
computational inefficiency makes it unsuitable for real-time, real-world
applications. To address this issue, we propose Cancelable Product
Quantization, a highly efficient framework for secure face representation
retrieval. Our hierarchical two-stage framework comprises: (i) a
high-throughput cancelable PQ indexing module for fast candidate filtering, and
(ii) a fine-grained cipher-space retrieval module for final precise face
ranking. A tailored protection mechanism is designed to secure the indexing
module for cancelable biometric authentication while ensuring efficiency.
Experiments on benchmark datasets demonstrate that our method achieves an
decent balance between effectiveness, efficiency and security.

</details>


### [69] [Aligned Anchor Groups Guided Line Segment Detector](https://arxiv.org/abs/2509.00786)
*Zeyu Li,Annan Shu*

Main category: cs.CV

TL;DR: 提出AAGLSD，一种以“对齐锚点组”为核心的层次化线段检测器，在保证高精度的同时提升线段完整性与召回，通过简单的验证与合并得到最终结果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有线段检测器常在复杂场景下出现线段断裂、完整性不足或需复杂后处理；作者希望在减少复杂精炼步骤的同时，提高检测到的线段连续性与覆盖度。

Method: 1) 分层提取不同显著性级别的候选像素：常规锚点与“对齐锚点组”。2) 以对齐锚点组为起点，顺序链接锚点，并在生长过程中同步更新当前预测线段。3) 通过简单的验证与相邻线段合并获得最终预测，避免复杂的细化与后处理。

Result: 在多个数据集上评测，定量实验表明相较其他先进线段检测器，AAGLSD能更有效地从图像中提取完整线段，兼顾精度与完整性。

Conclusion: AAGLSD利用对齐锚点组和层次化候选提取，实现了简单而有效的线段检测流程，减少复杂后处理，同时在多数据集上取得更优的完整性与性能；代码已开源。

Abstract: This paper introduces a novel line segment detector, the Aligned Anchor
Groups guided Line Segment Detector (AAGLSD), designed to detect line segments
from images with high precision and completeness. The algorithm employs a
hierarchical approach to extract candidate pixels with different saliency
levels, including regular anchors and aligned anchor groups. AAGLSD initiates
from these aligned anchor groups, sequentially linking anchors and updating the
currently predicted line segment simultaneously. The final predictions are
derived through straightforward validation and merging of adjacent line
segments, avoiding complex refinement strategies. AAGLSD is evaluated on
various datasets and quantitative experiments demonstrate that the proposed
method can effectively extract complete line segments from input images
compared to other advanced line segment detectors. The implementation is
available at https://github.com/LLiDaBao/AAGLSD.

</details>


### [70] [Diffusion-Based Image-to-Brain Signal Generation with Cross-Attention Mechanisms for Visual Prostheses](https://arxiv.org/abs/2509.00787)
*Ganxi Xu,Jinyi Long,Jia Zhang*

Main category: cs.CV

TL;DR: 提出一个基于DDPM并结合跨注意力的图像到脑信号生成框架，用CLIP特征作为条件，在THINGS-EEG2与THINGS-MEG上生成更具生物相似性的M/EEG信号，并可视化个体内外变异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉假体研究中，解码（脑→图像）受扩散模型带动进步明显，但编码（图像→脑）阶段难以生成与真实脑反应足够相似的信号；缺乏来自真实脑反应的有效监督或验证导致生成刺激的生物合理性不足。

Method: 使用预训练CLIP视觉编码器提取图像的高层语义特征；将这些特征通过跨注意力注入到U-Net式DDPM中，使视觉特征与脑信号表征在每个去噪步中动态对齐；与传统“拼接条件”不同，跨注意力提供细粒度条件控制；在THINGS-EEG2与THINGS-MEG数据集上训练与评估；同时可视化训练/测试阶段各受试者的M/EEG地形图以分析个体内与个体间差异。

Result: 在两个多模态数据集上，所提方法能生成更具生物可近性的M/EEG信号（摘要暗示优于基线，具体指标未列出）；可视化结果展示了受试者内外的信号变化模式。

Conclusion: 跨注意力增强的DDPM结合CLIP条件可提升图像→脑信号生成的生物合理性，并为视觉假体的脑编码阶段提供更优的建模路径；可视化分析有助于理解个体差异。

Abstract: Visual prostheses have shown great potential in restoring vision for blind
individuals. On the one hand, researchers have been continuously improving the
brain decoding framework of visual prostheses by leveraging the powerful image
generation capabilities of diffusion models. On the other hand, the brain
encoding stage of visual prostheses struggles to generate brain signals with
sufficient biological similarity. Although existing works have recognized this
problem, the quality of predicted stimuli still remains a critical issue, as
existing approaches typically lack supervised signals from real brain responses
to validate the biological plausibility of predicted stimuli. To address this
issue, we propose a novel image-to-brain framework based on denoising diffusion
probabilistic models (DDPMs) enhanced with cross-attention mechanisms. Our
framework consists of two key architectural components: a pre-trained CLIP
visual encoder that extracts rich semantic representations from input images,
and a cross-attention enhanced U-Net diffusion model that learns to reconstruct
biologically plausible brain signals through iterative denoising. Unlike
conventional generative models that rely on simple concatenation for
conditioning, our cross-attention modules enable dynamic interaction between
visual features and brain signal representations, facilitating fine-grained
alignment during the generation process. We evaluate our framework on two
multimodal datasets (THINGS-EEG2 and THINGS-MEG) to demonstrate its
effectiveness in generating biologically plausible brain signals. Moreover, we
visualize the training and test M/EEG topographies for all subjects on both
datasets to intuitively demonstrate the intra-subject variations and
inter-subject variations in M/EEG signals.

</details>


### [71] [OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving](https://arxiv.org/abs/2509.00789)
*Pei Liu,Qingtian Ning,Xinyan Lu,Haipeng Liu,Weiliang Ma,Dangen She,Peng Jia,Xianpeng Lang,Jun Ma*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Recent advances in vision-language models (VLMs) have demonstrated impressive
spatial reasoning capabilities for autonomous driving, yet existing methods
predominantly focus on static scene understanding while neglecting the
essential temporal dimension of real-world driving scenarios. To address this
critical limitation, we propose the OmniReason framework, which establishes
robust spatiotemporal reasoning by jointly modeling dynamic 3D environments and
their underlying decision-making processes. Our work makes two fundamental
advances: (1) We introduce OmniReason-Data, two large-scale
vision-language-action (VLA) datasets with dense spatiotemporal annotations and
natural language explanations, generated through a novel
hallucination-mitigated auto-labeling pipeline that ensures both physical
plausibility and temporal coherence; (2) We develop the OmniReason-Agent
architecture, which integrates a sparse temporal memory module for persistent
scene context modeling and an explanation generator that produces
human-interpretable decision rationales, facilitated by our spatiotemporal
knowledge distillation approach that effectively captures spatiotemporal causal
reasoning patterns. Comprehensive experiments demonstrate state-of-the-art
performance, where OmniReason-Agent achieves significant improvements in both
open-loop planning tasks and visual question answering (VQA) benchmarks, while
establishing new capabilities for interpretable, temporally-aware autonomous
vehicles operating in complex, dynamic environments.

</details>


### [72] [Multimodal Iterative RAG for Knowledge Visual Question Answering](https://arxiv.org/abs/2509.00798)
*Changin Choi,Wonseok Lee,Jungmin Ko,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出MI-RAG：一种多模态“检索—推理—再检索”迭代框架，利用跨模态多查询联合检索与累积推理记录，显著提升知识密集型VQA的检索召回与作答准确率。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在需要外部知识的视觉问答上受限。传统RAG单轮检索常不足以覆盖复杂、组合型知识需求，导致理解与回答不充分，亟需能在检索与推理间闭环迭代的方法。

Method: 提出MI-RAG：1) 以“推理记录”为核心记忆，跨迭代累积和更新；2) 每轮基于当前推理记录动态生成多查询（multi-query）；3) 面向异构知识库进行联合检索，涵盖视觉落地知识与文本知识；4) 将新检索到的证据综合写回推理记录，提升对问题的跨模态理解；5) 多轮迭代直至信息充分或达收敛并输出答案。

Result: 在Encyclopedic VQA、InfoSeek、OK-VQA等基准上，较基线显著提升：检索召回率与答案准确率均提高，体现对组合推理与知识覆盖的优势。

Conclusion: 迭代式、推理驱动的跨模态RAG能有效缓解单轮检索知识不足的问题，提供可扩展的知识密集型VQA方案，并为多模态组合推理奠定通用框架。

Abstract: While Multimodal Large Language Models (MLLMs) have significantly advanced
multimodal understanding, their performance remains limited on
knowledge-intensive visual questions that require external knowledge beyond the
image. Retrieval-Augmented Generation (RAG) has become a promising solution for
providing models with external knowledge, its conventional single-pass
framework often fails to gather sufficient knowledge. To overcome this
limitation, we propose MI-RAG, a Multimodal Iterative RAG framework that
leverages reasoning to enhance retrieval and update reasoning over newly
retrieved knowledge across modalities. At each iteration, MI-RAG leverages an
accumulated reasoning record to dynamically formulate a multi-query. These
queries then drive a joint search across heterogeneous knowledge bases
containing both visually-grounded and textual knowledge. The newly acquired
knowledge is synthesized into the reasoning record, progressively refining
understanding across iterations. Experiments on challenging benchmarks,
including Encyclopedic VQA, InfoSeek, and OK-VQA, show that MI-RAG
significantly improves both retrieval recall and answer accuracy, establishing
a scalable approach for compositional reasoning in knowledge-intensive VQA.

</details>


### [73] [SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting](https://arxiv.org/abs/2509.00800)
*Zhuodong Jiang,Haoran Wang,Guoxi Huang,Brett Seymour,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 提出将语义引导的3D Gaussian Splatting用于水下场景重建，通过融合CLIP语义特征与分阶段训练，在SeaThru-NeRF与Submerged3D上显著提升PSNR（平均最高+3.09 dB）与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 水下三维重建受折射、浑浊与能见度低等影响，传统/现有AI方法对语义与结构信息利用不足，尤其缺乏将大语言/多模态模型的高层语义融入重建过程，以提升在复杂水下条件下的稳定性与保真度。

Method: 在3D Gaussian Splatting框架中为每个Gaussian原语嵌入额外语义特征，使用CLIP提取的语义特征进行监督；设计语义一致性损失以约束高层语义与几何/外观的一致；采用分阶段（coarse-to-fine）训练与末期参数精修策略以稳定优化并提升质量。

Result: 在SeaThru-NeRF与Submerged3D数据集上，跨三项指标均优于SOTA，PSNR平均最高提升达3.09 dB，体现对浑浊与光照畸变等干扰的更强鲁棒性与高保真重建。

Conclusion: 语义引导的多模态跨知识融合与分阶段训练，可显著改善水下3D重建的稳定性与质量；该方案为海洋探测与水下感知等应用提供了有竞争力的候选方法。

Abstract: Accurate 3D reconstruction in underwater environments remains a complex
challenge due to issues such as light distortion, turbidity, and limited
visibility. AI-based techniques have been applied to address these issues,
however, existing methods have yet to fully exploit the potential of AI,
particularly in integrating language models with visual processing. In this
paper, we propose a novel framework that leverages multimodal cross-knowledge
to create semantic-guided 3D Gaussian Splatting for robust and high-fidelity
deep-sea scene reconstruction. By embedding an extra semantic feature into each
Gaussian primitive and supervised by the CLIP extracted semantic feature, our
method enforces semantic and structural awareness throughout the training. The
dedicated semantic consistency loss ensures alignment with high-level scene
understanding. Besides, we propose a novel stage-wise training strategy,
combining coarse-to-fine learning with late-stage parameter refinement, to
further enhance both stability and reconstruction quality. Extensive results
show that our approach consistently outperforms state-of-the-art methods on
SeaThru-NeRF and Submerged3D datasets across three metrics, with an improvement
of up to 3.09 dB on average in terms of PSNR, making it a strong candidate for
applications in underwater exploration and marine perception.

</details>


### [74] [Adaptive Contrast Adjustment Module: A Clinically-Inspired Plug-and-Play Approach for Enhanced Fetal Plane Classification](https://arxiv.org/abs/2509.00808)
*Yang Chen,Sanglin Zhao,Baoyu Chen,Mans Gustaf*

Main category: cs.CV

TL;DR: 提出一个可插拔的自适应对比度调整模块（ACAM），通过预测对比度参数、生成多种对比增强视图并在分类器中融合，提升胎儿超声标准切面分类在多中心数据上的稳健性与准确率。


<details>
  <summary>Details</summary>
Motivation: 胎儿超声图像存在低组织对比、边界模糊、且图像质量受操作者影响大的问题，导致标准切面自动分类困难。临床上医生会手动调节对比度以看清结构，论文试图把这种实践引入到模型中，以减少随机预处理的不确定性并提升在异质成像条件下的鲁棒性。

Method: 设计一个可插拔的ACAM：1）用浅层、纹理敏感的网络预测与临床合理性一致的对比度参数；2）通过可微映射将输入图像变换为多种对比增强视图；3）在下游分类器中进行多视图特征融合。该模块可与各种现有分类模型无缝结合。

Result: 在包含6个解剖类别、来自多中心的12,400张图像的数据集上验证：轻量级模型准确率提升2.02个百分点，传统模型提升1.29个百分点，SOTA模型提升1.15个百分点，且在不同模型和成像条件下表现稳定。

Conclusion: ACAM以内容感知、物理启发的方式替代随机预处理，贴合超声医师工作流；通过多视图融合提升对成像异质性的鲁棒性，把低层图像特征与高层语义有效衔接，为真实世界质量波动下的医学影像分析提供了新范式。

Abstract: Fetal ultrasound standard plane classification is essential for reliable
prenatal diagnosis but faces inherent challenges, including low tissue
contrast, boundary ambiguity, and operator-dependent image quality variations.
To overcome these limitations, we propose a plug-and-play adaptive contrast
adjustment module (ACAM), whose core design is inspired by the clinical
practice of doctors adjusting image contrast to obtain clearer and more
discriminative structural information. The module employs a shallow
texture-sensitive network to predict clinically plausible contrast parameters,
transforms input images into multiple contrast-enhanced views through
differentiable mapping, and fuses them within downstream classifiers. Validated
on a multi-center dataset of 12,400 images across six anatomical categories,
the module consistently improves performance across diverse models, with
accuracy of lightweight models increasing by 2.02 percent, accuracy of
traditional models increasing by 1.29 percent, and accuracy of state-of-the-art
models increasing by 1.15 percent. The innovation of the module lies in its
content-aware adaptation capability, replacing random preprocessing with
physics-informed transformations that align with sonographer workflows while
improving robustness to imaging heterogeneity through multi-view fusion. This
approach effectively bridges low-level image features with high-level
semantics, establishing a new paradigm for medical image analysis under
real-world image quality variations.

</details>


### [75] [Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization](https://arxiv.org/abs/2509.00826)
*Xinlei Liu,Tao Hu,Peng Yi,Weitao Han,Jichao Xie,Baolin Li*

Main category: cs.CV

TL;DR: 提出一种新的梯度攻击方法SDM，通过“循环-阶段-步”的三层优化框架与新损失DPDR，最大化非真标签概率上界与真标签概率的差，较SOTA更强更高效，并可与对抗训练结合增强防御。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型鲁棒性评估需要高效而强力的对抗攻击；传统目标或损失设计可能在搜索空间大、无效梯度或样本效率不足上受限，难以在有限计算预算下获得强攻击并为对抗训练提供更有效的难样本。

Method: 将对抗目标重构为：最大化“非真标签的概率上界”与“真标签概率”的差。基于此提出Sequential Difference Maximization（SDM）：1) 三层优化框架：循环-cycle、阶段-stage、迭代-step；循环与步内部流程相同，不同阶段采用不同损失。2) 阶段1用负真标签概率作为损失，先压缩解空间、推动远离真类；3) 后续阶段引入Directional Probability Difference Ratio（DPDR）损失，通过压缩无关标签概率、提升非真标签概率上界，逐步放大与真标签的差；4) 全过程为梯度驱动，可与常见约束（如L∞/L2）结合。

Result: 在多组实验中，相比SOTA攻击，SDM实现更高攻击成功率/更低查询或迭代次数（更高性价比）；同时与对抗训练结合时，可提升防御效果（鲁棒准确率提高）。开源代码提供可复现性。

Conclusion: 重构攻击目标并引入分阶段损失（DPDR）使SDM在攻击强度与效率上均优于现有方法，且具备作为对抗训练难负样本生成器的价值。

Abstract: Efficient adversarial attack methods are critical for assessing the
robustness of computer vision models. In this paper, we reconstruct the
optimization objective for generating adversarial examples as "maximizing the
difference between the non-true labels' probability upper bound and the true
label's probability," and propose a gradient-based attack method termed
Sequential Difference Maximization (SDM). SDM establishes a three-layer
optimization framework of "cycle-stage-step." The processes between cycles and
between iterative steps are respectively identical, while optimization stages
differ in terms of loss functions: in the initial stage, the negative
probability of the true label is used as the loss function to compress the
solution space; in subsequent stages, we introduce the Directional Probability
Difference Ratio (DPDR) loss function to gradually increase the non-true
labels' probability upper bound by compressing the irrelevant labels'
probabilities. Experiments demonstrate that compared with previous SOTA
methods, SDM not only exhibits stronger attack performance but also achieves
higher attack cost-effectiveness. Additionally, SDM can be combined with
adversarial training methods to enhance their defensive effects. The code is
available at https://github.com/X-L-Liu/SDM.

</details>


### [76] [Surface Defect Detection with Gabor Filter Using Reconstruction-Based Blurring U-Net-ViT](https://arxiv.org/abs/2509.00827)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出用U-Net+ViT融合的模糊U-Net-ViT模型，结合高斯滤波损失、SP掩膜增强与后置Gabor滤波，进行纹理类表面缺陷检测，跨多数据集AUC≈0.939，并通过参数与消融验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统纹理缺陷检测在复杂纹理、噪声与缺陷边界模糊情况下易漏检、误检；现有局部/全局单一建模方法难以兼顾局部细节与全局纹理一致性；此外噪声与背景纹理会掩盖缺陷模式，需要更鲁棒的损失与训练策略。

Method: - 模型：将U-Net的局部特征学习与ViT的全局上下文建模融合，构建“模糊”U-Net-ViT架构。
- 损失：引入基于高斯滤波的损失，抑制背景噪声、突出缺陷模式。
- 训练增强：采用盐椒噪声(SP)掩膜以强化纹理-缺陷边界鲁棒性。
- 后处理：应用Gabor滤波以突出缺陷的方向与频率特征。
- 参数优化：对Gabor参数（滤波器尺寸、sigma、波长、gamma、方向等）以及噪声概率进行搜索优化。

Result: 在MVTec-AD、Surface Crack Detection、Marble Surface Anomaly等数据集上达到平均AUC 0.939；消融表明合适的滤波器尺寸与噪声概率显著提升检测性能。

Conclusion: 融合局部与全局表征、配合噪声鲁棒的损失与训练策略，并借助后置Gabor强调频率/方向特性，可在多纹理场景下稳定提升缺陷检测；参数选择对性能影响关键。

Abstract: This paper proposes a novel approach to enhance the accuracy and reliability
of texture-based surface defect detection using Gabor filters and a blurring
U-Net-ViT model. By combining the local feature training of U-Net with the
global processing of the Vision Transformer(ViT), the model effectively detects
defects across various textures. A Gaussian filter-based loss function removes
background noise and highlights defect patterns, while Salt-and-Pepper(SP)
masking in the training process reinforces texture-defect boundaries, ensuring
robust performance in noisy environments. Gabor filters are applied in
post-processing to emphasize defect orientation and frequency characteristics.
Parameter optimization, including filter size, sigma, wavelength, gamma, and
orientation, maximizes performance across datasets like MVTec-AD, Surface Crack
Detection, and Marble Surface Anomaly Dataset, achieving an average Area Under
the Curve(AUC) of 0.939. The ablation studies validate that the optimal filter
size and noise probability significantly enhance defect detection performance.

</details>


### [77] [UPGS: Unified Pose-aware Gaussian Splatting for Dynamic Scene Deblurring](https://arxiv.org/abs/2509.00831)
*Zhijing Wu,Longguang Wang*

Main category: cs.CV

TL;DR: 提出端到端统一优化的动态3D重建方法，把相机位姿作为可学习参数与3D高斯(3DGS)一起训练，并用三阶段交替训练策略，在强运动模糊场景下显著提升重建质量和位姿精度。


<details>
  <summary>Details</summary>
Motivation: 单目视频在动态场景重建中常因相机/物体运动引起的强运动模糊而失败。现有“两步法”先估位姿再优化3DGS，模糊会破坏位姿估计，误差累积导致重建质量差。需要一种能同时处理模糊与位姿不准的问题的统一框架。

Method: 把相机与物体运动统一建模为对3D高斯的逐原语(primitive-level) SE(3)仿射变换，并把相机位姿设为可学习参数，和3DGS属性共同端到端优化。为稳定训练，采用三阶段交替训练：1) 固定位姿先训练3D高斯；2) 冻结3D高斯仅优化位姿；3) 解冻全部参数联合优化。整体以统一目标函数进行优化。

Result: 在Stereo Blur数据集及多种真实模糊视频上，相比既有动态去模糊/重建方法，取得显著更好的3D重建质量与位姿估计精度。

Conclusion: 将位姿估计与3DGS重建纳入同一端到端优化，并用三阶段稳定训练，可有效缓解运动模糊导致的位姿误差累积，显著提升动态场景重建与去模糊性能。

Abstract: Reconstructing dynamic 3D scenes from monocular video has broad applications
in AR/VR, robotics, and autonomous navigation, but often fails due to severe
motion blur caused by camera and object motion. Existing methods commonly
follow a two-step pipeline, where camera poses are first estimated and then 3D
Gaussians are optimized. Since blurring artifacts usually undermine pose
estimation, pose errors could be accumulated to produce inferior reconstruction
results. To address this issue, we introduce a unified optimization framework
by incorporating camera poses as learnable parameters complementary to 3DGS
attributes for end-to-end optimization. Specifically, we recast camera and
object motion as per-primitive SE(3) affine transformations on 3D Gaussians and
formulate a unified optimization objective. For stable optimization, we
introduce a three-stage training schedule that optimizes camera poses and
Gaussians alternatively. Particularly, 3D Gaussians are first trained with
poses being fixed, and then poses are optimized with 3D Gaussians being
untouched. Finally, all learnable parameters are optimized together. Extensive
experiments on the Stereo Blur dataset and challenging real-world sequences
demonstrate that our method achieves significant gains in reconstruction
quality and pose estimation accuracy over prior dynamic deblurring methods.

</details>


### [78] [SegDINO: An Efficient Design for Medical and Natural Image Segmentation with DINO-V3](https://arxiv.org/abs/2509.00833)
*Sicheng Yang,Hongqiu Wang,Zhaohu Xing,Sixiang Chen,Lei Zhu*

Main category: cs.CV

TL;DR: 提出SegDINO：在冻结DINOv3骨干的前提下，用极轻量解码器进行语义/实例分割；通过多层特征对齐+MLP直接预测掩码，在六个数据集上达到SOTA，同时显著减少参数与计算。


<details>
  <summary>Details</summary>
Motivation: DINO/DINOv3等自监督视觉模型迁移性强，但在分割任务上常需重型解码器（多尺度融合、复杂上采样），带来参数与算力负担，不利于高效部署与小样本/跨域应用。需要一种既能保留基础模型表达力又能以最小训练开销实现高质量分割的方案。

Method: - 冻结的DINOv3作为编码器，抽取多层（多级）特征。
- 将各层特征对齐到统一的空间分辨率与通道数（轻量投影/上采样与通道对齐）。
- 采用轻量MLP head直接输出分割掩码（避免重型U-Net/FPN式解码器与复杂上采样）。
- 端到端仅训练对齐模块与MLP解码头，最大限度降低可训练参数。

Result: 在六个数据集（医疗：TN3K、Kvasir-SEG、ISIC；自然：MSD、VMD-D、ViSha）上取得一致SOTA或领先表现；在保持或提升精度的同时，相比现有方法显著减少参数与计算开销。

Conclusion: 冻结DINOv3的强表征配合轻量解码器即可实现高效高精度分割，表明大型自监督骨干的通用特征足以支撑简化解码；SegDINO为高效部署与跨领域分割提供了实用范式，代码已开源。

Abstract: The DINO family of self-supervised vision models has shown remarkable
transferability, yet effectively adapting their representations for
segmentation remains challenging. Existing approaches often rely on heavy
decoders with multi-scale fusion or complex upsampling, which introduce
substantial parameter overhead and computational cost. In this work, we propose
SegDINO, an efficient segmentation framework that couples a frozen DINOv3
backbone with a lightweight decoder. SegDINO extracts multi-level features from
the pretrained encoder, aligns them to a common resolution and channel width,
and utilizes a lightweight MLP head to directly predict segmentation masks.
This design minimizes trainable parameters while preserving the
representational power of foundation features. Extensive experiments across six
benchmarks, including three medical datasets (TN3K, Kvasir-SEG, ISIC) and three
natural image datasets (MSD, VMD-D, ViSha), demonstrate that SegDINO
consistently achieves state-of-the-art performance compared to existing
methods. Code is available at https://github.com/script-Yang/SegDINO.

</details>


### [79] [Satellite Image Utilization for Dehazing with Swin Transformer-Hybrid U-Net and Watershed loss](https://arxiv.org/abs/2509.00835)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出SUFERNOBWA：融合Swin Transformer与U-Net的混合去雾框架，通过SwinRRDB模块与复合损失（L2+guided+新颖的watershed loss）在多种大气条件下实现更强的结构保持与像素级准确，RICE与SateHaze1K上优于SOTA（RICE: PSNR 33.24/SSIM 0.967）。


<details>
  <summary>Details</summary>
Motivation: 卫星图像常受大气散射和雾霾影响，导致清晰度下降与信息提取精度降低；现有方法在兼顾全局上下文与局部结构细节方面存在折衷。

Method: 构建SUFERNOBWA框架：在编码器与解码器中引入SwinRRDB（Swin Transformer版RRDB）以联合学习全局上下文与细粒度空间结构；整体采用U-Net样式的编解码器以保持多尺度特征与跳连；设计复合损失：L2损失、guided loss（结构引导/边缘一致性）与新提出的watershed loss（强调边界/分水岭结构）；以鲁棒去雾为目标。

Result: 在RICE与SateHaze1K数据集上超越SOTA；RICE数据集上PSNR 33.24 dB、SSIM 0.967，显示显著提升与更好的结构一致性。

Conclusion: 混合Swin Transformer+U-Net并结合watershed等结构感知损失，可在多种大气条件下实现鲁棒的卫星图像去雾与结构保持，对遥感应用具有推广价值。

Abstract: Satellite imagery plays a crucial role in various fields; however,
atmospheric interference and haze significantly degrade image clarity and
reduce the accuracy of information extraction. To address these challenges,
this paper proposes a hybrid dehazing framework that integrates Swin
Transformer and U-Net to balance global context learning and local detail
restoration, called SUFERNOBWA. The proposed network employs SwinRRDB, a Swin
Transformer-based Residual-in-Residual Dense Block, in both the encoder and
decoder to effectively extract features. This module enables the joint learning
of global contextual information and fine spatial structures, which is crucial
for structural preservation in satellite image. Furthermore, we introduce a
composite loss function that combines L2 loss, guided loss, and a novel
watershed loss, which enhances structural boundary preservation and ensures
pixel-level accuracy. This architecture enables robust dehazing under diverse
atmospheric conditions while maintaining structural consistency across restored
images. Experimental results demonstrate that the proposed method outperforms
state-of-the-art models on both the RICE and SateHaze1K datasets. Specifically,
on the RICE dataset, the proposed approach achieved a PSNR of 33.24 dB and an
SSIM of 0.967, which is a significant improvement over existing method. This
study provides an effective solution for mitigating atmospheric interference in
satellite imagery and highlights its potential applicability across diverse
remote sensing applications.

</details>


### [80] [Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion](https://arxiv.org/abs/2509.00843)
*Xueyang Kang,Zhengkang Xiang,Zezheng Zhang,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 从单张图生成新视角易因大面积未观测区域而崩坏，尤其长路径/闭环视角。本文将问题分解为：先生成360°全景场景，再做新视角插值，以关键帧约束和空间噪声扩散保持全局一致与正确对齐，显著优于以往方法并支持灵活相机轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有单视图NVS多只保证源视角与生成视角的一致性，但在长距离或闭环相机路径上常出现漂移、错位、语义不连贯，缺乏对全局场景结构的约束。需要一种既能补全大范围未观测区域，又能在长时间序列里保持视图对齐与场景一致性的方法。

Method: 两阶段框架：1）全景外推：用全景扩散模型从单张透视图学习场景先验，生成360°全景表示；2）视角插值：从全景中采样并扭曲得到透视关键帧，作为锚点输入预训练视频扩散模型；引入空间噪声扩散过程，以关键帧条件生成沿用户轨迹的连续新视角，确保多帧时空一致与闭环一致。支持灵活相机控制。

Result: 在多种场景数据集上，相比现有方法，生成的新视角在长轨迹和闭环情况下更加全局一致、视图对齐更好，视觉连贯性更强，且提供更灵活的相机路径控制。

Conclusion: 通过“全景外推+关键帧引导的视角插值”的分解策略以及空间噪声扩散，方法有效解决单视图NVS的长程一致性与闭环问题，提升全局连贯与可控性，并在实验中全面优于现有方法。

Abstract: Novel view synthesis (NVS) from a single image is highly ill-posed due to
large unobserved regions, especially for views that deviate significantly from
the input. While existing methods focus on consistency between the source and
generated views, they often fail to maintain coherence and correct view
alignment across long-range or looped trajectories. We propose a model that
addresses this by decomposing single-view NVS into a 360-degree scene
extrapolation followed by novel view interpolation. This design ensures
long-term view and scene consistency by conditioning on keyframes extracted and
warped from a generated panoramic representation. In the first stage, a
panorama diffusion model learns the scene prior from the input perspective
image. Perspective keyframes are then sampled and warped from the panorama and
used as anchor frames in a pre-trained video diffusion model, which generates
novel views through a proposed spatial noise diffusion process. Compared to
prior work, our method produces globally consistent novel views -- even in loop
closure scenarios -- while enabling flexible camera control. Experiments on
diverse scene datasets demonstrate that our approach outperforms existing
methods in generating coherent views along user-defined trajectories. Our
implementation is available at https://github.com/YiGuYT/LookBeyond.

</details>


### [81] [Quantization Meets OOD: Generalizable Quantization-aware Training from a Flatness Perspective](https://arxiv.org/abs/2509.00859)
*Jiacheng Jiang,Yuan Meng,Chen Tang,Han Yu,Qun Li,Zhi Wang,Wenwu Zhu*

Main category: cs.CV

TL;DR: 提出FQAT：一种面向平坦性的量化感知训练方法，通过分层冻结与“无序度”自适应冻结，缓解梯度冲突与层间干扰，在保持/提升I.D性能的同时显著改善OOD泛化。


<details>
  <summary>Details</summary>
Motivation: 现有QAT重视I.D性能，忽视OOD；实验发现QAT往往造成损失景观变尖锐，违背“平坦性有助于OOD泛化”的经验，从而导致OOD退化。亟需在QAT中显式促进平坦性，实现对OOD更稳健的泛化。

Method: FQAT两大设计：1) 分层冻结（layer-wise freezing），在同时优化QAT目标与平坦性目标时缓解梯度冲突；2) 基于“梯度无序度”的自适应冻结，按训练步动态挑选需冻结的层，降低层间干扰与不稳定。核心是定义梯度无序度度量以定位不稳定层并决定冻结策略。

Result: 在主流OOD基准上进行广泛实验，FQAT在I.D与OOD图像分类上均优于现有SOTA QAT方法，验证了提升平坦性可改善量化模型的OOD泛化。

Conclusion: QAT导致的尖锐损失景观是OOD退化的重要原因；通过分层与自适应冻结促平坦性的FQAT可在不牺牲I.D的情况下提升OOD鲁棒与泛化，提供了实现“可泛化QAT”的有效范式。

Abstract: Current quantization-aware training (QAT) methods primarily focus on
enhancing the performance of quantized models on in-distribution (I.D) data,
while overlooking the potential performance degradation on out-of-distribution
(OOD) data. In this paper, we first substantiate this problem through rigorous
experiment, showing that QAT can lead to a significant OOD generalization
performance degradation. Further, we find the contradiction between the
perspective that flatness of loss landscape gives rise to superior OOD
generalization and the phenomenon that QAT lead to a sharp loss landscape, can
cause the above problem. Therefore, we propose a flatness-oriented QAT method,
FQAT, to achieve generalizable QAT. Specifically, i) FQAT introduces a
layer-wise freezing mechanism to mitigate the gradient conflict issue between
dual optimization objectives (i.e., vanilla QAT and flatness). ii) FQAT
proposes an disorder-guided adaptive freezing algorithm to dynamically
determines which layers to freeze at each training step, effectively addressing
the challenges caused by interference between layers. A gradient disorder
metric is designed to help the algorithm identify unstable layers during
training. Extensive experiments on influential OOD benchmark demonstrate the
superiority of our method over state-of-the-art baselines under both I.D and
OOD image classification tasks.

</details>


### [82] [Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening](https://arxiv.org/abs/2509.00872)
*Zirui Zhou,Zizhao Peng,Dongyang Jin,Chao Fan,Fengwei An,Shiqi Yu*

Main category: cs.CV

TL;DR: 提出Scoliosis1K-Pose数据集与双重表示学习框架DRF，通过连续骨架图+离散姿势不对称向量(PAV)并用PAV引导注意力，实现基于姿态的脊柱侧弯筛查SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有AI筛查多依赖大规模轮廓数据，忽视临床重要的姿势不对称；而姿态数据更具骨骼直观性与可解释性。但基于姿态的方法受限于：缺少大规模标注姿态数据；原始关键点离散且噪声敏感，难以捕捉细微不对称。

Method: 1) 构建Scoliosis1K-Pose：在Scoliosis1K上标注1050名青少年的447,900帧2D关键点。2) 提出Dual Representation Framework（DRF）：将骨架信息编码为连续的skeleton map以保留空间结构，同时定义Postural Asymmetry Vector（PAV）对临床相关不对称（如肩、骨盆、躯干偏斜等）进行离散描述。3) 设计PAV-Guided Attention（PGA）模块：以PAV作为临床先验，引导从skeleton map中进行注意力特征提取，突出临床有意义的不对称区域。

Result: 在广泛实验中，DRF在基于姿态的脊柱侧弯筛查上达到SOTA；可视化显示模型确实关注临床不对称线索，并实现双重表示间的协同。

Conclusion: 通过大规模姿态标注与双重表示+先验引导注意力，既提升了性能又增强了临床可解释性；数据与代码公开，促进该领域后续研究。

Abstract: Recent AI-based scoliosis screening methods primarily rely on large-scale
silhouette datasets, often neglecting clinically relevant postural
asymmetries-key indicators in traditional screening. In contrast, pose data
provide an intuitive skeletal representation, enhancing clinical
interpretability across various medical applications. However, pose-based
scoliosis screening remains underexplored due to two main challenges: (1) the
scarcity of large-scale, annotated pose datasets; and (2) the discrete and
noise-sensitive nature of raw pose coordinates, which hinders the modeling of
subtle asymmetries. To address these limitations, we introduce
Scoliosis1K-Pose, a 2D human pose annotation set that extends the original
Scoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050
adolescents. Building on this dataset, we introduce the Dual Representation
Framework (DRF), which integrates a continuous skeleton map to preserve spatial
structure with a discrete Postural Asymmetry Vector (PAV) that encodes
clinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA)
module further uses the PAV as clinical prior to direct feature extraction from
the skeleton map, focusing on clinically meaningful asymmetries. Extensive
experiments demonstrate that DRF achieves state-of-the-art performance.
Visualizations further confirm that the model leverages clinical asymmetry cues
to guide feature extraction and promote synergy between its dual
representations. The dataset and code are publicly available at
https://zhouzi180.github.io/Scoliosis1K/.

</details>


### [83] [Spotlighter: Revisiting Prompt Tuning from a Representative Mining View](https://arxiv.org/abs/2509.00905)
*Yutong Gao,Maoyuan Shao,Xinyang Huang,Chuang Zhu,Lijuan Sun,Yu Weng,Xuan Liu,Guoshun Nan*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: CLIP's success has demonstrated that prompt tuning can achieve robust
cross-modal semantic alignment for tasks ranging from open-domain recognition
to fine-grained classification. However, redundant or weakly relevant feature
components introduce noise and incur unnecessary computational costs. In this
work, we propose Spotlighter, a lightweight token-selection framework that
simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter
evaluates each visual token's activation from both sample-wise and
semantic-wise perspectives and retains only the top-scoring tokens for
downstream prediction. A class-specific semantic memory bank of learned
prototypes refines this selection, ensuring semantic representativeness and
compensating for discarded features. To further prioritize informative signals,
we introduce a two-level ranking mechanism that dynamically weights
token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter
outperforms CLIP by up to 11.19\% in harmonic mean accuracy and achieves up to
0.8K additional FPS, with only 21 extra parameters. These results establish
Spotlighter as an effective and scalable baseline for prompt tuning. Code for
our method will be available at
https://github.com/greatest-gourmet/Spotlighter.

</details>


### [84] [DarkVRAI: Capture-Condition Conditioning and Burst-Order Selective Scan for Low-light RAW Video Denoising](https://arxiv.org/abs/2509.00917)
*Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出DarkVRAI，一种在AIM 2025低光RAW视频去噪竞赛中夺冠的方法，通过元数据条件引导与长程时序建模（BOSS）协同，实现最先进低光视频去噪。


<details>
  <summary>Details</summary>
Motivation: 低光环境下视频因高增益与短曝光导致信号严重退化，加之视频帧率限制使曝光不可随意加长，传统方法难以同时兼顾对齐与去噪并充分利用时序信息，亟需更有效的时空建模与先验利用。

Method: 1) 条件化方案：将图像去噪中基于捕获元数据（如增益、曝光等）的条件化策略扩展到视频，对对齐和去噪过程提供显式引导；2) BOSS（Burst-Order Selective Scan）：一种选择性扫描机制，能在噪声序列中有效捕获长程时序依赖；两者协同用于RAW视频的对齐与去噪。

Result: 在严格且真实的基准数据集上达到SOTA，并在AIM 2025低光RAW视频去噪挑战中获第一名。

Conclusion: 利用元数据条件化与BOSS长程建模的协同，可显著提升低光RAW视频去噪性能，树立新的基准。

Abstract: Low-light RAW video denoising is a fundamentally challenging task due to
severe signal degradation caused by high sensor gain and short exposure times,
which are inherently limited by video frame rate requirements. To address this,
we propose DarkVRAI, a novel framework that achieved first place in the AIM
2025 Low-light RAW Video Denoising Challenge. Our method introduces two primary
contributions: (1) a successful application of a conditioning scheme for image
denoising, which explicitly leverages capture metadata, to video denoising to
guide the alignment and denoising processes, and (2) a Burst-Order Selective
Scan (BOSS) mechanism that effectively models long-range temporal dependencies
within the noisy video sequence. By synergistically combining these components,
DarkVRAI demonstrates state-of-the-art performance on a rigorous and realistic
benchmark dataset, setting a new standard for low-light video denoising.

</details>


### [85] [Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors](https://arxiv.org/abs/2509.00969)
*Xiangchen Wang,Jinrui Zhang,Teng Wang,Haigang Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 提出LangDC：用轻量语言模型将视频片段转为“软字幕”作为视觉表征，并依据语义密度动态调节token压缩率，在降低49% FLOPs的同时保持与VideoGPT+相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频-语言模型需处理大量视觉token，计算开销大。固定压缩率的token压缩忽视不同片段语义密度差异：信息密集片段被过度压缩、静态/贫内容片段又被浪费计算。需要一种能按内容动态分配视觉token的机制。

Method: 提出LangDC（Language-aware Dynamic Token Compressor）：1）用轻量级语言模型对视频片段生成描述，将视频转化为“软caption token”作为视觉特征。2）引入“语义密度感知监督”，训练LangDC既覆盖下游推理所需关键视觉线索，又通过描述长度映射为压缩比，按场景复杂度自适应分配token。设计理念仿照人类：复杂场景说得多，简单场景说得少。

Result: 与VideoGPT+相比，FLOPs降低49%，整体性能保持竞争力；定性结果显示，模型能随片段丰富度自适应调整压缩率。

Conclusion: 语言感知的动态token压缩可在大幅降低计算的同时维持性能，缓解固定压缩策略的不足，并为视频理解提供更高效的可变计算路径。

Abstract: Recent advancements in large video-language models have revolutionized video
understanding tasks. However, their efficiency is significantly constrained by
processing high volumes of visual tokens. Existing token compression strategies
apply a fixed compression ratio, ignoring the variability in semantic density
among different video clips. Consequently, this lead to inadequate
representation of information-rich clips due to insufficient tokens and
unnecessary computation on static or content-poor ones. To address this, we
propose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages a
lightweight language model to describe video clips, converting them into soft
caption tokens as visual representations. Trained with our proposed semantic
density-aware supervision, LangDC aims to 1) cover key visual cues necessary
for downstream task reasoning and 2) dynamically adjust compression ratios
based on scene richness, reflected by descriptions length. Our design mimics
how humans dynamically express what they see: complex scenes (seeing more)
elicit more detailed language to convey nuances (saying more), whereas simpler
scenes are described with fewer words. Experimental results show that our
method reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitive
performance. Furthermore, qualitative results demonstrate our approach
adaptively adjusts the token compression ratio based on video segment richness.

</details>


### [86] [Towards Integrating Multi-Spectral Imaging with Gaussian Splatting](https://arxiv.org/abs/2509.00989)
*Josef Grün,Lukas Meyer,Maximilian Weiherer,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.CV

TL;DR: 研究如何把RGB与多光谱(红、绿、红边、近红外)数据整合进3D Gaussian Splatting(3DGS)以获得更快更高保真多视角3D重建。提出与对比三种优化策略，并发现联合优化最有效；把多光谱信息直接编码进球谐颜色分量能紧凑建模多光谱反射率，并总结何时/如何引入光谱带的权衡。


<details>
  <summary>Details</summary>
Motivation: 3DGS在RGB数据上表现优秀，但将多光谱逐波段地直接优化会因跨光谱外观不一致导致几何不稳定和重建差。现实中几何相同但光谱外观变化大，如何稳定地融合多模态观测、提升重建质量与效率，尚缺系统方法。

Method: 系统评估三种策略：1) 各波段独立重建(不共享结构)；2) 分裂优化：先用RGB优化几何，再复制并对每个新波段继续优化几何与该波段表示；3) 联合优化：多模态共同优化，可选先进行RGB预热。核心做法是将多光谱数据直接并入3DGS的球谐颜色分量，对每个高斯紧凑表示其多光谱反射特性，并研究不同引入时机与流程的权衡。

Result: 定量指标与新视角渲染显示：专门调校的联合优化策略在总体光谱重建上最佳，并通过光谱跨通道信息提升了RGB质量。与独立/分裂策略相比，重建更稳定、伪影更少。

Conclusion: 在3DGS中应直接把多光谱信息整合进球谐颜色表示以紧凑建模各高斯的多光谱反射；联合优化并适当设计引入时机能显著改进多光谱与RGB重建。文中还总结了何时/如何引入光谱带的关键权衡，为稳健多模态3DGS提供实践指南。

Abstract: We present a study of how to integrate color (RGB) and multi-spectral imagery
(red, green, red-edge, and near-infrared) into the 3D Gaussian Splatting (3DGS)
framework, a state-of-the-art explicit radiance-field-based method for fast and
high-fidelity 3D reconstruction from multi-view images. While 3DGS excels on
RGB data, naive per-band optimization of additional spectra yields poor
reconstructions due to inconsistently appearing geometry in the spectral
domain. This problem is prominent, even though the actual geometry is the same,
regardless of spectral modality. To investigate this, we evaluate three
strategies: 1) Separate per-band reconstruction with no shared structure. 2)
Splitting optimization, in which we first optimize RGB geometry, copy it, and
then fit each new band to the model by optimizing both geometry and band
representation. 3) Joint, in which the modalities are jointly optimized,
optionally with an initial RGB-only phase. We showcase through quantitative
metrics and qualitative novel-view renderings on multi-spectral datasets the
effectiveness of our dedicated optimized Joint strategy, increasing overall
spectral reconstruction as well as enhancing RGB results through spectral
cross-talk. We therefore suggest integrating multi-spectral data directly into
the spherical harmonics color components to compactly model each Gaussian's
multi-spectral reflectance. Moreover, our analysis reveals several key
trade-offs in when and how to introduce spectral bands during optimization,
offering practical insights for robust multi-modal 3DGS reconstruction.

</details>


### [87] [Weather-Dependent Variations in Driver Gaze Behavior: A Case Study in Rainy Conditions](https://arxiv.org/abs/2509.01013)
*Ghazal Farhani,Taufiq Rahman,Dominique Charlebois*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Rainy weather significantly increases the risk of road accidents due to
reduced visibility and vehicle traction. Understanding how experienced drivers
adapt their visual perception through gaze behavior under such conditions is
critical for designing robust driver monitoring systems (DMS) and for informing
advanced driver assistance systems (ADAS). This case study investigates the eye
gaze behavior of a driver operating the same highway route under both clear and
rainy conditions. To this end, gaze behavior was analyzed by a two-step
clustering approach: first, clustering gaze points within 10-second intervals,
and then aggregating cluster centroids into meta-clusters. This, along with
Markov transition matrices and metrics such as fixation duration, gaze
elevation, and azimuth distributions, reveals meaningful behavioral shifts.
While the overall gaze behavior focused on the road with occasional mirror
checks remains consistent, rainy conditions lead to more frequent dashboard
glances, longer fixation durations, and higher gaze elevation, indicating
increased cognitive focus. These findings offer valuable insight into visual
attention patterns under adverse conditions and highlight the potential of
leveraging gaze modeling to aid in the design of more robust ADAS and DMS.

</details>


### [88] [AI-driven Dispensing of Coral Reseeding Devices for Broad-scale Restoration of the Great Barrier Reef](https://arxiv.org/abs/2509.01019)
*Scarlett Raine,Benjamin Moshirian,Tobias Fischer*

Main category: cs.CV

TL;DR: 提出以AI与机器人实现珊瑚补播装置的自动化部署：用计算机视觉进行海底基质分类，选择适宜生长区域；实海试验在大堡礁达77.8%部署准确率、89.1%子图分类准确率、5.5 FPS实时推理，并公开大规模标注数据集。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁因气候变化、海洋酸化和污染快速衰退，未来十年可能失去70–90%的物种。传统人工修复规模有限、成本高、速度慢，需要自动化技术扩大修复规模与效率，减少对人类专家的依赖。

Method: 构建基于人工智能与计算机视觉的海底基质自动分类系统，集成于机器人/自动化补播装置上：通过对海底图像进行分割/分类，识别适宜珊瑚生长的基质区域；在真实海域（大堡礁）进行部署测试；并制作、标注并发布大规模海底基质图像数据集。

Result: 在实地测试中：部署准确率77.8%；子图（patch）分类准确率89.1%；实时推理速度5.5帧/秒；算法能在现场识别适宜基质并指导自动部署。

Conclusion: AI+机器人可显著提升珊瑚修复自动化与效率，减少对专家的依赖；实海试验证明可行性，但仍有性能提升空间。公开数据集将促进后续研究与方法改进，并推动大规模生态修复应用。

Abstract: Coral reefs are on the brink of collapse, with climate change, ocean
acidification, and pollution leading to a projected 70-90% loss of coral
species within the next decade. Restoration efforts are crucial, but their
success hinges on introducing automation to upscale efforts. We present
automated deployment of coral re-seeding devices powered by artificial
intelligence, computer vision, and robotics. Specifically, we perform automated
substrate classification, enabling detection of areas of the seafloor suitable
for coral growth, thus significantly reducing reliance on human experts and
increasing the range and efficiency of restoration. Real-world testing of the
algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,
sub-image patch classification of 89.1%, and real-time model inference at 5.5
frames per second. Further, we present and publicly contribute a large
collection of annotated substrate image data to foster future research in this
area.

</details>


### [89] [CompSlider: Compositional Slider for Disentangled Multiple-Attribute Image Generation](https://arxiv.org/abs/2509.01028)
*Zixin Zhu,Kevin Duarte,Mamshad Nayeem Rizve,Chengyuan Xu,Ratheesh Kalarot,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出CompSlider，在文本生成图像中用滑块精控多属性（如年龄、笑容），通过在条件先验的潜空间进行多属性解耦与合成，实现同时独立调控且无需重训基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有滑块/适配器方法对每个属性单独训练，忽视属性间耦合，导致相互干扰，难以同时精确控制多属性；需要一种能在不重训大模型的前提下，实现多属性独立、可组合控制的方法。

Method: 在T2I基础模型之外引入CompSlider，学习一个条件先验（conditional prior）的潜空间映射：1）为每个属性学习可组合的控制向量/模块；2）设计解耦损失，鼓励不同属性变化彼此独立；3）设计结构一致性损失，保证外观/结构不被破坏；4）通过在先验潜空间编辑来驱动生成，无需改动或重训底座扩散模型；5）扩展到视频时，沿时间维持一致性。

Result: 在多种图像属性上，相比单属性适配器和现有滑块法，能显著减轻属性干扰，提升多属性同时控制的准确性与可控范围；训练与推理更高效；还能推广到视频生成，保持时间一致性。

Conclusion: CompSlider通过在条件先验潜空间进行解耦与结构约束，实现对多属性的可组合、独立控制，减少计算开销并具备从图像到视频的通用性。

Abstract: In text-to-image (T2I) generation, achieving fine-grained control over
attributes - such as age or smile - remains challenging, even with detailed
text prompts. Slider-based methods offer a solution for precise control of
image attributes. Existing approaches typically train individual adapter for
each attribute separately, overlooking the entanglement among multiple
attributes. As a result, interference occurs among different attributes,
preventing precise control of multiple attributes together. To address this
challenge, we aim to disentangle multiple attributes in slider-based generation
to enbale more reliable and independent attribute manipulation. Our approach,
CompSlider, can generate a conditional prior for the T2I foundation model to
control multiple attributes simultaneously. Furthermore, we introduce novel
disentanglement and structure losses to compose multiple attribute changes
while maintaining structural consistency within the image. Since CompSlider
operates in the latent space of the conditional prior and does not require
retraining the foundation model, it reduces the computational burden for both
training and inference. We evaluate our approach on a variety of image
attributes and highlight its generality by extending to video generation.

</details>


### [90] [Seeing through Unclear Glass: Occlusion Removal with One Shot](https://arxiv.org/abs/2509.01033)
*Qiang Li,Yuanming Cao*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Images taken through window glass are often degraded by contaminants adhered
to the glass surfaces. Such contaminants cause occlusions that attenuate the
incoming light and scatter stray light towards the camera. Most of existing
deep learning methods for neutralizing the effects of contaminated glasses
relied on synthetic training data. Few researchers used real degraded and clean
image pairs, but they only considered removing or alleviating the effects of
rain drops on glasses. This paper is concerned with the more challenging task
of learning the restoration of images taken through glasses contaminated by a
wide range of occluders, including muddy water, dirt and other small foreign
particles found in reality. To facilitate the learning task we have gone to a
great length to acquire real paired images with and without glass contaminants.
More importantly, we propose an all-in-one model to neutralize contaminants of
different types by utilizing the one-shot test-time adaptation mechanism. It
involves a self-supervised auxiliary learning task to update the trained model
for the unique occlusion type of each test image. Experimental results show
that the proposed method outperforms the state-of-the-art methods
quantitatively and qualitatively in cleaning realistic contaminated images,
especially the unseen ones.

</details>


### [91] [A Unified Low-level Foundation Model for Enhancing Pathology Image Quality](https://arxiv.org/abs/2509.01071)
*Ziyi Liu,Zhe Xu,Jiabo Ma,Wenqaing Li,Junlin Hou,Fuxiang Huang,Xi Wang,Ronald Cheong Kin Chan,Terence Tsz Wai Wong,Hao Chen*

Main category: cs.CV

TL;DR: 提出首个统一的病理图像低层视觉大模型（LPFM），用对比预训练编码器+条件扩散框架，借助文本提示在单一架构中同时完成去噪、去模糊、超分与虚拟染色等任务；在大规模未标注/标注WSI数据上训练，较SOTA在大多数任务上显著提升PSNR和SSIM。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型多聚焦高层诊断（分类/分割），而真实病理图像普遍存在噪声、模糊、低分辨率和染色变异等低层退化问题；现有方法多为任务定制（如仅去噪或仅超分），通用性差，且实体染色成本高、周期长且不一致，亟需一个可统一处理多类低层增强与图像翻译（虚拟染色）的通用框架。

Method: 1) 对比预训练编码器：基于1.9亿未标注病理图像学习可迁移、染色不变的表征，以识别多样退化模式。2) 统一条件扩散模型：以单一扩散生成/复原框架，使用文本提示进行任务条件控制，动态适配去噪、去模糊、超分及虚拟染色（含H&E与特殊染色）。3) 大规模训练数据：87,810张WSI，涵盖34种组织与5种染色协议；通过任务条件与数据混合实现多任务学习。

Result: 在66个评测设置中，56项优于现有SOTA（p<0.01）；图像复原任务PSNR提升约10–15%，虚拟染色任务SSIM提升约12–18%。表现稳定且跨组织、跨染色泛化更好。

Conclusion: LPFM作为通用低层病理基础模型，能在单一架构下统一处理多种增强与翻译任务，并取得显著性能优势；对真实病理流程有望降低染色成本与延迟、提升图像质量与下游诊断可靠性。

Abstract: Foundation models have revolutionized computational pathology by achieving
remarkable success in high-level diagnostic tasks, yet the critical challenge
of low-level image enhancement remains largely unaddressed. Real-world
pathology images frequently suffer from degradations such as noise, blur, and
low resolution due to slide preparation artifacts, staining variability, and
imaging constraints, while the reliance on physical staining introduces
significant costs, delays, and inconsistency. Although existing methods target
individual problems like denoising or super-resolution, their task-specific
designs lack the versatility to handle the diverse low-level vision challenges
encountered in practice. To bridge this gap, we propose the first unified
Low-level Pathology Foundation Model (LPFM), capable of enhancing image quality
in restoration tasks, including super-resolution, deblurring, and denoising, as
well as facilitating image translation tasks like virtual staining (H&E and
special stains), all through a single adaptable architecture. Our approach
introduces a contrastive pre-trained encoder that learns transferable,
stain-invariant feature representations from 190 million unlabeled pathology
images, enabling robust identification of degradation patterns. A unified
conditional diffusion process dynamically adapts to specific tasks via textual
prompts, ensuring precise control over output quality. Trained on a curated
dataset of 87,810 whole slied images (WSIs) across 34 tissue types and 5
staining protocols, LPFM demonstrates statistically significant improvements
(p<0.01) over state-of-the-art methods in most tasks (56/66), achieving Peak
Signal-to-Noise Ratio (PSNR) gains of 10-15% for image restoration and
Structural Similarity Index Measure (SSIM) improvements of 12-18% for virtual
staining.

</details>


### [92] [SpectMamba: Integrating Frequency and State Space Models for Enhanced Medical Image Detection](https://arxiv.org/abs/2509.01080)
*Yao Wang,Dong Yang,Zhi Qiao,Wenjian Huang,Liuzhi Yang,Zhen Qian*

Main category: cs.CV

TL;DR: 提出SpectMamba：首个将Mamba引入医学图像异常检测的架构，结合空间-频域混合注意和视觉状态空间模块（含Hilbert曲线扫描），在多任务上以更高效的线性复杂度达到SOTA。


<details>
  <summary>Details</summary>
Motivation: CNN感受野有限，难以建模全局上下文；Transformer在高分辨率医学图像上计算和内存成本过高；需一种既高效又能处理长程依赖/全局信息的方法。Mamba在NLP中以线性复杂度处理长序列展现潜力，值得迁移到医学视觉任务。

Method: 1) 设计SpectMamba整体框架，基于Mamba/状态空间思想用于医学检测。2) Hybrid Spatial-Frequency Attention (HSFA)：分离学习高频与低频特征，缓解频率偏置导致的高频信息丢失，并将频域与空间域特征关联以增强全局上下文。3) Visual State-Space Module (VSSM)：适配视觉的状态空间建模以强化长程依赖。4) Hilbert Curve Scanning：用希尔伯特曲线顺序遍历像素/patch，增强局部与空间相关性，优化Mamba的序列化表示与信息传播。

Result: 在多种医学图像异常检测/医学检测任务上，取得最先进性能（SOTA），在准确率与效率间取得更佳平衡；与CNN/Transformer相比，在高分辨率下更高效（线性复杂度）且效果更好。

Conclusion: Mamba可有效迁移到医学视觉；通过HSFA、VSSM与Hilbert扫描，SpectMamba在捕获全局上下文、保留高频信息、处理长程依赖与效率方面优于现有方法，适合高分辨率医学检测场景。

Abstract: Abnormality detection in medical imaging is a critical task requiring both
high efficiency and accuracy to support effective diagnosis. While
convolutional neural networks (CNNs) and Transformer-based models are widely
used, both face intrinsic challenges: CNNs have limited receptive fields,
restricting their ability to capture broad contextual information, and
Transformers encounter prohibitive computational costs when processing
high-resolution medical images. Mamba, a recent innovation in natural language
processing, has gained attention for its ability to process long sequences with
linear complexity, offering a promising alternative. Building on this
foundation, we present SpectMamba, the first Mamba-based architecture designed
for medical image detection. A key component of SpectMamba is the Hybrid
Spatial-Frequency Attention (HSFA) block, which separately learns high- and
low-frequency features. This approach effectively mitigates the loss of
high-frequency information caused by frequency bias and correlates
frequency-domain features with spatial features, thereby enhancing the model's
ability to capture global context. To further improve long-range dependencies,
we propose the Visual State-Space Module (VSSM) and introduce a novel Hilbert
Curve Scanning technique to strengthen spatial correlations and local
dependencies, further optimizing the Mamba framework. Comprehensive experiments
show that SpectMamba achieves state-of-the-art performance while being both
effective and efficient across various medical image detection tasks.

</details>


### [93] [Bidirectional Sparse Attention for Faster Video Diffusion Training](https://arxiv.org/abs/2509.01085)
*Chenlu Zhan,Wen Li,Chuyu Shen,Jun Zhang,Suhui Wu,Hao Zhang*

Main category: cs.CV

TL;DR: 提出一种用于视频扩散Transformer(DiT)的双向稀疏注意力(BSA)，在3D全注意力中同时对Query和KV进行动态稀疏化，显著降低计算量（FLOPs最高降20x，注意力训练加速至17.79x），且生成质量不降反升。


<details>
  <summary>Details</summary>
Motivation: 视频DiT在高分辨率、长序列生成时，因全注意力的二次复杂度导致训练与推理成本过高。现有固定稀疏模式无法适应注意力的动态性，且查询与KV存在天然稀疏与冗余计算，造成效率低下。

Method: 提出BSA框架：1) Query稀疏：依据语义相似性选择最具信息量的query token，并结合动态时空训练策略以自适应不同帧与空间位置；2) KV稀疏：按统计学动态阈值保留最显著的KV块，仅对这些块计算注意力；整体在3D注意力内端到端执行，动态、数据驱动地决定稀疏模式。

Result: 在长序列视频生成任务中，BSA将FLOPs最高减少20倍，注意力训练速度提升至17.79倍，同时保持或超过全注意力的生成质量（未显著损失样本质量）。

Conclusion: 动态双向稀疏化的BSA有效缓解视频DiT的计算瓶颈，在不牺牲甚至提升生成质量的前提下显著加速训练与推理，适用于高分辨率和长时长视频生成场景。

Abstract: Video diffusion Transformer (DiT) models excel in generative quality but hit
major computational bottlenecks when producing high-resolution, long-duration
videos. The quadratic complexity of full attention leads to prohibitively high
training and inference costs. Full attention inefficiency stems from two key
challenges: excessive computation due to the inherent sparsity of Queries and
Key-Value pairs, and redundant computation as fixed sparse patterns fail to
leverage DiT's dynamic attention. To overcome this limitation, we propose a
Bidirectional Sparse Attention (BSA) framework for faster video DiT training,
the first to dynamically sparsify both Queries and Key-Value pairs within 3D
full attention, thereby substantially improving training and inference
efficiency. BSA addresses these issues through two key components. Query
sparsity is optimized by selecting the most informative query tokens via
semantic similarity and with a dynamic spatial-time training strategy, while KV
sparsity is achieved by computing a statistical dynamic threshold to retain
only the most salient KV blocks for computation. Extensive experiments
demonstrate that BSA significantly accelerates DiT training across long
sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention
training, while preserving or even surpassing the generative quality of full
attention.

</details>


### [94] [An End-to-End Framework for Video Multi-Person Pose Estimation](https://arxiv.org/abs/2509.01095)
*Zhihong Wei*

Main category: cs.CV

TL;DR: 提出VEPE端到端视频人体姿态估计框架，用三类时空Transformer模块与实例一致性机制，直接建模全局时空上下文，减少检测与后处理，效率和精度优于多数两阶段方法（Posetrack上推理效率提升约300%）。


<details>
  <summary>Details</summary>
Motivation: 现有视频姿态估计多为两阶段：逐帧检测人+单人时序估计。这种做法割裂空间与时间，难以端到端捕获全局时空上下文；还依赖独立检测器与RoI/NMS等后处理，带来误差传播与推理低效。

Method: 提出VEPE端到端框架，基于Transformer的三大组件：1) STPE时空姿态编码器，聚合空间-时间信息以表示人体关键点相关特征；2) STDME时空可变形记忆编码器，引入跨帧可变形注意与记忆机制，选择性对齐并聚合历史信息；3) STPD时空姿态解码器，从时空查询中解码多人的关键点。另设“实例一致性机制”，对跨帧实例查询进行一致性/区分性约束，实现跟踪并减少跨帧匹配的错配。整体端到端，无需独立检测、RoI裁剪与NMS。

Result: 在Posetrack数据集上，精度超过多数两阶段方法；推理速度显著提升，报告约300%的效率提升。

Conclusion: 端到端的时空Transformer与实例一致性机制能更好利用跨帧信息，提升视频姿态估计的准确性与效率；减少依赖检测与后处理，方法更简洁、可扩展。

Abstract: Video-based human pose estimation models aim to address scenarios that cannot
be effectively solved by static image models such as motion blur, out-of-focus
and occlusion. Most existing approaches consist of two stages: detecting human
instances in each image frame and then using a temporal model for single-person
pose estimation. This approach separates the spatial and temporal dimensions
and cannot capture the global spatio-temporal context between spatial instances
for end-to-end optimization. In addition, it relies on separate detectors and
complex post-processing such as RoI cropping and NMS, which reduces the
inference efficiency of the video scene. To address the above problems, we
propose VEPE (Video End-to-End Pose Estimation), a simple and flexible
framework for end-to-end pose estimation in video. The framework utilizes three
crucial spatio-temporal Transformer components: the Spatio-Temporal Pose
Encoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the
Spatio-Temporal Pose Decoder (STPD). These components are designed to
effectively utilize temporal context for optimizing human body pose estimation.
Furthermore, to reduce the mismatch problem during the cross-frame pose query
matching process, we propose an instance consistency mechanism, which aims to
enhance the consistency and discrepancy of the cross-frame instance query and
realize the instance tracking function, which in turn accurately guides the
pose query to perform cross-frame matching. Extensive experiments on the
Posetrack dataset show that our approach outperforms most two-stage models and
improves inference efficiency by 300%.

</details>


### [95] [PVINet: Point-Voxel Interlaced Network for Point Cloud Compression](https://arxiv.org/abs/2509.01097)
*Xuan Deng,Xingtao Wang,Xiandong Meng,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: In point cloud compression, the quality of a reconstructed point cloud relies
on both the global structure and the local context, with existing methods
usually processing global and local information sequentially and lacking
communication between these two types of information. In this paper, we propose
a point-voxel interlaced network (PVINet), which captures global structural
features and local contextual features in parallel and performs interactions at
each scale to enhance feature perception efficiency. Specifically, PVINet
contains a voxel-based encoder (Ev) for extracting global structural features
and a point-based encoder (Ep) that models local contexts centered at each
voxel. Particularly, a novel conditional sparse convolution is introduced,
which applies point embeddings to dynamically customize kernels for voxel
feature extraction, facilitating feature interactions from Ep to Ev. During
decoding, a voxel-based decoder employs conditional sparse convolutions to
incorporate point embeddings as guidance to reconstruct the point cloud.
Experiments on benchmark datasets show that PVINet delivers competitive
performance compared to state-of-the-art methods.

</details>


### [96] [FICGen: Frequency-Inspired Contextual Disentanglement for Layout-driven Degraded Image Generation](https://arxiv.org/abs/2509.01107)
*Wenzhuang Wang,Yifan Zhao,Mingcan Ma,Ming Liu,Zhonglin Jiang,Yong Chen,Jia Li*

Main category: cs.CV

TL;DR: 提出FICGen：在低照度、夜景、雾霾、水下、轻度模糊等退化场景中，利用频域知识注入扩散模型的潜空间，通过上下文频率原型与频率增强注意力来缓解“上下文错觉”导致的前景淹没，显著提升Layout-to-Image生成的保真度与版式对齐。


<details>
  <summary>Details</summary>
Motivation: 现有L2I在自然场景有效，但在退化场景中前景易被背景主导的频率分布淹没（上下文错觉），导致生成失真和对齐差；需要一种能把退化图像的频率先验带入生成过程、并在潜空间解耦前景与上下文的机制。

Method: 提出FICGen，两步：1) 频率原型提取：设计可学习的双查询机制，分别配备频率重采样器，从训练集中预收集的退化样本中抽取上下文频率原型；2) 频率注入与解耦：以视觉-频率增强注意力将频率原型注入L2I的扩散生成；引入实例一致性图约束潜空间中实例与背景的解耦，结合自适应时空-频率聚合模块，重构空间-频率混合的退化表示。

Result: 在5个基准、覆盖从严重低光到轻度模糊等多种退化场景上，FICGen在生成保真度、布局对齐度以及下游可训练性（如用于辅助训练的质量）上均显著优于现有L2I方法。

Conclusion: 频域先验与潜扩散的结合可有效缓解退化场景中的上下文错觉；通过频率原型、频率增强注意力与实例解耦约束，可稳定提升退化条件下的L2I质量与对齐，并具备更好的下游训练友好性。

Abstract: Layout-to-image (L2I) generation has exhibited promising results in natural
domains, but suffers from limited generative fidelity and weak alignment with
user-provided layouts when applied to degraded scenes (i.e., low-light,
underwater). We primarily attribute these limitations to the "contextual
illusion dilemma" in degraded conditions, where foreground instances are
overwhelmed by context-dominant frequency distributions. Motivated by this, our
paper proposes a new Frequency-Inspired Contextual Disentanglement Generative
(FICGen) paradigm, which seeks to transfer frequency knowledge of degraded
images into the latent diffusion space, thereby facilitating the rendering of
degraded instances and their surroundings via contextual frequency-aware
guidance. To be specific, FICGen consists of two major steps. Firstly, we
introduce a learnable dual-query mechanism, each paired with a dedicated
frequency resampler, to extract contextual frequency prototypes from
pre-collected degraded exemplars in the training set. Secondly, a
visual-frequency enhanced attention is employed to inject frequency prototypes
into the degraded generation process. To alleviate the contextual illusion and
attribute leakage, an instance coherence map is developed to regulate
latent-space disentanglement between individual instances and their
surroundings, coupled with an adaptive spatial-frequency aggregation module to
reconstruct spatial-frequency mixed degraded representations. Extensive
experiments on 5 benchmarks involving a variety of degraded scenarios-from
severe low-light to mild blur-demonstrate that FICGen consistently surpasses
existing L2I methods in terms of generative fidelity, alignment and downstream
auxiliary trainability.

</details>


### [97] [GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation](https://arxiv.org/abs/2509.01109)
*Zhengqiang Zhang,Rongyuan Wu,Lingchen Sun,Lei Zhang*

Main category: cs.CV

TL;DR: GPSToken 用可参数化二维高斯实现非均匀、自适应图像分词，将空间布局与纹理特征解耦，并通过可微渲染与标准解码器对接，在128个token下重建/生成取得SOTA（rFID 0.65，FID 1.50）。


<details>
  <summary>Details</summary>
Motivation: 传统基于均匀2D/1D网格的图像token化难以适应不同位置、形状和纹理复杂度，导致表示效率低、生成质量受限，需要一种能自适应空间与内容变化的非均匀token化方案。

Method: 1) 以熵驱动算法将图像划分为纹理同质的可变大小区域；2) 将每个区域参数化为二维高斯（均值表位置、协方差表形状）并配合纹理特征；3) 训练专门的Transformer优化高斯参数与特征，实现连续位置/形状自适应与内容感知表征；4) 解码阶段用可微splatting渲染将高斯参数化token重建为2D特征图，便于与标准解码器端到端训练；5) 通过将空间布局（高斯参数）与纹理特征解耦，支持两阶段生成（先结构后纹理）。

Result: 在图像重建与生成任务中，仅用128个token达到SOTA：rFID 0.65、FID 1.50。

Conclusion: GPSToken以高斯参数化实现非均匀、自适应的图像token化，有效提升表示效率与生成质量，并通过解耦空间布局与纹理实现高效两阶段生成；方法可与现有解码器无缝结合，具有实践落地潜力。

Abstract: Effective and efficient tokenization plays an important role in image
representation and generation. Conventional methods, constrained by uniform
2D/1D grid tokenization, are inflexible to represent regions with varying
shapes and textures and at different locations, limiting their efficacy of
feature representation. In this work, we propose $\textbf{GPSToken}$, a novel
$\textbf{G}$aussian $\textbf{P}$arameterized $\textbf{S}$patially-adaptive
$\textbf{Token}$ization framework, to achieve non-uniform image tokenization by
leveraging parametric 2D Gaussians to dynamically model the shape, position,
and textures of different image regions. We first employ an entropy-driven
algorithm to partition the image into texture-homogeneous regions of variable
sizes. Then, we parameterize each region as a 2D Gaussian (mean for position,
covariance for shape) coupled with texture features. A specialized transformer
is trained to optimize the Gaussian parameters, enabling continuous adaptation
of position/shape and content-aware feature extraction. During decoding,
Gaussian parameterized tokens are reconstructed into 2D feature maps through a
differentiable splatting-based renderer, bridging our adaptive tokenization
with standard decoders for end-to-end training. GPSToken disentangles spatial
layout (Gaussian parameters) from texture features to enable efficient
two-stage generation: structural layout synthesis using lightweight networks,
followed by structure-conditioned texture generation. Experiments demonstrate
the state-of-the-art performance of GPSToken, which achieves rFID and FID
scores of 0.65 and 1.50 on image reconstruction and generation tasks using 128
tokens, respectively. Codes and models of GPSToken can be found at
$\href{https://github.com/xtudbxk/GPSToken}{https://github.com/xtudbxk/GPSToken}$.

</details>


### [98] [MetaSSL: A General Heterogeneous Loss for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2509.01144)
*Weiren Zhao,Lanfeng Zhong,Xin Liao,Wenjun Liao,Sichuan Zhang,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 提出MetaSSL：一种可插拔的半监督医学图像分割通用框架，用空间异质加权损失联合利用不确定性与一致性信息，显著提升多数据集与多SSL基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SSL（Mean Teacher、FixMatch、CPS等）多依赖一致性或伪标签，对“参考预测”的生成策略关注多，但忽略：1) 有标签数据中潜在噪声；2) 不同无标签像素贡献的异质性价值。需要一种更关注损失层面信息挖掘、鲁棒于标注噪声且能泛化到多框架的方法。

Method: 提出MetaSSL的空间异质加权损失：对无标签样本的两路预测（参考预测与监督预测）同时度量一致性与不确定性，基于自适应阈值将像素划分为四类并赋予递减权重：UC（一致且自信）、US（一致但可疑）、DC（不一致且自信）、DS（不一致且可疑）。该加权损失同样用于有标签数据，以缓解标注噪声影响。框架为可插拔，可集成为多数现有SSL方法的损失模块。

Result: 在多数据集与多SSL基线（如Mean Teacher、FixMatch、CPS）上集成后，分割性能显著提升，证明方法的通用性与有效性；具体指标未在摘要中给出。

Conclusion: 相较于优化参考预测生成策略，围绕损失函数挖掘两路预测的互补信息更关键。通过不确定性与一致性的联合建模并对像素赋予异质权重，MetaSSL在半监督分割中提升性能且对噪声更鲁棒，具备通用可插拔性。

Abstract: Semi-Supervised Learning (SSL) is important for reducing the annotation cost
for medical image segmentation models. State-of-the-art SSL methods such as
Mean Teacher, FixMatch and Cross Pseudo Supervision (CPS) are mainly based on
consistency regularization or pseudo-label supervision between a reference
prediction and a supervised prediction. Despite the effectiveness, they have
overlooked the potential noise in the labeled data, and mainly focus on
strategies to generate the reference prediction, while ignoring the
heterogeneous values of different unlabeled pixels. We argue that effectively
mining the rich information contained by the two predictions in the loss
function, instead of the specific strategy to obtain a reference prediction, is
more essential for SSL, and propose a universal framework MetaSSL based on a
spatially heterogeneous loss that assigns different weights to pixels by
simultaneously leveraging the uncertainty and consistency information between
the reference and supervised predictions. Specifically, we split the
predictions on unlabeled data into four regions with decreasing weights in the
loss: Unanimous and Confident (UC), Unanimous and Suspicious (US), Discrepant
and Confident (DC), and Discrepant and Suspicious (DS), where an adaptive
threshold is proposed to distinguish confident predictions from suspicious
ones. The heterogeneous loss is also applied to labeled images for robust
learning considering the potential annotation noise. Our method is
plug-and-play and general to most existing SSL methods. The experimental
results showed that it improved the segmentation performance significantly when
integrated with existing SSL frameworks on different datasets. Code is
available at https://github.com/HiLab-git/MetaSSL.

</details>


### [99] [MVTrajecter: Multi-View Pedestrian Tracking with Trajectory Motion Cost and Trajectory Appearance Cost](https://arxiv.org/abs/2509.01157)
*Taiga Yamane,Ryo Masumura,Satoshi Suzuki,Shota Orihashi*

Main category: cs.CV

TL;DR: 提出MVTrajecter，一种端到端多视角行人跟踪方法，利用多历史时间步的轨迹信息，通过运动与外观的轨迹级代价和注意力机制，提升当前-历史关联的鲁棒性，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有端到端MVPT通常只利用当前帧及相邻上一帧的有限历史，忽略更长时域的轨迹，导致在遮挡、误检或短时关联错误时难以恢复正确关联。需要一种能整合多时间步运动与外观信息、进行鲁棒跨时关联的方法。

Method: 提出Multi-View Trajectory Tracker (MVTrajecter)：1) 在BEV上进行端到端检测与关联；2) 设计两个轨迹级代价：轨迹运动代价与轨迹外观代价，分别衡量当前检测与各历史时间步候选之间在运动与外观上的匹配可能性；3) 对多时间步信息进行聚合，若某些时间步出现错误关联，其他时间步的信息可纠偏；4) 采用注意力机制建模多时间步之间的关系与权重分配，实现信息的有效融合；5) 通过联合训练实现检测与关联的端到端优化。

Result: 消融实验验证两类代价项与注意力聚合的有效性；整体性能在公开数据集上超过先前SOTA（具体指标未在摘要给出）。

Conclusion: 多时间步轨迹信息与注意力融合显著提升多视角行人跟踪的关联鲁棒性与总体性能，优于仅用单一相邻历史帧的方法。

Abstract: Multi-View Pedestrian Tracking (MVPT) aims to track pedestrians in the form
of a bird's eye view occupancy map from multi-view videos. End-to-end methods
that detect and associate pedestrians within one model have shown great
progress in MVPT. The motion and appearance information of pedestrians is
important for the association, but previous end-to-end MVPT methods rely only
on the current and its single adjacent past timestamp, discarding the past
trajectories before that. This paper proposes a novel end-to-end MVPT method
called Multi-View Trajectory Tracker (MVTrajecter) that utilizes information
from multiple timestamps in past trajectories for robust association.
MVTrajecter introduces trajectory motion cost and trajectory appearance cost to
effectively incorporate motion and appearance information, respectively. These
costs calculate which pedestrians at the current and each past timestamp are
likely identical based on the information between those timestamps. Even if a
current pedestrian could be associated with a false pedestrian at some past
timestamp, these costs enable the model to associate that current pedestrian
with the correct past trajectory based on other past timestamps. In addition,
MVTrajecter effectively captures the relationships between multiple timestamps
leveraging the attention mechanism. Extensive experiments demonstrate the
effectiveness of each component in MVTrajecter and show that it outperforms the
previous state-of-the-art methods.

</details>


### [100] [Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models](https://arxiv.org/abs/2509.01167)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Recent advances in multimodal large language models (MLLMs) have led to much
progress in video understanding tasks. To avoid the heavy computational cost of
processing all frames, these models typically rely on keyframe sampling methods
guided by vision-language encoders (\textit{e.g.,} SigLIP). However, it remains
unclear whether such encoders can truly identify the most informative frames.
In this work, we provide several empirical pieces of evidence revealing that
popular vision encoders critically suffer from their limited capability to
identify where the MLLM should look inside the video to handle the given
textual query appropriately. Our findings suggest that the development of
better keyframe identification techniques may be necessary for efficient video
MLLMs.

</details>


### [101] [DynaMind: Reconstructing Dynamic Visual Scenes from EEG by Aligning Temporal Dynamics and Multimodal Semantics to Guided Diffusion](https://arxiv.org/abs/2509.01177)
*Junxiang Liu,Junming Lin,Jiangtong Li,Jie Li*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Reconstruction dynamic visual scenes from electroencephalography (EEG)
signals remains a primary challenge in brain decoding, limited by the low
spatial resolution of EEG, a temporal mismatch between neural recordings and
video dynamics, and the insufficient use of semantic information within brain
activity. Therefore, existing methods often inadequately resolve both the
dynamic coherence and the complex semantic context of the perceived visual
stimuli. To overcome these limitations, we introduce DynaMind, a novel
framework that reconstructs video by jointly modeling neural dynamics and
semantic features via three core modules: a Regional-aware Semantic Mapper
(RSM), a Temporal-aware Dynamic Aligner (TDA), and a Dual-Guidance Video
Reconstructor (DGVR). The RSM first utilizes a regional-aware encoder to
extract multimodal semantic features from EEG signals across distinct brain
regions, aggregating them into a unified diffusion prior. In the mean time, the
TDA generates a dynamic latent sequence, or blueprint, to enforce temporal
consistency between the feature representations and the original neural
recordings. Together, guided by the semantic diffusion prior, the DGVR
translates the temporal-aware blueprint into a high-fidelity video
reconstruction. On the SEED-DV dataset, DynaMind sets a new state-of-the-art
(SOTA), boosting reconstructed video accuracies (video- and frame-based) by
12.5 and 10.3 percentage points, respectively. It also achieves a leap in
pixel-level quality, showing exceptional visual fidelity and temporal coherence
with a 9.4% SSIM improvement and a 19.7% FVMD reduction. This marks a critical
advancement, bridging the gap between neural dynamics and high-fidelity visual
semantics.

</details>


### [102] [FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus](https://arxiv.org/abs/2509.01181)
*Qiaoqiao Jin,Siming Fu,Dong She,Weinan Jia,Hualiang Wang,Mu Liu,Jidong Jiang*

Main category: cs.CV

TL;DR: 提出 FocusDPO：在不需测试时优化的前提下，实现多主体个性化图像生成的更精细独立控制，兼顾主体保真与避免属性串扰；通过动态语义对应与焦点区域加权，在DPO训练中自适应分配关注，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多主体个性化生成难以同时保证：1) 每个主体的高保真还原；2) 主体间属性不泄漏（例如发色、服饰互串）；3) 无需测试时逐主体优化。需要一种训练期即可实现细粒度、独立控制的框架。

Method: 提出 FocusDPO：
- 动态语义对应：在生成图与参考图之间建立鲁棒的语义/主体部位对应关系。
- 自适应焦点区域：依据参考图语义复杂度与噪声时间步，自动识别并逐步调整“关注区域”。
- 加权策略：对信息量高的patch给予奖励、对低置信预测区域施加惩罚；在DPO过程中根据语义复杂度动态分配关注预算。
- 训练范式：以现有预训练个性化生成模型为底座，通过焦点化的DPO优化，提升多主体独立可控性与保真度。

Result: 在单主体与多主体个性化合成基准上达到SOTA；显著降低跨主体属性泄漏，同时提升主体保真度与可控性；对多种预训练个性化模型有普适增益。

Conclusion: FocusDPO通过“动态语义对应+自适应焦点分配+加权DPO”有效缓解多主体生成中的保真-串扰矛盾，实现高质量、可控的个性化合成，推进了可控多主体图像生成的最新进展。

Abstract: Multi-subject personalized image generation aims to synthesize customized
images containing multiple specified subjects without requiring test-time
optimization. However, achieving fine-grained independent control over multiple
subjects remains challenging due to difficulties in preserving subject fidelity
and preventing cross-subject attribute leakage. We present FocusDPO, a
framework that adaptively identifies focus regions based on dynamic semantic
correspondence and supervision image complexity. During training, our method
progressively adjusts these focal areas across noise timesteps, implementing a
weighted strategy that rewards information-rich patches while penalizing
regions with low prediction confidence. The framework dynamically adjusts focus
allocation during the DPO process according to the semantic complexity of
reference images and establishes robust correspondence mappings between
generated and reference subjects. Extensive experiments demonstrate that our
method substantially enhances the performance of existing pre-trained
personalized generation models, achieving state-of-the-art results on both
single-subject and multi-subject personalized image synthesis benchmarks. Our
method effectively mitigates attribute leakage while preserving superior
subject fidelity across diverse generation scenarios, advancing the frontier of
controllable multi-subject image synthesis.

</details>


### [103] [SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment](https://arxiv.org/abs/2509.01183)
*Bingnan Yang,Mi Zhang,Zhili Zhang,Zhan Zhang,Yuanxin Zhao,Xiangyun Hu,Jianya Gong*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: High-quality image segmentation is fundamental to pixel-level geospatial
analysis in remote sensing, necessitating robust segmentation quality
assessment (SQA), particularly in unsupervised settings lacking ground truth.
Although recent deep learning (DL) based unsupervised SQA methods show
potential, they often suffer from coarse evaluation granularity, incomplete
assessments, and poor transferability. To overcome these limitations, this
paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for
comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning
framework realizing this approach. SegAssess distinctively formulates SQA as a
fine-grained, four-class panoramic segmentation task, classifying pixels within
a segmentation mask under evaluation into true positive (TP), false positive
(FP), true negative (TN), and false negative (FN) categories, thereby
generating a complete quality map. Leveraging an enhanced Segment Anything
Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt
for effective feature integration via cross-attention. Key innovations include
an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF)
module to refine predictions near challenging object edges, and an Augmented
Mixup Sampling (AMS) training strategy integrating multi-source masks to
significantly boost cross-domain robustness and zero-shot transferability.
Comprehensive experiments across 32 datasets derived from 6 sources demonstrate
that SegAssess achieves state-of-the-art (SOTA) performance and exhibits
remarkable zero-shot transferability to unseen masks, establishing PQM via
SegAssess as a robust and transferable solution for unsupervised SQA. The code
is available at https://github.com/Yangbn97/SegAssess.

</details>


### [104] [PrediTree: A Multi-Temporal Sub-meter Dataset of Multi-Spectral Imagery Aligned With Canopy Height Maps](https://arxiv.org/abs/2509.01202)
*Hiyam Debary,Mustansar Fiaz,Levente Klein*

Main category: cs.CV

TL;DR: PrediTree 发布首个用于亚米级树高预测的开源大规模数据集与基线模型；多时相多光谱影像+高分辨率LiDAR树冠高图，支持学习树高时序生长预测；U-Net在该数据上性能最佳，显著优于ResNet-50与仅RGB配置；数据与代码开放。


<details>
  <summary>Details</summary>
Motivation: 森林监测需要精细尺度（<1 m）且可随时间预测树高/生长的模型，但缺乏既有多时相多光谱影像又有高精度树高标注的大规模公开数据集，限制了深度学习方法在树木生长预测上的研究与应用。

Method: 构建 PrediTree 数据集：法国多样森林生态系统覆盖，0.5 m LiDAR 树冠高图（CHM）与空间配准的多时相、多光谱影像，合计约314万张切片。提出编码器-解码器框架（如 U-Net），输入为多时相多光谱影像及每个时相相对目标CHM时间差（年），输出预测目标时刻的树冠高度图；对比不同网络（U-Net、ResNet-50等）与不同光谱带配置（全波段 vs 仅RGB）。

Result: 在 PrediTree 上，U-Net 取得最高（最佳）掩膜化MSE为11.78%，优于次优ResNet-50约12%，相较仅用RGB波段的实验将误差降低约30%。

Conclusion: PrediTree 填补了亚米级树高时序预测公开数据空白，验证了多时相多光谱信息与合适的编解码网络（U-Net）的有效性；数据集与处理/训练代码已在 HuggingFace 与 GitHub 开源，为森林监测与树木生长建模提供强有力基准与资源。

Abstract: We present PrediTree, the first comprehensive open-source dataset designed
for training and evaluating tree height prediction models at sub-meter
resolution. This dataset combines very high-resolution (0.5m) LiDAR-derived
canopy height maps, spatially aligned with multi-temporal and multi-spectral
imagery, across diverse forest ecosystems in France, totaling 3,141,568 images.
PrediTree addresses a critical gap in forest monitoring capabilities by
enabling the training of deep learning methods that can predict tree growth
based on multiple past observations. %\sout{Initially focused on French
forests, PrediTree is designed as an expanding resource with ongoing efforts to
incorporate data from other countries. } To make use of this PrediTree dataset,
we propose an encoder-decoder framework that requires the multi-temporal
multi-spectral imagery and the relative time differences in years between the
canopy height map timestamp (target) and each image acquisition date for which
this framework predicts the canopy height. The conducted experiments
demonstrate that a U-Net architecture trained on the PrediTree dataset provides
the highest masked mean squared error of $11.78\%$, outperforming the next-best
architecture, ResNet-50, by around $12\%$, and cutting the error of the same
experiments but on fewer bands (red, green, blue only), by around $30\%$. This
dataset is publicly available on \href{URL}{HuggingFace}, and both processing
and training codebases are available on \href{URL}{GitHub}.

</details>


### [105] [DcMatch: Unsupervised Multi-Shape Matching with Dual-Level Consistency](https://arxiv.org/abs/2509.01204)
*Tianwei Ye,Yong Ma,Xiaoguang Mei*

Main category: cs.CV

TL;DR: 提出DcMatch：一种无监督的非刚性多形状匹配框架，利用形状图注意力学习全数据集的共享潜在空间，并在空间/谱域对齐，通过新循环一致性损失提升一致性与精度，全面优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 多形状（非刚性）点到点对应是基础任务，但现有方法多从单个形状学习“标准”嵌入，难以捕获整套数据的流形结构，导致跨形状一致性差、鲁棒性不足。需要能从整集合建模并获得更稳定共享潜在空间的方法。

Method: 1) 构建形状图（节点为形状，边表相互关系），用形状图注意力网络建模集合级流形结构，得到更表达力的共享潜在空间；2) 通过“universe predictor”学习形状到共享“宇宙”基的对应（shape-to-universe）；3) 同时在空间域和谱域表示对应关系，并在共享宇宙空间中对两者施加对齐；4) 设计新颖的循环一致性损失，在多形状间与双域间同时约束，促进一致、可逆、鲁棒的匹配；5) 无监督训练，适配多种多形状匹配场景。

Result: 在多个具有挑战性的基准上进行大量实验，DcMatch在多种多形状匹配设置下均显著且稳定地超越现有SOTA方法。

Conclusion: 集合级流形建模+空间/谱双域对齐+循环一致性约束，可构建更鲁棒的共享潜在空间与一致的shape-to-universe对应，从而提升多形状匹配的准确性与一致性；方法通用、无监督、效果领先，并已开源。

Abstract: Establishing point-to-point correspondences across multiple 3D shapes is a
fundamental problem in computer vision and graphics. In this paper, we
introduce DcMatch, a novel unsupervised learning framework for non-rigid
multi-shape matching. Unlike existing methods that learn a canonical embedding
from a single shape, our approach leverages a shape graph attention network to
capture the underlying manifold structure of the entire shape collection. This
enables the construction of a more expressive and robust shared latent space,
leading to more consistent shape-to-universe correspondences via a universe
predictor. Simultaneously, we represent these correspondences in both the
spatial and spectral domains and enforce their alignment in the shared universe
space through a novel cycle consistency loss. This dual-level consistency
fosters more accurate and coherent mappings. Extensive experiments on several
challenging benchmarks demonstrate that our method consistently outperforms
previous state-of-the-art approaches across diverse multi-shape matching
scenarios. Code is available at https://github.com/YeTianwei/DcMatch.

</details>


### [106] [Generalizable Self-supervised Monocular Depth Estimation with Mixture of Low-Rank Experts for Diverse Endoscopic Scenes](https://arxiv.org/abs/2509.01206)
*Liangjing Shao,Benshuang Chen,Chenkang Du,Xueli Liu,Xinrong Chen*

Main category: cs.CV

TL;DR: 提出一种自监督单目深度估计框架，面向多种内镜场景，核心是块级动态低秩专家混合与亮度/反射不一致联合建模，在真实与仿真数据上优于SOTA并具备零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 内镜环境中光照变化大、组织材质多样、反射强，导致基于自监督的单目深度在跨场景泛化差；需要一种参数高效、可适配多域特征并能处理亮度与反射不一致性的方案，以提升在不同内镜器官/设备/条件下的鲁棒性。

Method: 1) 架构：在基础模型上引入“块级动态低秩专家混合”（block-wise mixture of dynamic low-rank experts, MoE-LR）。在每个网络块中配置若干低秩适配专家（参数量小），训练时依据输入特征由门控自适应选择并加权这些专家；专家分配与权重受各块训练质量指标引导，以实现参数高效的特征专化与共享。2) 训练：提出自监督学习框架，联合解决亮度与反射不一致问题（例如通过改进的光度一致性、反射鲁棒性重建损失、亮度归一/分解策略等），从未配准/无GT深度的视频序列中学习。3) 目标：面向多种内镜（真实与仿真）场景的单目深度估计与跨域泛化；支持零样本在新器官/设备上的推理。

Result: 在多个真实与仿真内镜数据集上超越现有SOTA；在零样本跨场景评测中达到最佳泛化性能；在参数效率方面通过低秩专家实现小幅调参即可适配多场景。

Conclusion: 块级动态低秩专家与反射/亮度一致性建模可显著提升自监督单目内镜深度估计的准确性与泛化能力，适用于微创测量与手术感知；代码将开源，演示视频已上线。

Abstract: Self-supervised monocular depth estimation is a significant task for low-cost
and efficient three-dimensional scene perception in endoscopy. The variety of
illumination conditions and scene features is still the primary challenge for
generalizable depth estimation in endoscopic scenes. In this work, a
self-supervised framework is proposed for monocular depth estimation in various
endoscopy. Firstly, due to various features in endoscopic scenes with different
tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to
efficiently finetuning the foundation model for endoscopic depth estimation. In
the proposed module, based on the input feature, different experts with a small
amount of trainable parameters are adaptively selected for weighted inference,
from various mixture of low-rank experts which are allocated based on the
training quality of each block. Moreover, a novel self-supervised training
framework is proposed to jointly cope with the inconsistency of brightness and
reflectance. The proposed method outperform state-of-the-art works on both
realistic and simulated endoscopic datasets. Furthermore, the proposed network
also achieves the best generalization based on zero-shot depth estimation on
diverse endoscopic scenes. The proposed method could contribute to accurate
endoscopic perception for minimally invasive measurement and surgery. The code
will be released upon acceptance, while the demo video can be found on here:
https://endo-gede.netlify.app/.

</details>


### [107] [Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation](https://arxiv.org/abs/2509.01209)
*Maëlic Neau,Zoe Falomir,Cédric Buche,Akihiro Sugimoto*

Main category: cs.CV

TL;DR: 提出一种无需参考标注的评价指标与一种基于区域特定提示调优的合成数据生成方法，以提升开放词汇场景图生成（SGG）的关系预测能力与泛化。


<details>
  <summary>Details</summary>
Motivation: 开放词汇SGG希望模型学习并预测更广泛的关系，但现有基准词汇量小、对开源模型评测不充分；同时预训练常依赖质量较差的弱监督数据，限制了模型泛化与真实开放词汇能力的衡量与提升。

Method: 1) 评价：设计一种“无参考（reference-free）”的度量，用于公平评估VLM在开放词汇关系预测上的能力，避免受限于小词表与稀缺标注。2) 数据：提出基于VLM的“区域级提示调优（region-specific prompt tuning）”，快速生成高质量、覆盖更广关系的合成训练数据，作为新的预训练数据拆分。

Result: 实验表明：用该合成数据进行预训练能显著提升开放词汇SGG模型在关系预测上的泛化表现；所提无参考指标可有效衡量模型开放词汇能力。

Conclusion: 通过新的无参考评测和高质量合成数据生成流程，提升了开放词汇SGG的评测公平性与模型泛化，缓解了词表受限与弱监督数据质量差的问题。

Abstract: Scene Graph Generation (SGG) encodes visual relationships between objects in
images as graph structures. Thanks to the advances of Vision-Language Models
(VLMs), the task of Open-Vocabulary SGG has been recently proposed where models
are evaluated on their functionality to learn a wide and diverse range of
relations. Current benchmarks in SGG, however, possess a very limited
vocabulary, making the evaluation of open-source models inefficient. In this
paper, we propose a new reference-free metric to fairly evaluate the
open-vocabulary capabilities of VLMs for relation prediction. Another
limitation of Open-Vocabulary SGG is the reliance on weakly supervised data of
poor quality for pre-training. We also propose a new solution for quickly
generating high-quality synthetic data through region-specific prompt tuning of
VLMs. Experimental results show that pre-training with this new data split can
benefit the generalization capabilities of Open-Voc SGG models.

</details>


### [108] [PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity](https://arxiv.org/abs/2509.01214)
*Yizhe Yuan,Bingsen Xue,Bangzheng Pu,Chengxiang Wang,Cheng Jin*

Main category: cs.CV

TL;DR: 提出PRINTER：弱监督、原型驱动的内容-染色模式解耦与形变感知对抗学习框架，实现从H&E到IHC的高保真虚拟染色，同时保持H&E细节并解决跨切片配准误差。


<details>
  <summary>Details</summary>
Motivation: 肿瘤空间异质性研究需要H&E形态与IHC生物标志物在空间上精准对应，但现有依赖连续切片的方法存在显著错位，影响原位病理解释；现有虚拟染色方法常在内容与风格耦合、跨模态形变配准不足、以及对抗训练忽视形变因素，导致细节丢失与风格不准。

Method: 提出PRINTER框架，核心包括：1) 原型驱动的染色模式迁移：通过显式内容-风格（染色模式）解耦，以原型表示指导IHC风格学习，避免内容被风格污染；2) GapBridge循环配准-合成：在H&E与IHC域间进行可形变结构对齐，已注册特征指导跨模态风格转移，且合成结果反向迭代优化配准，实现互促闭环；3) 形变感知对抗学习：生成器与形变感知配准网络联合对抗、对风格敏感的判别器，强化在形变条件下的风格真实性与细节保持；并采用弱监督训练以减少对严格成对标注的依赖。

Result: 在多组数据的广泛实验中，PRINTER在保留H&E细节与提升虚拟IHC染色保真度方面明显优于SOTA；在跨域对齐与抗形变鲁棒性上表现更佳（摘要未给出具体数值，但宣称全面领先）。

Conclusion: PRINTER为计算病理中的虚拟染色提供了鲁棒、可扩展的解决方案，能更准确地映射H&E到IHC模式，缓解连续切片错位带来的瓶颈，推动空间异质性分析与临床路径学解释。

Abstract: Tumor spatial heterogeneity analysis requires precise correlation between
Hematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker
expression, yet current methods suffer from spatial misalignment in consecutive
sections, severely compromising in situ pathological interpretation. In order
to obtain a more accurate virtual staining pattern, We propose PRINTER, a
weakly-supervised framework that integrates PRototype-drIven content and
staiNing patTERn decoupling and deformation-aware adversarial learning
strategies designed to accurately learn IHC staining patterns while preserving
H&E staining details. Our approach introduces three key innovations: (1) A
prototype-driven staining pattern transfer with explicit content-style
decoupling; and (2) A cyclic registration-synthesis framework GapBridge that
bridges H&E and IHC domains through deformable structural alignment, where
registered features guide cross-modal style transfer while synthesized outputs
iteratively refine the registration;(3) Deformation-Aware Adversarial Learning:
We propose a training framework where a generator and deformation-aware
registration network jointly adversarially optimize a style-focused
discriminator. Extensive experiments demonstrate that PRINTER effectively
achieves superior performance in preserving H&E staining details and virtual
staining fidelity, outperforming state-of-the-art methods. Our work provides a
robust and scalable solution for virtual staining, advancing the field of
computational pathology.

</details>


### [109] [POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion](https://arxiv.org/abs/2509.01215)
*Yuan Liu,Zhongyin Zhao,Le Tian,Haicheng Wang,Xubing Ye,Yangxiu You,Zilin Yu,Chuhan Wu,Xiao Zhou,Yang Yu,Jie Zhou*

Main category: cs.CV

TL;DR: 提出一个无蒸馏、全自动两阶段框架：先用大规模多样化合成数据训练统一抽取器，再用自标注+过滤+再训练的自改进循环适配真实文档，最终得到在复杂文档（表格、公式、多栏）上表现强的POINTS-Reader。


<details>
  <summary>Details</summary>
Motivation: 复杂版面文档的标注昂贵且慢，自动标注精度不足，导致基于教师-学生蒸馏的训练受限，难以在真实场景下取得好效果。需要一种能够在缺乏高质量人工标注情况下，仍能构建高质量数据与模型的方法。

Method: 两阶段：1) 合成数据阶段：生成大规模、多样化的合成文档与标注，以统一格式抽取关键元素，训练基模型；2) 自改进阶段：用基模型给真实文档打标→通过多种过滤策略验证与筛选高置信样本→在筛选集上再训练；循环迭代以持续提升模型与数据质量。框架不依赖教师蒸馏。

Result: 基于公开POINTS-1.5训练得到POINTS-Reader，在多种公开与商用同等或更大规模模型上取得更优性能，尤其在复杂版式场景下表现突出；代码与模型已开源。

Conclusion: 通过“合成预训练 + 自标注过滤迭代”的无蒸馏流水线，可在缺少人工标注的情况下构建高质量文档抽取模型与数据；方法泛化好、可扩展，并验证了合成数据结合自训练在复杂文档转换中的有效性。

Abstract: High-quality labeled data is essential for training accurate document
conversion models, particularly in domains with complex formats such as tables,
formulas, and multi-column text. However, manual annotation is both costly and
time-consuming, while automatic labeling using existing models often lacks
accuracy in handling such challenging scenarios. Consequently, training student
models by distilling outputs from teacher models can significantly limit their
performance in real-world applications. In this paper, we propose a fully
automated, distillation-free framework comprising two stages for constructing
high-quality document extraction datasets and models capable of handling
diverse document formats and layouts. In the first stage, we introduce a method
for generating large-scale, diverse synthetic data, which enables a model to
extract key elements in a unified format with strong initial performance. In
the second stage, we present a self-improvement approach that further adapts
the model, initially trained on synthetic data, to real-world documents.
Specifically, we first use the fine-tuned model to annotate real documents,
then apply a suite of filtering strategies to verify annotation quality, and
finally retrain the model on the verified dataset. By iteratively repeating
this process, we progressively enhance both the model's conversion capabilities
and the quality of the generated data. We train a public POINTS-1.5 model to
obtain POINTS-Reader, which surpasses many existing public and proprietary
models of comparable or larger size. Our model is available at
https://github.com/Tencent/POINTS-Reader.

</details>


### [110] [FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework](https://arxiv.org/abs/2509.01232)
*Lingzhou Mu,Qiang Wang,Fan Jiang,Mengchao Wang,Yaqi Fan,Mu Xu,Kai Zhang*

Main category: cs.CV

TL;DR: 提出FantasyHSI：一种无需成对数据的人-场景交互生成框架，通过多智能体与视频生成结合，面向长时程高层任务与未见场景的泛化，显著提升任务完成度与物理真实感。


<details>
  <summary>Details</summary>
Motivation: 现有HSI方法在长时序、高层次任务规划与跨场景泛化上表现欠佳，且生成动作常出现轨迹漂移、肢体扭曲与脚滑等物理不真实问题；缺乏闭环反馈与偏好对齐机制。

Method: 1) 将交互建模为动态有向图；2) 构建协同多智能体体系：场景导航代理负责环境感知与高层路径规划；规划代理将长远目标分解为原子动作；评论（critic）代理基于生成动作与规划路径的偏差提供闭环反馈，在线纠偏生成带来的轨迹漂移；3) 采用DPO训练动作生成器，依据偏好优化减少肢体变形与脚滑；4) 在自建SceneBench上评估；全流程不依赖配对数据。

Result: 在SceneBench上，相比现有方法，在跨场景泛化、长时程任务完成率与物理真实感方面均取得显著提升；动作伪影（如肢体扭曲、脚滑）明显减少，逻辑一致性更好。

Conclusion: 通过视频生成+多智能体+闭环评论纠偏+偏好优化，FantasyHSI有效解决HSI的长时程一致性与物理真实感难题，并在自建基准上超越现有方法，展示了无配对数据条件下的强泛化与任务执行能力。

Abstract: Human-Scene Interaction (HSI) seeks to generate realistic human behaviors
within complex environments, yet it faces significant challenges in handling
long-horizon, high-level tasks and generalizing to unseen scenes. To address
these limitations, we introduce FantasyHSI, a novel HSI framework centered on
video generation and multi-agent systems that operates without paired data. We
model the complex interaction process as a dynamic directed graph, upon which
we build a collaborative multi-agent system. This system comprises a scene
navigator agent for environmental perception and high-level path planning, and
a planning agent that decomposes long-horizon goals into atomic actions.
Critically, we introduce a critic agent that establishes a closed-loop feedback
mechanism by evaluating the deviation between generated actions and the planned
path. This allows for the dynamic correction of trajectory drifts caused by the
stochasticity of the generative model, thereby ensuring long-term logical
consistency. To enhance the physical realism of the generated motions, we
leverage Direct Preference Optimization (DPO) to train the action generator,
significantly reducing artifacts such as limb distortion and foot-sliding.
Extensive experiments on our custom SceneBench benchmark demonstrate that
FantasyHSI significantly outperforms existing methods in terms of
generalization, long-horizon task completion, and physical realism. Ours
project page: https://fantasy-amap.github.io/fantasy-hsi/

</details>


### [111] [RT-DETRv2 Explained in 8 Illustrations](https://arxiv.org/abs/2509.01241)
*Ethan Qi Yang Chua,Jen Hong Tan*

Main category: cs.CV

TL;DR: 用8幅图层层剖析RT-DETRv2的检测架构，从整体流程到编码器、解码器与多尺度可变形注意力，直观展示张量流与模块逻辑，帮助读者真正看懂其内部工作机理。


<details>
  <summary>Details</summary>
Motivation: 目标检测模型（尤其是DETR系）结构复杂、图示常不清晰，尽管RT-DETRv2在实时检测上很重要，但现有示意难以让读者理解其模块如何协同工作，亟需更直观、系统的解析。

Method: 以可视化为核心：设计八张循序渐进的示意图，从宏观管线到关键子模块（编码器、解码器、多尺度可变形注意力），标注张量流向与数据维度，并配以文字解说模块设计动机与交互关系，构建清晰的心智模型。

Result: 形成一套结构化的图解与配套说明，明确展示RT-DETRv2各组件之间的连接关系、计算流程与注意力机制的工作方式，降低理解门槛。

Conclusion: 通过系统可视化，读者能更直观、完整地把握RT-DETRv2的内部机制，为研究与工程实践提供理解支撑；方法也可迁移到其他复杂检测架构的教学与沟通中。

Abstract: Object detection architectures are notoriously difficult to understand, often
more so than large language models. While RT-DETRv2 represents an important
advance in real-time detection, most existing diagrams do little to clarify how
its components actually work and fit together. In this article, we explain the
architecture of RT-DETRv2 through a series of eight carefully designed
illustrations, moving from the overall pipeline down to critical components
such as the encoder, decoder, and multi-scale deformable attention. Our goal is
to make the existing one genuinely understandable. By visualizing the flow of
tensors and unpacking the logic behind each module, we hope to provide
researchers and practitioners with a clearer mental model of how RT-DETRv2
works under the hood.

</details>


### [112] [Learning Correlation-aware Aleatoric Uncertainty for 3D Hand Pose Estimation](https://arxiv.org/abs/2509.01242)
*Lee Chae-Yeon,Nam Hyeon-Woo,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出一种将数据不确定性（aleatoric）建模纳入3D手部姿态估计的简洁方法：用单个线性层对关节输出的概率分布进行参数化，从而高效刻画关节间相关性，并可作为即插即用的不确定性头；在保持/提升精度的同时，提供优于既有方法的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有3D手姿态方法难点在于手部运动复杂、自相似与遮挡频繁；同时普遍缺乏对数据不确定性的估计，尤其是能显式编码关节相关性的建模。既有不确定性方法要么忽视关节间结构，要么计算代价高。作者想用更经济而有效的方式将关节相关性融入不确定性建模。

Method: 将手部关节输出空间建模为概率分布（非点估计），设计一种新参数化：用单个线性层作为任务头对该分布的参数进行预测，使其能够隐式捕获关节间的内在相关性。该“不确定性头”可无缝叠加到现有3D手姿态网络上，几乎不改变主干结构；通过这种参数化在推理时同时输出姿态估计与每关节/整体的不确定性。

Result: 在多项实验中，新参数化的不确定性建模优于现有方法；加上该不确定性头的3D手姿态模型在精度上达到有竞争力甚至更优的结果，同时获得更可靠的不确定性估计。

Conclusion: 用简单的线性层实现对关节相关性的概率建模，既高效又有效；该模块可作为通用插件，为3D手姿态网络带来高质量的aleatoric不确定性估计且不牺牲准确率。

Abstract: 3D hand pose estimation is a fundamental task in understanding human hands.
However, accurately estimating 3D hand poses remains challenging due to the
complex movement of hands, self-similarity, and frequent occlusions. In this
work, we address two limitations: the inability of existing 3D hand pose
estimation methods to estimate aleatoric (data) uncertainty, and the lack of
uncertainty modeling that incorporates joint correlation knowledge, which has
not been thoroughly investigated. To this end, we introduce aleatoric
uncertainty modeling into the 3D hand pose estimation framework, aiming to
achieve a better trade-off between modeling joint correlations and
computational efficiency. We propose a novel parameterization that leverages a
single linear layer to capture intrinsic correlations among hand joints. This
is enabled by formulating the hand joint output space as a probabilistic
distribution, allowing the linear layer to capture joint correlations. Our
proposed parameterization is used as a task head layer, and can be applied as
an add-on module on top of the existing models. Our experiments demonstrate
that our parameterization for uncertainty modeling outperforms existing
approaches. Furthermore, the 3D hand pose estimation model equipped with our
uncertainty head achieves favorable accuracy in 3D hand pose estimation while
introducing new uncertainty modeling capability to the model. The project page
is available at https://hand-uncertainty.github.io/.

</details>


### [113] [Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views](https://arxiv.org/abs/2509.01250)
*Xiangdong Zhang,Shaofeng Zhang,Junchi Yan*

Main category: cs.CV

TL;DR: 提出Point-PQAE：用两视角点云的交叉重建进行自监督预训练，较单视角自重建显著提升下游分类（ScanObjectNN）表现。


<details>
  <summary>Details</summary>
Motivation: 现有点云自监督多为单视角掩码重建，信息与难度有限；两视角范式可引入更大变化与互补信息，潜在更强的表示学习能力，但缺乏适配点云的视角生成与位置对齐机制。

Method: 1) 两视角生成：提出点云裁剪（crop）机制，生成彼此解耦的两个局部视图；2) 位置编码：设计新型编码表示两视图间的3D相对位置；3) 交叉重建：以一视图作为条件重建另一视图（而非自重建），形成更难、更信息密集的生成式预训练范式；4) 框架名为Point-PQAE（暗示量化/自编码器式结构），在预训练后进行线性/MLP评估。

Result: 在ScanObjectNN三种变体上，采用Mlp-Linear协议，较强基线Point-MAE分别提升6.5%、7.0%、6.7%。

Conclusion: 两视角交叉重建的生成式自监督能比单模态自重建更好地学习3D表示；关键在于合理的视图生成与跨视图相对位置编码。方法通用、可扩展，具有实际应用潜力。

Abstract: Point cloud learning, especially in a self-supervised way without manual
labels, has gained growing attention in both vision and learning communities
due to its potential utility in a wide range of applications. Most existing
generative approaches for point cloud self-supervised learning focus on
recovering masked points from visible ones within a single view. Recognizing
that a two-view pre-training paradigm inherently introduces greater diversity
and variance, it may thus enable more challenging and informative pre-training.
Inspired by this, we explore the potential of two-view learning in this domain.
In this paper, we propose Point-PQAE, a cross-reconstruction generative
paradigm that first generates two decoupled point clouds/views and then
reconstructs one from the other. To achieve this goal, we develop a crop
mechanism for point cloud view generation for the first time and further
propose a novel positional encoding to represent the 3D relative position
between the two decoupled views. The cross-reconstruction significantly
increases the difficulty of pre-training compared to self-reconstruction, which
enables our method to surpass previous single-modal self-reconstruction methods
in 3D self-supervised learning. Specifically, it outperforms the
self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three
variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is
available at https://github.com/aHapBean/Point-PQAE.

</details>


### [114] [ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization](https://arxiv.org/abs/2509.01259)
*Thinh-Phuc Nguyen,Thanh-Hai Nguyen,Gia-Huy Dinh,Lam-Huy Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: ReCap提出一套“检索相关文章→提炼上下文→生成叙事化说明”的管线，用外部新闻文本弥补图像只见表层的缺陷，使图像描述更具事件语义与事实性；在EVENTA 2025挑战赛OpenEvents V1上排名第2。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述多停留在可见物体层面，缺乏时间、社会、历史等事件语境，难以满足新闻报道、档案等高风险场景对事实与叙事性的要求。需要把视觉内容与外部现实世界知识结合。

Method: 三部分：1) 文章检索：两阶段。先用DINOv2全局特征做相似度召回候选，再用patch级互为近邻重排序；2) 上下文抽取：融合文章摘要、通用caption与源元数据，综合出与事件相关的关键信息；3) caption生成：基于大语言模型，在生成时引入“语义高斯归一化（Semantic Gaussian Normalization）”以提升流畅度与相关性。

Result: 在EVENTA 2025 Grand Challenge（Track 1, OpenEvents V1）私测集上总体分0.54666，排名第2，表明方法有效。

Conclusion: 通过结合视觉检索、跨文本文脉提炼与LLM生成，ReCap能将图像与真实世界事件知识对齐，生成叙事化、事实更扎实的描述，适用于需要上下文感知的高风险应用；代码已开源。

Abstract: Image captioning systems often produce generic descriptions that fail to
capture event-level semantics which are crucial for applications like news
reporting and digital archiving. We present ReCap, a novel pipeline for
event-enriched image retrieval and captioning that incorporates broader
contextual information from relevant articles to generate narrative-rich,
factually grounded captions. Our approach addresses the limitations of standard
vision-language models that typically focus on visible content while missing
temporal, social, and historical contexts. ReCap comprises three integrated
components: (1) a robust two-stage article retrieval system using DINOv2
embeddings with global feature similarity for initial candidate selection
followed by patch-level mutual nearest neighbor similarity re-ranking; (2) a
context extraction framework that synthesizes information from article
summaries, generic captions, and original source metadata; and (3) a large
language model-based caption generation system with Semantic Gaussian
Normalization to enhance fluency and relevance. Evaluated on the OpenEvents V1
dataset as part of Track 1 in the EVENTA 2025 Grand Challenge, ReCap achieved a
strong overall score of 0.54666, ranking 2nd on the private test set. These
results highlight ReCap's effectiveness in bridging visual perception with
real-world knowledge, offering a practical solution for context-aware image
understanding in high-stakes domains. The code is available at
https://github.com/Noridom1/EVENTA2025-Event-Enriched-Image-Captioning.

</details>


### [115] [Novel Category Discovery with X-Agent Attention for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2509.01275)
*Jiahao Li Yang Lu,Yachao Zhang,Fangyong Wang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 论文提出X-Agent用于开放词汇语义分割，通过“潜在语义感知代理”优化跨模态注意并提升未见类的可辨识度，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: OVSS需要用文本引导做像素级分类，但训练只覆盖基类，推理要处理未见类，存在域间差异与潜在语义不可辨识的问题。现有基于VLM的方法虽有效，但对VLM中“潜在语义”如何形成与流动缺乏系统理解，成为性能瓶颈。

Method: 首先进行探针实验，分析VLM在归纳学习设置下潜在语义的分布与动态（模式、漂移、可见度）。据此设计X-Agent：在分割框架中引入“潜在语义感知代理”，通过编排跨模态注意来同时（1）优化潜在语义的动态（稳定、对齐、聚合），（2）放大潜在语义的可感知性（提高显著性/可分性）。整体以VLM预训练表征为基础，结合agent机制进行端到端训练与推理。

Result: 在多项OVSS基准上取得SOTA，同时实证表明该方法能提升潜在语义显著性（如更清晰的注意热力图、更高的未见类分割精度）。

Conclusion: 理解并调控VLM中的潜在语义动态是提升OVSS的关键。X-Agent通过语义感知代理有效地组织跨模态注意，缓解基类-开放域差异，提升未见类辨识与总体分割性能。

Abstract: Open-vocabulary semantic segmentation (OVSS) conducts pixel-level
classification via text-driven alignment, where the domain discrepancy between
base category training and open-vocabulary inference poses challenges in
discriminative modeling of latent unseen category. To address this challenge,
existing vision-language model (VLM)-based approaches demonstrate commendable
performance through pre-trained multi-modal representations. However, the
fundamental mechanisms of latent semantic comprehension remain underexplored,
making the bottleneck for OVSS. In this work, we initiate a probing experiment
to explore distribution patterns and dynamics of latent semantics in VLMs under
inductive learning paradigms. Building on these insights, we propose X-Agent,
an innovative OVSS framework employing latent semantic-aware ``agent'' to
orchestrate cross-modal attention mechanisms, simultaneously optimizing latent
semantic dynamic and amplifying its perceptibility. Extensive benchmark
evaluations demonstrate that X-Agent achieves state-of-the-art performance
while effectively enhancing the latent semantic saliency.

</details>


### [116] [SAR-NAS: Lightweight SAR Object Detection with Neural Architecture Search](https://arxiv.org/abs/2509.01279)
*Xinyi Yu,Zhiwei Lin,Yongtao Wang*

Main category: cs.CV

TL;DR: 用NAS优化轻量级YOLOv10用于SAR目标检测，自动搜索更优骨干网络，在SARDet-100K上以更高精度和更低算力开销优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SAR图像存在斑点噪声、小目标不清晰、机载算力有限，现有方法多做“为SAR定制”的网络改动，缺少系统性、可迁移且兼顾效率的优化路径。作者尝试将通用轻量检测器YOLOv10引入SAR，并用NAS自动寻找兼顾精度-参数量-计算量的结构，降低人工调参与过度定制。

Method: 以YOLOv10为基础，构建以骨干为主的广义搜索空间（可能包含层数/宽度/卷积类型/连接方式等），使用进化搜索算法在约束下（精度与FLOPs/参数）进行架构搜索，选取帕累托最优解作为最终模型，实现对精度与效率的权衡。

Result: 在大规模SARDet-100K数据集上，NAS优化后的YOLOv10取得更高检测精度，同时保持更低或相当的参数量与计算量，整体优于现有SAR检测方法。

Conclusion: 首次将NAS系统性引入SAR目标检测，验证了在真实约束下（算力与精度）的有效性，提供了从通用轻量检测器出发、通过自动化结构搜索提升SAR性能的可行范式。

Abstract: Synthetic Aperture Radar (SAR) object detection faces significant challenges
from speckle noise, small target ambiguities, and on-board computational
constraints. While existing approaches predominantly focus on SAR-specific
architectural modifications, this paper explores the application of the
existing lightweight object detector, i.e., YOLOv10, for SAR object detection
and enhances its performance through Neural Architecture Search (NAS).
Specifically, we employ NAS to systematically optimize the network structure,
especially focusing on the backbone architecture search. By constructing an
extensive search space and leveraging evolutionary search, our method
identifies a favorable architecture that balances accuracy, parameter
efficiency, and computational cost. Notably, this work introduces NAS to SAR
object detection for the first time. The experimental results on the
large-scale SARDet-100K dataset demonstrate that our optimized model
outperforms existing SAR detection methods, achieving superior detection
accuracy while maintaining lower computational overhead. We hope this work
offers a novel perspective on leveraging NAS for real-world applications.

</details>


### [117] [Multi-Representation Adapter with Neural Architecture Search for Efficient Range-Doppler Radar Object Detection](https://arxiv.org/abs/2509.01280)
*Zhiwei Lin,Weicheng Zheng,Yongtao Wang*

Main category: cs.CV

TL;DR: 提出一种针对雷达Range-Doppler(RD)图的高效目标检测模型：利用多表示(热力图+灰度图)提取互补特征，设计Adapter分支、双模Exchanger和主辅融合模块进行特征提取/交换/融合，并用One-Shot NAS在超网中搜索宽度与融合操作，最终在RADDet与CARRADA上获SOTA（mAP@50=71.9/57.1）且具良好效率。


<details>
  <summary>Details</summary>
Motivation: 雷达在弱光/恶劣天气下比相机更稳健，但RD雷达图目标检测面临效率与精度权衡、以及如何充分挖掘不同表示的互补信息的问题。现有方法要么模型大、推理慢，要么特征融合不足。

Method: 1) 输入表示：将RD图同时转为热力图(高层语义)与灰度图(细粒度纹理)。2) 结构设计：- Adapter分支：为多表示量身定制的轻量特征提取分支；- Exchanger模块(两种模式)：在多表示流之间进行双向信息交换；- 主辅融合模块：将主分支与辅助分支的特征进行高效融合。3) 架构搜索：在Adapter分支构建含不同宽度与融合操作的超网，采用One-Shot NAS寻找兼顾精度/效率的子网。

Result: 在RADDet与CARRADA数据集上取得新的SOTA：mAP@50分别为71.9与57.1；同时报告了良好的效率(推理速度/参数量未给出具体数值，但强调优于对比方法的权衡)。

Conclusion: 多表示输入结合特定的特征交换与融合机制，配合One-Shot NAS，可在RD雷达目标检测中实现更优的精度-效率折中，并刷新主流数据集表现。

Abstract: Detecting objects efficiently from radar sensors has recently become a
popular trend due to their robustness against adverse lighting and weather
conditions compared with cameras. This paper presents an efficient object
detection model for Range-Doppler (RD) radar maps. Specifically, we first
represent RD radar maps with multi-representation, i.e., heatmaps and grayscale
images, to gather high-level object and fine-grained texture features. Then, we
design an additional Adapter branch, an Exchanger Module with two modes, and a
Primary-Auxiliary Fusion Module to effectively extract, exchange, and fuse
features from the multi-representation inputs, respectively. Furthermore, we
construct a supernet with various width and fusion operations in the Adapter
branch for the proposed model and employ a One-Shot Neural Architecture Search
method to further improve the model's efficiency while maintaining high
performance. Experimental results demonstrate that our model obtains favorable
accuracy and efficiency trade-off. Moreover, we achieve new state-of-the-art
performance on RADDet and CARRADA datasets with mAP@50 of 71.9 and 57.1,
respectively.

</details>


### [118] [Cross-Domain Few-Shot Segmentation via Ordinary Differential Equations over Time Intervals](https://arxiv.org/abs/2509.01299)
*Huan Ni,Qingshan Liu,Xiaonan Niu,Danfeng Hong,Lingli Zhao,Haiyan Guan*

Main category: cs.CV

TL;DR: 提出FSS-TIs：用常微分方程+傅里叶域变换的一体化模块，迭代地将域相关特征谱映射为域无关表征，并在跨域少样本分割上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨域少样本分割通常堆叠多个相互独立的增强模块，知识流动受阻，难以充分发挥合力；需要一个结构简洁、端到端的一体化机制来提升跨域泛化与少样本适配能力。

Method: - 提出Few-Shot Segmentation over Time Intervals (FSS-TIs)。
- 在傅里叶域（幅度与相位谱）假设域相关→域无关特征谱满足某种ODE演化关系。
- 将该ODE离散为随时间区间迭代的谱空间变换，并叠加带随机扰动的仿射变换，等价于在ODE内在参数上进行优化。
- 通过这种迭代过程同时探索域无关表征空间并模拟多样目标域分布。
- 在目标域微调阶段严格约束支持样本的选择，符合真实FSS设置。

Result: 在5个差异较大的数据集上定义两组CD-FSS任务做评测；FSS-TIs在总体性能上优于现有CD-FSS方法。消融实验表明其跨域适应性与各组成设计有效。

Conclusion: 以ODE+傅里叶域的一体化模块替代多独立模块，能更有效地学习域无关表征并模拟目标域变化，从而在跨域少样本分割中取得领先表现；方法结构简洁、可端到端优化，具备良好的跨域泛化与适配能力。

Abstract: Cross-domain few-shot segmentation (CD-FSS) not only enables the segmentation
of unseen categories with very limited samples, but also improves cross-domain
generalization ability within the few-shot segmentation framework. Currently,
existing CD-FSS studies typically design multiple independent modules to
enhance the cross-domain generalization ability of feature representations.
However, the independence among these modules hinders the effective flow of
knowledge, making it difficult to fully leverage their collective potential. In
contrast, this paper proposes an all-in-one module based on ordinary
differential equations and Fourier transform, resulting in a structurally
concise method--Few-Shot Segmentation over Time Intervals (FSS-TIs). FSS-TIs
assumes the existence of an ODE relationship between the spectra (including
amplitude and phase spectra) of domain-specific features and domain-agnostic
features. This ODE formulation yields an iterative transformation process along
a sequence of time intervals, while simultaneously applying affine
transformations with randomized perturbations to the spectra. In doing so, the
exploration of domain-agnostic feature representation spaces and the simulation
of diverse potential target-domain distributions are reformulated as an
optimization process over the intrinsic parameters of the ODE. Moreover, we
strictly constrain the support-sample selection during target-domain
fine-tuning so that it is consistent with the requirements of real-world
few-shot segmentation tasks. For evaluation, we introduce five datasets from
substantially different domains and define two sets of cross-domain few-shot
segmentation tasks to comprehensively analyze the performance of FSS-TIs.
Experimental results demonstrate the superiority of FSS-TIs over existing
CD-FSS methods, and in-depth ablation studies further validate the cross-domain
adaptability of FSS-TIs.

</details>


### [119] [Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation](https://arxiv.org/abs/2509.01317)
*Alexandros Gkillas,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 提出首个端到端联合优化的LiDAR超分辨率+语义分割框架，用轻量模型和新SR损失在低线束(如16线)上逼近64线分割表现。


<details>
  <summary>Details</summary>
Motivation: 高分辨率(如64线)LiDAR对3D语义分割很重要但价格高，低成本16线点云稀疏导致分割精度显著下降；需要既提升点云密度又兼顾语义精度的实用方案。

Method: 构建联合训练的两分支：前端LiDAR超分辨率(SR)模块与后端分割网络端到端优化；SR模块在训练中接收语义反馈以保留小物体等细节；设计新的SR损失，强调兴趣区域(与语义相关)，并采用轻量、基于模型的SR架构，参数量远低于现有SR方法，同时与常见分割网络易于对接。

Result: 在实验中，使用低成本传感器(如16线)输入，经本方法处理的分割性能接近直接使用高分辨率、昂贵的64线LiDAR模型；在参数量上显著低于现有LiDAR SR方法。

Conclusion: 联合优化的轻量SR+分割框架能在保持低硬件成本的同时，显著提升低线束LiDAR的语义分割效果，尤其对小目标有益，并具备良好兼容性与部署潜力。

Abstract: High-resolution LiDAR data plays a critical role in 3D semantic segmentation
for autonomous driving, but the high cost of advanced sensors limits
large-scale deployment. In contrast, low-cost sensors such as 16-channel LiDAR
produce sparse point clouds that degrade segmentation accuracy. To overcome
this, we introduce the first end-to-end framework that jointly addresses LiDAR
super-resolution (SR) and semantic segmentation. The framework employs joint
optimization during training, allowing the SR module to incorporate semantic
cues and preserve fine details, particularly for smaller object classes. A new
SR loss function further directs the network to focus on regions of interest.
The proposed lightweight, model-based SR architecture uses significantly fewer
parameters than existing LiDAR SR approaches, while remaining easily compatible
with segmentation networks. Experiments show that our method achieves
segmentation performance comparable to models operating on high-resolution and
costly 64-channel LiDAR data.

</details>


### [120] [Prior-Guided Residual Diffusion: Calibrated and Efficient Medical Image Segmentation](https://arxiv.org/abs/2509.01330)
*Fuyou Mao,Beining Wu,Yanfeng Jiang,Han Xue,Yan Tang,Hao Zhang*

Main category: cs.CV

TL;DR: 提出Prior-Guided Residual Diffusion（PGRD），把分割标签嵌入连续空间，用“先验预测+残差扩散+深层扩散监督”学习体素级条件分布；在MRI/CT上优于贝叶斯、集成、Prob-UNet和普通扩散，Dice更高、NLL/ECE更低，采样更省步数。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割存在标注歧义与多模态不确定性，单一点估计无法刻画完整条件分布；现有概率分割方法（如Prob-UNet、贝叶斯/集成）在校准性、表达力或计算效率上折衷明显，而扩散模型虽表达力强但采样慢、校准不足，需要一种既能精确建模不确定性又高效、校准良好的方法。

Method: 1) 标签连续化：将离散分割标签以one-hot嵌入连续空间，使其与扩散噪声建模一致；2) 先验引导：训练一个粗粒度先验预测器，在扩散每个时间步提供条件引导；3) 残差学习：扩散网络仅学习相对先验的残差，提升收敛与校准；4) 深层扩散监督：对多个中间时间步施加监督，稳定训练；5) 以更少采样步得到高质量样本并输出体素级分布与不确定性估计。

Result: 在代表性的MRI与CT数据集上，PGRD较贝叶斯方法、模型集成、Probabilistic U-Net及普通扩散基线取得更高Dice、更低NLL/ECE；在达到强性能所需的采样步数更少，显示出更好的效率与校准。

Conclusion: PGRD能以高效采样学习体素级条件分布，兼顾分割精度与不确定性校准，优于现有概率分割与扩散基线；先验引导+残差扩散+深层监督是提升收敛、效率与校准的关键。

Abstract: Ambiguity in medical image segmentation calls for models that capture full
conditional distributions rather than a single point estimate. We present
Prior-Guided Residual Diffusion (PGRD), a diffusion-based framework that learns
voxel-wise distributions while maintaining strong calibration and practical
sampling efficiency. PGRD embeds discrete labels as one-hot targets in a
continuous space to align segmentation with diffusion modeling. A coarse prior
predictor provides step-wise guidance; the diffusion network then learns the
residual to the prior, accelerating convergence and improving calibration. A
deep diffusion supervision scheme further stabilizes training by supervising
intermediate time steps. Evaluated on representative MRI and CT datasets, PGRD
achieves higher Dice scores and lower NLL/ECE values than Bayesian, ensemble,
Probabilistic U-Net, and vanilla diffusion baselines, while requiring fewer
sampling steps to reach strong performance.

</details>


### [121] [Image Quality Enhancement and Detection of Small and Dense Objects in Industrial Recycling Processes](https://arxiv.org/abs/2509.01332)
*Oussama Messai,Abbass Zein-Eddine,Abdelouahid Bentamou,Mickaël Picq,Nicolas Duquesne,Stéphane Puydarrieux,Yann Gavet*

Main category: cs.CV

TL;DR: 评测与改进小目标密集重叠检测与工业噪声图像增强：基于新数据集对监督式方法做系统对比，并提出轻量FCN式增强模型与开源资源。


<details>
  <summary>Details</summary>
Motivation: 工业场景中常见的目标体积小、密集且相互重叠，传统检测难度大；同时图像噪声严重影响检测与下游质量。现有方法在这两方面仍存在鲁棒性、精度与效率的权衡问题，缺乏面向工业噪声与密集小目标的系统评测与专门模型。

Method: 1) 构建并公开一个包含1万+图像、12万+实例的新数据集，面向小、密集、重叠目标检测与噪声环境。2) 系统评测多种监督式深度学习检测方法，比较其性能、精度与计算开销，分析其在工业应用中能解决的具体挑战。3) 在图像增强方面，提出一个轻量级、基于全卷积网络的图像质量提升模型（DDSRNet），并给出未来优化方向。

Result: 基于新数据集的实验确定了若干在精度与效率上更可靠的检测系统，明确不同方法对工业中具体问题（小目标、重叠、噪声）的适用性。同时，所提出的轻量增强模型在噪声工业图像上取得有效的质量改进（抽象中未给出具体数值）。代码与数据集已在GitHub开源。

Conclusion: 工业场景下，面向小而密集且重叠目标的检测与噪声图像增强可通过监督式深度学习取得可靠表现。公开的数据集和轻量增强模型为后续研究提供基准与起点，并指出继续提升模型有效性的潜在方向。

Abstract: This paper tackles two key challenges: detecting small, dense, and
overlapping objects (a major hurdle in computer vision) and improving the
quality of noisy images, especially those encountered in industrial
environments. [1, 2]. Our focus is on evaluating methods built on supervised
deep learning. We perform an analysis of these methods, using a newly de-
veloped dataset comprising over 10k images and 120k in- stances. By evaluating
their performance, accuracy, and com- putational efficiency, we identify the
most reliable detection systems and highlight the specific challenges they
address in industrial applications. This paper also examines the use of deep
learning models to improve image quality in noisy industrial environments. We
introduce a lightweight model based on a fully connected convolutional network.
Addition- ally, we suggest potential future directions for further enhanc- ing
the effectiveness of the model. The repository of the dataset and proposed
model can be found at: https://github.com/o-messai/SDOOD,
https://github.com/o-messai/DDSRNet

</details>


### [122] [Street-Level Geolocalization Using Multimodal Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2509.01341)
*Yunus Serhat Bicakci,Joseph Shingleton,Anahid Basiri*

Main category: cs.CV

TL;DR: 提出用检索增强的多模态大模型做街景图像定位：用SigLIP建向量库，检索相似/不相似地点并拼接成提示词，喂给开源多模态LLM，无需微调即可在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉模型的地理定位依赖大规模训练/微调，成本高、难以随着社媒与手机照片激增而快速扩展；需要一种可扩展、可复用、低成本的方案利用开放模型与外部知识实现更准确的定位。

Method: 1) 使用SigLIP对两大数据集（EMP-16、OSV-5M）构建图像向量数据库；2) 对查询图像进行向量检索，取相似与不相似样本并生成包含地理线索的提示词；3) 将图像+检索到的正负地理信息作为上下文，输入开源、可公开访问的多模态大语言模型（M-LLM）执行地理位置推断；4) 采用RAG范式，无需对模型进行微调，可随数据增长无缝扩展。

Result: 在IM2GPS、IM2GPS3k、YFCC4k三大基准上取得SOTA精度，优于现有方法；同时保持零微调/零重训，展现了良好的可扩展性与实用性。

Conclusion: RAG+多模态LLM在图像地理定位任务上有效且具可扩展性，为不依赖从头训练的GeoAI提供了可行路径，能轻量集成新数据源并降低部署成本。

Abstract: Street-level geolocalization from images is crucial for a wide range of
essential applications and services, such as navigation, location-based
recommendations, and urban planning. With the growing popularity of social
media data and cameras embedded in smartphones, applying traditional computer
vision techniques to localize images has become increasingly challenging, yet
highly valuable. This paper introduces a novel approach that integrates
open-weight and publicly accessible multimodal large language models with
retrieval-augmented generation. The method constructs a vector database using
the SigLIP encoder on two large-scale datasets (EMP-16 and OSV-5M). Query
images are augmented with prompts containing both similar and dissimilar
geolocation information retrieved from this database before being processed by
the multimodal large language models. Our approach has demonstrated
state-of-the-art performance, achieving higher accuracy compared against three
widely used benchmark datasets (IM2GPS, IM2GPS3k, and YFCC4k). Importantly, our
solution eliminates the need for expensive fine-tuning or retraining and scales
seamlessly to incorporate new data sources. The effectiveness of
retrieval-augmented generation-based multimodal large language models in
geolocation estimation demonstrated by this paper suggests an alternative path
to the traditional methods which rely on the training models from scratch,
opening new possibilities for more accessible and scalable solutions in GeoAI.

</details>


### [123] [AgroSense: An Integrated Deep Learning System for Crop Recommendation via Soil Image Analysis and Nutrient Profiling](https://arxiv.org/abs/2509.01344)
*Vishal Pandey,Ranjita Das,Debasmita Biswas*

Main category: cs.CV

TL;DR: 提出AgroSense：结合土壤图像分类与营养要素表格数据的多模态深度学习框架，实现实时作物推荐，显著优于单模态与传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统土壤检测慢、费力、不适合田间实时决策；需要能在资源受限环境下、利用现成数据源，给出准确且具情境感知的作物推荐。

Method: 双模块多模态架构：1) 土壤分类模块用ResNet-18、EfficientNet-B0、ViT对土壤图像分七类；2) 作物推荐模块用MLP、XGBoost、LightGBM、TabNet处理营养、pH、降雨等结构化数据；融合两者输出。构建1万对配对样本＋5万张图像＋2.5万营养档案的多模态数据集；进行消融、t检验与ANOVA统计验证。

Result: 融合模型达98.0%准确率，Precision 97.8%，Recall 97.7%，F1 96.75%；回归误差RMSE 0.32、MAE 0.27；消融显示多模态耦合至关重要，统计检验显著优于基线。

Conclusion: AgroSense在精准农业中提供可扩展、可实用的实时决策支持，并为资源受限场景下的轻量多模态AI系统奠定基础。

Abstract: Meeting the increasing global demand for food security and sustainable
farming requires intelligent crop recommendation systems that operate in real
time. Traditional soil analysis techniques are often slow, labor-intensive, and
not suitable for on-field decision-making. To address these limitations, we
introduce AgroSense, a deep-learning framework that integrates soil image
classification and nutrient profiling to produce accurate and contextually
relevant crop recommendations. AgroSense comprises two main components: a Soil
Classification Module, which leverages ResNet-18, EfficientNet-B0, and Vision
Transformer architectures to categorize soil types from images; and a Crop
Recommendation Module, which employs a Multi-Layer Perceptron, XGBoost,
LightGBM, and TabNet to analyze structured soil data, including nutrient
levels, pH, and rainfall. We curated a multimodal dataset of 10,000 paired
samples drawn from publicly available Kaggle repositories, approximately 50,000
soil images across seven classes, and 25,000 nutrient profiles for experimental
evaluation. The fused model achieves 98.0% accuracy, with a precision of 97.8%,
a recall of 97.7%, and an F1-score of 96.75%, while RMSE and MAE drop to 0.32
and 0.27, respectively. Ablation studies underscore the critical role of
multimodal coupling, and statistical validation via t-tests and ANOVA confirms
the significance of our improvements. AgroSense offers a practical, scalable
solution for real-time decision support in precision agriculture and paves the
way for future lightweight multimodal AI systems in resource-constrained
environments.

</details>


### [124] [M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision](https://arxiv.org/abs/2509.01360)
*Che Liu,Zheng Jiang,Chengyu Fang,Heng Guo,Yan-Jie Zhou,Jiaqi Qu,Le Lu,Minfeng Xu*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Medical image retrieval is essential for clinical decision-making and
translational research, relying on discriminative visual representations. Yet,
current methods remain fragmented, relying on separate architectures and
training strategies for 2D, 3D, and video-based medical data. This
modality-specific design hampers scalability and inhibits the development of
unified representations. To enable unified learning, we curate a large-scale
hybrid-modality dataset comprising 867,653 medical imaging samples, including
2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging
this dataset, we train M3Ret, a unified visual encoder without any
modality-specific customization. It successfully learns transferable
representations using both generative (MAE) and contrastive (SimDINO)
self-supervised learning (SSL) paradigms. Our approach sets a new
state-of-the-art in zero-shot image-to-image retrieval across all individual
modalities, surpassing strong baselines such as DINOv3 and the text-supervised
BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired
data, and the model generalizes to unseen MRI tasks, despite never observing
MRI during pretraining, demonstrating the generalizability of purely visual
self-supervision to unseen modalities. Comprehensive analyses further validate
the scalability of our framework across model and data sizes. These findings
deliver a promising signal to the medical imaging community, positioning M3Ret
as a step toward foundation models for visual SSL in multimodal medical image
understanding.

</details>


### [125] [Identity-Preserving Text-to-Video Generation via Training-Free Prompt, Image, and Guidance Enhancement](https://arxiv.org/abs/2509.01362)
*Jiayi Gao,Changcheng Hua,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 提出TPIGE：在无需训练的前提下，通过提示词增强、参考图像增强和生成时指导增强，显著提升身份保持型文生视频生成的保真度与质量，并在ACM MM 2025挑战赛夺冠。


<details>
  <summary>Details</summary>
Motivation: 现有身份保持的文生视频依赖对大模型进行微调，需要同身份数据且成本高、数据稀缺，限制了泛化与落地；需要一种无需训练、可通用地提升身份一致性与视频质量的方法。

Method: TPIGE包含三部分：1) 面部感知的提示增强（FAPE）：用GPT-4o从参考人像中抽取/补全关键面部细节并扩展文本提示，使语义与人脸特征对齐；2) 提示感知的参考图像增强（PARG）：利用身份保持图像生成器，依据增强后的提示对参考图像进行修复与细节一致化，解决图像与文本冲突；3) 身份感知的时空采样指导增强（I-STGE）：在扩散采样过程中构造统一梯度，引入身份保持与视频质量（时空一致性、清晰度）的联合优化指导。全流程训练免调，只在推理时运行。

Result: 在1000段视频测试集上，自动指标与人工评测均优于现有方法；以最小成本实现显著增益，并在ACM MM 2025 身份保持视频生成挑战赛中获得第一名。

Conclusion: 通过在推理阶段对提示、参考图与采样指导进行双向对齐与统一优化，无需微调即可显著提升身份一致性与视频质量，方法通用、成本低、效果达SOTA，并已开源。

Abstract: Identity-preserving text-to-video (IPT2V) generation creates videos faithful
to both a reference subject image and a text prompt. While fine-tuning large
pretrained video diffusion models on ID-matched data achieves state-of-the-art
results on IPT2V, data scarcity and high tuning costs hinder broader
improvement. We thus introduce a Training-Free Prompt, Image, and Guidance
Enhancement (TPIGE) framework that bridges the semantic gap between the video
description and the reference image and design sampling guidance that enhances
identity preservation and video quality, achieving performance gains at minimal
cost.Specifically, we first propose Face Aware Prompt Enhancement, using GPT-4o
to enhance the text prompt with facial details derived from the reference
image. We then propose Prompt Aware Reference Image Enhancement, leveraging an
identity-preserving image generator to refine the reference image, rectifying
conflicts with the text prompt. The above mutual refinement significantly
improves input quality before video generation. Finally, we propose ID-Aware
Spatiotemporal Guidance Enhancement, utilizing unified gradients to optimize
identity preservation and video quality jointly during generation.Our method
outperforms prior work and is validated by automatic and human evaluations on a
1000 video test set, winning first place in the ACM Multimedia 2025
Identity-Preserving Video Generation Challenge, demonstrating state-of-the-art
performance and strong generality. The code is available at
https://github.com/Andyplus1/IPT2V.git.

</details>


### [126] [Uirapuru: Timely Video Analytics for High-Resolution Steerable Cameras on Edge Devices](https://arxiv.org/abs/2509.01371)
*Guilherme H. Apostolo,Pablo Bauszat,Vinod Nigade,Henri E. Bal,Lin Wang*

Main category: cs.CV

TL;DR: Uirapuru 是一个针对可转向（PTZ）高分辨率摄像头的实时边缘视频分析框架，通过将摄像机转动行为纳入系统并进行逐帧自适应切片，在满足时延预算下提高准确率或在不降准确的情况下显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有实时视频分析多聚焦于固定视角摄像头，无法处理PTZ摄像头带来的视角变化和场景动态，导致基于静态假设的切片/调度方法失效或效率低下。

Method: 1) 将摄像机的云台变焦（PTZ）动作建模并纳入系统设计；2) 依据当前帧与PTZ状态进行快速、逐帧的自适应切片（tiling）策略；3) 在边缘侧执行实时推理与调度，以兼顾时延与精度。

Result: 在带有典型PTZ动作的高分辨率数据集与真实PTZ视频上，较静态相机方法：在满足给定时延预算时，准确率提升最高1.45倍；在保持相当精度下，推理速度最高提升4.53倍。

Conclusion: 考虑PTZ动态并配合逐帧自适应切片能显著优于为静态相机设计的主流方法，Uirapuru在时延-精度权衡上取得更优表现，适用于交通管制、人群监测等实时边缘分析场景。

Abstract: Real-time video analytics on high-resolution cameras has become a popular
technology for various intelligent services like traffic control and crowd
monitoring. While extensive work has been done on improving analytics accuracy
with timing guarantees, virtually all of them target static viewpoint cameras.
In this paper, we present Uirapuru, a novel framework for real-time, edge-based
video analytics on high-resolution steerable cameras. The actuation performed
by those cameras brings significant dynamism to the scene, presenting a
critical challenge to existing popular approaches such as frame tiling. To
address this problem, Uirapuru incorporates a comprehensive understanding of
camera actuation into the system design paired with fast adaptive tiling at a
per-frame level. We evaluate Uirapuru on a high-resolution video dataset,
augmented by pan-tilt-zoom (PTZ) movements typical for steerable cameras and on
real-world videos collected from an actual PTZ camera. Our experimental results
show that Uirapuru provides up to 1.45x improvement in accuracy while
respecting specified latency budgets or reaches up to 4.53x inference speedup
with on-par accuracy compared to state-of-the-art static camera approaches.

</details>


### [127] [Unsupervised Ultra-High-Resolution UAV Low-Light Image Enhancement: A Benchmark, Metric and Framework](https://arxiv.org/abs/2509.01373)
*Wei Lu,Lingyu Zhu,Si-Bao Chen*

Main category: cs.CV

TL;DR: 提出面向无人机低照图像增强的完整方案：U3D无监督超高分辨率数据集+统一评测工具；EEI综合效率指标；U3LIE高效框架（含训练期APA与亮度区间损失）。在4K上单卡23.8FPS并达SOTA，适于机载实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有低照增强方法不适配无人机：超高分辨率带来算力/内存压力；缺乏成对标注数据；光照强烈不均；部署需兼顾速度、模型规模与显存。需要数据、评测与方法三位一体的解决方案以支持全天候（24/7）UAV视觉。

Method: 1) 数据与工具：构建U3D无监督UHR UAV低照数据集，并提供统一评测工具包。2) 指标：提出EEI（Edge Efficiency Index），在感知质量与部署因素（速度、分辨率、模型复杂度、显存）间加权平衡。3) 方法：U3LIE框架，强调高效推理；引入仅用于训练的两项设计——APA自适应预增强增广进行输入归一化；Luminance Interval Loss用于控制曝光分布，提升亮度一致性与鲁棒性。

Result: U3LIE在多个评测上达到SOTA；在单GPU上处理4K图像达23.8 FPS，兼顾质量与效率。在U3D与统一工具包下验证其泛化与部署友好性。

Conclusion: 通过数据集、指标与方法的协同，显著提升无人机低照视觉的实用性与稳健性，可实现实时机载应用。代码与数据已开源（U3D_Toolkit）。

Abstract: Low light conditions significantly degrade Unmanned Aerial Vehicles (UAVs)
performance in critical applications. Existing Low-light Image Enhancement
(LIE) methods struggle with the unique challenges of aerial imagery, including
Ultra-High Resolution (UHR), lack of paired data, severe non-uniform
illumination, and deployment constraints. To address these issues, we propose
three key contributions. First, we present U3D, the first unsupervised UHR UAV
dataset for LIE, with a unified evaluation toolkit. Second, we introduce the
Edge Efficiency Index (EEI), a novel metric balancing perceptual quality with
key deployment factors: speed, resolution, model complexity, and memory
footprint. Third, we develop U3LIE, an efficient framework with two
training-only designs-Adaptive Pre-enhancement Augmentation (APA) for input
normalization and a Luminance Interval Loss (L_int) for exposure control. U3LIE
achieves SOTA results, processing 4K images at 23.8 FPS on a single GPU, making
it ideal for real-time on-board deployment. In summary, these contributions
provide a holistic solution (dataset, metric, and method) for advancing robust
24/7 UAV vision. The code and datasets are available at
https://github.com/lwCVer/U3D_Toolkit.

</details>


### [128] [Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning](https://arxiv.org/abs/2509.01383)
*Long Zhang,Peipei Song,Jianfeng Dong,Kun Li,Xun Yang*

Main category: cs.CV

TL;DR: 提出PRVR任务中的鲁棒对齐学习（RAL），用高斯分布建模视频与文本不确定性，并用可学习置信门控加权相似度，在多种backbone上显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: PRVR场景存在数据不确定性：文本查询常含含糊/无信息词，视频是未裁剪的且只有部分片段相关，导致跨模态对齐受虚假相关性干扰。现有方法偏重多尺度片段表示与“最相关片段”检索，面对干扰视频易误检、鲁棒性不足。

Method: - 概率式对齐：将视频与查询编码为多变量高斯分布（均值+协方差），把不确定性显式纳入表示，并进行代理层级的分布匹配以刻画跨模态对应的多样性。
- 置信门控：针对查询词信息量异质性，设置可学习的词级置信门控，动态调节相似度贡献，抑制无信息/噪声词。
- 插拔式设计：RAL作为模块无缝接入现有PRVR架构与损失。

Result: 在多种检索骨干网络与多数据集上进行大量实验，RAL稳定带来性能提升，并在面对干扰视频时更鲁棒（摘要暗示SOTA或显著增益）。

Conclusion: 对PRVR的关键挑战（查询歧义与部分相关）采用概率建模与置信门控协同解决，提升跨模态对齐鲁棒性，且具有良好的通用性与易集成性。

Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve untrimmed videos
partially relevant to a given query. The core challenge lies in learning robust
query-video alignment against spurious semantic correlations arising from
inherent data uncertainty: 1) query ambiguity, where the query incompletely
characterizes the target video and often contains uninformative tokens, and 2)
partial video relevance, where abundant query-irrelevant segments introduce
contextual noise in cross-modal alignment. Existing methods often focus on
enhancing multi-scale clip representations and retrieving the most relevant
clip. However, the inherent data uncertainty in PRVR renders them vulnerable to
distractor videos with spurious similarities, leading to suboptimal
performance. To fill this research gap, we propose Robust Alignment Learning
(RAL) framework, which explicitly models the uncertainty in data. Key
innovations include: 1) we pioneer probabilistic modeling for PRVR by encoding
videos and queries as multivariate Gaussian distributions. This not only
quantifies data uncertainty but also enables proxy-level matching to capture
the variability in cross-modal correspondences; 2) we consider the
heterogeneous informativeness of query words and introduce learnable confidence
gates to dynamically weight similarity. As a plug-and-play solution, RAL can be
seamlessly integrated into the existing architectures. Extensive experiments
across diverse retrieval backbones demonstrate its effectiveness.

</details>


### [129] [RibPull: Implicit Occupancy Fields and Medial Axis Extraction for CT Ribcage Scans](https://arxiv.org/abs/2509.01402)
*Emmanouil Nikolakakis,Amine Ouasfi,Julie Digne,Razvan Marinescu*

Main category: cs.CV

TL;DR: 提出RibPull：用神经隐式占据场（implicit occupancy fields）对CT肋骨笼进行连续表示，并通过基于拉普拉斯的收缩提取肋骨笼骨架，中小样本上优于体素在稀疏、噪声和拓扑操作方面的表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像常用体素网格，但存在分辨率受限、拓扑信息丢失、稀疏性处理低效等问题。稀疏CT数据、噪声与复杂几何使离散方法受限；需要一种既能稳健表示稀疏/噪声数据，又利于后续形态学与几何运算（如骨架抽取）的3D表示。

Method: - 使用连续坐标到占据概率的神经网络（神经占据场）编码整个肋骨笼3D场景，输入任意3D坐标预测其在物体内/外；
- 将该隐式表示用于CT肋骨笼建模；
- 在得到的隐式场上执行基于拉普拉斯的收缩（Laplacian-based contraction）以提取肋骨笼的中轴（medial axis/骨架）；
- 与体素/体渲染表示对比，展示几何操作与稀疏补全优势；
- 在RibSeg（源自RibFrac）20例扫描上评估；代码发表时开源。

Result: 隐式占据场可在稀疏、含噪CT数据上形成连续、拓扑友好的3D表示，并支持稳定的骨架抽取；相较体素表示，在分辨率、拓扑保持与稀疏处理方面表现更优。摘要未给出具体数值指标。

Conclusion: 连续、坐标驱动的隐式3D表示适用于医学影像中复杂骨性结构的重建与形态学运算；Laplacian收缩在隐式场上更有效地得到肋骨笼中轴，表明该范式对下游几何任务具优势。未来工作与代码将随论文发布。

Abstract: We present RibPull, a methodology that utilizes implicit occupancy fields to
bridge computational geometry and medical imaging. Implicit 3D representations
use continuous functions that handle sparse and noisy data more effectively
than discrete methods. While voxel grids are standard for medical imaging, they
suffer from resolution limitations, topological information loss, and
inefficient handling of sparsity. Coordinate functions preserve complex
geometrical information and represent a better solution for sparse data
representation, while allowing for further morphological operations. Implicit
scene representations enable neural networks to encode entire 3D scenes within
their weights. The result is a continuous function that can implicitly
compesate for sparse signals and infer further information about the 3D scene
by passing any combination of 3D coordinates as input to the model. In this
work, we use neural occupancy fields that predict whether a 3D point lies
inside or outside an object to represent CT-scanned ribcages. We also apply a
Laplacian-based contraction to extract the medial axis of the ribcage, thus
demonstrating a geometrical operation that benefits greatly from continuous
coordinate-based 3D scene representations versus voxel-based representations.
We evaluate our methodology on 20 medical scans from the RibSeg dataset, which
is itself an extension of the RibFrac dataset. We will release our code upon
publication.

</details>


### [130] [Neural Scene Designer: Self-Styled Semantic Image Manipulation](https://arxiv.org/abs/2509.01405)
*Jianman Lin,Tianshui Chen,Chunmei Qing,Zhijing Yang,Shuangping Huang,Yuheng Ren,Liang Lin*

Main category: cs.CV

TL;DR: 提出“神经场景设计器（NSD）”，在图像编辑/修复中同时保证语义符合指令与风格与环境一致，通过双通道跨注意力处理文本与风格，并引入逐级自风格表征学习（PSRL）与风格对比损失；还构建了评测基准，实验显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式图像编辑主要强调生成内容的语义可控，却常忽视新插入或修复区域与原图整体风格的一致性，导致视觉割裂；同时缺少统一的评估协议与风格相关指标。

Method: 1) 框架：基于扩散模型，设计两套并行跨注意力通道，分别编码文本语义与风格信息，实现“语义控制+风格对齐”。2) PSRL：逐级自风格表征学习，假设同一图像不同区域风格一致、不同图像风格不同；通过风格对比损失拉近同图区域表征、拉远跨图表征，获得细粒度风格特征。3) 基准：汇总对比算法、定义风格度量指标、提供多数据集与设置，规范评价流程。

Result: 在所建基准上进行大量实验，NSD在风格一致性与语义匹配等指标上显著优于对比方法，并呈现更逼真的可视化结果。

Conclusion: NSD有效解决图像编辑/修复中的风格一致性与语义控制双目标问题；PSRL提供高辨识度风格表征；所构建基准为后续研究提供统一评测平台。

Abstract: Maintaining stylistic consistency is crucial for the cohesion and aesthetic
appeal of images, a fundamental requirement in effective image editing and
inpainting. However, existing methods primarily focus on the semantic control
of generated content, often neglecting the critical task of preserving this
consistency. In this work, we introduce the Neural Scene Designer (NSD), a
novel framework that enables photo-realistic manipulation of user-specified
scene regions while ensuring both semantic alignment with user intent and
stylistic consistency with the surrounding environment. NSD leverages an
advanced diffusion model, incorporating two parallel cross-attention mechanisms
that separately process text and style information to achieve the dual
objectives of semantic control and style consistency. To capture fine-grained
style representations, we propose the Progressive Self-style Representational
Learning (PSRL) module. This module is predicated on the intuitive premise that
different regions within a single image share a consistent style, whereas
regions from different images exhibit distinct styles. The PSRL module employs
a style contrastive loss that encourages high similarity between
representations from the same image while enforcing dissimilarity between those
from different images. Furthermore, to address the lack of standardized
evaluation protocols for this task, we establish a comprehensive benchmark.
This benchmark includes competing algorithms, dedicated style-related metrics,
and diverse datasets and settings to facilitate fair comparisons. Extensive
experiments conducted on our benchmark demonstrate the effectiveness of the
proposed framework.

</details>


### [131] [MILO: A Lightweight Perceptual Quality Metric for Image and Latent-Space Optimization](https://arxiv.org/abs/2509.01411)
*Uğur Çoğalan,Mojtaba Bemana,Karol Myszkowski,Hans-Peter Seidel,Colin Groth*

Main category: cs.CV

TL;DR: 提出MILO：轻量多尺度的感知型全参考图像质量评价指标，基于伪MOS训练，既提升FR-IQA基准表现又可作为图像与潜空间的感知损失，结合空间掩蔽与课程学习在扩散/生成管线中实现高效、感知一致的优化。


<details>
  <summary>Details</summary>
Motivation: 现有FR-IQA要么依赖大规模人工主观评分，成本高、难扩展；要么缺乏对视觉掩蔽等人类感知现象的建模，导致质量预测与人眼不一致，且难作为感知损失在生成任务中稳定高效优化。作者希望：1) 用无需人工标注的训练方式获得贴近人感知的指标；2) 在图像和潜空间中提供可用于实际优化的度量，提升如去噪、超分、人脸修复等任务的感知质量与效率。

Method: 1) 伪MOS监督：对多样图像施加可复现失真，使用一组近期考虑视觉掩蔽效果的指标集成打分，作为训练信号。2) MILO架构：轻量、分层多尺度特征提取，显式建模空间掩蔽，输出与人类感知一致的质量分数。3) 双域应用：既作为FR-IQA指标，也作为感知损失用于图像域与VAE潜空间。4) 稳定扩散中的掩蔽优化：对VAE编码特征施加空间掩蔽，先优化感知不敏感区域，再逐步转向失真更严重区域（课程学习），以提升效率与稳定性。

Result: - 在标准FR-IQA基准上优于现有指标（更高相关性/更低误差），且模型小、推理快，满足实时应用。- 作为感知损失用于去噪、超分、脸部修复等任务，带来显著感知质量提升，同时减少计算开销。- 在潜空间优化中，空间掩蔽+课程策略能更好对齐人类感知并提高收敛效率。

Conclusion: MILO同时是SOTA级别的全参考图像质量指标与可落地的感知优化工具。通过伪MOS训练与空间掩蔽建模，在图像与潜空间中实现感知一致、计算高效的优化，适用于扩散/生成管线中多种低层视觉任务。

Abstract: We present MILO (Metric for Image- and Latent-space Optimization), a
lightweight, multiscale, perceptual metric for full-reference image quality
assessment (FR-IQA). MILO is trained using pseudo-MOS (Mean Opinion Score)
supervision, in which reproducible distortions are applied to diverse images
and scored via an ensemble of recent quality metrics that account for visual
masking effects. This approach enables accurate learning without requiring
large-scale human-labeled datasets. Despite its compact architecture, MILO
outperforms existing metrics across standard FR-IQA benchmarks and offers fast
inference suitable for real-time applications. Beyond quality prediction, we
demonstrate the utility of MILO as a perceptual loss in both image and latent
domains. In particular, we show that spatial masking modeled by MILO, when
applied to latent representations from a VAE encoder within Stable Diffusion,
enables efficient and perceptually aligned optimization. By combining spatial
masking with a curriculum learning strategy, we first process perceptually less
relevant regions before progressively shifting the optimization to more
visually distorted areas. This strategy leads to significantly improved
performance in tasks like denoising, super-resolution, and face restoration,
while also reducing computational overhead. MILO thus functions as both a
state-of-the-art image quality metric and as a practical tool for perceptual
optimization in generative pipelines.

</details>


### [132] [Bangladeshi Street Food Calorie Estimation Using Improved YOLOv8 and Regression Model](https://arxiv.org/abs/2509.01415)
*Aparup Dhar,MD Tamim Hossain,Pritom Barua*

Main category: cs.CV

TL;DR: 提出面向孟加拉街头食品的自动热量估计系统，基于自建数据集与改进的YOLOv8实现更优分类/分割，并结合回归模型在热量预测上取得低误差与高R^2，接近实用。


<details>
  <summary>Details</summary>
Motivation: 现有自动热量追踪方法存在：输出热量常量化、难以识别多种食物、图像尺度与归一化处理不佳、偏向西式菜肴，导致对本地（如孟加拉）街头食品适配性差与准确度不足。

Method: 1) 构建覆盖孟加拉各地流行街头食品的多样化数据集；2) 在SOTA视觉模型YOLOv8基础上进行结构性改进（用于更优分类与实例分割），在仅略增计算量下提升性能；3) 将分割/识别结果输入机器学习回归模型进行热量估计。

Result: 改进的YOLOv8在分类与分割上优于基线；热量估计达到MAE 6.94、RMSE 11.03、R^2 96.0%，显示高精度；计算复杂度仅小幅上升。

Conclusion: 面向孟加拉街头食品的定制化端到端方案在真实场景中具备高效与准确性，可弥补通用方法对地域菜系的不足；未来或可扩展到更多菜系与更复杂就餐环境。

Abstract: As obesity rates continue to increase, automated calorie tracking has become
a vital tool for people seeking to maintain a healthy lifestyle or adhere to a
diet plan. Although numerous research efforts have addressed this issue,
existing approaches often face key limitations, such as providing only constant
caloric output, struggling with multiple food recognition challenges,
challenges in image scaling and normalization, and a predominant focus on
Western cuisines. In this paper, we propose a tailored solution that
specifically targets Bangladeshi street food. We first construct a diverse
dataset of popular street foods found across Bangladesh. Then, we develop a
refined calorie estimation system by modifying the state-of-the-art vision
model YOLOv8. Our modified model achieves superior classification and
segmentation results, with only a slight increase in computational complexity
compared to the base variant. Coupled with a machine learning regression model,
our system achieves an impressive 6.94 mean absolute error (MAE), 11.03 root
mean squared error (RMSE), and a 96.0% R^2 score in calorie estimation, making
it both highly effective and accurate for real-world food calorie calculations.

</details>


### [133] [InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information](https://arxiv.org/abs/2509.01421)
*Guohui Zhang,Jiangtong Tan,Linjiang Huang,Zhonghang Yuan,Naishan Zheng,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 论文提出InfoScale，一个面向扩散模型在不同分辨率（高/低）下统一提升生成质量的信息论视角框架，分别从卷积、注意力、初始噪声三处改造，解决跨尺度信息量差异带来的性能退化。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在测试分辨率与训练分辨率不一致时，生成质量明显下降；根因是不同分辨率对应的信息量与分布不同，现有组件（空洞卷积、注意力、初始噪声设定）未对信息量变化做自适应处理。

Method: 提出InfoScale框架，包含三模块：1）渐进频率补偿（PFC）：在高分辨率生成中补偿空洞卷积导致的高频信息损失；2）自适应信息聚合（AIA）：在低分辨率生成中更有效聚合信息，并在高分辨率下平衡局部与全局信息；3）噪声自适应（NA）：重分配初始噪声中的空间信息，使其与目标分辨率的信息分布对齐。方法可即插即用，适配多种扩散模型。

Result: 在多项实验证明下（未给出具体数据），InfoScale在变尺度图像生成任务上优于基线，特别是在高分辨率与低分辨率偏离训练尺度的情况下显著改善细节与一致性。

Conclusion: 跨尺度生成的核心是信息量与分布的变化。通过对卷积、注意力与初始噪声三处进行信息补偿、聚合自适应与分布对齐，InfoScale可作为通用插件提升扩散模型的变尺度鲁棒性与生成质量。

Abstract: Diffusion models (DMs) have become dominant in visual generation but suffer
performance drop when tested on resolutions that differ from the training
scale, whether lower or higher. In fact, the key challenge in generating
variable-scale images lies in the differing amounts of information across
resolutions, which requires information conversion procedures to be varied for
generating variable-scaled images. In this paper, we investigate the issues of
three critical aspects in DMs for a unified analysis in variable-scaled
generation: dilated convolution, attention mechanisms, and initial noise.
Specifically, 1) dilated convolution in DMs for the higher-resolution
generation loses high-frequency information. 2) Attention for variable-scaled
image generation struggles to adjust the information aggregation adaptively. 3)
The spatial distribution of information in the initial noise is misaligned with
variable-scaled image. To solve the above problems, we propose
\textbf{InfoScale}, an information-centric framework for variable-scaled image
generation by effectively utilizing information from three aspects
correspondingly. For information loss in 1), we introduce Progressive Frequency
Compensation module to compensate for high-frequency information lost by
dilated convolution in higher-resolution generation. For information
aggregation inflexibility in 2), we introduce Adaptive Information Aggregation
module to adaptively aggregate information in lower-resolution generation and
achieve an effective balance between local and global information in
higher-resolution generation. For information distribution misalignment in 3),
we design Noise Adaptation module to re-distribute information in initial noise
for variable-scaled generation. Our method is plug-and-play for DMs and
extensive experiments demonstrate the effectiveness in variable-scaled image
generation.

</details>


### [134] [Mamba-CNN: A Hybrid Architecture for Efficient and Accurate Facial Beauty Prediction](https://arxiv.org/abs/2509.01431)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The computational assessment of facial attractiveness, a challenging
subjective regression task, is dominated by architectures with a critical
trade-off: Convolutional Neural Networks (CNNs) offer efficiency but have
limited receptive fields, while Vision Transformers (ViTs) model global context
at a quadratic computational cost. To address this, we propose Mamba-CNN, a
novel and efficient hybrid architecture. Mamba-CNN integrates a lightweight,
Mamba-inspired State Space Model (SSM) gating mechanism into a hierarchical
convolutional backbone. This core innovation allows the network to dynamically
modulate feature maps and selectively emphasize salient facial features and
their long-range spatial relationships, mirroring human holistic perception
while maintaining computational efficiency. We conducted extensive experiments
on the widely-used SCUT-FBP5500 benchmark, where our model sets a new
state-of-the-art. Mamba-CNN achieves a Pearson Correlation (PC) of 0.9187, a
Mean Absolute Error (MAE) of 0.2022, and a Root Mean Square Error (RMSE) of
0.2610. Our findings validate the synergistic potential of combining CNNs with
selective SSMs and present a powerful new architectural paradigm for nuanced
visual understanding tasks.

</details>


### [135] [SoccerHigh: A Benchmark Dataset for Automatic Soccer Video Summarization](https://arxiv.org/abs/2509.01439)
*Artur Díaz-Juan,Coloma Ballester,Gloria Haro*

Main category: cs.CV

TL;DR: 提出并开源一个面向足球视频集锦生成的基准数据集与基线模型：237场西甲/法甲/意甲转播级比赛的镜头切分标注，附新评价指标；基线在测试集F1=0.3956。


<details>
  <summary>Details</summary>
Motivation: 体育集锦制作依赖人工筛选，耗时耗力；缺乏公开可复用的数据集限制了自动化方法的研究与公平比较，尤其在足球转播场景中。

Method: 1) 数据：基于SoccerNet转播视频，标注237场比赛的镜头边界，构建用于摘要/集锦生成的基准；2) 基线模型：为该任务设计的自动镜头选择与摘要生成模型（细节未在摘要中展开），在测试集报告F1；3) 评价：提出受目标摘要长度约束的新指标，使评估更客观、与实际集锦时长对齐。

Result: 基线模型在测试集取得F1=0.3956；新长度约束指标用于更公平评估生成内容质量。

Conclusion: 公开数据集、代码与新指标为足球视频摘要研究提供统一基准；基线给出起点但仍有提升空间，期待后续方法在该数据与指标上改进表现。

Abstract: Video summarization aims to extract key shots from longer videos to produce
concise and informative summaries. One of its most common applications is in
sports, where highlight reels capture the most important moments of a game,
along with notable reactions and specific contextual events. Automatic summary
generation can support video editors in the sports media industry by reducing
the time and effort required to identify key segments. However, the lack of
publicly available datasets poses a challenge in developing robust models for
sports highlight generation. In this paper, we address this gap by introducing
a curated dataset for soccer video summarization, designed to serve as a
benchmark for the task. The dataset includes shot boundaries for 237 matches
from the Spanish, French, and Italian leagues, using broadcast footage sourced
from the SoccerNet dataset. Alongside the dataset, we propose a baseline model
specifically designed for this task, which achieves an F1 score of 0.3956 in
the test set. Furthermore, we propose a new metric constrained by the length of
each target summary, enabling a more objective evaluation of the generated
content. The dataset and code are available at
https://ipcv.github.io/SoccerHigh/.

</details>


### [136] [Traces of Image Memorability in Vision Encoders: Activations, Attention Distributions and Autoencoder Losses](https://arxiv.org/abs/2509.01453)
*Ece Takmaz,Albert Gatt,Jakub Dotlacil*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Images vary in how memorable they are to humans. Inspired by findings from
cognitive science and computer vision, this paper explores the correlates of
image memorability in pretrained vision encoders, focusing on latent
activations, attention distributions, and the uniformity of image patches. We
find that these features correlate with memorability to some extent.
Additionally, we explore sparse autoencoder loss over the representations of
vision transformers as a proxy for memorability, which yields results
outperforming past methods using convolutional neural network representations.
Our results shed light on the relationship between model-internal features and
memorability. They show that some features are informative predictors of what
makes images memorable to humans.

</details>


### [137] [Im2Haircut: Single-view Strand-based Hair Reconstruction for Human Avatars](https://arxiv.org/abs/2509.01469)
*Vanessa Sklyarova,Egor Zakharov,Malte Prinzler,Giorgio Becherini,Michael J. Black,Justus Thies*

Main category: cs.CV

TL;DR: 提出一种结合全局先验与局部优化的单张图像三维头发重建方法，利用合成+真实数据训练的Transformer发型先验，并以高斯splatting执行重建，优于现有方法于细节方向、轮廓与背面一致性。


<details>
  <summary>Details</summary>
Motivation: 单目头发重建困难：几何复杂、发型多样、缺少真实标注；多视图/传统方法只能重建可见部分，内部结构缺失；现有先验多依赖手工合成数据，数量与质量受限。

Method: 1) 用合成数据训练Transformer先验以学习发型内部3D结构；2) 在训练中引入真实数据以学习外部可见结构；3) 推理时将该先验与局部优化结合；4) 采用Gaussian splatting实现从单张或多张图像的几何重建。

Result: 在定性与定量比较中，对头发细节方向、整体轮廓以及背面一致性均优于现有重建流程；可从单图生成结构连贯的头发；提供代码与更多结果。

Conclusion: 合成+真实数据协同训练的发型先验结合高斯splatting重建，有效补齐单目重建的内部结构缺失问题，在多项指标上取得领先，适用于单图/多图场景。

Abstract: We present a novel approach for 3D hair reconstruction from single
photographs based on a global hair prior combined with local optimization.
Capturing strand-based hair geometry from single photographs is challenging due
to the variety and geometric complexity of hairstyles and the lack of ground
truth training data. Classical reconstruction methods like multi-view stereo
only reconstruct the visible hair strands, missing the inner structure of
hairstyles and hampering realistic hair simulation. To address this, existing
methods leverage hairstyle priors trained on synthetic data. Such data,
however, is limited in both quantity and quality since it requires manual work
from skilled artists to model the 3D hairstyles and create near-photorealistic
renderings. To address this, we propose a novel approach that uses both, real
and synthetic data to learn an effective hairstyle prior. Specifically, we
train a transformer-based prior model on synthetic data to obtain knowledge of
the internal hairstyle geometry and introduce real data in the learning process
to model the outer structure. This training scheme is able to model the visible
hair strands depicted in an input image, while preserving the general 3D
structure of hairstyles. We exploit this prior to create a
Gaussian-splatting-based reconstruction method that creates hairstyles from one
or more images. Qualitative and quantitative comparisons with existing
reconstruction pipelines demonstrate the effectiveness and superior performance
of our method for capturing detailed hair orientation, overall silhouette, and
backside consistency. For additional results and code, please refer to
https://im2haircut.is.tue.mpg.de.

</details>


### [138] [PointSlice: Accurate and Efficient Slice-Based Representation for 3D Object Detection from Point Clouds](https://arxiv.org/abs/2509.01487)
*Liu Qifeng,Zhao Dawei,Dong Yabo,Xiao Liang,Wang Juan,Min Chen,Li Fuyang,Jiang Weizhong,Lu Dongming,Nie Yiming*

Main category: cs.CV

TL;DR: 提出PointSlice：将3D点云按水平切片为多组2D x–y切片，仅学习2D分布，并用Slice Interaction Network保持跨切片的垂直关系。在Waymo、nuScenes、Argoverse 2上兼顾速度、参数量与精度，部分对比优于/接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有体素法精度高但慢，pillar法快但精度不足；需要一种在不显著牺牲精度的前提下，进一步提升推理速度、降低参数量的点云3D检测方案。

Method: 1) 点云水平切片：将3D点云转成多组2D (x–y)切片，作为独立批次输入，网络仅学习2D分布以减小模型复杂度与提升速度；2) 切片交互网络SIN：在2D主干中引入跨切片信息交互模块，显式建模垂直(z向)关系，弥补仅用2D学习的3D感知不足；3) 定制检测网络：围绕上述表示设计高效检测头与训练流程。

Result: Waymo：较SOTA体素法SAFDNet推理1.13倍更快、参数0.79倍，精度仅-1.2 mAPH；nuScenes：达66.74 mAP（达SOTA）；Argoverse 2：1.10倍更快、参数0.66倍，精度仅-1.0 mAP。

Conclusion: PointSlice通过2D化切片与SIN跨切片交互，在维持较高3D检测精度的同时显著降低参数与提升速度，跨多数据集表现稳定，具备工程落地潜力。

Abstract: 3D object detection from point clouds plays a critical role in autonomous
driving. Currently, the primary methods for point cloud processing are
voxel-based and pillarbased approaches. Voxel-based methods offer high accuracy
through fine-grained spatial segmentation but suffer from slower inference
speeds. Pillar-based methods enhance inference speed but still fall short of
voxel-based methods in accuracy. To address these issues, we propose a novel
point cloud processing method, PointSlice, which slices point clouds along the
horizontal plane and includes a dedicated detection network. The main
contributions of PointSlice are: (1) A new point cloud processing technique
that converts 3D point clouds into multiple sets of 2D (x-y) data slices. The
model only learns 2D data distributions, treating the 3D point cloud as
separate batches of 2D data, which reduces the number of model parameters and
enhances inference speed; (2) The introduction of a Slice Interaction Network
(SIN). To maintain vertical relationships across slices, we incorporate SIN
into the 2D backbone network, which improves the model's 3D object perception
capability. Extensive experiments demonstrate that PointSlice achieves high
detection accuracy and inference speed. On the Waymo dataset, PointSlice is
1.13x faster and has 0.79x fewer parameters than the state-of-the-art
voxel-based method (SAFDNet), with only a 1.2 mAPH accuracy reduction. On the
nuScenes dataset, we achieve a state-of-the-art detection result of 66.74 mAP.
On the Argoverse 2 dataset, PointSlice is 1.10x faster, with 0.66x fewer
parameters and a 1.0 mAP accuracy reduction. The code will be available at
https://github.com/qifeng22/PointSlice2.

</details>


### [139] [A Continuous-Time Consistency Model for 3D Point Cloud Generation](https://arxiv.org/abs/2509.01492)
*Sebastian Eilermann,René Heesch,Oliver Niggemann*

Main category: cs.CV

TL;DR: 提出 ConTiCoM-3D：一种在连续时间上直接在点云空间生成3D形状的“一到两步”一致性模型，无需离散扩散步骤、教师模型或潜编码，在ShapeNet上达到或超越SOTA的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状生成多依赖迭代式扩散去噪或经由潜空间解码，推理慢、训练复杂（如需教师模型、JVP开销大），且点云高维度带来稳定性与计算成本问题。需要一种既快又准、可扩展、直接在点云空间操作的生成方法。

Method: 构建连续时间一致性模型：以时间条件网络直接在点空间生成；采用受TrigFlow启发的连续噪声日程以稳定训练；以Chamfer Distance为几何损失避免昂贵的Jacobian-vector products；整个流程不依赖预训练教师或潜空间编码，支持一到两步推理。

Result: 在ShapeNet基准上，ConTiCoM-3D在几何质量与生成效率上与现有扩散/潜一致性模型持平或更优，实现高保真且快速的点云生成。

Conclusion: 连续时间、一致性驱动、直接点空间建模是可扩展的3D形状生成新范式；ConTiCoM-3D在无需教师与潜空间的前提下，提供高效的一至两步推理与高几何精度，具备实际应用潜力。

Abstract: Fast and accurate 3D shape generation from point clouds is essential for
applications in robotics, AR/VR, and digital content creation. We introduce
ConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapes
directly in point space, without discretized diffusion steps, pre-trained
teacher models, or latent-space encodings. The method integrates a
TrigFlow-inspired continuous noise schedule with a Chamfer Distance-based
geometric loss, enabling stable training on high-dimensional point sets while
avoiding expensive Jacobian-vector products. This design supports efficient
one- to two-step inference with high geometric fidelity. In contrast to
previous approaches that rely on iterative denoising or latent decoders,
ConTiCoM-3D employs a time-conditioned neural network operating entirely in
continuous time, thereby achieving fast generation. Experiments on the ShapeNet
benchmark show that ConTiCoM-3D matches or outperforms state-of-the-art
diffusion and latent consistency models in both quality and efficiency,
establishing it as a practical framework for scalable 3D shape generation.

</details>


### [140] [MSA2-Net: Utilizing Self-Adaptive Convolution Module to Extract Multi-Scale Information in Medical Image Segmentation](https://arxiv.org/abs/2509.01498)
*Chao Deng,Xiaosen Li,Xiao Qin*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The nnUNet segmentation framework adeptly adjusts most hyperparameters in
training scripts automatically, but it overlooks the tuning of internal
hyperparameters within the segmentation network itself, which constrains the
model's ability to generalize. Addressing this limitation, this study presents
a novel Self-Adaptive Convolution Module that dynamically adjusts the size of
the convolution kernels depending on the unique fingerprints of different
datasets. This adjustment enables the MSA2-Net, when equipped with this module,
to proficiently capture both global and local features within the feature maps.
Self-Adaptive Convolution Module is strategically integrated into two key
components of the MSA2-Net: the Multi-Scale Convolution Bridge and the
Multi-Scale Amalgamation Decoder. In the MSConvBridge, the module enhances the
ability to refine outputs from various stages of the CSWin Transformer during
the skip connections, effectively eliminating redundant data that could
potentially impair the decoder's performance. Simultaneously, the MSADecoder,
utilizing the module, excels in capturing detailed information of organs
varying in size during the decoding phase. This capability ensures that the
decoder's output closely reproduces the intricate details within the feature
maps, thus yielding highly accurate segmentation images. MSA2-Net, bolstered by
this advanced architecture, has demonstrated exceptional performance, achieving
Dice coefficient scores of 86.49\%, 92.56\%, 93.37\%, and 92.98\% on the
Synapse, ACDC, Kvasir, and Skin Lesion Segmentation (ISIC2017) datasets,
respectively. This underscores MSA2-Net's robustness and precision in medical
image segmentation tasks across various datasets.

</details>


### [141] [Variation-aware Vision Token Dropping for Faster Large Vision-Language Models](https://arxiv.org/abs/2509.01552)
*Junjie Chen,Xuyang Liu,Zichen Wen,Yiyu Wang,Siteng Huang,Honggang Chen*

Main category: cs.CV

TL;DR: 提出V^2Drop：在LVLM推理中按层逐步丢弃“变化最小”的视觉token，以在几乎不损失性能的前提下显著加速并降显存。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像与长视频导致视觉token数暴涨，内置LLM压缩方法存在位置偏置与与高效算子不兼容的问题，影响真实部署效率，需要一种与任务无关、可与高效算子协同的压缩方式。

Method: 从“token变化”视角观察到视觉token在LLM中的变化幅度具有任务无关性。据此提出V^2Drop：在推理过程中度量各视觉token在层间或步间的表示变化（variation），优先、逐步丢弃变化最小的token；设计避免位置偏置的策略，并与高效算子配合；属于内-LLM的动态剪裁/压缩机制。

Result: 在多模型与多基准上，图像与视频任务分别保留原性能的94.0%与98.6%；同时将生成时延分别降低31.5%与74.2%。与高效算子结合后，进一步降低GPU峰值显存。

Conclusion: 基于变化感知的视觉token丢弃在不显著牺牲多模态理解性能的情况下显著提升LVLM推理效率，缓解了位置偏置与算子兼容性问题，具备实际部署价值。

Abstract: Large vision-language models (LVLMs) have demonstrated remarkable
capabilities in multimodal understanding tasks. However, the increasing demand
for high-resolution image and long-video understanding results in substantial
token counts, leading to reduced inference efficiency. Token compression offers
a direct solution by reducing the number of tokens to be processed, thereby
improving computational efficiency. Through extensive analysis, we identify two
critical limitations in existing inner-LLM token compression methods:
positional bias and incompatibility with efficient operators, which hinder
their practical deployment for LVLM acceleration. This paper presents the first
approach from a token variation perspective, revealing that visual token
variations within LLMs exhibit task-agnostic properties. We propose
Variation-aware Vision Token Dropping (\textit{i.e.}, \textbf{V$^2$Drop}),
which progressively removes visual tokens with minimal variation during LVLM
inference, thereby enhancing computational efficiency. Extensive experiments
across multiple models and benchmarks demonstrate that our V$^2$Drop is able to
maintain \textbf{94.0\%} and \textbf{98.6\%} of the original model performance
for image and video understanding tasks respectively, while reducing LLM
generation latency by \textbf{31.5\%} and \textbf{74.2\%}. When combined with
efficient operators, V$^2$Drop further reduces GPU peak memory usage.

</details>


### [142] [Unified Supervision For Vision-Language Modeling in 3D Computed Tomography](https://arxiv.org/abs/2509.01554)
*Hao-Chih Lee,Zelong Liu,Hamza Ahmed,Spencer Kim,Sean Huver,Vishwesh Nath,Zahi A. Fayad,Timothy Deyer,Xueyan Mei*

Main category: cs.CV

TL;DR: 提出Uniferum：统一分类与分割监督的三维CT视觉-语言模型，在CT-RATE上较CLIP类与传统多标签CNN AUROC提升7%，并在RAD-CHEST与INSPECT上表现出稳健OOD与零样本能力，指向更可靠、数据高效的3D医学影像VLM。


<details>
  <summary>Details</summary>
Motivation: 通用VLM虽具零样本优势，但在高风险的诊断放射学中判别力不足；同时公开三维CT数据稀缺且标注异质（分类标签与分割掩膜粒度不一），难以统一训练与临床可靠落地。

Method: 提出Uniferum，一个体素级（体积）VLM，将分类标签与分割掩膜等异构监督信号在单一训练框架中统一；对三个具有不同标注形式的公共3D CT数据进行“对齐/统一”；结合身体分割作为辅助，引导特征学习；采用VLM范式（类CLIP）扩展到3D体数据以实现零样本能力。

Result: 在CT-RATE基准上取得SOTA，AUROC较CLIP基和传统多标签卷积模型提升7%；在分布外数据集RAD-CHEST与INSPECT上表现稳健，并出现“意外”的零样本泛化迹象。

Conclusion: 统一异构标注（分类+分割）并融入身体分割可显著提升3D医学影像VLM的判别力与泛化，推动面向临床可靠、数据高效的三维放射学VLM方向。

Abstract: General-purpose vision-language models (VLMs) have emerged as promising tools
in radiology, offering zero-shot capabilities that mitigate the need for large
labeled datasets. However, in high-stakes domains like diagnostic radiology,
these models often lack the discriminative precision required for reliable
clinical use. This challenge is compounded by the scarcity and heterogeneity of
publicly available volumetric CT datasets, which vary widely in annotation
formats and granularity. To address these limitations, we introduce Uniferum, a
volumetric VLM that unifies diverse supervision signals, encoded in
classification labels and segmentation masks, into a single training framework.
By harmonizing three public 3D CT datasets with distinct annotations, Uniferum
achieves state-of-the-art performance, improving AUROC on the CT-RATE benchmark
by 7% compared to CLIP-based and conventional multi-label convolutional models.
The model demonstrates robust out-of-distribution generalization, with observed
evidence of unexpected zero-shot performance on the RAD-CHEST and INSPECT
datasets. Our results highlight the effectiveness of integrating heterogeneous
annotations and body segmentation to enhance model performance, setting a new
direction for clinically reliable, data-efficient VLMs in 3D medical imaging.

</details>


### [143] [Acoustic Interference Suppression in Ultrasound images for Real-Time HIFU Monitoring Using an Image-Based Latent Diffusion Model](https://arxiv.org/abs/2509.01557)
*Dejia Cai,Yao Ran,Kun Yang,Xinwang Shi,Yingying Zhou,Kexian Wu,Yang Xu,Yi Hu,Xiaowei Zhou*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: High-Intensity Focused Ultrasound (HIFU) is a non-invasive therapeutic
technique widely used for treating various diseases. However, the success and
safety of HIFU treatments depend on real-time monitoring, which is often
hindered by interference when using ultrasound to guide HIFU treatment. To
address these challenges, we developed HIFU-ILDiff, a novel deep learning-based
approach leveraging latent diffusion models to suppress HIFU-induced
interference in ultrasound images. The HIFU-ILDiff model employs a Vector
Quantized Variational Autoencoder (VQ-VAE) to encode noisy ultrasound images
into a lower-dimensional latent space, followed by a latent diffusion model
that iteratively removes interference. The denoised latent vectors are then
decoded to reconstruct high-resolution, interference-free ultrasound images. We
constructed a comprehensive dataset comprising 18,872 image pairs from in vitro
phantoms, ex vivo tissues, and in vivo animal data across multiple imaging
modalities and HIFU power levels to train and evaluate the model. Experimental
results demonstrate that HIFU-ILDiff significantly outperforms the commonly
used Notch Filter method, achieving a Structural Similarity Index (SSIM) of
0.796 and Peak Signal-to-Noise Ratio (PSNR) of 23.780 compared to SSIM of 0.443
and PSNR of 14.420 for the Notch Filter under in vitro scenarios. Additionally,
HIFU-ILDiff achieves real-time processing at 15 frames per second, markedly
faster than the Notch Filter's 5 seconds per frame. These findings indicate
that HIFU-ILDiff is able to denoise HIFU interference in ultrasound guiding
images for real-time monitoring during HIFU therapy, which will greatly improve
the treatment precision in current clinical applications.

</details>


### [144] [Kwai Keye-VL 1.5 Technical Report](https://arxiv.org/abs/2509.01563)
*Biao Yang,Bin Wen,Boyang Ding,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Guowang Zhang,Han Shen,Hao Peng,Haojie Ding,Hao Wang,Hengrui Ju,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Muhao Wei,Qiang Wang,Ruitao Wang,Sen Na,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zeyi Lu,Zhenhua Wu,Zhixin Ling,Zhuoran Yang,Ziming Li,Di Xu,Haixuan Gao,Hang Li,Jing Wang,Lejian Ren,Qigen Hu,Qianqian Wang,Shiyao Wang,Xinchen Luo,Yan Li,Yuhang Hu,Zixing Zhang*

Main category: cs.CV

TL;DR: 提出Keye-VL-1.5：通过慢-快视频编码、超长上下文预训练与强化对齐训练，显著提升视频理解能力，并兼顾通用多模态表现。


<details>
  <summary>Details</summary>
Motivation: 视频具有时空冗余与突发变化并存的特性，现有MLLM在处理视频时常面临空间分辨率与时间覆盖的权衡，难以在长视频中既看清细节又把握全局，同时推理与对齐（人类偏好）不足。

Method: 三大设计：1) Slow-Fast视频编码：基于帧间相似度自适应分配计算—变化大的关键帧用高分辨率“慢通道”，静态区段用低分辨率“快通道”以扩大时间覆盖；2) 渐进式四阶段预训练：上下文长度从8K逐步扩展到128K token，适配更长视频与更复杂视觉内容；3) 全面的后训练：以推理增强与人类偏好对齐为核心，包含5步CoT数据构造、基于GSPO的迭代强化学习（对困难样本逐步增加提示），以及最终对齐训练。

Result: 在公开基准与内部人工评测中，相比现有模型有显著提升，尤其在视频理解任务上表现突出，同时在通用多模态基准上保持竞争力。

Conclusion: 通过慢-快编码、超长上下文预训练与强化对齐的组合，Keye-VL-1.5有效缓解了视频理解中的分辨率-时长权衡，并提升长视频推理与人类偏好一致性，可作为面向视频场景的强健MLLM方案。

Abstract: In recent years, the development of Large Language Models (LLMs) has
significantly advanced, extending their capabilities to multimodal tasks
through Multimodal Large Language Models (MLLMs). However, video understanding
remains a challenging area due to the dynamic and information-dense nature of
videos. Existing models struggle with the trade-off between spatial resolution
and temporal coverage when processing video content. We present Keye-VL-1.5,
which addresses fundamental challenges in video comprehension through three key
innovations. First, we introduce a novel Slow-Fast video encoding strategy that
dynamically allocates computational resources based on inter-frame similarity,
processing key frames with significant visual changes at higher resolution
(Slow pathway) while handling relatively static frames with increased temporal
coverage at lower resolution (Fast pathway). Second, we implement a progressive
four-stage pre-training methodology that systematically extends the model's
context length from 8K to 128K tokens, enabling processing of longer videos and
more complex visual content. Third, we develop a comprehensive post-training
pipeline focusing on reasoning enhancement and human preference alignment,
incorporating a 5-step chain-of-thought data construction process, iterative
GSPO-based reinforcement learning with progressive prompt hinting for difficult
cases, and alignment training. Through extensive evaluation on public
benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates
significant improvements over existing models, particularly excelling in video
understanding tasks while maintaining competitive performance on general
multimodal benchmarks.

</details>


### [145] [ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association](https://arxiv.org/abs/2509.01584)
*Ganlin Zhang,Shenhan Qian,Xi Wang,Daniel Cremers*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We present ViSTA-SLAM as a real-time monocular visual SLAM system that
operates without requiring camera intrinsics, making it broadly applicable
across diverse camera setups. At its core, the system employs a lightweight
symmetric two-view association (STA) model as the frontend, which
simultaneously estimates relative camera poses and regresses local pointmaps
from only two RGB images. This design reduces model complexity significantly,
the size of our frontend is only 35\% that of comparable state-of-the-art
methods, while enhancing the quality of two-view constraints used in the
pipeline. In the backend, we construct a specially designed Sim(3) pose graph
that incorporates loop closures to address accumulated drift. Extensive
experiments demonstrate that our approach achieves superior performance in both
camera tracking and dense 3D reconstruction quality compared to current
methods. Github repository: https://github.com/zhangganlin/vista-slam

</details>


### [146] [O-DisCo-Edit: Object Distortion Control for Unified Realistic Video Editing](https://arxiv.org/abs/2509.01596)
*Yuqing Chen,Junjie Wang,Lin Liu,Ruihang Chu,Xiaopeng Zhang,Qi Tian,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出 O-DisCo-Edit：用统一的“对象畸变控制”(O-DisCo)信号与复制保持模块，实现多种视频编辑任务的可控、保真、统一训练与推理，性能超越 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散编辑方法对不同任务需要不同控制信号（如深度、光流、掩码、文本等），导致框架复杂、泛化弱、训练成本高，且难以精确控制对象属性（形状、外观、动作、风格等）。希望找到一个可统一表达多种编辑线索、同时保持未编辑区域稳定的控制方式。

Method: - 核心：O-DisCo（Object Distortion Control）。用“随机+自适应”噪声构造对象级畸变控制信号，在单一表示中编码多类编辑意图。该信号可对目标对象施加不同强度/形式的扰动，驱动扩散模型在对应维度上编辑。
- Copy-form preservation：在生成过程中复制未编辑区域的时空结构或特征，减少漂移与内容泄露，保证背景/非编辑对象的一致性与时间稳定性。
- 训练范式：以多任务统一数据/目标训练扩散视频编辑器，使其学会从 O-DisCo 信号解码到具体编辑效果；推理阶段仅需提供该控制信号即可完成不同编辑任务。

Result: 在多种视频编辑基准与任务上（对象替换、属性更改、风格化、动作/姿态调整等），与专用方法和多任务 SOTA 相比，O-DisCo-Edit在编辑保真度、时序一致性、未编辑区域保持、用户偏好等指标上全面领先；人主观评测也显著优于对比方法。

Conclusion: 一个统一、可控、资源高效的视频编辑框架。通过O-DisCo信号与复制保持模块，减少对多样控制通道与复杂训练的依赖，在多任务场景下实现高保真与稳健的编辑，并具备良好的泛化与实用价值。

Abstract: Diffusion models have recently advanced video editing, yet controllable
editing remains challenging due to the need for precise manipulation of diverse
object properties. Current methods require different control signal for diverse
editing tasks, which complicates model design and demands significant training
resources. To address this, we propose O-DisCo-Edit, a unified framework that
incorporates a novel object distortion control (O-DisCo). This signal, based on
random and adaptive noise, flexibly encapsulates a wide range of editing cues
within a single representation. Paired with a "copy-form" preservation module
for preserving non-edited regions, O-DisCo-Edit enables efficient,
high-fidelity editing through an effective training paradigm. Extensive
experiments and comprehensive human evaluations consistently demonstrate that
O-DisCo-Edit surpasses both specialized and multitask state-of-the-art methods
across various video editing tasks.
https://cyqii.github.io/O-DisCo-Edit.github.io/

</details>


### [147] [TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D Force Estimation in Catheterization](https://arxiv.org/abs/2509.01605)
*Pedram Fekri,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Recently, the emergence of multitask deep learning models has enhanced
catheterization procedures by providing tactile and visual perception data
through an end-to-end architec- ture. This information is derived from a
segmentation and force estimation head, which localizes the catheter in X-ray
images and estimates the applied pressure based on its deflection within the
image. These stereo vision architectures incorporate a CNN- based
encoder-decoder that captures the dependencies between X-ray images from two
viewpoints, enabling simultaneous 3D force estimation and stereo segmentation
of the catheter. With these tasks in mind, this work approaches the problem
from a new perspective. We propose a novel encoder-decoder Vision Transformer
model that processes two input X-ray images as separate sequences. Given
sequences of X-ray patches from two perspectives, the transformer captures
long-range dependencies without the need to gradually expand the receptive
field for either image. The embeddings generated by both the encoder and
decoder are fed into two shared segmentation heads, while a regression head
employs the fused information from the decoder for 3D force estimation. The
proposed model is a stereo Vision Transformer capable of simultaneously
segmenting the catheter from two angles while estimating the generated forces
at its tip in 3D. This model has undergone extensive experiments on synthetic
X-ray images with various noise levels and has been compared against
state-of-the-art pure segmentation models, vision-based catheter force
estimation methods, and a multitask catheter segmentation and force estimation
approach. It outperforms existing models, setting a new state-of-the-art in
both catheter segmentation and force estimation.

</details>


### [148] [Improving Large Vision and Language Models by Learning from a Panel of Peers](https://arxiv.org/abs/2509.01610)
*Jefferson Hernandez,Jing Shi,Simon Jenni,Vicente Ordonez,Kushal Kafle*

Main category: cs.CV

TL;DR: 提出“同侪小组（Panel-of-Peers）”框架，用多个LVLM互评与迭代自改进，减少对人工偏好数据的依赖，并在15项基准上把平均分从48%提升到57%。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM对齐依赖人工偏好数据，代价高；机器生成偏好数据质量欠佳；自监督偏好会引入幻觉。需要一种既可扩展又能保持质量的对齐方式。

Method: 构建由多个LVLM组成的“同侪面板”，对给定精选提示：模型先生成答案，再彼此评审（打分/反馈），基于集体评估进行迭代改写与学习，模拟人类的同行评议与课堂互评流程；整个过程减少或不依赖人工标注，通过多轮迭代实现自我提升。

Result: 在多项公开基准上显著提升，无需大量人工标签；具体地，15个基准的平均分由48%提升到57%，显示同侪评审可成为可扩展的自监督对齐替代方案。

Conclusion: 同侪式评审与迭代学习能有效对齐并提升LVLM性能，是低人工成本、可扩展的对齐策略；其在多基准上的显著增益证明了该框架的潜力。

Abstract: Traditional alignment methods for Large Vision and Language Models (LVLMs)
primarily rely on human-curated preference data. Human-generated preference
data is costly; machine-generated preference data is limited in quality; and
self-supervised preference data often introduces hallucinations. To overcome
these limitations, we propose a novel Panel-of-Peers learning framework
inspired by collaborative learning among humans. This approach leverages a
panel of LVLMs, each evaluating and learning from their collective outputs
through an iterative self-improvement process. By simulating a peer review
system, our models generate, assess, and refine outputs in response to a
curated set of prompts, mimicking a classroom learning environment. We
demonstrate that this methodology enhances model performance without requiring
extensive human-labeled datasets. Our experiments show significant improvement
across multiple benchmarks, demonstrating the potential of peer evaluations as
a scalable alternative to self-supervised alignment. Notably, we show that
Panel-of-Peers increases the average score on fifteen benchmarks from 48% to
57%

</details>


### [149] [Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling](https://arxiv.org/abs/2509.01624)
*Natalia Frumkin,Diana Marculescu*

Main category: cs.CV

TL;DR: 提出Q-Sched：通过修改扩散调度器而非权重量化，实现少步采样下的高保真生成，在不做全精度校准的情况下把模型体积缩小4倍且保持全精度质量，并在多基线之上显著提升FID和用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有文生图扩散模型推理成本高，即便少步模型仍依赖庞大未压缩骨干，难以在非数据中心GPU上全精度推理；传统后训练量化需要全精度校准，代价高且受限。因此需要一种无需全精度校准、与少步蒸馏兼容、能显著压缩模型且不牺牲质量的方法。

Method: 提出Q-Sched：不改权重量化，而是调整扩散采样调度器的轨迹与预调系数，使量化误差被调度补偿。核心是学习量化感知的预条件系数；为此设计JAQ损失，将文本-图像一致性与图像质量度量结合，且是无参考的，只需少量校准提示词，无需全精度推理。方法可与2–8步少步采样模型（LCM、PCM等）结合。

Result: 在FLUX.1[schnell]和SDXL-Turbo上，Q-Sched在4×模型压缩条件下达到接近/等同全精度质量；相较FP16基线：对4步LCM提升FID 15.5%，对8步PCM提升16.6%。大规模用户研究（>80,000条标注）进一步验证了主观质量与偏好优势。

Conclusion: 量化与少步蒸馏并非冲突，Q-Sched通过调度层面的量化感知校正，使少步扩散在受限硬件上实现高保真、低延迟推理，减少对全精度校准与数据中心GPU的依赖，提供了一条实用的后训练量化新范式。

Abstract: Text-to-image diffusion models are computationally intensive, often requiring
dozens of forward passes through large transformer backbones. For instance,
Stable Diffusion XL generates high-quality images with 50 evaluations of a
2.6B-parameter model, an expensive process even for a single batch. Few-step
diffusion models reduce this cost to 2-8 denoising steps but still depend on
large, uncompressed U-Net or diffusion transformer backbones, which are often
too costly for full-precision inference without datacenter GPUs. These
requirements also limit existing post-training quantization methods that rely
on full-precision calibration. We introduce Q-Sched, a new paradigm for
post-training quantization that modifies the diffusion model scheduler rather
than model weights. By adjusting the few-step sampling trajectory, Q-Sched
achieves full-precision accuracy with a 4x reduction in model size. To learn
quantization-aware pre-conditioning coefficients, we propose the JAQ loss,
which combines text-image compatibility with an image quality metric for
fine-grained optimization. JAQ is reference-free and requires only a handful of
calibration prompts, avoiding full-precision inference during calibration.
Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16
4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step
Phased Consistency Model, showing that quantization and few-step distillation
are complementary for high-fidelity generation. A large-scale user study with
more than 80,000 annotations further confirms Q-Sched's effectiveness on both
FLUX.1[schnell] and SDXL-Turbo.

</details>


### [150] [OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning](https://arxiv.org/abs/2509.01644)
*Yanqing Liu,Xianhang Li,Letian Zhang,Zirui Wang,Zeyu Zheng,Yuyin Zhou,Cihang Xie*

Main category: cs.CV

TL;DR: 提出OpenVision 2：去掉文本编码器与对比学习，仅保留生成式caption损失，训练更快更省显存且性能与原版相当。


<details>
  <summary>Details</summary>
Motivation: 在多模态预训练中，传统架构（如CapPa、AIMv2、LLaVA）往往同时采用文本编码器与对比损失+生成损失，带来较高的训练成本。作者希望在不牺牲性能的情况下，显著提升训练效率与可扩展性。

Method: 简化架构：删除文本编码器与对比损失，仅使用caption生成损失作为训练信号；保持视觉编码器（如ViT-L/14）与语言解码端的生成式训练；通过增大批次与更高效的内存占用实现规模扩展至>10亿参数。

Result: 在多模态基准上与原OpenVision性能相当；训练时间由83h降至57h（约1.5倍加速），显存占用由24.5GB降至13.8GB（约1.8倍节省），最大batch从2k增至8k；支持比原版更大的视觉编码器规模。

Conclusion: 纯生成式（去对比、去文本编码器）的轻量范式在多模态基础模型中具有强竞争力：以更低资源达成相当性能，并为更大规模的视觉编码器训练铺路。

Abstract: This paper provides a simplification on OpenVision's architecture and loss
design for enhancing its training efficiency. Following the prior
vision-language pretraining works CapPa and AIMv2, as well as modern multimodal
designs like LLaVA, our changes are straightforward: we remove the text encoder
(and therefore the contrastive loss), retaining only the captioning loss as a
purely generative training signal. We name this new version OpenVision 2. The
initial results are promising: despite this simplification, OpenVision 2
competitively matches the original model's performance on a broad set of
multimodal benchmarks while substantially cutting both training time and memory
consumption. For example, with ViT-L/14, it reduces training time by about 1.5x
(from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB,
equivalently allowing the maximum batch size to grow from 2k to 8k). This
superior training efficiency also allows us to scale far beyond the largest
vision encoder used in OpenVision, reaching more than 1 billion parameters. We
hold a strong belief that this lightweight, generative-only paradigm is
compelling for future vision encoder development in multimodal foundation
models.

</details>


### [151] [Reinforced Visual Perception with Tools](https://arxiv.org/abs/2509.01656)
*Zetong Zhou,Dongping Chen,Zixian Ma,Zhihan Hu,Mingyang Fu,Sinan Wang,Yao Wan,Zhou Zhao,Ranjay Krishna*

Main category: cs.CV

TL;DR: 提出ReVPT，用强化学习训练多模态LLM更好地调用视觉工具进行推理，在多个感知为主的基准上达SOTA，显著优于监督微调与纯文本RL基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理依赖LLM+视觉模型的监督微调，存在数据生成昂贵、需精细数据清洗且泛化性差等问题；需要能低成本、可泛化地让模型学会“如何使用视觉工具”进行复杂推理。

Method: 提出基于GRPO改造的RL算法，面向视觉工具使用的策略优化：让多模态LLM与四种视觉工具交互，通过奖励信号强化正确的工具选择、调用顺序与推理步骤；在多基准上训练/评估，并做大量消融以分析各组件贡献。

Result: 在SAT、CV-Bench、BLINK、MMStar等感知密集型基准上获得SOTA，显著超越监督微调和文本RL微调基线；在CV-Bench上，ReVPT-3B与ReVPT-7B分别比对应指令模型高9.03%与9.44%。

Conclusion: RL驱动的视觉工具使用能有效提升多模态LLM的视觉推理能力，较监督微调更具效率与泛化性；提出的ReVPT与基于GRPO的训练方案为视觉工具化推理提供可复用框架与实践要点，代码已开源。

Abstract: Visual reasoning, a cornerstone of human intelligence, encompasses complex
perceptual and logical processes essential for solving diverse visual problems.
While advances in computer vision have produced powerful models for various
perceptual tasks, leveraging these for general visual reasoning remains
challenging. Prior work demonstrates that augmenting LLMs with vision models
via supervised finetuning improves performance, but faces key limitations such
as expensive data generation, reliance on careful data filtering, and poor
generalization. To address these issues, we propose ReVPT to enhance
multi-modal LLMs' abilities to reason about and use visual tools through
reinforcement learning. We introduce a novel RL algorithm based on GRPO,
designed to train models to reason with a suite of four visual tools. Through
extensive experiments, we show that our method achieves state-of-the-art
performance on several perception-heavy benchmarks, including SAT, CV-Bench,
BLINK and MMStar, significantly outperforming the supervised and text-based RL
finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the
instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the
community new insights on RL-based visual tool-usage through extensive
ablations. Our code is available at https://github.com/ls-kelvin/REVPT.

</details>


### [152] [GaussianGAN: Real-Time Photorealistic controllable Human Avatars](https://arxiv.org/abs/2509.01681)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Photorealistic and controllable human avatars have gained popularity in the
research community thanks to rapid advances in neural rendering, providing fast
and realistic synthesis tools. However, a limitation of current solutions is
the presence of noticeable blurring. To solve this problem, we propose
GaussianGAN, an animatable avatar approach developed for photorealistic
rendering of people in real-time. We introduce a novel Gaussian splatting
densification strategy to build Gaussian points from the surface of cylindrical
structures around estimated skeletal limbs. Given the camera calibration, we
render an accurate semantic segmentation with our novel view segmentation
module. Finally, a UNet generator uses the rendered Gaussian splatting features
and the segmentation maps to create photorealistic digital avatars. Our method
runs in real-time with a rendering speed of 79 FPS. It outperforms previous
methods regarding visual perception and quality, achieving a state-of-the-art
results in terms of a pixel fidelity of 32.94db on the ZJU Mocap dataset and
33.39db on the Thuman4 dataset.

</details>


### [153] [Examination of PCA Utilisation for Multilabel Classifier of Multispectral Images](https://arxiv.org/abs/2509.01691)
*Filip Karpowicz,Wiktor Kępiński,Bartosz Staszyński,Grzegorz Sarwas*

Main category: cs.CV

TL;DR: 研究多标签多光谱图像分类中PCA的效用：在ResNet50与DINOv2管线中加入可选PCA降到3维，再接三层分类器；结论指PCA效果高度依赖架构与训练策略。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像维度高、处理开销大；多标签任务进一步增加特征提取难度。作者想评估在现代深度学习框架下，PCA是否能在不显著损失信息的情况下缓解维度与计算负担。

Method: 构建两条深度模型管线（ResNet50、DINOv2），在输入到三层分类头前加入“可选”的PCA步骤将特征/数据降至3维，并进行多标签训练与评估；比较不同架构与训练策略下是否使用PCA的表现差异。

Result: PCA对性能的影响不一致：在不同架构（ResNet50 vs DINOv2）与训练策略下，是否使用PCA会导致不同的分类效果；说明PCA并非普适增益，效果强依赖上下文选择。

Conclusion: PCA在多标签多光谱分类中的有效性取决于模型与训练方案。未来可探索自监督预训练、更合适的降维方法（替代或补充PCA），以在保持性能的同时降低维度与计算成本。

Abstract: This paper investigates the utility of Principal Component Analysis (PCA) for
multi-label classification of multispectral images using ResNet50 and DINOv2,
acknowledging the high dimensionality of such data and the associated
processing challenges. Multi-label classification, where each image may belong
to multiple classes, adds further complexity to feature extraction. Our
pipeline includes an optional PCA step that reduces the data to three
dimensions before feeding it into a three-layer classifier. The findings
demonstrate that the effectiveness of PCA for multi-label multispectral image
classification depends strongly on the chosen deep learning architecture and
training strategy, opening avenues for future research into self-supervised
pre-training and alternative dimensionality reduction approaches.

</details>


### [154] [Deep Learning-Based Rock Particulate Classification Using Attention-Enhanced ConvNeXt](https://arxiv.org/abs/2509.01704)
*Anthony Amankwah,Chris Aldrich*

Main category: cs.CV

TL;DR: 提出基于ConvNeXt并融合自注意力与通道注意力的CNSCA模型，用于岩石尺寸精细分类，在公开数据集上优于多种强基线。


<details>
  <summary>Details</summary>
Motivation: 岩石尺寸精确分类关系到地质工程、采矿与资源管理中的效率与安全；现有方法对自然纹理的细粒度区分与全局上下文建模不足，导致精度与鲁棒性受限。

Method: 在ConvNeXt主干上引入两类注意力：1) 自注意力用于捕获长程空间依赖与全局上下文；2) 通道注意力用于突出判别性特征通道。该混合注意力设计兼顾局部细节与全局关系。模型在岩石图像尺寸分类数据集上训练并与三种强基线比较。

Result: 与三种强基线相比，加入自注意力与通道注意力后，分类精度提升（具体数值未给出），在细粒度、自然纹理场景下表现更好且更稳健。

Conclusion: 混合注意力增强的ConvNeXt（CNSCA）能更有效地处理岩石尺寸的细粒度分类任务；注意力机制对自然纹理类任务具有显著增益，适合在地质与采矿场景推广应用。

Abstract: Accurate classification of rock sizes is a vital component in geotechnical
engineering, mining, and resource management, where precise estimation
influences operational efficiency and safety. In this paper, we propose an
enhanced deep learning model based on the ConvNeXt architecture, augmented with
both self-attention and channel attention mechanisms. Building upon the
foundation of ConvNext, our proposed model, termed CNSCA, introduces
self-attention to capture long-range spatial dependencies and channel attention
to emphasize informative feature channels. This hybrid design enables the model
to effectively capture both fine-grained local patterns and broader contextual
relationships within rock imagery, leading to improved classification accuracy
and robustness. We evaluate our model on a rock size classification dataset and
compare it against three strong baseline. The results demonstrate that the
incorporation of attention mechanisms significantly enhances the models
capability for fine-grained classification tasks involving natural textures
like rocks.

</details>


### [155] [Clinical Metadata Guided Limited-Angle CT Image Reconstruction](https://arxiv.org/abs/2509.01752)
*Yu Shi,Shuyi Fan,Changsheng Fang,Shuo Han,Haodong Li,Li Zhou,Bahareh Morovati,Dayang Wang,Hengyong Yu*

Main category: cs.CV

TL;DR: 提出一个由临床结构化元数据引导的两阶段扩散模型，用于解决心脏有限角CT重建中的伪影与不适定问题，并在每步采样中加入基于ADMM的物理一致性；在合成与真实数据上优于无元数据基线。


<details>
  <summary>Details</summary>
Motivation: 有限角CT可降低剂量、提升时间分辨率，但视角截断导致强伪影与信息缺失，传统重建与纯图像先验难以稳健恢复。临床检查中天然存在的丰富结构化元数据（采集参数、人口学、诊断印象）尚未被系统利用，可能提供补充先验，缓解不适定性并提升效率。

Method: 两阶段、元数据引导的扩散重建框架：Stage-1 仅以元数据为条件，通过Transformer扩散从噪声生成粗解剖先验；Stage-2 以粗先验+元数据联合条件，细化重建得到高保真图像。两阶段的每个采样步均嵌入ADMM数据一致性模块，强制与投影数据物理匹配。还进行消融分析不同元数据类型（采集参数、人口学、诊断）贡献。

Result: 在合成与真实心脏CT数据集上，相比不使用元数据的基线方法，在SSIM、PSNR、nMI、PCC等指标显著提升，尤其在严重角度截断时优势更大；消融显示不同元数据提供互补增益，诊断与人口学在有限角条件下尤为关键。

Conclusion: 结构化临床元数据既提升重建质量又提高效率；两阶段扩散+ADMM的物理一致性设计有效缓解LACT不适定性。建议未来在医学成像中系统纳入元数据以实现更稳健、准确的重建。

Abstract: Limited-angle computed tomography (LACT) offers improved temporal resolution
and reduced radiation dose for cardiac imaging, but suffers from severe
artifacts due to truncated projections. To address the ill-posedness of LACT
reconstruction, we propose a two-stage diffusion framework guided by structured
clinical metadata. In the first stage, a transformer-based diffusion model
conditioned exclusively on metadata, including acquisition parameters, patient
demographics, and diagnostic impressions, generates coarse anatomical priors
from noise. The second stage further refines the images by integrating both the
coarse prior and metadata to produce high-fidelity results. Physics-based data
consistency is enforced at each sampling step in both stages using an
Alternating Direction Method of Multipliers module, ensuring alignment with the
measured projections. Extensive experiments on both synthetic and real cardiac
CT datasets demonstrate that incorporating metadata significantly improves
reconstruction fidelity, particularly under severe angular truncation. Compared
to existing metadata-free baselines, our method achieves superior performance
in SSIM, PSNR, nMI, and PCC. Ablation studies confirm that different types of
metadata contribute complementary benefits, particularly diagnostic and
demographic priors under limited-angle conditions. These findings highlight the
dual role of clinical metadata in improving both reconstruction quality and
efficiency, supporting their integration into future metadata-guided medical
imaging frameworks.

</details>


### [156] [TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing](https://arxiv.org/abs/2509.01754)
*Mohsen Asghari Ilani,Yaser Mike Banad*

Main category: cs.CV

TL;DR: 提出TransMatch，用迁移学习+半监督小样本学习识别LPBF增材制造表面缺陷；在8,284张图上达98.91%准确率，多类缺陷精确召回均高。


<details>
  <summary>Details</summary>
Motivation: LPBF表面缺陷威胁结构可靠性，但缺陷数据标注昂贵稀缺，现有元学习方法对新类和小样本泛化受限，难以充分利用未标注数据。

Method: 构建TransMatch框架：结合预训练迁移学习获取通用特征表示，并引入半监督few-shot机制，在有少量标注的新类样本同时利用大量未标注图像进行一致性/伪标签等学习，从而增强新类识别能力。

Result: 在包含8,284张缺陷图的Surface Defects数据集上，整体准确率98.91%，损失低；多类（裂纹、针孔、孔洞、飞溅等）均取得高Precision/Recall/F1，表现稳定鲁棒。

Conclusion: TransMatch有效缓解标注稀缺下的缺陷检测难题，能准确识别多种LPBF表面缺陷，具备可扩展性与工业应用潜力，可用于品质保障与可靠性提升。

Abstract: Surface defects in Laser Powder Bed Fusion (LPBF) pose significant risks to
the structural integrity of additively manufactured components. This paper
introduces TransMatch, a novel framework that merges transfer learning and
semi-supervised few-shot learning to address the scarcity of labeled AM defect
data. By effectively leveraging both labeled and unlabeled novel-class images,
TransMatch circumvents the limitations of previous meta-learning approaches.
Experimental evaluations on a Surface Defects dataset of 8,284 images
demonstrate the efficacy of TransMatch, achieving 98.91% accuracy with minimal
loss, alongside high precision, recall, and F1-scores for multiple defect
classes. These findings underscore its robustness in accurately identifying
diverse defects, such as cracks, pinholes, holes, and spatter. TransMatch thus
represents a significant leap forward in additive manufacturing defect
detection, offering a practical and scalable solution for quality assurance and
reliability across a wide range of industrial applications.

</details>


### [157] [Mixture of Balanced Information Bottlenecks for Long-Tailed Visual Recognition](https://arxiv.org/abs/2509.01804)
*Yifan Lan,Xin Cai,Jun Cheng,Shan Tan*

Main category: cs.CV

TL;DR: 提出平衡信息瓶颈（BIB）与其多路混合结构（MBIB），将损失再平衡与自蒸馏融入信息瓶颈框架，面向长尾视觉识别取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实视觉数据长尾分布导致类别不平衡，标准DNN在大规模均衡数据上表现好，但在长尾上学习到的表示偏向头部类、泛化差。传统信息瓶颈(IB)可学到与标签相关的紧致表示，但未考虑类别不平衡与层间信息整合问题。作者希望在IB框架下兼顾“保留判别信息”和“类别公平”，提升长尾识别性能并实现端到端表征-分类联合优化。

Method: 1) BIB：在IB目标中引入两点改造：a) 损失函数再平衡（如按类别频次调整权重/重采样思想的等价损失），强化尾部类监督；b) 自蒸馏，将教师（同网络历史或深层）知识引入，使表示在压缩同时保留更多与标签相关的关键信息。整体仍遵循IB的最小化I(X;Z)并最大化I(Z;Y)的思想，但以再平衡策略和蒸馏约束实现。2) MBIB：构建“多路BIB混合体”，由多个BIB分支分别融合不同网络层（多尺度/多语义）信息，通过门控/加权（混合）机制组合，形成端到端的表征与分类联合学习，从信息论角度提升充足性与鲁棒性。

Result: 在CIFAR100-LT、ImageNet-LT、iNaturalist 2018上，BIB与MBIB均达到或超过现有SOTA，显示对尾部类与整体精度的显著提升（摘要未给数值，但强调全面领先）。

Conclusion: 将再平衡与自蒸馏嵌入IB能在长尾场景下学到“充分且公平”的表示；多BIB混合（MBIB）进一步整合多层知识，实现端到端优化并带来SOTA性能，验证信息论视角下的长尾表征学习有效。

Abstract: Deep neural networks (DNNs) have achieved significant success in various
applications with large-scale and balanced data. However, data in real-world
visual recognition are usually long-tailed, bringing challenges to efficient
training and deployment of DNNs. Information bottleneck (IB) is an elegant
approach for representation learning. In this paper, we propose a balanced
information bottleneck (BIB) approach, in which loss function re-balancing and
self-distillation techniques are integrated into the original IB network. BIB
is thus capable of learning a sufficient representation with essential
label-related information fully preserved for long-tailed visual recognition.
To further enhance the representation learning capability, we also propose a
novel structure of mixture of multiple balanced information bottlenecks (MBIB),
where different BIBs are responsible for combining knowledge from different
network layers. MBIB facilitates an end-to-end learning strategy that trains
representation and classification simultaneously from an information theory
perspective. We conduct experiments on commonly used long-tailed datasets,
including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. Both BIB and MBIB
reach state-of-the-art performance for long-tailed visual recognition.

</details>


### [158] [PractiLight: Practical Light Control Using Foundational Diffusion Models](https://arxiv.org/abs/2509.01837)
*Yotam Erel,Rishabh Dabral,Vladislav Golyanik,Amit H. Bermano,Christian Theobalt*

Main category: cs.CV

TL;DR: PractiLight 提出一种无需大规模特定数据、以生成模型的基础能力为核心的实用图像控光方法：把光照关系映射为自注意力中的token交互，并用轻量LoRA回归出直接辐照度图，再以分类器指导在扩散生成过程中注入目标光照，实现高质量、强泛化的重光照控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像光照控制多依赖庞大且领域特定的数据集训练，导致泛化差、成本高，且难以在多种场景/频段实现稳定控制。作者希望利用“基础模型已学到的通用视觉—物理知识”来进行可迁移、数据高效、参数高效的实用重光照。

Method: 1) 观察：图像中的光照关系与自注意力层中的token交互具有相似结构，且扩散早期步对光照成因关键。2) 训练：以少量训练图像，训练轻量LoRA回归器，预测输入图像的直接辐照度图。3) 注入：在生成另一张图像时，将回归到的目标辐照度通过Classifier Guidance并结合早期扩散迭代注入，引导模型在生成过程中实现目标光照。4) 设计强调通用性与效率：少量数据、少量参数即可适配多域图像。

Result: 在多种场景与条件下，相较主流方法实现更优的图像质量与光照可控性；在参数量与训练数据规模上均更高效，展现更强的跨域泛化能力，达到SOTA水平。

Conclusion: 通过把光照建模为自注意力中的交互，并以LoRA回归辐照度、再用分类器指导在扩散早期注入光照，PractiLight在无需大规模特定数据的前提下，实现实用且通用的图像重光照控制，验证了可借助基础生成模型的内在知识解决光照控制问题。

Abstract: Light control in generated images is a difficult task, posing specific
challenges, spanning over the entire image and frequency spectrum. Most
approaches tackle this problem by training on extensive yet domain-specific
datasets, limiting the inherent generalization and applicability of the
foundational backbones used. Instead, PractiLight is a practical approach,
effectively leveraging foundational understanding of recent generative models
for the task. Our key insight is that lighting relationships in an image are
similar in nature to token interaction in self-attention layers, and hence are
best represented there. Based on this and other analyses regarding the
importance of early diffusion iterations, PractiLight trains a lightweight LoRA
regressor to produce the direct irradiance map for a given image, using a small
set of training images. We then employ this regressor to incorporate the
desired lighting into the generation process of another image using Classifier
Guidance. This careful design generalizes well to diverse conditions and image
domains. We demonstrate state-of-the-art performance in terms of quality and
control with proven parameter and data efficiency compared to leading works
over a wide variety of scenes types. We hope this work affirms that image
lighting can feasibly be controlled by tapping into foundational knowledge,
enabling practical and general relighting.

</details>


### [159] [Latent Gene Diffusion for Spatial Transcriptomics Completion](https://arxiv.org/abs/2509.01864)
*Paula Cárdenas,Leonardo Manrique,Daniela Vega,Daniela Ruiz,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 提出LGDiST：首个无需外部参考的潜在基因扩散模型，用于修复空间转录组(ST)数据的缺失；在26个数据集上较SOTA平均MSE降低18%，并能将下游图像驱动的基因表达预测方法的MSE再降至多10%。关键在于利用“上下文基因”构建生物学意义的潜在空间并结合邻域条件；任何组件移除都会显著退化。


<details>
  <summary>Details</summary>
Motivation: 现有由病理图像预测空间基因表达的方法受ST数据缺失(dropout)严重影响。多数方法依赖scRNA-seq参考进行对齐与补全，导致对外部数据和批次效应敏感，并可能继承scRNA的缺失。需要一种不依赖外部参考、能在原始ST域内稳健补全的方案。

Method: 提出LGDiST：参考无关的潜在基因扩散模型。核心做法包括：1) 将“上下文基因”（以往被视为信息弱）纳入以学习丰富且生物学有意义的潜在基因空间(ST latent space)；2) 在该潜在空间中进行扩散/去噪以补全缺失基因表达；3) 邻域条件(neighbor conditioning)融入空间结构信息；4) 通过消融实验验证各组件作用。

Result: 在26个数据集上，LGDiST用于基因表达补全的平均MSE较前SOTA降低18%。将LGDiST补全后的ST数据输入6种SOTA图像到基因预测模型，可进一步将其MSE提升（降低）最多10%。消融显示去除上下文基因、ST潜在空间或邻域条件均导致显著性能下降。

Conclusion: LGDiST有效缓解ST数据缺失问题，且无需scRNA参考，避免对齐和批次效应风险。其完整架构（上下文基因+潜在空间+邻域条件+扩散）是性能关键，显著优于各独立组件，并能普遍提升下游基因预测任务。

Abstract: Computer Vision has proven to be a powerful tool for analyzing Spatial
Transcriptomics (ST) data. However, current models that predict spatially
resolved gene expression from histopathology images suffer from significant
limitations due to data dropout. Most existing approaches rely on single-cell
RNA sequencing references, making them dependent on alignment quality and
external datasets while also risking batch effects and inherited dropout. In
this paper, we address these limitations by introducing LGDiST, the first
reference-free latent gene diffusion model for ST data dropout. We show that
LGDiST outperforms the previous state-of-the-art in gene expression completion,
with an average Mean Squared Error that is 18% lower across 26 datasets.
Furthermore, we demonstrate that completing ST data with LGDiST improves gene
expression prediction performance on six state-of-the-art methods up to 10% in
MSE. A key innovation of LGDiST is using context genes previously considered
uninformative to build a rich and biologically meaningful genetic latent space.
Our experiments show that removing key components of LGDiST, such as the
context genes, the ST latent space, and the neighbor conditioning, leads to
considerable drops in performance. These findings underscore that the full
architecture of LGDiST achieves substantially better performance than any of
its isolated components.

</details>


### [160] [Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation](https://arxiv.org/abs/2509.01868)
*Komala Subramanyam Cherukuri,Kewei Sha,Zhenhua Huang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Object detection is crucial for Connected Autonomous Vehicles (CAVs) to
perceive their surroundings and make safe driving decisions. Centralized
training of object detection models often achieves promising accuracy, fast
convergence, and simplified training process, but it falls short in
scalability, adaptability, and privacy-preservation. Federated learning (FL),
by contrast, enables collaborative, privacy-preserving, and continuous training
across naturally distributed CAV fleets. However, deploying FL in real-world
CAVs remains challenging due to the substantial computational demands of
training and inference, coupled with highly diverse operating conditions.
Practical deployment must address three critical factors: (i) heterogeneity
from non-IID data distributions, (ii) constrained onboard computing hardware,
and (iii) environmental variability such as lighting and weather, alongside
systematic evaluation to ensure reliable performance. This work introduces the
first holistic deployment-oriented evaluation of FL-based object detection in
CAVs, integrating model performance, system-level resource profiling, and
environmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8,
YOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes
datasets, we analyze trade-offs between detection accuracy, computational cost,
and resource usage under diverse resolutions, batch sizes, weather and lighting
conditions, and dynamic client participation, paving the way for robust FL
deployment in CAVs.

</details>


### [161] [Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction](https://arxiv.org/abs/2509.01873)
*Xueyang Kang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Modern deep learning developments create new opportunities for 3D mapping
technology, scene reconstruction pipelines, and virtual reality development.
Despite advances in 3D deep learning technology, direct training of deep
learning models on 3D data faces challenges due to the high dimensionality
inherent in 3D data and the scarcity of labeled datasets. Structure-from-motion
(SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robust
performance when applied to structured indoor environments but often struggle
with ambiguous features in unstructured environments. These techniques often
struggle to generate detailed geometric representations effective for
downstream tasks such as rendering and semantic analysis. Current limitations
require the development of 3D representation methods that combine traditional
geometric techniques with deep learning capabilities to generate robust
geometry-aware deep learning models.
  The dissertation provides solutions to the fundamental challenges in 3D
vision by developing geometric deep learning methods tailored for essential
tasks such as camera pose estimation, point cloud registration, depth
prediction, and 3D reconstruction. The integration of geometric priors or
constraints, such as including depth information, surface normals, and
equivariance into deep learning models, enhances both the accuracy and
robustness of geometric representations. This study systematically investigates
key components of 3D vision, including camera pose estimation, point cloud
registration, depth estimation, and high-fidelity 3D reconstruction,
demonstrating their effectiveness across real-world applications such as
digital cultural heritage preservation and immersive VR/AR environments.

</details>


### [162] [HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision](https://arxiv.org/abs/2509.01882)
*Shubham Laxmikant Deshmukh,Matthew Wilchek,Feras A. Batarseh*

Main category: cs.CV

TL;DR: 提出HydroVision：用标准RGB水面图像，通过深度学习估计多种光学相关水质参数；在50万+USGS图像上迁移学习评估多种CNN与ViT，DenseNet121最佳（CDOM预测R²=0.89）；为低成本、可扩展监测与预警提供可行性，但目前对光照敏感，后续改进低光/遮挡鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统水质监测依赖接触式采样或昂贵的多/高光谱遥感，难以高频、广域、低成本覆盖，尤其在灾害应对与公共卫生场景需要快速预警与长时段监控。RGB图像普遍可得，若能可靠估计关键水质参数，可大幅提升监测可达性与时效。

Method: 构建HydroVision框架：从USGS 2022-2024季节性多样的50万+水面RGB图像训练；采用迁移学习比较VGG-16、ResNet50、MobileNetV2、DenseNet121与Vision Transformer，对多种水质光学参数（Chl-a、总叶绿素、CDOM、藻蓝素、悬浮泥沙、浊度）进行回归预测；以验证集性能（如R²）评估架构优劣。

Result: DenseNet121表现最佳，CDOM预测R²达0.89；总体显示用标准RGB即可对若些参数实现较高精度估计，具备跨条件场景分类与回归能力；模型在良好光照下效果最优。

Conclusion: HydroVision证明以RGB图像结合深度学习实现大规模、低成本水质监测的可行性，支持监管与早期污染趋势发现。当前受限于光照与遮挡，未来将提升低光与复杂场景鲁棒性以扩大应用范围。

Abstract: Ongoing advancements in computer vision, particularly in pattern recognition
and scene classification, have enabled new applications in environmental
monitoring. Deep learning now offers non-contact methods for assessing water
quality and detecting contamination, both critical for disaster response and
public health protection. This work introduces HydroVision, a deep
learning-based scene classification framework that estimates optically active
water quality parameters including Chlorophyll-Alpha, Chlorophylls, Colored
Dissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, and
Turbidity from standard Red-Green-Blue (RGB) images of surface water.
HydroVision supports early detection of contamination trends and strengthens
monitoring by regulatory agencies during external environmental stressors,
industrial activities, and force majeure events. The model is trained on more
than 500,000 seasonally varied images collected from the United States
Geological Survey Hydrologic Imagery Visualization and Information System
between 2022 and 2024. This approach leverages widely available RGB imagery as
a scalable, cost-effective alternative to traditional multispectral and
hyperspectral remote sensing. Four state-of-the-art convolutional neural
networks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformer
are evaluated through transfer learning to identify the best-performing
architecture. DenseNet121 achieves the highest validation performance, with an
R2 score of 0.89 in predicting CDOM, demonstrating the framework's promise for
real-world water quality monitoring across diverse conditions. While the
current model is optimized for well-lit imagery, future work will focus on
improving robustness under low-light and obstructed scenarios to expand its
operational utility.

</details>


### [163] [Automated Wildfire Damage Assessment from Multi view Ground level Imagery Via Vision Language Models](https://arxiv.org/abs/2509.01895)
*Miguel Esparza,Archit Gupta,Ali Mostafavi,Kai Yin,Yiming Xiao*

Main category: cs.CV

TL;DR: 提出零样本框架，用预训练视觉-语言模型（VLM）从地面多视角照片评估野火房屋受损，单视角效果差，多视角显著提升（F1 0.857–0.947）；VLM与VLM+LLM差异不显著，但多视角显著优于单视角。方法无需标注数据，可即刻部署用于救灾分流与优先级判定。


<details>
  <summary>Details</summary>
Motivation: 野火频率强度上升，灾后房产损失评估需快速准确。传统方法慢，现代视觉方法依赖大量标注数据，不利于灾后即时部署。需要无需监督训练、可解释、可快速落地的评估方案。

Method: 提出两条零样本流水线：A 仅用预训练VLM，B 结合VLM与LLM。两者基于“结构化提示”（包含野火损伤指示器）从地面图像分类建筑受损程度；引入多视角图像综合判断以捕捉细微损伤。对加州2025年Eaton与Palisades两起野火数据进行评估，并用McNemar检验显著性。

Result: 单视角分类F1=0.225–0.511，性能不佳；多视角融合后F1=0.857–0.947，显著提升。McNemar检验证实多视角带来统计显著的改进；Pipeline A与B之间改进不具统计显著性。

Conclusion: 多视角零样本VLM流程能有效综合不同视角信息识别细微损伤，显著优于单视角；LLM加入在本研究中未带来统计显著提升。所提流程无需监督训练、可解释、可灵活部署，适合灾后快速分流与优先响应。

Abstract: The escalating intensity and frequency of wildfires demand innovative
computational methods for rapid and accurate property damage assessment.
Traditional methods are often time consuming, while modern computer vision
approaches typically require extensive labeled datasets, hindering immediate
post-disaster deployment. This research introduces a novel, zero-shot framework
leveraging pre-trained vision language models (VLMs) to classify damage from
ground-level imagery. We propose and evaluate two pipelines applied to the 2025
Eaton and Palisades fires in California, a VLM (Pipeline A) and a VLM + large
language model (LLM) approach (Pipeline B), that integrate structured prompts
based on specific wildfire damage indicators. A primary scientific contribution
of this study is demonstrating the VLMs efficacy in synthesizing information
from multiple perspectives to identify nuanced damage, a critical limitation in
existing literature. Our findings reveal that while single view assessments
struggled to classify affected structures (F1 scores ranging from 0.225 to
0.511), the multi-view analysis yielded dramatic improvements (F1 scores
ranging from 0.857 to 0.947). Moreover, the McNemar test confirmed that
pipelines with a multi-view image assessment yields statistically significant
classification improvements; however, the improvements this research observed
between Pipeline A and B were not statistically significant. Thus, future
research can explore the potential of LLM prompting in damage assessment. The
practical contribution is an immediately deployable, flexible, and
interpretable workflow that bypasses the need for supervised training,
significantly accelerating triage and prioritization for disaster response
practitioners.

</details>


### [164] [DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective](https://arxiv.org/abs/2509.01898)
*Zhipeng Weng,Xiaopeng Liu,Ce Liu,Xingyuan Guo,Yukai Shi,Liang Lin*

Main category: cs.CV

TL;DR: 提出面向扩散模型的高斯量化表征学习（GARL），结合训练过程的过拟合监测机制，在小样本无人机红外超分重建任务中显著缓解大规模架构过拟合并提升鲁棒性；同时发布多源无人机红外数据集DroneSR，实验表明方法优于现有超分方法。


<details>
  <summary>Details</summary>
Motivation: 大规模扩散/生成式模型在图像超分中表现强，但在小样本（尤其是无人机红外场景）下易严重过拟合，削弱泛化与鲁棒性。现有工作多依赖缩小模型或正则化，难以在保持大模型能力的同时抑制过拟合。因此需要一种既保留大规模架构优势、又能应对小样本与复杂分布的学习机制。

Method: 1) 提出“高斯量化表征学习”：在扩散模型中引入基于高斯分布的量化/离散化表示与学习策略（细节未给出），以降低表征过拟合倾向并增强噪声鲁棒性；2) 设计训练期过拟合监测机制：在大规模架构训练过程中实时跟踪并检测过拟合迹象（可能基于验证损失、分布漂移或不确定性指标）；3) 构建多源无人机红外超分重建基准数据集DroneSR，用于系统评测模型在小样本与复杂条件下的过拟合与鲁棒性。

Result: 在DroneSR基准上的实验显示，新方法在复杂条件下的超分任务中优于现有方法，并显著缓解大规模架构的过拟合；在保持模型复杂度的同时提升泛化与鲁棒性。

Conclusion: 面向扩散模型的高斯量化表征学习配合训练监测，可在小样本无人机红外超分任务中有效抑制大模型过拟合并提升性能；所构建的DroneSR数据集揭示并放大了该场景下的过拟合问题。代码与数据集将开源。

Abstract: Although large scale models achieve significant improvements in performance,
the overfitting challenge still frequently undermines their generalization
ability. In super resolution tasks on images, diffusion models as
representatives of generative models typically adopt large scale architectures.
However, few-shot drone-captured infrared training data frequently induces
severe overfitting in large-scale architectures. To address this key challenge,
our method proposes a new Gaussian quantization representation learning method
oriented to diffusion models that alleviates overfitting and enhances
robustness. At the same time, an effective monitoring mechanism tracks large
scale architectures during training to detect signs of overfitting. By
introducing Gaussian quantization representation learning, our method
effectively reduces overfitting while maintaining architecture complexity. On
this basis, we construct a multi source drone-based infrared image benchmark
dataset for detection and use it to emphasize overfitting issues of large scale
architectures in few sample, drone-based diverse drone-based image
reconstruction scenarios. To verify the efficacy of the method in mitigating
overfitting, experiments are conducted on the constructed benchmark.
Experimental results demonstrate that our method outperforms existing super
resolution approaches and significantly mitigates overfitting of large scale
architectures under complex conditions. The code and DroneSR dataset will be
available at: https://github.com/wengzp1/GARLSR.

</details>


### [165] [RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events](https://arxiv.org/abs/2509.01907)
*Zhenyuan Chen,Chenxi Wang,Ningyu Zhang,Feng Zhang*

Main category: cs.CV

TL;DR: 提出RSCC数据集：62,315对灾前/灾后遥感图像及细粒度“变化描述”文本，用于训练评估灾害场景的时序-语义理解的视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有遥感灾害监测数据多为单时相、且缺乏详尽文本标注，难以表达灾害随时间的影响与可解释分析需求。需要包含时间配对图像和语义化变化描述的数据来提升模型对灾害变化的理解与推理。

Method: 构建大型RSCC基准：收集多灾种（地震、洪水、野火等）的灾前/灾后图像对，并配套类似人工撰写的细粒度“变化描述”文本；将其用于训练与评测面向灾害的双时相视觉-语言模型，并进行实验验证其有效性。

Result: RSCC显著提升模型在灾害相关、细节级别变化分析任务中的表现，支持更丰富、更精确的时序-语义理解与评测。

Conclusion: RSCC弥合了遥感领域时间维度与语义标注的缺口，使视觉-语言应用在灾害场景中更准确、可解释、可扩展；代码与数据集已开源（GitHub: Bili-Sakura/RSCC）。

Abstract: Remote sensing is critical for disaster monitoring, yet existing datasets
lack temporal image pairs and detailed textual annotations. While
single-snapshot imagery dominates current resources, it fails to capture
dynamic disaster impacts over time. To address this gap, we introduce the
Remote Sensing Change Caption (RSCC) dataset, a large-scale benchmark
comprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,
wildfires, and more) paired with rich, human-like change captions. By bridging
the temporal and semantic divide in remote sensing data, RSCC enables robust
training and evaluation of vision-language models for disaster-aware
bi-temporal understanding. Our results highlight RSCC's ability to facilitate
detailed disaster-related analysis, paving the way for more accurate,
interpretable, and scalable vision-language applications in remote sensing.
Code and dataset are available at https://github.com/Bili-Sakura/RSCC.

</details>


### [166] [Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework](https://arxiv.org/abs/2509.01910)
*Furong Jia,Lanxin Liu,Ce Hou,Fan Zhang,Xinyan Liu,Yu Liu*

Main category: cs.CV

TL;DR: 提出在全球图像地理定位中引入“概念瓶颈”的可解释框架：在图像与地理位置对齐的同时，投影到共享地理概念空间并最小化概念级损失，从而提升准确率与可解释性，优于GeoCLIP。


<details>
  <summary>Details</summary>
Motivation: 现有如GeoCLIP的对比学习方法虽能较准地预测位置，但缺乏可解释性；现有概念可解释方法与图像-位置对齐目标不匹配，导致解释与性能皆不理想。需要一种既保持对齐性能又提供地理概念层面解释的方案。

Method: 在全球地理定位模型中插入概念感知对齐模块（Concept-Aware Alignment Module）：将图像与位置嵌入共同投影到共享的地理概念库（如气候、地形、地标建筑等），并引入概念级损失函数，使对齐在概念子空间内进行；形成“概念瓶颈”，提供可解释的概念激活并稳定训练。

Result: 在广泛实验中，该方法在地理定位准确率上超过GeoCLIP；同时在多种地理空间预测任务上带来性能提升；能揭示更丰富的地理语义与模型决策依据。

Conclusion: 首次将可解释性系统性引入全球地理定位，通过概念对齐实现更强性能与可解释性，证明概念瓶颈在地理对齐任务中的有效性与泛化价值。

Abstract: Worldwide geo-localization involves determining the exact geographic location
of images captured globally, typically guided by geographic cues such as
climate, landmarks, and architectural styles. Despite advancements in
geo-localization models like GeoCLIP, which leverages images and location
alignment via contrastive learning for accurate predictions, the
interpretability of these models remains insufficiently explored. Current
concept-based interpretability methods fail to align effectively with
Geo-alignment image-location embedding objectives, resulting in suboptimal
interpretability and performance. To address this gap, we propose a novel
framework integrating global geo-localization with concept bottlenecks. Our
method inserts a Concept-Aware Alignment Module that jointly projects image and
location embeddings onto a shared bank of geographic concepts (e.g., tropical
climate, mountain, cathedral) and minimizes a concept-level loss, enhancing
alignment in a concept-specific subspace and enabling robust interpretability.
To our knowledge, this is the first work to introduce interpretability into
geo-localization. Extensive experiments demonstrate that our approach surpasses
GeoCLIP in geo-localization accuracy and boosts performance across diverse
geospatial prediction tasks, revealing richer semantic insights into geographic
decision-making processes.

</details>


### [167] [A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation](https://arxiv.org/abs/2509.01919)
*Seohyun Kim,Junyoung Lee,Jongho Park,Jinhyung Koo,Sungjin Lee,Yeseong Kim*

Main category: cs.CV

TL;DR: DiTTO 是一个基于扩散模型的生成框架，用于按需合成高保真、可配置且多样化的多设备存储访问轨迹，并能刻画时间动态与设备间关联；实验显示其在用户引导配置下误差约8%。


<details>
  <summary>Details</summary>
Motivation: 现实系统研究与评测亟需高质量、多样化且可控的存储访问轨迹，但真实采集代价高、隐私受限、覆盖面不足，传统合成方法难以同时保证保真度、可配置性与多设备相关性。

Method: 将扩散模型用于连续时间序列与多设备联合建模：1) 用扩散过程学习多设备存储轨迹的分布与时序依赖；2) 通过条件/引导机制实现用户可配置（如负载强度、相关性等）；3) 输出高保真连续轨迹并保持设备间依赖结构。

Result: 在实验中，生成的轨迹在保真度与多样性上表现优良；在用户配置指导下，生成结果与目标配置对齐，平均配置误差约8%。

Conclusion: DiTTO 能以较低配置误差生成高保真且可控的多设备存储轨迹，优于传统方法，为系统评测与研究提供可复制、隐私友好的数据来源。

Abstract: We propose DiTTO, a novel diffusion-based framework for generating realistic,
precisely configurable, and diverse multi-device storage traces. Leveraging
advanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity
continuous traces that capture temporal dynamics and inter-device dependencies
with user-defined configurations. Our experimental results demonstrate that
DiTTO can generate traces with high fidelity and diversity while aligning
closely with guided configurations with only 8% errors.

</details>


### [168] [Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models](https://arxiv.org/abs/2509.01959)
*Hiroshi Sasaki*

Main category: cs.CV

TL;DR: 提出一种面向图解（如流程图）的CLIP式多模态训练范式，通过专门的“困难样本”对比学习与两种利用结构信息的损失，显著提升图文匹配与VQA表现。


<details>
  <summary>Details</summary>
Motivation: 通用多模态模型（如CLIP）在自然图像上有效，但在结构化、符号化的图解（流程图等）上理解不足。图解包含节点、边、拓扑与语义关系，天然不同于自然图像纹理与对象分布，导致现有对比学习难以捕捉结构语义。需要面向图解结构的训练目标与数据挖掘策略。

Method: 提出“硬样本驱动”的对比学习框架：构造或挖掘更具迷惑性的负样本；并引入两种专为图解设计的损失函数，利用图解的固有结构（如节点-连边-拓扑约束、符号关系）来约束表征，使图像-文本对在结构与语义上更一致。整体集成到CLIP式训练流程中。

Result: 在流程图基准上评测，较标准CLIP与常规hard negative CLIP都有“显著提升”，任务包括图文匹配与视觉问答（VQA）。

Conclusion: 面向专门视觉域需要定制化训练目标。利用图解结构信息与困难负样本的对比学习能加强模型对图解的结构化、语义化理解，推动图解理解在视觉-语言融合中的发展。

Abstract: Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)
model, have demonstrated remarkable success in aligning visual and linguistic
representations. However, these models exhibit limitations when applied to
specialised visual domains, such as diagrams, which encode structured, symbolic
information distinct from that of natural imagery.
  In this paper, we introduce a novel training paradigm explicitly designed to
enhance the comprehension of diagrammatic images within vision-language models.
Our approach uses ``hard'' samples for our proposed contrastive learning that
incorporates two specialised loss functions that leverage the inherent
structural properties of diagrams. By integrating these objectives into model
training, our method enables models to develop a more structured and
semantically coherent understanding of diagrammatic content.
  We empirically validate our approach on a benchmark dataset of flowcharts, as
a representative class of diagrammatic imagery, demonstrating substantial
improvements over standard CLIP and conventional hard negative CLIP learning
paradigms for both image-text matching and visual question answering tasks. Our
findings underscore the significance of tailored training strategies for
specialised tasks and contribute to advancing diagrammatic understanding within
the broader landscape of vision-language integration.

</details>


### [169] [2D Gaussian Splatting with Semantic Alignment for Image Inpainting](https://arxiv.org/abs/2509.01964)
*Hongyu Li,Chaofeng Chen,Xiaoming Li,Guangming Lu*

Main category: cs.CV

TL;DR: 提出首个基于2D Gaussian Splatting的图像修复框架：把缺失图像编码为连续的高斯系数场，通过可微光栅化重建，并结合DINO全局特征与分块光栅化，兼顾像素连贯性、语义一致性与效率，在基准上取得有竞争力结果。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复需要同时满足局部纹理连续与全局语义一致，但常规卷积/扩散/生成式方法在像素级连续性或效率上存在折衷。GS在3D与超分中已展现将离散点映射为连续场的优势，但尚未用于修复任务。作者动机是利用GS的连续渲染促进像素连贯，并借助预训练的全局语义先验提升大掩膜下的一致性。

Method: 1) 将不完整图像编码为2D高斯溅射系数的连续场；2) 通过可微光栅化渲染得到修复图像，利用GS的连续性促进像素级连贯；3) 设计分块（patch-wise）光栅化以降低显存与加速推理；4) 融合预训练DINO的全局特征：小缺失区域天然鲁棒，大掩膜场景中引导语义对齐；5) 在标准数据集上训练与评测。

Result: 在多项标准基准上，定量指标与感知质量均具竞争力，显示在局部纹理连续性与全局语义一致性上具有优势；推理效率与可扩展性因分块光栅化得到提升。

Conclusion: 2D Gaussian Splatting可有效用于图像修复：连续渲染带来像素级平滑连贯，结合DINO实现全局语义一致；分块策略提升效率。方法为GS在2D图像处理开辟新方向。

Abstract: Gaussian Splatting (GS), a recent technique for converting discrete points
into continuous spatial representations, has shown promising results in 3D
scene modeling and 2D image super-resolution. In this paper, we explore its
untapped potential for image inpainting, which demands both locally coherent
pixel synthesis and globally consistent semantic restoration. We propose the
first image inpainting framework based on 2D Gaussian Splatting, which encodes
incomplete images into a continuous field of 2D Gaussian splat coefficients and
reconstructs the final image via a differentiable rasterization process. The
continuous rendering paradigm of GS inherently promotes pixel-level coherence
in the inpainted results. To improve efficiency and scalability, we introduce a
patch-wise rasterization strategy that reduces memory overhead and accelerates
inference. For global semantic consistency, we incorporate features from a
pretrained DINO model. We observe that DINO's global features are naturally
robust to small missing regions and can be effectively adapted to guide
semantic alignment in large-mask scenarios, ensuring that the inpainted content
remains contextually consistent with the surrounding scene. Extensive
experiments on standard benchmarks demonstrate that our method achieves
competitive performance in both quantitative metrics and perceptual quality,
establishing a new direction for applying Gaussian Splatting to 2D image
processing.

</details>


### [170] [Ensemble-Based Event Camera Place Recognition Under Varying Illumination](https://arxiv.org/abs/2509.01968)
*Therese Joseph,Tobias Fischer,Michael Milford*

Main category: cs.CV

TL;DR: 提出一种集成式事件相机VPR方法，跨多重重建方式、特征提取器与时间分辨率进行序列级融合，相比仅利用时间分辨率的集成显著提升强光照变化（昼夜）下鲁棒性；在两套长程驾驶数据上Recall@1跨昼夜提升57%。并给出设计要素消融与改进的序列匹配框架，代码与基准将开源。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高速、强动态范围场景中优于传统相机，但VPR在严苛光照变化（如昼夜转换）下仍不稳。既有事件VPR集成方法多只在时间分辨率上做融合，难以充分利用不同重建策略和特征表达的互补性，因此需要更广义的融合以提升跨光照鲁棒性。

Method: 提出一个序列级集成框架：对同一事件流执行多种事件到帧的重建（含不同bin策略、极性处理等），用多种VPR特征提取器在多个时间分辨率下提取描述子，并采用改进的序列匹配将这些通道的匹配结果融合，特别针对长序列匹配引入框架修改以提升稳定性。并进行系统性消融，分析binning、极性、重建法、特征提取器对性能的影响。

Result: 在两套每次往返约8公里的长期驾驶数据上、保持真实速度与停留（不做度量下采样）进行评测。与仅基于时间分辨率的事件集成相比，在昼夜跨域中Recall@1相对提升57%，并在多种光照（下午、日落、夜晚）下表现更稳。消融实验识别出对鲁棒性贡献最大的组件与设置。

Conclusion: 更广覆盖的多模态（重建-特征-时间）序列融合显著提升事件相机VPR在强光照变化下的鲁棒性；改进的序列匹配在长序列下更优。工作提供可复现代码与评测框架，便于后续研究与比较。

Abstract: Compared to conventional cameras, event cameras provide a high dynamic range
and low latency, offering greater robustness to rapid motion and challenging
lighting conditions. Although the potential of event cameras for visual place
recognition (VPR) has been established, developing robust VPR frameworks under
severe illumination changes remains an open research problem. In this paper, we
introduce an ensemble-based approach to event camera place recognition that
combines sequence-matched results from multiple event-to-frame reconstructions,
VPR feature extractors, and temporal resolutions. Unlike previous event-based
ensemble methods, which only utilise temporal resolution, our broader fusion
strategy delivers significantly improved robustness under varied lighting
conditions (e.g., afternoon, sunset, night), achieving a 57% relative
improvement in Recall@1 across day-night transitions. We evaluate our approach
on two long-term driving datasets (with 8 km per traverse) without metric
subsampling, thereby preserving natural variations in speed and stop duration
that influence event density. We also conduct a comprehensive analysis of key
design choices, including binning strategies, polarity handling, reconstruction
methods, and feature extractors, to identify the most critical components for
robust performance. Additionally, we propose a modification to the standard
sequence matching framework that enhances performance at longer sequence
lengths. To facilitate future research, we will release our codebase and
benchmarking framework.

</details>


### [171] [MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement](https://arxiv.org/abs/2509.01977)
*Dong She,Siming Fu,Mushui Liu,Qiaoqiao Jin,Hualiang Wang,Mu Liu,Jidong Jiang*

Main category: cs.CV

TL;DR: MOSAIC提出一个面向表示的多主体个性化图像生成框架，通过语义对应和特征正交解耦，解决多参考主体下的身份混淆与属性泄漏问题，能在4个以上主体时仍保持高保真度。


<details>
  <summary>Details</summary>
Motivation: 现有多主体生成在共享表示空间中易发生身份融合与属性泄漏，尤其主体数增加时语义错配严重；缺乏可用于精确语义对齐的标注数据集，导致很难确保不同参考主体在生成图像中各自负责的区域和特征得到准确映射。

Method: 1) 构建SemAlign-MS数据集：提供多参考主体与目标图像之间的细粒度语义对应标注（点到点/区域级）。2) 语义对应注意力损失（Semantic correspondence attention loss）：在生成过程中，对应点/区域的注意力必须高耦合，实现从每个参考到指定生成区域的精确对齐。3) 多参考解耦损失（Multi-reference disentanglement loss）：将不同主体压入相互正交的注意力子空间，减少特征干扰与信息泄漏，保留各自身份特征。整体框架MOSAIC以“表示为中心”，在注意力层面同时实现对齐与解耦。

Result: 在多项基准上达成SOTA。与现有方法相比，身份一致性与语义保真度显著提升；特别是在3个以上参考主体情境下，其他方法性能明显下降，而MOSAIC在4+主体时仍保持高保真生成质量。

Conclusion: 显式语义对应与正交特征解耦是解决多主体生成难题的关键。借助SemAlign-MS数据集及两类损失，MOSAIC有效避免身份混合与属性泄漏，扩展了多主体个性化合成在复杂场景中的可用性。

Abstract: Multi-subject personalized generation presents unique challenges in
maintaining identity fidelity and semantic coherence when synthesizing images
conditioned on multiple reference subjects. Existing methods often suffer from
identity blending and attribute leakage due to inadequate modeling of how
different subjects should interact within shared representation spaces. We
present MOSAIC, a representation-centric framework that rethinks multi-subject
generation through explicit semantic correspondence and orthogonal feature
disentanglement. Our key insight is that multi-subject generation requires
precise semantic alignment at the representation level - knowing exactly which
regions in the generated image should attend to which parts of each reference.
To enable this, we introduce SemAlign-MS, a meticulously annotated dataset
providing fine-grained semantic correspondences between multiple reference
subjects and target images, previously unavailable in this domain. Building on
this foundation, we propose the semantic correspondence attention loss to
enforce precise point-to-point semantic alignment, ensuring high consistency
from each reference to its designated regions. Furthermore, we develop the
multi-reference disentanglement loss to push different subjects into orthogonal
attention subspaces, preventing feature interference while preserving
individual identity characteristics. Extensive experiments demonstrate that
MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,
while existing methods typically degrade beyond 3 subjects, MOSAIC maintains
high fidelity with 4+ reference subjects, opening new possibilities for complex
multi-subject synthesis applications.

</details>


### [172] [Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing](https://arxiv.org/abs/2509.01984)
*Quan Dao,Xiaoxiao He,Ligong Han,Ngan Hoai Nguyen,Amin Heyrani Nobar,Faez Ahmed,Han Zhang,Viet Anh Nguyen,Dimitris Metaxas*

Main category: cs.CV

TL;DR: 提出VARIN：首个面向视觉自回归模型（VAR）的噪声反演式文本引导图像编辑方法，利用位置感知的argmax反演（LAI）构造逆Gumbel噪声，实现高保真重建与可控编辑，保留背景结构。


<details>
  <summary>Details</summary>
Motivation: VAR在文本生成图像上已接近扩散模型，但“零额外训练”的文本引导图像编辑能力尚缺。现有编辑多围绕扩散模型的噪声反演展开，VAR缺少对应机制；如何在离散argmax采样框架下进行可逆、可控编辑是关键空白。

Method: 提出VARIN（Visual AutoRegressive Inverse Noise）：
- 设计LAI（Location-aware Argmax Inversion）作为argmax采样的伪逆，构造可复原源图的逆Gumbel噪声。
- 用逆噪声实现精确重建，再在特定位置或语义区域注入/调节噪声以执行文本引导的局部或全局编辑。
- 保持自回归生成流程与权重不变，无需额外训练。

Result: 大量实验显示：在按提示修改前景语义的同时，显著保持原背景与结构细节；编辑的可控性与重建保真度高，定性与定量指标均优。

Conclusion: 证明了在VAR框架下进行噪声反演与无训练编辑的可行性与有效性，补齐了VAR相对扩散模型在编辑方面的能力缺口，为实用化文本引导编辑提供新途径。

Abstract: Visual autoregressive models (VAR) have recently emerged as a promising class
of generative models, achieving performance comparable to diffusion models in
text-to-image generation tasks. While conditional generation has been widely
explored, the ability to perform prompt-guided image editing without additional
training is equally critical, as it supports numerous practical real-world
applications. This paper investigates the text-to-image editing capabilities of
VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise
inversion-based editing technique designed explicitly for VAR models. VARIN
leverages a novel pseudo-inverse function for argmax sampling, named
Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These
inverse noises enable precise reconstruction of the source image and facilitate
targeted, controllable edits aligned with textual prompts. Extensive
experiments demonstrate that VARIN effectively modifies source images according
to specified prompts while significantly preserving the original background and
structural details, thus validating its efficacy as a practical editing
approach.

</details>


### [173] [Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination](https://arxiv.org/abs/2509.01986)
*Ziyun Zeng,Junhao Zhang,Wei Li,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出DIM数据集与小型统一模型DIM-4.6B，通过把“设计/规划”职责前置给理解模块，显著提升图像编辑精准度，超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态统一模型在T2I强，但图像编辑差，根因是“理解模块只翻译、生成模块既设计又绘制”的职责失衡；而理解模块反而数据更丰富却未被用来承担复杂设计与定位任务。

Method: 构建DIM数据集含两部分：(1) DIM-T2I：1400万长上下文图文对，强化复杂指令理解；(2) DIM-Edit：23.3万由GPT-4o生成的链式思维想象，作为显式编辑蓝图。模型结构：冻结Qwen2.5-VL-3B（理解）+ 可训练SANA1.5-1.6B（生成），用两层轻量MLP连接，并在DIM上训练，得到DIM-4.6B-T2I/Edit。

Result: 在ImgEdit与GEdit-Bench上达SOTA或具竞争力，优于更大模型（如UniWorld-V1、Step1X-Edit），证明小规模但职责清晰的架构能提升编辑质量。

Conclusion: 将“设计/规划”职责明确交给理解模块，并用CoT式蓝图监督，可显著改善图像编辑；规模不大也能达到或超越SOTA。代码与模型将开源。

Abstract: In recent years, integrating multimodal understanding and generation into a
single unified model has emerged as a promising paradigm. While this approach
achieves strong results in text-to-image (T2I) generation, it still struggles
with precise image editing. We attribute this limitation to an imbalanced
division of responsibilities. The understanding module primarily functions as a
translator that encodes user instructions into semantic conditions, while the
generation module must simultaneously act as designer and painter, inferring
the original layout, identifying the target editing region, and rendering the
new content. This imbalance is counterintuitive because the understanding
module is typically trained with several times more data on complex reasoning
tasks than the generation module. To address this issue, we introduce
Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i)
DIM-T2I, containing 14M long-context image-text pairs to enhance complex
instruction comprehension; and (ii) DIM-Edit, consisting of 233K
chain-of-thought imaginations generated by GPT-4o, serving as explicit design
blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable
SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM
dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale,
DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and
GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1
and Step1X-Edit. These findings demonstrate that explicitly assigning the
design responsibility to the understanding module provides significant benefits
for image editing. Our dataset and models will be available at
https://github.com/showlab/DIM.

</details>


### [174] [Explaining What Machines See: XAI Strategies in Deep Object Detection Models](https://arxiv.org/abs/2509.01991)
*FatemehSadat Seyedmomeni,Mohammad Ali Keyvanrad*

Main category: cs.CV

TL;DR: 这是一篇面向“目标检测”领域的XAI综述，系统梳理方法谱系（扰动、梯度、反向传播、图方法）、代表工作与适配的检测架构，并给出数据集/指标与趋势统计，以指导方法选型与未来研究。


<details>
  <summary>Details</summary>
Motivation: 深度检测模型在关键场景中广泛应用但“黑盒”且复杂，难以被人类信任与审计；现有XAI多聚焦分类，针对检测任务（定位+分类）的可解释性方法分散缺乏系统梳理与对比，研究者在方法选型与评估上缺少统一视角。

Method: 作为综述：1) 依据机理将检测XAI方法分为扰动、梯度、反向传播、图方法；2) 挑选代表方法（D-RISE、BODEM、D-CLOSE、FSOD）进行深度解读；3) 统计2022–2025上半年文献趋势；4) 汇总主流检测架构（YOLO/SSD/Faster R-CNN/EfficientDet）的适配性；5) 归纳常用数据集与评测指标；6) 提炼难点与挑战并形成方法选型指引与研究建议。

Result: 给出结构化分类与对比框架；证明该领域自2022起加速增长；总结各类方法在不同检测器上的适配与优劣；罗列常用数据集与指标；凝练出若干关键挑战。

Conclusion: 该综述为目标检测XAI提供统一的术语、分类与实践指南，有助于研究者与工程师依据任务需求选择合适方法，并指向未来需在跨架构泛化、定量评测标准、真实世界鲁棒性与人因验证等方面深化研究。

Abstract: In recent years, deep learning has achieved unprecedented success in various
computer vision tasks, particularly in object detection. However, the black-box
nature and high complexity of deep neural networks pose significant challenges
for interpretability, especially in critical domains such as autonomous
driving, medical imaging, and security systems. Explainable Artificial
Intelligence (XAI) aims to address this challenge by providing tools and
methods to make model decisions more transparent, interpretable, and
trust-worthy for humans. This review provides a comprehensive analysis of
state-of-the-art explain-ability methods specifically applied to object
detection models. The paper be-gins by categorizing existing XAI techniques
based on their underlying mechanisms-perturbation-based, gradient-based,
backpropagation-based, and graph-based methods. Notable methods such as D-RISE,
BODEM, D-CLOSE, and FSOD are discussed in detail. Furthermore, the paper
investigates their applicability to various object detection architectures,
including YOLO, SSD, Faster R-CNN, and EfficientDet. Statistical analysis of
publication trends from 2022 to mid-2025 shows an accelerating interest in
explainable object detection, indicating its increasing importance. The study
also explores common datasets and evaluation metrics, and highlights the major
challenges associated with model interpretability. By providing a structured
taxonomy and a critical assessment of existing methods, this review aims to
guide researchers and practitioners in selecting suitable explainability
techniques for object detection applications and to foster the development of
more interpretable AI systems.

</details>


### [175] [Palette Aligned Image Diffusion](https://arxiv.org/abs/2509.02000)
*Elad Aharoni,Noy Porat,Dani Lischinski,Ariel Shamir*

Main category: cs.CV

TL;DR: 提出Palette-Adapter：在扩散文生图中以用户色板作为条件，兼顾色板遵循与图像质量，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 创作流程常用色板直观简洁，但直接以色板做条件会导致颜色歧义与生成不稳定；现有方法难以在色板一致性与图像语义质量之间取得平衡。

Method: 将色板视为稀疏颜色直方图，引入两类可控标量：直方图熵（控制颜色多样性）与色板到直方图距离（控制对色板的遵循度）；提出负直方图机制以抑制不期望的色相，并与无分类器指导配合；基于覆盖稀有与常见颜色的均衡数据集进行训练，使模型在色域内广泛泛化。

Result: 在多种色板与文本提示下生成稳定、语义一致的图像；在定性、定量及用户研究中，相比现有方法同时获得更强的色板遵循性与更高图像质量。

Conclusion: Palette-Adapter能以简单可控的参数实现对颜色风格的稳健调控，提升文生图对用户色板的可控性与实用性，并在多项评测中取得领先表现。

Abstract: We introduce the Palette-Adapter, a novel method for conditioning
text-to-image diffusion models on a user-specified color palette. While
palettes are a compact and intuitive tool widely used in creative workflows,
they introduce significant ambiguity and instability when used for conditioning
image generation. Our approach addresses this challenge by interpreting
palettes as sparse histograms and introducing two scalar control parameters:
histogram entropy and palette-to-histogram distance, which allow flexible
control over the degree of palette adherence and color variation. We further
introduce a negative histogram mechanism that allows users to suppress specific
undesired hues, improving adherence to the intended palette under the standard
classifier-free guidance mechanism. To ensure broad generalization across the
color space, we train on a carefully curated dataset with balanced coverage of
rare and common colors. Our method enables stable, semantically coherent
generation across a wide range of palettes and prompts. We evaluate our method
qualitatively, quantitatively, and through a user study, and show that it
consistently outperforms existing approaches in achieving both strong palette
adherence and high image quality.

</details>


### [176] [Vision-Based Embedded System for Noncontact Monitoring of Preterm Infant Behavior in Low-Resource Care Settings](https://arxiv.org/abs/2509.02018)
*Stanley Mugisha,Rashid Kisitu,Francis Komakech,Excellence Favor*

Main category: cs.CV

TL;DR: 用量化的MobileNet在树莓派上实现新生儿行为（睡眠/清醒、哭声）视觉监测，达SOTA精度且满足边缘端实时性，为低资源NICU提供低成本、可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 早产儿死亡率高，低资源环境缺乏高级NICU；现有行为监测依赖人工或侵入式传感器，易出错、不可及且可能伤皮肤，迫切需要非侵入、自动化、低成本的连续监护手段。

Method: 构建一套嵌入式视觉监护框架：在树莓派上部署量化后的MobileNet进行实时状态识别；基于公共新生儿图像数据训练/评估；对比ResNet152、VGG19等大模型，分析精度、模型大小与延迟的权衡；优化树莓派视觉管线并集成安全物联网通信用于临床告警。

Result: 在公共数据上达成睡眠检测91.8%精度、哭/正常分类97.7%精度；模型量化使体积减少约68%，保持实时推理；较大架构仅带来有限精度提升但推理代价高，难以满足边缘端实时性。

Conclusion: 轻量化、优化的MobileNet最适合在边缘端实现可扩展、低成本且可临床应用的新生儿监护；所提系统为资源受限环境的早产儿护理提供切实可行的路径。

Abstract: Preterm birth remains a leading cause of neonatal mortality,
disproportionately affecting low-resource settings with limited access to
advanced neonatal intensive care units (NICUs).Continuous monitoring of infant
behavior, such as sleep/awake states and crying episodes, is critical but
relies on manual observation or invasive sensors, which are prone to error,
impractical, and can cause skin damage. This paper presents a novel,
noninvasive, and automated vision-based framework to address this gap. We
introduce an embedded monitoring system that utilizes a quantized MobileNet
model deployed on a Raspberry Pi for real-time behavioral state detection. When
trained and evaluated on public neonatal image datasets, our system achieves
state-of-the-art accuracy (91.8% for sleep detection and 97.7% for
crying/normal classification) while maintaining computational efficiency
suitable for edge deployment. Through comparative benchmarking, we provide a
critical analysis of the trade-offs between model size, inference latency, and
diagnostic accuracy. Our findings demonstrate that while larger architectures
(e.g., ResNet152, VGG19) offer marginal gains in accuracy, their computational
cost is prohibitive for real-time edge use. The proposed framework integrates
three key innovations: model quantization for memory-efficient inference (68%
reduction in size), Raspberry Pi-optimized vision pipelines, and secure IoT
communication for clinical alerts. This work conclusively shows that
lightweight, optimized models such as the MobileNet offer the most viable
foundation for scalable, low-cost, and clinically actionable NICU monitoring
systems, paving the way for improved preterm care in resource-constrained
environments.

</details>


### [177] [Unsupervised Training of Vision Transformers with Synthetic Negatives](https://arxiv.org/abs/2509.02024)
*Nikolaos Giakoumoglou,Andreas Floros,Kleanthis Marios Papadopoulos,Tania Stathaki*

Main category: cs.CV

TL;DR: 利用合成困难负样本强化ViT自监督表征学习，显著提升DeiT-S与Swin-T性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习中“困难负样本”潜力被忽视，尤其在视觉Transformer（ViT）上鲜有系统研究；现有方法多在CNN或不生成真正困难负样本的设置中，导致表征判别性不足。

Method: 在ViT自监督框架中显式生成并注入“合成困难负样本”（hard negatives），作为对比学习的负对，以增强区分度；方法简单、可直接集成于DeiT-S与Swin-T训练流程。

Result: 在DeiT-S与Swin-T上均带来稳定性能提升（准确率/下游任务指标更高）；学习到的表示更具判别性。

Conclusion: 无需新架构，仅通过在ViT自监督训练中加入合成困难负样本即可显著加强表示质量与下游表现，说明硬负样本在ViT自监督中的价值被低估。

Abstract: This paper does not introduce a novel method per se. Instead, we address the
neglected potential of hard negative samples in self-supervised learning.
Previous works explored synthetic hard negatives but rarely in the context of
vision transformers. We build on this observation and integrate synthetic hard
negatives to improve vision transformer representation learning. This simple
yet effective technique notably improves the discriminative power of learned
representations. Our experiments show performance improvements for both DeiT-S
and Swin-T architectures.

</details>


### [178] [See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems](https://arxiv.org/abs/2509.02028)
*Halima Bouzidi,Haoyu Liu,Mohammad Al Faruque*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Language-vision understanding has driven the development of advanced
perception systems, most notably the emerging paradigm of Referring
Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT
systems can selectively track objects that satisfy a given semantic
description, guided through Transformer-based spatial-temporal reasoning
modules. End-to-End (E2E) RMOT models further unify feature extraction,
temporal memory, and spatial reasoning within a Transformer backbone, enabling
long-range spatial-temporal modeling over fused textual-visual representations.
Despite these advances, the reliability and robustness of RMOT remain
underexplored. In this paper, we examine the security implications of RMOT
systems from a design-logic perspective, identifying adversarial
vulnerabilities that compromise both the linguistic-visual referring and
track-object matching components. Additionally, we uncover a novel
vulnerability in advanced RMOT models employing FIFO-based memory, whereby
targeted and consistent attacks on their spatial-temporal reasoning introduce
errors that persist within the history buffer over multiple subsequent frames.
We present VEIL, a novel adversarial framework designed to disrupt the unified
referring-matching mechanisms of RMOT models. We show that carefully crafted
digital and physical perturbations can corrupt the tracking logic reliability,
inducing track ID switches and terminations. We conduct comprehensive
evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL
and demonstrate the urgent need for security-aware RMOT designs for critical
large-scale applications.

</details>


### [179] [Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives](https://arxiv.org/abs/2509.02029)
*Nikolaos Giakoumoglou,Andreas Floros,Kleanthis Marios Papadopoulos,Tania Stathaki*

Main category: cs.CV

TL;DR: 提出Syn2Co框架：用生成模型提供合成图像数据与在表示空间生成合成“硬负例”，以增强对比式自监督学习在ViT(DeiT-S、Swin-T)上的表示学习与迁移能力；结果显示有潜力但也有局限。


<details>
  <summary>Details</summary>
Motivation: 传统对比自监督需要大量真实数据和精心挑选的硬负例，成本高、可扩展性与数据受限；作者想检验“能否用合成数据与合成硬负例来部分替代”，以降低数据与负例获取的依赖。

Method: 在视觉Transformer框架下，两个“假装”途径：1) 生成模型产生合成图像以扩增分布与多样性，用于无监督表征学习；2) 在表示空间程序化地产生合成硬负例，增加困难对比。将两者结合为Syn2Co，并在DeiT-S与Swin-T上进行对比自监督训练与评估。

Result: 实验证明：合成图像和合成硬负例能提升表征鲁棒性与迁移性，但改进幅度与场景相关，存在边际与分布偏移问题；显示出潜力与限制并存。

Conclusion: 合成数据与合成硬负例是对比自监督的可行增强手段，可减少对真实大规模数据与人工难负例的依赖；但需解决合成数据质量、分布差异与生成策略设计等问题，未来应优化生成质量与负例构造机制。

Abstract: This paper does not introduce a new method per se. Instead, we build on
existing self-supervised learning approaches for vision, drawing inspiration
from the adage "fake it till you make it". While contrastive self-supervised
learning has achieved remarkable success, it typically relies on vast amounts
of real-world data and carefully curated hard negatives. To explore
alternatives to these requirements, we investigate two forms of "faking it" in
vision transformers. First, we study the potential of generative models for
unsupervised representation learning, leveraging synthetic data to augment
sample diversity. Second, we examine the feasibility of generating synthetic
hard negatives in the representation space, creating diverse and challenging
contrasts. Our framework - dubbed Syn2Co - combines both approaches and
evaluates whether synthetically enhanced training can lead to more robust and
transferable visual representations on DeiT-S and Swin-T architectures. Our
findings highlight the promise and limitations of synthetic data in
self-supervised learning, offering insights for future work in this direction.

</details>


### [180] [ContextFusion and Bootstrap: An Effective Approach to Improve Slot Attention-Based Object-Centric Learning](https://arxiv.org/abs/2509.02032)
*Pinzhuo Tian,Shengjie Yang,Hang Yu,Alex C. Kot*

Main category: cs.CV

TL;DR: 提出在插槽注意力(object-centric)框架中加入ContextFusion阶段与Bootstrap Branch，利用前景/背景语义与自举式特征适配，缓解仅依赖低层特征与编码器不可调的两大痛点，显著提升多数据集分解与下游表现。


<details>
  <summary>Details</summary>
Motivation: 现有slot attention依赖颜色/纹理等低层特征进行区域到槽的分配，难以捕捉轮廓、形状等高层语义；且为保证从槽重建稳定，通常冻结或弱更新编码器，导致难以进行有效的特征适配与泛化。

Method: 在任意插槽注意力模型前/中加入两组件：1) ContextFusion阶段：从前景与背景提取并融合语义，辅以一个辅助指示器，作为上下文线索增强槽分配的语义信息；2) Bootstrap Branch：将特征适配与重建解耦，使用自举策略训练一个可适配的特征变换/门控机制，使编码器可灵活微调而不破坏重建稳定性。两者可无缝集成于现有架构。

Result: 在多个模拟与真实世界数据集上，相比各类SOTA插槽注意力模型，加入本方法后指标显著提升；泛化与鲁棒性提高，对低层变化（颜色、纹理）敏感性下降。

Conclusion: 通过引入语义上下文融合与自举式特征适配，方法缓解了语义贫乏与编码器不可调问题，为无监督对象中心学习提供更稳定、可迁移的框架，可作为通用插件提升现有slot attention模型。

Abstract: A key human ability is to decompose a scene into distinct objects and use
their relationships to understand the environment. Object-centric learning aims
to mimic this process in an unsupervised manner. Recently, the slot
attention-based framework has emerged as a leading approach in this area and
has been widely used in various downstream tasks. However, existing slot
attention methods face two key limitations: (1) a lack of high-level semantic
information. In current methods, image areas are assigned to slots based on
low-level features such as color and texture. This makes the model overly
sensitive to low-level features and limits its understanding of object
contours, shapes, or other semantic characteristics. (2) The inability to
fine-tune the encoder. Current methods require a stable feature space
throughout training to enable reconstruction from slots, which restricts the
flexibility needed for effective object-centric learning. To address these
limitations, we propose a novel ContextFusion stage and a Bootstrap Branch,
both of which can be seamlessly integrated into existing slot attention models.
In the ContextFusion stage, we exploit semantic information from the foreground
and background, incorporating an auxiliary indicator that provides additional
contextual cues about them to enrich the semantic content beyond low-level
features. In the Bootstrap Branch, we decouple feature adaptation from the
original reconstruction phase and introduce a bootstrap strategy to train a
feature-adaptive mechanism, allowing for more flexible adaptation. Experimental
results show that our method significantly improves the performance of
different SOTA slot attention models on both simulated and real-world datasets.

</details>


### [181] [A Data-Centric Approach to Pedestrian Attribute Recognition: Synthetic Augmentation via Prompt-driven Diffusion Models](https://arxiv.org/abs/2509.02099)
*Alejandro Alonso,Sawaiz A. Chaudhry,Juan C. SanMiguel,Álvaro García-Martín,Pablo Ayuso-Albizu,Pablo Carballeira*

Main category: cs.CV

TL;DR: 提出一种以数据为中心的PAR增强方法：用文本引导扩散模型生成有针对性的行人合成图像，结合规则标注与损失修改融入训练，从而提升稀缺属性与总体性能，并带来零样本泛化增强。


<details>
  <summary>Details</summary>
Motivation: 现有PAR方法多在模型结构上做复杂设计，但受训练数据分布偏斜与长尾属性稀缺限制，许多属性识别弱、泛化差。需要一种无需改模型、能弥补数据缺口并提升弱属性与整体性能的方案。

Method: 1) 跨数据集协议：识别多数据集上“弱识别/低代表”的属性；2) 文本提示驱动的合成：基于扩散模型按属性文本提示生成行人图像，并保持与PAR数据集风格与标注一致性；3) 训练整合策略：依据提示规则为合成样本赋予属性标签，并对损失函数做调整（如重加权/正负样本平衡），将合成样本无缝纳入训练。

Result: 在主流PAR数据集上，针对性合成数据显著提升了稀缺属性的识别指标，同时非目标属性也得到整体提升；在零样本场景下无需改动网络架构即可增强泛化能力，整体性能超越基线与若干现有方法。

Conclusion: 文本引导的扩散合成与规则化标注、损失调整相结合，提供了一种高效、可扩展的PAR数据增广方案，可补齐长尾属性并提升整体与零样本性能，无需更改模型架构。

Abstract: Pedestrian Attribute Recognition (PAR) is a challenging task as models are
required to generalize across numerous attributes in real-world data.
Traditional approaches focus on complex methods, yet recognition performance is
often constrained by training dataset limitations, particularly the
under-representation of certain attributes. In this paper, we propose a
data-centric approach to improve PAR by synthetic data augmentation guided by
textual descriptions. First, we define a protocol to identify weakly recognized
attributes across multiple datasets. Second, we propose a prompt-driven
pipeline that leverages diffusion models to generate synthetic pedestrian
images while preserving the consistency of PAR datasets. Finally, we derive a
strategy to seamlessly incorporate synthetic samples into training data, which
considers prompt-based annotation rules and modifies the loss function. Results
on popular PAR datasets demonstrate that our approach not only boosts
recognition of underrepresented attributes but also improves overall model
performance beyond the targeted attributes. Notably, this approach strengthens
zero-shot generalization without requiring architectural changes of the model,
presenting an efficient and scalable solution to improve the recognition of
attributes of pedestrians in the real world.

</details>


### [182] [SALAD -- Semantics-Aware Logical Anomaly Detection](https://arxiv.org/abs/2509.02101)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: SALAD 提出一种语义感知的逻辑异常检测方法，通过显式建模“组成图（composition maps）”的分布来学习部件间语义关系，在无需人工标注或类别先验的情况下，显著提升 MVTec LOCO 基准上的性能（图像级 AUROC 96.1%）。


<details>
  <summary>Details</summary>
Motivation: 现有表面缺陷检测多擅长几何/结构异常（划痕、凹痕），却难以识别逻辑异常（部件缺失/错位/不合逻辑的组合）。当前较优的逻辑异常方法大多依赖聚合的预训练特征或手工制定的组成图描述子，这些做法会丢失空间与语义信息，导致性能受限。

Method: 提出 SALAD：引入新颖的“组成分支（composition branch）”，显式建模对象组成图的分布，从而学习关键语义关系；进一步提出一种全新的组成图提取流程，无需手工标注或类别特定信息。整体是语义感知且判别式的逻辑异常检测框架，通过对组成图分布的有效建模来发现逻辑异常。

Result: 在逻辑异常检测标准基准 MVTec LOCO 上显著优于 SOTA，达到图像级 AUROC 96.1%。

Conclusion: 通过无标注/无类别先验的组成图提取与分布建模，SALAD 能更好捕获部件间语义关系，弥补以往方法在空间与语义信息上的缺失，实现对逻辑异常的强健检测，并刷新基准性能。

Abstract: Recent surface anomaly detection methods excel at identifying structural
anomalies, such as dents and scratches, but struggle with logical anomalies,
such as irregular or missing object components. The best-performing logical
anomaly detection approaches rely on aggregated pretrained features or
handcrafted descriptors (most often derived from composition maps), which
discard spatial and semantic information, leading to suboptimal performance. We
propose SALAD, a semantics-aware discriminative logical anomaly detection
method that incorporates a newly proposed composition branch to explicitly
model the distribution of object composition maps, consequently learning
important semantic relationships. Additionally, we introduce a novel procedure
for extracting composition maps that requires no hand-made labels or
category-specific information, in contrast to previous methods. By effectively
modelling the composition map distribution, SALAD significantly improves upon
state-of-the-art methods on the standard benchmark for logical anomaly
detection, MVTec LOCO, achieving an impressive image-level AUROC of 96.1%.
Code: https://github.com/MaticFuc/SALAD

</details>


### [183] [NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking](https://arxiv.org/abs/2509.02111)
*Benjamin Missaoui,Orcun Cetintas,Guillem Brasó,Tim Meinhardt,Laura Leal-Taixé*

Main category: cs.CV

TL;DR: 提出NOOUGAT：一个可在任意时间跨度运行的MOT框架，基于GNN处理非重叠子片段，并用自回归长程跟踪层ALT进行融合，统一在线与离线跟踪；在多数据集上达SOTA，在线模式显著提升AssA。


<details>
  <summary>Details</summary>
Motivation: 现实应用的时延需求多样，现有在线/离线MOT割裂：在线依赖逐帧启发式关联，难以处理长遮挡；离线虽能覆盖更长时间，但仍靠启发式拼接，难以扩展到任意长序列。需要一个既能低延迟又能利用长时上下文、可灵活切换时间范围的统一方法。

Method: - 统一GNN框架：对视频划分为不重叠子片段（subclips），在各子片段内用GNN建模目标之间的时空关系与关联。
- ALT（Autoregressive Long-term Tracking）层：自回归地将已处理子片段的轨迹信息与当前子片段的GNN表示融合，实现跨子片段的长程关联与轨迹延展。
- 子片段尺寸为可调超参：控制时延与上下文的权衡，实现从逐帧（极低延迟）到批量（更长上下文）的统一部署。

Result: 在多基准数据集取得SOTA：在线模式下AssA提升——DanceTrack +2.3，SportsMOT +9.2，MOT20 +5.0；离线模式还有更大提升。

Conclusion: NOOUGAT打破在线/离线二元分割，提供一个可伸缩的时域统一MOT方案。通过GNN子片段建模与ALT层的跨片段自回归融合，在不同时延与上下文需求下均实现更优跟踪表现，适用于从实时到离线的多种部署场景。

Abstract: The long-standing division between \textit{online} and \textit{offline}
Multi-Object Tracking (MOT) has led to fragmented solutions that fail to
address the flexible temporal requirements of real-world deployment scenarios.
Current \textit{online} trackers rely on frame-by-frame hand-crafted
association strategies and struggle with long-term occlusions, whereas
\textit{offline} approaches can cover larger time gaps, but still rely on
heuristic stitching for arbitrarily long sequences. In this paper, we introduce
NOOUGAT, the first tracker designed to operate with arbitrary temporal
horizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that
processes non-overlapping subclips, and fuses them through a novel
Autoregressive Long-term Tracking (ALT) layer. The subclip size controls the
trade-off between latency and temporal context, enabling a wide range of
deployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves
state-of-the-art performance across both tracking regimes, improving
\textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on
MOT20, with even greater gains in \textit{offline} mode.

</details>


### [184] [SegFormer Fine-Tuning with Dropout: Advancing Hair Artifact Removal in Skin Lesion Analysis](https://arxiv.org/abs/2509.02156)
*Asif Mohammed Saad,Umme Niraj Mahi*

Main category: cs.CV

TL;DR: 提出在皮肤镜图像中用带dropout的SegFormer进行毛发掩膜分割，10折交叉验证取得Dice≈0.96/IoU≈0.93等高分，显示对下游皮肤癌检测的预处理价值。


<details>
  <summary>Details</summary>
Motivation: 皮肤镜图像中的毛发会遮挡病灶关键特征，影响诊断与自动分析。现有去毛发/分割方法在复杂形态、细毛/密集毛发、不同光照和肤色条件下容易过拟合或精度不足，因此需要鲁棒、可泛化的毛发分割模型以提升后续病灶分析的可靠性。

Method: 在SegFormer框架上引入dropout正则化的分割头（p=0.3），采用MiT-B2编码器（ImageNet预训练，输入3通道，输出2类）。数据：500张带精细毛发掩膜标注的皮肤镜图像；训练策略：10折交叉验证、AdamW(lr=1e-3)、交叉熵损失、早停（patience=3，单折最多20 epoch）。评估指标：IoU、Dice、PSNR、SSIM、LPIPS。

Result: 跨折平均：Dice≈0.96、IoU≈0.93；PSNR≈34 dB、SSIM≈0.97、LPIPS≈0.06，显示分割准确且感知质量良好、伪影少。

Conclusion: 带dropout的SegFormer在毛发掩膜分割上表现稳健，有望作为皮肤癌计算机辅助诊断的有效预处理步骤。未来可在更大、更多域的数据集上验证泛化，并与端到端病灶分析流程集成评估总体收益。

Abstract: Hair artifacts in dermoscopic images present significant challenges for
accurate skin lesion analysis, potentially obscuring critical diagnostic
features in dermatological assessments. This work introduces a fine-tuned
SegFormer model augmented with dropout regularization to achieve precise hair
mask segmentation. The proposed SegformerWithDropout architecture leverages the
MiT-B2 encoder, pretrained on ImageNet, with an in-channel count of 3 and 2
output classes, incorporating a dropout probability of 0.3 in the segmentation
head to prevent overfitting. Training is conducted on a specialized dataset of
500 dermoscopic skin lesion images with fine-grained hair mask annotations,
employing 10-fold cross-validation, AdamW optimization with a learning rate of
0.001, and cross-entropy loss. Early stopping is applied based on validation
loss, with a patience of 3 epochs and a maximum of 20 epochs per fold.
Performance is evaluated using a comprehensive suite of metrics, including
Intersection over Union (IoU), Dice coefficient, Peak Signal-to-Noise Ratio
(PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch
Similarity (LPIPS). Experimental results from the cross-validation demonstrate
robust performance, with average Dice coefficients reaching approximately 0.96
and IoU values of 0.93, alongside favorable PSNR (around 34 dB), SSIM (0.97),
and low LPIPS (0.06), highlighting the model's effectiveness in accurate hair
artifact segmentation and its potential to enhance preprocessing for downstream
skin cancer detection tasks.

</details>


### [185] [Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models](https://arxiv.org/abs/2509.02161)
*Pablo Ayuso-Albizu,Juan C. SanMiguel,Pablo Carballeira*

Main category: cs.CV

TL;DR: 用扩散模型做img2img数据扩增，面向行人属性识别（PAR）；通过系统分析提示词与图像属性等参数，选出最佳方案来生成合成行人图像，帮助零样本数据训练；最终带来约4.5%的识别性能提升。


<details>
  <summary>Details</summary>
Motivation: PAR在复杂场景（遮挡、姿态变化、环境多样）中受限于大规模标注数据稀缺，导致泛化弱。近年扩散模型能生成多样且真实的图像，但其用于PAR风格的合成数据扩增研究不足；若可行，将提升真实世界鲁棒性与适应性。

Method: 系统性研究img2img扩散式数据扩增的关键要素：1) 文本提示词（prompt）与任务对齐策略；2) 输入图像属性（如姿态、背景、分辨率、遮挡程度）；3) 近期扩散增强技巧。对不同组合进行对比，评估生成质量与下游PAR效果。选取最优配置生成大规模合成行人图像，扩充零样本数据用于训练PAR模型。

Result: 实验证明：提示词与图像属性对生成质量与下游表现最关键。采用最优配置的扩增方案，PAR识别性能提升约4.5%。

Conclusion: 扩散模型可有效为PAR生成任务相关的高质量合成样本；合理的prompt对齐与图像属性选择是性能提升的关键。该方法能在标注稀缺、零样本场景下增强PAR的鲁棒性与泛化能力。

Abstract: Pedestrian Attribute Recognition (PAR) involves identifying various human
attributes from images with applications in intelligent monitoring systems. The
scarcity of large-scale annotated datasets hinders the generalization of PAR
models, specially in complex scenarios involving occlusions, varying poses, and
diverse environments. Recent advances in diffusion models have shown promise
for generating diverse and realistic synthetic images, allowing to expand the
size and variability of training data. However, the potential of
diffusion-based data expansion for generating PAR-like images remains
underexplored. Such expansion may enhance the robustness and adaptability of
PAR models in real-world scenarios. This paper investigates the effectiveness
of diffusion models in generating synthetic pedestrian images tailored to PAR
tasks. We identify key parameters of img2img diffusion-based data expansion;
including text prompts, image properties, and the latest enhancements in
diffusion-based data augmentation, and examine their impact on the quality of
generated images for PAR. Furthermore, we employ the best-performing expansion
approach to generate synthetic images for training PAR models, by enriching the
zero-shot datasets. Experimental results show that prompt alignment and image
properties are critical factors in image generation, with optimal selection
leading to a 4.5% improvement in PAR recognition performance.

</details>


### [186] [Omnidirectional Spatial Modeling from Correlated Panoramas](https://arxiv.org/abs/2509.02164)
*Xinshen Zhang,Tongxi Fu,Xu Zheng*

Main category: cs.CV

TL;DR: 提出CFpano首个跨帧相关全景VQA数据集与基于GRPO微调的MLLM方法，在多选与开放式任务上显著优于现有模型（总体+5.37%），为360°场景理解设立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有360°场景理解多聚焦单帧，忽视跨帧全景间的相关性；而全景图像存在几何畸变与复杂空间关系，导致在下游如具身智能、自动驾驶、沉浸式环境中表现受限。亟需能够在多帧间进行一致、稳健推理的基准与方法。

Method: 1) 构建CFpano：首个面向跨帧相关全景的VQA基准，包含2700+图像与8000+问答，含多选与开放式题型。2) 提出一种MLLM（未给出具体名，以\methodname代称），使用Group Relative Policy Optimization（GRPO）进行对齐微调，并设计多种奖励函数，鼓励跨帧一致性与稳健推理。3) 在CFpano上对现有MLLM进行基线评测并与所提方法对比。

Result: 所提MLLM在CFpano上于多选和开放式VQA两类任务均取得SOTA，在主要推理类别上全面超越强基线，总体性能提升+5.37%。消融与分析验证了GRPO与奖励设计对提升跨帧推理一致性的有效性。

Conclusion: CFpano填补了跨帧全景VQA基准空缺，结合GRPO微调的MLLM显著提升360°跨帧场景推理效果，建立了新的评测基准并为全景场景理解提供有效训练与评测资源。

Abstract: Omnidirectional scene understanding is vital for various downstream
applications, such as embodied AI, autonomous driving, and immersive
environments, yet remains challenging due to geometric distortion and complex
spatial relations in 360{\deg} imagery. Existing omnidirectional methods
achieve scene understanding within a single frame while neglecting cross-frame
correlated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the
\textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas
visual question answering in the holistic 360{\deg} scenes. CFpano consists of
over 2700 images together with over 8000 question-answer pairs, and the
question types include both multiple choice and open-ended VQA. Building upon
our CFpano, we further present \methodname, a multi-modal large language model
(MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of
tailored reward functions for robust and consistent reasoning with cross-frame
correlated panoramas. Benchmark experiments with existing MLLMs are conducted
with our CFpano. The experimental results demonstrate that \methodname achieves
state-of-the-art performance across both multiple-choice and open-ended VQA
tasks, outperforming strong baselines on all major reasoning categories
(\textbf{+5.37\%} in overall performance). Our analyses validate the
effectiveness of GRPO and establish a new benchmark for panoramic scene
understanding.

</details>


### [187] [Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks](https://arxiv.org/abs/2509.02175)
*Nils Hoehing,Mayug Maniparambil,Ellen Rushe,Noel E. O'Connor,Anthony Ventresque*

Main category: cs.CV

TL;DR: RocketScience 是一个针对空间关系理解的开源对比式多模态（VLM）基准，使用全新真实图文对，强调相对空间与物体顺序；对人类容易、对现有VLM困难。结果显示多数VLM在空间关系上薄弱，推理型模型表现意外出色；进一步分析表明瓶颈在空间推理非定位。数据与评测代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有VLM常在空间关系与物体相对位置理解上存在盲点，且多数基准可能不足以区分“看见物体”与“理解物体间关系”。作者希望提供一个对人类简单但能显著拉开VLM差距的测试，以客观评估并诊断空间理解能力，推动模型改进。

Method: 构建一个对比式基准：包含全新真实世界图文对，聚焦相对空间关系与物体顺序；设计任务使人类几乎不出错而VLM易犯错；测试开源与商用前沿VLM及推理型模型；对基于CoT的模型做解耦分析，将物体定位与空间推理的贡献分离，评估性能瓶颈来源。

Result: 1) 多数开源与商用前沿VLM在该基准上表现显著不足；2) 推理型模型（具链式思维/长推理能力）出乎意料地更强；3) 解耦分析显示性能受限主要来自空间推理能力，而非物体定位。

Conclusion: 当前VLM缺乏稳健的空间关系理解，提升应聚焦空间推理机制而非仅改进检测/定位。RocketScience作为开源基准与代码可用于客观评测与研究，预期将促进该方向的方法发展。

Abstract: We propose RocketScience, an open-source contrastive VLM benchmark that tests
for spatial relation understanding. It is comprised of entirely new real-world
image-text pairs covering mostly relative spatial understanding and the order
of objects. The benchmark is designed
  to be very easy for humans and hard for the current generation of VLMs, and
this is empirically verified. Our results show a striking lack of spatial
relation understanding in open source and frontier commercial VLMs and a
surprisingly high performance of reasoning models. Additionally, we perform a
disentanglement analysis to separate the contributions of object localization
and spatial reasoning in chain-of-thought-based models and find that the
performance on the benchmark is bottlenecked by spatial reasoning and not
object localization capabilities.
  We release the dataset with a CC-BY-4.0 license and make the evaluation code
available at: https://github.com/nilshoehing/rocketscience

</details>


### [188] [ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking](https://arxiv.org/abs/2509.02182)
*Shyma Alhuwaider,Motasem Alfarra,Juan C. Perez,Merey Ramazanova,Bernard Ghanem*

Main category: cs.CV

TL;DR: 提出基于tracklet的全新TTA基准ITD，真实体现时间依赖；并通过实验揭示现有TTA在时间相关场景的不足，提出对记忆模块的对抗式初始化显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA基准主要模拟分布偏移与非IID，但忽略现实中普遍存在的时间依赖（如视频连续帧同一目标），导致评价与实际部署脱节。

Method: 1) 构建ITD数据集：从目标跟踪数据集的检测框中抽取对象中心的tracklet序列，确保天然时间相关性；2) 系统评测多种主流TTA方法于ITD上，分析其在时间依赖条件下的薄弱点；3) 提出“对抗式记忆初始化”策略，为基于记忆的TTA方法提供更鲁棒的初始状态，以缓解因时间相关与漂移带来的误适应。

Result: 在ITD上，多数现有TTA方法在存在强时间相关时性能明显下降；采用对抗式记忆初始化后，多种记忆型TTA方法在该基准上取得显著性能提升。

Conclusion: 时间依赖是TTA评估中不可忽视的关键因素；ITD为更贴近真实部署的评测提供了平台。所提对抗式记忆初始化有效提升了记忆型TTA在时间依赖场景下的稳健性与效果。

Abstract: We introduce a novel tracklet-based dataset for benchmarking test-time
adaptation (TTA) methods. The aim of this dataset is to mimic the intricate
challenges encountered in real-world environments such as images captured by
hand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus
on how models face distribution shifts, when deployed, and on violations to the
customary independent-and-identically-distributed (i.i.d.) assumption in
machine learning. Yet, these benchmarks fail to faithfully represent realistic
scenarios that naturally display temporal dependencies, such as how consecutive
frames from a video stream likely show the same object across time. We address
this shortcoming of current datasets by proposing a novel TTA benchmark we call
the "Inherent Temporal Dependencies" (ITD) dataset. We ensure the instances in
ITD naturally embody temporal dependencies by collecting them from
tracklets-sequences of object-centric images we compile from the bounding boxes
of an object-tracking dataset. We use ITD to conduct a thorough experimental
analysis of current TTA methods, and shed light on the limitations of these
methods when faced with the challenges of temporal dependencies. Moreover, we
build upon these insights and propose a novel adversarial memory initialization
strategy to improve memory-based TTA methods. We find this strategy
substantially boosts the performance of various methods on our challenging
benchmark.

</details>


### [189] [Palmistry-Informed Feature Extraction and Analysis using Machine Learning](https://arxiv.org/abs/2509.02248)
*Shweta Patil*

Main category: cs.CV

TL;DR: 用计算机视觉从手掌图像中提取线条、纹理与形状等特征，并用机器学习在新建标注数据集上建模，验证了以数据驱动方式将掌形与外部特征/状态相关联的可行性，适用于数字体测与个性化分析，且可移动端部署。


<details>
  <summary>Details</summary>
Motivation: 传统掌纹/掌相分析多依赖主观判断、缺乏量化与可验证性。作者希望以客观、可重复的计算方法探索手掌形态与外部经验证的特质或状态之间的相关性，并评估其在数字人类测量与个性化应用中的潜力。

Method: 构建计算机视觉流水线，从手掌图像自动提取主线结构、纹理与形状度量等特征；基于带标注的新数据集训练预测模型；以数据驱动方式评估特征与外部标签的相关与预测能力，并考虑移动端可部署性。

Result: 模型能从手掌数据中识别复杂模式，显示特征与目标标签存在可利用的统计关系；整体流程在实验数据上表现出可行性。

Conclusion: 自动化的掌部特征提取与机器学习建模可为掌形研究提供量化框架，支持数字体测与个性化分析的应用场景，并为连接文化实践与计算方法的研究打开新方向。

Abstract: This paper explores the automated analysis of palmar features using machine
learning techniques. We present a computer vision pipeline that extracts key
characteristics from palm images, such as principal line structures, texture,
and shape metrics. These features are used to train predictive models on a
novel dataset curated from annotated palm images. Our approach moves beyond
traditional subjective interpretation by providing a data-driven, quantitative
framework for studying the correlations between palmar morphology and
externally validated traits or conditions. The methodology demonstrates
feasibility for applications in digital anthropometry and personalized user
analytics, with potential for deployment on mobile platforms. Results indicate
that machine learning models can identify complex patterns in palm data,
opening avenues for research that intersects cultural practices with
computational analysis.

</details>


### [190] [A Multimodal Cross-View Model for Predicting Postoperative Neck Pain in Cervical Spondylosis Patients](https://arxiv.org/abs/2509.02256)
*Jingyang Shan,Qishuai Yu,Jiacen Liu,Shaolin Zhang,Wen Shen,Yanxiao Zhao,Tianyi Wang,Xiaolin Qin,Yiheng Yin*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Neck pain is the primary symptom of cervical spondylosis, yet its underlying
mechanisms remain unclear, leading to uncertain treatment outcomes. To address
the challenges of multimodal feature fusion caused by imaging differences and
spatial mismatches, this paper proposes an Adaptive Bidirectional Pyramid
Difference Convolution (ABPDC) module that facilitates multimodal integration
by exploiting the advantages of difference convolution in texture extraction
and grayscale invariance, and a Feature Pyramid Registration Auxiliary Network
(FPRAN) to mitigate structural misalignment. Experiments on the MMCSD dataset
demonstrate that the proposed model achieves superior prediction accuracy of
postoperative neck pain recovery compared with existing methods, and ablation
studies further confirm its effectiveness.

</details>


### [191] [DSGC-Net: A Dual-Stream Graph Convolutional Network for Crowd Counting via Feature Correlation Mining](https://arxiv.org/abs/2509.02261)
*Yihong Wu,Jinqiao Wei,Xionghui Zhao,Yidi Li,Shaoyi Du,Bin Ren,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出DSGC-Net：通过双分支图卷积挖掘密度与表征相关性，在多密度、多视角/姿态人群计数上优于SOTA（如ShanghaiTech A/B上MAE 48.9/5.9）。


<details>
  <summary>Details</summary>
Motivation: 现有人群计数在复杂场景中难以适应区域间巨大密度差异，且因视角与姿态变化导致个体表征不一致，影响计数精度。需要一种能显式建模密度变化与表征变化相关性的框架。

Method: 双流架构：密度近似（DA）与表征近似（RA）。(1) DA分支：预测密度图，依据像素/区域的密度相似性构建密度驱动语义图；(2) RA分支：计算全局表征相似性构建表征驱动语义图；(3) 分别对两张语义图施加GCN以建模潜在语义关系，并融合两分支特征用于计数；核心是通过图结构显式挖掘“密度变化相关性”和“表征分布相关性”。

Result: 在三个常用数据集上超越SOTA；在ShanghaiTech Part A/B上MAE分别达到48.9/5.9，显示在高拥挤与相对稀疏场景中的稳健性。

Conclusion: 通过密度和表征的双语义图与GCN建模，提升模型对密度变化与多视角/多姿态的适应能力，从而显著提高计数精度；代码已开源，具有可复现性与推广潜力。

Abstract: Deep learning-based crowd counting methods have achieved remarkable progress
in recent years. However, in complex crowd scenarios, existing models still
face challenges when adapting to significant density distribution differences
between regions. Additionally, the inconsistency of individual representations
caused by viewpoint changes and body posture differences further limits the
counting accuracy of the models. To address these challenges, we propose
DSGC-Net, a Dual-Stream Graph Convolutional Network based on feature
correlation mining. DSGC-Net introduces a Density Approximation (DA) branch and
a Representation Approximation (RA) branch. By modeling two semantic graphs, it
captures the potential feature correlations in density variations and
representation distributions. The DA branch incorporates a density prediction
module that generates the density distribution map, and constructs a
density-driven semantic graph based on density similarity. The RA branch
establishes a representation-driven semantic graph by computing global
representation similarity. Then, graph convolutional networks are applied to
the two semantic graphs separately to model the latent semantic relationships,
which enhance the model's ability to adapt to density variations and improve
counting accuracy in multi-view and multi-pose scenarios. Extensive experiments
on three widely used datasets demonstrate that DSGC-Net outperforms current
state-of-the-art methods. In particular, we achieve MAE of 48.9 and 5.9 in
ShanghaiTech Part A and Part B datasets, respectively. The released code is
available at: https://github.com/Wu-eon/CrowdCounting-DSGCNet.

</details>


### [192] [RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution Detection in Remote Sensing](https://arxiv.org/abs/2509.02273)
*Yingrui Ji,Jiansheng Chen,Jingbo Chen,Anzhi Yue,Chenhao Wang,Kai Li,Yao Zhu*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Out-of-distribution (OOD) detection represents a critical challenge in remote
sensing applications, where reliable identification of novel or anomalous
patterns is essential for autonomous monitoring, disaster response, and
environmental assessment. Despite remarkable progress in OOD detection for
natural images, existing methods and benchmarks remain poorly suited to remote
sensing imagery due to data scarcity, complex multi-scale scene structures, and
pronounced distribution shifts. To this end, we propose RS-OOD, a novel
framework that leverages remote sensing-specific vision-language modeling to
enable robust few-shot OOD detection. Our approach introduces three key
innovations: spatial feature enhancement that improved scene discrimination, a
dual-prompt alignment mechanism that cross-verifies scene context against
fine-grained semantics for spatial-semantic consistency, and a
confidence-guided self-training loop that dynamically mines pseudo-labels to
expand training data without manual annotation. RS-OOD consistently outperforms
existing methods across multiple remote sensing benchmarks and enables
efficient adaptation with minimal labeled data, demonstrating the critical
value of spatial-semantic integration.

</details>


### [193] [SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images](https://arxiv.org/abs/2509.02287)
*Pushpendra Dhakara,Prachi Chachodhia,Vaibhav Kumar*

Main category: cs.CV

TL;DR: 提出SynthGenNet自监督学生-教师框架，结合ClassMix++、GMC损失与PLGCL对抗城市复杂场景域移，利用多源合成数据在测试时实现域泛化，在真实数据（如IDD）上达50% mIoU，优于单源SOTA。


<details>
  <summary>Details</summary>
Motivation: 城市非结构化场景在布局多样、标注稀缺、域间差异大（sim-to-real）下，现有方法（尤其单源）泛化差、对目标域标注依赖高。作者希望用多源合成数据与自监督策略，在无需目标域标注的情况下提升测试时域泛化能力。

Method: - 架构：学生-教师（teacher给软目标/伪标签，student学习并迭代蒸馏）。
- 数据：多源合成图像。
- ClassMix++：从多源标注图中进行类别级掩膜混合，保持语义连贯，生成更具多样性的训练样本。
- GMC（Grounded Mask Consistency Loss）：利用源域真值约束跨域预测的一致性与特征对齐，提高跨域稳健性。
- PLGCL（Pseudo-Label Guided Contrastive Learning）：在学生网络中用教师伪标签进行对比学习，拉近同类、分离异类，实现域不变特征学习。
- 测试时域泛化：无需目标域标注，依靠自监督损失和教师引导进行适应。

Result: 在真实数据集（如印度驾驶数据集IDD）上达到50% mIoU，超过依赖单一源数据的SOTA方法；表明在复杂城市场景下具有更强的泛化与鲁棒性。

Conclusion: 多源合成数据+自监督学生-教师框架，通过ClassMix++、GMC和PLGCL协同，显著缩小sim-to-real域间差距并提升无标注目标域的分割性能，适用于复杂城市环境的测试时域泛化。

Abstract: Unstructured urban environments present unique challenges for scene
understanding and generalization due to their complex and diverse layouts. We
introduce SynthGenNet, a self-supervised student-teacher architecture designed
to enable robust test-time domain generalization using synthetic multi-source
imagery. Our contributions include the novel ClassMix++ algorithm, which blends
labeled data from various synthetic sources while maintaining semantic
integrity, enhancing model adaptability. We further employ Grounded Mask
Consistency Loss (GMC), which leverages source ground truth to improve
cross-domain prediction consistency and feature alignment. The Pseudo-Label
Guided Contrastive Learning (PLGCL) mechanism is integrated into the student
network to facilitate domain-invariant feature learning through iterative
knowledge distillation from the teacher network. This self-supervised strategy
improves prediction accuracy, addresses real-world variability, bridges the
sim-to-real domain gap, and reliance on labeled target data, even in complex
urban areas. Outcomes show our model outperforms the state-of-the-art (relying
on single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on
real-world datasets like Indian Driving Dataset (IDD).

</details>


### [194] [Data-Driven Loss Functions for Inference-Time Optimization in Text-to-Image Generation](https://arxiv.org/abs/2509.02295)
*Sapir Esther Yiflach,Yuval Atzmon,Gal Chechik*

Main category: cs.CV

TL;DR: 提出 Learn-to-Steer：在扩散模型推理时，用从模型内部表征学习得到的“可学习损失”来优化空间关系生成，而非手工设计损失或微调。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在简单空间关系（左/右、上/下等）上频繁失败，尤其在不常见组合时更糟。现有修复方法依赖微调或手工损失，泛化与最优性欠佳。

Method: 1) 从扩散模型的跨注意力图中训练轻量分类器，解码对象间空间关系；2) 在推理时，将该分类器作为学习得到的损失，驱动测试时优化（steering）；3) 识别训练中的“语言捷径”，提出dual-inversion策略，迫使分类器学习几何而非文本痕迹。

Result: 在标准基准上显著提升空间准确率：FLUX.1-dev从0.20到0.61，SD2.1从0.07到0.54；对多种关系具有良好泛化并显著提升准确。

Conclusion: 学习自模型内部表征的可学习损失能在不微调基础模型的情况下，大幅改善空间推理与生成的准确性，并可扩展到多关系场景。

Abstract: Text-to-image diffusion models can generate stunning visuals, yet they often
fail at tasks children find trivial--like placing a dog to the right of a teddy
bear rather than to the left. When combinations get more unusual--a giraffe
above an airplane--these failures become even more pronounced. Existing methods
attempt to fix these spatial reasoning failures through model fine-tuning or
test-time optimization with handcrafted losses that are suboptimal. Rather than
imposing our assumptions about spatial encoding, we propose learning these
objectives directly from the model's internal representations. We introduce
Learn-to-Steer, a novel framework that learns data-driven objectives for
test-time optimization rather than handcrafting them. Our key insight is to
train a lightweight classifier that decodes spatial relationships from the
diffusion model's cross-attention maps, then deploy this classifier as a
learned loss function during inference. Training such classifiers poses a
surprising challenge: they can take shortcuts by detecting linguistic traces
rather than learning true spatial patterns. We solve this with a dual-inversion
strategy that enforces geometric understanding. Our method dramatically
improves spatial accuracy: from 0.20 to 0.61 on FLUX.1-dev and from 0.07 to
0.54 on SD2.1 across standard benchmarks. Moreover, our approach generalizes to
multiple relations and significantly improves accuracy.

</details>


### [195] [Hues and Cues: Human vs. CLIP](https://arxiv.org/abs/2509.02305)
*Nuria Alabau-Bosque,Jorge Vila-Tomás,Paula Daudén-Oliver,Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Valero Laparra,Jesús Malo*

Main category: cs.CV

TL;DR: 用桌游“Hues & Cues”作为评测框架，测试CLIP的颜色感知与命名，发现其总体与人类一致但存在文化偏见与抽象层次不一致的问题，说明用游戏等多样化任务能暴露模型在常规基准难以发现的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统评测常忽视“人类式任务”（如游戏）对模型人性化特征的检验；颜色感知与命名兼具感知与语言映射，适合作为检验窗口。作者希望用真实互动性的游戏来评估模型的人类对齐度，并揭示标准基准难发现的问题。

Method: 选用桌游“Hues & Cues”，将其色彩识别与基于提示词的颜色定位/命名过程映射到CLIP的视觉-文本匹配：给定颜色与语言线索，让CLIP进行匹配与选择；比较CLIP在不同提示抽象层次下的表现，并与人类玩家数据对齐度进行评估；分析错误分布与文化因素影响。

Result: CLIP在总体上与人类观察者表现较一致，但在不同抽象层次的提示（具体色名 vs 抽象描述）下表现不稳定；出现文化偏见（如对特定文化中常见/特有色名或联想的偏好/误判）；这些问题通过该游戏设置被清晰暴露。

Conclusion: 桌游等多样化、情境化任务可作为评估AI“类人性”与多模态对齐的新途径，能揭示模型的文化偏见与抽象处理不一致等缺陷，补充并超越常规基准测试的覆盖范围。

Abstract: Playing games is inherently human, and a lot of games are created to
challenge different human characteristics. However, these tasks are often left
out when evaluating the human-like nature of artificial models. The objective
of this work is proposing a new approach to evaluate artificial models via
board games. To this effect, we test the color perception and color naming
capabilities of CLIP by playing the board game Hues & Cues and assess its
alignment with humans. Our experiments show that CLIP is generally well aligned
with human observers, but our approach brings to light certain cultural biases
and inconsistencies when dealing with different abstraction levels that are
hard to identify with other testing strategies. Our findings indicate that
assessing models with different tasks like board games can make certain
deficiencies in the models stand out in ways that are difficult to test with
the commonly used benchmarks.

</details>


### [196] [OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds](https://arxiv.org/abs/2509.02322)
*Longrong Yang,Zhixiong Zeng,Yufeng Zhong,Jing Huang,Liming Zheng,Lei Chen,Haibo Qiu,Zequn Qin,Lin Ma,Xi Li*

Main category: cs.CV

TL;DR: 提出通用多模态智能体OmniActor，以“浅层共享、深层分离”的层异质MoE结构融合GUI与实体(embodied)数据，统一动作空间并大规模混合训练，显著提升两类任务表现。


<details>
  <summary>Details</summary>
Motivation: 现实复杂任务往往需要同时在二维GUI虚拟环境与三维实体环境中交替互动；直接混合两类数据训练会因表示冲突导致性能退化，但又存在浅层表征互补的潜在协同，亟需一种能化解深层冲突、保留浅层协同的通用智能体框架。

Method: 1) 架构：提出Layer-heterogeneity MoE（层异质专家混合），浅层参数共享以吸收GUI与实体数据的协同，深层通过专家分离以消除语义/策略冲突；2) 数据与接口：统一GUI与实体任务的动作空间，构建并汇聚大规模多源GUI与实体数据；3) 训练：在统一动作表达下对多模态数据进行联合训练，利用MoE路由在不同深度选择专家。

Result: 相比仅用GUI或仅用实体数据训练的智能体，OmniActor在两类任务上均取得更优表现，尤其在GUI任务上提升显著；验证了浅层协同、深层冲突的分析与所提架构的有效性。

Conclusion: 通过结构上“浅层共享+深层分离”的MoE与数据上动作空间统一与大规模混合，OmniActor实现高性能通用多模态智能体，能跨GUI与实体场景稳健泛化；代码将开源。

Abstract: Multimodal large language models are evolving toward multimodal agents
capable of proactively executing tasks. Most agent research focuses on GUI or
embodied scenarios, which correspond to agents interacting with 2D virtual
worlds or 3D real worlds, respectively. However, many complex tasks typically
require agents to interleavely interact with these two types of environment. We
initially mix GUI and embodied data to train, but find the performance
degeneration brought by the data conflict. Further analysis reveals that GUI
and embodied data exhibit synergy and conflict at the shallow and deep layers,
respectively, which resembles the cerebrum-cerebellum mechanism in the human
brain. To this end, we propose a high-performance generalist agent OmniActor,
designed from both structural and data perspectives. First, we propose
Layer-heterogeneity MoE to eliminate the conflict between GUI and embodied data
by separating deep-layer parameters, while leverage their synergy by sharing
shallow-layer parameters. By successfully leveraging the synergy and
eliminating the conflict, OmniActor outperforms agents only trained by GUI or
embodied data in GUI or embodied tasks. Furthermore, we unify the action spaces
of GUI and embodied tasks, and collect large-scale GUI and embodied data from
various sources for training. This significantly improves OmniActor under
different scenarios, especially in GUI tasks. The code will be publicly
available.

</details>


### [197] [Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels](https://arxiv.org/abs/2509.02351)
*Alireza Sedighi Moghaddam,Mohammad Reza Mohammadi*

Main category: cs.CV

TL;DR: 提出ORDAC：用标签分布学习自适应纠正序数分类中的噪声标签，不丢样本而调整每样本分布均值/方差；在年龄估计与糖网严重度上显著提升（如40%噪声MAE 0.86→0.62，Recall 0.37→0.49）。


<details>
  <summary>Details</summary>
Motivation: 序数分类（如年龄段、疾病分级）标注边界模糊、易含噪声；传统方法或忽略噪声、或删样本，导致性能和数据利用率受损。需要一种既能建模不确定性又能在训练中纠偏的方式，以提升鲁棒性与准确性。

Method: 提出ORDAC：基于标签分布学习（LDL），为每个样本用分布（而非单一标签）表示序数标签的不确定性，并在训练过程中动态自适应地调整该分布的均值与标准差，以实现对疑似噪声标签的纠正与软化；不丢弃样本。扩展版本ORDAC_C、ORDAC_R进一步增强纠错能力。

Result: 在Adience（年龄估计）与DR（糖网严重度）上、在多种不对称高斯噪声设定下显著优于对照：例如Adience在40%噪声时，ORDAC_R将MAE从0.86降至0.62、Recall从0.37升至0.49；同时能纠正数据集中固有噪声。

Conclusion: 利用标签分布并对每样本分布参数进行自适应更新，是提升序数分类在噪声标签环境下鲁棒性与准确性的有效策略；ORDAC系列在多数据集与噪声情景中验证了显著收益。

Abstract: Labeled data is a fundamental component in training supervised deep learning
models for computer vision tasks. However, the labeling process, especially for
ordinal image classification where class boundaries are often ambiguous, is
prone to error and noise. Such label noise can significantly degrade the
performance and reliability of machine learning models. This paper addresses
the problem of detecting and correcting label noise in ordinal image
classification tasks. To this end, a novel data-centric method called ORDinal
Adaptive Correction (ORDAC) is proposed for adaptive correction of noisy
labels. The proposed approach leverages the capabilities of Label Distribution
Learning (LDL) to model the inherent ambiguity and uncertainty present in
ordinal labels. During training, ORDAC dynamically adjusts the mean and
standard deviation of the label distribution for each sample. Rather than
discarding potentially noisy samples, this approach aims to correct them and
make optimal use of the entire training dataset. The effectiveness of the
proposed method is evaluated on benchmark datasets for age estimation (Adience)
and disease severity detection (Diabetic Retinopathy) under various asymmetric
Gaussian noise scenarios. Results show that ORDAC and its extended versions
(ORDAC_C and ORDAC_R) lead to significant improvements in model performance.
For instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean
absolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to
0.49. The method also demonstrated its effectiveness in correcting intrinsic
noise present in the original datasets. This research indicates that adaptive
label correction using label distributions is an effective strategy to enhance
the robustness and accuracy of ordinal classification models in the presence of
noisy data.

</details>


### [198] [Category-Aware 3D Object Composition with Disentangled Texture and Shape Multi-view Diffusion](https://arxiv.org/abs/2509.02357)
*Zeren Xiong,Zikun Chen,Zedong Zhang,Xiang Li,Ying Tai,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 提出C33D：把一个现有3D模型与另一类别的语义进行合成，生成结构连贯的新3D模型；通过多视角纹理与形状扩散，解决多来源内容融合导致的纹理不一致与形状不准问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本/图像/3D到3D的方法在多源融合时易产生纹理不一致、形状不准确，难以实现“以A类3D为基底+以B类语义为引导”的高质量3D合成。

Method: 1) 从输入3D渲染多视角RGB与法线图；2) 以正视图+目标类别文本，通过自适应文图和谐(ATIH)生成“新2D对象”；3) 纹理多视角扩散：以新2D对象为条件，统一并细化其余视角纹理；4) 形状多视角扩散：同样以新2D对象为条件，联合优化多视角RGB与法线形状一致性；5) 用优化后的多视角图像与法线重建完整新3D。

Result: 在广泛实验中取得结构与纹理一致的高质量3D合成效果，展示了如“鲨鱼(3D)+鳄鱼(文本)”等新颖组合，优于现有方法。

Conclusion: C33D能有效融合类别语义与现有3D几何，利用多视角纹理与形状扩散确保跨视角一致与形状准确，为类别+3D到3D的合成任务提供简单而强大的方案。

Abstract: In this paper, we tackle a new task of 3D object synthesis, where a 3D model
is composited with another object category to create a novel 3D model. However,
most existing text/image/3D-to-3D methods struggle to effectively integrate
multiple content sources, often resulting in inconsistent textures and
inaccurate shapes. To overcome these challenges, we propose a straightforward
yet powerful approach, category+3D-to-3D (C33D), for generating novel and
structurally coherent 3D models. Our method begins by rendering multi-view
images and normal maps from the input 3D model, then generating a novel 2D
object using adaptive text-image harmony (ATIH) with the front-view image and a
text description from another object category as inputs. To ensure texture
consistency, we introduce texture multi-view diffusion, which refines the
textures of the remaining multi-view RGB images based on the novel 2D object.
For enhanced shape accuracy, we propose shape multi-view diffusion to improve
the 2D shapes of both the multi-view RGB images and the normal maps, also
conditioned on the novel 2D object. Finally, these outputs are used to
reconstruct a complete and novel 3D model. Extensive experiments demonstrate
the effectiveness of our method, yielding impressive 3D creations, such as
shark(3D)-crocodile(text) in the first row of Fig. 1. A project page is
available at: https://xzr52.github.io/C33D/

</details>


### [199] [Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture](https://arxiv.org/abs/2509.02359)
*Wanyue Zhang,Yibin Huang,Yangbin Xu,JingJing Huang,Helu Zhi,Shuo Ren,Wang Xu,Jiajun Zhang*

Main category: cs.CV

TL;DR: 提出MulSeT基准，从单视图、多视图、视频三个场景系统评估MLLM的空间理解；发现仅扩充数据收益有限，视觉编码器的位置编码比语言侧更关键，并探讨注入推理与架构改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在具身环境中的感知、推理、规划需要空间理解，但现有评测零散（多为单一场景），难以系统揭示其局限与改进点。

Method: 1) 构建MulSeT基准，覆盖单视图、多视图、视频三种空间任务；2) 设计算法/训练与消融实验，从数据规模与模型架构两方面系统分析；3) 比较级联式与原生式MLLM，对比视觉与语言端位置编码、以及“推理注入”策略的作用。

Result: - 随训练数据增大，空间理解性能很快趋于饱和，尤其涉及空间想象的任务上上限偏低；- 在两类MLLM中，空间理解更依赖视觉编码器的位置编码，而非语言模型侧的位置编码；- 初步的推理注入显示一定改善但非根本性突破。

Conclusion: 单纯扩充数据难以显著提升空间理解；应从架构层面（尤其视觉侧位置表示与跨视角整合）优化，并结合更有效的推理机制。研究为通过数据扩展与架构调优提升空间推理提供了新方向。

Abstract: Spatial understanding is essential for Multimodal Large Language Models
(MLLMs) to support perception, reasoning, and planning in embodied
environments. Despite recent progress, existing studies reveal that MLLMs still
struggle with spatial understanding. However, existing research lacks a
comprehensive and systematic evaluation of these limitations, often restricted
to isolated scenarios, such as single-view or video. In this work, we present a
systematic analysis of spatial understanding from both data and architectural
perspectives across three representative scenarios: single-view, multi-view,
and video. We propose a benchmark named MulSeT (Multi-view Spatial
Understanding Tasks), and design a series of experiments to analyze the spatial
reasoning capabilities of MLLMs. From the data perspective, the performance of
spatial understanding converges quickly as the training data increases, and the
upper bound is relatively low, especially for tasks that require spatial
imagination. This indicates that merely expanding training data is insufficient
to achieve satisfactory performance. From the architectural perspective, we
find that spatial understanding relies more heavily on the positional encoding
within the visual encoder than within the language model, in both cascaded and
native MLLMs. Moreover, we explore reasoning injection and envision future
improvements through architectural design to optimize spatial understanding.
These insights shed light on the limitations of current MLLMs and suggest new
directions for improving spatial reasoning capabilities through data scaling
and architectural tuning.

</details>


### [200] [MedDINOv3: How to adapt vision foundation models for medical image segmentation?](https://arxiv.org/abs/2509.02379)
*Yuheng Li,Yizhou Wu,Yuxiang Lai,Mingzhe Hu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出MedDINOv3：基于DINOv3与改良ViT的医学分割框架，在四个基准上达到或超越SOTA。关键在多尺度token聚合与在CT-3M（387万轴向CT切片）上的领域自适应预训练。


<details>
  <summary>Details</summary>
Motivation: 医学CT/MRI器官与肿瘤分割对临床关键，但现有深度模型任务/模态/机构泛化弱。虽然视觉大模型在自然图像上学得强表征，但迁移到医学领域有两大瓶颈：ViT在医学分割上落后专用CNN；自然—医学图像域差距大降低可迁移性。

Method: 1) 模型：重访plain ViT，提出简单高效的多尺度token聚合架构以提升密集预测能力。2) 预训练：在CT-3M（约387万轴向CT切片）上进行多阶段DINOv3自监督领域自适应预训练，获取鲁棒的密集特征。3) 作为通用骨干驱动下游医学分割任务。

Result: 在四个分割基准上达到或超过当前最优方法，显示出较强的跨模态/跨机构泛化与统一骨干潜力。

Conclusion: 通过结构简化+多尺度聚合和大规模CT自监督预训练，可有效弥合ViT在医学分割上的性能缺口，验证视觉基础模型在医学图像分割中作为统一骨干的可行性与优势；代码已开源。

Abstract: Accurate segmentation of organs and tumors in CT and MRI scans is essential
for diagnosis, treatment planning, and disease monitoring. While deep learning
has advanced automated segmentation, most models remain task-specific, lacking
generalizability across modalities and institutions. Vision foundation models
(FMs) pretrained on billion-scale natural images offer powerful and
transferable representations. However, adapting them to medical imaging faces
two key challenges: (1) the ViT backbone of most foundation models still
underperform specialized CNNs on medical image segmentation, and (2) the large
domain gap between natural and medical images limits transferability. We
introduce \textbf{MedDINOv3}, a simple and effective framework for adapting
DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple
and effective architecture with multi-scale token aggregation. Then, we perform
domain-adaptive pretraining on \textbf{CT-3M}, a curated collection of 3.87M
axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense
features. MedDINOv3 matches or exceeds state-of-the-art performance across four
segmentation benchmarks, demonstrating the potential of vision foundation
models as unified backbones for medical image segmentation. The code is
available at https://github.com/ricklisz/MedDINOv3.

</details>


### [201] [Decoupling Bidirectional Geometric Representations of 4D cost volume with 2D convolution](https://arxiv.org/abs/2509.02415)
*Xiaobao Wei,Changyong Shu,Zhaokun Yue,Chang Huang,Weiwei Liu,Shuai Yang,Lirong Yang,Peng Gao,Wenbin Zhang,Gaochao Zhu,Chengxiang Wang*

Main category: cs.CV

TL;DR: 提出DBStereo：用纯2D卷积对4D代价体进行解耦聚合，实现移动端友好的实时高精度双目匹配，速度与精度均优于现有聚合法并超过IGEV-Stereo。


<details>
  <summary>Details</summary>
Motivation: 现有实时高性能双目匹配多依赖对4D代价体的3D正则化，算力与内存开销大、不利于移动端；而仅用2D正则化的方法在病态区域表现差。需一种既轻量可部署又能在困难区域保持精度的聚合策略。

Method: 提出基于纯2D卷积的4D代价体解耦聚合框架DBStereo：1) 对4D代价体的解耦特性进行系统分析；2) 设计轻量的双向几何聚合模块，分别沿空间与视差方向建模以捕获对应表示；3) 通过解耦学习实现成本体聚合与正则化，无需3D卷积或迭代优化。

Result: 在多项实验中，DBStereo在推理时间和准确率上均优于所有基于聚合的方法，且超越迭代式的IGEV-Stereo，实现实时与高精度兼得。

Conclusion: 打破“4D代价体需用3D卷积”的经验范式，验证解耦聚合（纯2D卷积）在双目匹配中的有效性，提供简单而强大的新基线并利于移动端部署；代码即将开源。

Abstract: High-performance real-time stereo matching methods invariably rely on 3D
regularization of the cost volume, which is unfriendly to mobile devices. And
2D regularization based methods struggle in ill-posed regions. In this paper,
we present a deployment-friendly 4D cost aggregation network DBStereo, which is
based on pure 2D convolutions. Specifically, we first provide a thorough
analysis of the decoupling characteristics of 4D cost volume. And design a
lightweight bidirectional geometry aggregation block to capture spatial and
disparity representation respectively. Through decoupled learning, our approach
achieves real-time performance and impressive accuracy simultaneously.
Extensive experiments demonstrate that our proposed DBStereo outperforms all
existing aggregation-based methods in both inference time and accuracy, even
surpassing the iterative-based method IGEV-Stereo. Our study break the
empirical design of using 3D convolutions for 4D cost volume and provides a
simple yet strong baseline of the proposed decouple aggregation paradigm for
further study. Code will be available at
(\href{https://github.com/happydummy/DBStereo}{https://github.com/happydummy/DBStereo})
soon.

</details>


### [202] [From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation](https://arxiv.org/abs/2509.02419)
*Tao Wang,Zhenxuan Zhang,Yuanbo Zhou,Xinlin Zhang,Yuanbin Chen,Tao Tan,Guang Yang,Tong Tong*

Main category: cs.CV

TL;DR: 提出GSD-Net，通过几何与结构双重引导，提升在噪声标注下的医学图像分割鲁棒性；在多数据集与多种噪声设定下达SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学分割依赖大量高质量标注，获取昂贵且不可避免含噪（主观性、粗糙边界），会误导特征学习、降低性能。需要能在噪声标注下仍稳定高效的分割方法。

Method: 提出Geometric-Structural Dual-Guided Network（GSD-Net），包含三大组件：1) 几何距离感知（Geometric Distance-Aware, GDA）模块：基于几何特征动态调整像素权重，在可靠区域强化监督、在可疑区域降权以抑制噪声；2) 结构引导标签优化（Structure-Guided Label Refinement, SLR）模块：利用结构先验对标签进行细化与校正；3) 知识迁移（Knowledge Transfer, KT）模块：引入额外监督路径，增强对局部细节的敏感度与泛化。整体是端到端训练，将几何与结构线索融合，提高对噪声标注的鲁棒性。

Result: 在6个公开数据集评测：4个含三类模拟噪声，2个为多专家标注（更贴近真实主观差异）。在SR模拟噪声条件下，GSD-Net达到SOTA：Kvasir提升2.52%，Shenzhen提升22.76%，BU-SUC提升8.87%，BraTS2020提升4.59%。总体在噪声标注下均优于现有方法。

Conclusion: 几何+结构双引导能够有效缓解噪声标注对分割训练的干扰；GDA、SLR与KT相互补充，提高鲁棒性与细节敏感性，并在多数据集验证中获得显著增益。代码已开源，具备可复现性与扩展潜力。

Abstract: The effectiveness of convolutional neural networks in medical image
segmentation relies on large-scale, high-quality annotations, which are costly
and time-consuming to obtain. Even expert-labeled datasets inevitably contain
noise arising from subjectivity and coarse delineations, which disrupt feature
learning and adversely impact model performance. To address these challenges,
this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which
integrates geometric and structural cues to improve robustness against noisy
annotations. It incorporates a Geometric Distance-Aware module that dynamically
adjusts pixel-level weights using geometric features, thereby strengthening
supervision in reliable regions while suppressing noise. A Structure-Guided
Label Refinement module further refines labels with structural priors, and a
Knowledge Transfer module enriches supervision and improves sensitivity to
local details. To comprehensively assess its effectiveness, we evaluated
GSD-Net on six publicly available datasets: four containing three types of
simulated label noise, and two with multi-expert annotations that reflect
real-world subjectivity and labeling inconsistencies. Experimental results
demonstrate that GSD-Net achieves state-of-the-art performance under noisy
annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen,
8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of
this study are available at https://github.com/ortonwang/GSD-Net.

</details>


### [203] [Faster and Better: Reinforced Collaborative Distillation and Self-Learning for Infrared-Visible Image Fusion](https://arxiv.org/abs/2509.02424)
*Yuhao Wang,Lingjuan Miao,Zhiqiang Zhou,Yajun Qiao,Lei Zhang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Infrared and visible image fusion plays a critical role in enhancing scene
perception by combining complementary information from different modalities.
Despite recent advances, achieving high-quality image fusion with lightweight
models remains a significant challenge. To bridge this gap, we propose a novel
collaborative distillation and self-learning framework for image fusion driven
by reinforcement learning. Unlike conventional distillation, this approach not
only enables the student model to absorb image fusion knowledge from the
teacher model, but more importantly, allows the student to perform
self-learning on more challenging samples to enhance its capabilities.
Particularly, in our framework, a reinforcement learning agent explores and
identifies a more suitable training strategy for the student.The agent takes
both the student's performance and the teacher-student gap as inputs, which
leads to the generation of challenging samples to facilitate the student's
self-learning. Simultaneously, it dynamically adjusts the teacher's guidance
strength based on the student's state to optimize the knowledge transfer.
Experimental results demonstrate that our method can significantly improve
student performance and achieve better fusion results compared to existing
techniques.

</details>


### [204] [Towards High-Fidelity, Identity-Preserving Real-Time Makeup Transfer: Decoupling Style Generation](https://arxiv.org/abs/2509.02445)
*Lydia Kin Ching Chau,Zhi Yu,Ruo Wei Jiang*

Main category: cs.CV

TL;DR: 提出一套用于实时虚拟美妆试妆的框架，通过“透明化妆蒙版提取 + 图形学渲染”两步实现高保真、保身份且具时间一致性的化妆迁移，并在多项实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有美妆迁移方法难以将半透明彩妆与肤色/身份特征解耦，易导致身份漂移与公平性问题；且缺乏实时性与时序一致性，限制了在线试妆应用。

Method: 将问题解耦为两阶段：1) 透明化妆蒙版提取（估计彩妆的alpha/颜色等）；2) 基于图形学的蒙版实时渲染。蒙版提取模型使用伪真值训练，伪真值由两条互补管线生成：图形学渲染管线与无监督k-means聚类。为提升透明度估计与颜色保真，引入专门的训练目标，如alpha加权重建损失与唇色损失；整体设计关注跨姿态、表情与肤色的稳健性与时序平滑。

Result: 在多种姿态、表情、肤色与动态场景中实现高保真彩妆细节复现、身份保持与时间稳定；具备实时渲染能力。实验显示在细节捕获、时序稳定与身份完整性上优于现有基线。

Conclusion: 通过将美妆迁移拆分为蒙版提取与实时渲染并结合伪真值训练与专门损失，能在保持身份与公平性的同时实现实时、时序一致的高质量虚拟试妆，具有实际应用潜力。

Abstract: We present a novel framework for real-time virtual makeup try-on that
achieves high-fidelity, identity-preserving cosmetic transfer with robust
temporal consistency. In live makeup transfer applications, it is critical to
synthesize temporally coherent results that accurately replicate fine-grained
makeup and preserve user's identity. However, existing methods often struggle
to disentangle semitransparent cosmetics from skin tones and other identify
features, causing identity shifts and raising fairness concerns. Furthermore,
current methods lack real-time capabilities and fail to maintain temporal
consistency, limiting practical adoption. To address these challenges, we
decouple makeup transfer into two steps: transparent makeup mask extraction and
graphics-based mask rendering. After the makeup extraction step, the makeup
rendering can be performed in real time, enabling live makeup try-on. Our
makeup extraction model trained on pseudo-ground-truth data generated via two
complementary methods: a graphics-based rendering pipeline and an unsupervised
k-means clustering approach. To further enhance transparency estimation and
color fidelity, we propose specialized training objectives, including
alpha-weighted reconstruction and lip color losses. Our method achieves robust
makeup transfer across diverse poses, expressions, and skin tones while
preserving temporal smoothness. Extensive experiments demonstrate that our
approach outperforms existing baselines in capturing fine details, maintaining
temporal stability, and preserving identity integrity.

</details>


### [205] [RiverScope: High-Resolution River Masking Dataset](https://arxiv.org/abs/2509.02451)
*Rangel Daroya,Taylor Rowley,Jonathan Flores,Elisa Friedmann,Fiona Bennitt,Heejin An,Travis Simmons,Marissa Jean Hughes,Camryn L Kluetmeier,Solomon Kica,J. Daniel Vélez,Sarah E. Esenther,Thomas E. Howard,Yanqi Ye,Audrey Turcotte,Colin Gleason,Subhransu Maji*

Main category: cs.CV

TL;DR: RiverScope 提供一个跨多传感器配准的高分辨率河流/地表水数据集和评测基准，显著提升河宽估计精度（中位误差7.2 m），并系统比较多种深度学习架构与预训练策略，证明结合迁移学习与多光谱通道适配器的方案最佳。


<details>
  <summary>Details</summary>
Motivation: 现有低分辨率卫星对狭窄或高含沙河流监测不足，限制了对生态、农业、防灾与可持续发展的支持；缺乏高分辨率、跨传感器、可用于评估成本-精度权衡的数据集与统一基准。

Method: 构建RiverScope数据集：包含1,145张高分辨率影像（覆盖2,577 km²），专家手工标注水体掩膜（>100小时），并与Sentinel-2、SWOT与SWORD共配准；建立全球高分辨率河宽估计算法基准；系统评测多种深度网络（CNN/Transformer）、预训练方式（监督/自监督）与训练数据来源（ImageNet/遥感），并引入利用全部PlanetScope多光谱通道的学习型适配器，结合迁移学习。

Result: 在河宽估计上实现中位误差7.2 m，显著优于现有基于卫星的方法；跨模型、预训练和数据源的实验显示，结合迁移学习与多光谱通道适配器的方案表现最佳；数据与基准支持对不同传感器在成本-精度上的权衡评估。

Conclusion: RiverScope成为高分辨率、多传感器水文建模的重要资源，建立了首个全球高分辨率河宽估计基准，提升细尺度地表水监测能力，可用于气候适应与可持续水资源管理。

Abstract: Surface water dynamics play a critical role in Earth's climate system,
influencing ecosystems, agriculture, disaster resilience, and sustainable
development. Yet monitoring rivers and surface water at fine spatial and
temporal scales remains challenging -- especially for narrow or sediment-rich
rivers that are poorly captured by low-resolution satellite data. To address
this, we introduce RiverScope, a high-resolution dataset developed through
collaboration between computer science and hydrology experts. RiverScope
comprises 1,145 high-resolution images (covering 2,577 square kilometers) with
expert-labeled river and surface water masks, requiring over 100 hours of
manual annotation. Each image is co-registered with Sentinel-2, SWOT, and the
SWOT River Database (SWORD), enabling the evaluation of cost-accuracy
trade-offs across sensors -- a key consideration for operational water
monitoring. We also establish the first global, high-resolution benchmark for
river width estimation, achieving a median error of 7.2 meters -- significantly
outperforming existing satellite-derived methods. We extensively evaluate deep
networks across multiple architectures (e.g., CNNs and transformers),
pretraining strategies (e.g., supervised and self-supervised), and training
datasets (e.g., ImageNet and satellite imagery). Our best-performing models
combine the benefits of transfer learning with the use of all the multispectral
PlanetScope channels via learned adaptors. RiverScope provides a valuable
resource for fine-scale and multi-sensor hydrological modeling, supporting
climate adaptation and sustainable water management.

</details>


### [206] [GenCompositor: Generative Video Compositing with Diffusion Transformer](https://arxiv.org/abs/2509.02460)
*Shuzhou Yang,Xiaoyu Li,Xiaodong Cun,Guangzhi Wang,Lingen Li,Ying Shan,Jian Zhang*

Main category: cs.CV

TL;DR: 提出“生成式视频合成”任务与系统：用DiT为核心，将外部前景视频的身份与运动自适应注入到目标视频，用户可交互控制大小、轨迹等；通过背景保留分支、前景融合模块与新位置编码ERoPE，实现高保真且一致的编辑；并发布61K视频数据集VideoComp，实验优于现有替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统视频合成（抠像、跟踪、匹配、合成）高度依赖专家与繁琐管线，耗时昂贵；现有生成模型虽能擦除/替换，但难以在保证目标视频一致性的同时将外部前景的身份与动态精确注入并支持用户可控编辑。因此需要一个同时兼顾背景一致性、前景动态继承与交互可控性的自动化生成式方案。

Method: 以Diffusion Transformer(DiT)为核心，构建三部分：1) 背景保留分支：轻量DiT并通过masked token injection保证编辑前后目标视频的外观/时序一致；2) 前景融合：设计DiT fusion block，采用全自注意力将前景身份与运动信息注入目标流，并配合简单有效的前景增强策略训练其鲁棒性；3) ERoPE位置编码：扩展旋转位置编码以适配用户控制下不同布局与尺度的背景/前景视频对齐；4) 交互控制接口：允许用户指定前景大小、运动轨迹等；5) 数据：构建VideoComp，含6.1万组“完整动态元素+高质目标视频”。

Result: 模型能在保持目标视频时空一致性的同时，将外部前景的身份与运动准确合成到目标视频中；在保真度与一致性指标上优于现有可比方法（如基于常规视频编辑或简单合成的可能替代方案），并展示交互可控的编辑效果。

Conclusion: 生成式视频合成是可行且有效的：通过DiT的背景保留、前景融合与ERoPE位置编码的协同，系统实现高保真、时序一致、用户可控的视频合成，并以大规模VideoComp数据集促进该方向研究。

Abstract: Video compositing combines live-action footage to create video production,
serving as a crucial technique in video creation and film production.
Traditional pipelines require intensive labor efforts and expert collaboration,
resulting in lengthy production cycles and high manpower costs. To address this
issue, we automate this process with generative models, called generative video
compositing. This new task strives to adaptively inject identity and motion
information of foreground video to the target video in an interactive manner,
allowing users to customize the size, motion trajectory, and other attributes
of the dynamic elements added in final video. Specifically, we designed a novel
Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To
maintain consistency of the target video before and after editing, we revised a
light-weight DiT-based background preservation branch with masked token
injection. As to inherit dynamic elements from other sources, a DiT fusion
block is proposed using full self-attention, along with a simple yet effective
foreground augmentation for training. Besides, for fusing background and
foreground videos with different layouts based on user control, we developed a
novel position embedding, named Extended Rotary Position Embedding (ERoPE).
Finally, we curated a dataset comprising 61K sets of videos for our new task,
called VideoComp. This data includes complete dynamic elements and high-quality
target videos. Experiments demonstrate that our method effectively realizes
generative video compositing, outperforming existing possible solutions in
fidelity and consistency.

</details>


### [207] [TeRA: Rethinking Text-driven Realistic 3D Avatar Generation](https://arxiv.org/abs/2509.02466)
*Yanwen Wang,Yiyu Zhuang,Jiawei Zhang,Li Wang,Yifei Zeng,Xun Cao,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: TeRA提出一种两阶段、非SDS的文本生成3D头像框架：先蒸馏出结构化3D潜空间，再在其中训练文本条件潜扩散，生成照片级人像头像，速度更快、可局部编辑，主客观指标优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成头像方法多依赖SDS或通用3D大模型，存在迭代优化慢、质量/一致性受限、难以进行可控局部编辑等问题。需要一个原生的3D头像生成模型，既高效又可控。

Method: 两阶段训练：1) 从大型人体重建模型蒸馏解码器，形成结构化的3D人类表示潜空间（native 3D avatar latent）。2) 在该潜空间上训练文本控制的潜扩散模型，直接生成潜向量，再由解码器解码成高保真3D头像；利用结构化表示支持基于文本的部分定制。

Result: 无需慢速的SDS迭代优化，推理更高效；在主观用户评测与客观指标上均优于现有文本到头像模型与通用3D生成模型；可实现文本驱动的局部属性编辑。

Conclusion: TeRA通过“潜空间蒸馏+文本条件潜扩散”的范式，构建出高效、可控、写实度高的原生3D头像生成模型，显著优于SDS基线与通用3D生成方案，并提升了实际应用中的交互与定制能力。

Abstract: In this paper, we rethink text-to-avatar generative models by proposing TeRA,
a more efficient and effective framework than the previous SDS-based models and
general large 3D generative models. Our approach employs a two-stage training
strategy for learning a native 3D avatar generative model. Initially, we
distill a decoder to derive a structured latent space from a large human
reconstruction model. Subsequently, a text-controlled latent diffusion model is
trained to generate photorealistic 3D human avatars within this latent space.
TeRA enhances the model performance by eliminating slow iterative optimization
and enables text-based partial customization through a structured 3D human
representation. Experiments have proven our approach's superiority over
previous text-to-avatar generative models in subjective and objective
evaluation.

</details>


### [208] [Anisotropic Fourier Features for Positional Encoding in Medical Imaging](https://arxiv.org/abs/2509.02488)
*Nabil Jabareen,Dongsheng Yuan,Dingming Liu,Foo-Wei Ten,Sören Lukassen*

Main category: cs.CV

TL;DR: 论文提出针对医学影像各向异性与复杂形状挑战的AFPE位置编码，在多项任务中优于现有PE，强调应根据数据各向异性与目标形状选择合适PE。


<details>
  <summary>Details</summary>
Motivation: 医学影像常具有高维、各向异性像素间距与复杂解剖形状，主流Transformer位置编码（如SPE、IFPE）要么在高维难保持欧氏距离，要么假设各向同性，导致对解剖结构定位与关系建模不足，亟需能表达各向异性与任务/域特性的PE。

Method: 提出各向异性傅里叶特征位置编码（AFPE），在IFPE基础上引入方向相关的尺度与频率参数，并可根据类别与域自适应（或可学习）地建模空间依赖；在三类任务上系统对比不同PE：胸片多标签分类、CT器官分类、超声心动图射血分数回归，评估各向异性与形状因素对PE选择的影响。

Result: 不同任务上，合适的PE可显著提升性能；最优PE与目标结构形状及数据各向异性密切相关。在所有各向异性设置中，AFPE显著优于SPE、IFPE等SOTA位置编码。

Conclusion: 在各向异性的医学图像与视频中，选择与数据各向异性与目标形状匹配的PE至关重要；AFPE提供了通用且更优的方案，能在多任务中稳定提升Transformer性能。

Abstract: The adoption of Transformer-based architectures in the medical domain is
growing rapidly. In medical imaging, the analysis of complex shapes - such as
organs, tissues, or other anatomical structures - combined with the often
anisotropic nature of high-dimensional images complicates these adaptations. In
this study, we critically examine the role of Positional Encodings (PEs),
arguing that commonly used approaches may be suboptimal for the specific
challenges of medical imaging. Sinusoidal Positional Encodings (SPEs) have
proven effective in vision tasks, but they struggle to preserve Euclidean
distances in higher-dimensional spaces. Isotropic Fourier Feature Positional
Encodings (IFPEs) have been proposed to better preserve Euclidean distances,
but they lack the ability to account for anisotropy in images. To address these
limitations, we propose Anisotropic Fourier Feature Positional Encoding (AFPE),
a generalization of IFPE that incorporates anisotropic, class-specific, and
domain-specific spatial dependencies. We systematically benchmark AFPE against
commonly used PEs on multi-label classification in chest X-rays, organ
classification in CT images, and ejection fraction regression in
echocardiography. Our results demonstrate that choosing the correct PE can
significantly improve model performance. We show that the optimal PE depends on
the shape of the structure of interest and the anisotropy of the data. Finally,
our proposed AFPE significantly outperforms state-of-the-art PEs in all tested
anisotropic settings. We conclude that, in anisotropic medical images and
videos, it is of paramount importance to choose an anisotropic PE that fits the
data and the shape of interest.

</details>


### [209] [Enhancing Fitness Movement Recognition with Attention Mechanism and Pre-Trained Feature Extractors](https://arxiv.org/abs/2509.02511)
*Shanjid Hasan Nishat,Srabonti Deb,Mohiuddin Ahmed*

Main category: cs.CV

TL;DR: 用轻量级2D特征提取器（ResNet50/EfficientNet/ViT）+带空间注意力的LSTM进行健身动作识别，在UCF101子集上达93.34%准确率，优于多种SOTA，同时具备实时与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有HAR/健身动作识别多依赖计算昂贵的3D模型，不利于实时和资源受限应用（健康监测、康复、个性化训练）。需要一种在不牺牲精度的前提下更轻量、可实时的方案。

Method: 采用预训练2D CNN或ViT提取逐帧空间特征；用LSTM建模时间依赖；引入空间注意力突出关键信息片段；在UCF101精选子集上训练与评估。对比ResNet50、EfficientNet与ViT作为前端。

Result: 在UCF101子集上，ResNet50+LSTM+空间注意力达到93.34%最高准确率；与多种SOTA HAR方法相比表现更优。

Conclusion: 该轻量框架在保持高准确率的同时显著降低计算开销，适用于实时与可扩展的健身动作识别，并可推广至视觉健康与活动监测场景。

Abstract: Fitness movement recognition, a focused subdomain of human activity
recognition (HAR), plays a vital role in health monitoring, rehabilitation, and
personalized fitness training by enabling automated exercise classification
from video data. However, many existing deep learning approaches rely on
computationally intensive 3D models, limiting their feasibility in real-time or
resource-constrained settings. In this paper, we present a lightweight and
effective framework that integrates pre-trained 2D Convolutional Neural
Networks (CNNs) such as ResNet50, EfficientNet, and Vision Transformers (ViT)
with a Long Short-Term Memory (LSTM) network enhanced by spatial attention.
These models efficiently extract spatial features while the LSTM captures
temporal dependencies, and the attention mechanism emphasizes informative
segments. We evaluate the framework on a curated subset of the UCF101 dataset,
achieving a peak accuracy of 93.34\% with the ResNet50-based configuration.
Comparative results demonstrate the superiority of our approach over several
state-of-the-art HAR systems. The proposed method offers a scalable and
real-time-capable solution for fitness activity recognition with broader
applications in vision-based health and activity monitoring.

</details>


### [210] [Mix-modal Federated Learning for MRI Image Segmentation](https://arxiv.org/abs/2509.02541)
*Guyue Hu,Siyuan Song,Jingpeng Sun,Zhe Jin,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出一种面向非中心化混合模态MRI分割的联邦学习新范式MixMFL，并给出包含“模态解耦+模态记忆”的MDM-MixMFL框架，解决客户端间模态与数据异质性及缺失模态问题，实验在两套公开数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实医疗中各医院拥有的MRI模态组合不一致、数据分布不同且无法集中共享，现有集中式多模态或传统联邦（多/跨模态）方案难以稳健处理“混合模态+非中心化”的场景，导致联邦聚合不稳定、信息利用不充分、缺失模态性能下降。

Method: 1) 定义新的联邦学习范式MixMFL，适配每个客户端拥有不同模态集合与数据分布的情况。2) 提出MDM-MixMFL框架：- 模态解耦：将每个模态的特征划分为“模态特有部分”和“模态共享部分”，对应地设计独立的编码器更新路径（tailored vs. shared），以在联邦更新中分别聚合；- 模态记忆：维护跨客户端共享的“模态原型库”，由各客户端的模态特有编码器动态刷新，用于在本地缺失某些模态时进行补偿与对齐，提升聚合与融合效果。

Result: 在两个公开MRI分割数据集上进行大量实验，方法在总体分割性能上优于现有多/跨模态联邦或集中式基线，表现为更高的指标（如Dice等）和更强的对异质性与缺失模态的鲁棒性。

Conclusion: MixMFL作为新联邦范式更契合真实混合模态医疗场景；MDM-MixMFL通过“解耦更新+原型记忆”有效应对模态和数据异质性与缺失模态，显著提升联邦MRI分割效果，具有推广潜力。

Abstract: Magnetic resonance imaging (MRI) image segmentation is crucial in diagnosing
and treating many diseases, such as brain tumors. Existing MRI image
segmentation methods mainly fall into a centralized multimodal paradigm, which
is inapplicable in engineering non-centralized mix-modal medical scenarios. In
this situation, each distributed client (hospital) processes multiple mixed MRI
modalities, and the modality set and image data for each client are diverse,
suffering from extensive client-wise modality heterogeneity and data
heterogeneity. In this paper, we first formulate non-centralized mix-modal MRI
image segmentation as a new paradigm for federated learning (FL) that involves
multiple modalities, called mix-modal federated learning (MixMFL). It
distinguishes from existing multimodal federating learning (MulMFL) and
cross-modal federating learning (CroMFL) paradigms. Then, we proposed a novel
modality decoupling and memorizing mix-modal federated learning framework
(MDM-MixMFL) for MRI image segmentation, which is characterized by a modality
decoupling strategy and a modality memorizing mechanism. Specifically, the
modality decoupling strategy disentangles each modality into modality-tailored
and modality-shared information. During mix-modal federated updating,
corresponding modality encoders undergo tailored and shared updating,
respectively. It facilitates stable and adaptive federating aggregation of
heterogeneous data and modalities from distributed clients. Besides, the
modality memorizing mechanism stores client-shared modality prototypes
dynamically refreshed from every modality-tailored encoder to compensate for
incomplete modalities in each local client. It further benefits modality
aggregation and fusion processes during mixmodal federated learning. Extensive
experiments on two public datasets for MRI image segmentation demonstrate the
effectiveness and superiority of our methods.

</details>


### [211] [Motion-Refined DINOSAUR for Unsupervised Multi-Object Discovery](https://arxiv.org/abs/2509.02545)
*Xinrui Gong,Oliver Hahn,Christoph Reich,Krishnakant Singh,Simone Schaub-Meyer,Daniel Cremers,Stefan Roth*

Main category: cs.CV

TL;DR: 提出MR-DINOSAUR：在无相机运动帧上用无监督光流做运动分割生成高质量伪标，细化DINOSAUR槽表示并学习槽停用以前景/背景分配，在TRI-PD与KITTI上无监督SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多目标无监督发现常借助OCL与视频运动线索，但往往要用带监督或半监督过程生成伪标训练OCL，违背“无监督”目标。需要一种真正无监督、能稳健利用运动线索且与强大的自监督OCL模型兼容的方法。

Method: 1) 基于自监督预训练的OCL模型DINOSAUR作为骨干与槽(slot)表示。2) 从视频中检索无相机运动的帧对/片段；对这些帧计算无监督光流并做运动分割，得到纯由物体运动驱动的高质量伪标。3) 用这些伪标细化DINOSAUR的槽表示（对齐槽与运动实例）。4) 训练槽停用模块（slot deactivation）以区分前景与背景并抑制多余槽，将槽明确分配给物体或背景。整体保持最小化改动、端到端无监督。

Result: 在TRI-PD与KITTI数据集上进行多目标发现实验，MR-DINOSAUR在无监督设定下超过此前SOTA，显示出更好的检测与定位质量（抽象中未给出具体数值）。

Conclusion: 利用无相机运动帧的无监督光流分割可生成高质量伪标，配合对DINOSAUR槽的精炼与槽停用机制，可在完全无监督条件下实现强大的多目标发现性能，推动OCL在真实视频场景中的无监督应用。

Abstract: Unsupervised multi-object discovery (MOD) aims to detect and localize
distinct object instances in visual scenes without any form of human
supervision. Recent approaches leverage object-centric learning (OCL) and
motion cues from video to identify individual objects. However, these
approaches use supervision to generate pseudo labels to train the OCL model. We
address this limitation with MR-DINOSAUR -- Motion-Refined DINOSAUR -- a
minimalistic unsupervised approach that extends the self-supervised pre-trained
OCL model, DINOSAUR, to the task of unsupervised multi-object discovery. We
generate high-quality unsupervised pseudo labels by retrieving video frames
without camera motion for which we perform motion segmentation of unsupervised
optical flow. We refine DINOSAUR's slot representations using these pseudo
labels and train a slot deactivation module to assign slots to foreground and
background. Despite its conceptual simplicity, MR-DINOSAUR achieves strong
multi-object discovery results on the TRI-PD and KITTI datasets, outperforming
the previous state of the art despite being fully unsupervised.

</details>


### [212] [FastVGGT: Training-Free Acceleration of Visual Geometry Transformer](https://arxiv.org/abs/2509.02560)
*You Shen,Zhipeng Zhang,Yansong Qu,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出 FastVGGT：在不重新训练的前提下，对 3D 视觉基础模型 VGGT 进行基于 token 合并的加速，针对长序列输入实现约 4 倍推理提速且误差更可控。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 视觉基础模型在长序列图像输入下推理耗时高。对 VGGT 的瓶颈分析与可视化发现注意力图出现 token 崩塌现象，提示存在大量冗余计算与信息聚合不均。需要一种既能减负又不牺牲重建能力的可扩展机制。

Method: 在不改动或微调 VGGT 的前提下，提出训练无关（training-free）的 token 合并方案：1）基于对 3D 架构与任务特性的分析，设计专门的 token 划分与合并策略，以避免简单套用 2D/通用合并方法的失效；2）在前馈推理阶段动态合并冗余 token，减少注意力与 MLP 计算；3）通过分区确保几何关键区域保真，缓解长序列中的误差累积。

Result: 在多个 3D 几何基准上验证有效性：对于 1000 张输入图像的长序列，FastVGGT 相比 VGGT 实现约 4× 推理加速，同时在长序列情形下的误差累积得到缓解，保持较强的重建性能。

Conclusion: 3D 领域引入面向任务/架构定制的 token 合并是可行且有效的，加速与精度兼顾，为可扩展的 3D 视觉系统提供了可行路径。作者公开了代码与项目页。

Abstract: Foundation models for 3D vision have recently demonstrated remarkable
capabilities in 3D perception. However, scaling these models to long-sequence
image inputs remains a significant challenge due to inference-time
inefficiency. In this work, we present a detailed analysis of VGGT, a
state-of-the-art feed-forward visual geometry model and identify its primary
bottleneck. Visualization further reveals a token collapse phenomenon in the
attention maps. Motivated by these findings, we explore the potential of token
merging in the feed-forward visual geometry model. Owing to the unique
architectural and task-specific properties of 3D models, directly applying
existing merging techniques proves challenging. To this end, we propose
FastVGGT, which, for the first time, leverages token merging in the 3D domain
through a training-free mechanism for accelerating VGGT. we devise a unique
token partitioning strategy tailored to 3D architectures and tasks, effectively
eliminating redundant computation while preserving VGGT's powerful
reconstruction capacity. Extensive experiments on multiple 3D geometry
benchmarks validate the effectiveness of our approach. Notably, with 1000 input
images, FastVGGT achieves a 4x speedup over VGGT while mitigating error
accumulation in long-sequence scenarios. These findings underscore the
potential of token merging as a principled solution for scalable 3D vision
systems. Code is available at: https://mystorm16.github.io/fastvggt/.

</details>
