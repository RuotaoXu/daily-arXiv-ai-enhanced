<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HyperCLOVA X 32B Think](https://arxiv.org/abs/2601.03286)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CV

TL;DR: HyperCLOVA X 32B Think 是一款强调韩语语境推理与代理能力的多模态视觉语言模型，预训练侧重推理，后训练增强多模态理解、推理、代理与对齐；在同等规模模型上于韩语文本与视觉任务及代理评测中表现优秀，并开源以促进研究与应用。


<details>
  <summary>Details</summary>
Motivation: 现有多模态与大语言模型在特定语言与文化（尤其是韩语）情境下的推理与代理能力不足，缺乏面向韩国语境优化且可用于学术与产业的开源方案。

Method: 采用“先强推理预训练、再多模态与对齐后训练”的两阶段流程：1）以推理为中心进行预训练；2）通过后训练加入视觉-语言能力、强化推理技巧、注入代理式行为能力，并进行人类偏好对齐；随后在同规模基线下进行系统评测。

Result: 在与可比规模模型的基准对比中，于韩语文本到文本、视觉到文本任务以及面向代理的评测上取得强劲表现。

Conclusion: HyperCLOVA X 32B Think 在韩语语境推理、多模态理解与代理能力方面效果突出；开源将促进学术与产业的广泛采用与进一步创新。

Abstract: In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.

</details>


### [2] [CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception](https://arxiv.org/abs/2601.03302)
*Mohammad Rostami,Atik Faysal,Hongtao Xia,Hadi Kasasbeh,Ziang Gao,Huaxia Wang*

Main category: cs.CV

TL;DR: CDRF 是一个结合真实采集与系统化合成扩增的大规模 RF 无人机检测/识别基准与工具箱，用于标准化评测与提升模型泛化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有 RF 无人机数据集稀缺、机型覆盖与采集条件单一、难以可重复评测，限制了鲁棒泛化模型的发展。

Method: 构建大规模数据集：在校园与受控 RF 笼环境录制原始信号，并提出可控的增强管线：精确控制 SNR、注入干扰发射源、进行频移并对检测任务做标签一致的边界框变换；同时发布可与现有公开基准互操作的开源数据生成、预处理、增强与评测工具。

Result: 得到覆盖多种当代无人机机型与多样采集条件的综合数据集与工具，支持分类、开放集识别与目标检测三类任务的标准化基准与可复现流程。

Conclusion: CDRF 为 RF 感知领域提供了丰富数据与统一评测框架，有望加速产生鲁棒、可泛化的无人机检测与识别模型。

Abstract: We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models.

</details>


### [3] [Mass Concept Erasure in Diffusion Models with Concept Hierarchy](https://arxiv.org/abs/2601.03305)
*Jiahang Tu,Ye Li,Yiming Wu,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.CV

TL;DR: 提出一种面向扩散模型的“超类型-子类型”概念层次与组内擦除方法，并结合SuPLoRA，在批量抑制相关概念时尽量保留上位概念的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法需为每个概念单独微调，参数开销大、效率低，且随着被擦除概念增多会显著降低整体生成质量；尤其当多个语义相关概念需要同时抑制时，退化更明显。

Method: 1) 将被擦除概念组织为“超类型(父节点)-子类型(子节点)”层次结构，将语义相关的子类型(如macaw、bald eagle)归入同一超类型(如bird)。2) 采用组内联合抑制：同组共享一组可学习参数，联合擦除以提升效率与一致性，并对未掩膜区域施加标准扩散正则以保持去噪过程稳定。3) 提出SuPLoRA：在LoRA中将承载超类型信息的下投影矩阵冻结，仅更新上投影矩阵，在抑制子类型时缓解对超类型生成质量的过度伤害。提供理论分析说明其减缓退化的有效性。4) 构建跨多域(名人、物体、色情)的更具挑战性基准，要求同时擦除多类概念。

Result: 组内联合抑制在多概念同时擦除场景中更高效，参数量更低；SuPLoRA在压制子类型的同时更好地保留超类型的生成能力。理论与实验共同表明，相比逐概念微调的基线方法，方法在生成质量与安全性上取得更优的权衡。

Conclusion: 通过超类型-子类型层次与SuPLoRA，实现对语义相关概念的高效、低参数、可扩展的联合擦除，同时显著缓解对上位概念生成质量的损害，并在多域挑战基准上验证了有效性。

Abstract: The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.

</details>


### [4] [VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models](https://arxiv.org/abs/2601.03309)
*Jianke Zhang,Xiaoyu Chen,Qiuyue Wang,Mingsheng Li,Yanjiang Guo,Yucheng Hu,Jiajun Zhang,Shuai Bai,Junyang Lin,Jianyu Chen*

Main category: cs.CV

TL;DR: 研究评估不同通用VLM作为策略骨干时对VLA性能的影响，提出一个极简适配管线VLM4VLA，系统实验证明：VLM初始化有益，但其通用/体感能力并不可靠预测下游控制表现，核心瓶颈在视觉编码器，与行动规划需求存在预训练目标鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前VLA常将大规模VLM并入策略以期获得泛化，但社区缺少对“选哪种VLM、其通用与具身能力能否转化为控制性能”的系统检验。需要一个可公平对比、参数增量小、效率高的适配方法来隔离影响因素。

Method: 提出VLM4VLA：以最小新增可学习参数将通用VLM转换为VLA策略，保持其视觉与语言模块基本结构，进行广泛基准评测；对七类具身辅助任务微调VLM以分析特定能力影响；进行模态级消融，分别评估视觉与语言模块对控制性能的贡献；尝试向视觉编码器注入与控制相关的监督，并在下游微调时保持编码器冻结以检验迁移。

Result: 1) 相比从零训练，采用VLM初始化在多基准、多任务上稳定提升；2) VLM的通用能力（以及在具身辅助任务上的分数）与下游VLA控制表现相关性弱，无法作为可靠预测器；3) 模态消融显示视觉模块是主要瓶颈而非语言模块；4) 向视觉编码器注入控制相关监督在编码器冻结情况下仍带来一致增益。

Conclusion: 标准VLM能力是必要但不足以支撑有效具身控制；当前VLM预训练目标与行动规划需求存在显著域差。应优先改进视觉编码器并引入与控制相关的监督信号，以缩小鸿沟而非仅依赖提升语言或通用VLM分数。

Abstract: Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.

</details>


### [5] [Deep Learning-Based Image Recognition for Soft-Shell Shrimp Classification](https://arxiv.org/abs/2601.03317)
*Yun-Hao Zhang,I-Hsien Ting,Dario Liberona,Yun-Hsiu Liu,Kazunori Minetaki*

Main category: cs.CV

TL;DR: 研究利用深度学习图像识别在白虾收获后进行自动分级，以提升效率与一致性、保持新鲜度并改善外观品质。


<details>
  <summary>Details</summary>
Motivation: 水产养殖信息化推动产量稳定增长，但消费者对高品质水产需求提升，尤其关注鲜度与外观完整性。白虾加工中软壳与熟/冻后头身分离影响卖相与接受度，人工分拣效率与一致性不足，需更精准快速的自动化分级方案。

Method: 构建并应用基于卷积神经网络（CNN）的图像识别模型，对刚收获的白虾进行自动分类；以深度学习替代人工分拣，提升分类准确率、效率与一致性。

Result: CNN分类器实现对白虾的自动识别与分级，较人工具有更高准确率与稳定性，并缩短处理时间。

Conclusion: 基于深度学习的自动分级可减少处理时间、保持鲜度、改善外观质量控制，帮助运输与加工环节更好满足客户需求。

Abstract: With the integration of information technology into aquaculture, production has become more stable and continues to grow annually. As consumer demand for high-quality aquatic products rises, freshness and appearance integrity are key concerns. In shrimp-based processed foods, freshness declines rapidly post-harvest, and soft-shell shrimp often suffer from head-body separation after cooking or freezing, affecting product appearance and consumer perception. To address these issues, this study leverages deep learning-based image recognition for automated classification of white shrimp immediately after harvest. A convolutional neural network (CNN) model replaces manual sorting, enhancing classification accuracy, efficiency, and consistency. By reducing processing time, this technology helps maintain freshness and ensures that shrimp transportation businesses meet customer demands more effectively.

</details>


### [6] [Higher order PCA-like rotation-invariant features for detailed shape descriptors modulo rotation](https://arxiv.org/abs/2601.03326)
*Jarek Duda*

Main category: cs.CV

TL;DR: 提出将PCA的二阶协方差不变性扩展到高阶中心矩/张量与多项式-高斯模型，以获得更强的旋转不变形状描述与高精度可解码表示。


<details>
  <summary>Details</summary>
Motivation: 仅用协方差椭球（PCA）刻画形状过于粗糙，无法表达真实复杂形状；需要在不进行昂贵旋转优化的前提下，实现更精细的旋转不变特征与高效相似度比较。

Method: 在PCA二阶矩p_ab基础上，引入三阶及更高阶中心矩张量p_abc等，以及“多项式×高斯”的密度/形状模型；构造这些张量/模型的旋转不变量（如迹的幂、张量不变量），形成可逐步提高精度的形状描述子，并保持可解码性。

Result: 得到一族从低到高阶的旋转不变形状特征，能以更高精度近似复杂形状，同时避免在比较时对旋转进行显式优化；可用于2D/3D对象、分子形状的快速匹配与识别。

Conclusion: 高阶中心矩与多项式-高斯扩展为PCA提供了通用的旋转不变描述框架，兼具可扩展精度与高效比较，适合形状检索、识别与相似度度量等应用。

Abstract: PCA can be used for rotation invariant features, describing a shape with its $p_{ab}=E[(x_i-E[x_a])(x_b-E[x_b])]$ covariance matrix approximating shape by ellipsoid, allowing for rotation invariants like its traces of powers. However, real shapes are usually much more complicated, hence there is proposed its extension to e.g. $p_{abc}=E[(x_a-E[x_a])(x_b-E[x_b])(x_c-E[x_c])]$ order-3 or higher tensors describing central moments, or polynomial times Gaussian allowing decodable shape descriptors of arbitrarily high accuracy, and their analogous rotation invariants. Its practical applications could be rotation-invariant features to include shape modulo rotation e.g. for molecular shape descriptors, or for up to rotation object recognition in 2D images/3D scans, or shape similarity metric allowing their inexpensive comparison (modulo rotation) without costly optimization over rotations.

</details>


### [7] [MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.03331)
*Yang Shi,Yifeng Xie,Minzhe Guo,Liangsi Lu,Mingxuan Huang,Jingchao Wang,Zhihong Zhu,Boyan Xu,Zhiqi Huang*

Main category: cs.CV

TL;DR: MMErroR是一个包含2013个样本的多模态基准，每个样本嵌入一种连贯的推理错误，用于评估VLM能否检测并分类视觉与文本中的错误类型。评测20个先进VLM，最佳模型仅66.47%正确分类，凸显错误识别难度与过程级评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态评测多聚焦答案正确性，无法判断模型是否真正理解并能发现推理过程中的具体错误类型。为诊断VLM的多模态推理能力与脆弱点，需要一个以“错误检测与归因”为核心、覆盖广泛领域的基准。

Method: 构建MMErroR基准：2013条多模态样本，覆盖6个顶级域、24个子域；每条样本仅包含一种一致性的推理错误。任务是让模型在视觉与语言上下文中识别推理不正确并分类错误类型。用该基准系统评测20个先进VLM（如Gemini-3.0-Pro等）。

Result: 在该基准上，整体性能受限：最佳模型Gemini-3.0-Pro在错误类型分类上的正确率为66.47%，其他模型更低，显示当前VLM在错误识别与归因方面存在明显差距。

Conclusion: 过程级、以错误为中心的评估揭示了VLM对多模态推理错误识别的不足；准确定位错误类型为理解模型能力与改进方向提供了更细粒度诊断信号，MMErroR可作为后续方法研究与训练目标设计的关键基准。

Abstract: Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io

</details>


### [8] [RelightAnyone: A Generalized Relightable 3D Gaussian Head Model](https://arxiv.org/abs/2601.03357)
*Yingyan Xu,Pramod Rao,Sebastian Weiss,Gaspard Zoss,Markus Gross,Christian Theobalt,Marc Habermann,Derek Bradley*

Main category: cs.CV

TL;DR: 提出一种通用可重光照的3D高斯头像模型，无需对目标进行OLAT采集，即可从单/多视图图像构建并进行高质量重光照。方法采用两阶段：先学“平光”3DGS头像并跨数据集泛化，再将其映射到物理反射参数，从而实现接近OLAT效果的重光照与单图拟合。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的头像重建在重光照方面依赖昂贵的时间复用光场（如OLAT），限制实际应用与泛化。需要一种能在缺乏OLAT的常规多视图或甚至单图条件下，依然获得高质量、物理一致的重光照能力的方法。

Method: 两阶段框架：1) 平光建模阶段：在无OLAT的多视图数据上训练平光3DGS头像，利用数据集级“lighting code”进行自监督的光照对齐与跨主体泛化；2) 物理参数映射阶段：在少量OLAT数据上学习将平光3DGS参数映射为可重光照的物理反射参数（如与PBR一致的属性），实现物理正确的渲染。最终可从单/多视图拟合一位新主体的平光头像，再经映射获得可重光照模型。

Result: 方法在无目标OLAT的情况下，仍能对任意主体实现高质量、物理一致的重光照效果；相比只在OLAT下训练的方案，实现跨主体与跨数据集的良好泛化；并能从单张图像拟合实现新视角合成与重光照。

Conclusion: 通过先平光建模、后物理映射的两阶段训练，结合少量OLAT监督，实现了对任意主体的逼真重光照与视角合成，显著降低采集成本并提升泛化能力，使3DGS头像在实际应用中更可行。

Abstract: 3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars. A major challenge is to relight the avatars to match any scene illumination. For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT). We propose a new generalized relightable 3D Gaussian head model that can relight any subject observed in a single- or multi-view images without requiring OLAT data for that subject. Our core idea is to learn a mapping from flat-lit 3DGS avatars to corresponding relightable Gaussian parameters for that avatar. Our model consists of two stages: a first stage that models flat-lit 3DGS avatars without OLAT lighting, and a second stage that learns the mapping to physically-based reflectance parameters for high-quality relighting. This two-stage design allows us to train the first stage across diverse existing multi-view datasets without OLAT lighting ensuring cross-subject generalization, where we learn a dataset-specific lighting code for self-supervised lighting alignment. Subsequently, the second stage can be trained on a significantly smaller dataset of subjects captured under OLAT illumination. Together, this allows our method to generalize well and relight any subject from the first stage as if we had captured them under OLAT lighting. Furthermore, we can fit our model to unseen subjects from as little as a single image, allowing several applications in novel view synthesis and relighting for digital avatars.

</details>


### [9] [Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views](https://arxiv.org/abs/2601.03362)
*Xiang Zhang,Yang Zhang,Lukas Mehl,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: 提出HairGuard框架，专攻“软边界”（如细发丝）在3D视觉中的恢复：利用抠图数据与深度修复网络精炼软边界深度，再以深度前向投影保留纹理、生成式补画填补遮挡区，并通过颜色融合获得几何一致且细节丰富的新视图；在单目深度、双目/视频转换与新视图合成上达SOTA，尤其改善软边界。


<details>
  <summary>Details</summary>
Motivation: 自然图像与CG中大量存在发丝等软边界，但前景/背景混合导致3D任务（深度估计、视图合成）在这些区域易伪影、误深度与纹理丢失；现有方法难兼顾全局深度质量与局部细节，需要一套能识别并专门处理软边界的通用方案。

Method: 1) 数据策划：利用图像抠图数据训练，构建能识别软边界的监督信号。2) 深度修复网络（Depth Fixer）：含门控残差模块，自动定位软边界并局部精修深度，同时保持全局深度稳定，可即插即用叠加于SOTA深度模型。3) 视图合成：先基于修复深度做前向投影以最大化保留高保真纹理；再用生成式场景补画（Scene Painter）填补视差揭露区并去除软边界内的冗余背景；最后以颜色融合模块自适应融合投影与补画结果，产出几何一致、细节完整的新视图。

Result: 在多个基准任务与场景（单目深度估计、双目图像/视频转换、新视图合成）中取得SOTA，定量与定性评估均显示软边界区域有显著提升，并维持或提升全局深度质量与视图一致性。

Conclusion: 针对软边界难题，HairGuard提供从数据到模型到渲染的端到端方案：精准软边界检测与深度精修结合深度驱动投影与生成式补画，再通过自适应融合提升几何与纹理一致性；方法通用、可插拔，显著改善含发丝等细节的3D视觉表现。

Abstract: Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.

</details>


### [10] [RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models](https://arxiv.org/abs/2601.03369)
*Sha Luo,Yogesh Prabhu,Tim Ossowski,Kaiping Chen,Junjie Hu*

Main category: cs.CV

TL;DR: 提出RiskCueBench基准：在视频中标注“风险信号片段”，用于更贴近真实场景地进行早期风险预判评测。实验证明现有方法在依据早期视觉线索预测未来风险方面存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频风险评估多为监督学习，且常可看到完整视频（包含事故发生段），导致任务被简化，难以反映实际部署时需要“事前预警”的需求。为推动能从早期线索预判风险的研究，需要一个贴近现实、关注早期风险信号的基准。

Method: 构建并发布RiskCueBench：对视频进行精细标注，明确“风险信号片段”（最早显示潜在安全隐患的时刻）。用该基准系统评测当前视频理解/风险预测模型，从仅有的早期视觉线索来预判后续风险事件。

Result: 在RiskCueBench上，当前主流系统在仅依赖早期风险信号进行未来风险预测时表现不佳，出现明显性能缺口。

Conclusion: 早期风险感知与预判仍是开放挑战。需要面向时间演化理解、弱线索聚合与不确定性建模的算法，方能在真实场景中可靠部署视频风险预测模型。

Abstract: With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.

</details>


### [11] [A Novel Unified Approach to Deepfake Detection](https://arxiv.org/abs/2601.03382)
*Lord Sen,Shyamapada Mukherjee*

Main category: cs.CV

TL;DR: 提出一种用于图像与视频深伪检测的统一架构：在空间域与频域特征之间引入跨注意力，并结合“血液检测”模块；在FF++与Celeb-DF上分别以Swin+BERТ达AUC 99.80%、99.88%，以EfficientNet-B4+BERТ达99.55、99.38，且具备良好跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 深伪生成与滥用威胁信任与安全，现有检测方法难以在不同数据集与操控类型间稳定泛化。需要一个能融合多模态线索、鲁棒且统一的检测框架。

Method: 构建统一深伪检测网络：1) 同时提取空间域与频域特征；2) 通过跨注意力机制交互两域信息，突出伪造痕迹；3) 设计“血液检测”模块（可能利用肤色/血流相关信号如rPPG或色彩分布）以捕捉生理一致性；4) 采用视觉骨干（Swin或EfficientNet-B4）与文本样式建模的BERТ模块进行判别；5) 统一用于图像与视频。

Result: 在FF++与Celeb-DF上达到或超过SOTA：Swin+BERТ配置AUC分别99.80%、99.88；EfficientNet-B4+BERТ配置AUC分别99.55、99.38；并显示良好的跨数据集泛化性能。

Conclusion: 跨注意力融合空间与频域特征并引入生理线索的统一架构能显著提升深伪检测精度与泛化，优于现有方法，适用于图像与视频场景。

Abstract: The advancements in the field of AI is increasingly giving rise to various threats. One of the most prominent of them is the synthesis and misuse of Deepfakes. To sustain trust in this digital age, detection and tagging of deepfakes is very necessary. In this paper, a novel architecture for Deepfake detection in images and videos is presented. The architecture uses cross attention between spatial and frequency domain features along with a blood detection module to classify an image as real or fake. This paper aims to develop a unified architecture and provide insights into each step. Though this approach we achieve results better than SOTA, specifically 99.80%, 99.88% AUC on FF++ and Celeb-DF upon using Swin Transformer and BERT and 99.55, 99.38 while using EfficientNet-B4 and BERT. The approach also generalizes very well achieving great cross dataset results as well.

</details>


### [12] [Better, But Not Sufficient: Testing Video ANNs Against Macaque IT Dynamics](https://arxiv.org/abs/2601.03392)
*Matteo Dunnhofer,Christian Micheloni,Kohitij Kar*

Main category: cs.CV

TL;DR: 作者比较猕猴IT皮层在自然视频中的时间响应与多类ANN（静态、循环、视频模型），发现视频模型仅略有提升，且无法在外观被移除但保留运动的“无外观”视频上泛化，而IT能泛化，说明现有模型只学到与外观绑定的动态而非IT的外观不变时间计算。


<details>
  <summary>Details</summary>
Motivation: 灵长类视觉系统在动态世界中运行，而主流ANN多基于静态图像，难以解释IT在自然视频中对运动速度等动态特征的编码。作者想回答：IT的时间响应是简单的时间展开前馈/浅层时间池化，还是更丰富的动态计算？

Method: 记录猕猴IT在观看自然视频时的群体神经响应；用多类模型拟合与比较，包括静态前馈ANN、递归/循环模型、以及视频专用ANN；评估神经可预测性，尤其在响应后期。设计“压力测试”：用在自然视频上训练的解码器去测试“无外观”视频（保留运动统计，移除形状与纹理），比较IT与各ANN在该跨域泛化上的表现。

Result: 视频模型相对静态与循环模型在神经预测上仅有温和改进，主要体现在较晚时间窗；在压力测试中，IT群体活动对无外观视频仍能泛化，但所有ANN类别（含视频模型）都失败，显示模型学到的是外观依赖的动态。

Conclusion: 当前视频ANN更好地拟合与外观绑定的动态，而未捕捉IT展现的外观不变时间计算。需要新的训练目标与归纳偏置来编码生物式的时间统计与不变性，以更贴近灵长类IT的动态表征。

Abstract: Feedforward artificial neural networks (ANNs) trained on static images remain the dominant models of the the primate ventral visual stream, yet they are intrinsically limited to static computations. The primate world is dynamic, and the macaque ventral visual pathways, specifically the inferior temporal (IT) cortex not only supports object recognition but also encodes object motion velocity during naturalistic video viewing. Does IT's temporal responses reflect nothing more than time-unfolded feedforward transformations, framewise features with shallow temporal pooling, or do they embody richer dynamic computations? We tested this by comparing macaque IT responses during naturalistic videos against static, recurrent, and video-based ANN models. Video models provided modest improvements in neural predictivity, particularly at later response stages, raising the question of what kind of dynamics they capture. To probe this, we applied a stress test: decoders trained on naturalistic videos were evaluated on "appearance-free" variants that preserve motion but remove shape and texture. IT population activity generalized across this manipulation, but all ANN classes failed. Thus, current video models better capture appearance-bound dynamics rather than the appearance-invariant temporal computations expressed in IT, underscoring the need for new objectives that encode biological temporal statistics and invariances.

</details>


### [13] [Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning](https://arxiv.org/abs/2601.03400)
*Ali Najar,Alireza Mirrokni,Arshia Izadyari,Sadegh Mohammadian,Amir Homayoon Sharifizade,Asal Meskin,Mobin Bagherian,Ehsaneddin Asgari*

Main category: cs.CV

TL;DR: 论文提出Eye-Q，一个用于评估视觉-语言模型深层推理与抽象关联能力的多语言视觉字谜基准，展示当前SOTA模型在抽象、跨语言任务上的显著短板（最高仅60.27%）。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在常规基准上看似强，但往往依赖表层匹配、OCR或检索式策略，缺乏对隐含线索的发现、假设生成与修正、以及从视觉证据到非字面概念的映射能力；需要一个能系统考察这些能力的挑战性评测。

Method: 构建Eye-Q数据集：1,343个多语言（英语、波斯语、阿拉伯语及跨语言）视觉字谜。每个样例给出概念密集图像与简短描述，目标是推断特定词/短语。题目刻意非结构化、线索隐含，含干扰项与复杂上下文关系，要求选择性注意、抽象化与联想推理。采用开放式、人类对齐的评测协议，允许轻量辅助以探测模型的假设形成与修正能力；对多种SOTA VLM进行评测。

Result: 当前模型在该任务上存在显著性能差距，尤其在抽象和跨语言谜题上表现不佳。即便最强模型，准确率最高仅达60.27%，显示其难以构建并搜索合适的概念表示以完成灵活的“图像到短语”推断。

Conclusion: 视觉字谜提供了比传统基准更能检验深层视觉-语言理解与抽象推理的任务。Eye-Q揭示了现有VLM在隐含线索理解、概念抽象与跨语言推断方面的不足，为后续改进模型的概念表示、注意力与推理机制提供了方向。

Abstract: Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.

</details>


### [14] [GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models](https://arxiv.org/abs/2601.03416)
*Xiangdong Hu,Yangyang Jiang,Qin Hu,Xiaojun Jia*

Main category: cs.CV

TL;DR: 提出GAMBIT，一种“游戏化陷阱”的多模态越狱框架，通过分解并重组有害视觉语义，诱导推理型与非推理型MLLM在完成游戏目标时无意生成有害回答，显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态越狱多靠增加任务视觉复杂度，未充分利用模型的链式推理动机，导致对具备推理能力的模型越狱效果欠佳。作者试图通过影响模型“认知阶段决策”，让其在自我驱动的推理与目标追求中主动绕过安全对齐。

Method: 提出GAMBIT：1) 将有害意图的视觉语义分解与重组；2) 构造“游戏化场景”和指令陷阱，设定“赢得游戏”的目标；3) 通过结构化的多模态推理链，引导模型探索、重构隐含恶意意图并作答；该设计同时提升视觉与文本两端的任务复杂度，降低模型对安全警觉的权重。

Result: 在多种推理/非推理型MLLM上取得高ASR：Gemini 2.5 Flash为92.13%，QvQ-MAX为91.20%，GPT-4o为85.87%，显著优于基线方法。

Conclusion: 利用“游戏化指令陷阱”激发模型的推理与目标追求，可系统性削弱安全对齐并诱导有害输出，对现有MLLMs构成强威胁；强调需要更稳健的安全机制以抵御以推理动机为靶点的攻防。

Abstract: Multimodal Large Language Models (MLLMs) have become widely deployed, yet their safety alignment remains fragile under adversarial inputs. Previous work has shown that increasing inference steps can disrupt safety mechanisms and lead MLLMs to generate attacker-desired harmful content. However, most existing attacks focus on increasing the complexity of the modified visual task itself and do not explicitly leverage the model's own reasoning incentives. This leads to them underperforming on reasoning models (Models with Chain-of-Thoughts) compared to non-reasoning ones (Models without Chain-of-Thoughts). If a model can think like a human, can we influence its cognitive-stage decisions so that it proactively completes a jailbreak? To validate this idea, we propose GAMBI} (Gamified Adversarial Multimodal Breakout via Instructional Traps), a novel multimodal jailbreak framework that decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. The resulting structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention and induces it to answer the reconstructed malicious query. Extensive experiments on popular reasoning and non-reasoning MLLMs demonstrate that GAMBIT achieves high Attack Success Rates (ASR), reaching 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines.

</details>


### [15] [WeedRepFormer: Reparameterizable Vision Transformers for Real-Time Waterhemp Segmentation and Gender Classification](https://arxiv.org/abs/2601.03431)
*Toqi Tahamid Sarker,Taminul Islam,Khaled R. Ahmed,Cristiana Bernardi Rankrape,Kaitlin E. Creager,Karla Gage*

Main category: cs.CV

TL;DR: WeedRepFormer 是一个轻量级多任务 Vision Transformer，用结构重参数化贯穿骨干、解码器与分类头，同时完成水麻黄（水麻？waterhemp）分割与雌雄分类；在自建含10,264帧的数据集上，以3.59M参数、3.80 GFLOPs 达到92.18% mIoU、81.91%分类准确，108.95 FPS，并以更小参数量优于iFormer-T的分类精度。


<details>
  <summary>Details</summary>
Motivation: 农业场景下需要在移动/边缘设备上实时完成杂草精细分割与生物学属性（如性别）分类，但现有方法难以兼顾细粒度特征提取与推理效率。

Method: 提出 WeedRepFormer：- 端到端多任务架构（分割+性别分类）。- 全局结构重参数化：在ViT骨干、Lite R-ASPP解码器与可重参数化分类头中，将训练期的多分支/大容量结构在推理期折叠为等效的单分支卷积/线性以降延。- 构建包含23株植株、10,264标注帧的waterhemp数据集作为基准。

Result: 在该数据集上：- 分割 mIoU 92.18%。- 性别分类准确率 81.91%。- 仅 3.59M 参数、3.80 GFLOPs，推理速度 108.95 FPS。- 相比 iFormer-T：分类准确率 +4.40%，分割性能可比，参数量降低约1.9倍。

Conclusion: 结构重参数化驱动的轻量级多任务ViT能在保持精细分割能力的同时显著提高实时性与性别分类表现，是面向农业现场部署的有效方案，并伴随开放的新数据集可作为后续研究基准。

Abstract: We present WeedRepFormer, a lightweight multi-task Vision Transformer designed for simultaneous waterhemp segmentation and gender classification. Existing agricultural models often struggle to balance the fine-grained feature extraction required for biological attribute classification with the efficiency needed for real-time deployment. To address this, WeedRepFormer systematically integrates structural reparameterization across the entire architecture - comprising a Vision Transformer backbone, a Lite R-ASPP decoder, and a novel reparameterizable classification head - to decouple training-time capacity from inference-time latency. We also introduce a comprehensive waterhemp dataset containing 10,264 annotated frames from 23 plants. On this benchmark, WeedRepFormer achieves 92.18% mIoU for segmentation and 81.91% accuracy for gender classification using only 3.59M parameters and 3.80 GFLOPs. At 108.95 FPS, our model outperforms the state-of-the-art iFormer-T by 4.40% in classification accuracy while maintaining competitive segmentation performance and significantly reducing parameter count by 1.9x.

</details>


### [16] [FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder](https://arxiv.org/abs/2601.03460)
*Zeyu Dong,Yimin Zhu,Yu Wu,Yu Sun*

Main category: cs.CV

TL;DR: 提出FROST-Drive：在端到端自动驾驶中冻结来自VLM的视觉编码器，配合变换器适配器与GRU解码器，并用直接优化RFS的损失；在Waymo Open E2E上显著优于全面微调方法，提升泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶常把传感输入直接映射为控制，但在新颖、长尾场景下泛化差。普遍做法是对视觉编码器在驾驶数据上全面微调，易导致过拟合与领域特化，牺牲大模型原有通用知识。作者质疑“必须全量微调”的范式，探索如何保留VLM的广泛世界知识以提升泛化与稳健性。

Method: 提出FROST-Drive：1) 冻结来自VLM的预训练视觉编码器，直接迁移其通用表征；2) 以Transformer式适配器做多模态融合（视觉与驾驶相关输入）；3) 采用GRU解码器生成平滑航迹/路标点；4) 设计面向Rater Feedback Score（RFS）的定制损失，直接优化注重稳健轨迹规划的指标。

Result: 在Waymo Open E2E（强调长尾复杂场景的大规模数据集）上，冻结编码器的方案显著超过全量微调模型，在鲁棒规划与泛化上取得更好表现；实验广泛，证实冻结策略的有效性。

Conclusion: 保留并利用VLM的广泛知识、避免领域内过度特化，比密集的域内微调更有效，可得到更鲁棒、可泛化的驾驶性能；为面向真实世界复杂性的视觉端到端驾驶提供了新的设计路径。

Abstract: End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder's weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a transformer-based adapter for multimodal fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.

</details>


### [17] [Experimental Comparison of Light-Weight and Deep CNN Models Across Diverse Datasets](https://arxiv.org/abs/2601.03463)
*Md. Hefzul Hossain Papon,Shadman Rabby*

Main category: cs.CV

TL;DR: 论文表明：经过良好正则化的浅层CNN在多种异构视觉任务上可作为强竞争基线，且无需大GPU或大型预训练模型；并在多个人工智能孟加拉视觉数据集上建立了统一可复现的基准，凸显轻量CNN在低资源场景的实用价值。


<details>
  <summary>Details</summary>
Motivation: 当前许多计算机视觉方法依赖深层网络、大规模预训练与高算力，不利于在资源受限环境与新兴本地数据集上部署。作者希望验证：在异构任务与数据集上，是否存在更轻量、可复现、可落地的统一基线，并为孟加拉相关视觉数据提供标准化比较基准。

Method: 采用浅层卷积神经网络（轻量CNN）并通过严格的正则化（如数据增强、权重衰减、dropout等）进行训练；在跨领域（智慧城市监控、农业品种分类等）多数据集上统一评测流程，构建可复现的基准设置，与更复杂模型需求进行对比。

Result: 良好正则化的浅层架构在多种异构数据集上取得与复杂模型相当的竞争性表现，同时训练与推理资源需求显著更低；为多个人工智能孟加拉视觉数据集提供了统一、可复现的实验基线。

Conclusion: 在低资源场景下，轻量且正则化得当的浅层CNN足以成为强基线与实用方案；建立的统一基准有助于社区在孟加拉多视觉任务上公平对比与快速迭代，减少对大规模预训练与高算力的依赖。

Abstract: Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.

</details>


### [18] [Latent Geometry of Taste: Scalable Low-Rank Matrix Factorization](https://arxiv.org/abs/2601.03466)
*Joshua Salako*

Main category: cs.CV

TL;DR: 在MovieLens 32M上用并行ALS探究用户偏好潜在几何：通过严谨调参发现低秩受限模型在泛化与排序上优于高维模型，嵌入空间自发形成类型簇，并在冷启动中通过可调打分在流行度与个性化间权衡。


<details>
  <summary>Details</summary>
Motivation: 协同过滤在超大规模与高稀疏交互数据下受制于可扩展性与稀疏性，亟需既高效又能泛化的表示学习方法；同时想验证仅凭交互数据能否捕捉语义结构，并解决冷启动中的流行度偏置问题。

Method: 实现高性能并行ALS矩阵分解框架；系统化超参数搜索比较不同秩/约束设置；评估RMSE与排名精度；可视化嵌入以观察语义聚类；设计冷启动实验并引入可调评分参数以控制流行度与个性化的权衡。

Result: 受限低秩模型在泛化上优于高维模型，在RMSE与排名精度间取得更佳平衡；嵌入空间无监督地产生与电影类型一致的簇；在冷启动中，通过调节评分参数，可以有效控制流行度偏置与个性化亲和度。

Conclusion: 低秩并行ALS在大规模稀疏数据上既高效又具良好泛化，能够仅由交互数据学习出语义结构；提出的可调评分为冷启动提供实用控制手段。

Abstract: Scalability and data sparsity remain critical bottlenecks for collaborative filtering on massive interaction datasets. This work investigates the latent geometry of user preferences using the MovieLens 32M dataset, implementing a high-performance, parallelized Alternating Least Squares (ALS) framework. Through extensive hyperparameter optimization, we demonstrate that constrained low-rank models significantly outperform higher dimensional counterparts in generalization, achieving an optimal balance between Root Mean Square Error (RMSE) and ranking precision. We visualize the learned embedding space to reveal the unsupervised emergence of semantic genre clusters, confirming that the model captures deep structural relationships solely from interaction data. Finally, we validate the system's practical utility in a cold-start scenario, introducing a tunable scoring parameter to manage the trade-off between popularity bias and personalized affinity effectively. The codebase for this research can be found here: https://github.com/joshsalako/recommender.git

</details>


### [19] [ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing](https://arxiv.org/abs/2601.03467)
*Hengjia Li,Liming Jiang,Qing Yan,Yizhi Song,Hao Kang,Zichuan Liu,Xin Lu,Boxi Wu,Deng Cai*

Main category: cs.CV

TL;DR: ThinkRL-Edit 通过在生成前引入链式思维推理采样、偏差更小的多维奖励聚合与二元清单式VLM奖励，显著提升面向推理的图像编辑效果。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态生成模型用于指令驱动图像编辑时，视觉推理能力不足，难以处理需要复杂理解与规划的编辑；现有RL方法在探索仅限去噪随机性、奖励融合偏置、以及VLM打分不稳定三方面存在瓶颈。

Method: 提出ThinkRL-Edit：1) 将视觉推理与图像合成解耦，在在线采样前引入CoT推理采样，包含规划与反思阶段，探索多种语义假设并自检可行性；2) 设计无偏的链式偏好分组（chain preference grouping）跨多奖励维度进行偏好学习，避免加权聚合带来的偏置；3) 用二元清单式VLM奖励替代区间分数，降低方差并提高可解释性。

Result: 在面向推理的图像编辑任务上显著优于现有方法，生成的结果更符合指令、视觉一致且语义扎实；实验表明三项组件均有贡献。

Conclusion: 通过在RL中显式建模并扩展推理探索、纠正多维奖励融合偏差、稳定VLM奖励，ThinkRL-Edit有效提升了推理中心图像编辑的质量与可靠性。

Abstract: Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.

</details>


### [20] [Understanding Reward Hacking in Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2601.03468)
*Yunqi Hong,Kuei-Chun Kao,Hengguang Zhou,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 论文分析了T2I模型在强化学习后训练中因奖励设计缺陷导致的“reward hacking”，提出用一个轻量自适应的“伪影奖励”模型作为正则器，显著提升真实感并降低投机取巧。


<details>
  <summary>Details</summary>
Motivation: 现有审美/偏好和文本一致性奖励只是人类判断的代理，存在偏差，模型会学会投机（生成不真实但高分图像）。需要系统理解这些黑客化模式并寻找简单可插拔的防护机制。

Method: 1) 系统实证分析：分别考察审美/偏好奖励与一致性奖励对reward hacking的贡献；评估多奖励集成的缓解能力。2) 发现共性失败模式：生成含伪影的图像。3) 构建轻量的“伪影奖励”模型：用小型精心标注数据（无伪影 vs 有伪影）训练，作为正则器加入现有T2I RL管线，以惩罚伪影倾向。

Result: 跨多种奖励模型与RL配置，单靠现有奖励或其集成仍会出现伪影型reward hacking；加入伪影奖励后，视觉真实感提升，reward hacking显著减少，且方法计算/数据成本低、易集成。

Conclusion: reward hacking在T2I RL后训练中普遍存在，核心失败模式是伪影产生。用轻量、可适配的伪影奖励作为附加正则可以有效抑制该问题，作为安全护栏提升生成质量与对齐度。

Abstract: Reinforcement learning (RL) has become a standard approach for post-training large language models and, more recently, for improving image generation models, which uses reward functions to enhance generation quality and human preference alignment. However, existing reward designs are often imperfect proxies for true human judgment, making models prone to reward hacking--producing unrealistic or low-quality images that nevertheless achieve high reward scores. In this work, we systematically analyze reward hacking behaviors in text-to-image (T2I) RL post-training. We investigate how both aesthetic/human preference rewards and prompt-image consistency rewards individually contribute to reward hacking and further show that ensembling multiple rewards can only partially mitigate this issue. Across diverse reward models, we identify a common failure mode: the generation of artifact-prone images. To address this, we propose a lightweight and adaptive artifact reward model, trained on a small curated dataset of artifact-free and artifact-containing samples. This model can be integrated into existing RL pipelines as an effective regularizer for commonly used reward models. Experiments demonstrate that incorporating our artifact reward significantly improves visual realism and reduces reward hacking across multiple T2I RL setups, demonstrating the effectiveness of lightweight reward augment serving as a safeguard against reward hacking.

</details>


### [21] [CroBIM-U: Uncertainty-Driven Referring Remote Sensing Image Segmentation](https://arxiv.org/abs/2601.03490)
*Yuzhe Sun,Zhe Dong,Haochen Jiang,Tianzhu Liu,Yanfeng Gu*

Main category: cs.CV

TL;DR: 提出一种用于遥感指代分割的“不确定性感知”框架：先用像素级指代不确定性图指导跨模态融合与局部细化，在高不确定区域强化语言约束、在低不确定区域抑制干扰，从而提升复杂场景下的鲁棒性与几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法对整幅图像执行统一的跨模态融合与细化，忽视了空间上指代难度的不均匀性：尺度变化极端、相似干扰密集、边界复杂导致的跨模态对齐在不同区域可靠性差异很大。统一处理会在清晰区域引入不必要语言噪声，同时又无法在混淆区域提供足够判别力。

Method: 1) 训练一个可插拔的像素级“指代不确定性评分器”(RUS)，通过在线误差一致性监督学习，输出空间分布的指代不确定性图；2) 基于该先验设计两模块：a) 不确定性门控融合(UGF)，按不确定性动态调节语言注入强度，高不确定处增强约束、低不确定处抑制噪声；b) 不确定性驱动的局部细化(UDLR)，利用不确定性软掩膜聚焦边界与细节等易错区域进行精修；3) 整体为即插即用，不改动主干架构。

Result: 在复杂遥感场景的多项实验中，该方法在鲁棒性与几何保真度上显著优于现有方法，且作为统一的可插拔方案可无缝提升不同骨干模型。

Conclusion: 显式建模像素级指代不确定性并据此进行自适应跨模态融合与局部细化，能有效缓解空间非均匀性的对齐问题，在无需改动主干的前提下稳健提升遥感指代分割性能。

Abstract: Referring remote sensing image segmentation aims to localize specific targets described by natural language within complex overhead imagery. However, due to extreme scale variations, dense similar distractors, and intricate boundary structures, the reliability of cross-modal alignment exhibits significant \textbf{spatial non-uniformity}. Existing methods typically employ uniform fusion and refinement strategies across the entire image, which often introduces unnecessary linguistic perturbations in visually clear regions while failing to provide sufficient disambiguation in confused areas. To address this, we propose an \textbf{uncertainty-guided framework} that explicitly leverages a pixel-wise \textbf{referring uncertainty map} as a spatial prior to orchestrate adaptive inference. Specifically, we introduce a plug-and-play \textbf{Referring Uncertainty Scorer (RUS)}, which is trained via an online error-consistency supervision strategy to interpretably predict the spatial distribution of referential ambiguity. Building on this prior, we design two plug-and-play modules: 1) \textbf{Uncertainty-Gated Fusion (UGF)}, which dynamically modulates language injection strength to enhance constraints in high-uncertainty regions while suppressing noise in low-uncertainty ones; and 2) \textbf{Uncertainty-Driven Local Refinement (UDLR)}, which utilizes uncertainty-derived soft masks to focus refinement on error-prone boundaries and fine details. Extensive experiments demonstrate that our method functions as a unified, plug-and-play solution that significantly improves robustness and geometric fidelity in complex remote sensing scenes without altering the backbone architecture.

</details>


### [22] [SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2601.03500)
*Yuxuan Xia,Siheng Wang,Peng Li*

Main category: cs.CV

TL;DR: 提出一种无需再训练的对比解码方法SDCD，通过引入“打乱结构”的视图进行置信度校准，抑制贴片纹理偏置，从而显著降低LVLM的物体幻觉并提升多模态能力。


<details>
  <summary>Details</summary>
Motivation: LVLM虽在多模态理解/推理上进步显著，但仍易产生物体幻觉。现有工作多从语言先验或高层统计偏置入手，忽视视觉编码过程中因弱结构监督导致的贴片化统计偏置（Bag-of-Patches），需要一种能直接针对视觉偏置且易于应用的方法。

Method: 提出Structure-Disrupted Contrastive Decoding（SDCD），推理时生成一个“结构被打乱”的视觉视图（如打乱patch顺序），与原视图进行对比解码；若某些输出token在无结构视图下仍维持高置信度，则判定为受纹理驱动的虚假自信并予以惩罚，从而校准输出分布；该方法训练无关、即插即用。

Result: 在多个数据集/基准上显著减少物体幻觉，同时整体提升LVLM多模态能力，效果稳定。

Conclusion: 视觉编码中的贴片统计偏置会促使幻觉；在推理阶段通过结构破坏的对比校准可有效压制纹理偏置，无需训练即可广泛适配并提升LVLM表现。

Abstract: Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.

</details>


### [23] [REFA: Real-time Egocentric Facial Animations for Virtual Reality](https://arxiv.org/abs/2601.03507)
*Qiang Zhang,Tong Xiao,Haroun Habeeb,Larissa Laich,Sofien Bouaziz,Patrick Snape,Wenjing Zhang,Matthew Cioffi,Peizhao Zhang,Pavel Pidlypenskyi,Winnie Lin,Luming Ma,Mengjiao Wang,Kunpeng Li,Chengjiang Long,Steven Song,Martin Prazak,Alexander Sjoholm,Ajinkya Deogade,Jaebong Lee,Julio Delgado Mangas,Amaury Aubel*

Main category: cs.CV

TL;DR: 提出一种在VR头显内置红外相机的自视角下，实时、免侵入、少校准的人脸表情跟踪系统，通过蒸馏学习融合多源异构数据与标签，实现高精度驱动虚拟角色。


<details>
  <summary>Details</summary>
Motivation: 现有VR中表情捕捉常需额外硬件、侵入式标记或繁琐校准，且真实/合成数据与标签异构难以统一训练，限制了实时、普适的表情驱动能力。

Method: 以知识蒸馏为核心，将来自合成与真实图像的异构标签统一到单一模型；构建含18k多样受试者的数据集，采用手机与定制多红外相机VR头显进行轻量采集；设计稳健的可微渲染流程自动提取表情参数标签，并据此训练实时推理模型。

Result: 获得能在自视角红外图像下实时、准确追踪表情的模型，实现对虚拟角色的高保真驱动，且对用户无侵入、无需或仅需很少校准。

Conclusion: 该系统实现了在VR场景中普适、实时的表情追踪，降低采集与标注成本，提升可用性，可广泛应用于视频会议、游戏、娱乐与远程协作等虚拟交互场景。

Abstract: We present a novel system for real-time tracking of facial expressions using egocentric views captured from a set of infrared cameras embedded in a virtual reality (VR) headset. Our technology facilitates any user to accurately drive the facial expressions of virtual characters in a non-intrusive manner and without the need of a lengthy calibration step. At the core of our system is a distillation based approach to train a machine learning model on heterogeneous data and labels coming form multiple sources, \eg synthetic and real images. As part of our dataset, we collected 18k diverse subjects using a lightweight capture setup consisting of a mobile phone and a custom VR headset with extra cameras. To process this data, we developed a robust differentiable rendering pipeline enabling us to automatically extract facial expression labels. Our system opens up new avenues for communication and expression in virtual environments, with applications in video conferencing, gaming, entertainment, and remote collaboration.

</details>


### [24] [G2P: Gaussian-to-Point Attribute Alignment for Boundary-Aware 3D Semantic Segmentation](https://arxiv.org/abs/2601.03510)
*Hojun Song,Chae-yeong Song,Jeong-hun Hong,Chaewon Moon,Dong-hwi Kim,Gahyeon Kim,Soo Ye Kim,Yiyi Liao,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: 提出G2P方法，把3D Gaussian Splatting中的外观相关属性迁移到点云上，以提升点云语义分割，显著改善外观相似/几何歧义类的区分与边界定位，无需2D或语言监督，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 点云稀疏且不规则，几何特征不足以区分外观不同但形状相近的物体，现有仅依赖几何的模型在外观歧义和复杂边界处表现欠佳。作者希望引入能表征颜色/材质/纹理等外观线索，同时与点云对齐并可用于分割训练。

Method: 提出Gaussian-to-Point (G2P)：1) 从3D Gaussian Splatting中提取外观相关属性（如不透明度、尺度等）；2) 通过建立点与优化后高斯的点对点对应，解决高斯与原始点几何错位问题；3) 将高斯的不透明度用于缓解几何歧义，提高类可分性；4) 利用高斯尺度实现精确的边界定位；5) 将这些外观感知属性作为点云特征，融合到分割网络中进行训练与推理。

Result: 在标准基准上取得优于现有方法的性能，特别是在几何上具有挑战性的类别上有显著提升；不依赖任何2D图像或语言监督也能获得强表现。

Conclusion: 将3D Gaussian的外观属性有效转移到点云可弥补几何特征不足，带来更判别、外观一致的分割以及更精确的边界，且无需跨模态监督，具有通用性与实用价值。

Abstract: Semantic segmentation on point clouds is critical for 3D scene understanding. However, sparse and irregular point distributions provide limited appearance evidence, making geometry-only features insufficient to distinguish objects with similar shapes but distinct appearances (e.g., color, texture, material). We propose Gaussian-to-Point (G2P), which transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds for more discriminative and appearance-consistent segmentation. Our G2P address the misalignment between optimized Gaussians and original point geometry by establishing point-wise correspondences. By leveraging Gaussian opacity attributes, we resolve the geometric ambiguity that limits existing models. Additionally, Gaussian scale attributes enable precise boundary localization in complex 3D scenes. Extensive experiments demonstrate that our approach achieves superior performance on standard benchmarks and shows significant improvements on geometrically challenging classes, all without any 2D or language supervision.

</details>


### [25] [Semantic Belief-State World Model for 3D Human Motion Prediction](https://arxiv.org/abs/2601.03517)
*Sarim Chaudhry*

Main category: cs.CV

TL;DR: 提出一种语义信念状态世界模型（SBWM），将人体运动预测从直接关节回归转为在SMPL-X对齐的潜在状态上做动态仿真，以稳定长时预测并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接预测姿态，未区分重建与动力学建模，缺乏显式潜在因果表示，导致长时滚动时漂移、均值塌缩与不可靠不确定性。

Method: 构建与SMPL-X骨架参数对齐的递归概率信念状态；采用与世界模型/MBRL相似的随机潜在转移与rollout中心的训练；将姿态重建与潜在动力学学习解耦，通过结构信息瓶颈抑制静态几何与噪声进入潜变量，使其专注于运动意图与可控动态。对比RSSM、Transformer、Diffusion等以重建为导向的方法，SBWM优先优化稳定前向仿真。

Result: 实现连贯的长时序滚动预测，在准确度上具竞争力，同时显著降低计算成本；长时稳定性优于以重建为主的方法。

Conclusion: 把人体纳入世界模型的状态空间而非输出，使得运动的仿真与预测方式发生根本改变：以潜在动力学为核心可获得更稳定、更高效的长时预测。

Abstract: Human motion prediction has traditionally been framed as a sequence regression problem where models extrapolate future joint coordinates from observed pose histories. While effective over short horizons this approach does not separate observation reconstruction with dynamics modeling and offers no explicit representation of the latent causes governing motion. As a result, existing methods exhibit compounding drift, mean-pose collapse, and poorly calibrated uncertainty when rolled forward beyond the training regime. Here we propose a Semantic Belief-State World Model (SBWM) that reframes human motion prediction as latent dynamical simulation on the human body manifold. Rather than predicting poses directly, SBWM maintains a recurrent probabilistic belief state whose evolution is learned independently of pose reconstruction and explicitly aligned with the SMPL-X anatomical parameterization. This alignment imposes a structural information bottleneck that prevents the latent state from encoding static geometry or sensor noise, forcing it to capture motion dynamics, intent, and control-relevant structure. Inspired by belief-state world models developed for model-based reinforcement learning, SBWM adapts stochastic latent transitions and rollout-centric training to the domain of human motion. In contrast to RSSM-based, transformer, and diffusion approaches optimized for reconstruction fidelity, SBWM prioritizes stable forward simulation. We demonstrate coherent long-horizon rollouts, and competitive accuracy at substantially lower computational cost. These results suggest that treating the human body as part of the world models state space rather than its output fundamentally changes how motion is simulated, and predicted.

</details>


### [26] [Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution](https://arxiv.org/abs/2601.03526)
*Zhicheng Zhao,Fengjiao Peng,Jinquan Yan,Wei Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出PCNet，通过交叉分辨率互增强与物理约束的光学引导，实现热红外无人机图像超分辨，同时抑制跨模态伪影；在VGTSR2.0与DroneVehicle上显著优于SOTA，并提升分割与检测等下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有热红外SR多依赖光学引导，但通常将光学特征压缩到热红外维度进行对齐，丢失有利的高频细节，并因忽视成像物理差异而产生纹理失真、边缘模糊等伪影。需要一种既能保留光学高频先验，又能遵循热传导物理以避免不一致伪影的方法。

Method: 提出PCNet，包含：1) 交叉分辨率互增强模块（CRME），联合优化热红外SR与光到热模态转换，实现跨分辨率的双向特征交互并保留光学高频；2) 物理驱动热传导模块（PDTM），在光学引导中引入二维热传导建模，学习空间变异的传导属性以抑制不一致伪影；3) 温度一致性损失，约束区域分布一致与边界梯度平滑，使生成热图符合热辐射规律。

Result: 在VGTSR2.0与DroneVehicle上，PCNet在重建质量指标上显著超越SOTA，并在语义分割、目标检测等下游任务上带来明显提升。

Conclusion: 通过物理约束的光学引导与跨分辨率互增强，PCNet能有效提升热红外无人机图像SR质量，减少跨模态伪影，并对下游感知任务有实际促进作用。

Abstract: Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.

</details>


### [27] [CloudMatch: Weak-to-Strong Consistency Learning for Semi-Supervised Cloud Detection](https://arxiv.org/abs/2601.03528)
*Jiayi Zhao,Changlu Chen,Jingsheng Li,Tianxiang Xue,Kun Zhan*

Main category: cs.CV

TL;DR: CloudMatch提出一种半监督云检测框架，通过视图一致性学习与跨景/同景混合增强，利用未标注遥感图像，提升伪标签质量与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 像素级精确标注代价高，限制了云检测模型对海量遥感数据的利用；需要一种能有效用未标注数据、同时适应不同场景中云形态结构多样性与上下文变化的方法。

Method: 为每个未标注样本生成1个弱增强视图与2个强增强视图：强增强A进行跨场景（inter-scene）补丁混合以模拟上下文多样性；强增强B进行同一场景内（intra-scene）混合以保持语义一致性。通过多视图一致性约束与这些混合策略引导伪标签生成与训练，促进模型学习结构与上下文多样性。

Result: 在大量实验中，CloudMatch在半监督云检测任务上取得优异表现，表明其能高效利用未标注数据并提升泛化。

Conclusion: 结合跨景与同景混合的视图一致性学习，可有效捕获云模式的结构与上下文多样性，提升半监督云检测性能。

Abstract: Due to the high cost of annotating accurate pixel-level labels, semi-supervised learning has emerged as a promising approach for cloud detection. In this paper, we propose CloudMatch, a semi-supervised framework that effectively leverages unlabeled remote sensing imagery through view-consistency learning combined with scene-mixing augmentations. An observation behind CloudMatch is that cloud patterns exhibit structural diversity and contextual variability across different scenes and within the same scene category. Our key insight is that enforcing prediction consistency across diversely augmented views, incorporating both inter-scene and intra-scene mixing, enables the model to capture the structural diversity and contextual richness of cloud patterns. Specifically, CloudMatch generates one weakly augmented view along with two complementary strongly augmented views for each unlabeled image: one integrates inter-scene patches to simulate contextual variety, while the other employs intra-scene mixing to preserve semantic coherence. This approach guides pseudolabel generation and enhances generalization. Extensive experiments show that CloudMatch achieves good performance, demonstrating its capability to utilize unlabeled data efficiently and advance semi-supervised cloud detection.

</details>


### [28] [EASLT: Emotion-Aware Sign Language Translation](https://arxiv.org/abs/2601.03549)
*Guobin Tu,Di Weng*

Main category: cs.CV

TL;DR: EASLT 将面部情绪作为语义锚点，通过情绪编码器与情绪感知融合模块提升无词素（gloss-free）手语翻译，解决相同手型不同含义的歧义，并在PHOENIX14T与CSL-Daily上取得领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有无词素SLT方法多聚焦手部动作，对面部表情等非手势信号重视不足，导致当不同概念共享相同手动姿态时出现语义歧义；需要一种能显式建模情绪/表情以增强语义判别力的方案。

Method: 提出EASLT框架：引入专门的情绪编码器，捕获连续的情感动态；通过情绪感知融合（EAF）模块，将情感表征与时空手语特征自适应融合与重标定，从而在解码阶段利用情感上下文消解语义歧义；整体为无词素端到端翻译系统。

Result: 在PHOENIX14T与CSL-Daily数据集上，作为无词素方法达到SOTA或先进水平：BLEU-4分别26.15与22.80，BLEURT分别61.0与57.8；消融实验显示显式情绪建模显著提升翻译保真度。

Conclusion: 将面部情绪由“辅助信息”提升为核心语义锚点并与手势特征深度耦合，可有效解耦“情感语义”与“手部动态”，缓解同形异义问题，显著提高SLT质量。

Abstract: Sign Language Translation (SLT) is a complex cross-modal task requiring the integration of Manual Signals (MS) and Non-Manual Signals (NMS). While recent gloss-free SLT methods have made strides in translating manual gestures, they frequently overlook the semantic criticality of facial expressions, resulting in ambiguity when distinct concepts share identical manual articulations. To address this, we present **EASLT** (**E**motion-**A**ware **S**ign **L**anguage **T**ranslation), a framework that treats facial affect not as auxiliary information, but as a robust semantic anchor. Unlike methods that relegate facial expressions to a secondary role, EASLT incorporates a dedicated emotional encoder to capture continuous affective dynamics. These representations are integrated via a novel *Emotion-Aware Fusion* (EAF) module, which adaptively recalibrates spatio-temporal sign features based on affective context to resolve semantic ambiguities. Extensive evaluations on the PHOENIX14T and CSL-Daily benchmarks demonstrate that EASLT establishes advanced performance among gloss-free methods, achieving BLEU-4 scores of 26.15 and 22.80, and BLEURT scores of 61.0 and 57.8, respectively. Ablation studies confirm that explicitly modeling emotion effectively decouples affective semantics from manual dynamics, significantly enhancing translation fidelity. Code is available at https://github.com/TuGuobin/EASLT.

</details>


### [29] [SpatiaLoc: Leveraging Multi-Level Spatial Enhanced Descriptors for Cross-Modal Localization](https://arxiv.org/abs/2601.03579)
*Tianyi Shang,Pengjie Xu,Zhaojun Deng,Zhenyu Li,Zhicong Chen,Lijun Wu*

Main category: cs.CV

TL;DR: SpatiaLoc提出面向“文本-点云”跨模态定位的粗到细框架，聚焦实例级与全局级空间关系建模，并用不确定性感知的高斯回归精修2D位置，在KITTI360Pose上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 跨模态定位场景中，文本与点云中的目标常反复出现，单靠语义或外观难区分，空间关系成为最具判别力的线索；现有方法对空间关系的实例级与全局级建模不足，且精定位阶段缺乏对预测不确定性的显式刻画。

Method: 采用粗到细两阶段：1) 粗阶段：a) BEOSE用二次贝塞尔曲线显式建模实例级目标间空间关系；b) FAE在频域提取全局空间表征以捕获宏观结构。2) 细阶段：UGFL将2D位置作为高斯分布回归，用不确定性感知损失进行精定位。

Result: 在KITTI360Pose数据集上进行广泛实验，SpatiaLoc在各项指标上显著超越现有SOTA方法。

Conclusion: 强调并分层建模空间关系（实例级与全局级），再结合不确定性感知的细化定位，可有效提升文本-点云跨模态定位精度，验证于KITTI360Pose上优于SOTA。

Abstract: Cross-modal localization using text and point clouds enables robots to localize themselves via natural language descriptions, with applications in autonomous navigation and interaction between humans and robots. In this task, objects often recur across text and point clouds, making spatial relationships the most discriminative cues for localization. Given this characteristic, we present SpatiaLoc, a framework utilizing a coarse-to-fine strategy that emphasizes spatial relationships at both the instance and global levels. In the coarse stage, we introduce a Bezier Enhanced Object Spatial Encoder (BEOSE) that models spatial relationships at the instance level using quadratic Bezier curves. Additionally, a Frequency Aware Encoder (FAE) generates spatial representations in the frequency domain at the global level. In the fine stage, an Uncertainty Aware Gaussian Fine Localizer (UGFL) regresses 2D positions by modeling predictions as Gaussian distributions with a loss function aware of uncertainty. Extensive experiments on KITTI360Pose demonstrate that SpatiaLoc significantly outperforms existing state-of-the-art (SOTA) methods.

</details>


### [30] [Detecting AI-Generated Images via Distributional Deviations from Real Images](https://arxiv.org/abs/2601.03586)
*Yakun Niu,Yingjian Chen,Lei Zhang*

Main category: cs.CV

TL;DR: 提出一种对冻结CLIP图像编码器进行“遮挡式微调”的方法MPFT，用纹理感知遮挡(TAM)引导模型关注生成图像相对真实图像的分布偏差，在少量样本下对未见生成模型有更强泛化，GenImage与UniversalFakeDetect上分别达98.2%与94.6%平均准确率。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型图像质量提升导致虚假信息与信任风险上升，检测生成图像尤其对未见模型的泛化成为难题。现有基于冻结CLIP的方法虽具泛化潜力，但仅把图像编码器当特征提取器，未充分发挥其能力，也无法真正区分真实与生成图像。

Method: 对冻结的CLIP-ViT进行分析，发现其高层特征能有效聚类真实图像但不能区分真伪。据此提出MPFT：在微调时引入纹理感知遮挡TAM，优先遮挡包含生成模型特定纹理模式的区域，迫使模型关注与真实分布的偏离（而非仅依赖纹理线索），以提升未见模型上的检测泛化。

Result: 在GenImage与UniversalFakeDetect数据集上，使用极少量微调样本即可显著优于现有方法，平均准确率分别达98.2%和94.6%。

Conclusion: 通过纹理感知遮挡引导的遮挡式微调，能让CLIP-ViT从“抽象聚类”转向“真伪判别”，在少样本、跨模型场景下实现更强泛化与更高准确率。

Abstract: The rapid advancement of generative models has significantly enhanced the quality of AI-generated images, raising concerns about misinformation and the erosion of public trust. Detecting AI-generated images has thus become a critical challenge, particularly in terms of generalizing to unseen generative models. Existing methods using frozen pre-trained CLIP models show promise in generalization but treat the image encoder as a basic feature extractor, failing to fully exploit its potential. In this paper, we perform an in-depth analysis of the frozen CLIP image encoder (CLIP-ViT), revealing that it effectively clusters real images in a high-level, abstract feature space. However, it does not truly possess the ability to distinguish between real and AI-generated images. Based on this analysis, we propose a Masking-based Pre-trained model Fine-Tuning (MPFT) strategy, which introduces a Texture-Aware Masking (TAM) mechanism to mask textured areas containing generative model-specific patterns during fine-tuning. This approach compels CLIP-ViT to attend to the "distributional deviations"from authentic images for AI-generated image detection, thereby achieving enhanced generalization performance. Extensive experiments on the GenImage and UniversalFakeDetect datasets demonstrate that our method, fine-tuned with only a minimal number of images, significantly outperforms existing approaches, achieving up to 98.2% and 94.6% average accuracy on the two datasets, respectively.

</details>


### [31] [Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions](https://arxiv.org/abs/2601.03590)
*Zhongbin Guo,Zhen Yang,Yushan Li,Xinyue Zhang,Wenyu Gao,Jiacheng Wang,Chengzhi Li,Xiangrui Liu,Ping Jian*

Main category: cs.CV

TL;DR: SiT-Bench提出以纯文本坐标化描述替代像素输入，系统评估LLM的空间智能，发现局部语义还行但全局一致性差；显式空间推理可显著提升表现，指向LLM潜在世界建模能力。


<details>
  <summary>Details</summary>
Motivation: 弄清空间理解究竟源于视觉编码器还是语言模型的推理主体，并提供一个不依赖像素输入、可隔离评估LLM空间推理能力的基准，以促进对VLM与具身智能中“空间缺口”的系统研究。

Method: 构建SiT-Bench：从单/多视角场景生成高保真、含坐标的文本描述，覆盖5大类17个子任务（如自我中心导航、视角变换、精细操控等），通过纯符号化文本推理考察LLM。对多种SOTA LLM进行评测，并对比是否加入显式空间推理步骤对性能的影响。

Result: SOTA LLM在局部语义相关任务上表现尚可，但在需要全局一致性的空间推理上存在明显“空间鸿沟”。引入显式空间推理策略后性能显著提升，显示LLM具备潜在的世界建模与空间推理能力。

Conclusion: SiT-Bench为评测与推动具有空间扎根能力的LLM与未来VLM/具身智能提供基础资源；要缩小空间差距，需在LLM骨干中注入或诱导显式空间推理机制。

Abstract: Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant "spatial gap" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .

</details>


### [32] [Adaptive Attention Distillation for Robust Few-Shot Segmentation under Environmental Perturbations](https://arxiv.org/abs/2601.03596)
*Qianyu Guo,Jingrong Wu,Jieji Ren,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 提出环境鲁棒的少样本分割设定与基准（ER-FSS），并引入自适应注意力蒸馏（AAD）以在复杂环境中提升对新类目标的聚焦能力， across 8 数据集平均提升 mIoU 3.3%–8.5%。


<details>
  <summary>Details</summary>
Motivation: 现实应用中测试图像受光照、背景、视角、运动模糊、小目标、伪装等干扰，实验室训练的FSS模型在真实复杂环境下鲁棒性不足，亟需面向环境干扰的评测与方法。

Method: 1) 定义环境鲁棒FSS设定，显式纳入复杂环境产生的困难样例；2) 构建覆盖8个真实场景数据集的ER-FSS基准；3) 提出自适应注意力蒸馏（AAD）：在支持/查询之间反复对比与蒸馏共享关键语义，生成面向新类别的类特异注意力，引导模型在复杂环境中聚焦正确目标。

Result: 在所有数据集与设定上，AAD带来3.3%–8.5%的mIoU增益，表现与泛化性均优于对比方法。

Conclusion: 将环境因素纳入FSS评测并通过AAD强化类特异注意力，可显著提升少样本分割在真实复杂场景下的鲁棒性与泛化能力；提供基准与代码数据以促进后续研究。

Abstract: Few-shot segmentation (FSS) aims to rapidly learn novel class concepts from limited examples to segment specific targets in unseen images, and has been widely applied in areas such as medical diagnosis and industrial inspection. However, existing studies largely overlook the complex environmental factors encountered in real world scenarios-such as illumination, background, and camera viewpoint-which can substantially increase the difficulty of test images. As a result, models trained under laboratory conditions often fall short of practical deployment requirements. To bridge this gap, in this paper, an environment-robust FSS setting is introduced that explicitly incorporates challenging test cases arising from complex environments-such as motion blur, small objects, and camouflaged targets-to enhance model's robustness under realistic, dynamic conditions. An environment robust FSS benchmark (ER-FSS) is established, covering eight datasets across multiple real world scenarios. In addition, an Adaptive Attention Distillation (AAD) method is proposed, which repeatedly contrasts and distills key shared semantics between known (support) and unknown (query) images to derive class-specific attention for novel categories. This strengthens the model's ability to focus on the correct targets in complex environments, thereby improving environmental robustness. Comparative experiments show that AAD improves mIoU by 3.3% - 8.5% across all datasets and settings, demonstrating superior performance and strong generalization. The source code and dataset are available at: https://github.com/guoqianyu-alberta/Adaptive-Attention-Distillation-for-FSS.

</details>


### [33] [Unveiling Text in Challenging Stone Inscriptions: A Character-Context-Aware Patching Strategy for Binarization](https://arxiv.org/abs/2601.03609)
*Pratyush Jena,Amal Joseph,Arnav Sharma,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: 提出一种结合自适应打补丁与注意力U-Net的碑刻二值化方法，并发布精标注数据集；在多种基线之上显著提升，且对跨脚本零样本泛化良好。


<details>
  <summary>Details</summary>
Motivation: 传统二值化在石刻图像上易失败，因字符与背景对比度低、表面退化不均、伪影多、布局与密度变化大。现有方法虽用图像分块提升局部解析度，但仍难稳定提取连贯字符区域。

Method: 设计“鲁棒自适应打补丁”策略：动态采样与补丁选择以覆盖含文本与噪声的关键区域；以这些补丁训练注意力U‑Net，注意力模块聚焦细微结构线索，从而抑制表面噪声与布局不规则影响；同时构建像素级、字符碎片级标注的印度文字刻石数据集。

Result: 新型打补丁机制在经典与深度基线之上显著提升二值化表现；仅在单一印度脚本上训练的模型，对其他印度及非印度脚本实现强零样本泛化。

Conclusion: 方法能产出干净、结构化的文本掩膜，为脚本识别、OCR与历史文本分析等下游任务奠定基础，并显示出脚本无关的鲁棒性与泛化能力。

Abstract: Binarization is a popular first step towards text extraction in historical artifacts. Stone inscription images pose severe challenges for binarization due to poor contrast between etched characters and the stone background, non-uniform surface degradation, distracting artifacts, and highly variable text density and layouts. These conditions frequently cause existing binarization techniques to fail and struggle to isolate coherent character regions. Many approaches sub-divide the image into patches to improve text fragment resolution and improve binarization performance. With this in mind, we present a robust and adaptive patching strategy to binarize challenging Indic inscriptions. The patches from our approach are used to train an Attention U-Net for binarization. The attention mechanism allows the model to focus on subtle structural cues, while our dynamic sampling and patch selection method ensures that the model learns to overcome surface noise and layout irregularities. We also introduce a carefully annotated, pixel-precise dataset of Indic stone inscriptions at the character-fragment level. We demonstrate that our novel patching mechanism significantly boosts binarization performance across classical and deep learning baselines. Despite training only on single script Indic dataset, our model exhibits strong zero-shot generalization to other Indic and non-indic scripts, highlighting its robustness and script-agnostic generalization capabilities. By producing clean, structured representations of inscription content, our method lays the foundation for downstream tasks such as script identification, OCR, and historical text analysis. Project page: https://ihdia.iiit.ac.in/shilalekhya-binarization/

</details>


### [34] [Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection](https://arxiv.org/abs/2601.03617)
*Samson Oseiwe Ajadalu*

Main category: cs.CV

TL;DR: 系统评估单目Pseudo-LiDAR流水线：在相同生成与PointRCNN下，NeWCRFs优于Depth Anything V2 Metric-Outdoor；外观/语义特征带来边际增益，语义掩码采样可能伤几何；深度-距离诊断显示粗深度正确性不足以预测严格3D IoU；总体上，深度骨干与几何保真度主导性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测成本低但精度受限，关键瓶颈在单幅图像的度量深度估计。作者希望厘清：在标准Pseudo-LiDAR+LiDAR检测器框架中，深度骨干差异与特征工程（外观、语义）到底对下游3D检测影响多大。

Method: 在KITTI验证集上，统一Pseudo-LiDAR生成与PointRCNN检测协议，比较两种度量深度骨干：NeWCRFs与Depth Anything V2 Metric-Outdoor（Base）。同时在点云中注入外观（灰度强度）与语义（实例分割置信度）特征，并测试基于掩码的采样策略。最后用基于真值2D框的深度准确度-距离诊断（含行人/骑行者）分析深度误差与3D IoU关系。

Result: NeWCRFs在下游3D检测上更强：在IoU=0.7、Moderate上AP3D=10.50%（使用灰度强度）。外观与语义特征仅带来轻微提升，语义掩码采样可能因移除上下文几何而降性能。诊断表明：即使深度粗略正确，也难以保证严格的3D IoU满足。

Conclusion: 在脱胎自LiDAR的检测器下，选择更优的深度骨干与保持几何保真度是性能关键，二级特征注入（外观/语义）影响较小；改进单目3D检测应优先提升度量深度质量与几何一致性，而非依赖语义增强。

Abstract: Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.

</details>


### [35] [Shape Classification using Approximately Convex Segment Features](https://arxiv.org/abs/2601.03625)
*Bimal Kumar Ray*

Main category: cs.CV

TL;DR: 通过将对象边界分割为近似凸段并按长度降序排序，构建“特征袋”（段长、极值点数、面积、底与宽），无需对齐即可度量边界相似性，实现对象分类并在数据集上获得可接受结果。


<details>
  <summary>Details</summary>
Motivation: 传统基于描述性特征的对象分类常需对象对齐来进行相似性计算，但对齐敏感、耗时且在形变、旋转、部分遮挡下不稳健。作者希望用一种对齐无关的方法提升鲁棒性与效率。

Method: 1) 对图像对象的边界进行归一化处理；2) 将边界分割为近似凸的边界段；3) 将这些段按长度降序排序；4) 为每段提取特征：段长度、段内极值点数量、段面积、段的“底”和“宽”；5) 将所有段特征组成无序但可排序的“特征袋”；6) 基于此特征集合计算边界间相似度并进行分类。

Result: 在所用数据集上，该对齐无关的排序—特征袋方法取得“可接受”的分类效果（摘要未给出具体数值或与基线的比较）。

Conclusion: 通过对边界段进行排序并使用多维段特征构成特征袋，可在不进行对象对齐的情况下实现对象分类，表现有效但仍需更详尽的定量评估与对比以验证其优势。

Abstract: The existing object classification techniques based on descriptive features rely on object alignment to compute the similarity of objects for classification. This paper replaces the necessity of object alignment through sorting of feature. The object boundary is normalized and segmented into approximately convex segments and the segments are then sorted in descending order of their length. The segment length, number of extreme points in segments, area of segments, the base and the width of the segments - a bag of features - is used to measure the similarity between image boundaries. The proposed method is tested on datasets and acceptable results are observed.

</details>


### [36] [MFC-RFNet: A Multi-scale Guided Rectified Flow Network for Radar Sequence Prediction](https://arxiv.org/abs/2601.03633)
*Wenjie Luo,Chuanhu Deng,Chaorong Li,Rongyao Deng,Qiang Yang*

Main category: cs.CV

TL;DR: 提出MFC-RFNet，用多尺度特征通信+纠正位移+频域细节保真，并用Rectified Flow与轻量RWKV捕获长程依赖，实现雷达回波多数据集上更清晰、更持久的降水临近预报。


<details>
  <summary>Details</summary>
Motivation: 雷达回波临近预报仍受多尺度演化复杂、帧间位移导致特征错位、以及在不损失空间细节下高效建模长程时空依赖等难题制约。

Method: 构建生成式MFC-RFNet：1) 频域-引导多尺度融合：WGSC保留高频细节，FCM实现跨尺度双向通信；2) 位移校正：CGSTF由条件回波学习空间变换，对浅层特征对齐；3) 采样与训练：采用Rectified Flow训练以近线性概率流，实现少步采样且保持稳定保真；4) 长程依赖：在编码尾、瓶颈与首个解码层插入轻量Vision-RWKV块，于低分辨率处捕获远距时空上下文、控制计算量。

Result: 在SEVIR、MeteoNet、Shanghai、CIKM四个公开数据集上，相比强基线持续提升，尤其在高雨强阈值下产生更清晰的回波形态，并在更长预报时效上保持技能。

Conclusion: 将Rectified Flow与尺度感知通信、空间对齐、频率感知融合协同，可有效且稳健地提升基于雷达的降水临近预报质量。

Abstract: Accurate and high-resolution precipitation nowcasting from radar echo sequences is crucial for disaster mitigation and economic planning, yet it remains a significant challenge. Key difficulties include modeling complex multi-scale evolution, correcting inter-frame feature misalignment caused by displacement, and efficiently capturing long-range spatiotemporal context without sacrificing spatial fidelity. To address these issues, we present the Multi-scale Feature Communication Rectified Flow (RF) Network (MFC-RFNet), a generative framework that integrates multi-scale communication with guided feature fusion. To enhance multi-scale fusion while retaining fine detail, a Wavelet-Guided Skip Connection (WGSC) preserves high-frequency components, and a Feature Communication Module (FCM) promotes bidirectional cross-scale interaction. To correct inter-frame displacement, a Condition-Guided Spatial Transform Fusion (CGSTF) learns spatial transforms from conditioning echoes to align shallow features. The backbone adopts rectified flow training to learn near-linear probability-flow trajectories, enabling few-step sampling with stable fidelity. Additionally, lightweight Vision-RWKV (RWKV) blocks are placed at the encoder tail, the bottleneck, and the first decoder layer to capture long-range spatiotemporal dependencies at low spatial resolutions with moderate compute. Evaluations on four public datasets (SEVIR, MeteoNet, Shanghai, and CIKM) demonstrate consistent improvements over strong baselines, yielding clearer echo morphology at higher rain-rate thresholds and sustained skill at longer lead times. These results suggest that the proposed synergy of RF training with scale-aware communication, spatial alignment, and frequency-aware fusion presents an effective and robust approach for radar-based nowcasting.

</details>


### [37] [CrackSegFlow: Controllable Flow-Matching Synthesis for Generalizable Crack Segmentation with the CSF-50K Benchmark](https://arxiv.org/abs/2601.03637)
*Babak Asadi,Peiyang Wu,Mani Golparvar-Fard,Ramez Hajj*

Main category: cs.CV

TL;DR: CrackSegFlow 提出一种可控的 flow-matching 合成框架，从二值裂缝掩膜出发生成与掩膜严格对齐的高真实感裂缝图像，并公开了50K成对数据集以提升裂缝分割的泛化与评测。


<details>
  <summary>Details</summary>
Motivation: 裂缝分割实用部署受两大瓶颈制约：像素级标注稀缺，以及跨传感器/光照/纹理/标注风格的严重域偏移。需要能在不依赖大量人工标注的情况下，生成高保真、可控且与掩膜精确对齐的合成数据，以提升薄结构（裂缝）识别与跨域泛化。

Method: 1) 提出 CrackSegFlow：基于可控 flow-matching 的图像生成器，以二值裂缝掩膜为条件输入；通过拓扑保持的掩膜注入与边界门控调制，兼顾细长结构连贯性并抑制由材质纹理诱发的假阳性。2) 第二个类别条件的 flow-matching 模型用于直接合成裂缝掩膜，并可显式控制裂缝覆盖率，从而得到拓扑多样且类别均衡的掩膜—图像成对数据。3) 将裂缝掩膜注入无裂背景，扩充光照与表面伪影的多样性，降低阴影、接缝、路标等导致的误检。4) 采用固定的分割骨干（混合 CNN-Transformer）与统一训练协议在多数据集上评测。

Result: 在4个沥青数据集与1个混凝土裂缝类的5个基准上，结合真实与合成对的数据，域内平均提升：+5.37 mIoU、+5.13 F1；在仅使用目标域有限掩膜统计的目标引导跨域合成下，平均提升：+13.12 mIoU、+14.82 F1。相较扩散式语义合成，CrackSegFlow 具备更快的确定性采样，并在细薄结构的几何保真与掩膜—图像对齐上更优。另发布CSF-50K：5万对像素精确掩膜与图像。

Conclusion: 可控 flow-matching 合成能在低标注和强域偏移场景下显著提升裂缝分割的精度与跨域泛化，优于扩散式方案；发布的大规模成对数据集为后续基准化研究提供支撑。

Abstract: Automated crack segmentation is essential for scalable condition assessment of pavements and civil infrastructure, yet practical deployment is limited by scarce pixel-level labels and severe domain shift across sensors, illumination, textures, and annotation conventions. This paper presents CrackSegFlow, a controllable flow-matching synthesis framework that generates photorealistic crack images conditioned on binary masks while preserving strict mask-image alignment. The generator combines topology-preserving mask injection with boundary-gated modulation to maintain thin-structure continuity and suppress texture-driven false positives. A second class-conditional flow-matching model synthesizes crack masks with explicit control over crack coverage, enabling balanced, topology-diverse paired data without additional manual annotation. We further inject crack masks into crack-free backgrounds to diversify illumination and surface artifacts and reduce false positives caused by shadows, joints, and pavement markings. Experiments on five benchmarks spanning four asphalt datasets and the crack class of a concrete-domain dataset demonstrate consistent improvements under an established hybrid CNN--Transformer segmentation backbone and a fixed training protocol. With real plus synthesized pairs, in-domain performance improves on average by 5.37 mIoU and 5.13 F1, and target-guided cross-domain synthesis yields average gains of 13.12 mIoU and 14.82 F1 using only limited target mask statistics. Compared with diffusion-based semantic synthesis, CrackSegFlow provides substantially faster deterministic sampling and improves fidelity and mask-image alignment for thin-structure crack geometry. Finally, we release CSF-50K, a public dataset of 50,000 paired crack images and pixel-accurate masks for large-scale benchmarking of generalizable crack segmentation.

</details>


### [38] [VideoMemory: Toward Consistent Video Generation via Memory Integration](https://arxiv.org/abs/2601.03655)
*Jinsong Zhou,Yihua Du,Xinli Xu,Luozhou Wang,Zijie Zhuang,Yehang Zhang,Shuaibo Li,Xiaojun Hu,Bolan Su,Ying-cong Chen*

Main category: cs.CV

TL;DR: 提出VideoMemory：通过动态记忆库与多智能体叙事规划，实现跨镜头角色/道具/背景一致的长视频生成，显著提升一致性与画质，并发布54例多镜头一致性基准。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型能做短片，但在场景切换或长时间间隔后难以保持实体身份与外观一致，缺乏面向叙事的跨镜头一致性机制与评测基准。

Method: 构建以实体为中心的框架VideoMemory：给定结构化剧本，由多智能体将叙事分解为分镜；通过动态记忆库检索角色/道具/背景的语义与视觉描述，条件化关键帧与视频生成；每个镜头后用故事驱动的变化更新记忆以保持身份与状态演化。

Result: 在自建包含角色、道具、背景持久性的54案多镜头一致性基准上，VideoMemory在实体级一致性与感知质量上显著优于现有方法，能在远距镜头间保持连贯。

Conclusion: 融合叙事规划与动态记忆检索-更新的框架有效解决长篇叙事视频的跨镜头一致性问题，并提供实证与基准支持其有效性。

Abstract: Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.

</details>


### [39] [MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding](https://arxiv.org/abs/2601.03660)
*Jiangyuan Liu,Hongxuan Ma,Yuhao Zhao,Zhe Liu,Jian Wang,Wei Zou*

Main category: cs.CV

TL;DR: MGPC提出一个融合点云、RGB图像与文本的通用点云补全框架，通过多模态融合与渐进式生成，在真实场景和大规模数据上显著提升泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有点云补全方法多依赖单一模态（点或体素）且受网络容量与生成能力限制，虽在合成数据上表现良好，但对新类别与真实复杂场景泛化差，亟需能融合多模态信息并具备强泛化与稳健性的统一框架与数据基座。

Method: 提出MGPC框架：1）多模态输入（点云、RGB、文本）统一编码；2）Transformer式跨模态融合模块提升语义与几何对齐；3）创新的“模态dropout”策略，随机屏蔽部分模态以提升鲁棒与可扩展性；4）渐进式生成器，逐步细化几何以增强形状建模与细节恢复；5）自动数据生成流水线，构建包含1000+类别、百万对训练样本的大规模基准MGPC-1M。

Result: 在MGPC-1M与真实“in-the-wild”数据上，MGPC一致优于现有CNN/Point/Transformer基线，并在跨类别与真实场景下展现强泛化能力。

Conclusion: 多模态融合结合模态dropout与渐进式生成，可显著提升点云补全在真实世界中的泛化与鲁棒性；大规模MGPC-1M数据集为该方向提供了坚实训练与评测基座。

Abstract: Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and Transformer-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable multimodal point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a Transformer-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.

</details>


### [40] [PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance](https://arxiv.org/abs/2601.03665)
*Siddarth Nilol Kundur Satish,Devesh Jaiswal,Hongyu Chen,Abhishek Bakshi*

Main category: cs.CV

TL;DR: 提出PhysVideoGenerator，将可学习物理先验注入扩散式视频生成，通过从噪声潜变量预测物理表征并注入时间注意力，实现更稳定、物理一致的视频生成的可行性验证。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成虽美观但缺乏真实物理一致性，表现为碰撞不自然、重力不一致、时间闪烁等，需要将物理动态知识显式融入生成过程。

Method: 在DiT-based视频生成器（Latte）中加入轻量预测器PredictorP，从扩散噪声潜变量直接回归预训练V-JEPA 2的高层物理特征（physics tokens），并通过专门的跨注意力将这些物理tokens注入生成器的时间注意力层；联合训练以保持扩散重建与物理特征回归的多任务优化稳定。

Result: 证明两点可行性：1）扩散潜变量包含足够信息可恢复V-JEPA 2的物理表征；2）在联合训练下优化过程稳定。给出架构细节、技术挑战与稳定性验证。

Conclusion: 该工作为将物理先验显式嵌入视频扩散生成奠定技术基础，验证了方法可行与训练稳定性，为后续大规模物理一致性评测与更强物理感知生成模型铺路。

Abstract: Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering. In this work, we propose PhysVideoGenerator, a proof-of-concept framework that explicitly embeds a learnable physics prior into the video generation process. We introduce a lightweight predictor network, PredictorP, which regresses high-level physical features extracted from a pre-trained Video Joint Embedding Predictive Architecture (V-JEPA 2) directly from noisy diffusion latents. These predicted physics tokens are injected into the temporal attention layers of a DiT-based generator (Latte) via a dedicated cross-attention mechanism. Our primary contribution is demonstrating the technical feasibility of this joint training paradigm: we show that diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and that multi-task optimization remains stable over training. This report documents the architectural design, technical challenges, and validation of training stability, establishing a foundation for future large-scale evaluation of physics-aware generative models.

</details>


### [41] [TRec: Egocentric Action Recognition using 2D Point Tracks](https://arxiv.org/abs/2601.03667)
*Dennis Holzmann,Sven Wachsmuth*

Main category: cs.CV

TL;DR: 提出一种利用2D点轨迹作为额外运动线索的第一人称动作识别方法：用CoTracker跟踪随机采样的图像点，将轨迹与帧一起送入Transformer，显著提升精度，甚至仅用首帧与其点轨迹也有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖RGB外观、人体姿态或二者结合，忽视更轻量的运动表征；希望在不显式检测手、物体或交互区域的情况下，获得稳健的运动线索，提升第一人称动作识别。

Method: 在每段视频中随机初始化若干图像点，使用CoTracker跨帧跟踪，得到2D轨迹；将轨迹与对应图像帧一并输入基于Transformer的识别模型进行动作分类；还探究仅使用首帧及其点轨迹的情形。

Result: 引入2D点轨迹能在相同模型基础上稳定提升识别性能；即便不使用整段视频，只用初始帧与其点轨迹也取得显著增益。

Conclusion: 2D点轨迹是轻量且有效的运动表示，可在无需检测手/物体/交互区域的前提下，持续增强第一人称动作理解的准确率。

Abstract: We present a novel approach for egocentric action recognition that leverages 2D point tracks as an additional motion cue. While most existing methods rely on RGB appearance, human pose estimation, or their combination, our work demonstrates that tracking randomly sampled image points across video frames can substantially improve recognition accuracy. Unlike prior approaches, we do not detect hands, objects, or interaction regions. Instead, we employ CoTracker to follow a set of randomly initialized points through each video and use the resulting trajectories, together with the corresponding image frames, as input to a Transformer-based recognition model. Surprisingly, our method achieves notable gains even when only the initial frame and its associated point tracks are provided, without incorporating the full video sequence. Experimental results confirm that integrating 2D point tracks consistently enhances performance compared to the same model trained without motion information, highlighting their potential as a lightweight yet effective representation for egocentric action understanding.

</details>


### [42] [BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion](https://arxiv.org/abs/2601.03713)
*Qingyao Tian,Bingyu Yang,Huai Liao,Xinyan Huang,Junyong Li,Dong Yi,Hongbin Liu*

Main category: cs.CV

TL;DR: 提出BREATH数据集与BREATH-VL框架，将VLM语义与几何配准融合，实现气道内镜6-DoF定位；并用“语言化运动历史”的轻量上下文机制高效建模时序，在准确性与延迟间取得平衡，较最佳基线平移误差降25.5%。


<details>
  <summary>Details</summary>
Motivation: VLM在导航定位具备强语义泛化，但医疗场景缺乏大规模定位标注数据；VLM对精细位姿回归与高效时序建模能力不足，尤其视频级特征提取计算开销大。作者希望结合VLM语义鲁棒性与几何配准精确性，弥补各自短板。

Method: 1) 构建大规模真实体内气道定位数据集BREATH。2) 设计混合框架BREATH-VL：VLM提取语义线索用于粗定位/约束，视觉配准（几何）提供精确对齐以输出6-DoF位姿。3) 提出轻量“上下文学习”机制：将历史运动编码为语言提示（prompts），在不进行昂贵视频级计算的前提下提升时序推理能力。4) 在多场景内镜数据上进行广泛实验与对比。

Result: VLM模块在复杂手术场景中实现鲁棒的语义定位；整体框架在准确性与泛化上优于SOTA视觉-only方法，平移误差较最佳基线降低25.5%，同时保持有竞争力的推理延迟。

Conclusion: 通过将VLM语义与几何配准融合并以语言化历史作为轻量时序建模，BREATH-VL在内镜6-DoF定位上兼顾精度、泛化与效率；BREATH数据集为该方向提供了关键资源。

Abstract: Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM's ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.

</details>


### [43] [Towards Real-world Lens Active Alignment with Unlabeled Data via Domain Adaptation](https://arxiv.org/abs/2601.03718)
*Wenyong Lia,Qi Jiang,Weijian Hu,Kailun Yang,Zhanjun Zhang,Wenjun Tian,Kaiwei Wang,Jian Bai*

Main category: cs.CV

TL;DR: 提出DA3：在少量无标注真实数据辅助下，缩小光学主动准直仿真-真实域间差距，显著提升失准预测准确度与装配效率。


<details>
  <summary>Details</summary>
Motivation: 纯仿真数字孪生能批量产出标注数据，但复杂成像条件造成显著仿真-真实域差，导致仿真训练模型在真实相机上的泛化差。需要在不进行高成本、逐机标注的情况下，让模型获得真实域的鲁棒性。

Method: 在仿真基线之上引入少量随机失准位置采集的无标注真实图像，提出DA3：1）自回归域变换生成器，将仿真图像逐步变换以逼近真实域分布；2）对抗式特征对齐，在判别器约束下使特征在仿真与真实域上不可区分；3）自监督蒸馏，学习域不变的成像退化表征，从而提升失准参数预测；整体实现端到端联合训练。

Result: 在两类镜头上，较纯仿真训练的基线准确率提升46%；在仅使用3个镜头样本精准实测标注训练的性能附近徘徊，同时将设备端数据采集时间降低98.7%。

Conclusion: 域自适应可有效将仿真训练迁移到真实生产环境，使数字孪生管线在大规模高精光学装配中切实可用，显著减少现场标注与采集成本。

Abstract: Active Alignment (AA) is a key technology for the large-scale automated assembly of high-precision optical systems. Compared with labor-intensive per-model on-device calibration, a digital-twin pipeline built on optical simulation offers a substantial advantage in generating large-scale labeled data. However, complex imaging conditions induce a domain gap between simulation and real-world images, limiting the generalization of simulation-trained models. To address this, we propose augmenting a simulation baseline with minimal unlabeled real-world images captured at random misalignment positions, mitigating the gap from a domain adaptation perspective. We introduce Domain Adaptive Active Alignment (DA3), which utilizes an autoregressive domain transformation generator and an adversarial-based feature alignment strategy to distill real-world domain information via self-supervised learning. This enables the extraction of domain-invariant image degradation features to facilitate robust misalignment prediction. Experiments on two lens types reveal that DA3 improves accuracy by 46% over a purely simulation pipeline. Notably, it approaches the performance achieved with precisely labeled real-world data collected on 3 lens samples, while reducing on-device data collection time by 98.7%. The results demonstrate that domain adaptation effectively endows simulation-trained models with robust real-world performance, validating the digital-twin pipeline as a practical solution to significantly enhance the efficiency of large-scale optical assembly.

</details>


### [44] [CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval](https://arxiv.org/abs/2601.03728)
*Zhipeng Qian,Zihan Liang,Yufei Ma,Ben Chen,Huangyu Dai,Yiwei Ma,Jiayi Ji,Chenyi Lei,Han Li,Xiaoshuai Sun*

Main category: cs.CV

TL;DR: 提出CSMCIR：通过MCoT生成目标图像语义兼容描述、共享参数的对称双塔Q-Former编码、以及基于熵的动态记忆库负样本，缓解CIR表征空间碎片化并达SOTA与高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有CIR使用图像+文本查询检索目标图，但查询与目标由异构模态与不同编码器产生，导致表示空间不对齐、三簇分离、只能事后对齐，根本限制性能与效率。

Method: 1) 多层次CoT提示MMLM为目标图像生成判别性、与查询语义兼容的描述，建立模态对称；2) 对称双塔：查询侧与目标侧共享同一Q-Former进行跨模态编码，保证一致表征；3) 借助对称结构，引入基于熵的时变记忆库，维护与模型状态一致的高质量负样本以优化对比学习。

Result: 在四个基准数据集上取得SOTA检索性能，并表现出更高的训练效率；消融实验验证三组件各自与协同贡献。

Conclusion: 通过在数据层（MCoT文本）、结构层（共享Q-Former对称双塔）和训练策略层（动态熵记忆库）的协同设计，显著缩小查询与目标的对齐鸿沟，缓解表征空间碎片化，实现高效高性能CIR。

Abstract: Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.

</details>


### [45] [MATANet: A Multi-context Attention and Taxonomy-Aware Network for Fine-Grained Underwater Recognition of Marine Species](https://arxiv.org/abs/2601.03729)
*Donghwan Lee,Byeongjin Kim,Geunhee Kim,Hyukjin Kwon,Nahyeon Maeng,Wooju Kim*

Main category: cs.CV

TL;DR: 提出MATANet用于细粒度海洋物种分类，通过环境上下文注意与分类层级编码取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视水下目标与周围环境的上下文互动，且未充分利用生物分类学的层级结构，导致对易混淆物种的判别力不足。

Method: 构建MATANet，包括：1) 多上下文环境注意模块MCEAM，学习ROI与周围环境区域的关系，融合实例与环境特征；2) 分层分离诱导学习模块HSLM，将生物分类层级（如科-属-种）编码到特征空间，通过分层约束提高类间分离与层内一致性；最终联合两者进行细粒度分类。

Result: 在FathomNet2025、FAIR1M与LifeCLEF2015-Fish数据集上达到或刷新SOTA表现；代码开源。

Conclusion: 将环境上下文与分类层级显式建模能够显著提升水下细粒度识别，MATANet为生态监测与保育决策提供更可靠的自动识别能力。

Abstract: Fine-grained classification of marine animals supports ecology, biodiversity and habitat conservation, and evidence-based policy-making. However, existing methods often overlook contextual interactions from the surrounding environment and insufficiently incorporate the hierarchical structure of marine biological taxonomy. To address these challenges, we propose MATANet (Multi-context Attention and Taxonomy-Aware Network), a novel model designed for fine-grained marine species classification. MATANet mimics expert strategies by using taxonomy and environmental context to interpret ambiguous features of underwater animals. It consists of two key components: a Multi-Context Environmental Attention Module (MCEAM), which learns relationships between regions of interest (ROIs) and their surrounding environments, and a Hierarchical Separation-Induced Learning Module (HSLM), which encodes taxonomic hierarchy into the feature space. MATANet combines instance and environmental features with taxonomic structure to enhance fine-grained classification. Experiments on the FathomNet2025, FAIR1M, and LifeCLEF2015-Fish datasets demonstrate state-of-the-art performance. The source code is available at: https://github.com/dhlee-work/fathomnet-cvpr2025-ssl

</details>


### [46] [RadDiff: Describing Differences in Radiology Image Sets with Natural Language](https://arxiv.org/abs/2601.03733)
*Xiaoxian Shen,Yuhui Zhang,Sahithi Ankireddy,Xiaohan Wang,Maya Varma,Henry Guo,Curtis Langlotz,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: RadDiff 是一个面向放射影像成对比较的多模态智能体系统，结合医学知识注入、图像-报告联合推理、多轮假设迭代与目标化视觉搜索，实现“像放射科医师一样”的差异描述；并提出含专家标注的基准 RadDiffBench，实证优于通用基线。


<details>
  <summary>Details</summary>
Motivation: 临床实践与医学AI解释都需准确理解两份放射学研究（影像与报告）的差异，但现有通用对比系统缺乏医学知识、不能有效整合报告与图像、难以进行多轮诊断式推理与细粒度定位，导致难以捕捉临床有意义的差异。

Method: 在 VisDiff 的 proposer-ranker 框架上做四项增强：(1) 通过医学领域适配的视觉-语言模型进行知识注入；(2) 融合影像与对应临床报告的多模态联合推理；(3) 多轮迭代假设生成与精炼；(4) 目标化视觉搜索，对可疑区域进行定位与放大以捕捉微小征象。并构建包含57对专家验证研究与标准差异描述的基准 RadDiffBench。

Result: 在 RadDiffBench 上，RadDiff 达到47%准确率；当提供真实报告引导时达50%，显著优于通用领域的 VisDiff 基线。还在多种临床任务上展示通用性，包括 COVID-19 表型比较、种族亚组分析与与生存结局相关的影像特征发现。

Conclusion: RadDiff 与 RadDiffBench 共同为系统化揭示放射学数据中具有临床意义的差异提供了首个成体系的方法与基准，推进了医学影像比较与可解释AI的发展。

Abstract: Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.

</details>


### [47] [HyperCOD: The First Challenging Benchmark and Baseline for Hyperspectral Camouflaged Object Detection](https://arxiv.org/abs/2601.03736)
*Shuyan Bai,Tingfa Xu,Peifu Liu,Yuhao Qiu,Huiyan Bai,Huan Chen,Yanyan Peng,Jianan Li*

Main category: cs.CV

TL;DR: 提出HyperCOD超光谱伪装目标检测基准（350幅HSI）与适配SAM的HSC-SAM方法：将HSI解耦为空间图输入SAM与光谱显著性图作为自适应提示，显著提升HCOD并具备跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: RGB在伪装场景中色彩/纹理线索模糊，检测困难；HSI能提供细粒度光谱特征，但缺乏专门且大规模的HCOD基准，制约进展。基础模型SAM崛起，若能跨模态适配，可望解决HSI伪装检测难题。

Method: 构建HyperCOD：包含350张高分辨率HSI，涵盖极少目标、复杂形状、严重遮挡、动态光照等挑战。提出HSC-SAM：将HSI解耦为（1）空间映射送入SAM图像编码器；（2）光谱显著性图作为自适应prompt，引导SAM分割，从而弥合模态差异。

Result: 在HyperCOD上达成新的SOTA，并在其他公开HSI数据集上表现稳健、具有良好泛化性。

Conclusion: HyperCOD为HCOD提供首个系统性基准，HSC-SAM有效把HSI信息转译给SAM，实现强性能与泛化；数据集与基线将推动该领域后续研究。

Abstract: RGB-based camouflaged object detection struggles in real-world scenarios where color and texture cues are ambiguous. While hyperspectral image offers a powerful alternative by capturing fine-grained spectral signatures, progress in hyperspectral camouflaged object detection (HCOD) has been critically hampered by the absence of a dedicated, large-scale benchmark. To spur innovation, we introduce HyperCOD, the first challenging benchmark for HCOD. Comprising 350 high-resolution hyperspectral images, It features complex real-world scenarios with minimal objects, intricate shapes, severe occlusions, and dynamic lighting to challenge current models. The advent of foundation models like the Segment Anything Model (SAM) presents a compelling opportunity. To adapt the Segment Anything Model (SAM) for HCOD, we propose HyperSpectral Camouflage-aware SAM (HSC-SAM). HSC-SAM ingeniously reformulates the hyperspectral image by decoupling it into a spatial map fed to SAM's image encoder and a spectral saliency map that serves as an adaptive prompt. This translation effectively bridges the modality gap. Extensive experiments show that HSC-SAM sets a new state-of-the-art on HyperCOD and generalizes robustly to other public HSI datasets. The HyperCOD dataset and our HSC-SAM baseline provide a robust foundation to foster future research in this emerging area.

</details>


### [48] [I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing](https://arxiv.org/abs/2601.03741)
*Jinghan Yu,Junhao Xiao,Chenyu Zhu,Jiaming Li,Jia Li,HanMing Deng,Xirui Wang,Guoli Jia,Jianjun Li,Zhiyuan Ma,Xiang Bai,Bowen Zhou*

Main category: cs.CV

TL;DR: 提出I2E：将图像编辑从端到端像素修补转为“分解-行动”的结构化流程，用可操纵的对象层和具物理感知的VLA代理，通过链式推理执行原子操作，在复杂组合编辑、物理合理性与多轮稳定性上显著优于现有方法，并推出专门基准I2E-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像编辑在组合场景中表现不佳，根源在于：规划与执行耦合、缺乏对象级控制、以像素为中心的非结构化建模难以进行多对象空间推理与精确局部控制。

Method: 提出I2E框架：1) Decomposer将原图分解为离散、可操作的对象图层；2) 引入具物理建模意识的视觉-语言-行动代理(VLA Agent)，把复杂指令经Chain-of-Thought解析为一系列原子动作，并在结构化环境中按层操作；3) 构建I2E-Bench，用于评测多实例空间推理与高精度编辑。

Result: 在I2E-Bench及多项公开数据集上，I2E相较SOTA在处理复杂组合指令、保持物理合理性以及多轮编辑稳定性方面取得显著提升。

Conclusion: 将图像编辑重塑为“分解-行动”的结构化交互过程，可实现对象级精细控制与更强的空间推理能力；I2E验证了该范式的有效性并提供了针对性基准以推动该方向发展。

Abstract: Existing text-guided image editing methods primarily rely on end-to-end pixel-level inpainting paradigm. Despite its success in simple scenarios, this paradigm still significantly struggles with compositional editing tasks that require precise local control and complex multi-object spatial reasoning. This paradigm is severely limited by 1) the implicit coupling of planning and execution, 2) the lack of object-level control granularity, and 3) the reliance on unstructured, pixel-centric modeling. To address these limitations, we propose I2E, a novel "Decompose-then-Action" paradigm that revisits image editing as an actionable interaction process within a structured environment. I2E utilizes a Decomposer to transform unstructured images into discrete, manipulable object layers and then introduces a physics-aware Vision-Language-Action Agent to parse complex instructions into a series of atomic actions via Chain-of-Thought reasoning. Further, we also construct I2E-Bench, a benchmark designed for multi-instance spatial reasoning and high-precision editing. Experimental results on I2E-Bench and multiple public benchmarks demonstrate that I2E significantly outperforms state-of-the-art methods in handling complex compositional instructions, maintaining physical plausibility, and ensuring multi-turn editing stability.

</details>


### [49] [MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction](https://arxiv.org/abs/2601.03781)
*Xiaokun Sun,Zezhong Wu,Zewen Ding,Linli Xu*

Main category: cs.CV

TL;DR: 提出一种用于视频大模型后训练的新目标Masked Video Prediction（MVP），通过重建被遮挡的连续片段来强化时间顺序与因果理解，并结合可扩展数据合成管线与GRPO细粒度奖励，显著提升视频推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的VideoLLM后训练多优化描述或问答等视觉语义任务，主要提升整体内容理解，但缺乏对内在时间连贯性与帧间关联的显式监督，导致对复杂动态与细粒度视觉因果捕捉不足。

Method: 1) 设计MVP目标：给定视频与若干具有迷惑性的候选，要求模型重建被遮挡的连续片段，迫使关注事件的时序逻辑与上下文。2) 构建可扩展数据合成流水线：将任意视频语料转化为MVP训练样本。3) 采用Group Relative Policy Optimization（GRPO）与细粒度奖励函数，对模型的时序与上下文理解进行强化学习优化。

Result: 在综合评测中，采用MVP的模型在视频推理、时间推断与因果理解等方面取得显著提升，相比仅做描述/问答优化的后训练范式更强。

Conclusion: 显式引入MVP目标并配合可扩展数据与GRPO优化，直接强化时间与因果推理能力，为视频大模型后训练提供了有效且可扩展的范式。

Abstract: Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.

</details>


### [50] [A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products](https://arxiv.org/abs/2601.03784)
*Steven Moonen,Rob Salaets,Kenneth Batstone,Abdellatif Bey-Temsamani,Nick Michiels*

Main category: cs.CV

TL;DR: 论文探讨在缺乏CAD模型的农业场景中，如何用替代方式生成合成数据来训练目标检测（分拣石头与马铃薯），比较不同3D模型来源的效果，并显示小规模真实数据微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 制造业常以CAD驱动的合成数据降低真实数据采集与标注成本，但农业领域缺少可用CAD模型，导致合成数据难以利用；因此需要探索在无CAD条件下的替代数据生成方案及其对检测性能的影响。

Method: 提出并对比多种替代CAD的3D资产获取策略（3D扫描与图像到3D重建等），用这些模型合成训练集，训练用于分拣场景中“石头 vs 马铃薯”的目标检测器，并评估纯合成训练与在小规模真实数据上微调的效果。

Result: 高保真、代表性的3D模型（来自扫描或图像到3D）能生成有效的合成数据并训练出可用的检测器；在小规模真实集上进行微调可显著提升性能；即便模型代表性较低，微调后也能获得与高保真模型接近的性能。

Conclusion: 在缺少CAD的农业应用中，可以通过扫描或图像重建获取3D模型以替代CAD生成合成数据，配合少量真实数据微调即可达到良好甚至接近最佳的检测性能。

Abstract: In the manufacturing industry, computer vision systems based on artificial intelligence (AI) are widely used to reduce costs and increase production. Training these AI models requires a large amount of training data that is costly to acquire and annotate, especially in high-variance, low-volume manufacturing environments. A popular approach to reduce the need for real data is the use of synthetic data that is generated by leveraging computer-aided design (CAD) models available in the industry. However, in the agricultural industry these models are not readily available, increasing the difficulty in leveraging synthetic data. In this paper, we present different techniques for substituting CAD files to create synthetic datasets. We measure their relative performance when used to train an AI object detection model to separate stones and potatoes in a bin picking environment. We demonstrate that using highly representative 3D models acquired by scanning or using image-to-3D approaches can be used to generate synthetic data for training object detection models. Finetuning on a small real dataset can significantly improve the performance of the models and even get similar performance when less representative models are used.

</details>


### [51] [From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs](https://arxiv.org/abs/2601.03808)
*Usha Shrestha,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 提出一种在NNGPT生态中基于经验性能闭环的增广代码生成方法，用成对性能排序监督微调LLM，使其无需强化学习或穷举即可生成更优PyTorch数据增广函数，评测显示以最多600倍更少候选达到接近峰值准确率，并证实直接提示优于CoT。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码合成强，但“数据增广”的设计仍依赖启发式或暴力搜索，昂贵低效；需要一种能让模型从经验性能信号中学会面向任务的变换设计的方法。

Method: 构建包含6000+经下游准确率标注的PyTorch增广函数库；用LoRA微调LLM，以“成对排序(好-差)”作为监督信号进行对齐，无需RL、奖励模型或显式符号目标；在NNGPT闭环中让模型依据经验性能线索自主演化变换；对提示方式进行消融（CoT vs 直接提示）。

Result: 相较暴力搜索，所需评估候选数最多减少约600倍，同时保持有竞争力的峰值准确率，生成从随机合成转向任务对齐设计；定量与定性分析显示模型学习到语义层面的性能线索而非语法记忆；CoT引入语法噪声、降低性能，直接提示更稳。

Conclusion: LLM可通过非文本的经验反馈形成任务级推理能力，在无需显式符号奖励的情况下实现性能对齐，提升增广代码生成效率与效果。

Abstract: Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.

</details>


### [52] [EvalBlocks: A Modular Pipeline for Rapidly Evaluating Foundation Models in Medical Imaging](https://arxiv.org/abs/2601.03811)
*Jan Tagscherer,Sarah de Boer,Lena Philipp,Fennie van der Graaf,Dré Peeters,Joeran Bosma,Lars Leijten,Bogdan Obreja,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: EvalBlocks 是一个基于 Snakemake 的可插拔评测框架，用于在医学影像基础模型开发过程中高效、可复现实验评测与追踪，支持数据集/模型/聚合与评估策略的无缝扩展，并通过缓存与并行在共享算力上可扩展运行。


<details>
  <summary>Details</summary>
Motivation: 医学影像基础模型开发需要持续监控下游任务表现，但研究人员常被大量实验与设计选择的跟踪所拖累；现有手工化、临时拼接的流程慢且易出错，缺乏系统化、可复现、可扩展的评测管线。

Method: 提出 EvalBlocks：以 Snakemake 为工作流引擎的模块化框架。提供标准化接口以插入新的数据集、基础模型、结果聚合方法与评估策略；集中记录实验配置与结果；支持一键复现实验；通过高效缓存与并行执行适配共享计算环境，便于规模化评测。

Result: 在五个当前最先进的基础模型与三个医学影像分类任务上展示了框架的适用性与效率，能够加速评测迭代与实验管理。

Conclusion: EvalBlocks 将评测流程标准化与自动化，减轻研究者在评测与追踪上的负担，使其更专注于模型创新；作为开源项目公开发布，便于社区复用与扩展。

Abstract: Developing foundation models in medical imaging requires continuous monitoring of downstream performance. Researchers are burdened with tracking numerous experiments, design choices, and their effects on performance, often relying on ad-hoc, manual workflows that are inherently slow and error-prone. We introduce EvalBlocks, a modular, plug-and-play framework for efficient evaluation of foundation models during development. Built on Snakemake, EvalBlocks supports seamless integration of new datasets, foundation models, aggregation methods, and evaluation strategies. All experiments and results are tracked centrally and are reproducible with a single command, while efficient caching and parallel execution enable scalable use on shared compute infrastructure. Demonstrated on five state-of-the-art foundation models and three medical imaging classification tasks, EvalBlocks streamlines model evaluation, enabling researchers to iterate faster and focus on model innovation rather than evaluation logistics. The framework is released as open source software at https://github.com/DIAGNijmegen/eval-blocks.

</details>


### [53] [IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting](https://arxiv.org/abs/2601.03824)
*Wei Long,Haifeng Wu,Shiyin Jiang,Jinhua Zhang,Xinchun Ji,Shuhang Gu*

Main category: cs.CV

TL;DR: IDESplat 通过多次级联的“迭代 warp + 概率增强”来稳健估计深度，再反投影得到高精度高斯中心，实现泛化的 3D Gaussian Splatting，效果更好更轻更快。


<details>
  <summary>Details</summary>
Motivation: 现有直接预测高斯参数的通用 3DGS 方法，最难准确回归的是高斯中心（均值）。主流做法先估深度再反投影，但通常只用一次视角 warp 得到的深度概率，无法充分利用跨视角几何约束，导致深度不稳定、粗糙，影响重建质量与泛化。

Method: 提出 IDESplat：1) 设计深度概率增强单元（DPBU），将多次级联 warp 产生的极线注意力图以乘法方式融合，抑制噪声、强化一致候选；2) 堆叠多个 DPBU，形成迭代深度估计流程，逐步筛选并更新高置信深度候选；3) 由迭代优化后的深度图反投影获得精确的高斯均值；整体为前馈、可实时。

Result: 在 RealEstate10K、ACID、DL3DV 上实现 SOTA 重建质量与实时效率；在 RE10K 上较 DepthSplat 提升 PSNR 0.33 dB，参数量仅为其 10.7%、显存仅为其 70%；跨数据集在 DTU 上较 DepthSplat 提升 PSNR 2.95 dB，显示强泛化。

Conclusion: 通过迭代式多 warp 的深度概率增强，IDESplat 稳定细化深度并准确预测高斯中心，兼顾精度、效率与泛化，优于基线 DepthSplat。

Abstract: Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction. Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers. Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps. To address this limitation, we propose IDESplat, which iteratively applies warp operations to boost depth probability estimation for accurate Gaussian mean prediction. First, to eliminate the inherent instability of a single warp, we introduce a Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps produced by cascading warp operations in a multiplicative manner. Next, we construct an iterative depth estimation process by stacking multiple DPBUs, progressively identifying potential depth candidates with high likelihood. As IDESplat iteratively boosts depth probability estimates and updates the depth candidates, the depth map is gradually refined, resulting in accurate Gaussian means. We conduct experiments on RealEstate10K, ACID, and DL3DV. IDESplat achieves outstanding reconstruction quality and state-of-the-art performance with real-time efficiency. On RE10K, it outperforms DepthSplat by 0.33 dB in PSNR, using only 10.7% of the parameters and 70% of the memory. Additionally, our IDESplat improves PSNR by 2.95 dB over DepthSplat on the DTU dataset in cross-dataset experiments, demonstrating its strong generalization ability.

</details>


### [54] [Bayesian Monocular Depth Refinement via Neural Radiance Fields](https://arxiv.org/abs/2601.03869)
*Arun Muthukkumar*

Main category: cs.CV

TL;DR: 提出MDENeRF：将单目深度与NeRF深度及其不确定性通过贝叶斯融合，迭代细化深度，兼顾全局结构与高频细节；在SUN RGB-D室内数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计广泛应用，但现有方法深度图过于平滑，缺乏细粒度几何细节，影响精确场景理解。需要一种既保留全局结构又注入高频细节的机制。

Method: 构建MDENeRF三模块迭代框架：(1) 单目深度初始估计提供全局结构先验；(2) 基于扰动视点训练的NeRF，利用体渲染过程导出逐像素深度及不确定性；(3) 将噪声的单目深度与NeRF深度通过贝叶斯融合，按不确定性自适应权重，逐轮注入高频细节并保持全局一致性。

Result: 在SUN RGB-D室内场景上，关键评测指标上取得优于现有方法的性能，显示出更精细的几何细节与更可靠的深度恢复。

Conclusion: 通过利用NeRF的不确定性和贝叶斯融合，MDENeRF在保持全局结构的同时有效增强高频细节，实现更高质量的单目深度估计，证明了NeRF先验与单目先验互补的有效性。

Abstract: Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate superior performance on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.

</details>


### [55] [FLNet: Flood-Induced Agriculture Damage Assessment using Super Resolution of Satellite Images](https://arxiv.org/abs/2601.03884)
*Sanidhya Ghosal,Anurag Sharma,Sushil Ghildiyal,Mukesh Saini*

Main category: cs.CV

TL;DR: 提出FLNet：先用超分把Sentinel‑2从10 m提到≈3 m，再做作物受灾分类，在BFCD‑22上将关键“完全损毁”F1从0.83提升到0.89，接近商用高分影像（0.89），为低成本、可扩展的洪灾后农损评估提供路径。


<details>
  <summary>Details</summary>
Motivation: 手工灾损调查慢且主观；现有卫星方法易受云覆盖与分辨率低限制。印度洪涝频发、农作物受影响大，急需快速、客观、高精度的农损评估以指导政府救助与资源分配。

Method: 设计FLNet端到端管线：先对Sentinel‑2多光谱影像进行深度学习超分辨率，将10 m提升到约3 m；随后利用改进的分类网络对作物受灾程度进行像素/地块级判别。以BFCD‑22数据集进行训练与评估，并与商用高分影像基线与原始10 m方法对比。

Result: 在BFCD‑22上，“完全损毁”类别F1由0.83提升至0.89，几乎与商用高分影像方法的0.89持平；整体表现显示更高精度且具成本优势。

Conclusion: 通过在低成本公开卫星数据上嵌入超分辨率模块，能显著提升洪灾农损分类精度，接近商用品质；方案可扩展、经济，支持从人工到自动化的全国范围高保真灾损评估转型。

Abstract: Distributing government relief efforts after a flood is challenging. In India, the crops are widely affected by floods; therefore, making rapid and accurate crop damage assessment is crucial for effective post-disaster agricultural management. Traditional manual surveys are slow and biased, while current satellite-based methods face challenges like cloud cover and low spatial resolution. Therefore, to bridge this gap, this paper introduced FLNet, a novel deep learning based architecture that used super-resolution to enhance the 10 m spatial resolution of Sentinel-2 satellite images into 3 m resolution before classifying damage. We tested our model on the Bihar Flood Impacted Croplands Dataset (BFCD-22), and the results showed an improved critical "Full Damage" F1-score from 0.83 to 0.89, nearly matching the 0.89 score of commercial high-resolution imagery. This work presented a cost-effective and scalable solution, paving the way for a nationwide shift from manual to automated, high-fidelity damage assessment.

</details>


### [56] [HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis](https://arxiv.org/abs/2601.03915)
*Julie van Logtestijn,Petru Manescu*

Main category: cs.CV

TL;DR: 提出HemBLIP，一个可解释的血细胞形态描述VLM，基于14k配对图像-属性标注数据，通过全量微调与LoRA高效适配，优于MedGEMMA，在 caption 质量与形态准确性上提升，并以更低算力成本实现可扩展、透明的血液学诊断支持。


<details>
  <summary>Details</summary>
Motivation: 白细胞形态学是白血病诊断核心，但现有深度模型“黑箱”且解释性差，影响临床信任与落地；需要能直接产出形态学可读描述、并兼顾准确性与效率的模型。

Method: 构建含约1.4万张健康与白血病外周血细胞图像及专家属性描述的数据集；在通用VLM上进行两种适配：1) 全量微调；2) 基于LoRA的参数高效微调；并与生物医学VLM基座MedGEMMA对比评测，指标涵盖caption质量与形态学一致性。

Result: HemBLIP在生成质量与形态学准确度上均优于MedGEMMA；采用LoRA后在显著降低计算成本的同时进一步提升性能。

Conclusion: VLM可用于生成可解释的细胞形态描述，提升透明度与临床可用性；LoRA使该方案在资源受限环境中更具可扩展性，显示在血液学诊断中的应用前景。

Abstract: Microscopic evaluation of white blood cell morphology is central to leukemia diagnosis, yet current deep learning models often act as black boxes, limiting clinical trust and adoption. We introduce HemBLIP, a vision language model designed to generate interpretable, morphology aware descriptions of peripheral blood cells. Using a newly constructed dataset of 14k healthy and leukemic cells paired with expert-derived attribute captions, we adapt a general-purpose VLM via both full fine-tuning and LoRA based parameter efficient training, and benchmark against the biomedical foundation model MedGEMMA. HemBLIP achieves higher caption quality and morphological accuracy, while LoRA adaptation provides further gains with significantly reduced computational cost. These results highlight the promise of vision language models for transparent and scalable hematological diagnostics.

</details>


### [57] [FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection](https://arxiv.org/abs/2601.03928)
*Mingyu Ouyang,Kevin Qinghong Lin,Mike Zheng Shou,Hwee Tou Ng*

Main category: cs.CV

TL;DR: 提出FocusUI：在UI定位任务中高效选择与指令相关的视觉补丁，并用PosPad保持位置连续性，从而在大幅减少视觉token的同时维持/提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在UI截图上需处理成千上万视觉token，计算开销大且注意力被稀释；人类交互聚焦于兴趣区域。需要一种既高效又不损失定位精度的UI grounding方法。

Method: 1) 通过融合指令条件得分与基于UI图的规则得分，构建补丁级监督，选择与指令相关、去冗余的视觉token，并降低大面积同质区域权重；2) 提出PosPad：将被丢弃token的每段连续区间压缩为一个特殊标记，放在该段最后索引处，以保留位置连续性，避免常规剪枝导致的定位精度下降。

Result: 在四个UI grounding基准上优于GUI特定基线；在ScreenSpot-Pro上，FocusUI-7B较GUI-Actor-7B提升3.7%。在仅保留30%视觉token时，性能仅下降3.2%，推理最高加速1.44倍，峰值显存降低17%。

Conclusion: 通过相关性驱动的token选择与PosPad位置保持策略，FocusUI在显著降低计算与显存成本的同时，维持乃至提升UI grounding精度，验证了高效UI grounding的可行性。

Abstract: Vision-Language Models (VLMs) have shown remarkable performance in User Interface (UI) grounding tasks, driven by their ability to process increasingly high-resolution screenshots. However, screenshots are tokenized into thousands of visual tokens (e.g., about 4700 for 2K resolution), incurring significant computational overhead and diluting attention. In contrast, humans typically focus on regions of interest when interacting with UI. In this work, we pioneer the task of efficient UI grounding. Guided by practical analysis of the task's characteristics and challenges, we propose FocusUI, an efficient UI grounding framework that selects patches most relevant to the instruction while preserving positional continuity for precise grounding. FocusUI addresses two key challenges: (1) Eliminating redundant tokens in visual encoding. We construct patch-level supervision by fusing an instruction-conditioned score with a rule-based UI-graph score that down-weights large homogeneous regions to select distinct and instruction-relevant visual tokens. (2) Preserving positional continuity during visual token selection. We find that general visual token pruning methods suffer from severe accuracy degradation on UI grounding tasks due to broken positional information. We introduce a novel PosPad strategy, which compresses each contiguous sequence of dropped visual tokens into a single special marker placed at the sequence's last index to preserve positional continuity. Comprehensive experiments on four grounding benchmarks demonstrate that FocusUI surpasses GUI-specific baselines. On the ScreenSpot-Pro benchmark, FocusUI-7B achieves a performance improvement of 3.7% over GUI-Actor-7B. Even with only 30% visual token retention, FocusUI-7B drops by only 3.2% while achieving up to 1.44x faster inference and 17% lower peak GPU memory.

</details>


### [58] [ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.03955)
*Xu Zhang,Cheng Da,Huan Yang,Kun Gai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: ResTok提出一种具备层级残差结构的一维视觉Tokenizer与对应的分层自回归生成器，以恢复视觉模型中的“层级+残差”先验，从而在更少采样步数下提升AR图像生成质量（ImageNet-256 gFID 2.34，9步）。


<details>
  <summary>Details</summary>
Motivation: 现有一维视觉Tokenizer多沿用语言建模思路：将图像展平为序列，产生单层潜变量，忽视视觉领域普遍有效的层级表示与残差连接，导致表示能力与训练/采样效率受限、潜空间分布冗余且难以建模。作者希望把“视觉的先验”带回视觉分词与AR生成中。

Method: 1) 设计Residual Tokenizer（ResTok）：在图像token与潜token两端构建多层级、逐层合并与残差的表示结构；通过跨层特征融合增强表达，并以层间的语义残差避免信息重叠，使潜分布更集中、易于AR建模；跨层绑定关系由结构自发涌现。2) 提出分层AR生成器：按层整体预测一整层潜token，而非逐token自回归，以显著减少采样步数。

Result: 在多项实验中，采用ResTok的AR图像生成显著提升质量与效率：在ImageNet-256上以仅9个采样步达到gFID 2.34，并在广泛对比中优于现有一维视觉Tokenizer方案。

Conclusion: 将层级残差先验引入一维视觉分词与AR生成，可在更集中潜空间与跨层融合的加持下，兼顾高保真与高效率；分层整体预测进一步加速采样，表明视觉模型不应简单仿照语言序列建模。

Abstract: Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.

</details>


### [59] [FUSION: Full-Body Unified Motion Prior for Body and Hands via Diffusion](https://arxiv.org/abs/2601.03959)
*Enes Duran,Nikos Athanasiou,Muhammed Kocabas,Michael J. Black,Omid Taheri*

Main category: cs.CV

TL;DR: 提出FUSION：首个面向全身（含手部）的无条件扩散式动作先验，通过整合手/身体数据集实现联合建模，在HumanML3D关键点跟踪与动作自然性上超越SOTA，并支持物体驱动的手部交互生成与LLM约束的自交互生成。


<details>
  <summary>Details</summary>
Motivation: 现有动作合成方法忽略或弱化手部，且数据集要么缺规模（同时含手+身体），要么缺多样性；大规模数据通常只含身体或只含手，导致难以建立联合的全身先验模型。

Method: 数据层面：整理并统一多源手部动作数据与大规模身体运动数据，合成为包含手指细节与全身的序列；模型层面：提出无条件扩散模型FUSION，以姿态驱动的表示联合建模手与身体；应用层面：设计在扩散潜空间中的优化管线，将外部约束（物体运动、LLM生成的语言约束）转化为可微目标，进行任务特定的动作生成与细化。

Result: 在HumanML3D的Keypoint Tracking任务上优于最先进骨骼控制模型，并获得更高的动作自然度；在两个新应用中实现对手部精细控制且保持全身协调：1）给定物体运动生成含手指细节的交互动作；2）利用LLM将自然语言转为约束生成自交互动作。

Conclusion: 统一数据+扩散式全身先验可显著提升含手部的动作质量与可控性；FUSION不仅在标准基准上领先，还能通过潜空间优化灵活适配多种约束场景。代码将开源。

Abstract: Hands are central to interacting with our surroundings and conveying gestures, making their inclusion essential for full-body motion synthesis. Despite this, existing human motion synthesis methods fall short: some ignore hand motions entirely, while others generate full-body motions only for narrowly scoped tasks under highly constrained settings. A key obstacle is the lack of large-scale datasets that jointly capture diverse full-body motion with detailed hand articulation. While some datasets capture both, they are limited in scale and diversity. Conversely, large-scale datasets typically focus either on body motion without hands or on hand motions without the body. To overcome this, we curate and unify existing hand motion datasets with large-scale body motion data to generate full-body sequences that capture both hand and body. We then propose the first diffusion-based unconditional full-body motion prior, FUSION, which jointly models body and hand motion. Despite using a pose-based motion representation, FUSION surpasses state-of-the-art skeletal control models on the Keypoint Tracking task in the HumanML3D dataset and achieves superior motion naturalness. Beyond standard benchmarks, we demonstrate that FUSION can go beyond typical uses of motion priors through two applications: (1) generating detailed full-body motion including fingers during interaction given the motion of an object, and (2) generating Self-Interaction motions using an LLM to transform natural language cues into actionable motion constraints. For these applications, we develop an optimization pipeline that refines the latent space of our diffusion model to generate task-specific motions. Experiments on these tasks highlight precise control over hand motion while maintaining plausible full-body coordination. The code will be public.

</details>


### [60] [PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography](https://arxiv.org/abs/2601.03993)
*Junle Liu,Peirong Zhang,Yuyi Zhang,Pengyu Yan,Hui Zhou,Xinyue Zhou,Fengjun Guo,Lianwen Jin*

Main category: cs.CV

TL;DR: PosterVerse 提出一套端到端、商业级海报自动生成流程：用微调LLM做蓝图、定制扩散模型生成背景、MLLM+HTML引擎统一布局与文本渲染，并配套发布首个含HTML排版标注的中文海报数据集PosterDNA，显著提升高密度文本可读性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有自动海报系统流程不完整、文本渲染差、商业灵活性不足，难以在真实场景中生成既美观又信息密集且可编辑的成品。

Method: 三阶段流水线：1) 通过微调LLM从需求中抽取主题、层级、配色、版式等并生成设计蓝图；2) 用定制扩散模型生成高质感图像背景；3) 采用MLLM驱动的HTML引擎执行统一布局与高精度文本排版，实现可缩放、高密度文字渲染与灵活定制。此外构建PosterDNA数据集（HTML排版文件+中文海报）。

Result: 实验显示系统能稳定产出商业级视觉质量海报，文本对齐与排版准确，布局可定制，尤其在小字与高密度文本渲染上显著优于现有方法。

Conclusion: PosterVerse将LLM、扩散模型与MLLM+HTML排版融合，打通商业海报从需求到成品的全流程；PosterDNA提供关键数据基础，使小字与高密度文本渲染问题得到根本性缓解，具备实际商用潜力。

Abstract: Commercial-grade poster design demands the seamless integration of aesthetic appeal with precise, informative content delivery. Current automated poster generation systems face significant limitations, including incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. To address these challenges, we propose PosterVerse, a full-workflow, commercial-grade poster generation method that seamlessly automates the entire design process while delivering high-density and scalable text rendering. PosterVerse replicates professional design through three key stages: (1) blueprint creation using fine-tuned LLMs to extract key design elements from user requirements, (2) graphical background generation via customized diffusion models to create visually appealing imagery, and (3) unified layout-text rendering with an MLLM-powered HTML engine to guarantee high text accuracy and flexible customization. In addition, we introduce PosterDNA, a commercial-grade, HTML-based dataset tailored for training and validating poster design models. To the best of our knowledge, PosterDNA is the first Chinese poster generation dataset to introduce HTML typography files, enabling scalable text rendering and fundamentally solving the challenges of rendering small and high-density text. Experimental results demonstrate that PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts, making it a promising solution for automating commercial poster design. The code and model are available at https://github.com/wuhaer/PosterVerse.

</details>


### [61] [Padé Neurons for Efficient Neural Models](https://arxiv.org/abs/2601.04005)
*Onur Keleş,A. Murat Tekalp*

Main category: cs.CV

TL;DR: 提出一种新的非线性神经元“Padé neurons（Paons）”，以Padé逼近为灵感，替代传统线性+点激活的McCulloch-Pitts模型；在多项ResNet系任务（超分、压缩、分类）中以更少层数达到不劣甚至更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经元多为线性变换后接点式非线性，非线性受限、需堆叠多层近似复杂函数。尽管已有二次、广义算子、生成式、超级神经元等模型增强非线性，但缺乏统一性与层效率。作者希望用一种更强且更通用的非线性单元，在减少网络深度的同时提升或保持性能。

Method: 设计以Padé逼近为基础的神经元Paon，使单元内部直接学习输入的有理函数（分子/分母多项式）映射。每个Paon可学习不同的非线性；理论上包含此前提出的非线性神经元作为特例，从而可无缝替换任意网络中的神经元。与把Padé当作点激活的做法不同，Paon将非线性集成到神经元的映射中。

Result: 在若干基于ResNet的图像超分辨率、压缩、与分类模型上，将经典神经元替换为Paon后，在相同或更少层数下取得等同或更优性能；实验与分析显示Paon提高了非线性表达能力与层效率。

Conclusion: Paon作为统一、表达力强的非线性神经元，可用更浅的网络达到传统深层网络的效果，并可作为现有多种神经元模型的上位替代。作者开源了PyTorch实现。

Abstract: Neural networks commonly employ the McCulloch-Pitts neuron model, which is a linear model followed by a point-wise non-linear activation. Various researchers have already advanced inherently non-linear neuron models, such as quadratic neurons, generalized operational neurons, generative neurons, and super neurons, which offer stronger non-linearity compared to point-wise activation functions. In this paper, we introduce a novel and better non-linear neuron model called Padé neurons (Paons), inspired by Padé approximants. Paons offer several advantages, such as diversity of non-linearity, since each Paon learns a different non-linear function of its inputs, and layer efficiency, since Paons provide stronger non-linearity in much fewer layers compared to piecewise linear approximation. Furthermore, Paons include all previously proposed neuron models as special cases, thus any neuron model in any network can be replaced by Paons. We note that there has been a proposal to employ the Padé approximation as a generalized point-wise activation function, which is fundamentally different from our model. To validate the efficacy of Paons, in our experiments, we replace classic neurons in some well-known neural image super-resolution, compression, and classification models based on the ResNet architecture with Paons. Our comprehensive experimental results and analyses demonstrate that neural models built by Paons provide better or equal performance than their classic counterparts with a smaller number of layers. The PyTorch implementation code for Paon is open-sourced at https://github.com/onur-keles/Paon.

</details>


### [62] [Thinking with Frames: Generative Video Distortion Evaluation via Frame Reward Model](https://arxiv.org/abs/2601.04033)
*Yuan Wang,Borui Liao,Huijuan Huang,Jinda Lu,Ouxiang Li,Kuien Liu,Meng Wang,Xiang Wang*

Main category: cs.CV

TL;DR: 提出REACT：一个面向结构失真评估的逐帧视频奖励模型，结合人类偏好数据与CoT合成数据，采用两阶段训练（SFT+GRPO RL）并引入动态采样与REACT-Bench，实现对生成视频结构失真的准确量化与可解释归因，补足现有T2V奖励模型的盲点。


<details>
  <summary>Details</summary>
Motivation: 现有T2V奖励模型多关注画质、运动与文本对齐，容易忽视对象外观异常与交互不合理等“结构性失真”，导致对视频整体质量评估不充分；缺乏针对结构失真的标注、评价基准与可解释分析工具。

Method: 1) 提出结构失真分类法并构建大规模人类偏好数据；辅以高效CoT数据合成。2) 设计REACT：逐帧推理，输出点状分数与失真归因标签。3) 两阶段训练：SFT（带遮罩损失灌输领域知识）→ RL（GRPO与成对奖励以增强推理并与人类偏好对齐）。4) 推理时采用动态采样聚焦疑似失真帧。5) 构建REACT-Bench用于评测。

Result: REACT在结构失真评估上与现有奖励模型互补，提供更准确的量化评分与可解释归因；在REACT-Bench与相关实验中取得优异表现，能更好识别对象异常与交互问题。

Conclusion: 面向T2V生成，REACT有效填补结构失真评估缺口，通过逐帧推理+两阶段训练+动态采样，实现准确、可解释的失真检测；提供REACT-Bench促进该方向标准化评测。

Abstract: Recent advances in video reward models and post-training strategies have improved text-to-video (T2V) generation. While these models typically assess visual quality, motion quality, and text alignment, they often overlook key structural distortions, such as abnormal object appearances and interactions, which can degrade the overall quality of the generative video. To address this gap, we introduce REACT, a frame-level reward model designed specifically for structural distortions evaluation in generative videos. REACT assigns point-wise scores and attribution labels by reasoning over video frames, focusing on recognizing distortions. To support this, we construct a large-scale human preference dataset, annotated based on our proposed taxonomy of structural distortions, and generate additional data using a efficient Chain-of-Thought (CoT) synthesis pipeline. REACT is trained with a two-stage framework: ((1) supervised fine-tuning with masked loss for domain knowledge injection, followed by (2) reinforcement learning with Group Relative Policy Optimization (GRPO) and pairwise rewards to enhance reasoning capability and align output scores with human preferences. During inference, a dynamic sampling mechanism is introduced to focus on frames most likely to exhibit distortion. We also present REACT-Bench, a benchmark for generative video distortion evaluation. Experimental results demonstrate that REACT complements existing reward models in assessing structutal distortion, achieving both accurate quantitative evaluations and interpretable attribution analysis.

</details>


### [63] [Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation](https://arxiv.org/abs/2601.04065)
*Raül Pérez-Gonzalo,Riccardo Magro,Andreas Espersen,Antonio Agudo*

Main category: cs.CV

TL;DR: 通过将像素级分割转化为区域二分类，并结合无监督区域生成与合并，以及区域级数据增强，方法在风机叶片分割上实现SOTA且具跨场景泛化。


<details>
  <summary>Details</summary>
Motivation: 风机叶片表面细小损伤会显著影响气动性能与发电效率，需要高频巡检。传统像素级深度分割依赖大量标注，成本高且难以扩展，因此需要一种标注高效、可泛化的分割方法。

Method: 将分割重构为“区域生成+区域二分类”。1) 无监督可解释的模块化自适应区域生长（MARG）在图像内基于自适应阈值生成候选区域；2) 区域合并（Region Merging）将碎片化区域整合为连贯片段；3) 基于区域的二分类器判断“叶片/非叶片”；4) 引入RegionMix数据增强，将不同图像中的区域组合合成新样本以提升鲁棒性与泛化。

Result: 在多风场数据上取得SOTA级别的分割精度，并能在跨站点（跨风场）条件下稳定地分割叶片，显示出强泛化与鲁棒性。

Conclusion: 区域级、标注高效的分割范式可在低标注成本下实现高精度与良好跨场景泛化，适用于自动化风机巡检与部署扩展。

Abstract: Reliable operation of wind turbines requires frequent inspections, as even minor surface damages can degrade aerodynamic performance, reduce energy output, and accelerate blade wear. Central to automating these inspections is the accurate segmentation of turbine blades from visual data. This task is traditionally addressed through dense, pixel-wise deep learning models. However, such methods demand extensive annotated datasets, posing scalability challenges. In this work, we introduce an annotation-efficient segmentation approach that reframes the pixel-level task into a binary region classification problem. Image regions are generated using a fully unsupervised, interpretable Modular Adaptive Region Growing technique, guided by image-specific Adaptive Thresholding and enhanced by a Region Merging process that consolidates fragmented areas into coherent segments. To improve generalization and classification robustness, we introduce RegionMix, an augmentation strategy that synthesizes new training samples by combining distinct regions. Our framework demonstrates state-of-the-art segmentation accuracy and strong cross-site generalization by consistently segmenting turbine blades across distinct windfarms.

</details>


### [64] [Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models](https://arxiv.org/abs/2601.04068)
*Zitong Huang,Kaidong Zhang,Yukang Ding,Chao Gao,Rui Ding,Ying Chen,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出LocalDPO：将真实视频局部破坏并局部恢复，构造局部偏好对，用区域感知DPO损失对T2V扩散模型进行对齐，提升保真、时序一致与人偏好，且高效无评判器。


<details>
  <summary>Details</summary>
Motivation: 现有DPO对齐依赖多样本排序与特定判别器，采集昂贵、监督模糊且效率低，难以对视频生成中的细粒度时空问题进行有效对齐。

Method: 以高质量真实视频为正样本；随机生成时空mask并用冻结的基座模型仅恢复被遮挡区域，得到负样本；形成“正-负”局部偏好对。训练阶段引入区域感知DPO损失，仅在被破坏的区域内计算偏好梯度，实现局部对齐与快速收敛。提供自动化数据管线：每个提示只需一次推断、无外部评论员或人工标注。

Result: 在Wan2.1与CogVideoX上，相比其他后训练方法，稳定提升视频保真度、时序一致性与人类偏好评分；证明局部对齐的有效性与效率。

Conclusion: LocalDPO为文本到视频扩散模型提供了一种高效、细粒度的偏好对齐范式：利用真实视频与局部损坏/恢复构造偏好对，并通过区域感知DPO实现快速而精准的对齐，优于传统多样本/判别器依赖的方案。

Abstract: Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.

</details>


### [65] [Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts](https://arxiv.org/abs/2601.04073)
*Zhihao Zhu,Jiafeng Liang,Shixin Jiang,Jinlan Fu,Ming Liu,Guanglu Sun,See-Kiong Ng,Bing Qin*

Main category: cs.CV

TL;DR: 论文发现视频推理中的大型多模态模型在出现文本幻觉后会固执沿用错误链条，并提出无训练的推理范式以抑制该问题。


<details>
  <summary>Details</summary>
Motivation: 虽LMM借助CoT在视频推理上表现出色，但其推理链对视觉证据的依赖与自纠错能力存疑；需要系统评估并理解当推理中产生文本幻觉时模型是否会被“文本惯性”牵引。

Method: 1) 提出LogicGraph Perturbation Protocol：对不同LMM（原生推理结构与提示驱动）生成的推理链进行结构化扰动，测评其自反思与自纠错能力。2) 提出Active Visual-Context Refinement（训练自由）：通过主动的视觉再对齐/再定位机制进行细粒度验证，并配合自适应的上下文精炼来总结与去噪推理历史。

Result: 在扰动评测下，各模型自动自纠错率<10%，大多出现盲目文本错误传播。所提无训练范式显著抑制幻觉传播并提升推理鲁棒性（相对提升，未给定具体数值）。

Conclusion: LMM在视频推理的CoT中存在显著“文本惯性”，导致错误持续传播；通过主动视觉重定位与上下文精炼的推理流程可在无需额外训练的前提下提升鲁棒性。

Abstract: Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.

</details>


### [66] [Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction](https://arxiv.org/abs/2601.04090)
*Jiaxin Huang,Yuanbo Yang,Bangbang Yang,Lin Ma,Yuewen Ma,Yiyi Liao*

Main category: cs.CV

TL;DR: Gen3R 通过将重建模型的几何潜变量与视频扩散模型的外观潜变量对齐，联合生成视频与对应的3D几何（位姿、深度、点云），在单/多图条件下实现SOTA场景级3D生成，并用生成先验提升重建鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有场景级3D生成常分离重建与生成：重建模型几何强但外观生成能力弱，视频扩散模型外观强却缺乏显式几何与相机一致性。需要一种方法把两者优势结合，既生成逼真的视频外观，又输出一致的3D几何与相机参数。

Method: 将VGGT重建模型作为几何先验提供“几何潜变量”，在其token上训练适配器；并通过正则化让这些几何潜变量与预训练视频扩散模型的“外观潜变量”对齐。推理时联合生成两类相互解耦但对齐的潜变量，从而同时得到RGB视频与几何（相机位姿、深度和全局点云）。

Result: 在单图与多图条件的场景级3D生成上达到SOTA；相比基线，生成的视频与几何一致性更好。同时，借助生成先验，重建在困难场景下更鲁棒。

Conclusion: 紧密耦合重建模型与生成模型、并在潜空间对齐，可在一次生成中产出高保真外观与结构一致的3D场景；这种耦合不仅提升生成质量，也反向增强传统重建的稳健性。

Abstract: We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.

</details>


### [67] [GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning](https://arxiv.org/abs/2601.04118)
*Wenshuai Li,Xiantai Xiang,Zixiao Wen,Guangyao Zhou,Ben Niu,Feng Wang,Lijia Huang,Qiantong Wang,Yuxin Hu*

Main category: cs.CV

TL;DR: GeoReason提出一个用于遥感视觉-语言模型的“思维—决策同步”框架，通过逻辑一致性约束减少逻辑幻觉，借助自建GeoReason-Bench与两阶段训练（监督知识初始化+一致性感知强化学习），在复杂空间推理任务上显著提升可靠性与可解释性并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RS-VLM在高层空间推理中常出现“答案对但推理错”的逻辑幻觉，或依赖位置捷径而非真正的空间逻辑，削弱在策略性空间决策中的可信度与可用性。作者希望让模型的内部推理过程与最终答案保持一致并可验证。

Method: 1) 构建GeoReason-Bench：基于几何原语与专家知识合成4,000条逻辑推理轨迹，强调可验证的空间逻辑；2) 两阶段训练：a) 监督知识初始化，使模型掌握推理语法与领域知识；b) 一致性感知强化学习，引入“逻辑一致性奖励”，通过选项置换策略检测并惩罚逻辑漂移，使决策锚定在可验证推理链上。

Result: 在多项遥感空间推理与决策任务上，相比先进基线显著提升：更高的推理一致性、更少的逻辑幻觉、更强的可解释性，并达到SOTA表现。

Conclusion: 通过用逻辑一致性奖励和专门的推理数据集对RS-VLM进行“思维—决策同步”训练，可有效降低逻辑幻觉，提升复杂空间任务中的认知可靠性与可解释性，为遥感VLM的高层推理提供通用范式。

Abstract: The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.

</details>


### [68] [Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images](https://arxiv.org/abs/2601.04127)
*Leandro Stival,Ricardo da Silva Torres,Helio Pedrini*

Main category: cs.CV

TL;DR: 提出用像素级二维递归图表征SITS，并以对比学习PIMC联合RSI进行自监督，多任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型通常处理整幅影像或完整时序，难以细粒度捕获像素级时变特征；SITS数据巨大且噪声多，需要更有效、可泛化的表征，同时希望统一利用时间序列与光学影像两种模态。

Method: 将像素级植被指数（NDVI/EVI/SAVI）时间序列转换为二维递归图（recurrence plots），作为比原始序列更信息密集的2D表征；提出像素级多模态对比自监督PIMC，将2D像素时序表示与遥感影像（RSI）进行跨模态对比学习，训练得到有效编码器；在下游任务上微调/评估。

Result: 在PASTIS上做像素级预测与分类、在EuroSAT上做地物分类，均优于或可比当前SOTA；使用2D表示显著提升SITS特征提取，加入对比学习进一步提升像素时序与RSI的表示质量。

Conclusion: 二维递归图+多模态对比自监督形成稳健框架，能更好处理SITS与RSI，提升多种地球观测任务表现；方法通用且具可扩展性。

Abstract: Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on

</details>


### [69] [Klear: Unified Multi-Task Audio-Video Joint Generation](https://arxiv.org/abs/2601.04151)
*Jun Wang,Chunyu Qiang,Yuxin Guo,Yiran Wang,Xijuan Zeng,Chen Zhang,Pengfei Wan*

Main category: cs.CV

TL;DR: Klear提出单塔DiT与全域注意力、配合多任务渐进训练与多阶段课程学习，并构建大规模高质量音视频-密集字幕数据集，实现高保真、语义与时序对齐、可指令跟随的音视频联合与单模态生成，显著超越以往方法并接近Veo 3。


<details>
  <summary>Details</summary>
Motivation: 现有非商用音视频生成存在音画不同步、唇形与语音错位、单模态坍塌等问题，根因在于音视频对应关系建模薄弱、泛化不足以及缺乏高质量密集字幕数据。

Method: 提出Klear：1) 架构：单塔统一DiT块与Omni-Full Attention以实现紧密A-V对齐与可扩展性；2) 训练：从随机模态遮蔽到跨任务联合优化的渐进多任务策略，配合多阶段课程学习，增强表征、世界知识与防止单模态坍塌；3) 数据：首个大规模音视频密集字幕数据集与自动化数据构建流水线，严格标注与筛选高质量对齐的音视频-字幕三元组。

Result: Klear可扩展至大规模数据，在联合与单模态生成中产生高保真、语义与时间对齐、可指令跟随的结果，并对OOD情景具备强泛化；在多项任务上大幅优于以往方法，性能接近Veo 3。

Conclusion: 统一、可扩展的单塔架构与渐进多任务训练、配合高质量密集字幕数据，提供了通往下一代音视频合成的有效路径，兼顾质量、对齐与泛化，达到接近最先进商用系统的水平。

Abstract: Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.

</details>


### [70] [Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning](https://arxiv.org/abs/2601.04153)
*Yifan Wang,Yanyu Li,Sergey Tulyakov,Yun Fu,Anil Kag*

Main category: cs.CV

TL;DR: 提出Diffusion-DRF：用冻结VLM作可微判别器，直接把VLM反馈反传到视频扩散模型，提升画质与文本对齐，避免奖励黑客与不稳定训练，无需额外偏好数据或奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有用于T2V的DPO依赖人工偏好或学得的奖励模型，信号不可微、成本高、易带偏与被“投机”，导致奖励黑客与训练不稳定。需要一种训练友好、可微且无需额外标注/奖励模型的优化途径。

Method: 使用冻结的VLM作为无训练批判器，设计可微的“奖励流”Diffusion-DRF：将VLM的logit级响应转为按token加权的梯度，沿扩散去噪链路端到端反传；采用面向多维属性（语义、时序、一致性等）的自动化结构化提示获取稳定反馈；在末端去噪阶段进行梯度检查点以节省显存并高效更新。方法对扩散模型与任务均保持模型无关性。

Result: 在视频质量与文本语义对齐上显著提升，同时缓解奖励黑客与模型坍塌；在无需额外奖励模型或偏好数据的条件下实现稳定微调；方法可推广到其他基于扩散的生成任务。

Conclusion: Diffusion-DRF用可微的VLM反馈替代非可微偏好信号，构建端到端可反传的优化框架，在提升T2V质量与对齐的同时减少标注依赖与训练风险，具备通用性与实用性。

Abstract: Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.

</details>


### [71] [ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography](https://arxiv.org/abs/2601.04159)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

TL;DR: 提出ToTMNet：用FFT加速的Toeplitz时间混合替代注意力，实现轻量高效的rPPG心率估计（63k参数），在UBFC与跨域SCAMPS→UBFC上达SOTA级精度。


<details>
  <summary>Details</summary>
Motivation: 现有rPPG深度模型虽稳健但参数量与计算开销大；基于注意力的时间建模在序列长度上呈二次复杂度，不利于长序列与轻量部署。需一种既具全局时间感受野又计算/参数高效的替代方案。

Method: 设计ToTMNet：以Toeplitz结构的全局时间混合层取代自注意力。通过循环嵌入与FFT卷积近线性时间实现，并且参数量线性随片段长度扩展。将该全局Toeplitz算子与局部深度可分离时间卷积分支通过门控机制融合，构成紧凑的门控时间混合模块，全网仅约63k参数。

Result: 在UBFC-rPPG上，intra-dataset达到MAE 1.055 bpm、Pearson 0.996；在SCAMPS→UBFC跨域设定下达到MAE 1.582 bpm、Pearson 0.994。消融表明门控机制对在域移情况下充分利用全局Toeplitz混合至关重要。

Conclusion: Toeplitz结构的时间混合为注意力在rPPG中的实用高效替代，可在极小参数量下提供长程依赖建模与强精度；当前限制是仅在两数据集上验证，需更广泛评估。

Abstract: Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.

</details>


### [72] [ImLoc: Revisiting Visual Localization with Image-based Representation](https://arxiv.org/abs/2601.04185)
*Xudong Jiang,Fangjinhua Wang,Silvano Galliani,Christoph Vogel,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出用带深度图的2D图像表示（配合稠密匹配与GPU加速LO-RANSAC）进行视觉定位，兼具易构建维护与高精度，高效存储计算，并在多基准上达SOTA，优于同等地图大小的内存高效方法。


<details>
  <summary>Details</summary>
Motivation: 现有定位在2D基于图像的方法几何推理弱但易维护，3D结构化方法精度高但需集中重建、更新困难、资源开销大。期望获得既易构建维护又具强几何能力与高精度、内存高效的方案。

Method: 重访2D表征，但为每张图像估计深度图以显式编码几何；使用稠密匹配器进行可靠对应；采用压缩策略压缩表示；在几何验证阶段使用GPU加速的LO-RANSAC；提供可在精度与内存之间灵活权衡的层次化管线。

Result: 在标准基准上取得新的SOTA精度；在相似地图大小下优于现有内存高效方法；整体在存储与计算上高效。

Conclusion: 通过“2D图像+深度”的表示与高效几何验证，可在保持易构建维护的同时达到或超越3D方法的定位精度，并支持内存/精度权衡，适合大规模与动态更新场景。

Abstract: Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.

</details>


### [73] [Choreographing a World of Dynamic Objects](https://arxiv.org/abs/2601.04194)
*Yanzhe Lyu,Chen Geng,Karthik Dharmarajan,Yunzhi Zhang,Hadi Alzayer,Shangzhe Wu,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出CHORD通用生成管线，从2D视频蒸馏出拉格朗日运动信息，生成多物体4D(3D+时间)动态，通用、类无关，并可用于机器人操作策略生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的图形学流程依赖类别特定启发式，费时且不具扩展性；学习方法需大规模数据集且覆盖有限。需要一种既通用又可扩展、无需特定类别先验的数据/模型，以生成复杂多体4D动态并支持下游任务。

Method: 提出蒸馏式管线CHORD：以通用视频生成模型为教师，从欧拉表征(像素/光流等)中提取并转化为拉格朗日运动轨迹与形变表示；以此驱动3D时空表示生成多体4D场景动态。方法强调类别无关、可从2D视频中学习对象的运动与交互，并用于合成多样4D序列。

Result: 能在多种对象与场景上生成逼真的多体4D动态，优于现有方法；展示通用性与多样性，并将生成结果用于机器人操作策略，体现实际应用价值。

Conclusion: CHORD通过从2D视频蒸馏拉格朗日运动信息，实现通用、类无关的4D动态生成，兼具可扩展性与实用性，在多体场景与机器人任务中表现出优势。

Abstract: Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [74] [Staged Voxel-Level Deep Reinforcement Learning for 3D Medical Image Segmentation with Noisy Annotations](https://arxiv.org/abs/2601.03875)
*Yuyang Fu,Xiuzhen Guo,Ji Shi*

Main category: eess.IV

TL;DR: 提出SVL-DRL框架，通过分阶段体素级强化学习在噪声标注下实现稳健医学图像分割，平均提升逾3%的Dice与IoU。


<details>
  <summary>Details</summary>
Motivation: 医学分割高度依赖大规模高质量标注，但器官形态复杂与标注者差异导致噪声标注普遍存在，显著影响模型性能。受临床标注者可基于先验纠错启发，需一种可自动辨别并减弱错误标注影响的端到端方法。

Method: 将噪声标注建模为体素依赖问题，提出分阶段强化学习框架以保证稳健收敛；引入体素级异步优势演员-评论家（vA3C），把每个体素视作智能体，训练中自适应更新其状态表示以直接削弱错误标注影响；设计新的动作空间与复合奖励（Dice + 空间连续性）以提升精度并保持语义完整性；采用动态迭代更新策略，无需人工干预。

Result: 在三个公共医学图像数据集与多种实验设定下取得SoTA表现，Dice与IoU平均提升均超过3%。

Conclusion: 体素级分阶段DRL能够在噪声标注下稳健训练分割模型，通过vA3C与复合奖励有效缓解标注错误影响并提升精度，具备通用性与实用潜力。

Abstract: Deep learning has achieved significant advancements in medical image segmentation. Currently, obtaining accurate segmentation outcomes is critically reliant on large-scale datasets with high-quality annotations. However, noisy annotations are frequently encountered owing to the complex morphological structures of organs in medical images and variations among different annotators, which can substantially limit the efficacy of segmentation models. Motivated by the fact that medical imaging annotator can correct labeling errors during segmentation based on prior knowledge, we propose an end-to-end Staged Voxel-Level Deep Reinforcement Learning (SVL-DRL) framework for robust medical image segmentation under noisy annotations. This framework employs a dynamic iterative update strategy to automatically mitigate the impact of erroneous labels without requiring manual intervention. The key advancements of SVL-DRL over existing works include: i) formulating noisy annotations as a voxel-dependent problem and addressing it through a novel staged reinforcement learning framework which guarantees robust model convergence; ii) incorporating a voxel-level asynchronous advantage actor-critic (vA3C) module that conceptualizes each voxel as an autonomous agent, which allows each agent to dynamically refine its own state representation during training, thereby directly mitigating the influence of erroneous labels; iii) designing a novel action space for the agents, along with a composite reward function that strategically combines the Dice value and a spatial continuity metric to significantly boost segmentation accuracy while maintain semantic integrity. Experiments on three public medical image datasets demonstrates State-of-The-Art (SoTA) performance under various experimental settings, with an average improvement of over 3\% in both Dice and IoU scores.

</details>
