<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 159]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Ground: Map-Free LiDAR Relocalization for UAVs](https://arxiv.org/abs/2602.13267)
*Hengyu Mu,Jianshi Wu,Yuxin Guo,XianLian Lin,Qingyong Hu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: 提出MAILS：面向无人机的免地图LiDAR重定位框架，通过局部保持滑动窗口注意力与坐标无关初始化+局部不变位置编码，适应大偏航与高度变化，并发布含多场景/多轨迹的大规模无人机LiDAR数据集，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR重定位多针对自动驾驶，面对无人机稀疏点云、剧烈姿态与高度变化时精度显著下降；同时缺乏能反映真实无人机飞行特性的公开数据集（不规则轨迹、变高度），阻碍方法评测与发展。

Method: 1) Locality-Preserving Sliding Window Attention，从稀疏点云中提取局部判别几何特征；2) 坐标无关的特征初始化与局部不变位置编码，提升对大幅偏航和高度变化的鲁棒性；3) 构建包含四个场景、不同飞行轨迹的大规模无人机LiDAR定位数据集，用于评测。

Result: 在构建的数据集及多种基准上，MAILS取得令人满意的定位精度，且在多项指标上持续、显著优于现有重定位方法。

Conclusion: MAILS有效解决无人机场景下免地图LiDAR重定位的鲁棒性与精度问题，并提供首个（之一）贴近真实飞行特性的评测数据集；代码与数据集即将开源，具有推广与研究价值。

Abstract: Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.

</details>


### [2] [Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification](https://arxiv.org/abs/2602.13286)
*Nathanya Satriani,Djordje Slijepčević,Markus Schedl,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: 论文探讨在易受数据偏见影响的视觉分类（如性别分类）中，利用可解释交互式学习（XIL）通过用户对模型解释的反馈来引导训练，从而抑制偏见与伪相关；评估CAIPI、RRR及其混合方法，并用GradCAM与BLA量化解释与目标区域的一致性；结果显示XIL能让模型更关注相关特征、降低性别偏差，整体精度略降但CAIPI有望提升精度。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习容易依赖伪相关与数据偏见，导致在性别等敏感任务上出现不公平与可解释性差的问题。XIL允许用户基于模型解释提供纠偏信号，可能同时提升透明度与公平性，但尚需系统评估不同XIL策略在视觉偏见场景中的有效性与权衡。

Method: 选取两种前沿XIL策略：CAIPI与Right for the Right Reasons（RRR），并提出二者的混合策略；在性别分类等易偏见任务上训练视觉分类器；利用GradCAM与Bounded Logit Attention（BLA）生成模型解释，并与分割掩码对齐以量化模型关注区域是否与任务相关；比较对分类性能、公平性（性别间误分类率平衡）与解释对齐度的影响。

Result: (i) XIL总体能引导模型关注与预测相关的图像区域，尤其CAIPI效果突出；(ii) XIL可降低性别偏见，使男女误分类率更为平衡；(iii) 透明度与公平性提升通常伴随轻微性能下降，但CAIPI例外，显示出提升分类准确率的潜力。

Conclusion: XIL在有偏数据情境下有助于抑制伪相关并改进公平性；CAIPI与RRR均有效，混合方法亦可，但CAIPI在对齐性、公平性与潜在精度方面最具优势；尽管存在小幅性能权衡，XIL为提高视觉分类器的可解释性与公平性提供了可行路径。

Abstract: Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the model's explanations, thereby helping it to focus on features that are relevant to the prediction from the user's perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in scenarios prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The results are evaluated quantitatively by comparing segmentation masks with explanations generated using Gradient-weighted Class Activation Mapping (GradCAM) and Bounded Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used, and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases with an exception being CAIPI, which shows potential to even improve classification accuracy.

</details>


### [3] [COOPERTRIM: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception](https://arxiv.org/abs/2602.13287)
*Shilpa Mukhopadhyay,Amit Roy-Chowdhury,Hang Qiu*

Main category: cs.CV

TL;DR: 提出COOPERTRIM：基于时间连续性的自适应特征选择框架，用于协同感知，在保证精度的同时将带宽最高降至约1.46%（结合压缩），在分割/检测任务上分别节省至多80.28%/72.52%带宽，并显著优于现有选择策略。


<details>
  <summary>Details</summary>
Motivation: 协同感知需要通过无线共享特征以提升态势感知，但受限带宽难以承载丰富的传感信息。现有方法虽进行子集选择，但仍带来较大带宽压力，且缺乏对时序冗余（静态信息重复传输）的充分利用。需要一种能利用时间连续性、随环境复杂度自适应调度带宽的机制。

Method: 提出COOPERTRIM：1）提出“符合性（conformal）时间不确定性”度量，衡量特征对环境动态的相关性；2）基于该不确定性进行时序感知的特征选择，避免重复传输静态信息；3）数据驱动地动态决定每帧的共享数量，实现随场景复杂度自适应带宽分配；4）在语义分割与3D检测任务中作为插件集成并可与压缩策略组合。

Result: 在多种开源协同分割与检测模型上，COOPERTRIM相较全量/固定选择：- 分割带宽最高降80.28%，检测降72.52%，精度基本不降；- 相对其他选择策略，IoU最高提升45.54%，同时带宽降至多72%；- 结合压缩，带宽可降至1.46%且IoU不受损；- 在定性分析中对环境动态、定位误差和通信时延具有适应性。

Conclusion: 通过显式建模时间不确定性与自适应共享量，COOPERTRIM显著缓解协同感知的带宽-性能矛盾，在多任务和多模型上稳定有效，并具备对真实系统关键扰动的鲁棒性，为落地部署提供可行路径。

Abstract: Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other's live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information hinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To fundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness, agents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature relevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation and 3D detection as example tasks. Across multiple open-source cooperative segmentation and detection models, COOPERTRIM achieves up to 80.28% and 72.52% bandwidth reduction respectively while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Combined with compression strategies, COOPERTRIM can further reduce bandwidth usage to as low as 1.46% without compromising IoU performance. Qualitative results show COOPERTRIM gracefully adapts to environmental dynamics, localization error, and communication latency, demonstrating flexibility and paving the way for real-world deployment.

</details>


### [4] [Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs](https://arxiv.org/abs/2602.13289)
*Paul Jonas Kurz,Tobias Jan Wieczorek,Mohamed A. Abdelsalam,Rahaf Aljundi,Marcus Rohrbach*

Main category: cs.CV

TL;DR: 研究量化对多模态大模型在VQA中的准确性与“可靠性（校准/置信度）”影响，并用Selector置信度估计器缓解量化带来的过度自信，达到效率-可靠性权衡的最佳点。


<details>
  <summary>Details</summary>
Motivation: MLLM在实际部署需兼顾可靠性与效率：现有模型存在过度自信问题且体量大、难以在边缘端部署，需要量化压缩。缺少系统性研究刻画“量化—可靠性”的关联，尤其在多模态场景。

Method: 对Qwen2-VL-7B与Idefics3-8B进行后训练量化（PTQ），比较数据无关HQQ与数据感知MBQ，在多比特宽设置下评估VQA任务的准确性与可靠性（置信度校准、OOD）。将Selector置信度估计器适配到量化的多模态模型，检验其在不同量化等级和OOD条件下的稳健性。

Result: PTQ同时降低准确性与可靠性；数据感知（MBQ）能减轻退化；Selector显著缓解可靠性下降。采用int4的MBQ并结合Selector，在内存减少约75%的同时，效率-可靠性权衡最佳，性能接近未压缩模型。

Conclusion: 首次系统性连接多模态量化与可靠性：量化会削弱模型校准与准确度，但可通过数据感知量化与Selector置信度估计联合缓解，实用上推荐int4 MBQ + Selector以在边缘部署中取得接近原模型的可靠性与效率平衡。

Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessitating compression. We study the intersection of these two challenges by analyzing how Post-Training Quantization (PTQ) compression affects both accuracy and reliability in Visual Question Answering (VQA). We evaluate two MLLMs, Qwen2-VL-7B and Idefics3-8B, quantized with data-free (HQQ) and data-aware (MBQ) methods across multiple bit widths. To counteract the reduction in reliability caused by quantization, we adapt the Selector confidence estimator for quantized multimodal settings and test its robustness across various quantization levels and out-of-distribution (OOD) scenarios. We find that PTQ degrades both accuracy and reliability. Data-aware methods soften the effect thereof. The Selector substantially mitigates the reliability impact. The combination of int4 MBQ and the Selector achieves the best efficiency-reliability trade-off, closing in on uncompressed performance at approx. 75% less memory demand. Overall, we present the first systematic study linking quantization and reliability in multimodal settings.

</details>


### [5] [NutVLM: A Self-Adaptive Defense Framework against Full-Dimension Attacks for Vision Language Models in Autonomous Driving](https://arxiv.org/abs/2602.13293)
*Xiaoxu Peng,Dong Zhou,Jianwen Zhang,Guanghui Sun,Anh Tu Ngo,Anupam Chattopadhyay*

Main category: cs.CV

TL;DR: NutVLM是一套面向自动驾驶VLM的自适应防御框架：先用NutNet++三分类检测样本是否干净/局部补丁/全局扰动；对局部威胁用灰度遮罩净化，对全局扰动触发专家引导的对抗提示调优（EAPT），通过梯度潜空间优化与离散投影生成纠偏驾驶提示，无需全模型微调；在Dolphins基准上整体指标提升4.89%。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的多模态视觉语言模型易受对抗攻击（物理补丁与不可感知全局扰动），现有防御在鲁棒性与干净样本性能间难以兼顾且覆盖流程不全。需要一种端到端、可扩展、开销低且对干净样本友好的防御方案。

Method: 提出NutVLM自适应防御流程：1) NutNet++哨兵进行三向分类（干净/局部补丁/全局扰动），并统一为检测-净化模块；2) 对局部补丁采用高效灰度遮罩净化；3) 对全局扰动启动EAPT：不做全模型微调，改为梯度驱动的潜空间优化并经离散投影，生成“纠偏驾驶提示”以重定向VLM注意力。

Result: 在Dolphins基准上，相比基线带来整体指标（准确率、语言分数、GPT评分等）平均提升4.89%，表现出在攻击与干净样本下的稳健性改进。

Conclusion: NutVLM通过检测-净化-提示调优的自适应联动，在不牺牲推理效率与无需全模型更新的前提下，提高了自动驾驶VLM的鲁棒性与性能，展现出面向智能交通的可扩展安全防护潜力。

Abstract: Vision Language Models (VLMs) have advanced perception in autonomous driving (AD), but they remain vulnerable to adversarial threats. These risks range from localized physical patches to imperceptible global perturbations. Existing defense methods for VLMs remain limited and often fail to reconcile robustness with clean-sample performance. To bridge these gaps, we propose NutVLM, a comprehensive self-adaptive defense framework designed to secure the entire perception-decision lifecycle. Specifically, we first employ NutNet++ as a sentinel, which is a unified detection-purification mechanism. It identifies benign samples, local patches, and global perturbations through three-way classification. Subsequently, localized threats are purified via efficient grayscale masking, while global perturbations trigger Expert-guided Adversarial Prompt Tuning (EAPT). Instead of the costly parameter updates of full-model fine-tuning, EAPT generates "corrective driving prompts" via gradient-based latent optimization and discrete projection. These prompts refocus the VLM's attention without requiring exhaustive full-model retraining. Evaluated on the Dolphins benchmark, our NutVLM yields a 4.89% improvement in overall metrics (e.g., Accuracy, Language Score, and GPT Score). These results validate NutVLM as a scalable security solution for intelligent transportation. Our code is available at https://github.com/PXX/NutVLM.

</details>


### [6] [VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction](https://arxiv.org/abs/2602.13294)
*Jiarong Liang,Max Ku,Ka-Hei Hui,Ping Nie,Wenhu Chen*

Main category: cs.CV

TL;DR: 本文提出VisPhyWorld：让模型从视觉观测生成可执行模拟器代码，以检验其物理推理；并构建VisPhyBench对外观重建与物理运动一致性进行系统评测。结果显示现有MLLM具备语义理解但难以准确还原物理参数与稳定仿真。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM物理推理评测多依赖VQA或VoE等识别式任务，模型可在不形成可检验物理假设下作答，难以区分“看懂语义”与“会做物理推理”。亟需一种可显式、可反驳地测试模型是否能重建世界并进行物理模拟的评估范式。

Method: 提出执行式评测框架VisPhyWorld：给定视觉观测，模型需产出可运行的物理引擎/模拟器代码，显式重建场景几何、外观与物理参数，并在引擎中前向仿真得到视频。由此将“物理推理/世界建模”与“渲染能力”解耦，且生成的代码可检查、编辑与证伪。基于此构建VisPhyBench：含108个物理模板派生的209个评测场景，提供系统协议评估两方面：(1) 外观/结构重建质量；(2) 物理运动的合理性与一致性。并报告重建视频生成有效率。

Result: 在VisPhyBench上，评测流水线可在97.7%的样例中生成有效重建视频。SOTA多模态大模型在语义场景理解上表现强，但在物理参数估计与稳定、一致的物理动力学仿真方面明显不足。

Conclusion: 执行式、代码生成驱动的评测能更直接检验物理推理，使世界表征可检视与可证伪。当前MLLM仍缺乏精确的物理参数推断与动力学一致性能力，提示需要面向可执行世界建模与物理学习的训练与模型设计改进。

Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.

</details>


### [7] [MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models](https://arxiv.org/abs/2602.13296)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 论文提出基于物理分解的两项新指标，用于评估HRRP生成数据的质量与可判别性，避免依赖黑盒分类器，并在高成本数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有HRRP生成数据的评估依赖分类模型，缺乏可解释性与多层次评价；而生成数据评估本就困难，需要能与物理含义对齐的方法。

Method: 将HRRP分解为三部分：mask（目标散射中心的稀疏结构/可见性）、features（物理/几何相关的主要特征）、noise（测量/环境噪声），并基于该物理分解构建两种评估指标；利用一套昂贵且挑战性强的数据集进行实验评测。

Result: 所提两项指标在实验中表现出良好的判别能力，能够区分真实与生成数据或不同生成模型产物，并提供多层级、可解释的评估信息。

Conclusion: 通过物理驱动的HRRP分解与相应指标，可对生成数据进行更可解释与细粒度的评估，减少对黑盒分类器的依赖，并在挑战性任务上验证了其实用性。

Abstract: High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.

</details>


### [8] [Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset](https://arxiv.org/abs/2602.13297)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 论文在大规模海事数据集上研究条件式HRRP合成，发现获取几何（舰船尺寸与视角）是关键驱动，条件生成的签名能重现真实数据中的视线几何趋势，从而提升跨场景鲁棒性。


<details>
  <summary>Details</summary>
Motivation: HRRP便于舰载/机载快速ATR，但对采集条件高度敏感，跨场景鲁棒性差。以往条件生成研究受限于小而特定的数据集，难以概括复杂海岸监视环境，因而需要在更大规模、更多变的场景下系统评估影响因素并提升合成的通用性。

Method: 基于大规模海事HRRP数据库，分析场景变化的主导因素；确定几何变量（舰船尺寸与目标视角）为关键条件；据此构建并训练条件式生成模型（如条件生成框架）以合成HRRP；评估合成签名是否再现真实数据中的视线几何趋势。

Result: 在以舰船尺寸与视角为条件的生成设置下，合成的HRRP能与真实数据一致地体现随视线几何变化的特征趋势，表明模型成功捕捉并重现了关键几何驱动。

Conclusion: 采集几何在稳健HRRP生成中居核心地位；以几何条件进行建模与生成能提高合成签名与真实观测的一致性，从而有望增强跨作战场景的ATR鲁棒性。

Abstract: High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.

</details>


### [9] [Effect of Convolutional Depth on Image Recognition Performance: VGG vs. ResNet vs. GoogLeNet](https://arxiv.org/abs/2602.13298)
*Manfred M. Fischer,Joshua Pitts*

Main category: cs.CV

TL;DR: 论文比较VGG、ResNet、GoogLeNet，强调“有效深度”而非“名义深度”决定性能与效率；残差与Inception能将更深网络转化为更高准确率与更好算力权衡，而纯堆深会早饱和且不稳定。


<details>
  <summary>Details</summary>
Motivation: 深度被视为提升视觉识别的关键，但简单加深常带来收敛不稳、精度不升与算力低效；现有工作混淆了名义深度与训练中可被有效利用的深度。作者希望在统一训练设置下，厘清不同架构中深度如何真正影响准确率、收敛与计算效率。

Method: 在标准化数据与训练协议下，对VGG（plain）、ResNet（残差）、GoogLeNet（Inception）进行受控对比：1) 明确区分名义深度与有效深度（梯度可达/信息路径长度等）；2) 观察不同深度下的分类准确率、收敛曲线与稳定性；3) 评估准确率-计算量（FLOPs/延迟）权衡。

Result: 纯VGG随深度增加出现早期准确率饱和与优化不稳定；ResNet与GoogLeNet可在更低的有效深度下，将增加的名义深度转化为持续的准确率提升，并在相同或更低计算成本下取得更优的精度-计算权衡。

Conclusion: 决定深度有效性的并非层数本身，而是架构机制（残差、Inception）对“有效深度”的约束与塑形。有效深度是卷积网络作为可扩展维度的关键度量，应优先考虑能降低训练有效深度的设计以获得更好的精度、收敛与效率。

Abstract: Increasing convolutional depth has been central to advances in image recognition, yet deeper networks do not uniformly yield higher accuracy, stable optimization, or efficient computation. We present a controlled comparative study of three canonical convolutional neural network architectures - VGG, ResNet, and GoogLeNet - to isolate how depth influences classification performance, convergence behavior, and computational efficiency. By standardizing training protocols and explicitly distinguishing between nominal and effective depth, we show that the benefits of depth depend critically on architectural mechanisms that constrain its effective manifestation during training rather than on nominal depth alone. Although plain deep networks exhibit early accuracy saturation and optimization instability, residual and inception-based architectures consistently translate additional depth into improved accuracy at lower effective depth and favorable accuracy-compute trade-offs. These findings demonstrate that effective depth, not nominal depth, is the operative quantity governing depth's role as a productive scaling dimension in convolutional networks.

</details>


### [10] [KidMesh: Computational Mesh Reconstruction for Pediatric Congenital Hydronephrosis Using Deep Neural Networks](https://arxiv.org/abs/2602.13299)
*Haoran Sun,Zhanpeng Zhu,Anguo Zhang,Bo Liu,Zhaohua Lin,Liqin Huang,Mingjing Yang,Lei Liu,Shan Lin,Wangbin Ding*

Main category: cs.CV

TL;DR: KidMesh提出一种端到端从MRU直接重建先天性肾积水(CH)三维网格的深度网络，省去分割到网格的复杂后处理，在0.4秒内生成无自交的可用于尿流动力学仿真的网格，精度与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为体素级分割，只捕捉形态学信息，若要进行功能评估（如尿动力学仿真）需额外繁琐的网格化后处理，且高质量网格标注难以获取。

Method: 提出KidMesh：从MRU提取特征图，经网格采样得到特征顶点，基于模板网格进行形变生成个体化CH网格；并设计无需精确网格标注的训练策略，适配稀疏切片的MRU数据。

Result: KidMesh平均0.4秒重建网格；无自交；顶点误差>3.2mm与>6.4mm分别仅3.7%与0.2%；网格光栅化后与人工分割的Dice为0.86；性能可与传统流程媲美且无需后处理。

Conclusion: KidMesh实现了从MRU到可仿真的CH网格的端到端重建，为临床尿流动力学评估提供高效可靠的几何基础，降低标注与后处理成本。

Abstract: Pediatric congenital hydronephrosis (CH) is a common urinary tract disorder, primarily caused by obstruction at the renal pelvis-ureter junction. Magnetic resonance urography (MRU) can visualize hydronephrosis, including renal pelvis and calyces, by utilizing the natural contrast provided by water. Existing voxel-based segmentation approaches can extract CH regions from MRU, facilitating disease diagnosis and prognosis. However, these segmentation methods predominantly focus on morphological features, such as size, shape, and structure. To enable functional assessments, such as urodynamic simulations, external complex post-processing steps are required to convert these results into mesh-level representations. To address this limitation, we propose an end-to-end method based on deep neural networks, namely KidMesh, which could automatically reconstruct CH meshes directly from MRU. Generally, KidMesh extracts feature maps from MRU images and converts them into feature vertices through grid sampling. It then deforms a template mesh according to these feature vertices to generate the specific CH meshes of MRU images. Meanwhile, we develop a novel schema to train KidMesh without relying on accurate mesh-level annotations, which are difficult to obtain due to the sparsely sampled MRU slices. Experimental results show that KidMesh could reconstruct CH meshes in an average of 0.4 seconds, and achieve comparable performance to conventional methods without requiring post-processing. The reconstructed meshes exhibited no self-intersections, with only 3.7% and 0.2% of the vertices having error distances exceeding 3.2mm and 6.4mm, respectively. After rasterization, these meshes achieved a Dice score of 0.86 against manually delineated CH masks. Furthermore, these meshes could be used in renal urine flow simulations, providing valuable urodynamic information for clinical practice.

</details>


### [11] [DriveMamba: Task-Centric Scalable State Space Model for Efficient End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13301)
*Haisheng Su,Wei Wu,Feixiang Song,Junjie Zhang,Zhenjie Yang,Junchi Yan*

Main category: cs.CV

TL;DR: DriveMamba提出单阶段Unified Mamba解码器，将任务关系建模、视角对应与长时序融合统一到线性复杂度序列建模中，以稀疏token与轨迹引导扫描实现高效E2E自动驾驶，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有E2E-AD多采取顺序式感知-预测-规划并依赖密集BEV与可分解Transformer，导致信息割裂、误差累积、跨任务/跨传感器关系建模受限；同时图像主干训练不足与注意力二次复杂度制约了长时空扩展与效率。

Method: 提出DriveMamba：1）将图像特征与各任务期望输出统一为稀疏token，并按3D位置排序；2）以Unified Mamba解码器进行线性复杂度长序列建模，联合建模任务间依赖；3）引入隐式视角对应学习与长时序融合；4）设计双向、轨迹引导的“局部到全局”扫描以保持自车视角空间局部性，利于规划。

Result: 在nuScenes与Bench2Drive上取得更优性能与更高效率，并体现良好的泛化能力（相对现有UniAD等基线）。

Conclusion: 以任务为中心的统一Mamba范式可在保持或提升精度的同时，大幅改善E2E-AD在长时空场景下的可扩展性与效率，缓解顺序式框架的信息损失与误差累积。

Abstract: Recent advances towards End-to-End Autonomous Driving (E2E-AD) have been often devoted on integrating modular designs into a unified framework for joint optimization e.g. UniAD, which follow a sequential paradigm (i.e., perception-prediction-planning) based on separable Transformer decoders and rely on dense BEV features to encode scene representations. However, such manual ordering design can inevitably cause information loss and cumulative errors, lacking flexible and diverse relation modeling among different modules and sensors. Meanwhile, insufficient training of image backbone and quadratic-complexity of attention mechanism also hinder the scalability and efficiency of E2E-AD system to handle spatiotemporal input. To this end, we propose DriveMamba, a Task-Centric Scalable paradigm for efficient E2E-AD, which integrates dynamic task relation modeling, implicit view correspondence learning and long-term temporal fusion into a single-stage Unified Mamba decoder. Specifically, both extracted image features and expected task outputs are converted into token-level sparse representations in advance, which are then sorted by their instantiated positions in 3D space. The linear-complexity operator enables efficient long-context sequential token modeling to capture task-related inter-dependencies simultaneously. Additionally, a bidirectional trajectory-guided "local-to-global" scan method is designed to preserve spatial locality from ego-perspective, thus facilitating the ego-planning. Extensive experiments conducted on nuScenes and Bench2Drive datasets demonstrate the superiority, generalizability and great efficiency of DriveMamba.

</details>


### [12] [Spectral Collapse in Diffusion Inversion](https://arxiv.org/abs/2602.13303)
*Nicolas Bourriez,Alexandre Verine,Auguste Genovesio*

Main category: cs.CV

TL;DR: 论文指出：在源域频谱稀疏（如超分、素描到图像）时，用标准确定性扩散反演（DDIM）做无配对图像翻译会出现“频谱坍塌”，导致生成过度平滑、缺少纹理；随机化方法虽能加噪补纹理，却会引发结构漂移。为此提出推理时方法Orthogonal Variance Guidance（OVG），在结构梯度的零空间内校正ODE噪声幅度，既恢复纹理又保持结构。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的条件反演在跨域翻译很有效，但当源域缺少高频而目标域富含高频时，确定性反演无法保持理论高斯噪声分布，导致纹理缺失；而简单引入随机性会损伤输入的语义结构。需要一种同时兼顾结构一致性与纹理真实感的推理策略。

Method: 提出OVG：在推理时修改扩散ODE的动态，强制噪声方差达到理论高斯幅度，但仅在与结构梯度正交的子空间（即结构“零空间”）中注入与校正噪声，从而避免破坏与输入的语义/几何对应；相当于在不改变结构相关方向的前提下补足纹理相关方向的随机性。

Result: 在BBBC021显微超分与Edges2Shoes素描到图像任务上，OVG恢复了真实感纹理并保持结构拟合，克服了DDIM导致的过度平滑，也避免了纯随机方法的结构漂移。实验广泛验证其有效性。

Conclusion: 频谱坍塌是源域频谱稀疏场景中扩散确定性反演的关键失败模式。通过在结构梯度正交空间中恢复理论噪声幅度，OVG能在不牺牲结构的前提下恢复纹理，实现更好的无配对图像翻译。

Abstract: Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.

</details>


### [13] [Progressive Contrast Registration for High-Fidelity Bidirectional Photoacoustic Microscopy Alignment](https://arxiv.org/abs/2602.13304)
*Jiahao Qin*

Main category: cs.CV

TL;DR: 提出PCReg-Net，一种用于高速OR-PAM双向扫描图像的渐进式对比引导配准方法，通过粗到细的四模块架构实现高精度、实时校准，并引入TNCC/TNCG进行无参考时间一致性评估；在OR-PAM-Reg-4K上显著优于SOTA（PSNR提升>14 dB，NCC=0.983，SSIM=0.982）。


<details>
  <summary>Details</summary>
Motivation: 双向光声显微（OR-PAM）为提速采用往返栅格扫描，但前/后向扫描之间既有域偏移又有几何错位。现有方法依赖亮度恒常假设，难以在对比度变化与结构差异下实现高质量对齐（NCC≤0.96），限制了成像质量与下游分析。

Method: 提出PCReg-Net：1）注册U-Net进行粗配准；2）参考特征提取器提取多尺度结构特征；3）对比模块比较粗配准特征与参考特征以显式定位残余错位；4）带特征注入的精炼U-Net输出高保真结果。训练/推理采用渐进式粗到细流程；并提出无参考度量TNCC与TNCG评价跨帧时间一致性。

Result: 在OR-PAM-Reg-4K（432测试样本）上，PCReg-Net取得NCC=0.983、SSIM=0.982、PSNR=46.96 dB，以实时速度显著超过SOTA，PSNR优势>14 dB。

Conclusion: PCReg-Net有效解决双向扫描引起的域偏移与几何错位问题，通过对比引导的渐进式配准实现高精度与实时性，并提供TNCC/TNCG作为无参考时间一致性指标，为高速OR-PAM配准设定新基线。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing methods, constrained by brightness constancy assumptions, achieve limited alignment quality (NCC~$\leq 0.96$). We propose PCReg-Net, a progressive contrast-guided registration framework that performs coarse-to-fine alignment through four lightweight modules: (1)~a registration U-Net for coarse alignment, (2)~a reference feature extractor capturing multi-scale structural cues, (3)~a contrast module that identifies residual misalignment by comparing coarse-registered and reference features, and (4)~a refinement U-Net with feature injection for high-fidelity output. We further propose the Temporal NCC (TNCC) and Temporal NCC Gap (TNCG) for reference-free evaluation of inter-frame temporal consistency. On OR-PAM-Reg-4K (432 test samples), PCReg-Net achieves NCC of 0.983, SSIM of 0.982, and PSNR of 46.96 dB, surpassing the state-of-the-art by over 14 dB at real-time speed. Code is available at https://github.com/JiahaoQin/PCReg-Net

</details>


### [14] [WildfireVLM: AI-powered Analysis for Early Wildfire Detection and Risk Assessment Using Satellite Imagery](https://arxiv.org/abs/2602.13305)
*Aydin Ayanzadeh,Prakhar Dixit,Sadia Kamal,Milton Halem*

Main category: cs.CV

TL;DR: 提出WildfireVLM：以卫星图像为基础，用YOLOv12检测火点与烟羽，并由多模态大模型生成情境化风险评估与响应建议；构建多源对齐数据集，实时部署与仪表板展示；用LLM-as-judge评估推理质量，展示视觉与语言结合在大范围野火监测中的可扩展价值。


<details>
  <summary>Details</summary>
Motivation: 野火频率与强度因气候变化和人类活动上升；现有卫星监测因烟雾微弱、天气多变、需大范围实时处理而难以早期发现与评估风险，亟需能兼顾检测精度与决策支持的端到端方案。

Method: 1) 构建多源标注数据集：Landsat-8/9、GOES-16及其他公开观测源，并进行光谱带对齐与产品统一；2) 使用YOLOv12在卫星影像中检测火区与烟羽，侧重小目标与复杂纹理；3) 将检测结果输入多模态大语言模型，生成情境化风险评估与响应优先级建议；4) 以LLM-as-judge结合统一评分细则评估风险推理质量；5) 采用面向服务的架构实现实时处理、可视化风险仪表板与长期跟踪。

Result: 在构建的数据集和实时系统上实现对火点与烟羽的有效检测，并产出可操作的风险与响应建议；通过LLM-as-judge验证语言推理质量；系统实现可扩展的在线监测与历史追踪能力。

Conclusion: 计算机视觉与语言推理的结合能提升大范围野火早期发现与应对的可扩展性与实用性；多源数据与服务化部署支持实时应用，LLM评估机制为风险推理提供质量保障。

Abstract: Wildfires are a growing threat to ecosystems, human lives, and infrastructure, with their frequency and intensity rising due to climate change and human activities. Early detection is critical, yet satellite-based monitoring remains challenging due to faint smoke signals, dynamic weather conditions, and the need for real-time analysis over large areas. We introduce WildfireVLM, an AI framework that combines satellite imagery wildfire detection with language-driven risk assessment. We construct a labeled wildfire and smoke dataset using imagery from Landsat-8/9, GOES-16, and other publicly available Earth observation sources, including harmonized products with aligned spectral bands. WildfireVLM employs YOLOv12 to detect fire zones and smoke plumes, leveraging its ability to detect small, complex patterns in satellite imagery. We integrate Multimodal Large Language Models (MLLMs) that convert detection outputs into contextualized risk assessments and prioritized response recommendations for disaster management. We validate the quality of risk reasoning using an LLM-as-judge evaluation with a shared rubric. The system is deployed using a service-oriented architecture that supports real-time processing, visual risk dashboards, and long-term wildfire tracking, demonstrating the value of combining computer vision with language-based reasoning for scalable wildfire monitoring.

</details>


### [15] [Fine-Tuning a Large Vision-Language Model for Artwork's Scoring and Critique](https://arxiv.org/abs/2602.13306)
*Zhehan Zhang,Meihua Qian,Li Luo,Siyu Huang,Chaoyi Zhou,Ripon Saha,Xinxin Song*

Main category: cs.CV

TL;DR: 提出用多任务微调Qwen2‑VL‑7B，对人类绘画进行自动创造力评分与解释性反馈，同步输出数值分与基于五维量表的点评；在含1000幅作品的数据集上取得高相关(r>0.97)与低误差(MAE≈3.95)，生成反馈与专家点评语义接近。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估(如TTCT)人工评分费时，难以规模化；既有机器学习方法多依赖图像特征，缺乏可解释、与量表一致的反馈。需要一种能同时给出可信数值评分与结构化文字点评的自动化方案，以服务科研与教学场景。

Method: 构建含1000幅人类绘画的数据集，每幅给1–100分的总分与简短描述，并由两位专家按五维(原创性、色彩、材质/纹理、构图、内容)评分并给文字评语；80/20划分。以Qwen2‑VL‑7B为基座，进行多任务微调：在视觉编码器上接轻量回归头预测数值分，同时通过嵌入结构化量表与作品描述到系统提示中，生成与量化预测一致的量表对齐反馈，实现单次前向同时给分与点评。

Result: 在测试集上取得Pearson相关>0.97、MAE≈3.95/100分；生成反馈与专家评语的SBERT余弦相似度平均0.798，显示较好的语义贴合度。

Conclusion: 多任务微调的视觉-语言框架能有效、可解释地自动评估绘画创造力，兼顾高精度打分与与量表一致的文字反馈，为视觉艺术测评与课堂反馈提供可扩展工具，连接计算机视觉与艺术教育评估。

Abstract: Assessing artistic creativity is foundational to creativity research and arts education, yet manual scoring (e.g., Torrance Tests of Creative Thinking) is labor-intensive at scale. Prior machine-learning approaches show promise for visual creativity scoring, but many rely mainly on image features and provide limited or no explanatory feedback. We propose a framework for automated creativity assessment of human paintings by fine-tuning the vision-language model Qwen2-VL-7B with multi-task learning. Our dataset contains 1000 human-created paintings scored on a 1-100 scale and paired with a short human-written description (content or artist explanation). Two expert raters evaluated each work using a five-dimension rubric (originality, color, texture, composition, content) and provided written critiques; we use an 80/20 train-test split. We add a lightweight regression head on the visual encoder output so the model can predict a numerical score and generate rubric-aligned feedback in a single forward pass. By embedding the structured rubric and the artwork description in the system prompt, we constrain the generated text to match the quantitative prediction. Experiments show strong accuracy, achieving Pearson r > 0.97 and MAE about 3.95 on the 100-point scale. Qualitative evaluation indicates the generated feedback is semantically close to expert critiques (average SBERT cosine similarity = 0.798). The proposed approach bridges computer vision and art assessment and offers a scalable tool for creativity research and classroom feedback.

</details>


### [16] [Visual Para-Thinker: Divide-and-Conquer Reasoning for Visual Comprehension](https://arxiv.org/abs/2602.13310)
*Haoran Xu,Hongyu Wang,Jiaze Li,Shunpeng Chen,Zizhao Tong,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 提出“Visual Para-Thinker”，首个将并行思维扩展到多模态大模型（MLLM）的框架，通过视觉分割与并行推理避免单一路径陷入模式锁定，并在V*、CountBench、RefCOCO、HallusionBench等基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试时扩展（test-time scaling）多依赖加深推理链长度以诱发自反思，但会出现探索停滞与思维模式锁定；在语言域并行思维可缓解，但如何在视觉多模态场景实现仍未解决。

Method: 1) 系统性分析视觉划分（visual partitioning）在并行化推理中的作用；2) 提出两类视觉并行策略；3) 设计Visual Para-Thinker框架，引入Pa-Attention与LPRoPE以保持路径独立与多样性；4) 基于vLLM实现原生多模态并行推理，提升吞吐与效率。

Result: 在V*、CountBench、RefCOCO、HallusionBench等基准上取得显著改进，证明并行推理在视觉域同样带来收益；表现出更强的多样化探索与减少模式锁定。

Conclusion: 由加深到并行的范式迁移在视觉多模态推理中可行且有效；通过专门的注意力与位置编码设计，可在保证路径独立的同时获得效率提升与性能增益。

Abstract: Existing LLM test-time scaling laws emphasize the emergence of self-reflective behaviors through extended reasoning length. Nevertheless, this vertical scaling strategy often encounters plateaus in exploration as the model becomes locked into specific thinking pattern. By shifting from depth to parallelism, parallel thinking mitigates the narrowing of exploration. However, the extension of this paradigm to visual domain remains an open research question. In this paper, we first examine the role of visual partitioning in parallelized reasoning and subsequently propose two distinct strategies. Based on the above, we introduce Visual Para-Thinker, representing the inaugural parallel reasoning framework for MLLMs. To maintain path independence and promote diversity in reasoning, our approach integrates Pa-Attention alongside LPRoPE. Leveraging the vLLM framework, we have developed a native multimodal implementation that facilitates high-efficiency parallel processing. Empirical results on benchmark datasets such as V*, CountBench, RefCOCO, and HallusionBench confirm that Visual Para-Thinker successfully extends the benefits of parallel reasoning to the visual domain.

</details>


### [17] [Agentic Spatio-Temporal Grounding via Collaborative Reasoning](https://arxiv.org/abs/2602.13313)
*Heng Zhao,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 提出ASTG，一个无需训练的开放世界时空视频指代框架，利用MLLM驱动的空间与时间双智能体协同，在“提出—评估”范式下完成目标管道的生成、验证与定位，显著提高效率，并在基准上超越弱/零样本方法、接近全监督表现。


<details>
  <summary>Details</summary>
Motivation: 现有STVG多在预测到的时间段内逐帧定位，导致计算冗余、监督成本高、泛化差；弱监督虽省标注，但仍受限于数据集拟合范式且性能不佳。需要一种训练自由、可开放世界泛化、效率更高的方法。

Method: 构建基于多模态大模型的双智能体：SRA负责空间推理（候选框/管道生成与验证），TRA负责时间推理（起止时间定位）。采用“提出-评估”范式，将时空推理解耦；引入视觉记忆与对话上下文以复用信息、减少重复计算；实现自引导的管道抽取、验证与时序定位，全流程训练免疫。

Result: 在主流数据集上，ASTG显著优于现有弱监督与零样本方法，并与部分全监督方法性能相当；同时检索效率提升。

Conclusion: ASTG证明了MLLM驱动的代理式、训练自由的时空指代可行且高效，通过时空解耦与记忆/对话机制实现强泛化与较低计算与标注成本，为开放世界STVG提供新范式。

Abstract: Spatio-Temporal Video Grounding (STVG) aims to retrieve the spatio-temporal tube of a target object or person in a video given a text query. Most existing approaches perform frame-wise spatial localization within a predicted temporal span, resulting in redundant computation, heavy supervision requirements, and limited generalization. Weakly-supervised variants mitigate annotation costs but remain constrained by the dataset-level train-and-fit paradigm with an inferior performance. To address these challenges, we propose the Agentic Spatio-Temporal Grounder (ASTG) framework for the task of STVG towards an open-world and training-free scenario. Specifically, two specialized agents SRA (Spatial Reasoning Agent) and TRA (Temporal Reasoning Agent) constructed leveraging on modern Multimoal Large Language Models (MLLMs) work collaboratively to retrieve the target tube in an autonomous and self-guided manner. Following a propose-and-evaluation paradigm, ASTG duly decouples spatio-temporal reasoning and automates the tube extraction, verification and temporal localization processes. With a dedicate visual memory and dialogue context, the retrieval efficiency is significantly enhanced. Experiments on popular benchmarks demonstrate the superiority of the proposed approach where it outperforms existing weakly-supervised and zero-shot approaches by a margin and is comparable to some of the fully-supervised methods.

</details>


### [18] [Sim2Radar: Toward Bridging the Radar Sim-to-Real Gap with VLM-Guided Scene Reconstruction](https://arxiv.org/abs/2602.13314)
*Emily Bejerano,Federico Tondolo,Aayan Qayyum,Xiaofan Yu,Xiaofan Jiang*

Main category: cs.CV

TL;DR: Sim2Radar 从单幅RGB图像合成毫米波雷达训练数据：先重建含材质信息的3D场景，再用物理可配置的射线追踪与Fresnel反射模型进行雷达仿真；用合成数据预训练、再用真实数据微调，可显著提升室内3D雷达检测（最高+3.7 AP@IoU0.3）。


<details>
  <summary>Details</summary>
Motivation: 室内低能见度环境中，毫米波雷达感知可靠但数据获取与标注昂贵稀缺，限制了学习型方法的性能与泛化。需要一种可规模化、低成本的雷达训练数据生成方式，且能保留与雷达物理一致的几何与材质反射特性。

Method: 给定单目RGB：1) 单目深度估计与语义分割重建几何与实例；2) 借助视觉-语言推理推断物体材质；3) 基于ITU-R电磁参数，用Fresnel反射模型配置的物理射线追踪模拟mmWave传播，生成雷达点云/回波；4) 用生成数据预训练3D雷达检测器，再在真实雷达数据上微调。

Result: 在真实室内场景上，使用合成数据预训练后微调，可在3D目标检测中提升最高+3.7 AP（IoU 0.3），改进主要体现在空间定位精度上。

Conclusion: 以视觉驱动的物理雷达仿真能为雷达学习提供有效几何先验，在真实数据有限时可显著提升3D雷达感知；单目到雷达的端到端合成是一条可扩展、低成本的数据增广途径。

Abstract: Millimeter-wave (mmWave) radar provides reliable perception in visually degraded indoor environments (e.g., smoke, dust, and low light), but learning-based radar perception is bottlenecked by the scarcity and cost of collecting and annotating large-scale radar datasets. We present Sim2Radar, an end-to-end framework that synthesizes training radar data directly from single-view RGB images, enabling scalable data generation without manual scene modeling. Sim2Radar reconstructs a material-aware 3D scene by combining monocular depth estimation, segmentation, and vision-language reasoning to infer object materials, then simulates mmWave propagation with a configurable physics-based ray tracer using Fresnel reflection models parameterized by ITU-R electromagnetic properties. Evaluated on real-world indoor scenes, Sim2Radar improves downstream 3D radar perception via transfer learning: pre-training a radar point-cloud object detection model on synthetic data and fine-tuning on real radar yields up to +3.7 3D AP (IoU 0.3), with gains driven primarily by improved spatial localization. These results suggest that physics-based, vision-driven radar simulation can provide effective geometric priors for radar learning and measurably improve performance under limited real-data supervision.

</details>


### [19] [IDPruner: Harmonizing Importance and Diversity in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2602.13315)
*Yifan Tan,Yifu Sun,Shirui Huang,Hong Liu,Guanghua Yu,Jianchen Zhu,Yangdong Deng*

Main category: cs.CV

TL;DR: 提出IDPruner：用MMR在“重要性-多样性”权衡下进行视觉token剪枝，无需注意力图，单次剪枝即可，高速兼容FlashAttention，在多架构/任务上SOTA；在Qwen2.5-VL-7B上剪75%仍保留95.18%性能，剪90%仍有86.40%。


<details>
  <summary>Details</summary>
Motivation: MLLM推理受海量视觉token带来算力瓶颈。现有剪枝方法或只看重要性、或只看多样性、或经验性结合，缺乏统一、可证明的最优整合框架，难以在效率与性能间取得稳健平衡。

Method: 理论上系统分析“token重要性—语义多样性”的权衡；基于此引入MMR（最大边际相关性）作为选取策略，在相关性（重要性）与去冗余（多样性）之间实现帕累托最优平衡。方法不依赖注意力图，支持FlashAttention，并以one-shot方式完成剪枝，便于高效部署。

Result: 跨多种MLLM架构与多模态基准的广泛实验显示，IDPruner达到SOTA与更强的泛化：在Qwen2.5-VL-7B-Instruct上，剪除75%视觉token仍保留95.18%基线性能；在极端90%剪枝下仍保持86.40%。

Conclusion: 在视觉token剪枝中，同时兼顾重要性与多样性并通过MMR实现帕累托平衡，可在不依赖注意力图的前提下，显著加速MLLM推理且保持高性能，具有良好的通用性与工程可部署性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities, yet they encounter significant computational bottlenecks due to the massive volume of visual tokens. Consequently, visual token pruning, which substantially reduces the token count, has emerged as a critical technique for accelerating MLLM inference. Existing approaches focus on token importance, diversity, or an intuitive combination of both, without a principled framework for their optimal integration. To address this issue, we first conduct a systematic analysis to characterize the trade-off between token importance and semantic diversity. Guided by this analysis, we propose the \textbf{I}mportance and \textbf{D}iversity Pruner (\textbf{IDPruner}), which leverages the Maximal Marginal Relevance (MMR) algorithm to achieve a Pareto-optimal balance between these two objectives. Crucially, our method operates without requiring attention maps, ensuring full compatibility with FlashAttention and efficient deployment via one-shot pruning. We conduct extensive experiments across various model architectures and multimodal benchmarks, demonstrating that IDPruner achieves state-of-the-art performance and superior generalization across diverse architectures and tasks. Notably, on Qwen2.5-VL-7B-Instruct, IDPruner retains 95.18\% of baseline performance when pruning 75\% of the tokens, and still maintains 86.40\% even under an extreme 90\% pruning ratio. Our code is available at https://github.com/Tencent/AngelSlim.

</details>


### [20] [Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture](https://arxiv.org/abs/2602.13322)
*Datorien L. Anderson*

Main category: cs.CV

TL;DR: PSI 数据集用于剥离纹理相关性，专测拓扑不变性；Eidos 架构在三项诊断任务中表现卓越（>99% PSI、零样本字体迁移81.67%），支持“形式优先”假设：结构化架构的泛化源于几何完整性而非数据规模。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基准多被纹理统计相关主导，难以衡量模型是否真正学习到形状/拓扑不变性。为精确评估模型对仿射变换下结构恒等的保持能力，需要一个能隔离纹理线索的诊断数据与任务套件。

Method: 构建 PolyShapes-Ideal(PSI) 诊断基准，包含三种探针：1) 含噪多边形分类；2) 从 MNIST 零样本迁移到30种未见字体的数字识别；3) 在逐步形变下的几何坍缩映射评估。用这些任务评测强调几何/结构归纳偏置的 Eidos 架构。

Result: Eidos 在 PSI 上准确率>99%，在30种未见字体上的零样本迁移达81.67%，无需预训练；在几何坍缩进程中保持稳定的结构识别能力。

Conclusion: 实验验证“形式优先”假设：当架构具备强几何与拓扑归纳偏置时，其泛化主要取决于几何完整性而非海量数据或纹理统计；PSI 提供了隔离并测量这种能力的实证基准。

Abstract: We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the "Form-First" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.

</details>


### [21] [Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge](https://arxiv.org/abs/2602.13324)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.CV

TL;DR: 提出一种用于边缘自主机器人在军事动态环境中的分层零样本框架：先用轻量目标检测提出候选，再由小型VLM语义核验；在BF6合成视频上实现高准确率，并通过“受控输入”方法诊断不同VLM的感知与推理失效模式，验证该架构在安全关键场景的可行性。


<details>
  <summary>Details</summary>
Motivation: 边缘机器人在军事环境中面临两大瓶颈：缺乏特定领域标注数据与边缘算力受限，导致端侧模型难以兼顾高召回感知与可靠推理；需一种无需再训练、可在低算力上运行且可诊断可靠性的体系。

Method: 采用分层级联：1) 用Grounding DINO作为高召回、文本可提示的候选区域提议器；2) 仅将高置信帧/区域送入紧凑VLM（Qwen/Gemma 4B–12B）进行语义核验与任务推理；3) 在BF6的55段高保真合成视频上评测三个任务（误报过滤、损伤评估、细粒度载具分类）；4) 扩展为“侦察-指挥官”代理式工作流；5) 提出“受控输入”方法，将感知输出与推理输入解耦以定位失败表型。

Result: - 误报过滤：最高100%准确率；- 损伤评估：最高97.5%；- 细粒度载具分类：55–90%；- 代理工作流：资产部署100%正确，GPT-4o评分推理9.8/10，端到端延迟<75秒；- 失效表型：Gemma3-12B战术推理强但视觉感知弱；Gemma3-4B即便给准确信息也出现推理崩溃。

Conclusion: 层次化零样本策略在边缘场景可行且高效，可在不额外训练的情况下达成高召回感知与可靠推理，并通过“受控输入”提供VLM适用性的可认证诊断框架，适用于安全关键应用的模型选型与部署。

Abstract: Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel "Controlled Input" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.

</details>


### [22] [MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation](https://arxiv.org/abs/2602.13326)
*Xirui Hu,Yanbo Ding,Jiahao Wang,Tingting Shi,Yali Wang,Guo Zhi Zhi,Weizhan Zhang*

Main category: cs.CV

TL;DR: 提出MotionWeaver：面向多类类人形体的多主体形象动画生成框架，通过统一的身份无关运动表示与4D锚定融合范式，实现对复杂互动与遮挡的稳健合成，并在自建46小时数据与300视频基准上达SOTA与强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有角色图像动画方法主要面向单人，难以泛化到多类人形、多主体互动与频繁遮挡的情形；缺乏能跨形体泛化的运动表征与能稳健处理互动/遮挡的时空建模与监督机制。

Method: 1) 统一运动表示：从驱动姿态中提取身份无关的运动，并显式绑定到对应角色通道，实现跨多样人形体与多主体扩展；2) 全局4D锚定范式：构建共享4D空间，将运动表示与视频潜表示在该空间融合，并通过分层的4D级监督强化，对互动与遮挡更鲁棒；3) 实体化为端到端框架MotionWeaver，并配套多人互动数据与评测基准。

Result: 在自建包含46小时多人人机/人体互动视频的数据上训练，并在包含成对类人角色的300视频基准上评测，定量与可视化均优于现有方法；在多样类人形体、复杂互动与多主体遮挡场景中展现良好泛化与稳定性。

Conclusion: 基于身份无关的统一运动表示与4D锚定融合与监督，可将单人角色动画自然扩展至多类人形/多主体场景，显著提升对互动与遮挡的处理能力，并取得SOTA表现与强泛化潜力。

Abstract: Character image animation, which synthesizes videos of reference characters driven by pose sequences, has advanced rapidly but remains largely limited to single-human settings. Existing methods struggle to generalize to multi-humanoid scenarios, which involve diverse humanoid forms, complex interactions, and frequent occlusions. We address this gap with two key innovations. First, we introduce unified motion representations that extract identity-agnostic motions and explicitly bind them to corresponding characters, enabling generalization across diverse humanoid forms and seamless extension to multi-humanoid scenarios. Second, we propose a holistic 4D-anchored paradigm that constructs a shared 4D space to fuse motion representations with video latents, and further reinforces this process with hierarchical 4D-level supervision to better handle interactions and occlusions. We instantiate these ideas in MotionWeaver, an end-to-end framework for multi-humanoid image animation. To support this setting, we curate a 46-hour dataset of multi-human videos with rich interactions, and construct a 300-video benchmark featuring paired humanoid characters. Quantitative and qualitative experiments demonstrate that MotionWeaver not only achieves state-of-the-art results on our benchmark but also generalizes effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.

</details>


### [23] [HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13329)
*Yiru Wang,Zichong Gu,Yu Gao,Anqing Jiang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun*

Main category: cs.CV

TL;DR: HiST-VLA提出分层时空VLA用于自动驾驶轨迹生成，通过几何感知+历史状态提示增强3D/时间推理，采用动态token稀疏化与层级Transformer规划器，从粗VLA航点细化到精细轨迹，并以动态潜变量正则把语言指令严格对齐空间与时间；在NAVSIM v2上达SOTA（Navtest EPDMS 88.6，Navhard 50.9）。


<details>
  <summary>Details</summary>
Motivation: 现有VLA在自动驾驶中的关键短板：数值推理不精确、3D空间理解弱、对上下文敏感，导致安全关键场景下不可靠；需要一种既高效又能在几何/时序上更稳健的VLA以生成可执行轨迹。

Method: 1) 层级时空VLA架构HiST-VLA。2) 融合几何感知（显式几何表示）与细粒度驾驶指令、状态历史提示，强化3D与时间推理。3) 动态token稀疏化：融合冗余token而非简单丢弃，降冗余、保性能。4) 层级Transformer规划器：逐步把粗VLA航点细化为精细轨迹。5) 动态潜变量正则化：将语言指令融入规划器，保证空间落地与时间一致性。

Result: 在NAVSIM v2基准上取得SOTA：Navtest上EPDMS=88.6；伪闭环Navhard上EPDMS=50.9，显示可靠性与泛化提升。

Conclusion: 通过分层时空建模、几何/历史提示与高效token处理，以及将语言指令以动态正则方式注入规划器，HiST-VLA实现更可靠的轨迹生成并在NAVSIM v2上达到SOTA，缓解VLA在数值、3D与上下文敏感方面的不足。

Abstract: Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation.
  Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.

</details>


### [24] [Zwitscherkasten -- DIY Audiovisual bird monitoring](https://arxiv.org/abs/2602.13330)
*Dominik Blum,Elias Häring,Fabian Jirges,Martin Schäffer,David Schick,Florian Schulenberg,Torsten Schön*

Main category: cs.CV

TL;DR: 论文提出Zwitscherkasten：在边缘设备上融合声学与视觉的鸟类监测DiY系统，利用轻量深度学习实现实时、低功耗、非侵入式的鸟种识别，并验证在嵌入式平台上可达高精度，适用于大规模生物多样性监测与公民科学。


<details>
  <summary>Details</summary>
Motivation: 传统鸟类监测依赖人工听辨与目视，成本高、覆盖有限；现有AI方案多在云端或单一模态，受能耗、隐私、带宽与鲁棒性限制。需要一种能在资源受限的边缘设备上，结合声学与视觉的多模态、低功耗、可扩展方案。

Method: 在边缘硬件上部署两条深度学习管线：1) 生物声学：先用声学活动检测（AAD）门控录音与推理以节能，再用轻量化音频分类模型进行鸟种识别；2) 视觉：采用细粒度检测+分类流水线进行图像级识别。整体为DiY系统，强调实时、非侵入式与资源受限优化。

Result: 在嵌入式平台上实现了准确的鸟种识别；声学活动检测明显降低能耗；多模态管线可实时运行，验证了在边缘端实现高精度监测的可行性。

Conclusion: 多模态、边缘部署的Zwitscherkasten可在低功耗条件下实现可靠的鸟类识别，具备可扩展性并有助于生物多样性监测与公民科学应用。

Abstract: This paper presents Zwitscherkasten, a DiY, multimodal system for bird species monitoring using audio and visual data on edge devices. Deep learning models for bioacoustic and image-based classification are deployed on resource-constrained hardware, enabling real-time, non-invasive monitoring. An acoustic activity detector reduces energy consumption, while visual recognition is performed using fine-grained detection and classification pipelines. Results show that accurate bird species identification is feasible on embedded platforms, supporting scalable biodiversity monitoring and citizen science applications.

</details>


### [25] [MedScope: Incentivizing "Think with Videos" for Clinical Reasoning via Coarse-to-Fine Tool Calling](https://arxiv.org/abs/2602.13332)
*Wenjie Li,Yujie Zhang,Haoran Sun,Xingqi He,Hongcheng Gao,Chenglong Ma,Ming Hu,Guankun Wang,Shiyi Yao,Renhao Yang,Hongliang Ren,Lei Wang,Junjun He,Yankai Jiang*

Main category: cs.CV

TL;DR: 提出MedScope：一种面向长时程临床视频的工具型推理模型，采用粗到细的证据检索与验证，并以GA-GRPO强化学习优化；配套构建ClinVideoSuite数据套件；在多基准上达SOTA，提升可解释与可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型对长临床视频多为被动采样或弱定位，难以迭代地在时间轴上精准定位证据、验证并给出可辩护结论；缺乏高保真监督数据也限制了可靠推理能力。

Method: 1) 设计MedScope：在推理链中交替进行中间思考、定向工具调用与对取回观测的验证，实现粗到细的时序证据搜寻与显式可视化对齐；2) 构建ClinVideoSuite：强调证据标注与细粒度标记的临床视频套件；3) 提出GA-GRPO：在GRPO框架上加入与证据定位对齐的奖励与证据加权优势，直接强化工具使用与 grounding。

Result: 在完整与细粒度视频理解基准（含域内与跨域）上取得SOTA，同时提高预测的可解释性与可信度（通过时间定位的视觉证据支撑）。

Conclusion: 工具集成的逐层证据搜寻与grounding-aware强化学习，使模型能够“用视频思考”，为可验证的医疗视频AI代理提供可行路径；代码、模型与数据将开源。

Abstract: Long-form clinical videos are central to visual evidence-based decision-making, with growing importance for applications such as surgical robotics and related settings. However, current multimodal large language models typically process videos with passive sampling or weakly grounded inspection, which limits their ability to iteratively locate, verify, and justify predictions with temporally targeted evidence. To close this gap, we propose MedScope, a tool-using clinical video reasoning model that performs coarse-to-fine evidence seeking over long-form procedures. By interleaving intermediate reasoning with targeted tool calls and verification on retrieved observations, MedScope produces more accurate and trustworthy predictions that are explicitly grounded in temporally localized visual evidence. To address the lack of high-fidelity supervision, we build ClinVideoSuite, an evidence-centric, fine-grained clinical video suite. We then optimize MedScope with Grounding-Aware Group Relative Policy Optimization (GA-GRPO), which directly reinforces tool use with grounding-aligned rewards and evidence-weighted advantages. On full and fine-grained video understanding benchmarks, MedScope achieves state-of-the-art performance in both in-domain and out-of-domain evaluations. Our approach illuminates a path toward medical AI agents that can genuinely "think with videos" through tool-integrated reasoning. We will release our code, models, and data.

</details>


### [26] [Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators](https://arxiv.org/abs/2602.13334)
*Hao Liu,Suhaib A. Fahmy*

Main category: cs.CV

TL;DR: 提出一种边-近边协同推理框架：边端轻量通用ViT先判断，低置信样本按Top-k路由到近边多位专家ViT；并用渐进式专家训练提升专精度。实验在CIFAR-100上显著提升准确率并降低时延与能耗。


<details>
  <summary>Details</summary>
Motivation: 边缘设备难以承载ViT计算，完全上云/近边会带来高时延；需要一种既保证精度又降低时延与能耗的协同方案，并能针对类别子集培养“专家”以提升困难样本表现。

Method: - 架构：边端部署轻量通用ViT；近边加速器部署多个中等规模专家ViT。
- 路由：利用边端模型Top-k预测与置信度，若样本低置信则根据Top-k相关性动态选择最合适的专家进行二阶段推理。
- 训练：提出渐进式专家专化训练（progressive specialist training），逐步在目标子集上精调专家，使其对相应类别子集更准确。
- 系统：真实边—近边测试平台评测。

Result: 在CIFAR-100与真实测试床：
- 专家专化准确率在目标子集上提升4.12%。
- 相比静态专家，整体准确率提高2.76%。
- 相比纯边端执行，时延最高降45%。
- 相比仅近边卸载，能耗最高降46%。

Conclusion: 协同推理+动态Top-k路由+渐进式专家训练可在边-近边场景中同时提升准确率并显著降低时延与能耗，优于静态专家与单端执行方案，适合资源受限的真实部署。

Abstract: Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.

</details>


### [27] [Meningioma Analysis and Diagnosis using Limited Labeled Samples](https://arxiv.org/abs/2602.13335)
*Jiamiao Lu,Wei Wu,Ke Gao,Ping Mao,Weichuan Zhang,Tuo Wang,Lingkun Ma,Jiapan Guo,Zanyi Wu,Yuqing Hu,Changming Sun*

Main category: cs.CV

TL;DR: 提出一种自适应多域特征融合（空间域+小波频带）的方法，用于小样本脑膜瘤MRI分级/分类，在三个数据集上优于SOTA，并发布新数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 脑膜瘤的生物学行为与治疗反应依赖分级，准确诊断影响治疗与预后；现有方法忽视不同图像在离散小波频带贡献的差异，限制了分类性能，尤其在小样本场景。

Method: 利用离散小波变换提取多频带特征，并与空间域特征进行加权融合；权重为自适应学习，以强调对当前样本最具判别力的频带/空间信息；构建少样本学习架构（AMSF-Net），并引入新的脑膜瘤MRI数据集进行验证。

Result: 在三个数据集上，所提方法在少样本分类任务中优于现有SOTA；新引入的数据集支持验证；代码将开源（GitHub: ICL-SUST/AMSF-Net）。

Conclusion: 自适应权重的空间-频率特征融合能有效提升少样本脑膜瘤分类准确性，证明频带贡献的样本依赖性应被建模；方法具有临床分级与治疗规划的潜在价值。

Abstract: The biological behavior and treatment response of meningiomas depend on their grade, making an accurate diagnosis essential for treatment planning and prognosis assessment. We observed that the weighted fusion of spatial-frequency domain features significantly influences meningioma classification performance. Notably, the contribution of specific frequency bands obtained by discrete wavelet transform varies considerably across different images. A feature fusion architecture with adaptive weights of different frequency band information and spatial domain information is proposed for few-shot meningioma learning. To verify the effectiveness of the proposed method, a new MRI dataset of meningiomas is introduced. The experimental results demonstrate the superiority of the proposed method compared with existing state-of-the-art methods in three datasets. The code will be available at: https://github.com/ICL-SUST/AMSF-Net

</details>


### [28] [An Integrated Causal Inference Framework for Traffic Safety Modeling with Semantic Street-View Visual Features](https://arxiv.org/abs/2602.13339)
*Lishan Sun,Yujia Cheng,Pengfei Cui,Lei Han,Mohamed Abdel-Aty,Yunhan Zheng,Xingchen Zhang*

Main category: cs.CV

TL;DR: 研究以谷歌街景图像的语义分割提取视觉环境特征，并用双重机器学习与因果森林等方法，识别“绿化比例”对区域交通事故的因果影响；结果显示绿化显著降低事故，且效应在密集、脆弱城区更强，但对弱势道路使用者保护有限。


<details>
  <summary>Details</summary>
Motivation: 现有宏观交通安全模型多依赖静态社会经济与设施指标，忽视驾驶者对环境的视觉感知；已有视觉—安全关系多为相关性，缺乏在复杂空间环境中的稳健因果识别，难以支撑政策评估。

Method: - 以谷歌街景图像进行语义分割，量化视觉环境（如绿化比例）。
- 构建双重机器学习（Double ML）框架以估计视觉特征对区域事故的平均处理效应（ATE），控制高维混杂。
- 使用SHAP解释混杂变量的非线性影响机制。
- 采用因果森林估计条件平均处理效应（CATE），揭示空间异质性。
- 数据：迈阿密都会区事故记录与22万张街景图像。

Result: 绿化比例对交通事故具有显著且稳健的负向因果效应：ATE = -6.38，p = 0.005。该保护效应在密集、社会脆弱的城市核心区最强；对事故类型而言，绿化显著降低角碰与追尾事故；对弱势道路使用者（VRUs）的保护效益有限。

Conclusion: 视觉环境中的绿化具有因果性的安全改善作用，可作为定向治理手段，优先在高风险视觉环境与脆弱城区实施绿化干预；同时需针对VRUs开展差异化的设计优化，以弥补绿化对其有限的保护效果。

Abstract: Macroscopic traffic safety modeling aims to identify critical risk factors for regional crashes, thereby informing targeted policy interventions for safety improvement. However, current approaches rely heavily on static sociodemographic and infrastructure metrics, frequently overlooking the impacts from drivers' visual perception of driving environment. Although visual environment features have been found to impact driving and traffic crashes, existing evidence remains largely observational, failing to establish the robust causality for traffic policy evaluation under complex spatial environment. To fill these gaps, we applied semantic segmentation on Google Street View imageries to extract visual environmental features and proposed a Double Machine Learning framework to quantify their causal effects on regional crashes. Meanwhile, we utilized SHAP values to characterize the nonlinear influence mechanisms of confounding variables in the models and applied causal forests to estimate conditional average treatment effects. Leveraging crash records from the Miami metropolitan area, Florida, and 220,000 street view images, evidence shows that greenery proportion exerts a significant and robust negative causal effect on traffic crashes (Average Treatment Effect = -6.38, p = 0.005). This protective effect exhibits spatial heterogeneity, being most pronounced in densely populated and socially vulnerable urban cores. While greenery significantly mitigates angle and rear-end crashes, its protective benefit for vulnerable road users (VRUs) remains limited. Our findings provide causal evidence for greening as a potential safety intervention, prioritizing hazardous visual environments while highlighting the need for distinct design optimizations to protect VRUs.

</details>


### [29] [FireRed-Image-Edit-1.0 Techinical Report](https://arxiv.org/abs/2602.13344)
*Super Intelligence Team,Changhao Qiao,Chao Hui,Chen Li,Cunzheng Wang,Dejia Song,Jiale Zhang,Jing Li,Qiang Xiang,Runqi Wang,Shuang Sun,Wei Zhu,Xu Tang,Yao Hu,Yibo Chen,Yuhao Huang,Yuxuan Duan,Zhiyi Chen,Ziyuan Guo*

Main category: cs.CV

TL;DR: FireRed-Image-Edit 是一套面向指令图像编辑的扩散Transformer系统，通过大规模高质数据、分阶段训练与新型优化/评估方法，取得SOTA性能并公开代码与基准。


<details>
  <summary>Details</summary>
Motivation: 现有指令图像编辑方法在数据质量、训练稳定性、可控性与评测覆盖面上存在不足，难以在复杂、多样的编辑任务（含文本编辑、身份保持、低层次增强等）上取得稳健一致的效果。

Method: 1) 数据：构建16亿样本语料（9亿文生图、7亿图像编辑），经清洗、分层、自动标注与两阶段筛选，保留1亿+高质量、生成与编辑均衡的数据。2) 训练：多阶段流程——预训练→有监督微调→强化学习；引入多条件感知分桶采样以适配可变分辨率与提升数据效率，并采用随机指令对齐（动态重索引）增强鲁棒性。3) 优化与控制：提出DPO的非对称梯度优化；用于文本编辑的DiffusionNFT结合版面感知OCR奖励；为身份保持设计可微一致性损失。4) 评测：建立覆盖15类编辑任务的新基准REDEdit-Bench，新增美化与低层增强等类目，并在ImgEdit、GEdit等公开基准对比。

Result: 在REDEdit-Bench与公开基准上，对比开源与商用系统获得有竞争力或更优的性能，显示在多类别编辑（含文本编辑、身份保持、低层增强）上的领先。

Conclusion: 系统化的数据、训练与优化设计显著提升指令图像编辑的通用性与可控性；新基准促进全面评测。公开代码、模型与基准有望推动后续研究与应用落地。

Abstract: We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.

</details>


### [30] [Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots](https://arxiv.org/abs/2602.13347)
*Lijun Zhang,Nikhil Chacko,Petter Nilsson,Ruinian Xu,Shantanu Thakar,Bai Lou,Harpreet Sawhney,Zhebin Zhang,Mudit Agrawal,Bhavana Chandrashekhar,Aaron Parness*

Main category: cs.CV

TL;DR: FOREST提出一种面向仓储“入库(stow)”操作的、受意图（计划）条件化的世界模型，利用潜变量扩散+Transformer，从当前观测与计划动作预测入库后的货箱实例级掩码布局；在几何一致性和下游任务中优于启发式基线，能为仓储规划提供前瞻信号。


<details>
  <summary>Details</summary>
Motivation: 自动化仓库每天有海量入库操作，实际执行前若能预见入库后的箱内布局，可用于质量评估、多步推理与规划，减少试错与成本。现有方法多为启发式或粗粒度表示，难以精准预测物品级几何布局。

Method: 提出FOREST：将箱内状态表示为与物品对齐的实例掩码；以“入库意图”（计划的stow行为）作为条件输入；采用潜变量扩散模型与Transformer相结合的生成式世界模型，从当前观测上下文生成入库后的掩码配置，输出可直接用于下游任务的实例布局。

Result: 与启发式基线相比，FOREST在预测与真实入库后布局的几何一致性上显著提升；在两个下游任务（装载质量评估与多次入库推理）中，用预测掩码替换真实掩码仅带来小幅性能下降。

Conclusion: FOREST能有效提供对入库后箱内布局的高保真预测，具有实用的前瞻价值，可无缝支持仓库规划与评估任务，优于传统启发式方法。

Abstract: Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.

</details>


### [31] [From Prompt to Production:Automating Brand-Safe Marketing Imagery with Text-to-Image Models](https://arxiv.org/abs/2602.13349)
*Parmida Atighehchian,Henry Wang,Andrei Kapustin,Boris Lerner,Tiancheng Jiang,Taylor Jensen,Negin Sokhandan*

Main category: cs.CV

TL;DR: 提出一条面向商用品宣的文本生成图像生产流水线，在保证规模化自动化的同时引入必要的人类监督，最终在保持质量与多样性的前提下，显著提升对象保真与人类偏好指标。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型效果显著，但在真实生产中难以兼顾“规模化自动化”与“质量/创意一致性”的平衡；营销场景需要既遵循品牌/指南又保持创意，并确保生成结果与产品高度一致，因此需要一套可扩展且可控的部署管线。

Method: 设计一套端到端生产管线：以文本到图像模型为核心，结合自动化质量与保真度评估（使用DINOV2等特征度量）与人类反馈环节，实现自动大批量生成、自动筛选与少量人工校对；同时引入促进创意多样性的机制并约束其符合营销规范。

Result: 在营销对象保真度上（借助DINOV2评估）提升30.77%，在人类偏好评测上相对提升52.00%，同时保持图像质量与创意多样性。

Conclusion: 该管线可在真实业务中可扩展地部署文本到图像生成，兼顾自动化与人类监督，显著提升营销图像的保真与用户偏好，证明了生产级T2I流水线的可行性与价值。

Abstract: Text-to-image models have made significant strides, producing impressive results in generating images from textual descriptions. However, creating a scalable pipeline for deploying these models in production remains a challenge. Achieving the right balance between automation and human feedback is critical to maintain both scale and quality. While automation can handle large volumes, human oversight is still an essential component to ensure that the generated images meet the desired standards and are aligned with the creative vision. This paper presents a new pipeline that offers a fully automated, scalable solution for generating marketing images of commercial products using text-to-image models. The proposed system maintains the quality and fidelity of images, while also introducing sufficient creative variation to adhere to marketing guidelines. By streamlining this process, we ensure a seamless blend of efficiency and human oversight, achieving a $30.77\%$ increase in marketing object fidelity using DINOV2 and a $52.00\%$ increase in human preference over the generated outcome.

</details>


### [32] [Detecting Brick Kiln Infrastructure at Scale: Graph, Foundation, and Remote Sensing Models for Satellite Imagery Data](https://arxiv.org/abs/2602.13350)
*Usman Nazir,Xidong Chen,Hafiz Muhammad Abubakar,Hadia Abu Bakar,Raahim Arbaz,Fezan Rasool,Bin Chen,Sara Khalid*

Main category: cs.CV

TL;DR: 论文提出用于从高分辨率卫星影像大规模监测南亚、西亚砖窑的自动检测方法与数据集，比较图学习、遥感管线与卫星基础模型的优劣，并给出实践性指导。


<details>
  <summary>Details</summary>
Motivation: 砖窑在南亚等地是重要的空气污染与强迫劳动来源，但现有地面数据稀疏且陈旧，难以开展大规模、持续监测；需要依赖高分辨率遥感来实现自动、可扩展的检测。

Method: 1) 构建涵盖南亚与中亚五个区域、缩放级别20（约0.149 m/像素）、逾130万影像切片的数据集；2) 提出ClimateGraph：一种区域自适应的基于图的模型，建模砖窑布局的空间与方向结构；并与主流图学习基线比较；3) 设计一条基于传统遥感的检测管线，并与近期卫星影像基础模型进行系统基准对比。

Result: 不同范式展现互补优势：图模型在捕捉布局结构方面表现突出；基础模型在泛化与表征能力上表现强；传统遥感管线在某些场景具有稳健性与可解释性。整体结果为规模化砖窑监测提供量化证据。

Conclusion: 图学习、基础模型与遥感管线各有所长，结合或按情境选用可提升从卫星影像进行砖窑检测的可扩展性与可靠性；所发布的数据与评测为后续研究与实际部署提供参考。

Abstract: Brick kilns are a major source of air pollution and forced labor in South Asia, yet large-scale monitoring remains limited by sparse and outdated ground data. We study brick kiln detection at scale using high-resolution satellite imagery and curate a multi city zoom-20 (0.149 meters per pixel) resolution dataset comprising over 1.3 million image tiles across five regions in South and Central Asia. We propose ClimateGraph, a region-adaptive graph-based model that captures spatial and directional structure in kiln layouts, and evaluate it against established graph learning baselines. In parallel, we assess a remote sensing based detection pipeline and benchmark it against recent foundation models for satellite imagery. Our results highlight complementary strengths across graph, foundation, and remote sensing approaches, providing practical guidance for scalable brick kiln monitoring from satellite imagery.

</details>


### [33] [Using Deep Learning to Generate Semantically Correct Hindi Captions](https://arxiv.org/abs/2602.13352)
*Wasim Akram Khan,Anil Kumar Vuppala*

Main category: cs.CV

TL;DR: 研究在Flickr8k上生成印地语图像描述，比较多种视觉特征与文本编码（含注意力、双向LSTM、预训练CNN），以BLEU评估；最佳为注意力+BiLSTM+VGG16（BLEU-1=0.59，BLEU-4=0.19）。


<details>
  <summary>Details</summary>
Motivation: 现有自动图像描述主要聚焦英语，缺少对高使用量外语（如印地语）的系统研究；需要探索多模态架构在非英语场景的有效性并建立基线。

Method: - 用Google Cloud Translator将Flickr8k英文描述翻译为印地语形成训练/评测数据。
- 视觉编码：预训练CNN（VGG16、ResNet50、InceptionV3）提取全局/局部特征。
- 文本编码：单向与双向LSTM；引入注意力层，按时间步对图像特征加权聚合为句级向量。
- 多种组合实验并用BLEU-1与BLEU-4比较。

Result: 注意力型双向LSTM结合VGG16取得最好性能：BLEU-1=0.59，BLEU-4=0.19；以此作为强基线。

Conclusion: 所提多模态框架能在印地语上生成语义相关的图像描述，达到可用水平；为后续改进与跨语言扩展提供参考与基线。

Abstract: Automated image captioning using the content from the image is very appealing when done by harnessing the capability of computer vision and natural language processing. Extensive research has been done in the field with a major focus on the English language which gives the scope for further developments in the same with consideration of popular foreign languages. This research utilizes distinct models for translating the image caption into Hindi, the fourth most popular language across the world. Exploring the multi-modal architectures this research comprises local visual features, global visual features, attention mechanisms, and pre-trained models. Using google cloud translator on the image dataset from Flickr8k, Hindi image descriptions have been generated. Pre-trained CNNs like VGG16, ResNet50, and Inception V3 helped in retrieving image characteristics, while the uni-directional and bi-directional techniques of text encoding are used for the text encoding process. An additional Attention layer helps to generate a weight vector and, by multiplying it, combine image characteristics from each time step into a sentence-level feature vector. Bilingual evaluation understudy scores are used to compare the research outcome. Many experiments that serve as a baseline are done for the comparative analysis of the research. An image with a score of BLEU-1 is considered sufficient, whereas one with a score of BLEU-4 is considered to have fluid image captioning. For both BLEU scores, the attention-based bidirectional LSTM with VGG16 produced the best results of 0.59 and 0.19 respectively. The experiments conclude that researchs ability to produce relevant, semantically accurate image captions in Hindi. The research accomplishes the goals and future research can be guided by this research model.

</details>


### [34] [AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers](https://arxiv.org/abs/2602.13357)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.CV

TL;DR: 提出AdaCorrection：一种在扩散Transformer推理中自适应纠偏的缓存重用框架，以极低开销保持接近原始FID并带来中等加速。


<details>
  <summary>Details</summary>
Motivation: DiT推断迭代去噪成本高。已有缓存复用方法依赖静态/粗粒度启发式，易产生时序漂移与缓存错配，严重损害生成质量。需要一种既能有效复用缓存又能动态保证时空一致性的机制。

Method: 在每个时间步，利用轻量级时空信号估计缓存有效性，并对缓存激活与新计算激活进行自适应加权融合（offset校正）。无需额外监督或再训练，跨Transformer层进行缓存复用与在线纠偏。

Result: 在图像与视频扩散基准上，以极小计算开销达到接近原始的FID，且实现中等推理加速；在多项指标上较先前缓存复用方法稳定提升生成质量。

Conclusion: AdaCorrection能在不牺牲保真度的前提下高效复用缓存并缓解时序漂移/错配问题，为DiT推断提供实用的质量-效率折中。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance.

</details>


### [35] [The Diffusion Duet: Harmonizing Dual Channels with Wavelet Suppression for Image Separation](https://arxiv.org/abs/2602.13361)
*Jingwei Li,Wei Pu*

Main category: cs.CV

TL;DR: 提出一种用于双通道盲图像分离的扩散模型DCDSM，并引入小波抑制模块WSM，利用源间耦合噪声交互实现更好的细节分离；在雨/雪去除与复杂混合分离任务上显著优于现有方法（PSNR/SSIM均提升）。


<details>
  <summary>Details</summary>
Motivation: 传统基于独立性假设或CNN/GAN的方法难以准确表征真实场景中复杂且非线性的特征分布，强噪声和非线性混合下易产生估计偏差、纹理失真与伪影残留。亟需一种能更好建模源图像分布并稳健分离重构的框架。

Method: 将扩散生成模型引入双通道BIS，构建Dual-Channel Diffusion Separation Model（DCDSM），在反向去噪的双分支中加入Wavelet Suppression Module（WSM），基于源图像间的互耦合噪声特性进行交互抑制与细节增强，学习源分布并重建结构；在合成雨/雪与复杂混合数据上训练与评估。

Result: 在雨/雪去除上PSNR/SSIM分别达35.0023/0.9549与29.8108/0.9243，平均较Histoformer与LDRCNet提升约1.26 dB/0.93 dB（PSNR）和0.026/0.029（SSIM）；在复杂混合分离上，双源平均PSNR/SSIM达25.00/0.7997，较对比方法提升4.12 dB与0.093。主客观指标均显示明显优势。

Conclusion: 扩散模型结合双通道交互与小波抑制有效学习与分离源图像分布，显著缓解强噪声与非线性混合下的残留与纹理损失问题，达成SOTA表现，适用于雨/雪去除及复杂混合分离场景。

Abstract: Blind image separation (BIS) refers to the inverse problem of simultaneously estimating and restoring multiple independent source images from a single observation image under conditions of unknown mixing mode and without prior knowledge of the source images. Traditional methods relying on statistical independence assumptions or CNN/GAN variants struggle to characterize complex feature distributions in real scenes, leading to estimation bias, texture distortion, and artifact residue under strong noise and nonlinear mixing. This paper innovatively introduces diffusion models into dual-channel BIS, proposing an efficient Dual-Channel Diffusion Separation Model (DCDSM). DCDSM leverages diffusion models' powerful generative capability to learn source image feature distributions and reconstruct feature structures effectively. A novel Wavelet Suppression Module (WSM) is designed within the dual-branch reverse denoising process, forming an interactive separation network that enhances detail separation by exploiting the mutual coupling noise characteristic between source images. Extensive experiments on synthetic datasets containing rain/snow and complex mixtures demonstrate that DCDSM achieves state-of-the-art performance: 1) In image restoration tasks, it obtains PSNR/SSIM values of 35.0023 dB/0.9549 and 29.8108 dB/0.9243 for rain and snow removal respectively, outperforming Histoformer and LDRCNet by 1.2570 dB/0.9272 dB (PSNR) and 0.0262/0.0289 (SSIM) on average; 2) For complex mixture separation, the restored dual-source images achieve average PSNR and SSIM of 25.0049 dB and 0.7997, surpassing comparative methods by 4.1249 dB and 0.0926. Both subjective and objective evaluations confirm DCDSM's superiority in addressing rain/snow residue removal and detail preservation challenges.

</details>


### [36] [An Online Reference-Free Evaluation Framework for Flowchart Image-to-Code Generation](https://arxiv.org/abs/2602.13376)
*Giang Son Nguyen,Zi Pong Lim,Sarthak Ketanbhai Modi,Yon Shin Teo,Wenya Wang*

Main category: cs.CV

TL;DR: 提出一种在生产环境中对将流程图图像转为代码（如Mermaid）的VLM输出进行“无参考”质量评估的方法，仅用输入图像与生成代码即可评估质量。核心是两个自动指标：基于OCR的召回与基于视觉蕴含的精度，并用其F1作为总体分数；在FlowVQA上与有参考指标高度相关。


<details>
  <summary>Details</summary>
Motivation: 实际生产中处理任意流程图图像，往往没有对应的“真值代码”，难以持续监控模型输出质量与检测幻觉或遗漏。需要一种不依赖标注参考、仍能可靠衡量覆盖度与准确性的在线评估机制。

Method: 设计无参考评估框架：1) Recall_OCR：对输入图像做OCR，抽取文本作为代理参考，衡量生成代码是否覆盖了图中内容；2) Precision_VE：以视觉蕴含（Visual Entailment）检测生成代码中元素是否被原图支持，从而识别幻觉；3) 以两者的调和平均F1_OCR-VE给出统一质量分。

Result: 在FlowVQA数据集上验证，与基于真值的评估高度一致：Recall、Precision、F1的Pearson相关系数分别为0.97、0.91、0.94，显示该框架能可靠反映真实质量。

Conclusion: 该无参考评估方法能在生产环境持续监控流程图图像到代码生成质量，既评估覆盖度又抑制幻觉，提供实用的统一分数，可作为有参考评估的有效替代。

Abstract: Vision-Language Models (VLMs) are increasingly used in document processing pipelines to convert flowchart images into structured code (e.g., Mermaid). In production, these systems process arbitrary inputs for which no ground-truth code exists, making output quality difficult to assess. We propose a reference-free evaluation framework that monitors flowchart image-to-code generation quality at inference time, using only the input image and the generated output. The framework introduces two automated metrics: $\text{Recall}{\text{OCR}}$, which estimates content coverage by extracting text from the input image via OCR as a proxy reference, and $\text{Precision}{\text{VE}}$, which detects hallucinated elements through Visual Entailment against the original image. Their harmonic mean, $\text{F1}{\text{OCR-VE}}$, provides a unified quality score. Validation on the FlowVQA dataset shows strong agreement with ground-truth metrics (average Pearson's $r = 0.97$, $0.91$, and $0.94$ for Recall, Precision, and F1, respectively), confirming the framework's reliability as a practical, reference-free alternative for continuous quality monitoring in production settings.

</details>


### [37] [LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery](https://arxiv.org/abs/2602.13378)
*Sohail Ali Farooqui,Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: 提出LAF-YOLOv10：在YOLOv10n基础上集成PC-C2f、AG-FPN、P2检测头与Wise-IoU v3，以低参数量提升无人机小目标检测；在VisDrone与UAVDT上取得约35% mAP@0.5，并于Jetson Orin Nano达24.3 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有空中目标检测面临小目标尺寸极小、背景杂乱、遮挡严重及板载算力受限等瓶颈；常规YOLO系列在UAV特性下精度与速度难兼顾，需要有针对性的结构改造以提升小目标检测并保持嵌入式实时性。

Method: 以YOLOv10n为基座，联合四项改造：1) PC-C2f：只对四分之一通道做空间卷积，降冗余计算；2) AG-FPN：在多尺度融合前引入SE通道注意力，并用DySample替代最近邻上采样；3) 增设160×160的P2检测头、移除P5以重分配参数，覆盖<8×8像素目标；4) 用Wise-IoU v3替代CIoU，减弱拥挤场景噪声标注对回归的负面影响。

Result: 在VisDrone-DET2019上，3次不同seed训练得到35.1±0.3% mAP@0.5，参数量2.3M，较YOLOv10n提升3.3点；跨数据集UAVDT为35.8±0.4% mAP@0.5；部署于Jetson Orin Nano（FP16）达24.3 FPS。

Conclusion: 虽各组件并非原创，但其在YOLOv10框架中的协同整合分别缓解计算、特征融合、分辨率及回归稳定性四大瓶颈，实现小目标检测精度与嵌入式实时性的平衡，适合UAV端到端应用。

Abstract: Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\times$160 resolution extends localization to objects below 8$\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\pm$0.3\% mAP@0.5 on VisDrone-DET2019 with 2.3\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\pm$0.4\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.

</details>


### [38] [Handling Supervision Scarcity in Chest X-ray Classification: Long-Tailed and Zero-Shot Learning](https://arxiv.org/abs/2602.13430)
*Ha-Hieu Pham,Hai-Dang Nguyen,Thanh-Huy Nguyen,Min Xu,Ulas Bagci,Trung-Nghia Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 该论文针对CXR-LT 2026挑战，在PadChest数据集上提出面向临床胸片多标签分类的两项任务方案：一是应对极端长尾分布的多标签分类；二是零样本识别未见过的疾病类别。方法在宏平均mAP上两项任务均取得领先，并在开发阶段公共榜单排名第一。


<details>
  <summary>Details</summary>
Motivation: 临床CXR分类存在监督不完美：1) 多标签疾病分布极度长尾，尾部类别难以学习；2) 稀有或新出现的发现常无标注，导致训练与测试标签空间不一致。为评估和推动该方向，CXR-LT挑战构建了36类标签，其中30类用于训练、6类为OOD零样本评测，迫切需要既能提升尾部类别，又能在无监督样本下识别未见类别的方法。

Method: - 任务1（长尾多标签）：采用“对不均衡敏感”的多标签学习策略，核心是在保持头部类稳定性能的同时提升尾部类识别率（可能包含重加权/重采样、损失调节、阈值校准与多标签解耦等机制）。
- 任务2（零样本OOD识别）：提出一种无需使用OOD类标签或样本的预测方法，为未见疾病类别生成分数（推测基于文本描述或类别原型的对齐、开放集打分或语义嵌入映射）。
- 评估：以宏平均mAP作为指标，统一比较两项任务表现。

Result: 在CXR-LT 2026开发阶段公共榜单中，两项任务均取得强性能并位列第一，显示方法兼顾了长尾场景下尾部类提升与零样本OOD识别能力。

Conclusion: 面向临床胸片实际监督缺陷，所提策略在长尾多标签与零样本OOD两方面均有效，且无需在训练时引入OOD类别标注/样本即可对未见疾病打分；为实际部署中处理稀有与新发疾病提供了可行路径，并已开放代码与预训练模型以促进复现与应用。

Abstract: Chest X-ray (CXR) classification in clinical practice is often limited by imperfect supervision, arising from (i) extreme long-tailed multi-label disease distributions and (ii) missing annotations for rare or previously unseen findings. The CXR-LT 2026 challenge addresses these issues on a PadChest-based benchmark with a 36-class label space split into 30 in-distribution classes for training and 6 out-of-distribution (OOD) classes for zero-shot evaluation. We present task-specific solutions tailored to the distinct supervision regimes. For Task 1 (long-tailed multi-label classification), we adopt an imbalance-aware multi-label learning strategy to improve recognition of tail classes while maintaining stable performance on frequent findings. For Task 2 (zero-shot OOD recognition), we propose a prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from the OOD classes during training. Evaluated with macro-averaged mean Average Precision (mAP), our method achieves strong performance on both tasks, ranking first on the public leaderboard of the development phase. Code and pre-trained models are available at https://github.com/hieuphamha19/CXR_LT.

</details>


### [39] [Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones](https://arxiv.org/abs/2602.13440)
*Sebastian-Ion Nae,Mihai-Eugen Barbu,Sebastian Mocanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出一个室内UAV视频数据集（14,400帧，时间一致性强），并在YOLOv11-nano上评估三种重放式增量学习（ER/MIR/FAR）。在5–10%回放内存下，FAR表现最佳（5%回放ACC≈82.96% mAP50-95）。Grad-CAM揭示混合场景中注意力迁移导致无人机定位质量下降，表明重放式持续学习可在边缘空中系统上有效。


<details>
  <summary>Details</summary>
Motivation: 室内自主无人机需要在线学习新类别且避免灾难性遗忘，但现有UAV数据多为室外且缺乏时间一致的室内视频；需要一个适配CIL的室内数据集与在受限资源平台上的方法评测。

Method: 1) 构建并标注一个含14,400帧、具时间一致性的室内多平台（空中/地面）视频数据集，采用半自动标注流程，初次标注一致性98.6%，后经人工校验；2) 以YOLOv11-nano为检测器，在类增量场景中对三种重放方法（ER、MIR、FAR）进行基准评测；3) 通过Grad-CAM分析注意力分布与定位质量的关系。

Result: 在严格回放内存（5–10%）条件下，FAR优于ER与MIR：5%回放时平均ACC（各增量mAP50-95均值）达82.96%。Grad-CAM显示在混合类场景中模型注意力在类别间转移，导致对无人机类的定位质量下降。

Conclusion: 所提数据集为室内UAV提供时间一致视频基准；在受限回放预算与边缘算力条件下，重放式CIL可行且FAR最稳健。注意力漂移是影响无人机定位的关键因素，提示后续应改进关注引导与类间干扰抑制。

Abstract: Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\%$ with $5\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl

</details>


### [40] [GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables](https://arxiv.org/abs/2602.13479)
*Akhil Ramachandran,Ankit Arun,Ashish Shenoy,Abhay Harpale,Srihari Jayakumar,Debojeet Chatterjee,Mohsen Moslehpour,Pierce Chuang,Yichao Lu,Vikas Bhardwaj,Peyman Heidari*

Main category: cs.CV

TL;DR: 提出一种混合架构：可穿戴端选择性地对关键帧/区域做高分辨率OCR，同时只上传低分辨率视频供场景理解，从而在保持Text VQA准确度的同时显著降低功耗与热负载。


<details>
  <summary>Details</summary>
Motivation: Text VQA在可穿戴设备上需要既看清文字又保持实时性，但高分辨率视频串流耗电高、易过热；同时，现有Video LLM难以在多帧流中保持文字的时序一致性。作者观察到OCR与视觉推理对分辨率需求不对称：OCR需细节，场景理解可容忍低清。

Method: 设计混合式系统：在设备端执行选择性高分辨率OCR（针对文字候选区域/关键帧），同时将低分辨率视频流发送到后端/LLM用于全局语境和推理；在时间上整合多帧文字识别结果以维持上下文一致；评测覆盖五类Text VQA任务。

Result: 在五类Text VQA基准上达到72%准确率；在与全分辨率视频串流相比的设置下，仅用0.49倍功耗，维持持续的VQA会话能力且不损失文字理解质量。

Conclusion: 利用分辨率需求不对称性，通过端侧选择性高分辨率OCR+低分辨率场景流的混合架构，可在资源受限可穿戴设备上实现高效稳定的Text VQA，兼顾准确率与能耗/热管理。

Abstract: Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.

</details>


### [41] [Benchmarking Video Foundation Models for Remote Parkinson's Disease Screening](https://arxiv.org/abs/2602.13507)
*Md Saiful Islam,Ekram Hossain,Abdelrahman Abdelkader,Tariq Adnan,Fazla Rabbi Mashrur,Sooyong Park,Praveen Kumar,Qasim Sudais,Natalia Chunga,Nami Shah,Jan Freyberg,Christopher Kanan,Ruth Schneider,Ehsan Hoque*

Main category: cs.CV

TL;DR: 基于1888名受试者、16项标准化任务、7种视频基础模型的大规模对比，发现不同VFM对任务敏感性差异显著：VideoPrism擅长无声视觉言语与面部表情，V-JEPA优于上肢运动，TimeSformer在节律性任务上强。冻结特征+线性头可达AUC 76.4–85.3%、准确率71.5–80.6%，高特异性（至90.3%）但敏感性偏低（43.2–57.3%），提示需任务感知校准与多任务/多模态融合。提供公开代码与匿名数据，建立PD远程筛查基线与选型路线图。


<details>
  <summary>Details</summary>
Motivation: 远程视频评估可扩展地支持帕金森病筛查，但以往依赖手工特征，泛化与可扩展性受限。新兴VFM能端到端学到通用视频表征，但尚不清楚不同架构在多类临床任务中的相对效果与稳健性，缺乏系统性基准与选型指南。

Method: 构建包含1888名参与者（727 PD）、16项标准化临床任务、32,847段视频的数据集；选取7个SOTA VFM（如 VideoPrism、V-JEPA、ViViT、VideoMAE、TimeSformer 等），冻结其视频嵌入，仅训练线性分类头；在各任务上比较AUC、准确率、敏感性、特异性，并分析不同模型对任务的显著性与稳健性。

Result: 总体性能AUC 76.4–85.3%，准确率71.5–80.6%；特异性最高达90.3%，敏感性为43.2–57.3%。模型与任务高度相关：VideoPrism在无音频的视觉言语运动学与面部表情上最佳；V-JEPA在上肢运动任务上领先；TimeSformer在节律性手指敲击等任务上竞争力强。

Conclusion: 冻结VFM特征已可为PD远程筛查提供强基线，但单一任务/模型难以兼顾敏感性；应进行任务感知的阈值/校准，并融合多任务与多模态以提升检出率。本文给出体系化基准与任务-架构配对建议，并开放代码与匿名数据以促进可复现研究。

Abstract: Remote, video-based assessments offer a scalable pathway for Parkinson's disease (PD) screening. While traditional approaches rely on handcrafted features mimicking clinical scales, recent advances in video foundation models (VFMs) enable representation learning without task-specific customization. However, the comparative effectiveness of different VFM architectures across diverse clinical tasks remains poorly understood. We present a large-scale systematic study using a novel video dataset from 1,888 participants (727 with PD), comprising 32,847 videos across 16 standardized clinical tasks. We evaluate seven state-of-the-art VFMs -- including VideoPrism, V-JEPA, ViViT, and VideoMAE -- to determine their robustness in clinical screening. By evaluating frozen embeddings with a linear classification head, we demonstrate that task saliency is highly model-dependent: VideoPrism excels in capturing visual speech kinematics (no audio) and facial expressivity, while V-JEPA proves superior for upper-limb motor tasks. Notably, TimeSformer remains highly competitive for rhythmic tasks like finger tapping. Our experiments yield AUCs of 76.4-85.3% and accuracies of 71.5-80.6%. While high specificity (up to 90.3%) suggests strong potential for ruling out healthy individuals, the lower sensitivity (43.2-57.3%) highlights the need for task-aware calibration and integration of multiple tasks and modalities. Overall, this work establishes a rigorous baseline for VFM-based PD screening and provides a roadmap for selecting suitable tasks and architectures in remote neurological monitoring. Code and anonymized structured data are publicly available: https://anonymous.4open.science/r/parkinson\_video\_benchmarking-A2C5

</details>


### [42] [SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning](https://arxiv.org/abs/2602.13515)
*Jintao Zhang,Kai Jiang,Chendong Xiang,Weiqi Feng,Yuezhou Hu,Haocheng Xi,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: 提出SpargeAttention2：一种可训练稀疏注意力，结合Top-k与Top-p混合掩码、效率实现与蒸馏式微调目标，在视频扩散模型上实现95%注意力稀疏与16.2倍注意力加速且不降质。


<details>
  <summary>Details</summary>
Motivation: 训练免费（training-free）的稀疏注意力能加速扩散模型，但在高稀疏度下常出现掩码失效与画质退化。近期可训练稀疏注意力显示可在不牺牲质量的前提下进一步提升稀疏度，但其成功机制、失败情形与微调目标的局限尚不清晰，需要系统分析与改进。

Method: 系统分析三问题：（1）Top-k与Top-p掩码的失效条件与规避策略；（2）可训练稀疏为何优于训练免费；（3）仅用扩散损失微调的局限与改进路径。基于分析提出SpargeAttention2：i) 混合掩码（Top-k+Top-p）提升高稀疏下的鲁棒性；ii) 高效可训练稀疏注意力实现；iii) 受蒸馏启发的微调目标以更好保留生成质量。

Result: 在视频扩散模型实验中，实现95%注意力稀疏与16.2×注意力加速，同时保持生成质量，并在一致性上优于以往稀疏注意力方法。

Conclusion: 通过混合掩码、优化实现与蒸馏式微调目标，可训练稀疏注意力在不降质的情况下实现极高稀疏与显著加速，优于现有训练免费与先前可训练方案。

Abstract: Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.

</details>


### [43] [Nighttime Autonomous Driving Scene Reconstruction with Physically-Based Gaussian Splatting](https://arxiv.org/abs/2602.13549)
*Tae-Kyeong Kim,Xingxin Chen,Guile Wu,Chengjie Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出将物理基渲染融入3D高斯喷溅（3DGS），显式建模漫反射全局光照与各向异性高光，使夜间自动驾驶场景重建质量与实时渲染兼得，并在nuScenes与Waymo上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF与3DGS在正常光照下效果出色，但夜间低照度下因复杂光照/材质导致退化；需要一种能正确分离与重建夜间场景中漫反射、镜面反射与全局照明的方案，以提升重建质量且保持实时性。

Method: 在复合场景Gaussians表示中引入PBR：联合优化基于BRDF的材质参数；用全局照明模块显式建模漫反射分量；用各向异性球面高斯表示与渲染镜面分量；保持3DGS的实时渲染优势。

Result: 在两个真实世界自动驾驶数据集（nuScenes、Waymo）的多种夜间场景上进行大量实验，定量与定性均超越现有最先进方法，同时实现实时渲染。

Conclusion: 物理基渲染与3DGS的融合能有效处理夜间复杂光照与外观变化，显著提升户外夜间自动驾驶场景重建质量，并能在实时约束下运行，适合作为夜间场景重建的新基线。

Abstract: This paper focuses on scene reconstruction under nighttime conditions in autonomous driving simulation. Recent methods based on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved photorealistic modeling in autonomous driving scene reconstruction, but they primarily focus on normal-light conditions. Low-light driving scenes are more challenging to model due to their complex lighting and appearance conditions, which often causes performance degradation of existing methods. To address this problem, this work presents a novel approach that integrates physically based rendering into 3DGS to enhance nighttime scene reconstruction for autonomous driving. Specifically, our approach integrates physically based rendering into composite scene Gaussian representations and jointly optimizes Bidirectional Reflectance Distribution Function (BRDF) based material properties. We explicitly model diffuse components through a global illumination module and specular components by anisotropic spherical Gaussians. As a result, our approach improves reconstruction quality for outdoor nighttime driving scenes, while maintaining real-time rendering. Extensive experiments across diverse nighttime scenarios on two real-world autonomous driving datasets, including nuScenes and Waymo, demonstrate that our approach outperforms the state-of-the-art methods both quantitatively and qualitatively.

</details>


### [44] [Privacy-Concealing Cooperative Perception for BEV Scene Segmentation](https://arxiv.org/abs/2602.13555)
*Song Wang,Lingling Li,Marcus Santos,Guanghui Wang*

Main category: cs.CV

TL;DR: 提出一种用于自动驾驶协同感知的隐私隐藏框架PCC：在共享BEV特征上加入“隐藏网络”，用对抗训练压制图像重建，同时保持BEV语义分割性能，实验证明显著降低重建质量且分割精度几乎不受影响。


<details>
  <summary>Details</summary>
Motivation: 协同感知通过车车/车路通信共享感知信息可扩展感知范围，但共享特征可能被逆向重建出敏感原始图像，引发隐私泄露风险；因此需要在不显著牺牲下游感知性能的前提下抑制可视内容的可重建性。

Method: 在常用的共享BEV特征上插入一个“隐藏网络”，与一个图像重建网络进行对抗博弈：隐藏网络学习去除/扰动能被重建器利用的视觉线索，重建网络力图从被隐藏的特征中恢复输入图像；同时将感知（BEV语义分割）网络与隐藏网络端到端联合优化，以兼顾隐私与任务性能。

Result: 在实验中，PCC显著降低了由共享特征可重建图像的质量（隐私泄露程度），而对BEV语义分割精度仅造成很小的影响。

Conclusion: PCC能在协同感知场景中有效保护隐私：通过对抗式特征隐藏，在不显著牺牲分割性能的情况下抑制图像重建；代码将于论文发表后开源。

Abstract: Cooperative perception systems for autonomous driving aim to overcome the limited perception range of a single vehicle by communicating with adjacent agents to share sensing information. While this improves perception performance, these systems also face a significant privacy-leakage issue, as sensitive visual content can potentially be reconstructed from the shared data. In this paper, we propose a novel Privacy-Concealing Cooperation (PCC) framework for Bird's Eye View (BEV) semantic segmentation. Based on commonly shared BEV features, we design a hiding network to prevent an image reconstruction network from recovering the input images from the shared features. An adversarial learning mechanism is employed to train the network, where the hiding network works to conceal the visual clues in the BEV features while the reconstruction network attempts to uncover these clues. To maintain segmentation performance, the perception network is integrated with the hiding network and optimized end-to-end. The experimental results demonstrate that the proposed PCC framework effectively degrades the quality of the reconstructed images with minimal impact on segmentation performance, providing privacy protection for cooperating vehicles. The source code will be made publicly available upon publication.

</details>


### [45] [Diff-Aid: Inference-time Adaptive Interaction Denoising for Rectified Text-to-Image Generation](https://arxiv.org/abs/2602.13585)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: 提出Diff-Aid：一种在推理时自适应地调节文本-图像跨块与跨时间步交互的轻量方法，显著提升T2I扩散模型对复杂提示的遵循与画质，并具可解释的调制模式，适配多种下游任务，在SD3.5与FLUX上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型难以严格遵循复杂文本，因为文本与视觉特征交互不足；既有方法或依赖固定架构/手工权重，缺乏对不同Transformer块与去噪阶段的动态适配与灵活性。

Method: 提出Diff-Aid作为即插即用的推理期模块：对每个文本token与图像特征，在不同Transformer块与去噪时间步上自适应地调整交互强度（调制权重），以更好地对齐语义；方法轻量、无需训练，产生可解释的调制图样。

Result: 在强基线（SD 3.5、FLUX）上，Diff-Aid在多项指标上提升提示遵循、视觉质量与人类偏好；可无缝结合风格LoRA、可控生成与零样本编辑并进一步增益。

Conclusion: 推理期的自适应跨块/跨时间步文本-图像交互调制能有效提升T2I模型的语义对齐与画质，具有可解释性与通用性，可作为多种应用的即插即用增强模块；代码与模型将开源。

Abstract: Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages. To provide a more flexible and efficient solution to this problem, we propose Diff-Aid, a lightweight inference-time method that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. Beyond improving generation quality, Diff-Aid yields interpretable modulation patterns that reveal how different blocks, timesteps, and textual tokens contribute to semantic alignment during denoising. As a plug-and-play module, Diff-Aid can be seamlessly integrated into downstream applications for further improvement, including style LoRAs, controllable generation, and zero-shot editing. Experiments on strong baselines (SD 3.5 and FLUX) demonstrate consistent improvements in prompt adherence, visual quality, and human preference across various metrics. Our code and models will be released.

</details>


### [46] [Two-Stream Interactive Joint Learning of Scene Parsing and Geometric Vision Tasks](https://arxiv.org/abs/2602.13588)
*Guanfeng Tang,Hongbo Zhao,Ziwei Long,Jiayao Li,Bohong Xiao,Wei Ye,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 提出TwInS：受人类视觉双通路启发的联合学习框架，同时进行场景解析与几何视觉任务，双向交互、跨任务适配器与半监督训练带来SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 人类视觉通过并行的情景（上下文）与空间（几何）通路协同处理信息；现有方法往往单任务或弱交互，且几何任务依赖昂贵的人工对应标签。需要一种在不依赖大量人工标注的前提下，能高效联合建模上下文与几何并相互增强的通用框架。

Method: 提出TwInS双流架构：一条“场景解析流”提取多层次上下文特征并注入到“几何视觉流”中以迭代细化；反向将解码后的几何特征投影到上下文空间，通过新颖的跨任务适配器进行选择性异构特征融合，利用跨视角几何线索增强场景解析。配套半监督训练策略，利用大规模多视角数据，在无对应真值的情况下自我进化。

Result: 在三个公开数据集上进行广泛实验，核心组件经验证有效，整体性能优于现有SOTA方法。

Conclusion: TwInS以生物启发的双向交互与跨任务适配器实现场景解析与几何任务的协同提升，并通过半监督策略降低标注依赖，展示出强泛化与先进性能；代码将公开。

Abstract: Inspired by the human visual system, which operates on two parallel yet interactive streams for contextual and spatial understanding, this article presents Two Interactive Streams (TwInS), a novel bio-inspired joint learning framework capable of simultaneously performing scene parsing and geometric vision tasks. TwInS adopts a unified, general-purpose architecture in which multi-level contextual features from the scene parsing stream are infused into the geometric vision stream to guide its iterative refinement. In the reverse direction, decoded geometric features are projected into the contextual feature space for selective heterogeneous feature fusion via a novel cross-task adapter, which leverages rich cross-view geometric cues to enhance scene parsing. To eliminate the dependence on costly human-annotated correspondence ground truth, TwInS is further equipped with a tailored semi-supervised training strategy, which unleashes the potential of large-scale multi-view data and enables continuous self-evolution without requiring ground-truth correspondences. Extensive experiments conducted on three public datasets validate the effectiveness of TwInS's core components and demonstrate its superior performance over existing state-of-the-art approaches. The source code will be made publicly available upon publication.

</details>


### [47] [AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting](https://arxiv.org/abs/2602.13600)
*Jiacheng Zhang,Feng Liu,Chao Du,Tianyu Pang*

Main category: cs.CV

TL;DR: 提出AdaVBoost：自适应逐token视觉注意力增强，通过VGE估计幻觉风险，高风险强增强、低风险弱增强，在多LVLM与基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM视觉注意力增强方法用预设缩放系数对特定视觉token加权，存在步骤依赖性：同一固定系数在某些生成步太弱无法抑制幻觉，在另一些步又过强诱发新幻觉。需要一种能按步、按token自适应调整增强强度的方法。

Method: 提出AdaVBoost框架：在自回归生成过程中逐token计算“视觉落点熵”(VGE)，结合视觉指向/对齐信号与常规熵以衡量证据不匹配与不确定性；以VGE作为风险度量，对高风险token施加强视觉注意力放大，对低风险token施加弱放大，实现细粒度自适应干预。

Result: 在多种LVLM和多项幻觉评测基准上，AdaVBoost显著优于固定缩放的基线方法，降低幻觉率并提升回答一致性与可靠性（文中报告“显著”的整体增益）。

Conclusion: 固定缩放的视觉注意力增强存在固有权衡；引入以VGE为导向的逐token自适应增强能有效缓解LVLM幻觉，并在多模型多基准上稳定收益。

Abstract: Visual attention boosting has emerged as a promising direction for mitigating hallucinations in Large Vision-Language Models (LVLMs), where existing methods primarily focus on where to boost by applying a predefined scaling to the attention of method-specific visual tokens during autoregressive generation. In this paper, we identify a fundamental trade-off in these methods: a predefined scaling factor can be too weak at some generation steps, leaving hallucinations unresolved, yet too strong at others, leading to new hallucinations. Motivated by this finding, we propose AdaVBoost, a token-level visual attention boosting framework that adaptively determines how much attention to boost at each generation step. Specifically, we introduce Visual Grounding Entropy (VGE) to estimate hallucination risk, which leverages visual grounding as a complementary signal to capture evidence mismatches beyond entropy. Guided by VGE, AdaVBoost applies stronger visual attention boosting to high-risk tokens and weaker boosting to low-risk tokens, enabling token-level adaptive intervention at each generation step. Extensive experiments show that AdaVBoost significantly outperforms baseline methods across multiple LVLMs and hallucination benchmarks.

</details>


### [48] [Towards Sparse Video Understanding and Reasoning](https://arxiv.org/abs/2602.13602)
*Chenwei Xu,Zhen Ye,Shang Wu,Weijian Li,Zihan Wang,Zhuofan Xia,Lie Lu,Pranav Maneriker,Fan Du,Manling Li,Han Liu*

Main category: cs.CV

TL;DR: 提出REVISE：一种用于视频问答的多轮代理，稀疏挑选少量关键帧、跨轮维护摘要状态，并在有把握时提前停止；可即插即用地支持专有VLM，也可用于开源模型的强化微调。配套无标注奖励EAGER由置信增益、摘要充分性和“正确且早停”三部分构成。结果：在多项VQA基准上同时提升准确率并减少帧数、轮数与提示token，体现高效稀疏视频推理。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答常均匀采样大量帧，计算与提示开销高且含大量冗余，缺少能有效利用稀疏证据、可与不同VLM对接、并能在无标注下进行强化微调的通用方案。

Method: REVISE多轮选择信息量大的新帧，并维护“摘要即状态”用于后续轮次决策；在有足够把握时提前停止回答。对专有VLM以即插即用方式运行；对开源模型以强化学习微调。提出无标注奖励EAGER：1) 置信增益：新增帧后正确选项与最强替代项的对数几率差的提升；2) 摘要充分性：仅用最终摘要复问并答对则奖励；3) 正确且早停：在小轮次预算内正确作答获得奖励。

Result: 在多个视频问答基准上，REVISE相较基线提升准确率，同时显著减少输入帧数、对话轮数和提示token使用量。

Conclusion: 稀疏帧选择与摘要驱动的多轮推理结合无标注强化信号，可在不依赖密集帧的情况下实现高效且准确的视频问答，并具备对专有与开源VLM的广泛适配性。

Abstract: We present \revise (\underline{Re}asoning with \underline{Vi}deo \underline{S}parsity), a multi-round agent for video question answering (VQA). Instead of uniformly sampling frames, \revise selects a small set of informative frames, maintains a summary-as-state across rounds, and stops early when confident. It supports proprietary vision-language models (VLMs) in a ``plug-and-play'' setting and enables reinforcement fine-tuning for open-source models. For fine-tuning, we introduce EAGER (Evidence-Adjusted Gain for Efficient Reasoning), an annotation-free reward with three terms: (1) Confidence gain: after new frames are added, we reward the increase in the log-odds gap between the correct option and the strongest alternative; (2) Summary sufficiency: at answer time we re-ask using only the last committed summary and reward success; (3) Correct-and-early stop: answering correctly within a small turn budget is rewarded. Across multiple VQA benchmarks, \revise improves accuracy while reducing frames, rounds, and prompt tokens, demonstrating practical sparse video reasoning.

</details>


### [49] [A generalizable foundation model for intraoperative understanding across surgical procedures](https://arxiv.org/abs/2602.13633)
*Kanggil Park,Yongjun Jeon,Soyoung Lim,Seonmin Park,Jongmin Shin,Jung Yong Kim,Sehyeon An,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: 提出ZEN：面向术中视频理解的通用基础模型，基于自监督多教师蒸馏，训练于超400万帧、21+类手术；在20个下游任务与多种数据效率设定中均优于现有模型，展现跨手术稳健泛化，推进统一表征与临床辅助应用。


<details>
  <summary>Details</summary>
Motivation: 术中视觉判断主导微创外科决策，但不同术者/术式差异大，导致评估与训练不一致；现有外科AI多为窄任务、弱泛化，难跨手术/机构迁移。需要一个在多手术场景中可泛化的统一表征模型，支撑评估、导航与训练。

Method: 构建大规模多样化术中视频数据集（>400万帧，21+术式）；在统一基准下系统比较表征学习策略；提出ZEN：采用自监督学习结合多教师蒸馏框架，训练获得通用视觉表征；在全量微调、冻结骨干、少样本与零样本等设置下进行评测。

Result: ZEN在20个下游任务上总体优于现有外科基础模型；在全微调、冻结、few-shot、zero-shot四类设置中均保持领先，并在跨手术任务上展现稳健泛化能力。

Conclusion: ZEN为术中场景理解提供了更统一且可迁移的表征，向跨手术通用模型迈进，可支持未来术中辅助与外科训练评估等应用。

Abstract: In minimally invasive surgery, clinical decisions depend on real-time visual interpretation, yet intraoperative perception varies substantially across surgeons and procedures. This variability limits consistent assessment, training, and the development of reliable artificial intelligence systems, as most surgical AI models are designed for narrowly defined tasks and do not generalize across procedures or institutions. Here we introduce ZEN, a generalizable foundation model for intraoperative surgical video understanding trained on more than 4 million frames from over 21 procedures using a self-supervised multi-teacher distillation framework. We curated a large and diverse dataset and systematically evaluated multiple representation learning strategies within a unified benchmark. Across 20 downstream tasks and full fine-tuning, frozen-backbone, few-shot and zero-shot settings, ZEN consistently outperforms existing surgical foundation models and demonstrates robust cross-procedure generalization. These results suggest a step toward unified representations for surgical scene understanding and support future applications in intraoperative assistance and surgical training assessment.

</details>


### [50] [Layer-Guided UAV Tracking: Enhancing Efficiency and Occlusion Robustness](https://arxiv.org/abs/2602.13636)
*Yang Zhou,Derui Ding,Ran Sun,Ying Sun,Haohua Zhang*

Main category: cs.CV

TL;DR: LGTrack是一种面向无人机视觉跟踪的统一框架，通过轻量全局分组坐标注意(GGCA)与相似度引导层自适应(SGLA)实现高效且鲁棒的跟踪，在保持精度的同时达到实时甚至超实时速度。


<details>
  <summary>Details</summary>
Motivation: UAV场景下跟踪需在机载算力受限条件下兼顾精度与效率，并应对遮挡、快速运动与背景干扰；现有方法在复杂场景下易退化，或依赖耗时的蒸馏与重型注意模块。

Method: 1) 设计轻量的Global-Grouped Coordinate Attention(GGCA)以捕获长程依赖与全局上下文，增强判别特征且计算开销小；2) 提出Similarity-Guided Layer Adaptation(SGLA)进行动态层选择/适配，以样本相似度指导特征层使用，替代知识蒸馏；3) 统一成LGTrack框架，结合高效特征增强与遮挡鲁棒表征学习。

Result: 在三个数据集上验证：在UAVDT上实现258.7 FPS的实时速度，精度82.8%（precision），同时在其余基准上也具备竞争性精度，综合性能达SOTA或接近SOTA。

Conclusion: LGTrack在UAV跟踪中有效缓解精度-效率权衡：以轻量注意与相似度引导层自适应实现对遮挡与复杂场景的鲁棒追踪，并以极高帧率运行，具备工程落地潜力。

Abstract: Visual object tracking (VOT) plays a pivotal role in unmanned aerial vehicle (UAV) applications. Addressing the trade-off between accuracy and efficiency, especially under challenging conditions like unpredictable occlusion, remains a significant challenge. This paper introduces LGTrack, a unified UAV tracking framework that integrates dynamic layer selection, efficient feature enhancement, and robust representation learning for occlusions. By employing a novel lightweight Global-Grouped Coordinate Attention (GGCA) module, LGTrack captures long-range dependencies and global contexts, enhancing feature discriminability with minimal computational overhead. Additionally, a lightweight Similarity-Guided Layer Adaptation (SGLA) module replaces knowledge distillation, achieving an optimal balance between tracking precision and inference efficiency. Experiments on three datasets demonstrate LGTrack's state-of-the-art real-time speed (258.7 FPS on UAVDT) while maintaining competitive tracking accuracy (82.8\% precision). Code is available at https://github.com/XiaoMoc/LGTrack

</details>


### [51] [DCDM: Divide-and-Conquer Diffusion Models for Consistency-Preserving Video Generation](https://arxiv.org/abs/2602.13637)
*Haoyu Zhao,Yuang Zhang,Junqi Cheng,Jiaxi Gu,Zenghui Lu,Peng Shu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出Divide-and-Conquer Diffusion Model (DCDM)，将视频一致性问题拆分为三类（片内语义/世界知识一致、片间相机一致、镜头间元素一致），在统一扩散生成骨干下用专门模块分别解决，并在AAAI’26 CVM测试集验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽具备高保真度，但在语义、几何与身份一致性上表现不稳；跨时间、跨镜头与相机运动控制缺乏精确性与可控性，难以支撑长时序与多镜头叙事需求。

Method: 系统级“分而治之”框架：1) 片内一致性：用大语言模型将文本提示解析为结构化语义表示，再由扩散Transformer生成一致视频；2) 片间相机一致性：在噪声空间建立时间相机表示以稳定控制相机运动，并引入文生图初始化增强可控性；3) 镜头间一致性：整体场景生成范式，采用窗口化跨注意力与稀疏镜头间自注意力，实现长程叙事一致且计算高效；共享统一视频扩散骨干。

Result: 在AAAI’26 CVM竞赛测试集上，所提策略有效提升片内语义/知识一致、片间相机运动稳定性与镜头间元素一致性，实验结果验证方法有效。

Conclusion: 通过分而治之的模块化一致性建模与统一扩散骨干，DCDM显著改善长时序、多镜头视频生成中的一致性与可控性，具备良好的可扩展性与计算效率。

Abstract: Recent video generative models have demonstrated impressive visual fidelity, yet they often struggle with semantic, geometric, and identity consistency. In this paper, we propose a system-level framework, termed the Divide-and-Conquer Diffusion Model (DCDM), to address three key challenges: (1) intra-clip world knowledge consistency, (2) inter-clip camera consistency, and (3) inter-shot element consistency. DCDM decomposes video consistency modeling under these scenarios into three dedicated components while sharing a unified video generation backbone. For intra-clip consistency, DCDM leverages a large language model to parse input prompts into structured semantic representations, which are subsequently translated into coherent video content by a diffusion transformer. For inter-clip camera consistency, we propose a temporal camera representation in the noise space that enables precise and stable camera motion control, along with a text-to-image initialization mechanism to further enhance controllability. For inter-shot consistency, DCDM adopts a holistic scene generation paradigm with windowed cross-attention and sparse inter-shot self-attention, ensuring long-range narrative coherence while maintaining computational efficiency. We validate our framework on the test set of the CVM Competition at AAAI'26, and the results demonstrate that the proposed strategies effectively address these challenges.

</details>


### [52] [KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination](https://arxiv.org/abs/2602.13650)
*Byungjin Choi,Seongsu Bae,Sunjun Kweon,Edward Choi*

Main category: cs.CV

TL;DR: KorMedMCQA-V 是一个韩语医考风格的多模态选择题基准，含1534题与2043张临床图像（约30%多图）。评测50+视觉语言模型，零样本下专有模型最好（96.9%），开源次之（83.7%），韩语专门模型落后（43.2%）。推理型模型显著优于指令微调型，医学专域化收益不稳定，多图题普遍掉分，模态间差异明显。与文本版KorMedMCQA互补，构成统一韩语医学推理评测套件。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多为英语与通用领域，缺乏针对韩语医学、尤其是执照考试风格与多模态（含多图整合）的系统评测基准，难以客观比较专有、开源、通用与医学/韩语专门模型的真实临床推理能力。

Method: 从2012-2023年韩国医师国家考试收集构建多模态多选题，涵盖X光、CT、ECG、超声、内镜等，并有约30%多图题；在统一零样本协议下，对50+专有与开源VLM（通用、医学专门、韩语专门、以及“思维/推理”变体）进行对比评测；按题型（单/多图）与模态细分分析。

Result: 最佳专有模型Gemini-3.0-Pro达96.9%准确率；最佳开源Qwen3-VL-32B-Thinking为83.7%；最佳韩语专门VARCO-VISION-2.0-14B仅43.2%。推理导向变体可较指令微调版提升最高约20个百分点；医学专门化相较强通用模型收益不稳定；所有模型在多图题上性能下降，且不同影像模态间差异显著。

Conclusion: KorMedMCQA-V为韩语医学多模态推理提供了标准化、难度分层的基准，揭示了多图整合与模态差异是当前VLM薄弱环节；强通用与推理型模型在零样本下具备领先性能，而单纯语言/领域专门化并不保证收益。该数据集与文本版KorMedMCQA互补，形成统一评测套件并已在Hugging Face开放。

Abstract: We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.

</details>


### [53] [Optimizing Point-of-Care Ultrasound Video Acquisition for Probabilistic Multi-Task Heart Failure Detection](https://arxiv.org/abs/2602.13658)
*Armin Saadat,Nima Hashemi,Bahar Khodabakhshian,Michael Y. Tsang,Christina Luong,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 提出一种在床旁超声POCUS场景下的个性化序贯采集策略：RL代理根据已获取的部分多视图视频决定下一视图或终止；终止后用共享多视图Transformer同时评估主动脉瓣狭窄（AS）分级与左室射血分数（LVEF），并输出不确定性；在1820例测试中以少32%视频匹配全量性能（两任务均衡准确率均值77.2%）。


<details>
  <summary>Details</summary>
Motivation: POCUS需要在有限时间与操作者精力下迅速获得足以支持心衰相关决策的关键信息。传统固定协议或全量采集耗时且不一定提高诊断收益；需要面向个体、能权衡采集成本与诊断价值的智能采集策略，并能同时评估多心脏指标并量化不确定性。

Method: 将POCUS建模为序贯决策问题：强化学习视频选择器在每一步基于已观察到的多视图视频选择下一视图或终止采集；终止时，使用共享多视图Transformer进行多任务推断：AS为序数分类、LVEF为回归，输出高斯预测分布，转化为AS等级与EF阈值的概率。以这些概率构建回报函数，平衡期望诊断收益与采集成本，从而得到病人特异的采集路径。

Result: 在12,180例患者层面研究（训练/验证/测试=75/15/15）上评估；于1,820例测试集中，在减少约32%视频的情况下，整体性能与使用全量研究相当，AS分级与LVEF估计的均衡准确率均值为77.2%，表现出在采集预算下的稳健多任务性能。

Conclusion: 该患者定制、成本感知的采集框架可在维持决策质量的同时精简POCUS工作流程，生成可解释的采集路径，适合床旁应用；方法可扩展至更多心脏终点，值得前瞻性研究以促进临床落地。

Abstract: Purpose: Echocardiography with point-of-care ultrasound (POCUS) must support clinical decision-making under tight bedside time and operator-effort constraints. We introduce a personalized data acquisition strategy in which an RL agent, given a partially observed multi-view study, selects the next view to acquire or terminates acquisition to support heart-failure (HF) assessment. Upon termination, a diagnostic model jointly predicts aortic stenosis (AS) severity and left ventricular ejection fraction (LVEF), two key HF biomarkers, and outputs uncertainty, enabling an explicit trade-off between diagnostic performance and acquisition cost. Methods: We model POCUS as a sequential acquisition problem: at each step, a video selector (RL agent) chooses the next view to acquire or terminates acquisition. Upon termination, a shared multi-view transformer performs multi-task inference with two heads, ordinal AS classification, and LVEF regression, and outputs Gaussian predictive distributions yielding ordinal probabilities over AS classes and EF thresholds. These probabilities drive a reward that balances expected diagnostic benefit against acquisition cost, producing patient-specific acquisition pathways. Results: The dataset comprises 12,180 patient-level studies, split into training/validation/test sets (75/15/15). On the 1,820 test studies, our method matches full-study performance while using 32% fewer videos, achieving 77.2% mean balanced accuracy (bACC) across AS severity classification and LVEF estimation, demonstrating robust multi-task performance under acquisition budgets. Conclusion: Patient-tailored, cost-aware acquisition can streamline POCUS workflows while preserving decision quality, producing interpretable scan pathways suited to bedside use. The framework is extensible to additional cardiac endpoints and merits prospective evaluation for clinical integration.

</details>


### [54] [LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases](https://arxiv.org/abs/2602.13662)
*Khang Nguyen Quoc,Phuong D. Dao,Luyl-Da Quach*

Main category: cs.CV

TL;DR: 提出LeafNet数据集与LeafBench基准，用于评估/提升VLM在植物病理任务上的能力；实验表明多模态模型在细粒度诊断上优于视觉-only，但仍存在显著性能短板。


<details>
  <summary>Details</summary>
Motivation: 通用VLM虽强，但在农业领域（尤其是植物病害诊断）受限于缺乏大规模、多模态图文数据与系统化基准，导致模型难以可靠落地。

Method: 构建包含18.6万张叶片图像、97类病害及相关元数据的多模态数据集LeafNet；基于此设计LeafBench VQA基准，生成13,950个覆盖六类农业关键任务的问题，考察症状识别、分类学关系与诊断推理等；以12个SOTA VLM进行系统评测，并与纯视觉模型对比，含微调设置。

Result: 任务间性能差异明显：健康/患病二分类>90%准确率；病原体与物种的细粒度识别<65%；微调后的VLM显著优于传统视觉模型，显示语言表示的融合作为多模态架构的关键优势。

Conclusion: 当前VLM在植物病理细粒度理解与诊断仍存在显著缺口；LeafBench为方法改进与进展评估提供严格框架，推动可靠的AI植物病害诊断研究。

Abstract: Foundation models and vision-language pre-training have significantly advanced Vision-Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their application in domain-specific agricultural tasks, such as plant pathology, remains limited due to the lack of large-scale, comprehensive multimodal image--text datasets and benchmarks. To address this gap, we introduce LeafNet, a comprehensive multimodal dataset, and LeafBench, a visual question-answering benchmark developed to systematically evaluate the capabilities of VLMs in understanding plant diseases. The dataset comprises 186,000 leaf digital images spanning 97 disease classes, paired with metadata, generating 13,950 question-answer pairs spanning six critical agricultural tasks. The questions assess various aspects of plant pathology understanding, including visual symptom recognition, taxonomic relationships, and diagnostic reasoning. Benchmarking 12 state-of-the-art VLMs on our LeafBench dataset, we reveal substantial disparity in their disease understanding capabilities. Our study shows performance varies markedly across tasks: binary healthy--diseased classification exceeds 90\% accuracy, while fine-grained pathogen and species identification remains below 65\%. Direct comparison between vision-only models and VLMs demonstrates the critical advantage of multimodal architectures: fine-tuned VLMs outperform traditional vision models, confirming that integrating linguistic representations significantly enhances diagnostic precision. These findings highlight critical gaps in current VLMs for plant pathology applications and underscore the need for LeafBench as a rigorous framework for methodological advancement and progress evaluation toward reliable AI-assisted plant disease diagnosis. Code is available at https://github.com/EnalisUs/LeafBench.

</details>


### [55] [EchoTorrent: Towards Swift, Sustained, and Streaming Multi-Modal Video Generation](https://arxiv.org/abs/2602.13669)
*Rang Meng,Weipeng Wu,Yingjie Yin,Yuming Li,Chenguang Ma*

Main category: cs.CV

TL;DR: EchoTorrent提出一套面向流式多模态视频生成的四件套：多教师顺序蒸馏、ACC-DMD自适应CFG校准、混合长尾强制对齐、以及VAE解码器像素域精修，实现少次自回归、延长时序一致性并改进身份和唇形同步，显著降低流式推理退化与延迟。


<details>
  <summary>Details</summary>
Motivation: 现有多模态视频生成虽画质高，但推理延迟大、时序稳定性差；流式推理更会放大多模态退化（模糊、漂移、唇形不同步），形成效率-性能难两全的矛盾，急需兼顾实时性与质量的方案。

Method: 四部分：1）多教师训练：在不同偏好域微调得到专家，按序将域知识迁移给学生；2）ACC-DMD：在扩散模型中用分阶段时空调度校准音频CFG增益误差，去除冗余CFG计算，实现每步单次前向；3）混合长尾强制：在长时自滚动训练中，仅对尾帧做对齐约束，并采用因果-双向混合架构以稳住流式时空质量并贴合参考帧；4）VAE解码器精修：在像素域优化VAE解码器，恢复高频细节，规避潜空间歧义。

Result: 在广泛实验中实现少次自回归生成，显著延长时序一致性、提升身份保持与音频-唇形同步，同时降低流式模式下的空间模糊与漂移，推理更高效。

Conclusion: EchoTorrent在保持实时效率的同时缓解流式多模态视频生成的核心退化问题，提供兼顾时空稳定、身份一致和唇形对齐的实用方案。

Abstract: Recent multi-modal video generation models have achieved high visual quality, but their prohibitive latency and limited temporal stability hinder real-time deployment. Streaming inference exacerbates these issues, leading to pronounced multimodal degradation, such as spatial blurring, temporal drift, and lip desynchronization, which creates an unresolved efficiency-performance trade-off. To this end, we propose EchoTorrent, a novel schema with a fourfold design: (1) Multi-Teacher Training fine-tunes a pre-trained model on distinct preference domains to obtain specialized domain experts, which sequentially transfer domain-specific knowledge to a student model; (2) Adaptive CFG Calibration (ACC-DMD), which calibrates the audio CFG augmentation errors in DMD via a phased spatiotemporal schedule, eliminating redundant CFG computations and enabling single-pass inference per step; (3) Hybrid Long Tail Forcing, which enforces alignment exclusively on tail frames during long-horizon self-rollout training via a causal-bidirectional hybrid architecture, effectively mitigates spatiotemporal degradation in streaming mode while enhancing fidelity to reference frames; and (4) VAE Decoder Refiner through pixel-domain optimization of the VAE decoder to recover high-frequency details while circumventing latent-space ambiguities. Extensive experiments and analysis demonstrate that EchoTorrent achieves few-pass autoregressive generation with substantially extended temporal consistency, identity preservation, and audio-lip synchronization.

</details>


### [56] [An Ensemble Learning Approach towards Waste Segmentation in Cluttered Environment](https://arxiv.org/abs/2602.13681)
*Maimoona Jafar,Syed Imran Ali,Ahsan Saadat,Muhammad Bilal,Shah Khalid*

Main category: cs.CV

TL;DR: 论文提出将U-Net与FPN通过加权平均进行集成（EL-4），用于复杂真实场景下的垃圾分割，在真实感数据集上取得IoU 0.8306、Dice损失0.09019，优于单模型（U-Net IoU 0.8065，FPN Dice损失0.1183）。


<details>
  <summary>Details</summary>
Motivation: 实际固废分拣需要精准的分割掩码以驱动机器人抓取，但真实垃圾环境中物体形变、无固定纹理且相互重叠，单一分割模型在边界细节与多尺度上下文上各有短板，影响回收分拣效率与自动化水平。

Method: 构建集成学习框架：选取在分割任务中互补的U-Net（边界与细节强）与FPN（多尺度与上下文强），对两者输出的分割概率图进行加权平均生成最终掩码；同时使用贴近真实物料回收场景的数据集并进行预处理以增强特征学习。

Result: 所提EL-4在测试中取得IoU 0.8306，较U-Net的0.8065提升；Dice损失降至0.09019，优于FPN的0.1183，表明在边界精细度与多尺度鲁棒性上实现互补增益。

Conclusion: U-Net与FPN的加权集成能在复杂垃圾分割中显著提升精度，支持物料回收设施的自动化分拣，减少人工、提高吞吐与原料获取质量。

Abstract: Environmental pollution is a critical global issue, with recycling emerging as one of the most viable solutions. This study focuses on waste segregation, a crucial step in recycling processes to obtain raw material. Recent advancements in computer vision have significantly contributed to waste classification and recognition. In waste segregation, segmentation masks are essential for robots to accurately localize and pick objects from conveyor belts. The complexity of real-world waste environments, characterized by deformed items without specific patterns and overlapping objects, further complicates waste segmentation tasks. This paper proposes an Ensemble Learning approach to improve segmentation accuracy by combining high performing segmentation models, U-Net and FPN, using a weighted average method. U-Net excels in capturing fine details and boundaries in segmentation tasks, while FPN effectively handles scale variation and context in complex environments, and their combined masks result in more precise predictions. The dataset used closely mimics real-life waste scenarios, and preprocessing techniques were applied to enhance feature learning for deep learning segmentation models. The ensemble model, referred to as EL-4, achieved an IoU value of 0.8306, an improvement over U-Net's 0.8065, and reduced Dice loss to 0.09019 from FPN's 0.1183. This study could contribute to the efficiency of waste sorting at Material Recovery Facility, facilitating better raw material acquisition for recycling with minimal human intervention and enhancing the overall throughput.

</details>


### [57] [A WDLoRA-Based Multimodal Generative Framework for Clinically Guided Corneal Confocal Microscopy Image Synthesis in Diabetic Neuropathy](https://arxiv.org/abs/2602.13693)
*Xin Zhang,Liangxiu Han,Yue Shi,Yalin Zheng,Uazman Alam,Maryam Ferdousi,Rayaz Malik*

Main category: cs.CV

TL;DR: 提出一种基于WDLoRA的多模态生成框架，用神经分割掩码与临床提示共同条件化，生成解剖一致、临床可信的角膜共焦显微图像，优于GAN与标准扩散模型，并能提升下游DPN诊断与分割性能。


<details>
  <summary>Details</summary>
Motivation: CCM可敏感评估DPN小纤维损伤，但标注数据稀缺、神经形态差异精细，限制了深度学习诊断模型。通用生成模型在医学影像上因领域训练不足而难以保持解剖与临床真实性，需一种参数高效且兼顾结构与对比度的微调方法来合成高保真、可控的医学图像。

Method: 提出Weight-Decomposed Low-Rank Adaptation（WDLoRA）作为参数高效微调机制，将权重更新分解为幅值与方向两部分，分别学习神经拓扑（方向）与基质对比（强度）。在分割掩码与疾病特异临床文本提示（Control、T1NoDPN、T1DPN）双重条件下，进行扩散式多模态图像生成，并通过三支柱评估（视觉保真、结构完整、临床一致）。

Result: 在CCM合成上实现SOTA：FID=5.18、SSIM=0.630，显著优于GAN与常规扩散基线；合成图像在金标准临床生物标志方面与真实数据统计等价；用合成数据训练可将自动诊断准确率提升2.1%，分割性能提升2.2%。

Conclusion: WDLoRA多模态框架能生成兼具解剖一致性与临床可信度的CCM图像，缓解医学AI数据瓶颈并实证提升下游诊断与分割任务表现，显示在DPN影像合成与辅助建模中的实用潜力。

Abstract: Corneal Confocal Microscopy (CCM) is a sensitive tool for assessing small-fiber damage in Diabetic Peripheral Neuropathy (DPN), yet the development of robust, automated deep learning-based diagnostic models is limited by scarce labelled data and fine-grained variability in corneal nerve morphology. Although Artificial Intelligence (AI)-driven foundation generative models excel at natural image synthesis, they often struggle in medical imaging due to limited domain-specific training, compromising the anatomical fidelity required for clinical analysis. To overcome these limitations, we propose a Weight-Decomposed Low-Rank Adaptation (WDLoRA)-based multimodal generative framework for clinically guided CCM image synthesis. WDLoRA is a parameter-efficient fine-tuning (PEFT) mechanism that decouples magnitude and directional weight updates, enabling foundation generative models to independently learn the orientation (nerve topology) and intensity (stromal contrast) required for medical realism. By jointly conditioning on nerve segmentation masks and disease-specific clinical prompts, the model synthesises anatomically coherent images across the DPN spectrum (Control, T1NoDPN, T1DPN). A comprehensive three-pillar evaluation demonstrates that the proposed framework achieves state-of-the-art visual fidelity (Fréchet Inception Distance (FID): 5.18) and structural integrity (Structural Similarity Index Measure (SSIM): 0.630), significantly outperforming GAN and standard diffusion baselines. Crucially, the synthetic images preserve gold-standard clinical biomarkers and are statistically equivalent to real patient data. When used to train automated diagnostic models, the synthetic dataset improves downstream diagnostic accuracy by 2.1% and segmentation performance by 2.2%, validating the framework's potential to alleviate data bottlenecks in medical AI.

</details>


### [58] [Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images](https://arxiv.org/abs/2602.13712)
*Chan Hao Sien,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CV

TL;DR: 利用微軟 Florence 等視覺語言模型微調做寄生蟲卵在顯微影像中的定位，mIOU 達 0.94，優於 EfficientDet，顯示可作為自動化寄生蟲學診斷核心組件的潛力。


<details>
  <summary>Details</summary>
Motivation: STH（經土傳播線蟲）感染廣泛流行於熱帶與亞熱帶，但當地常缺乏專業顯微診斷資源。傳統人工顯微鏡辨識雖為金標準，但費力、耗時且易受人為誤差影響，因此需要可擴展、準確且自動化的影像診斷方案。

Method: 採用視覺語言模型（以 Microsoft Florence 為例），對其進行微調以在顯微影像中定位各類寄生蟲卵，並與主流物件偵測方法（如 EfficientDet）比較定位效能，使用 mIOU 作為評估指標。

Result: 微調後的定位型 VLM 在顯微影像上達到 mIOU 0.94，表現優於 EfficientDet 等傳統物件偵測器。

Conclusion: VLM 可作為智慧化寄生蟲學診斷框架的核心元件，具工程可擴展性與臨床轉譯潛力，有望降低人力負擔並提升診斷一致性。

Abstract: Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.

</details>


### [59] [RGA-Net: A Vision Enhancement Framework for Robotic Surgical Systems Using Reciprocal Attention Mechanisms](https://arxiv.org/abs/2602.13726)
*Quanjun Li,Weixuan Li,Han Xia,Junhua Zhou,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出RGA-Net用于手术烟雾去除，结合双流混合注意力与轴向分解注意力，通过互递门控在编解码间双向调制特征，在两外科数据集上达到更优清晰度，提升机器人手术可视化与安全性。


<details>
  <summary>Details</summary>
Motivation: 机器人微创手术高度依赖清晰内镜视频；能量器械产生的手术烟雾导致画面浑浊、对比度下降与散射伪影，削弱人机交互与操作精度，潜在增加医源性风险。现有去烟/去雾方法难以处理外科场景中高密度、非均匀分布与复杂散射光照变化的问题，且需兼顾实时性与细节保真。

Method: 构建分层编码-解码框架RGA-Net，核心包括：1) 双流混合注意力DHA：结合Shifted Window自注意力与频域分支，既抓取局部器械/组织细节又建模全局照明与烟雾分布；2) 轴向分解注意力ADA：将二维注意力因式分解为按轴处理以高效建模多尺度上下文；3) 编码器-解码器间以Reciprocal Cross-Gating双向门控耦合，实现语义与细节的相互调制与信息回流。

Result: 在DesmokeData与LSD3K外科数据集上进行大量实验，RGA-Net在去烟质量指标上优于现有方法，能稳定恢复手术视野清晰度，具备集成到机器人手术流程的潜力。

Conclusion: RGA-Net通过互递门控+注意力融合有效应对手术烟雾的高密度与复杂散射，显著改善内镜可视化与人机界面，潜在降低认知负荷与并发症风险；后续需通过包含外科医生可用性评估的临床试验进一步验证与落地。

Abstract: Robotic surgical systems rely heavily on high-quality visual feedback for precise teleoperation; yet, surgical smoke from energy-based devices significantly degrades endoscopic video feeds, compromising the human-robot interface and surgical outcomes. This paper presents RGA-Net (Reciprocal Gating and Attention-fusion Network), a novel deep learning framework specifically designed for smoke removal in robotic surgery workflows. Our approach addresses the unique challenges of surgical smoke-including dense, non-homogeneous distribution and complex light scattering-through a hierarchical encoder-decoder architecture featuring two key innovations: (1) a Dual-Stream Hybrid Attention (DHA) module that combines shifted window attention with frequency-domain processing to capture both local surgical details and global illumination changes, and (2) an Axis-Decomposed Attention (ADA) module that efficiently processes multi-scale features through factorized attention mechanisms. These components are connected via reciprocal cross-gating blocks that enable bidirectional feature modulation between encoder and decoder pathways. Extensive experiments on the DesmokeData and LSD3K surgical datasets demonstrate that RGA-Net achieves superior performance in restoring visual clarity suitable for robotic surgery integration. Our method enhances the surgeon-robot interface by providing consistently clear visualization, laying a technical foundation for alleviating surgeons' cognitive burden, optimizing operation workflows, and reducing iatrogenic injury risks in minimally invasive procedures. These practical benefits could be further validated through future clinical trials involving surgeon usability assessments. The proposed framework represents a significant step toward more reliable and safer robotic surgical systems through computational vision enhancement.

</details>


### [60] [Explore Intrinsic Geometry for Query-based Tiny and Oriented Object Detector with Momentum-based Bipartite Matching](https://arxiv.org/abs/2602.13728)
*Junpeng Zhang,Zewei Yang,Jie Feng,Yuhui Zheng,Ronghua Shang,Mengxuan Zhang*

Main category: cs.CV

TL;DR: IGOFormer提出一种面向任意朝向目标的查询式检测器，通过几何感知解码器与动量式二分匹配，缓解微小目标与跨阶段匹配不一致问题，在DOTA-V1.0单尺度、Swin-T下达到AP50 78.00%。


<details>
  <summary>Details</summary>
Motivation: 现有query-based检测器在处理任意朝向、尤其是纹理信息稀少的微小目标时受限：1) 像素级解码阶段未充分利用内在几何信息，难以准确建模目标布局与朝向；2) 分阶段二分图匹配导致跨阶段监督不一致，产生冲突信号，影响收敛与稳定性。

Method: 提出IGOFormer：1) 内在几何感知解码器（Intrinsic Geometry-aware Decoder），基于对象query与像素特征的相关性外推互补几何嵌入，并注入解码过程，捕获目标几何布局与朝向；2) 动量式二分匹配（Momentum-based Bipartite Matching），以指数滑动平均聚合历史匹配代价，并为每个query自适应设定平滑系数，缓解跨阶段匹配不一致带来的监督冲突。

Result: 在航拍定向目标检测任务上进行大量实验与消融：在DOTA-V1.0数据集、Swin-T主干、单尺度设定下，AP50达到78.00%，优于同类方法；代码将开源。

Conclusion: 通过显式注入内在几何与稳定跨阶段匹配，IGOFormer显著提升任意朝向与微小目标的检测精度与训练稳定性，为查询式定向检测提供有效范式。

Abstract: Recent query-based detectors have achieved remarkable progress, yet their performance remains constrained when handling objects with arbitrary orientations, especially for tiny objects capturing limited texture information. This limitation primarily stems from the underutilization of intrinsic geometry during pixel-based feature decoding and the occurrence of inter-stage matching inconsistency caused by stage-wise bipartite matching. To tackle these challenges, we present IGOFormer, a novel query-based oriented object detector that explicitly integrates intrinsic geometry into feature decoding and enhances inter-stage matching stability. Specifically, we design an Intrinsic Geometry-aware Decoder, which enhances the object-related features conditioned on an object query by injecting complementary geometric embeddings extrapolated from their correlations to capture the geometric layout of the object, thereby offering a critical geometric insight into its orientation. Meanwhile, a Momentum-based Bipartite Matching scheme is developed to adaptively aggregate historical matching costs by formulating an exponential moving average with query-specific smoothing factors, effectively preventing conflicting supervisory signals arising from inter-stage matching inconsistency. Extensive experiments and ablation studies demonstrate the superiority of our IGOFormer for aerial oriented object detection, achieving an AP$_{50}$ score of 78.00\% on DOTA-V1.0 using Swin-T backbone under the single-scale setting. The code will be made publicly available.

</details>


### [61] [Generative Latent Representations of 3D Brain MRI for Multi-Task Downstream Analysis in Down Syndrome](https://arxiv.org/abs/2602.13731)
*Jordi Malé,Juan Fortea,Mateus Rozalem-Aranha,Neus Martínez-Abadías,Xavier Sevillano*

Main category: cs.CV

TL;DR: 论文训练多种VAE将3D脑MRI编码为紧凑潜变量，并评估重建质量、潜空间结构（PCA）与下游分类（唐氏综合征vs正常）。结果：高保真重建，潜空间可分群，能区分唐氏个体。


<details>
  <summary>Details</summary>
Motivation: 3D脑MRI维度高且生成模型潜变量潜力大，但其结构与可用于临床任务的价值尚缺系统研究。理解潜表示有助于推进神经影像生成与临床决策应用。

Method: 构建多种VAE对3D脑MRI进行编码与重建；三方面评估：1) 定量/定性重建质量；2) 用PCA可视化潜空间结构；3) 在专有数据集上进行唐氏综合征与正常二分类以测试下游可用性。

Result: VAE捕获关键脑部特征并保持较高重建保真度；潜空间在PCA中呈现清晰聚类，特别能将唐氏综合征与对照区分；下游分类表现良好（摘要未给出具体数值）。

Conclusion: VAE学习到的潜表示对3D脑MRI既能优质重建又具判别性，对区分唐氏综合征等临床任务有潜力；潜空间结构具有可解释性与可视化分离特性。

Abstract: Generative models have emerged as powerful tools in medical imaging, enabling tasks such as segmentation, anomaly detection, and high-quality synthetic data generation. These models typically rely on learning meaningful latent representations, which are particularly valuable given the high-dimensional nature of 3D medical images like brain magnetic resonance imaging (MRI) scans. Despite their potential, latent representations remain underexplored in terms of their structure, information content, and applicability to downstream clinical tasks. Investigating these representations is crucial for advancing the use of generative models in neuroimaging research and clinical decision-making. In this work, we develop multiple variational autoencoders (VAEs) to encode 3D brain MRI scans into compact latent space representations for generative and predictive applications. We systematically evaluate the effectiveness of the learned representations through three key analyses: (i) a quantitative and qualitative assessment of MRI reconstruction quality, (ii) a visualisation of the latent space structure using Principal Component Analysis, and (iii) downstream classification tasks on a proprietary dataset of euploid and Down syndrome individuals brain MRI scans. Our results demonstrate that the VAE successfully captures essential brain features while maintaining high reconstruction fidelity. The latent space exhibits clear clustering patterns, particularly in distinguishing individuals with Down syndrome from euploid controls.

</details>


### [62] [T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation](https://arxiv.org/abs/2602.13751)
*Bin Yang,Rong Ou,Weisheng Xu,Jiaqi Xiong,Xintao Li,Taowen Wang,Luyu Zhu,Xu Jiang,Jing Tan,Renjing Xu*

Main category: cs.CV

TL;DR: 提出一个专为OOD文本到动作生成评测的基准：含1025条OOD提示、统一评测框架（三类评估）及对14个基线的系统分析；结果显示多数方法在细粒度准确性上显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有评测多局限于分布内文本和少量指标，难以系统检验模型在复杂OOD文本下的泛化与生成质量；需要更全面、可操作的评估基准来发现方法短板并指导实际应用。

Method: 1) 构建包含1025条描述的OOD提示数据集；2) 设计统一评估框架：a) 基于LLM的文本-动作对齐评估，b) 多因素动作质量评估（如语义对齐、泛化、物理质量等），c) 细粒度准确性评估；3) 对14个代表性基线模型进行系统实验，并据此派生两类数据集用于进一步分析。

Result: 不同模型在语义对齐、泛化能力、物理质量等方面各有所长，但在细粒度准确性评估上整体表现不佳；实验产出两套由评测结果派生的数据集。

Conclusion: 现有方法在OOD场景下存在明显局限，尤其在细粒度动作准确性上。所提基准与框架为未来面向生产级文本到动作模型的设计与评测提供了实证依据与实践指南。

Abstract: Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results. Specifically, we construct an OOD prompt dataset consisting of 1,025 textual descriptions. Based on this prompt dataset, we introduce a unified evaluation framework that integrates LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation. Our experimental results reveal that while different baseline models demonstrate strengths in areas such as text-to-motion semantic alignment, motion generalizability, and physical quality, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation. These findings highlight the limitations of existing methods in OOD scenarios and offer practical guidance for the design and evaluation of future production-level text-to-motion models.

</details>


### [63] [OmniScience: A Large-scale Multi-modal Dataset for Scientific Image Understanding](https://arxiv.org/abs/2602.13758)
*Haoyi Tao,Chaozheng Huang,Nan Wang,Han Lyu,Linfeng Zhang,Guolin Ke,Xi Fang*

Main category: cs.CV

TL;DR: 提出OmniScience：150万科学图像-字幕-上下文三元组的大规模高质多模态数据集，并通过动态模型路由重字幕与严苛质控显著提升图文对齐；在其上微调的小模型在科学图像理解任务上大幅超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在自然图像上强但在科学图像（示意图、实验表征、分析图表等）理解薄弱，尤其开源模型更明显。主要原因是训练数据领域覆盖不足、结构标注粗糙、语义对齐弱，导致模型难以获得高密度、可推理的监督信号。

Method: 构建OmniScience数据集，涵盖10+学科、150万“图-原始字幕-文本上下文”三元组。提出动态模型路由的重字幕流水线：综合视觉特征、原图字幕、正文引用，由多种SOTA多模态大模型按能力路由生成高密度、自洽描述；配合严格质量过滤与与人类专家对齐，确保事实正确与语义完整，并显著提升图文相似度（0.769→0.956）。提出以“Caption QA”作为视觉理解代理评测协议。

Result: 数据与流水线将图文多模态相似度从0.769提升到0.956。用OmniScience微调的Qwen2.5-VL-3B在MM-MT-Bench上提升0.378，在MMMU上提升0.140，显著优于基线。

Conclusion: 高保真、跨学科的大规模科学图像数据与高质量重字幕对齐机制，能明显弥补开源MLLM在科学图像理解上的短板；Caption QA可作为有效的评测代理。OmniScience为训练更强的科学多模态模型提供了数据与方法支撑。

Abstract: Multimodal Large Language Models demonstrate strong performance on natural image understanding, yet exhibit limited capability in interpreting scientific images, including but not limited to schematic diagrams, experimental characterizations, and analytical charts. This limitation is particularly pronounced in open-source MLLMs. The gap largely stems from existing datasets with limited domain coverage, coarse structural annotations, and weak semantic grounding. We introduce OmniScience, a large-scale, high-fidelity multi-modal dataset comprising 1.5 million figure-caption-context triplets, spanning more than 10 major scientific disciplines. To obtain image caption data with higher information density and accuracy for multi-modal large-model training, we develop a dynamic model-routing re-captioning pipeline that leverages state-of-the-art multi-modal large language models to generate dense, self-contained descriptions by jointly synthesizing visual features, original figure captions, and corresponding in-text references authored by human scientists. The pipeline is further reinforced with rigorous quality filtering and alignment with human expert judgments, ensuring both factual accuracy and semantic completeness, and boosts the image-text multi-modal similarity score from 0.769 to 0.956. We further propose a caption QA protocol as a proxy task for evaluating visual understanding. Under this setting, Qwen2.5-VL-3B model finetuned on OmniScience show substantial gains over baselines, achieving a gain of 0.378 on MM-MT-Bench and a gain of 0.140 on MMMU.

</details>


### [64] [SAM4Dcap: Training-free Biomechanical Twin System from Monocular Video](https://arxiv.org/abs/2602.13760)
*Li Wang,HaoYu Wang,Xi Chen,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: SAM4Dcap是一个开源、端到端的单目视频生物力学评估框架，结合SAM-Body4D的人体4D网格恢复与OpenSim求解器，无需额外训练即可从单目视频估计运动学等指标，并将网格转为适配多种肌骨模型的轨迹文件；在步行与下落跳测试中，膝关节运动学接近多视角系统，但髋屈曲与抖动仍有偏差。


<details>
  <summary>Details</summary>
Motivation: 现有高精度生物力学量化依赖昂贵的实验室级光学动捕；多视角视频虽降门槛，但不便于居家场景。亟需一种在单目视频条件下也能获得可靠生物力学指标的方案。

Method: 将具有时间一致性的SAM-Body4D 4D人体网格恢复与OpenSim生物力学求解器整合：1) 从单目视频重建时序一致的人体网格；2) 将网格自动转换为与多样肌骨模型兼容的轨迹文件；3) 通过自动化提示策略与Linux原生构建实现无额外训练的端到端处理。

Result: 在步行与下落跳的初步评估中，框架的膝关节运动学预测与多视角系统可比，但在髋关节屈曲存在差异，且结果仍有残余抖动。

Conclusion: SAM4Dcap将先进的计算机视觉与成熟的生物力学仿真连接，提供了灵活、可及的非实验室运动分析基础；虽已展示潜力，但需改进以降低髋部偏差与时序抖动。

Abstract: Quantitative biomechanical analysis is essential for clinical diagnosis and injury prevention but is often restricted to laboratories due to the high cost of optical motion capture systems. While multi-view video approaches have lowered barriers, they remain impractical for home-based scenarios requiring monocular capture. This paper presents SAM4Dcap, an open-source, end-to-end framework for estimating biomechanical metrics from monocular video without additional training. SAM4Dcap integrates the temporally consistent 4D human mesh recovery of SAM-Body4D with the OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with diverse musculoskeletal models. We introduce automated prompting strategies and a Linux-native build for processing. Preliminary evaluations on walking and drop-jump tasks indicate that SAM4Dcap has the potential to achieve knee kinematic predictions comparable to multi-view systems, although some discrepancies in hip flexion and residual jitter remain. By bridging advanced computer vision with established biomechanical simulation, SAM4Dcap provides a flexible, accessible foundation for non-laboratory motion analysis.

</details>


### [65] [Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking](https://arxiv.org/abs/2602.13772)
*Xiaoyu Li,Yitao Wu,Xian Wu,Haolin Zhuo,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: 提出Offline-Poly：一种与上游检测/跟踪器解耦的离线3D MOT通用方法，基于“Tracking-by-Tracking”范式，仅依赖任意现成跟踪输出，通过预处理、分层匹配融合与轨迹细化三阶段，利用离线的全局优化与未来可观性消除伪轨、重识别片段、跨源关联并细化运动，最终在nuScenes与KITTI上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有离线3D MOT多为在线方法的直接延伸，未充分利用离线条件的计算/时域优势，且强依赖固定上游与定制架构，泛化与适配性差。需要一种能充分用好离线“资源不受限”和“可见未来”属性、并与上游解耦的通用离线跟踪方案。

Method: 提出Tracking-by-Tracking (TBT)范式：只接收一个或多个现成粗跟踪结果作为输入，输出离线优化后的tracklets。管线包含：（1）预处理：用全局场景上下文剔除短期幽灵轨迹并重识别被切断片段；（2）分层匹配与融合：构建场景级相似度，将多源tracklets进行跨源关联与融合；（3）轨迹细化：联合利用局部与全局运动模式进行轨迹平滑与纠正。各模块显式利用离线两大属性——可进行超实时的全局优化与利用完整时域信息进行推理。

Result: 在nuScenes上取得77.6% AMOTA，在KITTI上取得83.00% HOTA，达到或刷新SOTA。消融与综合实验展示了方法在灵活性、泛化性与模块有效性上的优势。

Conclusion: Offline-Poly通过TBT实现与上游解耦的离线3D MOT，充分利用离线的全局与未来信息，显著提升轨迹质量并在主流数据集上达SOTA，证明其通用性与有效性。

Abstract: Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.

</details>


### [66] [Skeleton2Stage: Reward-Guided Fine-Tuning for Physically Plausible Dance Generation](https://arxiv.org/abs/2602.13778)
*Jidong Jia,Youjian Zhang,Huan Fu,Dacheng Tao*

Main category: cs.CV

TL;DR: 用基于网格物理奖励的RL微调扩散模型，弥合骨架到网格的物理差距，显著减少穿插与脚地异常，同时通过反冻结奖励避免动作僵停，生成更逼真舞蹈。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈生成多在骨架域训练，忽视人体网格层面的物理约束，导致在网格可视化下出现自穿插与脚-地接触异常，影响美感与落地应用，需要能在网格层面提升物理可信度的方法。

Method: 从人体网格推导物理奖励，用RL微调扩散舞蹈生成模型。奖励包括：(i) 模仿奖励：基于物理模拟中的可模仿性评估，惩罚自穿插与脚滑；(ii) 脚-地偏差(FGD)奖励，并在测试时加入FGD引导以强化脚-地动态；发现奖励易致动作“冻结”，提出反冻结奖励以保持运动动态。

Result: 在多种舞蹈数据集上，一致性地降低自穿插与脚地异常，提升物理可信度与美学表现，较基线生成更真实自然的舞蹈。

Conclusion: 网格级物理奖励+RLFT能有效弥合骨架到网格的物理鸿沟；结合FGD与反冻结奖励，在维持动态性的同时提升物理可行性，生成更具现实感与审美性的舞蹈。

Abstract: Despite advances in dance generation, most methods are trained in the skeletal domain and ignore mesh-level physical constraints. As a result, motions that look plausible as joint trajectories often exhibit body self-penetration and Foot-Ground Contact (FGC) anomalies when visualized with a human body mesh, reducing the aesthetic appeal of generated dances and limiting their real-world applications. We address this skeleton-to-mesh gap by deriving physics-based rewards from the body mesh and applying Reinforcement Learning Fine-Tuning (RLFT) to steer the diffusion model toward physically plausible motion synthesis under mesh visualization. Our reward design combines (i) an imitation reward that measures a motion's general plausibility by its imitability in a physical simulator (penalizing penetration and foot skating), and (ii) a Foot-Ground Deviation (FGD) reward with test-time FGD guidance to better capture the dynamic foot-ground interaction in dance. However, we find that the physics-based rewards tend to push the model to generate freezing motions for fewer physical anomalies and better imitability. To mitigate it, we propose an anti-freezing reward to preserve motion dynamics while maintaining physical plausibility. Experiments on multiple dance datasets consistently demonstrate that our method can significantly improve the physical plausibility of generated motions, yielding more realistic and aesthetically pleasing dances. The project page is available at: https://jjd1123.github.io/Skeleton2Stage/

</details>


### [67] [Foundation Model-Driven Semantic Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.13780)
*Hengtong Shen,Li Yan,Hong Xie,Yaxuan Wei,Xinhao Li,Wenfei Shen,Peixian Lv,Fei Tan*

Main category: cs.CV

TL;DR: 提出PerASCD：以RS基础模型PerA为核心的语义变化检测方法，配套级联门控解码器与软语义一致性损失，简化范式、增强多尺度语义理解，并在两套基准上达成SOTA，跨多种编码器适配良好。


<details>
  <summary>Details</summary>
Motivation: 现有语义变化检测（SCD）因模型语义理解有限与任务复杂性高，常在性能与范式复杂度间权衡：多分支/多阶段解码器繁琐，训练数值不稳，且难以跨不同遥感基础模型通用。亟需一种既简洁又强健、可充分利用基础模型多尺度语义的SCD方案。

Method: 1) 以PerA为主干（亦评估多种RS基础模型），2) 设计级联门控解码器（CG-Decoder），通过级联与门控机制促进多层特征交互与融合，同时简化解码流水线；3) 提出软语义一致性损失（SSCLoss），缓解SCD训练中的数值不稳定，增强语义一致性；4) 将所提解码器无缝适配不同视觉编码器以统一范式。

Result: CG-Decoder显著简化SCD解码范式，能与多种遥感基础模型无缝对接；在两个公开基准数据集上取得SOTA性能。

Conclusion: PerASCD在不增加范式复杂度的前提下提升多尺度语义理解与整体性能；所提CG-Decoder与SSCLoss稳定、通用且高效，验证了结合RS基础模型进行SCD的有效性与可迁移性。

Abstract: Remote sensing (RS) change detection methods can extract critical information on surface dynamics and are an essential means for humans to understand changes in the earth's surface and environment. Among these methods, semantic change detection (SCD) can more effectively interpret the multi-class information contained in bi-temporal RS imagery, providing semantic-level predictions that support dynamic change monitoring. However, due to the limited semantic understanding capability of the model and the inherent complexity of the SCD tasks, existing SCD methods face significant challenges in both performance and paradigm complexity. In this paper, we propose PerASCD, a SCD method driven by RS foundation model PerA, designed to enhance the multi-scale semantic understanding and overall performance. We introduce a modular Cascaded Gated Decoder (CG-Decoder) that simplifies complex SCD decoding pipelines while promoting effective multi-level feature interaction and fusion. In addition, we propose a Soft Semantic Consistency Loss (SSCLoss) to mitigate the numerical instability commonly encountered during SCD training. We further explore the applicability of multiple existing RS foundation models on the SCD task when equipped with the proposed decoder. Experimental results demonstrate that our decoder not only effectively simplifies the paradigm of SCD, but also achieves seamless adaptation across various vision encoders. Our method achieves state-of-the-art (SOTA) performance on two public benchmark datasets, validating its effectiveness. The code is available at https://github.com/SathShen/PerASCD.git.

</details>


### [68] [Joint Orientation and Weight Optimization for Robust Watertight Surface Reconstruction via Dirichlet-Regularized Winding Fields](https://arxiv.org/abs/2602.13801)
*Jiaze Li,Daisheng Jin,Fei Hou,Junhui Hou,Zheng Liu,Shiqing Xin,Wenping Wang,Ying He*

Main category: cs.CV

TL;DR: 提出DiWR：基于广义绕数场的隐式重建，通过联合优化点法向、面积权重与置信度，在含噪、非均匀采样、含离群点的无向点云上重建闭合水密曲面，并优于多阶段与近期联合法向-重建方法。


<details>
  <summary>Details</summary>
Motivation: 现有从无向、非均匀、含噪点云重建水密曲面的方法常依赖预处理（法向估计、去噪、重采样），对噪声/离群点敏感，且在采样非均匀时稳定性差；同时联合优化与隐式场的统一框架不足。作者希望通过以GWN为目标场并端到端联合优化，提升鲁棒性与质量。

Method: 以广义绕数（GWN）作为目标隐式表示，构造其Dirichlet能量为主目标；在同一优化中同时估计：每点取向（法向方向）、每点面积权重、每点置信系数。辅以基于GWN的额外约束，抑制噪声与离群点、补偿采样不均。无需独立预处理。优化完成后从场中提取水密表面。

Result: 在3D Gaussian Splatting产生的点云、计算机视觉流程输出及被人为污染的图形基准上评测；DiWR在这些具有挑战的数据上生成合理的水密曲面，且定量/定性均优于传统多阶段管线与近期联合取向-重建方法。

Conclusion: 将GWN与Dirichlet能联合、并行优化法向、面积权与置信度可显著提升无向、非均匀、含噪点云的水密重建鲁棒性与质量；方法减少对预处理依赖并在多数据集上达先进性能。

Abstract: We propose Dirichlet Winding Reconstruction (DiWR), a robust method for reconstructing watertight surfaces from unoriented point clouds with non-uniform sampling, noise, and outliers. Our method uses the generalized winding number (GWN) field as the target implicit representation and jointly optimizes point orientations, per-point area weights, and confidence coefficients in a single pipeline. The optimization minimizes the Dirichlet energy of the induced winding field together with additional GWN-based constraints, allowing DiWR to compensate for non-uniform sampling, reduce the impact of noise, and downweight outliers during reconstruction, with no reliance on separate preprocessing. We evaluate DiWR on point clouds from 3D Gaussian Splatting, a computer-vision pipeline, and corrupted graphics benchmarks. Experiments show that DiWR produces plausible watertight surfaces on these challenging inputs and outperforms both traditional multi-stage pipelines and recent joint orientation-reconstruction methods.

</details>


### [69] [Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos](https://arxiv.org/abs/2602.13806)
*Can Li,Jie Gu,Jingmin Chen,Fangzhou Qiu,Lei Sun*

Main category: cs.CV

TL;DR: 提出一种基于多尺度动力学的动态3D高斯序列表征，在仅单目随手视频条件下实现更准确、全局一致的4D重建，并在动态新视角合成任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 严格单目条件下的4D重建高度病态，缺乏足够几何与运动约束；而真实世界的动态具有从物体到粒子层面的多尺度规律，可被利用来缓解歧义并提升物理合理性。

Method: 1) 设计多尺度动力学机制，将复杂运动场分解为多层级（对象/部件/粒子等）可组合的运动；2) 基于此提出“具有多尺度动力学的高斯序列”，通过多层运动组合得到动态3D高斯表示；3) 引入视觉基础模型提供的多模态先验（如语义/深度/光流等）作为互补监督，收缩解空间并提升重建保真度。

Result: 在基准和真实操作数据集上的动态新视角合成任务中，相比现有方法取得显著性能提升，并能从单目随手视频恢复准确且全局一致的4D场景。

Conclusion: 多尺度动力学分解结合多模态先验有效缓解单目4D重建的不适定性，产生物理上更合理的运动与更高保真度的重建，为可扩展的机器人学习提供可靠的动态场景建模手段。

Abstract: Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.

</details>


### [70] [VAR-3D: View-aware Auto-Regressive Model for Text-to-3D Generation via a 3D Tokenizer](https://arxiv.org/abs/2602.13818)
*Zongcheng Han,Dongyan Cao,Haoran Sun,Yu Hong*

Main category: cs.CV

TL;DR: 提出VAR-3D：通过视角感知的3D VQ-VAE与渲染监督的自回归训练，缓解量化与两阶段训练带来的信息丢失与目标不匹配，显著提升文本到3D的质量与对齐。


<details>
  <summary>Details</summary>
Motivation: 文本到3D生成受限于离散3D表征的学习：编码阶段信息丢失在量化前已发生，之后的向量量化放大失真，破坏几何一致性；同时传统两阶段（重建→自回归）训练目标不一致，影响文本条件生成的效果。

Method: 1) 设计视角感知的3D VQ-VAE，将3D模型复杂几何转换为离散token，并引入view-aware机制以减轻跨视角信息丢失与失真；2) 提出渲染监督的训练：把离散token预测与可微渲染的视觉重建耦合，让自回归生成直接受到图像/视图重建信号监督，促进视觉保真与结构一致性；3) 统一端到端地优化，使token学习与文本条件生成目标对齐。

Result: 在实验中，VAR-3D在生成质量与文本-3D对齐度上显著优于现有方法（定量与定性均提升）。

Conclusion: 通过视角感知的离散表示与渲染耦合的训练，VAR-3D缓解了量化引起的表征失真与训练目标不匹配问题，实现更高保真且与文本更一致的3D生成。

Abstract: Recent advances in auto-regressive transformers have achieved remarkable success in generative modeling. However, text-to-3D generation remains challenging, primarily due to bottlenecks in learning discrete 3D representations. Specifically, existing approaches often suffer from information loss during encoding, causing representational distortion before the quantization process. This effect is further amplified by vector quantization, ultimately degrading the geometric coherence of text-conditioned 3D shapes. Moreover, the conventional two-stage training paradigm induces an objective mismatch between reconstruction and text-conditioned auto-regressive generation. To address these issues, we propose View-aware Auto-Regressive 3D (VAR-3D), which intergrates a view-aware 3D Vector Quantized-Variational AutoEncoder (VQ-VAE) to convert the complex geometric structure of 3D models into discrete tokens. Additionally, we introduce a rendering-supervised training strategy that couples discrete token prediction with visual reconstruction, encouraging the generative process to better preserve visual fidelity and structural consistency relative to the input text. Experiments demonstrate that VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment.

</details>


### [71] [Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings](https://arxiv.org/abs/2602.13823)
*Haonan Jiang,Yuji Wang,Yongjie Zhu,Xin Lu,Wenyu Qin,Meng Wang,Pengfei Wan,Yansong Tang*

Main category: cs.CV

TL;DR: 提出一种面向通用多模态表征(UME)的“推理驱动”框架，通过Embedder引导的强化学习(EG-RL)优化Reasoner生成可追溯的推理链(T-CoT)，以提升跨模态检索与匹配效果，并在MMEB-V2与UVRB上超过已有生成式嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成式嵌入方法虽利用CoT提升任务表示，但其推理多停留在纯文本分析，缺乏与检索目标的相关性与多模态证据聚焦，导致跨模态对齐与细粒度匹配不足。作者希望通过将推理过程与嵌入任务直接对齐，改进多模态一致性与泛化。

Method: 提出EG-RL框架：由Embedder对Reasoner提供显式监督信号，强化学习优化Reasoner生成与检索相关的可追溯推理链(T-CoT)。T-CoT从多模态线索中抽取关键证据，聚焦检索相关元素，并将结构化多模态证据信息输入Embedder，实现检索导向对齐与结构化推理结合。

Result: 在有限算力条件下，方法在MMEB-V2与UVRB两个基准上优于领先的生成式嵌入模型，体现更强的跨模态语义一致性、细粒度匹配能力与复杂场景下的泛化能力。

Conclusion: 通过将多模态证据融入结构化推理并以检索为目标对齐，用EG-RL优化Reasoner生成T-CoT，可显著提升多模态嵌入质量，提供一种高效实用的推理驱动UME方案。

Abstract: Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.

</details>


### [72] [Prior-guided Hierarchical Instance-pixel Contrastive Learning for Ultrasound Speckle Noise Suppression](https://arxiv.org/abs/2602.13831)
*Zhenyu Bu,Yuanxin Xie,Guang-Quan Zhou*

Main category: cs.CV

TL;DR: 提出一种先验引导的层级实例-像素对比学习超声去噪模型，通过像素与实例两层面对比学习、结合Transformer编码器与CNN解码器，实现噪声不变且结构感知的表征，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声图像存在斑点噪声，噪声既包含纹理信息又与细微解剖结构耦合，传统去噪方法难以在抑噪与结构保真之间取得平衡，亟需一种能在保结构的同时鲁棒抑噪的学习框架。

Method: 1) 先验/统计引导的像素级对比学习：利用噪声与干净像素的统计差异，最大化其分布可分性，提升局部结构一致性；2) 记忆库支持的实例级对比学习：在特征空间拉近干净分布、远离噪声分布，学习噪声不变表征；3) 混合Transformer-CNN架构：Transformer编码全局上下文，CNN解码细粒度结构，实现长程依赖与局部纹理的互补；4) 在两个公开超声数据集上进行训练与评估。

Result: 在两套公开超声数据集上，所提方法在主流定量指标与定性可视化上均优于现有去噪方法，表现出更好的结构保真与噪声抑制能力。

Conclusion: 层级对比学习结合混合架构可有效学习噪声不变且结构感知的表征，从而实现更优的超声去噪性能，具备推广潜力与临床应用价值。

Abstract: Ultrasound denoising is essential for mitigating speckle-induced degradations, thereby enhancing image quality and improving diagnostic reliability. Nevertheless, because speckle patterns inherently encode both texture and fine anatomical details, effectively suppressing noise while preserving structural fidelity remains a significant challenge. In this study, we propose a prior-guided hierarchical instance-pixel contrastive learning model for ultrasound denoising, designed to promote noise-invariant and structure-aware feature representations by maximizing the separability between noisy and clean samples at both pixel and instance levels. Specifically, a statistics-guided pixel-level contrastive learning strategy is introduced to enhance distributional discrepancies between noisy and clean pixels, thereby improving local structural consistency. Concurrently, a memory bank is employed to facilitate instance-level contrastive learning in the feature space, encouraging representations that more faithfully approximate the underlying data distribution. Furthermore, a hybrid Transformer-CNN architecture is adopted, coupling a Transformer-based encoder for global context modeling with a CNN-based decoder optimized for fine-grained anatomical structure restoration, thus enabling complementary exploitation of long-range dependencies and local texture details. Extensive evaluations on two publicly available ultrasound datasets demonstrate that the proposed model consistently outperforms existing methods, confirming its effectiveness and superiority.

</details>


### [73] [High-Fidelity Causal Video Diffusion Models for Real-Time Ultra-Low-Bitrate Semantic Communication](https://arxiv.org/abs/2602.13837)
*Cem Eteke,Batuhan Tosun,Alexander Griessel,Wolfgang Kellerer,Eckehard Steinbach*

Main category: cs.CV

TL;DR: 提出一种在超低比特率语义通信约束下实现高保真、因果、实时视频生成的扩散模型，通过语义结构+低清纹理流输入，结合语义控制、修复适配器、时序适配器与高效时序蒸馏，实现参数与训练效率大幅改进并在多数据集显著优于多类基线。


<details>
  <summary>Details</summary>
Motivation: 现有视频压缩/生成在极低码率下很难同时保持语义一致性、纹理保真与时序稳定，且难以满足实时与因果合成需求；需要一种能够在受限带宽下可靠传递语义并重建高质量视频的生成框架。

Method: 1) 采用有损语义视频编码传输场景语义结构；2) 额外传输强压缩的低分辨率帧以提供纹理先验；3) 设计模块化视频扩散模型：语义控制模块对齐语义结构，修复适配器进行细节/去噪还原，时序适配器建模跨帧一致性；4) 提出高效时序蒸馏，使模型支持因果与实时合成，显著减少可训练参数与训练时长，同时满足通信码率约束。

Result: 在多个数据集上，于超低码率(<0.0003 bpp)条件下实现高感知质量、语义一致与时序连贯，客观/主观评测均超过传统、神经与生成式基线；参数量减少约300倍、训练时间约减半。

Conclusion: 语义驱动的模块化视频扩散结合低清纹理辅流与时序蒸馏，可在超低带宽下实现高保真、因果、实时的视频生成，并在效率与质量上全面优于现有方法。

Abstract: We introduce a video diffusion model for high-fidelity, causal, and real-time video generation under ultra-low-bitrate semantic communication constraints. Our approach utilizes lossy semantic video coding to transmit the semantic scene structure, complemented by a stream of highly compressed, low-resolution frames that provide sufficient texture information to preserve fidelity. Building on these inputs, we introduce a modular video diffusion model that contains Semantic Control, Restoration Adapter, and Temporal Adapter. We further introduce an efficient temporal distillation procedure that enables extension to real-time and causal synthesis, reducing trainable parameters by 300x and training time by 2x, while adhering to communication constraints. Evaluated across diverse datasets, the framework achieves strong perceptual quality, semantic fidelity, and temporal consistency at ultra-low bitrates (< 0.0003 bpp), outperforming classical, neural, and generative baselines in extensive quantitative, qualitative, and subjective evaluations.

</details>


### [74] [Automated Prediction of Paravalvular Regurgitation before Transcatheter Aortic Valve Implantation](https://arxiv.org/abs/2602.13842)
*Michele Cannito,Riccardo Renzulli,Adson Duarte,Farzad Nikfam,Carlo Alberto Barbano,Enrico Chiesa,Francesco Bruno,Federico Giacobbe,Wojciech Wanha,Arturo Giordano,Marco Grangetto,Fabrizio D'Ascenzo*

Main category: cs.CV

TL;DR: 用术前心脏CT训练3D卷积网络预测TAVI术后旁路性主动脉返流（PVR），结果显示体素级深度学习能提取细微解剖特征，助力个体化风险评估与手术优化。


<details>
  <summary>Details</summary>
Motivation: PVR是TAVI后最常见且影响预后的并发症之一，术前若能从CT预测其风险，可改进患者分层、瓣膜选择与手术策略，减少并发症、提升长期结局。

Method: 收集TAVI术前患者数据，构建各向同性重采样的三维CT体数据；基于3D卷积神经网络进行端到端训练与预测，以体积级输入学习与PVR发生相关的形态学特征；开源代码提供实现细节。

Result: 模型能够从术前CT中识别与PVR风险相关的细微解剖特征，显示出以体积深度学习进行PVR预测的可行性与潜在有效性（具体量化指标未在摘要中给出）。

Conclusion: 三维体数据的深度学习对于PVR术前预测具前景，可用于个体化风险评估与术式优化；需在更大、前瞻性数据集与明确基准上验证并报告量化性能。

Abstract: Severe aortic stenosis is a common and life-threatening condition in elderly patients, often treated with Transcatheter Aortic Valve Implantation (TAVI). Despite procedural advances, paravalvular aortic regurgitation (PVR) remains one of the most frequent post-TAVI complications, with a proven impact on long-term prognosis.
  In this work, we investigate the potential of deep learning to predict the occurrence of PVR from preoperative cardiac CT. To this end, a dataset of preoperative TAVI patients was collected, and 3D convolutional neural networks were trained on isotropic CT volumes. The results achieved suggest that volumetric deep learning can capture subtle anatomical features from pre-TAVI imaging, opening new perspectives for personalized risk assessment and procedural optimization. Source code is available at https://github.com/EIDOSLAB/tavi.

</details>


### [75] [Synthetic Dataset Generation and Validation for Robotic Surgery Instrument Segmentation](https://arxiv.org/abs/2602.13844)
*Giorgio Chiesa,Rossella Borra,Vittorio Lauro,Sabrina De Cillis,Daniele Amparore,Cristian Fiori,Riccardo Renzulli,Marco Grangetto*

Main category: cs.CV

TL;DR: 提出用于机器人手术器械分割的合成数据生成与验证工作流：基于Maya与自动化Python管线生成可标注的拟真视频，含随机运动、光照与血迹纹理；用不同真/合成比例训练分割模型，发现适度混合优于仅真数据，过多合成会引入域移。


<details>
  <summary>Details</summary>
Motivation: 真实手术数据采集与像素级标注昂贵且稀缺，且手术场景多样性强，现有数据难以覆盖；需要一种可扩展、可控、带精确标注的合成数据源来提升模型泛化，并支持数据增广与域适配研究。

Method: 1) 对达芬奇机械臂进行3D重建、在Autodesk Maya中精修并通过Python全自动管线进行动画与渲染；2) 随机化运动轨迹、光照条件与合成血液纹理，生成具像素精度真值掩码的拟真视频序列；3) 设计受控实验，用不同真/合成数据配比训练多种分割模型，评估泛化与域移。

Result: 与仅用真实数据训练相比，真-合成数据的平衡配比显著提升分割模型的泛化能力；但当合成数据占比过高时，性能因域间差异而下降，出现可测量的域移。

Conclusion: 该工作流可复现实验、可扩展，能为手术计算机视觉提供高质量合成数据，促进数据增广、域自适应与仿真预训练等研究；代码与数据已开源，便于社区复用与扩展。

Abstract: This paper presents a comprehensive workflow for generating and validating a synthetic dataset designed for robotic surgery instrument segmentation. A 3D reconstruction of the Da Vinci robotic arms was refined and animated in Autodesk Maya through a fully automated Python-based pipeline capable of producing photorealistic, labeled video sequences. Each scene integrates randomized motion patterns, lighting variations, and synthetic blood textures to mimic intraoperative variability while preserving pixel-accurate ground truth masks. To validate the realism and effectiveness of the generated data, several segmentation models were trained under controlled ratios of real and synthetic data. Results demonstrate that a balanced composition of real and synthetic samples significantly improves model generalization compared to training on real data only, while excessive reliance on synthetic data introduces a measurable domain shift. The proposed framework provides a reproducible and scalable tool for surgical computer vision, supporting future research in data augmentation, domain adaptation, and simulation-based pretraining for robotic-assisted surgery. Data and code are available at https://github.com/EIDOSLAB/Sintetic-dataset-DaVinci.

</details>


### [76] [Cardiac Output Prediction from Echocardiograms: Self-Supervised Learning with Limited Data](https://arxiv.org/abs/2602.13846)
*Adson Duarte,Davide Vitturini,Emanuele Milillo,Andrea Bragagnolo,Carlo Alberto Barbano,Riccardo Renzulli,Michele Cannito,Federico Giacobbe,Francesco Bruno,Ovidio de Filippo,Fabrizio D'Ascenzo,Marco Grangetto*

Main category: cs.CV

TL;DR: 提出一种基于SimCLR的自监督预训练方法，在小规模四腔心超视频数据上学习表征，用于非侵入性心输出量预测；相比直接监督或大规模外部模型，能减轻过拟合并提高相关性（测试集Pearson r≈0.41），显示数据稀缺下SSL的价值。


<details>
  <summary>Details</summary>
Motivation: 心输出量是心血管管理的关键指标，但金标准右心导管检查具侵入性、耗时且资源密集；现有超声法仍受精标数据匮乏与过拟合限制。需要在小数据场景下提升视频表征学习与泛化能力的非侵入式方法。

Method: 在相同、有限的四腔心超视频数据上进行SimCLR式对比学习预训练（数据增强生成正负样本对），随后以该预训练编码器进行下游CO回归微调；评估与监督训练及大规模外部模型PanEcho对比，关注过拟合与相关性指标。

Result: 自监督预训练缓解过拟合并提升表征质量；在测试集上实现平均Pearson相关系数0.41，优于基于逾百万超声检查训练的PanEcho基线；代码已开源。

Conclusion: 在数据稀缺条件下，使用同一小数据集进行SimCLR自监督预训练能显著改善心超视频CO预测性能与泛化，证明SSL在心血管影像定量中的可行性与效益；为开发更可靠的非侵入CO估计提供路径。

Abstract: Cardiac Output (CO) is a key parameter in the diagnosis and management of cardiovascular diseases. However, its accurate measurement requires right-heart catheterization, an invasive and time-consuming procedure, motivating the development of reliable non-invasive alternatives using echocardiography. In this work, we propose a self-supervised learning (SSL) pretraining strategy based on SimCLR to improve CO prediction from apical four-chamber echocardiographic videos. The pretraining is performed using the same limited dataset available for the downstream task, demonstrating the potential of SSL even under data scarcity. Our results show that SSL mitigates overfitting and improves representation learning, achieving an average Pearson correlation of 0.41 on the test set and outperforming PanEcho, a model trained on over one million echocardiographic exams. Source code is available at https://github.com/EIDOSLAB/cardiac-output.

</details>


### [77] [Low-Pass Filtering Improves Behavioral Alignment of Vision Models](https://arxiv.org/abs/2602.13859)
*Max Wolff,Thomas Klein,Evgenia Rusak,Felix Wichmann,Wieland Brendel*

Main category: cs.CV

TL;DR: 论文发现：生成模型与人类视觉行为更一致的优势，主要源自其内置的图像重采样造成的低通滤波效应；对判别模型在推理时简单施加模糊（低通）即可显著提升与人类的一致性并达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管DNN在视觉基准上表现优异，但在与人类视觉行为的一致性（错误一致性、形状偏好）上仍存在差距。近期有观点认为生成式分类器能根本改善这一一致性，暗示人类视觉应更像生成推断。作者质疑这种结论，怀疑改进源于实现细节（如重采样/低通），而非生成范式本身。

Method: 进行受控实验：1) 比较生成式模型中图像重采样带来的低通效应与行为一致性的关系；2) 在CLIP等判别模型上仅在测试时对输入做高频抑制（高斯模糊/低通），不改变训练；3) 直接优化频域滤波器以最大化模型-人类一致性；4) 计算该基准下所有可能的帕累托前沿以定位最优滤波策略；5) 将最优高斯滤波的频谱与人类视觉的对比敏感函数（CSF）进行对比。

Result: - 对判别模型仅在测试时进行模糊即可显著提升与人类行为的一致性，取得新的SOTA，并将现有一致性差距减半；- 直接优化得到的最优滤波器与简单的低通（高斯）非常接近，说明低通近乎最优；- 得到该基准的帕累托前沿，展示在性能与一致性之间的权衡；- 最优滤波的频谱与人类视觉系统的带通/CSF高度吻合。

Conclusion: 生成模型更高的一致性主要由低通预处理而非生成范式导致；在判别模型上采用简单的测试时低通滤波即可达到甚至超过生成模型的人类一致性水平。人类CSF与最优高斯滤波宽度的匹配为这一现象提供了生理学启示。

Abstract: Despite their impressive performance on computer vision benchmarks, Deep Neural Networks (DNNs) still fall short of adequately modeling human visual behavior, as measured by error consistency and shape bias. Recent work hypothesized that behavioral alignment can be drastically improved through \emph{generative} -- rather than \emph{discriminative} -- classifiers, with far-reaching implications for models of human vision.
  Here, we instead show that the increased alignment of generative models can be largely explained by a seemingly innocuous resizing operation in the generative model which effectively acts as a low-pass filter. In a series of controlled experiments, we show that removing high-frequency spatial information from discriminative models like CLIP drastically increases their behavioral alignment. Simply blurring images at test-time -- rather than training on blurred images -- achieves a new state-of-the-art score on the model-vs-human benchmark, halving the current alignment gap between DNNs and human observers. Furthermore, low-pass filters are likely optimal, which we demonstrate by directly optimizing filters for alignment. To contextualize the performance of optimal filters, we compute the frontier of all possible pareto-optimal solutions to the benchmark, which was formerly unknown.
  We explain our findings by observing that the frequency spectrum of optimal Gaussian filters roughly matches the spectrum of band-pass filters implemented by the human visual system. We show that the contrast sensitivity function, describing the inverse of the contrast threshold required for humans to detect a sinusoidal grating as a function of spatiotemporal frequency, is approximated well by Gaussian filters of the specific width that also maximizes error consistency.

</details>


### [78] [Human-Aligned Evaluation of a Pixel-wise DNN Color Constancy Model](https://arxiv.org/abs/2602.13887)
*Hamed Heidari-Gorji,Raquel Gil Rodriguez,Karl R. Gegenfurtner*

Main category: cs.CV

TL;DR: 在VR中用预训练的ResNet-U-Net并经迁移学习，只微调解码器，以人类相同“无彩选择”任务评估颜色恒常；模型与人类高度一致：基线高恒常，去除局部环绕或空间平均线索时性能相似下降。


<details>
  <summary>Details</summary>
Motivation: 检验深度模型在复杂、近真实场景中的颜色恒常是否与人类一致，并解析哪些经典线索（局部环绕、最大通量、空间平均）驱动这种一致性。

Method: 使用先前训练用于从渲染图像预测表面反射率的ResNet版U‑Net；在VR基线条件数据上仅微调解码器（迁移学习）。不以物理真值为准，而让模型在各条件下执行与人类相同的“选择无彩目标”任务，并比较表现。

Result: 模型与人类在基线条件下均表现出高颜色恒常；当移除局部环绕或空间平均颜色线索时，二者的表现均出现相似、条件依赖的下降，表现高度对应。

Conclusion: 深度模型在经有限微调后可捕捉与人类相似的颜色恒常机制，特别依赖局部环绕与空间平均线索；以任务为准的评估能有效对齐人与模型的比较。

Abstract: We previously investigated color constancy in photorealistic virtual reality (VR) and developed a Deep Neural Network (DNN) that predicts reflectance from rendered images. Here, we combine both approaches to compare and study a model and human performance with respect to established color constancy mechanisms: local surround, maximum flux and spatial mean. Rather than evaluating the model against physical ground truth, model performance was assessed using the same achromatic object selection task employed in the human experiments. The model, a ResNet based U-Net from our previous work, was pre-trained on rendered images to predict surface reflectance. We then applied transfer learning, fine-tuning only the network's decoder on images from the baseline VR condition. To parallel the human experiment, the model's output was used to perform the same achromatic object selection task across all conditions. Results show a strong correspondence between the model and human behavior. Both achieved high constancy under baseline conditions and showed similar, condition-dependent performance declines when the local surround or spatial mean color cues were removed.

</details>


### [79] [Parameter-Efficient Fine-Tuning of DINOv2 for Large-Scale Font Classification](https://arxiv.org/abs/2602.13889)
*Daniel Chen,Zaria Zinn,Marcus Lowe*

Main category: cs.CV

TL;DR: 用LoRA微调DINOv2 ViT，在合成数据上训练，实现394种字体家族分类，Top-1约86%，训练参数<1%，并开源模型/数据/管线与部署端点。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要从图像中自动识别字体，但真实标注数据稀缺、风格多样、噪声复杂，且全参数训练成本高，部署需与训练前处理一致并具可复现性与可扩展性。

Method: 构建大规模合成字体数据集：用Google Fonts渲染文本，随机化颜色、对齐、换行、高斯噪声等增强；在DINOv2 ViT上用LoRA进行低秩适配，只训练<1%参数；在模型内部集成一致的预处理；在HuggingFace Inference Endpoint部署。

Result: 在394个字体家族分类任务中达成约86%的Top-1准确率；LoRA显著减少可训练参数与训练成本；合成数据在真实排版样本上具良好泛化。

Conclusion: 合成数据+LoRA微调的ViT能够高效、准确地进行大规模字体分类，并可通过一致预处理与云端推理端点顺利落地；相关资源已完全开源，便于复现与扩展。

Abstract: We present a font classification system capable of identifying 394 font families from rendered text images. Our approach fine-tunes a DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA), achieving approximately 86% top-1 accuracy while training fewer than 1% of the model's 87.2M parameters. We introduce a synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations including randomized colors, alignment, line wrapping, and Gaussian noise, producing training images that generalize to real-world typographic samples. The model incorporates built-in preprocessing to ensure consistency between training and inference, and is deployed as a HuggingFace Inference Endpoint. We release the model, dataset, and full training pipeline as open-source resources.

</details>


### [80] [RPGD: RANSAC-P3P Gradient Descent for Extrinsic Calibration in 3D Human Pose Estimation](https://arxiv.org/abs/2602.13901)
*Zhanyu Tuo*

Main category: cs.CV

TL;DR: 提出RPGD：用仅自然人体动作，将MoCap骨架与单/多目RGB相机进行外参标定，先RANSAC-P3P求粗解，再用梯度下降细化；在多数据集上达成近真值、亚像素级重投影MPJPE误差，鲁棒于噪声，适用于大规模3D人体姿态数据采集。


<details>
  <summary>Details</summary>
Motivation: 传统相机外参标定常需棋盘格或特制装置，流程繁琐、不适用于大规模、在野环境的人体姿态采集。希望利用天然的人体运动作为“标定对象”，在噪声与遮挡条件下仍能自动、准确、鲁棒地完成外参标定。

Method: 将外参标定建模为针对人体姿态的粗到细优化：1) 通过RANSAC-P3P在2D关节点与3D骨架对应上全局鲁棒地估计初始位姿；2) 以该解为初始化，采用基于重投影误差的梯度下降进行连续优化；3) 适用于单目或多目设置，并对噪声与错误匹配采用采样一致性与优化正则化策略。

Result: 在三个大规模公开3D人体姿态估计数据集与一个自采在野数据集上评测，RPGD恢复外参精度与真值相当；在具有噪声与挑战性的场景中，仍实现亚像素级MPJPE重投影误差，优于或可与基准方法匹敌。

Conclusion: RPGD无需特制标定物，仅凭自然人体运动即可稳健、自动地进行相机外参标定，具备高精度与鲁棒性，适合大规模3D HPE数据采集场景的实用落地。

Abstract: In this paper, we propose RPGD (RANSAC-P3P Gradient Descent), a human-pose-driven extrinsic calibration framework that robustly aligns MoCap-based 3D skeletal data with monocular or multi-view RGB cameras using only natural human motion. RPGD formulates extrinsic calibration as a coarse-to-fine problem tailored to human poses, combining the global robustness of RANSAC-P3P with Gradient-Descent-based refinement. We evaluate RPGD on three large-scale public 3D HPE datasets as well as on a self-collected in-the-wild dataset. Experimental results demonstrate that RPGD consistently recovers extrinsic parameters with accuracy comparable to the provided ground truth, achieving sub-pixel MPJPE reprojection error even in challenging, noisy settings. These results indicate that RPGD provides a practical and automatic solution for reliable extrinsic calibration of large-scale 3D HPE dataset collection.

</details>


### [81] [MamaDino: A Hybrid Vision Model for Breast Cancer 3-Year Risk Prediction](https://arxiv.org/abs/2602.13930)
*Ruggiero Santeramo,Igor Zubarev,Florian Jug*

Main category: cs.CV

TL;DR: MamaDino 将自监督 ViT（DINOv3）与可训练 CNN 在 512×512 低分辨率图像上融合，并通过显式对侧乳房信息的 BilateralMixer 实现三年乳腺癌风险预测；在内外部分布上与 Mirai 持平，用像素量约少 13 倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于影像的深度学习风险模型（如 Mirai）依赖高分辨率、以卷积为主且多视角融合简单，未充分建模对侧不对称；希望在显著降低分辨率的同时，通过更强先验与结构化融合保持或提升预测性能，便于部署与泛化。

Method: 提出 MamaDino：在 512×512 输入上，冻结自监督 DINOv3 ViT-S 提取特征，配合可训练 CNN 编码器；通过 BilateralMixer 显式聚合双侧乳房信息，输出 3 年风险分数。训练于 OPTIMAM 的 53,883 名女性；评估于配对的 3 年病例-对照队列，包括四站点的分布内测试与一个未见站点的分布外测试。

Result: 在乳房级别，MamaDino 与 Mirai 表现匹配，同时输入像素约减少 13 倍。加入 BilateralMixer 将 AUC 从 0.713 提升至 0.736（分布内），从 0.666 提升至 0.677（分布外）；在年龄、种族、设备、肿瘤类型与分级上表现一致。

Conclusion: 结合卷积与变换器的互补归纳偏置，并显式建模对侧不对称，可在显著更低分辨率的乳腺X线片上实现与 SOTA（Mirai）相当的三年风险预测，为更高效、可推广的风险分层提供路径。

Abstract: Breast cancer screening programmes increasingly seek to move from one-size-fits-all interval to risk-adapted and personalized strategies. Deep learning (DL) has enabled image-based risk models with stronger 1- to 5-year prediction than traditional clinical models, but leading systems (e.g., Mirai) typically use convolutional backbones, very high-resolution inputs (>1M pixels) and simple multi-view fusion, with limited explicit modelling of contralateral asymmetry.
  We hypothesised that combining complementary inductive biases (convolutional and transformer-based) with explicit contralateral asymmetry modelling would allow us to match state-of-the-art 3-year risk prediction performance even when operating on substantially lower-resolution mammograms, indicating that using less detailed images in a more structured way can recover state-of-the-art accuracy.
  We present MamaDino, a mammography-aware multi-view attentional DINO model. MamaDino fuses frozen self-supervised DINOv3 ViT-S features with a trainable CNN encoder at 512x512 resolution, and aggregates bilateral breast information via a BilateralMixer to output a 3-year breast cancer risk score. We train on 53,883 women from OPTIMAM (UK) and evaluate on matched 3-year case-control cohorts: an in-distribution test set from four screening sites and an external out-of-distribution cohort from an unseen site.
  At breast-level, MamaDino matches Mirai on both internal and external tests while using ~13x fewer input pixels. Adding the BilateralMixer improves discrimination to AUC 0.736 (vs 0.713) in-distribution and 0.677 (vs 0.666) out-of-distribution, with consistent performance across age, ethnicity, scanner, tumour type and grade. These findings demonstrate that explicit contralateral modelling and complementary inductive biases enable predictions that match Mirai, despite operating on substantially lower-resolution mammograms.

</details>


### [82] [Fusing Pixels and Genes: Spatially-Aware Learning in Computational Pathology](https://arxiv.org/abs/2602.13944)
*Minghao Han,Dingkang Yang,Linhao Qu,Zizhi Chen,Gang Li,Han Wang,Jiacong Wang,Lihua Zhang*

Main category: cs.CV

TL;DR: 提出STAMP框架，将空间转录组(Visium)与病理图像进行分子引导的联合表征学习，通过自监督、分层多尺度对比对齐与跨尺度定位，提升病理表示的通用性与性能；并构建最大规模的Visium数据集SpaVis-6M与空间感知基因编码器，在6个数据集4类任务上验证效果优异。


<details>
  <summary>Details</summary>
Motivation: 仅依赖视觉+语言的多模态病理方法缺乏分子特异性与足够的病理监督，导致表征瓶颈；需要将空间分辨的分子信息引入，以获得更具生物学含义和可泛化的图像表示。

Method: 构建SpaVis-6M大规模Visium空间转录组数据；训练空间感知基因编码器；提出STAMP，通过层级多尺度对比对齐与跨尺度patch定位机制，将病理图像与空间转录组进行分子引导的自监督联合嵌入，显式利用空间上下文与多尺度信息以捕捉空间结构与分子变异。

Result: 在6个数据集、4类下游任务上，STAMP均取得强劲表现，展现出较强的泛化与鲁棒性；证明了空间分辨分子监督对病理多模态学习的价值。

Conclusion: 将空间转录组纳入病理多模态学习可显著提升表征质量与任务性能；STAMP与SpaVis-6M为该方向提供了强有力的基座与资源，支持未来在分子引导的计算病理中进一步发展；代码、预训练权重与数据已开源。

Abstract: Recent years have witnessed remarkable progress in multimodal learning within computational pathology. Existing models primarily rely on vision and language modalities; however, language alone lacks molecular specificity and offers limited pathological supervision, leading to representational bottlenecks. In this paper, we propose STAMP, a Spatial Transcriptomics-Augmented Multimodal Pathology representation learning framework that integrates spatially-resolved gene expression profiles to enable molecule-guided joint embedding of pathology images and transcriptomic data. Our study shows that self-supervised, gene-guided training provides a robust and task-agnostic signal for learning pathology image representations. Incorporating spatial context and multi-scale information further enhances model performance and generalizability. To support this, we constructed SpaVis-6M, the largest Visium-based spatial transcriptomics dataset to date, and trained a spatially-aware gene encoder on this resource. Leveraging hierarchical multi-scale contrastive alignment and cross-scale patch localization mechanisms, STAMP effectively aligns spatial transcriptomics with pathology images, capturing spatial structure and molecular variation. We validate STAMP across six datasets and four downstream tasks, where it consistently achieves strong performance. These results highlight the value and necessity of integrating spatially resolved molecular supervision for advancing multimodal learning in computational pathology. The code is included in the supplementary materials. The pretrained weights and SpaVis-6M are available at: https://github.com/Hanminghao/STAMP.

</details>


### [83] [MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars](https://arxiv.org/abs/2602.13961)
*Shuoyuan Wang,Yiran Wang,Hongxin Wei*

Main category: cs.CV

TL;DR: 提出MarsRetrieval基准，评估用于火星地貌发现的视觉-语言模型，涵盖配对图文、地貌类别检索与全球定位三任务；统一检索协议对比对比式与生成式架构。结果显示任务具有挑战性，通用大模型难区分专业地貌，领域微调显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有行星科学中的多模态基准多局限于闭集监督视觉任务，缺乏支持文本引导的地理空间检索工具，难以满足在火星上进行跨尺度、跨地貌来源的发现与定位需求。

Method: 构建MarsRetrieval基准，包含三项检索任务：1) 图像-文本配对检索；2) 地貌检索（按地貌语义查询相似地貌）；3) 全球地理定位（在行星表面尺度定位）。提出统一的以检索为中心的评测协议，系统评估多模态嵌入体系，包括对比式双塔编码器与生成式视觉-语言模型，并比较是否进行行星领域微调。

Result: 基准具有高难度：即便是强大的通用基础模型在火星地貌细粒度区分上表现不佳。经过火星/行星领域特定微调后，模型在三项检索任务上的泛化与性能明显提升。

Conclusion: MarsRetrieval为火星地理空间发现提供了首个统一的检索型多模态评测框架，揭示了通用模型的域差距，并表明领域微调对提升行星场景下的可泛化检索至关重要；代码公开以促进后续研究。

Abstract: Data-driven approaches like deep learning are rapidly advancing planetary science, particularly in Mars exploration. Despite recent progress, most existing benchmarks remain confined to closed-set supervised visual tasks and do not support text-guided retrieval for geospatial discovery. We introduce MarsRetrieval, a retrieval benchmark for evaluating vision-language models for Martian geospatial discovery. MarsRetrieval includes three tasks: (1) paired image-text retrieval, (2) landform retrieval, and (3) global geo-localization, covering multiple spatial scales and diverse geomorphic origins. We propose a unified retrieval-centric protocol to benchmark multimodal embedding architectures, including contrastive dual-tower encoders and generative vision-language models. Our evaluation shows MarsRetrieval is challenging: even strong foundation models often fail to capture domain-specific geomorphic distinctions. We further show that domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings. Our code is available at https://github.com/ml-stat-Sustech/MarsRetrieval

</details>


### [84] [Elastic Diffusion Transformer](https://arxiv.org/abs/2602.13993)
*Jiangshan Wang,Zeqiang Lai,Jiarui Chen,Jiayi Guo,Hang Guo,Xiu Li,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: 提出E-DiT：在DiT中加入轻量路由器以自适应跳层与压缩MLP宽度，并结合训练外的块级特征缓存，在2D与3D生成任务上实现约2倍加速且质量几乎不降。


<details>
  <summary>Details</summary>
Motivation: 现有对DiT的加速（剪枝、蒸馏等）多为固定计算预算，无法适配样本差异的稀疏性，导致加速不足或质量下降。作者观察到DiT生成过程中存在显著且样本依赖的稀疏计算可被跳过，激发了自适应加速框架的需求。

Method: 在每个DiT块加入轻量级路由器：1) 判断该块是否可跳过；2) 若执行，则预测块内MLP的最优宽度缩减比。推理时引入训练外的块级特征缓存，依据路由器预测复用冗余计算，减少计算量。整体形成对样本自适应的跳层+通道稀疏+缓存的三重机制。

Result: 在Qwen-Image与FLUX（2D）以及Hunyuan3D-3.0（3D）上验证，可实现最高约2×推理加速，生成质量几乎无损。

Conclusion: E-DiT能利用样本依赖的稀疏性，通过动态跳层、MLP宽度自适应与缓存策略，在不显著牺牲质量的前提下显著加速DiT推理，优于固定预算的传统加速方法。

Abstract: Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\sim$2$\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.

</details>


### [85] [Inject Where It Matters: Training-Free Spatially-Adaptive Identity Preservation for Text-to-Image Personalization](https://arxiv.org/abs/2602.13994)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出SpatialID：一种免训练、空间自适应的人像个性化文生图框架，通过注意力派生的空间掩码与时空调度，避免身份特征污染背景并提升文本对齐与图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有免微调个性化文生图方法使用空间均匀的视觉注入，导致身份（面部）特征扩散到非面部区域，如背景和光照，从而削弱文本遵循和画面质量；需要一种无需昂贵微调、能在空间上区分身份相关与无关区域的机制。

Method: 1) Spatial Mask Extractor：从扩散模型的跨注意力响应中提取与人脸/身份相关的空间掩码，实现在面部相关与上下文无关区域的解耦注入；2) SpatialID调制：对面部相关区域强化身份注入，对非相关区域抑制，避免污染；3) Temporal-Spatial Scheduling：随扩散步数动态调整空间约束，早期用高斯先验，随后切换到注意力掩码，并在后期自适应放宽约束以匹配扩散动态。

Result: 在IBench基准上取得SOTA：文本对齐CLIP-T=0.281、视觉一致性CLIP-I=0.827、图像质量IQ=0.523，并显著减少背景污染同时保持强身份保真。

Conclusion: SpatialID在无需训练的前提下，通过跨注意力掩码与时空调度，实现对身份注入的空间自适应控制，兼顾身份保真、文本遵循与画质，优于现有免微调方法。

Abstract: Personalized text-to-image generation aims to integrate specific identities into arbitrary contexts. However, existing tuning-free methods typically employ Spatially Uniform Visual Injection, causing identity features to contaminate non-facial regions (e.g., backgrounds and lighting) and degrading text adherence. To address this without expensive fine-tuning, we propose SpatialID, a training-free spatially-adaptive identity modulation framework. SpatialID fundamentally decouples identity injection into face-relevant and context-free regions using a Spatial Mask Extractor derived from cross-attention responses. Furthermore, we introduce a Temporal-Spatial Scheduling strategy that dynamically adjusts spatial constraints - transitioning from Gaussian priors to attention-based masks and adaptive relaxation - to align with the diffusion generation dynamics. Extensive experiments on IBench demonstrate that SpatialID achieves state-of-the-art performance in text adherence (CLIP-T: 0.281), visual consistency (CLIP-I: 0.827), and image quality (IQ: 0.523), significantly eliminating background contamination while maintaining robust identity preservation.

</details>


### [86] [A Deployment-Friendly Foundational Framework for Efficient Computational Pathology](https://arxiv.org/abs/2602.14010)
*Yu Cai,Cheng Jin,Jiabo Ma,Fengtao Zhou,Yingxue Xu,Zhengrui Guo,Yihui Wang,Zhengyu Zhang,Ling Liang,Yonghao Tan,Pingcheng Dong,Du Cai,On Ki Tang,Chenglong Zhao,Xi Wang,Can Yang,Yali Xu,Jing Cui,Zhenhui Li,Ronald Cheong Kin Chan,Yueping Liu,Feng Gao,Xiuming Zhang,Li Liang,Hao Chen,Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: LitePath是一套面向部署的病理基础模型框架，通过小型蒸馏模型LiteFM与自适应补丁选择APS，极大降低参数与算力需求，同时保持接近SOTA的准确度，并在低功耗边缘设备上实现高吞吐与低能耗；提出D-Score综合衡量精度与效率，LitePath得分最高。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型在千兆像素全视野切片上计算代价高，限制了临床可及性、可扩展性与绿色部署需求；存在模型过度参数化与补丁级冗余两大瓶颈，迫切需要在不显著牺牲性能的前提下降本增效。

Method: 提出LitePath框架：1) LiteFM，小型模型，通过在1.9亿病理补丁上从三大PFM（Virchow2、H-Optimus-1、UNI2）进行知识蒸馏获得；2) APS（Adaptive Patch Selector），针对任务的轻量级补丁选择模块，减少无效/冗余补丁计算；3) 定义部署性指标D-Score（归一化AUC与归一化FLOPs的加权几何平均）来量化精度-效率权衡。

Result: 相对Virchow2参数减少28倍、FLOPs降低403.5倍；在Jetson Orin Nano Super上可达208张/小时吞吐，较Virchow2快104.5倍；处理3000张仅耗0.36 kWh，能耗比在RTX3090上的Virchow2低171倍；在4器官26任务、37队列（含外部与前瞻）共15,672张切片、9,808名患者上验证：总体AUC保留Virchow2的99.71%，在19模型中综合性能排名第二并超过多款更大模型（H-Optimus-1、mSTAR、UNI2、GPFM）；D-Score最高，比Virchow2高10.64%。

Conclusion: LitePath在边缘硬件上实现快速、低成本、低能耗的病理图像分析，精度接近甚至优于更大PFM；D-Score为部署友好性提供统一评估标准，展示了减碳与可及性导向的PFM设计路径。

Abstract: Pathology foundation models (PFMs) have enabled robust generalization in computational pathology through large-scale datasets and expansive architectures, but their substantial computational cost, particularly for gigapixel whole slide images, limits clinical accessibility and scalability. Here, we present LitePath, a deployment-friendly foundational framework designed to mitigate model over-parameterization and patch level redundancy. LitePath integrates LiteFM, a compact model distilled from three large PFMs (Virchow2, H-Optimus-1 and UNI2) using 190 million patches, and the Adaptive Patch Selector (APS), a lightweight component for task-specific patch selection. The framework reduces model parameters by 28x and lowers FLOPs by 403.5x relative to Virchow2, enabling deployment on low-power edge hardware such as the NVIDIA Jetson Orin Nano Super. On this device, LitePath processes 208 slides per hour, 104.5x faster than Virchow2, and consumes 0.36 kWh per 3,000 slides, 171x lower than Virchow2 on an RTX3090 GPU. We validated accuracy using 37 cohorts across four organs and 26 tasks (26 internal, 9 external, and 2 prospective), comprising 15,672 slides from 9,808 patients disjoint from the pretraining data. LitePath ranks second among 19 evaluated models and outperforms larger models including H-Optimus-1, mSTAR, UNI2 and GPFM, while retaining 99.71% of the AUC of Virchow2 on average. To quantify the balance between accuracy and efficiency, we propose the Deployability Score (D-Score), defined as the weighted geometric mean of normalized AUC and normalized FLOP, where LitePath achieves the highest value, surpassing Virchow2 by 10.64%. These results demonstrate that LitePath enables rapid, cost-effective and energy-efficient pathology image analysis on accessible hardware while maintaining accuracy comparable to state-of-the-art PFMs and reducing the carbon footprint of AI deployment.

</details>


### [87] [Flow4R: Unifying 4D Reconstruction and Tracking with Scene Flow](https://arxiv.org/abs/2602.14021)
*Shenhan Qian,Ganlin Zhang,Shangzhe Wu,Daniel Cremers*

Main category: cs.CV

TL;DR: Flow4R以相机坐标系场景流为核心表征，统一建模3D结构、物体与相机运动，在两视图输入下以ViT一次前向预测每像素的3D点、场景流、姿态权重和置信度，无需显式位姿回归或BA，达到4D重建与跟踪SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法将几何与运动割裂：多视几何假定静态场景，而动态跟踪依赖显式位姿估计或独立运动模型，难以统一处理相机与物体共同运动的动态3D场景。需要一种能同时耦合几何、相机与物体运动的统一表示与推理框架。

Method: 提出以相机空间scene flow为中心的统一表示Flow4R。输入为两视图，使用Vision Transformer编码-解码结构，单次前向对每像素预测四种最小属性：3D点位置、双向场景流、姿态权重以及置信度。通过共享解码器对局部几何与双向运动进行对称推理，无需显式相机位姿回归或bundle adjustment；在静态与动态数据上联合训练。

Result: 在4D重建与跟踪任务上取得SOTA表现（相较现有多视重建与动态跟踪方法），验证了以场景流为中心的统一建模带来的精度与稳健性提升。

Conclusion: 将场景流置于核心，把几何与运动耦合在统一框架中，可在两视图条件下端到端、单次前向完成结构与运动估计，省去位姿回归/BA，显著提升动态场景的时空理解与重建效果。

Abstract: Reconstructing and tracking dynamic 3D scenes remains a fundamental challenge in computer vision. Existing approaches often decouple geometry from motion: multi-view reconstruction methods assume static scenes, while dynamic tracking frameworks rely on explicit camera pose estimation or separate motion models. We propose Flow4R, a unified framework that treats camera-space scene flow as the central representation linking 3D structure, object motion, and camera motion. Flow4R predicts a minimal per-pixel property set-3D point position, scene flow, pose weight, and confidence-from two-view inputs using a Vision Transformer. This flow-centric formulation allows local geometry and bidirectional motion to be inferred symmetrically with a shared decoder in a single forward pass, without requiring explicit pose regressors or bundle adjustment. Trained jointly on static and dynamic datasets, Flow4R achieves state-of-the-art performance on 4D reconstruction and tracking tasks, demonstrating the effectiveness of the flow-central representation for spatiotemporal scene understanding.

</details>


### [88] [Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation](https://arxiv.org/abs/2602.14027)
*Jia Li,Xiaomeng Fu,Xurui Peng,Weifeng Chen,Youwei Zheng,Tianyu Zhao,Jiexi Wang,Fangmin Chen,Xing Wang,Hayden Kwok-Hay So*

Main category: cs.CV

TL;DR: 提出FLEX：在推理阶段、无需再训练的“频率感知+动态先验”框架，修复自回归视频扩散在长时延拓展中的外推崩溃；通过频率感知RoPE调制、反相噪声采样与推理期注意力锚点，显著提升30s~60s甚至数分钟生成稳定性与动态一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散在长视频生成时会随步数累积误差，导致时序退化；根源被指向3D位置编码的频谱偏置与噪声采样缺乏动态先验。希望在不再训练的前提下，把只在短时窗口训练的模型稳健地延伸到长时推理。

Method: 提出FLEX推理框架：1) 频率感知RoPE调制（Frequency-aware RoPE Modulation），对低频分量做自适应插值、对高频分量做外推，保留多尺度时间判别性；2) 反相噪声采样（ANS）注入高频动态先验；3) 仅在推理期的注意力“sink”用于锚定全局结构。整体为即插即用、训练无关。

Result: 在VBench上，FLEX在6×外推（30s）显著优于SOTA；在12×（60s）与长视频微调基线持平；可无缝集成到如LongLive的管线，将一致且具动态的生成推进至约4分钟。

Conclusion: 通过频率调制与动态先验注入，FLEX在无需再训练的前提下缓解光谱偏置与长时外推崩溃，成为可即插即用的长时视频生成延拓方案。

Abstract: Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the \textit{spectral bias} of 3D positional embeddings and the lack of \textit{dynamic priors} in noise sampling. To address these issues, we propose \textbf{FLEX} (\textbf{F}requency-aware \textbf{L}ength \textbf{EX}tension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at $6\times$ extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at $12\times$ scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at \href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}.

</details>


### [89] [Explainability-Inspired Layer-Wise Pruning of Deep Neural Networks for Efficient Object Detection](https://arxiv.org/abs/2602.14040)
*Abhinav Shukla,Nachiket Tapas*

Main category: cs.CV

TL;DR: 提出一种基于可解释性的按层剪枝框架，用梯度-激活的SHAP式归因评估层重要性，在多种检测器与COCO2017上优于L1范数剪枝，带来更佳精度-效率权衡（如ShuffleNetV2提速10%且无精度损失，L1反而-13.7%）。


<details>
  <summary>Details</summary>
Motivation: 传统基于权重幅值（如L1）的剪枝与网络组件对任务性能的真实功能贡献未必一致，尤其在资源受限平台上需要既高效又可靠的剪枝准则。作者希望用数据驱动、与功能相关的可解释性指标来度量层的重要性，从而更稳健地压缩检测模型。

Method: 提出一种按层（layer-wise）的剪枝流程：利用受SHAP启发的“梯度×激活”归因，估计各层对检测性能的贡献，依据该重要性排序选择要剪的层/通道，而非仅看静态权重幅度；在多种主干与检测头（ResNet-50、MobileNetV2、ShuffleNetV2、Faster R-CNN、RetinaNet、YOLOv8）上实施并评测。

Result: 归因式剪枝挑选的“最不重要层”与L1范数方法显著不同，带来更优权衡：ShuffleNetV2上推理速度提升约10%，而L1剪枝造成13.7%的性能下降；在RetinaNet上，新方法基本保持基线mAP（0.151）且速度几乎不变，而L1剪枝为6.2%提速却导致1.3% mAP下降。总体显示该方法在多架构上稳定优于L1。

Conclusion: 以可解释性驱动的、数据导向的层重要性评估更能反映功能贡献，能在边缘与资源受限部署中实现更好的精度-效率权衡，同时兼顾可解释性；这为检测模型压缩提供了更有原则的方向。

Abstract: Deep neural networks (DNNs) have achieved remarkable success in object detection tasks, but their increasing complexity poses significant challenges for deployment on resource-constrained platforms. While model compression techniques such as pruning have emerged as essential tools, traditional magnitude-based pruning methods do not necessarily align with the true functional contribution of network components to task-specific performance. In this work, we present an explainability-inspired, layer-wise pruning framework tailored for efficient object detection. Our approach leverages a SHAP-inspired gradient--activation attribution to estimate layer importance, providing a data-driven proxy for functional contribution rather than relying solely on static weight magnitudes. We conduct comprehensive experiments across diverse object detection architectures, including ResNet-50, MobileNetV2, ShuffleNetV2, Faster R-CNN, RetinaNet, and YOLOv8, evaluating performance on the Microsoft COCO 2017 validation set. The results show that the proposed attribution-inspired pruning consistently identifies different layers as least important compared to L1-norm-based methods, leading to improved accuracy--efficiency trade-offs. Notably, for ShuffleNetV2, our method yields a 10\% empirical increase in inference speed, whereas L1-pruning degrades performance by 13.7\%. For RetinaNet, the proposed approach preserves the baseline mAP (0.151) with negligible impact on inference speed, while L1-pruning incurs a 1.3\% mAP drop for a 6.2\% speed increase. These findings highlight the importance of data-driven layer importance assessment and demonstrate that explainability-inspired compression offers a principled direction for deploying deep neural networks on edge and resource-constrained platforms while preserving both performance and interpretability.

</details>


### [90] [BitDance: Scaling Autoregressive Generative Models with Binary Tokens](https://arxiv.org/abs/2602.14041)
*Yuang Ai,Jiaming Han,Shaobin Zhuang,Weijia Mao,Xuefeng Hu,Ziyan Yang,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen*

Main category: cs.CV

TL;DR: BitDance 用二进制视觉token替代码本索引，并用“二进制扩散头”在连续空间生成高熵二进制潜变量；配合“next-patch diffusion”并行预测多token，显著提升AR图像生成的质量与速度，在ImageNet与T2I上均达SOTA级表现，参数更少、速度更快。


<details>
  <summary>Details</summary>
Motivation: 传统AR图像生成多基于离散码本索引，token空间受限，softmax分类在巨大词表上训练与采样均困难，推理速度慢、并行性差；需要一种既具极高表示能力、又能高效采样与并行解码的AR框架。

Method: 1) 表示层：将视觉表示改为高熵二进制token（每token最多表示2^256状态），形成紧凑且表达力强的离散表示。2) 头部：以“二进制扩散头”替代softmax分类头，在连续空间通过扩散过程生成二进制token，避免超大词表分类。3) 解码：提出“next-patch diffusion”，一次并行预测多个token，在保持准确度的同时大幅加速推理。4) 文本到图像：在多模态token上进行大规模训练，实现高分辨率、逼真图像生成与良好扩展性。

Result: - ImageNet 256×256：FID=1.24，为AR模型中最佳。- 与并行AR SOTA（1.4B参数）相比：仅260M参数（少5.4×），速度快8.7×，质量更优。- 文本到图像：在高分辨率（如1024×1024）生成上，相比以往AR模型超过30×的速度提升，展示出强性能与可扩展性。

Conclusion: 通过二进制token与扩散式预测头，BitDance在不牺牲质量的前提下显著提升AR图像生成的效率与并行性；配合next-patch diffusion，实现少参数、高速度与高保真图像生成，适用于通用AR基础模型，并具有良好扩展与研究价值。

Abstract: We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.

</details>


### [91] [Restoration Adaptation for Semantic Segmentation on Low Quality Images](https://arxiv.org/abs/2602.14042)
*Kai Guan,Rongyuan Wu,Shuai Li,Wentao Zhu,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出RASS框架：用语义约束修复(SCR)提升低质图像的语义一致性，并通过LoRA合并与微调将修复知识迁移到分割模型；在合成与真实LQ数据上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实中分割常遇到低质图像，现有分割模型对退化不鲁棒；传统真实场景图像修复模型偏像素保真，难以恢复对下游分割有用的语义线索，直接套用无法提升任务表现。需要一种能兼顾语义一致性的修复，并让分割模型受益的方法。

Method: 1) 语义约束修复SCR：在修复网络中引入分割先验，通过将其跨注意力图与分割掩码对齐，促使重建结果在语义上忠实；2) RASS知识迁移：将语义修复知识通过LoRA模块合并与任务特定微调注入到分割模型，使其直接在低质图像上获得高质量分割。并构建带高质量标注的真实LQ分割数据集用于训练/评测。

Result: 在合成与真实世界低质基准上，SCR在修复任务、RASS在分割任务上均显著超越一系列SOTA方法，表现出更强的鲁棒性与语义一致性。

Conclusion: 将语义感知的修复与分割紧密耦合，通过LoRA进行跨任务知识迁移，可有效提升低质图像上的语义分割效果；提供的新数据集与代码资源支持复现与后续研究。

Abstract: In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks. Conversely, existing segmentation models trained on high-quality data lack robustness under real-world degradations. In this paper, we propose Restoration Adaptation for Semantic Segmentation (RASS), which effectively integrates semantic image restoration into the segmentation process, enabling high-quality semantic segmentation on the LQ images directly. Specifically, we first propose a Semantic-Constrained Restoration (SCR) model, which injects segmentation priors into the restoration model by aligning its cross-attention maps with segmentation masks, encouraging semantically faithful image reconstruction. Then, RASS transfers semantic restoration knowledge into segmentation through LoRA-based module merging and task-specific fine-tuning, thereby enhancing the model's robustness to LQ images. To validate the effectiveness of our framework, we construct a real-world LQ image segmentation dataset with high-quality annotations, and conduct extensive experiments on both synthetic and real-world LQ benchmarks. The results show that SCR and RASS significantly outperform state-of-the-art methods in segmentation and restoration tasks. Code, models, and datasets will be available at https://github.com/Ka1Guan/RASS.git.

</details>


### [92] [CoCoEdit: Content-Consistent Image Editing via Region Regularized Reinforcement Learning](https://arxiv.org/abs/2602.14068)
*Yuhui Wu,Chenxi Xie,Ruibin Li,Liyi Chen,Qiaosi Yi,Lei Zhang*

Main category: cs.CV

TL;DR: 提出CoCoEdit：通过区域正则化强化学习的后训练框架，兼顾编辑质量与内容一致性；在两大基准上显著提升PSNR/SSIM与人评，同时保持SOTA编辑分数。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型常在目标外区域产生不期望的改动，缺乏对未编辑区域的内容一致性约束；评测也缺少像素级一致性指标与对应标注。

Method: 1) 数据：为现有编辑数据集补充精炼指令与编辑掩码，筛选4万高质样本。2) 奖励：在MLLM-based奖励外，引入像素级相似度奖励以同时衡量编辑质量和内容一致性。3) 正则：提出区域级正则器；对高奖励样本约束非编辑区域保持、对低奖励样本鼓励编辑区域产生更明显改动，缓解奖励的空间无感性。4) 评测：为GEdit-Bench与ImgEdit-Bench标注掩码，并加入像素级相似度指标。

Result: 将CoCoEdit应用于Qwen-Image-Edit与FLUX-Kontext，获得与SOTA相当的编辑效果评分，同时在PSNR/SSIM与主观人评上的内容一致性显著优于对比方法。

Conclusion: 区域正则化强化学习与像素级奖励的结合能在不牺牲编辑质量的前提下有效抑制非目标区域的意外修改；新的数据与评测方案为内容一致性提供了可量化的标准。

Abstract: Image editing has achieved impressive results with the development of large-scale generative models. However, existing models mainly focus on the editing effects of intended objects and regions, often leading to unwanted changes in unintended regions. We present a post-training framework for Content-Consistent Editing (CoCoEdit) via region regularized reinforcement learning. We first augment existing editing datasets with refined instructions and masks, from which 40K diverse and high quality samples are curated as training set. We then introduce a pixel-level similarity reward to complement MLLM-based rewards, enabling models to ensure both editing quality and content consistency during the editing process. To overcome the spatial-agnostic nature of the rewards, we propose a region-based regularizer, aiming to preserve non-edited regions for high-reward samples while encouraging editing effects for low-reward samples. For evaluation, we annotate editing masks for GEdit-Bench and ImgEdit-Bench, introducing pixel-level similarity metrics to measure content consistency and editing quality. Applying CoCoEdit to Qwen-Image-Edit and FLUX-Kontext, we achieve not only competitive editing scores with state-of-the-art models, but also significantly better content consistency, measured by PSNR/SSIM metrics and human subjective ratings.

</details>


### [93] [ForgeryVCR: Visual-Centric Reasoning via Efficient Forensic Tools in MLLMs for Image Forgery Detection and Localization](https://arxiv.org/abs/2602.14098)
*Youqi Wang,Shen Chen,Haowei Wang,Rongxuan Peng,Taiping Yao,Shunquan Tan,Changsheng Chen,Bin Li,Shouhong Ding*

Main category: cs.CV

TL;DR: 提出ForgeryVCR：用视觉为中心的工具链把不可见篡改痕迹“具象化”，并通过策略化工具学习（SFT+RL）让MLLM主动选择多视角取证路径，从而在图像篡改检测与定位上达成SOTA、具备更强泛化与鲁棒性、且工具调用冗余更小。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多用文本CoT来描述微小像素级篡改痕迹，语言模态难以承载低层次取证信息，易诱发幻觉；需要一种能直接处理和呈现不可感知取证线索的推理范式。

Method: 构建ForgeryVCR框架：引入取证工具箱，将压缩史、噪声残差、频域等隐形痕迹转化为可视化中间结果，采用视觉中心推理；提出“策略化工具学习”后训练流程：1) 基于收益驱动的轨迹构造进行SFT；2) 以工具效用奖励为目标进行RL优化，使模型学会何时、如何调用包括局部放大、多视图与多域分析的工具链。

Result: 在图像篡改检测与定位上取得SOTA；展现更好泛化、鲁棒性，并减少不必要的工具调用。

Conclusion: 通过将不可见篡改线索显式可视化并用策略学习优化工具调用，MLLM能更可靠地执行取证推理，缓解文本CoT导致的幻觉，实现高效、稳健的检测与定位。

Abstract: Existing Multimodal Large Language Models (MLLMs) for image forgery detection and localization predominantly operate under a text-centric Chain-of-Thought (CoT) paradigm. However, forcing these models to textually characterize imperceptible low-level tampering traces inevitably leads to hallucinations, as linguistic modalities are insufficient to capture such fine-grained pixel-level inconsistencies. To overcome this, we propose ForgeryVCR, a framework that incorporates a forensic toolbox to materialize imperceptible traces into explicit visual intermediates via Visual-Centric Reasoning. To enable efficient tool utilization, we introduce a Strategic Tool Learning post-training paradigm, encompassing gain-driven trajectory construction for Supervised Fine-Tuning (SFT) and subsequent Reinforcement Learning (RL) optimization guided by a tool utility reward. This paradigm empowers the MLLM to act as a proactive decision-maker, learning to spontaneously invoke multi-view reasoning paths including local zoom-in for fine-grained inspection and the analysis of invisible inconsistencies in compression history, noise residuals, and frequency domains. Extensive experiments reveal that ForgeryVCR achieves state-of-the-art (SOTA) performance in both detection and localization tasks, demonstrating superior generalization and robustness with minimal tool redundancy. The project page is available at https://youqiwong.github.io/projects/ForgeryVCR/.

</details>


### [94] [GeoFusionLRM: Geometry-Aware Self-Correction for Consistent 3D Reconstruction](https://arxiv.org/abs/2602.14119)
*Ahmet Burak Yildirim,Tuna Saygin,Duygu Ceylan,Aysegul Dundar*

Main category: cs.CV

TL;DR: 提出GeoFusionLRM：利用模型自身的法线与深度预测做几何自校正的单图3D重建框架，在无需额外监督下提升网格几何锐度、法线一致性与整体保真度，优于现有LRM基线。


<details>
  <summary>Details</summary>
Motivation: 现有大型重建模型虽能从单张图像生成3D，但常出现几何不一致、细节错位与与视图对齐差的问题，限制了重建保真度。亟需一种能在不依赖外部监督或信号的前提下，利用模型可得的几何线索来自我纠错、强化几何一致性的方法。

Method: 提出几何感知自校正框架GeoFusionLRM：将模型对法线和深度的预测作为几何线索回馈，经过专门的transformer与融合模块，与图像特征联合推断。该闭环让模型在推理中修正结构错误、约束与输入图像的一致性，无需额外监督或外部信号。

Result: 在大量实验中，相较于最先进LRM基线，GeoFusionLRM获得更锐利的几何、更加一致的法线与更高的整体保真度；重建网格与输入视图的对齐更佳。

Conclusion: 几何反馈驱动的自校正能够有效提升单图3D重建的结构准确性与视觉保真度，且无需额外监督，提供了改进LRM的通用、可扩展思路。

Abstract: Single-image 3D reconstruction with large reconstruction models (LRMs) has advanced rapidly, yet reconstructions often exhibit geometric inconsistencies and misaligned details that limit fidelity. We introduce GeoFusionLRM, a geometry-aware self-correction framework that leverages the model's own normal and depth predictions to refine structural accuracy. Unlike prior approaches that rely solely on features extracted from the input image, GeoFusionLRM feeds back geometric cues through a dedicated transformer and fusion module, enabling the model to correct errors and enforce consistency with the conditioning image. This design improves the alignment between the reconstructed mesh and the input views without additional supervision or external signals. Extensive experiments demonstrate that GeoFusionLRM achieves sharper geometry, more consistent normals, and higher fidelity than state-of-the-art LRM baselines.

</details>


### [95] [EgoSound: Benchmarking Sound Understanding in Egocentric Videos](https://arxiv.org/abs/2602.14122)
*Bingwen Zhu,Yuqian Fu,Qiaole Dong,Guolei Sun,Tianwen Qian,Yuzheng Wu,Danda Pani Paudel,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出EgoSound基准，系统评估MLLM在“第一人称（egocentric）声音理解”上的能力，涵盖7类任务、900段视频与7315条QA；实验显示模型有初步听觉推理，但在精细空间与因果理解上仍不足。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM多聚焦于视觉-语言，忽视人类多感官（尤其是声音）的关键作用；在第一人称场景中，声音与视觉高度耦合，可提供空间布局、屏外事件与因果线索，缺少系统评测标准阻碍进展。

Method: 构建EgoSound基准：整合Ego4D与EgoBlind数据，覆盖有视与依赖声音的体验；定义7类任务（本征声音感知、空间定位、因果推断、跨模态推理等）；采用多阶段自动生成流水线产出问答，并进行验证，最终形成7315个QA、900个视频；用9个SOTA MLLM做全面评测。

Result: 现有MLLM展现出初步的听觉推理能力，但在细粒度空间定位与因果理解上表现欠佳；EgoSound任务整体具有挑战性，暴露了模型在多模态（含音频）耦合推理方面的明显短板。

Conclusion: EgoSound为推进多感官第一人称智能提供了标准化、具有挑战性的评测平台，有望推动模型从“能看会说”迈向“能看会听、能因果推理”的方向，缩小视觉与听觉理解间的鸿沟。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.

</details>


### [96] [DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors](https://arxiv.org/abs/2602.14134)
*Yi Li,Hongze Shen,Lexiang Tang,Xin Li,Xinpeng Ding,Yinsong Liu,Deqiang Jiang,Xing Sun,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出DenseMLLM，在不引入任务特定解码器的前提下，让标准MLLM完成语义分割、深度估计等稠密预测，并在多项稠密与视觉-语言基准上达到了具有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在高层视觉理解强，但做稠密预测往往需复杂、任务定制的解码器，破坏通用架构、增加复杂度与部署成本。作者希望保持标准MLLM的一体化与通用性，同时获得稠密感知能力。

Method: 基于标准MLLM架构，引入对视觉token的监督策略，使单一模型通过多标签/多任务的token级监督学习稠密预测，无需为不同任务设计专用解码器或额外头部。

Result: 在语义分割、深度估计等多种稠密预测与多模态基准上取得与现有方法相当或有竞争力的性能，证明最小化改动即可实现强稠密能力。

Conclusion: 标准、通用的MLLM在配合新颖的视觉token监督后，能够无需架构专化即可有效支持稠密感知，缓解了“通用性 vs. 任务特化”的矛盾。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

</details>


### [97] [Detection of On-Ground Chestnuts Using Artificial Intelligence Toward Automated Picking](https://arxiv.org/abs/2602.14140)
*Kaixuan Fang,Yuzhen Lu,Xinyang Mu*

Main category: cs.CV

TL;DR: 评测319张图像（6524标注）上29个实时检测器；YOLO总体优于RT‑DETR；最佳mAP@0.5为YOLOv12m的95.1%，最佳mAP@[0.5:0.95]为YOLOv11x的80.1%；数据与代码开源。


<details>
  <summary>Details</summary>
Motivation: 小规模板栗种植者缺乏低成本、选择性强且不损果的机械采收方案；要实现视觉引导的自动采收，首要难题是地面板栗在复杂自然环境中的稳健检测（遮阴、光照变化、杂草落叶石块等干扰）。

Method: 采集并标注 orchard 地面板栗数据集（319图、6524实例）；系统性复现实验评估29个主流实时目标检测器：YOLO家族（v11–v13，不同模型规模）与RT‑DETR家族（v1–v4，不同骨干与尺度）；比较mAP@0.5、mAP@[0.5:0.95]与推理速度，面向车载实时应用。

Result: YOLOv12m在mAP@0.5上最高（95.1%）；RT‑DETR中以RT‑DETRv2‑R101最佳（91.1% mAP@0.5）；mAP@[0.5:0.95]最高为YOLOv11x（80.1%）；总体上YOLO在精度与推理均优于RT‑DETR，均具实时潜力。

Conclusion: 在复杂果园地面环境下，YOLO系更适合车载部署以用于低成本自动化板栗采收；公开数据集与代码为后续研究与工程落地提供基准与资源。

Abstract: Traditional mechanized chestnut harvesting is too costly for small producers, non-selective, and prone to damaging nuts. Accurate, reliable detection of chestnuts on the orchard floor is crucial for developing low-cost, vision-guided automated harvesting technology. However, developing a reliable chestnut detection system faces challenges in complex environments with shading, varying natural light conditions, and interference from weeds, fallen leaves, stones, and other foreign on-ground objects, which have remained unaddressed. This study collected 319 images of chestnuts on the orchard floor, containing 6524 annotated chestnuts. A comprehensive set of 29 state-of-the-art real-time object detectors, including 14 in the YOLO (v11-13) and 15 in the RT-DETR (v1-v4) families at varied model scales, was systematically evaluated through replicated modeling experiments for chestnut detection. Experimental results show that the YOLOv12m model achieves the best mAP@0.5 of 95.1% among all the evaluated models, while the RT-DETRv2-R101 was the most accurate variant among RT-DETR models, with mAP@0.5 of 91.1%. In terms of mAP@[0.5:0.95], the YOLOv11x model achieved the best accuracy of 80.1%. All models demonstrate significant potential for real-time chestnut detection, and YOLO models outperformed RT-DETR models in terms of both detection accuracy and inference, making them better suited for on-board deployment. Both the dataset and software programs in this study have been made publicly available at https://github.com/AgFood-Sensing-and-Intelligence-Lab/ChestnutDetection.

</details>


### [98] [LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models](https://arxiv.org/abs/2602.14147)
*Shufan Li,Yuchen Zhu,Jiuxiang Gu,Kangning Liu,Zhe Lin,Yongxin Chen,Molei Tao,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: LaViDa-R1 是一种多模态扩散语言模型，用统一的后训练框架把SFT与多任务RL结合，借助答案强制、树搜索与互补似然估计等技巧，在视觉数学、推理型指代/定位与图像编辑等任务上取得强性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理型dLLM多依赖任务特定的强化学习，难以统一覆盖理解与生成等多模态任务，且在可扩展性与有效性上受限。作者希望构建一个通用、多任务的多模态推理dLLM，并探索将SFT与多任务RL无缝整合的训练范式。

Method: 提出LaViDa-R1：在扩散语言模型框架下，采用统一的后训练流程结合SFT与多任务RL；引入答案强制(answer-forcing)以稳定目标导向学习，利用树搜索提升策略探索与推理质量，并用互补似然估计(complementary likelihood estimation)改善训练信号与可扩展性；在多种多模态理解与生成任务上进行联合训练。

Result: 在广泛的多模态任务上表现强劲，特别是在视觉数学推理、重度推理的定位/对齐(grounding)以及图像编辑等方面取得领先或显著提升的实验结果。

Conclusion: 统一的SFT+多任务RL框架与新训练技巧能有效提升dLLM的通用多模态推理与生成能力，证明了扩散式LLM在多模态领域作为通用推理引擎的可行性与竞争力。

Abstract: Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.

</details>


### [99] [ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery](https://arxiv.org/abs/2602.14153)
*Zheng Han,Zixin Yang,Yonghao Long,Lin Zhang,Peter Kazanzides,Qi Dou*

Main category: cs.CV

TL;DR: 提出ARport：在无标记、无外部传感器的OST-HMD上，将术前规划的机械臂穿刺口布局精确投射到患者体表，提升端口放置的准确与效率。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术中，端口（穿刺套管）布局直接影响视野与器械可操作性；现有从术前规划到术中执行之间缺少直观、准确且流程友好的对接方式。

Method: 在光学透视式头显上融合RGB、深度与位姿数据重建术区；利用基础模型分割提取患者体表；对术前解剖模型与提取的体表进行基于表面的无标记配准；将规划的穿刺口布局原位可视化叠加到体表，全流程无需外部传感器或标记物。

Result: 在等比例人体仿真模型实验中，系统能将规划的穿刺点稳定、准确地叠加到实体模型上，虚实解剖保持一致的空间对应。

Conclusion: ARport以最小硬件、完全无标记的方式，将术前端口规划直接投射到患者体表，简化术中准备并具备进入常规临床流程的潜力。

Abstract: Purpose: Precise port placement is a critical step in robot-assisted surgery, where port configuration influences both visual access to the operative field and instrument maneuverability. To bridge the gap between preoperative planning and intraoperative execution, we present ARport, an augmented reality (AR) system that automatically maps pre-planned trocar layouts onto the patient's body surface, providing intuitive spatial guidance during surgical preparation. Methods: ARport, implemented on an optical see-through head-mounted display (OST-HMD), operates without any external sensors or markers, simplifying setup and enhancing workflow integration. It reconstructs the operative scene from RGB, depth, and pose data captured by the OST-HMD, extracts the patient's body surface using a foundation model, and performs surface-based markerless registration to align preoperative anatomical models to the extracted patient's body surface, enabling in-situ visualization of planned trocar layouts. A demonstration video illustrating the overall workflow is available online. Results: In full-scale human-phantom experiments, ARport accurately overlaid pre-planned trocar sites onto the physical phantom, achieving consistent spatial correspondence between virtual plans and real anatomy. Conclusion: ARport provides a fully marker-free and hardware-minimal solution for visualizing preoperative trocar plans directly on the patient's body surface. The system facilitates efficient intraoperative setup and demonstrates potential for seamless integration into routine clinical workflows.

</details>


### [100] [When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance](https://arxiv.org/abs/2602.14157)
*Ahmed Ghorbel,Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: 提出一种无需VJP的测试时引导用于文本驱动图像/视频修补编辑，理论分析并扩展实证；在大规模基准上达到或超越训练式方法。


<details>
  <summary>Details</summary>
Motivation: 文本驱动的编辑可视为修补任务，需要在保持观测一致性的同时满足文本提示。但现有扩散/流模型的测试时引导依赖昂贵的VJP近似指导项，计算代价高、限制实用性。近期工作提出VJP-free近似，但缺乏系统理论解释与大规模验证。

Method: 基于Moufad等(2025)的VJP-free测试时引导，给出其近似的理论洞见；将该方案用于图像与视频的掩膜修补式编辑，并在大规模基准上系统评测，与训练式方法对比。

Result: 仅依赖测试时引导（无需再训练）即可在图像与视频编辑基准上达到与训练式方法相当，部分情形还更优；显著减少计算代价与工程复杂度。

Conclusion: VJP-free测试时引导为文本驱动编辑提供了高效、通用的方案，兼具理论合理性与强实证表现，有潜力成为训练密集型方法的可行替代。

Abstract: Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.

</details>


### [101] [Towards Spatial Transcriptomics-driven Pathology Foundation Models](https://arxiv.org/abs/2602.14177)
*Konstantin Hemker,Andrew H. Song,Cristina Almagro-Pérez,Guillaume Jaume,Sophia J. Wagner,Anurag Vaidya,Nikola Simidjievski,Mateja Jamnik,Faisal Mahmood*

Main category: cs.CV

TL;DR: SEAL 提出一种将空间转录组学（ST）的局部分子信息注入病理视觉编码器的参数高效自监督微调框架，在多器官大规模配对数据上训练，广泛下游任务上相较纯视觉与ST预测基线显著提升，并具备跨域泛化与基因到图像检索等新跨模态能力。


<details>
  <summary>Details</summary>
Motivation: 传统病理视觉模型仅依赖形态学，忽视了与之紧密耦合的局部分子表达，限制了对分子状态、通路活性与治疗反应等任务的表征力；与此同时，多模态基础模型成功启示通过形态-分子耦合可系统性提升视觉表征，因此需要一个能将ST信号无缝注入现有病理基础模型的通用、低成本方案。

Method: 提出Spatial Expression-Aligned Learning（SEAL）：一种愿景-组学自监督微调方法，不从零训练，而以参数高效的方式对主流病理视觉基础模型进行“组学对齐”微调。以>70万对基因表达spot与组织局部区域的配对样本（14个器官、肿瘤与正常）进行训练，使编码器学习局部形态与分子表达的对齐表示；作为“即插即用”替代件应用于多种下游任务与评测。

Result: 在38个切片级与15个patch级任务中，相较纯视觉基础模型与基于ST的预测基线，SEAL稳定提升性能，覆盖分子状态、通路活性、治疗反应预测及patch级基因表达预测；展示了对OOD数据的稳健域泛化；实现了新的跨模态功能，如基因到图像的检索。

Conclusion: 用ST引导对病理基础模型进行参数高效的组学对齐微调，是提升视觉表征与扩展跨模态能力的有效、可行通用路径；SEAL可作为现有病理视觉模型的增强模块，推动形态-分子耦合在实际应用中的落地。

Abstract: Spatial transcriptomics (ST) provides spatially resolved measurements of gene expression, enabling characterization of the molecular landscape of human tissue beyond histological assessment as well as localized readouts that can be aligned with morphology. Concurrently, the success of multimodal foundation models that integrate vision with complementary modalities suggests that morphomolecular coupling between local expression and morphology can be systematically used to improve histological representations themselves. We introduce Spatial Expression-Aligned Learning (SEAL), a vision-omics self-supervised learning framework that infuses localized molecular information into pathology vision encoders. Rather than training new encoders from scratch, SEAL is designed as a parameter-efficient vision-omics finetuning method that can be flexibly applied to widely used pathology foundation models. We instantiate SEAL by training on over 700,000 paired gene expression spot-tissue region examples spanning tumor and normal samples from 14 organs. Tested across 38 slide-level and 15 patch-level downstream tasks, SEAL provides a drop-in replacement for pathology foundation models that consistently improves performance over widely used vision-only and ST prediction baselines on slide-level molecular status, pathway activity, and treatment response prediction, as well as patch-level gene expression prediction tasks. Additionally, SEAL encoders exhibit robust domain generalization on out-of-distribution evaluations and enable new cross-modal capabilities such as gene-to-image retrieval. Our work proposes a general framework for ST-guided finetuning of pathology foundation models, showing that augmenting existing models with localized molecular supervision is an effective and practical step for improving visual representations and expanding their cross-modal utility.

</details>


### [102] [UniWeTok: An Unified Binary Tokenizer with Codebook Size $\mathit{2^{128}}$ for Unified Multimodal Large Language Model](https://arxiv.org/abs/2602.14178)
*Shaobin Zhuang,Yuang Ai,Jiaming Han,Weijia Mao,Xiaohui Li,Fangyikang Wang,Xiao Wang,Yan Li,Shanchuan Lin,Kun Xu,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen,Yali Wang*

Main category: cs.CV

TL;DR: UniWeTok 提出一个面向统一多模态大模型的离散视觉 tokenizer，用超大二进制码本（2^128）与三阶段训练，兼顾高保真重建、丰富语义与生成适配，在ImageNet生成、通用多模态理解/生成/编辑上达SOTA或强竞争力，并以更低训练算力达成。


<details>
  <summary>Details</summary>
Motivation: 现有视觉tokenizer难以同时满足三目标：高保真重建、复杂语义抽取、对生成任务的友好性；训练中还存在语义蒸馏不稳定、token熵与commitment损失的优化冲突，以及跨分辨率与对人脸/文本等敏感感知场景的适配问题。

Method: 1) 设计统一离散tokenizer：使用巨大二进制码本（2^128）。2) 训练框架：引入Pre-Post Distillation以增强语义抽取，加入Generative-Aware Prior以提升生成先验；三阶段训练以适配多分辨率和敏感场景。3) 架构：卷积-注意力混合编码器，提出SigLu激活以界定编码器输出范围、稳定语义蒸馏，并缓解token熵损失与commitment损失的优化冲突。

Result: - ImageNet 生成：FID 1.38（优于REPA 1.42），训练token量33B（远低于REPA 262B）。- 通用域：多模态理解、图像生成（DPG 86.63优于FLUX.1[Dev] 83.84）、图像编辑（GEdit 5.09优于OmniGen 5.06）表现强。

Conclusion: UniWeTok作为统一离散视觉tokenizer，在低算力下同时实现高重建保真、强语义与生成适配，在标准与通用多模态任务上达SOTA或接近SOTA，代码与模型已开源以促进统一tokenizer与MLLM研究。

Abstract: Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.

</details>


### [103] [UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing](https://arxiv.org/abs/2602.14186)
*Hongyang Wei,Bin Wen,Yancheng Long,Yankai Yang,Yuhang Hu,Tianke Zhang,Wei Chen,Haonan Fan,Kaiyu Jiang,Jiankang Chen,Changyi Liu,Kaiyu Tang,Haojie Ding,Xiao Yang,Jia Sun,Huaiqing Wang,Zhenyu Yang,Xinyu Wei,Xianglong He,Yangguang Li,Fan Yang,Tingting Gao,Lei Zhang,Guorui Zhou,Han Li*

Main category: cs.CV

TL;DR: UniRef-Image-Edit 统一单图编辑与多图合成，通过序列化多参考图像为共享潜空间序列（SELF），配合两阶段训练（SFT+RL）与渐进像素预算，提升细节与跨参考一致性；RL 中提出 MSGRPO 以解决多源约束冲突，显著增强组合一致性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式编辑在多条件、多参考场景下难保持一致性，主要因参考间交互受限与输入表示割裂；需要统一的输入表示与优化机制来同时覆盖单图编辑与多图合成并权衡冲突约束。

Method: 1) 提出 SELF：将多张参考图动态序列化为固定长度潜空间序列，在全局像素预算下联合约束。2) 两阶段训练：SFT 同时在单图编辑与多图合成上训练，采用渐进序列长度/像素预算（1024^2→1536^2→2048^2）以逐步放宽压缩并提升细节与对齐；3) RL 阶段提出 MSGRPO，面向多参考图像生成的强化学习框架，优化在冲突视觉约束下的组合一致性。

Result: 模型在跨参考一致性、视觉保真与组合能力上优于现有方法，能稳定处理单图编辑与多图合成；RL 的 MSGRPO 进一步提升在冲突条件下的对齐与一致性。

Conclusion: 统一的 SELF 表示加上 SFT+RL（MSGRPO）训练范式，可在同一框架内高效解决单图编辑与多图合成，兼顾细节、稳定性与一致性；代码、模型与数据将开源以促进社区研究。

Abstract: We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.

</details>


### [104] [GeoEyes: On-Demand Visual Focusing for Evidence-Grounded Understanding of Ultra-High-Resolution Remote Sensing Imagery](https://arxiv.org/abs/2602.14201)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yifan Zhang,Long Lan,Xue Yang,Hongda Sun,Yulin Wang,Di Wang,Jun Song,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 提出GeoEyes训练框架，结合UHR-CoZ冷启动数据与AdaZoom-GRPO强化学习，缓解MLLM在超高分辨率遥感VQA中的同质化缩放工具使用问题，学会按需缩放与适时停止，在XLRS-Bench上达54.23%准确率。


<details>
  <summary>Details</summary>
Motivation: 在超高分辨率遥感VQA中，关键信息稀疏且微小，需通过“带图思维”的缩放工具主动探索。但现有支持缩放的MLLM出现“工具使用同质化”，即与任务无关的固定缩放模式，导致证据获取低效、答案不佳。需要一种能按任务需求动态缩放并有效停止的训练机制。

Method: 提出GeoEyes分阶段训练：1）构建覆盖多种缩放策略的SFT数据UHR-CoZ进行冷启动，教会模型基本的多阶段缩放与证据链；2）设计代理式强化学习AdaZoom-GRPO，在交互中对“证据增益”和“答案改进”进行显式奖励，引导模型学会何时放大、放大到哪里、何时停止。

Result: 训练后的模型学会按需缩放并具备合理停止行为，在多个UHR遥感基准上显著提升，XLRS-Bench准确率达到54.23%。

Conclusion: 通过结合覆盖多样缩放范式的监督与以证据为中心的强化学习，GeoEyes缓解工具使用同质化，提升了UHR遥感VQA中的证据采集与回答质量，验证了面向按需缩放与停止策略的训练范式的有效性。

Abstract: The "thinking-with-images" paradigm enables multimodal large language models (MLLMs) to actively explore visual scenes via zoom-in tools. This is essential for ultra-high-resolution (UHR) remote sensing VQA, where task-relevant cues are sparse and tiny. However, we observe a consistent failure mode in existing zoom-enabled MLLMs: Tool Usage Homogenization, where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition. To address this, we propose GeoEyes, a staged training framework consisting of (1) a cold-start SFT dataset, UHR Chain-of-Zoom (UHR-CoZ), which covers diverse zooming regimes, and (2) an agentic reinforcement learning method, AdaZoom-GRPO, that explicitly rewards evidence gain and answer improvement during zoom interactions. The resulting model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.

</details>


### [105] [HiVid: LLM-Guided Video Saliency For Content-Aware VOD And Live Streaming](https://arxiv.org/abs/2602.14214)
*Jiahui Chen,Bo Peng,Lianchen Jia,Zeyu Zhang,Tianchi Huang,Lifeng Sun*

Main category: cs.CV

TL;DR: HiVid利用LLM充当可扩展“人类代理”，为VOD与直播按片段生成高保真内容权重，解决多模态受限、全局一致性与低延迟预测三大难题，在准确率与QoE相关性上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 内容感知流媒体需要精细到切片级的重要性权重以优化主观QoE，但人工标注成本高、视觉显著性模型泛化差，缺乏可扩展且高保真的权重生成途径。

Method: 提出HiVid框架：1) 感知模块：在局部时窗内让LLM逐帧评估并自回归整合，缓解多模态与token上限限制；2) 排序模块（用于VOD）：设计LLM引导的“归并排序”式全局重排，修正不同局部窗口评分不一致；3) 预测模块（用于直播）：构建多模态时间序列模型，含内容感知注意力与自适应预测视野，适配异步LLM推理，在无未来信息、低时延条件下在线预测未来权重。

Result: 相较SOTA，VOD权重预测准确率提升最高11.5%，直播场景提升26%；用户研究显示与QoE的相关性提升14.7%。

Conclusion: LLM可作为可扩展的人类代理，为点播与直播生成高保真内容权重；通过局部感知、自回归整合、全局重排与低时延预测，使内容感知流媒体的权重估计更准确、更贴近用户QoE，具有实际部署潜力。

Abstract: Content-aware streaming requires dynamic, chunk-level importance weights to optimize subjective quality of experience (QoE). However, direct human annotation is prohibitively expensive while vision-saliency models generalize poorly. We introduce HiVid, the first framework to leverage Large Language Models (LLMs) as a scalable human proxy to generate high-fidelity weights for both Video-on-Demand (VOD) and live streaming. We address 3 non-trivial challenges: (1) To extend LLMs' limited modality and circumvent token limits, we propose a perception module to assess frames in a local context window, autoregressively building a coherent understanding of the video. (2) For VOD with rating inconsistency across local windows, we propose a ranking module to perform global re-ranking with a novel LLM-guided merge-sort algorithm. (3) For live streaming which requires low-latency, online inference without future knowledge, we propose a prediction module to predict future weights with a multi-modal time series model, which comprises a content-aware attention and adaptive horizon to accommodate asynchronous LLM inference. Extensive experiments show HiVid improves weight prediction accuracy by up to 11.5\% for VOD and 26\% for live streaming over SOTA baselines. Real-world user study validates HiVid boosts streaming QoE correlation by 14.7\%.

</details>


### [106] [Freq-DP Net: A Dual-Branch Network for Fence Removal using Dual-Pixel and Fourier Priors](https://arxiv.org/abs/2602.14226)
*Kunal Swami,Sudha Velusamy,Chandra Sekhar Seelamantula*

Main category: cs.CV

TL;DR: 提出 Freq-DP Net：利用双像素(DP)传感器的散焦视差与栅格结构频域先验，实现单张图像去围栏遮挡，刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 单张图像的围栏遮挡严重影响视觉质量与下游任务。现有方法要么依赖多帧运动线索、要么在静态场景失效。DP 传感器可提供左右子视角导致的散焦视差信息，潜在能在单帧中解析前景围栏与背景分离，填补该空白。

Method: 构建双分支网络 Freq-DP Net：1) 几何分支：基于 DP 产生的散焦视差，显式构建 cost volume 获取几何先验；2) 结构分支：用 Fast Fourier Convolution(FFC) 学习围栏的全局周期/栅格结构先验；3) 注意力融合模块自适应地融合两类先验，得到高精度围栏分割；并据此执行围栏去除。另构建多样化围栏数据基准用于训练与评测。

Result: 在新建立的多样化围栏基准上，方法显著优于强通用基线，在单张图像、基于 DP 的去围栏任务上达到新的 SOTA。

Conclusion: 将 DP 散焦几何与频域结构先验结合，并通过注意力融合，可在无需多帧运动的前提下实现高精度围栏分割与去除；数据集与实验验证其有效性并树立新基准。

Abstract: Removing fence occlusions from single images is a challenging task that degrades visual quality and limits downstream computer vision applications. Existing methods often fail on static scenes or require motion cues from multiple frames. To overcome these limitations, we introduce the first framework to leverage dual-pixel (DP) sensors for this problem. We propose Freq-DP Net, a novel dual-branch network that fuses two complementary priors: a geometric prior from defocus disparity, modeled using an explicit cost volume, and a structural prior of the fence's global pattern, learned via Fast Fourier Convolution (FFC). An attention mechanism intelligently merges these cues for highly accurate fence segmentation. To validate our approach, we build and release a diverse benchmark with different fence varieties. Experiments demonstrate that our method significantly outperforms strong general-purpose baselines, establishing a new state-of-the-art for single-image, DP-based fence removal.

</details>


### [107] [Learning Significant Persistent Homology Features for 3D Shape Understanding](https://arxiv.org/abs/2602.14228)
*Prachi Kudeshia,Jiju Poovvancheri*

Main category: cs.CV

TL;DR: 提出为ModelNet40与ShapeNet构建含持续同调（PH）特征的拓扑增强数据集，并提出TopoGAT从几何数据与PH签名中学习选择显著持久点，优于手工统计准则，提升点云分类与分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云基准主要表征几何，忽略拓扑结构，限制了对拓扑感知深度模型的系统研究与评测。需要统一几何-拓扑学习的数据与方法，以将持续同调更广泛融入实际深度学习流程。

Method: 1) 为ModelNet40与ShapeNet生成点云对应的持续同调签名（如持久图/条形图），作为拓扑增强基准。2) 提出TopoGAT：基于图注意力的端到端方法，从输入点云与其PH特征联合学习，选择最具判别力与稳定性的显著持久点，替代手工统计阈值或启发式。3) 将所选显著点作为附加信号并入常规分类与部件分割网络。

Result: 与传统基于统计的显著点选择相比，TopoGAT在稳定性与判别力上更优；将其输出整合到标准点云分类与分割管线，取得更高的分类准确率与分割指标。

Conclusion: 拓扑增强数据集与可学习的显著持久点选择共同促进了持续同调在3D点云深度学习中的实用集成，带来性能提升并为评测拓扑感知架构奠定基准。

Abstract: Geometry and topology constitute complementary descriptors of three-dimensional shape, yet existing benchmark datasets primarily capture geometric information while neglecting topological structure. This work addresses this limitation by introducing topologically-enriched versions of ModelNet40 and ShapeNet, where each point cloud is augmented with its corresponding persistent homology features. These benchmarks with the topological signatures establish a foundation for unified geometry-topology learning and enable systematic evaluation of topology-aware deep learning architectures for 3D shape analysis. Building on this foundation, we propose a deep learning-based significant persistent point selection method, \textit{TopoGAT}, that learns to identify the most informative topological features directly from input data and the corresponding topological signatures, circumventing the limitations of hand-crafted statistical selection criteria. A comparative study verifies the superiority of the proposed method over traditional statistical approaches in terms of stability and discriminative power. Integrating the selected significant persistent points into standard point cloud classification and part-segmentation pipelines yields improvements in both classification accuracy and segmentation metrics. The presented topologically-enriched datasets, coupled with our learnable significant feature selection approach, enable the broader integration of persistent homology into the practical deep learning workflows for 3D point cloud analysis.

</details>


### [108] [Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236)
*Vishnu Sai,Dheeraj Sai,Srinath B,Girish Varma,Priyesh Shukla*

Main category: cs.CV

TL;DR: 提出Sali-Cache，在注意力计算前用时空双信号筛选冗余视频内容，实现KV缓存主动管理，在LLaVA‑1.6上以约2.2倍内存压缩保持各指标100%准确，适配长视频高效推理。


<details>
  <summary>Details</summary>
Motivation: VLM在长视频处理中，KV缓存随序列线性增长引发内存瓶颈；现有方法多在计算完全量注意力后再逐步淘汰token，导致显著的计算浪费与无法在消费级硬件上长时程处理。

Method: 提出“先验”主动缓存管理框架Sali-Cache：1) 时间过滤器：基于光流检测帧间冗余，减少时间维重复信息进入KV；2) 空间过滤器：基于显著性检测定位视觉重要区域，仅缓存显著patch/token；两者构成双信号自适应缓存，在进入自注意力前进行筛选与配额分配，实现动态KV存储管理。

Result: 在LLaVA 1.6上，Sali-Cache将有效内存使用压缩至原来的约1/2.20，同时在BLEU、ROUGE-L、Exact Match等指标上保持100%准确；在相同内存预算下，能保留更长时间范围内的上下文特征而不降性能。

Conclusion: 通过时空显著性驱动的先验KV缓存管理，Sali-Cache避免了反应式淘汰的计算浪费，在消费级硬件上实现长视频高效推理且不损失精度，为VLM长序列处理提供了一种实用的内存优化范式。

Abstract: Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.

</details>


### [109] [AbracADDbra: Touch-Guided Object Addition by Decoupling Placement and Editing Subtasks](https://arxiv.org/abs/2602.14237)
*Kunal Swami,Raghu Chittersu,Yuvraj Rathore,Rajeev Irny,Shashavali Doodekula,Alok Shukla*

Main category: cs.CV

TL;DR: AbracADDbra通过“触碰先验”将简短指令落地到具体空间位置，采用解耦架构：VLM做触点引导的放置预测，扩散模型联合生成目标与实例掩码以实现高保真融合；并提出Touch2Add基准。实验显示放置模型显著优于随机与通用VLM，且初始放置精度与最终编辑质量强相关。


<details>
  <summary>Details</summary>
Motivation: 文本指令添加物体常因语义歧义或需要手工掩码而影响可用性与效率。需要一种更直观的人机交互方式，把简短语言意图准确地映射到图像中的具体位置，并能稳定产生高质量合成结果与可评测标准。

Method: 提出AbracADDbra：1) 解耦两阶段架构；2) 阶段一使用视觉-语言Transformer，根据用户触点（touch prior）和简短文本预测物体的空间放置；3) 阶段二用扩散模型联合生成目标对象和其实例掩码，实现自然融合；4) 构建Touch2Add基准，用于交互式添加任务的标准化评测；5) 对比随机放置和通用VLM基线，并分析放置精度与编辑质量的相关性。

Result: 放置模型在位置预测上显著优于随机与通用VLM基线；整体框架能产生高保真的编辑结果。实验表明初始放置准确度与最终编辑质量高度相关，支撑了解耦设计的有效性。

Conclusion: 触点引导+解耦架构有效缩小了文本歧义与掩码繁琐带来的可用性鸿沟，提供更易用且高效的创作工具；Touch2Add为该交互式任务提供评测标准，放置精度是最终质量的关键因素。

Abstract: Instruction-based object addition is often hindered by the ambiguity of text-only prompts or the tedious nature of mask-based inputs. To address this usability gap, we introduce AbracADDbra, a user-friendly framework that leverages intuitive touch priors to spatially ground succinct instructions for precise placement. Our efficient, decoupled architecture uses a vision-language transformer for touch-guided placement, followed by a diffusion model that jointly generates the object and an instance mask for high-fidelity blending. To facilitate standardized evaluation, we contribute the Touch2Add benchmark for this interactive task. Our extensive evaluations, where our placement model significantly outperforms both random placement and general-purpose VLM baselines, confirm the framework's ability to produce high-fidelity edits. Furthermore, our analysis reveals a strong correlation between initial placement accuracy and final edit quality, validating our decoupled approach. This work thus paves the way for more accessible and efficient creative tools.

</details>


### [110] [Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision](https://arxiv.org/abs/2602.14276)
*A. Said Gurbuz,Sunghwan Hong,Ahmed Nassar,Marc Pollefeys,Peter Staar*

Main category: cs.CV

TL;DR: 提出ScreenParse数据集与ScreenVLM模型，实现密集UI屏幕解析，显著提升结构化识别与跨基准迁移，并以小模型高效低延迟运行。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理需要对屏幕进行结构化理解，但可用的数据集多为稀疏标注、标签多样性低、覆盖不足，限制了泛化；同时实际应用要求高效率、低延迟与端侧可用。

Method: 构建ScreenParse：通过自动化Webshot管线渲染海量网页截图，提取UI框、55类类型与文本，并用VLM重标与质量过滤，得到771K截图、2100万元素的密集标注；提出ScreenTag紧凑标记表示，并训练316M参数的ScreenVLM，采用结构感知损失上调关键结构token权重，实现端到端解码。

Result: ScreenVLM在密集解析上显著优于更大基础VLM（如在ScreenParse上PageIoU 0.592 vs 0.294），并在公共基准上强迁移；对基础VLM进行ScreenParse微调也稳定提升其grounding性能。

Conclusion: 密集屏幕监督提供可迁移的结构先验，小型VLM借助ScreenParse与结构化目标可达更高精度与效率，适用于低延迟、端侧UI理解与指令落地场景。

Abstract: Modern computer-use agents (CUA) must perceive a screen as a structured state, what elements are visible, where they are, and what text they contain, before they can reliably ground instructions and act. Yet, most available grounding datasets provide sparse supervision, with insufficient and low-diversity labels that annotate only a small subset of task-relevant elements per screen, which limits both coverage and generalization; moreover, practical deployment requires efficiency to enable low-latency, on-device use. We introduce ScreenParse, a large-scale dataset for complete screen parsing, with dense annotations of all visible UI elements (boxes, 55-class types, and text) across 771K web screenshots (21M elements). ScreenParse is generated by Webshot, an automated, scalable pipeline that renders diverse urls, extracts annotations and applies VLM-based relabeling and quality filtering. Using ScreenParse, we train ScreenVLM, a compact, 316M-parameter vision language model (VLM) that decodes a compact ScreenTag markup representation with a structure-aware loss that upweights structure-critical tokens. ScreenVLM substantially outperforms much larger foundation VLMs on dense parsing (e.g., 0.592 vs. 0.294 PageIoU on ScreenParse) and shows strong transfer to public benchmarks. Moreover, finetuning foundation VLMs on ScreenParse consistently improves their grounding performance, suggesting that dense screen supervision provides transferable structural priors for UI understanding. Project page: https://saidgurbuz.github.io/screenparse/.

</details>


### [111] [Differential pose optimization in descriptor space -- Combining Geometric and Photometric Methods for Motion Estimation](https://arxiv.org/abs/2602.14297)
*Andreas L. Teigen,Annette Stahl,Rudolf Mester*

Main category: cs.CV

TL;DR: 论文探讨在两帧相对位姿优化中，用“致密几何描述子残差”替代传统光度误差/重投影误差的统一方案，结果虽能精确跟踪但整体不如基于重投影误差的方法；原因可能是描述子相似度的梯度过于平缓、与关键点定位精度不强对应。


<details>
  <summary>Details</summary>
Motivation: 传统两帧位姿估计通常二选一：光度误差（密集、可亚像素，但易受光照/遮挡影响、闭环较难）或重投影误差（稀疏/几何特征，鲁棒、易闭环）。作者希望兼得二者优势：保持密集优化的亚像素可微特性，同时利用几何描述子的判别性与鲁棒性。

Method: 在密集配准框架中，用“致密采样的几何特征描述子”的残差替代光度残差：对图像网格提取描述子，构建跨帧的描述子匹配/相似度度量，进行可微的梯度下降求解位姿，从而实现亚像素级优化并注入几何表达能力。

Result: 实验显示该策略可实现精确跟踪，但总体性能仍落后于基于重投影误差（几何点-位姿）优化的方法，即便使用了更多信息。

Conclusion: 作者提出假设：常用描述子相似度度量的能量景观变化过缓（梯度弱/平坦），且其相似度不严格对应关键点的精确定位误差，导致优化对位姿的约束不如重投影误差有效；因此该统一方案难以超越重投影基线。

Abstract: One of the fundamental problems in computer vision is the two-frame relative pose optimization problem. Primarily, two different kinds of error values are used: photometric error and re-projection error. The selection of error value is usually directly dependent on the selection of feature paradigm, photometric features, or geometric features. It is a trade-off between accuracy, robustness, and the possibility of loop closing. We investigate a third method that combines the strengths of both paradigms into a unified approach. Using densely sampled geometric feature descriptors, we replace the photometric error with a descriptor residual from a dense set of descriptors, thereby enabling the employment of sub-pixel accuracy in differential photometric methods, along with the expressiveness of the geometric feature descriptor. Experiments show that although the proposed strategy is an interesting approach that results in accurate tracking, it ultimately does not outperform pose optimization strategies based on re-projection error despite utilizing more information. We proceed to analyze the underlying reason for this discrepancy and present the hypothesis that the descriptor similarity metric is too slowly varying and does not necessarily correspond strictly to keypoint placement accuracy.

</details>


### [112] [A Generative AI Approach for Reducing Skin Tone Bias in Skin Cancer Classification](https://arxiv.org/abs/2602.14356)
*Areez Muhammed Shabu,Mohammad Samar Ansari,Asra Aslam*

Main category: cs.CV

TL;DR: 提出用LoRA微调Stable Diffusion在深色皮肤子集上，生成按病灶类型与肤色条件的合成皮肤镜图像，用于数据增强，缓解ISIC数据集中肤色失衡；在分割与二分类任务上均提升，分类Accuracy达92.14%。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤癌AI多训练于浅色皮肤，ISIC数据集>70%浅色、<8%深色，导致对深色人群性能与公平性下降；需方法弥补人口学多样性不足以实现更公平的医疗影像诊断。

Method: 构建生成式增强流水线：以ISIC深色皮肤子集对预训练Stable Diffusion进行LoRA微调；按病灶类型与肤色进行条件生成，产生合成皮肤镜图；将真实+合成数据用于下游任务训练。评估两项任务：1) 病灶分割（IoU、Dice、边界精度）；2) 二分类（EfficientNet-B0）。

Result: 分割：在保留的真实测试集上，使用增强数据训练的模型在IoU、Dice与边界精度上均有一致提升，验证了合成数据的实用性；分类：EfficientNet-B0在增强数据上训练，达到92.14%准确率。

Conclusion: 生成式AI数据增强可缓解皮肤镜中肤色失衡带来的偏差，提升深色皮肤相关任务表现与公平性；该方向为更公平的皮肤科诊断提供可行路径，同时仍需更系统的公平性评估与真实世界验证。

Abstract: Skin cancer is one of the most common cancers worldwide and early detection is critical for effective treatment. However, current AI diagnostic tools are often trained on datasets dominated by lighter skin tones, leading to reduced accuracy and fairness for people with darker skin. The International Skin Imaging Collaboration (ISIC) dataset, one of the most widely used benchmarks, contains over 70% light skin images while dark skins fewer than 8%. This imbalance poses a significant barrier to equitable healthcare delivery and highlights the urgent need for methods that address demographic diversity in medical imaging. This paper addresses this challenge of skin tone imbalance in automated skin cancer detection using dermoscopic images. To overcome this, we present a generative augmentation pipeline that fine-tunes a pre-trained Stable Diffusion model using Low-Rank Adaptation (LoRA) on the image dark-skin subset of the ISIC dataset and generates synthetic dermoscopic images conditioned on lesion type and skin tone. In this study, we investigated the utility of these images on two downstream tasks: lesion segmentation and binary classification. For segmentation, models trained on the augmented dataset and evaluated on held-out real images show consistent improvements in IoU, Dice coefficient, and boundary accuracy. These evalutions provides the verification of Generated dataset. For classification, an EfficientNet-B0 model trained on the augmented dataset achieved 92.14% accuracy. This paper demonstrates that synthetic data augmentation with Generative AI integration can substantially reduce bias with increase fairness in conventional dermatological diagnostics and open challenges for future directions.

</details>


### [113] [Image-based Joint-level Detection for Inflammation in Rheumatoid Arthritis from Small and Imbalanced Data](https://arxiv.org/abs/2602.14365)
*Shun Kato,Yasushi Kondo,Shuntaro Saito,Yoshimitsu Aoki,Mariko Isogawa*

Main category: cs.CV

TL;DR: 论文提出一种从家庭拍摄的RGB手部图像检测类风湿关节炎（RA）炎症的方法，构建专用数据集，结合自监督预训练的全局-局部编码器与不平衡感知训练，在困难小样本、不平衡场景下，将F1提升0.2、G-mean提升0.25。


<details>
  <summary>Details</summary>
Motivation: RA需早诊断与紧密随访，但就诊延迟常导致不可逆关节损伤。家庭易得的RGB图像若能辅助炎症检测，可缓解专科稀缺与延迟问题。然而，基于RGB的RA炎症检测面临阳性样本稀缺、类别不平衡与视觉判别难度高，且缺乏系统性方法专门应对这些挑战。

Method: 1) 构建RA手部RGB图像数据集并量化任务难度；2) 提出全局-局部（global-local）编码器框架：在大规模健康手部图像上进行自监督预训练获取通用表征；3) 在下游RA炎症检测中引入不平衡感知训练（如重加权/重采样/阈值调整等机制）以缓解类别不平衡与阳性稀缺；4) 将全局上下文与关节局部特征融合进行判别。

Result: 与基线相比，方法的F1提高0.2，G-mean提高0.25，显示在不平衡小样本设置下检测性能更稳健。

Conclusion: RGB手部图像可用于RA炎症初筛与远程随访。通过自监督预训练的全局-局部编码器并结合不平衡训练，可有效缓解样本稀缺与不平衡带来的困难，取得较基线更好的判别性能。

Abstract: Rheumatoid arthritis (RA) is an autoimmune disease characterized by systemic joint inflammation. Early diagnosis and tight follow-up are essential to the management of RA, as ongoing inflammation can cause irreversible joint damage. The detection of arthritis is important for diagnosis and assessment of disease activity; however, it often takes a long time for patients to receive appropriate specialist care. Therefore, there is a strong need to develop systems that can detect joint inflammation easily using RGB images captured at home. Consequently, we tackle the task of RA inflammation detection from RGB hand images. This task is highly challenging due to general issues in medical imaging, such as the scarcity of positive samples, data imbalance, and the inherent difficulty of the task itself. However, to the best of our knowledge, no existing work has explicitly addressed these challenges in RGB-based RA inflammation detection. This paper quantitatively demonstrates the difficulty of visually detecting inflammation by constructing a dedicated dataset, and we propose a inflammation detection framework with global local encoder that combines self-supervised pretraining on large-scale healthy hand images with imbalance-aware training to detect RA-related joint inflammation from RGB hand images. Our experiments demonstrated that the proposed approach improves F1-score by 0.2 points and Gmean by 0.25 points compared with the baseline model.

</details>


### [114] [Event-based Visual Deformation Measurement](https://arxiv.org/abs/2602.14376)
*Yuliang Wu,Wei Zhai,Yuxin Cui,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出事件-帧融合的视觉形变测量方法，通过仿射不变单纯形分片与邻域贪心优化，在高动态场景中实现长时域、稠密形变跟踪，存储与算力开销仅为高速视频的约19%，在新建数据集上存活率提升1.6%。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的VDM依赖小位移假设与高帧率以缩小匹配搜索范围，但这在快速形变或资源受限下不现实；事件相机提供高时间分辨但稀疏且噪声大，单用也易歧义。作者动机是融合二者优势，并用物理先验缓解事件稀疏噪声，达到高动态、长时间、稠密且资源友好的形变估计。

Method: 1) 事件-帧融合框架：用事件提供时间上密集的运动线索，用帧提供空间上稠密且精确的几何与外观约束。2) 仿射不变单纯形(AIS)模型：将形变场分片为线性化的单纯形子区域，以低参数表示，利用固体弹性先验并具仿射不变性，降低由事件稀疏/噪声引起的匹配歧义。3) 邻域贪心优化：先让易收敛的子区稳健收敛，再以其结果引导相邻难收敛子区，抑制局部误差在长时域的积累并加速搜索。4) 构建包含>120序列、事件与帧时间对齐的基准数据集用于训练/评测。

Result: 在新数据集上，相较最新基线方法，存活率提升1.6%；相对于高速视频方案，仅用约18.9%的数据存储与处理资源即可达到更优或可比性能。

Conclusion: 事件-帧融合结合AIS分片与邻域贪心优化，可在高动态与长时序场景中实现稳健、稠密的视觉形变测量，兼顾精度与资源效率，并提供了标准化数据集促进后续研究。

Abstract: Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.

</details>


### [115] [Adapting VACE for Real-Time Autoregressive Video Diffusion](https://arxiv.org/abs/2602.14381)
*Ryan Fosdick*

Main category: cs.CV

TL;DR: 将VACE改造成可流式自回归视频生成：把参考帧从扩散潜空间转到并行条件通道，以保持因果注意力与固定块/KV缓存；可直接复用预训练权重。结构控制与修补仅增加20–30%时延、显存几乎不变，但参考一致性较批处理VACE显著下降；已开源实现。


<details>
  <summary>Details</summary>
Motivation: 原VACE依赖全序列双向注意力，无法适配需要固定分块与因果注意力的实时流式生成。业界需要统一控制能力（参考、结构、修补、续帧）且具低延迟的自回归视频生成方案。

Method: 核心改动：将参考帧从扩散潜变量通路移出，置入并行条件路径，从而不破坏自回归的因果注意力与KV缓存；保持固定chunk尺寸与流式推理。复用已有VACE权重，无需再训练。在1.3B与14B规模评估流式结构控制、修补、参考引导与时域扩展。

Result: 在1.3B与14B模型下，结构控制与修补仅带来20–30%额外时延，对显存开销可忽略；可流式化并维持统一控制。但参考到视频的保真度较批处理双向VACE显著下降（因因果注意力限制）。

Conclusion: 该改造实现了VACE在自回归流式场景的可用性与工程可行性（低额外延迟、显存几乎不变、复用权重），但以牺牲参考一致性为代价。代码已发布，适合实时应用与后续改进因果参考建模的研究。

Abstract: We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.

</details>


### [116] [Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models](https://arxiv.org/abs/2602.14399)
*In Chong Choi,Jiacheng Zhang,Feng Liu,Yiliao Song*

Main category: cs.CV

TL;DR: 提出MAPA，一种多轮自适应提示攻击，对齐LVLM中交替文本-视觉攻击并跨轮迭代调整，显著提升越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱在文本LLM中有效，但直接扩展到LVLM时，加入视觉输入常触发更强的安全防护，导致回复更保守，从而降低攻击成功率。需要一种能在多模态与多轮对话中逐步放大恶意且规避防御的策略。

Method: 两层自适应策略：1）回合内，在文本与视觉攻击动作之间交替，选择能诱导更恶意回复的模态与提示；2）回合间，根据模型反馈迭代“拉扯式”微调攻击轨迹，逐步放大恶意程度。整体为多轮自适应提示框架MAPA。

Result: 在多个基准上对四个LVLM（LLaVA-V1.6-Mistral-7B、Qwen2.5-VL-7B-Instruct、Llama-3.2-Vision-11B-Instruct、GPT-4o-mini）显著提升攻击成功率，相比SOTA提高约11–35%。

Conclusion: 多模态场景下，多轮、跨模态自适应的攻击策略更能绕过安全对齐。MAPA的双层设计在多种LVLM上稳定提升越狱效果，显示该类方法对现有防御具有普适威胁。

Abstract: Multi-turn jailbreak attacks are effective against text-only large language models (LLMs) by gradually introducing malicious content across turns. When extended to large vision-language models (LVLMs), we find that naively adding visual inputs can cause existing multi-turn jailbreaks to be easily defended. For example, overly malicious visual input will easily trigger the defense mechanism of safety-aligned LVLMs, making the response more conservative. To address this, we propose MAPA: a multi-turn adaptive prompting attack that 1) at each turn, alternates text-vision attack actions to elicit the most malicious response; and 2) across turns, adjusts the attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness. This two-level design enables MAPA to consistently outperform state-of-the-art methods, improving attack success rates by 11-35% on recent benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.

</details>


### [117] [pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI](https://arxiv.org/abs/2602.14401)
*Qingqian Yang,Hao Wang,Sai Qian Zhang,Jian Li,Yang Hua,Miao Pan,Tao Song,Zhengwei Qi,Haibing Guan*

Main category: cs.CV

TL;DR: 提出 pFedNavi：面向视觉-语言导航（VLN）的结构感知、动态自适应个性化联邦学习框架，通过层级混合系数选择并融合关键层，兼顾全局共享与本地特化；在 R2R/RxR 与 ResNet/CLIP 设置下，相比 FedAvg 基线提升成功率最高7.5%、轨迹保真度最高7.8%，并在非IID条件下收敛加速1.38倍。


<details>
  <summary>Details</summary>
Motivation: VLN 训练依赖大量来自私有室内环境的轨迹-指令数据，存在隐私风险。联邦学习可将数据留在端侧，但传统 FedAvg 在跨客户端环境与指令风格强异质性下效果欠佳，单一全局模型次优，亟需能在保持共享知识的同时实现细粒度个性化的方案。

Method: 提出 pFedNavi：1) 结构感知个性化——通过层级混合系数自适应识别对客户端敏感的网络组件（如编码-解码投影、对环境敏感的解码层）；2) 细粒度参数融合——对选中的组件进行个性化的参数混合/聚合，平衡全局共享与本地特化；3) 动态自适应——依据客户端特性与训练动态调节混合强度与层选择，以缓解非IID带来的偏差并加速收敛。

Result: 在 R2R 与 RxR 基准上、采用 ResNet 与 CLIP 视觉表征的多种设置中，pFedNavi 相比基于 FedAvg 的 VLN 基线：导航成功率最高+7.5%，轨迹保真度最高+7.8%，在非IID场景下收敛速度提升至1.38倍，且在各项指标上稳定领先。

Conclusion: 针对 VLN 的极端跨客户端异质性，pFedNavi 通过结构感知与动态层级个性化实现更好的全局-本地权衡，带来更高精度与更快收敛，验证了“在关键层个性化”的有效性与通用性。

Abstract: Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.

</details>


### [118] [Feature Recalibration Based Olfactory-Visual Multimodal Model for Fine-Grained Rice Deterioration Detection](https://arxiv.org/abs/2602.14408)
*Rongqiang Zhao,Hengrui Hu,Yijing Wang,Mingchun Sun,Jie Liu*

Main category: cs.CV

TL;DR: 提出一种基于嗅觉-视觉的多模态细粒度大米劣变检测方法，通过特征重标定与注意力机制，在不依赖高成本设备的前提下，实现99.89%分类准确率，并简化实地检测流程。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检测对细粒度异常表征/提取能力不足，且依赖高成本、耗时的设备（如高光谱相机、质谱仪），限制了实际应用的效率与普及。

Method: 1) 构建细粒度劣变嵌入构造器（FDEC），对带标注的多模态嵌入特征进行重建以增强样本表示；2) 设计细粒度劣变重校准注意网络（FDRA-Net），强调信号变化、提升对米粒表面细粒度劣变的敏感性；3) 采用嗅觉（气味/挥发性信息）与视觉融合的多模态框架，进行特征重标定与检测。

Result: 在公开/自建数据上达到99.89%分类准确率；相较SOTA方法精度更高、流程更简化；在实地检测中同样表现出较高精度与操作简便性。

Conclusion: 所提嗅觉-视觉多模态与特征重校准策略能有效提升细粒度劣变检测性能与实用性，降低设备与时间成本，并具备向其他农食场景推广的潜力。

Abstract: Multimodal methods are widely used in rice deterioration detection, which exhibit limited capability in representing and extracting fine-grained abnormal features. Moreover, these methods rely on devices, such as hyperspectral cameras and mass spectrometers, increasing detection costs and prolonging data acquisition time. To address these issues, we propose a feature recalibration based olfactory-visual multimodal model for fine-grained rice deterioration detection. The fine-grained deterioration embedding constructor (FDEC) is proposed to reconstruct the labeled multimodal embedded-feature dataset, enhancing sample representation. The fine-grained deterioration recalibration attention network (FDRA-Net) is proposed to emphasize signal variations and increase sensitivity to fine-grained deterioration on the rice surface. Experiments show that the proposed method achieves a classification accuracy of 99.89%. Compared with state-of-the-art methods, the detection accuracy is improved and the procedure is simplified. Furthermore, field detection demonstrates the advantages of accuracy and operational simplicity. The proposed method can also be extended to other agrifood in agriculture and food industry.

</details>


### [119] [Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.14409)
*Haichao Zhu,Zhaorui Yang,Qian Zhang*

Main category: cs.CV

TL;DR: 论文探讨将学习与几何在空间感知中的角色重新划分：让学习提出几何假设，几何算法负责裁决与估计。以RGB-D相对位姿估计为例，发现单用学习的位姿不可靠；学习生成的几何若与内参不对齐会伤性能；而对齐后的学习深度配合ICP几何处置能在中等难度刚体场景稳定提升。结论是几何不是简单的精化器，而是关键的判别与吸收环节，支持模块化、几何意识的系统设计。


<details>
  <summary>Details</summary>
Motivation: 空间感知需从视觉中恢复相机运动与场景结构。传统依赖几何与物理一致性；新兴学习方法表现强，但尚不清楚应直接替代几何估计，还是作为几何管线中的中间模块。作者动机是系统性评估“学习提案+几何裁决”的端到端模块化框架在实际相对位姿任务中的有效性与边界。

Method: 构建端到端模块化框架：以VGGT为代表的学习模型分别提出位姿与深度假设；随后采用经典点到平面RGB-D ICP作为几何后端进行估计“处置”。在TUM RGB-D数据集上，系统性改变运动幅度与场景动态性，评估不同组合：仅学习位姿、学习几何与相机内参对齐/不对齐、以及学习深度+几何裁决。

Result: 三点一致性发现：1) 仅依赖学习生成的位姿提案不可靠；2) 学习生成的几何（特别是深度）若与相机内参未正确对齐会显著降低性能；3) 当学习生成的深度经过几何对齐并交由ICP裁决时，在中等难度的刚体场景获得稳定提升。

Conclusion: 几何不只是细化模块，而是核心的判别与吸收机制，能验证并消解学习产生的几何观测噪声。为鲁棒空间感知，推荐采用模块化、几何感知的系统设计：学习负责提出多样假设，几何负责一致性检查与最终估计。

Abstract: Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.

</details>


### [120] [Understanding Sensor Vulnerabilities in Industrial XR Tracking](https://arxiv.org/abs/2602.14413)
*Sourya Saha,Md. Nurul Absur*

Main category: cs.CV

TL;DR: 论文对工业环境中XR系统的VIO在传感退化下的表现进行受控实验评估，发现视觉退化多导致厘米级有界误差，而惯性退化可引发百米到千米级轨迹偏差，提示应更加重视惯性可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业/作业场景中的XR依赖VIO进行6DoF跟踪，但真实环境的传感条件偏离理想；现有评测多聚焦于标称传感器，缺乏对持续退化与故障情形的系统理解，影响系统设计与安全性。

Method: 构建受控实验平台，对视觉与惯性传感通道进行系统性故障注入（不同退化类型与强度，覆盖多种运行工况），并以定量指标评估VIO轨迹误差与稳定性，比较不同退化类型的影响。

Result: 出现显著非对称性：视觉通道退化通常只导致厘米量级的有界位姿误差；而惯性通道退化可造成远大于视觉的轨迹漂移，极端情况下偏差达数百至上千米。

Conclusion: XR系统在工业场景的设计与评测需优先关注惯性通道的可靠性与鲁棒性，包括传感器质量、健康监测、容错与冗余方案；仅关注视觉质量不足以保障总体稳定性。

Abstract: Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.

</details>


### [121] [Hierarchical Vision-Language Interaction for Facial Action Unit Detection](https://arxiv.org/abs/2602.14425)
*Yong Li,Yi Ren,Yizhe Zhang,Wenhua Zhang,Tianyi Zhang,Muyun Jiang,Guo-Sen Xie,Cuntai Guan*

Main category: cs.CV

TL;DR: 提出HiVA层级视觉-语言交互方法，用大语言模型生成AU文本先验，结合AU感知动态图与双重跨模态注意（细粒度DDCA与全局CDCA）学习多粒度视觉与语义表示，显著提升AU检测性能并具可解释性。


<details>
  <summary>Details</summary>
Motivation: AU检测标注昂贵且数据有限，单纯视觉特征难以学习到具有辨识力且可泛化的AU表示；需要利用语义先验与跨模态对齐来增强鲁棒性与可解释性。

Method: 1) 用大语言模型生成多样、语境丰富的AU文本描述作为语义先验；2) 设计AU感知动态图模块，学习AU特定的视觉表示并建模AU间关系；3) 构建层级跨模态注意架构：a) DDCA进行细粒度、AU级别的视觉-文本交互与解耦；b) CDCA建模全局跨AU依赖与上下文；4) 融合多粒度视觉特征与细化的语言细节进行检测。

Result: 在多个基准上稳定超越SOTA；可视化显示模型学到与语义一致的激活模式与跨模态对应关系，解释性增强。

Conclusion: 引入层级视觉-语言交互与语义先验能在低标注条件下显著提升AU检测的准确性与泛化，并提供更强的可解释性。

Abstract: Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.

</details>


### [122] [D-SECURE: Dual-Source Evidence Combination for Unified Reasoning in Misinformation Detection](https://arxiv.org/abs/2602.14441)
*Gagandeep Singh,Samudi Amarasinghe,Priyanka Singh*

Main category: cs.CV

TL;DR: 提出D-SECURE，将图像-文本内部篡改检测与外部证据检索式核查融合，用于新闻风格多模态谣言；DEFAME先做广泛事实核验，HAMMER再精细查疑难/残留案例，给出统一可解释报告；在DGM4与ClaimReview上显示互补优势。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息常把逼真的图像编辑与流畅却误导的文本结合，现有方法要么做内容一致性检测（但难以判定全球事实真伪），要么做检索式事实核查（但把输入当粗粒度主张，易漏掉细微像素/词级操控）。这种割裂导致内部一致的伪造逃过操控检测，而含精细篡改的主张又被事实核查错误认证。

Method: 提出D-SECURE框架，融合内部操控检测器HAMMER与外证检索核查管线DEFAME：先由DEFAME进行广谱外部证据核验；对不确定或有残留疑点的样本，再由HAMMER做细粒度图像/文本操控分析；最终生成融合操控线索与外部证据的统一、可解释报告。

Result: 在DGM4与ClaimReview样本上的实验显示，两系统具互补性，联合使用能覆盖彼此盲点；案例分析与量化结果表明融合后能更好识别含细粒度编辑但文本上自洽的伪造，以及避免对含像素/词级腐化的声明误判为真。

Conclusion: 将内部操控检测与外部证据推理联动可提升多模态谣言核验的鲁棒性与可解释性；D-SECURE通过“先广后精”的流程与统一报告实现实用化，并由实验证明该策略的有效性与必要性。

Abstract: Multimodal misinformation increasingly mixes realistic im-age edits with fluent but misleading text, producing persuasive posts that are difficult to verify. Existing systems usually rely on a single evidence source. Content-based detectors identify local inconsistencies within an image and its caption but cannot determine global factual truth. Retrieval-based fact-checkers reason over external evidence but treat inputs as coarse claims and often miss subtle visual or textual manipulations. This separation creates failure cases where internally consistent fabrications bypass manipulation detectors and fact-checkers verify claims that contain pixel-level or token-level corruption. We present D-SECURE, a framework that combines internal manipulation detection with external evidence-based reasoning for news-style posts. D-SECURE integrates the HAMMER manipulation detector with the DEFAME retrieval pipeline. DEFAME performs broad verification, and HAMMER analyses residual or uncertain cases that may contain fine-grained edits. Experiments on DGM4 and ClaimReview samples highlight the complementary strengths of both systems and motivate their fusion. We provide a unified, explainable report that incorporates manipulation cues and external evidence.

</details>


### [123] [Controlling Your Image via Simplified Vector Graphics](https://arxiv.org/abs/2602.14443)
*Lanqing Guo,Xi Liu,Yufei Wang,Zhihao Li,Siyu Huang*

Main category: cs.CV

TL;DR: 提出通过简化向量图形(VG)实现分层可控的图像生成：先把图像解析为语义对齐、结构一致的层级VG，再用VG引导扩散式合成，以精确控制形状、颜色与对象语义，实现编辑与对象级操控。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型视觉质量高但难以在元素层面可控（如改形状、换颜色、增删对象）。需要一种兼具结构与语义的中间表示，实现直观、细粒度、可编辑的控制。

Method: 1) 高效将图像解析为层级化、语义对齐的简化VG表示；2) 设计由VG引导的图像合成框架，将用户对VG的编辑通过与噪声预测结合映射为逼真图像；3) 结合结构与语义特征，实现对几何、颜色、对象语义的精确控制。

Result: 在图像编辑、对象级操控、细粒度内容创作等多种任务上表现出色，能将VG层面的修改稳定、逼真地反映到输出图像。

Conclusion: 以简化VG为中间表示的分层可控生成为图像生成提供了新的范式，兼顾可编辑性与照片级质量，显著提升元素级控制能力。

Abstract: Recent advances in image generation have achieved remarkable visual quality, while a fundamental challenge remains: Can image generation be controlled at the element level, enabling intuitive modifications such as adjusting shapes, altering colors, or adding and removing objects? In this work, we address this challenge by introducing layer-wise controllable generation through simplified vector graphics (VGs). Our approach first efficiently parses images into hierarchical VG representations that are semantic-aligned and structurally coherent. Building on this representation, we design a novel image synthesis framework guided by VGs, allowing users to freely modify elements and seamlessly translate these edits into photorealistic outputs. By leveraging the structural and semantic features of VGs in conjunction with noise prediction, our method provides precise control over geometry, color, and object semantics. Extensive experiments demonstrate the effectiveness of our approach in diverse applications, including image editing, object-level manipulation, and fine-grained content creation, establishing a new paradigm for controllable image generation. Project page: https://guolanqing.github.io/Vec2Pix/

</details>


### [124] [CoCoDiff: Correspondence-Consistent Diffusion Model for Fine-grained Style Transfer](https://arxiv.org/abs/2602.14464)
*Wenbo Nie,Zixiang Li,Renshuai Tao,Bin Wu,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 提出CoCoDiff：一种无需训练、低成本，基于预训练潜变量扩散模型的精细语义一致风格迁移方法，通过像素级对应与循环一致性实现区域/对象级几何与细节保真，并在视觉质量与定量指标上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像风格迁移多在全局层面操作，忽视区域/像素级的语义对应，导致相似对象间的结构与内容一致性差；扩散模型中潜在的对应线索未被充分利用。

Method: 利用预训练的潜变量扩散模型，无需额外训练。1) 像素级语义对应模块：挖掘扩散过程中的中间特征，构建内容图与风格图的稠密对齐映射；2) 循环一致性模块：在迭代过程中约束结构与感知一致性，强化对象/区域级的对齐与细节保留；整体为训练自由、低计算开销的框架。

Result: 实现细粒度、语义一致的风格化，在对象与区域层面保持几何与细节；在视觉主观质量与客观定量指标上优于需要额外训练或标注的现有方法，达到SOTA表现。

Conclusion: 挖掘扩散模型中的对应线索并结合循环一致性，可在无需额外监督的前提下实现高质量、语义一致的风格迁移；CoCoDiff证明了训练自由方法在精细风格化上的有效性与优势。

Abstract: Transferring visual style between images while preserving semantic correspondence between similar objects remains a central challenge in computer vision. While existing methods have made great strides, most of them operate at global level but overlook region-wise and even pixel-wise semantic correspondence. To address this, we propose CoCoDiff, a novel training-free and low-cost style transfer framework that leverages pretrained latent diffusion models to achieve fine-grained, semantically consistent stylization. We identify that correspondence cues within generative diffusion models are under-explored and that content consistency across semantically matched regions is often neglected. CoCoDiff introduces a pixel-wise semantic correspondence module that mines intermediate diffusion features to construct a dense alignment map between content and style images. Furthermore, a cycle-consistency module then enforces structural and perceptual alignment across iterations, yielding object and region level stylization that preserves geometry and detail. Despite requiring no additional training or supervision, CoCoDiff delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.

</details>


### [125] [TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.14482)
*Hao Ding,Zhichuan Yang,Weijie Ge,Ziqin Gao,Chaoyi Lu,Lei Zhao*

Main category: cs.CV

TL;DR: TikArt提出“思考-光圈-观察”的循环，用缩放与分割两种光圈动作在高分辨率图像上逐步聚焦局部区域，并在每步将观察结果转成语言记忆；结合AGRPO强化学习与两阶段课程，显著提升细粒度视觉推理与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM常以单一全局编码处理整图，易丢失微小目标、杂乱区域或细微标记中的关键信息，限制高分辨率细粒度推理能力与可解释性。

Method: 将多步视觉-语言推理建模为对兴趣区域的决策过程：在Think-Aperture-Observe循环中，模型交替进行语言生成与两类光圈操作——Zoom进行矩形裁剪，Segment调用SAM2获得不规则掩膜裁剪；每次动作后显式生成“观察”文本形成持久记忆。基于Qwen3-VL-8B，并用AGRPO（GRPO式）强化学习优化策略，采用两阶段课程：先热身分割动作，再联合优化视觉数学、细粒度VQA与分割；奖励同时考虑任务成功与光圈使用的目的性。

Result: 在V*、HR-Bench-4K/8K、MME-RealWorld-Lite、MMStar、RefCOCO、ReasonSeg等基准上相较基座模型取得一致提升，并产出可解释的高分辨率“光圈轨迹”。

Conclusion: 区域级决策与显式观察记忆结合强化学习与课程设计，可有效提升MLLM的细粒度视觉推理性能与可解释性，适用于高分辨率、多目标与不规则目标场景。

Abstract: We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.

</details>


### [126] [Gaussian Mesh Renderer for Lightweight Differentiable Rendering](https://arxiv.org/abs/2602.14493)
*Xinpeng Liu,Fumio Okura*

Main category: cs.CV

TL;DR: 提出Gaussian Mesh Renderer（GMR）：把三角网格三角形解析地映射为高斯元，利用3DGS式高效光栅化实现轻量可微网格渲染；相较传统可微网格渲染器，梯度更平滑、内存占用更低、用小batch也能稳定优化。


<details>
  <summary>Details</summary>
Motivation: 3DGS在新视角合成上渲染快、优化快，但主流网格重建仍依赖三角网格；传统网格可微渲染器优化慢、计算与内存开销大、梯度噪声高。需要一种既保留网格结构优势又具备3DGS效率与稳定性的可微渲染方法。

Method: 将网格与高斯表示紧耦合：对每个三角形解析地构造对应的3D高斯基元，并沿用3DGS的高效栅格化/累积流程进行前向渲染与反向传播；通过解析映射保证结构一致性和可导性，从而获得更加平滑的梯度与稳定训练。

Result: 相较传统网格可微渲染器，GMR提供更平滑的梯度流，能在更小batch与有限显存下实现更好的优化效果与性能；实现已开源，显示其实用性。

Conclusion: GMR把三角网格转为高斯进行轻量级可微渲染，兼得3DGS的效率与网格的结构保真，改善梯度质量和内存/批大小效率，适合高保真场景重建与新视角合成优化。

Abstract: 3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR), which tightly integrates the Gaussian and mesh representations. Each Gaussian primitive is analytically derived from the corresponding mesh triangle, preserving structural fidelity and enabling the gradient flow. Compared to the traditional mesh renderers, our method achieves smoother gradients, which especially contributes to better optimization using smaller batch sizes with limited memory. Our implementation is available in the public GitHub repository at https://github.com/huntorochi/Gaussian-Mesh-Renderer.

</details>


### [127] [Uncertainty-Aware Vision-Language Segmentation for Medical Imaging](https://arxiv.org/abs/2602.14498)
*Aryan Das,Tanishq Rachamalla,Koushik Biswas,Swalpa Kumar Roy,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: 提出一种结合影像与临床文本的多模态不确定性感知分割框架，含MoDAB+SSMix跨模态融合与SEU损失，在多数据集上以更低计算量优于SoTA。


<details>
  <summary>Details</summary>
Motivation: 医疗影像常存在噪声、质量差与模态间语义错配；现有多模态分割对长程依赖、跨模态对齐与不确定性刻画不足，影响在复杂临床情境下的可靠性与泛化。

Method: 1) 设计MoDAB（模态解码注意力模块）结合轻量State Space Mixer（SSMix），实现图像-文本高效融合与长程依赖建模；2) 提出SEU损失，将空间重叠（如Dice/IoU）、频域/谱一致性与预测熵/不确定性联合优化；3) 在QATA-COVID19、MosMed++、Kvasir-SEG上进行实验与效率评估。

Result: 在三个公开数据集上取得优于现有SoTA的分割性能，同时计算开销显著更低（更高效率）；在图像质量差的情境下表现更稳健。

Conclusion: 不确定性建模与结构化的跨模态对齐对医学视觉-语言分割至关重要；所提框架在准确性与效率上均具优势，适合复杂临床环境。

Abstract: We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS

</details>


### [128] [Prototype Instance-semantic Disentanglement with Low-rank Regularized Subspace Clustering for WSIs Explainable Recognition](https://arxiv.org/abs/2602.14501)
*Chentao Li,Pan Huang*

Main category: cs.CV

TL;DR: 提出PID-LRSC框架，通过低秩子空间聚类与原型对比学习解耦WSI中的实例-语义纠缠，提升表示、可解释性与诊断可靠性，并在多中心病理数据上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: WSI中肿瘤区域对病理诊断至关重要，但肿瘤与癌前病变形态相似、且非肿瘤实例数量远多于肿瘤实例，导致多实例学习中实例与语义纠缠，削弱模型的判别力与可解释性。

Method: 提出端到端的原型实例语义解耦与低秩正则子空间聚类框架PID-LRSC：1）二级实例子空间学习构建低秩正则子空间聚类（LRSC），以处理非肿瘤实例占比过高引发的实例纠缠；2）基于增强的对比学习进行原型实例语义解耦（PID），缓解肿瘤与癌前组织高相似度引发的语义纠缠。

Result: 在多中心病理数据集上进行大量实验，PID-LRSC在性能上超过现有SOTA方法，并在决策过程中给出更清晰的实例语义。

Conclusion: PID-LRSC有效解耦WSI中的实例与语义纠缠，提升模型表示能力、可解释性与辅助诊断的可靠性，具有实际临床应用潜力。

Abstract: The tumor region plays a key role in pathological diagnosis. Tumor tissues are highly similar to precancerous lesions and non tumor instances often greatly exceed tumor instances in whole slide images (WSIs). These issues cause instance-semantic entanglement in multi-instance learning frameworks, degrading both model representation capability and interpretability. To address this, we propose an end-to-end prototype instance semantic disentanglement framework with low-rank regularized subspace clustering, PID-LRSC, in two aspects. First, we use secondary instance subspace learning to construct low-rank regularized subspace clustering (LRSC), addressing instance entanglement caused by an excessive proportion of non tumor instances. Second, we employ enhanced contrastive learning to design prototype instance semantic disentanglement (PID), resolving semantic entanglement caused by the high similarity between tumor and precancerous tissues. We conduct extensive experiments on multicentre pathology datasets, implying that PID-LRSC outperforms other SOTA methods. Overall, PID-LRSC provides clearer instance semantics during decision-making and significantly enhances the reliability of auxiliary diagnostic outcomes.

</details>


### [129] [MacNet: An End-to-End Manifold-Constrained Adaptive Clustering Network for Interpretable Whole Slide Image Classification](https://arxiv.org/abs/2602.14509)
*Mingrui Ma,Chentao Li,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出一种面向病理全切片图像（WSI）的端到端MIL框架，结合Grassmann重嵌入与流形自适应聚类，并用先验知识引导代理实例标注与聚合，兼顾性能与可解释性，且计算开销可接受。


<details>
  <summary>Details</summary>
Motivation: 现有两步式流程依赖离线、非特定领域特征编码器；注意力MIL虽以结果为导向但可解释性不足；聚类法可解释但受高维特征与语义含糊质心困扰。需要一种同时提升可解释性与判别性能、并能端到端学习更优表征的方法。

Method: 构建端到端的MIL：1) 引入Grassmann重嵌入与流形自适应聚类，利用流形几何结构获得稳健聚类并缓解高维问题与质心语义模糊；2) 设计先验知识引导的代理实例标注与聚合策略，近似片块标签、聚焦肿瘤相关区域；3) 通过联合训练细化特征表示。

Result: 在多中心WSI数据上，模型在分级准确率和可解释性方面优于现有方法；端到端学习得到更优特征表示，同时计算资源需求在可接受范围内。

Conclusion: 该方法在保证计算可行性的同时，实现了更高的WSI分级性能与更强可解释性；通过流形感知聚类与先验引导的代理标注，有效对准病理相关区域，适合作为可解释的端到端病理分析方案。

Abstract: Whole slide images (WSIs) are the gold standard for pathological diagnosis and sub-typing. Current main-stream two-step frameworks employ offline feature encoders trained without domain-specific knowledge. Among them, attention-based multiple instance learning (MIL) methods are outcome-oriented and offer limited interpretability. Clustering-based approaches can provide explainable decision-making process but suffer from high dimension features and semantically ambiguous centroids. To this end, we propose an end-to-end MIL framework that integrates Grassmann re-embedding and manifold adaptive clustering, where the manifold geometric structure facilitates robust clustering results. Furthermore, we design a prior knowledge guiding proxy instance labeling and aggregation strategy to approximate patch labels and focus on pathologically relevant tumor regions. Experiments on multicentre WSI datasets demonstrate that: 1) our cluster-incorporated model achieves superior performance in both grading accuracy and interpretability; 2) end-to-end learning refines better feature representations and it requires acceptable computation resources.

</details>


### [130] [MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction](https://arxiv.org/abs/2602.14512)
*Zhicheng He,Yunpeng Zhao,Junde Wu,Ziwei Niu,Zijun Li,Lanfen Lin,Yueming Jin*

Main category: cs.CV

TL;DR: MedVAR提出一种面向医学影像的自回归基础模型，通过“下一尺度预测”的分层自粗到细生成，实现高效、可扩展的CT/MRI图像合成，并在保真度、多样性与可扩展性上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像生成方法在架构效率、跨器官大规模数据与规范化评测方面存在不足，限制了在数据增强与隐私共享等现实场景的落地与扩展。作者旨在提供一种可快速、可扩展、并能产出可用于下游任务表征的生成骨干。

Method: 提出MedVAR：基于自回归、采用“下一尺度预测”的层级生成策略，从粗到细逐级生成，并显式产出多尺度结构化表征；同时构建包含约44万张CT/MRI、覆盖六个解剖区域的统一数据集以支撑分层训练与评测。

Result: 在涵盖保真度、样本多样性与可扩展性的全面实验中，MedVAR优于现有方法，展现SOTA生成质量与良好的扩展性能；其多尺度表征也适配下游任务使用。

Conclusion: MedVAR证明了自回归+下一尺度预测在医学影像生成中的有效性与可扩展性，提供了一个面向未来医疗生成基础模型的有前景的架构方向。

Abstract: Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.

</details>


### [131] [Efficient Text-Guided Convolutional Adapter for the Diffusion Model](https://arxiv.org/abs/2602.14514)
*Aryan Das,Koushik Biswas,Swalpa Kumar Roy,Badri Narayana Patro,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: 提出Nexus Adapters：在扩散模型中用于结构保持条件生成的、由文本与结构共同引导的高效适配器（Prime与Slim），在更少参数下优于现有T2I-Adapter。


<details>
  <summary>Details</summary>
Motivation: 现有结构保持生成（如草图/深度→图像）通常用基底扩散模型处理文本提示、另配一个大型结构适配器；适配器对文本不敏感、参数量接近或超过基模，训练与推理成本高、效率低，且结构与语义匹配欠佳。

Method: 设计两种文本引导的多模态适配器Nexus Prime与Nexus Slim：在适配器中引入跨注意力，将文本提示与结构信号共同注入；通过Nexus Block进行多模态条件融合，使适配器既理解提示语义又保持结构。整体以扩散框架的SPCG为目标，无需在基模型上大改动，仅增加少量参数。

Result: 广泛实验显示：Nexus Prime较T2I-Adapter仅多8M参数却显著提升性能；Nexus Slim比T2I-Adapter少18M参数仍达SOTA效果；在多种结构条件（如草图、深度）和提示下取得优异表现。代码开放。

Conclusion: 通过引入跨注意力的文本引导适配器，能在小幅或更少参数开销下实现更强的结构保持与语义一致性；Prime适合追求性能，Slim适合极致轻量，整体在效率与效果上优于现有T2I-Adapter方案。

Abstract: We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters

</details>


### [132] [Architectural Insights for Post-Tornado Damage Recognition](https://arxiv.org/abs/2602.14523)
*Robinson Umeike,Thang Dao,Shane Crawford,John van de Lindt,Blythe Johnston,Wanting,Wang,Trung Do,Ajibola Mofikoya,Sarbesh Banjara,Cuong Pham*

Main category: cs.CV

TL;DR: 该论文针对龙卷风灾后建筑损伤评估，构建QSTD基准并在79个模型、2300+实验中系统比较，发现优化器与学习率对性能影响远超架构选择；以低学习率1e-4与SGD可显著提升Transformer类模型F1；最佳方案ConvNeXt-Base在跨事件数据上实现显著泛化提升。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法在龙卷风场景中因视觉破坏形态复杂、与常规模型预训练域存在显著偏移，以及真实灾害数据严重类别不均衡而表现不佳。需要一个系统化框架客观评估不同深度学习模型并找出实现“可运营级”性能的关键训练策略。

Method: 构建并发布Quad-State Tornado Damage (QSTD)基准；系统评测79个开源深度学习模型（CNN与ViT家族）在超过2300个受控实验中的表现；从架构与优化配置（优化器、学习率等）交互作用角度分析性能；并在独立的Tuscaloosa-Moore Tornado Damage (TMTD)数据集上进行跨事件泛化验证。

Result: 发现优化器选择可决定性地改变Transformer类模型排名：将Adam切换为SGD可使ViT与Swin的宏F1提升约+25至+38点；统一采用1e-4低学习率对所有架构平均提升+10.2点F1；在优化设置下，ConvNeXt-Base成为冠军模型，并在TMTD上获得46.4%宏F1（较基线+34.6点）与85.5%序等级Top-1准确率，显示出对时间与传感器域偏移的鲁棒性。

Conclusion: 实现龙卷风损伤评估的运营级表现更多取决于训练优化策略（尤其是优化器与学习率）与架构的交互，而非单纯的架构选择。推荐采用SGD与低学习率1e-4作为强默认设置；ConvNeXt-Base在该设置下具备良好的跨事件泛化能力。

Abstract: Rapid and accurate building damage assessment in the immediate aftermath of tornadoes is critical for coordinating life-saving search and rescue operations, optimizing emergency resource allocation, and accelerating community recovery. However, current automated methods struggle with the unique visual complexity of tornado-induced wreckage, primarily due to severe domain shift from standard pre-training datasets and extreme class imbalance in real-world disaster data. To address these challenges, we introduce a systematic experimental framework evaluating 79 open-source deep learning models, encompassing both Convolutional Neural Networks (CNNs) and Vision Transformers, across over 2,300 controlled experiments on our newly curated Quad-State Tornado Damage (QSTD) benchmark dataset. Our findings reveal that achieving operational-grade performance hinges on a complex interaction between architecture and optimization, rather than architectural selection alone. Most strikingly, we demonstrate that optimizer choice can be more consequential than architecture: switching from Adam to SGD provided dramatic F1 gains of +25 to +38 points for Vision Transformer and Swin Transformer families, fundamentally reversing their ranking from bottom-tier to competitive with top-performing CNNs. Furthermore, a low learning rate of 1x10^(-4) proved universally critical, boosting average F1 performance by +10.2 points across all architectures. Our champion model, ConvNeXt-Base trained with these optimized settings, demonstrated strong cross-event generalization on the held-out Tuscaloosa-Moore Tornado Damage (TMTD) dataset, achieving 46.4% Macro F1 (+34.6 points over baseline) and retaining 85.5% Ordinal Top-1 Accuracy despite temporal and sensor domain shifts.

</details>


### [133] [Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model](https://arxiv.org/abs/2602.14524)
*Ari Vesalainen,Eetu Mäkelä,Laura Ruotsalainen,Mikko Tolonen*

Main category: cs.CV

TL;DR: 研究比较TrOCR与通用VLM（Qwen）在18世纪英文行级OCR上的表现，发现两者总体准确率相近但错误结构迥异，对学术数字化的风险与评估提出架构敏感的度量与分析。


<details>
  <summary>Details</summary>
Motivation: 传统CER/WER等汇总指标无法揭示模型在历史文本场景下的可靠性与学术风险；需要理解不同模型架构如何系统性影响错误类型、可检测性与对历史形式的保真度。

Method: 在退化印刷、古体字形与非标准拼写的18世纪英文行级数据上，对比专用OCR Transformer（TrOCR）与通用VLM（Qwen）；采用长度加权的准确率指标，并进行以假设驱动的错误分析（关注错误局部性、正字法保真、级联传播、对退化输入的鲁棒性）。

Result: Qwen整体CER/WER更低且对退化输入更稳健，但会选择性进行语言规则化与正字法“修正”，可能无声地改变具有历史意义的形式；TrOCR更好保留原始正字法，但更易出现级联错误。两者在错误局部性、可检测性与学术风险上显著不同。

Conclusion: 模型架构的归纳偏置系统性塑造OCR错误结构；即便总体准确率相当，学术用途的可靠性差异很大。历史数字化应采用架构感知的评估与指标，以平衡鲁棒性与正字法保真，降低下游学术风险。

Abstract: Optical Character Recognition (OCR) of eighteenth-century printed texts remains challenging due to degraded print quality, archaic glyphs, and non-standardized orthography. Although transformer-based OCR systems and Vision-Language Models (VLMs) achieve strong aggregate accuracy, metrics such as Character Error Rate (CER) and Word Error Rate (WER) provide limited insight into their reliability for scholarly use. We compare a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis driven error analysis.
  While Qwen achieves lower CER/WER and greater robustness to degraded input, it exhibits selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserves orthographic fidelity more consistently but is more prone to cascading error propagation. Our findings show that architectural inductive biases shape OCR error structure in systematic ways. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, underscoring the need for architecture-aware evaluation in historical digitization workflows.

</details>


### [134] [Cross-view Domain Generalization via Geometric Consistency for LiDAR Semantic Segmentation](https://arxiv.org/abs/2602.14525)
*Jindong Zhao,Yuan Gao,Yang Xia,Sheng Nie,Jun Yue,Weiwei Sun,Shaobo Xia*

Main category: cs.CV

TL;DR: 提出CVGC框架，通过跨视角几何增强与一致性约束，提升LiDAR语义分割在跨视角域泛化能力，并在六个数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LSS方法多假设与训练相似的采集视角（如车载），在跨视角（如车载→固定杆/背包/无人机）时因可见性差异与点密度不均导致结构不完整，泛化显著退化。现实部署常遇多视角异构数据，迫切需要能跨视角稳健泛化的方案。

Method: 1) 跨视角几何增强：显式建模视点引起的可见性变化与采样密度差异，对同一场景生成多种模拟视角的点云观测；2) 几何一致性：对这些几何增强的点云施加语义与占据预测一致性约束，使模型学习与视角无关的判别特征；3) 单源训练，测试于多未见异构视角域。

Result: 在六个公开LiDAR数据集上首次系统评测跨视角域泛化；从单一源域迁移到多目标异构视角时，CVGC在mIoU等指标上持续超过当前SOTA。

Conclusion: 通过跨视角几何建模与一致性学习，可显著缓解视角引起的点云观测偏差，提升LSS的跨视角域泛化；方法通用、无需目标域数据，适合真实应用场景。

Abstract: Domain-generalized LiDAR semantic segmentation (LSS) seeks to train models on source-domain point clouds that generalize reliably to multiple unseen target domains, which is essential for real-world LiDAR applications. However, existing approaches assume similar acquisition views (e.g., vehicle-mounted) and struggle in cross-view scenarios, where observations differ substantially due to viewpoint-dependent structural incompleteness and non-uniform point density. Accordingly, we formulate cross-view domain generalization for LiDAR semantic segmentation and propose a novel framework, termed CVGC (Cross-View Geometric Consistency). Specifically, we introduce a cross-view geometric augmentation module that models viewpoint-induced variations in visibility and sampling density, generating multiple cross-view observations of the same scene. Subsequently, a geometric consistency module enforces consistent semantic and occupancy predictions across geometrically augmented point clouds of the same scene. Extensive experiments on six public LiDAR datasets establish the first systematic evaluation of cross-view domain generalization for LiDAR semantic segmentation, demonstrating that CVGC consistently outperforms state-of-the-art methods when generalizing from a single source domain to multiple target domains with heterogeneous acquisition viewpoints. The source code will be publicly available at https://github.com/KintomZi/CVGC-DG

</details>


### [135] [MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation](https://arxiv.org/abs/2602.14534)
*Hongpeng Wang,Zeyu Zhang,Wenhao Li,Hao Tang*

Main category: cs.CV

TL;DR: MoRL 是一个将监督微调与带可验证奖励的强化学习相结合的统一多模态人类动作模型，并配合测试时“动作链”推理（CoM），在理解与生成任务上同时提升逻辑推理与感知真实感，在 HumanML3D 与 KIT-ML 上显著优于 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有动作理解与生成模型在推理能力不足、测试时规划薄弱，且往往难以兼顾语义/逻辑一致性与物理可行性，限制了在视觉与机器人应用中的实用性。

Method: 1) 统一多模态模型 MoRL：先经监督微调，再用具备可验证目标的强化学习优化；2) 任务定制奖励：理解侧结合语义对齐与推理连贯，生成侧结合物理可行与文-动一致；3) 测试时推理 CoM：逐步规划与反思的“链式动作”解法；4) 构建两套CoT数据集：MoUnd-CoT-140K 与 MoGen-CoT-140K，将动作序列与推理轨迹/动作描述对齐。

Result: 在 HumanML3D 与 KIT-ML 的理解与生成基准上，MoRL 相比现有 SOTA 显著提升（摘要未给出具体数值），显示其在逻辑推理、物理合理性与文本一致性上的综合优势。

Conclusion: 通过可验证奖励的RL与CoM测试时推理，再配合大规模CoT数据集对齐，MoRL 同时强化了动作理解与生成的推理与真实感，证明统一范式和测试时链式规划对人类动作任务有效。

Abstract: Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.

</details>


### [136] [OmniVTON++: Training-Free Universal Virtual Try-On with Principal Pose Guidance](https://arxiv.org/abs/2602.14552)
*Zhaotong Yang,Yong Du,Shengfeng He,Yuhui Li,Xinzhe Li,Yangyang Xu,Junyu Dong,Jian Yang*

Main category: cs.CV

TL;DR: OmniVTON++ 是一个无需训练、通用适配的图像虚拟试衣框架，通过结构化衣物变形、主导姿态引导与连续边界拼接三个模块协同，解决衣物对齐、人物结构一致性与边界连续性问题，在跨数据集/跨服饰类型等泛化场景达成SOTA，并支持多衣物、多人物与二次元角色试穿。


<details>
  <summary>Details</summary>
Motivation: 现有VTON方法依赖特定数据条件与再训练，难以在不同数据集、服饰类型、扩散骨干或多场景下统一应用，泛化与部署成本高。需要一种无需再训练、可跨场景稳健工作的统一方案。

Method: 提出训练免调参的三阶段协同管线：1) Structured Garment Morphing：基于对应关系驱动的衣物形变与对齐；2) Principal Pose Guidance：在扩散采样过程中分步施加姿态/结构主导引导，维持人体结构一致性；3) Continuous Boundary Stitching：边界感知的细化与拼接，确保衣物与人体边界连续自然。方法可兼容多种扩散骨干且无需任务特定再训练。

Result: 在跨数据集与跨服饰类型评测中达到SOTA；在单一框架下对多场景与不同扩散模型均稳定有效。除单衣物/单人外，还能处理多衣物、多人物与动漫角色试穿。

Conclusion: OmniVTON++ 提供了一个统一、训练自由且泛化强的VTON解决方案，通过三模块协作兼顾对齐、结构与边界质量，在多种设置下取得领先表现并拓展了试穿应用边界。

Abstract: Image-based Virtual Try-On (VTON) concerns the synthesis of realistic person imagery through garment re-rendering under human pose and body constraints. In practice, however, existing approaches are typically optimized for specific data conditions, making their deployment reliant on retraining and limiting their generalization as a unified solution. We present OmniVTON++, a training-free VTON framework designed for universal applicability. It addresses the intertwined challenges of garment alignment, human structural coherence, and boundary continuity by coordinating Structured Garment Morphing for correspondence-driven garment adaptation, Principal Pose Guidance for step-wise structural regulation during diffusion sampling, and Continuous Boundary Stitching for boundary-aware refinement, forming a cohesive pipeline without task-specific retraining. Experimental results demonstrate that OmniVTON++ achieves state-of-the-art performance across diverse generalization settings, including cross-dataset and cross-garment-type evaluations, while reliably operating across scenarios and diffusion backbones within a single formulation. In addition to single-garment, single-human cases, the framework supports multi-garment, multi-human, and anime character virtual try-on, expanding the scope of virtual try-on applications. The source code will be released to the public.

</details>


### [137] [DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving](https://arxiv.org/abs/2602.14577)
*Chenxu Dang,Sining Ang,Yongkang Li,Haochen Tian,Jie Wang,Guang Li,Hangjun Ye,Jie Ma,Long Chen,Yan Wang*

Main category: cs.CV

TL;DR: DriveFine 提出一种用于自动驾驶的掩码扩散式视觉-语言-动作（VLA）规划器，通过块级MoE结构将“生成专家”和“精炼专家”解耦结合，配合混合强化学习，兼具可逆/灵活解码与自纠错能力，在NAVSIM v1/v2与Navhard基准上表现稳健优越。


<details>
  <summary>Details</summary>
Motivation: 现有两类主流生成式规划器各有硬伤：扩散式方法存在模态对齐难、训练低效、泛化受限；基于token的自回归方法易产生因果累积误差且解码不可逆。需要一种既能灵活解码又能在推理中自我纠错、训练稳定且具可扩展性的统一框架。

Method: 提出DriveFine：1）掩码扩散规划范式，支持灵活、可并行的生成与修补；2）块级MoE（block-MoE）插拔式设计，在“生成专家”之上无缝注入“精炼专家”，推理时显式选择专家；训练中对梯度进行阻断，实现两专家完全解耦，保留预训练权重的通用能力与模式；3）混合强化学习策略，鼓励对“精炼专家”的有效探索，同时保持训练稳定性。

Result: 在NAVSIM v1、v2与Navhard基准上进行了大量实验，显示DriveFine在有效性与鲁棒性上优于现有方法。

Conclusion: 将掩码扩散与块级MoE精炼机制相结合，并辅以混合RL，可在自动驾驶VLA规划中兼得灵活解码与自纠错、提升性能与稳健性；设计具有良好的灵活性与可扩展性，保留并利用预训练模型的基础能力。

Abstract: Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.

</details>


### [138] [YOLO26: A Comprehensive Architecture Overview and Key Improvements](https://arxiv.org/abs/2602.14582)
*Priyanto Hidayatullah,Refdinal Tubagus*

Main category: cs.CV

TL;DR: 论文梳理YOLO26的核心架构与关键改进，基于源码与官方文档做系统性解析，给出首个CNN版YOLO26架构图，并总结其在无DFL、端到端免NMS、ProgLoss+STAL与MuSGD等方面的变更，主打CPU推理提速与多任务能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有资料多为零散技术文档与发布日志，缺少对YOLO26真实实现细节与整体架构的系统化解析；作者希望为研究者与开发者提供准确的架构理解，指导后续优化，使YOLO系列继续在CV中保持领先。

Method: 以官方GitHub源码为主、文档为辅，进行“由下而上”的架构逆向与机制拆解，提炼关键训练/推理设计并绘制统一的架构图；对新损失、标注分配、优化器与推理流程（免NMS）进行机理分析与对比。

Result: 识别并归纳YOLO26的四大核心改动：1) 去除DFL；2) 端到端免NMS推理；3) ProgLoss + 小目标感知标注分配（STAL）；4) MuSGD优化器；宣称在CPU模式下推理速度提升约43%，并在实例分割、姿态估计、OBB解码等任务上带来改进；产出首个CNN版YOLO26架构图。

Conclusion: YOLO26在保持精度的前提下显著优化CPU/边缘侧实时性，并通过新损失、标注与优化器联动强化多任务适配。本文填补了架构层面系统化描述的空白，为后续改进与工程落地提供清晰蓝图。

Abstract: You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series. The elimination of Distribution Focal Loss (DFL), implementation of End-to-End NMS-Free Inference, introduction of ProgLoss + Small-Target-Aware Label Assignment (STAL), and use of the MuSGD optimizer are the primary enhancements designed to improve inference speed, which is claimed to achieve a 43% boost in CPU mode. This is designed to allow YOLO26 to attain real-time performance on edge devices or those without GPUs. Additionally, YOLO26 offers improvements in many computer vision tasks, including instance segmentation, pose estimation, and oriented bounding box (OBB) decoding. We aim for this effort to provide more value than just consolidating information already included in the existing technical documentation. Therefore, we performed a rigorous architectural investigation into YOLO26, mostly using the source code available in its GitHub repository and its official documentation. The authentic and detailed operational mechanisms of YOLO26 are inside the source code, which is seldom extracted by others. The YOLO26 architectural diagram is shown as the outcome of the investigation. This study is, to our knowledge, the first one presenting the CNN-based YOLO26 architecture, which is the core of YOLO26. Our objective is to provide a precise architectural comprehension of YOLO26 for researchers and developers aspiring to enhance the YOLO model, ensuring it remains the leading deep learning model in computer vision.

</details>


### [139] [VariViT: A Vision Transformer for Variable Image Sizes](https://arxiv.org/abs/2602.14615)
*Aswathi Varma,Suprosanna Shit,Chinmay Prabhakar,Daniel Scholz,Hongwei Bran Li,Bjoern Menze,Daniel Rueckert,Benedikt Wiestler*

Main category: cs.CV

TL;DR: 提出VariViT：在保持固定patch尺寸下，可接纳可变图像大小的ViT，配以位置嵌入重采样和新型批处理策略；在两套3D脑MRI任务上优于标准ViT与ResNet，并将训练/推理时间降至多30%。


<details>
  <summary>Details</summary>
Motivation: 标准ViT需要固定大小输入，医学图像常含不规则结构（如肿瘤），固定裁剪与缩放会导致前景/背景比例波动、信息丢失与伪影；大图计算昂贵，小图损失细节，存在计算-精度权衡，亟需能处理可变尺寸且高效的模型。

Method: - 设计VariViT，在不改变patch大小的前提下，允许输入图像的空间尺寸变化。
- 提出新的位置嵌入调整方案，支持可变patch数量。
- 实施新的批处理/分组策略，降低计算复杂度并加速训练与推理。
- 在两套3D脑MRI数据集上进行评测（如胶质瘤基因型预测、脑肿瘤分类）。

Result: VariViT在两项任务上超过vanilla ViT和ResNet，F1分别为75.5%与76.3%；新的批处理策略相较常规架构可将计算时间减少最多约30%，并学习到更具判别力的特征。

Conclusion: 允许可变输入尺寸并配合位置嵌入重采样与高效批处理的VariViT，可在医学影像表征学习中提升性能并降低计算成本，适于处理不规则结构场景；代码已开源。

Abstract: Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit

</details>


### [140] [VIGIL: Tackling Hallucination Detection in Image Recontextualization](https://arxiv.org/abs/2602.14633)
*Joanna Wojciechowicz,Maria Łubniewska,Jakub Antczak,Justyna Baczyńska,Wojciech Gromski,Wojciech Kozłowski,Maciej Zięba*

Main category: cs.CV

TL;DR: VIGIL 提出首个用于多模态图像再语境化任务的细粒度幻觉评测基准与框架，并开源数据与检测流水线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态研究常把“幻觉”当作单一问题，缺乏对不同错误类型的系统拆分与可解释评估，尤其在图像再语境化任务中评测空白明显。

Method: 构建包含五类幻觉的基准（粘贴物体、背景幻觉、目标遗漏、位置与逻辑不一致、物理规律违背）；提出多阶段检测流水线，分步检查目标层次保真、背景一致性与遗漏检测，并采用协同的开源模型集成进行分类与解释。

Result: 通过大量实验验证该流水线在识别与分解多种幻觉类型上的有效性，能够定位模型失效环节并给出解释性输出。

Conclusion: VIGIL 填补了多模态再语境化领域对幻觉的细粒度分类与分解评测缺口，提供可复现的基准、代码与数据，为后续研究和模型改进提供透明、可解释的工具。

Abstract: We introduce VIGIL (Visual Inconsistency & Generative In-context Lucidity), the first benchmark dataset and framework providing a fine-grained categorization of hallucinations in the multimodal image recontextualization task for large multimodal models (LMMs). While existing research often treats hallucinations as a uniform issue, our work addresses a significant gap in multimodal evaluation by decomposing these errors into five categories: pasted object hallucinations, background hallucinations, object omission, positional & logical inconsistencies, and physical law violations. To address these complexities, we propose a multi-stage detection pipeline. Our architecture processes recontextualized images through a series of specialized steps targeting object-level fidelity, background consistency, and omission detection, leveraging a coordinated ensemble of open-source models, whose effectiveness is demonstrated through extensive experimental evaluations. Our approach enables a deeper understanding of where the models fail with an explanation; thus, we fill a gap in the field, as no prior methods offer such categorization and decomposition for this task. To promote transparency and further exploration, we openly release VIGIL, along with the detection pipeline and benchmark code, through our GitHub repository: https://github.com/mlubneuskaya/vigil and Data repository: https://huggingface.co/datasets/joannaww/VIGIL.

</details>


### [141] [SketchingReality: From Freehand Scene Sketches To Photorealistic Images](https://arxiv.org/abs/2602.14648)
*Ahmed Bourouis,Mikhail Bessmeltsev,Yulia Gryaditskaya*

Main category: cs.CV

TL;DR: 提出一种基于“调制”的生成方法，用自由手草图作为条件，强调语义一致而非像素对齐，并设计无需像素对齐真值的训练损失；在保持写实性的同时更好遵循草图语义，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言已成为主流条件，但用户需要更细粒度的可控性；现有工作多依赖边缘图而非真实自由手草图，后者存在抽象与形变，缺乏像素对齐真值，导致很难在写实与遵循草图之间取得平衡。

Method: 提出“调制式”条件机制：将草图作为高层语义信号进行语义优先的特征调制，而非逐像素对齐；同时引入一种新损失，使模型可在无像素对齐真值的自由手草图数据上训练，鼓励语义一致与全局结构匹配。

Result: 在与现有方法比较中，本方法在两方面显著更优：1) 与自由手草图的语义对齐度更高；2) 生成图像的写实性与整体质量更佳。

Conclusion: 通过语义优先的调制机制与无对齐训练损失，可有效用自由手草图控制高质量图像生成，在草图遵循与写实性间取得更好平衡，并超越现有基线。

Abstract: Recent years have witnessed remarkable progress in generative AI, with natural language emerging as the most common conditioning input. As underlying models grow more powerful, researchers are exploring increasingly diverse conditioning signals, such as depth maps, edge maps, camera parameters, and reference images, to give users finer control over generation. Among different modalities, sketches are a natural and long-standing form of human communication, enabling rapid expression of visual concepts. Previous literature has largely focused on edge maps, often misnamed 'sketches', yet algorithms that effectively handle true freehand sketches, with their inherent abstraction and distortions, remain underexplored. We pursue the challenging goal of balancing photorealism with sketch adherence when generating images from freehand input. A key obstacle is the absence of ground-truth, pixel-aligned images: by their nature, freehand sketches do not have a single correct alignment. To address this, we propose a modulation-based approach that prioritizes semantic interpretation of the sketch over strict adherence to individual edge positions. We further introduce a novel loss that enables training on freehand sketches without requiring ground-truth pixel-aligned images. We show that our method outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of the generated images.

</details>


### [142] [Advances in Global Solvers for 3D Vision](https://arxiv.org/abs/2602.14662)
*Zhenjun Zhao,Heng Yang,Bangyan Liao,Yingping Zeng,Shaocheng Yan,Yingdong Gu,Peidong Liu,Yi Zhou,Haoang Li,Javier Civera*

Main category: cs.CV

TL;DR: 该综述系统梳理3D视觉中的全局求解器，围绕BnB、凸松弛、GNC三大范式，分析其理论、算法、鲁棒与可扩展改进，并在十类任务上对比最优性-鲁棒性-可扩展性权衡，提出未来方向与资源链接。


<details>
  <summary>Details</summary>
Motivation: 传统几何估计多用局部或启发式方法，容易陷入局部最优、难以给出最优性与鲁棒性保证。近年来全局求解器能为非凸几何优化给出可证明的解与证书，但研究分散、缺乏统一视角与系统比较，亟需综述统一理论框架、应用范围与实践指南。

Method: 构建按求解范式的分类法：Branch-and-Bound、Convex Relaxation、Graduated Non-Convexity；阐述各自理论基础、算法设计与工程增强（鲁棒性、尺度扩展），并在十个核心3D视觉任务（如Wahba、位姿配准、位姿图、PnP、BA等）上对比分析，提炼选择权衡与适用场景；总结开放问题并提供持续更新的资料与代码教程。

Result: 给出三类范式的统一视角与系统比较，明确不同任务下的最优性-鲁棒性-可扩展性取舍；梳理实践技巧与提升途径；产出社区资源（文献清单与代码教程）以促进复现与应用。

Conclusion: 全局求解器正成为几何视觉中的关键工具，但需在保证与规模化之间取得更好平衡。未来应重点：提升可扩展性而不牺牲证书、将数据驱动先验与可认证优化融合、建立标准化基准、关注安全关键场景的社会影响。综述为迈向可认证、可信赖的真实世界感知提供路线图。

Abstract: Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.

</details>


### [143] [MeFEm: Medical Face Embedding model](https://arxiv.org/abs/2602.14672)
*Yury Borets,Stepan Botman*

Main category: cs.CV

TL;DR: MeFEm 是基于改造版 JEPA 的面部生物计量/医疗视觉模型，通过轴向条带遮罩、环形损失加权与 CLS 令牌概率重分配等改进，在较少数据下超越 FaRL、Franca 等基线；在 BMI 估计与多个人体测量任务上表现突出，并提供模型权重以供复现与拓展。


<details>
  <summary>Details</summary>
Motivation: 现有面部表征模型在医学与生物计量任务上受限：训练数据需求大、对语义关键区域关注不足、线性探测质量不稳，且 BMI 等任务存在数据域偏置与公开数据缺口。作者欲构建在有限数据下仍具强泛化的表征，并缓解域偏置问题。

Method: 在 JEPA 框架上进行三项核心改动：1) 轴向条带遮罩（axial stripe masking）引导模型聚焦面部语义相关区域；2) 环形（circular）损失加权以更合理地衡量角度或周期性相关误差；3) 对 CLS 令牌进行概率式重分配以提升线性探测质量。模型在经整合与筛选的人脸数据集上训练，并在标准人体测量与 BMI 任务上评估。

Result: 在核心人体测量（anthropometric）任务上，MeFEm 以显著更少的数据超越强基线 FaRL 与 Franca；在 BMI 估计上取得“有前景”的结果，并使用一个新的闭源整合数据集来减轻现有数据的域偏差。作者公开了模型权重（Hugging Face 链接）。

Conclusion: MeFEm 通过对 JEPA 的针对性改造，在低数据条件下实现对面部生物计量/医学任务的强表征学习，既提升了线性探测与任务性能，又为 BMI 等敏感任务提供了更稳健的评估基线；公开权重可作为后续研究的有力起点。

Abstract: We present MeFEm, a vision model based on a modified Joint Embedding Predictive Architecture (JEPA) for biometric and medical analysis from facial images. Key modifications include an axial stripe masking strategy to focus learning on semantically relevant regions, a circular loss weighting scheme, and the probabilistic reassignment of the CLS token for high quality linear probing. Trained on a consolidated dataset of curated images, MeFEm outperforms strong baselines like FaRL and Franca on core anthropometric tasks despite using significantly less data. It also shows promising results on Body Mass Index (BMI) estimation, evaluated on a novel, consolidated closed-source dataset that addresses the domain bias prevalent in existing data. Model weights are available at https://huggingface.co/boretsyury/MeFEm , offering a strong baseline for future work in this domain.

</details>


### [144] [Universal Image Immunization against Diffusion-based Image Editing via Semantic Injection](https://arxiv.org/abs/2602.14679)
*Chanhui Lee,Seunghyun Shin,Donggyu Choi,Hae-gon Jeon,Jeany Son*

Main category: cs.CV

TL;DR: 提出一种首个面向扩散模型编辑流程的“通用免疫”方法：用单一通用扰动（UAP）嵌入目标语义并抑制原始内容，从而在编辑时误导模型、阻断恶意语义篡改；无需数据，具备强黑盒迁移与接近图像特定方法的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型文本引导的强大编辑能力带来深伪与版权滥用等伦理法律风险。现有“免疫”多依赖逐图优化的对抗扰动，难以规模化部署。因此需要一种无需为每张图单独优化、可广泛适用且实际可用的防护机制。

Method: 受通用对抗扰动（UAP）定向攻击启发，构造一个单一的通用扰动：1) 向被保护图像中嵌入预设语义目标；2) 同时抑制原始图像语义，以在扩散式编辑过程中重定向注意力与特征表征。该扰动在数据自由（无训练数据/领域先验）条件下生成，并专为扩散编辑流水线设计。

Result: 作为首个通用免疫方案，在UAP设定下显著优于多种基线；在更严格的扰动预算下，其效果可与逐图方法相当；对不同扩散模型表现出强黑盒可迁移性。

Conclusion: 单一通用扰动即可为广泛图像提供有效的编辑防护，兼具可扩展性、数据无关性与跨模型鲁棒性，为应对扩散模型语义篡改风险提供了实用路径。

Abstract: Recent advances in diffusion models have enabled powerful image editing capabilities guided by natural language prompts, unlocking new creative possibilities. However, they introduce significant ethical and legal risks, such as deepfakes and unauthorized use of copyrighted visual content. To address these risks, image immunization has emerged as a promising defense against AI-driven semantic manipulation. Yet, most existing approaches rely on image-specific adversarial perturbations that require individual optimization for each image, thereby limiting scalability and practicality. In this paper, we propose the first universal image immunization framework that generates a single, broadly applicable adversarial perturbation specifically designed for diffusion-based editing pipelines. Inspired by universal adversarial perturbation (UAP) techniques used in targeted attacks, our method generates a UAP that embeds a semantic target into images to be protected. Simultaneously, it suppresses original content to effectively misdirect the model's attention during editing. As a result, our approach effectively blocks malicious editing attempts by overwriting the original semantic content in the image via the UAP. Moreover, our method operates effectively even in data-free settings without requiring access to training data or domain knowledge, further enhancing its practicality and broad applicability in real-world scenarios. Extensive experiments show that our method, as the first universal immunization approach, significantly outperforms several baselines in the UAP setting. In addition, despite the inherent difficulty of universal perturbations, our method also achieves performance on par with image-specific methods under a more restricted perturbation budget, while also exhibiting strong black-box transferability across different diffusion models.

</details>


### [145] [It's a Matter of Time: Three Lessons on Long-Term Motion for Perception](https://arxiv.org/abs/2602.14705)
*Willem Davison,Xinyue Hao,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 论文利用点级轨迹（point-track）估计来学习长时程运动表示，发现它不仅能理解动作，还能表征物体/材质/空间信息；在低数据与零样本设置下泛化更好；因维度极低，算力-精度权衡优于常规视频表示，并与之结合可进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 感知研究多聚焦于图像静态信息，长时程时间维度的作用仍不清晰。作者希望回答：长期运动能教会模型哪些关于世界的知识？它具有什么学习属性？以及在计算效率与性能上的价值。

Method: 利用最新的点级轨迹估计技术，构造长时程运动表示，并在多种感知任务上系统评估其信息含量、泛化能力（低数据/零样本）与算力-精度权衡，同时与图像与标准视频表示进行对比与融合实验。

Result: 1) 长时程运动表示能有效刻画动作、物体、材质与空间关系，很多情形下优于纯图像特征；2) 在少样本与零样本设定中，其泛化显著好于图像表示；3) 由于表示维度很低，单位GFLOPs的精度更高；与标准视频表示结合可进一步刷新性能。

Conclusion: 长时程运动信息蕴含丰富跨范畴感知线索、具强泛化与高效表示优势，是未来感知模型应重点利用的信号；与视频表示互补，联合使用收益最大。

Abstract: Temporal information has long been considered to be essential for perception. While there is extensive research on the role of image information for perceptual tasks, the role of the temporal dimension remains less well understood: What can we learn about the world from long-term motion information? What properties does long-term motion information have for visual learning? We leverage recent success in point-track estimation, which offers an excellent opportunity to learn temporal representations and experiment on a variety of perceptual tasks. We draw 3 clear lessons: 1) Long-term motion representations contain information to understand actions, but also objects, materials, and spatial information, often even better than images. 2) Long-term motion representations generalize far better than image representations in low-data settings and in zero-shot tasks. 3) The very low dimensionality of motion information makes motion representations a better trade-off between GFLOPs and accuracy than standard video representations, and used together they achieve higher performance than video representations alone. We hope these insights will pave the way for the design of future models that leverage the power of long-term motion information for perception.

</details>


### [146] [Depth Completion as Parameter-Efficient Test-Time Adaptation](https://arxiv.org/abs/2602.14751)
*Bingxin Ke,Qunjie Zhou,Jiahui Huang,Xuanchi Ren,Tianchang Shen,Konrad Schindler,Laura Leal-Taixé,Shengyu Huang*

Main category: cs.CV

TL;DR: CAPA是一种在测试时利用稀疏几何观测对预训练3D基础模型进行参数高效适配的框架，仅更新少量可插拔参数（如LoRA/VPT），冻结主干，实现高质量深度补全并具备视频一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法为处理稀疏辅助输入常需训练任务特定编码器，易过拟合、泛化差；需要一种在不重新训练大模型主体的前提下，直接利用推理时的稀疏观测来校正基础模型先验、提升跨场景鲁棒性的方案。

Method: 冻结ViT类基础模型主干，只对少量可学习模块（PEFT，如LoRA或VPT）在推理时根据稀疏深度/几何观测的梯度进行更新，将场景观测对齐到模型几何先验；对视频引入序列级参数共享，联合适配所有帧，利用时序相关性提升稳健性并保证多帧一致性；方法与具体ViT基础模型无关。

Result: 在室内外多数据集、不同稀疏条件下取得SOTA深度补全性能；对失真与结构错位有显著纠正能力，视频场景下表现出更强鲁棒性与时序一致性。

Conclusion: 测试时、参数高效的适配可将3D基础模型的几何先验与场景稀疏观测有效结合，避免训练任务特定编码器的过拟合问题，提供通用、稳健且高效的深度补全方案，适用于任意ViT类基础模型。

Abstract: We introduce CAPA, a parameter-efficient test-time optimization framework that adapts pre-trained 3D foundation models (FMs) for depth completion, using sparse geometric cues. Unlike prior methods that train task-specific encoders for auxiliary inputs, which often overfit and generalize poorly, CAPA freezes the FM backbone. Instead, it updates only a minimal set of parameters using Parameter-Efficient Fine-Tuning (e.g. LoRA or VPT), guided by gradients calculated directly from the sparse observations available at inference time. This approach effectively grounds the foundation model's geometric prior in the scene-specific measurements, correcting distortions and misplaced structures. For videos, CAPA introduces sequence-level parameter sharing, jointly adapting all frames to exploit temporal correlations, improve robustness, and enforce multi-frame consistency. CAPA is model-agnostic, compatible with any ViT-based FM, and achieves state-of-the-art results across diverse condition patterns on both indoor and outdoor datasets. Project page: research.nvidia.com/labs/dvl/projects/capa.

</details>


### [147] [SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning](https://arxiv.org/abs/2602.14767)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.CV

TL;DR: 提出SAILS：无需训练的增量语义分割框架，利用SAM零样本区域分割+固定特征空间原型匹配，并通过类内选择性聚类多原型建模，避免遗忘、低算力、仍优于训练式方法，并出现正向回传。


<details>
  <summary>Details</summary>
Motivation: 增量学习受制于反复训练、高算力消耗与灾难性遗忘，尤其在长序列任务中表现退化。作者希望在现实场景中避免昂贵的增量训练与遗忘问题。

Method: 将CISS解耦为两阶段：1）用SAM进行零样本区域提取；2）在冻结的特征空间中进行语义关联，通过原型匹配完成分类。引入选择性类内聚类，为每个类别学习多个原型以覆盖类内多样性；全流程不更新参数、无需增量训练。

Result: 在标准CISS数据集上，无需任何增量训练即可通常超越现有需训练的方法，尤其在长且困难的任务序列中表现更好；由于不更新参数，完全消除遗忘并保持任务不变的一致性能；还观察到正向反向迁移（新增类提升旧类性能）。

Conclusion: 训练自由的SAILS通过SAM+原型多中心建模实现稳定且强大的增量语义分割：零遗忘、低计算、可在长序列中优于训练式基线，并可能带来对旧类的性能提升。

Abstract: Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.

</details>


### [148] [GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2602.14771)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 提出GOT-JEPA与OccuSolver，通过预测“跟踪模型”与点式可见性估计，显著提升目标跟踪在遮挡与未知场景下的泛化与鲁棒性；在七大基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有通用跟踪器常对训练目标过拟合，遇到未见场景与遮挡时泛化弱；且遮挡推理多为粗粒度，难以刻画细致的遮挡模式。作者希望通过更稳健的表征学习与显式遮挡建模来提升鲁棒性与泛化。

Method: 1) GOT-JEPA：将JEPA从“预测图像特征”扩展为“预测跟踪模型”。给定相同历史信息，教师在干净当前帧上生成伪跟踪模型；学生在被扰动的当前帧上学习预测同一伪模型，实现稳健的模型预测式预训练，显式对抗遮挡、干扰物与劣质观测。2) OccuSolver：基于点为中心的点式跟踪器进行对象感知的可见性估计与细粒度遮挡模式捕获；利用跟踪器迭代生成的目标先验，逐步细化可见性状态并产出更高质量的参考标签，反哺后续模型预测。

Result: 在七个跟踪基准上进行广泛评测，方法显著提升跟踪器在动态环境、遮挡与干扰下的泛化与鲁棒性（摘要未给出具体数值）。

Conclusion: 模型预测式预训练（GOT-JEPA）结合点式遮挡求解器（OccuSolver）可提供稳定监督与细致可见性建模，从而提升通用目标跟踪的泛化与遮挡处理能力，并在多基准上证明有效。

Abstract: The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.

</details>


### [149] [VIPA: Visual Informative Part Attention for Referring Image Segmentation](https://arxiv.org/abs/2602.14788)
*Yubin Cho,Hyunwoo Yu,Kyeongbo Kong,Kyomin Sohn,Bongjoon Hyun,Suk-Ju Kang*

Main category: cs.CV

TL;DR: 提出VIPA用于指代图像分割：先生成“视觉表达”以挑选并精炼关键信息视觉token，再以注意力对齐细粒度目标；在四个数据集上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RIS方法多把视觉信息简单注入语言token，跨模态投影方差大、语义对齐不稳定，难以精确到细粒度目标区域。需要一种更稳健的方式充分利用视觉上下文并保持语义一致性。

Method: 提出Visual Informative Part Attention (VIPA) 框架：1) 生成“视觉表达”(visual expression)，即从视觉上下文中过滤出信息量大的局部区域token；2) 设计Visual Expression Generator (VEG)，利用局部-全局语言线索检索并精炼视觉token，抑制噪声并共享有用的视觉属性；3) 将精炼后的视觉表达用于注意力对齐，降低高方差跨模态投影、提升语义一致性，从而引导网络关注细粒度ROI。

Result: 大量实验与可视化分析显示VIPA在四个公开RIS基准上优于现有SOTA。

Conclusion: 通过构建并利用“视觉表达”以强化跨模态注意力对齐，VIPA在细粒度区域定位与分割上更稳健有效，带来全面SOTA性能提升。

Abstract: Referring Image Segmentation (RIS) aims to segment a target object described by a natural language expression. Existing methods have evolved by leveraging the vision information into the language tokens. To more effectively exploit visual contexts for fine-grained segmentation, we propose a novel Visual Informative Part Attention (VIPA) framework for referring image segmentation. VIPA leverages the informative parts of visual contexts, called a visual expression, which can effectively provide the structural and semantic visual target information to the network. This design reduces high-variance cross-modal projection and enhances semantic consistency in an attention mechanism of the referring image segmentation. We also design a visual expression generator (VEG) module, which retrieves informative visual tokens via local-global linguistic context cues and refines the retrieved tokens for reducing noise information and sharing informative visual attributes. This module allows the visual expression to consider comprehensive contexts and capture semantic visual contexts of informative regions. In this way, our framework enables the network's attention to robustly align with the fine-grained regions of interest. Extensive experiments and visual analysis demonstrate the effectiveness of our approach. Our VIPA outperforms the existing state-of-the-art methods on four public RIS benchmarks.

</details>


### [150] [Debiasing Central Fixation Confounds Reveals a Peripheral "Sweet Spot" for Human-like Scanpaths in Hard-Attention Vision](https://arxiv.org/abs/2602.14834)
*Pengcheng Pan,Yonekura Shogo,Yasuo Kuniyosh*

Main category: cs.CV

TL;DR: 论文指出传统扫视路径评估易被数据集中心偏置误导；在Gaze-CIFAR-10上，简单中心注视基线即可取得高分。作者提出去中心偏置并加入运动相似性的GCS指标，发现仅在中等视野/贴片大小且兼具中央与周边视觉时，模型的注视频率与人类最一致；视野过大会出现“捷径”行为。


<details>
  <summary>Details</summary>
Motivation: 当前基于任务的硬注意力视觉模型常用“与人类凝视一致性”来评价，但常见扫视路径指标受中心偏置强烈干扰，尤其在以目标为中心的数据上，导致对齐度被高估、难以区分真正的行为对齐与简单的中央趋向。

Method: 1) 在Gaze-CIFAR-10上用“始终看中心”的朴素基线与多种学得策略比较，量化常用指标中的中心偏置效应；2) 在受限视觉设定下，对硬注意力分类器系统性扫描：改变中心凹贴片大小与周边上下文强度，分析扫视动态与性能；3) 设计GCS（Gaze Consistency Score）：对常用路径相似度做中心去偏，并加入时序运动统计相似性，形成复合评分。

Result: 发现中心注视基线即可逼近乃至匹配多种学习策略的扫视分数，使原有指标乐观；在参数扫描中存在“周边甜点区”，仅在狭窄的感知约束范围内（中等贴片大小且同时具备中心凹与周边视野）能在去偏后超越中心基线并展现与人类相近的时序运动统计；GCS揭示当视野过大时模型出现“捷径模式”。

Conclusion: 评估主动感知与扫视对齐需显式处理中心偏置；GCS更能区分真实的行为对齐与中央趋向。在面向目标的数据集上，应以去偏且包含运动统计的指标与合适的感知约束来设计基准，避免因过大会场视野导致的“捷径”，促进更真实的人类样式对齐。

Abstract: Human eye movements in visual recognition reflect a balance between foveal sampling and peripheral context. Task-driven hard-attention models for vision are often evaluated by how well their scanpaths match human gaze. However, common scanpath metrics can be strongly confounded by dataset-specific center bias, especially on object-centric datasets. Using Gaze-CIFAR-10, we show that a trivial center-fixation baseline achieves surprisingly strong scanpath scores, approaching many learned policies. This makes standard metrics optimistic and blurs the distinction between genuine behavioral alignment and mere central tendency. We then analyze a hard-attention classifier under constrained vision by sweeping foveal patch size and peripheral context, revealing a peripheral sweet spot: only a narrow range of sensory constraints yields scanpaths that are simultaneously (i) above the center baseline after debiasing and (ii) temporally human-like in movement statistics. To address center bias, we propose GCS (Gaze Consistency Score), a center-debiased composite metric augmented with movement similarity. GCS uncovers a robust sweet spot at medium patch size with both foveal and peripheral vision, that is not obvious from raw scanpath metrics or accuracy alone, and also highlights a "shortcut regime" when the field-of-view becomes too large. We discuss implications for evaluating active perception on object-centric datasets and for designing gaze benchmarks that better separate behavioral alignment from center bias.

</details>


### [151] [Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation](https://arxiv.org/abs/2602.14837)
*Lorenzo Mur Labadia,Ruben Martinez-Cantin,Jose J. Guerrero,Giovanni M. Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 提出用于短期物体交互预测（STA）的新架构与可供性建模，显著提升Ego4D与EPIC-Kitchens上的Top-5 mAP（最高+23/+31个百分点），并公开代码与标注。


<details>
  <summary>Details</summary>
Motivation: STA对可穿戴助手与人机协作至关重要：需从第一视角视频预测即将交互的物体位置、交互动词/名词以及接触时间。现有方法对时间聚合、跨模态融合与对人类行为（可供性）的显式建模不足，限制了预测的准确性与可解释性。

Method: 1) 架构：提出STAformer与STAformer++，采用帧引导的时间池化、图像-视频双重注意力、与多尺度特征融合，从图像-视频对输入进行STA预测；2) 可供性建模：a) 环境可供性模型，作为场景中可发生交互的持久记忆，提供晚期融合与自适应学习融合两种策略；b) 交互热点预测，基于手与物体轨迹推断潜在接触区域，并提升该区域附近STA预测置信度。

Result: 在Ego4D与新整理的EPIC-Kitchens STA标签上，Overall Top-5 mAP显著提升：Ego4D最高+23个百分点，EPIC-Kitchens最高+31个百分点。

Conclusion: 注意力架构结合多尺度与帧引导聚合，并将环境与行为可供性显式融入STA，可大幅改进短期物体交互预测的准确性与定位可信度；资源已开源，利于后续研究。

Abstract: Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.

</details>


### [152] [Multi-dimensional Persistent Sheaf Laplacians for Image Analysis](https://arxiv.org/abs/2602.14846)
*Xiang Xiang Wang,Guo-Wei Wei*

Main category: cs.CV

TL;DR: 提出多维持久层析（sheaf）拉普拉斯框架（MPSL），在不同降维维度与多尺度上提取图像的局部拓扑谱特征，并跨维度聚合，用于更稳健的分类。


<details>
  <summary>Details</summary>
Motivation: 传统降维（如PCA）对所选目标维度高度敏感；单一维度或简单平均难以兼顾不同维度的互补信息与稳定性，因此需要能在多维度上同时利用和融合信息的表示方法。

Method: 1) 将每个图像样本建模为单纯形复形；2) 在给定降维维度下，计算持久sheaf拉普拉斯以获得多尺度、局部化的拓扑谱表示；3) 对各尺度的谱进行统计汇总；4) 在多个降维维度间进一步聚合这些统计量，形成多尺度-多维度表示；5) 用于下游分类评估。

Result: 在COIL20与ETH80数据集的标准分类协议下，方法在广泛的目标维度范围内表现更稳定；在中等维度区间，相比PCA基线取得一致性提升。

Conclusion: 跨维度整合持久sheaf拉普拉斯谱能缓解降维维度选择敏感性，提供稳健的多尺度拓扑表征，并在实际分类中优于PCA基线，尤其在中等降维设置下。

Abstract: We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.

</details>


### [153] [CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography](https://arxiv.org/abs/2602.14879)
*Qingqing Zhu,Qiao Jin,Tejas S. Mathai,Yin Fang,Zhizheng Wang,Yifan Yang,Maame Sarfo-Gyamfi,Benjamin Hou,Ran Gu,Praveen T. S. Balamuralikrishna,Kenneth C. Wang,Ronald M. Summers,Zhiyong Lu*

Main category: cs.CV

TL;DR: 提出CT-Bench：首个含病灶级标注的大规模CT基准，含20,335个病灶与2,850条多任务VQA；评测多模态SOTA并与放射科医师对比，微调后显著提升，证明其临床与研究价值。


<details>
  <summary>Details</summary>
Motivation: 当前AI可在CT上勾画病灶并自动生成报告，但缺乏公开、病灶级标注的数据集限制了算法发展与客观评测。因此需要一个覆盖检测、定位、描述、测量与属性分类的标准化基准，以贴近临床实际并促进模型泛化与可比性。

Method: 构建CT-Bench，包含两部分：1) 病灶图像与元数据集（20,335个病灶/7,795个CT研究，提供bbox、文字描述与尺寸信息）；2) 多任务VQA基准（2,850问答），涵盖定位、描述、尺寸估计与属性分类，并引入困难负样本。用多种SOTA多模态模型（视觉-语言、医疗CLIP变体）进行评测，并与放射科医师表现对比；同时在病灶数据上微调模型，观察对两部分任务的影响。

Result: CT-Bench实现了系统性评测：包含困难负例能更真实反映临床挑战。SOTA多模态模型在该基准上的表现与放射科医师进行对照，显示当前模型仍有差距；在Lesion Image and Metadata Set上微调后，模型在检测/定位、描述、尺寸与属性分类等多项指标上显著提升，且对VQA子任务也有增益。

Conclusion: CT-Bench为病灶级CT分析提供了全面、可复现的评测与训练资源，既能客观比较多模态模型，也能通过微调显著提升性能；其设计（含困难负样本与多任务设置）增强了临床相关性，预期将推动CT病灶分析与报告生成研究发展。

Abstract: Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.

</details>


### [154] [Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery](https://arxiv.org/abs/2602.14929)
*Chandrakanth Gudavalli,Tajuddin Manhar Mohammed,Abhay Yadav,Ananth Vishnu Bhaskar,Hardik Prajapati,Cheng Peng,Rama Chellappa,Shivkumar Chandrasekaran,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出Wrivinder：一种零样本、几何驱动的方法，将多张地面照片聚合成一致3D场景并与卫星图对齐；并发布多视角地面-卫星数据集MC-Sat。方法融合SfM、3D高斯Splatting、语义锚定与单目深度度量线索，渲染稳定天顶视图与卫星直接匹配，实现无配对监督的跨视角定位，在多类场景中达成<30m精度。


<details>
  <summary>Details</summary>
Motivation: 地面图像与地理配准的卫星图跨视角对齐对制图、导航、态势感知至关重要，但在大视角差、GPS不可靠时依旧困难，且缺乏系统评测基准。作者动机是构建无需成对监督、可鲁棒跨视角对齐的几何中心方法与评测数据集。

Method: 提出Wrivinder：1) 基于多张地面图做SfM重建初始稀疏几何与相机；2) 用3D Gaussian Splatting实现高效连续表征与新视图合成；3) 引入语义锚定提升跨模态一致性；4) 融合单目深度得到度量尺度线索；5) 从重建中渲染稳定的天顶视图，与卫星图直接匹配以实现相机地理定位。并发布MC-Sat数据集，将多视角地面图像与地理配准卫星瓦片在多样户外场景中配对用于评测。

Result: 零样本实验中，在密集与大范围场景均实现亚30米级地理定位精度；作为首个全面基线与测试台，方法在无配对监督的跨视角对齐任务中表现稳健。

Conclusion: 几何驱动的多视图聚合与稳定天顶视渲染可有效弥合地面—卫星视角鸿沟；Wrivinder与MC-Sat为研究无配对监督的跨视角对齐提供了方法与基准，显示出在复杂环境下实现鲁棒地理定位的潜力。

Abstract: Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.

</details>


### [155] [AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories](https://arxiv.org/abs/2602.14941)
*Zun Wang,Han Lin,Jaehong Yoon,Jaemin Cho,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出AnchorWeave：以多局部几何记忆替代单一全局记忆，并在生成时进行“编织”式整合，从而显著提升可控相机视频生成的长时空一致性与画质。


<details>
  <summary>Details</summary>
Motivation: 基于记忆的相机可控视频生成常依赖从多视角重建的全局3D场景来渲染锚点视频，但多视角融合会因位姿/深度误差导致跨视角错位，进而累积成噪声几何，污染条件信号，损害生成质量。需要一种避免全局重建错位、仍能提供强几何约束的记忆机制。

Method: 以“多局部几何记忆”取代单一全局记忆：1）覆盖度驱动的局部记忆检索，与目标相机轨迹对齐以选取若干干净的局部几何块；2）在生成过程中通过“多锚（multi-anchor）编织控制器”将所选局部记忆进行协调与融合，显式处理跨视角不一致；3）在内存增广框架中进行端到端训练/推理，以实现一致性与画质的平衡。

Result: 在多项实验中，方法在长时间场景一致性上显著优于现有方法，同时保持较强的视觉质量。消融与分析表明：局部几何条件、多锚控制器、以及覆盖度驱动检索均是有效且关键的组件。

Conclusion: 避免误差累积的关键在于抛弃单一全局几何记忆，转而使用可检索、可编织的多局部几何记忆。AnchorWeave在长时一致性与画质之间取得更佳权衡，并为相机可控视频生成提供了稳健的记忆与控制范式。

Abstract: Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.

</details>


### [156] [PAct: Part-Decomposed Single-View Articulated Object Generation](https://arxiv.org/abs/2602.14965)
*Qingming Liu,Xinyue Yao,Shuyuan Zhang,Yueci Deng,Guiliang Liu,Zhen Liu,Kui Jia*

Main category: cs.CV

TL;DR: 提出一种面向零件的生成式框架，从单张图像快速生成具有正确零件分解与运动学的三维可动对象，兼具实例一致性、零件准确性和合理关节运动，并显著快于优化与检索范式。


<details>
  <summary>Details</summary>
Motivation: 现有三维可动物体资产生成难以规模化：优化/蒸馏方法精度高但每例耗时长；基于模板/检索的方法推理快但难以匹配输入实例的具体结构与外观。需要一种既高效又能保持实例级对应与正确运动学的方案。

Method: 提出以“零件”为中心的生成式表示：对象由一组可运动零件构成，每个零件以包含零件身份与运动线索的潜在token编码。模型在显式零件条件下，同时合成零件几何、组合关系与关节运动。给定单张图像，采用前馈推理生成保持实例对应关系的可动3D资产，并支持可控装配与关节驱动。

Result: 在常见可动类别（如抽屉、门）上，相比优化式与检索式基线，提升了输入一致性、零件识别/分解准确度与运动合理性，同时将推理时间大幅降低（避免逐例优化）。

Conclusion: 面向零件的生成框架在单图条件下实现快速、可控且结构/运动一致的三维可动对象生成，为具身交互应用提供高效资产生产途径。

Abstract: Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.

</details>


### [157] [ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery](https://arxiv.org/abs/2602.14989)
*Ayush Shrivastava,Kirtan Gangani,Laksh Jain,Mayank Goel,Nipun Batra*

Main category: cs.CV

TL;DR: 提出ThermEval基准（含约5.5万热成像VQA对）与新数据集ThermEval-D（含像素级温度与语义部位标注），系统评测25个VLM在热成像理解上的能力，发现现有VLM在温度关联推理、伪彩色变化稳健性等方面显著失效，提示热成像需要独立于RGB的评测与建模。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要在RGB数据上训练与评测，难以泛化到热像任务；而热感知在夜间安防、救援、自动驾驶与医疗等关键场景至关重要，且热图像编码的是物理温度而非颜色/纹理，现有RGB基准无法评估所需的物理感知与推理原语。

Method: 构建ThermEval-B热像VQA基准，整合公开热像数据并引入自采的ThermEval-D：提供跨室内外多场景的稠密逐像素温度图与人体部位语义标注；设计覆盖温度感知、相对/绝对温度比较、部位-温度关联、环境与物体识别、伪彩色鲁棒性等子任务；在统一协议下评测25个开源/闭源VLM，并测试提示工程与监督微调的增益。

Result: 所有评测VLM在温度绑定的推理任务上显著失效；对伪彩色（colormap）变化缺乏鲁棒性；常回退到语言先验或固定答案；即便通过prompt或有监督微调，仅获得边际提升。

Conclusion: 热成像理解不应沿用RGB中心假设，需要专门的评测与方法；ThermEval为推动热视觉语言建模提供了系统化基准与数据资源。

Abstract: Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.

</details>


### [158] [Image Generation with a Sphere Encoder](https://arxiv.org/abs/2602.15030)
*Kaiyu Yue,Menglin Jia,Ji Hou,Tom Goldstein*

Main category: cs.CV

TL;DR: 提出Sphere Encoder：单次前向或极少步即可生成，与多步扩散模型性能相当、推理成本显著更低。方法：学习把自然图像均匀映射到球面潜空间的编码器和从球面点解码回图像的解码器，仅用重建损失训练；随机取球面点解码即为生成；可条件生成，并可循环编解码数次提质。多数据集上结果具竞争力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽强但推理需多步、成本高；GAN不稳定或有模式崩溃。需要一种既高质量又高效率（少步甚至一步）的生成框架，且训练简单、可条件化与可扩展。

Method: 1) 学习一个编码器，将真实图像均匀映射到球面潜变量（单位球）上；2) 学习一个解码器，从球面潜向量重建图像；3) 仅用重建损失端到端训练（无对抗、无噪声调度）；4) 生成时在球面上均匀采样随机点并用解码器解码；5) 可迭代地将生成结果再编码-解码数次以提升质量；6) 体系自然支持条件信息注入以做条件生成。

Result: 在多个数据集上，Sphere Encoder用单步或<5步的推理即可达到与最先进扩散模型相当的生成质量，同时显著降低推理成本；循环编解码进一步提升画质；条件生成同样有效。

Conclusion: 球面潜空间+重建式训练可实现高效高质生成，显著减少推理步骤和计算，兼具条件生成与迭代提质能力；为扩散之外提供了实用替代方案。

Abstract: We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .

</details>


### [159] [EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing](https://arxiv.org/abs/2602.15031)
*Yehonathan Litman,Shikun Liu,Dario Seyb,Nicholas Milef,Yang Zhou,Carl Marshall,Shubham Tulsiani,Caleb Leak*

Main category: cs.CV

TL;DR: 提出EditCtrl：只在掩膜区域进行生成与注意力计算，并用轻量级全局时序嵌入保证一致性，实现高质量、低成本的视频修复式编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频基础模型的生成式视频编辑质量高但代价大：无论编辑区域多小，通常对整段视频做全局注意力/推理，导致计算与显存浪费并限制实际应用。

Method: 1) 局部视频上下文模块：仅对掩膜区域（masked tokens）执行扩散/注意力与特征传播，计算复杂度与编辑面积成正比；2) 轻量级时间全局上下文嵌入器：在不进行完整全注意力的前提下，提取视频级全局表征，指导局部生成以保持跨帧内容与风格一致；3) 以局部优先的生成流程，并支持多区域掩膜、文本提示控制与自回归式内容传播。

Result: 在计算成本上较SOTA生成式编辑方法提升约10倍效率，同时在编辑质量上也优于采用全注意力的基线；并展示了多区域文本编辑与跨帧自回传播的新能力。

Conclusion: 通过将计算聚焦于掩膜区域并用轻量全局时序约束维持一致性，EditCtrl实现了更高效且更高质的视频编辑，为实际交互式、多区域与长视频编辑解锁了新可能。

Abstract: High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.

</details>
