<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 112]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 提出一个两阶段生成式数据增广框架：规则口罩扭曲+无配对GAN翻译，生成更逼真的口罩人脸；加入非口罩保持损失和随机噪声提升稳定性与多样性；相较仅规则扭曲与现有方法（如 IAMGAN）有一致改进。


<details>
  <summary>Details</summary>
Motivation: 口罩遮挡导致人脸检测/识别在数据稀缺与分布移位下性能下降，现有仅规则合成或单一GAN方法难以生成足够真实且多样的遮挡样本，需要更有效的数据增广策略。

Method: 两步生成：1) 规则驱动的口罩形变/贴合，将口罩模板与人脸几何对齐生成初始合成图；2) 基于无配对图像到图像翻译的GAN对合成图进行风格/纹理/光照逼真化。训练中引入非口罩区域保持损失以保护未遮挡面部内容，并注入随机噪声提高多样性与训练稳定性；与现有GAN如 IAMGAN互补。

Result: 相较仅规则扭曲，生成图在质感与逼真度上有一致的定性提升；与IAMGAN等方法互补。实验表明非口罩保持损失与噪声注入能稳定训练并提升样本多样性。

Conclusion: 两阶段生成式增广能缓解口罩人脸数据稀缺与分布偏移，对检测/识别任务有效；提出的损失与噪声机制关键。未来可在数据中心化增广、组件改进与更系统的量化评估上进一步提升。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出JARVIS：在MLLM训练中引入JEPA式自监督视觉增强，以缓解仅依赖文本监督导致的视觉推理不足，统一提升多种LLM家族的视觉基准表现且不损多模态推理。


<details>
  <summary>Details</summary>
Motivation: MLLM依赖文本描述学习视觉理解，监督主观且不完整；多模态指令调优规模小于大规模纯文本预训练，模型易过拟合语言先验、忽视视觉细节，导致基础视觉推理薄弱。

Method: 将I-JEPA范式融入标准视觉-语言对齐流程：使用冻结的视觉基础模型分别作为context与target编码器；将LLM的前若干层作为可训练的predictor，学习图像的结构与语义规律，减少对语言监督的依赖，实现自监督的视觉表示增强。

Result: 在标准MLLM基准上，JARVIS在不同LLM家族中均显著提升以视觉为中心的任务表现，同时保持甚至不降低多模态推理能力。

Conclusion: JEPA风格的自监督视觉增强可有效缓解MLLM对语言先验的过拟合，提升视觉细节与结构理解，作为通用插件式训练范式可稳健增益多家族MLLM而不牺牲多模态推理。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: 提出Sparsely Grounded Visual Navigation任务与CityNav基准，评估MLLM仅凭视觉与内在推理进行长程城市导航；现有模型显著失效，作者提出VoP通过显式言语化路径与地标认知图大幅提升成功率。


<details>
  <summary>Details</summary>
Motivation: 现有评测偏语言或仿真，缺少对真实世界、知识密集、细粒度顺序决策的检验；需要能在真实城市环境中考察MLLM的视觉定位、空间推理与策略规划能力。

Method: 1) 定义Sparsely Grounded Visual Navigation任务：仅用视觉输入，无环境标注/特化结构，连续50+步决策；2) 构建CityNav基准：覆盖四座全球城市，要求自主地标识别、定位、空间推理与路径规划；3) 系统评测多种SOTA MLLM与推理范式（CoT、Reflection）；4) 提出VoP（Verbalization of Path）：从MLLM显式抽取并维持“认知地图”（关键地标+朝向/方向），将内部推理外显化并用于引导后续行动。

Result: 在CityNav上，现有SOTA MLLMs与标准推理技巧显著低于期望，难以完成长程导航；采用VoP后，导航成功率与稳定性显著提升（摘要未给出具体数值，但呈现大幅度改进）。

Conclusion: 语言-中心或仿真基准不足以衡量真实世界城市导航能力；SGVN与CityNav揭示当前MLLM在视觉定位与序列规划上的不足；通过将认知地图言语化的VoP能有效加强MLLM的可解释与可用的导航推理。

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4提出一种无需训练的4D（空间+时间）检索增强推理框架，为VLM构建可共享、持久的对象级世界记忆，并在查询时以语义/空间/时间键检索相关观测，从而显著提升具身问答和导航中的时空推理能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过持久且结构化的多模态记忆在4D中理解与推理，能回忆、补全未观测状态并将新信息置于上下文中；现有RAG多在文本或2D/3D层面，缺乏直接在4D时空中进行的结构化检索与协作式、情景化推理能力。

Method: 构建R4框架：持续将对象级语义描述锚定到度量空间与时间，形成可跨主体共享的4D知识库；推理时把自然语言查询分解为语义、空间、时间三类键，在4D数据库中检索对应观测，并将检索结果与VLM结合进行推理；全流程无需额外训练。

Result: 在具身问答与导航基准上，R4在时空信息的检索和推理上显著优于基线，支持情景式与协作式推理，展示出强鲁棒性与泛化性。

Conclusion: 直接在4D空间进行检索与记忆整合，为VLM带来持久、可共享的世界模型，显著提升动态环境中的推理与决策，开启具身4D推理的新范式。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 提出“The Perceptual Observatory”评测框架，用受控扰动与多垂直任务系统刻画MLLM的感知与视觉落地能力，超越仅看终端准确率的评估。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多扩大语言部分而复用相似视觉编码器，进步可能来自文本知识而非真实视觉落地；现有评估忽视鲁棒性、归因保真和受控扰动下的推理。需要一个能系统检验感知与结构保持的框架。

Method: 构建多个“垂直”评估：①简单视觉任务（如人脸匹配、图中文本理解）；②由局部到全局理解（图像匹配、网格指点游戏、属性定位）以测试通用视觉落地。为每个垂直提供具备真值的人脸与词语数据集，并施加像素级增强与基于扩散的风格化幻觉等系统扰动，观察模型在扰动下的表现与结构保持。

Result: 该框架能在排行榜准确率之外揭示MLLM在保留感知落地与关系结构方面的表现差异，暴露其强项与弱项；初步显示不同规模/家族模型在鲁棒性与归因方面存在显著分化。

Conclusion: The Perceptual Observatory为分析当前与未来MLLM的感知与视觉落地能力提供了有原则的基础，促进对鲁棒性、归因与受控推理的全面理解。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 提出CAMP-VLM，一个从第三人称视角进行多人体行为预测的VLM框架，融合视觉上下文与场景图空间关系，用合成数据SFT+DPO微调，并在合成与真实序列上验证，较最佳基线最高提升66.9%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有工作多为单人、第一视角的动作预测，难以满足机器人在拥有人群环境中对多人的交互理解需求；缺乏适合第三人称多人体行为预测的数据集。

Method: 构建CAMP-VLM：从视觉输入提取上下文特征，并结合场景图获得空间与人-物关系；使用写真级模拟器生成多人体行为合成数据，对VLM进行监督微调（SFT）与偏好优化（DPO）；在合成与真实世界序列上评测泛化。

Result: 在预测准确率上显著优于基线，最高超出66.9%；模型在合成和真实数据上均表现出较好的泛化能力。

Conclusion: 融合视觉上下文与场景图的VLM并配合合成数据与SFT+DPO训练，可有效进行第三人称多人体行为预测，为人群环境中的机器人理解与规划提供有力支持。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 将Grounding DINO与YOLO-World两类VLM检测器扩展到可处理可见光+热红外多光谱输入，在FLIR与M3FD上验证：在少样本下显著优于专用多光谱模型，且全监督下也具竞争力；表明大规模VLM的语义先验可迁移到未见光谱，提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 多光谱检测对自动驾驶/安防至关重要，但标注数据稀缺；需要利用文本类别先验与VLM的跨模态知识，在少样本场景提升检测性能与数据效率。

Method: 将两种代表性的VLM型检测器（Grounding DINO、YOLO-World）适配为多光谱：设计融合机制整合文本提示、可见光与热红外特征（跨模态/跨光谱对齐与融合），在FLIR与M3FD上进行少样本与全监督实验评估。

Result: 在少样本（few-shot）条件下显著超越同等数据量训练的专用多光谱模型；在全监督条件下达到有竞争力甚至更优的性能。

Conclusion: 大规模VLM学习到的语义先验可有效迁移到未见的光谱模态，为数据高效的多光谱感知提供了有效路径。

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 基准评测27个农业分类数据集：零样本VLM显著落后于监督式YOLO11；选择题提示下最佳模型约62%准确率，开放式远低（<25%），用LLM语义判分可把开放式提升至~30%并改变排名。物种识别较易，病虫害/损伤最难。当前VLM不适合作为独立诊断，但在受限界面与本体约束下可作辅助。


<details>
  <summary>Details</summary>
Motivation: 农业领域需要可靠的视觉识别来支持病虫害诊断与作物管理。VLM被宣称为通用方案，但其在农业高风险场景的可靠性和评估方法尚不清楚，需系统性基准来量化能力与限制。

Method: 对27个AgML数据集（162类，涵盖病害、虫害/损伤、植物/杂草识别）进行统一评测。比较多种开源与闭源VLM在零样本、开放式与多选提示下的表现；与监督式任务专用基线YOLO11对比。引入LLM语义判分以评估开放式回答，并进行任务层面的难度分析与模型排名。

Result: YOLO11在全部任务上显著优于所有VLM。最佳闭源VLM（Gemini-3 Pro）在多选提示下平均约62%准确率；开放式提示下原始准确率多<25%。语义判分可将开放式准确率从约21%提升到30%左右并改变模型排名。开源中Qwen-VL-72B最强，在受限提示下接近闭源但仍落后。物种识别相对容易，虫害/损伤最难。

Conclusion: 现成VLM尚不足以独立承担农业诊断；在受限界面、明确标签本体与领域感知评估策略下可作为辅助组件。评估方法显著影响结论，未来应结合领域知识、改进提示与微调以提升可靠性。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

</details>


### [9] [Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings](https://arxiv.org/abs/2512.15993)
*Lars Beckers,Arno Waes,Aaron Van Campenhout,Toon Goedemé*

Main category: cs.CV

TL;DR: 提出一种通过视觉感知与自适应决策提升花园生物多样性的机器人割草框架，利用深度特征空间评估多样性并选择性停刀保护多样斑块，实机与数据集验证其与专家评估高度相关。


<details>
  <summary>Details</summary>
Motivation: 传统机器人割草机将草坪维持为单一、贫生境的“绿色地毯”，被动“再野化”方法缺少可操作的细粒度控制。需要一种可在家用场景中主动识别并保护潜在高多样性的植被斑块、同时兼顾整洁度的智能系统。

Method: 使用在 PlantNet300K 上预训练的 ResNet50 提取植物图像嵌入；在嵌入空间中计算全局离散/偏离度作为无监督的“视觉多样性”估计；将该估计接入选择性割草策略，在移动过程中根据当前视野的多样性动态切换“割草/保护”（启停刀片）；在改装商用品质的机器人割草机上实现并在模拟草坪与真实花园数据上评估。

Result: 嵌入空间离散度与专家的生物多样性评估具有显著相关性；系统能在保持割草功能的同时对高多样性区域选择性停刀，实现对多样性斑块的保留。

Conclusion: 深度视觉嵌入可作为无监督的生物多样性代理指标，驱动家用割草机的选择性保护行为。若广泛部署，可将单一草坪转化为提升城市生物多样性的微型生境。

Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.

</details>


### [10] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出一种从初始观测图像与机器人关节状态出发，按文本指令生成视频-动作对的方法，用于训练/监督视频扩散模型并提升机器人策略学习。核心是并联动作扩散、跨模态桥接注意力与动作精炼模块，显著优于现有两阶段或单模态适配方法。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型常缺少动作标注，限制其在机器人策略学习中的效用。现有方法要么两阶段导致跨模态耦合弱，要么把单模态模型硬适配为联合分布，难以充分利用预训练视频知识。需要能自动生成带动作标签的视频、并实现高效跨模态交互的框架。

Method: 1) 在预训练视频扩散模型旁并联一个专用动作扩散模型，保持视频侧知识完整；2) 提出Bridge Attention，使视频与动作在扩散过程中高效交互共享信息；3) 设计动作精炼模块，将粗粒度动作变为低分辨率数据可用的精确控制；输入为初始图像与关节状态和文本指令，输出为与视频同步的动作序列。

Result: 在多项公开基准与真实数据上，生成视频质量更高、动作更准确，且显著超过各类基线；验证了方法在大规模视频数据上用于机器人学习的可扩展性与有效性。

Conclusion: 并联动作扩散+桥接注意力+动作精炼构成的跨模态生成框架可自动生成带动作标签的视频，充分利用预训练视频模型，改善机器人策略学习表现，优于两阶段与单模态联合适配方案。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

</details>


### [11] [Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.16055)
*Jiaheng Geng,Jiatong Du,Xinyu Zhang,Ye Li,Panqu Wang,Yanjun Huang*

Main category: cs.CV

TL;DR: 提出一个用于真实世界端到端自动驾驶的闭环对抗评测平台，结合基于flow matching的真实图像生成器与对抗式交通策略，自动构造难例角落场景，并在UniAD、VAD等模型上验证其能显著诱发性能退化，从而发现潜在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有对抗评测多依赖简化仿真环境，难以反映真实道路复杂性与感知-决策闭环效应；而真实世界难例（安全关键角落场景）稀缺却对安全评估至关重要，需一种能在真实外观与交互动力学下稳定生成难例并评测端到端模型的方法。

Method: 搭建闭环评测平台：1）真实世界图像生成器：基于flow matching的条件生成，根据交通环境信息高效稳定地产生逼真驾驶图像；2）对抗式周车策略：设计高效的周边车辆行为策略，主动制造具有挑战性的交互与角落场景；3）平台与多种端到端模型（如UniAD、VAD）联动，在闭环中对模型进行压力测试并度量性能变化。

Result: 平台能高效生成高保真驾驶图像；在对UniAD、VAD等模型的实验中，对抗策略诱发明显的性能下降，表明构造的角落场景具有难度且能揭示模型脆弱性。

Conclusion: 该闭环对抗评测平台可在真实外观与交互场景下有效暴露端到端自动驾驶模型的潜在问题，有助于提升其安全性与鲁棒性；为真实世界层面的对抗评测提供可行路径。

Abstract: Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.

</details>


### [12] [FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution](https://arxiv.org/abs/2512.16075)
*Hao Tang,Hanyu Liu,Alessandro Perelli,Xi Chen,Chao Li*

Main category: cs.CV

TL;DR: 提出一种3D多通道补丁扩散模型，从单壳低角分辨率dMRI预测多壳高角分辨率的FOD，结合FOD-patch适配器、体素级条件协调与SH注意力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单壳低角分辨率dMRI获取快但FOD估计不准；多壳高角分辨率虽精确但采集时间长。现有扩散模型可从LAR-FOD生成HAR-FOD，但FOD以球谐系数表示，维度高、相关性复杂，直接生成效率与效果受限。

Method: 构建3D多通道patch级扩散生成框架：1) FOD-patch adapter引入解剖先验，提升补丁级学习的效率与对齐性；2) 体素级条件协调模块，强化全局一致性与上下文建模；3) SH注意力模块，显式建模球谐系数间的复杂相关；以LAR-FOD为条件生成HAR-FOD。

Result: 在多项实验中，该方法在HAR-FOD预测上取得最好性能，超过现有SOTA（定量指标与可视化均优）。

Conclusion: 结合解剖先验、体素条件协调与SH注意力的3D补丁扩散模型，能高效、准确地从LAR-FOD生成HAR-FOD，兼顾采集效率与重建精度，具备实际应用潜力。

Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.

</details>


### [13] [Auto-Vocabulary 3D Object Detection](https://arxiv.org/abs/2512.16077)
*Haomeng Zhang,Kuan-Chuan Peng,Suhas Lohit,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 提出AV3DOD：自动生成检测到的3D物体类别名称并进行3D目标检测，无需用户提供词表；通过引入Semantic Score评估语义质量，结合2D视觉-语言模型进行图像描述、伪3D框生成与特征语义扩展，实现在ScanNetV2与SUNRGB-D上定位与语义双SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有“开放词表”3D检测仍需用户在训练/推理时指定类别，限制了真正开放环境下的无先验类别检测与标注能力；缺少衡量自动生成类别名称质量的标准。

Method: 1) 定义Semantic Score(SS)量化自动生成类名的语义质量；2) 提出AV3DOD框架：利用2D VLM进行图像描述生成丰富语义候选；3) 从2D预测构建伪3D框并与点云/体素特征对齐；4) 在特征空间进行语义扩展与匹配，实现自动命名与3D框定位的联合优化。

Result: 在ScanNetV2与SUNRGB-D上，定位mAP与语义质量SS均达SOTA；相较SOTA方法CoDA，在ScanNetV2上整体mAP提升3.48，SS相对提升24.5%。

Conclusion: AV3DOD无需用户词表即可完成3D检测与自动命名，实现更开放、更自动化的3D理解；SS为评估语义质量提供了客观指标，方法在多数据集上验证有效。

Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.

</details>


### [14] [LAPX: Lightweight Hourglass Network with Global Context](https://arxiv.org/abs/2512.16089)
*Haopeng Zhao,Marsha Mariya Kappan,Mahdi Bamdad,Francisco Cruz*

Main category: cs.CV

TL;DR: 提出LAPX：在轻量Hourglass骨干上引入自注意力与改进阶段/注意力设计，以2.3M参数在MPII与COCO上取得具竞争力精度并实现边缘端实时。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA姿态估计精度高但参数巨大、算力开销大；部分轻量化方法仍含不利于边缘部署的算子；而追求速度的模型又因过度简化而精度受限。

Method: 在先前LAP基础上：1) 采用带全局上下文的自注意力模块；2) 改进Hourglass分阶段（stage）结构设计；3) 重新设计并轻量化注意力模块，以便高效部署；总体形成轻量Hourglass网络LAPX。

Result: 在MPII与COCO两个基准上获得具竞争力结果，模型仅2.3M参数，并在边缘设备上达到实时推理。

Conclusion: LAPX在极小参数量下兼顾精度与速度，验证了自注意力与改进阶段/注意力设计对边缘端姿态估计的有效性与可部署性。

Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.

</details>


### [15] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种利用配有闪烁星点图案的准直仪进行事件相机几何标定的方法，先基于准直仪球面运动模型线性求解内外参，再通过非线性优化精炼，在多种真实环境下较现有方法更准更稳。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高动态范围与高时间分辨率方面有独特优势，但在长距高精度测量场景中，几何标定（内外参）仍难：传统标定板难以在远距离保持高角分辨与点位稳定，事件数据的异步稀疏特性也使常规图像方法失效。需要一种既适配事件特性又可在长距离提供高精度几何约束的标定方案。

Method: 利用带“闪烁星点”图案的准直仪作为远距离稳定的准无穷远目标：1）建模准直仪视为在球面上产生已知运动（或视线方向变化）的点光源，建立事件相机观测与球面运动之间的线性关系，线性求解相机内外参初值；2）在此基础上进行非线性优化（可能包含重投影误差、时间同步与事件极性/触发模型等因素）以精炼参数，获得高精度标定。

Result: 在多种真实环境与不同条件下的实验中，该方法在精度与稳定性上均优于现有事件相机标定方法，表现为更低的重投影误差与更一致的参数估计。

Conclusion: 基于准直仪与闪烁星点的标定框架能有效解决事件相机在长距离高精度场景的几何标定难题：先线性解再非线性精炼的两阶段策略实现了准确、可靠的内外参估计，并优于现有方法。

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [16] [TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times](https://arxiv.org/abs/2512.16093)
*Jintao Zhang,Kaiwen Zheng,Kai Jiang,Haoxu Wang,Ion Stoica,Joseph E. Gonzalez,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: TurboDiffusion是一套面向视频扩散模型的加速框架，在单张RTX 5090上实现约100–200倍的端到端生成提速，同时基本保持视频质量。其核心在于注意力加速、步数蒸馏与8比特量化，并辅以多项工程优化。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散生成耗时且资源开销大，限制了大模型在实际生产和交互场景中的落地。需要一种既能极大降低时延与成本，又尽量不牺牲画质的系统化加速方案。

Method: 提出TurboDiffusion框架，由三大技术组成：1) 注意力加速：结合低比特SageAttention与可训练的稀疏线性注意力（SLA）以减少注意力计算成本；2) 步数蒸馏：采用rCM进行高效的采样步数蒸馏，减少扩散步骤；3) W8A8量化：将权重与激活量化到8比特，加速线性层并压缩模型；并加入若干工程优化。

Result: 在Wan系列多种视频生成模型（Wan2.2 I2V 14B 720P、Wan2.1 T2V 1.3B 480P、Wan2.1 T2V 14B 720P/480P）上实验，单卡RTX 5090即可实现端到端100–200×速度提升，视频质量与基线相当。代码与模型在GitHub开源。

Conclusion: 通过融合注意力近似、步数蒸馏与8比特量化等手段，TurboDiffusion可在不显著损画质的前提下，将视频扩散生成大幅加速，具有实际部署价值与通用性。

Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.

</details>


### [17] [Flexible Camera Calibration using a Collimator System](https://arxiv.org/abs/2512.16113)
*Shunkun Liang,Banglei Guan,Zhenbao Yu,Dongcai Tan,Pengju Sun,Zibin Liu,Qifeng Yu,Yang Shang*

Main category: cs.CV

TL;DR: 提出基于专用准直仪系统的相机标定新方法：利用“角度不变性”使相机与靶标相对运动退化为球面运动，从而给出多图像线性闭式解、双图像最小解，以及单张准直图像的免运动标定算法，实验优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统标定依赖棋盘格、平移旋转等6自由度运动，易受场景几何与运动控制影响；室外/大焦段/狭小空间等应用需要可控、稳定、快速的标定环境与更简洁的运动约束。作者设计准直仪系统以提供稳定几何，并利用其独特光学性质构造新的理论约束。

Method: 1) 设计可控准直仪形成远心/等效无穷远成像条件；2) 证明校准靶与相机的相对运动符合球面运动模型，提出“角度不变性”约束，将原6DOF相对运动降至3DOF纯旋转；3) 基于球面运动：a) 多图像闭式线性求解内参；b) 两张图像的最小求解器；4) 进一步提出单张准直图像标定算法，无需相机运动；5) 在合成与实拍数据上评估并与基线比较。

Result: 理论上给出角度不变性与球面运动证明；实现三个求解器（多图像闭式、双图像最小、单图像免运动）；合成与真实实验显示精度与鲁棒性优于现有基线，验证准直仪系统可行性与优势。

Conclusion: 利用准直仪的光学几何可将标定问题从一般SE(3)运动简化为SO(3)或更弱条件，带来闭式与最小求解，并可实现单张图像快速标定；方法在实测中表现优于基线，适合需要高效、可控标定的应用场景。

Abstract: Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration

</details>


### [18] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 提出CattleAct：从单张图像自动检测放牧牛只之间的行为交互，利用先学动作潜表示、再用对比学习微调以嵌入稀有交互，并在实际牧场视频+GPS系统中验证优于基线。


<details>
  <summary>Details</summary>
Motivation: 放牧牛交互（如发情相关行为）对智慧牧场至关重要，但交互属于稀有事件、缺少包含交互的全面数据集，直接做交互检测困难。需要一种数据高效、能从常见动作学习并泛化到稀有交互的方法。

Method: 提出将“交互”分解为“个体动作”的组合。步骤：1) 用大规模牛只动作数据学习动作潜在空间；2) 以对比学习在该潜空间上微调，将少量稀有交互样本嵌入，构建统一的动作-交互潜空间；3) 基于该空间进行单图交互检测；4) 构建实用系统，融合视频与GPS输入。

Result: 在商业化规模牧场数据上，方法在交互检测准确率上优于多种基线；证明了以动作潜空间迁移到稀有交互的有效性，并可在实际系统中稳定运行。

Conclusion: 通过动作-交互统一潜空间与对比学习微调，实现了数据高效的牛只交互检测；方法在真实场景表现良好，适合智慧牧场如发情检测等应用，并提供开源实现。

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [19] [ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT](https://arxiv.org/abs/2512.16140)
*Ze Yuan,Wenbin Li,Shusen Zhao*

Main category: cs.CV

TL;DR: 提出一种结合迭代重建与深度学习的双能CT重建框架：先用OPMT快速得到可分解的中间基材图，再用改进的ResDynUNet++细化，减少通道不平衡与界面伪影，实验表明优于对比方法。


<details>
  <summary>Details</summary>
Motivation: DSCT重建易受噪声、通道不平衡与界面伪影影响，传统迭代法稳健但慢、深度法效果好但易欠物理约束。需要一个既高效又可靠、兼顾物理先验与数据驱动能力的混合方案。

Method: 两阶段混合：1) 知识驱动阶段采用斜投影修正技术（OPMT）从投影数据快速重建得到基材图的中间解，实现快速且可分解的初值；2) 数据驱动阶段引入ResDynUNet++对中间解进行细化。该网络以UNet++为骨干，用残差动态卷积替换标准卷积，融合动态卷积的输入自适应特征提取与残差的稳定训练，以缓解通道不平衡并抑制界面大伪影。

Result: 在仿真体模与真实临床数据上进行大规模实验，所提方法在定量指标与视觉质量上均优于现有方法，得到更干净、准确的基材重建结果。

Conclusion: 知识驱动的OPMT与数据驱动的ResDynUNet++互补，可快速获得物理一致的初解并通过学习型细化提升质量，有效缓解DSCT中的通道不平衡与界面伪影问题，整体性能领先。

Abstract: We propose a hybrid reconstruction framework for dual-spectral CT (DSCT) that integrates iterative methods with deep learning models. The reconstruction process consists of two complementary components: a knowledge-driven module and a data-driven module. In the knowledge-driven phase, we employ the oblique projection modification technique (OPMT) to reconstruct an intermediate solution of the basis material images from the projection data. We select OPMT for this role because of its fast convergence, which allows it to rapidly generate an intermediate solution that successfully achieves basis material decomposition. Subsequently, in the data-driven phase, we introduce a novel neural network, ResDynUNet++, to refine this intermediate solution. The ResDynUNet++ is built upon a UNet++ backbone by replacing standard convolutions with residual dynamic convolution blocks, which combine the adaptive, input-specific feature extraction of dynamic convolution with the stable training of residual connections. This architecture is designed to address challenges like channel imbalance and near-interface large artifacts in DSCT, producing clean and accurate final solutions. Extensive experiments on both synthetic phantoms and real clinical datasets validate the efficacy and superior performance of the proposed method.

</details>


### [20] [SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation](https://arxiv.org/abs/2512.16143)
*Yueyang Hu,Haiyong Jiang,Haoxuan Song,Jun Xiao,Hao Pan*

Main category: cs.CV

TL;DR: 提出SegGraph：基于SAM分割的段图传播框架，用少量样本实现3D部件分割。通过建图编码段间重叠/邻接几何关系，结合GNN全局传播，并以视角加权将段特征映射到3D点，显著提升小部件与边界分割，较基线mIoU提升≥6.9%。


<details>
  <summary>Details</summary>
Motivation: 2D大模型（如SAM、CLIP）在低样本3D分割中有效，但如何高效把2D知识聚合到3D仍未解决。现有方法要么忽视3D几何结构导致欠分割、标签不一致，要么未充分利用SAM的高质量分组线索。

Method: 利用SAM在多视图产生的分割掩膜构建“段图”（类似图谱）：节点为图像段，边表示段间重叠/邻接以编码几何关系；每个节点自适应调制2D基模特征，经图神经网络在段图上进行全局传播以学习几何；为保证段内语义一致，将段特征以新颖的视角方向加权融合映射到3D点，抑制低质量段贡献。

Result: 在PartNet-E上超过所有对比基线，mIoU至少提升6.9%；在小部件与部件边界处表现尤为突出，显示出更强的几何理解能力。

Conclusion: 通过显式建模SAM段的几何关系并在段图上传播，SegGraph能稳健地把2D知识转移到3D，改善少样本3D部件分割的精度与一致性，特别利于小结构与边界。

Abstract: This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.

</details>


### [21] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: 提出C-DGPA，用双分支生成式提示自适应同时对齐边缘分布与条件分布，缓解VLM在UDA中的域偏移，达成多基准SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM提示微调在UDA中多聚焦边缘分布对齐，忽视条件分布差异，造成类别原型错位与语义判别性下降，需要方法同时对齐两种分布并避免对源域过依赖。

Method: 构建“类中心”双分支生成式提示自适应框架：1) 边缘对齐分支采用动态对抗训练以缩小源/目标边缘分布差异；2) 条件对齐分支引入类映射机制（CMM），标准化类语义提示、对齐类别条件分布并抑制源域依赖；两分支协同优化，将域知识融入提示学习，获得域不变且判别性强的表示。

Result: 在OfficeHome、Office31、VisDA-2017上进行大量实验，全面优于现有方法，达到新的SOTA。

Conclusion: 通过双重对齐与CMM的生成式提示自适应，能有效缓解VLM在UDA中的域差异与语义错配，提升跨域泛化与判别能力，并在多基准上验证其优越性。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [22] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 研究探讨端到端自动驾驶在昼夜光照差异（领域间差异）下的鲁棒性，提出用事件相机替代传统相机以减小跨域性能衰减，并在跨域场景中取得更稳定、更优的表现。


<details>
  <summary>Details</summary>
Motivation: 传统RGB/灰度相机在训练与部署环境不匹配时（如白天训练、夜晚部署）性能显著下降，影响端到端驾驶安全与泛化。需要一种对照明变化更不敏感、可跨域稳定工作的视觉传感方案。

Method: 将事件相机作为主要传感器用于端到端驾驶任务，比较其在日夜两种光照域内与跨域（如日→夜、夜→日）下的表现，并与灰度帧基线进行对比，量化“领域迁移惩罚（domain-shift penalty）”。

Result: 事件相机在不同光照条件下性能更一致；其领域迁移惩罚与灰度帧相当或更小；在跨域（训练与测试光照不一致）场景中，相较于灰度基线取得更高的基线性能。

Conclusion: 事件相机可作为端到端驾驶在昼夜光照差异下的有效替代传感器，无需额外调整即可缓解域间差异导致的性能退化，并提高跨域鲁棒性。

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [23] [Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation](https://arxiv.org/abs/2512.16199)
*Jerrin Bright,Zhibo Wang,Dmytro Klepachevskyi,Yuhao Chen,Sirisha Rambhatla,David Clausi,John Zelek*

Main category: cs.CV

TL;DR: Avatar4D提出可迁移的合成人体运动数据生成管线，能精细控制姿态、外观、相机与环境，无需人工标注；构建了体育领域的大规模合成数据集Syn2Sport，并验证对监督学习、零样本迁移与跨运动泛化的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有合成人体数据多聚焦日常动作、可控性差，难以覆盖体育等特定领域的复杂动作与多样场景，且真实标注成本高。需要一种可扩展、可控、无需标注且能与真实域良好迁移的方法与数据集。

Method: 提出Avatar4D管线：生成高保真4D人体运动序列，能独立控制身体姿态、人物外观、相机视角与环境；在体育场景中实例化，构建Syn2Sport，覆盖棒球、冰球等，渲染多外观与环境。以多种SOTA姿态估计模型在该数据上进行训练与评测，并做零样本迁移与跨运动泛化分析；同时以特征空间相似性评估合成数据与真实数据贴近度。

Result: 在Syn2Sport上，多种姿态估计模型表现良好：用于有监督训练、对真实数据的零样本迁移以及跨运动泛化均取得有效结果；特征空间对齐表明合成数据与真实数据分布接近。

Conclusion: Avatar4D可在无需领域真实数据和人工标注的情况下，生成可控、可扩展、可迁移的人体运动数据；Syn2Sport证明了其在体育等特定领域的价值，为运动理解等下游任务提供高质量数据来源。

Abstract: We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.

</details>


### [24] [Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation](https://arxiv.org/abs/2512.16201)
*Sarosij Bose,Ravi K. Rajendran,Biplob Debnath,Konstantinos Karydis,Amit K. Roy-Chowdhury,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 提出VALOR框架，通过强化学习的后对齐策略，先用文本奖励提升术语准确性，再用视觉对齐优化视觉投影模块，显著提升放射学报告生成的事实性与可视化落地能力。


<details>
  <summary>Details</summary>
Motivation: 当前Med-VLM在放射报告生成中仍存在跨模态对齐不足导致的幻觉与不准确，现有依赖大规模标注预训练、昂贵偏好数据或检索的方法难以根治该问题，需要一种直接提升视觉-语言对齐与临床事实性的训练策略。

Method: 提出VALOR，一个基于强化学习的后对齐框架，采用GRPO优化。两阶段训练：1) 文本奖励阶段，驱动模型使用临床精确术语、减少事实性错误；2) 视觉对齐阶段，仅对视觉投影模块进行对齐，使模型将注意力聚焦于与疾病发现相关的影像区域，实现视觉-语言的对齐与落地。

Result: 在多个基准上进行广泛实验，VALOR在事实准确性与视觉落地性方面显著优于现有最先进方法，得到明显性能提升。

Conclusion: 通过两阶段GRPO后对齐，VALOR有效缓解跨模态错配与幻觉，提升RRG的临床可靠性与可解释性，对自动化医疗工作流具有实际价值。

Abstract: Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.

</details>


### [25] [Open Ad-hoc Categorization with Contextualized Feature Learning](https://arxiv.org/abs/2512.16202)
*Zilin Wang,Sangwoo Mo,Stella X. Yu,Sima Behpour,Liu Ren*

Main category: cs.CV

TL;DR: 提出OAK模型：在冻结CLIP前端前添加少量可学习“上下文token”，联合优化图文对齐与无监督视觉聚类，以在开放情境下进行临时（ad‑hoc）类别的发现与扩展；在Stanford与Clevr‑4上达SOTA，并具可解释显著图。


<details>
  <summary>Details</summary>
Motivation: 现实任务常需为特定目标临时构造类别（如“适合野餐的物品”），而非固定的通用类目；给少量标注与大量未标注数据时，如何自动发现情境并扩展这些临时类是关键。现有方法要么依赖固定语义/监督（如CLIP的文本提示），要么只做视觉聚类（如GCD），难以同时捕获语义情境与视觉结构。作者认为临时与通用分类共享感知机制，可利用预训练多模态表征并轻量适配。

Method: 提出OAK：在冻结的CLIP输入端加入少量可学习的“context tokens”，作为情境提示；端到端训练仅这些token（或极少量参数），联合两个目标：(1) CLIP图文对齐损失，促使模型进行语义扩展与上下文建模；(2) GCD式视觉聚类损失，利用未标注数据进行结构化聚类；从少量标注样本出发，在未标注集中扩展并发现新概念/类别。

Result: 在Stanford与Clevr‑4多种情境分类任务上取得SOTA，包括在Stanford Mood任务上新类别准确率87.4%，较CLIP与GCD超过50%绝对提升（按文中描述）。同时产生可解释显著图：动作看手、情绪看脸、地点看背景。

Conclusion: 通过在CLIP前加入可学习上下文并联合语义对齐与视觉聚类，OAK能在开放临时分类场景中同时进行概念发现与准确分类，具备可解释性与良好泛化，为自适应、可扩展的AI场景理解提供简洁有效的途径。

Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.

</details>


### [26] [Enhanced 3D Shape Analysis via Information Geometry](https://arxiv.org/abs/2512.16213)
*Amit Vishwakarma,K. S. Subrahamanian Moosath*

Main category: cs.CV

TL;DR: 提出将点云表示为高斯混合模型，并在统计流形上用改进的对称KL散度（MSKL）作为形状比较度量，具有上下界保证与数值稳定性，实验优于Hausdorff/Chamfer及现有KL近似。


<details>
  <summary>Details</summary>
Motivation: 传统几何距离（Hausdorff/Chamfer）对离群点敏感、难以反映全局统计结构；现有对GMM的KL近似可能无界或数值不稳定。需要一种既能刻画形状全局统计差异，又具稳定、可比较性的度量。

Method: 将3D点云拟合为GMM，并证明GMM集合构成统计流形；在此框架下提出改进的对称KL散度MSKL，给出严格的上下界保证以确保各GMM之间比较的数值稳定性；在两类数据集上进行评估。

Result: MSKL在人体姿态区分（MPI-FAUST）与动物形状对比（G-PCD）任务中产生稳定、单调随几何变化而变化的值，表现优于Hausdorff/Chamfer以及既有KL近似（鲁棒性、可解释性与稳定性更好）。

Conclusion: 信息几何视角下的GMM统计流形建模结合MSKL度量能有效、稳定地比较3D点云形状，克服传统几何度量与KL近似的缺陷，适用于需要鲁棒形状比较的应用场景。

Abstract: Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.

</details>


### [27] [Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models](https://arxiv.org/abs/2512.16219)
*Zhihao Zhang,Xuejun Yang,Weihua Liu,Mouquan Shen*

Main category: cs.CV

TL;DR: 提出一种学习高质量初始噪声的框架，用于单视图扩散式新视角合成，通过欧拉离散反演注入语义并用EDN将随机噪声映射为高质量噪声，显著提升SV3D、MV-Adapter等模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在NVS中表现强，但生成质量对初始噪声敏感；缺乏能学习并生成“高质量噪声”的系统方法，从而限制了单视图到多视图生成的稳定性与细节一致性。

Method: 1) 设计离散化欧拉反演，将图像语义注入随机高斯噪声，构建随机噪声与高质量噪声的成对数据；2) 提出编码器-解码器网络EDN，直接学习从随机噪声到高质量噪声的映射；3) 将EDN作为可插拔模块接入现有NVS扩散模型（如SV3D、MV-Adapter）。

Result: 在多数据集上，接入EDN后，多种NVS模型的评测指标显著提升；实验表明EDN具有良好的通用性与即插即用特性。

Conclusion: 学习并使用高质量初始噪声能系统性提升单视图NVS的生成质量；通过欧拉反演构造监督与EDN变换实现简单有效，且对现有模型兼容性强。

Abstract: Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.

</details>


### [28] [Image Compression Using Singular Value Decomposition](https://arxiv.org/abs/2512.16226)
*Justin Jiang*

Main category: cs.CV

TL;DR: 研究评估用SVD低秩近似做图像压缩：外观常可接受，但压缩率和效率普遍不如JPEG/JPEG2000/WEBP；在低误差容忍下甚至比原图更大。


<details>
  <summary>Details</summary>
Motivation: 互联网图像占比高，存储与带宽压力大；希望用线性代数（SVD低秩近似）探索一种通用、可分析的压缩方式，并量化其在不同图像（灰度与多通道）上的表现。

Method: 对图像矩阵做SVD并截断至秩k，产生低秩重建；以相对Frobenius范数误差衡量失真，并以压缩比衡量文件大小收益；在灰度与彩色图像上系统比较，并与JPEG/JPEG2000/WEBP在相似误差水平下对比。

Result: 低秩近似在视觉上常与原图相似；但在相同误差水平下，其压缩比持续劣于主流编解码器；当误差容忍很低（高保真）时，SVD表示的参数量可能超过原始图像存储量。

Conclusion: SVD低秩压缩虽有直观与理论上的简洁性与一定视觉质量，但在实际压缩任务中不具竞争力，特别是在高保真场景；现有工业标准（JPEG家族、WEBP）在压缩效率与实用性上明显更优。

Abstract: Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.

</details>


### [29] [ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation](https://arxiv.org/abs/2512.16234)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Hesheng Wang,Ajmal Mian*

Main category: cs.CV

TL;DR: ARMFlow提出一种基于MeanFlow的自回归3D人类“反应”动作生成框架，同时兼顾高保真、实时推理与在线自回归稳定性；并给出离线变体ReMFlow，速度与精度均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D人类互动/反应生成方法难以同时满足三目标：动作高保真、实时低延迟，以及在线自回归时的误差累积鲁棒性。在线场景还需要对演员与反应者的时序依赖进行高效建模。

Method: 提出ARMFlow：以MeanFlow为基础的自回归框架，包含因果上下文编码器和MLP速度预测器；训练阶段引入Bootstrap Contextual Encoding（BSCE），用模型生成的历史而非GT历史进行编码，缓解自回归误差漂移；同时提出离线版ReMFlow，作为非自回归/批量推理的高速SOTA。在线方面通过全局上下文编码器增强语义对齐、单步推理实现高精度低延迟，并以BSCE减小误差累积。

Result: 在InterHuman与InterX数据集上，单步在线生成较现有在线方法FID提升超过40%；离线ReMFlow在精度与推理速度上达到或优于SOTA；在仅使用部分序列条件的情况下，在线性能仍可接近离线SOTA。

Conclusion: ARMFlow有效解决在线3D反应动作生成的三难题：保真度、实时性、自回归稳定性；BSCE显著降低误差积累；ReMFlow在离线场景达成更快更准的SOTA。方法在标准数据集上验证了显著优势，具有实际在线部署潜力。

Abstract: 3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.

</details>


### [30] [AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection](https://arxiv.org/abs/2512.16235)
*Satya Narayana Panda,Vaishnavi Kukkala,Spandana Iyer*

Main category: cs.CV

TL;DR: 提出一种将家族史与皮肤影像结合的可解释多模态AI框架，专家验证显示纳入家族史可提升对遗传相关皮肤病的诊断准确性，拟开展前瞻性多中心临床试验以进一步验证与落地。


<details>
  <summary>Details</summary>
Motivation: 皮肤病影响人群庞大而专科资源有限；家族史对易感性与疗效有重要作用却常被低估。作者希望通过把家族史结构化纳入AI诊断流程，提升准确性、早筛能力与临床可用性，并为未来临床试验和真实世界应用铺路。

Method: 构建多模态框架：以可解释的卷积神经网络处理皮肤影像，并与包含遗传风险因素的临床决策树相集成；整合结构化临床数据与详细家族史；与临床专家进行对照评估，并规划在多样化医疗环境中的前瞻性临床试验进行外部验证与实施评估。

Result: 在专家评估与临床期望对照中，纳入家族史信息后，AI系统对黑色素瘤、银屑病、特应性皮炎等遗传相关皮肤病的诊断表现提升；显示出潜在的早期发现与个体化建议价值。正式的前瞻性多中心试验尚未完成，属于计划阶段。

Conclusion: 多模态、可解释的AI将家族史与影像联合可提升皮肤病诊断并有望改善早筛与个体化管理；框架具备融入临床流程的可行性，但仍需前瞻性临床试验证实有效性与泛化性。

Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.

</details>


### [31] [Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models](https://arxiv.org/abs/2512.16243)
*Qi Zhang,Yunfei Gong,Zhidan Xie,Zhizi Wang,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 提出两种基于“多视角融合模型排序”的半监督人群计数框架：利用视角数量单调性对预测值或不确定性进行排序约束，缓解多视角数据稀缺问题，并优于现有半监督方法。


<details>
  <summary>Details</summary>
Motivation: 多视角计数能缓解大场景严重遮挡，但真实多视角数据采集标注代价高，现有数据集帧数与场景少。需要一种在有限标注下仍能有效训练多视角模型的策略。

Method: 构建一组可接收不同视角数输入的多视角融合模型，并引入“排序约束”：1) 预测值单调性（vanilla）：少视角预测≤多视角预测；2) 不确定性单调性：在误差指导下校准估计的不确定性，使多视角输入的不确定性≤少视角。将这些约束作为半监督损失加入训练，用未标注数据施加一致性/单调性约束，少量标注用于监督误差。

Result: 在多视角计数任务上，相比其他半监督计数方法，加入模型排序约束后取得更优性能（更低误差/更稳健），表明多视角数目带来的信息增益可通过排序约束有效利用。

Conclusion: 利用视角数量的先验单调性对预测与不确定性进行排序约束，可在标注数据有限时显著提升多视角计数。方法通用、可与不同融合架构结合，并在实验中优于现有半监督方法。

Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.

</details>


### [32] [Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning](https://arxiv.org/abs/2512.16266)
*Paloma Casteleiro Costa,Parnian Ghapandar Kashani,Xuhui Liu,Alexander Chen,Ary Portes,Julien Bec,Laura Marcu,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出FLIM_PSR_k：用cGAN实现多通道像素超分辨，从低采样FLIM重建高分辨，达成k=5放大、显著提速并提升SNR与图像质量。


<details>
  <summary>Details</summary>
Motivation: FLIM临床转化受限于像素驻留时间长、SNR低与分辨率-速度权衡；需要在不更换硬件的前提下加速采集、提升分辨率与成像质量。

Method: 构建深度学习多通道PSR框架FLIM_PSR_k，以条件GAN训练，从大像素（降采样）FLIM数据重建高分辨率多通道寿命图；与扩散模型相比侧重更稳健的重建与更短推理时间，适配低NA与微型化平台。

Result: 在盲测的患者来源肿瘤组织上，稳定实现k=5超分辨，输出空间带宽积提升25倍，揭示在低分辨输入中丢失的细微结构；多项图像质量指标统计学显著提升，同时缓解自发荧光FLIM的SNR不足。

Conclusion: FLIM_PSR_k提高FLIM的有效空间分辨率并加速采集，提供硬件灵活、接近实时的重建能力，促进FLIM向更快、更高分辨率与可临床转化的方向发展。

Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.

</details>


### [33] [TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering](https://arxiv.org/abs/2512.16270)
*Rui Gui,Yang Wan,Haochen Han,Dongxing Mao,Fangming Liu,Min Li,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 该论文提出并发布一个专注于图像中文本编辑能力的评测基准TextEditBench，并引入“语义期望（SE）”维度，以系统评估模型在保持语义一致性、上下文连贯与跨模态对齐上的推理能力；实验显示现有方法能完成简单指令，但在上下文推理、物理一致性与版式融合方面显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散与多模态模型虽在文本渲染上进展迅速，但图像内文本编辑仍缺乏系统评测与专门任务设定；该任务不仅要生成可读文字，还需兼顾语义、几何与上下文一致性与物理可行性，现有评测无法全面刻画这些能力缺口。

Method: 构建TextEditBench：针对图像中文本区域的多维度编辑评测，覆盖超越像素层面的“推理型”编辑场景；提出新的评价维度“Semantic Expectation (SE)”，量化模型在语义一致性、上下文连贯与跨模态对齐方面的推理保持能力；在多种SOTA编辑系统上进行系统实验与比较分析。

Result: 实验表明：当任务简单且直接时，当前模型能基本遵循编辑指令；但在需要上下文依赖推理、物理合理性约束以及考虑版式/布局的融合时，性能显著下降，难以满足SE维度的要求。

Conclusion: TextEditBench为文本引导的图像编辑与多模态推理提供了新的、更严格的测试场域；通过强调语义期望与推理型编辑，该基准揭示了现有系统的关键短板，呼吁未来在语义保持、物理一致性与版面感知上的方法改进。

Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.

</details>


### [34] [GFLAN: Generative Functional Layouts](https://arxiv.org/abs/2512.16275)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: 论文提出GFLAN：将户型生成显式分解为“拓扑规划+几何实现”的两阶段生成框架，先用双编码卷积网络依次布置房间质心，再以Transformer增强GNN在异质图上联合回归房间边界。


<details>
  <summary>Details</summary>
Motivation: 现有端到端或像素/墙体追踪式深度生成方法难以体现建筑推理：拓扑关系优先于几何细节、功能约束在邻接网络中的传播、由局部连通性驱动的流线涌现等。因此需要一个能先抓拓扑后落几何的统一计算框架。

Method: 两阶段分解。输入外部边界与前门位置。A阶段：专用CNN，双编码器结构（将不变空间上下文与随布局演化的状态分离），以离散概率图在可行域内顺序分配各房间质心。B阶段：构建连接房间节点与边界顶点的异质图，使用加入Transformer的GNN，联合回归各房间边界，实现从拓扑到几何的映射。

Result: 相较直接像素到像素或墙体追踪式生成，框架能更好地捕捉拓扑优先与功能约束传播，生成具有合理邻接与流线的户型；（摘要未给定具体数值指标，但表明在SOTA基础上有改进与更强建筑合理性）。

Conclusion: 显式因子化为拓扑与几何两阶段、并结合双编码CNN与Transformer增强GNN的生成范式，能更好地满足建筑设计中的功能与拓扑需求，为自动化户型生成提供了更具可解释性的统一方法。

Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.

</details>


### [35] [MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.16294)
*Amna Amir,Erchan Aptoula*

Main category: cs.CV

TL;DR: 提出MACL方法，通过多标签自适应对比学习解决遥感多标签检索中的语义重叠、类别不均衡与共现复杂问题，在三个基准集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 多标签遥感图像检索面临三难题：类别语义重叠导致特征混淆；标签分布极度不均衡致使长尾类别学习不足；类别间复杂共现模式使相似度学习困难。现有对比学习和多标签方法无法兼顾这些挑战，需有针对性地在采样、损失权重与温度等方面做自适应。

Method: 提出Multi-Label Adaptive Contrastive Learning（MACL）：1）标签感知采样（label-aware sampling）根据标签共现与稀有度构造正负对，提升难例与长尾类别的参与度；2）频率敏感加权（frequency-sensitive weighting）依据类别出现频次为样本对赋权，缓解多数类主导；3）动态温度调节（dynamic-temperature scaling）根据对/样本不确定性自适应调整对比损失温度，稳定区分语义重叠类；整体以对比学习框架学习平衡的多标签表征，用于检索。

Result: 在DLRSD、ML-AID、WHDLD三数据集上，MACL较对比学习基线持续提升，多指标下实现更稳健的检索表现，显著减轻语义不均衡影响。

Conclusion: MACL通过在采样、权重和温度上的自适应机制，使多标签对比学习更关注稀有类别与复杂共现，学得更平衡的表征，从而提升大规模遥感库的检索可靠性；代码与模型将开源。

Abstract: Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.

</details>


### [36] [PixelArena: A benchmark for Pixel-Precision Visual Intelligence](https://arxiv.org/abs/2512.16303)
*Feng Liang,Sizhe Cheng,Chenqi Yi*

Main category: cs.CV

TL;DR: PixelArena提出用语义分割作为新基准，以像素级精度客观评测具备图像输出能力的多模态大模型的细粒度生成；实验发现Gemini 3 Pro Image在零样本下能高保真生成语义掩膜，显示出新兴的生成与泛化能力，并给出对比、失败案例与未来研究启示。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成基准多偏重美学与主观评估，难以衡量模型在细粒度、可解释、可量化的生成智能；需要一个能客观评测多模态模型“生成+理解+对齐”的像素级任务。

Method: 提出PixelArena：以语义分割为载体评测图像输出型多模态LLM的像素级生成能力；在零样本设定下让模型直接生成语义掩膜；对主流模型（重点是Gemini 3 Pro Image）进行定量与定性比较，并分析失败模式。

Result: Gemini 3 Pro Image在零样本条件下生成的语义掩膜具有高保真度，超过或显著优于其他对比模型；定量指标与可视化均显示其细粒度结构与边界一致性更好，同时给出若干失败情形。

Conclusion: 语义分割可作为评测图像生成模型细粒度智能的客观基准；Gemini 3 Pro Image展现出前所未有的零样本像素级生成与泛化能力；研究对多模态推理、可解释性与基准设计具有启示，未来应拓展任务、完善指标并系统化分析失败模式。

Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.

</details>


### [37] [LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation](https://arxiv.org/abs/2512.16313)
*Haiyu Zhao,Yiwen Shan,Yuanbiao Gou,Xi Peng*

Main category: cs.CV

TL;DR: 提出轻量级一体化视频复原网络LaverNet（362K参数），通过仅传播“与退化无关”的时域特征来避免时变退化干扰，实现小模型却具有与大模型相当甚至更优的跨基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有一体化视频复原在时变退化下有两大痛点：1）退化主导时序建模，模型更关注伪影而非真实内容；2）依赖超大模型掩盖问题本质，参数开销高、部署不便。

Method: 设计轻量级网络LaverNet，并提出新的时序传播机制：在帧间仅选择性传递“退化无关”特征，抑制退化因素在时间维的累积与放大，从而稳定时序建模与内容对齐。

Result: 在多个基准上，以不足现有方法1%的参数量，获得可比甚至更优的复原效果。

Conclusion: 通过退化无关特征传播与极简模型设计，小参数量也能实现强大的一体化视频复原，缓解时变退化对时序建模的干扰，并具备更好的效率与部署友好性。

Abstract: Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.

</details>


### [38] [Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs](https://arxiv.org/abs/2512.16314)
*Huayu Huang,Chen Chen,Banglei Guan,Ze Tan,Yang Shang,Zhang Li,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于岭估计的多源传感器融合定位方法，融合序列影像的丰富场景信息与激光测距的高精度，在观测受限（远距离、小交会角、大倾角）导致多重共线时，通过岭回归提升稳健性与精度，实验优于单一信息的地面定位。


<details>
  <summary>Details</summary>
Motivation: 传统最小二乘在远距离、小交会角、大倾角等受限观测条件下，设计矩阵列向量高度相关，产生多重共线与病态问题，导致解不稳定、鲁棒性差；需要一种在信息受限时仍能稳定、精确定位的融合方法。

Method: 将无人机搭载的序列影像与激光测距数据进行融合建模，在定位的线性化观测方程中引入岭估计（在正规方程中加入kI惩罚项），缓解设计矩阵列向量间的多重共线，稳定求解目标位置参数。

Result: 与仅基于单一信息（如仅影像或仅地面算法）的定位方法相比，所提方法在实验中取得更高的定位精度；在受限观测条件下，岭估计显著提升了解的稳定性与鲁棒性。

Conclusion: 基于岭估计的影像-激光测距融合定位能在多重共线严重、观测受限场景下有效提高定位精度与鲁棒性，优于单源信息方法，适用于远距离、小交会角和大倾角等困难条件。

Abstract: Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.

</details>


### [39] [QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing](https://arxiv.org/abs/2512.16325)
*Nan Zhou,Zuxin Li,Fanhang Man,Xuecheng Chen,Susu Xu,Fan Dang,Chaopeng Hong,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出QUIDS：在预算约束下，通过质量感知与激励驱动的多智能体调度，实现车载众包的高覆盖与高可靠性；引入ASQ指标并提出互助式信念感知调度算法，实证较基线显著提升QoI并降低重建误差。


<details>
  <summary>Details</summary>
Motivation: 非专用车载众包(NVMCS)难以同时保证感知覆盖与可靠性，且车辆参与具有动态性与不确定性，现有方法难以在预算内兼顾QoI与激励分配。

Method: 1) 定义聚合感知质量(ASQ)作为QoI度量，将覆盖与可靠性统一建模；2) 设计质量知情的激励机制与预算约束；3) 提出互助式、信念感知的车辆调度算法，基于不确定性估计感知可靠性并进行激励与任务分配；4) 以真实都市NVMCS数据进行评估。

Result: 相较无调度提升ASQ 38%，相较SOTA提升10%；在多种算法下重建地图误差降低39%–74%。

Conclusion: 通过联动覆盖与可靠性的质量知情激励与调度，QUIDS在无需专用基础设施下实现低成本高质量的城市监测，适用于交通与环境等智慧城市场景。

Abstract: This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.

</details>


### [40] [Collaborative Edge-to-Server Inference for Vision-Language Models](https://arxiv.org/abs/2512.16349)
*Soochang Song,Yongjune Kim*

Main category: cs.CV

TL;DR: 提出一种边缘-服务器协同的VLM推理框架：先用全局图像在服务器上粗推理并基于注意力选ROI与token最小熵置信度判定是否重传，再请求边缘上传高细节局部图像进行二次推理；在不显著损失精度下显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 典型VLM部署中，边缘设备需上传整图或下采样图像；为适配视觉编码器分辨率而缩放会丢失细节，导致精度下降，同时直接上传高分辨率图像通信代价太高。需要一种在通信受限条件下仍保持精度的方法。

Method: 设计两阶段协同推理：1) 服务器对下采样的全局图像进行VLM推理，利用模型内部注意力定位ROI；计算输出token的最小熵作为置信度，如果高于阈值则触发重传请求；2) 客户端仅上传ROI的高细节局部图像，服务器将全局与局部图像联合推理以细化答案。通过选择性重传，仅在必要时发送关键视觉内容。

Result: 在多种VLM架构上验证，该框架在基本保持原有推理准确度的同时，显著降低了边端到服务器的通信成本。

Conclusion: 基于注意力选ROI与最小熵置信度驱动的选择性重传，可有效在边缘-服务器协同场景中兼顾通信效率与VLM精度；该思路具通用性，可扩展到不同VLM。

Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.

</details>


### [41] [GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction](https://arxiv.org/abs/2512.16357)
*Tao Hu,Weiyu Zhou,Yanjie Tu,Peng Wu,Wei Dong,Qingsen Yan,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出GMODiff：用增益图驱动、一步扩散的多曝光HDR重建框架，兼顾速度、动态范围表示与减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有将LDM用于HDR的方案面临三难：8位潜空间压缩限制动态范围、多步去噪推理慢、生成式带来内容幻觉。需要一种既能高效推理又能准确表达高动态范围、并抑制幻觉的方法。

Method: 将HDR重建重构为条件引导的增益图（Gain Map, GM）估计：GM在与LDR同位深下编码扩展动态范围。采用“信息化初始化”的一步扩散：以回归模型的GM初值而非纯噪声作为去噪起点，实现单步生成高质量GM。并利用回归先验双重引导：在扩散去噪阶段与潜空间解码阶段共同约束LDM，提升结构保真、抑制幻觉。

Result: 在多组实验中优于多种SOTA基线，且较以往基于LDM的方法快约100倍，同时保持高感知质量与结构准确性。

Conclusion: 通过增益图表征与回归先验的一步扩散框架，GMODiff有效缓解动态范围受限、推理耗时及内容幻觉问题，实现快速且高保真的多曝光HDR重建。

Abstract: Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.

</details>


### [42] [EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation](https://arxiv.org/abs/2512.16360)
*Haotian Ling,Zequn Chen,Qiuying Chen,Donglin Di,Yongjia Ma,Hao Li,Chen Wei,Zhulin Tao,Xun Yang*

Main category: cs.CV

TL;DR: 提出EverybodyDance框架，围绕身份匹配图与掩码查询注意力，面向多角色姿态驱动动画中的身份对应（IC）正确性进行建模与优化，显著提升多角色场景下的身份一致与画面质量。


<details>
  <summary>Details</summary>
Motivation: 单人物姿态驱动动画已较成熟，但多人物场景尤其位置交换时，难以确保参考与生成帧之间的身份一一对应，现有方法易身份混淆，缺乏评测基准。

Method: 1) 构建“身份匹配图”(IMG)：将参考帧与生成帧中的各角色作为二部图两侧节点，边权由提出的“掩码-查询注意力”(MQA)计算，衡量任意角色对的亲和度；2) 将IC正确性形式化为图结构度量，并在训练中直接优化；3) 设计配套策略：身份嵌入引导、跨尺度匹配、多类别预分类采样，使匹配稳健；4) 构建IC评测基准用于客观评估。

Result: 在多角色身份对应与视觉保真度上，相较现有SOTA显著提升；在新建的IC评测上取得领先分数，并在包含位置交换等复杂场景中保持稳定的身份一致。

Conclusion: 把身份对应问题转化为可优化的图匹配任务，并辅以注意力与训练策略，可系统性解决多角色动画中的身份混淆；EverybodyDance提供了方法与评测基准，推进多角色姿态驱动生成发展。

Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.

</details>


### [43] [Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models](https://arxiv.org/abs/2512.16371)
*Mariam Hassan,Bastien Van Delft,Wuyang Li,Alexandre Alahi*

Main category: cs.CV

TL;DR: 他们把文本生成视频拆解为三步：LLM重写初始场景、T2I生成高质量首帧、视频模型只负责动画与时序。这样能更好遵循复杂指令、提升指标，并加速采样。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型常在复杂场景与逻辑时序上失败，很多问题源自首帧语义与逻辑不一致，导致后续运动与剧情崩溃。作者希望通过结构化流程先把“场景构建”与“时间演化”解耦，减少歧义并提升稳健性与效率。

Method: 提出Factorized Video Generation（FVG）：1）Reasoning：用LLM将视频提示词改写为只描述初始场景，消除时间歧义；2）Composition：用T2I从改写的提示生成构图正确、细节充分的锚点首帧；3）Temporal Synthesis：微调后的视频模型以该锚点为条件，将全部容量用于动画与指令跟随，并与原提示对齐。

Result: 在T2V CompBench上达成新的SOTA，在VBench2上对多种模型都有显著提升；此外使用可视锚定后，采样步数可减少约70%而性能不降，实现明显加速。

Conclusion: 将T2V任务因子化，把语义首帧的正确性与后续时序生成解耦，可以提升复杂指令可控性、稳健性与效率，为更实用的视频生成提供一条简单可行的路径。

Abstract: State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis

</details>


### [44] [Adaptive Frequency Domain Alignment Network for Medical image segmentation](https://arxiv.org/abs/2512.16393)
*Zhanwei Li,Liang Li,Jiawan Zhang*

Main category: cs.CV

TL;DR: 提出AFDAN，一种在频域对齐的跨域自适应分割框架，通过对抗域学习、源-目标频率融合与时空-频率联合提升医学图像分割，IoU在VITILIGO2025达90.9%、在DRIVE达82.6%，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学图像高质量标注稀缺、标注成本高，导致监督分割难以在新域保持性能。需要一种能在数据稀缺与域移位下进行有效知识迁移的方法，尤其利用频域信息提升跨域鲁棒性。

Method: 提出自适应频域对齐网络AFDAN，含三模块：1) 对抗域学习模块：通过对抗训练将源域特征迁移到目标域，实现域不变表示；2) 源-目标频率融合模块：在频域混合两域的频率表示，增强目标域对高/低频结构的适配；3) 空间-频率整合模块：将频域特征与空间域特征融合，提升跨域分割的细节与全局一致性。

Result: 在新建的VITILIGO2025白癜风分割数据集上获得IoU 90.9%；在视网膜血管分割基准DRIVE上获得IoU 82.6%；均超过现有SOTA方法。

Conclusion: 频域对齐与融合结合对抗学习，能有效缓解医学图像分割中的数据稀缺与域移位问题；AFDAN在多个数据集上取得领先性能，证明该框架具有通用且有效的跨域分割能力。

Abstract: High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.

</details>


### [45] [Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture](https://arxiv.org/abs/2512.16397)
*Haodi He,Jihun Yu,Ronald Fedkiw*

Main category: cs.CV

TL;DR: 提出一种基于高斯点渲(gaussian splatting)的统一人脸重建与纹理化方法，从少量未标定图像重建中性人脸几何，并将高斯投射到纹理空间作为视角相关神经纹理，以无缝融入传统图形管线。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF等隐式表示难以受约束、与图形管线耦合差，且高质量人脸重建通常依赖长视频与一致光照。需要一种更显式、可受结构先验与语义约束的方法，从少量、光照各异的图像中得到可编辑的几何与可用的高质量纹理，并与标准渲染流程兼容。

Method: - 采用高斯点渲作为3D表示，结合人脸语义分割对齐各部位，从仅11张未标定图像重建中性姿态。
- 将高斯软约束到三角网格表面，获得结构化的高斯场；利用该结构指导迭代微扰，提升底层三角网格几何精度。
- 基于准确几何，将高斯映射到UV纹理空间，作为视角相关的神经纹理，无需改动其他资产或渲染管线。
- 使用可重光(relightable)高斯模型分离纹理与光照，得到去光照的高分辨率反照率贴图；支持不同光照、分辨率的杂合训练以增强正则化。
- 展示文本驱动的资产创建流程应用。

Result: 在仅11张不一致光照的图像下，重建出中性人脸的高质量三角网格与可直接用于标准管线的高分辨率反照率贴图；通过将高斯转到纹理空间，实现对任意资产的高保真视角相关渲染且无需修改几何、光照或渲染器；展示文本驱动资产生成的有效性。

Conclusion: 显式的高斯点渲结合语义与表面约束，可从少量、非一致数据中重建精确几何，并将神经表示无缝接入传统图形管线；通过可重光建模获得干净的纹理与跨资产适用的神经纹理，为高保真、可编辑的角色/资产制作提供实用方案。

Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.

</details>


### [46] [BrepLLM: Native Boundary Representation Understanding with Large Language Models](https://arxiv.org/abs/2512.16413)
*Liyuan Deng,Hao Guo,Yunpeng Bai,Yongkang Dai,Huaxi Huang,Yilei Shi*

Main category: cs.CV

TL;DR: 提出BrepLLM，使LLM能直接解析与推理原始Brep（边界表示）数据，通过两阶段训练（跨模态对齐预训练 + 多阶段LLM微调），在3D分类与描述任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于token序列的LLM难以直接处理包含复杂几何与拓扑信息的3D Brep模型，存在结构化3D几何与自然语言之间的模态鸿沟，需要一种能将Brep高保真几何/拓扑表示对齐到语言空间、让LLM进行理解与推理的框架。

Method: 两阶段框架：1) 跨模态对齐预训练：自适应UV采样将Brep转为含几何与拓扑的图；设计层次化BrepEncoder，从面/边与拓扑中提取特征，输出全局token和节点序列；用对比学习将全局token与冻结的CLIP文本编码（ViT-L/14）对齐。2) 多阶段LLM微调：将预训练BrepEncoder接入LLM，对节点序列做三步渐进对齐：(i) 通过MLP语义映射借助2D-LLM先验做Brep→2D对齐；(ii) 微调LLM；(iii) 引入Mixture-of-Query Experts以增强几何多样性建模。并构建Brep2Text数据集（269,444个Brep-文本QA对）。

Result: 在3D目标分类与图像描述（文本生成）任务上达到SOTA表现，证明该框架能有效将Brep几何/拓扑信息与语言对齐并提升多模态理解能力。

Conclusion: BrepLLM首次打通LLM与原始Brep数据的桥梁，通过对齐预训练与渐进式微调，实现对3D几何与拓扑的语言化解析与推理，配合大规模Brep2Text数据集取得SOTA，为结构化3D与语言融合提供了通用方案。

Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.

</details>


### [47] [CountZES: Counting via Zero-Shot Exemplar Selection](https://arxiv.org/abs/2512.16415)
*Muhammad Ibraheem Siddiqui,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出CountZES：一种无需训练的零样本目标计数框架，通过三阶段互补的样本（exemplar）选择，提升未见类的计数准确性，并在多域数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 零样本计数仅由类名引导，现有方法要么依赖开放词汇检测器但常产生多实例候选、要么随机裁块难以对应单个实例，导致样本不准、计数偏差与泛化不足。

Method: 提出CountZES的三阶段零样本样本选择流程：1) DAE（Detection-Anchored Exemplar）基于开放词汇检测结果进行细化，筛出单实例、精确的样本；2) DGE（Density-Guided Exemplar）利用密度估计的自监督信号，选择统计一致、语义紧致的样本，保证与全局计数一致性；3) FCE（Feature-Consensus Exemplar）在特征空间聚类，强化视觉一致性与代表性。三者协同获得多样且互补的样本集以驱动计数。

Result: 在多样数据集（自然、航拍、医疗等）上，相比现有ZOC方法取得更优计数性能，并表现出良好的跨域泛化。

Conclusion: 通过分阶段的样本自动发现与筛选，无需额外训练即可在零样本目标计数中兼顾文本对齐、计数一致性与特征代表性，显著提升精度与泛化。

Abstract: Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.

</details>


### [48] [Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt](https://arxiv.org/abs/2512.16443)
*Shangxun Li,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出一种无需训练的文本嵌入几何精炼方法，在多场景生成中缓解语义纠缠，显著提升主体一致性与文本对齐。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在多张连贯图像（如视觉叙事）中难以保持同一主体的一致性。现有方法需微调或图像条件，计算昂贵且需逐主体优化；训练-free方案（如1Prompt1Story）易产生语义泄漏导致文本错配，亟需简单高效的替代。

Method: 将问题视作文本嵌入空间的几何纠偏：在拼接长提示的场景下，对各帧/子句对应的token嵌入进行精炼与重标定，通过抑制非目标语义投影（消除跨帧纠缠）来减少语义泄漏，无需训练与额外图像条件。

Result: 在多种数据设置与评价指标上，相较1Prompt1Story和其他基线，显著提升主体一致性（跨帧外观/身份保持）与文本对齐（每帧语义准确度），实验广泛且一致。

Conclusion: 以几何视角精炼文本嵌入可在不训练、不增加推理成本太多的情况下，有效缓解语义纠缠并提升多图生成的连贯性，是现有训练-free策略的简洁有效替代。

Abstract: Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.

</details>


### [49] [Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach](https://arxiv.org/abs/2512.16456)
*Masashi Hatano,Saptarshi Sinha,Jacob Chalk,Wei-Hong Li,Hideo Saito,Dima Damen*

Main category: cs.CV

TL;DR: 论文构建了首个“注视启动-到达抓取”人类动作子集，并在其上微调文本条件扩散模型，用目标位姿/位置条件生成带注视启动与到达动作的人体运动；提出Prime Success指标并在HD-EPIC上达成60%注视成功、89%到达成功。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成多关注行走、抓取等，但缺少对“远距离注视目标（gaze priming）→接近→到达/抓取”的自然行为链的建模与评估数据与指标，难以生成更贴近人类的趋近与操作动作。

Method: 1) 从HD-EPIC、MoGaze、HOT3D、ADT、GIMO五个公开集筛选并清洗出2.37万条包含注视启动并到达目标位置的序列；2) 先在文本条件下预训练扩散式动作生成模型；3) 再以目标位姿或位置作为条件进行微调；4) 设计评估，包括Reach Success与新增的Prime Success衡量注视启动与到达。

Result: 在最大的数据集HD-EPIC上，使用目标位置条件时，模型达到Prime Success 60%，Reach Success 89%。

Conclusion: 构建的数据与指标使模型能生成含“注视→接近→到达”的更自然动作；所提出方法在大规模数据上取得较高到达率与中等注视成功率，为更逼真的人类运动生成与人机交互奠定基础。

Abstract: Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.

</details>


### [50] [SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning](https://arxiv.org/abs/2512.16461)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: SNOW是一个无需训练、与骨干无关的统一4D场景理解框架，将VLM语义与点云几何和时间一致性融合，生成可查询的4D场景图并作为具身推理与机器人导航的结构化先验，达到多项SOTA。


<details>
  <summary>Details</summary>
Motivation: VLM具开放世界语义却缺少3D几何与时间动态的落地；传统几何感知虽擅于结构与运动，但语义稀疏。为实现机器人在动态环境中的可靠导航与交互，需要同时具备开放语义、几何结构与时序一致性的统一表示。

Method: 输入同步的RGB与3D点云：1) 以HDBSCAN进行点云聚类得到目标级提议；2) 用SAM2在图像中执行受提议引导的分割；3) 提出STEP编码，将每个分割区域的语义、几何与时间属性令牌化为多模态token；4) 将token增量融合到4D场景图(4DSG)中，形成统一世界模型；5) 轻量级SLAM后端为STEP token提供全局坐标锚定，保证跨时刻空间对齐；6) 下游通过可查询的4DSG实现空间落地的推理。

Result: 在多种基准上实现精确的4D场景理解与空间落地推理，达到多项设置下的最新最优（SOTA）表现，显示4D结构化先验在具身推理与机器人中的价值。

Conclusion: SNOW无训练、骨干无关地融合VLM语义与几何-时间信息，通过STEP与4DSG提供统一、可查询的4D世界模型与稳健空间对齐，为机器人与具身智能带来更强的场景理解与推理能力。

Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.

</details>


### [51] [StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models](https://arxiv.org/abs/2512.16483)
*Senmao Li,Kai Wang,Salman Khan,Fahad Shahbaz Khan,Jian Yang,Yaxing Wang*

Main category: cs.CV

TL;DR: 提出StageVAR：一种对视觉自回归（VAR）模型的“分阶段感知”加速框架，前期步骤保持不变确保语义/结构，后期步骤利用语义不相关性与低秩性质进行剪枝/近似，无需额外训练，在保持几乎不降质的同时最高加速3.4倍，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: VAR采用下一尺度预测，生成质量高但当尺度步数增大时计算与时延激增；现有加速需手动选择步数且忽视生成不同阶段的重要性差异，缺乏系统性的阶段分析与自适应加速。

Method: 对VAR生成过程做阶段性分析：早期步骤决定语义与结构，晚期主要做细节润色。基于此提出StageVAR：保持早期步骤完整；对后期步骤引入即插即用的加速策略，利用语义不相关性进行剪枝、利用低秩性质做近似计算；无需额外训练与改模型。

Result: 在GenEval上质量仅下降0.01、DPG下降0.26的情况下，实现最高3.4×加速，且在不同设置下持续优于现有加速基线。

Conclusion: “阶段感知”的设计原则对VAR高效图像生成有效：早期保真、后期近似/剪枝可在几乎不损伤质量下显著提速，提供无需再训练、可插拔的通用加速方案。

Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.

</details>


### [52] [Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment](https://arxiv.org/abs/2512.16484)
*Yuan Li,Yahan Yu,Youyuan Lin,Yong-Hao Yang,Chenhui Chu,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 论文提出通过强化学习让BIQA模型同时具备“类人”和“自洽”的感知-推理能力：用人类标注作为奖励对齐感知与解释，并用自描述一致性奖励迫使模型仅依赖自生成描述来推断画质，最终在相关系数上达SOTA可比，在解释与人类的一致性（ROUGE-1=0.512）显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统BIQA多直接从图像回归质量分数，缺乏与人类相似的感知-推理链条与可解释性；现有方法难以保证模型解释与评分自洽，也缺少量化“人-模解释一致性”的标准。作者希望构建能像人一样先感知再推理、并在解释与评分之间保持一致的BIQA模型，同时提供可量化的人机对齐评估。

Method: 1) 收集涵盖人类“感知-推理”多要素的人类评估数据（含质量分、感知描述与推理链）。2) 强化学习：以人类标注为奖励，推动模型学得类人的感知与推理。3) 自洽性奖励：设计一种奖励，要求模型依据其自生成的描述来推断图像质量，从而内化自洽推理能力。4) 评估：常规模型相关系数（PLCC、SRCC）与解释一致性（用ROUGE-1比较模型与人类的推理链）。

Result: 在常规质量预测指标上（Pearson、Spearman）达到了与SOTA相当的表现；在人机解释一致性上，模型在1000+标注样本上ROUGE-1=0.512，相比基线0.443有显著提升，显示对人类解释的覆盖度更高。

Conclusion: 通过将人类标注引入强化学习并引入自洽性奖励，模型不仅维持了SOTA级别的BIQA分数预测能力，还显著提升了与人类解释的一致性，向类人、可解释的无参考图像质量评估迈进一步。

Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.

</details>


### [53] [Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors](https://arxiv.org/abs/2512.16485)
*Kejun Liu,Yuanyuan Liu,Lin Wei,Chang Tang,Yibing Zhan,Zijing Chen,Zhe Chen*

Main category: cs.CV

TL;DR: 提出EMER数据集与EMERT模型，用眼动行为补充面部表情，实现更鲁棒的多模态情感识别，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FER虽常用，但面部表情具有社会属性，未必反映真实内在情绪，导致FER与真实情感识别（ER）存在鸿沟；需引入更贴近内在情绪的线索，如眼动行为。

Method: 构建EMER数据集：采用自发情绪诱发范式采集真实情绪，同时非侵入式记录眼动序列与注视热图，并同步面部表情视频；分别为多模态ER与单模态FER提供多视角标签。提出EMERT：包含模态对抗式特征解耦与多任务Transformer，将眼动行为作为强补充以弥合FER与ER差距。设计七个多模态评测协议进行全面实验。

Result: 在EMER上，EMERT较现有多模态SOTA方法取得显著领先，验证了建模眼动对提升ER鲁棒性的价值。

Conclusion: 眼动行为是关键情感线索，结合面部表情能有效缩小FER与ER差距并提升鲁棒ER表现。公开EMER数据集与EMERT模型以促进研究。

Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

</details>


### [54] [YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images](https://arxiv.org/abs/2512.16493)
*Huma Hafeez,Matthew Garratt,Jo Plested,Sankaran Iyer,Arcot Sowmya*

Main category: cs.CV

TL;DR: 提出YOLO11-4K，一种面向4K全景（360°）图像的高效实时目标检测框架，在CVIP360标注数据上以28.3ms/帧实现mAP@0.50=0.95，并较YOLO11将延迟降75%。


<details>
  <summary>Details</summary>
Motivation: 传统YOLO等检测器针对标准分辨率优化，难以应对360°图像的超高分辨率、畸变与超大视场，计算成本高、小目标易漏检，缺乏专门的数据与基准。

Method: 1) 模型：加入P2层的多尺度检测头以增强小目标感知；骨干采用GhostConv以降低计算量但保持表征力。2) 数据：对CVIP360进行人工标注，提供6876个帧级框，形成公开可用的4K全景检测基准。3) 评估：对比YOLO11的速度与精度。

Result: 在4K全景场景上达到mAP@0.50=0.95，单帧推理28.3ms；相较YOLO11（112.3ms、mAP@0.50=0.908）实现约75%延迟下降并提升精度。

Conclusion: YOLO11-4K在4K全景图像上实现了兼顾效率与精度的实时检测，适用于导航、安防、AR等高分辨率应用；方法对高分辨率检测任务具有可迁移性。

Abstract: The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.

</details>


### [55] [PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2512.16494)
*Mengyuan Liu,Jiajie Liu,Jinyan Zhang,Wenhao Li,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出PoseMoE：一种将2D骨架与深度特征解耦并通过专家混合与跨专家聚合提升单目3D姿态的框架，在Human3.6M、MPI-INF-3DHP、3DPW上优于传统lifting方法。


<details>
  <summary>Details</summary>
Motivation: 传统lifting方法把检测到的2D关键点与需从零估计的深度在同一特征空间联合编码，深度不确定性会“污染”2D表示，限制整体精度。作者发现：当深度初始完全未知时，与2D联合编码有害；当深度先被网络初步可靠化时，再与2D交互有益。

Method: 设计PoseMoE（Mixture-of-Experts）：1）专家混合结构，分别由专门的专家模块精炼2D姿态特征并学习/估计深度特征，实现2D与深度的特征解耦；2）跨专家知识聚合模块，进行2D与深度之间的双向信息映射与时空上下文聚合，在深度初步可靠后再进行交互以增强表示。

Result: 在Human3.6M、MPI-INF-3DHP、3DPW上实现优于常规lifting方法的性能（文中称“广泛实验显示超越”，未给出具体数值）。

Conclusion: 深度表示对单目3D姿态至关重要。通过在深度尚不可靠阶段与2D解耦、在深度初步可靠后再进行跨模态聚合，PoseMoE有效缓解深度不确定性对2D的干扰并提升整体精度。

Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.

</details>


### [56] [VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks](https://arxiv.org/abs/2512.16501)
*Beitong Zhou,Zhexiao Huang,Yuan Guo,Zhangxuan Gu,Tianyu Xia,Zichen Luo,Fei Tang,Dehan Kong,Yanyi Shang,Suling Ou,Zhenlin Guo,Changhua Meng,Shuheng Shen*

Main category: cs.CV

TL;DR: 提出VenusBench-GD，一个跨平台、双语、层级化的GUI要素定位（grounding）基准，用以全面评测通用与专用模型；结果显示通用多模态模型在基础任务已追平/超越专用GUI模型，但在更高级任务上仍落后，专用模型则存在过拟合与鲁棒性不足。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding基准数据量小、领域覆盖窄，或过度聚焦单一平台且需要强领域知识，难以支撑真实世界多平台GUI智能体评测。

Method: 构建大规模跨平台、双语数据集，覆盖多类应用与多样UI元素；设计高质量标注流水线以提升标注准确性；提出分层任务体系，将grounding划分为基础与高级两类，包含六种子任务，从互补视角评测模型。

Result: 实验证明：通用多模态模型在基础grounding任务上已可匹敌或超越专用GUI模型；高级任务仍由专用模型占优，但其普遍存在显著过拟合与鲁棒性差的问题。

Conclusion: 需要综合、分层的评测框架来真实反映模型在不同难度与场景下的能力；未来应提升高级任务上的泛化与鲁棒性，并避免专用模型的过拟合。

Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.

</details>


### [57] [Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization](https://arxiv.org/abs/2512.16504)
*Qiushuo Cheng,Jingjing Liu,Catherine Morgan,Alan Whone,Majid Mirmehdi*

Main category: cs.CV

TL;DR: 提出一种针对骨架序列的自监督预训练方法，用片段判别和U形特征融合提升时间动作定位的边界敏感特征，在BABEL与PKUMMD等数据集上取得SOTA或一致提升。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习在3D骨架动作识别上成功，但用于时间动作定位不足，因为定位需要对相邻帧细微差异与边界变化更敏感的表征；现有方法多为视频级判别，难以捕捉帧级边界特征与高分辨率时序信息。

Method: 1) 片段判别预训练：将骨架序列密集地划分为互不重叠的snippet，对跨视频的同位置/不同片段进行对比学习，促使模型学习可区分不同时间片段的细粒度表征；2) 架构改进：在强骨架识别骨干上加入U形模块，融合中间层特征，提升时间分辨率以利于帧级定位。

Result: 在BABEL多子集和多种评测协议上，相比现有骨架对比自监督方法均有稳定提升；以在NTU RGB+D与BABEL上预训练迁移到PKUMMD，获得当前最优或SOTA迁移性能。

Conclusion: 面向骨架序列的片段判别式自监督预训练结合U形多尺度融合，可有效学习边界敏感的时序特征，显著提升时间动作定位与跨数据集迁移能力。

Abstract: The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.

</details>


### [58] [Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images](https://arxiv.org/abs/2512.16511)
*Hossein Javidnia*

Main category: cs.CV

TL;DR: 提出MAGINet，单张人脸RGB图像输入，输出高分辨率（512→1024）的光照归一化漫反射反照率，并联合预测其余五个物理渲染通道，达到SOTA的内在分解与重光照质量。


<details>
  <summary>Details</summary>
Motivation: 在非受控光照下准确获得面部内在分解（尤其是光照无关的漫反射反照率）是实现照片级重光照、数字替身与AR特效的关键，但现有U-Net类方法边界模糊、光照不变性弱、与完整渲染栈的协同不足。

Method: 1) 提出MAGINet：层级残差编码器+瓶颈空间-通道注意力+自适应多尺度解码融合，预测512×512的光照归一化反照率；2) 通过轻量三层CNN（RefinementNet）上采样并细化至1024×1024；3) 以细化反照率为条件，使用基于Pix2PixHD的转换器预测其余五个通道：环境光遮蔽、表面法线、高光反射率、半透明度、原始漫反（含残余光）；4) 训练损失：masked-MSE、VGG感知、边缘、patch-LPIPS；数据集：FFHQ-UV-Intrinsics。

Result: 在漫反射反照率估计上达到SOTA，边界更锐利、光照不变性更强；完整六通道内在分解在重光照与材质编辑任务中显著优于以往方法。

Conclusion: 多尺度注意力引导的反照率预测与条件翻译器的协同，可提供高分辨率、物理一致的面部内在分解，为高保真重光照与材质编辑提供可靠基础。

Abstract: Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.

</details>


### [59] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: 提出一种无需再训练的测试时防御TTP：先检测后自适应，通过“填充前后特征余弦相似度变化”检测对抗样本，对检测到的样本用可训练填充与相似度感知集成提升鲁棒性；对干净样本保持不变或可接入现有TTA。在多种CLIP骨干与细粒度数据集上提升鲁棒性且不降干净精度。


<details>
  <summary>Details</summary>
Motivation: VLM/CLIP在零样本识别上强但对对抗扰动极其脆弱。训练阶段防御需标注与昂贵微调，测试时方法又难以可靠区分干净与对抗样本，导致鲁棒性与干净精度无法兼得。因此需要一种轻量、无需再训练、在推理阶段即可稳定检测并处理对抗输入的方案。

Method: 提出Test-Time Padding (TTP)：1) 检测：对输入做空间padding前后，计算CLIP特征余弦相似度的变化幅度作为判别信号，利用跨架构/数据集通用阈值检测对抗样本。2) 适应：若检测为对抗样本，使用“可训练填充”恢复被扰乱的注意模式，并采用“相似度感知集成”生成更稳健的最终预测；若为干净样本，默认不改动，或可选集成现有测试时自适应以进一步增益。

Result: 在多种CLIP骨干与多类细粒度基准上，相比现有最先进测试时防御，TTP显著提升对抗鲁棒性，同时基本不牺牲甚至提升干净样本准确率；阈值具有跨架构与跨数据集的通用性。

Conclusion: 利用填充前后表征一致性作为检测信号，并对检测到的对抗样本进行针对性自适应，可在无需再训练的前提下兼顾鲁棒性与干净精度，提供实用的VLM测试时防御框架。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [60] [N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.16561)
*Yuxin Wang,Lei Ke,Boqiang Zhang,Tianyuan Qu,Hanxun Yu,Zhenpeng Huang,Meng Yu,Dan Xu,Dong Yu*

Main category: cs.CV

TL;DR: 提出N3D-VLM，将原生3D物体感知与3D感知的视觉推理统一，能在文本条件下直接进行3D定位并做可解释的空间推理，数据上通过深度估计把大规模2D标注“抬升”为3D以联合训练定位与CoT推理，取得3D grounding与3D推理SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型主要基于2D图像，缺乏内在的3D物体感知，难以理解真实场景中的空间关系与深度线索，限制了3D场景问答与定位能力。

Method: 构建统一框架N3D-VLM：1) 赋予模型原生3D物体感知，能依据文本在3D空间直接定位目标；2) 在准确的3D定位之上进行显式3D推理，实现结构化、可解释的空间理解；3) 设计可扩展数据管线，利用深度估计将大规模2D标注提升为3D，显著扩充3D grounding数据规模；4) 生成面向3D链式推理的空间问答数据，实现3D定位与3D推理的联合训练。

Result: 在3D object grounding任务上达到SOTA，在3D空间推理（VLM场景）上总体优于现有方法，性能稳定领先。

Conclusion: 将原生3D感知与3D解释性推理统一，配合可扩展的2D→3D数据构建与联合训练，显著提升3D定位与空间推理能力，为更可解释的3D多模态理解提供了有效路径。

Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.

</details>


### [61] [4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction](https://arxiv.org/abs/2512.16564)
*Kirill Mazur,Marwan Taher,Andrew J. Davison*

Main category: cs.CV

TL;DR: 提出一种从随手单目RGB视频生成完整、可重放的4D（时变3D）场景重建系统，能保持先前观察部分并处理遮挡消失物体的运动外推，在多对象与扫描任务上定量定性均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景重建多仅重建当前可见区域或依赖多视角/受限设置，难以在单目随拍条件下实现“对象永久性”和可回放的全时段重建；对遮挡与离场再入、关节/多对象运动的一致建模不足。

Method: 将场景分解为一组刚性3D基元，假设其在时序中运动；利用密集2D对应关系，通过联合优化估计各基元的刚体运动，得到4D重建。为处理不可见阶段，引入基于运动分组的运动外推机制以维持时空连续性，并实现跨时间的持久表示与重放。

Result: 系统可输出可重放的时变3D几何，支持关节物体、多人/多物体扫描与对象永久性；在对象扫描与多对象数据集上，定量指标与视觉质量均显著优于现有方法。

Conclusion: 以单目随拍视频实现了完整、持久的4D场景重建，通过刚性基元分解+密集对应联合优化与不可见期运动外推，达成时空感知与对象永久性，效果领先现有方法。

Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.
  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.
  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.

</details>


### [62] [Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2512.16567)
*Yin Zhang,Yongqiang Zhang,Yaoyue Zheng,Bogdan Raducanu,Dan Liu*

Main category: cs.CV

TL;DR: 提出Causal-Tune：在频域剖分并保留因果成分、抑制非因成分，以小参数微调视觉基础模型用于域泛化语义分割，显著提升尤其在恶劣天气下的mIoU。


<details>
  <summary>Details</summary>
Motivation: 现有DGSS微调多用轻量适配器或中间特征校准，但忽视VFM长期预训练带来的伪影与非因因素，这些会阻碍有效表示并降低泛化，尤其跨域与恶劣条件。作者以因果视角认为这些伪影多位于特征的低/高频段，需显式区分与去除。

Method: 设计Causal-Tune频域微调策略：1) 对各层特征做DCT获取频谱；2) 用高斯带通滤波将频谱分为因果与非因部分；3) 在频域引入“因果感知”可学习token，仅作用并细化因果部分，非因部分丢弃；4) 通过逆DCT还原至空间域并继续前向；整体为小参数微调以增强域不变因果表征。

Result: 在多种跨域任务上有效，尤其恶劣天气下表现突出；在雪景条件较基线提升+4.8% mIoU，其它设置也有一致增益。

Conclusion: 在VFM特征中显式辨析并仅优化因果频段可增强DGSS泛化。Causal-Tune以频域带通+可学习因果token实现小参数微调，抑制伪影与非因因素，在多域、恶劣天气中取得显著收益。

Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.

</details>


### [63] [CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series](https://arxiv.org/abs/2512.16577)
*Nico Albert Disch,Saikat Roy,Constantin Ulrich,Yannick Kirchhoff,Maximilian Rokuss,Robin Peretzke,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: CRONOS提出一种支持连续时间、基于多次既往3D扫描进行单次目标预测的框架，通过学习三维体素空间中的时空速度场，将上下文体数据“运输”到任意目标时间点，跨三种公共数据集均优于基线且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像预测常受限于：仅依赖单次先验扫描、要求固定时间网格或只预测全局标签，导致在不规则采样下难以进行体素级预测。需要一个能处理多时刻输入、支持连续时间并直接在3D体素空间工作的统一方法。

Method: 提出CRONOS：一个多到一的序列到图像预测框架。输入为多个既往3D体数据及其时间戳（可离散或连续）。模型学习时空速度场（spatio-temporal velocity field），在3D体素空间内对上下文体进行“运输/变形”，生成任意目标时间的体数据。单一模型同时支持网格化和实值时间戳，实现连续时间的sequence-to-image预测。

Result: 在三类公开数据集（Cine-MRI、灌注CT、纵向MRI）上，CRONOS在预测性能上优于多种基线，同时保持计算竞争力。

Conclusion: CRONOS实现了首个面向3D医学数据的连续时间多上下文到单目标预测框架，能在不规则采样下进行体素级预测，并具备跨数据集的优势；作者将开源代码与评测协议以促进可复现、多数据集基准研究。

Abstract: Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.

</details>


### [64] [Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs](https://arxiv.org/abs/2512.16584)
*Jintao Tong,Jiaqi Gu,Yujing Lou,Lubin Fan,Yixiong Zou,Yue Wu,Jieping Ye,Ruixuan Li*

Main category: cs.CV

TL;DR: 提出SkiLa：在MLLM的自回归推理流中原生生成“潜在素描”视觉嵌入，与文本思维交替，从而在同一潜空间完成统一的文视觉想象与推理，显著提升视觉中心任务并具良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多靠文本链式推理理解图像，但缺乏“视觉想象”能力；以往方法依赖预定义外部工具或在思考中显式生成图像，缺乏灵活统一的内在表征。受人脑在统一空间中进行文-视觉联想的启发，作者希望在同一特征空间内让模型在推理时自由插入视觉思维。

Method: 提出Sketch-in-Latents (SkiLa)：扩展MLLM的自回归解码头，使其除生成文本token外，还可生成连续的“潜在素描token”（视觉嵌入）。在多步推理中，模型在“文本思维模式”(text think tokens) 与“视觉素描模式”(latent sketch tokens) 间动态切换；并通过“潜在视觉语义重建机制”保证这些潜在素描token具备语义对齐与可还原性，从而让视觉思维在潜空间被编码与利用。

Result: 在多项视觉中心任务上取得优于现有方法的性能，同时在更广泛的多模态基准上表现出良好泛化能力。

Conclusion: 在统一潜空间中交替生成文本与视觉潜在token，可使MLLM获得原生的视觉想象与推理能力，无需预定义外部工具；SkiLa验证了该范式的有效性并带来性能与泛化提升。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

</details>


### [65] [Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks](https://arxiv.org/abs/2512.16586)
*Shaohua Wu,Tong Yu,Shenling Wang,Xudong Zhao*

Main category: cs.CV

TL;DR: 提出Yuan-TecSwin：用Swin Transformer替代CNN块的文本条件扩散模型，增强全局建模；改进文本对齐与条件注入；引入自适应时间步推理，性能提升约10%；在ImageNet上取得FID 1.37的SOTA且无需多阶段额外模型，生成质量接近人画。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型多以U-Net/CNN为骨干，卷积的局部性限制了对长程语义关系的捕获，影响文本-图像一致性与全局结构质量。作者希望提升非局部表征能力与文本条件对齐，同时降低推理开销并提升生成质量。

Method: - 将U形扩散网络的编码器/解码器中的CNN块整体替换为Swin-Transformer块，利用移位窗口与层级特征实现高效全局建模。
- 采用更合适的文本编码器与嵌入使用策略，精心设计文本条件与视觉特征的融合（如在多层次注入/交互）。
- 在推理阶段使用“自适应时间步搜索”，在不同扩散阶段动态选择步数或步长以优化质量-效率权衡。

Result: - 推理阶段通过自适应时间步带来约10%的性能提升。
- 在ImageNet生成基准上达到FID 1.37的最新最优成绩。
- 无需在不同去噪阶段引入额外子模型。
- 人体主观评测中，生成图与人类绘制图难以区分。

Conclusion: 用Swin替代CNN提升了扩散模型的非局部建模与文本对齐，自适应时间步进一步优化推理，整体在ImageNet上达成SOTA且具实用性与高保真度。

Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.

</details>


### [66] [Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment](https://arxiv.org/abs/2512.16609)
*Ayush Bhavsar*

Main category: cs.CV

TL;DR: Hazedefy 是一个面向实时视频/相机的轻量去雾流水线，基于大气散射模型与暗原色先验，强调在消费级设备上无GPU即可部署与运行。


<details>
  <summary>Details</summary>
Motivation: 现有深度或复杂去雾方法计算开销大、部署难，不利于移动与嵌入式实时应用；需要一种在资源受限环境中也能显著提升可见度与对比度的方案。

Method: 在大气散射模型与DCP框架下，提出：1) 伽马自适应重建；2) 具有下界约束的快速透射率近似以提升数值稳定性；3) 基于分数最高亮度像素平均的稳定大气光估计；4) 可选色彩平衡阶段。整体为面向实时的轻量级管线。

Result: 在真实图像与视频上实验展示可见度与对比度提升，且无需GPU即可实时运行，适配移动与嵌入式设备。

Conclusion: Hazedefy在不牺牲太多质量的前提下，以低计算成本实现稳定去雾，适合资源受限场景的实时增强与部署。

Abstract: This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.

</details>


### [67] [Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers](https://arxiv.org/abs/2512.16615)
*Yifan Zhou,Zeqi Xiao,Tianyi Wei,Shuai Yang,Xingang Pan*

Main category: cs.CV

TL;DR: 提出Log-linear Sparse Attention（LLSA），用分层Top-K选择与分层KV富集，将DiT在极长序列上的选择与注意力复杂度从二次降至对数线性，并在不使用补丁/VAE的像素空间生成上显著提速且保持质量。


<details>
  <summary>Details</summary>
Motivation: 现有DiT在长序列上因自注意力二次复杂度受限；Top-K稀疏注意虽加速，但仍存在压缩后的块级选择开销仍为二次、序列变长需更大K以保质、单层粗粒度难以表征全局结构等问题。需要一种能同时降低“候选选择”和“注意力计算”复杂度、且保持全局上下文质量的机制。

Method: 提出LLSA：1) 分层结构上的渐进式Top-K选择，上一层选出的索引指导下一层稀疏搜索，将选择与注意力的总体复杂度降为对数线性；2) 引入Hierarchical KV Enrichment（分层KV富集），在不同粒度使用更少token参与注意力同时保留全局上下文；3) 实现仅基于稀疏索引的高性能GPU前向/反向，不需要稠密mask，支持端到端训练。

Result: 在256x256像素级token序列、无patchification与VAE的高分辨率像素空间生成上评测：注意力推理加速28.27倍、DiT训练加速6.09倍，同时保持生成质量（未显著下降）。

Conclusion: 分层化、可训练的稀疏注意能在极长序列DiT中兼顾全局语义与高效性，将选择与注意力从二次降至对数线性，是高分辨率、长序列生成可扩展训练的有力路径；工程实现表明该方法在实际GPU上高效可用。

Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA

</details>


### [68] [Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation](https://arxiv.org/abs/2512.16620)
*Kanwal Aftab,Graham Adams,Mark Scanlon*

Main category: cs.CV

TL;DR: 提出以电源插座为室内地理定位线索的三阶段深度学习管线：检测插座、分类插座类型、映射到国家；在Hotels-50K/TraffickCam上验证，显示对现实取证场景的实用潜力。


<details>
  <summary>Details</summary>
Motivation: 室外多媒体地理定位已成熟，但室内场景因布局相似、翻新频繁、光照多变、GPS失效及敏感领域数据稀缺而困难。需要稳定且具有地域指示性的室内标记物以助力打击人口贩运、儿童剥削等犯罪的取证与定位。

Method: 构建以“插座类型”作为国家/地区指示的三阶段深度学习管线：1) 插座检测（YOLOv11，mAP@0.5=0.843）；2) 插座类型分类（12类，Xception，准确率0.912）；3) 将类型映射到国家，根据置信度阈值进行国家推断（>90%阈值下准确率0.96）。为缓解数据稀缺，制作两套专用数据集：检测集2328张并增广至4072张；分类集3187张、12类。使用Hotels-50K中更贴近现实的TraffickCam子集进行评估。

Result: 在真实、非理想拍摄条件下（弱光、非专业角度）仍能较可靠地检测与分类插座，并在高置信度时以0.96准确率映射到国家；整体管线在TraffickCam上表现稳健，证明可行性。

Conclusion: 以插座类型为室内地理定位线索是可行且具有取证价值的思路；三阶段管线在真实数据上取得良好效果，开源代码、模型与数据为后续研究与实际部署提供基础。

Abstract: Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.

</details>


### [69] [DeContext as Defense: Safe Image Editing in Diffusion Transformers](https://arxiv.org/abs/2512.16625)
*Linghui Shen,Mingyue Cui,Xingyi Yang*

Main category: cs.CV

TL;DR: 提出DeContext：通过对跨模态注意力通路施加微小、定向扰动来阻断源图像上下文向输出传播，从而防止未经授权的图像编辑；在Flux Kontext与Step1X-Edit上有效阻断编辑且保持画质。


<details>
  <summary>Details</summary>
Motivation: 大规模基于DiT的in-context图像编辑极其便捷，但易被用于身份冒充、虚假信息等恶意操作；现有输入扰动防护多针对个性化T2I模型，对现代大模型的鲁棒性与工作机理研究不足。

Method: 提出DeContext：分析上下文主要通过多模态注意力层传播；在这些跨注意力通道中注入小幅、定向的对抗扰动以削弱连接，重点针对早期去噪步骤与特定Transformer block集中施加扰动，从而解耦输入与输出的上下文关联。方法高效、简单。

Result: 在Flux Kontext与Step1X-Edit基准上，DeContext稳定阻断不受欢迎的图像编辑，同时保持视觉质量；显示早期去噪与特定块主导上下文传播，验证了聚焦扰动位置的有效性。

Conclusion: 基于注意力的输入扰动可作为对抗图像操纵的有力防御手段；DeContext在现代DiT in-context编辑模型上高效、鲁棒地降低上下文依赖并保护隐私。

Abstract: In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.

</details>


### [70] [SARMAE: Masked Autoencoder for SAR Representation Learning](https://arxiv.org/abs/2512.16635)
*Danxu Liu,Di Wang,Hebaixu Wang,Haoyang Chen,Wentao Jiang,Yilin Cheng,Haonan Guo,Wei Cui,Jing Zhang*

Main category: cs.CV

TL;DR: 提出SARMAE：面向SAR的噪声感知MAE，自监督学得稳健表示；构建百万级SAR-1M并配光学对；通过注入散斑噪声（SARE）与语义锚对齐（SARC）提升跨任务表现，达SOTA。


<details>
  <summary>Details</summary>
Motivation: SAR在全天候遥感中关键，但深度学习受两难：1) 数据稀缺、规模小难以预训练；2) SAR固有散斑噪声破坏细粒度语义，监督与自监督均难学到稳健表征。需要既能大规模预训练、又能显式建模散斑影响，并借助跨模态（光学）先验提升语义一致性的方案。

Method: 1) 数据：构建百万级SAR-1M数据集，并提供配对光学图像，支持大规模自监督。2) 预训练框架：提出SARMAE（Noise-Aware Masked Autoencoder）。3) SARE：在MAE中显式注入SAR特有散斑噪声，进行噪声感知的掩码重建，促使编码器学习对散斑鲁棒的表示。4) SARC：利用配对光学图像提供语义锚，通过特征对齐/约束，使SAR特征与光学先验在语义空间一致。

Result: 在多个SAR基准上的分类、检测、分割任务取得SOTA性能，证明所学表征的泛化与鲁棒性优于现有方法。

Conclusion: 以数据规模（SAR-1M）+噪声建模（SARE）+跨模态语义对齐（SARC）的组合，SARMAE在多任务上显著提升SAR表征质量与下游性能；代码与模型将开源。

Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.

</details>


### [71] [REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion](https://arxiv.org/abs/2512.16636)
*Giorgos Petsangourakis,Christos Sgouropoulos,Bill Psomas,Theodoros Giannakopoulos,Giorgos Sfikas,Ioannis Kakogeorgiou*

Main category: cs.CV

TL;DR: 提出REGLUE：在一个统一的SiT骨干中共同建模VAE潜变量、局部VFM语义以及全局[CLS]，通过非线性压缩与外部对齐加速收敛并提升ImageNet 256×256生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统LDM的重建式去噪目标对语义监督间接，导致高层语义形成缓慢、训练时间长、样本质量受限。现有将VFM语义引入扩散的做法要么只做外部表示对齐，要么只在扩散里联建少量VFM层，未充分利用VFM跨层、非线性、空间细粒度的丰富语义。

Method: 提出REGLUE框架：在单一SiT骨干内联合建模三类信号——(i) VAE图像潜变量，(ii) 通过轻量卷积语义压缩器从多层VFM特征非线性聚合得到的低维空间化局部语义表示，(iii) 全局图像级[CLS]标记；并加入外部对齐损失将内部表示正则到冻结的VFM目标。语义表示与VAE潜变量在扩散过程中“纠缠”联合建模。

Result: 在ImageNet 256×256上，相比SiT-B/2、SiT-XL/2基线及REPA、ReDi、REG，REGLUE获得更低FID、更快收敛。消融表明：(a) 空间VFM语义至关重要；(b) 非线性压缩是发挥其效益的关键；(c) 全局token与外部对齐作为轻量增强与全局-局部-潜变量联合建模互补。

Conclusion: 联合建模VAE潜变量与全局-局部VFM语义，并结合非线性语义压缩与外部对齐，可显著提升扩散生成的语义学习效率与样本质量，优于先前仅对齐或部分特征建模的方法。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .

</details>


### [72] [FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering](https://arxiv.org/abs/2512.16670)
*Ole Beisswenger,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: FrameDiffuser 是一种自回归的神经渲染框架，利用G-buffer与前一帧结果的双重条件，在交互场景中以低开销生成光照真实、时间一致的图像；通过ControlNet+ControlLoRA和三阶段训练实现长期稳定性，并以环境专用训练换取更快推理与更高质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的G-buffer条件生成要么逐帧独立（缺乏时间一致性），要么依赖整段视频且计算昂贵（不适合交互式应用）。需要一种既能保证时间一致性又满足交互实时性的神经渲染方法。

Method: 提出FrameDiffuser：自回归扩散框架。1) 输入为当前帧G-buffer（几何、材质、表面属性）和先前生成帧；2) 双条件架构：ControlNet负责结构/几何对齐，ControlLoRA注入时间一致性约束；3) 三阶段训练策略以稳定自回归推理；4) 采用按环境专用训练，提高一致性和速度。

Result: 在数百至上千帧的序列中维持稳定的时间一致性，生成具有准确光照、阴影、反射的高质量图像；相较通用模型，在特定环境上获得更好的写实度与推理速度，适合消费级设备上的交互式应用。

Conclusion: 自回归+双重条件的设计有效解决了交互式神经渲染中的时间一致性与计算开销矛盾；环境专用训练是实现高质量、实时性的务实路径。

Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.

</details>


### [73] [Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray](https://arxiv.org/abs/2512.16685)
*Gonçalo Gaspar Alves,Shekoufeh Gorgi Zadeh,Andreas Husch,Ben Bausch*

Main category: cs.CV

TL;DR: 该工作提出利用“受试者指纹”在潜在空间中重识别个体，以检测/避免开放数据集合并时的同一受试者重复，从而防止数据泄漏。以ResNet-50+triplet loss实现，在ChestXray-14与BraTS-2021上少样本与大规模N-way情形下均获得很高的召回@K。


<details>
  <summary>Details</summary>
Motivation: 开放医学影像数据常被合并使用，但若同一受试者出现在多个数据集，会导致训练/测试泄漏并夸大模型性能。需要一种通用、可扩展的手段在合并前后识别同一受试者，以量化和避免泄漏风险。

Method: 提出“主体指纹”思路：将同一受试者的所有图像映射到潜在空间中的紧致簇，不同受试者相互分离。采用ResNet-50为骨干，使用triplet margin loss进行度量学习；通过相似度匹配实现重识别。评估少样本（N-way K-shot）设置，包括20-way 1-shot、1000-way 1-shot等，在2D胸片（ChestXray-14）与3D MRI（BraTS-2021）上进行。指标为Mean-Recall-@-K。

Result: ChestXray-14上：20-way 1-shot的Mean-Recall-@-K达99.10%，500-way 5-shot也保持90.06%。BraTS-2021上：20-way 1-shot为99.20%，100-way 3-shot为98.86%。显示在高难度少样本场景中仍具高准确重识别能力。

Conclusion: 主体指纹可有效进行受试者级别的重识别，帮助发现并避免跨数据集的受试者重叠，从而降低数据泄漏风险；方法在2D与3D医学影像中均表现稳健、可扩展于大规模N-way检索场景。

Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.

</details>


### [74] [Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?](https://arxiv.org/abs/2512.16688)
*Serafino Pandolfini,Lorenzo Pellegrini,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 评估深伪检测器在局部修补(inpainting)场景下的泛化：大规模多生成器训练的模型对中大面积或“再生成式”修补有较强可检测性，优于多种专用方法，但对小区域更具挑战。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI能进行局部编辑/修补，常保留原始上下文，易被用于安全威胁；已有检测器多针对“全合成”图像，其在“局部操控”上的泛化能力缺乏系统评估。

Method: 系统化基准：选取面向全合成深伪检测的SOTA模型，跨多个数据集评测，覆盖多种生成器、掩膜大小与修补策略（含再生成式inpainting）；比较其对局部修补检测性能，并与若干专用/即席检测方法对照。

Result: 经过多生成器大规模训练的检测器对局部修补具一定迁移性：能稳定识别中、大面积或再生成式修补，性能优于多种ad hoc方法；但对小掩膜、细粒度编辑的检测能力较弱。

Conclusion: 深伪检测器在局部修补检测上具有“部分可迁移”能力，尤其在较大区域和再生成式场景；要全面覆盖细小区域编辑仍需针对性训练或新特征/方法设计。

Abstract: The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.

</details>


### [75] [SDFoam: Signed-Distance Foam for explicit surface reconstruction](https://arxiv.org/abs/2512.16706)
*Antonella Rech,Nicola Conci,Nicola Garau*

Main category: cs.CV

TL;DR: 提出SDFoam：在RadiantFoam的显式Voronoi辐射组织上，联合学习隐式SDF，通过光线追踪优化与Eikonal正则，使Voronoi面贴合SDF零水平集，显著提升网格重建精度，同时保持与RF相当的外观质量与训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF、3DGS与RadiantFoam尽管渲染快/质优，但网格重建仍不精确，易出现浮点噪点、表面不锐利、拓扑差。需要一种既保留高效渲染与外观质量、又能产生高保真、可提取网格的表示。

Method: 采用混合隐式-显式表征：显式Voronoi Diagram组织辐射场用于可微光线追踪；同时学习一个隐式SDF并以Eikonal损失正则。训练时通过体渲染/光线追踪优化外观，SDF提供度量一致的等值面，诱导并约束Voronoi单元近表面几何，使其面趋向SDF零水平集，从而获得几何对齐。

Result: 在多样场景上，相比RF/3DGS等，显著降低Chamfer距离，表面更锐利、浮点更少、拓扑更好；同时PSNR/SSIM与RF相当；训练速度与RF基本持平。

Conclusion: SDFoam通过SDF引导Voronoi几何对齐，实现高质量几何与高效外观兼得，显著提升网格重建而不牺牲渲染质量与效率。

Abstract: Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.

</details>


### [76] [A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry](https://arxiv.org/abs/2512.16710)
*Chiara Di Vece,Zhehua Mao,Netanell Avisdris,Brian Dromey,Raffaele Napolitano,Dafna Ben Bashat,Francisco Vasconcelos,Danail Stoyanov,Leo Joskowicz,Sophia Bano*

Main category: cs.CV

TL;DR: 他们发布了一个公开的多中心、多设备胎儿超声图像基准数据集，含专家标注的关键解剖地标与评估代码，揭示单中心训练会高估性能，并为跨中心泛化与域自适应研究提供标准平台。


<details>
  <summary>Details</summary>
Motivation: 手工在标准超声切面上定位解剖地标以进行胎儿生长测量既耗时又依赖操作者，且易受不同设备与医院差异影响，导致自动化方法可重复性差。缺乏多来源带标注的数据集成为发展AI辅助胎儿生物测量与评估的瓶颈。

Method: 构建并公开一个包含4513幅、来自1904名受试者、覆盖3家临床中心与7种超声设备的胎儿超声静态图像数据集；为头围相关（BPD、OFD）、腹围相关（横径、前后径）和股骨长度等常用生物测量提供专家地标标注；制定标准且受试者不重叠的训练/测试划分，提供评测代码与基线模型；用自动生物测量模型系统评估域迁移/中心间泛化性能。

Result: 基线实验显示：若在单中心内训练并评估，会显著高估模型性能；在多中心测试条件下性能下降，量化了明显的域偏移。数据、标注、训练与评测流水线全部公开，可复现实验并公平比较方法。

Conclusion: 该数据集是首个覆盖主要胎儿生物测量、具多中心多设备与地标标注的公开资源，为域自适应与跨中心泛化研究提供强有力基准，有望提升AI辅助胎儿生长评估在不同机构间的可靠性与可推广性。

Abstract: Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.

</details>


### [77] [OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition](https://arxiv.org/abs/2512.16727)
*Haochen Chang,Pengfei Ren,Buyuan Zhang,Da Li,Tianhao Han,Haoyang Zhang,Liang Xie,Hongbo Chen,Erwei Yin*

Main category: cs.CV

TL;DR: 提出OMG-Bench大规模在线微手势（骨架）基准与HMATr模型；通过多视角自监督管线生成骨架、半自动标注；HMATr用层级记忆与位置感知查询统一检测与分类，SOTA检测率提升7.6%。


<details>
  <summary>Details</summary>
Motivation: VR/AR交互需要在线微手势识别，但微小、快速且连续的动作难以获取高质量骨架与逐帧标注；公共数据和通用算法匮乏，限制研究与落地。

Method: 1) 多视角自监督重建与融合生成高精度手部骨架，结合启发式规则与专家复核实现半自动逐帧标注；据此构建OMG-Bench数据集（40类、13,948实例、1,272序列）。2) 提出HMATr：层级记忆增强Transformer，使用帧级与窗口级记忆库保存历史细节与语义，采用可学习位置感知查询从记忆初始化，隐式编码手势的位置与语义，实现端到端的检测+分类。

Result: 在OMG-Bench上，HMATr较现有方法检测率提升7.6%，在具有细微运动、快速动态、连续执行的在线场景中建立强基线；整体优于SOTA。

Conclusion: OMG-Bench填补了在线微手势骨架数据集空白；HMATr通过层级记忆与位置感知查询有效统一检测与分类，显著提升性能，为该领域提供强有力基准与方法。

Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/

</details>


### [78] [Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2512.16740)
*Yunkai Yang,Yudong Zhang,Kunquan Zhang,Jinxiao Zhang,Xinying Chen,Haohuan Fu,Runmin Dong*

Main category: cs.CV

TL;DR: 提出TODSynth：以多模态扩散Transformer与任务反馈采样，提升遥感语义分割用合成数据的质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 遥感语义分割标注昂贵、难以覆盖复杂场景；虽可用可控生成扩充数据，但语义掩码控制复杂、采样质量不稳，导致下游收益有限。

Method: 1) 设计多模态扩散Transformer（MM-DiT），采用文本-图像-掩码统一三重注意力，并对图像与掩码分支全量微调；系统比较不同控制方案。2) 提出基于任务反馈的即插即用采样：控制-纠正流匹配（CRFM），在扩散早期高可塑阶段由语义损失引导动态调整采样方向，稳定生成并贴合分割任务。

Result: 在少样本与复杂场景下，相比现有可控生成方法，生成更稳定、与任务更一致的遥感合成数据，显著提升下游语义分割表现。

Conclusion: 联合三重注意力与任务反馈采样能有效缩小合成数据与分割任务的鸿沟，为RS语义分割提供更可靠的数据扩充方案。

Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.

</details>


### [79] [TreeNet: A Light Weight Model for Low Bitrate Image Compression](https://arxiv.org/abs/2512.16743)
*Mahadev Prasad Panda,Purnachandra Rao Makkena,Srivatsa Prativadibhayankaram,Siegfried Fößel,André Kaup*

Main category: cs.CV

TL;DR: TreeNet提出一种二叉树式的编解码架构与注意力特征融合，实现低复杂度的学习型图像压缩；在低码率下优于JPEG AI并显著降复杂度。


<details>
  <summary>Details</summary>
Motivation: 学习式图像压缩在性能上具优势，但计算复杂度高、模型庞大，限制了实际部署与普及。需要在保持或提升压缩质量的同时显著降低复杂度。

Method: 设计二叉树结构的编码器-解码器，将特征逐层分叉与聚合；引入注意力特征融合整合多分支信息；构建多种潜变量配置并进行消融，分析不同潜在表示对重建的贡献；在三大基准数据集上与包括JPEG AI在内的方法比较。

Result: 在低码率区间，相比JPEG AI，BD-rate平均提升4.83%（越低越好），同时模型复杂度降低87.82%。实验覆盖三类常用基准数据集，并通过广泛消融验证设计选择的有效性。

Conclusion: 树形分支与注意力融合的组合在不显著牺牲、甚至提升压缩性能的同时大幅降低复杂度，适合资源受限场景；潜表示设计对重建质量影响显著，提供了后续优化方向。

Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.

</details>


### [80] [Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation](https://arxiv.org/abs/2512.16767)
*Zhiyang Guo,Ori Zhang,Jax Xiang,Alan Zhao,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: 提出Make-It-Poseable：把3D角色摆姿从网格顶点变形转为潜空间变换，用骨骼驱动的潜表示编辑实现更高质量、更鲁棒的姿态生成与编辑。


<details>
  <summary>Details</summary>
Motivation: 传统自动绑定与基于姿态的生成易受不准确蒙皮权重、拓扑缺陷、姿态贴合差等问题影响，鲁棒性与泛化性不足，需要一种不依赖显式网格变形、能处理高保真几何与拓扑变化的新范式。

Method: 提出前馈式框架，将角色重建为目标姿态的过程表述为“潜空间中的姿态变换”。核心为“潜姿态变换器”，以骨骼运动驱动对形状token的操作，并配合稠密pose表征实现精细控制；为保证高保真和兼容拓扑变化，使用潜空间监督策略与自适应补全模块完成几何细节与缺失部位。

Result: 在姿态质量指标上优于现有方法，能稳定生成高保真几何并更好适配不同拓扑；方法还支持零/少改设置下的3D编辑（如部件替换与细化）。

Conclusion: 将摆姿问题转化为潜表示操控，结合潜监督与自适应补全，有效克服传统网格变形管线的痛点，提升鲁棒性与泛化，并自然拓展到多种3D编辑任务。

Abstract: Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.

</details>


### [81] [FlowDet: Unifying Object Detection and Generative Transport Flows](https://arxiv.org/abs/2512.16771)
*Enis Baty,C. P. Bridges,Simon Hadfield*

Main category: cs.CV

TL;DR: FlowDet将目标检测表述为条件流匹配（CFM）的生成式传输问题，以直线化、确定性的“流”替代扩散式去噪路径，得到更快的推理步数-性能缩放，在COCO与LVIS上优于DiffusionDet和非生成基线，AP最高提升约+3.6%、稀有类AP_rare最高+4.2%。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检测（如DiffusionDet）把框预测视为扩散去噪，但其随机、弯曲的传输路径在推理步数扩展时效率欠佳，且在召回受限设置下需要大量提案来补偿。作者希望找到更简单高效的生成式传输方式，既能保留可变框数与推理步数的优点，又能在更少步骤中取得更好精度，尤其在召回受限场景更好体现生成式传输的优势。

Method: 提出FlowDet：用Conditional Flow Matching将检测建模为从简单先验到目标框分布的条件流式传输，学习近似直线的确定性轨迹。保留可变框数量与可变推理步数的训练-推理一致性；在多骨干与多数据集上训练/评测，比较不同步数下的性能扩展与在召回约束下的表现。

Result: FlowDet在不同特征骨干与数据集上均超过扩散检测与非生成基线；随着推理步数增加，性能提升更快；在召回受限评测中无需大量提案即可展现优势。在COCO与LVIS上相对DiffusionDet分别带来最高约+3.6% AP与+4.2% AP_rare的增益。

Conclusion: 将检测的生成式建模从扩散转为条件流匹配可学习更直、更高效的传输路径，带来更好的步数-性能缩放与总体精度提升，尤其在召回受限条件下优势明显，同时保持可变框数和推理步数的灵活性。

Abstract: We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.

</details>


### [82] [Kling-Omni Technical Report](https://arxiv.org/abs/2512.16776)
*Kling Team,Jialu Chen,Yuanzheng Ci,Xiangyu Du,Zipeng Feng,Kun Gai,Sainan Guo,Feng Han,Jingbin He,Kang He,Xiao Hu,Xiaohua Hu,Boyuan Jiang,Fangyuan Kong,Hang Li,Jie Li,Qingyu Li,Shen Li,Xiaohan Li,Yan Li,Jiajun Liang,Borui Liao,Yiqiao Liao,Weihong Lin,Quande Liu,Xiaokun Liu,Yilun Liu,Yuliang Liu,Shun Lu,Hangyu Mao,Yunyao Mao,Haodong Ouyang,Wenyu Qin,Wanqi Shi,Xiaoyu Shi,Lianghao Su,Haozhi Sun,Peiqin Sun,Pengfei Wan,Chao Wang,Chenyu Wang,Meng Wang,Qiulin Wang,Runqi Wang,Xintao Wang,Xuebo Wang,Zekun Wang,Min Wei,Tiancheng Wen,Guohao Wu,Xiaoshi Wu,Zhenhua Wu,Da Xie,Yingtong Xiong,Yulong Xu,Sile Yang,Zikang Yang,Weicai Ye,Ziyang Yuan,Shenglong Zhang,Shuaiyu Zhang,Yuanxing Zhang,Yufan Zhang,Wenzheng Zhao,Ruiliang Zhou,Yan Zhou,Guosheng Zhu,Yongjie Zhu*

Main category: cs.CV

TL;DR: Kling-Omni 是一个端到端的通用多模态视频生成框架，能把文本、图像、视频等输入统一表示，生成高保真且具智能性的影片，并在推理驱动编辑与指令跟随方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成多采用割裂的流水线，难以统一处理多种输入与任务（生成、编辑、推理），导致能力碎片化、质量与一致性不足。作者希望用一个端到端通用框架，打通多模态输入、统一表示与高质量视频合成。

Method: 提出 Kling-Omni：1) 统一的多模态表示层，将文本指令、参考图像、视频上下文融合；2) 端到端训练以同时支持生成、编辑、推理等任务；3) 构建覆盖多模态视频创作的数据系统；4) 大规模高效预训练策略与推理加速优化，用于高保真“电影级”视频输出。

Result: 在全面评测中，Kling-Omni 在情境内生成（in-context generation）、基于推理的编辑、多模态指令跟随等方面表现优异，展示了高质量与高智能性的综合能力。

Conclusion: Kling-Omni 不仅是内容创作工具，更朝向“多模态世界模拟器”的方向迈进，具备感知、推理、生成与交互动态复杂世界的潜力。

Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.

</details>


### [83] [R3ST: A Synthetic 3D Dataset With Realistic Trajectories](https://arxiv.org/abs/2512.16784)
*Simone Teglia,Claudia Melis Tonti,Francesco Pro,Leonardo Russo,Andrea Alfarano,Leonardo Pentassuglia,Irene Amerini*

Main category: cs.CV

TL;DR: R3ST 是一个将真实车辆轨迹嵌入合成3D环境的合成数据集，用于交通视觉与轨迹预测，兼具精确标注与真实人类驾驶轨迹。


<details>
  <summary>Details</summary>
Motivation: 现实数据集虽具真实行为但缺乏精确标注；合成数据集标注充分却常缺乏真实车辆运动。需要一种既保留真实轨迹动力学又提供高质量标注的数据源，以提升道路车辆轨迹预测研究。

Method: 构建合成3D道路场景，并将来自无人机俯视数据集 SinD 的真实世界车辆轨迹注入其中，形成具有多模态精确真值标注和逼真驾驶行为的合成序列。

Result: 得到名为 R3ST 的数据集，在合成环境中提供真实人类驾驶轨迹与精确、丰富的标注，缩小了合成数据与现实轨迹间的差距。

Conclusion: R3ST 为交通轨迹预测提供了兼具真实性与标注质量的数据，推动利用合成数据进行更可靠模型训练与评测。

Abstract: Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.

</details>


### [84] [KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals](https://arxiv.org/abs/2512.16791)
*Shuting Zhao,Zeyu Xiao,Xinrong Chen*

Main category: cs.CV

TL;DR: 提出KineST：一种由运动学先验引导的状态空间模型，用少量HMD信号高效重建全身姿态，兼顾精度、时序一致性与轻量化。


<details>
  <summary>Details</summary>
Motivation: AR/VR中仅凭HMD等稀疏传感信号重建真实、多样且稳定的全身姿态很难；现有方法要么代价高、要么将时空依赖割裂建模，难以兼顾精度、平滑与效率。

Method: 构建KineST状态空间模型，核心包括：1) 基于“状态空间对偶”框架，设计嵌入运动学先验的双向扫描策略，强化关节间复杂依赖；2) 采用混合时空表示学习，紧耦合空间与时间上下文，实现精度与平滑度的平衡；3) 引入几何角速度损失，对旋转变化施加物理约束，提升运动稳定性。

Result: 在多项实验中，KineST在精度与时间一致性上优于现有方法，同时保持轻量级和高效推理。

Conclusion: 运动学引导的双向扫描与混合时空学习、配合几何角速度损失，可在轻量框架下从稀疏HMD信号高质量重建全身姿态，达成精度、稳定与效率的兼顾。

Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/

</details>


### [85] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict 是一个几何感知的视觉-语言-动作框架，在不增加推理时3D解码负担的前提下，用预测性的运动/几何先验监督训练，从而显著提升需要精确3D推理的机器人操作泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型多为反应式、偏2D，对需要精确3D空间推理与几何理解的操控任务表现不稳，缺乏对未来运动与场景几何的前瞻性建模。

Method: 在连续动作策略上叠加预测先验：1) 轨迹级模块编码运动历史，预测多步机械臂3D关键点轨迹；2) 预测性3D高斯几何模块，沿未来关键点轨迹进行轨迹引导的工作空间几何预测与细化。两模块仅用于训练阶段，通过基于深度的可微渲染提供监督；推理时仅增加少量查询token，无需任何3D解码。

Result: 在RoboCasa Human-50、LIBERO及真实世界操控任务上，相比强VLA基线均有一致提升，尤其在几何密集与强空间需求的场景中优势明显。

Conclusion: 将预测性的运动与几何先验作为训练监督，可在不增加推理复杂度的情况下显著增强VLA对3D几何与长视野空间推理的能力，提升机器人操控泛化与可靠性。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [86] [DenseBEV: Transforming BEV Grid Cells into 3D Objects](https://arxiv.org/abs/2512.16818)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: DenseBEV 直接将 BEV 特征网格单元作为锚点（查询），配合 BEV-NMS 和混合时序建模，实现端到端的多相机 3D 目标检测，在 nuScenes 与 Waymo 上显著提升 NDS/mAP，尤其对小目标有效。


<details>
  <summary>Details</summary>
Motivation: 现有 BEV-transformer 检测多依赖随机查询或外部辅助检测生成查询，导致训练效率低、查询与 BEV 时空信息脱节，且大量查询带来注意力计算扩展性差；需要一种更直观、高效、可扩展且能充分利用时序 BEV 信息的锚点/查询生成方案。

Method: 1) 将稠密 BEV 特征栅格的每个 cell 直接作为锚点/对象查询，形成两阶段锚点生成流程；2) 引入基于 BEV 的 NMS，在训练时仅对未被抑制的候选传递梯度，缓解大规模查询的注意力开销并去除后处理；3) 以 BEV 编码器（如 BEVFormer）输出作为查询，天然包含时序信息；4) 结合先验检测结果，提出混合式时序建模，进一步增强时序一致性与检测性能。

Result: 在 nuScenes 上相较基线获得稳定、显著的 NDS 和 mAP 提升，即使用更稀疏的 BEV 网格也优于基线；对小目标尤为有效，行人检测 mAP 提升 3.8%；在 Waymo 上 LET-mAP 提升 8%，并以 60.7% 的 LET-mAP 刷新 SOTA，较上一最佳高 5.4%。

Conclusion: 将 BEV 栅格直接作为锚点并结合 BEV-NMS 与混合时序建模，能在不依赖外部查询生成器的情况下实现高效、端到端的多相机 3D 检测，显著提升精度与小目标性能，具有良好的扩展性与实践价值。

Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.

</details>


### [87] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 论文评估YOLOv8变体在车牌检测与字符识别上的表现，提出用YOLOv8-N做LPR、YOLOv8-S做字符识别并配合按x轴排序的字符序列化方法，在两数据集上实现高精度与高效率，适合边缘设备实时部署。


<details>
  <summary>Details</summary>
Motivation: 在复杂多变的道路环境中，传统车牌检测与识别方法难以兼顾实时性与稳定高精度。为支撑智能交通系统在边缘设备上的落地，需要验证轻量级目标检测模型在LPR与字符识别上的有效性并优化端到端流程。

Method: 采用YOLOv8多个变体，分别训练并评估车牌检测（LPR）与字符识别两任务；使用两套数据集；提出基于检测框x轴坐标的自定义字符排序策略；构建以YOLOv8-N用于LPR、YOLOv8-S用于字符识别的优化流水线。

Result: LPR任务上，YOLOv8-N达到precision 0.964、mAP50 0.918；字符识别任务上，YOLOv8-S达到precision 0.92、mAP50 0.91；所提字符排序方法能有效完成字符序列化；整体方案在保证计算效率的同时维持较高准确率。

Conclusion: 组合使用YOLOv8-N（LPR）与YOLOv8-S（字符识别）并配合x轴排序，可在边缘设备上实现高效高准确的车牌识别，为智能交通系统的实际部署提供稳健方案。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [88] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 提出一种紧凑的单视角胸片到文字模型：用冻结的DINOv3-ViT作编码器、GPT‑2作解码器，并在解码端引入“分层高斯平滑”的解剖注意力，利用肺/心脏分割掩膜引导生成。无需多视角、元数据或大规模多模态预训练，却在MIMIC‑CXR上显著提升CheXpert与RadGraph指标。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA（如MAIRA‑2、MedPaLM‑M）依赖大规模多模态训练、临床元数据和多视图，代价高、可及性差。作者希望在资源受限环境下，仅凭单张正位胸片生成可靠的“Findings”，同时提升空间对齐与临床相关性。

Method: 采用冻结的DINOv3 ViT提取图像特征；GPT‑2作为文本解码器。在解码端加入层级式解剖注意力：由肺与心脏分割掩膜经多尺度高斯平滑形成分层权重，作为attention bias融入解码层，强化对解剖相关区域的关注且不增加可训练参数。仅用图像条件，无元数据、多视图或大规模多模态对齐。

Result: 在MIMIC‑CXR上，CheXpert五大关键病变的Macro‑F1从0.083→0.238（+168%），Micro‑F1从0.137→0.337（+146%）；14项观测总体从0.170→0.316（+86%）。RadGraph F1提升9.7%，显示结构一致性与空间落地更好。

Conclusion: 小型、纯图像条件的架构，借助解码端的解剖先验即可显著提升胸片报告“Findings”的临床相关性与结构连贯性，降低资源门槛。说明在报告生成中，解剖引导的attention是高效且实用的设计。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [89] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch提出首个“在野”第一人称全手触觉数据集，含视频-触觉-手部姿态同步数据与标注，并据此构建检索/分类基准，展示触觉在抓握理解与跨模态对齐中的价值，推动多模态自我中心感知与具身学习。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心视觉很难获知手与环境接触的时间、位置和力度；耐用可穿戴触觉传感器稀缺，且缺乏将第一人称视频与全手触觉对齐的“在野”数据集，限制了感知、行动理解与机器人操作研究。

Method: 构建OpenTouch数据集：5.1小时同步视频-触觉-手部姿态数据，外加2,900条带细粒度文本标注的片段；基于该数据提出检索与分类基准任务，用于评估触觉信号在抓握理解、跨模态对齐与从视频检索触觉的能力。

Result: 实验表明：触觉信号是紧凑而强有力的抓握理解线索；引入触觉可显著增强视觉-触觉-姿态的跨模态对齐；在真实场景视频查询中可可靠地检索出相应触觉模式。

Conclusion: 发布带注释的视觉-触觉-姿态数据与基准，有望推动多模态自我中心感知、具身学习以及高接触丰富度的机器人操作研究。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [90] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 论文指出当前T2I评测存在“基准漂移”：静态评测的裁判与题库难以跟上模型能力升级，导致分数与人评逐渐偏离。作者实证显示热门基准GenEval已显著漂移，并提出更具覆盖与组合理解难度的新基准GenEval 2，以及更细粒度、据称更稳健的人对齐评价方法Soft-TIFA。


<details>
  <summary>Details</summary>
Motivation: 自动化评测T2I模型依赖机器裁判与固定题库，但模型快速进步会使裁判和题库不再合适，产生与人评偏离。作者发现社区广泛使用的GenEval出现显著误判与饱和，亟需更难、更广覆盖且更稳健对齐的新评测方案。

Method: 1) 定义并分析“基准漂移”现象；2) 以GenEval为例，通过对当代T2I模型的大规模实验与人评对比，量化其与人类判断的偏差；3) 设计GenEval 2：扩展原子视觉概念覆盖、提升组合度与难度；4) 提出Soft-TIFA：将对视觉原语的判定以软方式聚合成整体评分，并与人评、以及较“整体化”评判器（如VQAScore）进行对齐度对比。

Result: 发现GenEval对当前模型的绝对误差最高达17.7%，显示明显漂移与基准饱和；GenEval 2对现有模型更具挑战性；Soft-TIFA与人评的相关性更高，且相较整体型评判器更不易随时间漂移。

Conclusion: 静态、整体式的T2I评测易随模型演化产生漂移与饱和。GenEval已不再可靠；GenEval 2与Soft-TIFA在覆盖、组合性与人对齐方面更优，可能更耐久，但仍需持续审计与迭代，以避免长期漂移。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [91] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出RePlan：先规划后执行的指令图像编辑框架，在复杂指令与杂乱场景（IV-Complexity）下显著提升区域精度与整体保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑在面对复杂指令与复杂/歧义场景时表现不稳，难以细粒度对齐编辑目标区域并保持多区域一致性，缺乏强推理与可控性评估基准。

Method: 采用“规划-执行”两阶段：1）视觉语言规划器将指令分解为逐步推理并显式对齐到目标区域；2）扩散式编辑器通过训练自由的注意力-区域注入机制执行修改，实现精确、可并行的多区域编辑，无需迭代修补（inpainting）。此外，使用基于GRPO的强化学习在仅1K条指令数据上强化规划器，提升推理正确性与输出格式可靠性；并构建IV-Edit基准评测细粒度定位与知识密集型编辑。

Result: 在IV-Complex场景与IV-Edit基准上，相比使用更大数据训练的强基线，RePlan在区域对齐精度与总体编辑保真度上持续领先；RL微量数据训练带来显著推理与格式稳定性增益。

Conclusion: 结合区域对齐规划与训练自由的扩散编辑实现复杂场景下的高精度多区域编辑；小规模RL可有效增强规划能力；IV-Edit提供面向复杂编辑的标准化评测。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [92] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: Pixel Seal提出一种更稳健且不可感知的图像/视频隐形水印框架，通过摒弃像素级感知损失、分阶段训练与高分辨率自适应，显著提升鲁棒性与不可见性，并可高效扩展到视频。


<details>
  <summary>Details</summary>
Motivation: 现有隐形水印方法在三方面受限：1) 依赖MSE/LPIPS等代理感知损失，仍会产生可见伪影，无法真实反映人类感知；2) 鲁棒性与不可感知性的目标相互冲突，训练不稳定、需大量超参调优；3) 模型在高分辨率图像/视频上扩展困难，出现上采样伪影，导致鲁棒性与不可见性下降。

Method: - 训练范式：采用仅对抗（adversarial-only）训练，去除像素级不可感知损失，以对抗判别器直接约束不可见性。
- 训练流程：三阶段训练，解耦鲁棒性与不可见性，稳定收敛。
- 高分辨率适配：引入JND（可察觉差异阈值）驱动的水印强度衰减，以及在训练中模拟推理时的上采样/缩放过程，消除高分辨率与上采样带来的伪影。
- 视频扩展：使用时间维度水印池化（temporal watermark pooling），实现从图像到视频的高效迁移。

Result: 在多种图像类型与广泛变换（攻击）下，Pixel Seal在鲁棒性与不可感知性上均优于现有SOTA；在高分辨率场景减少上采样伪影；视频任务中通过时间池化实现有效适配并保持性能优势。

Conclusion: 通过对抗式训练、阶段化收敛与高分辨率自适应，Pixel Seal在图像与视频隐形水印上实现新的SOTA，兼顾强鲁棒性与真正不可感知性，并具备实际可扩展性以支持真实世界溯源需求。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [93] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 提出ReMeDI-SAM3：在无需训练的前提下，通过相关性过滤、分段插值扩容记忆、特征重识别与时间投票，显著提升内镜手术场景的视频器械分割鲁棒性，EndoVis17/18零样本下较SAM3分别+7%/+16% mcIoU，并超越多种训练型方法。


<details>
  <summary>Details</summary>
Motivation: 内镜视频中器械分割受遮挡、快速运动、强反光与长期再入场影响，导致视频分割模型易误更新记忆、记忆容量受限且遮挡后身份恢复差。现有强基线SAM3在手术场景下受这些问题制约，需在不额外训练的条件下提升稳健性与身份一致性。

Method: 基于SAM3提出训练免微调的记忆增强框架ReMeDI-SAM3：1) 相关性感知的记忆过滤，并引入“遮挡感知记忆”专门保存遮挡前帧，避免错误写入；2) 分段（piecewise）插值策略，扩展有效记忆容量、降低频繁存取导致的污染；3) 特征级重识别模块+时间投票，针对遮挡后器械再出现进行身份消歧与一致性恢复。

Result: 在EndoVis17与EndoVis18数据集的零样本设定下，相较原始SAM3，mcIoU绝对提升约7%与16%；整体表现超过多种需训练的既有方法。

Conclusion: 通过相关性过滤、记忆扩容与特征重识别的协同，ReMeDI-SAM3有效抑制误差累积并在遮挡后实现可靠身份恢复，成为在手术内镜场景中无需训练即可显著优于基线与部分训练式方法的实用方案。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [94] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 提出M-PhyGs，从短视频中同时分割多材料部件并估计其连续介质力学参数（含重力），并在新建Phlowers花朵交互数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实物体常由多种材料与复杂几何构成，现有从视觉估计物理参数的方法多假设单一材料、预学习动力学或简化拓扑，难以泛化到自然环境中的复杂对象（如花朵）。

Method: 提出Multi-material Physical Gaussians（M-PhyGs）：从自然场景的短视频输入，联合完成材料相似性分割与各材料连续介质参数估计，并显式建模重力；为高效与稳定优化，引入级联的3D与2D损失，以及时间mini-batching策略；构建Phlowers数据集（人与花交互）用于评估。

Result: 在Phlowers数据集上进行实验，显示M-PhyGs能够准确恢复多材料物理参数及分割，组件（级联损失、时间mini-batch）的有效性通过消融得到验证。

Conclusion: M-PhyGs能在自然场景视频中高效地联合估计多材料分割与物理参数，适用于复杂几何和材料组成的花朵等对象，并在新数据集上证明准确与有效。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [95] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: 提出LinkedOut：从视频帧中提取VLLM的知识化视觉token，支持多视频历史、去语言瓶颈、低延迟顺序推理，用于视频推荐并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM在视频理解上强，但用于推荐受限：仅解码式生成推理慢；接口不支持多视频输入；输出限制为语言丢失像素级细节。缺少既保留细粒度视觉细节又能利用世界知识的表示。

Method: 提出LinkedOut表示：用VLLM在原始帧上、在可提示查询和可选辅助模态引导下，提取语义落地、知识感知的视觉token；设计跨层知识融合的MoE，自动选择VLLM特征的抽象层级，支持个性化、可解释和低延迟；接口支持多视频历史，去除语言生成瓶颈，实现快速顺序推理。

Result: 在标准视频推荐基准上取得SOTA，且无需手工标签即可直接从原始帧工作；解释性研究与消融显示跨层多样性与分层融合显著提升效果与可解释性。

Conclusion: 通过LinkedOut将VLLM的世界知识以视觉token形式直接对接到推荐任务，实现多视频、低延迟、可解释、无需标签的端到端方案，为利用VLLM先验与视觉推理赋能下游视觉任务提供了实用路径。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [96] [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/abs/2512.16893)
*Kaiwen Jiang,Xueting Li,Seonwook Park,Ravi Ramamoorthi,Shalini De Mello,Koki Nagano*

Main category: cs.CV

TL;DR: 将2D扩散模型的高保真表情与3D显式表征的高速一致性结合：用扩散模型蒸馏到前馈编码器，单张野外人脸→可即时、三维一致且富表达的动画表示；采用轻量局部融合取代重型全局融合，达107.31 FPS并保持SOTA级质量。


<details>
  <summary>Details</summary>
Motivation: 2D视频扩散动画质量高但缺乏3D一致性且推理慢；3D-aware前馈方法快且一致性好但表情细节差。现实应用（数字孪生、远程临场）需要同时兼顾速度、3D一致性与表情表现力。

Method: 以2D扩散式人像动画方法为教师，蒸馏到一个前馈编码器。输入单张野外图像，生成与面部3D表征解耦的动画表示；运动从数据中隐式学习，无需预定义参数模型。融合3D结构与动画信息时，采用高效轻量的“局部融合”策略，避免多层全局注意力等高计算开销机制。基于显式3D表示（NeRF/高斯泼溅等）实现3D一致性与高速。

Result: 在动画与姿态控制任务上达到107.31 FPS；动画质量与SOTA方法相当，并优于其他在速度与质量间折中的设计。

Conclusion: 提出一种将2D扩散模型优势蒸馏进3D-aware前馈系统的框架，实现单图到三维一致、快速且富表情的可动画表示；轻量局部融合在保证表现力的同时显著提高效率，适用于实际场景如数字孪生与远程临场。

Abstract: Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d

</details>


### [97] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait 是一个端到端视频扩散 Transformer，用正规化的表情特征对齐与动态滑窗推理，实现长人像动画的身份一致性，并通过高阶潜变量外推将去噪步数跳跃，推理最高加速约6倍。


<details>
  <summary>Details</summary>
Motivation: 现有面向长时长人像（long-portrait）的扩散式加速方法难以兼顾身份一致性（ID consistency）与速度，常出现面部漂移、过渡不平滑或推理耗时高的问题。需要一种既保持身份稳定、又能高效生成无限时长视频的方案。

Method: 1) 先用现成模型提取与身份无关的面部表情特征；2) 提出“归一化表情块”（Normalized Facial Expression Block），按均值/方差归一化并与扩散潜变量对齐，增强身份稳定建模；3) 推理采用动态滑动窗口并在重叠区加权融合，保证跨窗口平滑与ID一致；4) 在每个上下文窗口内，依据特定时间步的潜变量变化率与扩散层间导数幅值比，使用当前时刻的高阶潜变量导数外推未来潜变量，跳过若干去噪步，实现最高约6×加速。

Result: 在基准数据集上，定性与定量实验均显示：长视频生成更平滑、ID一致性更高；同时在不显著牺牲质量的情况下实现最高约6倍推理加速。

Conclusion: 通过表情特征归一化对齐、动态滑窗融合与高阶潜变量外推，FlashPortrait兼顾身份稳定与高效推理，可生成无限长度的人像动画，并在速度与质量上优于现有方法。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [98] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 提出Alchemist：一种用于T2I训练数据自动筛选的可扩展元梯度框架，通过评分与剪枝两阶段，从大规模文本-图像对中选出对训练最有益的子集，在仅用50%数据时优于全量训练。


<details>
  <summary>Details</summary>
Motivation: T2I模型（Imagen、Stable Diffusion、FLUX）受训练数据质量与冗余限制，导致画质下降、训练不稳定、计算低效。现有方法要么人工挑选成本高，要么基于单维启发式分数，难以全面衡量样本影响；LLM中有元学习式数据选择，但图像模态缺乏对应方案。

Method: 提出Alchemist：以数据为中心、基于元梯度的自动数据选择框架。两阶段：1）数据评分：训练轻量级rater，利用梯度信息并结合多粒度感知，估计每个样本对模型优化的影响；2）数据剪枝：采用Shift-Gsampling策略，从大规模数据中挑选信息量高的子集以高效训练。

Result: 在合成与网络抓取数据上，Alchemist稳定提升生成质量与下游任务表现；用其选取的50%数据训练可超过使用全量数据训练的效果。

Conclusion: Alchemist是首个面向T2I训练、自动化且可扩展的基于元梯度的数据选择框架，能在不增加数据量的情况下提升视觉质量与效率，并优于传统手工或启发式筛选。

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [99] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出VIVA：结合VLM引导编码与相对奖励优化的可扩展指令式视频编辑框架，提升对复杂真实指令的泛化、时空一致与编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式视频编辑多依赖“简单操作+配对数据”监督，难以处理多样且复杂的自然语言指令，导致指令遵循性与泛化不足；同时需要在保持内容与时间一致性的前提下生成美观结果。

Method: 1) VLM引导师：将文本指令、源视频首帧与可选参考图编码为视觉对齐的指令表示，向扩散Transformer提供细粒度空间与语义条件；2) Edit-GRPO：将群相对策略优化引入视频编辑后训练，以相对奖励直接优化指令遵循、内容保真与美学质量；3) 合成数据管线：合成多样高保真“基础编辑操作-视频-指令”配对数据，以支撑训练与泛化。

Result: 在广泛实验中，VIVA在指令跟随、泛化能力与编辑质量上均优于现有SOTA方法；能更好保持内容与时间一致并处理更复杂真实世界指令。

Conclusion: VIVA通过VLM引导表示与相对奖励后训练，结合合成数据扩展，显著缩小指令式视频编辑的泛化差距，为复杂真实指令下的高质量视频编辑提供了有效方案。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [100] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出EgoMAN数据集与模型：用视觉-语言推理驱动3D手部轨迹生成，通过“轨迹token”接口联通推理与动作，并分阶段训练，实现更准确、具交互阶段感知且可泛化的手部6DoF轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有3D手轨迹预测受两大限制：1) 数据集将运动与语义监督割裂，缺乏交互阶段与语义-空间-运动一体化标注；2) 模型中推理与动作生成耦合弱，难以让高层语义推理有效指导具体运动。

Method: 1) 构建EgoMAN大规模自视角数据集：包含219K个6DoF手轨迹与300万条结构化QA，覆盖语义、空间与运动推理，并提供交互阶段标注；2) 提出EgoMAN模型：以“推理到运动”框架，将视觉-语言推理模块与运动生成模块通过轨迹token接口对齐；3) 采用渐进式训练，使推理信号与运动动力学逐步对齐，产出阶段感知的轨迹。

Result: 模型能生成准确且具阶段感知的6DoF手部轨迹，并在真实场景中表现出较强的泛化能力，优于既有方法（摘要未给出具体数值）。

Conclusion: 通过数据与方法的双重改进，实现语义/阶段感知的手轨迹预测：结构化多模态监督与推理-动作的显式联接，有助于提升精度与跨场景泛化。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [101] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 提出SceneDiff Benchmark与SceneDiff方法，用多预训练3D/分割/图像编码模型对多视角场景的新增、移除、移动对象进行无训练检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多时刻、不同视角拍摄的同一场景中，视角变化常导致物体被误判为变化。应用如机器人整理与施工监测需要鲁棒的多视角对象级变化检测，但缺乏带实例标注的基准与有效方法。

Method: 构建包含350对多样视频、数千变化实例的多视角变化检测基准SceneDiff Benchmark。提出无需训练的SceneDiff方法：1) 将两次采集对齐到3D；2) 进行对象区域提取（分割/实例级）；3) 对比区域的空间与语义特征（由预训练3D、分割与图像编码模型提供）以判定新增/移除/移动；在多基准上直接推理。

Result: 在多视角与双视角基准上取得显著领先，报告相对AP提升分别为94%与37.4%。

Conclusion: 多视角对象级变化检测可通过3D对齐与多模态特征对比在无训练设置下实现强鲁棒性；提供了首个对象级多视角基准与强基线，基准与代码将开源。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [102] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 提出MomaGraph：面向家用移动操作机器人的统一场景图表示，配套首个大规模任务驱动场景图数据集与评测基准，并训练了基于强化学习的7B多模态模型MomaGraph-R1，在零样本任务规划与场景理解上SOTA（基准准确率71.6%，较最佳开源基线+11.4%），可泛化到公开基准与真实机器人。


<details>
  <summary>Details</summary>
Motivation: 现有家庭机器人需要同时导航与操作，但场景表示分割空间与功能关系、忽略对象状态与时间更新、缺少与任务相关的信息；同时缺乏适配的数据与系统性评测来推动统一、可操作的场景图研究。

Method: 1) 提出MomaGraph：将空间-功能关系统一到同一场景图，并显式建模可交互的部件级元素与对象状态/时序更新；2) 构建MomaGraph-Scenes数据集：大规模、任务驱动、富语义的家庭环境场景图标注；3) 设计MomaGraph-Bench：覆盖从高层规划到细粒度理解的六类推理能力评测；4) 训练MomaGraph-R1：基于7B视觉-语言模型，利用MomaGraph-Scenes进行强化学习训练，使其预测面向任务的场景图，并在“先建图后规划（Graph-then-Plan）”框架下充当零样本任务规划器。

Result: 在MomaGraph-Bench上取得71.6%准确率，较最佳开源基线提升11.4%；在多个公共基准上具备良好泛化；在真实机器人实验中有效迁移，验证实用性。

Conclusion: 统一的空间-功能-部件级场景图表示与任务驱动数据/评测能够显著提升移动操作任务的感知-推理-规划闭环能力；MomaGraph-R1作为零样本任务规划器表现SOTA并具备跨基准与真实世界的迁移潜力。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [103] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SFTok提出一种多步离散图像tokenizer，通过自驱式重建与去偏-拟合训练，在仅64个token/图像下，重建与生成质量达SOTA（ImageNet rFID 1.21、class-to-image gFID 2.29）。


<details>
  <summary>Details</summary>
Motivation: 当前多模态高分辨率生成依赖图像token化以降维，提高效率。离散tokenizer更契合自回归生成，但重建质量落后于连续表征，且多步推理与训练存在不一致，限制实际应用。

Method: 提出SFTok：一种离散tokenizer，采用多步迭代重建机制。核心包括：1）自强制（self-forcing）引导的视觉重建，在生成过程中利用模型自身预测来逐步细化；2）去偏与拟合的训练策略，缓解训练-推理不一致，提升多步过程的稳定性与精度；在高压缩（每图64 token）设定下进行训练与评估。

Result: 在ImageNet上达到最优或同级SOTA的重建指标，报告rFID=1.21；在类别到图像生成任务上，gFID=2.29，显示离散表征在高压缩场景下的强竞争力。

Conclusion: 通过多步迭代与自引导训练，SFTok弥合了离散tokenizer在重建质量上的劣势与训练-推理不一致问题，使其在高压缩下取得SOTA，为多模态与自回归生成提供更实用的离散表征方案。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [104] [Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation](https://arxiv.org/abs/2512.16913)
*Xin Lin,Meixi Song,Dizhe Zhang,Wenxuan Lu,Haodong Li,Bo Du,Ming-Hsuan Yang,Truong Nguyen,Lu Qi*

Main category: cs.CV

TL;DR: 提出一个可泛化多场景距离的全景度量深度基础模型，结合多源数据与三阶段伪标签管线，并在模型中加入范围掩码头与以锐度/几何为中心的优化，实现对真实复杂场景的稳健零样本度量深度预测。


<details>
  <summary>Details</summary>
Motivation: 现有全景深度估计在跨室内/室外、真实/合成和远近距离变化时易退化，标注成本高、域间差异大，缺乏统一且稳健的度量深度基础模型。

Method: 数据侧：融合公开数据、UE5高质量合成、文生图合成与网络真实全景图；通过三阶段伪标签筛选与校正，缩小室内/室外与合成/真实域差异。模型侧：以DINOv3-Large为骨干，引入可插拔的范围掩码头以适配不同距离分布；设计以锐度为中心的优化提升边缘与细节、以几何为中心的优化保证多视几何一致性。

Result: 在Stanford2D3D、Matterport3D、Deep360等基准上取得强性能和良好零样本泛化，尤其在多样真实场景中实现稳定的度量深度预测。

Conclusion: 结合数据闭环构建与模块化模型设计，可显著缓解域间与距离变化带来的挑战，形成对真实世界稳健的全景度量深度基础模型。

Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}

</details>


### [105] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出UniStereo数据集与StereoPilot模型，统一并高效地实现单目到双目视频转换，摆脱传统DWI流程的深度与修复误差累积，兼容并优化平行/汇聚两种立体格式，取得更高保真度与效率。


<details>
  <summary>Details</summary>
Motivation: 立体显示需求激增，但3D内容生产昂贵，自动单目转双目依赖的“深度-重映射-修复”多阶段流程存在误差传播、深度歧义与不同立体格式（平行vs汇聚）不一致的问题，需要统一数据与更稳健高效的方法。

Method: 1) 构建首个覆盖平行与汇聚两种立体格式的大规模统一数据集UniStereo，用于公平评测与鲁棒训练；2) 提出StereoPilot：前馈式端到端目标视图合成模型，不显式预测深度、无扩散迭代；包含可学习的“域切换器”以适配两种立体格式，并引入循环一致性损失以提升跨视图一致性。

Result: 在广泛实验中，StereoPilot在视觉保真与计算效率上均显著优于现有方法；验证了在两种立体格式上的适配性与一致性改进。

Conclusion: 统一数据（UniStereo）与端到端前馈生成（StereoPilot）能够有效替代传统DWI流程，减少误差累积并兼容不同立体格式，推进单目到双目视频转换的性能与实用性。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [106] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出AdaTooler-V：一种能自适应决定是否调用视觉工具的多模态大模型，结合基于工具收益自适应加权的RL算法AT-GRPO和两套训练数据，实现更高效、更准确的多模态推理，在多项基准上超越现有方法与商用模型。


<details>
  <summary>Details</summary>
Motivation: 开源MLLM在多模态CoT中常“盲目用工具”，即便无需工具也会调用，导致推理开销上升且性能下降。需要一种机制仅在工具真正带来收益时才调用，实现精益的工具使用策略。

Method: 1) 设计AdaTooler-V，可在推理时判定视觉问题是否“需要工具”，实现自适应工具使用；2) 提出AT-GRPO强化学习：依据每个样本的Tool Benefit Score自适应缩放奖励，鼓励只在工具带来增益时调用；3) 构建两套数据：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于可验证奖励的RL训练，覆盖单图、多图与视频。

Result: 在12个基准上展现强推理能力；在高分辨率基准V*上，AdaTooler-V-7B达89.8%准确率，超过GPT-4o与Gemini 1.5 Pro；整体推理更高效（减少不必要的工具调用）。

Conclusion: 自适应工具使用结合基于收益加权的RL训练可显著降低无效工具调用、提升多模态推理精度与效率；所提出模型与数据在多任务上取得SOTA或超越商用模型的表现，并已全部开源。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [107] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出DVGT：从未配准多视角序列直接重建全局稠密3D点图与相机位姿，适配任意车载相机，无需显式几何先验与后对齐，跨多驾驶数据集显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要稳健、通用、可度量尺度的稠密3D几何感知，但现有方法依赖精确相机参数或传感器对齐，难以跨场景/相机配置泛化，缺少面向驾驶的统一模型。

Method: 以DINO作为视觉骨干提取特征；通过交替的三类注意力建模跨图像几何关系：帧内局部注意、跨视角空间注意、跨时间注意；多头解码器在首帧自车坐标系下直接预测全局稠密点图，并同时回归每帧自车位姿；训练时不使用显式3D几何先验或精确内外参。

Result: 在nuScenes、OpenScene、Waymo、KITTI、DDAD的混合训练设置下，对多种场景与相机布置具有强泛化能力，指标显著优于现有模型；能够直接输出度量尺度的几何结果，无需依赖外部传感器进行后对齐。

Conclusion: DVGT作为面向驾驶的通用视觉几何Transformer，可从未配准视频中端到端重建全局稠密3D点图和位姿，摆脱相机参数依赖并提升跨数据集性能，为多摄像头异构系统提供灵活几何感知方案。

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [108] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V 提出一种简单高效的指令式视频编辑框架：用现成编辑专家与快速逆过程构造多样视频对、将图像编辑迁移到视频并引入过渡监督；模型以预训练文生视频为底座，仅用序列拼接条件与轻量LoRA微调；统一时空控制用单一掩码并可选参考图像，支持多种输入组合，达到SOTA并超越商用系统。


<details>
  <summary>Details</summary>
Motivation: 视频编辑仍落后于图像编辑，主要瓶颈在一致性、可控性与泛化。现有方法要么数据昂贵、要么模型复杂且难训练。作者希望以更低成本的数据构造与更简化的模型设计，提升指令驱动的视频编辑质量与易用性。

Method: 数据：1) 组合现有图像/视频编辑专家并配以快速逆过程，生成多样视频编辑对；2) 将图像编辑对通过单帧监督与共享仿射运动伪配对“提升”为视频；3) 挖掘密集字幕片段形成视频对；4) 增加过渡监督教会编辑随时间展开。模型：基于预训练Text-to-Video，观察其自带编辑能力，采用简单的条件序列拼接并用轻量LoRA微调。控制：以单一掩码统一时空控制，并支持可选参考图像。输入模式灵活：视频+文本 / 视频+掩码+文本 / 视频+掩码+参考+文本。

Result: 在多种视频编辑基准上取得SOTA，编辑质量、一致性与可控性优于同期学术与商用系统。框架在不同输入设置下均表现稳健。

Conclusion: 通过数据构造、简化的架构与统一控制机制，EasyV2V以低改动实现强大的指令式视频编辑，展示了预训练文生视频模型经轻量微调即可成为高质量视频编辑器的可行性。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [109] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM 通过训练一个“审计员”式多模态模型，主动生成能引发模型分歧的问答与反事实图像，系统性挖掘与修复多模态大模型的失败模式；基于这些无标注发现再微调，可显著提升多项基准表现，甚至使3B模型超越28B模型。


<details>
  <summary>Details</summary>
Motivation: 现有对MLLM的评测缺乏可解释性，难以显露模型间关键能力差距；随着纯数据扩增收益递减，需一种能主动暴露弱点并指导改进的评估与数据生成机制。

Method: 提出AuditDM：将一个MLLM经强化学习微调为“审计员”，其目标是生成能最大化目标模型间分歧的挑战性问题与反事实图像；审计员产出的多样、可解释实例用于发现失败类型，并作为无需人工标注的数据对目标模型进行再训练/微调以纠正弱点。

Result: 在Gemma-3、PaliGemma-2等SOTA模型上，审计员发现20+类失败模式；基于这些数据微调后，所有模型在16项基准上稳定提升，并出现3B模型超越其28B版本的案例。

Conclusion: 面向分歧的自动审计能系统揭示并修复MLLM弱点，是替代单纯数据扩增的高效路径；有助于提升可解释性、效率与小模型性价比。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [110] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 本文提出以生成式预训练思路做视觉自监督：直接预测“下一块的嵌入”而非像素或对比学习特征，方法称为NEPA，在ImageNet-1K上用ViT简单预训练即获得强劲下游表现，并良好迁移到分割任务。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自监督多依赖像素重建、离散token化、对比损失或复杂头部设计，训练与下游任务存在目标错配与工程复杂度高的问题。语言中的生成式预训练显示“预测下一个表示”可学到强泛化模型，促使作者探索视觉中是否可用“预测未来嵌入”替代传统表征学习。

Method: 提出Next-Embedding Predictive Autoregression（NEPA）：将图像划分为patch并获得其嵌入，采用因果mask仅以过去patch为条件，预测下一patch的嵌入；对目标嵌入使用stop-gradient避免塌陷与捷径学习；以Transformer为骨干，不使用像素重建、离散token、对比损失或任务特定头，仅以“下一嵌入预测”为唯一训练目标进行预训练。

Result: 在ImageNet-1K上，仅用该目标预训练后微调：ViT-B达83.8% top-1，ViT-L达85.3% top-1；在ADE20K语义分割上也有效迁移，显示良好的通用性与可扩展性。

Conclusion: 从“学表征”转向“学模型”的嵌入级生成式预训练在视觉中可行且高效；NEPA以最小设计复杂度实现强性能与跨任务迁移，提示该范式具有可扩展、简单、并可能跨模态通用的潜力。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [111] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出“生成式再对焦”：先去模糊得全清晰，再可控合成虚化；半监督训练结合合成对和真实散景（用EXIF约束光学特性）；在去焦模糊、散景合成、再对焦上达SOTA，并支持文本与自定义光圈形状控制。


<details>
  <summary>Details</summary>
Motivation: 单幅再对焦困难：需同时恢复清晰内容与真实散景，现有方法依赖全清晰输入或纯模拟数据，且光圈可控性差，模拟器难涵盖真实镜头像差与散景形态。

Method: 两阶段流水线：DeblurNet从多种输入恢复全清晰图；BokehNet基于清晰图生成可控散景（光圈大小/形状）。训练采用半监督：使用模拟器生成的成对数据进行有监督学习，同时引入未配对的真实散景图像，并利用照片EXIF元数据（焦距、光圈、对焦距离等）作为条件与约束，使模型学习真实光学特征。支持文本引导与自定义光圈形状。

Result: 在去焦去模糊、散景合成及再对焦基准上取得领先指标与视觉质量；能从非全清晰输入恢复并生成自然散景；实现可控光圈、文本引导和自定义光圈形状的效果。

Conclusion: 生成式再对焦通过半监督融合真实散景统计与合成对数据，解决了对全清晰依赖与模拟域差距，实现端到端的可控再对焦并达SOTA；方法通用、可扩展到更多相机/光圈先验与交互控制。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [112] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个可通过提示进行世界事件生成的多模态框架，将文本、运动轨迹与参考图像结合，生成具有可控多主体互动、物体出入场与外观一致性的连贯视频。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动或仅轨迹控制的视频生成方法难以同时实现语义意图、精确运动/时间/可见性控制和外观一致性的统一。需要一种在语义、运动和外观三方面均可控的生成框架，以支持更丰富、交互式的世界事件模拟。

Method: 提出一个多模态管线：以自然语言描述语义意图；以轨迹编码主体的运动、时序与可见性（进出场）；以参考图像约束主体外观。将三者融合驱动视频生成，支持多主体交互、参考引导的外观控制与反直觉事件。强调时序连贯与物体身份/场景在临时遮挡后的保持。

Result: 生成的视频在时间连贯性和“涌现一致性”（即使临时消失后仍保持身份与场景一致）方面表现良好，能实现多主体互动、物体进出、参考外观控制及复杂事件。

Conclusion: WorldCanvas将世界模型由被动预测推进为可交互、用户塑形的模拟器，提供对语义、运动与外观的联合可控视频事件生成；项目页提供更多展示。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>
