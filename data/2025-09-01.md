<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving](https://arxiv.org/abs/2508.21080)
*Ali K. AlShami,Ryan Rabinowitz,Maged Shoman,Jianwu Fang,Lukas Picek,Shao-Yuan Lo,Steve Cruz,Khang Nhut Lam,Nachiket Kamod,Lei-Lei Li,Jugal Kalita,Terrance E. Boult*

Main category: cs.CV

TL;DR: 本摘要介绍了2COOOL工作坊：聚焦自动驾驶中的“标签外/新奇场景”与危害处理，将于ICCV 2025举行，旨在推进异常/开放集/开放词汇等方法以提升安全与泛化。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉与多传感器融合推动了自动驾驶，但现实部署仍被“新奇/未见场景与危害”所阻碍——这是导致尚未实现“完全安全自动驾驶”的关键原因，需要专门平台汇聚研究力量。

Method: 通过举办专题工作坊：汇集学术与产业，围绕新奇处理展开议题与基准，包括OOD危害检测、用于危害理解的视觉-语言模型、新基准与方法学、安全驾驶实践，以及相关方向（异常检测、开放集识别、开放词汇建模、领域自适应）。

Result: 预期产出为推动社区建立更好的评价基准、方法和系统，促进跨界交流与协作；基于首届（WACV 2025）成功经验，扩大影响与参与度。

Conclusion: 2COOOL旨在成为自动驾驶“新奇/危险情境”研究的核心平台，借助多学科方法提升危害规避与系统安全性，加速迈向更可靠的实际部署。

Abstract: As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.

</details>


### [2] [Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images](https://arxiv.org/abs/2508.21088)
*Alireza Golkarieh,Kiana Kiashemshaki,Sajjad Rezvani Boroujeni*

Main category: cs.CV

TL;DR: 利用全景牙片X光，比较自建CNN、CNN+传统分类器混合、以及微调预训练模型。5折交叉验证下，混合的CNN+随机森林最佳，准确率85.4%，优于自建CNN(74.3%)和最佳预训练VGG16(82.3%)。表明CNN特征+集成分类器在区分相似病灶上更稳健，但仍需更大数据与临床验证。


<details>
  <summary>Details</summary>
Motivation: 牙科全景片包含多种病变特征（补牙、龋齿、种植体、阻生牙），人工判读耗时且主观性强。现有深度学习方法在牙科领域受限于数据规模、类别相似性与临床可用性，需探索在中等数据规模下兼顾性能与可用性的实用方案。

Method: 构建包含1,512幅全景片、11,137条标注的多类别数据集；预处理与类别均衡；比较三类策略：1) 自建CNN端到端分类；2) CNN提取特征+传统分类器（如随机森林）；3) 微调预训练模型（VGG16、Xception、ResNet50）；采用5折交叉验证，评估准确率、精确率、召回率、F1。

Result: 混合模型（CNN+随机森林）最高准确率85.4%，显著优于自建CNN的74.3%；预训练模型中VGG16达82.3%，Xception与ResNet50次之。混合模型在区分形态相近类别时更有辨识力且稳定。

Conclusion: 在有限数据场景下，CNN特征结合集成分类器是一条实用的牙科自动诊断路径，能提升泛化与鲁棒性；但仍需更大规模数据、外部测试与临床验证来确证可用性与安全性。

Abstract: This study investigates deep learning methods for automated classification of
dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs
with 11,137 expert-verified annotations across four conditions fillings,
cavities, implants, and impacted teeth was used. After preprocessing and class
balancing, three approaches were evaluated: a custom convolutional neural
network (CNN), hybrid models combining CNN feature extraction with traditional
classifiers, and fine-tuned pre-trained architectures. Experiments employed 5
fold cross validation with accuracy, precision, recall, and F1 score as
evaluation metrics. The hybrid CNN Random Forest model achieved the highest
performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.
Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by
Xception and ResNet50. Results show that hybrid models improve discrimination
of morphologically similar conditions and provide efficient, reliable
performance. These findings suggest that combining CNN-based feature extraction
with ensemble classifiers offers a practical path toward automated dental
diagnostic support, while also highlighting the need for larger datasets and
further clinical validation.

</details>


### [3] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We observe that zero-shot appearance transfer with large-scale image
generation models faces a significant challenge: Attention Leakage. This
challenge arises when the semantic mapping between two images is captured by
the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing
Query-Query alignment to mitigate attention leakage and improve the semantic
alignment in zero-shot appearance transfer. Q-Align incorporates three core
contributions: (1) Query-Query alignment, facilitating the sophisticated
spatial semantic mapping between two images; (2) Key-Value rearrangement,
enhancing feature correspondence through realignment; and (3) Attention
refinement using rearranged keys and values to maintain semantic consistency.
We validate the effectiveness of Q-Align through extensive experiments and
analysis, and Q-Align outperforms state-of-the-art methods in appearance
fidelity while maintaining competitive structure preservation.

</details>


### [4] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

TL;DR: 提出ERTACache：通过离线残差画像、轨迹感知校正系数与闭式残差线性化，精确控制缓存误差，实现扩散模型推理最高2倍加速且质量无明显下降（甚至提升）。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推理需要多步迭代，计算开销大；特征缓存可复用中间结果但天真复用会带来质量下降。需要系统化地刻画缓存误差并设计能同时抑制误差来源的方案。

Method: 1) 误差分解：将缓存引入的累计误差分为特征漂移误差（缓存不准）与步长放大误差（固定时间步下误差传播）。2) ERTACache框架：- 离线残差画像：预分析各时间步可复用性，选取可缓存的步骤；- 轨迹感知校正系数：动态调整积分区间，适配当前采样轨迹，缓解误差放大；- 闭式残差线性化：对缓存诱发误差进行解析近似，提供校正项。三者结合以在“激进缓存复用”下维持精度。

Result: 在标准图像与视频生成基准上实现最高2倍推理加速，同时保持或提升视觉质量；在Wan2.1视频扩散模型上实现2倍加速且VBench仅有极小退化，基本保持基线保真度。

Conclusion: 通过理论误差刻画与针对性校正，ERTACache可在不牺牲（甚至提升）质量的前提下显著加速扩散模型采样，并具备在大规模SOTA视频扩散模型上的实用性。

Abstract: Diffusion models suffer from substantial computational overhead due to their
inherently iterative inference process. While feature caching offers a
promising acceleration strategy by reusing intermediate outputs across
timesteps, naive reuse often incurs noticeable quality degradation. In this
work, we formally analyze the cumulative error introduced by caching and
decompose it into two principal components: feature shift error, caused by
inaccuracies in cached outputs, and step amplification error, which arises from
error propagation under fixed timestep schedules. To address these issues, we
propose ERTACache, a principled caching framework that jointly rectifies both
error types. Our method employs an offline residual profiling stage to identify
reusable steps, dynamically adjusts integration intervals via a
trajectory-aware correction coefficient, and analytically approximates
cache-induced errors through a closed-form residual linearization model.
Together, these components enable accurate and efficient sampling under
aggressive cache reuse. Extensive experiments across standard image and video
generation benchmarks show that ERTACache achieves up to 2x inference speedup
while consistently preserving or even improving visual quality. Notably, on the
state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x
acceleration with minimal VBench degradation, effectively maintaining baseline
fidelity while significantly improving efficiency. The code is available at
https://github.com/bytedance/ERTACache.

</details>


### [5] [Video-LLMs with Temporal Visual Screening](https://arxiv.org/abs/2508.21094)
*Zheyu Fan,Jiateng Liu,Yuji Zhang,Zihan Wang,Yi R.,Fung,Manling Li,Heng Ji*

Main category: cs.CV

TL;DR: 提出“时间视觉筛选（TVS）”前端模块，先筛出视频关键片段并重写问题，使训练与推理都更聚焦，从而显著提升Video-LLM的时序理解与问答表现。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs因稀疏采帧与缺乏帧间推理监督，难以获取细粒度时序语义；与人类会拖动进度条聚焦关键时刻的自然行为不一致，导致推理负担重、问答对齐差。

Method: 提出TVS作为可插拔前端适配任务：1) 保留“关注关键”的视频时间段；2) 同步将原问题重写为更直接的表达，保持答案一致；3) 对所有可能答案保持不变性与一致性。TVS可无缝接入视频指令微调（训练）与视频问答（推理）流程，在训练阶段对齐问题与关键视觉信息，在推理阶段实现“问题感知”的片段聚焦与精简问题表示。同时构建首个TVS基准，并给出基线ReSimplifyIt。

Result: 在视频裁剪（trimming）上，ReSimplifyIt较以往相近任务方法F1提升0.47，并在问题重写上获得有竞争力表现；在整合TVS后，Video-LLMs在训练阶段相对提升7.33%，在推理阶段相对提升34.6%。

Conclusion: 通过在训练与推理前端进行时间信息筛选与问题重写，可优化推理负担与认知负荷，显著增强视频-语言理解与问答性能；TVS具备通用、模块化、可无缝集成的优势，并以基准与基线验证其有效性。

Abstract: Humans naturally perform temporal screening by dragging the progress bar and
focusing on salient temporal segments, but current Video Large Language Models
(Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse
frame sampling and insufficient inter-frame reasoning supervision during their
training. To address this, Inspired by well-established cognitive science
principles, we propose Temporal Visual Screening (TVS), a new task that
universally pre-processes video question answering and instruction tuning data
by: (1) retaining focus-critical video segments, (2) synchronously
reconstructing queries to their most direct form while preserving answer
consistency, and (3) keeping the invariance and consistency for any possible
answer. TVS is formulated as a modular front-end adapter task that can be
seamlessly integrated into both Video Instruction Tuning (training) and Video
Question Answering (inference) pipelines. TVS optimizes distribution of
reasoning burden and cognitive load; during training, it aligns queries with
focus-critical visual information; at inference, it enables query-aware segment
focus and streamlined query representations. In particular, we curate the first
benchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior
approaches on seemingly similar tasks by 0.47 in F-1 score on video trimming
while achieving competitive query rewriting performance. Experiments
demonstrate that incorporating TVS yields relative gains of 7.33% (training)
and 34.6% (inference), demonstrating the effectiveness of temporal information
screening for improving video-language understanding.

</details>


### [6] [ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments](https://arxiv.org/abs/2508.21096)
*Zhe Han,Charlie Budd,Gongyu Zhang,Huanyu Tian,Christos Bergeles,Tom Vercauteren*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Localisation of surgical tools constitutes a foundational building block for
computer-assisted interventional technologies. Works in this field typically
focus on training deep learning models to perform segmentation tasks.
Performance of learning-based approaches is limited by the availability of
diverse annotated data. We argue that skeletal pose annotations are a more
efficient annotation approach for surgical tools, striking a balance between
richness of semantic information and ease of annotation, thus allowing for
accelerated growth of available annotated data. To encourage adoption of this
annotation style, we present, ROBUST-MIPS, a combined tool pose and tool
instance segmentation dataset derived from the existing ROBUST-MIS dataset. Our
enriched dataset facilitates the joint study of these two annotation styles and
allow head-to-head comparison on various downstream tasks. To demonstrate the
adequacy of pose annotations for surgical tool localisation, we set up a simple
benchmark using popular pose estimation methods and observe high-quality
results. To ease adoption, together with the dataset, we release our benchmark
models and custom tool pose annotation software.

</details>


### [7] [Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](https://arxiv.org/abs/2508.21099)
*Xiangtao Meng,Yingkai Dong,Ning Yu,Li Wang,Zheng Li,Shanqing Guo*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their
potential for misuse or even abuse raises serious safety concerns. Model
developers have made tremendous efforts to introduce safety mechanisms that can
address these concerns in T2I models. However, the existing safety mechanisms,
whether external or internal, either remain susceptible to evasion under
distribution shifts or require extensive model-specific adjustments. To address
these limitations, we introduce Safe-Control, an innovative plug-and-play
safety patch designed to mitigate unsafe content generation in T2I models.
Using data-driven strategies and safety-aware conditions, Safe-Control injects
safety control signals into the locked T2I model, acting as an update in a
patch-like manner. Model developers can also construct various safety patches
to meet the evolving safety requirements, which can be flexibly merged into a
single, unified patch. Its plug-and-play design further ensures adaptability,
making it compatible with other T2I models of similar denoising architecture.
We conduct extensive evaluations on six diverse and public T2I models.
Empirical results highlight that Safe-Control is effective in reducing unsafe
content generation across six diverse T2I models with similar generative
architectures, yet it successfully maintains the quality and text alignment of
benign images. Compared to seven state-of-the-art safety mechanisms, including
both external and internal defenses, Safe-Control significantly outperforms all
baselines in reducing unsafe content generation. For example, it reduces the
probability of unsafe content generation to 7%, compared to approximately 20%
for most baseline methods, under both unsafe prompts and the latest adversarial
attacks.

</details>


### [8] [GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions](https://arxiv.org/abs/2508.21102)
*Kei Katsumata,Yui Iioka,Naoki Hosomi,Teruhisa Misu,Kentaro Yamada,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出GENNAV：面向移动载体前视相机与自然语言指令的目标区域定位，联合判断“是否存在”并生成多处stuff类目标的分割；在新建GRiN-Drive基准（含无/单/多目标）与五城实车零样本测试中均优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导的视觉定位/分割在“stuff”类（如道路、草地、天空等边界模糊、非离散实例）上表现欠佳，且难以同时处理目标缺失或多目标情形。自动驾驶与轮椅/服务机器人等移动场景需要从语言中找出前方目标区域并作出存在判断与精确分割，因此需要新的方法与评测基准。

Method: 提出GENNAV：一个同时进行存在性预测与分割的生成式框架，专门面向stuff类目标与多目标情况。方法从自然语言与前视图像联合编码，输出目标是否存在，并生成一个或多个目标区域的掩码；支持无目标、单目标、多目标。为评测构建GRiN-Drive数据集，覆盖三类样本，并提供标准指标。还在五个城市、四辆车上做零样本实车测试。

Result: 在GRiN-Drive上，GENNAV在标准指标上全面优于多种基线（包括存在检测与分割性能），对无目标与多目标样本也更稳健。零样本跨城实车实验中，GENNAV在真实驾驶环境下继续领先，表现出较强的域泛化与鲁棒性。

Conclusion: GENNAV有效解决语言引导下stuff类目标的存在判断与多目标分割难题，在新基准和实车零样本测试中均领先，显示出实际部署潜力；GRiN-Drive为该方向提供了系统化评测。

Abstract: We focus on the task of identifying the location of target regions from a
natural language instruction and a front camera image captured by a mobility.
This task is challenging because it requires both existence prediction and
segmentation, particularly for stuff-type target regions with ambiguous
boundaries. Existing methods often underperform in handling stuff-type target
regions, in addition to absent or multiple targets. To overcome these
limitations, we propose GENNAV, which predicts target existence and generates
segmentation masks for multiple stuff-type target regions. To evaluate GENNAV,
we constructed a novel benchmark called GRiN-Drive, which includes three
distinct types of samples: no-target, single-target, and multi-target. GENNAV
achieved superior performance over baseline methods on standard evaluation
metrics. Furthermore, we conducted real-world experiments with four automobiles
operated in five geographically distinct urban areas to validate its zero-shot
transfer performance. In these experiments, GENNAV outperformed baseline
methods and demonstrated its robustness across diverse real-world environments.
The project page is available at https://gennav.vercel.app/.

</details>


### [9] [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113)
*Jie Jiang,Qi Yang,Bolin Ni,Shiming Xiang,Han Hu,Houwen Peng*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking
capabilities have demonstrated remarkable performance on complex reasoning
problems. However, this thinking process is redundant for simple problems
solvable without complex reasoning. To address this inefficiency, we propose
R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on
problem complexity. The central idea of R-4B is to empower the model with both
thinking and non-thinking capabilities using bi-mode annealing, and apply
Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in
determining whether to activate the thinking process. Specifically, we first
train the model on a carefully curated dataset spanning various topics, which
contains samples from both thinking and non-thinking modes. Then it undergoes a
second phase of training under an improved GRPO framework, where the policy
model is forced to generate responses from both modes for each input query.
Experimental results show that R-4B achieves state-of-the-art performance
across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks
and achieves performance comparable to larger models such as
Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower
computational cost.

</details>


### [10] [HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection](https://arxiv.org/abs/2508.21135)
*Harris Song,Tuan-Anh Vu,Sanjith Menon,Sriram Narasimhan,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 提出HiddenObject：用Mamba为核心的RGB-热成像-深度三模态融合框架，在遮挡、伪装、光照变化等恶劣条件下实现更鲁棒的目标检测，并在多基准上达SOTA或具竞争力。


<details>
  <summary>Details</summary>
Motivation: 单一RGB检测在遮挡、伪装、低光等情况下性能显著下降，现有简单或朴素的多模态融合策略也难以稳定发挥作用，亟需一种对模态更鲁棒、能充分利用跨模态互补信息的通用融合方法。

Method: 设计名为HiddenObject的多模态检测框架：分别提取RGB、热成像与深度的模态特征，利用基于Mamba（选择性状态空间模型）的融合模块进行特征对齐与交互，将模态特异信息与共享表征统一到单一表示空间中，以提升对被遮挡或伪装目标的感知。

Result: 在多个公开基准上验证，整体达到SOTA或接近SOTA的检测性能，相比单模态与朴素融合方法显著提升，展示了在复杂、退化视觉条件下的优势。

Conclusion: Mamba驱动的多模态融合架构能有效捕捉跨模态互补性，提升在遮挡/伪装/低光等复杂场景中的检测能力；同时揭示了当前单模态与简单融合策略的局限性，表明此类融合思路对多模态目标检测具有推广潜力。

Abstract: Detecting hidden or partially concealed objects remains a fundamental
challenge in multimodal environments, where factors like occlusion, camouflage,
and lighting variations significantly hinder performance. Traditional RGB-based
detection methods often fail under such adverse conditions, motivating the need
for more robust, modality-agnostic approaches. In this work, we present
HiddenObject, a fusion framework that integrates RGB, thermal, and depth data
using a Mamba-based fusion mechanism. Our method captures complementary signals
across modalities, enabling enhanced detection of obscured or camouflaged
targets. Specifically, the proposed approach identifies modality-specific
features and fuses them in a unified representation that generalizes well
across challenging scenarios. We validate HiddenObject across multiple
benchmark datasets, demonstrating state-of-the-art or competitive performance
compared to existing methods. These results highlight the efficacy of our
fusion design and expose key limitations in current unimodal and na\"ive fusion
strategies. More broadly, our findings suggest that Mamba-based fusion
architectures can significantly advance the field of multimodal object
detection, especially under visually degraded or complex conditions.

</details>


### [11] [RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration](https://arxiv.org/abs/2508.21154)
*Ao Shen,Xueming Fu,Junfeng Jiang,Qiang Zeng,Ye Tang,Zhengming Chen,Luming Nong,Feng Wang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出RadGS-Reg：在椎体级别通过联合3D Radiative Gaussians重建与3D/3D配准实现CT/X线注册，兼顾高精度与实时性；在双平面X线稀视角、噪声条件下仍具鲁棒性，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统“渲染-比较”的2D/3D注册迭代投影带来空间信息损失与域间差异，实时与精度难兼顾；基于双平面X线重建虽可补充形状/空间信息，但通常需要密集视角且对噪声敏感。临床导航需要在稀视角与噪声场景中实现稳定高效的CT/X线配准。

Method: 提出RadGS-Reg框架：1) 基于双平面X线的椎体级3D Radiative Gaussians（RadGS）重建模块，采用学习式重建并引入Counterfactual Attention Learning（CAL）以在噪声X线中突出椎体区域；2) 患者特异的预训练策略，从模拟到真实逐步适配，同时学习椎体形状先验；3) 将配准从传统2D/3D转化为3D/3D（重建的RadGS与CT体数据）以减小域差与信息丢失。

Result: 在自建数据集上，无论是重建质量还是配准精度与速度均达到SOTA，较现有方法显著提升，且对稀疏视角与噪声X线更鲁棒。

Conclusion: 联合RadGS重建与3D/3D配准、辅以CAL与患者特异预训练，可在双平面X线条件下实现高精度、鲁棒、接近实时的CT/X线注册，优于既有“渲染-比较”与传统重建方案；代码已开源。

Abstract: Computed Tomography (CT)/X-ray registration in image-guided navigation
remains challenging because of its stringent requirements for high accuracy and
real-time performance. Traditional "render and compare" methods, relying on
iterative projection and comparison, suffer from spatial information loss and
domain gap. 3D reconstruction from biplanar X-rays supplements spatial and
shape information for 2D/3D registration, but current methods are limited by
dense-view requirements and struggles with noisy X-rays. To address these
limitations, we introduce RadGS-Reg, a novel framework for vertebral-level
CT/X-ray registration through joint 3D Radiative Gaussians (RadGS)
reconstruction and 3D/3D registration. Specifically, our biplanar X-rays
vertebral RadGS reconstruction module explores learning-based RadGS
reconstruction method with a Counterfactual Attention Learning (CAL) mechanism,
focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific
pre-training strategy progressively adapts the RadGS-Reg from simulated to real
data while simultaneously learning vertebral shape prior knowledge. Experiments
on in-house datasets demonstrate the state-of-the-art performance for both
tasks, surpassing existing methods. The code is available at:
https://github.com/shenao1995/RadGS_Reg.

</details>


### [12] [SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4](https://arxiv.org/abs/2508.21169)
*Kevin Mayer,Alex Vesel,Xinyi Zhao,Martin Fischer*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: 3D building models are critical for applications in architecture, energy
simulation, and navigation. Yet, generating accurate and semantically rich 3D
buildings automatically remains a major challenge due to the lack of
large-scale annotated datasets in the public domain. Inspired by the success of
synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,
and multi-modal dataset of over 6.2 million synthetic 3D residential buildings
at Level of Detail (LoD) 4. In the dataset, each building is represented
through three distinct modalities: a semantically enriched 3D wireframe graph
at LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a
LiDAR-like roof point cloud (Modality III). The semantic annotations for each
building wireframe are derived from the corresponding floor plan images and
include information on rooms, doors, and windows. Through its tri-modal nature,
future work can use SYNBUILD-3D to develop novel generative AI algorithms that
automate the creation of 3D building models at LoD 4, subject to predefined
floor plan layouts and roof geometries, while enforcing semantic-geometric
consistency. Dataset and code samples are publicly available at
https://github.com/kdmayer/SYNBUILD-3D.

</details>


### [13] [Radially Distorted Homographies, Revisited](https://arxiv.org/abs/2508.21190)
*Mårten Wadenbäck,Marcus Valtonen Örnhag,Johan Edstedt*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Homographies are among the most prevalent transformations occurring in
geometric computer vision and projective geometry, and homography estimation is
consequently a crucial step in a wide assortment of computer vision tasks. When
working with real images, which are often afflicted with geometric distortions
caused by the camera lens, it may be necessary to determine both the homography
and the lens distortion-particularly the radial component, called radial
distortion-simultaneously to obtain anything resembling useful estimates. When
considering a homography with radial distortion between two images, there are
three conceptually distinct configurations for the radial distortion; (i)
distortion in only one image, (ii) identical distortion in the two images, and
(iii) independent distortion in the two images. While these cases have been
addressed separately in the past, the present paper provides a novel and
unified approach to solve all three cases. We demonstrate how the proposed
approach can be used to construct new fast, stable, and accurate minimal
solvers for radially distorted homographies. In all three cases, our proposed
solvers are faster than the existing state-of-the-art solvers while maintaining
similar accuracy. The solvers are tested on well-established benchmarks
including images taken with fisheye cameras. The source code for our solvers
will be made available in the event our paper is accepted for publication.

</details>


### [14] [GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability](https://arxiv.org/abs/2508.21197)
*Zhenghao He,Sanchit Sinha,Guangzhi Xiong,Aidong Zhang*

Main category: cs.CV

TL;DR: 提出GCAV，将不同层的CAV通过对比学习对齐并用注意力融合为统一表示，减少TCAV方差、提升跨层一致性、定位与鲁棒性；并提出TGCAV用于评估。


<details>
  <summary>Details</summary>
Motivation: 传统CAV在不同层独立训练，语义不一致、跨层可比性差，导致TCAV不稳定、解释难以复用。需要一种能跨层统一概念表示的方法，提高解释一致性与可靠性。

Method: 1) 跨层对比学习：将同一概念在不同层的表示拉近、不同概念拉远，获得对齐的概念嵌入；2) 注意力融合：学习各层对该概念的贡献权重，构建全局概念激活向量(GCAV)；3) 定义TGCAV：将TCAV的灵敏度测度应用于GCAV表示，实现全局概念归因与检验。

Result: 在多种DNN上，GCAV显著降低TCAV分数的方差（更稳定），保持或提升概念相关性；改进概念定位；在对抗扰动下更鲁棒；总体缓解跨层概念不一致问题。

Conclusion: GCAV将跨层信息整合为语义一致的全局概念向量，提供更稳定、可比、鲁棒的概念归因；TGCAV使其可评估与应用，提升对模型编码人类概念方式的可解释性。

Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for
interpreting deep neural networks by quantifying their sensitivity to
human-defined concepts. However, when computed independently at different
layers, CAVs often exhibit inconsistencies, making cross-layer comparisons
unreliable. To address this issue, we propose the Global Concept Activation
Vector (GCAV), a novel framework that unifies CAVs into a single, semantically
consistent representation. Our method leverages contrastive learning to align
concept representations across layers and employs an attention-based fusion
mechanism to construct a globally integrated CAV. By doing so, our method
significantly reduces the variance in TCAV scores while preserving concept
relevance, ensuring more stable and reliable concept attributions. To evaluate
the effectiveness of GCAV, we introduce Testing with Global Concept Activation
Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We
conduct extensive experiments on multiple deep neural networks, demonstrating
that our method effectively mitigates concept inconsistency across layers,
enhances concept localization, and improves robustness against adversarial
perturbations. By integrating cross-layer information into a coherent
framework, our method offers a more comprehensive and interpretable
understanding of how deep learning models encode human-defined concepts. Code
and models are available at https://github.com/Zhenghao-He/GCAV.

</details>


### [15] [Generalizable Object Re-Identification via Visual In-Context Prompting](https://arxiv.org/abs/2508.21222)
*Zhizhong Huang,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出VICP：用少量正负样例作为“上下文提示”，结合LLM推理的语义身份规则与视觉基础模型的动态视觉提示，实现零参数适配地泛化到未见类别，并在新构建的ShopID10K与多种ReID基准上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有ReID多为特定类别、强依赖标注且对新类别泛化差；自监督虽省标注但难学到身份敏感特征。作者希望无需再训练就能把已有模型迁移到未见类别。

Method: 提出Visual In-Context Prompting (VICP)：1) 给LLM少量正/负对作为任务特定提示，让LLM抽取“身份判别”语义规则；2) 将这些语义概念转化为对视觉基础模型（如DINO）的动态视觉提示，引导其提取ID区分特征；3) 不做参数更新，通过对齐LLM语义与VFM先验来直接用于新类别ReID。

Result: 在新建的ShopID10K（含电商多视角与跨域测试）及多种ReID基准上，对未见类别的检索性能明显超过各类基线。

Conclusion: 通过LLM+VFM的对齐与动态提示，可在不重新训练的情况下把身份判别能力泛化到新类别，降低标注与适配成本，并验证了跨域与跨类别场景下的有效性。

Abstract: Current object re-identification (ReID) methods train domain-specific models
(e.g., for persons or vehicles), which lack generalization and demand costly
labeled data for new categories. While self-supervised learning reduces
annotation needs by learning instance-wise invariance, it struggles to capture
\textit{identity-sensitive} features critical for ReID. This paper proposes
Visual In-Context Prompting~(VICP), a novel framework where models trained on
seen categories can directly generalize to unseen novel categories using only
\textit{in-context examples} as prompts, without requiring parameter
adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer
semantic identity rules from few-shot positive/negative pairs through
task-specific prompting, which then guides a VFM (\eg, DINO) to extract
ID-discriminative features via \textit{dynamic visual prompts}. By aligning
LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables
generalization to novel categories, eliminating the need for dataset-specific
retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K
object instances from e-commerce platforms, featuring multi-view images and
cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks
demonstrate that VICP outperforms baselines by a clear margin on unseen
categories. Code is available at https://github.com/Hzzone/VICP.

</details>


### [16] [Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with Auto3DSeg](https://arxiv.org/abs/2508.21227)
*Keshav Jha,William Sharp,Dominic LaBella*

Main category: cs.CV

TL;DR: 用Auto3DSeg中的SegResNet在PANTHER 2025两项MRI胰腺肿瘤分割任务上做实验，小数据集+序列差异导致表现一般（Task1 DSC 0.56；Task2 0.33），提示需更大、标准化数据以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤自动分割对临床决策重要，但受解剖变异、MRI序列差异和数据量小限制，现有方法效果有限。作者希望评估基于Auto3DSeg/SegResNet在不同MRI任务上的可行性与局限。

Method: 基于Auto3DSeg框架的SegResNet；先做解剖相关ROI聚焦；5折交叉验证；用STAPLE对折内结果集成。两任务：Task1（91例T1动脉期增强MRI）、Task2（50例T2 MR-Linac），均有专家胰腺与肿瘤标注。评估指标：DSC、5mm DSC、HD95、MASD、RMSE。

Result: Task1：DSC 0.56、5mm DSC 0.73、HD95 41.1 mm、MASD 26.0 mm、RMSE 5164 mm。Task2：DSC 0.33、5mm DSC 0.50、HD95 20.1 mm、MASD 7.2 mm、RMSE 17,203 mm。总体上Task2较差，显示序列/成像条件差异显著影响性能。

Conclusion: 当前在小型、异质MRI数据上的胰腺肿瘤自动分割仍具挑战；尽管效果一般，但证明了自动勾画的可行性与改进空间。需要更大且标准化的MRI数据集来提升模型鲁棒性与临床实用性。

Abstract: Accurate delineation of pancreatic tumors is critical for diagnosis,
treatment planning, and outcome assessment, yet automated segmentation remains
challenging due to anatomical variability and limited dataset availability. In
this study, SegResNet models, as part of the Auto3DSeg architecture, were
trained and evaluated on two MRI-based pancreatic tumor segmentation tasks as
part of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold
cross-validation with STAPLE ensembling after focusing on an anatomically
relevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic
MRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI
with expert annotated pancreas and tumor labels. The Pancreatic Tumor
Segmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases
with expert annotated pancreas and tumor labels. Algorithm-automated
segmentation performance of pancreatic tumor was assessed using Dice Similarity
Coefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean
Average Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1,
the algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD
of 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC
of 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203
mm. These findings illustrate the challenges of MRI-based pancreatic tumor
segmentation with small datasets, highlighting variability introduced by
different MRI sequences. Despite modest performance, the results demonstrate
potential for automated delineation and emphasize the need for larger,
standardized MRI datasets to improve model robustness and clinical utility.

</details>


### [17] [Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation](https://arxiv.org/abs/2508.21254)
*Yidong Zhao,Peter Kellman,Hui Xue,Tongyun Yang,Yi Zhang,Yuchi Han,Orlando Simonetti,Qian Tao*

Main category: cs.CV

TL;DR: 提出“Reverse Imaging”：先从已获得的心脏MRI图像反推底层自旋参数（质子密度、T1、T2），再基于这些参数合成任意序列的图像，用于数据增强与域自适应，从而显著提升跨序列分割泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有预训练分割模型在不同MRI序列之间泛化差；根因是对比度差异大，而对比度虽变但受同一物理自旋特性（PD/T1/T2）支配。因此若能回到物理本源（自旋参数）并再正向生成任意序列图像，就能消除协议/序列差异带来的域移。

Method: 物理驱动+生成先验的反问题框架：1) 将从观测心脏MRI到自旋参数的估计建模为病态非线性逆问题；2) 以自旋参数的先验分布进行正则化；3) 该“自旋先验”由在mSASHA多参数数据（联合T1/T2图）上训练的扩散生成模型学习得到；4) 估得的自旋参数作为可解释潜变量，再通过前向成像方程合成任意新序列/对比度图像，实现数据增强与域自适应。

Result: 在跨多种心脏MRI序列/协议上，使用反向成像获得的自旋参数并合成目标域风格图像进行训练/适配，可显著提高分割精度与稳健性；相较直接迁移，表现出“广谱”泛化能力。

Conclusion: 通过把图像表征提升到物理自旋参数空间，并以扩散模型学习的先验解决逆问题，可实现可解释、灵活的序列合成与域适配，根本缓解因对比度变化导致的分割泛化问题。

Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI)
struggle to generalize across different imaging sequences due to significant
variations in image contrast. These variations arise from changes in imaging
protocols, yet the same fundamental spin properties, including proton density,
T1, and T2 values, govern all acquired images. With this core principle, we
introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data
augmentation and domain adaptation to fundamentally solve the generalization
problem. Our method reversely infers the underlying spin properties from
observed cardiac MRI images, by solving ill-posed nonlinear inverse problems
regularized by the prior distribution of spin properties. We acquire this "spin
prior" by learning a generative diffusion model from the multiparametric
SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which
offers joint cardiac T1 and T2 maps. Our method enables approximate but
meaningful spin-property estimates from MR images, which provide an
interpretable "latent variable" that lead to highly flexible image synthesis of
arbitrary novel sequences. We show that Reverse Imaging enables highly accurate
segmentation across vastly different image contrasts and imaging protocols,
realizing wide-spectrum generalization of cardiac MRI segmentation.

</details>


### [18] [PHD: Personalized 3D Human Body Fitting with Point Diffusion](https://arxiv.org/abs/2508.21257)
*Hsuan-I Ho,Chen Guo,Po-Chen Wu,Ivan Shugurov,Chengcheng Tang,Abhay Mittal,Sizhe An,Manuel Kaufmann,Linguang Zhang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We introduce PHD, a novel approach for personalized 3D human mesh recovery
(HMR) and body fitting that leverages user-specific shape information to
improve pose estimation accuracy from videos. Traditional HMR methods are
designed to be user-agnostic and optimized for generalization. While these
methods often refine poses using constraints derived from the 2D image to
improve alignment, this process compromises 3D accuracy by failing to jointly
account for person-specific body shapes and the plausibility of 3D poses. In
contrast, our pipeline decouples this process by first calibrating the user's
body shape and then employing a personalized pose fitting process conditioned
on that shape. To achieve this, we develop a body shape-conditioned 3D pose
prior, implemented as a Point Diffusion Transformer, which iteratively guides
the pose fitting via a Point Distillation Sampling loss. This learned 3D pose
prior effectively mitigates errors arising from an over-reliance on 2D
constraints. Consequently, our approach improves not only pelvis-aligned pose
accuracy but also absolute pose accuracy -- an important metric often
overlooked by prior work. Furthermore, our method is highly data-efficient,
requiring only synthetic data for training, and serves as a versatile
plug-and-play module that can be seamlessly integrated with existing 3D pose
estimators to enhance their performance. Project page:
https://phd-pose.github.io/

</details>


### [19] [Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning](https://arxiv.org/abs/2508.21363)
*Yuquan Bi,Hongsong Wang,Xinli Shi,Zhipeng Gui,Jie Gui,Yuan Yan Tang*

Main category: cs.CV

TL;DR: 提出一种用于3D人体姿态估计的高效扩散模型框架，通过分层时序剪枝在保证运动关键信息的同时显著降低计算量并实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在3D姿态生成上效果好，但迭代采样与多假设预测导致训练与推理计算成本高、速度慢，需要一种在不牺牲精度的前提下降低计算的机制。

Method: 提出层级式时序剪枝（HTP），自顶向下包含三步：1）TCEP：构建自适应时序图，基于帧间运动相关性选出关键帧并剪除冗余帧；2）SFT-MHSA：利用帧级稀疏性在多头自注意中仅关注与运动相关的token以减小注意力开销；3）MGPTP：基于聚类与掩码引导在语义层面对姿态token细粒度剪枝，仅保留最具信息量的token。

Result: 在Human3.6M与MPI-INF-3DHP上，训练MAC下降38.5%，推理MAC下降56.8%，平均推理速度提升81.1%，同时达到或超过现有扩散法的精度（SOTA）。

Conclusion: 分层时序剪枝能在扩散式3D姿态估计中有效去除冗余时空与语义token，显著提高效率且保持甚至提升精度，为高效扩散推理提供通用思路。

Abstract: Diffusion models have demonstrated strong capabilities in generating
high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis
requirements incur substantial computational cost. In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics. HTP operates in a staged, top-down manner: (1)
Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by
analyzing inter-frame motion correlations through adaptive temporal graph
construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the
resulting frame-level sparsity to reduce attention computation, focusing on
motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs
fine-grained semantic pruning via clustering, retaining only the most
informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.

</details>


### [20] [Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image](https://arxiv.org/abs/2508.21371)
*Qingran Miao,Haixia Wang,Haohao Sun,Yilong Zhang*

Main category: cs.CV

TL;DR: 提出Print2Volume，将2D指纹合成高保真3D OCT指纹体数据，缓解真实OCT数据稀缺，显著提升识别性能（EER从15.62%降至2.50%）。


<details>
  <summary>Details</summary>
Motivation: OCT能提供含皮下结构的高分辨率3D指纹，有利于鲁棒识别，但采集昂贵且耗时，公开大规模数据稀缺，限制了深度学习方法效果与泛化能力。需要一种可生成真实感3D OCT指纹数据的方案以缓解数据瓶颈。

Method: 提出三阶段生成框架Print2Volume：1）2D风格迁移：将二值指纹转为模拟OCT Z向均值投影风格的灰度图；2）3D结构扩展网络：把2D图外推为符合解剖结构的3D体数据；3）OCT真实感精炼器：基于3D GAN，对体数据注入真实纹理、散斑噪声与成像特性。随后合成42万样本用于预训练与微调。

Result: 合成数据在定量评估上质量较高；在ZJUT-EIFD基准上，用合成数据预训练并在少量真实数据上微调，可将EER从15.62%显著降至2.50%。

Conclusion: Print2Volume能从2D指纹有效生成高保真3D OCT指纹体数据，缓解数据稀缺并显著提升识别性能；适合作为预训练数据源推动OCT指纹识别发展。

Abstract: Optical Coherence Tomography (OCT) enables the acquisition of
high-resolution, three-dimensional fingerprint data, capturing rich subsurface
structures for robust biometric recognition. However, the high cost and
time-consuming nature of OCT data acquisition have led to a scarcity of
large-scale public datasets, significantly hindering the development of
advanced algorithms, particularly data-hungry deep learning models. To address
this critical bottleneck, this paper introduces Print2Volume, a novel framework
for generating realistic, synthetic OCT-based 3D fingerprints from 2D
fingerprint image. Our framework operates in three sequential stages: (1) a 2D
style transfer module that converts a binary fingerprint into a grayscale
images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D
Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D
anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that
renders the structural volume with authentic textures, speckle noise, and other
imaging characteristics. Using Print2Volume, we generated a large-scale
synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the
high quality of our synthetic data and its significant impact on recognition
performance. By pre-training a recognition model on our synthetic data and
fine-tuning it on a small real-world dataset, we achieved a remarkable
reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD
benchmark, proving the effectiveness of our approach in overcoming data
scarcity.

</details>


### [21] [GLENDA: Gynecologic Laparoscopy Endometriosis Dataset](https://arxiv.org/abs/2508.21398)
*Andreas Leibetseder,Sabrina Kletz,Klaus Schoeffmann,Simon Keckstein,Jörg Keckstein*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is
performed via a live feed of a patient's abdomen surveying the insertion and
handling of various instruments for conducting treatment. Adopting this kind of
surgical intervention not only facilitates a great variety of treatments, the
possibility of recording said video streams is as well essential for numerous
post-surgical activities, such as treatment planning, case documentation and
education. Nonetheless, the process of manually analyzing surgical recordings,
as it is carried out in current practice, usually proves tediously
time-consuming. In order to improve upon this situation, more sophisticated
computer vision as well as machine learning approaches are actively developed.
Since most of such approaches heavily rely on sample data, which especially in
the medical field is only sparsely available, with this work we publish the
Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset
containing region-based annotations of a common medical condition named
endometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the
first of its kind and it has been created in collaboration with leading medical
experts in the field.

</details>


### [22] [Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation](https://arxiv.org/abs/2508.21399)
*Sabrina Kletz,Klaus Schoeffmann,Jenny Benois-Pineau,Heinrich Husslein*

Main category: cs.CV

TL;DR: 用区域式全卷积网络在腹腔镜妇科手术视频中进行手术器械实例分割与识别；分割精度高，但器械类别识别因外观相似性仍较困难。


<details>
  <summary>Details</summary>
Motivation: 手术视频数量激增，但缺乏自动内容索引与检索手段；器械的准确定位与识别是实现内容检索与术野理解的关键。

Method: 采用实例感知的区域型全卷积网络（实例级FCN/Region-based FCN）对腹腔镜视频帧进行：(1) 二分类实例分割（器械vs背景）；(2) 多分类器械识别（器械类型）。在中等规模训练集上进行训练与评估。

Result: 在训练样本中等偏少的情况下，器械的定位与分割取得较高准确率；但在多类别识别任务上性能显著下降，表明类别区分困难。

Conclusion: 实例级分割在医疗内窥视频中已具备实用潜力；但由于器械外观高度相似、形变与遮挡等因素，多类别识别仍具挑战，需要更丰富数据与更强判别特征或时序/多模态信息以提升识别性能。

Abstract: Recorded videos from surgeries have become an increasingly important
information source for the field of medical endoscopy, since the recorded
footage shows every single detail of the surgery. However, while video
recording is straightforward these days, automatic content indexing - the basis
for content-based search in a medical video archive - is still a great
challenge due to the very special video content. In this work, we investigate
segmentation and recognition of surgical instruments in videos recorded from
laparoscopic gynecology. More precisely, we evaluate the achievable performance
of segmenting surgical instruments from their background by using a
region-based fully convolutional network for instance-aware (1) instrument
segmentation as well as (2) instrument recognition. While the first part
addresses only binary segmentation of instances (i.e., distinguishing between
instrument or background) we also investigate multi-class instrument
recognition (i.e., identifying the type of instrument). Our evaluation results
show that even with a moderately low number of training examples, we are able
to localize and segment instrument regions with a pretty high accuracy.
However, the results also reveal that determining the particular instrument is
still very challenging, due to the inherently high similarity of surgical
instruments.

</details>


### [23] [SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing](https://arxiv.org/abs/2508.21402)
*Jakub Straka,Ivan Gruber*

Main category: cs.CV

TL;DR: 提出SatDINO：将DINO式对比自监督学习适配到遥感影像，在多数据集和多设置上优于主流MAE方案，并给出消融与若干增强（GSD编码、自适应视角采样）；代码开源。


<details>
  <summary>Details</summary>
Motivation: 遥感领域有海量无标注影像，需高效的自监督表征学习。现有主流是MAE，但在遥感多尺度、多分辨率（GSD）与视角多样性下未必最优，亟需针对遥感特点优化的对比自监督方法。

Method: 以DINO为基础构建SatDINO：1) 用对比/知识蒸馏式自监督预训练获取通用表征；2) 设计能处理卫星影像特性的模块与训练策略；3) 引入新组件：GSD编码以显式建模空间分辨率差异；自适应view采样以更合理地生成多视角增广；4) 进行系统消融，验证每个组件贡献。

Result: 在多个遥感数据集和多种评测协议下，SatDINO整体优于基于MAE的SOTA，并在若干基准上达到有竞争力的表现。消融显示各组件（尤其GSD编码与自适应view）带来稳定增益。

Conclusion: 对比自监督（DINO范式）在遥感中非常有效；SatDINO提供实证和模块化增强（GSD编码、视角采样），可独立复用。模型与代码已开源，便于社区采用与扩展。

Abstract: Self-supervised learning has emerged as a powerful tool for remote sensing,
where large amounts of unlabeled data are available. In this work, we
investigate the use of DINO, a contrastive self-supervised method, for
pretraining on remote sensing imagery. We introduce SatDINO, a model tailored
for representation learning in satellite imagery. Through extensive experiments
on multiple datasets in multiple testing setups, we demonstrate that SatDINO
outperforms other state-of-the-art methods based on much more common masked
autoencoders (MAE) and achieves competitive results in multiple benchmarks.
  We also provide a rigorous ablation study evaluating SatDINO's individual
components. Finally, we propose a few novel enhancements, such as a new way to
incorporate ground sample distance (GSD) encoding and adaptive view sampling.
These enhancements can be used independently on our SatDINO model. Our code and
trained models are available at: https://github.com/strakaj/SatDINO.

</details>


### [24] [Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives](https://arxiv.org/abs/2508.21418)
*Gernot Fiala,Markus Plass,Robert Harb,Peter Regitnig,Kristijan Skok,Wael Al Zoughbi,Carmen Zerner,Paul Torke,Michaela Kargl,Heimo Müller,Tomas Brazdil,Matej Gallo,Jaroslav Kubín,Roman Stoklasa,Rudolf Nenutil,Norman Zerbe,Andreas Holzinger,Petr Holub*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: A Whole Slide Image (WSI) is a high-resolution digital image created by
scanning an entire glass slide containing a biological specimen, such as tissue
sections or cell samples, at multiple magnifications. These images can be
viewed, analyzed, shared digitally, and are used today for Artificial
Intelligence (AI) algorithm development. WSIs are used in a variety of fields,
including pathology for diagnosing diseases and oncology for cancer research.
They are also utilized in neurology, veterinary medicine, hematology,
microbiology, dermatology, pharmacology, toxicology, immunology, and forensic
science.
  When assembling cohorts for the training or validation of an AI algorithm, it
is essential to know what is present on such a WSI. However, there is currently
no standard for this metadata, so such selection has mainly been done through
manual inspection, which is not suitable for large collections with several
million objects.
  We propose a general framework to generate a 2D index map for WSI and a
profiling mechanism for specific application domains. We demonstrate this
approach in the field of clinical pathology, using common syntax and semantics
to achieve interoperability between different catalogs.
  Our approach augments each WSI collection with a detailed tissue map that
provides fine-grained information about the WSI content. The tissue map is
organized into three layers: source, tissue type, and pathological alterations,
with each layer assigning segments of the WSI to specific classes.
  We illustrate the advantages and applicability of the proposed standard
through specific examples in WSI catalogs, Machine Learning (ML), and
graph-based WSI representations.

</details>


### [25] [Unsupervised Incremental Learning Using Confidence-Based Pseudo-Labels](https://arxiv.org/abs/2508.21424)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出ICPL：一种基于置信度伪标签的无监督增量学习方法，在无需人工标注的情境下进行类增量学习；在CIFAR100、ImageNet100及细粒度数据上取得与监督方法接近、且较state-of-the-art class-iNCD方法高5%以上的最终准确率。


<details>
  <summary>Details</summary>
Motivation: 现实中增量数据常无标签，传统类增量学习（CIL）假设增量阶段有完整标注不切实际；需在保持旧类知识的同时，从未标注的新类数据中学习，且在资源受限设备上可行。

Method: 提出ICPL：为增量未标注数据生成伪标签，并采用基于置信度的筛选机制以提升伪标签质量；将该伪标签策略无缝集成到多种现有CIL框架中，评估不同CIL方法在引入伪标签后的退化；与class-iNCD方法对比；在CIFAR100、ImageNet100和细粒度数据集上实验，并评估计算复杂度以验证资源受限适用性。

Result: 在CIFAR100与ImageNet100上，ICPL在最终准确率上与监督CIL方法相当；相较主流class-iNCD方法，最终准确率提升超过5%；在细粒度数据集仍保持竞争力；计算复杂度符合资源受限场景需求。

Conclusion: 通过置信度驱动的伪标签选择，ICPL可在无标注增量场景中有效扩展类别且保持旧知识，兼具准确性与效率，实用性强，并为无监督类增量学习提供了通用、可与多种CIL框架结合的方案。

Abstract: Deep learning models have achieved state-of-the-art performance in many
computer vision tasks. However, in real-world scenarios, novel classes that
were unseen during training often emerge, requiring models to acquire new
knowledge incrementally. Class-Incremental Learning (CIL) methods enable a
model to learn novel classes while retaining knowledge of previous classes.
However, these methods make the strong assumption that the incremental dataset
is fully labeled, which is unrealistic in practice. In this work, we propose an
unsupervised Incremental Learning method using Confidence-based Pseudo-labels
(ICPL), which replaces human annotations with pseudo-labels, enabling
incremental learning from unlabeled datasets. We integrate these pseudo-labels
into various CIL methods with confidence-based selection and evaluate
performance degradation on CIFAR100 and ImageNet100. Then, we compare our
approach to popular Class Incremental Novel Category Discovery (class-iNCD)
methods addressing similar challenges. Additionally, we apply our method to
fine-grained datasets to demonstrate its real-world practicality and measure
its computational complexity to validate its suitability for
resource-constrained environments. ICPL achieves competitive results compared
to supervised methods and outperforms state-of-the-art class-iNCD methods by
more than 5% in final accuracy.

</details>


### [26] [MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation](https://arxiv.org/abs/2508.21435)
*Francisco Caetano,Christiaan Viviers,Peter H. H. de With,Fons van der Sommen*

Main category: cs.CV

TL;DR: 提出MedShift：一种基于Flow Matching与薛定谔桥的统一类条件生成模型，实现合成与真实头部X光的无配对高保真跨域翻译；引入X-DigiSkull数据集。模型尺寸小于扩散模型但表现强，推理时可在感知逼真与结构一致间权衡，具备可扩展与通用的医学影像域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于合成数据训练的模型在真实临床落地时受域间差异（衰减、噪声、软组织表征）制约，且许多方法需要配对数据或域特定训练，缺乏可扩展的多域通用方案。

Method: 提出MedShift：统一类条件生成框架，结合Flow Matching与Schrödinger Bridges，学习跨多域的共享域无关潜在空间；无需成对样本，支持任意训练域间的双向翻译；推理阶段可通过超参调节在感知保真与结构一致之间权衡。

Result: 在新提出的X-DigiSkull数据集（含对齐的合成与真实头颅X光、多剂量设置）上，MedShift在更小模型规模下达到与或优于扩散法的性能，并在跨域翻译质量、灵活性与推理可控性上表现优异。

Conclusion: MedShift为医学影像域适应提供了可扩展、通用且高效的解决方案；无需配对数据或域特定训练，具备多域互译与推理可控性。作者开放代码与数据以促进复现与后续研究。

Abstract: Synthetic medical data offers a scalable solution for training robust models,
but significant domain gaps limit its generalizability to real-world clinical
settings. This paper addresses the challenge of cross-domain translation
between synthetic and real X-ray images of the head, focusing on bridging
discrepancies in attenuation behavior, noise characteristics, and soft tissue
representation. We propose MedShift, a unified class-conditional generative
model based on Flow Matching and Schrodinger Bridges, which enables
high-fidelity, unpaired image translation across multiple domains. Unlike prior
approaches that require domain-specific training or rely on paired data,
MedShift learns a shared domain-agnostic latent space and supports seamless
translation between any pair of domains seen during training. We introduce
X-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays
under varying radiation doses, to benchmark domain translation models.
Experimental results demonstrate that, despite its smaller model size compared
to diffusion-based approaches, MedShift offers strong performance and remains
flexible at inference time, as it can be tuned to prioritize either perceptual
fidelity or structural consistency, making it a scalable and generalizable
solution for domain adaptation in medical imaging. The code and dataset are
available at https://caetas.github.io/medshift.html

</details>


### [27] [Trees as Gaussians: Large-Scale Individual Tree Mapping](https://arxiv.org/abs/2508.21437)
*Dimitri Gominski,Martin Brandt,Xiaoye Tong,Siyu Liu,Maurice Mugabowindekwe,Sizhuo Li,Florian Reiner,Andrew Davies,Rasmus Fensholt*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的方法，用3米PlanetScope影像在全球尺度检测大树个体，利用高斯核模拟树冠以提取树冠中心并生成二值树覆盖；用从机载激光雷达自动提取的数十亿训练点监督训练，取得对比激光雷达R^2=0.81的性能，并可通过少量人工微调进一步提升，适用于林内外、跨生物群区的高分辨率树木监测。


<details>
  <summary>Details</summary>
Motivation: 现有全球产品多为二元树覆盖或冠层高度，无法在个体尺度识别树木，限制了对树木在生态、气候与生物经济中作用的精细监测与管理；亟需可扩展、全球适用、能识别个体树的模型与数据产品。

Method: - 使用3 m PlanetScope卫星影像作为输入。
- 用可伸缩高斯核模拟树冠，生成用于训练的“软”目标，从而在预测时可提取树冠中心并派生二值树覆盖图。
- 以机载激光雷达自动提取的数十亿点作为监督信号，覆盖林内外环境。
- 在全球尺度训练深度学习检测模型，并可通过少量人工标注进行微调以提升局地效果。
- 与现有树覆盖产品及机载激光雷达进行基准对比评估。

Result: - 与航空激光雷达的分数覆盖对比达到R^2=0.81，属SOTA水平。
- 在不同生物群系中保持平衡的检测指标，能识别森林内外的单株树。
- 通过人工标注微调可进一步提高检测精度。
- 能生成树冠中心与二值树覆盖图两类产品。

Conclusion: 该方法实现了可扩展的全球高分辨率个体树检测框架，性能优于或可与现有产品媲美；依托普适的影像数据与可迁移训练流程，具有良好的跨区域泛化与适配未来更高质量卫星影像的潜力，支持精细化生态监测与管理。

Abstract: Trees are key components of the terrestrial biosphere, playing vital roles in
ecosystem function, climate regulation, and the bioeconomy. However,
large-scale monitoring of individual trees remains limited by inadequate
modelling. Available global products have focused on binary tree cover or
canopy height, which do not explicitely identify trees at individual level. In
this study, we present a deep learning approach for detecting large individual
trees in 3-m resolution PlanetScope imagery at a global scale. We simulate tree
crowns with Gaussian kernels of scalable size, allowing the extraction of crown
centers and the generation of binary tree cover maps. Training is based on
billions of points automatically extracted from airborne lidar data, enabling
the model to successfully identify trees both inside and outside forests. We
compare against existing tree cover maps and airborne lidar with
state-of-the-art performance (fractional cover R$^2 = 0.81$ against aerial
lidar), report balanced detection metrics across biomes, and demonstrate how
detection can be further improved through fine-tuning with manual labels. Our
method offers a scalable framework for global, high-resolution tree monitoring,
and is adaptable to future satellite missions offering improved imagery.

</details>


### [28] [Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content](https://arxiv.org/abs/2508.21444)
*Jiayu Yang,Weijian Su,Songqian Zhang,Yuqi Han,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出一种面向流式/动态场景的可扩展3D Gaussian Splatting框架，通过层级锚点结构、混合形变+生成策略与双向自适应掩膜，大幅减少训练时间与数据冗余，同时保持甚至提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS在动态/流式场景中面临两大瓶颈：1）为保证细节而需要大量稠密高斯，导致数据规模与内存/IO压力过大；2）逐帧训练耗时长，不利于实时与在线应用。论文旨在在不牺牲画质的前提下，提高训练与更新效率，以满足沉浸式应用的实时性需求。

Method: - 层级锚点式高斯组织：按尺度将高斯球体分层。粗层描述场景低频结构，细层仅在被粗层激活的区域启用，用于高保真细节渲染。
- 混合运动建模：跨帧采用“形变+生成（deformation + spawning）”。小/中等位移用连续形变表示，大范围运动通过触发新高斯生成。
- 双向自适应掩膜：基于时空冗余进行前向（剔除静态区域）与反向（按信息量选择视角）掩膜，降低无效梯度与计算负担。
- 面向流式训练：在视频/序列输入下增量更新参数，强调可扩展与高效。

Result: 在多组动态场景实验中，相比SOTA方法，实现更佳视觉质量（更清晰细节、更少伪影）并显著缩短训练时间；同时减少所需高斯数量与计算/存储开销。

Conclusion: 通过层级激活、混合运动建模与自适应掩膜，框架在动态场景的流式训练中兼顾画质与效率，证明了3DGS在实时与大规模应用中的可扩展性。

Abstract: 3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key
requirement for immersive applications. However, the extension of 3DGS to
dynamic scenes remains limitations on the substantial data volume of dense
Gaussians and the prolonged training time required for each frame. This paper
presents \M, a scalable Gaussian Splatting framework designed for efficient
training in streaming tasks. Specifically, Gaussian spheres are hierarchically
organized by scale within an anchor-based structure. Coarser-level Gaussians
represent the low-resolution structure of the scene, while finer-level
Gaussians, responsible for detailed high-fidelity rendering, are selectively
activated by the coarser-level Gaussians. To further reduce computational
overhead, we introduce a hybrid deformation and spawning strategy that models
motion of inter-frame through Gaussian deformation and triggers Gaussian
spawning to characterize wide-range motion. Additionally, a bidirectional
adaptive masking mechanism enhances training efficiency by removing static
regions and prioritizing informative viewpoints. Extensive experiments
demonstrate that \M~ achieves superior visual quality while significantly
reducing training time compared to state-of-the-art methods.

</details>


### [29] [One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist](https://arxiv.org/abs/2508.21451)
*Junha Song,Yongsik Jo,So Yeon Min,Quanting Xie,Taehwan Kim,Yonatan Bisk,Jaegul Choo*

Main category: cs.CV

TL;DR: 提出一款仅125M参数的图像描述“小专家”模型，并引入Sharp-Eyed Refinement（含DeepLens）以改进视觉定位与注意力，显著提升单句与细粒度描述性能，接近甚至优于大型多模态通用模型，且适合端侧部署。


<details>
  <summary>Details</summary>
Motivation: MLLM在端侧部署受算力与内存限制；现有小模型虽轻量但性能不足，且普遍存在“视觉失明”（视觉对齐差、语义张冠李戴）。作者希望在极小参数规模下获得强描述能力，并系统定位与缓解视觉失明成因。

Method: 1) 设计125M参数的图像描述专用小语言模型（specialist），评测单句与详细描述任务；2) 通过玩具实验分析错误来源：注意力无效与视觉表示不足；3) 提出Sharp-Eyed Refinement框架：先粗看生成候选/关注区域，再用DeepLens聚焦初次凝视中识别的关键信息区域，提取更细致的视觉特征并进行重述/修正，从而提升视觉落地与描述准确性。

Result: 小专家在多个基准上与大型通用MLLM相当，且优于以往小型caption模型；存在的视觉失明经新框架明显缓解，详细描述与语义准确性提升。

Conclusion: 轻量化“视觉专家”在端侧可行；视觉失明主要来自注意力与视觉表示不足；通过Sharp-Eyed Refinement与DeepLens的更强视觉扎根，可在保持小参数的同时逼近或超越大模型通用体性能，适合本地设备与机器人等应用。

Abstract: Image captioning is fundamental for applications like video instruction
systems and exploration robots, yet deploying such models on local devices is
challenging due to the high computational demands of multimodal large language
models (MLLMs). To address this, we first explore lightweight captioning by
implementing a specialist based on a 125M-parameter language model, 56 times
smaller than LLaMA-7B, and evaluating its performance on both single-sentence
and detailed captioning tasks. Surprisingly, we find that our model can achieve
performance comparable to large multimodal generalists, suggesting its
potential to serve as a strong visual specialist for on-device applications.
While promising, our model also exhibits a limitation: like other MLLMs, it
suffers from visual blindness, occasionally resulting in semantic captioning
errors. We carry out toy experiments and investigate the underlying causes,
where we observe that the problems arise from ineffective attention mechanisms
and limited visual representations. To alleviate them, we develop a novel
captioning framework, Sharp-Eyed Refinement, which enhances caption quality
through improved visual grounding. At its core, our DeepLens extracts detailed
visual representations by concentrating on informative regions identified
during the initial glance. Our experiments confirm both the advantages of our
specialist over prior small captioning models and large generalists and the
effectiveness of our framework.

</details>


### [30] [Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification](https://arxiv.org/abs/2508.21458)
*Kaouther Mouheb,Marawan Elbatel,Janne Papma,Geert Jan Biessels,Jurgen Claassen,Huub Middelkoop,Barbara van Munster,Wiesje van der Flier,Inez Ramakers,Stefan Klein,Esther E. Bron*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: While foundation models (FMs) offer strong potential for AI-based dementia
diagnosis, their integration into federated learning (FL) systems remains
underexplored. In this benchmarking study, we systematically evaluate the
impact of key design choices: classification head architecture, fine-tuning
strategy, and aggregation method, on the performance and efficiency of
federated FM tuning using brain MRI data. Using a large multi-cohort dataset,
we find that the architecture of the classification head substantially
influences performance, freezing the FM encoder achieves comparable results to
full fine-tuning, and advanced aggregation methods outperform standard
federated averaging. Our results offer practical insights for deploying FMs in
decentralized clinical settings and highlight trade-offs that should guide
future method development.

</details>


### [31] [Multi-Method Ensemble for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21463)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出将特征截断与多种打分函数进行集成的OOD检测方法MME，在多种数据集和场景下显著优于SOTA（ImageNet-1K上FPR95=27.57%，较最好基线提升6%）。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测多依赖两条路线：特征截断增强ID/OOD可分性、打分函数进行区分。但方法多各自为战、或只在单一类型OOD上验证，忽视了跨方法与跨分布类型的互补性与稳健性提升空间。

Method: 理论与实证表明：SOTA特征截断与打分函数可以有效叠加；进一步通过聚合多种打分函数提升对不同OOD类型的鲁棒性。基于此提出MME分数：将多种先进OOD检测器统一为一个集成打分框架（包含特征截断阶段与多打分函数集成/加权或聚合）。

Result: 在大/小规模、近OOD与远OOD等基准上广泛实验，MME在所有基准上显著超越近期SOTA。以BiT模型在ImageNet-1K上FPR95为27.57%，较最佳基线提升约6%。

Conclusion: 组合特征截断与多打分函数能够系统性提升OOD检测，MME提供了统一且更稳健的打分方案，对多种OOD类型更鲁棒并在主流基准上达成SOTA。

Abstract: Detecting out-of-distribution (OOD) samples is essential for neural networks
operating in open-world settings, particularly in safety-critical applications.
Existing methods have improved OOD detection by leveraging two main techniques:
feature truncation, which increases the separation between in-distribution (ID)
and OOD samples, and scoring functions, which assign scores to distinguish
between ID and OOD data. However, most approaches either focus on a single
family of techniques or evaluate their effectiveness on a specific type of OOD
dataset, overlooking the potential of combining multiple existing solutions.
Motivated by this observation, we theoretically and empirically demonstrate
that state-of-the-art feature truncation and scoring functions can be
effectively combined. Moreover, we show that aggregating multiple scoring
functions enhances robustness against various types of OOD samples. Based on
these insights, we propose the Multi-Method Ensemble (MME) score, which unifies
state-of-the-art OOD detectors into a single, more effective scoring function.
Extensive experiments on both large-scale and small-scale benchmarks, covering
near-OOD and far-OOD scenarios, show that MME significantly outperforms recent
state-of-the-art methods across all benchmarks. Notably, using the BiT model,
our method achieves an average FPR95 of 27.57% on the challenging ImageNet-1K
benchmark, improving performance by 6% over the best existing baseline.

</details>


### [32] [Adversarial Patch Attack for Ship Detection via Localized Augmentation](https://arxiv.org/abs/2508.21472)
*Chun Liu,Panpan Ding,Zheng Zheng,Hailong Wang,Bingqian Zhu,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 提出“目标区域局部数据增强”来生成更可迁移的对抗补丁，用于遥感图像中的船舶检测攻击；只增强目标区域，减少背景干扰，在 HRSC2016 上显著提升攻击成功率与迁移性。


<details>
  <summary>Details</summary>
Motivation: DNN 在遥感船舶检测上易受对抗补丁攻击；现有通过数据变换提升迁移性的做法往往对整幅图像进行强增广，导致背景/非目标区被过度扰动，引发检测器的伪检与误检，这些错误并非补丁本身能力所致，反而掩盖了优化目标。需要一种避免背景过增广、聚焦目标影响的训练策略。

Method: 提出“局部化增广”：在生成对抗补丁时，仅对目标（船舶）区域进行随机数据增强（如几何/光照/尺度等），而保持非目标背景不变；这样在计算损失时，优化集中刻画补丁对检测器的破坏作用，减少背景噪声的梯度干扰。流程：1) 基于检测框/掩膜定位目标；2) 仅在这些区域施加增广并粘贴/优化补丁；3) 以目标被误检/漏检为目标函数，迭代更新补丁；4) 评估跨模型/跨场景迁移。

Result: 在 HRSC2016 数据集上，相比全图增广或无增广的基线，对抗补丁的攻击成功率更高，对不同检测模型的黑盒迁移性能更好；同时减少了由背景过增广引起的伪检现象。

Conclusion: 局部化目标区域增广能有效提升对抗补丁对遥感船舶检测模型的攻击有效性与可迁移性，关键在于降低背景干扰、让损失更专注于补丁影响。该思路对其他目标检测场景也具有可推广性。

Abstract: Current ship detection techniques based on remote sensing imagery primarily
rely on the object detection capabilities of deep neural networks (DNNs).
However, DNNs are vulnerable to adversarial patch attacks, which can lead to
misclassification by the detection model or complete evasion of the targets.
Numerous studies have demonstrated that data transformation-based methods can
improve the transferability of adversarial examples. However, excessive
augmentation of image backgrounds or irrelevant regions may introduce
unnecessary interference, resulting in false detections of the object detection
model. These errors are not caused by the adversarial patches themselves but
rather by the over-augmentation of background and non-target areas. This paper
proposes a localized augmentation method that applies augmentation only to the
target regions, avoiding any influence on non-target areas. By reducing
background interference, this approach enables the loss function to focus more
directly on the impact of the adversarial patch on the detection model, thereby
improving the attack success rate. Experiments conducted on the HRSC2016
dataset demonstrate that the proposed method effectively increases the success
rate of adversarial patch attacks and enhances their transferability.

</details>


### [33] [ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding](https://arxiv.org/abs/2508.21496)
*Hao Lu,Jiahao Wang,Yaolun Zhang,Ruohui Wang,Xuanyu Zheng,Yepeng Tang,Dahua Lin,Lewei Lu*

Main category: cs.CV

TL;DR: 提出ELV-Halluc长视频幻觉基准，系统研究“语义聚合幻觉（SAH）”；发现SAH随语义复杂度和快速语义变化而上升，并通过位置编码与DPO训练（基于8K对抗数据）显著缓解，SAH比例降27.7%，在ELV-Halluc与Video-MME上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLM在长视频理解中常出现与视频不一致的幻觉；既有基准多针对短视频，归因于语言先验、缺帧或视觉编码偏差，但忽略了一类在帧级语义正确却在事件级聚合出错的幻觉，且长视频多事件、多语义变化使该问题更突出，需专门分离并研究。

Method: 1) 构建首个面向长视频幻觉的基准ELV-Halluc，用于系统分析“语义聚合幻觉（SAH）”。2) 通过实验量化SAH与语义复杂度、语义变化速率的关系。3) 探索缓解策略：设计/选择更合适的位置编码以帮助时序对齐与事件边界建模；采用DPO（偏好优化）训练，构建8K对抗数据对，强化模型区分事件内与跨事件语义的能力。

Result: 实验证实SAH客观存在，并且在语义更复杂或快速变化的视频中更易发生。采用改进的位置编码和DPO训练后，在ELV-Halluc与Video-MME两基准上均取得性能提升；SAH比例显著下降，最大降幅达27.7%。

Conclusion: 长视频中的主要幻觉来源之一是帧级到事件级的语义聚合失真（SAH）。通过针对时序与事件结构的建模（位置编码）及偏好优化（DPO）可有效缓解。ELV-Halluc为研究与评测SAH提供了标准化平台，所提出的数据与训练策略在多基准上验证了有效性。

Abstract: Video multimodal large language models (Video-MLLMs) have achieved remarkable
progress in video understanding. However, they remain vulnerable to
hallucination-producing content inconsistent with or unrelated to video inputs.
Previous video hallucination benchmarks primarily focus on short-videos. They
attribute hallucinations to factors such as strong language priors, missing
frames, or vision-language biases introduced by the visual encoder. While these
causes indeed account for most hallucinations in short videos, they still
oversimplify the cause of hallucinations. Sometimes, models generate incorrect
outputs but with correct frame-level semantics. We refer to this type of
hallucination as Semantic Aggregation Hallucination (SAH), which arises during
the process of aggregating frame-level semantics into event-level semantic
groups. Given that SAH becomes particularly critical in long videos due to
increased semantic complexity across multiple events, it is essential to
separate and thoroughly investigate the causes of this type of hallucination.
To address the above issues, we introduce ELV-Halluc, the first benchmark
dedicated to long-video hallucination, enabling a systematic investigation of
SAH. Our experiments confirm the existence of SAH and show that it increases
with semantic complexity. Additionally, we find that models are more prone to
SAH on rapidly changing semantics. Moreover, we discuss potential approaches to
mitigate SAH. We demonstrate that positional encoding strategy contributes to
alleviating SAH, and further adopt DPO strategy to enhance the model's ability
to distinguish semantics within and across events. To support this, we curate a
dataset of 8K adversarial data pairs and achieve improvements on both
ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.

</details>


### [34] [Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation](https://arxiv.org/abs/2508.21529)
*Ronan Docherty,Antonis Vamvakeros,Samuel J. Cooper*

Main category: cs.CV

TL;DR: 提出一种上采样网络，将低分辨率的视觉基础模型特征在图像引导下细化为高分辨率特征，从而高效、少标注地实现显微图像的高质量（含交互式）分割与难分相位分离。


<details>
  <summary>Details</summary>
Motivation: 现有特征基础模型多为ViT，输出以patch为单位，计算高效但难以捕捉显微图像中的细粒度结构（如发丝裂纹），且面对超大幅面图像时易受限；需要一种方法在不显著增加计算的情况下，获得既丰富又高分辨率的语义特征以服务分割与检测。

Method: 训练一个卷积神经网络作为特征上采样器：输入为图像与低分辨率（大patch）基础模型特征，输出为与图像对齐的高分辨率特征。训练完成后，固定该上采样器，不再针对下游任务做额外训练；以此生成细粒度深度特征，再用于交互式或自动分割。

Result: 在多种显微数据（植物细胞、锂电池正极、有机晶体）上，无需额外微调即可高效表征并实现分割；上采样后的特征可区分难分相位（如细小裂纹）。交互式分割使用这些深特征能以更少标注、更短时间得到高质量结果，优于重新训练或微调传统CNN。

Conclusion: 图像引导的特征上采样能弥补基础模型patch特征的细节缺失与大图适配问题，为显微图像提供高分辨率、可分离性强的表示，显著提升交互式分割效率与质量，且具有良好的通用性与零/少训练优势。

Abstract: Feature foundation models - usually vision transformers - offer rich semantic
descriptors of images, useful for downstream tasks such as (interactive)
segmentation and object detection. For computational efficiency these
descriptors are often patch-based, and so struggle to represent the fine
features often present in micrographs; they also struggle with the large image
sizes present in materials and biological image analysis. In this work, we
train a convolutional neural network to upsample low-resolution (i.e, large
patch size) foundation model features with reference to the input image. We
apply this upsampler network (without any further training) to efficiently
featurise and then segment a variety of microscopy images, including plant
cells, a lithium-ion battery cathode and organic crystals. The richness of
these upsampled features admits separation of hard to segment phases, like
hairline cracks. We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.

</details>


### [35] [HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones](https://arxiv.org/abs/2508.21539)
*Hao Ruan,Jinliang Lin,Yingxin Lai,Zhiming Luo,Shaozi Li*

Main category: cs.CV

TL;DR: 提出HCCM框架，用层级跨粒度对比/匹配学习与动量对比蒸馏，解决无人机场景下VLM细粒度与组合语义难题，在GeoText-1652与ERA上达SOTA与强零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 无人机场景视野广、语义组合复杂；主流VLM偏重全局对齐，缺乏细粒度；现有层级方法依赖准确分割与严格包含关系，难适应动态环境；文本描述常不完整/含糊，导致对齐不稳定。

Method: 提出HCCM，包括：1) RG-ITC：区域-全局图文对比学习，不做精确区域切分，通过局部视觉vs全局文本、以及全局视觉vs局部文本对比，捕获层级的局部到全局语义。2) RG-ITM：区域-全局图文匹配，在全局跨模态表示中评估局部语义一致性，弱化硬性包含约束，提升组合推理。引入MCD（动量对比+蒸馏）机制，缓解不完整/模糊文本带来的不稳定，增强鲁棒与一致性。

Result: 在GeoText-1652上，图检文Recall@1=28.8%，文检图Recall@1=14.7%，均为SOTA；在未见ERA数据集上，零样本平均召回mR=39.93%，优于微调基线。

Conclusion: 跨粒度、无精确分割与弱约束的对比/匹配策略，加上动量对比蒸馏，使模型在无人机场景下获得更稳健的细粒度对齐与组合推理能力，带来SOTA与强零样本泛化。

Abstract: Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such
as target matching and navigation. However, the wide field of view and complex
compositional semantics in drone scenarios pose challenges for vision-language
understanding. Mainstream Vision-Language Models (VLMs) emphasize global
alignment while lacking fine-grained semantics, and existing hierarchical
methods depend on precise entity partitioning and strict containment, limiting
effectiveness in dynamic environments. To address this, we propose the
Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM)
framework with two components: (1) Region-Global Image-Text Contrastive
Learning (RG-ITC), which avoids precise scene partitioning and captures
hierarchical local-to-global semantics by contrasting local visual regions with
global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),
which dispenses with rigid constraints and instead evaluates local semantic
consistency within global cross-modal representations, enhancing compositional
reasoning. Moreover, drone text descriptions are often incomplete or ambiguous,
destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation
(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM
achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text
retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot
generalization with 39.93% mean recall (mR), outperforming fine-tuned
baselines.

</details>


### [36] [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](https://arxiv.org/abs/2508.21542)
*Ziwei Liao,Mohamed Sayed,Steven L. Waslander,Sara Vicente,Daniyar Turmukhambetov,Michael Firman*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single "mode" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.

</details>


### [37] [EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting](https://arxiv.org/abs/2508.21550)
*Yujin Park,Haejun Chung,Ikbeom Jang*

Main category: cs.CV

TL;DR: 提出EZ-Sort：结合CLIP零样本预排序与不确定性引导的人机合并排序，显著减少成对比较的人工标注成本（相对穷举-90.5%、相对已有方法-19.8%在n=100），并维持/提升一致性。


<details>
  <summary>Details</summary>
Motivation: 成对比较更可靠但代价高（O(n^2)）。已有基于排序的主动采样降至O(n log n)，但仍需要大量人工。希望进一步减少人工、用模型先验替代“显而易见”的比较，同时保持排序质量与一致性。

Method: - 以CLIP进行零样本层级式预排序，得到粗序。
- 基于预排序分桶并初始化Elo分数（bucket-aware）。
- 进行不确定性引导的人在环MergeSort：仅对高不确定性对进行人工比较；“容易/明显”的对由自动比较（基于CLIP/Elo）替代。
- 贯穿使用主动采样策略以最小化查询数量。

Result: 在FGNET（人脸年龄）、DHCI（历史图像年代）、EyePACS（视网膜图像质量）上验证：在n=100时，相比穷举成对比较减少人工成本90.5%；较此前基于排序的主动方法再降19.8%；同时保持或提升评审者间一致性（IRR）。

Conclusion: 将CLIP先验与不确定性感知采样结合可在多领域实现高效、可扩展的成对排序，显著节省人工且不损害可靠性，适合主观或困难标注任务。

Abstract: Pairwise comparison is often favored over absolute rating or ordinal
classification in subjective or difficult annotation tasks due to its improved
reliability. However, exhaustive comparisons require a massive number of
annotations (O(n^2)). Recent work has greatly reduced the annotation burden
(O(n log n)) by actively sampling pairwise comparisons using a sorting
algorithm. We further improve annotation efficiency by (1) roughly pre-ordering
items using the Contrastive Language-Image Pre-training (CLIP) model
hierarchically without training, and (2) replacing easy, obvious human
comparisons with automated comparisons. The proposed EZ-Sort first produces a
CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,
and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation
was conducted using various datasets: face-age estimation (FGNET), historical
image chronology (DHCI), and retinal image quality assessment (EyePACS). It
showed that EZ-Sort reduced human annotation cost by 90.5% compared to
exhaustive pairwise comparisons and by 19.8% compared to prior work (when n =
100), while improving or maintaining inter-rater reliability. These results
demonstrate that combining CLIP-based priors with uncertainty-aware sampling
yields an efficient and scalable solution for pairwise ranking.

</details>


### [38] [ECHO: Ego-Centric modeling of Human-Object interactions](https://arxiv.org/abs/2508.21556)
*Ilya A. Petrov,Vladimir Guzov,Riccardo Marin,Emre Aksan,Xu Chen,Daniel Cremers,Thabo Beeler,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: ECHO提出从仅有头部与手腕追踪的极简输入中，同时重建人体姿态、物体运动与接触序列，基于三变量扩散的Diffusion Transformer，并在头中心坐标系与传送带式推理下实现任意长度序列的鲁棒重建，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备普及带来大量第一人称数据，但仅依赖头/腕追踪如何重建完整的以人-物交互为核心的三模态信息（人姿、物体轨迹、接触）仍缺乏统一而灵活的方案。

Method: 提出ECHO：1) 采用Diffusion Transformer；2) 设计三变量（人、物、接触）联合扩散过程，统一建模并允许多种输入配置；3) 在头中心（head-centric）规范空间中运作，降低对全局朝向的敏感；4) 提出“传送带式”推理，将帧位置与扩散时间步渐进对齐，从而可处理任意长度序列。

Result: 在多项评测中优于现有方法，特别是在灵活输入和三模态联合重建方面达到新的SOTA表现。

Conclusion: 极简传感输入（头+手腕）即可通过三变量扩散与Transformer联合建模，稳定、灵活地重建第一人称的人-物交互三模态信息，为可穿戴与自我中心HOI重建奠定新基线。

Abstract: Modeling human-object interactions (HOI) from an egocentric perspective is a
largely unexplored yet important problem due to the increasing adoption of
wearable devices, such as smart glasses and watches. We investigate how much
information about interaction can be recovered from only head and wrists
tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object
interactions), which, for the first time, proposes a unified framework to
recover three modalities: human pose, object motion, and contact from such
minimal observation. ECHO employs a Diffusion Transformer architecture and a
unique three-variate diffusion process, which jointly models human motion,
object trajectory, and contact sequence, allowing for flexible input
configurations. Our method operates in a head-centric canonical space,
enhancing robustness to global orientation. We propose a conveyor-based
inference, which progressively increases the diffusion timestamp with the frame
position, allowing us to process sequences of any length. Through extensive
evaluation, we demonstrate that ECHO outperforms existing methods that do not
offer the same flexibility, setting a state-of-the-art in egocentric HOI
reconstruction.

</details>


### [39] [How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images](https://arxiv.org/abs/2508.21565)
*Juneyoung Ro,Namwoo Kim,Yoonjin Yoon*

Main category: cs.CV

TL;DR: 对比评估三种主流VLM（BLIP-2、InstructBLIP、LLaVA-1.5）在城市场景细粒度空间推理上的表现，发现基于合成的城市VQA+链式思维数据进行微调能显著提升，尤其在否定与反事实问题上。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多在通用场景预训练，缺乏对城市街景中对象、布局与深度等细粒度空间推理能力的系统评估与适配方法。需要检验其零样本可迁移性，并探索高效的领域自适应路径。

Method: 1) 构建城市场景合成VQA数据集：利用街景图像的分割、深度、检测模型预测生成结构化标注；2) 由LLM生成带CoT的逐步推理答案；3) 选取BLIP-2、InstructBLIP、LLaVA-1.5进行零样本评测与在该数据上的微调；4) 针对难题型（否定、反事实）做分项评估。

Result: 零样本下三种模型表现尚可；使用合成、带CoT监督的数据微调后，整体与在难题型上的表现大幅提升，显示CoT与领域特化数据对空间推理尤为有效。

Conclusion: 城市空间推理可作为VLM的新基准任务；利用感知模型生成的合成数据并配合LLM的CoT监督，是将通用VLM高效适配到专用城市场景的可行方案。

Abstract: Effectively understanding urban scenes requires fine-grained spatial
reasoning about objects, layouts, and depth cues. However, how well current
vision-language models (VLMs), pretrained on general scenes, transfer these
abilities to urban domain remains underexplored. To address this gap, we
conduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,
and LLaVA-1.5-evaluating both zero-shot performance and the effects of
fine-tuning with a synthetic VQA dataset specific to urban scenes. We construct
such dataset from segmentation, depth, and object detection predictions of
street-view images, pairing each question with LLM-generated Chain-of-Thought
(CoT) answers for step-by-step reasoning supervision. Results show that while
VLMs perform reasonably well in zero-shot settings, fine-tuning with our
synthetic CoT-supervised dataset substantially boosts performance, especially
for challenging question types such as negation and counterfactuals. This study
introduces urban spatial reasoning as a new challenge for VLMs and demonstrates
synthetic dataset construction as a practical path for adapting general-purpose
models to specialized domains.

</details>


### [40] [Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging](https://arxiv.org/abs/2508.21580)
*Nico Albert Disch,Yannick Kirchhoff,Robin Peretzke,Maximilian Rokuss,Saikat Roy,Constantin Ulrich,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 提出Temporal Flow Matching（TFM），一种统一的生成式时序轨迹学习方法，可处理3D体数据、多个历史扫描、不规则采样，能退化为最近图像预测基线（LCI）。在三个纵向公开数据集上超过自然图像领域的时空方法，树立4D医学图像预测新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习多聚焦单时刻或分类/回归任务，难以进行精细空间预测；已有方法常受限于单时间点、特定疾病或技术限制。需要一种通用、可扩展、能学习潜在时间分布并适配医学成像多样时序场景的方法。

Method: 提出Temporal Flow Matching（TFM）：以生成式“流匹配/轨迹”框架学习随时间演化的影像分布；设计上可退化为“最近图像预测（LCI）”作为特例；支持3D体数据、多个先验扫描、以及时间点不规则采样，进行4D（3D+时间）预测。

Result: 在三个公开的纵向医学影像数据集上，TFM较现有来自自然图像领域的时空方法取得一致超越，形成新的稳健基线。

Conclusion: TFM弥补了医学影像时序建模的核心空缺：统一、生成式、可扩展，能在多样时序设置下进行细粒度空间预测，并提供强健SOTA基线与可退化到LCI的安全下界。

Abstract: Understanding temporal dynamics in medical imaging is crucial for
applications such as disease progression modeling, treatment planning and
anatomical development tracking. However, most deep learning methods either
consider only single temporal contexts, or focus on tasks like classification
or regression, limiting their ability for fine-grained spatial predictions.
While some approaches have been explored, they are often limited to single
timepoints, specific diseases or have other technical restrictions. To address
this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified
generative trajectory method that (i) aims to learn the underlying temporal
distribution, (ii) by design can fall back to a nearest image predictor, i.e.
predicting the last context image (LCI), as a special case, and (iii) supports
$3D$ volumes, multiple prior scans, and irregular sampling. Extensive
benchmarks on three public longitudinal datasets show that TFM consistently
surpasses spatio-temporal methods from natural imaging, establishing a new
state-of-the-art and robust baseline for $4D$ medical image prediction.

</details>


### [41] [Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer](https://arxiv.org/abs/2508.21581)
*Daniël Boeke,Cedrik Blommestijn,Rebecca N. Wray,Kalina Chupetlovska,Shangqi Gao,Zeyu Gao,Regina G. H. Beets-Tan,Mireia Crispin-Ortuzar,James O. Jones,Wilson Silva,Ines P. Machado*

Main category: cs.CV

TL;DR: 该研究在肾透明细胞癌(ccRCC)中比较并融合术前CT与术后病理WSI，用深度学习与Cox生存模型预测复发风险。病理单模态优于CT，跨模态中间层融合最佳，性能接近经校正的Leibovich评分；简单拼接仅在融合时体现CT增益。提示多模态基础模型整合可用于个体化风险预测，但需要更大数据与更强融合/CT编码器以进一步提升。


<details>
  <summary>Details</summary>
Motivation: 现有Leibovich评分虽常用，但分辨率有限且不纳入影像信息，难以实现患者级个体化复发风险评估。基础模型兴起为多模态整合（影像+病理）提供了新契机，需要验证其在真实队列中的可行性与增益。

Method: 构建模块化深度学习框架：使用预训练编码器分别提取CT与WSI特征，基于Cox比例风险进行生存建模；比较单模态、后期融合（late fusion）与中间融合（intermediate fusion）多种设置。评估包括不同编码器（如ResNet-18）与特定模型（TITAN-CONCH），并与（调整后的）Leibovich评分对照；采用随机打破并列排名以检验离散化对性能的影响。

Result: - WSI单模态稳健且显著优于仅用CT，显示病理对预后更有信息量。
- 中间融合进一步提升，最佳组合（TITAN-CONCH+ResNet-18）性能接近调整后的Leibovich评分。
- 随机tie-breaking后，临床基线与学习模型的差距缩小，提示基于离散分层的评分可能夸大个体化性能。
- 简单向量拼接下，CT的主要价值体现在与WSI融合时，而非单独使用。

Conclusion: 多模态（CT+WSI）整合在ccRCC复发风险个体化预测上可行且有益，尤其以中间融合为佳；病理信息是核心驱动。需在更大多模态数据集上探索更强的融合策略与通用CT编码器，以匹配病理建模能力并进一步超越临床评分。

Abstract: Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is
essential for guiding postoperative surveillance and treatment. The Leibovich
score remains widely used for stratifying distant recurrence risk but offers
limited patient-level resolution and excludes imaging information. This study
evaluates multimodal recurrence prediction by integrating preoperative computed
tomography (CT) and postoperative histopathology whole-slide images (WSIs). A
modular deep learning framework with pretrained encoders and Cox-based survival
modeling was tested across unimodal, late fusion, and intermediate fusion
setups. In a real-world ccRCC cohort, WSI-based models consistently
outperformed CT-only models, underscoring the prognostic strength of pathology.
Intermediate fusion further improved performance, with the best model
(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random
tie-breaking narrowed the gap between the clinical baseline and learned models,
suggesting discretization may overstate individualized performance. Using
simple embedding concatenation, radiology added value primarily through fusion.
These findings demonstrate the feasibility of foundation model-based multimodal
integration for personalized ccRCC risk prediction. Future work should explore
more expressive fusion strategies, larger multimodal datasets, and
general-purpose CT encoders to better match pathology modeling capacity.

</details>


### [42] [Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation](https://arxiv.org/abs/2508.21657)
*Haomiao Zhang,Zhangyuan Li,Yanling Piao,Zhi Li,Xiaodong Wang,Miao Cao,Xiongfei Su,Qiang Song,Xin Yuan*

Main category: cs.CV

TL;DR: 提出一种结合物理可解释优化与深度学习的解缠式网络，用自适应带宽传播与复数域注意力去噪，扩大工作距离、增强全局建模，在模拟与实测CGH重建中达SOTA（PSNR>35 dB）。


<details>
  <summary>Details</summary>
Motivation: 现有CGH深度方法存在三大痛点：1）端到端黑箱忽略物理先验，解释性与可控性差；2）CNN感受野有限，难捕获长程依赖与全局上下文；3）基于ASM的传播受近场限制，工作距离窄，影响稳定与精度。

Method: 采用深度展开（基于梯度下降）框架，将一次迭代分解为两模块：1）ABPM（自适应带宽保持模型）：在传播算子中自适应限制/分配频带，突破ASM近场限制，支持更远工作距离；2）PCD（相位域复数值去噪器）：在相位/复数域使用可变形自注意力（complex-valued deformable self-attention）建模全局依赖，抑制噪声与伪影。两模块端到端联合训练，具有物理约束与学习能力的平衡。

Result: 在模拟与真实数据上取得SOTA，重建质量显著提升；PSNR超过35 dB；对长距离传播与复杂场景具有更强稳健性与泛化。

Conclusion: 通过将物理传播先验（ABPM）与复数域全局建模去噪（PCD）嵌入深度展开框架，方法在解释性、灵活性、全局上下文建模与工作距离上均优于传统端到端与ASM/CNN方案，提供稳定高精度的CGH重建。

Abstract: Computer-generated holography (CGH) has gained wide attention with deep
learning-based algorithms. However, due to its nonlinear and ill-posed nature,
challenges remain in achieving accurate and stable reconstruction.
Specifically, ($i$) the widely used end-to-end networks treat the
reconstruction model as a black box, ignoring underlying physical
relationships, which reduces interpretability and flexibility. ($ii$) CNN-based
CGH algorithms have limited receptive fields, hindering their ability to
capture long-range dependencies and global context. ($iii$) Angular spectrum
method (ASM)-based models are constrained to finite near-fields.In this paper,
we propose a Deep Unfolding Network (DUN) that decomposes gradient descent into
two modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domain
complex-valued denoiser (PCD), providing more flexibility. ABPM allows for
wider working distances compared to ASM-based methods. At the same time, PCD
leverages its complex-valued deformable self-attention module to capture global
features and enhance performance, achieving a PSNR over 35 dB. Experiments on
simulated and real data show state-of-the-art results.

</details>


### [43] [Towards Interactive Lesion Segmentation in Whole-Body PET/CT with Promptable Models](https://arxiv.org/abs/2508.21680)
*Maximilian Rokuss,Yannick Kirchhoff,Fabian Isensee,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Whole-body PET/CT is a cornerstone of oncological imaging, yet accurate
lesion segmentation remains challenging due to tracer heterogeneity,
physiological uptake, and multi-center variability. While fully automated
methods have advanced substantially, clinical practice benefits from approaches
that keep humans in the loop to efficiently refine predicted masks. The
autoPET/CT IV challenge addresses this need by introducing interactive
segmentation tasks based on simulated user prompts. In this work, we present
our submission to Task 1. Building on the winning autoPET III nnU-Net pipeline,
we extend the framework with promptable capabilities by encoding user-provided
foreground and background clicks as additional input channels. We
systematically investigate representations for spatial prompts and demonstrate
that Euclidean Distance Transform (EDT) encodings consistently outperform
Gaussian kernels. Furthermore, we propose online simulation of user
interactions and a custom point sampling strategy to improve robustness under
realistic prompting conditions. Our ensemble of EDT-based models, trained with
and without external data, achieves the strongest cross-validation performance,
reducing both false positives and false negatives compared to baseline models.
These results highlight the potential of promptable models to enable efficient,
user-guided segmentation workflows in multi-tracer, multi-center PET/CT. Code
is publicly available at https://github.com/MIC-DKFZ/autoPET-interactive

</details>


### [44] [Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping](https://arxiv.org/abs/2508.21689)
*Fatih Erdoğan,Merve Rabia Barın,Fatma Güney*

Main category: cs.CV

TL;DR: 提出一种基于几何先验并带概率置信投影的BEV映射方法，替代仅靠注意力投影，显著减少幻觉元素、提升长距离与跨数据集泛化；在nuScenes与Argoverse2新划分上优于SOTA，并通过置信驱动的时序累积进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有从图像到BEV的HD地图构建常依赖注意力等学习式投影，易受域移影响出现要素“幻觉”，投影精度不足限制最终矢量地图质量；需要一种能利用相机几何、并具备自适应与不确定性评估的投影机制，以提升对真实道路要素的对齐与筛选能力，尤其在长距离感知与跨数据集泛化上。

Method: 1) 以相机内外参为先验进行几何映射，将图像要素初投影到BEV；2) 设计概率投影机制：为每个投影赋予可学习的置信度，用于微调映射使其贴合场景，并对噪声/无关元素进行滤除；3) 置信度引导的时序融合：跨帧仅选择性累积高置信信息，抑制漂移与噪声；4) 在nuScenes与Argoverse2上以新划分评估，与SOTA对比。

Result: 在nuScenes与Argoverse2的新数据划分上均取得优于SOTA的指标，提升在nuScenes更为显著；在远距离（长感知范围）场景下优势突出，表明方法在困难设置与域泛化方面更稳健；开源代码与模型权重可复现。

Conclusion: 基于几何先验并结合可学习置信的概率投影，可在图像到BEV的HD地图构建中同时提升对齐精度、抑制幻觉并增强时序融合的可靠性，从而带来更好的泛化与远距性能；该范式为取代纯注意力投影提供了有效路径。

Abstract: Constructing high-definition (HD) maps from sensory input requires accurately
mapping the road elements in image space to the Bird's Eye View (BEV) space.
The precision of this mapping directly impacts the quality of the final
vectorized HD map. Existing HD mapping approaches outsource the projection to
standard mapping techniques, such as attention-based ones. However, these
methods struggle with accuracy due to generalization problems, often
hallucinating non-existent road elements. Our key idea is to start with a
geometric mapping based on camera parameters and adapt it to the scene to
extract relevant map information from camera images. To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation. In addition,
we improve temporal processing by using confidence scores to selectively
accumulate reliable information over time. Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization. The improvements
are particularly pronounced on nuScenes and in the challenging long perception
range. Our code and model checkpoints are available at
https://github.com/Fatih-Erdogan/mapping-like-skeptic .

</details>


### [45] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 提出将OCR从“词级检测+识别”升级为“行级端到端识别”，以绕过易错的词切分并更好利用长上下文语言模型；并发布了一个251页、带行级标注的数据集。实验显示端到端准确率提升5.4%，效率提升4倍。


<details>
  <summary>Details</summary>
Motivation: 传统OCR先分割字符再识别，误差集中在字符切分且缺乏上下文；现代方法改为词级检测再序列识别，但瓶颈转移到词切分（检测）误差，且上下文仍局限。为减少切分错误并增强语言模型利用，需要进一步从词级过渡到行级。

Method: 构建行级OCR管线：直接在整行图像上进行序列到序列识别，跳过词检测与词切分；结合语言模型以利用更长的句子级上下文。为训练与评测，作者整理并发布了251张英文文档页、带行级标注的数据集。

Result: 在所建数据集与实验设置下，相比词级OCR，端到端准确率提升5.4%，推理效率提升约4倍。

Conclusion: 行级OCR在准确率与效率上均优于词级流程，且更能受益于不断进步的（大）语言模型；公开的数据集为该方向提供了训练与基准基础。

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


### [46] [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](https://arxiv.org/abs/2508.21712)
*Alvaro Patricio,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: 提出FLORA：用LoRA微调Flux 1.1 Dev扩散模型，低算力生成高质量合成数据，显著提升小样本目标检测性能；用500张合成图超越ODGEN用5000张的效果，mAP@0.5:0.95最高+21.3%。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的数据增强依赖对大模型全量微调与海量合成图，算力与数据成本高（需V100级GPU与数千张图），限制了在资源受限场景中的可用性与可及性。作者希望在消费级GPU上以更少的合成样本获得更好的检测增益。

Method: 提出FLORA管线：选用Flux 1.1 Dev扩散基础模型，仅通过LoRA进行轻量微调以适配目标域；在生成阶段聚焦质量与多样性控制（隐含包括类别/属性条件、采样与过滤策略等），以小规模（~500张）高质量合成图增强目标检测训练；与ODGEN等强基线在多个检测数据集上对比。

Result: 在7个多样化目标检测数据集上，使用仅500张FLORA生成图训练的检测器，性能优于使用ODGEN生成的5000张图的模型，mAP@0.5:0.95最高提升21.3%；所需算力显著降低（消费级RTX 4090即可），数据与计算成本均大幅下降。

Conclusion: 质量与效率优先的轻量生成策略可替代“堆量”式合成，能以更少数据与更低算力实现更优检测增益；FLORA提升了合成数据增强在现实落地中的可行性与可及性。

Abstract: Recent advances in diffusion-based generative models have demonstrated
significant potential in augmenting scarce datasets for object detection tasks.
Nevertheless, most recent models rely on resource-intensive full fine-tuning of
large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA
V100) and thousands of synthetic images. To address these limitations, we
propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation
pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces
computational requirements, enabling synthetic dataset generation with a
consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our
approach on seven diverse object detection datasets. Our results demonstrate
that training object detectors with just 500 synthetic images generated by our
approach yields superior detection performance compared to models trained on
5000 synthetic images from the ODGEN baseline, achieving improvements of up to
21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost. This work demonstrates that a quality and efficiency-focused approach is
more effective than brute-force generation, making advanced synthetic data
creation more practical and accessible for real-world scenarios.

</details>


### [47] [Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks](https://arxiv.org/abs/2508.21715)
*Amirhossein Nazeri,Wael Hafez*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Convolutional Neural Networks (CNNs) have become the foundation of modern
computer vision, achieving unprecedented accuracy across diverse image
recognition tasks. While these networks excel on in-distribution data, they
remain vulnerable to adversarial perturbations imperceptible input
modifications that cause misclassification with high confidence. However,
existing detection methods either require expensive retraining, modify network
architecture, or degrade performance on clean inputs. Here we show that
adversarial perturbations create immediate, detectable entropy signatures in
CNN activations that can be monitored without any model modification. Using
parallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs
consistently shift activation entropy by 7% in early convolutional layers,
enabling 90% detection accuracy with false positives and false negative rates
below 20%. The complete separation between clean and adversarial entropy
distributions reveals that CNNs inherently encode distribution shifts in their
activation patterns. This work establishes that CNN reliability can be assessed
through activation entropy alone, enabling practical deployment of
self-diagnostic vision systems that detect adversarial inputs in real-time
without compromising original model performance.

</details>


### [48] [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](https://arxiv.org/abs/2508.21732)
*João Valente,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: 提出CAD2DMD-SET合成数据生成工具与DMDBench实测验证集，以提升LVLM在数字测量设备读数场景（杂乱、遮挡、极端视角、运动模糊等）上的VQA能力；用LoRA微调后，多款SOTA LVLM在ANLS上显著提升，InternVL提升约200%，且对其他任务无负迁移；工具将开源。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在一般多模态任务表现强，但在看似简单的DMD读数任务上在真实复杂环境中失败率高，尤其AR/头戴相机常见的杂乱、遮挡、视角极端与模糊等条件下。缺乏针对性、高多样性的标注数据集与可靠评测基准是瓶颈。

Method: 1) CAD2DMD-SET：基于3D CAD模型、物理一致渲染与高保真图像合成，自动生成多样化、带VQA标签的DMD合成数据，涵盖多种设备与扰动；2) DMDBench：1000张真实世界标注图像的验证集，体现实际约束；3) 选取三款SOTA LVLM，采用ANLS评估；4) 使用LoRA在CAD2DMD-SET上微调，并检验对其他任务的影响。

Result: 在DMDBench上，三种SOTA LVLM经LoRA微调后均显著提升；InternVL的ANLS分数约提升200%；未观察到对其他任务性能的明显退化。

Conclusion: 合成数据驱动的针对性训练能显著增强LVLM在DMD读数等“看似简单、实则苛刻”的应用鲁棒性与准确性。CAD2DMD-SET+DMDBench为社区提供了可扩展的数据生成与评测框架，预期开源将促进更多设备类型的覆盖与实用落地。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
impressive capabilities across various multimodal tasks. They continue,
however, to struggle with trivial scenarios such as reading values from Digital
Measurement Devices (DMDs), particularly in real-world conditions involving
clutter, occlusions, extreme viewpoints, and motion blur; common in
head-mounted cameras and Augmented Reality (AR) applications. Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.
Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks. This
demonstrates that the CAD2DMD-SET training dataset substantially improves the
robustness and performance of LVLMs when operating under the previously stated
challenging conditions. The CAD2DMD-SET tool is expected to be released as
open-source once the final version of this manuscript is prepared, allowing the
community to add different measurement devices and generate their own datasets.

</details>


### [49] [Learning from Silence and Noise for Visual Sound Source Localization](https://arxiv.org/abs/2508.21761)
*Xavier Juanola,Giovana Morais,Magdalena Fuentes,Gloria Haro*

Main category: cs.CV

TL;DR: 提出SSL-SaN：在训练中显式加入静音与噪声等“负音频”，提升可见声源定位与跨模态检索，并在正负样本上更鲁棒；同时提出衡量对齐-可分离权衡的新指标，并发布包含负音频的合成数据集IS3+。


<details>
  <summary>Details</summary>
Motivation: 现有视觉声源定位方法在“负音频”（静音、噪声、离屏声）场景表现差，且评测多局限于单一可见声源的正样本，无法全面反映模型在复杂真实场景中的鲁棒性与泛化能力。

Method: 1) 训练策略：在自监督框架中显式注入静音与噪声样本，设计损失与采样机制，促使音频-视觉表征既能在正配对上对齐，也能在负配对上分离；模型命名为SSL-SaN。2) 指标：提出新度量，量化正负配对下听觉-视觉特征的对齐与可分离的权衡。3) 数据：发布扩展的合成数据集IS3+，包含负音频，以支持训练与更全面评测。

Result: SSL-SaN在声源定位与跨模态检索两项上均达到自监督方法的SOTA；在含负音频与低语义对应的情形下更稳健。新指标能更全面评估模型对齐-分离能力；IS3+为社区提供包含负音频的测试基准。

Conclusion: 通过在训练中系统性引入负音频并配套新指标与数据集，SSL-SaN显著提升正样本表现与对负样本的鲁棒性，为视觉声源定位的自监督学习与评测建立更可靠的基线与标准。

Abstract: Visual sound source localization is a fundamental perception task that aims
to detect the location of sounding sources in a video given its audio. Despite
recent progress, we identify two shortcomings in current methods: 1) most
approaches perform poorly in cases with low audio-visual semantic
correspondence such as silence, noise, and offscreen sounds, i.e. in the
presence of negative audio; and 2) most prior evaluations are limited to
positive cases, where both datasets and metrics convey scenarios with a single
visible sound source in the scene. To address this, we introduce three key
contributions. First, we propose a new training strategy that incorporates
silence and noise, which improves performance in positive cases, while being
more robust against negative sounds. Our resulting self-supervised model,
SSL-SaN, achieves state-of-the-art performance compared to other
self-supervised models, both in sound localization and cross-modal retrieval.
Second, we propose a new metric that quantifies the trade-off between alignment
and separability of auditory and visual features across positive and negative
audio-visual pairs. Third, we present IS3+, an extended and improved version of
the IS3 synthetic dataset with negative audio.
  Our data, metrics and code are available on the
https://xavijuanola.github.io/SSL-SaN/.

</details>


### [50] [UItron: Foundational GUI Agent with Advanced Perception and Planning](https://arxiv.org/abs/2508.21767)
*Zhixiong Zeng,Jing Huang,Liming Zheng,Wenkang Han,Yufeng Zhong,Lei Chen,Longrong Yang,Yingjie Chu,Yuzhi He,Lin Ma*

Main category: cs.CV

TL;DR: UItron 是一个面向移动/PC 的开源 GUI 智能体基础模型，结合系统化数据工程与交互基础设施，通过监督微调与课程式强化学习，在GUI感知、指代落地与规划上取得SOTA，尤其在中文移动App场景显著领先。


<details>
  <summary>Details</summary>
Motivation: 现有 GUI 智能体受限于：可用操作轨迹稀缺、缺乏统一交互环境、基础模型在GUI任务上的能力不足，尤其在中文场景表现不佳。作者希望提供一套从数据到系统到训练范式的完整方案，提升真实设备/应用上的自动化操作能力。

Method: 1) 构建交互基础设施：联通移动与PC设备，支持离线与在线评测。2) 大规模数据工程：系统性策略收集并清洗标注数据，重点补齐中文移动App，人工采集100万+步骤、覆盖最热门100款App；构建感知、指代落地（grounding）、规划等多任务数据。3) 训练范式：先对感知与规划任务做监督微调（SFT），再以课程式强化学习（逐步提升任务复杂度）在在线环境中进行探索与复杂推理能力的学习。

Result: 在GUI感知、指代落地与规划基准上取得优越成绩；在中文移动App交互中显著超过现有方案。离线与在线评测环境显示其在真实应用中的操作成功率大幅提升。

Conclusion: 系统性的数据工程+交互基础设施+SFT→课程式RL的范式能显著提升GUI智能体的通用性与实用性，尤其弥补了中文场景短板。UItron 将GUI智能体更进一步推向真实世界落地。

Abstract: GUI agent aims to enable automated operations on Mobile/PC devices, which is
an important task toward achieving artificial general intelligence. The rapid
advancement of VLMs accelerates the development of GUI agents, owing to their
powerful capabilities in visual understanding and task planning. However,
building a GUI agent remains a challenging task due to the scarcity of
operation trajectories, the availability of interactive infrastructure, and the
limitation of initial capabilities in foundation models. In this work, we
introduce UItron, an open-source foundational model for automatic GUI agents,
featuring advanced GUI perception, grounding, and planning capabilities. UItron
highlights the necessity of systemic data engineering and interactive
infrastructure as foundational components for advancing GUI agent development.
It not only systematically studies a series of data engineering strategies to
enhance training effects, but also establishes an interactive environment
connecting both Mobile and PC devices. In training, UItron adopts supervised
finetuning over perception and planning tasks in various GUI scenarios, and
then develop a curriculum reinforcement learning framework to enable complex
reasoning and exploration for online environments. As a result, UItron achieves
superior performance in benchmarks of GUI perception, grounding, and planning.
In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions. To this end, we manually collect over one
million steps of operation trajectories across the top 100 most popular apps,
and build the offline and online agent evaluation environments. Experimental
results demonstrate that UItron achieves significant progress in Chinese app
scenarios, propelling GUI agents one step closer to real-world application.

</details>


### [51] [Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations](https://arxiv.org/abs/2508.21769)
*Ha Min Son,Zhe Zhao,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is
challenging, as web-scale pretraining data potentially covers many existing
benchmarks. Consequently, current DG evaluation may neither be sufficiently
challenging nor adequately test genuinely unseen data scenarios. To better
assess the performance of CLIP on DG in-the-wild, a scenario where CLIP
encounters challenging unseen data, we consider two approaches: (1) evaluating
on 33 diverse datasets with quantified out-of-distribution (OOD) scores after
fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'
some domains as an approximation. We observe that CLIP's performance
deteriorates significantly on more OOD datasets. To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations). Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization. We instead
hypothesize that enhancing domain awareness is a prerequisite for effective
domain-invariant classification in foundation models. CLIP-DCA identifies and
enhances domain awareness within CLIP's encoders using a separate domain head
and synthetically generated diverse domain data. Simultaneously, it encourages
domain-invariant classification through disentanglement from the domain
features. CLIP-DCA shows significant improvements within this challenging
evaluation compared to existing methods, particularly on datasets that are more
OOD.

</details>


### [52] [What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos](https://arxiv.org/abs/2508.21770)
*Qiyue Sun,Qiming Huang,Yang Yang,Hongjun Wang,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出并构建包含科幻、动画等“非典型”视频的新数据集，研究其在开放世界学习中的作用；将这些视频用于表示学习后，在OOD检测、类别发现与零样本动作识别上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频研究多依赖封闭集的常见/典型数据，缺乏对开放世界中新奇、罕见概念的探索；现实世界充满异常分布与未见类别，模型需要具备发现与泛化能力。

Method: 1) 收集多源、多风格的非典型视频数据集；2) 将该数据用于表示学习阶段（用作训练“辅料”而非特定任务监督）；3) 在三类开放世界任务上评估：OOD检测、NCD（新类别发现）、ZSAR（零样本动作识别）；4) 系统比较不同语义/类别多样性与数据规模设置对性能的影响。

Result: - 引入非典型视频后，即便使用简单学习策略，各任务均有稳定提升；- 提高非典型样本的“类别多样性”显著增强OOD检测；- 对NCD而言，小而语义多样的非典型集优于大而典型的集；- 在ZSAR中，非典型视频的语义多样性提升对未见动作类别的泛化。

Conclusion: 非典型视频能有效增强开放世界视觉表示学习；语义与类别多样性是关键杠杆；所给新数据集与发现为后续在开放世界视频理解方向提供了有力基线与研究契机。

Abstract: Humans usually show exceptional generalisation and discovery ability in the
open world, when being shown uncommon new concepts. Whereas most existing
studies in the literature focus on common typical data from closed sets,
open-world novel discovery is under-explored in videos. In this paper, we are
interested in asking: \textit{What if atypical unusual videos are exposed in
the learning process?} To this end, we collect a new video dataset consisting
of various types of unusual atypical data (\eg sci-fi, animation, \etc). To
study how such atypical data may benefit open-world learning, we feed them into
the model training process for representation learning. Focusing on three key
tasks in open-world learning: out-of-distribution (OOD) detection, novel
category discovery (NCD), and zero-shot action recognition (ZSAR), we found
that even straightforward learning approaches with atypical data consistently
improve performance across various settings. Furthermore, we found that
increasing the categorical diversity of the atypical samples further boosts OOD
detection performance. Additionally, in the NCD task, using a smaller yet more
semantically diverse set of atypical samples leads to better performance
compared to using a larger but more typical dataset. In the ZSAR setting, the
semantic diversity of atypical videos helps the model generalise better to
unseen action classes. These observations in our extensive experimental
evaluations reveal the benefits of atypical videos for visual representation
learning in the open world, together with the newly proposed dataset,
encouraging further studies in this direction.

</details>


### [53] [Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering](https://arxiv.org/abs/2508.21773)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 论文提出无监督视频持续学习(uVCL)的现实设定与非参数解法：无标签、无任务边界，利用无监督视频Transformer特征+KDE建模，结合新颖性检测与记忆簇动态扩展，并通过跨任务迁移提升性能；在UCF101、HMDB51、Something-Something V2上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习多为有监督并依赖标签与任务边界，难以适用于获取成本高且无结构的视频场景；视频相较图像有更高时空复杂度与资源开销，无监督持续学习几乎未被系统探索，亟需通用基准与方法。

Method: 1) 设定无监督视频持续学习协议：各任务包含无结构视频类别且无边界/标签；2) 用无监督视频Transformer提取深度时空特征；3) 以核密度估计(KDE)对特征进行非参数概率建模，形成记忆簇表示；4) 基于新颖性检测判别新任务数据分布漂移，动态扩展或更新记忆簇以吸纳新知识；5) 采用跨任务迁移，将已学参数作为当前任务的初始化以稳定学习与缓解遗忘。

Result: 在UCF101、HMDB51、Something-Something V2上，无需任何标签或类别边界即可实现显著优于基线的连续多任务学习性能；方法在长期任务序列中表现稳健，体现更好的知识保留与新知识获取。

Conclusion: 非参数KDE结合无监督视频特征与新颖性驱动的记忆簇扩展，为无监督视频持续学习提供了有效框架；通用基准与协议促进该领域研究，方法在多数据集上验证了可扩展性与有效性。

Abstract: We propose a realistic scenario for the unsupervised video learning where
neither task boundaries nor labels are provided when learning a succession of
tasks. We also provide a non-parametric learning solution for the
under-explored problem of unsupervised video continual learning. Videos
represent a complex and rich spatio-temporal media information, widely used in
many applications, but which have not been sufficiently explored in
unsupervised continual learning. Prior studies have only focused on supervised
continual learning, relying on the knowledge of labels and task boundaries,
while having labeled data is costly and not practical. To address this gap, we
study the unsupervised video continual learning (uVCL). uVCL raises more
challenges due to the additional computational and memory requirements of
processing videos when compared to images. We introduce a general benchmark
experimental protocol for uVCL by considering the learning of unstructured
video data categories during each task. We propose to use the Kernel Density
Estimation (KDE) of deep embedded video features extracted by unsupervised
video transformer networks as a non-parametric probabilistic representation of
the data. We introduce a novelty detection criterion for the incoming new task
data, dynamically enabling the expansion of memory clusters, aiming to capture
new knowledge when learning a succession of tasks. We leverage the use of
transfer learning from the previous tasks as an initial state for the knowledge
transfer to the current learning task. We found that the proposed methodology
substantially enhances the performance of the model when successively learning
many tasks. We perform in-depth evaluations on three standard video action
recognition datasets, including UCF101, HMDB51, and Something-to-Something V2,
without using any labels or class boundaries.

</details>


### [54] [A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI](https://arxiv.org/abs/2508.21775)
*Omer Faruk Durugol,Maximilian Rokuss,Yannick Kirchhoff,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: 提出基于nnU-Net的PDAC MRI自动分割方案，通过多阶段级联预训练与度量感知集成，在PANTHER挑战两项任务上取得领先表现（Task1 Dice 0.661、Task2 Dice 0.523，边界指标SOTA）。


<details>
  <summary>Details</summary>
Motivation: PDAC在MRI上肿瘤与组织对比差、标注稀缺，导致自动分割困难；临床需要可靠的体积与边界定位以支持诊断与治疗规划。

Method: 以nnU-Net为基座，采用深度多阶段级联预训练：从通用解剖基础模型→CT胰腺病灶数据→目标MRI（T1用于诊断、T2用于治疗）逐步微调；五折交叉验证系统评估数据增强与训练日程；发现“体积准确度 vs 边界精度”的关键权衡（强增强提升Dice，默认增强提升MASD/HD95）；据此构建异质“专家混合”集成：面向不同指标训练的专长模型按度量感知进行组合。

Result: 在Task1上实现SOTA边界（MASD 5.46 mm，HD95 17.33 mm），并获得Dice 0.661；在Task2上Dice 0.523；跨验证稳定。

Conclusion: 多阶段级联预训练+度量感知异质集成能在小数据、低对比的医学影像任务中取得鲁棒高性能；针对不同评测指标训练专长模型并进行集成可显著提升总体表现。

Abstract: Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is
critical for clinical workflows but is hindered by poor tumor-tissue contrast
and a scarcity of annotated data. This paper details our submission to the
PANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and
therapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the
nnU-Net framework and leverages a deep, multi-stage cascaded pre-training
strategy, starting from a general anatomical foundation model and sequentially
fine-tuning on CT pancreatic lesion datasets and the target MRI modalities.
Through extensive five-fold cross-validation, we systematically evaluated data
augmentation schemes and training schedules. Our analysis revealed a critical
trade-off, where aggressive data augmentation produced the highest volumetric
accuracy, while default augmentations yielded superior boundary precision
(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).
For our final submission, we exploited this finding by constructing custom,
heterogeneous ensembles of specialist models, essentially creating a mix of
experts. This metric-aware ensembling strategy proved highly effective,
achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523
for Task 2. Our work presents a robust methodology for developing specialized,
high-performance models in the context of limited data and complex medical
imaging tasks (Team MIC-DKFZ).

</details>


### [55] [Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight](https://arxiv.org/abs/2508.21777)
*Ugur Dinc,Jibak Sarkar,Philipp Schubert,Sabine Semrau,Thomas Weissmann,Andre Karius,Johann Brand,Bernd-Niklas Axer,Ahmed Gomaa,Pluvio Stephan,Ishita Sheth,Sogand Beirami,Annette Schwarz,Udo Gaipl,Benjamin Frey,Christoph Bert,Stefanie Corradini,Rainer Fietkau,Florian Putz*

Main category: cs.CV

TL;DR: GPT-5在放疗考试与真实病例方案生成上显著优于GPT-4/3.5，准确性与完整性高、幻觉少，但在复杂情境下仍有关键性错误，需专家把关。


<details>
  <summary>Details</summary>
Motivation: 评估专为肿瘤场景宣传的GPT-5在放射肿瘤学中的临床决策支持能力与可靠性，比较其相对前代模型的进步与局限。

Method: 采用两类基准：(1) 2021年ACR放疗住培考试（TXIT）300题多选题测准确率；(2) 60例多病种真实放疗病例小结，要求模型给出简要治疗方案，由4名主治评估正确性、全面性与幻觉；用Fleiss’ kappa评估评审一致性，并分析错误集中点。

Result: TXIT准确率92.8%，显著高于GPT-4（78.8%）与GPT-3.5（62.1%），在剂量与诊断子域提升最大。病例评估中：正确性均分3.24/4（95%CI 3.11–3.38）、全面性3.59/4（95%CI 3.49–3.69）；幻觉罕见且无多数共识个案。评审一致性低（kappa=0.083），错误多见于需精确试验证据或复杂个体化调整的场景。

Conclusion: GPT-5在客观题与方案生成方面均较前代显著提升，但仍存在在复杂病例中的实质性错误。临床应用前需严格专家审核与持续改进，不能替代临床判断。

Abstract: Introduction: Large language models (LLM) have shown great potential in
clinical decision support. GPT-5 is a novel LLM system that has been
specifically marketed towards oncology use.
  Methods: Performance was assessed using two complementary benchmarks: (i) the
ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300
multiple-choice items, and (ii) a curated set of 60 authentic radiation
oncologic vignettes representing diverse disease sites and treatment
indications. For the vignette evaluation, GPT-5 was instructed to generate
concise therapeutic plans. Four board-certified radiation oncologists rated
correctness, comprehensiveness, and hallucinations. Inter-rater reliability was
quantified using Fleiss' \k{appa}.
  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,
outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were
most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's
treatment recommendations were rated highly for correctness (mean 3.24/4, 95%
CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).
Hallucinations were rare with no case reaching majority consensus for their
presence. Inter-rater agreement was low (Fleiss' \k{appa} 0.083 for
correctness), reflecting inherent variability in clinical judgment. Errors
clustered in complex scenarios requiring precise trial knowledge or detailed
clinical adaptation.
  Discussion: GPT-5 clearly outperformed prior model variants on the radiation
oncology multiple-choice benchmark. Although GPT-5 exhibited favorable
performance in generating real-world radiation oncology treatment
recommendations, correctness ratings indicate room for further improvement.
While hallucinations were infrequent, the presence of substantive errors
underscores that GPT-5-generated recommendations require rigorous expert
oversight before clinical implementation.

</details>


### [56] [TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank](https://arxiv.org/abs/2508.21795)
*Jiawei Liu,Jiahe Hou,Wei Wang,Jinsong Du,Yang Cong,Huijie Fan*

Main category: cs.CV

TL;DR: 提出TMUAD：通过三类记忆库（文本/对象级图像/patch级图像）统一结构与逻辑异常检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有统一异常检测多依赖图像特征与图像记忆库，难以捕捉跨对象的语义逻辑关系，且正常数据有限导致泛化不足。作者引入文本层面的知识以增强对逻辑异常（例如关系不一致、语义违背）的检测能力。

Method: 构建“三记忆”框架：1) 逻辑感知文本提取器从输入图像提炼对象及其关系的文字描述，形成类级文本记忆库，对逻辑异常进行匹配与评分；2) 基于分割得到的对象级特征，建立对象级图像记忆库，保留完整轮廓应对对象形态异常；3) 使用视觉编码器提取patch级特征建立patch级记忆库，进行结构级异常检测。三库分别检索与查询图像最相似的正常样本，计算多层次异常分数并融合为最终分数。

Result: 在工业与医疗七个公开数据集上达到SOTA性能（统一结构与逻辑场景），优于依赖单一图像记忆或特征抽取的现有方法。

Conclusion: 协同的文本与多粒度视觉记忆库可有效统一结构与逻辑异常检测，提高检出率；提供可复现代码与模型，具备跨域实用价值。

Abstract: Anomaly detection, which aims to identify anomalies deviating from normal
patterns, is challenging due to the limited amount of normal data available.
Unlike most existing unified methods that rely on carefully designed image
feature extractors and memory banks to capture logical relationships between
objects, we introduce a text memory bank to enhance the detection of logical
anomalies. Specifically, we propose a Three-Memory framework for Unified
structural and logical Anomaly Detection (TMUAD). First, we build a class-level
text memory bank for logical anomaly detection by the proposed logic-aware text
extractor, which can capture rich logical descriptions of objects from input
images. Second, we construct an object-level image memory bank that preserves
complete object contours by extracting features from segmented objects. Third,
we employ visual encoders to extract patch-level image features for
constructing a patch-level memory bank for structural anomaly detection. These
three complementary memory banks are used to retrieve and compare normal images
that are most similar to the query image, compute anomaly scores at multiple
levels, and fuse them into a final anomaly score. By unifying structural and
logical anomaly detection through collaborative memory banks, TMUAD achieves
state-of-the-art performance across seven publicly available datasets involving
industrial and medical domains. The model and code are available at
https://github.com/SIA-IDE/TMUAD.

</details>


### [57] [VoCap: Video Object Captioning and Segmentation from Any Prompt](https://arxiv.org/abs/2508.21809)
*Jasper Uijlings,Xingyi Zhou,Xiuye Gu,Arsha Nagrani,Anurag Arnab,Alireza Fathi,David Ross,Cordelia Schmid*

Main category: cs.CV

TL;DR: VoCap提出一个统一模型：给定视频与多模态提示（文本/框/掩码），输出时空目标掩码与对象级字幕，同时覆盖可提示视频分割、指代表达分割和对象字幕生成，并通过为SAV数据集自动生成对象字幕（SAV-Caption）实现大规模训练，达到SOTA或具竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 视频理解需要同时获得精细的目标定位（掩码）与丰富的语义描述，但现有方法通常各自为政（分割或描述分离），且缺乏带对象级字幕的标注数据，限制联合学习与评测。

Method: 提出VoCap：输入视频+多模态提示（文本/框/掩码），输出时空masklet与对象级caption。为数据瓶颈，基于SAV分割数据，用GT掩码高亮目标后输入大规模VLM自动生成对象字幕，构建SAV-Caption；对验证集做人审标以避免偏置。训练时在SAV-Caption并混合其他图像/视频数据上进行大规模训练。

Result: 在指代表达视频目标分割上达SOTA；半监督视频目标分割上具竞争力；首次为视频对象级字幕任务建立基线/基准。

Conclusion: VoCap实现统一的多模态提示-到-时空分割与对象字幕生成框架，并通过SAV-Caption数据有效支撑大规模训练，推进视频细粒度理解的分割与描述一体化研究。

Abstract: Understanding objects in videos in terms of fine-grained localization masks
and detailed semantic properties is a fundamental task in video understanding.
In this paper, we propose VoCap, a flexible video model that consumes a video
and a prompt of various modalities (text, box or mask), and produces a
spatio-temporal masklet with a corresponding object-centric caption. As such
our model addresses simultaneously the tasks of promptable video object
segmentation, referring expression segmentation, and object captioning. Since
obtaining data for this task is tedious and expensive, we propose to annotate
an existing large-scale segmentation dataset (SAV) with pseudo object captions.
We do so by preprocessing videos with their ground-truth masks to highlight the
object of interest and feed this to a large Vision Language Model (VLM). For an
unbiased evaluation, we collect manual annotations on the validation set. We
call the resulting dataset SAV-Caption. We train our VoCap model at scale on a
SAV-Caption together with a mix of other image and video datasets. Our model
yields state-of-the-art results on referring expression video object
segmentation, is competitive on semi-supervised video object segmentation, and
establishes a benchmark for video object captioning. Our dataset will be made
available at https://github.com/google-deepmind/vocap.

</details>


### [58] [The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning](https://arxiv.org/abs/2508.21816)
*Yiming Lin,Yuchen Niu,Shang Wang,Kaizhu Huang,Qiufeng Wang,Xiao-Bo Jin*

Main category: cs.CV

TL;DR: 将视觉情境识别中的动词分类从单标签改为多标签：把其建模为仅有正样本的多标签学习（SPMLL），并提出图增强的Verb-MLP与多标签评测基准，显著提升MAP（>3%）且保持top-1/5竞争力。


<details>
  <summary>Details</summary>
Motivation: 视觉情境识别需要识别主要事件（动词）及其角色与位置。现实中同一图像可被多个动词合理描述，动词类别间语义高度重叠，传统单标签假设不成立；但多标签全面标注成本高、难以获得。

Method: 1) 将动词分类重构为仅正样本多标签学习（SPMLL），在只有部分正标签、无明确负标签的设置下学习；2) 设计多标签评测基准，公平评估SR模型在多标签场景下的表现；3) 提出GE-VerbMLP：用图神经网络建模动词间相关性，用对抗式训练优化判别边界，并与多层感知机结合进行预测。

Result: 在真实数据集上，相比现有方法，MAP提升超过3%，同时在传统top-1/top-5准确率上保持竞争力。

Conclusion: 动词分类本质上是多标签问题。将其表述为SPMLL并结合图增强与对抗训练，可在不需全面多标签标注的前提下带来显著收益；新评测基准为多标签SR提供了更公平的评价方式。

Abstract: Context recognition (SR) is a fundamental task in computer vision that aims
to extract structured semantic summaries from images by identifying key events
and their associated entities. Specifically, given an input image, the model
must first classify the main visual events (verb classification), then identify
the participating entities and their semantic roles (semantic role labeling),
and finally localize these entities in the image (semantic role localization).
Existing methods treat verb classification as a single-label problem, but we
show through a comprehensive analysis that this formulation fails to address
the inherent ambiguity in visual event recognition, as multiple verb categories
may reasonably describe the same image. This paper makes three key
contributions: First, we reveal through empirical analysis that verb
classification is inherently a multi-label problem due to the ubiquitous
semantic overlap between verb categories. Second, given the impracticality of
fully annotating large-scale datasets with multiple labels, we propose to
reformulate verb classification as a single positive multi-label learning
(SPMLL) problem - a novel perspective in SR research. Third, we design a
comprehensive multi-label evaluation benchmark for SR that is carefully
designed to fairly evaluate model performance in a multi-label setting. To
address the challenges of SPMLL, we futher develop the Graph Enhanced Verb
Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to
capture label correlations and adversarial training to optimize decision
boundaries. Extensive experiments on real-world datasets show that our approach
achieves more than 3\% MAP improvement while remaining competitive on
traditional top-1 and top-5 accuracy metrics.

</details>


### [59] [DriveQA: Passing the Driving Knowledge Test](https://arxiv.org/abs/2508.21824)
*Maolin Wei,Wanzhou Liu,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: 提出DriveQA，一个覆盖交通规则与场景的文本与多模态基准，系统评测LLM/MLLM在驾驶知识上的能力，发现其在基础规则上可行但在数值推理、复杂路权、标志变体与空间布局上薄弱；微调与预训练在DriveQA上可显著提升模型在多类任务与真实数据集上的表现，并揭示对环境因素的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶与VQA基准更多关注视觉与空间问答，缺乏对交通法规、路权、罕见边缘案例与标志变体的系统性覆盖；需要一个能够像“驾驶笔试”一样全面检验模型驾驶知识与推理能力的基准。

Method: 构建DriveQA基准：包含纯文本与多模态（DriveQA-V）问题，覆盖交通法规、路权、标志识别、交叉口决策、数值推理等；设计受控的视觉变换（光照、视角、距离、天气）以分析模型敏感性；在多个主流LLM/MLLM上评测，并进行在DriveQA上的微调与预训练；再在下游真实数据集（nuScenes、BDD）上验证迁移效果。

Result: 1) 现有LLM/MLLM能处理基础规则，但在数值推理、复杂路权、标志变体与空间布局理解上表现显著欠佳；2) 在DriveQA上微调提升多类别准确率，尤其是法规标志识别与路口决策；3) DriveQA-V受控变换揭示模型对光照、视角、距离、天气等环境因素敏感；4) 在DriveQA上预训练可提升下游驾驶任务与真实数据集（nuScenes、BDD）的表现，并能将文本与合成交通知识迁移至其他QA任务。

Conclusion: DriveQA是一个覆盖全面的驾驶知识评测与训练基准，能够揭示并缓解LLM/MLLM在驾驶知识、数值与路权推理、以及环境鲁棒性方面的缺陷；在其上微调/预训练可提升模型对真实驾驶场景与下游任务的泛化能力。

Abstract: If a Large Language Model (LLM) were to take a driving knowledge test today,
would it pass? Beyond standard spatial and visual question-answering (QA) tasks
on current autonomous driving benchmarks, driving knowledge tests require a
complete understanding of all traffic rules, signage, and right-of-way
principles. To pass this test, human drivers must discern various edge cases
that rarely appear in real-world datasets. In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios. Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.

</details>
