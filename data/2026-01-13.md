<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 134]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders](https://arxiv.org/abs/2601.06067)
*Chimdi Walter Ndubuisi,Toni Kazic*

Main category: cs.CV

TL;DR: 提出HyperTopo-Adapters：在冻结视觉编码器上接入轻量头，将特征嵌入超曲率+欧式+球面乘积流形，并引入可微拓扑先验，显著改善病斑分割的边界与拓扑一致性（如β1孔洞误差↓9%），Dice/IoU保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 叶片病斑分割对拓扑极为敏感；微小的合并、断裂或假孔洞具有生物学意义，但传统像素级损失在欧式潜空间中对这些错误惩罚不足。因此需要在表示与训练目标中显式编码层级结构、全局闭合与局部细节的拓扑/几何先验。

Method: 在冻结视觉编码器上添加参数高效的适配头，将特征嵌入H+E+S乘积流形：超曲空间用于层级分离，欧式用于局部线性细节，球面用于全局闭合。训练时在Dice/BCE之上加入拓扑先验：用于评估/选择的PH距离，以及用于反向传播的可微代理（软Euler特征匹配+总变分正则）。设计超曲对比项与拓扑先验的warm-up；引入按样本的结构感知指标评估（Boundary-F1、Betti误差、持久同调距离），并采用Top-K Dice中最小PD的checkpoint选择策略。进行消融（曲率学习、潜维度、对比温度、代理设置）。

Result: 在Kaggle叶片病斑数据集（N=2,940）上，早期结果显示边界与拓扑指标持续改进，Δβ1孔洞误差降低约9%，同时Dice/IoU与基线相当。

Conclusion: 组合乘积流形嵌入与可微拓扑先验能在不牺牲常规重叠指标的情况下提升拓扑一致性。开放的可复现实验套件有助于隔离几何/拓扑因素、暴露失败模式，并为更强的拓扑保真的分割架构提供指导。

Abstract: Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.

</details>


### [2] [OptFormer: Optical Flow-Guided Attention and Phase Space Reconstruction for SST Forecasting](https://arxiv.org/abs/2601.06078)
*Yin Wang,Chunlin Gong,Zhuozhen Xu,Lehan Zhang,Xiang Wu*

Main category: cs.CV

TL;DR: 提出OptFormer：结合相空间重构与基于光流引导的运动感知注意力的编码器-解码器，用于SST长期预测；在NOAA多尺度数据上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: SST对气候建模与灾害预警关键，但其非线性时空动力学与长预测期使准确预测困难；现有方法在捕获长程时序依赖和关注动态区域方面不足。

Method: 构建OptFormer：在输入序列上进行相空间重构以丰富时序嵌入；在Transformer式注意力中引入光流作为引导，构造运动感知注意力，利用帧间运动线索突出空间场的相对变化，聚焦动态区域，增强长程依赖建模；采用编码器-解码器架构，1:1训练-预测设置。

Result: 在NOAA SST多空间尺度数据集上，OptFormer在准确性与鲁棒性上显著优于现有基线，长时段预测表现更佳。

Conclusion: 将光流引导的运动注意力与相空间重构相结合，有效提升SST长期预测的时空建模能力；方法稳健、可扩展，适用于多尺度海温预测场景。

Abstract: Sea Surface Temperature (SST) prediction plays a vital role in climate modeling and disaster forecasting. However, it remains challenging due to its nonlinear spatiotemporal dynamics and extended prediction horizons. To address this, we propose OptFormer, a novel encoder-decoder model that integrates phase-space reconstruction with a motion-aware attention mechanism guided by optical flow. Unlike conventional attention, our approach leverages inter-frame motion cues to highlight relative changes in the spatial field, allowing the model to focus on dynamic regions and capture long-range temporal dependencies more effectively. Experiments on NOAA SST datasets across multiple spatial scales demonstrate that OptFormer achieves superior performance under a 1:1 training-to-prediction setting, significantly outperforming existing baselines in accuracy and robustness.

</details>


### [3] [Semantic Event Graphs for Long-Form Video Question Answering](https://arxiv.org/abs/2601.06097)
*Aradhya Dixit,Tianxi Liang*

Main category: cs.CV

TL;DR: 提出Semantic Event Graphs（SEG），以符号化的时间交互日志替代原始视频帧，通过查询感知的子图裁剪，将长视频问答的token与成本显著降低，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 长视频问答需要跨小时级时域推理，直接喂入稠密帧或视觉特征会超出上下文与计算预算；现有取样会丢失关键长期依赖，短上下文更几乎失效，亟需一种既保留长期记忆又高效的表示。

Method: 1) 以YOLOv11进行目标检测与追踪；2) 依据人-物体空间近邻将交互转为带时间戳的START/END事件；3) 构建Temporal Scene Graph (TSG) 表示对象与事件序列；4) 推理时进行查询感知的剪枝：定位锚点实体与词汇相关事件，抽取小型子图；5) 将子图文本化输入Gemini 2.5 Flash生成答案。

Result: 在5个YouTube视频（每个300-500交互）与120个自动生成的长程问题上，SEG以每问3.47k tokens达成65.0%准确率，接近使用完整日志的62.5%但仅用40.39k的8.6% token（节省91.4%）；仅看最后30秒的短上下文基线仅2.5%准确率。

Conclusion: 符号化时间图可作为即插即用的长程记忆层，为现成视觉-语言模型提供高效的长视频推理能力，大幅降低token与成本且保持精度；工具与代码将开放以复现。

Abstract: Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START/END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.

</details>


### [4] [COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control](https://arxiv.org/abs/2601.06122)
*Canming Xia,Peixi Peng,Guang Tan,Zhan Su,Haoran Xu,Zhenxian Liu,Luntong Li*

Main category: cs.CV

TL;DR: 提出COVR：让VLM与视觉RL互相促进。用RL采集的交互数据微调VLM增强语义推理，再用增强后的VLM通过动作先验指导策略学习，并配合探索驱动动态筛选与回报感知自适应损失加权，以及渐进式微调策略，显著提升视觉控制任务表现与样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉RL因高维视觉观测导致样本效率差；虽有工作用VLM辅助RL，但多为单向从VLM向RL蒸馏，忽视RL在环境中产生的大量交互数据可反哺VLM以提升其与任务一致的语义与决策能力。需要一个能让二者互利共生、提高训练稳定性与资源效率的框架。

Method: 提出协同优化框架COVR：1) 用RL生成的数据对VLM进行任务一致的微调，增强语义推理；2) 用增强后的VLM为RL提供动作先验指导策略学习。为高效微调，引入两模块：a) 探索驱动动态过滤（EDD-Filter），依据探索程度设自适应阈值保留高价值探索样本；b) 回报感知自适应损失加权（RA-ALW），利用回报信号量化采样动作的不一致性并调节损失权重，提升训练稳定性。另设计渐进式微调策略以降低资源消耗。

Result: 在多种具有挑战性的视觉控制任务上取得强性能，相较基线在样本效率与最终回报上均有显著提升（摘要未给出具体数值）。

Conclusion: 通过让VLM与RL策略互为增益，并配合动态样本筛选、回报加权与渐进微调，COVR能稳定且高效地提升视觉RL表现，表明利用RL交互数据反哺VLM是有效途径。

Abstract: Visual reinforcement learning (RL) suffers from poor sample efficiency due to high-dimensional observations in complex tasks. While existing works have shown that vision-language models (VLMs) can assist RL, they often focus on knowledge distillation from the VLM to RL, overlooking the potential of RL-generated interaction data to enhance the VLM. To address this, we propose COVR, a collaborative optimization framework that enables the mutual enhancement of the VLM and RL policies. Specifically, COVR fine-tunes the VLM with RL-generated data to enhance the semantic reasoning ability consistent with the target task, and uses the enhanced VLM to further guide policy learning via action priors. To improve fine-tuning efficiency, we introduce two key modules: (1) an Exploration-Driven Dynamic Filter module that preserves valuable exploration samples using adaptive thresholds based on the degree of exploration, and (2) a Return-Aware Adaptive Loss Weight module that improves the stability of training by quantifying the inconsistency of sampling actions via return signals of RL. We further design a progressive fine-tuning strategy to reduce resource consumption. Extensive experiments show that COVR achieves strong performance across various challenging visual control tasks.

</details>


### [5] [Low-Back Pain Physical Rehabilitation by Movement Analysis in Clinical Trial](https://arxiv.org/abs/2601.06138)
*Sao Mai Nguyen*

Main category: cs.CV

TL;DR: 提出Keraal临床康复数据集，用于智能辅导系统评估腰痛康复动作，并对SOTA人体动作分析进行基准；聚焦动作评估、错误识别、空间与时间定位四大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为健康受试者或受控环境，难以反映真实临床康复场景；为开发和评估面向康复的智能辅导系统，需要包含真实患者、真实训练过程及多维度标注的数据。

Method: 在临床环境采集低背痛患者执行康复动作的数据，构建Keraal数据集；并对主流人体运动分析算法进行基准测试，以验证数据集对四类任务（动作质量评估、错误识别、空间定位、时间定位）的价值。

Result: 形成覆盖临床康复场景的多模态/多任务标注数据集，并给出在该数据集上的SOTA算法基线表现，用于量化上述四项任务的难度与可行性。

Conclusion: Keraal数据集为构建和评估康复领域的智能辅导系统提供了关键资源，能够推动动作评估与错误检测等任务在真实临床环境中的研究与应用。

Abstract: To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we propose a medical dataset of clinical patients carrying out low back-pain rehabilitation exercises and benchmark on state of the art human movement analysis algorithms. This dataset is valuable because it includes rehabilitation motions in a clinical setting with patients in their rehabilitation program. This paper introduces the Keraal dataset, a clinically collected dataset to enable intelligent tutoring systems (ITS) for rehabilitation. It addresses four challenges in exercise monitoring: motion assessment, error recognition, spatial localization, temporal localization

</details>


### [6] [Forget-It-All: Multi-Concept Machine Unlearning via Concept-Aware Neuron Masking](https://arxiv.org/abs/2601.06163)
*Kaiyuan Deng,Bo Hui,Gen Li,Jie Ji,Minghai Qin,Geng Yuan,Xiaolong Ma*

Main category: cs.CV

TL;DR: 提出FIA框架，通过稀疏化与掩码融合实现多概念机器遗忘：用对比概念显著性度量权重贡献，选出概念敏感神经元，构建并融合多概念掩码，训练免调、稳健忘却且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型可能生成版权/不当/敏感内容；现实应用需要一次性移除多个概念。现有方法多针对单概念，扩展到多概念会带来遗忘不彻底、质量下降、以及对超参和数据集敏感的问题。

Method: 1) 对比概念显著性：评估每个连接对目标概念的贡献；2) 时空一致性筛选概念敏感神经元，确保稳定响应目标概念；3) 基于所选神经元构建掩码，并跨概念融合成统一掩码，保留概念无关（通用）神经元、剪除概念特异神经元；整体为训练免调、少超参的即插即用流程。

Result: 在三类多概念遗忘任务上，FIA比现有方法更可靠：更强的遗忘效果，同时更好地保持语义保真与图像质量，且对超参数与数据敏感性更低。

Conclusion: 利用模型稀疏性与概念敏感神经元掩码融合，可实现稳健的多概念遗忘；FIA无需再训练、易于迁移，适合作为实用的多概念机器遗忘方案。

Abstract: The widespread adoption of text-to-image (T2I) diffusion models has raised concerns about their potential to generate copyrighted, inappropriate, or sensitive imagery learned from massive training corpora. As a practical solution, machine unlearning aims to selectively erase unwanted concepts from a pre-trained model without retraining from scratch. While most existing methods are effective for single-concept unlearning, they often struggle in real-world scenarios that require removing multiple concepts, since extending them to this setting is both non-trivial and problematic, causing significant challenges in unlearning effectiveness, generation quality, and sensitivity to hyperparameters and datasets. In this paper, we take a unique perspective on multi-concept unlearning by leveraging model sparsity and propose the Forget It All (FIA) framework. FIA first introduces Contrastive Concept Saliency to quantify each weight connection's contribution to a target concept. It then identifies Concept-Sensitive Neurons by combining temporal and spatial information, ensuring that only neurons consistently responsive to the target concept are selected. Finally, FIA constructs masks from the identified neurons and fuses them into a unified multi-concept mask, where Concept-Agnostic Neurons that broadly support general content generation are preserved while concept-specific neurons are pruned to remove the targets. FIA is training-free and requires only minimal hyperparameter tuning for new tasks, thereby promoting a plug-and-play paradigm. Extensive experiments across three distinct unlearning tasks demonstrate that FIA achieves more reliable multi-concept unlearning, improving forgetting effectiveness while maintaining semantic fidelity and image quality.

</details>


### [7] [What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models](https://arxiv.org/abs/2601.06165)
*Dasol Choi,Guijin Son,Hanwool Lee,Minhyuk Kim,Hyunwoo Ko,Teabin Lim,Ahn Eungyeol,Jungwhan Kim,Seunghyeok Hong,Youngsook Song*

Main category: cs.CV

TL;DR: HAERAE-Vision提出由真实韩语社区提问构成的图文问答基准，强调自然、含糊的用户查询给VLM带来显著困难；把隐含信息显式化可显著提升性能，且小模型收益更大，检索也难弥补用户未说明的信息。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言基准多为结构化、明确提示的提问，无法反映真实用户非正式、欠规范、依赖图像语境的查询，导致评测与实际部署存在偏差。

Method: 收集86K真实韩语视觉问题，严格筛选得到653条原始欠说明查询，并为每条提供一条显式改写，形成1,306个查询变体；评测39个VLM（含SOTA如GPT-5、Gemini 2.5 Pro），比较原始 vs 显式化查询的表现，并测试结合网页检索的情形。

Result: SOTA在原始查询上正确率不足50%；单纯做查询显式化即可带来约8–22个百分点提升，小模型受益更明显；即便引入网页检索，欠说明查询仍不如无需检索的显式查询，说明检索难以补足用户未陈述信息。

Conclusion: VLM在真实场景的困难很大程度源于自然查询的欠说明，而非模型固有能力极限；当前基准与真实使用之间存在关键鸿沟，应在评测与系统设计中重视查询显式化与对欠说明的鲁棒性。

Abstract: Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.

</details>


### [8] [B-FIRE: Binning-Free Diffusion Implicit Neural Representation for Hyper-Accelerated Motion-Resolved MRI](https://arxiv.org/abs/2601.06166)
*Di Xu,Hengjie Liu,Yang Yang,Mary Feng,Jin Ning,Xin Miao,Jessica E. Scholey,Alexandra E. Hotca-cho,William C. Chen,Michael Ohliger,Martina Descovich,Huiming Dong,Wensha Yang,Ke Sheng*

Main category: cs.CV

TL;DR: 提出B-FIRE，一种无需分箱的扩散式隐式神经表示框架，用于在极端欠采样非笛卡尔k空间条件下重建4DMRI，捕捉瞬时三维腹部解剖；在StarVIBE肝MRI上对比NuFFT、GRASP-CS与反卷积CNN，评估重建保真度、运动一致性与推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统4DMRI常依赖呼吸相位分箱与时间平均，导致瞬时动态被模糊化、运动轨迹被误表征；在超高加速（极端欠采样）非笛卡尔采样条件下，现有重建方法难以恢复瞬时三维信息，需要新的重建范式。

Method: 构建B-FIRE：以CNN-INR编码器-解码器为骨干，引入扩散式优化策略与联合损失，兼顾图像域保真与频域敏感约束；训练阶段使用分箱得到的成对图像作为参考，推理阶段对无需分箱的欠采样数据直接重建；在T1加权StarVIBE肝MRI上，从每帧8条射线至1条射线（RV8→RV1）进行实验；与NuFFT、GRASP-CS和一种unrolled CNN对比。

Result: B-FIRE在超高加速条件下实现更高的重建保真度与更一致的运动轨迹，同时具备较低推理延迟；能够从极端欠采样非笛卡尔数据中恢复瞬时三维腹部解剖。

Conclusion: 无需分箱的扩散-INR框架可在极端欠采样下准确重建瞬时4DMRI，优于传统稀疏重建与卷积展开方法，并为动态腹部MRI提供更可靠的运动解析与高效推理。

Abstract: Accelerated dynamic volumetric magnetic resonance imaging (4DMRI) is essential for applications relying on motion resolution. Existing 4DMRI produces acceptable artifacts of averaged breathing phases, which can blur and misrepresent instantaneous dynamic information. Recovery of such information requires a new paradigm to reconstruct extremely undersampled non-Cartesian k-space data. We propose B-FIRE, a binning-free diffusion implicit neural representation framework for hyper-accelerated MR reconstruction capable of reflecting instantaneous 3D abdominal anatomy. B-FIRE employs a CNN-INR encoder-decoder backbone optimized using diffusion with a comprehensive loss that enforces image-domain fidelity and frequency-aware constraints. Motion binned image pairs were used as training references, while inference was performed on binning-free undersampled data. Experiments were conducted on a T1-weighted StarVIBE liver MRI cohort, with accelerations ranging from 8 spokes per frame (RV8) to RV1. B-FIRE was compared against direct NuFFT, GRASP-CS, and an unrolled CNN method. Reconstruction fidelity, motion trajectory consistency, and inference latency were evaluated.

</details>


### [9] [Analyzing the Structure of Handwritten Digits: A Comparative Study of PCA, Factor Analysis, and UMAP](https://arxiv.org/abs/2601.06168)
*Jyotiraditya Gupta*

Main category: cs.CV

TL;DR: 研究用PCA、FA、UMAP三种降维方法分析MNIST手写数字的潜在结构，不追求分类，而是揭示内在维度、共享变异与非线性几何。结果显示数字分布在结构化的低维流形上，各方法互补呈现其结构特征。


<details>
  <summary>Details</summary>
Motivation: 手写数字虽处于高维像素空间，但具有强几何与统计结构；希望理解其潜在组织形式，而非仅提升分类精度，评估不同统计框架如何揭示内在维度与结构。

Method: 对MNIST应用三类降维：PCA用于捕捉全局主方差方向与重构；FA用于将数据分解为可解释的潜在因子（笔画、环、对称等“书写基元”）；UMAP用于学习非线性流形，刻画类别间的连续风格过渡。比较三者在内在维度、共享变异与非线性几何上的表征。

Result: PCA以少量主成分实现高保真重构并揭示主方差方向；FA得到与笔画结构对应的可解释因子；UMAP展现平滑的跨数字类别风格轨迹与簇状非线性流形。三者从不同角度揭示数字的低维结构。

Conclusion: MNIST手写数字位于结构化的低维流形上。PCA刻画全局线性方差与重构，FA提供语义化的书写基元分解，UMAP揭示非线性几何与风格连续性；这些方法相互补充，共同阐明数据的潜在组织。

Abstract: Handwritten digit images lie in a high-dimensional pixel space but exhibit strong geometric and statistical structure. This paper investigates the latent organization of handwritten digits in the MNIST dataset using three complementary dimensionality reduction techniques: Principal Component Analysis (PCA), Factor Analysis (FA), and Uniform Manifold Approximation and Projection (UMAP). Rather than focusing on classification accuracy, we study how each method characterizes intrinsic dimensionality, shared variation, and nonlinear geometry. PCA reveals dominant global variance directions and enables high-fidelity reconstructions using a small number of components. FA decomposes digits into interpretable latent handwriting primitives corresponding to strokes, loops, and symmetry. UMAP uncovers nonlinear manifolds that reflect smooth stylistic transitions between digit classes. Together, these results demonstrate that handwritten digits occupy a structured low-dimensional manifold and that different statistical frameworks expose complementary aspects of this structure.

</details>


### [10] [Think Bright, Diffuse Nice: Enhancing T2I-ICL via Inductive-Bias Hint Instruction and Query Contrastive Decoding](https://arxiv.org/abs/2601.06169)
*Zhiyong Ma,Zhenpeng Li,Yuanjie Shi,Zhengping Li,Jiahao Chen,Qingyuan Chuai*

Main category: cs.CV

TL;DR: 提出TBDN，一个无需再训练的T2I-ICL推理框架，通过提示注入与对比解码同时缓解“合规性失败”和“先验主导的幻觉”，在多基准上达SOTA，并具备良好泛化与概念保持能力。


<details>
  <summary>Details</summary>
Motivation: T2I-ICL依赖少量示例进行定制化合成，但容易违反上下文映射规则（不按示例/提示生成）与被模型先验牵引产生幻觉，两者相互强化，显著降低生成质量。现有方法多需专门训练，灵活性差、部署成本高，亟需一种训练无关、可泛化的推理方案。

Method: 提出TBDN，包含两个闭环机制：1) Hint Instruction(HI)：通过轻量提示工程注入任务感知归纳偏置，显式强调从示例到目标的映射规则，稳定模型对上下文的对齐与遵循；2) Query Contrastive Decoding(QCD)：对比“完整输入”与“去除查询关键信息”的解码分布，抑制由模型先验引起的概率峰值，重加权生成分布以降低幻觉。两者互补，分别缓解合规性失败与先验幻觉。

Result: 在CoBSAT与Text-to-Image Fast Mini-ImageNet上取得SOTA；对不同模型骨干、提示设计与超参数具有稳健泛化；在Dreambench++的概念保真与提示遵循上也保持竞争力。

Conclusion: 无需额外训练的TBDN通过HI与QCD打破T2I-ICL的两大瓶颈，提供简单高效、可靠的推理框架，显著提升定制生成的合规性与抗幻觉能力，并具备跨设置的鲁棒泛化。

Abstract: Text-to-Image In-Context Learning (T2I-ICL) enables customized image synthesis via interleaved text-image examples but faces two mutually reinforcing bottlenecks, compliance failure and prior-dominated hallucination, that form a vicious cycle degrading generation quality. Existing methods rely on tailored training, which limits flexibility and raises deployment costs. To address these challenges effectively, we propose TBDN, a training-free framework integrating two complementary closed-loop mechanisms: Hint Instruction (HI) and Query Contrastive Decoding (QCD). HI injects task-aware inductive bias via lightweight prompt engineering to anchor models on contextual mapping rules, thereby mitigating compliance failure. QCD adjusts the decoding distributions of language models by contrasting full-input and query-omitted distributions, suppressing prior-dominated hallucination. TBDN achieves State-of-the-Art performance on CoBSAT and Text-to-Image Fast Mini-ImageNet, with robust generalization across model backbones, prompt designs, and hyperparameters. It also maintains promising performance in concept preservation and prompt following on Dreambench++. By breaking the two bottlenecks, TBDN establishes a simple yet effective framework for efficient and reliable T2I-ICL.

</details>


### [11] [TIR-Flow: Active Video Search and Reasoning with Frozen VLMs](https://arxiv.org/abs/2601.06176)
*Hongbo Jin,Siyi Xie,Jiayu Ding,Kuanwei Lin,Ge Li*

Main category: cs.CV

TL;DR: 提出TIR-Flow，使冻结的Video-LLMs无需额外数据或参数更新，通过主动搜索与推理显著提升长时视频推理，平均+5.9%，Egoschema上+10.5%。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在推理上薄弱，主流依赖构造大规模CoT数据并进行SFT/RL，以提高采样与输出对齐，但缺乏激活“主动感知+系统2”式的内在推理能力，尤其在需要动态视觉探索的长时视频任务上受限。

Method: 提出无需微调的三模块框架TIR-Flow：HDD（层次难度分解）将复杂查询拆为可验证子任务；HAP（层次注意规划）主动在视频中定位并收集高分辨率证据以验证假设；EBA（证据记忆/结算）维护持久工作区，累积并更新线索以推进逻辑推理。整体实现“主动搜索—证据获取—证据整合—推理”闭环。

Result: 在7个基准上全面评估，相对近期强基线平均提升5.9%，在Egoschema上最高提升10.5%，显示出在长时视频推理与主动感知方面的优势。

Conclusion: 将“系统2式”主动感知能力注入冻结的VLM是一条可扩展路径；TIR-Flow通过任务分解、注意力引导与证据累积，实现无需额外数据/训练的显著推理增益。

Abstract: While Large Video-Language Models (Video-LLMs) have achieved remarkable progress in perception, their reasoning capabilities remain a bottleneck. Existing solutions typically resort to a heavy "data engineering" paradigm-synthesizing large-scale Chain-of-Thought (CoT) datasets followed by Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). This pipeline primarily optimizes probability sampling efficiency and aligns output distributions, but fails to activate the intrinsic intelligence required for dynamic visual exploration. In this work, we propose TIR-Flow, a novel framework that shifts the paradigm from passive processing to active video searching and reasoning without additional data or parameter updating. Concretely, our framework operates through three synergistic modules: HDD decomposes complex queries into a set of verifiable sub-tasks; HAP actively directs visual attention to gather high-resolution evidence for hypothesis validation; EBA maintains a persistent workspace to accumulate and update the discovered clues for logical reasoning. Extensive experiments on seven benchmarks demonstrate that TIR-Flow significantly outperforms recent strong baselines, delivering an average performance boost of 5.9%, with gains reaching 10.5% on Egoschema. Our analysis confirms that empowering frozen VLMs with System-2-like active perception is a scalable path toward solving long-horizon video reasoning.

</details>


### [12] [A Unified Attention U-Net Framework for Cross-Modality Tumor Segmentation in MRI and CT](https://arxiv.org/abs/2601.06187)
*Nishan Rai,Pushpa R. Dahal*

Main category: cs.CV

TL;DR: 提出一个统一的Attention U-Net，在MRI（BraTS2021）与CT（LIDC-IDRI）上联合训练，实现跨模态、跨解剖部位的肿瘤分割，取得与单模态方法相当的指标表现。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型多需为不同模态/解剖部位定制网络或做域自适应，难以复用与推广。作者希望验证：一个不依赖模态特定编码器或域自适应的单一模型，是否能在MRI与CT两种显著不同成像模态和任务上都保持有竞争力的分割性能，从而作为跨模态肿瘤分割的可复现实证基线。

Method: 基于Attention U-Net，采用：1）模态统一化预处理（harmonization）以减小MRI与CT间分布差异；2）注意力门控的跳跃连接以突出肿瘤相关特征并抑制噪声；3）模态感知的Focal Tversky损失以处理类别不平衡并适应不同模态特性；在BraTS2021（MRI）与LIDC-IDRI（CT）上进行联合训练，不引入模态专属编码器或显式域自适应。

Result: 统一模型在两数据域均取得有竞争力的Dice、IoU与AUC，表现接近或达到各自领域基线，显示跨模态泛化能力；给出了一个稳健、可复现的跨模态肿瘤分割基线。

Conclusion: 单一Attention U-Net可在无需模态专属结构或域自适应的前提下，同时处理MRI与CT肿瘤分割并获得良好性能，为跨模态肿瘤分割提供了有效基线与实践路径。

Abstract: This study presents a unified Attention U-Net architecture trained jointly on MRI (BraTS 2021) and CT (LIDC-IDRI) datasets to investigate the generalizability of a single model across diverse imaging modalities and anatomical sites. Our proposed pipeline incorporates modality-harmonized preprocessing, attention-gated skip connections, and a modality-aware Focal Tversky loss function. To the best of our knowledge, this study is among the first to evaluate a single Attention U-Net trained simultaneously on separate MRI (BraTS) and CT (LIDC-IDRI) tumor datasets, without relying on modality-specific encoders or domain adaptation. The unified model demonstrates competitive performance in terms of Dice coefficient, IoU, and AUC on both domains, thereby establishing a robust and reproducible baseline for future research in cross-modality tumor segmentation.

</details>


### [13] [How Does India Cook Biryani?](https://arxiv.org/abs/2601.06198)
*Shubham Goel,Farzana S,C V Rishi,Aditya Arun,C V Jawahar*

Main category: cs.CV

TL;DR: 提出首个大规模、精细标注的印度各地区烩饭（Biryani）烹饪视频数据集与评测基准，并构建利用多阶段VLM管线进行步骤分割、跨模态对齐与跨地区差异比较，以评估和推动VLM在结构化多模态推理上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法难以捕捉程序化烹饪视频中的细粒度、跨模态、具文化语境的差异；需要系统化、可复现的资源与工具来研究区域菜式的程序差异，并检验VLM在结构化推理中的能力。

Method: 收集并整理120个覆盖12种地区风格的高质量YouTube烩饭视频；提出多阶段框架：利用VLM进行细粒度步骤分割；将视觉片段与音频转录、标准食谱文本对齐；在对齐表征上构建自动视频比较管线识别并解释地区间程序差异；设计多层次推理的QA基准；多模型互补、引入人机协作校验；在零样本与微调设置下对多种SOTA模型进行评测。

Result: 得到首个针对烩饭的跨地区程序化视频数据集、自动差异比较方法与覆盖多层推理的QA基准；实证展示多VLM管线在高精任务上需人类校验、且在零样本与微调下性能差异明显，为后续研究提供可量化参考。

Conclusion: 该数据与方法为VLM的结构化、多模态、文化敏感推理提供新测试台，促进对烹饪视频与文化遗产的计算分析；资源与代码已开放，便于社区复现与扩展。

Abstract: Biryani, one of India's most celebrated dishes, exhibits remarkable regional diversity in its preparation, ingredients, and presentation. With the growing availability of online cooking videos, there is unprecedented potential to study such culinary variations using computational tools systematically. However, existing video understanding methods fail to capture the fine-grained, multimodal, and culturally grounded differences in procedural cooking videos. This work presents the first large-scale, curated dataset of biryani preparation videos, comprising 120 high-quality YouTube recordings across 12 distinct regional styles. We propose a multi-stage framework leveraging recent advances in vision-language models (VLMs) to segment videos into fine-grained procedural units and align them with audio transcripts and canonical recipe text. Building on these aligned representations, we introduce a video comparison pipeline that automatically identifies and explains procedural differences between regional variants. We construct a comprehensive question-answer (QA) benchmark spanning multiple reasoning levels to evaluate procedural understanding in VLMs. Our approach employs multiple VLMs in complementary roles, incorporates human-in-the-loop verification for high-precision tasks, and benchmarks several state-of-the-art models under zero-shot and fine-tuned settings. The resulting dataset, comparison methodology, and QA benchmark provide a new testbed for evaluating VLMs on structured, multimodal reasoning tasks and open new directions for computational analysis of cultural heritage through cooking videos. We release all data, code, and the project website at https://farzanashaju.github.io/how-does-india-cook-biryani/.

</details>


### [14] [QwenStyle: Content-Preserving Style Transfer with Qwen-Image-Edit](https://arxiv.org/abs/2601.06202)
*Shiwen Zhang,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出QwenStyle：在Qwen-Image-Edit基础上训练的内容保持型风格迁移DiT模型，通过课程式持续学习与合成三元组数据，实现对未见风格的泛化，同时保持内容一致性，风格相似度与美学质量达SOTA。


<details>
  <summary>Details</summary>
Motivation: DiT在风格迁移中内容与风格特征耦合，难以同时保持内容与实现精准风格定制；缺乏高质量、可扩展的数据与稳健训练机制以支持对开放域风格的泛化。

Method: 1) 构建并筛选少量高质量、特定风格数据；2) 从海量野外风格图像合成内容-风格-目标三元组；3) 采用课程式持续学习框架，混合“干净+噪声”三元组进行分阶段训练；4) 以Qwen-Image-Edit为底座，激活其强内容保真与风格定制能力，形成QwenStyle。

Result: QwenStyle V1在三项核心指标上达到SOTA：风格相似度、内容一致性与美学质量；并能对未见风格保持良好泛化且不牺牲内容保真。

Conclusion: 通过数据合成与课程式持续学习，解决了DiT中内容-风格耦合带来的权衡难题，QwenStyle在内容保持与风格可控性上实现兼顾并具备开放域泛化能力。

Abstract: Content-Preserving Style transfer, given content and style references, remains challenging for Diffusion Transformers (DiTs) due to its internal entangled content and style features. In this technical report, we propose the first content-preserving style transfer model trained on Qwen-Image-Edit, which activates Qwen-Image-Edit's strong content preservation and style customization capability. We collected and filtered high quality data of limited specific styles and synthesized triplets with thousands categories of style images in-the-wild. We introduce the Curriculum Continual Learning framework to train QwenStyle with such mixture of clean and noisy triplets, which enables QwenStyle to generalize to unseen styles without degradation of the precise content preservation capability. Our QwenStyle V1 achieves state-of-the-art performance in three core metrics: style similarity, content consistency, and aesthetic quality.

</details>


### [15] [Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification](https://arxiv.org/abs/2601.06204)
*Tayyab Rehman,Giovanni De Gasperis,Aly Shmahell*

Main category: cs.CV

TL;DR: 提出一种级联多智能体异常检测框架：前端重建与目标级评估做快速筛选，遇到语义不确定再按需调用高层推理；通过自适应升级阈值与发布-订阅总线实现异步协同与可扩展部署；在大规模监控上将时延降至1/3，同时保持高感知质量与一致语义标注。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态视觉环境中难以兼顾实时性与语义可解释性：重建法缺少上下文语义，检测器语义有限但快，视觉-语言系统可解释但代价高。需要一种能融合三者优势、在资源受限下仍具可解释性的方案。

Method: 构建级联多智能体架构：早期模块执行重建门控过滤与目标级评估，按自适应升级阈值仅在语义含糊时唤起高层推理智能体；采用发布-订阅通信骨干实现异步协调与跨异构硬件的可扩展部署；通过早退机制与选择性解释实现效率与可解释性的平衡。

Result: 在大规模监控数据上，相比直接用视觉-语言推理，级联方案将延迟降低至约三分之一，同时达到PSNR 38.3 dB、SSIM 0.965，并保持一致的语义标注。

Conclusion: 该框架通过早退效率、自适应多智能体推理与可解释异常归因，超越传统检测流水线，提供可复现实、节能且可扩展的智能视觉监控基础。

Abstract: Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.

</details>


### [16] [When Imbalance Comes Twice: Active Learning under Simulated Class Imbalance and Label Shift in Binary Semantic Segmentation](https://arxiv.org/abs/2601.06209)
*Julien Combes,Alexandre Derville,Jean-François Coeurjolly*

Main category: cs.CV

TL;DR: 研究在机器视觉高不良率稀少与存储受限导致的类别不平衡与标签漂移下，主动学习策略的效果。比较随机、熵采样与core-set，评估其鲁棒性与效率损失。


<details>
  <summary>Details</summary>
Motivation: 工业视觉/医疗影像中标注昂贵、数据量巨大且多数为无缺陷图像；同时无法长期存储全部影像，可能导致训练样本与真实分布不一致（标签漂移）。需弄清这些不平衡因素如何影响主动学习的选择策略与效益。

Method: 基于两个开源数据集构造可控的模拟环境：人为调节类别不平衡程度与标签漂移强度；对比三种常见主动学习策略——随机采样、基于熵的不确定性采样、core-set（代表性/覆盖性选择），评估其在不同不平衡与漂移情形下的表现。

Result: 在高度类别不平衡条件下，主动学习依然优于随机，尤其熵采样与core-set表现出较好的效率与效果；当存在强标签漂移（因存储限制造成训练与真实分布偏差）时，各策略效率下降，并可量化其性能损失。

Conclusion: 主动学习在不良样本稀少的视觉任务中仍具价值，熵采样与core-set对不平衡较为稳健；但当标签漂移显著时需预期效率下降，实际系统应关注数据保留与采样机制以缓解漂移影响。

Abstract: The aim of Active Learning is to select the most informative samples from an unlabelled set of data. This is useful in cases where the amount of data is large and labelling is expensive, such as in machine vision or medical imaging. Two particularities of machine vision are first, that most of the images produced are free of defects, and second, that the amount of images produced is so big that we cannot store all acquired images. This results, on the one hand, in a strong class imbalance in defect distribution and, on the other hand, in a potential label shift caused by limited storage. To understand how these two forms of imbalance affect active learning algorithms, we propose a simulation study based on two open-source datasets. We artificially create datasets for which we control the levels of class imbalance and label shift. Three standard active learning selection strategies are compared: random sampling, entropy-based selection, and core-set selection. We demonstrate that active learning strategies, and in particular the entropy-based and core-set selections, remain interesting and efficient even for highly imbalanced datasets. We also illustrate and measure the loss of efficiency that occurs in the situation a strong label shift.

</details>


### [17] [Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur](https://arxiv.org/abs/2601.06212)
*Yani Meziani*

Main category: cs.CV

TL;DR: Akasha 2 是一种将哈密顿状态空间对偶(H-SSD)与视觉-语言联合嵌入预测架构(VL-JEPA)融合的多模态模型，结合选择性状态空间模型Mamba-3与稀疏哈密顿专家混合(SMoE-HE)，并通过辛积分注入物理守恒偏置；在视觉合成上引入哈密顿流匹配(HFM)与持久化3D高斯点渲染(3DGS)，在移动端实现<50ms延迟。结果：SOTA视频预测(FVD=287)、比扩散快4倍、推理较Transformer快3-18倍，并在长时程保持能量守恒。


<details>
  <summary>Details</summary>
Motivation: 提升多模态世界模型的时空一致性与推理/合成效率，同时将物理启发的归纳偏置（如能量守恒、辛结构）显式注入深度架构，以突破扩散与Transformer在长时程一致性、效率与物理可解释性方面的瓶颈。

Method: 1) 架构：将H-SSD与VL-JEPA融合，形成同时处理视觉与语言的预测式潜在世界模型。2) 动力学建模：采用Mamba-3选择性SSM为主干，并引入稀疏哈密顿专家混合(SMoE-HE)，通过辛积分保证潜在动力学的物理守恒。3) 视觉合成：提出哈密顿流匹配(HFM)替代扩散，配合持久化3D高斯点(3DGS)以实现快速、连贯的渲染与视频生成。4) 记忆：使用“全息”记忆以维持跨时间的潜在状态一致性与检索。5) 训练/推理：在JEPA框架下进行预测式对比/一致性学习，侧重跨模态对齐与长时程稳定。

Result: 在基准上实现SOTA视频预测(FVD=287)；视觉合成速度较扩散提升4倍；相较Transformer基线推理加速3–18倍；在长时程预测中维持能量守恒与高时空一致性，并可在移动硬件上实现<50ms延迟。

Conclusion: 将物理启发的哈密顿结构与选择性状态空间模型、JEPA式对齐和3DGS渲染相结合，可在多模态世界建模中同时获得高效、稳定与物理一致的长时程生成/预测性能，提供一种面向移动端与实时应用的新范式。

Abstract: We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (<50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over transformer baselines while maintaining energy conservation over extended horizons.

</details>


### [18] [Two-step Authentication: Multi-biometric System Using Voice and Facial Recognition](https://arxiv.org/abs/2601.06218)
*Kuan Wei Chen,Ting Yi Lin,Wen Ren Yang,Aryan Kesarwani,Riya Singh*

Main category: cs.CV

TL;DR: 提出一种仅用常见设备的摄像头与麦克风即可运行的两步式多模态认证：先人脸识别锁定候选身份，再对该身份做声纹验证，以降算力与提鲁棒性；在人脸与声纹公开数据/小规模自建集上的准确率分别约95.1%与98.9%，声纹EER为3.456%。


<details>
  <summary>Details</summary>
Motivation: 单一生物特征在实际环境中易受噪声、遮挡与伪造影响且计算成本受限；需要一种在常见硬件上即可运行、兼顾准确度、鲁棒性与成本的多模态用户认证方案。

Method: 两步流水线：1) 人脸识别阶段使用MTCNN做人脸检测与对齐，并以剪枝后的VGG-16分类器在5名受试者、经数据增强的924张图像上训练；2) 声纹阶段仅对第一步匹配到的身份进行一对一说话人验证，模型为在LibriSpeech train-other-360上训练的CNN，在test-clean上评估。通过“先缩小搜索空间再验证”的策略减少计算和错配。

Result: 人脸识别在小规模自建数据上达到95.1%准确率；声纹验证在LibriSpeech test-clean上达到98.9%准确率，EER为3.456%。组合流程据称提升鲁棒性并降低计算量；代码与模型已开源。

Conclusion: 成本低、部署简便的两步式人脸+声纹认证在各自任务上取得较好指标，并通过级联减少计算与误拒/误受风险；但人脸部分受小样本与受试者数量限制，泛化与安全性（对抗攻击、欺骗防御）仍需在更大规模与实际场景中验证。

Abstract: We present a cost-effective two-step authentication system that integrates face identification and speaker verification using only a camera and microphone available on common devices. The pipeline first performs face recognition to identify a candidate user from a small enrolled group, then performs voice recognition only against the matched identity to reduce computation and improve robustness. For face recognition, a pruned VGG-16 based classifier is trained on an augmented dataset of 924 images from five subjects, with faces localized by MTCNN; it achieves 95.1% accuracy. For voice recognition, a CNN speaker-verification model trained on LibriSpeech (train-other-360) attains 98.9% accuracy and 3.456% EER on test-clean. Source code and trained models are available at https://github.com/NCUE-EE-AIAL/Two-step-Authentication-Multi-biometric-System.

</details>


### [19] [SAPL: Semantic-Agnostic Prompt Learning in CLIP for Weakly Supervised Image Manipulation Localization](https://arxiv.org/abs/2601.06222)
*Xinghao Wang,Changtao Miao,Dianmo Sheng,Tao Gong,Qi Chu,Nenghai Yu,Quanchen Zou,Deyue Zhang,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: 提出SAPL：在CLIP中通过边界敏感的文本提示和层级边缘对比学习，在仅有图像级标签下实现更精准的伪造局部化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 像素级标注代价高，弱监督方法只用二分类标签且侧重全局语义，忽略对伪造边界的关键局部线索；观察到伪造边界处特征变化更大，期望利用此性质提升弱监督定位。

Method: 在CLIP框架中引入语义无关的提示学习SAPL，由两模块组成：1) ECPL（边缘感知上下文提示学习）：用边缘增强的视觉特征通过注意力生成可学习文本提示，将与语义无关的边界信息嵌入文本特征，引导跨模态相似性聚焦操纵边缘；2) HECL（层级边缘对比学习）：提取真实与伪造的边缘patch，进行对比学习，提升两者区分度。最后以处理后的相似度图进行区域预测。

Result: 在多个公开基准上实现了显著优于现有方法的弱监督伪造定位性能，达到SOTA。

Conclusion: 利用CLIP的跨模态对齐能力并通过边界导向的提示与对比学习，使相似性关注操纵边界而非高层语义，在仅图像级标签下实现高精度伪造区域定位，成本更低、效果更佳。

Abstract: Malicious image manipulation threatens public safety and requires efficient localization methods. Existing approaches depend on costly pixel-level annotations which make training expensive. Existing weakly supervised methods rely only on image-level binary labels and focus on global classification, often overlooking local edge cues that are critical for precise localization. We observe that feature variations at manipulated boundaries are substantially larger than in interior regions. To address this gap, we propose Semantic-Agnostic Prompt Learning (SAPL) in CLIP, which learns text prompts that intentionally encode non-semantic, boundary-centric cues so that CLIPs multimodal similarity highlights manipulation edges rather than high-level object semantics. SAPL combines two complementary modules Edge-aware Contextual Prompt Learning (ECPL) and Hierarchical Edge Contrastive Learning (HECL) to exploit edge information in both textual and visual spaces. The proposed ECPL leverages edge-enhanced image features to generate learnable textual prompts via an attention mechanism, embedding semantic-irrelevant information into text features, to guide CLIP focusing on manipulation edges. The proposed HECL extract genuine and manipulated edge patches, and utilize contrastive learning to boost the discrimination between genuine edge patches and manipulated edge patches. Finally, we predict the manipulated regions from the similarity map after processing. Extensive experiments on multiple public benchmarks demonstrate that SAPL significantly outperforms existing approaches, achieving state-of-the-art localization performance.

</details>


### [20] [Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization](https://arxiv.org/abs/2601.06224)
*Miao Pan,Wangjie Gan,Jintao Chen,Wenqi Zhang,Bing Sun,Jianwei Yin,Xuhong Zhang*

Main category: cs.CV

TL;DR: 论文针对RL优化中的MLLM幻觉问题，提出三模块一体化框架：规划与字幕先锚定、基于奖励方差的探索优先、基于NTK相似度的样本去干扰（InfoNCE对比约束），显著降低幻觉并提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: MLLM在多任务上成功但在RL训练中更易产生幻觉，影响部署。需要系统定位成因并提出能直接在训练流程中缓解的机制。

Method: 三部分：1) 视觉定位增强：在推理前加入规划与图像字幕阶段，并以质量驱动的字幕奖励确保初始锚点准确；2) 探索增强：依据奖励分布的均值与方差对样本分桶，优先训练高方差样本以增加策略探索多样性；3) 样本干扰抑制：计算样本对的NTK相似度，使用InfoNCE损失拉远过相似、拉近过不相似的样本，引导梯度交互处于平衡区间。

Result: 实验显示该方法显著降低幻觉率，并提升MLLM推理准确率。

Conclusion: 通过在视觉锚定、探索策略与样本相互作用三个层面联合优化，可系统缓解RL阶段的MLLM幻觉，并提升整体推理表现。

Abstract: While Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse tasks, their practical deployment is severely hindered by hallucination issues, which become particularly acute during Reinforcement Learning (RL) optimization. This paper systematically analyzes the root causes of hallucinations in MLLMs under RL training, identifying three critical factors: (1) an over-reliance on chained visual reasoning, where inaccurate initial descriptions or redundant information anchor subsequent inferences to incorrect premises; (2) insufficient exploration diversity during policy optimization, leading the model to generate overly confident but erroneous outputs; and (3) destructive conflicts between training samples, where Neural Tangent Kernel (NTK) similarity causes false associations and unstable parameter updates. To address these challenges, we propose a comprehensive framework comprising three core modules. First, we enhance visual localization by introducing dedicated planning and captioning stages before the reasoning phase, employing a quality-based caption reward to ensure accurate initial anchoring. Second, to improve exploration, we categorize samples based on the mean and variance of their reward distributions, prioritizing samples with high variance to focus the model on diverse and informative data. Finally, to mitigate sample interference, we regulate NTK similarity by grouping sample pairs and applying an InfoNCE loss to push overly similar pairs apart and pull dissimilar ones closer, thereby guiding gradient interactions toward a balanced range. Experimental results demonstrate that our proposed method significantly reduces hallucination rates and effectively enhances the inference accuracy of MLLMs.

</details>


### [21] [Synthetic FMCW Radar Range Azimuth Maps Augmentation with Generative Diffusion Model](https://arxiv.org/abs/2601.06228)
*Zhaoze Wang,Changxu Zhang,Tai Fei,Christopher Grimm,Yi Jin,Claas Tebruegge,Ernst Warsitz,Markus Gardill*

Main category: cs.CV

TL;DR: 提出一种条件生成扩散模型，合成逼真的FMCW雷达距离-方位图，通过置信度图条件、几何感知与时序一致性正则，提升重建质量与下游检测性能。


<details>
  <summary>Details</summary>
Motivation: 真实、标注完善且多样的汽车雷达数据稀缺，限制了深度学习感知模型的性能与泛化；需要能生成物理合理、可控且多类别的雷达数据以缓解数据瓶颈。

Method: 采用条件扩散生成框架生成FMCW雷达Range-Azimuth Map。以多通道置信度图作为条件，每个通道对应语义类别（行人、汽车、骑行者），在目标位置以高斯分布编码。为适配雷达特性，引入几何感知条件（考虑方位/距离成像几何与物理先验）和时序一致性正则（跨帧约束，保持目标随时间的连续性），指导扩散过程。

Result: 在ROD2021数据集上，相比基线，信号重建PSNR提升3.6 dB；用真实+合成数据联合训练，下游检测mAP较传统图像处理式增强提升4.15%。生成结果物理合理且多样性良好。

Conclusion: 该条件扩散生成框架能合成高保真、可控的雷达谱图，并作为数据增广显著提升下游感知模型的泛化能力，缓解雷达数据稀缺与多样性不足的问题。

Abstract: The scarcity and low diversity of well-annotated automotive radar datasets often limit the performance of deep-learning-based environmental perception. To overcome these challenges, we propose a conditional generative framework for synthesizing realistic Frequency-Modulated Continuous-Wave radar Range-Azimuth Maps. Our approach leverages a generative diffusion model to generate radar data for multiple object categories, including pedestrians, cars, and cyclists. Specifically, conditioning is achieved via Confidence Maps, where each channel represents a semantic class and encodes Gaussian-distributed annotations at target locations. To address radar-specific characteristics, we incorporate Geometry Aware Conditioning and Temporal Consistency Regularization into the generative process. Experiments on the ROD2021 dataset demonstrate that signal reconstruction quality improves by \SI{3.6}{dB} in Peak Signal-to-Noise Ratio over baseline methods, while training with a combination of real and synthetic datasets improves overall mean Average Precision by 4.15% compared with conventional image-processing-based augmentation. These results indicate that our generative framework not only produces physically plausible and diverse radar spectrum but also substantially improves model generalization in downstream tasks.

</details>


### [22] [A survey of facial recognition techniques](https://arxiv.org/abs/2601.06239)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: 综述面向人脸识别的挑战与主流方法，比较多种数据库上的实验，汇总方法优缺点与应用。


<details>
  <summary>Details</summary>
Motivation: 多媒体激增推动人脸识别需求，但人脸受光照、年龄、姿态、遮挡、表情等因素显著影响，亟需系统梳理挑战与解决思路，指导有效算法与应用落地。

Method: 文献综述与对比：围绕关键挑战分类整理方法，重点覆盖HMM、PCA/Eigenfaces、弹性图匹配、SVM、Gabor、小波/卷积思想相关、ANN、ICA、3D形变模型等；并基于常见数据库（JAFFE、FEI、Yale、LFW、AT&T/ORL、AR）汇集并分析实验表现。

Result: 各方法在不同干扰因素下性能差异明显：PCA/Eigenfaces易受光照与表情影响；Gabor与SVM在表情与小姿态变化下更稳健；ICA能提取独立特征但计算较重；HMM与弹性图匹配对结构变化更鲁棒；3D形变模型对姿态/光照有优势但代价高。跨库如LFW更具挑战，实验体现传统方法在非受控环境下性能受限。

Conclusion: 人脸识别需针对光照、姿态、遮挡、表情、年龄等多因素综合建模。传统方法各有长短，受非受控环境限制明显；结合多特征、多模型与三维信息可提升鲁棒性。数据库基准对比对方法选择与评估至关重要。

Abstract: As multimedia content is quickly growing, the field of facial recognition has become one of the major research fields, particularly in the recent years. The most problematic area to researchers in image processing and computer vision is the human face which is a complex object with myriads of distinctive features that can be used to identify the face. The survey of this survey is particularly focused on most challenging facial characteristics, including differences in the light, ageing, variation in poses, partial occlusion, and facial expression and presents methodological solutions. The factors, therefore, are inevitable in the creation of effective facial recognition mechanisms used on facial images. This paper reviews the most sophisticated methods of facial detection which are Hidden Markov Models, Principal Component Analysis (PCA), Elastic Cluster Plot Matching, Support Vector Machine (SVM), Gabor Waves, Artificial Neural Networks (ANN), Eigenfaces, Independent Component Analysis (ICA), and 3D Morphable Model. Alongside the works mentioned above, we have also analyzed the images of a number of facial databases, namely JAFEE, FEI, Yale, LFW, AT&T (then called ORL), and AR (created by Martinez and Benavente), to analyze the results. However, this survey is aimed at giving a thorough literature review of face recognition, and its applications, and some experimental results are provided at the end after a detailed discussion.

</details>


### [23] [EyeTheia: A Lightweight and Accessible Eye-Tracking Toolbox](https://arxiv.org/abs/2601.06279)
*Stevenson Pather,Niels Martignène,Arnaud Bugnet,Fouad Boutaleb,Fabien D'Hondt,Deise Santana Maia*

Main category: cs.CV

TL;DR: EyeTheia 是一套开源、轻量的基于笔记本摄像头的凝视估计管线，结合 MediaPipe 关键点与 iTracker 风格的CNN，并支持个体化微调；在公开数据与真实任务中表现与商业方案相近，适合可扩展、可复现的研究。


<details>
  <summary>Details</summary>
Motivation: 浏览器与现实场景中的认知与临床研究需要低成本、可部署、可复现实验的凝视追踪方案；现有商业工具闭源且成本高，学术方案常依赖专用硬件或难以在网页端实时运行。

Method: 提出 EyeTheia：1) 使用 MediaPipe 提取面部/眼部关键点；2) 采用受 iTracker 启发的卷积网络进行凝视回归；3) 提供两条训练路径：a) 迁移一份在移动端数据上预训练的模型到桌面场景；b) 在桌面数据集上从零训练相同架构；4) 支持轻量级用户特定微调与简易校准；5) 在 MPIIFaceGaze 与真实 Dot-Probe 任务中验证，并与 SeeSo SDK 对比。

Result: 在 MPIIFaceGaze 上，两种训练策略在无个体校准前性能相当；加入轻量级用户微调可稳定降低凝视误差。在 Dot-Probe 任务中，与 SeeSo 在左右注意分配上高度一致，但时间稳定性较差（更高的时序变异）。

Conclusion: EyeTheia 实现了仅用普通摄像头的实时凝视追踪，开源透明、易扩展，成本低且适合大规模、可复现的实验与临床研究；代码、模型与材料均已公开。

Abstract: We introduce EyeTheia, a lightweight and open deep learning pipeline for webcam-based gaze estimation, designed for browser-based experimental platforms and real-world cognitive and clinical research. EyeTheia enables real-time gaze tracking using only a standard laptop webcam, combining MediaPipe-based landmark extraction with a convolutional neural network inspired by iTracker and optional user-specific fine-tuning. We investigate two complementary strategies: adapting a model pretrained on mobile data and training the same architecture from scratch on a desktop-oriented dataset. Validation results on MPIIFaceGaze show comparable performance between both approaches prior to calibration, while lightweight user-specific fine-tuning consistently reduces gaze prediction error. We further evaluate EyeTheia in a realistic Dot-Probe task and compare it to the commercial webcam-based tracker SeeSo SDK. Results indicate strong agreement in left-right gaze allocation during stimulus presentation, despite higher temporal variability. Overall, EyeTheia provides a transparent and extensible solution for low-cost gaze tracking, suitable for scalable and reproducible experimental and clinical studies. The code, trained models, and experimental materials are publicly available.

</details>


### [24] [NAS-GS: Noise-Aware Sonar Gaussian Splatting](https://arxiv.org/abs/2601.06285)
*Shida Xu,Jingqi Jiang,Jonatan Scharff Willners,Sen Wang*

Main category: cs.CV

TL;DR: NAS-GS提出针对声呐成像的噪声感知高斯溅射框架，结合双向溅射与GMM噪声建模，在模拟与真实海上场景中实现更快更准的视图合成与三维重建。


<details>
  <summary>Details</summary>
Motivation: 声呐图像噪声复杂且缺少高度信息，导致传统3D重建与新视角合成效果差、速度慢，需有专门针对声呐物理与噪声机理的渲染/重建方法。

Method: (1) Two-Ways Splatting：分别为声呐成像中的强度累积与透射度计算建立双向建模，加速同时保持质量；(2) 基于GMM的噪声模型，显式刻画旁瓣、散斑与多径等噪声分布，并在渲染与优化中使用，以避免3D高斯对噪声过拟合。

Result: 在模拟与真实大规模近海声呐数据上取得SOTA的新视角合成与三维重建表现，并显著提升渲染速度。

Conclusion: 面向声呐成像的NAS-GS通过物理一致的双向溅射与GMM噪声建模，提高渲染效率与重建精度，为水下应用提供更可靠的视图合成与3D重建方案。

Abstract: Underwater sonar imaging plays a crucial role in various applications, including autonomous navigation in murky water, marine archaeology, and environmental monitoring. However, the unique characteristics of sonar images, such as complex noise patterns and the lack of elevation information, pose significant challenges for 3D reconstruction and novel view synthesis. In this paper, we present NAS-GS, a novel Noise-Aware Sonar Gaussian Splatting framework specifically designed to address these challenges. Our approach introduces a Two-Ways Splatting technique that accurately models the dual directions for intensity accumulation and transmittance calculation inherent in sonar imaging, significantly improving rendering speed without sacrificing quality. Moreover, we propose a Gaussian Mixture Model (GMM) based noise model that captures complex sonar noise patterns, including side-lobes, speckle, and multi-path noise. This model enhances the realism of synthesized images while preventing 3D Gaussian overfitting to noise, thereby improving reconstruction accuracy. We demonstrate state-of-the-art performance on both simulated and real-world large-scale offshore sonar scenarios, achieving superior results in novel view synthesis and 3D reconstruction.

</details>


### [25] [Perception Test 2025: Challenge Summary and a Unified VQA Extension](https://arxiv.org/abs/2601.06287)
*Joseph Heyward,Nikhil Pathasarathy,Tyler Zhu,Aravindh Mahendran,João Carreira,Dima Damen,Andrew Zisserman,Viorica Pătrăucean*

Main category: cs.CV

TL;DR: ICCV 2025 的 Perception Test 挑战聚焦“统一化”评测，整合多种视频理解与定位任务，要求参赛者用统一模型而非任务特定管线，结果显示现有 SOTA 多模态模型在统一接口下仍面临显著困难。


<details>
  <summary>Details</summary>
Motivation: 现有多模态感知研究常以任务专用模型与独立赛道评测，难以反映真实应用中的通用性与可扩展性。主办方旨在通过统一化任务设置，客观衡量视频语言模型在跨任务、跨模态的泛化能力与上限。

Method: 以挑战赛形式构建基准，设置五个“统一”赛道：统一视频问答、统一目标与点跟踪、统一动作与声音定位、具象化（grounded）视频问答、超长时长（小时级）视频问答，并保留分析与可解释性开放赛道；将传统感知任务（如点跟踪、时间动作定位）重新表述为多选视频QA问题；强制参赛者采用统一方法而非工程化拼接。

Result: 对各赛道提交进行评测与汇总：统一化设定显著提升了任务难度，揭示当前SOTA多模态模型在跨任务一致性、长时视频理解、以及时空-音频联合定位方面的不足。报告总结了各赛道表现与新基准的测量结果（未给出具体数值）。

Conclusion: 统一化基准有效暴露了现有模型在多样视频感知任务中的瓶颈，提示未来需要构建真正通用的多模态视频理解框架，改进长时记忆、时空对齐与跨模态融合能力；分析与可解释性赛道仍开放，鼓励持续改进。

Abstract: The Third Perception Test challenge was organised as a full-day workshop alongside the IEEE/CVF International Conference on Computer Vision (ICCV) 2025. Its primary goal is to benchmark state-of-the-art video models and measure the progress in multimodal perception. This year, the workshop featured 2 guest tracks as well: KiVA (an image understanding challenge) and Physic-IQ (a video generation challenge). In this report, we summarise the results from the main Perception Test challenge, detailing both the existing tasks as well as novel additions to the benchmark. In this iteration, we placed an emphasis on task unification, as this poses a more challenging test for current SOTA multimodal models. The challenge included five consolidated tracks: unified video QA, unified object and point tracking, unified action and sound localisation, grounded video QA, and hour-long video QA, alongside an analysis and interpretability track that is still open for submissions. Notably, the unified video QA track introduced a novel subset that reformulates traditional perception tasks (such as point tracking and temporal action localisation) as multiple-choice video QA questions that video-language models can natively tackle. The unified object and point tracking merged the original object tracking and point tracking tasks, whereas the unified action and sound localisation merged the original temporal action localisation and temporal sound localisation tracks. Accordingly, we required competitors to use unified approaches rather than engineered pipelines with task-specific models. By proposing such a unified challenge, Perception Test 2025 highlights the significant difficulties existing models face when tackling diverse perception tasks through unified interfaces.

</details>


### [26] [VideoWeave: A Data-Centric Approach for Efficient Video Understanding](https://arxiv.org/abs/2601.06309)
*Zane Durante,Silky Singh,Arpandeep Khatua,Shobhit Agarwal,Reuben Tan,Yong Jae Lee,Jianfeng Gao,Ehsan Adeli,Li Fei-Fei*

Main category: cs.CV

TL;DR: VideoWeave通过将多段带字幕的短视频拼接成合成长上下文样本，在不改模型结构或训练目标的前提下，以相同计算预算提升视频-语言模型的性能与数据效率。


<details>
  <summary>Details</summary>
Motivation: 长视频处理计算昂贵且标注稀缺，导致训练视频-语言模型成本高、数据效率低；需要一种无需修改架构、能扩展时间多样性的训练方式。

Method: 从现有数据集中选取多段短视频及其字幕，进行“编织式”拼接以构造长上下文训练样本；系统比较不同数据构成策略（随机拼接 vs. 按视觉相似度聚类拼接）以及字幕增强对下游视频问答任务的影响；在固定计算预算下进行训练与评测。

Result: 在相同计算约束下，采用VideoWeave的数据重组策略进行微调的模型在视频问答任务上取得更高准确率，优于常规视频微调流程。

Conclusion: 通过重组训练数据（时间跨度与多样性）而非修改模型架构或损失函数，可成为训练视频-语言模型的一条简单、可扩展且更具数据效率的途径；代码已公开。

Abstract: Training video-language models is often prohibitively expensive due to the high cost of processing long frame sequences and the limited availability of annotated long videos. We present VideoWeave, a simple yet effective approach to improve data efficiency by constructing synthetic long-context training samples that splice together short, captioned videos from existing datasets. Rather than modifying model architectures or optimization objectives, VideoWeave reorganizes available video-text pairs to expand temporal diversity within fixed compute. We systematically study how different data composition strategies like random versus visually clustered splicing and caption enrichment affect downstream performance on downstream video question answering. Under identical compute constraints, models trained with VideoWeave achieve higher accuracy than conventional video finetuning. Our results highlight that reorganizing training data, rather than altering architectures, may offer a simple and scalable path for training video-language models. We link our code for all experiments here.

</details>


### [27] [Object-WIPER : Training-Free Object and Associated Effect Removal in Videos](https://arxiv.org/abs/2601.06391)
*Saksham Singh Kushwaha,Sayan Nag,Yapeng Tian,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: Object-WIPER 是一个无需训练的文本引导视频去物体与去效应框架，基于预训练的文本到视频扩散Transformer，通过跨/自注意力定位并替换前景token，结合反演与背景token拷贝实现时空一致的重绘，并提出新的评价指标；在 DAVIS 与新建的 WIPER-Bench 上优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 现有视频目标移除方法要么需要再训练、要么难以同时保证语义一致与时间稳定，且缺乏专门评价含“伴随视觉效应”的动态目标移除质量的指标。

Method: 利用预训练Text-to-Video DiT：1) 输入视频、用户对象掩码与对象描述文本；2) 通过视觉-文本跨注意力与视觉自注意力定位与对象及其效应相关的视觉token，生成中间效应掩码并与用户掩码融合，得到最终前景token掩码；3) 先对视频进行DiT反演得到结构化噪声；4) 用高斯噪声重置前景token，同时保持背景token；5) 在去噪过程中拷贝反演得到的背景token值以维持场景保真；6) 提出面向前景时序一致、前景-背景帧内协调、以及前景变化性的综合新指标。

Result: 在 DAVIS 和作者构建的真实世界“伴随效应”基准 WIPER-Bench 上，Object-WIPER 在新指标上显著优于训练型与免训练基线，实现更干净的移除与时间稳定的重建，无需任何再训练。

Conclusion: 预训练DiT的反演与token级掩码/拷贝结合，可在无需训练的前提下实现高质量、时空一致的动态目标与效应移除；所提评价指标与公开基准/代码促进该方向的可比较研究。

Abstract: In this paper, we introduce Object-WIPER, a training-free framework for removing dynamic objects and their associated visual effects from videos, and inpainting them with semantically consistent and temporally coherent content. Our approach leverages a pre-trained text-to-video diffusion transformer (DiT). Given an input video, a user-provided object mask, and query tokens describing the target object and its effects, we localize relevant visual tokens via visual-text cross-attention and visual self-attention. This produces an intermediate effect mask that we fuse with the user mask to obtain a final foreground token mask to replace. We first invert the video through the DiT to obtain structured noise, then reinitialize the masked tokens with Gaussian noise while preserving background tokens. During denoising, we copy values for the background tokens saved during inversion to maintain scene fidelity. To address the lack of suitable evaluation, we introduce a new object removal metric that rewards temporal consistency among foreground tokens across consecutive frames, coherence between foreground and background tokens within each frame, and dissimilarity between the input and output foreground tokens. Experiments on DAVIS and a newly curated real-world associated effect benchmark (WIPER-Bench) show that Object-WIPER surpasses both training-based and training-free baselines in terms of the metric, achieving clean removal and temporally stable reconstruction without any retraining. Our new benchmark, source code, and pre-trained models will be publicly available.

</details>


### [28] [Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification](https://arxiv.org/abs/2601.06394)
*Ahmed Abdelkawy,Ahmed Elsayed,Asem Ali,Aly Farag,Thomas Tretter,Michael McIntyre*

Main category: cs.CV

TL;DR: 提出三阶段框架：少样本微调视觉-语言模型识别学生动作；滑动时间窗将2分钟视频分段并序列化动作；大语言模型结合同伴上下文对整段行为序列判定投入度。实验表明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有学生投入度预测依赖大量标注且因隐私难以共享数据，且忽视课堂同伴行为等情境信息。需要能在少样本与跨域条件下利用上下文的方案。

Method: 三阶段：1) 少样本自适应的VLM，对学生动作类别进行微调以小样本识别；2) 用滑动/非重叠时间窗把每位学生的2分钟视频切片，对每段用微调后的VLM预测动作，形成动作序列；3) 将该序列连同课堂上下文输入LLM，输出投入/不投入分类。

Result: 实验显示该框架能有效区分投入与不投入学生，优于或至少可行于现有方法（摘要未给具体数值）。

Conclusion: 结合少样本VLM的动作识别与LLM对时序与课堂上下文的整合，可在数据受限与隐私约束下提升视频式学生投入度测量的有效性。

Abstract: Understanding student behavior in the classroom is essential to improve both pedagogical quality and student engagement. Existing methods for predicting student engagement typically require substantial annotated data to model the diversity of student behaviors, yet privacy concerns often restrict researchers to their own proprietary datasets. Moreover, the classroom context, represented in peers' actions, is ignored. To address the aforementioned limitation, we propose a novel three-stage framework for video-based student engagement measurement. First, we explore the few-shot adaptation of the vision-language model for student action recognition, which is fine-tuned to distinguish among action categories with a few training samples. Second, to handle continuous and unpredictable student actions, we utilize the sliding temporal window technique to divide each student's 2-minute-long video into non-overlapping segments. Each segment is assigned an action category via the fine-tuned VLM model, generating a sequence of action predictions. Finally, we leverage the large language model to classify this entire sequence of actions, together with the classroom context, as belonging to an engaged or disengaged student. The experimental results demonstrate the effectiveness of the proposed approach in identifying student engagement.

</details>


### [29] [GlobalPaint: Spatiotemporal Coherent Video Outpainting with Global Feature Guidance](https://arxiv.org/abs/2601.06413)
*Yueming Pan,Ruoyu Feng,Jianmin Bao,Chong Luo,Nanning Zheng*

Main category: cs.CV

TL;DR: GlobalPaint是一种基于扩散模型的视频外延（outpainting）方法，通过先关键帧外延再插帧的分层流程，并结合增强的时空模块与全局特征引导，实现更高的空间合理性与长程时序一致性。


<details>
  <summary>Details</summary>
Motivation: 视频外延比图像外延更难，因为需要跨帧的一致性：随着相机或物体运动，外延区域会在时间轴上逐渐显露，要求既要每帧空间合理，又要长程时序连贯。现有顺序处理易误差累积、运动不自然。

Method: 提出GlobalPaint框架：1) 分层pipeline：先对关键帧进行外延，再用受边界条件约束的插值模型完成中间帧，降低顺序误差累积；2) 模型增强：在预训练图像修复骨干上加入( i ) Enhanced Spatial-Temporal模块，采用3D窗口注意力以加强时空交互；( ii ) 全局特征引导，从所有帧可见区域提取OpenCLIP特征，经专门的提取器蒸馏压缩为全局token，指导生成。

Result: 在基准数据集上实现更优的重建质量与更自然的运动，相比以往方法有明显提升；提供演示页面展示效果。

Conclusion: 分层关键帧外延+条件插帧结合3D时空注意力与全局语义token可有效提升视频外延的时空一致性与画面质量，减少误差累积，具有实用价值。

Abstract: Video outpainting extends a video beyond its original boundaries by synthesizing missing border content. Compared with image outpainting, it requires not only per-frame spatial plausibility but also long-range temporal coherence, especially when outpainted content becomes visible across time under camera or object motion. We propose GlobalPaint, a diffusion-based framework for spatiotemporal coherent video outpainting. Our approach adopts a hierarchical pipeline that first outpaints key frames and then completes intermediate frames via an interpolation model conditioned on the completed boundaries, reducing error accumulation in sequential processing. At the model level, we augment a pretrained image inpainting backbone with (i) an Enhanced Spatial-Temporal module featuring 3D windowed attention for stronger spatiotemporal interaction, and (ii) global feature guidance that distills OpenCLIP features from observed regions across all frames into compact global tokens using a dedicated extractor. Comprehensive evaluations on benchmark datasets demonstrate improved reconstruction quality and more natural motion compared to prior methods. Our demo page is https://yuemingpan.github.io/GlobalPaint/

</details>


### [30] [WHU-PCPR: A cross-platform heterogeneous point cloud dataset for place recognition in complex urban scenes](https://arxiv.org/abs/2601.06442)
*Xianghong Zou,Jianping Li,Yandi Yang,Weitong Wu,Yuan Wang,Qiegen Liu,Zhen Dong*

Main category: cs.CV

TL;DR: 提出WHU-PCPR，一个跨平台、跨传感器、跨场景的大规模点云定位识别数据集，并基于该数据集系统评测多种PCPR方法，揭示挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有点云地点识别数据集在场景、平台与传感器多样性不足，难以支撑在真实应用（跨平台/跨LiDAR、长期与复杂场景）中的泛化与鲁棒性研究。

Method: 构建WHU-PCPR数据集：从测绘级车载MLS与低成本头戴式PLS两类平台采集，包含机械式与固态LiDAR；覆盖城市与校园道路，含实时与长期环境变化；轨迹总长82.3 km，时间跨度60个月，约30 km不重复路线。基于该数据集，对多种代表性PCPR方法进行系统评测与深入分析，并发布基准代码。

Result: 得到一个跨平台异构、场景复杂、覆盖广的大规模PCPR数据集；完成对多种主流方法的广泛评测，给出在跨平台/长期变化等设置下的性能表现与差异观察。

Conclusion: WHU-PCPR填补了PCPR在跨平台与长期场景下数据不足的空白，可作为新的标准基准推动方法泛化与鲁棒性研究；作者总结关键挑战并指向未来研究方向，数据与基准代码已开源。

Abstract: Point Cloud-based Place Recognition (PCPR) demonstrates considerable potential in applications such as autonomous driving, robot localization and navigation, and map update. In practical applications, point clouds used for place recognition are often acquired from different platforms and LiDARs across varying scene. However, existing PCPR datasets lack diversity in scenes, platforms, and sensors, which limits the effective development of related research. To address this gap, we establish WHU-PCPR, a cross-platform heterogeneous point cloud dataset designed for place recognition. The dataset differentiates itself from existing datasets through its distinctive characteristics: 1) cross-platform heterogeneous point clouds: collected from survey-grade vehicle-mounted Mobile Laser Scanning (MLS) systems and low-cost Portable helmet-mounted Laser Scanning (PLS) systems, each equipped with distinct mechanical and solid-state LiDAR sensors. 2) Complex localization scenes: encompassing real-time and long-term changes in both urban and campus road scenes. 3) Large-scale spatial coverage: featuring 82.3 km of trajectory over a 60-month period and an unrepeated route of approximately 30 km. Based on WHU-PCPR, we conduct extensive evaluation and in-depth analysis of several representative PCPR methods, and provide a concise discussion of key challenges and future research directions. The dataset and benchmark code are available at https://github.com/zouxianghong/WHU-PCPR.

</details>


### [31] [How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research](https://arxiv.org/abs/2601.06443)
*Xiaoya Tang,Xiaohe Yue,Heran Mane,Dapeng Li,Quynh Nguyen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: 研究评估并给出在小样本、少标注场景下，将通用视觉基础模型迁移到街景（GSV）等邻里环境影像的实用方案，比较无监督自适应前后性能与可视化表现，提供可操作建议。


<details>
  <summary>Details</summary>
Motivation: 邻里建成环境与健康结果显著相关，但将ImageNet等域的模型迁移到风格迥异的GSV影像存在泛化不确定；实践者还面临模型选择、是否采用无监督策略、可行训练规模及其收益等决策成本高、门槛高的问题。

Method: 基于有限有标注数据与更大规模无标注街景数据，选择多种基础视觉模型进行比较；采用无监督/自监督适配（如域自适应或对比学习）对模型在目标域进行预训练或微调；开展系统的定量评测与可视化分析，比较适配前后在下游任务上的表现差异。

Result: 无监督域内适配能在计算受限条件下显著提升下游任务性能；不同基础模型在GSV等目标域的迁移能力存在差异，适配规模与收益呈递增但非线性关系；可视化显示特征更聚类、对目标域结构更敏感。

Conclusion: 针对小样本邻里环境影像，优先选用合适的基础模型并结合无监督域内适配，可在有限算力下取得可观增益；提供了模型选择与训练规模的实用指南，以支持社会健康研究的可扩展表征与分析。

Abstract: A substantial body of health research demonstrates a strong link between neighborhood environments and health outcomes. Recently, there has been increasing interest in leveraging advances in computer vision to enable large-scale, systematic characterization of neighborhood built environments. However, the generalizability of vision models across fundamentally different domains remains uncertain, for example, transferring knowledge from ImageNet to the distinct visual characteristics of Google Street View (GSV) imagery. In applied fields such as social health research, several critical questions arise: which models are most appropriate, whether to adopt unsupervised training strategies, what training scale is feasible under computational constraints, and how much such strategies benefit downstream performance. These decisions are often costly and require specialized expertise.
  In this paper, we answer these questions through empirical analysis and provide practical insights into how to select and adapt foundation models for datasets with limited size and labels, while leveraging larger, unlabeled datasets through unsupervised training. Our study includes comprehensive quantitative and visual analyses comparing model performance before and after unsupervised adaptation.

</details>


### [32] [Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs](https://arxiv.org/abs/2601.06460)
*Weihao Hong,Zhiyuan Jiang,Bingyu Shen,Xinlei Guan,Yangyi Feng,Meng Xu,Boyang Li*

Main category: cs.CV

TL;DR: 论文探究提示语“压力”如何系统性诱发/抑制视觉语言模型（VLM）的视觉幻觉，提出Ghost-100合成数据集与五级提示强度框架，评测三款开源权重VLM，发现幻觉率随提示强度并非单调上升，高强度时在不同阈值出现下降，但在极端强制下不一定持续降低，显示模型对语义敌意更敏感、对结构性强制不够鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有对VLM幻觉的评测多聚焦“有/无对象”的事实性错误，忽视了提示语措辞与格式约束等“提示压力”如何诱导缺失信息上的幻觉；为在安全关键场景中实现可靠视觉对齐，需要系统化、可控地刻画这类行为。

Method: 构造Ghost-100：程序化生成的合成场景，刻意移除关键视觉细节，用于研究“缺失基础上的幻觉”。提出五级提示强度框架，从中性查询到带有敌意/强制语气与严格格式约束。选取MiniCPM-V 2.6-8B、Qwen2-VL-7B、Qwen3-VL-8B三种开源VLM，比较不同强度下的幻觉率变化。

Result: 三个模型的幻觉率均不随提示强度单调上升；在更高强度区间出现下降，但阈值各异；在最强强制条件下并非所有模型都保持下降趋势。表明安全对齐对语义层面的敌意检测更有效，而对结构性强制（格式、刚性要求）的抵御不足，存在模型特异的脆弱性。

Conclusion: 提示语的语气与结构会系统性影响VLM的缺失型幻觉；当前对齐策略对敌意语义更敏感，对结构化强制不够稳健。Ghost-100与五级框架为更细粒度评测与鲁棒对齐提供基准与分析工具，代码数据已开源。

Abstract: Vision-Language Models (VLMs) are increasingly used in safety-critical applications that require reliable visual grounding. However, these models often hallucinate details that are not present in the image to satisfy user prompts. While recent datasets and benchmarks have been introduced to evaluate systematic hallucinations in VLMs, many hallucination behaviors remain insufficiently characterized. In particular, prior work primarily focuses on object presence or absence, leaving it unclear how prompt phrasing and structural constraints can systematically induce hallucinations. In this paper, we investigate how different forms of prompt pressure influence hallucination behavior. We introduce Ghost-100, a procedurally generated dataset of synthetic scenes in which key visual details are deliberately removed, enabling controlled analysis of absence-based hallucinations. Using a structured 5-Level Prompt Intensity Framework, we vary prompts from neutral queries to toxic demands and rigid formatting constraints. We evaluate three representative open-weight VLMs: MiniCPM-V 2.6-8B, Qwen2-VL-7B, and Qwen3-VL-8B. Across all three models, hallucination rates do not increase monotonically with prompt intensity. All models exhibit reductions at higher intensity levels at different thresholds, though not all show sustained reduction under maximum coercion. These results suggest that current safety alignment is more effective at detecting semantic hostility than structural coercion, revealing model-specific limitations in handling compliance pressure. Our dataset is available at: https://github.com/bli1/tone-matters

</details>


### [33] [On the Adversarial Robustness of 3D Large Vision-Language Models](https://arxiv.org/abs/2601.06464)
*Chao Liu,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 论文对点云类3D视觉-语言模型的对抗鲁棒性做了首个系统研究，提出两类攻击（视觉攻击与描述攻击），包含无目标与有目标变体。结果显示3D VLM在无目标攻击下脆弱，但比2D VLM更能抵抗强制生成特定有害输出的有目标攻击。


<details>
  <summary>Details</summary>
Motivation: 2D VLM已被证明因视觉输入而更易受对抗攻击，可能被操纵输出有害或误导内容；而3D VLM（如PointLLM、GPT4Point）虽在推理与泛化上强，但其鲁棒性尚未研究。作者欲回答：引入3D视觉是否同样削弱鲁棒性，并为安全关键应用提供依据。

Method: 面向点云3D VLM构建两条互补攻击链：1) Vision Attack：扰动3D编码器与投影器产生的视觉token特征，检验视觉-语言对齐鲁棒性；2) Caption Attack：直接操纵输出token序列，评估端到端系统鲁棒性。两类攻击均含无目标与有目标版本，用于量化一般脆弱性与受控操纵易感性。

Result: 实验显示：在无目标攻击下，3D VLM出现显著性能退化，表明存在明显脆弱性；但在有目标攻击（强迫生成特定有害输出）方面，其抗性强于2D VLM对照。

Conclusion: 3D VLM对抗鲁棒性问题真实存在，尤其在无目标场景；但在强制生成特定有害内容上较2D VLM更稳健。应进一步提升3D VLM的对抗鲁棒性，以满足安全关键部署需求。

Abstract: 3D Vision-Language Models (VLMs), such as PointLLM and GPT4Point, have shown strong reasoning and generalization abilities in 3D understanding tasks. However, their adversarial robustness remains largely unexplored. Prior work in 2D VLMs has shown that the integration of visual inputs significantly increases vulnerability to adversarial attacks, making these models easier to manipulate into generating toxic or misleading outputs. In this paper, we investigate whether incorporating 3D vision similarly compromises the robustness of 3D VLMs. To this end, we present the first systematic study of adversarial robustness in point-based 3D VLMs. We propose two complementary attack strategies: \textit{Vision Attack}, which perturbs the visual token features produced by the 3D encoder and projector to assess the robustness of vision-language alignment; and \textit{Caption Attack}, which directly manipulates output token sequences to evaluate end-to-end system robustness. Each attack includes both untargeted and targeted variants to measure general vulnerability and susceptibility to controlled manipulation. Our experiments reveal that 3D VLMs exhibit significant adversarial vulnerabilities under untargeted attacks, while demonstrating greater resilience against targeted attacks aimed at forcing specific harmful outputs, compared to their 2D counterparts. These findings highlight the importance of improving the adversarial robustness of 3D VLMs, especially as they are deployed in safety-critical applications.

</details>


### [34] [SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning](https://arxiv.org/abs/2601.06474)
*Chenxu Dang,Jie Wang,Guang Li,Zhiwen Hou,Zihan You,Hangjun Ye,Jie Ma,Long Chen,Yan Wang*

Main category: cs.CV

TL;DR: 提出SparseOccVLA：以稀疏占用查询为桥，统一场景理解、占用预测与轨迹规划的VLA框架，兼顾VLM高层推理与占用图细粒度空间表达，并以LLM引导的锚点扩散规划器实现SOTA性能提升。


<details>
  <summary>Details</summary>
Motivation: VLM擅长高层语义与推理但存在token爆炸与时空推理受限；语义占用提供统一显式空间表示但过于稠密、难与VLM高效对接。缺乏一个能高效融合二者、在自动驾驶中同时完成理解、预测与规划的统一方法。

Method: 1) 轻量级Sparse Occupancy Encoder从多视角感知中提取紧凑且信息密集的稀疏占用查询；2) 将这些查询对齐到语言空间，交由LLM进行统一的场景理解与未来占用推理；3) 提出LLM-guided Anchor-Diffusion Planner：将锚点评分与去噪解耦，并进行跨模态的轨迹条件融合，实现高效、可控的规划。整个系统作为VLA模型统一视觉-语言-行动。

Result: 在OmniDrive-nuScenes上CIDEr相对提升7%；在Occ3D-nuScenes上mIoU提升0.5；在nuScenes开放环规划指标上达成SOTA，显示出端到端理解、预测与规划的整体优势。

Conclusion: 稀疏占用查询有效连接VLM与占用表示，兼顾效率与表达力；LLM引导的锚点扩散规划提升规划质量。SparseOccVLA在多基准上达SOTA，证明了统一VLA范式在自动驾驶中的可行性与优越性。

Abstract: In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.

</details>


### [35] [VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment](https://arxiv.org/abs/2601.06475)
*Kai Cheng,Ruoqi Wang,Qiong Luo*

Main category: cs.CV

TL;DR: 提出VVTRec：把稀疏可见度数据同时转换为图像特征与文本特征，并用VLM无训练增益，引导重建更干净的射电天文图像。


<details>
  <summary>Details</summary>
Motivation: 射电干涉测量得到的可见度经成像会产生“脏图”，包含真实源与伪影。现有重建多仅利用单一模态的稀疏可见度，导致伪影残留、空间结构与相关性建模不足。需要一种能更充分利用可见度并在图像域强调质量的多模态方法。

Method: 提出VVTRec：将稀疏可见度映射到两种增强通道——图像形态特征（空间结构）与文本形态特征（语义描述），进行可见度引导的视觉与文本模态融合重建；并借助预训练视觉-语言模型（VLM）进行“训练免”性能增益，使VLM在未见过的可见度模态下也能提取并迁移先验知识以补充重建。

Result: 实验表明，VVTRec能在不显著增加计算开销的情况下，利用多模态信息显著提升成像质量，减少伪影并提高结构完整性与准确性。

Conclusion: 多模态、可见度引导的视觉与文本增强结合VLM先验，可在稀疏测量下显著改进射电干涉成像；方法简单高效，具备训练外泛化与实际应用潜力。

Abstract: Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be transformed into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a multimodal radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is transformed into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting multimodal information without introducing excessive computational overhead.

</details>


### [36] [SRFlow: A Dataset and Regularization Model for High-Resolution Facial Optical Flow via Splatting Rasterization](https://arxiv.org/abs/2601.06479)
*JiaLin Zhang,Dong Li*

Main category: cs.CV

TL;DR: 提出SRFlow高分辨率人脸光流数据集与SRFlowNet模型，通过基于高斯splatting栅格化的面部运动监督与正则化，显著提升人脸光流与微表情识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有人脸光流数据集分辨率不足且对纹理稀疏/重复区域的流估计易噪，限制了高精度面部运动分析与微表情识别的发展。

Method: 1) 构建SRFlow高分辨率人脸光流数据集；2) 设计SRFlowNet，结合高斯splatting栅格化引导，并加入掩膜与基于差分/ Sobel算子的梯度正则，抑制纹理缺失或重复区域的高频噪声与大尺度误差，从而更好地捕获皮肤精细运动。

Result: 在多种光流模型上，用SRFlow训练可将EPE最高降42%(0.5081→0.2953)；结合SRFlow数据集，SRFlowNet在三种微表情数据集组合上F1最高提升48%(0.4733→0.6947)。

Conclusion: 高分辨率数据与针对性正则化协同提升了人脸光流估计与微表情识别，SRFlow与SRFlowNet为捕获细粒度皮肤运动提供了有效途径。

Abstract: Facial optical flow supports a wide range of tasks in facial motion analysis. However, the lack of high-resolution facial optical flow datasets has hindered progress in this area. In this paper, we introduce Splatting Rasterization Flow (SRFlow), a high-resolution facial optical flow dataset, and Splatting Rasterization Guided FlowNet (SRFlowNet), a facial optical flow model with tailored regularization losses. These losses constrain flow predictions using masks and gradients computed via difference or Sobel operator. This effectively suppresses high-frequency noise and large-scale errors in texture-less or repetitive-pattern regions, enabling SRFlowNet to be the first model explicitly capable of capturing high-resolution skin motion guided by Gaussian splatting rasterization. Experiments show that training with the SRFlow dataset improves facial optical flow estimation across various optical flow models, reducing end-point error (EPE) by up to 42% (from 0.5081 to 0.2953). Furthermore, when coupled with the SRFlow dataset, SRFlowNet achieves up to a 48% improvement in F1-score (from 0.4733 to 0.6947) on a composite of three micro-expression datasets. These results demonstrate the value of advancing both facial optical flow estimation and micro-expression recognition.

</details>


### [37] [Learning Domain Agnostic Latent Embeddings of 3D Faces for Zero-shot Animal Expression Transfer](https://arxiv.org/abs/2601.06484)
*Yue Wang,Lawrence Amadi,Xiang Gao,Yazheng Chen,Yuanpeng Liu,Ning Lu,Xianfeng Gu*

Main category: cs.CV

TL;DR: 提出零样本的人脸表情到3D动物面部网格的迁移框架，通过几何本征描述符与解耦身份/表情的网格无关潜空间，实现无需动物表情数据的跨物种表情转移。


<details>
  <summary>Details</summary>
Motivation: 跨物种面部几何差异大、标注昂贵且缺乏动物表情数据；现有方法多依赖同类数据或对应关系，难以在物种间泛化。

Method: 结合HKS/WKS等本征几何描述符与两个潜空间：ID空间捕获与物种无关的面部结构，Expression空间编码可跨人/动物泛化的变形模式。使用仅有人类表情对进行训练，学习表达的解耦与再耦合；损失包含顶点位置、Laplacian与Jacobian以保证几何与局部变形一致性。

Result: 在无动物表情数据参与训练的情况下，实现了合理的跨物种表情迁移，显著缩小人脸与动物面部形状的几何差距。

Conclusion: 通过本征几何+潜空间解耦的零样本框架，可在跨物种条件下进行表情转移，具有良好的泛化性与几何一致性约束。

Abstract: We present a zero-shot framework for transferring human facial expressions to 3D animal face meshes. Our method combines intrinsic geometric descriptors (HKS/WKS) with a mesh-agnostic latent embedding that disentangles facial identity and expression. The ID latent space captures species-independent facial structure, while the expression latent space encodes deformation patterns that generalize across humans and animals. Trained only with human expression pairs, the model learns the embeddings, decoupling, and recoupling of cross-identity expressions, enabling expression transfer without requiring animal expression data. To enforce geometric consistency, we employ Jacobian loss together with vertex-position and Laplacian losses. Experiments show that our approach achieves plausible cross-species expression transfer, effectively narrowing the geometric gap between human and animal facial shapes.

</details>


### [38] [3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence](https://arxiv.org/abs/2601.06496)
*Hao Tang,Ting Huang,Zeyu Zhang*

Main category: cs.CV

TL;DR: 提出3D CoCa v2：结合对比学习与生成的通用3D场景描述框架，并在推理阶段采用测试时搜索以提升稳健性与泛化。相比前作在多数据集上显著提升CIDEr与OOD能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云描述受点云稀疏/不规则、弱语义对齐和跨场景（室内/室外）OOD泛化差所限，导致描述准确性与稳健性不足。需要一种既能强语义对齐又能强泛化的3D caption方法。

Method: 采用冻结的CLIP语义先验、具空间感知的3D场景编码器（几何）、以及多模态解码器，联合优化对比式视觉-语言学习与描述生成；不依赖外部检测器或手工候选。推理时引入测试时搜索（TTS）：生成多样候选描述，并利用紧凑场景摘要进行基于奖励的选择，无需更新模型参数。

Result: 在ScanRefer上+1.50 CIDEr@0.5IoU、Nr3D上+1.61 CIDEr@0.5IoU，相比3D CoCa显著提升；在OOD的TOD3Cap零样本评测上+3.8 CIDEr@0.25，表明更强的跨域泛化与稳健性。

Conclusion: 3D CoCa v2通过联合对比学习与生成、结合冻结CLIP先验与空间编码器，并配合推理阶段的TTS，在无需额外检测器与参数更新的前提下，提升3D场景描述的语义对齐、鲁棒性与OOD泛化。代码将开源。

Abstract: Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.

</details>


### [39] [Bridging Robustness and Efficiency: Real-Time Low-Light Enhancement via Attention U-Net GAN](https://arxiv.org/abs/2601.06518)
*Yash Thesia,Meera Suthar*

Main category: cs.CV

TL;DR: 提出一种轻量级Attention U-Net GAN，用单次前向实现接近扩散模型的纹理恢复，在SID上以0.06s延迟取得高感知质量（LPIPS 0.112），比潜空间扩散快约40倍。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在极低照度增强中感知质量高但推理缓慢；CNN实时但易过度平滑、纹理缺失。实际亟需同时兼具生成级细节与边缘可部署速度的模型。

Method: 采用轻量级U-Net为骨干，在编码器-解码器的跳连处引入Attention Gate以选择性传递细节特征，并在条件对抗学习框架（cGAN）下训练，以逼近扩散模型带来的高频纹理恢复，同时避免多步迭代采样，实现单次前向推理。

Result: 在SID数据集上，相较高效基线（如SID、EnlightenGAN）取得更低的LPIPS（0.112，文中高效模型最佳），同时维持0.06秒/张的推理延迟；相较潜空间扩散模型实现约40倍加速。

Conclusion: 无需扩散模型的多步采样也能恢复高频纹理；通过Attention Gate + 轻量U-Net + 条件GAN，可在保持实时性的同时获得接近生成模型的感知质量，适用于近实时低照度增强场景。

Abstract: Recent advancements in Low-Light Image Enhancement (LLIE) have focused heavily on Diffusion Probabilistic Models, which achieve high perceptual quality but suffer from significant computational latency (often exceeding 2-4 seconds per image). Conversely, traditional CNN-based baselines offer real-time inference but struggle with "over-smoothing," failing to recover fine structural details in extreme low-light conditions. This creates a practical gap in the literature: the lack of a model that provides generative-level texture recovery at edge-deployable speeds. In this paper, we address this trade-off by proposing a hybrid Attention U-Net GAN. We demonstrate that the heavy iterative sampling of diffusion models is not strictly necessary for texture recovery. Instead, by integrating Attention Gates into a lightweight U-Net backbone and training within a conditional adversarial framework, we can approximate the high-frequency fidelity of generative models in a single forward pass. Extensive experiments on the SID dataset show that our method achieves a best-in-class LPIPS score of 0.112 among efficient models, significantly outperforming efficient baselines (SID, EnlightenGAN) while maintaining an inference latency of 0.06s. This represents a 40x speedup over latent diffusion models, making our approach suitable for near real-time applications.

</details>


### [40] [BabyVision: Visual Reasoning Beyond Language](https://arxiv.org/abs/2601.06521)
*Liang Chen,Weichu Xie,Yiyan Liang,Hongfeng He,Hans Zhao,Zhibo Yang,Zhiqi Huang,Haoning Wu,Haoyu Lu,Y. charles,Yiping Bao,Yuantao Fan,Guopeng Li,Haiyang Shen,Xuanzhong Chen,Wendong Xu,Shuzheng Si,Zefan Cai,Wenhao Chai,Ziqi Huang,Fangfu Liu,Tianyu Liu,Baobao Chang,Xiaobo Hu,Kaiyuan Chen,Yixin Ren,Yang Liu,Yuan Gong,Kuan Li*

Main category: cs.CV

TL;DR: BabyVision基准揭示：当前先进MLLM在基础视觉任务上远逊于人类幼童，显示其视觉原语能力薄弱；作者发布数据与工具以推动更接近人类水平的视觉理解与推理。


<details>
  <summary>Details</summary>
Motivation: 人类在学会语言前已具备稳健的核心视觉能力，但MLLM常依赖语言先验“弥补”视觉理解的不足。作者发现最强MLLM在幼儿可轻松完成的基础视觉任务上稳定失败，缺乏独立于语言的评估基准，需系统检验与推动模型的基础视觉能力。

Method: 提出BabyVision基准，覆盖4大类、22子类共388项基础视觉任务，尽量剔除语言线索以测量核心视觉能力；进行大规模实验与人类评测，比较多种先进MLLM与不同年龄段人类表现；进一步提出BabyVision-Gen与自动评测工具，探索用生成式方法解决视觉推理并实现可复现评估。

Result: SOTA MLLM表现显著低于人类：如Gemini3-Pro-Preview得分49.7，低于6岁儿童，远低于成人均值94.1；总体显示模型在知识型评测中虽强，但在基础视觉原语方面明显不足。

Conclusion: 当前MLLM缺乏独立于语言的核心视觉能力；BabyVision为衡量与提升这类能力提供标准化基准和工具。该方向的进步将是迈向人类级视觉感知与推理的重要一步；代码与数据已开源以促进复现与研究。

Abstract: While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.

</details>


### [41] [Toward Generalizable Deblurring: Leveraging Massive Blur Priors with Linear Attention for Real-World Scenarios](https://arxiv.org/abs/2601.06525)
*Yuanting Gao,Shuo Cao,Xiaohui Li,Yuandong Pu,Yihao Liu,Kai Zhang*

Main category: cs.CV

TL;DR: 提出GLOWDeblur：一种结合“模糊模式预训练(BPP)+运动与语义引导(MoSeG)+轻量扩散骨干+卷积式预重建/域对齐”的通用化去模糊方案，显著提升跨数据集与真实场景泛化。


<details>
  <summary>Details</summary>
Motivation: 现有深度去模糊方法在训练集外普遍失效：数据集在真实度与模糊多样性之间存在权衡；像素损失偏好局部细节忽视结构与语义一致性；扩散方法虽感知质量高，但在狭窄数据与简单训练策略下仍难泛化。

Method: 1) 模糊模式预训练(BPP)：在多样化仿真模糊数据上学习“模糊先验”，再与真实数据联合微调迁移；2) MoSeG：在严重退化下提供运动与语义引导以强化先验；3) GLOWDeblur架构：卷积式预重建与域对齐模块+轻量扩散骨干，二者协同实现结构一致与细节恢复。

Result: 在6个常用基准与2个真实数据集上取得优于现有方法的泛化与感知质量；证明模糊先验的关键作用，同时维持轻量级、实用的推理成本。

Conclusion: 决定泛化能力的核心是“模糊模式多样性”；通过BPP+MoSeG并在GLOWDeblur中集成，可在保持轻量的同时显著提升真实场景鲁棒去模糊性能。

Abstract: Image deblurring has advanced rapidly with deep learning, yet most methods exhibit poor generalization beyond their training datasets, with performance dropping significantly in real-world scenarios. Our analysis shows this limitation stems from two factors: datasets face an inherent trade-off between realism and coverage of diverse blur patterns, and algorithmic designs remain restrictive, as pixel-wise losses drive models toward local detail recovery while overlooking structural and semantic consistency, whereas diffusion-based approaches, though perceptually strong, still fail to generalize when trained on narrow datasets with simplistic strategies. Through systematic investigation, we identify blur pattern diversity as the decisive factor for robust generalization and propose Blur Pattern Pretraining (BPP), which acquires blur priors from simulation datasets and transfers them through joint fine-tuning on real data. We further introduce Motion and Semantic Guidance (MoSeG) to strengthen blur priors under severe degradation, and integrate it into GLOWDeblur, a Generalizable reaL-wOrld lightWeight Deblur model that combines convolution-based pre-reconstruction & domain alignment module with a lightweight diffusion backbone. Extensive experiments on six widely-used benchmarks and two real-world datasets validate our approach, confirming the importance of blur priors for robust generalization and demonstrating that the lightweight design of GLOWDeblur ensures practicality in real-world applications. The project page is available at https://vegdog007.github.io/GLOWDeblur_Website/.

</details>


### [42] [Towards Egocentric 3D Hand Pose Estimation in Unseen Domains](https://arxiv.org/abs/2601.06537)
*Wiktor Mucha,Michael Wray,Martin Kampel*

Main category: cs.CV

TL;DR: V-HPOT通过相机无关的深度表示与测试时自监督优化，显著提升跨域自我中心视角3D手部关键点估计的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在同域表现良好，但在跨域（不同相机内参、环境、数据分布）时由于训练数据有限与深度估计依赖相机内参而泛化差，容易过拟合特定相机；需要一种能在未知域保持稳定深度感知的方案。

Method: 1) 在“虚拟相机空间”预测关键点的z坐标，并以焦距与图像尺寸进行归一化，使深度预测对相机内参不敏感，实现相机无关表示。2) 基于该不变性设计测试时自监督优化（TTO）：在推理阶段对模型进行微调，利用预测手势与其在空间中尺度变换后的姿态之间的3D一致性损失，无需标注即可适配目标域的深度特性。

Result: 在跨域评测中显著降低误差：H2O数据集平均姿态误差降低71%，AssemblyHands降低41%；与SOTA对比，单阶段方法全面领先，并与两阶段方法接近，同时所需训练数据仅为其约1/3.5到1/14。

Conclusion: 通过相机无关深度表示与测试时自监督一致性优化，V-HPOT有效缓解跨域泛化难题，在低数据设置下仍能达到或接近SOTA性能，适合真实场景中多相机、未知域的手部姿态估计。

Abstract: We present V-HPOT, a novel approach for improving the cross-domain performance of 3D hand pose estimation from egocentric images across diverse, unseen domains. State-of-the-art methods demonstrate strong performance when trained and tested within the same domain. However, they struggle to generalise to new environments due to limited training data and depth perception -- overfitting to specific camera intrinsics. Our method addresses this by estimating keypoint z-coordinates in a virtual camera space, normalised by focal length and image size, enabling camera-agnostic depth prediction. We further leverage this invariance to camera intrinsics to propose a self-supervised test-time optimisation strategy that refines the model's depth perception during inference. This is achieved by applying a 3D consistency loss between predicted and in-space scale-transformed hand poses, allowing the model to adapt to target domain characteristics without requiring ground truth annotations. V-HPOT significantly improves 3D hand pose estimation performance in cross-domain scenarios, achieving a 71% reduction in mean pose error on the H2O dataset and a 41% reduction on the AssemblyHands dataset. Compared to state-of-the-art methods, V-HPOT outperforms all single-stage approaches across all datasets and competes closely with two-stage methods, despite needing approximately x3.5 to x14 less data.

</details>


### [43] [LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models](https://arxiv.org/abs/2601.06550)
*Pan Liao,Feng Yang,Di Wu,Jinwen Yu,Yuhua Zhu,Wenhui Zhao*

Main category: cs.CV

TL;DR: 提出LLMTrack，将传统MOT拓展为语义多目标跟踪（SMOT），以“眼睛+大脑”架构融合定位与语义理解，结合时空融合与三阶段训练，在BenSMOT上实现SOTA，兼顾稳定跟踪与语义任务（描述、交互、摘要）。


<details>
  <summary>Details</summary>
Motivation: 传统MOT只回答“在哪里、是谁”，缺乏对目标行为与意图的“是什么、为什么”的语义理解，难以进行交互识别与视频级叙事；需要把几何跟踪与认知推理打通，构建能理解轨迹语义与上下文的系统。

Method: 采用“仿生”解耦设计：Grounding DINO负责强定位与检测作为“眼睛”，LLaVA-OneVision 作为“脑”。提出时空融合模块（Spatio-Temporal Fusion）汇聚实例交互特征与视频级上下文，使LLM理解复杂轨迹。训练上采用三级渐进策略：1) 视觉对齐（对齐视觉编码与LLM），2) 时间微调（引入时序建模能力），3) 通过LoRA进行语义注入（高效适配追踪领域）。端到端SMOT框架。

Result: 在BenSMOT基准上取得SOTA；在实例描述、交互识别、视频摘要等语义任务显著优于现有方法，同时保持稳健的跟踪稳定性。

Conclusion: LLMTrack有效弥合了几何感知与语义推理的鸿沟，通过“定位—理解”解耦、时空融合与渐进式训练，实现了端到端的语义多目标跟踪，在多项语义指标与稳定性上领先。

Abstract: Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.

</details>


### [44] [ArrowGEV: Grounding Events in Video via Learning the Arrow of Time](https://arxiv.org/abs/2601.06559)
*Fangxu Yu,Ziyao Lu,Liqiang Niu,Fandong Meng,Jie Zhou*

Main category: cs.CV

TL;DR: ArrowGEV通过引入时间箭头理念，用强化学习让VLM同时学习正放/倒放视频的事件锚定与方向性判别，显著提升时间敏感事件的区分与时间不敏感事件的一致对齐，从而增强事件定位、方向识别与泛化理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频事件定位多只在正向视频上训练，忽视事件的内在时间结构与方向性，导致模型在时序扰动、倒放视频或跨域场景下鲁棒性与泛化能力不足。需要一种显式建模时间方向性的训练范式。

Method: 提出ArrowGEV：基于强化学习的训练框架。将事件划分为时间敏感与时间不敏感两类；为敏感事件设计奖励以鼓励模型区分正放与倒放视频；为不敏感事件约束正反方向锚定一致。通过策略优化使VLM学会时间方向性与稳健的事件对齐。

Result: 在多项实验中，模型不仅在事件定位精度和时间方向识别上优于基线，还带来一般性视频理解与推理能力的提升，显示更强的鲁棒性与泛化。

Conclusion: 显式引入时间箭头并结合强化学习，可有效补足VLM对时间结构与方向性的建模缺陷，统一提升事件定位、方向判别与通用视频理解表现。

Abstract: Grounding events in videos serves as a fundamental capability in video analysis. While Vision-Language Models (VLMs) are increasingly employed for this task, existing approaches predominantly train models to associate events with timestamps in the forward video only. This paradigm hinders VLMs from capturing the inherent temporal structure and directionality of events, thereby limiting robustness and generalization. To address this limitation, inspired by the arrow of time in physics, which characterizes the intrinsic directionality of temporal processes, we propose ArrowGEV, a reinforcement learning framework that explicitly models temporal directionality in events to improve both event grounding and temporal directionality understanding in VLMs. Specifically, we categorize events into time-sensitive (e.g., putting down a bag) and time-insensitive (e.g., holding a towel in the left hand). The former denote events whose reversal substantially alters their meaning, while the latter remain semantically unchanged under reversal. For time-sensitive events, ArrowGEV introduces a reward that encourages VLMs to discriminate between forward and backward videos, whereas for time-insensitive events, it enforces consistent grounding across both directions. Extensive experiments demonstrate that ArrowGEV not only improves grounding precision and temporal directionality recognition, but also enhances general video understanding and reasoning ability.

</details>


### [45] [QCaption: Video Captioning and Q&A through Fusion of Large Multimodal Models](https://arxiv.org/abs/2601.06566)
*Jiale Wang,Gee Wah Ng,Lee Onn Mak,Randall Cher,Ng Ding Hei Ryan,Davis Wang*

Main category: cs.CV

TL;DR: QCaption通过融合关键帧提取、图像-文本LMM与文本LLM，构建端到端本地可部署的视频描述与问答流水线，在基准上显著提升性能（描述+44.2%，问答+48.9%），并以消融与多方法对比验证了融合策略的价值。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述与视频问答方法往往单一模态或依赖云端，难以在本地环境实现对文本、图像与视频的统一分析，性能与可用性受限。作者希望在保证本地自洽部署的前提下，提升视频理解的准确性与鲁棒性。

Method: 提出QCaption三阶段流水线：1）关键帧提取以压缩冗余并保留语义；2）利用大型多模态模型对关键帧进行图像-文本理解与描述；3）使用大型语言模型对来自多帧的文本证据进行融合、推理与问答。并进行消融实验以考察LLM在融合中的贡献；同时提出若干备选的视频描述方案并进行对比。

Result: 在视频描述与视频问答任务上分别获得最高达44.2%与48.9%的性能提升；消融结果显示LLM在跨帧信息融合与推理中起关键作用。

Conclusion: 多模型融合（关键帧+LMM+LLM）是推进视频分析的有效路径，QCaption在保证本地化部署的同时显著超越现有方法，验证了融合策略的优势与可扩展性。

Abstract: This paper introduces QCaption, a novel video captioning and Q&A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.

</details>


### [46] [APEX: Learning Adaptive Priorities for Multi-Objective Alignment in Vision-Language Generation](https://arxiv.org/abs/2601.06574)
*Dongliang Chen,Xinlin Zhuang,Junjie Xu,Luojian Xie,Zehui Wang,Jiaxi Zhuang,Haolin Yang,Liang Dou,Xiao He,Xingjiao Wu,Ying Qian*

Main category: cs.CV

TL;DR: 该论文提出APEX用于文本到图像生成的多目标对齐，解决静态线性加权在异质奖励下的失衡问题，通过双阶段自适应归一化与P^3自适应优先级调度来缓解“方差劫持”和梯度冲突，在SD3.5上取得更均衡的帕累托改进（PickScore、DeQA、Aesthetics提升且OCR保持）。


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐常用固定权重线性标量化，但当不同目标的奖励方差与响应度差异大时，会导致训练信号被高方差目标主导（如OCR），从而牺牲感知质量等目标；同时，不同目标的梯度方向冲突带来优化振荡。需要一种既稳定各异质奖励尺度，又能动态分配训练关注度的方法。

Method: 提出APEX，包括两大核心：1）Dual-Stage Adaptive Normalization（双阶段自适应归一化），在奖励层面与训练信号层面分别进行稳定化，抑制由方差引起的隐式重加权（方差劫持）。2）P^3 Adaptive Priorities（学习潜力Potential、冲突惩罚Penalty、进展需求Progress三因子）动态调度各目标的优先级，综合考虑剩余可学收益、与其他目标的梯度冲突程度以及当前进展不足，进而在训练过程中自适应调整各目标的权重与采样/更新频率。

Result: 在Stable Diffusion 3.5上，针对包含OCR、感知质量（PickScore、DeQA）、美学（Aesthetics）等四类异质目标，APEX实现更好的帕累托折中：PickScore +1.31、DeQA +0.35、Aesthetics +0.53，同时保持有竞争力的OCR准确率，并显著减轻训练不稳定与目标之间的摇摆。

Conclusion: 静态线性加权在异质奖励环境下易出现优化失衡与震荡。APEX通过自适应归一化与基于潜力-惩罚-进展的优先级调度，有效缓解“方差劫持”和梯度冲突，实现多目标对齐的稳定、均衡提升，并在实际大模型（SD3.5）上验证了更优的帕累托表现。

Abstract: Multi-objective alignment for text-to-image generation is commonly implemented via static linear scalarization, but fixed weights often fail under heterogeneous rewards, leading to optimization imbalance where models overfit high-variance, high-responsiveness objectives (e.g., OCR) while under-optimizing perceptual goals. We identify two mechanistic causes: variance hijacking, where reward dispersion induces implicit reweighting that dominates the normalized training signal, and gradient conflicts, where competing objectives produce opposing update directions and trigger seesaw-like oscillations. We propose APEX (Adaptive Priority-based Efficient X-objective Alignment), which stabilizes heterogeneous rewards with Dual-Stage Adaptive Normalization and dynamically schedules objectives via P^3 Adaptive Priorities that combine learning potential, conflict penalty, and progress need. On Stable Diffusion 3.5, APEX achieves improved Pareto trade-offs across four heterogeneous objectives, with balanced gains of +1.31 PickScore, +0.35 DeQA, and +0.53 Aesthetics while maintaining competitive OCR accuracy, mitigating the instability of multi-objective alignment.

</details>


### [47] [Sissi: Zero-shot Style-guided Image Synthesis via Semantic-style Integration](https://arxiv.org/abs/2601.06605)
*Yingying Deng,Xiangyu He,Fan Tang,Weiming Dong,Xucheng Yin*

Main category: cs.CV

TL;DR: 提出一个无需训练的风格引导图像生成框架，将风格迁移视为“上下文学习”问题：把风格参考图与带掩膜的目标图拼接输入到预训练的ReFlow修复模型，通过多模态注意力融合实现语义与风格的无缝整合；并引入动态语义-风格集成(DSSI)重加权注意力，解决语义与风格冲突，达到更高的风格保真与语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有风格化方法常需重训练或代价高的反演，易牺牲内容与风格保真，且难在语义遵循与风格一致间取得平衡；需要一种简单、无训练、质量高且能精确风格化的方案。

Method: 将风格引导合成重塑为一种“in-context learning”：把参考风格图与带掩膜的目标图级联输入预训练的ReFlow-based图像修复模型，利用多模态注意力融合语义提示与风格视觉特征；分析多模态注意失衡与噪声敏感问题，提出DSSI机制对文本语义token与风格视觉token的注意权重进行动态重分配，缓解引导冲突并提升一致性。

Result: 在实验中实现高保真的风格化效果，较以往方法在语义-风格平衡、视觉质量与稳定性上更优，减少伪影与失真。

Conclusion: 该无训练框架与DSSI机制提供了简洁而有效的风格化途径，可在保持语义指令的同时提升风格保真与整体质量，作为复杂且易出伪影方法的有力替代。

Abstract: Text-guided image generation has advanced rapidly with large-scale diffusion models, yet achieving precise stylization with visual exemplars remains difficult. Existing approaches often depend on task-specific retraining or expensive inversion procedures, which can compromise content integrity, reduce style fidelity, and lead to an unsatisfactory trade-off between semantic prompt adherence and style alignment. In this work, we introduce a training-free framework that reformulates style-guided synthesis as an in-context learning task. Guided by textual semantic prompts, our method concatenates a reference style image with a masked target image, leveraging a pretrained ReFlow-based inpainting model to seamlessly integrate semantic content with the desired style through multimodal attention fusion. We further analyze the imbalance and noise sensitivity inherent in multimodal attention fusion and propose a Dynamic Semantic-Style Integration (DSSI) mechanism that reweights attention between textual semantic and style visual tokens, effectively resolving guidance conflicts and enhancing output coherence. Experiments show that our approach achieves high-fidelity stylization with superior semantic-style balance and visual quality, offering a simple yet powerful alternative to complex, artifact-prone prior methods.

</details>


### [48] [Boosting Overlapping Organoid Instance Segmentation Using Pseudo-Label Unmixing and Synthesis-Assisted Learning](https://arxiv.org/abs/2601.06642)
*Gui Huang,Kangyuan Zheng,Xuan Cai,Jiaqi Wang,Jianjia Zhang,Kaida Ning,Wenbo Wei,Yujuan Zhu,Jiong Zhang,Mengting Liu*

Main category: cs.CV

TL;DR: 提出一种用于类器官实例分割的半监督框架：在合成辅助SSL基础上，引入伪标签解混(PLU)、基于轮廓的实例合成与实例级增强，专门处理重叠器官体导致的伪标签偏差，10%标注即可接近全监督并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 类器官用于药效评估与精准医疗，需要精确的实例分割以量化动态行为。但显微图像中重叠严重且高质量标注稀缺，传统SSL易受重叠区域伪标签噪声影响，导致训练偏差与粘连实例无法正确分离。已有SA-SSL在语义分割中缓解偏差，但直接用于实例分割仍难以解缠重叠实例。

Method: 将SA-SSL首次适配到类器官实例分割，并提出伪标签解混(PLU)：识别重叠处错误伪标签并进行实例分解以再生成标签；设计基于轮廓的高效实例合成策略，特别针对重叠情况；在合成前对伪标签进行实例级增强(IA)，并采用增强感知训练以提升合成数据效力。整体以少量标注+大量未标注图像，通过伪标签、合成数据与一致性学习联合训练。

Result: 在两个类器官数据集上，使用仅10%标注即可达到接近全监督模型的性能，并取得SOTA。消融实验证实PLU、轮廓合成与增强感知训练均有显著贡献。

Conclusion: 通过在伪标签层面与合成层面同时解决重叠问题，所提方法显著降低对昂贵标注的依赖，提升类器官实例分割的可扩展性与鲁棒性，有望用于高通量精准医疗应用。

Abstract: Organoids, sophisticated in vitro models of human tissues, are crucial for medical research due to their ability to simulate organ functions and assess drug responses accurately. Accurate organoid instance segmentation is critical for quantifying their dynamic behaviors, yet remains profoundly limited by high-quality annotated datasets and pervasive overlap in microscopy imaging. While semi-supervised learning (SSL) offers a solution to alleviate reliance on scarce labeled data, conventional SSL frameworks suffer from biases induced by noisy pseudo-labels, particularly in overlapping regions. Synthesis-assisted SSL (SA-SSL) has been proposed for mitigating training biases in semi-supervised semantic segmentation. We present the first adaptation of SA-SSL to organoid instance segmentation and reveal that SA-SSL struggles to disentangle intertwined organoids, often misrepresenting overlapping instances as a single entity. To overcome this, we propose Pseudo-Label Unmixing (PLU), which identifies erroneous pseudo-labels for overlapping instances and then regenerates organoid labels through instance decomposition. For image synthesis, we apply a contour-based approach to synthesize organoid instances efficiently, particularly for overlapping cases. Instance-level augmentations (IA) on pseudo-labels before image synthesis further enhances the effect of synthetic data (SD). Rigorous experiments on two organoid datasets demonstrate our method's effectiveness, achieving performance comparable to fully supervised models using only 10% labeled data, and state-of-the-art results. Ablation studies validate the contributions of PLU, contour-based synthesis, and augmentation-aware training. By addressing overlap at both pseudo-label and synthesis levels, our work advances scalable, label-efficient organoid analysis, unlocking new potential for high-throughput applications in precision medicine.

</details>


### [49] [eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers](https://arxiv.org/abs/2601.06647)
*Krishna Vinod,Joseph Raj Vishal,Kaustav Chanda,Prithvi Jai Ramesh,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 提出eSkiTB：首个冬季运动事件相机滑雪跟踪基准；在含广播遮挡/叠层的复杂场景中，事件跟踪优于RGB（IoU提升约+20点），显示时间对比线索对高速“弹道式”运动跟踪有效。


<details>
  <summary>Details</summary>
Motivation: RGB转播画面中滑雪运动员高速运动常被运动模糊、台标/计时条等静态叠层以及杂乱背景干扰，影响目标跟踪稳定性；缺少一个可控且可公平对比RGB与事件相机模态的冬季项目基准，限制了方法评估与发展。

Method: 从现有SkiTB视频通过直接视频到事件（非神经插值）转换，构建合成事件数据集eSkiTB，实现与原RGB数据的信息等价对比；在该基准上对比事件域的SDTrack（脉冲Transformer）与RGB域的STARK（Transformer）跟踪器，并在含静态叠层/杂乱的多场景下评测IoU等指标。

Result: 在静态叠层主导的广播干扰场景中，事件基方法显著更稳健：SDTrack取得0.685 IoU，较RGB基准提升约+20.0点；在全数据集上SDTrack平均IoU=0.711，表明时间对比信号在视觉拥挤与高速运动下提供强健跟踪线索。

Conclusion: eSkiTB为冬季运动事件相机跟踪提供首个可控评测环境，验证事件相机在滑雪转播场景中的优势与潜力；数据集与代码将开源，鼓励进一步研究事件驱动的运动目标跟踪。

Abstract: Tracking skiers in RGB broadcast footage is challenging due to motion blur, static overlays, and clutter that obscure the fast-moving athlete. Event cameras, with their asynchronous contrast sensing, offer natural robustness to such artifacts, yet a controlled benchmark for winter-sport tracking has been missing. We introduce event SkiTB (eSkiTB), a synthetic event-based ski tracking dataset generated from SkiTB using direct video-to-event conversion without neural interpolation, enabling an iso-informational comparison between RGB and event modalities. Benchmarking SDTrack (spiking transformer) against STARK (RGB transformer), we find that event-based tracking is substantially resilient to broadcast clutter in scenes dominated by static overlays, achieving 0.685 IoU, outperforming RGB by +20.0 points. Across the dataset, SDTrack attains a mean IoU of 0.711, demonstrating that temporal contrast is a reliable cue for tracking ballistic motion in visually congested environments. eSkiTB establishes the first controlled setting for event-based tracking in winter sports and highlights the promise of event cameras for ski tracking. The dataset and code will be released at https://github.com/eventbasedvision/eSkiTB.

</details>


### [50] [Quantification and Classification of Carbon Nanotubes in Electron Micrographs using Vision Foundation Models](https://arxiv.org/abs/2601.06673)
*Sanjay Pradeep,Chen Wang,Matthew M. Dahm,Jeff D. Eldredge,Candace S. J. Tsai*

Main category: cs.CV

TL;DR: 利用视觉基础模型自动化CNT在电子显微图中的分割与形貌分类：SAM交互分割+掩膜约束的DINOv2特征提取，在1,800张TEM上四类形貌达95.5%准确，能在混合样本中做实例级识别，显著优于基线并减少标注/训练需求。


<details>
  <summary>Details</summary>
Motivation: 手工分割CNT既慢又主观，限制了暴露评估与毒理研究的可重复、高通量分析；需要一个可扩展、鲁棒且减少人工依赖的自动化流程。

Method: 两部分：1) 基于SAM的交互式量化工具，通过少量用户提示实现近乎完美的颗粒分割；2) 利用分割掩膜对DINOv2视觉Transformer进行空间约束，仅从颗粒区域提取自监督特征、抑制背景噪声，实现形貌分类与实例级处理。

Result: 在1,800张TEM图像上对四种CNT形貌分类达到95.5%准确度，明显优于当前基线且使用更少训练数据；支持在同一视野内对多类颗粒的正确分类。

Conclusion: 零样本分割与自监督特征学习的结合可实现高通量、可重复的纳米材料图像分析，将以往劳动密集的瓶颈转变为可扩展的数据驱动流程。

Abstract: Accurate characterization of carbon nanotube morphologies in electron microscopy images is vital for exposure assessment and toxicological studies, yet current workflows rely on slow, subjective manual segmentation. This work presents a unified framework leveraging vision foundation models to automate the quantification and classification of CNTs in electron microscopy images. First, we introduce an interactive quantification tool built on the Segment Anything Model (SAM) that segments particles with near-perfect accuracy using minimal user input. Second, we propose a novel classification pipeline that utilizes these segmentation masks to spatially constrain a DINOv2 vision transformer, extracting features exclusively from particle regions while suppressing background noise. Evaluated on a dataset of 1,800 TEM images, this architecture achieves 95.5% accuracy in distinguishing between four different CNT morphologies, significantly outperforming the current baseline despite using a fraction of the training data. Crucially, this instance-level processing allows the framework to resolve mixed samples, correctly classifying distinct particle types co-existing within a single field of view. These results demonstrate that integrating zero-shot segmentation with self-supervised feature learning enables high-throughput, reproducible nanomaterial analysis, transforming a labor-intensive bottleneck into a scalable, data-driven process.

</details>


### [51] [When Humans Judge Irises: Pupil Size Normalization as an Aid and Synthetic Irises as a Challenge](https://arxiv.org/abs/2601.06725)
*Mahsa Mitcheff,Adam Czajka*

Main category: cs.CV

TL;DR: 研究评估人在虹膜核验中的表现：瞳孔大小对齐显著提升人类判别准确；对真/伪（合成）图像，人类在真-真、伪-伪配对上表现良好，但在真 vs 高质量同眼合成时准确率下降。


<details>
  <summary>Details</summary>
Motivation: 法证场景中需人类专家复核虹膜比对结果，尤其样本退化或需鉴别呈现攻击时；同时，生成式模型可造高保真合成虹膜，需评估人在此背景下的核验能力与影响因素（如瞳孔大小变化）。

Method: 两类受控实验：1) 比较不同瞳孔大小下的人类核验表现，并测试线性/非线性瞳孔对齐（基于自编码器的身份保持型图像到图像翻译模型）对准确率的影响；2) 使用合成生成的真（同眼）与伪（异眼）虹膜对，评估人在真-真、伪-伪以及真 vs 同眼合成的判别能力。提供数据与人类评判以便复现。

Result: 瞳孔大小归一化显著提高人类核验准确率；在人类判断中，若配对均为真实或均为合成，能较好分辨同眼/异眼；当比较真实与高质量同眼合成时，准确率下降，人类更倾向将同眼合成对判为异眼。

Conclusion: (a) 涉及人类参与的虹膜匹配任务中，瞳孔大小对齐至关重要；(b) 尽管生成模型能产生高保真同眼合成虹膜，人类更常将其判为不同眼，显示人类对同眼合成的偏差与脆弱性。

Abstract: Iris recognition is a mature biometric technology offering remarkable precision and speed, and allowing for large-scale deployments to populations exceeding a billion enrolled users (e.g., AADHAAR in India). However, in forensic applications, a human expert may be needed to review and confirm a positive identification before an iris matching result can be presented as evidence in court, especially in cases where processed samples are degraded (e.g., in post-mortem cases) or where there is a need to judge whether the sample is authentic, rather than a result of a presentation attack.
  This paper presents a study that examines human performance in iris verification in two controlled scenarios: (a) under varying pupil sizes, with and without a linear/nonlinear alignment of the pupil size between compared images, and (b) when both genuine and impostor iris image pairs are synthetically generated. The results demonstrate that pupil size normalization carried out by a modern autoencoder-based identity-preserving image-to-image translation model significantly improves verification accuracy. Participants were also able to determine whether iris pairs corresponded to the same or different eyes when both images were either authentic or synthetic. However, accuracy declined when subjects were comparing authentic irises against high-quality, same-eye synthetic counterparts. These findings (a) demonstrate the importance of pupil-size alignment for iris matching tasks in which humans are involved, and (b) indicate that despite the high fidelity of modern generative models, same-eye synthetic iris images are more often judged by humans as different-eye images, compared to same-eye authentic image pairs.
  We offer data and human judgments along with this paper to allow full replicability of this study and future works.

</details>


### [52] [Benchmarking Egocentric Clinical Intent Understanding Capability for Medical Multimodal Large Language Models](https://arxiv.org/abs/2601.06750)
*Shaonan Liu,Guo Yu,Xiaoling Luo,Shiyi Zheng,Wenting Chen,Jie Liu,Linlin Shen*

Main category: cs.CV

TL;DR: 提出MedGaze-Bench，用临床医师凝视作为“认知光标”评估Med-MLLM在手术、急救仿真与诊断任务中的临床意图理解；并以空间/时间/规范三维框架与Trap QA机制系统考察可靠性，发现现有模型过度依赖全局特征，易幻觉与迎合错误指令。


<details>
  <summary>Details</summary>
Motivation: 现实临床需要模型理解操作者当下“意图”，但现有基准缺乏对自我视角（egocentric）意图的系统评测，且医学场景存在解剖视觉同质、流程强因果与隐性安全规范等难点，促使作者构建能映射注意与意图的评测方法。

Method: 引入MedGaze-Bench：用临床凝视轨迹作为“认知光标”；设计三维临床意图框架——空间意图（在视觉噪声中定位精准目标）、时间意图（回顾与前瞻因果推理）、规范意图（核查流程与安全规范遵循）；超越简单准确率，加入Trap QA以惩罚幻觉与“认知阿谀”；在手术、急救与诊断多场景上评测现有MLLM。

Result: 实验显示当前MLLM对自我视角意图理解薄弱，过分依赖全局特征，出现编造性观察与对无效指令的不加批判接受，整体临床可靠性受限。

Conclusion: MedGaze-Bench填补了医学多模态模型在临床意图理解评测的空白；三维框架与Trap QA能更全面衡量可靠性，强调未来模型需增强对局部关注、因果时间推理与安全规范的内化。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) require egocentric clinical intent understanding for real-world deployment, yet existing benchmarks fail to evaluate this critical capability. To address these challenges, we introduce MedGaze-Bench, the first benchmark leveraging clinician gaze as a Cognitive Cursor to assess intent understanding across surgery, emergency simulation, and diagnostic interpretation. Our benchmark addresses three fundamental challenges: visual homogeneity of anatomical structures, strict temporal-causal dependencies in clinical workflows, and implicit adherence to safety protocols. We propose a Three-Dimensional Clinical Intent Framework evaluating: (1) Spatial Intent: discriminating precise targets amid visual noise, (2) Temporal Intent: inferring causal rationale through retrospective and prospective reasoning, and (3) Standard Intent: verifying protocol compliance through safety checks. Beyond accuracy metrics, we introduce Trap QA mechanisms to stress-test clinical reliability by penalizing hallucinations and cognitive sycophancy. Experiments reveal current MLLMs struggle with egocentric intent due to over-reliance on global features, leading to fabricated observations and uncritical acceptance of invalid instructions.

</details>


### [53] [The Normalized Difference Layer: A Differentiable Spectral Index Formulation for Deep Learning](https://arxiv.org/abs/2601.06777)
*Ali Lotfi,Adam Carter,Mohammad Meysami,Thuan Ha,Kwabena Nketia,Steve Shirtliffe*

Main category: cs.CV

TL;DR: 提出可学习的“归一化差分层”，在深度网络中端到端学习波段权重，保留NDI的稳健性与有界性，并以更少参数达到与MLP相当或更优的性能，且对乘性噪声更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统归一化差分指数（NDI）在遥感中稳健、照度不变、输出有界，但通常作为固定预处理，系数默认为1，难以针对具体任务自适应优化。需要一种既保留NDI优点又能通过学习适配任务的方式。

Method: 提出“Normalized Difference Layer（NDL）”，作为可微分的网络模块：对各光谱通道引入可学习的正系数（通过softplus重参数化保证正值并避免分母接近0），输出形式保持归一化差分并保证[-1,1]有界；给出前向与反向传播的数学框架，实现端到端训练；扩展至可处理有符号输入，便于层级堆叠。

Result: 在分类任务中，含NDL的模型以约75%更少参数达到与标准MLP相当的准确率；在10%乘性噪声下，准确率仅下降0.17%，显著优于MLP的3.03%；学习到的波段系数模式随网络深度保持一致性。

Conclusion: NDL在保持NDI照度不变与输出有界等优势的同时，能通过梯度下降学习任务特定权重，提升参数效率与噪声鲁棒性，适合集成到更深的端到端遥感深度学习架构中。

Abstract: Normalized difference indices have been a staple in remote sensing for decades. They stay reliable under lighting changes produce bounded values and connect well to biophysical signals. Even so, they are usually treated as a fixed pre processing step with coefficients set to one, which limits how well they can adapt to a specific learning task. In this study, we introduce the Normalized Difference Layer that is a differentiable neural network module. The proposed method keeps the classical idea but learns the band coefficients from data. We present a complete mathematical framework for integrating this layer into deep learning architectures that uses softplus reparameterization to ensure positive coefficients and bounded denominators. We describe forward and backward pass algorithms enabling end to end training through backpropagation. This approach preserves the key benefits of normalized differences, namely illumination invariance and outputs bounded to $[-1,1]$ while allowing gradient descent to discover task specific band weightings. We extend the method to work with signed inputs, so the layer can be stacked inside larger architectures. Experiments show that models using this layer reach similar classification accuracy to standard multilayer perceptrons while using about 75\% fewer parameters. They also handle multiplicative noise well, at 10\% noise accuracy drops only 0.17\% versus 3.03\% for baseline MLPs. The learned coefficient patterns stay consistent across different depths.

</details>


### [54] [CliffordNet: All You Need is Geometric Algebra](https://arxiv.org/abs/2601.06793)
*Zhongping Ji*

Main category: cs.CV

TL;DR: 提出Clifford Algebra Network（CliffordNet），用几何代数的几何乘积统一完成空间/通道混合与记忆，不再需要FFN，并以线性复杂度实现，在CIFAR-100上以极小参数达到或超越同级SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉骨干（CNN/Transformer）依赖手工堆叠的空间混合（卷积/注意力）与通道混合（FFN），缺乏从一阶原理推导的统一交互；作者希望以严格的代数结构取代启发式模块，提升表达密度与效率。

Method: 以Clifford几何乘积uv = u·v + u∧v为核心交互，内积刻画特征一致性，外积刻画结构变化；通过稀疏rolling机制实现严格O(N)复杂度；由几何交互的高表达性，移除传统FFN；形成从局部到全局的统一几何交互网络骨干。

Result: 在CIFAR-100上，Nano版1.4M参数达76.41%准确率，媲美11.2M参数的ResNet-18（约8倍更小）；Base版达78.05%，刷新小模型SOTA；建立新的精度-参数帕累托前沿。

Conclusion: 以几何代数为核心的统一交互可在仅局部操作下涌现全局理解，提示“几何即一切”的可能；FFN在此框架下可冗余，带来高效且表达密集的视觉骨干。

Abstract: Modern computer vision architectures, from CNNs to Transformers, predominantly rely on the stacking of heuristic modules: spatial mixers (Attention/Conv) followed by channel mixers (FFNs). In this work, we challenge this paradigm by returning to mathematical first principles. We propose the \textbf{Clifford Algebra Network (CAN)}, also referred to as CliffordNet, a vision backbone grounded purely in Geometric Algebra. Instead of engineering separate modules for mixing and memory, we derive a unified interaction mechanism based on the \textbf{Clifford Geometric Product} ($uv = u \cdot v + u \wedge v$). This operation ensures algebraic completeness regarding the Geometric Product by simultaneously capturing feature coherence (via the generalized inner product) and structural variation (via the exterior wedge product).
  Implemented via an efficient sparse rolling mechanism with \textbf{strict linear complexity $\mathcal{O}(N)$}, our model reveals a surprising emergent property: the geometric interaction is so representationally dense that standard Feed-Forward Networks (FFNs) become redundant. Empirically, CliffordNet establishes a new Pareto frontier: our \textbf{Nano} variant achieves \textbf{76.41\%} accuracy on CIFAR-100 with only \textbf{1.4M} parameters, effectively matching the heavy-weight ResNet-18 (11.2M) with \textbf{$8\times$ fewer parameters}, while our \textbf{Base} variant sets a new SOTA for tiny models at \textbf{78.05\%}. Our results suggest that global understanding can emerge solely from rigorous, algebraically complete local interactions, potentially signaling a shift where \textit{geometry is all you need}. Code is available at https://github.com/ParaMind2025/CAN.

</details>


### [55] [SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2601.06806)
*Jiwen Zhang,Zejun Li,Siyuan Wang,Xiangyu Shi,Zhongyu Wei,Qi Wu*

Main category: cs.CV

TL;DR: 提出SpatialNav：在零样本VLN中先完整探索环境，构建空间场景图（SSG），再利用全局空间语义进行高效导航，显著优于现有零样本方法并逼近有监督SOTA。


<details>
  <summary>Details</summary>
Motivation: 零样本VLN无法从大规模数据中学习隐式空间知识，通常仅依赖局部观察，导致探索效率低、性能落后；需要一种能在不训练的前提下获得全局空间结构与语义的机制，以提升泛化与效率。

Method: 在任务开始前允许代理完整探索环境；基于探索数据构建Spatial Scene Graph（SSG），显式编码全局空间拓扑与对象语义。提出SpatialNav框架：1）以代理为中心的空间地图；2）与罗盘对齐的视觉表征以统一方位；3）远端目标定位策略以在全局层面规划；结合SSG进行路径规划与决策。

Result: 在离散与连续环境的综合实验中，SpatialNav显著优于现有零样本VLN代理，并明显缩小与学习型SOTA方法的性能差距。

Conclusion: 显式的全局空间表示（如SSG）对零样本VLN至关重要：先探索再执行的范式能显著提升导航效率与泛化能力，凸显整合全局拓扑与语义的价值。

Abstract: Although learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce SpatialNav, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.

</details>


### [56] [SARA: Scene-Aware Reconstruction Accelerator](https://arxiv.org/abs/2601.06831)
*Jee Won Lee,Hansol Lim,Minhyeok Im,Dohyeon Lee,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: SARA 是一个几何先导的配对选择模块，用于 SfM，在匹配前依据“重叠×视差”的重建信息量评分来选对，显著减少配对与计算量，同时降低姿态误差并保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统 SfM 多按视觉相似度穷举或宽松筛选图像对，导致匹配代价高、冗余严重、还可能偏好短基线而影响几何稳定性。需要一种在匹配前即依据几何可重建性来挑选信息量高的图像对，以提升效率与精度。

Method: 提出 SARA：先用轻量预匹配（互为最近邻+RANSAC）粗估图像对的重叠与视差，计算“信息量=重叠×视差”；据此构建信息加权生成树（IWST）作为骨干，并加入用于闭环、长基线锚点与弱视角强化的定向边；仅对这些对进行昂贵的特征匹配和估计。

Result: 相较穷举配对，在多种现代学习型特征下，旋转误差降低约46.5±5.5%，平移误差降低约12.5±6.5%；配对数由30,848降至约580（减少98%），整体最多提速约50倍；复杂度由二次降为拟线性。

Conclusion: 几何驱动的信息量评分与IWST主干的配对策略，在大幅减少匹配成本的同时提升/保持位姿精度，并在3D Gaussian Splatting与SVRaster中重建指标仅±3%内波动，证明其可作为高效、稳健的SfM配对加速器。

Abstract: We present SARA (Scene-Aware Reconstruction Accelerator), a geometry-driven pair selection module for Structure-from-Motion (SfM). Unlike conventional pipelines that select pairs based on visual similarity alone, SARA introduces geometry-first pair selection by scoring reconstruction informativeness - the product of overlap and parallax - before expensive matching. A lightweight pre-matching stage uses mutual nearest neighbors and RANSAC to estimate these cues, then constructs an Information-Weighted Spanning Tree (IWST) augmented with targeted edges for loop closure, long-baseline anchors, and weak-view reinforcement. Compared to exhaustive matching, SARA reduces rotation errors by 46.5+-5.5% and translation errors by 12.5+-6.5% across modern learned detectors, while achieving at most 50x speedup through 98% pair reduction (from 30,848 to 580 pairs). This reduces matching complexity from quadratic to quasi-linear, maintaining within +-3% of baseline reconstruction metrics for 3D Gaussian Splatting and SVRaster.

</details>


### [57] [Enhancing Low-resolution Image Representation Through Normalizing Flows](https://arxiv.org/abs/2601.06834)
*Chenglong Bao,Tongyao Pang,Zuowei Shen,Dihan Zheng,Yihang Zou*

Main category: cs.CV

TL;DR: 提出LR2Flow：在小波紧框架域与可逆流模型结合，学习低分辨率图像表征，兼顾低存储与可重建性，在重建、压缩、去噪等任务上有效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 低分辨率表征能降存储与传输并利于多种任务，但难点是既保留关键信息又能高保真重建。现有方法缺少对可逆性与频域结构的统一建模与理论误差分析。

Method: 构建非线性框架LR2Flow：将小波紧框块与normalizing flows耦合，在小波域进行可逆映射以学习低频主导的稀疏表征；并给出重建误差分析，论证在小波紧框域设计可逆网络的必要性。

Result: 在图像重缩放、压缩、去噪等任务上，所学表征实现更低重建误差与更强鲁棒性，展现有效的压缩/传输优势。

Conclusion: 在小波紧框域引入可逆流的LR2Flow能学习保真且紧凑的低分辨率表征，理论分析支持其设计选择，并在多任务上验证其有效与鲁棒。

Abstract: Low-resolution image representation is a special form of sparse representation that retains only low-frequency information while discarding high-frequency components. This property reduces storage and transmission costs and benefits various image processing tasks. However, a key challenge is to preserve essential visual content while maintaining the ability to accurately reconstruct the original images. This work proposes LR2Flow, a nonlinear framework that learns low-resolution image representations by integrating wavelet tight frame blocks with normalizing flows. We conduct a reconstruction error analysis of the proposed network, which demonstrates the necessity of designing invertible neural networks in the wavelet tight frame domain. Experimental results on various tasks, including image rescaling, compression, and denoising, demonstrate the effectiveness of the learned representations and the robustness of the proposed framework.

</details>


### [58] [OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation](https://arxiv.org/abs/2601.06835)
*Hyunseo Lee,Sang Min Kim,Ho Kyung Shin,Taeheon Kim,Woo-Jeoung Nam*

Main category: cs.CV

TL;DR: 提出一种将SAR翻译为光学图像的框架，通过跨模态语义对齐、语义引导生成与不确定性感知目标，缓解斑点噪声与几何畸变导致的误解与伪影，实验上在感知质量与语义一致性优于SOTA。


<details>
  <summary>Details</summary>
Motivation: SAR具全天候优势但含斑点噪声与几何畸变，直接翻译为光学图像易出现语义错判、纹理含糊与结构幻觉；现有方法难以可靠恢复语义与结构。

Method: (i) 跨模态语义对齐：用光学教师网络向SAR学生蒸馏，得到“光学感知”的SAR编码器；(ii) 语义落地的生成引导：设计语义落地的ControlNet，结合类别感知文本提示提供全局上下文，并用分层视觉提示进行局部空间引导；(iii) 不确定性感知损失：显式建模观测的偶然不确定性，动态调节重建关注区域，缓解斑点引起的伪影。

Result: 在广泛实验中，所提方法在感知质量与语义一致性上均超越现有SOTA方法。

Conclusion: 通过语义对齐、语义引导与不确定性建模的联合，显著提升SAR到光学的翻译稳定性与真实性，减少噪声与畸变导致的伪影与歧义。

Abstract: Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.

</details>


### [59] [PRISM: Color-Stratified Point Cloud Sampling](https://arxiv.org/abs/2601.06839)
*Hansol Lim,Minhyeok Im,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: PRISM：用颜色引导的分层采样，对RGB-LiDAR点云按色彩多样性分配采样密度，在纹理丰富区域多采，在色彩同质区域少采，从而以更稀疏点云保留重建关键特征。


<details>
  <summary>Details</summary>
Motivation: 传统下采样仅追求空间均匀（随机、体素网格、法向空间采样），忽视影像的光度/颜色信息；而场景中区分度高的结构常伴随色彩多样性，重复冗余区域通常颜色同质。需要一种能利用颜色表征视觉复杂度、在保证重建关键性的同时显著降采的策略。

Method: 以RGB颜色空间为分层域，将点按颜色划分至色彩bins，并为每个bin设最大容量k。采样密度与色彩多样性正相关：颜色变化大的区域得到更多样本，颜色同质区域被强力下采。相较空间均匀采样，核心转变为“按视觉复杂度分配”而非“按空间覆盖分配”。

Result: 得到更稀疏但信息保真的点云：纹理/颜色丰富处细节被保留，视觉同质平面被大幅压缩，对3D重建等任务保留了关键几何与外观特征。

Conclusion: 颜色分层与容量约束实现按视觉复杂度的自适应采样，相比传统方法在相同点数下更好保留关键特征，适合需要高效、保真点云输入的3D重建流程。

Abstract: We present PRISM, a novel color-guided stratified sampling method for RGB-LiDAR point clouds. Our approach is motivated by the observation that unique scene features often exhibit chromatic diversity while repetitive, redundant features are homogeneous in color. Conventional downsampling methods (Random Sampling, Voxel Grid, Normal Space Sampling) enforce spatial uniformity while ignoring this photometric content. In contrast, PRISM allocates sampling density proportional to chormatic diversity. By treating RGB color space as the stratification domain and imposing a maximum capacity k per color bin, the method preserves texture-rich regions with high color variation while substantially reducing visually homogeneous surfaces. This shifts the sampling space from spatial coverage to visual complexity to produce sparser point clouds that retain essential features for 3D reconstruction tasks.

</details>


### [60] [Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models](https://arxiv.org/abs/2601.06843)
*Junyan Lin,Junlong Tong,Hao Wu,Jialiang Zhang,Jinming Liu,Xin Jin,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 提出一种并行流式框架，打破传统位置编码带来的感知-生成耦合，实现“边看边说”的实时多模态理解，在保持准确流畅的同时显著降时延（最高2倍加速），其中Group-Decoupled方案效率与性能最佳。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多为离线推理或串行的流式感知-生成流程，需完整输入或严格交替，导致高时延、弱并行，尤其在实时视频理解中，标准位置编码的全局连续性约束把感知与生成紧密绑死，难以同时进行。

Method: 提出并行流式框架，通过放宽位置连续性约束的三种设计：1) Overlapped：在时间轴上重叠感知与生成窗口；2) Group-Decoupled：将序列分组并在组间解耦位置，使感知与生成并行；3) Gap-Isolated：在片段间插入隔离空隙打破连续依赖。三者均允许输入处理与输出生成同步推进。

Result: 大量实验表明Group-Decoupled在延迟与性能间最优，保持较高流畅度与准确度，并在感知与生成负载均衡时实现最高约2倍加速。

Conclusion: 放松位置连续性约束可实现真正并行的流式MLLM，显著降低实时交互延迟；Group-Decoupled是实用首选，为“边看边说”系统提供了可行路径与工程落地基础（代码已开源）。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance across many tasks, yet most systems remain limited to offline inference, requiring complete inputs before generating outputs. Recent streaming methods reduce latency by interleaving perception and generation, but still enforce a sequential perception-generation cycle, limiting real-time interaction. In this work, we target a fundamental bottleneck that arises when extending MLLMs to real-time video understanding: the global positional continuity constraint imposed by standard positional encoding schemes. While natural in offline inference, this constraint tightly couples perception and generation, preventing effective input-output parallelism. To address this limitation, we propose a parallel streaming framework that relaxes positional continuity through three designs: Overlapped, Group-Decoupled, and Gap-Isolated. These designs enable simultaneous perception and generation, allowing the model to process incoming inputs while producing responses in real time. Extensive experiments reveal that Group-Decoupled achieves the best efficiency-performance balance, maintaining high fluency and accuracy while significantly reducing latency. We further show that the proposed framework yields up to 2x acceleration under balanced perception-generation workloads, establishing a principled pathway toward speak-while-watching real-time systems. We make all our code publicly available: https://github.com/EIT-NLP/Speak-While-Watching.

</details>


### [61] [MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data](https://arxiv.org/abs/2601.06847)
*Mengmeng Zhang,Xiaoping Wu,Hao Luo,Fan Wang,Yisheng Lv*

Main category: cs.CV

TL;DR: 提出MedGround管线，将分割掩码资源自动转化为高质量“医学指代表述-定位”配对数据，并构建MedGround-35K数据集，显著提升VLM在医疗可视化落地（referring grounding）与多目标消歧及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在医疗图像上能生成可信叙述，但缺乏对语句的视觉锚定，关键原因是缺少大规模高质量的“语言-定位”数据对。需要一种可扩展方式从现有分割资源中构建可验证的指代表述与定位监督。

Method: 提出自动化管线MedGround：以专家分割掩码为空间锚点，精确确定定位目标；抽取形状与空间线索；引导VLM合成自然、具临床语义且与形态/位置一致的查询；并通过多阶段校验（格式约束、几何与医学先验规则、基于图像的视觉评审）过滤歧义或缺乏视觉支撑的样本；最终形成多模态指代表述-定位数据。

Result: 构建MedGround-35K数据集。使用该数据训练的VLM在指代表述定位任务上稳定提升，能更好区分多对象语义，且对未见的落地设定具有较强泛化。

Conclusion: MedGround提供了一条可扩展、数据驱动的路径，将医学语言锚定到可验证的视觉证据，实证显示能系统性增强VLM的临床可视化落地能力；数据与代码将于接收后开源。

Abstract: Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.

</details>


### [62] [MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation](https://arxiv.org/abs/2601.06874)
*Changli Wu,Haodong Wang,Jiayi Ji,Yutian Yao,Chunsai Du,Jihua Kang,Yanwei Fu,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出MV-3DRES任务：从稀疏多视角RGB图像直接执行指代表达的3D分割；给出端到端模型MVGGT与优化策略PVSO，并构建评测基准MVRefer。MVGGT在准确性与速度上优于两阶段及现有替代方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用（机器人/手机）常仅有少量视角与延迟约束，难以获得稠密高质点云；现有两阶段“重建点云→分割”在稀疏条件下几何质量差、目标区域粗糙且推理慢。需要一种能在稀疏多视图下直接结合语言进行3D指代分割的高效方法与统一评测标准。

Method: 提出MV-3DRES设定与端到端框架MVGGT：采用双分支设计，将语言信息融入稀疏多视图几何推理，实现直接从图像到3D目标分割。识别稀疏3D监督导致的前景梯度稀释问题（FGD），并提出逐视角的无目标抑制优化PVSO，强化并平衡各视角梯度，提升稳定高效训练；同时构建MVRefer基准与标准化度量。

Result: MVGGT作为首个强基线在MV-3DRES上实现高精度与快速推理，优于传统两阶段和其他替代方法；代码与模型已开源。

Conclusion: 在稀疏多视图与低延迟场景下，可通过语言引导的端到端几何推理与PVSO优化有效缓解监督不足，稳定训练并获得更好精度/效率；MVRefer为社区提供统一评测平台。

Abstract: Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.

</details>


### [63] [Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation](https://arxiv.org/abs/2601.06882)
*Dillan Imans,Phuoc-Nguyen Bui,Duc-Tai Le,Hyunseung Choo*

Main category: cs.CV

TL;DR: 提出将SAM与RefiSeR结合用于无监督域适应脑肿瘤分割，显著提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割在不同数据域（扫描仪/中心/协议）间存在显著域移，标注昂贵且难以跨域泛化，需要无监督域适应方法在目标域无标注条件下保持高分割精度。

Method: 构建基于Segment Anything Model (SAM) 的伪标注/先验提示，与RefiSeR精炼器结合：以源域有标注训练基本分割器；在目标域利用SAM产生多尺度/多提示的初始掩码作为结构先验，经RefiSeR迭代精炼与一致性/自训练损失进行适配；可能包含特征对齐、风格增强与不确定性过滤以提升伪标签质量。

Result: 在多个跨域脑肿瘤MRI基准（如BraTS跨中心/跨序列设置）上，相比现有UDA与纯自训练方法，Dice、Hausdorff等指标显著提升，特别是在增强肿瘤、核心与整体区域上均有稳定收益；对不同扫描协议与噪声条件具备更强鲁棒性。

Conclusion: SAM提供强大的结构先验，RefiSeR有效精炼与筛选伪标签，两者结合能在无标注目标域显著提升脑肿瘤分割的跨域泛化，具有实际临床迁移潜力。

Abstract: Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation

</details>


### [64] [MixRI: Mixing Features of Reference Images for Novel Object Pose Estimation](https://arxiv.org/abs/2601.06883)
*Xinhang Liu,Jiawei Shi,Zheng Dang,Yuchao Dai*

Main category: cs.CV

TL;DR: MixRI是一种轻量级网络，用于RGB图像中的基于CAD的新物体位姿估计，无需微调即可在测试时直接用于新物体；通过参考图像融合与点级多视匹配，显著减少参考图像与参数规模，同时保持与SOTA相当的精度并提升速度与内存效率。


<details>
  <summary>Details</summary>
Motivation: 实际应用需要对未见过的新物体快速、低内存且无需微调地进行位姿估计；现有方法依赖大量参考图像与庞大模型，推理慢、存储开销大，限制落地。

Method: 提出MixRI轻量级网络：在查询与参考图像间进行多视点级匹配；设计参考图像融合策略以聚合多视信息，从而减少参考图像数量；整体网络参数更小、推理更快，同时直接使用CAD渲染/参考图像，无需对新物体微调。

Result: 在BOP挑战的7个核心数据集上，尽管使用更少参考图像与更小模型参数，仍取得与需要更多参考图像/更大模型的方法相当的性能；推理时间与内存占用显著降低。

Conclusion: MixRI在保证竞争性准确度的同时，大幅降低参考图像需求、模型规模与推理延迟，实现对新物体的即插即用位姿估计，适合实际部署。

Abstract: We present MixRI, a lightweight network that solves the CAD-based novel object pose estimation problem in RGB images. It can be instantly applied to a novel object at test time without finetuning. We design our network to meet the demands of real-world applications, emphasizing reduced memory requirements and fast inference time. Unlike existing works that utilize many reference images and have large network parameters, we directly match points based on the multi-view information between the query and reference images with a lightweight network. Thanks to our reference image fusion strategy, we significantly decrease the number of reference images, thus decreasing the time needed to process these images and the memory required to store them. Furthermore, with our lightweight network, our method requires less inference time. Though with fewer reference images, experiments on seven core datasets in the BOP challenge show that our method achieves comparable results with other methods that require more reference images and larger network parameters.

</details>


### [65] [CLIMP: Contrastive Language-Image Mamba Pretraining](https://arxiv.org/abs/2601.06891)
*Nimrod Shabtay,Itamar Zimerman,Eli Schwartz,Raja Giryes*

Main category: cs.CV

TL;DR: 提出CLIMP：首个全Mamba架构的对比式视觉-语言预训练模型，用Mamba替代ViT与Transformer，提升鲁棒性与检索性能，降低内存/FLOPs，并支持可变分辨率与更长文本上下文。


<details>
  <summary>Details</summary>
Motivation: CLIP依赖ViT注意力，存在对虚假相关的敏感、计算随分辨率二次增长、固定文本上下文长度和位置编码插值等局限；需要一种既高效又鲁棒、能处理变分辨率与长文本的替代架构。

Method: 将视觉与文本编码器全部换为Mamba序列模型：视觉端使用VMamba以顺序建模方式注入空间归纳偏置；文本端使用自回归Mamba处理可变、长上下文。采用对比学习目标进行跨模态对齐，并在可变输入分辨率下训练推理，无需位置编码插值或专门技巧。

Result: 相较OpenAI CLIP-ViT-B，在ImageNet-O上提升7.5%的OOD鲁棒性；在16×训练分辨率时检索准确率最高提升6.6%，同时内存占用降低约5倍、FLOPs减少约1.8倍；支持密集描述检索，突破CLIP固定上下文限制。

Conclusion: Mamba在视觉-语言学习中展现出优越特性，可作为Transformer式CLIP的有力替代：更鲁棒、更高效、可扩展至可变分辨率与长文本场景。

Abstract: Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI's CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP's fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.

</details>


### [66] [UDPNet: Unleashing Depth-based Priors for Robust Image Dehazing](https://arxiv.org/abs/2601.06909)
*Zengyuan Zuo,Junjun Jiang,Gang Wu,Xianming Liu*

Main category: cs.CV

TL;DR: 提出UDPNet，将大规模预训练深度估计模型（DepthAnything V2）的深度先验引入去雾网络，通过深度引导注意与深度先验融合两模块，在多数据集上显著提升PSNR（SOTS +0.85dB、Haze4K +1.19dB、NHR +1.79dB），在合成与真实场景均具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法多仅用RGB信息或与深度联合训练但未充分利用准确深度，导致在不同雾密度、光照与域偏移下表现不稳、上限受限。利用强大的预训练深度模型提供稳健几何先验，有望提升去雾的判别性与泛化性。

Method: 提出通用可插拔框架UDPNet，以DepthAnything V2生成的深度先验为外部条件：1) 深度引导注意模块（DGAM）以轻量级通道注意自适应调制特征；2) 深度先验融合模块（DPFM）通过双滑窗多头跨注意实现多尺度深度特征的层级融合。整体保证计算高效与先验有效整合，并可兼容多种现有去雾主干。

Result: 在主流数据集上超越SOTA：SOTS提升0.85 dB PSNR、Haze4K提升1.19 dB、NHR提升1.79 dB，展现跨域（合成/真实）与不同雾密度、光照条件下的稳健性。

Conclusion: 将大规模预训练深度先验系统性注入去雾流程，可持续、通用地提升现有模型性能；DGAM与DPFM在保证效率的同时有效融合几何信息，为深度感知去雾树立新基线，代码与模型将开源。

Abstract: Image dehazing has witnessed significant advancements with the development of deep learning models. However, a few methods predominantly focus on single-modal RGB features, neglecting the inherent correlation between scene depth and haze distribution. Even those that jointly optimize depth estimation and image dehazing often suffer from suboptimal performance due to inadequate utilization of accurate depth information. In this paper, we present UDPNet, a general framework that leverages depth-based priors from large-scale pretrained depth estimation model DepthAnything V2 to boost existing image dehazing models. Specifically, our architecture comprises two typical components: the Depth-Guided Attention Module (DGAM) adaptively modulates features via lightweight depth-guided channel attention, and the Depth Prior Fusion Module (DPFM) enables hierarchical fusion of multi-scale depth map features by dual sliding-window multi-head cross-attention mechanism. These modules ensure both computational efficiency and effective integration of depth priors. Moreover, the intrinsic robustness of depth priors empowers the network to dynamically adapt to varying haze densities, illumination conditions, and domain gaps across synthetic and real-world data. Extensive experimental results demonstrate the effectiveness of our UDPNet, outperforming the state-of-the-art methods on popular dehazing datasets, such as 0.85 dB PSNR improvement on the SOTS dataset, 1.19 dB on the Haze4K dataset and 1.79 dB PSNR on the NHR dataset. Our proposed solution establishes a new benchmark for depth-aware dehazing across various scenarios. Pretrained models and codes will be released at our project https://github.com/Harbinzzy/UDPNet.

</details>


### [67] [RenderFlow: Single-Step Neural Rendering via Flow Matching](https://arxiv.org/abs/2601.06928)
*Shenghao Zhang,Runtao Liu,Christopher Schroers,Yang Zhang*

Main category: cs.CV

TL;DR: 提出RenderFlow：基于flow matching的单步确定性神经渲染，结合稀疏关键帧引导，实现接近实时的高质量、物理更一致的渲染，并通过轻量适配器扩展到内在分解等逆渲染任务。


<details>
  <summary>Details</summary>
Motivation: 现有PBR物理精确但计算昂贵；扩散模型渲染虽无需显式光传输、效果好，但迭代慢且随机性导致物理不准与时间不一致，难以实时与稳定应用。

Method: 构建端到端单步确定性渲染框架RenderFlow，采用flow matching范式训练从G-buffer（等条件）到图像的映射，避免扩散的多步采样；引入高效稀疏关键帧引导模块，在提供少量PBR渲染帧时提升物理可信度与泛化；并通过轻量适配器，将预训练前向模型复用于逆渲染任务（内在分解）。

Result: 在保持照片级视觉质量的同时显著加速渲染，接近实时；在提供稀疏关键帧时，进一步提升物理一致性与整体画质；框架可迁移到内在分解并取得有效结果。

Conclusion: RenderFlow以确定性、单步的flow-matching神经渲染弥合了高效生成模型与传统PBR精度之间的鸿沟，既快又稳；关键帧引导与适配器机制使其具备更强的泛化与可拓展性。

Abstract: Conventional physically based rendering (PBR) pipelines generate photorealistic images through computationally intensive light transport simulations. Although recent deep learning approaches leverage diffusion model priors with geometry buffers (G-buffers) to produce visually compelling results without explicit scene geometry or light simulation, they remain constrained by two major limitations. First, the iterative nature of the diffusion process introduces substantial latency. Second, the inherent stochasticity of these generative models compromises physical accuracy and temporal consistency. In response to these challenges, we propose a novel, end-to-end, deterministic, single-step neural rendering framework, RenderFlow, built upon a flow matching paradigm. To further strengthen both rendering quality and generalization, we propose an efficient and effective module for sparse keyframe guidance. Our method significantly accelerates the rendering process and, by optionally incorporating sparsely rendered keyframes as guidance, enhances both the physical plausibility and overall visual quality of the output. The resulting pipeline achieves near real-time performance with photorealistic rendering quality, effectively bridging the gap between the efficiency of modern generative models and the precision of traditional physically based rendering. Furthermore, we demonstrate the versatility of our framework by introducing a lightweight, adapter-based module that efficiently repurposes the pretrained forward model for the inverse rendering task of intrinsic decomposition.

</details>


### [68] [Measuring Social Bias in Vision-Language Models with Face-Only Counterfactuals from Real Photos](https://arxiv.org/abs/2601.06931)
*Haodong Chen,Qiang Huang,Jiaqi Zhao,Qiuping Jiang,Xiaojun Chang,Jun Yu*

Main category: cs.CV

TL;DR: 提出面向视觉-语言模型偏见审计的“只改脸”反事实评估：在保持场景不变的前提下，仅编辑人脸的种族与性别属性；据此构建FOCUS数据与REFLECT基准，发现在严格视觉控制下偏差仍存在且随任务形式显著变化。


<details>
  <summary>Details</summary>
Motivation: 现实图像中人口属性与背景、服饰等强相关，导致偏见测量难以归因；缺乏在保持真实感的同时能隔离人口属性影响的可控评估方案与标准化基准。

Method: 从真实照片出发，生成仅修改人脸种族和性别的反事实版本，其他视觉因素保持不变；据此构建包含六类职业、十个人口群体、480组场景匹配图像的FOCUS数据集；设计REFLECT基准，含三类决策导向任务：二选一偏好、社会经济多选推断、以及数字化薪资推荐；在五个SOTA VLM上进行系统测试。

Result: 在严格控制视觉混杂的条件下，VLM对不同人口群体的输出仍表现出显著差异，且偏差强度与方向随任务形式发生较大变化；不同模型之间也存在差异。

Conclusion: 需要采用受控的反事实审计来更可靠地评估多模态模型的社会偏见；任务设计本身对偏见显现具有关键影响，评估应多任务、多维度进行。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in socially consequential settings, raising concerns about social bias driven by demographic cues. A central challenge in measuring such social bias is attribution under visual confounding: real-world images entangle race and gender with correlated factors such as background and clothing, obscuring attribution. We propose a \textbf{face-only counterfactual evaluation paradigm} that isolates demographic effects while preserving real-image realism. Starting from real photographs, we generate counterfactual variants by editing only facial attributes related to race and gender, keeping all other visual factors fixed. Based on this paradigm, we construct \textbf{FOCUS}, a dataset of 480 scene-matched counterfactual images across six occupations and ten demographic groups, and propose \textbf{REFLECT}, a benchmark comprising three decision-oriented tasks: two-alternative forced choice, multiple-choice socioeconomic inference, and numeric salary recommendation. Experiments on five state-of-the-art VLMs reveal that demographic disparities persist under strict visual control and vary substantially across task formulations. These findings underscore the necessity of controlled, counterfactual audits and highlight task design as a critical factor in evaluating social bias in multimodal models.

</details>


### [69] [Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning](https://arxiv.org/abs/2601.06943)
*Chengwen Liu,Xiaomin Yu,Zhuoyue Chang,Zhe Huang,Shuo Zhang,Heng Lian,Kunyi Wang,Rui Xu,Sen Hu,Jianheng Hou,Hao Peng,Chengwei Qin,Xiaobin Hu,Hong Peng,Ronghao Chen,Huacan Wang*

Main category: cs.CV

TL;DR: VideoDR提出首个面向开放网络检索的“视频深度研究”基准，要求模型在视频线索引导下进行跨帧锚点抽取、交互式检索与多跳验证；实验显示Agentic范式并非总优于Workflow，核心瓶颈在目标漂移与长程一致性。


<details>
  <summary>Details</summary>
Motivation: 现实中的视频问答常仅含局部线索，而可验证答案分散在开放网络；现有评测无法系统检验模型在“视频引导+开网检索+多跳推理”上的综合能力，因而需要新的基准来弥补这一评测空白。

Method: 构建VideoDR数据集：以视频条件的开放域VQA为中心，要求从跨帧中抽取视觉锚点，进行交互式网页检索，并对视频与网页证据进行多跳推理与验证；通过人工标注与质控覆盖六个语义领域。基于Workflow与Agentic两种范式评测多种开闭源多模态大模型，分析检索链中的锚点保持与一致性问题。

Result: 在VideoDR上，Agentic相较Workflow并非稳定更优，其收益取决于模型能否在长检索链中保持初始视频锚点；发现目标漂移与长程一致性缺失是主要性能瓶颈。

Conclusion: VideoDR为开放网络环境下研究视频智能体提供系统化基准，并揭示下一代视频深度研究智能体需重点解决的两个关键挑战：抑制目标漂移与增强长程一致性。

Abstract: In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.

</details>


### [70] [SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models](https://arxiv.org/abs/2601.06944)
*Yuhang Su,Mei Wang,Yaoyao Zhong,Guozhang Li,Shixing Li,Yihan Feng,Hua Huang*

Main category: cs.CV

TL;DR: 提出SketchJudge基准，用于评估MLLM在手绘STEM图的判分与诊断能力；包含1015份学生手绘答案覆盖几何、物理、图表、流程图，揭示先进MLLM在杂讯与符号化场景远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽然擅长视觉理解，但对人类手绘草图的非结构化与歧义性适应差。视觉判分任务不仅要得出答案，还需定位并解释图中错误，涉及结构、语义与元认知推理；该领域缺少系统评测基准。

Method: 构建SketchJudge数据集与评测框架：收集1015份学生手绘STEM图，覆盖四个领域与多种风格/错误类型；定义判分与错误诊断任务与指标；用多种先进MLLM在该基准上评测并与人类表现对比；开放数据与代码。

Result: 在SketchJudge上，先进MLLM显著落后于人类，暴露了当前视觉-语言对齐在符号化、噪声环境下的脆弱性；基准有效区分模型能力不足之处。

Conclusion: SketchJudge为MLLM在手绘图判分与诊断的评估提供标准化平台，显示当前方法仍有较大差距；未来需加强结构化推理、鲁棒对齐与错误定位能力。

Abstract: While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.

</details>


### [71] [Unified Personalized Understanding, Generating and Editing](https://arxiv.org/abs/2601.06965)
*Yu Zhong,Tianwei Lin,Ruike Zhu,Yuqian Yuan,Haoyu Zheng,Liang Liang,Wenqiao Zhang,Feifei Shao,Haoyuan Li,Wanggui He,Hao Jiang,Yueting Zhuang*

Main category: cs.CV

TL;DR: OmniPersona 是一个端到端的个性化统一大多模（LMM）框架，在同一架构内实现个性化理解、生成与图像编辑，并通过结构性解耦的概念token与显式知识回放机制，减少跨任务干扰，提升个性化一致性与可控性；并提出评测基准 OmniPBench。


<details>
  <summary>Details</summary>
Motivation: 现有统一LMM多采用“一刀切”范式，难以稳定地表示和操控用户特定概念（如专属人物/物体）；外部检索方案效率低且与统一流水线耦合差；软提示个性化方法常将理解与生成耦合或需要复杂多阶段训练，导致跨任务干扰与个性化知识模糊/错位。

Method: 提出 OmniPersona：1）结构性解耦的概念token，为不同任务（理解、生成、编辑）分配独立子空间，降低干扰；2）显式知识回放，将个性化属性知识在任务间传播，保障一致个性化行为；3）在统一架构内端到端训练与推理。并构建 OmniPBench，扩展 UnifyBench 概念集，加入个性化编辑任务与跨任务评测协议。

Result: 在多种个性化任务与跨任务评估上取得具有竞争力且鲁棒的表现，相比现有方法在一致性、可控性与融合统一流程方面更优。

Conclusion: OmniPersona 为统一LMM的可控个性化提供强基线，证明结构解耦与知识回放能有效缓解跨任务干扰并提升个性化一致性；OmniPBench为后续研究提供系统化评测框架。

Abstract: Unified large multimodal models (LMMs) have achieved remarkable progress in general-purpose multimodal understanding and generation. However, they still operate under a ``one-size-fits-all'' paradigm and struggle to model user-specific concepts (e.g., generate a photo of \texttt{<maeve>}) in a consistent and controllable manner. Existing personalization methods typically rely on external retrieval, which is inefficient and poorly integrated into unified multimodal pipelines. Recent personalized unified models introduce learnable soft prompts to encode concept information, yet they either couple understanding and generation or depend on complex multi-stage training, leading to cross-task interference and ultimately to fuzzy or misaligned personalized knowledge. We present \textbf{OmniPersona}, an end-to-end personalization framework for unified LMMs that, for the first time, integrates personalized understanding, generation, and image editing within a single architecture. OmniPersona introduces structurally decoupled concept tokens, allocating dedicated subspaces for different tasks to minimize interference, and incorporates an explicit knowledge replay mechanism that propagates personalized attribute knowledge across tasks, enabling consistent personalized behavior. To systematically evaluate unified personalization, we propose \textbf{\texttt{OmniPBench}}, extending the public UnifyBench concept set with personalized editing tasks and cross-task evaluation protocols integrating understanding, generation, and editing. Experimental results demonstrate that OmniPersona delivers competitive and robust performance across diverse personalization tasks. We hope OmniPersona will serve as a strong baseline and spur further research on controllable, unified personalization.

</details>


### [72] [Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?](https://arxiv.org/abs/2601.06993)
*Jie Zhu,Yiyang Su,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文发现：在细粒度视觉分类中，链式思维的“长推理”会稳定降低准确率（称为“思考成本”）。据此提出归一化算法和基于多重奖励与长度约束的训练框架ReFine-RFT，在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: MLLM在通用任务强，但在要求细微视觉辨析的FGVC上表现欠佳。虽然CoT在数学、编程等任务有效，但有工作显示其对视觉感知有害，且原因未明。作者动机是系统厘清CoT在FGVC中的作用与弊端，并寻求改进方法。

Method: 1) 系统评估：在零样本与多种训练范式下，分析CoT对FGVC的影响，重点考察“推理长度”与准确率的关系。2) 提出\alg：一种用于多奖励优化的通用、可插拔归一化方法，平衡异质奖励信号。3) ReFine-RFT：将集成（ensemble）奖励与\alg结合，在提供以准确率为导向的稠密反馈的同时，显式约束推理长度。

Result: 发现核心悖论：CoT导致的性能下降主要由“文本推理过长”驱动，推理越长分类准确率越低。所提ReFine-RFT在多个FGVC基准上取得SOTA，验证了长度约束与奖励归一化的有效性。

Conclusion: 视觉感知任务中，盲目延长CoT推理会带来“思考成本”，损害细粒度分类性能。通过奖励归一化与长度约束，可以在保持（或提升）感知准确率的同时利用有限推理，ReFine-RFT为MLLM在FGVC中的训练提供了有效范式。

Abstract: Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \href{https://github.com/jiezhu23/ReFine-RFT}{Project Link}.

</details>


### [73] [Spatial Multi-Task Learning for Breast Cancer Molecular Subtype Prediction from Single-Phase DCE-MRI](https://arxiv.org/abs/2601.07001)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: 提出一种针对单期DCE‑MRI的空间多任务学习框架，同时预测ER/PR/HER2与Ki‑67，实现非侵入式分子分型；在960例上显著优于放射组学与单任务深度学习，AUC约0.82–0.89，Ki‑67 MAE为8.2%。


<details>
  <summary>Details</summary>
Motivation: 临床常规多依赖免疫组化（IHC）获取乳腺癌分子分型，但需侵入性活检且存在取样偏倚；而DCE‑MRI虽可非侵入性表征肿瘤，但现实流程常仅获取单期后增强以缩短时间与降低造影剂剂量，限制了动态图像信息的利用。需要一种在“单期DCE‑MRI”条件下仍能准确预测分子标志物的模型，减少侵入性检查并契合临床可行性。

Method: 构建空间多任务学习框架：1) 深度特征提取网络结合多尺度空间注意力，捕获肿瘤内与肿瘤周围特征；2) 感兴趣区域加权模块，突出肿瘤核心、边缘与周边组织；3) 共享表示+任务特异分支的多任务结构，联合学习ER、PR、HER2分类与Ki‑67回归，利用生物学相关性提升性能。数据集含960例（内部886例按7:1:2划分训练/验证/测试；外部74例五折交叉验证）。

Result: 在内部与外部数据上，方法取得ER AUC=0.893、PR AUC=0.824、HER2 AUC=0.857，Ki‑67平均绝对误差为8.2%，显著优于放射组学与单任务深度学习基线。

Conclusion: 在仅用临床常规的单期DCE‑MRI条件下，所提空间多任务框架可准确、非侵入式地预测乳腺癌关键生物标志物，为分子分型提供可行途径，有望减少活检与取样偏倚影响，具备临床转化潜力。

Abstract: Accurate molecular subtype classification is essential for personalized breast cancer treatment, yet conventional immunohistochemical analysis relies on invasive biopsies and is prone to sampling bias. Although dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) enables non-invasive tumor characterization, clinical workflows typically acquire only single-phase post-contrast images to reduce scan time and contrast agent dose. In this study, we propose a spatial multi-task learning framework for breast cancer molecular subtype prediction from clinically practical single-phase DCE-MRI. The framework simultaneously predicts estrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2) status, and the Ki-67 proliferation index -- biomarkers that collectively define molecular subtypes. The architecture integrates a deep feature extraction network with multi-scale spatial attention to capture intratumoral and peritumoral characteristics, together with a region-of-interest weighting module that emphasizes the tumor core, rim, and surrounding tissue. Multi-task learning exploits biological correlations among biomarkers through shared representations with task-specific prediction branches. Experiments on a dataset of 960 cases (886 internal cases split 7:1:2 for training/validation/testing, and 74 external cases evaluated via five-fold cross-validation) demonstrate that the proposed method achieves an AUC of 0.893, 0.824, and 0.857 for ER, PR, and HER2 classification, respectively, and a mean absolute error of 8.2\% for Ki-67 regression, significantly outperforming radiomics and single-task deep learning baselines. These results indicate the feasibility of accurate, non-invasive molecular subtype prediction using standard imaging protocols.

</details>


### [74] [Adversarial Attacks on Medical Hyperspectral Imaging Exploiting Spectral-Spatial Dependencies and Multiscale Features](https://arxiv.org/abs/2601.07056)
*Yunrui Gu,Zhenzhe Gao,Cong Kong,Zhaoxia Yin*

Main category: cs.CV

TL;DR: 提出针对医疗高光谱成像(HSI)的定向对抗攻击框架，通过局部像素依赖与多尺度谱-空特征扰动，显著降低肿瘤区域分类性能且难以察觉，揭示HSI模型独特脆弱性，呼吁结构感知的鲁棒防御。


<details>
  <summary>Details</summary>
Motivation: 医疗HSI依赖丰富的谱-空信息进行病灶识别，但深度模型被发现易受对抗攻击。作者分析HSI模型脆弱性根源，期望用面向HSI结构特性的攻击揭示风险并推动鲁棒防御研究。

Method: 基于两点脆弱性假设：1) 组织结构依赖局部像素相关；2) 表征依赖多尺度谱-空层级。提出两种攻击：— 局部像素依赖攻击(LPDA)：在空间邻域内联合扰动，利用邻近像素相关性破坏结构保持。— 多尺度信息攻击(MIA)：在层级谱-空尺度上施加扰动，破坏多尺度编码。二者构成定向攻击框架，确保扰动在视觉上不可感知。

Result: 在Brain与MDC数据集上，两类攻击显著降低分类准确率，肿瘤区域降幅更为明显，同时保持视觉不可察觉；与现有方法相比更能暴露HSI模型的特有脆弱点。

Conclusion: 医疗HSI模型对利用局部依赖和多尺度表征的定向攻击非常敏感。应发展面向结构与多尺度的鲁棒训练与防御机制，以满足临床安全可靠性需求。

Abstract: Medical hyperspectral imaging (HSI) enables accurate disease diagnosis by capturing rich spectral-spatial tissue information, but recent advances in deep learning have exposed its vulnerability to adversarial attacks. In this work, we identify two fundamental causes of this fragility: the reliance on local pixel dependencies for preserving tissue structure and the dependence on multiscale spectral-spatial representations for hierarchical feature encoding. Building on these insights, we propose a targeted adversarial attack framework for medical HSI, consisting of a Local Pixel Dependency Attack that exploits spatial correlations among neighboring pixels, and a Multiscale Information Attack that perturbs features across hierarchical spectral-spatial scales. Experiments on the Brain and MDC datasets demonstrate that our attacks significantly degrade classification performance, especially in tumor regions, while remaining visually imperceptible. Compared with existing methods, our approach reveals the unique vulnerabilities of medical HSI models and underscores the need for robust, structure-aware defenses in clinical applications.

</details>


### [75] [Billboard in Focus: Estimating Driver Gaze Duration from a Single Image](https://arxiv.org/abs/2601.07073)
*Carlos Pizarroso,Zuzana Berger Haladová,Zuzana Černeková,Viktor Kocur*

Main category: cs.CV

TL;DR: 提出一条无需人工标注与眼动仪的自动流水线：先检测广告牌，再基于位置与视觉特征估计驾驶员对广告牌的凝视时长；在BillboardLamac上mAP@50=94%（检测）与68.1%（帧级凝视分类），并在GSV图像上验证。


<details>
  <summary>Details</summary>
Motivation: 路侧广告牌可能引发驾驶分心并增加事故风险，但现有评估依赖人工标注或眼动追踪，成本高、难规模化。因此需要一种可自动化、可扩展的方法来识别广告牌并估计驾驶员对其关注度，以量化广告相关性与潜在安全影响。

Method: 两阶段流水线：1) 使用在Mapillary Vistas上预训练并在BillboardLamac上微调的YOLO进行广告牌检测，得到边界框（mAP@50=94%）；2) 利用检测到的框位置特征与DINOv2视觉特征，训练分类器，对单帧估计驾驶员对广告牌的凝视时长/相关性。还在Google Street View数据上进行外部验证。

Result: 在BillboardLamac上，检测阶段达到94% mAP@50；帧级凝视估计准确率68.1%。方法在GSV图像上也能工作，表明具有一定的跨域可用性。

Conclusion: 该自动化管线能在无需人工标注或眼动仪的前提下，对广告牌进行高精度检测并粗粒度估计驾驶员凝视时长，具备实际评估广告相关性与安全性的潜力；仍存在从帧级提升到时序级、更强泛化与更高准确率的空间。

Abstract: Roadside billboards represent a central element of outdoor advertising, yet their presence may contribute to driver distraction and accident risk. This study introduces a fully automated pipeline for billboard detection and driver gaze duration estimation, aiming to evaluate billboard relevance without reliance on manual annotations or eye-tracking devices. Our pipeline operates in two stages: (1) a YOLO-based object detection model trained on Mapillary Vistas and fine-tuned on BillboardLamac images achieved 94% mAP@50 in the billboard detection task (2) a classifier based on the detected bounding box positions and DINOv2 features. The proposed pipeline enables estimation of billboard driver gaze duration from individual frames. We show that our method is able to achieve 68.1% accuracy on BillboardLamac when considering individual frames. These results are further validated using images collected from Google Street View.

</details>


### [76] [Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression](https://arxiv.org/abs/2601.07092)
*Yuliang Cai,Dongqiangzi Ye,Zitian Chen,Chongruo Wu*

Main category: cs.CV

TL;DR: 提出SRC-Pipeline：通过对早期帧token压缩与近期帧保留全patch，实现在自动驾驶VQA中显著降本提速且性能基本不降。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶VQA需要极低延迟与实时性，但现有大规模视觉语言模型为追求精度而以密集patch处理所有帧，FLOPs与延迟高，难以部署在安全关键的实时场景。

Method: 设计SRC-Pipeline：对时间上较早的帧进行token压缩，将其浓缩为少量高层次token；对最近帧保留全patch token，以保证对当前情境的细粒度理解。整体为高效VLM框架，减少冗余时序信息的计算。

Result: 在自动驾驶视频问答任务上，相比基线减少约66%的FLOPs，同时保持与SOTA相当的性能与回答质量。

Conclusion: 时间感知的选择性token压缩可在不明显牺牲精度的情况下，大幅降低计算与延迟，使VLM更适用于实时、安规敏感的自动驾驶VQA应用。

Abstract: Autonomous driving increasingly relies on Visual Question Answering (VQA) to enable vehicles to understand complex surroundings by analyzing visual inputs and textual queries. Currently, a paramount concern for VQA in this domain is the stringent requirement for fast latency and real-time processing, as delays directly impact real-world safety in this safety-critical application. However, current state-of-the-art VQA models, particularly large vision-language models (VLMs), often prioritize performance over computational efficiency. These models typically process dense patch tokens for every frame, leading to prohibitive computational costs (FLOPs) and significant inference latency, especially with long video sequences. This focus limits their practical deployment in real-time autonomous driving scenarios. To tackle this issue, we propose an efficient VLM framework for autonomous driving VQA tasks, SRC-Pipeline. It learns to compress early frame tokens into a small number of high-level tokens while retaining full patch tokens for recent frames. Experiments on autonomous driving video question answering tasks show that our approach achieves 66% FLOPs reduction while maintaining comparable performance, enabling VLMs to operate more effectively in real-time, safety-critical autonomous driving settings.

</details>


### [77] [3D Wavelet-Based Structural Priors for Controlled Diffusion in Whole-Body Low-Dose PET Denoising](https://arxiv.org/abs/2601.07093)
*Peiyuan Jing,Yue Tang,Chun-Wun Cheng,Zhenxuan Zhang,Liutao Yang,Thiago V. Lima,Klaus Strobel,Antoine Leimgruber,Angelica Aviles-Rivero,Guang Yang,Javier Montoya*

Main category: cs.CV

TL;DR: 提出WCC-Net：在全3D扩散模型中引入小波频域结构先验，显著提升低剂量全身PET去噪的质量与解剖一致性，并优于CNN/GAN/扩散基线且具良好跨剂量泛化。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET可降低辐射但噪声大、结构细节易丢失；现有扩散模型虽擅长去噪，但随机性导致解剖结构难以稳定、一致，尤其在低信噪比与三维全身体积数据中。需要在保持扩散生成力的同时引入明确的结构先验以提升体素级解剖一致性。

Method: 提出Wavelet-Conditioned ControlNet（WCC-Net）：在冻结的预训练3D扩散骨干上新增轻量控制分支，将体积小波变换获得的多尺度频域结构特征作为条件输入，显式引导去噪过程，实现结构与噪声解耦、保持3D连续性与生成灵活性。

Result: 在内部1/20剂量测试集上，相比强扩散基线，PSNR提升+1.21 dB、SSIM提升+0.008，同时降低GMSD与NMAE；在未见过的1/50与1/4剂量上也取得更优量化指标与更好的体积解剖一致性。

Conclusion: 频域小波结构先验通过控制分支有效约束3D扩散去噪，兼顾细节与稳定的解剖一致性，较现有CNN/GAN/扩散方法更优，并具备跨剂量鲁棒泛化能力。

Abstract: Low-dose Positron Emission Tomography (PET) imaging reduces patient radiation exposure but suffers from increased noise that degrades image quality and diagnostic reliability. Although diffusion models have demonstrated strong denoising capability, their stochastic nature makes it challenging to enforce anatomically consistent structures, particularly in low signal-to-noise regimes and volumetric whole-body imaging. We propose Wavelet-Conditioned ControlNet (WCC-Net), a fully 3D diffusion-based framework that introduces explicit frequency-domain structural priors via wavelet representations to guide volumetric PET denoising. By injecting wavelet-based structural guidance into a frozen pretrained diffusion backbone through a lightweight control branch, WCC-Net decouples anatomical structure from noise while preserving generative expressiveness and 3D structural continuity. Extensive experiments demonstrate that WCC-Net consistently outperforms CNN-, GAN-, and diffusion-based baselines. On the internal 1/20-dose test set, WCC-Net improves PSNR by +1.21 dB and SSIM by +0.008 over a strong diffusion baseline, while reducing structural distortion (GMSD) and intensity error (NMAE). Moreover, WCC-Net generalizes robustly to unseen dose levels (1/50 and 1/4), achieving superior quantitative performance and improved volumetric anatomical consistency.

</details>


### [78] [MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.07107)
*Meng Lu,Yuxing Lu,Yuchen Zhuang,Megan Mullins,Yang Xie,Guanghua Xiao,Charles Fleming,Wenqi Shi,Xuan Wang*

Main category: cs.CV

TL;DR: 提出MedVistaGym，一个用于医疗图像的交互式工具整合推理训练环境，并训练出MedVistaGym-R1，显著提升多步视觉-语言推理与医疗VQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用VLM虽擅长通用图像理解，但在医疗图像上的多步推理薄弱。多数医疗VLM使用静态视觉嵌入与单次前向推理，无法在过程中反复检视与校验证据。尽管引入外部工具可能助推推理，但开源VLM缺乏在多模态医疗场景中学习“何时、为何、如何”选择与协调工具的训练基础设施。

Method: 构建MedVistaGym：一个可扩展、可交互的训练环境，提供统一、可执行接口，鼓励VLM进行工具整合的视觉推理。模型需决策是否/何时调用何种工具、定位相关图像区域、整合单个或多个子图证据，并与语言推理交错进行。基于该环境，采用轨迹采样与端到端强化学习训练MedVistaGym-R1，使其在推理过程中与工具交替互动。

Result: 在6个医疗VQA基准上，MedVistaGym-R1-8B较同尺寸的工具增强基线提升19.10%—24.21%，显示结构化的代理式训练带来显著收益。

Conclusion: 仅提供工具访问不足以实现有效的医疗图像工具化推理；需要在像MedVistaGym这样的交互环境中进行结构化代理式训练，以学习工具选择、调用与证据整合，从而显著提升医疗多模态推理表现。

Abstract: Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.

</details>


### [79] [Few-shot Class-Incremental Learning via Generative Co-Memory Regularization](https://arxiv.org/abs/2601.07117)
*Kexin Bao,Yong Li,Dan Zeng,Shiming Ge*

Main category: cs.CV

TL;DR: 提出一种用于小样本类增量学习（FSCIL）的生成式共记忆正则化方法，通过生成式域自适应微调获得可迁移表示，并在增量阶段用“表示记忆 + 权重记忆”共同约束分类器更新，兼顾新类适应与旧类不遗忘。


<details>
  <summary>Details</summary>
Motivation: FSCIL需要在极少样本下持续学习新类，易发生旧类灾难性遗忘与新类过拟合。现有方法要么表示泛化不足，要么缺少有效的增量约束。作者希望借助生成式重建信号提升表示稳健性，并用记忆化先验稳定增量优化。

Method: 1) 生成式域自适应微调：以预训练生成式编码器为 backbone，在基础阶段同时接入MAE解码器做特征重建、全连接分类器做监督分类，共同优化以学习既可重建又可判别的通用表示。2) 构建双记忆：用微调后的编码器与分类器，建立按类的表示记忆（类均值特征）与权重记忆（分类器权重）。3) 增量学习：在每个会话用少量新类样本训练分类器，同时最小化分类损失与共记忆正则（对齐当前特征与表示记忆、当前权重与权重记忆），并按类增量更新两种记忆。

Result: 在多个主流FSCIL基准上，方法的平均准确率与末次会话准确率均优于现有最优方法，表现出更好的稳健性与对遗忘的抑制（文中称“显著超越SOTA”）。

Conclusion: 生成式重建与判别学习的联合微调提供更通用的表示；类级别的表示与权重双记忆在增量阶段形成有效正则，能兼顾旧类保留与新类适配，从而提升FSCIL整体性能。

Abstract: Few-shot class-incremental learning (FSCIL) aims to incrementally learn models from a small amount of novel data, which requires strong representation and adaptation ability of models learned under few-example supervision to avoid catastrophic forgetting on old classes and overfitting to novel classes. This work proposes a generative co-memory regularization approach to facilitate FSCIL. In the approach, the base learning leverages generative domain adaptation finetuning to finetune a pretrained generative encoder on a few examples of base classes by jointly incorporating a masked autoencoder (MAE) decoder for feature reconstruction and a fully-connected classifier for feature classification, which enables the model to efficiently capture general and adaptable representations. Using the finetuned encoder and learned classifier, we construct two class-wise memories: representation memory for storing the mean features for each class, and weight memory for storing the classifier weights. After that, the memory-regularized incremental learning is performed to train the classifier dynamically on the examples of few-shot classes in each incremental session by simultaneously optimizing feature classification and co-memory regularization. The memories are updated in a class-incremental manner and they collaboratively regularize the incremental learning. In this way, the learned models improve recognition accuracy, while mitigating catastrophic forgetting over old classes and overfitting to novel classes. Extensive experiments on popular benchmarks clearly demonstrate that our approach outperforms the state-of-the-arts.

</details>


### [80] [Motion Focus Recognition in Fast-Moving Egocentric Video](https://arxiv.org/abs/2601.07154)
*Daniel Hong,James Tribble,Hao Wang,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 提出一种面向自运动意图的实时“运动焦点”识别方法，从任意第一人称视频估计被摄者的行走/移动意图，并通过系统优化实现边缘侧高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有第一人称（egocentric）数据与方法多聚焦动作识别，忽视在体育与快速运动场景中至关重要的运动/位姿与运动意图分析；需要能实时、可扩展地从第一人称视频推断运动意图的方案，以补足VLA与机器人等应用。

Method: 利用相机位姿估计的基础模型作为核心信号源，提出系统级优化与滑动批量（sliding batch）推理策略，降低内存占用并实现可扩展实时推理；从任意第一人称视频估计主体的“移动/机动”意图。

Result: 在自采的第一人称动作数据集上验证，方法在实时速度下运行，并通过滑动批推理实现可控/可管理的内存占用。

Conclusion: 方法使以运动为中心的分析在边缘部署中变得可行，为体育与快速运动类第一人称研究提供补充视角，可与现有动作识别研究形成互补。

Abstract: From Vision-Language-Action (VLA) systems to robotics, existing egocentric datasets primarily focus on action recognition tasks, while largely overlooking the inherent role of motion analysis in sports and other fast-movement scenarios. To bridge this gap, we propose a real-time motion focus recognition method that estimates the subject's locomotion intention from any egocentric video. Our approach leverages the foundation model for camera pose estimation and introduces system-level optimizations to enable efficient and scalable inference. Evaluated on a collected egocentric action dataset, our method achieves real-time performance with manageable memory consumption through a sliding batch inference strategy. This work makes motion-centric analysis practical for edge deployment and offers a complementary perspective to existing egocentric studies on sports and fast-movement activities.

</details>


### [81] [Test-time Adaptive Hierarchical Co-enhanced Denoising Network for Reliable Multimodal Classification](https://arxiv.org/abs/2601.07163)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 提出TAHCD，在含噪多模态数据上实现稳健、可自适应的学习：分层对齐去噪（全局/实例、模态内/跨模态）+测试时协同增强自适应更新，无需标签，显著提升分类、鲁棒与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有多模态鲁棒学习难以可靠去除异构噪声（模态内与跨模态、全局与样本级），且对未见噪声的适应与泛化不足，影响安全关键场景中的可靠性。

Method: 提出测试时自适应的分层协同去噪网络TAHCD：1) 自适应稳定子空间对齐（全局层面）与样本自适应置信对齐（实例层面），联合处理模态特异与跨模态噪声；2) 测试时协同增强，无需标签，依据输入样本噪声动态更新模型与对齐过程，联动全局与实例层面去噪。

Result: 在多项基准上，分类性能、对噪声的鲁棒性与对未见噪声的泛化均优于最先进的可靠多模态学习方法。

Conclusion: TAHCD通过分层对齐去噪与测试时协同自适应，可靠去除异构噪声并提升对未知噪声的适应与泛化，实现稳健多模态学习，实验验证其显著优越性。

Abstract: Reliable learning on low-quality multimodal data is a widely concerning issue, especially in safety-critical applications. However, multimodal noise poses a major challenge in this domain and leads existing methods to suffer from two key limitations. First, they struggle to reliably remove heterogeneous data noise, hindering robust multimodal representation learning. Second, they exhibit limited adaptability and generalization when encountering previously unseen noise. To address these issues, we propose Test-time Adaptive Hierarchical Co-enhanced Denoising Network (TAHCD). On one hand, TAHCD introduces the Adaptive Stable Subspace Alignment and Sample-Adaptive Confidence Alignment to reliably remove heterogeneous noise. They account for noise at both global and instance levels and enable jointly removal of modality-specific and cross-modality noise, achieving robust learning. On the other hand, TAHCD introduces test-time cooperative enhancement, which adaptively updates the model in response to input noise in a label-free manner, improving adaptability and generalization. This is achieved by collaboratively enhancing the joint removal process of modality-specific and cross-modality noise across global and instance levels according to sample noise. Experiments on multiple benchmarks demonstrate that the proposed method achieves superior classification performance, robustness, and generalization compared with state-of-the-art reliable multimodal learning approaches.

</details>


### [82] [DIVER: Dynamic Iterative Visual Evidence Reasoning for Multimodal Fake News Detection](https://arxiv.org/abs/2601.07178)
*Weilin Zhou,Zonghao Ying,Chunlei Meng,Jiahui Liu,Hengyang Zhou,Quanchen Zou,Deyue Zhang,Dongdong Yang,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: 提出DIVER框架：先做文本一致性推理，证据不足再引入视觉，并按需调用精细视觉工具，通过不确定性感知融合迭代整合证据，在多数据集上提升准确与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态虚假新闻检测依赖静态融合或LLM，因视觉基础薄弱导致计算冗余与幻觉风险；需要一种以证据为中心、按需使用视觉信息且高效稳健的方法。

Method: 构建DIVER（Dynamic Iterative Visual Evidence Reasoning）：1) 以语言分析建立强文本基线，利用模态内一致性过滤不可靠/幻觉性声明；2) 当文本证据不足时，再引入视觉信息做跨模态对齐验证，自适应判断是否需要更深视觉检查；3) 对存在显著跨模态语义不一致的样本，选择性调用细粒度视觉工具（OCR、密集字幕）；4) 通过不确定性感知融合迭代聚合证据，逐步细化多模态推理。

Result: 在Weibo、Weibo21、GossipCop上平均比SOTA提升2.72%准确等指标，同时将推理延迟平均降低4.12秒。

Conclusion: 循证、按需、迭代的多模态推理能在减少计算与幻觉的同时提升虚假新闻检测效果；强化文本基线并自适应引入精细视觉证据是有效路线。

Abstract: Multimodal fake news detection is crucial for mitigating adversarial misinformation. Existing methods, relying on static fusion or LLMs, face computational redundancy and hallucination risks due to weak visual foundations. To address this, we propose DIVER (Dynamic Iterative Visual Evidence Reasoning), a framework grounded in a progressive, evidence-driven reasoning paradigm. DIVER first establishes a strong text-based baseline through language analysis, leveraging intra-modal consistency to filter unreliable or hallucinated claims. Only when textual evidence is insufficient does the framework introduce visual information, where inter-modal alignment verification adaptively determines whether deeper visual inspection is necessary. For samples exhibiting significant cross-modal semantic discrepancies, DIVER selectively invokes fine-grained visual tools (e.g., OCR and dense captioning) to extract task-relevant evidence, which is iteratively aggregated via uncertainty-aware fusion to refine multimodal reasoning. Experiments on Weibo, Weibo21, and GossipCop demonstrate that DIVER outperforms state-of-the-art baselines by an average of 2.72\%, while optimizing inference efficiency with a reduced latency of 4.12 s.

</details>


### [83] [ShowUI-Aloha: Human-Taught GUI Agent](https://arxiv.org/abs/2601.07181)
*Yichun Zhang,Xiangwu Guo,Yauhong Goh,Jessica Hu,Zhiheng Chen,Xin Wang,Difei Gao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出ShowUI-Aloha：将“野外”桌面屏幕录制转化为可学习的结构化GUI任务数据，用于训练通用GUI智能体。


<details>
  <summary>Details</summary>
Motivation: 自动化复杂GUI任务受限于缺乏可扩展、高质量的训练数据。人类演示虽丰富但通常冗长、无结构、无标注，难以直接用于学习。需要一种从原始屏幕录制中提取结构化、可执行监督信号的方案。

Method: 构建端到端数据与执行管线ShowUI-Aloha，含四部分：1) Recorder：同步捕获屏幕视频与精确交互事件（点击、键击、滚动）；2) Learner：结合视觉上下文对原始交互进行语义解析，生成自然语言描述；3) Planner：读取解析后的演示，维护任务状态，基于上下文推理动态生成下一步高层动作计划；4) Executor：在操作系统层忠实执行计划（点击、拖拽、输入、窗口操作），含安全检查与实时反馈。

Result: 该管线可将无结构的人类屏幕录制转化为结构化、可操作的任务序列，为真实世界数据的收集与解析提供可扩展方案，并在构建能从人类观察中学习的通用GUI智能体方面展示可行性。

Conclusion: ShowUI-Aloha证明了从“观察人类”出发构建GUI智能体的有效路径：以录制-语义学习-规划-执行的闭环，将野外演示变为可训练与可执行的数据，提升GUI代理的泛化与实用潜力。

Abstract: Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.

</details>


### [84] [SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model](https://arxiv.org/abs/2601.07209)
*Yu Guo,Zhiqiang Lao,Xiyun Song,Yubin Zhou,Heather Yu*

Main category: cs.CV

TL;DR: 提出一种物理真实的合成数据与LMM微调方法，提升单张图像反射去除与层分离性能。


<details>
  <summary>Details</summary>
Motivation: SIRR受玻璃反射与透射叠加影响，现有数据集要么物理不真实（合成），要么规模不足（实拍），限制模型训练与泛化。

Method: 1) 构建合成数据生成框架：对3D玻璃模型进行路径追踪，将其置于真实背景图像上，系统化变化玻璃材质、相机参数与后期效果，得到物理准确的反射场景；2) 为利用大型多模态模型（LMM），将多层图像拼接为单一复合输入，进行联合描述（captioning）；3) 采用任务特定的LoRA进行微调，避免全参训练。

Result: 在反射去除与反射/透射层分离任务上，相较现有SOTA方法取得更优性能。

Conclusion: 基于物理真实的合成数据与LMM的轻量化任务微调，可显著提升SIRR及层分离效果，证明该范式有效。

Abstract: Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.

</details>


### [85] [SceneNAT: Masked Generative Modeling for Language-Guided Indoor Scene Synthesis](https://arxiv.org/abs/2601.07218)
*Jeongjun Choi,Yeonsoo Park,H. Jin Kim*

Main category: cs.CV

TL;DR: SceneNAT 是一种单阶段、掩码式、非自回归 Transformer，可通过少量并行解码步从自然语言生成完整 3D 室内场景，在语义一致性、空间布局准确性与效率上均优于自回归与扩散式方法。


<details>
  <summary>Details</summary>
Motivation: 现有从文本到 3D 场景生成的方法多为自回归或扩散推理，生成效率低且难以同时捕获对象内部属性与对象间关系；需要一种既高效又能建模语义与空间关系的模型。

Method: 将场景的语义与空间属性完全离散化，并以掩码建模训练；在属性级与实例级同时施加掩码以学习物体内外部结构；采用少量并行解码步进行非自回归生成；引入可学习关系查询，并通过三元组（主语-谓语-宾语）预测器对布局与对象关系进行稀疏符号化建模。

Result: 在 3D-FRONT 数据集上，相比 SOTA 的自回归与扩散基线，SceneNAT 在语义符合度与空间排列精度上取得更好结果，同时计算成本显著更低。

Conclusion: 掩码式非自回归与显式三元组关系建模的结合，可高效且高质量地从文本合成完整 3D 室内场景，优于现有主流方法。

Abstract: We present SceneNAT, a single-stage masked non-autoregressive Transformer that synthesizes complete 3D indoor scenes from natural language instructions through only a few parallel decoding passes, offering improved performance and efficiency compared to prior state-of-the-art approaches. SceneNAT is trained via masked modeling over fully discretized representations of both semantic and spatial attributes. By applying a masking strategy at both the attribute level and the instance level, the model can better capture intra-object and inter-object structure. To boost relational reasoning, SceneNAT employs a dedicated triplet predictor for modeling the scene's layout and object relationships by mapping a set of learnable relation queries to a sparse set of symbolic triplets (subject, predicate, object). Extensive experiments on the 3D-FRONT dataset demonstrate that SceneNAT achieves superior performance compared to state-of-the-art autoregressive and diffusion baselines in both semantic compliance and spatial arrangement accuracy, while operating with substantially lower computational cost.

</details>


### [86] [VENUS: Visual Editing with Noise Inversion Using Scene Graphs](https://arxiv.org/abs/2601.07219)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: VENUS 提出一种无需训练的场景图引导图像编辑框架，通过分离目标与背景的提示条件并结合噪声反演，显著提升背景保真与语义一致性，同时将推理时间从分钟级降至数十秒，并在多个基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像编辑常在保留背景与实现语义编辑间失衡；基于场景图的方法更可控，但依赖微调，计算代价高、可扩展性差。需要一种既可控又无需额外训练、能更好保留未编辑区域的方案。

Method: 提出 VENUS：1) 分裂式提示条件，将编辑目标与背景上下文解耦；2) 引入噪声反演以在扩散过程中锁定未编辑区域的保真度；3) 利用多模态大语言模型提取场景图并与扩散模型对接，无需微调或额外训练。

Result: 在 PIE-Bench：PSNR 由 22.45→24.80，SSIM 0.79→0.84，LPIPS 0.100→0.070，CLIP 相似度 24.19→24.97；在 EditVal：DINO 得分 0.87，单图运行时从 6–10 分钟降至 20–30 秒；同时优于 LEDIT++ 与 P2P+DirInv 等强文本编辑基线。

Conclusion: VENUS 在无需训练的前提下，实现对场景图引导编辑的高保真与高语义一致性，兼具效率与可扩展性，并在场景图与文本两类编辑范式上均取得一致领先。

Abstract: State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.

</details>


### [87] [Language-Grounded Multi-Domain Image Translation via Semantic Difference Guidance](https://arxiv.org/abs/2601.07221)
*Jongwon Ryu,Joonhyung Park,Jaeho Han,Yeong-Seok Kim,Hye-rin Kim,Sunjae Yoon,Junyeong Kim*

Main category: cs.CV

TL;DR: 提出LACE用于多域图像到图像翻译：用GLIP-Adapter保持结构一致性，用多域控制引导将语言差异映射为按属性的翻译向量，实现可组合、可调强度的属性级控制；在CelebA(Dialog)与BDD100K上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多域图像翻译需把自然语言中的语义差异准确落地到视觉变化，同时保持与无关内容的结构一致。但现有方法在多域场景下难以兼顾结构保真与细粒度属性控制，尤其是多属性独立调节与可解释性不足。

Method: 提出LACE，包含两部分：1) GLIP-Adapter，将全局语义与局部结构特征融合，提升跨域一致性与结构保留；2) 多域控制引导（Multi-Domain Control Guidance），显式计算源与目标文本提示语之间的语义“增量”，并将其分解为逐属性的翻译向量，实现语言语义与域级视觉变化对齐，并可对每个属性独立调强度，支持组合控制。

Result: 在CelebA(Dialog)与BDD100K数据集上，LACE获得更高的视觉保真度与结构保持度，且实现了可解释的域/属性级控制，整体性能超过现有基线。

Conclusion: LACE有效桥接语言语义与可控视觉翻译，提供细粒度、多属性、可组合的多域图像编辑能力，并在保持结构一致性的同时提升可控性与可解释性。

Abstract: Multi-domain image-to-image translation re quires grounding semantic differences ex pressed in natural language prompts into corresponding visual transformations, while preserving unrelated structural and seman tic content. Existing methods struggle to maintain structural integrity and provide fine grained, attribute-specific control, especially when multiple domains are involved. We propose LACE (Language-grounded Attribute Controllable Translation), built on two compo nents: (1) a GLIP-Adapter that fuses global semantics with local structural features to pre serve consistency, and (2) a Multi-Domain Control Guidance mechanism that explicitly grounds the semantic delta between source and target prompts into per-attribute translation vec tors, aligning linguistic semantics with domain level visual changes. Together, these modules enable compositional multi-domain control with independent strength modulation for each attribute. Experiments on CelebA(Dialog) and BDD100K demonstrate that LACE achieves high visual fidelity, structural preservation, and interpretable domain-specific control, surpass ing prior baselines. This positions LACE as a cross-modal content generation framework bridging language semantics and controllable visual translation.

</details>


### [88] [Universal Adversarial Purification with DDIM Metric Loss for Stable Diffusion](https://arxiv.org/abs/2601.07253)
*Li Zheng,Liangbin Xie,Jiantao Zhou,He YiMin*

Main category: cs.CV

TL;DR: 提出UDAP用于稳定扩散的对抗净化，通过DDIM反演的重建差异进行自适应优化，配合动态轮次调整，能高效去除针对VAE、UNet及混合攻击的噪声，并跨版本与文本提示泛化。


<details>
  <summary>Details</summary>
Motivation: 现有对抗净化方法多面向分类模型，无法应对Stable Diffusion特有的攻击面（VAE编码器、UNet去噪器或二者）。需要一种专门面向生成式扩散模型的净化方案，提升在被污染训练数据或输入下的鲁棒性与实用性。

Method: 提出Universal Diffusion Adversarial Purification（UDAP）。核心思想：利用干净与对抗样本在DDIM反演与重建过程中的行为差异，定义并最小化DDIM度量损失以优化净化；并引入基于重建误差的动态epoch调整策略，在保证净化质量前提下减少迭代次数，提高效率。

Result: 在多种针对SD的攻击上表现稳健，包括PID（VAE定向）、Anti-DreamBooth（UNet定向）、MIST（混合）、以及更强鲁棒变体如Anti-DF与MetaCloak。实验显示UDAP能有效去除对抗噪声，并在不同SD版本与多种文本提示下保持性能。

Conclusion: UDAP为稳定扩散提供了一种通用且高效的对抗净化框架，能抵御多种定向与混合攻击，具备良好的跨版本与跨提示泛化能力，适用于实际场景。

Abstract: Stable Diffusion (SD) often produces degraded outputs when the training dataset contains adversarial noise. Adversarial purification offers a promising solution by removing adversarial noise from contaminated data. However, existing purification methods are primarily designed for classification tasks and fail to address SD-specific adversarial strategies, such as attacks targeting the VAE encoder, UNet denoiser, or both. To address the gap in SD security, we propose Universal Diffusion Adversarial Purification (UDAP), a novel framework tailored for defending adversarial attacks targeting SD models. UDAP leverages the distinct reconstruction behaviors of clean and adversarial images during Denoising Diffusion Implicit Models (DDIM) inversion to optimize the purification process. By minimizing the DDIM metric loss, UDAP can effectively remove adversarial noise. Additionally, we introduce a dynamic epoch adjustment strategy that adapts optimization iterations based on reconstruction errors, significantly improving efficiency without sacrificing purification quality. Experiments demonstrate UDAP's robustness against diverse adversarial methods, including PID (VAE-targeted), Anti-DreamBooth (UNet-targeted), MIST (hybrid), and robustness-enhanced variants like Anti-Diffusion (Anti-DF) and MetaCloak. UDAP also generalizes well across SD versions and text prompts, showcasing its practical applicability in real-world scenarios.

</details>


### [89] [From Landslide Conditioning Factors to Satellite Embeddings: Evaluating the Utilisation of Google AlphaEarth for Landslide Susceptibility Mapping using Deep Learning](https://arxiv.org/abs/2601.07268)
*Yusen Cheng,Qinfeng Zhu,Lei Fan*

Main category: cs.CV

TL;DR: 研究评估Google AlphaEarth嵌入作为滑坡易发性制图的替代预测因子，跨三地区与多深度学习模型比较，发现AE嵌入（尤其64波段全量）相较传统致灾因子显著提升F1与AUC，并呈更清晰空间对应与更稳定误差；时间对齐度影响效果。


<details>
  <summary>Details</summary>
Motivation: 传统滑坡致灾因子（LCFs）来源异质、可得性不均、预处理不确定性大，限制了数据驱动LSM的可靠性。AlphaEarth提供从多源地理观测学得的统一表征，可能减少特征工程负担、提升泛化与信息密度，值得系统检验其在LSM中的效用。

Method: 在台湾南投、香港、意大利艾米利亚-罗马涅三个区域，使用三种深度模型（CNN1D、CNN2D、ViT），对比两种AE表征（主成分保留集与完整64波段）与传统LCFs。通过多指标评估：F1、ROC-AUC、误差统计与空间格局一致性分析，并考察时间对齐对性能的影响。

Result: AE特征在所有区域与模型下均优于LCFs：F1提升约4%–15%，AUC提升0.04–0.11；误差分布更稳定，空间上与已观测滑坡更吻合，对局地易滑条件更敏感。使用完整64波段优于仅用主成分。改进在南投与艾米利亚更明显，香港相对较小；时间上AE与滑坡清单越对齐，效果越好。

Conclusion: AE嵌入是标准化、信息丰富的LSM预测因子，可替代传统LCFs并减少特征工程，尤其建议使用全64波段；同时需关注与滑坡清单的时间匹配以最大化增益。

Abstract: Data-driven landslide susceptibility mapping (LSM) typically relies on landslide conditioning factors (LCFs), whose availability, heterogeneity, and preprocessing-related uncertainties can constrain mapping reliability. Recently, Google AlphaEarth (AE) embeddings, derived from multi-source geospatial observations, have emerged as a unified representation of Earth surface conditions. This study evaluated the potential of AE embeddings as alternative predictors for LSM. Two AE representations, including retained principal components and the full set of 64 embedding bands, were systematically compared with conventional LCFs across three study areas (Nantou County, Taiwan; Hong Kong; and part of Emilia-Romagna, Italy) using three deep learning models (CNN1D, CNN2D, and Vision Transformer). Performance was assessed using multiple evaluation metrics, ROC-AUC analysis, error statistics, and spatial pattern assessment. Results showed that AE-based models consistently outperformed LCFs across all regions and models, yielding higher F1-scores, AUC values, and more stable error distributions. Such improvement was most pronounced when using the full 64-band AE representation, with F1-score improvements of approximately 4% to 15% and AUC increased ranging from 0.04 to 0.11, depending on the study area and model. AE-based susceptibility maps also exhibited clearer spatial correspondence with observed landslide occurrences and enhanced sensitivity to localised landslide-prone conditions. Performance improvements were more evident in Nantou and Emilia than in Hong Kong, revealing that closer temporal alignment between AE embeddings and landslide inventories may lead to more effective LSM outcomes. These findings highlight the strong potential of AE embeddings as a standardised and information-rich alternative to conventional LCFs for LSM.

</details>


### [90] [PALUM: Part-based Attention Learning for Unified Motion Retargeting](https://arxiv.org/abs/2601.07272)
*Siqi Liu,Maoyu Wang,Bo Dai,Cewu Lu*

Main category: cs.CV

TL;DR: PALUM提出一种与骨架拓扑无关的动作重定向方法：按语义身体部位分区并用注意力建模时空关系，结合循环一致性，能在多样骨架间保持语义与真实感，泛化到未见组合。


<details>
  <summary>Details</summary>
Motivation: 不同角色骨架差异大（关节数量/拓扑不同）时，传统重定向难以保持动作语义与质量，需要能跨骨架学习通用表示的方法。

Method: 将关节按语义身体部位划分，基于注意力机制学习跨部位的时空关系以形成骨架无关的运动表示；在迁移到目标骨架时融合目标骨架的结构信息；引入循环一致性约束以在往返映射中保持语义一致与稳定训练。

Result: 在多种骨架结构与未见的骨架-动作组合上，实验显示其在动作真实感与语义保真方面优于现有方法。

Conclusion: 基于部位分区与注意力的骨架无关表示结合循环一致性，可更稳健地在异构骨架间进行动作重定向；方法具有强泛化性，代码将开源。

Abstract: Retargeting motion between characters with different skeleton structures is a fundamental challenge in computer animation. When source and target characters have vastly different bone arrangements, maintaining the original motion's semantics and quality becomes increasingly difficult. We present PALUM, a novel approach that learns common motion representations across diverse skeleton topologies by partitioning joints into semantic body parts and applying attention mechanisms to capture spatio-temporal relationships. Our method transfers motion to target skeletons by leveraging these skeleton-agnostic representations alongside target-specific structural information. To ensure robust learning and preserve motion fidelity, we introduce a cycle consistency mechanism that maintains semantic coherence throughout the retargeting process. Extensive experiments demonstrate superior performance in handling diverse skeletal structures while maintaining motion realism and semantic fidelity, even when generalizing to previously unseen skeleton-motion combinations. We will make our implementation publicly available to support future research.

</details>


### [91] [GenDet: Painting Colored Bounding Boxes on Images via Diffusion Model for Object Detection](https://arxiv.org/abs/2601.07273)
*Chen Min,Chengyang Li,Fanjie Kong,Qi Zhu,Dawei Zhao,Liang Xiao*

Main category: cs.CV

TL;DR: GenDet把目标检测重构为“生成”问题：在图像条件下直接生成带类别语义的框，基于Stable Diffusion的条件生成架构，在潜空间施加语义与几何约束，实验证明精度可与判别式检测器相当且保留生成式灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测多为判别式（回归/分类）范式，难以与生成式模型统一，而且生成式模型在视觉理解任务中的可控性与可用性尚未充分挖掘；需要一种方法桥接生成与判别、在保持生成灵活性的同时实现高质量检测。

Method: 以Stable Diffusion为基础，构建条件生成框架：输入图像作为条件，在潜空间引入语义与几何约束，直接生成包含位置（边界框）与类别的输出；实现对框位置和类别属性的可控生成，并在原图像空间对齐。

Result: 系统实验证明，在标准基准上达到与主流判别式检测器有竞争力的精度，同时展现生成式方法的可控与灵活特性。

Conclusion: GenDet有效将生成式扩散模型用于目标检测，实现生成-判别任务的桥接，为统一的视觉理解系统提供新思路；在保持灵活性的同时取得接近SOTA的检测表现。

Abstract: This paper presents GenDet, a novel framework that redefines object detection as an image generation task. In contrast to traditional approaches, GenDet adopts a pioneering approach by leveraging generative modeling: it conditions on the input image and directly generates bounding boxes with semantic annotations in the original image space. GenDet establishes a conditional generation architecture built upon the large-scale pre-trained Stable Diffusion model, formulating the detection task as semantic constraints within the latent space. It enables precise control over bounding box positions and category attributes, while preserving the flexibility of the generative model. This novel methodology effectively bridges the gap between generative models and discriminative tasks, providing a fresh perspective for constructing unified visual understanding systems. Systematic experiments demonstrate that GenDet achieves competitive accuracy compared to discriminative detectors, while retaining the flexibility characteristic of generative methods.

</details>


### [92] [Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models](https://arxiv.org/abs/2601.07287)
*Yuanyang Yin,Yufan Deng,Shenghai Yuan,Kaipeng Zhang,Xiao Yang,Feng Zhao*

Main category: cs.CV

TL;DR: 提出“焦点引导”(Focal Guidance, FG)以缓解I2V扩散Transformer中语义弱层对文本的脱钩问题，通过细粒度语义引导与注意力缓存，将文本语义显式注入弱层，显著提升指令遵循与文本贴合度，并建立新基准验证通用性。


<details>
  <summary>Details</summary>
Motivation: I2V需要同时满足参考图像的高频视觉一致性与文本提示的低频语义约束，现有方法偏重视觉一致性，文本遵循不足。作者观察到DiT中存在“语义弱层”，其文本-视觉相似度下降，源于条件隔离：视觉注意与文本条件部分脱耦、过度依赖视觉先验。

Method: 提出Focal Guidance：1) 细粒度语义引导（FSG）：用CLIP从参考帧中定位关键区域，作为锚点引导语义弱层的更新；2) 注意力缓存（Attention Cache）：把语义响应更强的层的注意力图迁移到语义弱层，显式注入语义信号，缓解对视觉先验的过度依赖。同时构建I2V指令遵循评测基准。

Result: 在新基准上，FG在Wan2.1-I2V上总分提升至0.7250（+3.97%），在MMDiT架构的HunyuanVideo-I2V上提升至0.5571（+7.44%），显示有效性与可迁移性。

Conclusion: 通过针对语义弱层的焦点引导，I2V模型在保持视觉一致性的同时显著增强对文本指令的遵循；方法通用、可与不同DiT架构配合，并提供了评测基准以推动该方向研究。

Abstract: The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\%).

</details>


### [93] [VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding](https://arxiv.org/abs/2601.07290)
*Jiapeng Shi,Junke Wang,Zuyao You,Bo He,Zuxuan Wu*

Main category: cs.CV

TL;DR: 提出VideoLoom，一个统一的Video LLM，用于联合空间-时间理解；构建LoomData-8.7k数据集与LoomBench基准；在多项空间与时间任务上达SOTA或强竞争表现。


<details>
  <summary>Details</summary>
Motivation: 现有Video LLM在细粒度时空定位（像素级目标与精确时间段）方面不足，缺乏同时评估空间与时间能力的统一数据与基准。需要一个既能学习又能全面评测的套件，推动视频多模态智能的联合时空理解。

Method: 1) 设计VideoLoom统一架构，面向时空联合理解与定位；2) 构建LoomData-8.7k：以人为中心，带有时间锚定与空间定位标注的字幕/指令数据；3) 训练并在多任务上评测；4) 提出LoomBench，包含时间、空间及组合理解的视频问答对，用于系统化评估。

Result: 在多项基准上达SOTA或强竞争：例如ReVOS指代视频目标分割J&F=63.1；Charades-STA时间定位R1@0.7=48.3；在多维度时空任务上表现优异。

Conclusion: VideoLoom结合专门数据与基准，实现统一的细粒度时空视频理解框架，提供通用有效的训练与评测套件，为多模态智能设定新标准。

Abstract: This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.

</details>


### [94] [A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model](https://arxiv.org/abs/2601.07291)
*Qi Zheng,Shuliang Liu,Yu Huang,Sihang Jia,Jungang Li,Lyuhao Chen,Junhao Chen,Hanqian Li,Aiwei Liu,Yibo Yan,Xuming Hu*

Main category: cs.CV

TL;DR: VISA-Mark是一种面向LVLM的视觉语义自适应水印：用轻量前缀调优提取视觉证据权重，聚焦视觉支持的token进行自适应词表划分与logits扰动，从而在保持视觉/语义一致性的同时，实现高检测率与强抗攻击且几乎无额外时延。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM水印要么与视觉无关，向生成序列注入与图像无关的随机偏置，破坏视觉对齐与定位；要么通过拒绝采样等语义感知策略实现，但带来显著推理延迟。需要一种既能与视觉证据对齐、保证视觉/语义保真，又具备高检测率与低开销的水印方案。

Method: 提出VISA-Mark：训练一个轻量prefix-tuner，从图像条件下为候选token计算动态“视觉证据权重”（Visual-Evidence Weights, VEW）；据此执行自适应词表划分，将水印强度集中到视觉支持更强的token上，并对其logits进行有控制的扰动，从而将可检测信号嵌入到与图像一致的生成中。

Result: 在多项评测中，相比传统方法，视觉一致性提升7.8%（Chair-I），语义保真度更优；检测AUC达96.88%，在攻击场景下保持99.3%的鲁棒性；同时几乎不牺牲推理效率。

Conclusion: 通过将水印与视觉证据对齐，VISA-Mark在不损害视觉/语义保真的前提下实现高可检、强鲁棒、低开销的多模态水印，为可靠性优先的LVLM水印设定了新基线。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases, while some semantic-aware methods incur prohibitive inference latency due to rejection sampling. In this paper, we propose the VIsual Semantic Adaptive Watermark (VISA-Mark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. Our approach employs a lightweight, efficiently trained prefix-tuner to extract dynamic Visual-Evidence Weights, which quantify the evidentiary support for candidate tokens based on the visual input. These weights guide an adaptive vocabulary partitioning and logits perturbation mechanism, concentrating watermark strength specifically on visually-supported tokens. By actively aligning the watermark with visual evidence, VISA-Mark effectively maintains visual fidelity. Empirical results confirm that VISA-Mark outperforms conventional methods with a 7.8% improvement in visual consistency (Chair-I) and superior semantic fidelity. The framework maintains highly competitive detection accuracy (96.88% AUC) and robust attack resilience (99.3%) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multimodal watermarking.

</details>


### [95] [Inference-Time Scaling for Visual AutoRegressive modeling by Searching Representative Samples](https://arxiv.org/abs/2601.07293)
*Weidong Tang,Xinyan Wan,Siyu Li,Xiumei Wang*

Main category: cs.CV

TL;DR: 提出VAR-Scaling：在向量量化视觉自回归模型上实现推理时刻缩放（inference-time scaling），通过将离散采样空间映射到近似连续的特征空间并采用密度自适应混合采样（Top-k与Random-k），显著提升生成质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 推理时刻缩放已在LLM与扩散模型中显著改善生成质量，但在VQ视觉自回归（VAR）模型中尚无通用方案，主要难点是离散潜空间无法进行连续路径搜索与有效分布导航。

Method: 1) 观察VAR缩放呈两类模式：早期“通用模式”和后期“特定模式”，后者条件性优化前者；2) 用核密度估计将离散采样映射到准连续特征空间，高密度样本近似稳定高质量解；3) 提出密度自适应混合采样：在高密度区用Top-k保真，在低密度区用Random-k维持多样性，防止过早收敛；4) 在关键尺度对采样进行优化。

Result: 在类条件与文生图任务的实验中，采用VAR-Scaling的推理过程显著提升生成质量（保真度与多样性）与稳定性。

Conclusion: 通过KDE驱动的准连续空间映射与密度自适应混合采样，VAR-Scaling为VQ型VAR提供了首个通用的推理时刻缩放框架，能在不改动训练的前提下系统提升输出质量。

Abstract: While inference-time scaling has significantly enhanced generative quality in large language and diffusion models, its application to vector-quantized (VQ) visual autoregressive modeling (VAR) remains unexplored. We introduce VAR-Scaling, the first general framework for inference-time scaling in VAR, addressing the critical challenge of discrete latent spaces that prohibit continuous path search. We find that VAR scales exhibit two distinct pattern types: general patterns and specific patterns, where later-stage specific patterns conditionally optimize early-stage general patterns. To overcome the discrete latent space barrier in VQ models, we map sampling spaces to quasi-continuous feature spaces via kernel density estimation (KDE), where high-density samples approximate stable, high-quality solutions. This transformation enables effective navigation of sampling distributions. We propose a density-adaptive hybrid sampling strategy: Top-k sampling focuses on high-density regions to preserve quality near distribution modes, while Random-k sampling explores low-density areas to maintain diversity and prevent premature convergence. Consequently, VAR-Scaling optimizes sample fidelity at critical scales to enhance output quality. Experiments in class-conditional and text-to-image evaluations demonstrate significant improvements in inference process. The code is available at https://github.com/WD7ang/VAR-Scaling.

</details>


### [96] [Mimic Human Cognition, Master Multi-Image Reasoning: A Meta-Action Framework for Enhanced Visual Understanding](https://arxiv.org/abs/2601.07298)
*Jianghao Yin,Qingbin Li,Kun Sun,Cheng Ding,Jie Wang,Qin Chen,Jie Zhou,Nan Wang,Changqing Li,Pei Wu,Jian Xu,Zheming Yang,Liang He*

Main category: cs.CV

TL;DR: 提出CINEMA框架，将多图推理分解为五个元动作（Global、Focus、Hint、Think、Answer），并结合检索树采样冷启动与两阶段强化学习策略，构建~115k数据，在多图、视频与单图基准上取得SOTA，超越GPT-4o于MUIR与MVMath。


<details>
  <summary>Details</summary>
Motivation: MLLM在单图理解强，但多图推理明显退化，因跨图关系复杂、关键信息分散。作者受人类分步认知启发，试图通过结构化的认知过程与训练范式弥补这一缺口。

Method: 1) 认知式元动作CINEMA：按Global→Focus→Hint→Think→Answer序列显式建模推理步骤；2) 冷启动：检索式树采样生成高质量元动作轨迹以注入推理模式；3) 强化学习：两阶段——探索期用多样性保持策略防止熵塌缩；利用期采用退火的DAPO逐步加强利用；4) 数据：57k冷启动+58k RL实例，覆盖多图、多帧与单图任务。

Result: 在多图推理、视频理解、与单图基准上取得竞争性SOTA；在MUIR与MVMath上超过GPT-4o，并显著优于专用视频推理模型。

Conclusion: 将人类认知流程结构化为可学习的元动作，并配合检索增强的冷启动与两阶段RL，可有效提升MLLM的多图/多帧推理泛化与性能，具备跨任务通用性。

Abstract: While Multimodal Large Language Models (MLLMs) excel at single-image understanding, they exhibit significantly degraded performance in multi-image reasoning scenarios. Multi-image reasoning presents fundamental challenges including complex inter-relationships between images and scattered critical information across image sets. Inspired by human cognitive processes, we propose the Cognition-Inspired Meta-Action Framework (CINEMA), a novel approach that decomposes multi-image reasoning into five structured meta-actions: Global, Focus, Hint, Think, and Answer which explicitly modeling the sequential cognitive steps humans naturally employ. For cold-start training, we introduce a Retrieval-Based Tree Sampling strategy that generates high-quality meta-action trajectories to bootstrap the model with reasoning patterns. During reinforcement learning, we adopt a two-stage paradigm: an exploration phase with Diversity-Preserving Strategy to avoid entropy collapse, followed by an annealed exploitation phase with DAPO to gradually strengthen exploitation. To train our model, we construct a dataset of 57k cold-start and 58k reinforcement learning instances spanning multi-image, multi-frame, and single-image tasks. We conduct extensive evaluations on multi-image reasoning benchmarks, video understanding benchmarks, and single-image benchmarks, achieving competitive state-of-the-art performance on several key benchmarks. Our model surpasses GPT-4o on the MUIR and MVMath benchmarks and notably outperforms specialized video reasoning models on video understanding benchmarks, demonstrating the effectiveness and generalizability of our human cognition-inspired reasoning framework.

</details>


### [97] [Revisiting the Ordering of Channel and Spatial Attention: A Comprehensive Study on Sequential and Parallel Designs](https://arxiv.org/abs/2601.07310)
*Zhongming Liu,Bingbing Jiang*

Main category: cs.CV

TL;DR: 系统化比较通道注意力与空间注意力的融合拓扑，提出按数据规模选型的实证法则与构建指南。


<details>
  <summary>Details</summary>
Motivation: 现有通道/空间注意力融合多凭经验选顺序或并联，缺乏统一框架与系统性对比，难以为不同数据规模与任务提供可复用原则。

Method: 在统一评测框架下构建四大类共18种拓扑（串行、并行、多尺度、残差），覆盖不同通道-空间组合与顺序；在2个视觉与9个医疗数据集上进行对比，分析性能随数据规模与任务类型的耦合规律，并考察可学习并联、动态门控、以及残差连接等设计。

Result: 发现“数据规模—方法—性能”耦合规律：1) 小样本任务中，通道 + 多尺度空间串联最佳；2) 中等规模任务中，并联的可学习融合优于其他；3) 大规模任务中，带动态门控的并联结构最优。另：细粒度分类中“空间→通道”顺序更稳健且效果更好；残差连接在各规模下均有助于缓解梯度消失。

Conclusion: 提出面向场景的注意力模块选型指南：小样本用通道-多尺度空间串联；中规模用并联可学习融合；大规模用并联+动态门控；细粒度偏好空间后通道；普遍加入残差以稳训练，并开放代码供复现。

Abstract: Attention mechanisms have become a core component of deep learning models, with Channel Attention and Spatial Attention being the two most representative architectures. Current research on their fusion strategies primarily bifurcates into sequential and parallel paradigms, yet the selection process remains largely empirical, lacking systematic analysis and unified principles. We systematically compare channel-spatial attention combinations under a unified framework, building an evaluation suite of 18 topologies across four classes: sequential, parallel, multi-scale, and residual. Across two vision and nine medical datasets, we uncover a "data scale-method-performance" coupling law: (1) in few-shot tasks, the "Channel-Multi-scale Spatial" cascaded structure achieves optimal performance; (2) in medium-scale tasks, parallel learnable fusion architectures demonstrate superior results; (3) in large-scale tasks, parallel structures with dynamic gating yield the best performance. Additionally, experiments indicate that the "Spatial-Channel" order is more stable and effective for fine-grained classification, while residual connections mitigate vanishing gradient problems across varying data scales. We thus propose scenario-based guidelines for building future attention modules. Code is open-sourced at https://github.com/DWlzm.

</details>


### [98] [OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image](https://arxiv.org/abs/2601.07333)
*Tessa Pulli,Jean-Baptiste Weibel,Peter Hönig,Matthias Hirschmanner,Markus Vincze,Andreas Holzinger*

Main category: cs.CV

TL;DR: 提出OSCAR：从语言提示与单张图像在开放集无标注3D库中检索匹配CAD模型，并用于零样本6D位姿估计的自动建模获取。两阶段：CLIP文本过滤+ DINOv2图像细化；上游用GroundedSAM分割ROI、数据库多视渲染并由图像描述标注。跨域检索SOTA，YCB-V检索AP达90.48%，与Megapose结合优于重建式基线。


<details>
  <summary>Details</summary>
Motivation: 现实应用中目标集合不断变化，零样本位姿估计依赖CAD模型，但部署后往往难以获得精确实例模型，且庞大无标注库中难以锁定目标。需要一种无需训练、能从开放集数据库中依据语言与单张图像快速找出最相似模型的方法，以支撑后续6D位姿估计。

Method: 训练免方法OSCAR：1) 上线阶段对数据库3D模型多视角渲染，并用图像描述模型生成文本caption作为语义标签。2) 推理时用GroundedSAM基于语言提示在输入图像中检测并分割ROI；3) 计算ROI与数据库caption的多模态嵌入，用CLIP进行第一阶段文本检索筛选候选；4) 对候选进行基于DINOv2的图像相似度细化，选择外观最相似模型；5) 将检索到的最相似模型直接用于6D位姿估计（如Megapose）。

Result: 在MI3DOR跨域3D模型检索基准上超过所有现有方法；在YCB-V数据集上对象检索平均精度90.48%。将最相似模型接入Megapose进行位姿估计，性能优于基于重建的对照方法。

Conclusion: OSCAR在无需额外训练和精确实例模型的条件下，实现从开放集无标注库中高效检索可替代CAD模型，并可直接提升零样本6D位姿估计流程，具有实用性与可扩展性。

Abstract: 6D object pose estimation plays a crucial role in scene understanding for applications such as robotics and augmented reality. To support the needs of ever-changing object sets in such context, modern zero-shot object pose estimators were developed to not require object-specific training but only rely on CAD models. Such models are hard to obtain once deployed, and a continuously changing and growing set of objects makes it harder to reliably identify the instance model of interest. To address this challenge, we introduce an Open-Set CAD Retrieval from a Language Prompt and a Single Image (OSCAR), a novel training-free method that retrieves a matching object model from an unlabeled 3D object database. During onboarding, OSCAR generates multi-view renderings of database models and annotates them with descriptive captions using an image captioning model. At inference, GroundedSAM detects the queried object in the input image, and multi-modal embeddings are computed for both the Region-of-Interest and the database captions. OSCAR employs a two-stage retrieval: text-based filtering using CLIP identifies candidate models, followed by image-based refinement using DINOv2 to select the most visually similar object. In our experiments we demonstrate that OSCAR outperforms all state-of-the-art methods on the cross-domain 3D model retrieval benchmark MI3DOR. Furthermore, we demonstrate OSCAR's direct applicability in automating object model sourcing for 6D object pose estimation. We propose using the most similar object model for pose estimation if the exact instance is not available and show that OSCAR achieves an average precision of 90.48\% during object retrieval on the YCB-V object dataset. Moreover, we demonstrate that the most similar object model can be utilized for pose estimation using Megapose achieving better results than a reconstruction-based approach.

</details>


### [99] [Reconstruction Guided Few-shot Network For Remote Sensing Image Classification](https://arxiv.org/abs/2601.07335)
*Mohit Jaiswal,Naman Jain,Shivani Pathak,Mainak Singha,Nikunja Bihari Kar,Ankit Jha,Biplab Banerjee*

Main category: cs.CV

TL;DR: 提出RGFS-Net，通过遮挡重建辅助任务提升遥感图像小样本分类泛化与一致性；在EuroSAT与PatternNet上于1/5-shot均优于基线，方法简单且兼容常见骨干。


<details>
  <summary>Details</summary>
Motivation: 小样本遥感分类样本稀缺、地物类别变化大，现有方法难以在未见类上泛化且易丢失已见类一致性，需要在少标注下学习更具语义和空间理解的特征。

Method: 在标准少样本框架中引入重建引导：对输入图像进行遮挡（masked）并要求网络重建缺失区域，作为辅助任务共同训练分类器。该任务促使特征学习语义丰富与空间结构信息，从而提升类间可分性与对未见类的泛化。模型简单、可插拔，兼容常见骨干网络。

Result: 在EuroSAT和PatternNet数据集的1-shot与5-shot协议下，RGFS-Net均稳定超越现有基线方法（摘要未给出具体数值）。

Conclusion: 重建引导的辅助学习能在低数据场景提升遥感图像Few-shot分类的泛化与判别能力；方法高效、易用、可与标准骨干结合，具有实际应用价值。

Abstract: Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.

</details>


### [100] [PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis](https://arxiv.org/abs/2601.07344)
*Jiao Xu,Junwei Liu,Jiangwei Lao,Qi Zhu,Yunpeng Zhao,Congyun Jin,Shinan Liu,Zhihong Lu,Lihe Zhang,Xin Chen,Jian Wang,Ping Wang*

Main category: cs.CV

TL;DR: PulseMind提出一套面向真实临床多模态诊断的完整方案：大规模多轮问诊与影像数据集MediScope、四维度评测基准、以及以比较反馈为核心的CRPO训练框架，在多项基准上表现竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态模型多聚焦单一影像场景（皮肤、病理、放射），无法覆盖真实临床的异构输入与多轮交互需求。需要一个同时考虑多模态、多轮对话、并与人类诊断过程对齐的模型与评测体系。

Method: 1) 构建MediScope数据集：98,000例真实多轮会诊、601,500张医疗图像，覆盖10+科室、200+亚专科；2) 设计PulseMind Benchmark：围绕多轮问诊，给出主动性、准确性、有用性、语言质量四维评估协议；3) 提出训练框架CRPO：以相对偏好比较信号替代绝对分数奖励，进行稳定且更符合人类偏好的强化策略优化。

Result: 在自建诊疗咨询基准与公开医疗基准上，PulseMind取得竞争性结果，显示其在多模态、多轮诊断场景中的有效性与稳健性。

Conclusion: 通过数据集、评测与CRPO训练的协同设计，PulseMind更贴近真实临床诊断流程，提升多轮多模态诊断模型的实用性与人类对齐程度。

Abstract: Recent advances in medical multi-modal models focus on specialized image analysis like dermatology, pathology, or radiology. However, they do not fully capture the complexity of real-world clinical diagnostics, which involve heterogeneous inputs and require ongoing contextual understanding during patient-physician interactions. To bridge this gap, we introduce PulseMind, a new family of multi-modal diagnostic models that integrates a systematically curated dataset, a comprehensive evaluation benchmark, and a tailored training framework. Specifically, we first construct a diagnostic dataset, MediScope, which comprises 98,000 real-world multi-turn consultations and 601,500 medical images, spanning over 10 major clinical departments and more than 200 sub-specialties. Then, to better reflect the requirements of real-world clinical diagnosis, we develop the PulseMind Benchmark, a multi-turn diagnostic consultation benchmark with a four-dimensional evaluation protocol comprising proactiveness, accuracy, usefulness, and language quality. Finally, we design a training framework tailored for multi-modal clinical diagnostics, centered around a core component named Comparison-based Reinforcement Policy Optimization (CRPO). Compared to absolute score rewards, CRPO uses relative preference signals from multi-dimensional com-parisons to provide stable and human-aligned training guidance. Extensive experiments demonstrate that PulseMind achieves competitive performance on both the diagnostic consultation benchmark and public medical benchmarks.

</details>


### [101] [Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training](https://arxiv.org/abs/2601.07359)
*Shezheng Song,Shasha Li,Jie Yu*

Main category: cs.CV

TL;DR: 提出DualPD，无需训练即可通过层间对比logits与头级信息过滤，缓解MLLM“看对说错”的注意力不一致，稳健提升多模态基准上的准确率。


<details>
  <summary>Details</summary>
Motivation: MLLM在多种视觉-语言任务上强，但存在内部推理不一致：深层注意到对的区域，但早期层噪声注意会误导最终答案，导致“看对却说错”。需要在不额外训练的前提下纠正解码过程，使模型表达与其正确视觉理解对齐。

Method: 提出DualPD解码后处理策略，含两部分：1) 层级注意引导的对比logits模块：识别注意力转移最大的层间对，比较其输出logits变化，刻画对正确答案的信念演化并据此调整最终预测；2) 头级信息过滤模块：在每层内抑制对无关区域关注、贡献低的注意力头，提升注意质量。整体为无训练、推理时增强。

Result: 在LLaVA与Qwen-VL系列、多个多模态基准上，一致提升准确率，显示方法有效且具有普适性。

Conclusion: DualPD可在不训练的情况下修正注意噪声带来的表达偏差，使模型输出更契合其正确视觉关注，具有通用且高效的解码改进价值；代码将于发表时开放。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong capabilities across a variety of vision-language tasks. However, their internal reasoning often exhibits a critical inconsistency: although deeper layers may attend to the correct visual regions, final predictions are frequently misled by noisy attention from earlier layers. This results in a disconnect between what the model internally understands and what it ultimately expresses, a phenomenon we describe as seeing it right but saying it wrong. To address this issue, we propose DualPD, a dual-perspective decoding refinement strategy that enhances the visual understanding without any additional training. DualPD consists of two components. (1) The layer-wise attention-guided contrastive logits module captures how the belief in the correct answer evolves by comparing output logits between layers that exhibit the largest attention shift. (2) The head-wise information filtering module suppresses low-contribution attention heads that focus on irrelevant regions, thereby improving attention quality within each layer. Experiments conducted on both the LLaVA and Qwen-VL model families across multiple multimodal benchmarks demonstrate that DualPD consistently improves accuracy without training, confirming its effectiveness and generalizability. The code will be released upon publication.

</details>


### [102] [HiVid-Narrator: Hierarchical Video Narrative Generation with Scene-Primed ASR-anchored Compression](https://arxiv.org/abs/2601.07366)
*Haoxuan Li,Mengyan Li,Junjun Zheng*

Main category: cs.CV

TL;DR: 提出E-HVC数据集与HiVid-Narrator框架，通过“时间链式思维+章节总结”双粒度标注与SPA-Compressor高效压缩，实现电商视频的高质量、时间对齐、结构化叙事，同时减少输入token并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频讲述方法难以同时捕捉电商视频中的细粒度视觉细节并组织成高层次、连贯的故事；电商视频节奏快、信息密集，多模态token占用训练与推理成本高，亟需兼顾事实对齐、时间定位与效率的解决方案。

Method: 1) 构建E-HVC数据集：提供时间对齐的“双粒度”标注——事件级的Temporal Chain-of-Thought与章节级的Chapter Summary。采用分阶段构建：先用精选ASR与帧级描述收集可靠语义与视觉证据，再在Chain-of-Thought条件下细化章节边界与标题，获得事实支撑、时间对齐的叙事。2) 提出SPA-Compressor：以ASR语义线索引导，将多模态token压缩为分层的“场景-事件”表示，降低输入长度、保留关键信息。3) 基于上述设计实现HiVid-Narrator：在压缩表示上进行训练与推理，生成结构化、章节化的叙事。

Result: HiVid-Narrator在更少输入token的条件下，生成的叙事质量优于现有方法；叙事时间对齐、事实一致性更强。

Conclusion: 双粒度、时间锚定的数据标注配合ASR引导的多模态压缩，可显著提升电商视频叙事的质量与效率；HiVid-Narrator验证了该范式的有效性。

Abstract: Generating structured narrations for real-world e-commerce videos requires models to perceive fine-grained visual details and organize them into coherent, high-level stories--capabilities that existing approaches struggle to unify. We introduce the E-commerce Hierarchical Video Captioning (E-HVC) dataset with dual-granularity, temporally grounded annotations: a Temporal Chain-of-Thought that anchors event-level observations and Chapter Summary that compose them into concise, story-centric summaries. Rather than directly prompting chapters, we adopt a staged construction that first gathers reliable linguistic and visual evidence via curated ASR and frame-level descriptions, then refines coarse annotations into precise chapter boundaries and titles conditioned on the Temporal Chain-of-Thought, yielding fact-grounded, time-aligned narratives. We also observe that e-commerce videos are fast-paced and information-dense, with visual tokens dominating the input sequence. To enable efficient training while reducing input tokens, we propose the Scene-Primed ASR-anchored Compressor (SPA-Compressor), which compresses multimodal tokens into hierarchical scene and event representations guided by ASR semantic cues. Built upon these designs, our HiVid-Narrator framework achieves superior narrative quality with fewer input tokens compared to existing methods.

</details>


### [103] [Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation](https://arxiv.org/abs/2601.07377)
*Jiao Xu,Xin Chen,Lihe Zhang*

Main category: cs.CV

TL;DR: 提出DiCo：一种动态协作的半监督3D血管分割网络，通过动态互换师生角色、多视角融合与对抗监督，在三项基准上刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统Mean Teacher将教师/学生角色固定，假设教师恒优，但复杂3D血管数据下教师不一定更强，导致伪标签偏差与性能受限；同时单视角与标注不一致问题影响分割形状与泛化。

Method: 1) 动态协作：根据模型表现/不确定性自适应切换师生角色，缓解认知偏差；2) 多视角融合：对输入进行多视图集成，模拟临床从多角度判读；3) 对抗监督与2D投影：将3D体数据投影到2D视图，用判别器约束无标注数据的血管形状分布，减轻标注不一致带来的噪声。

Result: 在三个3D血管分割基准上获得新的SOTA表现（具体数值未给出）。

Conclusion: 动态角色切换+多视角融合+2D投影对抗监督能有效缓解半监督3D血管分割中的教师偏差与形状约束难题，显著提升泛化性能；代码已开源。

Abstract: In this paper, we present a new dynamic collaborative network for semi-supervised 3D vessel segmentation, termed DiCo. Conventional mean teacher (MT) methods typically employ a static approach, where the roles of the teacher and student models are fixed. However, due to the complexity of 3D vessel data, the teacher model may not always outperform the student model, leading to cognitive biases that can limit performance. To address this issue, we propose a dynamic collaborative network that allows the two models to dynamically switch their teacher-student roles. Additionally, we introduce a multi-view integration module to capture various perspectives of the inputs, mirroring the way doctors conduct medical analysis. We also incorporate adversarial supervision to constrain the shape of the segmented vessels in unlabeled data. In this process, the 3D volume is projected into 2D views to mitigate the impact of label inconsistencies. Experiments demonstrate that our DiCo method sets new state-of-the-art performance on three 3D vessel segmentation benchmarks. The code repository address is https://github.com/xujiaommcome/DiCo

</details>


### [104] [Forecast the Principal, Stabilize the Residual: Subspace-Aware Feature Caching for Efficient Diffusion Transformers](https://arxiv.org/abs/2601.07396)
*Guantao Chen,Shikang Zheng,Yuqi Lin,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出SVD-Cache：用SVD将DiT特征分解为主子空间与残差子空间，对主成分做EMA预测、残差直接复用，从而在几乎无损的质量下大幅加速扩散Transformer推理（最高约5.55×），且与蒸馏、量化、稀疏注意力等加速技术兼容。


<details>
  <summary>Details</summary>
Motivation: DiT推理需多步迭代，计算代价高。现有缓存方法统一处理全部特征，但作者发现特征空间中存在时间行为迥异的两个子空间：主子空间平滑可预测，残差子空间能量低且震荡难预测。若区别对待，可更高效、稳定地进行跨步复用。

Method: 1) 对每步中间特征做SVD分解，得到低秩主成分与残差；2) 对主子空间系数使用指数滑动平均（EMA）进行时间预测并缓存；3) 对残差子空间直接复用（无需复杂预测）；4) 将该缓存框架集成到多种DiT与采样器中，评估与其他加速（蒸馏、量化、稀疏注意力）兼容性。

Result: 在多种图像与视频DiT上实现近乎无损的生成质量，同时带来显著加速：在FLUX与HunyuanVideo上最高约5.55×；对不同模型与采样方法均有效，并与蒸馏、量化、稀疏注意力叠加兼容。

Conclusion: DiT特征存在可分离的主/残差子空间且时间动态不同。利用SVD分解与子空间感知的缓存策略，可对主成分做可预测复用、对残差直接缓存，从而在不显著牺牲质量的情况下显著加速推理，并可与现有加速技术协同。

Abstract: Diffusion Transformer (DiT) models have achieved unprecedented quality in image and video generation, yet their iterative sampling process remains computationally prohibitive. To accelerate inference, feature caching methods have emerged by reusing intermediate representations across timesteps. However, existing caching approaches treat all feature components uniformly. We reveal that DiT feature spaces contain distinct principal and residual subspaces with divergent temporal behavior: the principal subspace evolves smoothly and predictably, while the residual subspace exhibits volatile, low-energy oscillations that resist accurate prediction. Building on this insight, we propose SVD-Cache, a subspace-aware caching framework that decomposes diffusion features via Singular Value Decomposition (SVD), applies exponential moving average (EMA) prediction to the dominant low-rank components, and directly reuses the residual subspace. Extensive experiments demonstrate that SVD-Cache achieves near-lossless across diverse models and methods, including 5.55$\times$ speedup on FLUX and HunyuanVideo, and compatibility with model acceleration techniques including distillation, quantization and sparse attention. Our code is in supplementary material and will be released on Github.

</details>


### [105] [SDHSI-Net: Learning Better Representations for Hyperspectral Images via Self-Distillation](https://arxiv.org/abs/2601.07416)
*Prachet Dev Singh,Shyamsundar Paramasivam,Sneha Barman,Mainak Singha,Ankit Jha,Girish Mishra,Biplab Banerjee*

Main category: cs.CV

TL;DR: 将自蒸馏用于高光谱图像分类：用模型早期输出作为软标签，约束中间与最终预测一致性，提升类内紧致度与类间可分性，在两个基准数据集上显著提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: HSI具有高光谱维度与标注稀缺，传统深度模型易过拟合、计算开销大。无需外部教师的自蒸馏可降低复杂度并提升泛化，值得探索其在光谱-空间学习中的作用。

Method: 在同一网络内，将早期层/阶段的预测作为软目标，约束中间表示与最终输出的一致性；通过一致性损失增强特征分布的类内紧致与类间分离；在两个HSI基准上实现并评估。

Result: 在两个基准HSI数据集上，相比基线显著提升分类准确率与鲁棒性（具体数值未给出），证明方法有效。

Conclusion: 自蒸馏无需外部教师即可有效提升HSI光谱-空间分类性能，改善特征结构与泛化，具有较好的实用性与扩展潜力（代码已开源）。

Abstract: Hyperspectral image (HSI) classification presents unique challenges due to its high spectral dimensionality and limited labeled data. Traditional deep learning models often suffer from overfitting and high computational costs. Self-distillation (SD), a variant of knowledge distillation where a network learns from its own predictions, has recently emerged as a promising strategy to enhance model performance without requiring external teacher networks. In this work, we explore the application of SD to HSI by treating earlier outputs as soft targets, thereby enforcing consistency between intermediate and final predictions. This process improves intra-class compactness and inter-class separability in the learned feature space. Our approach is validated on two benchmark HSI datasets and demonstrates significant improvements in classification accuracy and robustness, highlighting the effectiveness of SD for spectral-spatial learning. Codes are available at https://github.com/Prachet-Dev-Singh/SDHSI.

</details>


### [106] [PanoSAMic: Panoramic Image Segmentation from SAM Feature Encoding and Dual View Fusion](https://arxiv.org/abs/2601.07447)
*Mahdi Chamseddine,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 提出PanoSAMic，将SAM编码器改造并与多模态融合与球面注意力解码器结合，用于全景图语义分割，跨RGB/RGB-D/RGB-D-N在多个数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有大模型主要在透视图上训练，难以适应全景图的畸变与边界不连续，且多模态信息难以有效利用，需一种能利用SAM先验并针对球面图像优化的分割框架。

Method: 1) 改造预训练SAM编码器，输出多阶段特征；2) 提出新型时空/空间-模态融合模块（spatio-modal fusion），按区域自适应选择各模态最优特征；3) 语义解码器使用球面注意力与双视图融合以缓解畸变与拼接边缘不连续问题。

Result: 在Stanford2D3DS上对RGB、RGB-D、RGB-D-N均达SOTA；在Matterport3D上对RGB与RGB-D达SOTA，显示优于现有全景分割方法。

Conclusion: 利用SAM的通用表征并结合球面感知的解码与自适应多模态融合，可显著提升全景语义分割性能，证明在多数据集和多模态下的有效性。

Abstract: Existing image foundation models are not optimized for spherical images having been trained primarily on perspective images. PanoSAMic integrates the pre-trained Segment Anything (SAM) encoder to make use of its extensive training and integrate it into a semantic segmentation model for panoramic images using multiple modalities. We modify the SAM encoder to output multi-stage features and introduce a novel spatio-modal fusion module that allows the model to select the relevant modalities and best features from each modality for different areas of the input. Furthermore, our semantic decoder uses spherical attention and dual view fusion to overcome the distortions and edge discontinuity often associated with panoramic images. PanoSAMic achieves state-of-the-art (SotA) results on Stanford2D3DS for RGB, RGB-D, and RGB-D-N modalities and on Matterport3D for RGB and RGB-D modalities. https://github.com/dfki-av/PanoSAMic

</details>


### [107] [Improving Video Question Answering through query-based frame selection](https://arxiv.org/abs/2601.07459)
*Himanshu Patil,Geo Jolly,Ramana Raja Buddala,Ganesh Ramakrishnan,Rohit Saluja*

Main category: cs.CV

TL;DR: 提出一种基于查询的问题相关帧选择方法，用次模互信息(SMI)挑选与问题最相关且互补的帧，替代统一抽帧，在MVBench上结合Video-LLaVA与LLaVA-NeXT显著提升VideoQA准确率，最高+4%。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA的大型视觉语言模型因算力限制通常对视频进行固定数量的均匀抽帧，但这无法保证捕捉到关键信息与上下文，导致回答不准确。需要一种能依据问题选择关键帧、在有限帧预算下最大化信息量的方法。

Method: 将均匀采样替换为基于问题的帧选择：利用次模互信息(SMI)函数，在给定问题的条件下，从视频帧集合中挑选能最大化与问题相关性且彼此互补的信息子集。该选择策略可作为前端模块接入现有VLM（如Video-LLaVA、LLaVA-NeXT），在相同帧预算下输入更有信息量的帧。

Result: 在MVBench多动作视频任务上，对比统一采样与基于查询的采样，在Video-LLaVA与LLaVA-NeXT两种VLM中均有提升，准确率最高提高约4%；定性结果显示所选帧与问题语义对齐更好。

Conclusion: 基于查询的SMI帧选择在不增加帧数和模型规模的情况下提升VideoQA表现，具有通用性，可推广到任何只使用视频子集的任务中。

Abstract: Video Question Answering (VideoQA) models enhance understanding and interaction with audiovisual content, making it more accessible, searchable, and useful for a wide range of fields such as education, surveillance, entertainment, and content creation. Due to heavy compute requirements, most large visual language models (VLMs) for VideoQA rely on a fixed number of frames by uniformly sampling the video. However, this process does not pick important frames or capture the context of the video. We present a novel query-based selection of frames relevant to the questions based on the submodular mutual Information (SMI) functions. By replacing uniform frame sampling with query-based selection, our method ensures that the chosen frames provide complementary and essential visual information for accurate VideoQA. We evaluate our approach on the MVBench dataset, which spans a diverse set of multi-action video tasks. VideoQA accuracy on this dataset was assessed using two VLMs, namely Video-LLaVA and LLaVA-NeXT, both of which originally employed uniform frame sampling. Experiments were conducted using both uniform and query-based sampling strategies. An accuracy improvement of up to \textbf{4\%} was observed when using query-based frame selection over uniform sampling. Qualitative analysis further highlights that query-based selection, using SMI functions, consistently picks frames better aligned with the question. We opine that such query-based frame selection can enhance accuracy in a wide range of tasks that rely on only a subset of video frames.

</details>


### [108] [From Sketch to Fresco: Efficient Diffusion Transformer with Progressive Resolution](https://arxiv.org/abs/2601.07462)
*Shikang Zheng,Guantao Chen,Lixuan He,Jiacheng Liu,Yuqi Lin,Chang Zou,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出Fresco：在扩散Transformer中通过动态分辨率、渐进上采样，实现几乎无损的加速（最多10×在FLUX、5×在HunyuanVideo，与蒸馏等结合可达22×），避免以往重加噪与一次性上采样导致的一致性破坏和伪影。


<details>
  <summary>Details</summary>
Motivation: 迭代采样使扩散Transformer推理代价高；动态分辨率可加速但现有方法在分辨率切换时启发式重加噪，打破跨阶段一致性，并且对整幅潜空间“一刀切”上采样，忽略已收敛区域，造成误差积累与可见伪影。

Method: 提出Fresco框架：将多阶段采样统一到同一最终目标，对齐重加噪与全局结构；采用渐进式上采样而非一次性全图上采样，依据区域收敛度逐步提升分辨率，保持低分辨率起草的效率与高分辨率细化的保真度。

Result: 在多个领域与模型上实现近乎无损的加速：FLUX达10×、HunyuanVideo达5×；与蒸馏、量化、特征缓存等正交，可叠加至22×。

Conclusion: Fresco在不牺牲质量的前提下显著加速扩散Transformer采样，通过保持跨阶段一致性与渐进上采样，缓解伪影与误差积累，并可与其他加速技术叠加。

Abstract: Diffusion Transformers achieve impressive generative quality but remain computationally expensive due to iterative sampling. Recently, dynamic resolution sampling has emerged as a promising acceleration technique by reducing the resolution of early sampling steps. However, existing methods rely on heuristic re-noising at every resolution transition, injecting noise that breaks cross-stage consistency and forces the model to relearn global structure. In addition, these methods indiscriminately upsample the entire latent space at once without checking which regions have actually converged, causing accumulated errors, and visible artifacts. Therefore, we propose \textbf{Fresco}, a dynamic resolution framework that unifies re-noise and global structure across stages with progressive upsampling, preserving both the efficiency of low-resolution drafting and the fidelity of high-resolution refinement, with all stages aligned toward the same final target. Fresco achieves near-lossless acceleration across diverse domains and models, including 10$\times$ speedup on FLUX, and 5$\times$ on HunyuanVideo, while remaining orthogonal to distillation, quantization and feature caching, reaching 22$\times$ speedup when combined with distilled models. Our code is in supplementary material and will be released on Github.

</details>


### [109] [FocalOrder: Focal Preference Optimization for Reading Order Detection](https://arxiv.org/abs/2601.07483)
*Fuyuan Liu,Dianyu Yu,He Ren,Nayu Liu,Xiaomian Kang,Delai Qiu,Fa Zhang,Genpeng Zhen,Shengping Liu,Jiaen Liang,Wei Huang,Yining Wang,Junnan Zhu*

Main category: cs.CV

TL;DR: 提出FocalOrder，通过Focal Preference Optimization聚焦难样本，解决阅读顺序检测中的“位置不均衡”问题，刷新多项基准SOTA，轻量模型胜过大型VLM。


<details>
  <summary>Details</summary>
Motivation: 现有阅读顺序方法假设各布局区域难度一致，训练被大量容易样本主导，导致模型仅在起止区域表现好，而在中间复杂区域性能崩塌（位置差异/Positional Disparity）。需要一种机制放大困难布局的学习信号。

Method: 提出FocalOrder框架：1) 自适应难度发现：用指数滑动平均跟踪与定位难学的转移对；2) 难度校准的成对排序目标（pairwise ranking），在优化中给予困难转移更高权重，强化全局逻辑一致性；整体以Focal Preference Optimization组织训练。

Result: 在OmniDocBench v1.0与Comp-HRDoc取得新的SOTA；所提紧凑模型在两基准上均超过专业基线，且显著优于大规模通用VLM。

Conclusion: 将优化与文档内在结构歧义对齐、聚焦困难转移能显著提升复杂文档的阅读顺序理解；FocalOrder验证了面向难度的偏好优化在该任务上的有效性并具备参数效率优势。

Abstract: Reading order detection is the foundation of document understanding. Most existing methods rely on uniform supervision, implicitly assuming a constant difficulty distribution across layout regions. In this work, we challenge this assumption by revealing a critical flaw: \textbf{Positional Disparity}, a phenomenon where models demonstrate mastery over the deterministic start and end regions but suffer a performance collapse in the complex intermediate sections. This degradation arises because standard training allows the massive volume of easy patterns to drown out the learning signals from difficult layouts. To address this, we propose \textbf{FocalOrder}, a framework driven by \textbf{Focal Preference Optimization (FPO)}. Specifically, FocalOrder employs adaptive difficulty discovery with exponential moving average mechanism to dynamically pinpoint hard-to-learn transitions, while introducing a difficulty-calibrated pairwise ranking objective to enforce global logical consistency. Extensive experiments demonstrate that FocalOrder establishes new state-of-the-art results on OmniDocBench v1.0 and Comp-HRDoc. Our compact model not only outperforms competitive specialized baselines but also significantly surpasses large-scale general VLMs. These results demonstrate that aligning the optimization with intrinsic structural ambiguity of documents is critical for mastering complex document structures.

</details>


### [110] [Anatomy Aware Cascade Network: Bridging Epistemic Uncertainty and Geometric Manifold for 3D Tooth Segmentation](https://arxiv.org/abs/2601.07499)
*Bing Yu,Liu Shi,Haitao Wang,Deran Qi,Xiang Cai,Wei Zhong,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出AACNet用于CBCT牙齿三维分割，通过层级粗到细和两个关键模块（AGBR与SDMAA）缓解咬合粘连导致的边界歧义，显著提升Dice和HD95并具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: CBCT牙齿分割在数字牙科流程中至关重要，但自然咬合时因低对比与上下牙弓边界不清导致粘连伪影，传统网络易丢失边界与拓扑一致性，影响临床应用。

Method: 提出解剖感知级联网络AACNet：1) 粗到细两阶段，先保障全局结构再精修边界；2) AGBR以熵为门控在高不确定过渡区进行特征校正，缓解边界歧义；3) SDMAA引入签名距离图作为几何先验，利用解剖注意力在下采样中保持拓扑与细节，避免标准池化导致的信息丢失。

Result: 在125例CBCT上获得Dice 90.17%、HD95 3.63 mm，优于SOTA；在外部数据集上HD95 2.19 mm，显示良好泛化与稳健性。

Conclusion: AACNet能在保持全局结构一致性的同时精确分割黏连边界，性能与泛化均优于现有方法，适用于临床下游如手术规划；代码已开源。

Abstract: Accurate three-dimensional (3D) tooth segmentation from Cone-Beam Computed Tomography (CBCT) is a prerequisite for digital dental workflows. However, achieving high-fidelity segmentation remains challenging due to adhesion artifacts in naturally occluded scans, which are caused by low contrast and indistinct inter-arch boundaries. To address these limitations, we propose the Anatomy Aware Cascade Network (AACNet), a coarse-to-fine framework designed to resolve boundary ambiguity while maintaining global structural consistency. Specifically, we introduce two mechanisms: the Ambiguity Gated Boundary Refiner (AGBR) and the Signed Distance Map guided Anatomical Attention (SDMAA). The AGBR employs an entropy based gating mechanism to perform targeted feature rectification in high uncertainty transition zones. Meanwhile, the SDMAA integrates implicit geometric constraints via signed distance map to enforce topological consistency, preventing the loss of spatial details associated with standard pooling. Experimental results on a dataset of 125 CBCT volumes demonstrate that AACNet achieves a Dice Similarity Coefficient of 90.17 \% and a 95\% Hausdorff Distance of 3.63 mm, significantly outperforming state-of-the-art methods. Furthermore, the model exhibits strong generalization on an external dataset with an HD95 of 2.19 mm, validating its reliability for downstream clinical applications such as surgical planning. Code for AACNet is available at https://github.com/shiliu0114/AACNet.

</details>


### [111] [Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization](https://arxiv.org/abs/2601.07518)
*Fangyu Lin,Yingdong Hu,Zhening Liu,Yufan Zhuang,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: Mon3tr提出一种单目3D远程临场系统：先离线重建用户特定的3DGS参数化人体头像，在线用单目RGB驱动，并在端侧以轻量网络对3DGS属性做动态校正，实现低带宽（<0.2 Mbps）、低时延（~80 ms）与高帧率（~60 FPS）的逼真全身远程化身。


<details>
  <summary>Details</summary>
Motivation: 现有沉浸式临场需多摄像头和大带宽体素/点云流，难以在移动端实时运行且成本高、网络鲁棒性差。需要一种既低硬件复杂度又低带宽、仍能保持高保真和低时延的单目方案。

Method: 采用“摊销计算”两阶段：1) 离线多视几何重建用户专属头像，以3D Gaussian Splatting参数化人体与外观；2) 在线阶段仅用单目RGB捕捉体态与表情，提取紧凑的运动与外观特征，经WebRTC数据通道传输（<0.2 Mbps）。接收端（如Quest 3）用轻量3DGS属性形变网络对预构建头像进行校正，实时合成照片级外观与动作。

Result: 实现约60 FPS渲染、端到端~80 ms时延；新姿态PSNR>28 dB；相较点云流媒体带宽降低>1000倍；在多场景中从单目输入获得稳定实时表现。

Conclusion: 单目驱动的3DGS参数化人体结合离线头像与在线轻量属性形变，显著降低硬件与带宽需求，同时保持高保真与低时延，推动AR/VR端上可用的实时全身远程临场。

Abstract: Immersive telepresence aims to transform human interaction in AR/VR applications by enabling lifelike full-body holographic representations for enhanced remote collaboration. However, existing systems rely on hardware-intensive multi-camera setups and demand high bandwidth for volumetric streaming, limiting their real-time performance on mobile devices. To overcome these challenges, we propose Mon3tr, a novel Monocular 3D telepresence framework that integrates 3D Gaussian splatting (3DGS) based parametric human modeling into telepresence for the first time. Mon3tr adopts an amortized computation strategy, dividing the process into a one-time offline multi-view reconstruction phase to build a user-specific avatar and a monocular online inference phase during live telepresence sessions. A single monocular RGB camera is used to capture body motions and facial expressions in real time to drive the 3DGS-based parametric human model, significantly reducing system complexity and cost. The extracted motion and appearance features are transmitted at < 0.2 Mbps over WebRTC's data channel, allowing robust adaptation to network fluctuations. On the receiver side, e.g., Meta Quest 3, we develop a lightweight 3DGS attribute deformation network to dynamically generate corrective 3DGS attribute adjustments on the pre-built avatar, synthesizing photorealistic motion and appearance at ~ 60 FPS. Extensive experiments demonstrate the state-of-the-art performance of our method, achieving a PSNR of > 28 dB for novel poses, an end-to-end latency of ~ 80 ms, and > 1000x bandwidth reduction compared to point-cloud streaming, while supporting real-time operation from monocular inputs across diverse scenarios. Our demos can be found at https://mon3tr3d.github.io.

</details>


### [112] [ViewMorpher3D: A 3D-aware Diffusion Framework for Multi-Camera Novel View Synthesis in Autonomous Driving](https://arxiv.org/abs/2601.07540)
*Farhad G. Zanjani,Hong Cai,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 提出ViewMorpher3D：用多视图条件的扩散模型对基于重建/渲染的自动驾驶多相机图像进行增强，缓解新视角伪影、补全细节并保证跨视一致性，适配可变相机与灵活参考-目标配置，在真实驾驶数据上显著提升图像质量且保持几何保真。


<details>
  <summary>Details</summary>
Motivation: 闭环仿真器需要高保真、多视一致的多视图图像以开发评估自动驾驶感知与规划；但当前以高斯泼洒等3D重建驱动的渲染在外推视角或稀疏观测下易出现伪影与细节缺失，影响仿真可靠性与下游算法表现。

Method: 构建基于图像扩散模型的多视图增强框架ViewMorpher3D：联合处理一组渲染图像，条件包含相机位姿、3D几何先验，以及时间相邻或空间重叠的参考视图；通过联合去噪/生成来补全缺失细节、抑制伪影并显式约束跨视一致性；框架可处理可变数量相机与灵活参考/目标视图配置。

Result: 在真实道路数据集上，较基线显著提升多项图像质量指标，明显减少渲染伪影，同时维持或提升几何一致性/保真度。

Conclusion: 多视图条件的扩散式增强能有效弥补3D重建渲染在自动驾驶场景中的不足，为构建高保真、可扩展的闭环仿真器提供实用路径，并适配多样传感器配置。

Abstract: Autonomous driving systems rely heavily on multi-view images to ensure accurate perception and robust decision-making. To effectively develop and evaluate perception stacks and planning algorithms, realistic closed-loop simulators are indispensable. While 3D reconstruction techniques such as Gaussian Splatting offer promising avenues for simulator construction, the rendered novel views often exhibit artifacts, particularly in extrapolated perspectives or when available observations are sparse.
  We introduce ViewMorpher3D, a multi-view image enhancement framework based on image diffusion models, designed to elevate photorealism and multi-view coherence in driving scenes. Unlike single-view approaches, ViewMorpher3D jointly processes a set of rendered views conditioned on camera poses, 3D geometric priors, and temporally adjacent or spatially overlapping reference views. This enables the model to infer missing details, suppress rendering artifacts, and enforce cross-view consistency.
  Our framework accommodates variable numbers of cameras and flexible reference/target view configurations, making it adaptable to diverse sensor setups. Experiments on real-world driving datasets demonstrate substantial improvements in image quality metrics, effectively reducing artifacts while preserving geometric fidelity.

</details>


### [113] [BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation](https://arxiv.org/abs/2601.07581)
*Ahmad AlMughrabi,Guillermo Rivo,Carlos Jiménez-Farfán,Umair Haroon,Farid Al-Areqi,Hyunjun Jung,Benjamin Busam,Ricardo Marques,Petia Radeva*

Main category: cs.CV

TL;DR: 提出BenchSeg多视角食物视频分割数据集与基准，覆盖360°自由相机运动，系统评测20种SOTA模型并结合视频记忆模块。结果显示传统图像分割在新视角显著退化，而记忆增强方法保持时序一致性；最佳组合SeTR-MLA+XMem2优于先前方法（mAP提升约2.63%）。


<details>
  <summary>Details</summary>
Motivation: 饮食分析需精确的食物体积与营养估计，依赖高质量分割。但现有方法多基于单视图/静态图像，缺少多视角视频数据，导致对新视角泛化差、跨帧一致性不足。需要一个覆盖360°视角、带高质量标注的基准来客观评测并推动方法发展。

Method: 构建BenchSeg：聚合Nutrition5k、Vegetables & Fruits、MetaFood3D、FoodKit四源数据，共55个菜品场景、25,284帧，均在自由360°相机运动下逐帧精标。基准评测20种SOTA分割器（SAM系、Transformer、CNN、大多模态），先在FoodSeg103做基线，再在BenchSeg上单独与结合视频记忆模块（如XMem2）进行评测；从定量与定性分析新视角性能与时序一致性。

Result: 标准图像分割器在新视角上性能显著下降；引入视频记忆模块能跨帧保持掩膜稳定、减小漂移。最佳方案SeTR-MLA与XMem2结合，在BenchSeg上取得最优，较FoodMem约提升2.63% mAP；验证了记忆增强在多视角视频分割中的有效性。

Conclusion: BenchSeg提供了首个覆盖自由360°多视角的食物视频分割基准，揭示了现有图像分割对视角变化脆弱、而记忆增强策略可提升时序一致性与整体性能。数据与代码的开放有望推动食物分割与跟踪在饮食评估等应用中的发展。

Abstract: Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables & Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer, CNN, and large multimodal) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.

</details>


### [114] [Robust Multicentre Detection and Classification of Colorectal Liver Metastases on CT: Application of Foundation Models](https://arxiv.org/abs/2601.07585)
*Shruti Atul Mali,Zohaib Salahuddin,Yumeng Zhang,Andre Aichert,Xian Zhong,Henry C. Woodruff,Maciej Bobowicz,Katrine Riklund,Juozas Kupčinskas,Lorenzo Faggioni,Roberto Francischello,Razvan L Miclea,Philippe Lambin*

Main category: cs.CV

TL;DR: 提出一个基于基础模型的AI流程，在多中心对比增强CT上进行结直肠癌肝转移（CRLM）的患者级分类与病灶级检测，并集成不确定性与可解释性；在EuCanImage与TCIA数据上取得稳健表现（分类AUC 0.90；灵敏度0.82，外部0.85；检测总体召回69.1%，随病灶尺寸提高），不确定性筛除可进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: CRLM是癌症致死的重要原因，临床上跨中心CT成像异质性高、检测困难，缺乏既稳健又可解释且能量化不确定性的通用AI方案。

Method: 评估多种预训练基础模型后选用UMedPT为骨干：在患者级分类上接入MLP头，并在病灶级检测上使用FCOS检测头；训练/验证于EuCanImage（n=2437）并在外部TCIA（n=197）测试；集成不确定性量化（用于筛除高不确定病例）与Grad-CAM可解释性；采用AUC、敏感度、平衡准确率、决策曲线等指标评估。

Result: 分类：综合测试集AUC 0.90、敏感度0.82；外部队列敏感度0.85；排除不确定性最高的20%病例后，AUC升至0.91、平衡准确率0.86；决策曲线在阈值0.30–0.40范围显示正向临床净获益。检测：总体检出率69.1%，随病灶尺寸四分位从约30%提升至98%。Grad-CAM在高置信病例中突出与病灶对应区域。

Conclusion: 以UMedPT为核心的基础模型管线在异质多中心CT上实现了稳健、可解释的CRLM分类与检测；结合不确定性策略可提升临床决策可用性，展示出向实际部署转化的潜力。

Abstract: Colorectal liver metastases (CRLM) are a major cause of cancer-related mortality, and reliable detection on CT remains challenging in multi-centre settings. We developed a foundation model-based AI pipeline for patient-level classification and lesion-level detection of CRLM on contrast-enhanced CT, integrating uncertainty quantification and explainability. CT data from the EuCanImage consortium (n=2437) and an external TCIA cohort (n=197) were used. Among several pretrained models, UMedPT achieved the best performance and was fine-tuned with an MLP head for classification and an FCOS-based head for lesion detection. The classification model achieved an AUC of 0.90 and a sensitivity of 0.82 on the combined test set, with a sensitivity of 0.85 on the external cohort. Excluding the most uncertain 20 percent of cases improved AUC to 0.91 and balanced accuracy to 0.86. Decision curve analysis showed clinical benefit for threshold probabilities between 0.30 and 0.40. The detection model identified 69.1 percent of lesions overall, increasing from 30 percent to 98 percent across lesion size quartiles. Grad-CAM highlighted lesion-corresponding regions in high-confidence cases. These results demonstrate that foundation model-based pipelines can support robust and interpretable CRLM detection and classification across heterogeneous CT data.

</details>


### [115] [Diffusion in SPAD Signals](https://arxiv.org/abs/2601.07599)
*Lior Dvir,Nadav Torem,Yoav Y. Schechner*

Main category: cs.CV

TL;DR: 论文推导了在固定光子通量下，SPAD原始信号（事件到达时间序列）的似然函数，并进一步得到其score function，以用于基于SPAD信号的逆问题求解；结合扩散模型作为图像先验，分析低/高光子计数与利用到达时间信息的影响，并展示相应性能差异。


<details>
  <summary>Details</summary>
Motivation: SPAD输出是随机的事件时间而非线性强度测量，传统成像/重建假设（如泊松计数或线性高斯）难以准确表征。要在极低光照或高动态范围场景中做稳健重建，需要精确的信号统计模型与可用于现代生成式先验（如扩散模型）的梯度信息（score）。

Method: 1) 在固定光子通量模型下，推导SPAD事件到达时间的完整似然（考虑淬灭/死区和非线性响应）；2) 从该似然得到对数似然的梯度（score function）；3) 将该score嵌入逆问题框架，使用扩散模型表达图像先验（score-based/diffusion inference）；4) 系统分析不同光子计数水平及是否利用时间戳信息对重建的影响。

Result: 得到封闭或可计算的SPAD信号似然与对应score，使得可在扩散先验下进行端到端的逆问题求解。在低光子计数时，时间到达信息显著提升可辨性；在高计数或计数饱和区，模型能处理非线性与死区效应，改善重建质量。

Conclusion: 以物理一致的SPAD似然与score为核心，将扩散先验与事件时间信息结合，可在不同光子计数条件下稳定求解成像逆问题；时间信息的利用尤为关键，显著优于仅用计数的做法。

Abstract: We derive the likelihood of a raw signal in a single photon avalanche diode (SPAD), given a fixed photon flux. The raw signal comprises timing of detection events, which are nonlinearly related to the flux. Moreover, they are naturally stochastic. We then derive a score function of the signal. This is a key for solving inverse problems based on SPAD signals. We focus on deriving solutions involving a diffusion model, to express image priors. We demonstrate the effect of low or high photon counts, and the consequence of exploiting timing of detection events.

</details>


### [116] [UIKA: Fast Universal Head Avatar from Pose-Free Images](https://arxiv.org/abs/2601.07603)
*Zijian Wu,Boyao Zhou,Liangxiao Hu,Hongyu Liu,Yuan Sun,Xuan Wang,Xun Cao,Yujun Shen,Hao Zhu*

Main category: cs.CV

TL;DR: UIKA 提出一种前馈的可动画高斯人头模型，可从单张到多视角/手机视频等无姿态输入直接重建并驱动头像，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有头像重建通常依赖棚拍级多机位与长时间个体优化，难以从随手采集的少量/无姿态数据快速得到可动画、可泛化的高质量人头模型，需要一种能统一不同输入形态、快速前馈、且具备强泛化能力的表示与训练范式。

Method: 1) UV 引导建模：对每张输入图估计逐像素面部对应关系，将屏幕空间有效像素颜色重投影到与视角、表情无关的 UV 空间。2) 设计可学习的 UV tokens，并在屏幕与 UV 两个层级上施加注意力，聚合多视角/多帧信息；将学到的 UV tokens 解码为规范姿态下的高斯属性（位置、尺度、颜色等）。3) 构建大规模、身份多样的合成训练集，端到端训练大模型，实现从单/多视角前馈重建与动画。

Result: 在单目与多视角设置中均显著优于现有方法；能从任意数量、无姿态约束的输入（含手机视频）快速生成可动画高斯头像，鲁棒性与质量提升。

Conclusion: 通过UV对应与多层级注意力上的可学习UV tokens，再配合大规模合成数据训练，UIKA实现了从随手采集到高质量、可动画头像的前馈重建，减少采集与优化成本，并在多种输入场景下优于现有方法。

Abstract: We present UIKA, a feed-forward animatable Gaussian head model from an arbitrary number of unposed inputs, including a single image, multi-view captures, and smartphone-captured videos. Unlike the traditional avatar method, which requires a studio-level multi-view capture system and reconstructs a human-specific model through a long-time optimization process, we rethink the task through the lenses of model representation, network design, and data preparation. First, we introduce a UV-guided avatar modeling strategy, in which each input image is associated with a pixel-wise facial correspondence estimation. Such correspondence estimation allows us to reproject each valid pixel color from screen space to UV space, which is independent of camera pose and character expression. Furthermore, we design learnable UV tokens on which the attention mechanism can be applied at both the screen and UV levels. The learned UV tokens can be decoded into canonical Gaussian attributes using aggregated UV information from all input views. To train our large avatar model, we additionally prepare a large-scale, identity-rich synthetic training dataset. Our method significantly outperforms existing approaches in both monocular and multi-view settings. Project page: https://zijian-wu.github.io/uika-page/

</details>


### [117] [PARL: Position-Aware Relation Learning Network for Document Layout Analysis](https://arxiv.org/abs/2601.07620)
*Fuyuan Liu,Dianyu Yu,He Ren,Nayu Liu,Xiaomian Kang,Delai Qiu,Fa Zhang,Genpeng Zhen,Shengping Liu,Jiaen Liang,Wei Huang,Yining Wang,Junnan Zhu*

Main category: cs.CV

TL;DR: 提出PARL，一个纯视觉、无OCR的文档版面分析框架，通过位置感知与关系建模，在DocLayNet与M6Doc上达SOTA，同时参数量仅65M，显著高效稳健。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法依赖高质量OCR融合文本与视觉，易受识别误差传播影响且计算开销大，限制鲁棒性与实用性。作者主张版面分析的关键在于对文档内在视觉结构的深刻建模，而非文本-视觉融合。

Method: 提出PARL框架：1) 双向空间位置引导的可变形注意（Bi-SPG-Deformable Attention），将显式位置依赖嵌入视觉特征；2) 图细化分类器（GRC），基于动态构建的版面图建模上下文关系，细化类别预测；整体为OCR-free、vision-only。

Result: 在DocLayNet上建立新的纯视觉基线，在M6Doc上甚至超过强多模态模型；以65M参数实现SOTA，相比256M多模态模型更高效（约1/4参数）。

Conclusion: 精细的视觉结构建模可在无OCR与较小模型规模下实现更强与更稳健的版面分析，证明多模态融合并非必要条件。

Abstract: Document layout analysis aims to detect and categorize structural elements (e.g., titles, tables, figures) in scanned or digital documents. Popular methods often rely on high-quality Optical Character Recognition (OCR) to merge visual features with extracted text. This dependency introduces two major drawbacks: propagation of text recognition errors and substantial computational overhead, limiting the robustness and practical applicability of multimodal approaches. In contrast to the prevailing multimodal trend, we argue that effective layout analysis depends not on text-visual fusion, but on a deep understanding of documents' intrinsic visual structure. To this end, we propose PARL (Position-Aware Relation Learning Network), a novel OCR-free, vision-only framework that models layout through positional sensitivity and relational structure. Specifically, we first introduce a Bidirectional Spatial Position-Guided Deformable Attention module to embed explicit positional dependencies among layout elements directly into visual features. Second, we design a Graph Refinement Classifier (GRC) to refine predictions by modeling contextual relationships through a dynamically constructed layout graph. Extensive experiments show PARL achieves state-of-the-art results. It establishes a new benchmark for vision-only methods on DocLayNet and, notably, surpasses even strong multimodal models on M6Doc. Crucially, PARL (65M) is highly efficient, using roughly four times fewer parameters than large multimodal models (256M), demonstrating that sophisticated visual structure modeling can be both more efficient and robust than multimodal fusion.

</details>


### [118] [GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models](https://arxiv.org/abs/2601.07632)
*Zhankai Ye,Bofan Li,Yukai Jin,Shuoqiu Li,Wei Wang,Yanfu Zhang,Shangqian Gao,Xin Liu*

Main category: cs.CV

TL;DR: 提出通过在运动码本与LLM嵌入空间同时施加正交/正交归一约束，使两者共享统一几何基，显著提升运动理解与运动语言推理（HumanML3D上较SOTA提升约20%）。


<details>
  <summary>Details</summary>
Motivation: 现有离散运动标记管线将运动量化与语义嵌入学习解耦，仅以token ID相连，导致运动空间内在几何与嵌入空间错配，削弱LLM对细粒度运动推理能力。作者认为当两种模态共享统一几何基时，跨模态对齐最有效。

Method: 1) 使用仅解码量化器+Gumbel-Softmax，实现可微训练与均衡码本使用；2) 稀疏投影将运动码映射至LLM嵌入并保持正交性；3) 在分两阶段的正交/正交归一正则日程中，先对tokenizer训练施加软约束，再在LLM微调时维持几何对齐且不妨碍语义适配。

Result: 在HumanML3D上相较现有SOTA取得约20%整体性能提升，显示统一几何基显著增强LLM的运动推理与理解。

Conclusion: 通过在运动码本与LLM嵌入空间显式正交对齐并以稀疏投影桥接模态，可自然镜像两者关系结构，从而提升细粒度运动语言推理能力。

Abstract: Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.

</details>


### [119] [StdGEN++: A Comprehensive System for Semantic-Decomposed 3D Character Generation](https://arxiv.org/abs/2601.07660)
*Yuze He,Yanning Zhou,Wang Zhao,Jingwen Ye,Zhongkai Wu,Ran Yi,Yong-Jin Liu*

Main category: cs.CV

TL;DR: StdGEN++提出一个可工业化的3D角色生成系统，基于双分支语义感知重建模型，联合重建几何、颜色与部件语义，并通过新型语义表面提取与粗到细方案实现高分辨率网格；再以视频扩散进行纹理分解，获得可编辑分层。结果在几何精度与语义解耦上达SOTA，支持无损编辑、物理一致动画与凝视追踪等下游能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成多输出整体网格，难以满足游戏/动画生产对可分部件、可编辑、物理一致的结构化资产需求，尤其在面部区域的语义混淆与纹理耦合影响下游管线。

Method: 提出Dual-Branch S-LRM双分支语义感知大重建模型，前馈联合重建几何、颜色与按部件的语义；设计兼容混合隐式场的语义表面提取形式，并用粗到细候选加速以降低显存并生成高分辨率网格；引入基于视频扩散的纹理分解模块，将外观解耦为可编辑图层（如虹膜与皮肤分离），缓解面部语义混淆。

Result: 相比现有方法，在几何精度与语义解耦指标上显著领先（SOTA），可从多样输入生成高保真、语义分解的3D角色。

Conclusion: StdGEN++实现结构独立与高保真重建，解锁非破坏编辑、符合物理的动画与凝视追踪等生产级能力，适用于自动化角色资产生产。

Abstract: We present StdGEN++, a novel and comprehensive system for generating high-fidelity, semantically decomposed 3D characters from diverse inputs. Existing 3D generative methods often produce monolithic meshes that lack the structural flexibility required by industrial pipelines in gaming and animation. Addressing this gap, StdGEN++ is built upon a Dual-branch Semantic-aware Large Reconstruction Model (Dual-Branch S-LRM), which jointly reconstructs geometry, color, and per-component semantics in a feed-forward manner. To achieve production-level fidelity, we introduce a novel semantic surface extraction formalism compatible with hybrid implicit fields. This mechanism is accelerated by a coarse-to-fine proposal scheme, which significantly reduces memory footprint and enables high-resolution mesh generation. Furthermore, we propose a video-diffusion-based texture decomposition module that disentangles appearance into editable layers (e.g., separated iris and skin), resolving semantic confusion in facial regions. Experiments demonstrate that StdGEN++ achieves state-of-the-art performance, significantly outperforming existing methods in geometric accuracy and semantic disentanglement. Crucially, the resulting structural independence unlocks advanced downstream capabilities, including non-destructive editing, physics-compliant animation, and gaze tracking, making it a robust solution for automated character asset production.

</details>


### [120] [Variational Contrastive Learning for Skeleton-based Action Recognition](https://arxiv.org/abs/2601.07666)
*Dang Dinh Nguyen,Decky Aspandi Latif,Titus Zaharia*

Main category: cs.CV

TL;DR: 提出一种将变分潜变量建模与对比自监督学习结合的框架，用于骨架动作识别的自监督表征学习，能更好捕获人体运动的不确定性与多样性，并在多个数据集、尤其低标注场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习范式偏判别式，难以表达人体运动中固有的多样性与不确定性；需要学习更具结构性与语义性的表示，能跨数据集和监督强度泛化。

Method: 构建“变分+对比”的自监督框架：以概率潜变量模型编码骨架序列的分布特性，通过对比学习目标约束潜空间，使同一语义的样本在潜空间中接近并具备不确定性表达；学习到的表示关注重要关节并具有可解释性。

Result: 在三个主流骨架动作识别基准上进行大量实验，方法在总体和低标注比例（low-label）设置下均显著优于现有方法；定性分析显示特征与运动与样本特征更相关，对关键关节的关注度更高。

Conclusion: 变分对比学习能联合捕获动作表征的判别性与不确定性，得到结构化且语义有意义的表示，具备更强的跨数据集与监督水平的泛化能力。

Abstract: In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most of contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods.

</details>


### [121] [Advancing Multinational License Plate Recognition Through Synthetic and Real Data Fusion: A Comprehensive Evaluation](https://arxiv.org/abs/2601.07671)
*Rayson Laroca,Valter Estevam,Gladston J. P. Moreira,Rodrigo Minetto,David Menotti*

Main category: cs.CV

TL;DR: 用大量高质量合成数据与真实数据结合，系统评测多种OCR/LPR模型与数据集，显著提升车牌识别的跨域与域内性能，并在准确率与速度上给出最优折中。


<details>
  <summary>Details</summary>
Motivation: 现有LPR研究常用合成图像增益，但存在方法零散、评测不全面、跨数据集泛化有限、与商业系统比较不足、以及在小样本场景下的效果不明等问题。作者希望系统化地回答：如何更好地生成与整合合成数据？不同生成策略带来何种增益？在多模型、多数据集下的性能与速度权衡如何？

Method: 1) 构建基准：16个OCR/LPR模型×12个来自不同区域的公开数据集，评测域内与跨域识别。2) 合成数据方案对比与组合：模板生成、字符置换、GAN生成三类；系统化混合并与真实数据共同训练。3) 资源受限实验：逐步减少真实训练数据占比以评估小样本下的鲁棒性。4) 速度-精度权衡分析：识别准确率与推理速度的综合评估，寻找不同场景的最优模型。

Result: - 大规模引入合成数据在域内与跨域均显著提升性能；三种合成策略各有贡献且组合存在协同增益。- 端到端结果超过SOTA与商用系统。- 在仅用少量真实数据时仍能取得接近或优于全量训练的效果。- 明确了不同模型在精度与速度之间的最佳折中，分别适用于域内与跨域设置。

Conclusion: 合成数据与真实数据的深度融合是提升LPR泛化与效率的关键。模板、字符置换、GAN三类方法的混合能产生协同效应，在数据受限场景尤为有效。同时应依据应用需求在精度与速度间选择合适模型。

Abstract: Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.

</details>


### [122] [Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation](https://arxiv.org/abs/2601.07692)
*Nicolas Sereyjol-Garros,Ellington Kirby,Victor Besnier,Nermin Samet*

Main category: cs.CV

TL;DR: 提出R3DPA：将大规模图像预训练先验与自监督3D表征引入LiDAR场景生成，显著提升质量并支持无条件模型的可控生成，在KITTI-360上达SOTA。


<details>
  <summary>Details</summary>
Motivation: LiDAR场景生成受限于3D数据稀缺，现有扩散/flow方法难以匹配RGB领域的大规模先验，导致生成质量与可控性受限；亟需利用图像大模型与自监督3D特征缓解数据不足并提升生成。

Method: 1) 在生成模型中对齐中间特征与自监督3D特征，增强几何语义一致性；2) 将大规模图像预训练的生成先验迁移到LiDAR生成以弥补数据规模不足；3) 在推理阶段对无条件模型实现点云可控（如目标修补与场景混合）。

Result: 在KITTI-360基准上取得最新最优性能（SOTA），并展示了对象修补与场景混合等控制能力；开源代码与预训练模型提供复现。

Conclusion: 跨模态先验与自监督3D特征的结合能显著提升LiDAR场景生成质量与可控性，为数据稀缺条件下的3D合成提供有效路径。

Abstract: LiDAR scene synthesis is an emerging solution to scarcity in 3D data for robotic tasks such as autonomous driving. Recent approaches employ diffusion or flow matching models to generate realistic scenes, but 3D data remains limited compared to RGB datasets with millions of samples. We introduce R3DPA, the first LiDAR scene generation method to unlock image-pretrained priors for LiDAR point clouds, and leverage self-supervised 3D representations for state-of-the-art results. Specifically, we (i) align intermediate features of our generative model with self-supervised 3D features, which substantially improves generation quality; (ii) transfer knowledge from large-scale image-pretrained generative models to LiDAR generation, mitigating limited LiDAR datasets; and (iii) enable point cloud control at inference for object inpainting and scene mixing with solely an unconditional model. On the KITTI-360 benchmark R3DPA achieves state of the art performance. Code and pretrained models are available at https://github.com/valeoai/R3DPA.

</details>


### [123] [Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model](https://arxiv.org/abs/2601.07695)
*Siwen Jiao,Tianxiong Lv,Kangan Qian,Chenxu Zhao,Xiuyuan Zhu,Tianlun Li,Xiaolong Cheng,Jinyu Li,Zhihao Liao,Yang Cai*

Main category: cs.CV

TL;DR: 提出SNRA与AP-GRPO，缓解VLM在3D数值预测中的奖励稀疏与优势塌缩问题，用绝对保留梯度与平滑奖励提升数据利用率，并在Numerical3D-50k上达到与大规模监督相当的性能且更高数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在3D场景理解的精确数值预测上表现不佳。以相对排序为核心的RL（如标准GRPO）存在奖励稀疏、梯度不稳与对“擦边”样本（小但非零误差）的优势塌缩，导致难以利用3D物理约束可验证信号与边界样本，造成数据利用率低。

Method: 1) 提出SNRA：用动态参数化Sigmoid把原始反馈映射为稠密、平滑、可微的数值奖励。2) 提出AP-GRPO：在GRPO中引入绝对标量梯度，保留数值幅度信息，缓解相对归一化导致的信息丢失和优势塌缩。3) 构建Numerical3D-50k数据集（5万可验证3D子任务），用于训练与评测。

Result: 在Numerical3D-50k上，AP-GRPO无需改模型结构即可激活VLM的潜在3D数值推理，达到与大规模监督学习相当的性能，并表现出更高的数据效率与稳定性。

Conclusion: 通过SNRA和AP-GRPO，将稀疏、离散的反馈转化为平滑且信息保留的奖励与梯度信号，充分利用边界样本与3D物理可验证信号，显著提升VLM在3D数值预测上的训练效率与效果。

Abstract: Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes "near-miss" samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.

</details>


### [124] [Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition](https://arxiv.org/abs/2601.07700)
*Jakob Paul Zimmermann,Georg Loho*

Main category: cs.CV

TL;DR: 论文提出利用单调性提升可解释性的两条路径：把ReLU网络分解为两个单调且凸的部分以改进显著图，并训练“两个单调网络之差”的模型以获得自解释性。


<details>
  <summary>Details</summary>
Motivation: 单调性通常有助于模型可解释，但许多目标函数难以被单调网络良好逼近。作者希望在不牺牲表达力的情况下，仍借助单调性带来解释优势。

Method: 1) 对已训练的ReLU网络进行分解：将其表示为两个单调且凸的部分，提出数值稳健的改进以避免权重爆炸；基于该分解设计两种显著性方法SplitCAM与SplitLRP。2) 训练结构化模型：把整体函数表示为两个单调神经网络的差(DMN: difference of monotone nets)，以内生地获得可解释结构。

Result: 在ImageNet-S基准上，基于VGG16与ResNet18，SplitCAM与SplitLRP在Quantus的所有显著性评价类别上优于当前SOTA。

Conclusion: 即使目标函数本身不单调，利用“单调+凸”分解与“两个单调网之差”的结构，仍可显著提升显著图质量与模型自解释性，且提出的数值处理克服了分解中的权重膨胀问题。

Abstract: It has been demonstrated in various contexts that monotonicity leads to better explainability in neural networks. However, not every function can be well approximated by a monotone neural network. We demonstrate that monotonicity can still be used in two ways to boost explainability. First, we use an adaptation of the decomposition of a trained ReLU network into two monotone and convex parts, thereby overcoming numerical obstacles from an inherent blowup of the weights in this procedure. Our proposed saliency methods -- SplitCAM and SplitLRP -- improve on state of the art results on both VGG16 and Resnet18 networks on ImageNet-S across all Quantus saliency metric categories. Second, we exhibit that training a model as the difference between two monotone neural networks results in a system with strong self-explainability properties.

</details>


### [125] [FMAC: a Fair Fiducial Marker Accuracy Comparison Software](https://arxiv.org/abs/2601.07723)
*Guillaume J. Laurent,Patrick Sandoz*

Main category: cs.CV

TL;DR: 提出一种利用高保真合成图像对基于标志点的位姿估计进行公平、系统比较的方法，并开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有关于标志点位姿估计精度的比较常受数据集覆盖不足、成像物理不一致与评测方法不统一影响，难以得出可重复且可分解到6自由度的结论。

Method: 构建物理真实感的光线追踪渲染管线，直接使用相机标定参数，模拟畸变、散焦、衍射与亚像素锐边采样；对6DoF采用低差异采样，系统遍历姿态空间；通过36对自由度-误差关联图分析各自由度与位姿误差相关性；提出标准化的精度评估流程，并在多种经典标志点上应用。

Result: 合成数据在实验上得到验证；评测揭示不同标志点在姿态估计上的优势与短板（如对特定角度、距离、畸变的敏感性差异）；提供可复现实验与对比基准。

Conclusion: 物理一致的高保真渲染与低差异采样可支持对标志点位姿估计进行公平且可解释的比较；所给评测流程和开源代码为后续方法与标志点设计提供通用基准与分析工具。

Abstract: This paper presents a method for carrying fair comparisons of the accuracy of pose estimation using fiducial markers. These comparisons rely on large sets of high-fidelity synthetic images enabling deep exploration of the 6 degrees of freedom. A low-discrepancy sampling of the space allows to check the correlations between each degree of freedom and the pose errors by plotting the 36 pairs of combinations. The images are rendered using a physically based ray tracing code that has been specifically developed to use the standard calibration coefficients of any camera directly. The software reproduces image distortions, defocus and diffraction blur. Furthermore, sub-pixel sampling is applied to sharp edges to enhance the fidelity of the rendered image. After introducing the rendering algorithm and its experimental validation, the paper proposes a method for evaluating the pose accuracy. This method is applied to well-known markers, revealing their strengths and weaknesses for pose estimation. The code is open source and available on GitHub.

</details>


### [126] [Evaluating the encoding competence of visual language models using uncommon actions](https://arxiv.org/abs/2601.07737)
*Chen Ling,Nai Ding*

Main category: cs.CV

TL;DR: UAIT 是一个用于评估视觉语言模型在“反常识”动作场景中语义理解能力的新基准，包含由LLM与文本到图像生成半自动合成的图文对与多选题，用于检验模型对施受关系与物理可行性的细粒度推理；SOTA VLM显著落后于人类，但微调可明显提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多偏向常见场景与统计共现，模型可凭表面模式或语言偏置取巧，难以检验对物理因果、施受关系与语义可行性的真正理解，故需一个专门考察“语法正确但语义违常识”的评测基准。

Method: 提出UAIT数据集：利用大语言模型与少样本提示生成反常识动作文本，再用文生图模型合成对应图像；对每个样本设计细粒度多选题以区分语法正确与语义合理；搭建评测协议，对多种SOTA VLM与对比学习模型进行测试，并开展微调实验。

Result: 所有被测模型在语义判别任务上均显著低于人类表现，尤其在区分语法正确与语义合理方面；然而对轻量模型进行定向微调后，准确率明显提升，显示适应性空间。

Conclusion: UAIT揭示VLM在反常识语义推理、施受关系与物理可行性理解上的关键薄弱点，并提供可复现实验基准与诊断工具；定向微调能缓解不足，指向构建更健壮、具真实视觉语义推理能力模型的研究方向。

Abstract: We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model's competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.

</details>


### [127] [On the application of the Wasserstein metric to 2D curves classification](https://arxiv.org/abs/2601.07749)
*Agnieszka Kaliszewska,Monika Syga*

Main category: cs.CV

TL;DR: 提出一组加权/片段敏感的Wasserstein距离变体，用离散概率权重聚焦曲线指定片段，在考古2D曲线聚类上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统Wasserstein距离在整条曲线上平均度量，难以在分类/聚类时强调对任务关键的曲线片段；考古曲线常有缺失、磨损或仅部分片段承载鉴别信息，需能“聚焦”的距离度量。

Method: 为曲线的不同片段构造若干离散概率测度（权重分布），并据此定义Wasserstein距离的变体，使运输代价按片段重要性重加权；在2D曲线数据上，用这些变体进行聚类实验并比较表现。

Result: 在考古领域的2D曲线聚类任务中，这些片段敏感的Wasserstein变体优于标准做法，能更准确地区分由关键片段主导的类别。

Conclusion: 通过为曲线片段分配权重并据此调整Wasserstein距离，可有效提升仅部分片段具判别力场景下的聚类/分类性能，适用于考古等部分信息主导的曲线分析。

Abstract: In this work we analyse a number of variants of the Wasserstein distance which allow to focus the classification on the prescribed parts (fragments) of classified 2D curves. These variants are based on the use of a number of discrete probability measures which reflect the importance of given fragments of curves. The performance of this approach is tested through a series of experiments related to the clustering analysis of 2D curves performed on data coming from the field of archaeology.

</details>


### [128] [Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding](https://arxiv.org/abs/2601.07761)
*Yanxiang Huang,Guohua Gao,Zhaoyang Wei,Jianyuan Ni*

Main category: cs.CV

TL;DR: 提出Chain of Evidence (CoE)框架，解耦“感知落地”和“推理效率”，以减少视频推理中的幻觉并降低计算成本；通过轻量证据定位模块与基于强化学习的证据锚定协议，在多个基准上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: LVLM在视频推理中面临两难：要么详尽思维链导致计算昂贵，要么高效但不受视觉证据约束易产生幻觉。需要一种既高效又有强感知对齐的机制。

Method: 1) 证据落地模块（EGM）：面向查询的轻量过滤器，动态选取少量高置信度视觉证据/时间锚点；2) 证据锚定协议：用强化学习优化，设计复合奖励，强制推理过程对齐并引用已识别的时间锚；3) 构建CoE-Instruct数据集（164k），采用感知与推理双重标注，分别监督。

Result: 在Video-MME、MVBench、VSI-Bench等五个基准上达到或刷新SOTA，相比现有方法显著提升准确率，且更可靠、幻觉更少。

Conclusion: CoE通过架构解耦与RL对齐，既提高推理效率又增强感知可靠性，为视频理解提供了实用且可扩展的范式。

Abstract: Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.

</details>


### [129] [Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training](https://arxiv.org/abs/2601.07773)
*Lingchen Sun,Rongyuan Wu,Zhengqiang Zhang,Ruibin Li,Yujing Sun,Shuaizheng Liu,Lei Zhang*

Main category: cs.CV

TL;DR: 提出“Self-Transcendence”方法，用模型自身的中间特征作为监督，替代外部语义特征，引导并加速DiT训练，在无需外部预训练网络的前提下实现更快收敛与更高生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有如 REPA 的方法依赖外部预训练语义网络（如 DINO）指导DiT，加速收敛但引入依赖、降低灵活性。作者认为DiT内部已具备可用于自我指导的表征，若能有效挖掘并强化这些特征，可在保持自包含的同时获得同等或更优的加速与质量。

Method: 两阶段内部特征监督：1）短暂预训练（约40个epoch），将DiT浅层特征与预训练VAE的潜空间表示对齐，缓解浅层表征学习困难；2）对中间层特征施加classifier-free guidance以提升判别性和语义表达；随后使用这些经丰富的内部特征作为监督信号，重新训练新的DiT，实现自我引导（self-transcendence）。

Result: 相对自包含基线显著提升，收敛更快、生成质量更高；在无需任何外部预训练模型的情况下，甚至在质量与速度上超过依赖外部特征的REPA；方法对不同骨干更灵活，可拓展到更广的扩散生成任务。

Conclusion: DiT的缓慢收敛主要源于浅层表征学习难；通过短期VAE对齐和对中间特征的无分类器引导，可从模型内部产生强监督信号，引导新模型训练，达到比外部引导更高效与更通用的训练加速。

Abstract: Recent works such as REPA have shown that guiding diffusion models with external semantic features (e.g., DINO) can significantly accelerate the training of diffusion transformers (DiTs). However, this requires the use of pretrained external networks, introducing additional dependencies and reducing flexibility. In this work, we argue that DiTs actually have the power to guide the training of themselves, and propose \textbf{Self-Transcendence}, a simple yet effective method that achieves fast convergence using internal feature supervision only. It is found that the slow convergence in DiT training primarily stems from the difficulty of representation learning in shallow layers. To address this, we initially train the DiT model by aligning its shallow features with the latent representations from the pretrained VAE for a short phase (e.g., 40 epochs), then apply classifier-free guidance to the intermediate features, enhancing their discriminative capability and semantic expressiveness. These enriched internal features, learned entirely within the model, are used as supervision signals to guide a new DiT training. Compared to existing self-contained methods, our approach brings a significant performance boost. It can even surpass REPA in terms of generation quality and convergence speed, but without the need for any external pretrained models. Our method is not only more flexible for different backbones but also has the potential to be adopted for a wider range of diffusion-based generative tasks. The source code of our method can be found at https://github.com/csslc/Self-Transcendence.

</details>


### [130] [Vision-Language Model for Accurate Crater Detection](https://arxiv.org/abs/2601.07795)
*Patrick Bauer,Marius Schwinning,Florian Renk,Andreas Weinmann,Hichem Snoussi*

Main category: cs.CV

TL;DR: 基于Vision Transformer的OWLv2模型，结合LoRA参数高效微调与CIoU定位损失+对比学习分类损失，在IMPACT高分辨率月球相机数据上实现高召回（94%）与较高精度（73.1%），在复杂光照与崎岖地形下实现稳健陨石坑检测，支持未来登月任务安全着陆与地形分析。


<details>
  <summary>Details</summary>
Motivation: 登月着陆安全高度依赖对着陆区陨石坑的准确识别与定位；现有深度学习CDA受多尺度形态、多变光照与复杂地形影响，鲁棒性与泛化仍具挑战。ESA的Argonaut任务需求驱动更可靠的方法。

Method: 以OWLv2（Vision Transformer架构）为检测基础；使用IMPACT项目高分辨率LROC CDR图像的人工标注数据进行微调；采用LoRA进行参数高效微调以减少训练成本；损失函数为CIoU（定位）+对比学习（分类）的组合；在多样成像条件下训练与评估。

Result: 在IMPACT测试集上取得最高召回率94.0%、最高精度73.1%，并呈现良好可视化检测效果；在多样光照与崎岖地形条件下表现稳健。

Conclusion: 所提基于OWLv2并结合LoRA与复合损失的CDA方法可在复杂月表成像条件下实现可靠的陨石坑检测，为未来月球探测，尤其安全着陆与陨石坑分析提供了有力技术支撑。

Abstract: The European Space Agency (ESA), driven by its ambitions on planned lunar missions with the Argonaut lander, has a profound interest in reliable crater detection, since craters pose a risk to safe lunar landings. This task is usually addressed with automated crater detection algorithms (CDA) based on deep learning techniques. It is non-trivial due to the vast amount of craters of various sizes and shapes, as well as challenging conditions such as varying illumination and rugged terrain. Therefore, we propose a deep-learning CDA based on the OWLv2 model, which is built on a Vision Transformer, that has proven highly effective in various computer vision tasks. For fine-tuning, we utilize a manually labeled dataset fom the IMPACT project, that provides crater annotations on high-resolution Lunar Reconnaissance Orbiter Camera Calibrated Data Record images. We insert trainable parameters using a parameter-efficient fine-tuning strategy with Low-Rank Adaptation, and optimize a combined loss function consisting of Complete Intersection over Union (CIoU) for localization and a contrastive loss for classification. We achieve satisfactory visual results, along with a maximum recall of 94.0% and a maximum precision of 73.1% on a test dataset from IMPACT. Our method achieves reliable crater detection across challenging lunar imaging conditions, paving the way for robust crater analysis in future lunar exploration.

</details>


### [131] [Exchange Is All You Need for Remote Sensing Change Detection](https://arxiv.org/abs/2601.07805)
*Sijun Dong,Siming Fu,Kaiyu Li,Xiangyong Cao,Xiaoliang Meng,Bo Du*

Main category: cs.CV

TL;DR: 提出SEED：用“特征交换”替代显式差分的双时相遥感变化检测简化范式，在多数据集和多骨干上达SOTA水平或更优，并可将分割模型经交换模块转化为变化检测器（SEG2CD）。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测常依赖孪生编码器+显式差分（减法/拼接），带来信息丢失与结构复杂度；作者希望用更简单且信息保持的机制实现高效双时相特征融合与判别。

Method: 提出SEED（Siamese Encoder-Exchange-Decoder）：共享权重的孪生编码器与解码器，中间不做算术差分，而是进行“参数无关的特征交换”。将交换形式化为正交置换算子，并在像素一致性假设下证明可保持互信息与贝叶斯最优风险；对比指出算术融合可能导致信息损失。进一步提出SEG2CD：在标准语义分割模型中仅插入交换机制即可用于变化检测。

Result: 在五个基准（SYSU-CD、LEVIR-CD、PX-CLCD、WaterCD、CDD）与三种骨干（SwinT、EfficientNet、ResNet）上，SEED匹配或超过SOTA。实验广泛且表明在保持简单结构和单参数集的情况下仍具强竞争力。

Conclusion: 简单的特征交换即可实现高性能的信息融合与变化检测，SEED提供了统一、鲁棒、可解释的框架，同时将常规分割模型通过交换机制转化为强竞争力的变化检测器；代码和训练/评测协议将开放。

Abstract: Remote sensing change detection fundamentally relies on the effective fusion and discrimination of bi-temporal features. Prevailing paradigms typically utilize Siamese encoders bridged by explicit difference computation modules, such as subtraction or concatenation, to identify changes. In this work, we challenge this complexity with SEED (Siamese Encoder-Exchange-Decoder), a streamlined paradigm that replaces explicit differencing with parameter-free feature exchange. By sharing weights across both Siamese encoders and decoders, SEED effectively operates as a single parameter set model. Theoretically, we formalize feature exchange as an orthogonal permutation operator and prove that, under pixel consistency, this mechanism preserves mutual information and Bayes optimal risk, whereas common arithmetic fusion methods often introduce information loss. Extensive experiments across five benchmarks, including SYSU-CD, LEVIR-CD, PX-CLCD, WaterCD, and CDD, and three backbones, namely SwinT, EfficientNet, and ResNet, demonstrate that SEED matches or surpasses state of the art methods despite its simplicity. Furthermore, we reveal that standard semantic segmentation models can be transformed into competitive change detectors solely by inserting this exchange mechanism, referred to as SEG2CD. The proposed paradigm offers a robust, unified, and interpretable framework for change detection, demonstrating that simple feature exchange is sufficient for high performance information fusion. Code and full training and evaluation protocols will be released at https://github.com/dyzy41/open-rscd.

</details>


### [132] [More Images, More Problems? A Controlled Analysis of VLM Failure Modes](https://arxiv.org/abs/2601.07812)
*Anurag Das,Adrian Bulat,Alberto Baldrati,Ioannis Maniadis Metaxas,Bernt Schiele,Georgios Tzimiropoulos,Brais Martinez*

Main category: cs.CV

TL;DR: 提出MIMIC基准与两种补救策略，系统诊断并提升LVLM在多图理解中的跨图聚合与多概念追踪能力，刷新多图任务SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在多图场景下的理解与推理能力研究不足，已有基准未系统揭示其核心弱点与成因，缺乏有针对性的训练与优化方法。

Method: 1) 构建MIMIC基准，设计多项诊断任务评估跨图信息聚合与多概念关注能力。2) 数据侧：通过程序化组合，将单图标注合成为目标明确的多图训练样本。3) 优化侧：分析分层注意力模式，提出适配多图输入的注意力掩码方案。

Result: 诊断实验发现LVLM普遍难以跨图聚合信息并同时跟踪多概念。采用所提数据生成与注意力掩码后，跨图聚合显著改善，并在现有多图基准上取得优于既有方法的性能。

Conclusion: MIMIC为多图能力提供系统评估；数据与优化两端的策略能有效缓解LVLM多图弱点，推进多图理解SOTA；代码与数据将开源。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.

</details>


### [133] [MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head](https://arxiv.org/abs/2601.07832)
*Kewei Zhang,Ye Huang,Yufan Deng,Jincheng Yu,Junsong Chen,Huan Ling,Enze Xie,Daquan Zhou*

Main category: cs.CV

TL;DR: 提出多头线性注意力（MHLA），在保持线性复杂度的同时缓解线性注意力的全局上下文坍缩，显著提升图像、文本、图像生成与视频生成表现。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力的二次复杂度限制了大规模应用；线性注意力更高效但性能常下降，现有改进多依赖额外模块带来额外开销。作者发现性能劣化的关键原因是“全局上下文坍缩”，即表示多样性丢失。

Method: 提出MHLA：在token维度将序列分割到多个头内分别进行线性注意力计算，以保持跨位置表示的多样性；理论上证明其仍为线性复杂度，同时恢复接近softmax注意力的表达能力。

Result: 在相同时间复杂度下，多域显著增益：ImageNet分类+3.6%，NLP任务+6.3%，图像生成+12.6%，视频生成+41%。

Conclusion: MHLA在不增加计算复杂度的前提下，缓解线性注意力的表示坍缩问题，提升多个领域性能，成为兼顾效率与表现的替代方案。

Abstract: While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity.

</details>


### [134] [Tuning-free Visual Effect Transfer across Videos](https://arxiv.org/abs/2601.07833)
*Maxwell Jones,Rameen Abdal,Or Patashnik,Ruslan Salakhutdinov,Sergey Tulyakov,Jun-Yan Zhu,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: RefVFX提出一种参考视频驱动的特效转移框架，可将复杂时间动态效果从参考视频转移到目标视频/图像，较基于文本或关键帧的方法在时序一致性与效果可控性上更强。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法擅长文本提示或静态条件，但难以刻画与迁移动态光照、角色变形等时间变化强的效果；需要一种能直接从示例视频学习并转移其时序效果的方案。

Method: 构建大规模三元组数据集（参考效果视频、输入视频/图像、输出效果视频）。为获得高质量视频到视频配对，提出自动化数据生成管线：在保持输入运动与结构的同时，引入可重复的固定效果；并通过LoRA适配器生成图像到视频效果，以及程序化合成的代码型时间效果进行数据增强。基于该数据集，采用最新文本到视频骨干训练参考条件模型，实现前馈式效果转移。

Result: RefVFX能产生视觉一致、时间连贯的编辑效果，在未见过的效果类别上具有泛化能力；在定量指标与人工偏好测试上均优于仅用文本提示的基线。

Conclusion: 参考视频条件的前馈特效转移是处理复杂时间效果的有效途径；通过可扩展的数据生成与多源增强，模型实现稳定泛化与优于提示法的表现。

Abstract: We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website $\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\ this\ URL}$.

</details>
