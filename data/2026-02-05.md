<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 该论文是一篇关于3D Gaussian Splatting（3DGS）知识产权保护的首个系统性综述，提出自底向上的统一框架，总结机制、范式与鲁棒性威胁，并给出未来六大研究方向。


<details>
  <summary>Details</summary>
Motivation: 3DGS在实时三维重建与渲染中广泛应用且具商业价值，其显式参数结构易被复制与滥用，现有IP保护研究零散、缺乏统一视角和系统方法，需要梳理机制、范式与鲁棒性挑战以指导后续研究与实践。

Method: 进行系统性综述与框架构建：从底层高斯参数扰动机制出发，上升到被动/主动两类保护范式，再到在生成式AI时代面临的鲁棒性威胁；对文献进行对比与归纳，提炼技术空白与评测不足，并提出研究议程。

Result: 提出首个统一的三层框架：（i）高斯基扰动机制；（ii）被动与主动保护策略；（iii）在生成式模型冲击下的鲁棒性威胁与评测缺口。识别出技术基础与鲁棒性刻画的空白，归纳现有方法的局限与脆弱点。

Conclusion: 当前3DGS IP保护仍不成熟，缺乏坚实机制与系统鲁棒性评估。作者给出围绕鲁棒性、效率与保护范式的六条研究路线，为构建可靠可信的3DGS资产保护提供路线图。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

</details>


### [2] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: 提出 TruKAN：以可学习激活的截断幂函数替代 KAN 的B样条基，集成进 EfficientNet-V2 框架，在视觉基准上训练更快、更准、更省显存。


<details>
  <summary>Details</summary>
Motivation: KAN 强调可解释性与逼近能力，但 B 样条基带来训练开销大、实现复杂、在深模型/大数据上的效率与可扩展性受限。需要一种既保持 KAN 原有可解释与表达优势、又提升计算效率与训练稳定性的替代方案。

Method: 以 k 阶样条理论为依据，用一族截断幂函数作为基函数，构成 TruKAN 层：多项式项 + 截断幂项，支持共享或独立的“结点”（knots）；在训练上采用混合优化（hybrid optimization）与层归一化变体；将 TruKAN 模块嵌入 EfficientNet-V2，系统对比 MLP/KAN/SineKAN/TruKAN 版本（小型与深层架构），并评估共享 vs. 独立结点。

Result: 在复杂视觉任务上，TruKAN 在准确率、训练时间与显存占用均优于其他 KAN 变体；在不同深度与规模的 EfficientNet-V2 设置中保持优势；层归一化和结点共享策略对性能有正向影响（细节未给出但有系统评估）。

Conclusion: 以可解释的截断幂基替代 B 样条，可在维持 KAN 表达力和可解释性的同时显著提升效率与稳定性。TruKAN 在真实视觉基准中超越既有 KAN 模型，显示其在更大规模与更复杂任务中的实用价值。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

</details>


### [3] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 提出DiGAN：用潜变量扩散模型生成纵向脑影像轨迹，并结合注意力引导卷积，提升AD早期识别。


<details>
  <summary>Details</summary>
Motivation: 真实临床纵向数据稀缺、时间间隔不均且多模态不规整，现有深度学习难以建模时间连续性，依赖大量随访数据，导致早期AD识别受限。

Method: 构建Diffusion-Guided Attention Network（DiGAN）：(1) 潜变量扩散模型从有限训练样本合成逼真的纵向神经影像轨迹，补充时间上下文、增强对不均匀访视的鲁棒性；(2) 注意力引导的卷积层捕获结构—时间判别模式，用于区分正常、轻度认知障碍与主观认知下降个体。

Result: 在合成数据与ADNI数据上均优于当前SOTA基线，表明在不均匀纵向设置下表现更佳。

Conclusion: 融合扩散生成与注意力卷积能有效缓解数据稀缺与时间不规则问题，提升AD早期检测潜力。

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

</details>


### [4] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: 提出PriorProbe方法，用于从个体行为中精细恢复个人化先验，并将其与深度网络融合，实现对含糊面部表情的个体化预测显著提升，同时不破坏对真值标签的判断。


<details>
  <summary>Details</summary>
Motivation: 个体的认知先验差异会影响他们对模糊刺激的判断。想要个性化神经网络，需要准确获取这些个体先验。但现有方法要么不可辨识、要么带偏差，难以可靠用于个体层面建模。

Method: 提出基于“人与马尔可夫链蒙特卡洛”（MCMCP）的PriorProbe，对每位受试者进行先验抽样与重构，得到细粒度、可解释的个人先验分布；再将该先验与最先进的表情识别网络融合，用于预测受试者对模糊面孔的分类；并与仅用网络或其他先验来源进行对比评估，同时检验对真值标签推断是否受影响。

Result: 采用PriorProbe得到的个体先验与网络融合后，在预测个体对含糊刺激的分类上取得显著提升，优于仅用网络和其他先验方案；同时保持网络对标注真值数据的辨识能力不下降。

Conclusion: PriorProbe提供了一种通用、可解释的先验获取与融合框架，能有效实现深度神经网络的个性化，在不损害客观准确性的前提下提升对个体判断的预测性能。

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

</details>


### [5] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 提出一个可解释的三维孔隙检测与临界性评估框架：基于CT体数据重建与阈值分割识别500个孔隙，构建24,950条孔隙相互作用连接，并用ML+SHAP解释；发现“归一化表面距离”远超其他特征主导临界性，孔径影响很小，几何形状几乎无关，揭示边界主导的失效机制。


<details>
  <summary>Details</summary>
Motivation: AM构件内部孔隙削弱结构性能且阻碍产业应用；现有自动缺陷检测缺乏可解释性，工程师难以理解临界性判定的物理依据，需要一个既能检测又能给出可解释物理因素的评估框架。

Method: 将连续灰度切片重建为体数据；用基于强度阈值与连通域分析识别孔隙；提取孔隙尺寸、长宽比、占据度、与边界的归一化距离等几何与位置特征；用百分位的欧氏距离准则构建孔隙交互网络；以这些特征训练机器学习模型预测孔隙临界性分数，并用SHAP量化各特征贡献。

Result: 识别500个孔隙并生成24,950条孔隙连接；SHAP显示归一化表面距离对预测的重要性比其他所有特征加总高一个数量级以上；孔径贡献很小，其他几何参数几乎无影响；与表面距离呈强负相关关系的临界性被揭示。

Conclusion: 孔隙临界性主要由其靠近外表面的程度决定，而非尺寸或形状；边界主导的失效机制是关键。所提可解释框架为工艺优化与质量控制提供透明的缺陷评估与可操作洞见。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

</details>


### [6] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 提出4DPC^2hat：首个面向动态点云序列的多模态大语言模型（MLLM），含大规模跨模态数据集与时序推理架构，显著提升动作理解与时间关系推理。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多聚焦静态点云，缺乏对动态点云（4D：时空序列）的理解能力，根因是缺少大规模跨模态数据与难以建模时空运动依赖。

Method: 1) 数据：构建4DPC^2hat-200K，两阶段流程：拓扑一致的4D点构建+两层级描述生成；涵盖44K+动态对象序列、70万帧点云、20万QA，问题类型含计数、时间关系、动作、空间关系、外观。2) 模型：引入Mamba增强的时序推理MLLM，捕获长程依赖与动态模式。3) 训练：提出“失败感知的自举学习”，迭代定位模型薄弱点并生成针对性QA监督以强化相应能力。

Result: 在动作理解与时间关系推理任务上，相比现有模型取得显著性能提升，建立4D动态点云理解强基线。

Conclusion: 结合大规模4D跨模态数据、Mamba时序建模与失败感知自举学习，可有效推进MLLM对动态点云的理解，为4D点云时空推理提供有力基础。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

</details>


### [7] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 提出Ref-AVS场景下的无参考掩膜质量评估任务（MQA-RefAVS），构建包含多类几何与语义错误的基准MQ-RAVSBench，并提出多模态LLM审计器MQ-Auditor，可在不依赖GT的情况下估计IoU、识别错误类型并给出质控建议，显著优于开源与商用MLLM并可提升现有Ref-AVS系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有语言指引的视听分割主要关注产出掩膜，而忽视对掩膜质量的可解释诊断与在线质控；实际应用需要在缺乏GT的推理阶段自动评估与纠错，以提高系统稳健性与可用性。

Method: 1) 定义MQA-RefAVS任务：输入多模态信息与候选掩膜，输出IoU估计、错误类型归因与可执行决策；2) 构建MQ-RAVSBench，覆盖几何与语义错误模式；3) 设计基于MLLM的MQ-Auditor，显式推理多模态与掩膜信息，生成量化（IoU）与质化（错误类型、建议）评估，可与Ref-AVS系统集成。

Result: 在基准上，MQ-Auditor在IoU估计精度、错误识别与决策质量上优于强力开源与商业MLLM；与现有Ref-AVS系统集成后，能有效检测分割失败并为后续改进提供信号。

Conclusion: 面向Ref-AVS的无参考掩膜质量评估是可行且有效的；提出的数据集与方法为多模态分割的可靠性评测与闭环改进提供了新范式，具有实用价值与推广潜力。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

</details>


### [8] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: 提出一种超快三维光声断层成像迭代重建方法GPAIR，通过高斯核表示与GPU加速，在动物实验中对含840万体素的三维目标实现亚秒级重建，显著加速IR并推动临床应用。


<details>
  <summary>Details</summary>
Motivation: 传统三维PACT的迭代重建(IR)能校正伪影，但计算量巨大、耗时从数百秒到数小时，限制了大规模三维成像的实用性，需要一种同时保留IR成像质量且大幅提速的方法。

Method: 将空间离散网格替换为连续各向同性高斯核表示；为声压波推导解析闭式表达；结合GPU上的可微分Triton算子实现高效并行计算，从而在IR框架内实现快速前向与反向投影。

Result: 在动物实验中，对包含约840万体素的三维成像任务实现亚秒级重建速度，显示数量级的加速效果，同时保持IR优势（伪影校正与成像质量）。

Conclusion: GPAIR实现了近实时的大规模三维光声图像重建，显著降低IR的计算负担，为三维PACT的临床转化提供了关键技术推动。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

</details>


### [9] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 研究评估ViT基础模型能否将大量未标注动物图像直接聚成物种级簇。基于5个ViT、5种降维、4种聚类（含监督与无监督）在60物种上系统基准，发现DINOv3嵌入+t-SNE+监督层次聚类可达近乎完美的物种聚类（V-measure 0.958），无监督也有竞争力（0.943），仅拒绝1.14%为需专家复核的离群样本；对长尾分布具鲁棒性，并可通过有意过聚类提取物种内差异（性别、年龄、毛色）。


<details>
  <summary>Details</summary>
Motivation: 生态学中的相机陷阱与图像数据激增，但人工标注耗时费力，成为生物多样性监测规模化的瓶颈。需要评估前沿视觉基础模型是否能在零/低标注条件下，将未标注动物图像自动聚为物种级与物种内生态学有意义的簇，从而减少人工负担并提升分析效率。

Method: 搭建基准：组合5种ViT（含DINOv3等）生成通用嵌入；配合5种降维（如t-SNE等）与4种聚类（2监督、2无监督，包括层次聚类）。在60个物种（30哺乳、30鸟类），每类随机各取200张经验证图像进行实验。评估物种级聚类成功/失败情形，并检查过聚类是否能揭示性别、年龄、表型差异；同时测试对长尾分布的鲁棒性与离群点拒绝。

Result: 最佳配置为DINOv3嵌入+t-SNE+监督层次聚类，V-measure达0.958；纯无监督方法也达0.943且无需先验物种知识，仅1.14%图像被判为离群需专家复核。方法对长尾类别分布稳健；通过刻意过聚类能稳定分离物种内特征（年龄段、性二态、毛色差异）。

Conclusion: ViT基础模型结合适当的降维与聚类，可将大量未标注动物图像有效聚为物种级簇，并挖掘物种内生态学结构，大幅减轻标注负担。提供开源基准与实践建议，便于生态学家按其类群与数据特性选择方法。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

</details>


### [10] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 提出NH-Fair基准，统一评估视觉与多模态（LVLM）在“无伤害的公平性”上的表现，给出系统化ERM调参结论，发现多数去偏方法不优于良好调参的ERM，组合式数据增强在不损失效用下稳定提升公平；LVLM虽更准但仍有子群体差异，扩展规模收益不如架构与训练协议选择。


<details>
  <summary>Details</summary>
Motivation: 现有公平性研究难比较：数据集与指标不统一、视觉与多模态分裂评估、调参不足导致结论不可靠。需一个可复现、调参敏感、跨任务与范式（监督与零样本）的统一基准来衡量“公平且不伤害效用”。

Method: 构建NH-Fair基准：统一数据、指标与训练协议；覆盖图像模型与LVLM、监督与零样本。进行系统化ERM调参研究，评估多种去偏算法（含数据增强），并对比LVLM与纯视觉模型在公平/效用上的权衡与子群体差异。

Result: 1) 识别出显著影响效用与差异的关键训练选择，形成实证调参指南，减少调参搜索成本。2) 多数去偏法在严谨调参下不稳定优于ERM；组合式数据增强稳定带来公平性提升且不损失精度。3) LVLM平均准确率更高但仍有子群体差异；“变大”带来的公平/效用提升小于架构或训练协议优化。

Conclusion: NH-Fair提供可复现、调参感知的公平评估流水线。实践上，先做好严谨ERM调参与采用组合数据增强，往往比使用复杂去偏法更可靠；在多模态与扩展规模上仍需关注子群体差异，优先优化架构与训练协议。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

</details>


### [11] [HY3D-Bench: Generation of 3D Assets](https://arxiv.org/abs/2602.03907)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Dongyuan Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jiaao Yu,Jiachen Xu,Jingwei Huang,Kunhong Li,Lifu Wang,Linus,Penghao Wang,Qingxiang Lin,Ruining Tang,Xianghui Yang,Yang Li,Yirui Guan,Yunfei Zhao,Yunhan Yang,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: HY3D-Bench 提供统一高质量的3D数据生态：25万高保真对象（密闭网格+多视渲染）、结构化部件级分解、和12.5万AIGC合成资产以覆盖长尾分布，并通过训练Hunyuan3D-2.1-Small验证有效性，旨在推动3D生成、感知与应用。


<details>
  <summary>Details</summary>
Motivation: 3D生成领域受限于数据处理与获取瓶颈：缺乏统一高质量可训练数据、缺少可控编辑所需的部件级结构、以及真实世界分布差异与长尾类别稀缺，阻碍模型效果与泛化。

Method: 构建开放生态HY3D-Bench：1) 从大规模仓库蒸馏与清洗25万高保真3D对象，产出可训练制品（密闭网格、Multi-view渲染）；2) 提供结构化的零件级分解以支持细粒度感知与编辑；3) 通过可扩展AIGC合成流程生成12.5万合成资产以弥合真实分布差距与增强长尾多样性；并以此训练小型模型Hunyuan3D-2.1-Small做实证。

Result: 获得统一、规模化且高质量的3D数据基座，包含真实与合成混合资产及部件级标注；训练的Hunyuan3D-2.1-Small显示该基座对模型性能的实证效益。

Conclusion: HY3D-Bench民主化高质量3D数据获取，缓解处理瓶颈，提升可控性与多样性，可推动3D感知、机器人与数字内容创作的研究与应用。

Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.

</details>


### [12] [Entropy-Aware Structural Alignment for Zero-Shot Handwritten Chinese Character Recognition](https://arxiv.org/abs/2602.03913)
*Qiuming Luo,Tao Zeng,Feng Li,Heming Liu,Rui Mao,Chang Kong*

Main category: cs.CV

TL;DR: 提出一种基于信息论的结构对齐网络，用熵先验、双视角部件树和Top-K语义融合来缩小视觉-语义差距，实现零样本汉字识别的SOTA与高数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有零样本汉字识别多将汉字视为扁平的部件序列，忽略层次拓扑与不同部件的信息密度差异，导致视觉-语义对齐不足、歧义难消解。

Method: 1) 信息熵先验：通过对位置信息进行乘性调制，提升辨别性高的“根/部件”，抑制常见但信息量低的组件；2) 双视角部件树：构建能抽取多粒度结构特征的树表示，并用Sigmoid门控自适应融合全局布局与局部空间角色；3) Top-K语义特征融合：在解码时利用语义邻居的质心进行特征级一致性校正，缓解视觉歧义；整体实现视觉-语义结构对齐。

Result: 在具有挑战性的零样本设置下显著优于CLIP等基线，达到新的SOTA；在少量样本支持下仍能快速适配，显示出优异的数据效率。

Conclusion: 信息熵驱动的结构对齐和多视角层次表示能有效弥合视觉与语义的差距，提升零样本汉字识别的准确率与适配效率。

Abstract: Zero-shot Handwritten Chinese Character Recognition (HCCR) aims to recognize unseen characters by leveraging radical-based semantic compositions. However, existing approaches often treat characters as flat radical sequences, neglecting the hierarchical topology and the uneven information density of different components. To address these limitations, we propose an Entropy-Aware Structural Alignment Network that bridges the visual-semantic gap through information-theoretic modeling. First, we introduce an Information Entropy Prior to dynamically modulate positional embeddings via multiplicative interaction, acting as a saliency detector that prioritizes discriminative roots over ubiquitous components. Second, we construct a Dual-View Radical Tree to extract multi-granularity structural features, which are integrated via an adaptive Sigmoid-based gating network to encode both global layout and local spatial roles. Finally, a Top-K Semantic Feature Fusion mechanism is devised to augment the decoding process by utilizing the centroid of semantic neighbors, effectively rectifying visual ambiguities through feature-level consensus. Extensive experiments demonstrate that our method establishes new state-of-the-art performance, significantly outperforming existing CLIP-based baselines in the challenging zero-shot setting. Furthermore, the framework exhibits exceptional data efficiency, demonstrating rapid adaptability with minimal support samples.

</details>


### [13] [Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science](https://arxiv.org/abs/2602.03915)
*Levi Lingsch,Georgios Kissas,Johannes Jakubik,Siddhartha Mishra*

Main category: cs.CV

TL;DR: 论文提出Phaedra，一种面向科学图像/场数据的图像分词器，强调保持PDE物理与谱特性；相较现有视觉导向的tokenizer，在重建精度与OOD泛化上更优。


<details>
  <summary>Details</summary>
Motivation: 现有图像tokenizer主要为真实视觉感知优化，难以兼顾科学数据中的大动态范围、精确幅值与细节，以及需要在物理/频谱空间保真（PDE相关量、谱能量分布等）。因此需要新的tokenizer以更好地服务物理模拟与科学任务。

Method: 系统评估多种现有tokenizer在PDE相关指标上的表现（物理与频谱空间的保真度）。发现它们在细节与幅值精准度上均有不足。据此提出Phaedra：受形状-增益量化（shape-gain quantization）与POD（Proper Orthogonal Decomposition）启发的分解与量化策略，用以更好地捕获结构形态与能量幅值，实现更高保真编码与重建。

Result: 在多类PDE数据集上，Phaedra的重建误差低于基线，在物理与频谱指标上均显著提升；并在分布外任务上表现强：包括已知PDE不同条件、未知PDE、以及真实地球观测与天气数据。

Conclusion: 针对科学图像和物理场数据的需求，Phaedra相较传统视觉tokenizer能更好保留PDE相关的细节与幅值，提升重建与泛化能力，适合下游物理模拟与科学建模。

Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.

</details>


### [14] [SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?](https://arxiv.org/abs/2602.03916)
*Azmine Toushik Wasi,Wahid Faisal,Abdur Rahman,Mahfuz Ahmed Anik,Munem Shahriar,Mohsin Mahmud Topu,Sadia Tasnim Meem,Rahatun Nesa Priti,Sabrina Afroz Mitu,Md. Iqramul Hoque,Shahriyar Zaman Ridoy,Mohammed Eunus Ali,Majd Hawasly,Mohammad Raza,Md Rizwan Parvez*

Main category: cs.CV

TL;DR: SpatiaLab 提出一个面向现实场景的空间推理基准，含1400个问答、6大类30子任务，系统评测多类VLM并显示与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有VLM空间推理评测多基于合成或LLM生成、任务单一、类似益智题，难以反映真实世界的复杂性、噪声与多样空间关系，因此需要一个更真实、全面的评测框架。

Method: 构建SpatiaLab：涵盖相对位置、深度与遮挡、朝向、尺寸与尺度、空间导航、三维几何6大类，每类含5个子类共30种任务；每子类≥25题、每主类≥200题；支持多选与开放式回答；在多种SOTA开源/闭源、通用/推理强化/空间专长VLM上进行系统评测，并与人类对照。

Result: 多选设置下最佳模型InternVL3.5-72B准确率54.93%，人类87.57%；开放式设置各模型再降10–25个百分点，GPT-5-mini最高40.93%，人类64.93%。模型在复杂空间关系、深度知觉、导航与3D几何上表现乏力。

Conclusion: SpatiaLab以真实多样的评测揭示VLM空间推理关键短板，提供面向未来研究的对标基准与改进方向，促使朝更稳健、贴近人类的空间理解发展。

Abstract: Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.

</details>


### [15] [Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers](https://arxiv.org/abs/2602.03918)
*Peihao Xiang,Kaida Wu,Ou Bai*

Main category: cs.CV

TL;DR: 提出Gardener：一种无需数据的一次性块级剪枝方法，通过测量预训练ViT各块权重的信息熵来估计其重要性，从而高效压缩masked自监督视觉Transformer，同时保持迁移性能。


<details>
  <summary>Details</summary>
Motivation: Masked自监督ViT预训练广泛应用，但模型巨大，部署与迁移成本高；尚不清楚各Transformer块对下游性能是否同等重要，缺乏无需数据、低成本评估块重要性的原则性方法。

Method: 观察并验证：预训练后各块权重的信息熵与通过迭代移除+微调得到的“敏感度（oracle）”高度相关。基于此提出Gardener：计算每个块权重的信息熵作为重要性评分，一次性选出冗余块进行删减（无需任何数据与微调过程中的敏感度估计），适用于VideoMAE-B并在不同剪枝率下评测。

Result: Gardener在几乎零额外计算开销下，稳定匹配或优于现有无需数据的剪枝基线，性能接近基于敏感度的剪枝；在最高达91.7%的块被剪的极端情况下仍保持具有竞争力的迁移识别性能。

Conclusion: masked自监督ViT存在显著块级冗余；权重信息熵可作为可靠的重要性指标，信息论分析为模型压缩与资源高效迁移学习提供了原则性、实用的路径。

Abstract: Masked self-supervised vision transformers have become a dominant pretraining paradigm, yet their substantial model size poses significant challenges for resource-constrained deployment and efficient transfer learning. A fundamental question remains: are all transformer blocks equally important for downstream performance? In this paper, we show that block importance in masked self-supervised vision transformers can be accurately estimated without access to any data. Our key finding is that the information entropy of pretrained block weights strongly correlates with oracle sensitivity obtained via iterative block removal and finetuning. This observation enables Gardener, a data-free, one-shot, block-level pruning principle that identifies redundant blocks through simple information-theoretic measurements. We evaluate Gardener on VideoMAE-B across multiple pruning ratios and downstream video recognition benchmarks. Despite its negligible computational overhead, Gardener consistently matches or outperforms existing data-free pruning baselines and closely approaches sensitivity-based pruning. Remarkably, even after pruning up to 91.7\% of blocks, the pruned model retains competitive transfer performance. Our results reveal substantial block-level redundancy in masked self-supervised vision transformers and demonstrate that information-theoretic analysis offers a principled and efficient pathway for model compression and resource-efficient transfer learning.

</details>


### [16] [TiCLS : Tightly Coupled Language Text Spotter](https://arxiv.org/abs/2602.04030)
*Leeje Jang,Yijun Lin,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: 提出TiCLS：在场景文本检测识别中引入字符级预训练语言模型（PLM）的外部语言知识，通过语言学解码器融合视觉与语言特征，缓解短、碎片、歧义文本识别难题，并在ICDAR15与Total-Text上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本spotting方法主要依赖视觉与局部字符依赖，缺乏显式外部语言知识；既有语言整合要么不引入外部知识、要么使用与词级粒度不匹配的预训练模型，导致对含噪、碎片化、歧义文本的鲁棒性不足。

Method: 提出TiCLS端到端框架：引入可由字符级PLM初始化的“语言学解码器”，在解码阶段融合视觉特征与语言特征，以显式语言先验指导字符序列建模，从而对歧义或局部缺失的视觉线索进行补全与约束。

Result: 在ICDAR 2015与Total-Text数据集实验中，性能达到或刷新SOTA，验证了PLM引导的语言整合对场景文本spotting的有效性。

Conclusion: 字符级PLM作为外部语言知识与视觉特征融合，能显著提升场景文本识别的鲁棒性与精度；语言学解码器提供了可扩展的集成路径，值得在更广数据与任务上推广。

Abstract: Scene text spotting aims to detect and recognize text in real-world images, where instances are often short, fragmented, or visually ambiguous. Existing methods primarily rely on visual cues and implicitly capture local character dependencies, but they overlook the benefits of external linguistic knowledge. Prior attempts to integrate language models either adapt language modeling objectives without external knowledge or apply pretrained models that are misaligned with the word-level granularity of scene text. We propose TiCLS, an end-to-end text spotter that explicitly incorporates external linguistic knowledge from a character-level pretrained language model. TiCLS introduces a linguistic decoder that fuses visual and linguistic features, yet can be initialized by a pretrained language model, enabling robust recognition of ambiguous or fragmented text. Experiments on ICDAR 2015 and Total-Text demonstrate that TiCLS achieves state-of-the-art performance, validating the effectiveness of PLM-guided linguistic integration for scene text spotting.

</details>


### [17] [AnyStyle: Single-Pass Multimodal Stylization for 3D Gaussian Splatting](https://arxiv.org/abs/2602.04043)
*Joanna Kaleta,Bartosz Świrta,Kacper Kania,Przemysław Spurek,Marek Kowalski*

Main category: cs.CV

TL;DR: AnyStyle提出一种可前馈、免位姿、零样本的3D重建与风格化框架，支持文本与图像多模态条件，在保持3D几何质量的同时显著提升风格可控性与灵活性，并优于现有SOTA的前馈风格化方法。


<details>
  <summary>Details</summary>
Motivation: 现有无位姿图像集合的前馈3D重建已可行（如基于3DGS），但在重建过程中进行可控风格化仍受限：主要依赖图像条件，导致可控性/灵活性差，难以用自然语言或多模态进行外观控制。

Method: 基于3D Gaussian Splatting的前馈重建骨干，提出可插拔的模块化风格化架构，最小化对主干的结构改动；引入多模态条件输入（文本、参考图像）以进行外观控制；实现免位姿（pose-free）的零样本风格化与重建联合推理。

Result: 在多个数据集上，既保持高质量几何重建，又在风格一致性与可控性上优于现有前馈风格化方法；用户研究显示主观风格质量显著领先于一项SOTA基线。

Conclusion: AnyStyle将多模态可控风格化无缝集成到前馈、免位姿的3D重建中，以极小改动实现通用可插拔方案，兼顾几何质量与风格可控性，优于当前方法并具备良好可扩展性。

Abstract: The growing demand for rapid and scalable 3D asset creation has driven interest in feed-forward 3D reconstruction methods, with 3D Gaussian Splatting (3DGS) emerging as an effective scene representation. While recent approaches have demonstrated pose-free reconstruction from unposed image collections, integrating stylization or appearance control into such pipelines remains underexplored. Existing attempts largely rely on image-based conditioning, which limits both controllability and flexibility. In this work, we introduce AnyStyle, a feed-forward 3D reconstruction and stylization framework that enables pose-free, zero-shot stylization through multimodal conditioning. Our method supports both textual and visual style inputs, allowing users to control the scene appearance using natural language descriptions or reference images. We propose a modular stylization architecture that requires only minimal architectural modifications and can be integrated into existing feed-forward 3D reconstruction backbones. Experiments demonstrate that AnyStyle improves style controllability over prior feed-forward stylization methods while preserving high-quality geometric reconstruction. A user study further confirms that AnyStyle achieves superior stylization quality compared to an existing state-of-the-art approach. Repository: https://github.com/joaxkal/AnyStyle.

</details>


### [18] [A Parameterizable Convolution Accelerator for Embedded Deep Learning Applications](https://arxiv.org/abs/2602.04044)
*Panagiotis Mousouliotis,Georgios Keramidas*

Main category: cs.CV

TL;DR: 提出一种基于HLS的可参数化CNN加速器HW/SW协同设计方法，在多约束（延迟、功耗、面积、成本）下实现更优折中，实验优于非参数化方案且可推广到其他DL应用。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA上CNN加速器多以GOPS为单一目标，忽视嵌入式场景对延迟、功耗、面积、成本等多重约束与权衡的需求，需要一种能在多目标下灵活优化的设计方法。

Method: 采用硬件-软件协同设计：利用高层次综合（HLS）描述CNN加速器并进行参数化（如并行度、数据复用、存储层次、位宽等），通过调整参数在多约束空间进行设计空间探索，便于在不同应用需求下做权衡与优化。

Result: 实验显示该方法在多约束优化上优于非参数化设计，达到更好的综合指标（尽管摘要未给出具体数值），且验证了方法的通用性。

Conclusion: 参数化、基于HLS的HW/SW协同设计能在嵌入式DL应用中实现更优的性能-功耗-面积-成本折中，并具备可扩展性与可迁移性，可推广到其他DL任务。

Abstract: Convolutional neural network (CNN) accelerators implemented on Field-Programmable Gate Arrays (FPGAs) are typically designed with a primary focus on maximizing performance, often measured in giga-operations per second (GOPS). However, real-life embedded deep learning (DL) applications impose multiple constraints related to latency, power consumption, area, and cost. This work presents a hardware-software (HW/SW) co-design methodology in which a CNN accelerator is described using high-level synthesis (HLS) tools that ease the parameterization of the design, facilitating more effective optimizations across multiple design constraints. Our experimental results demonstrate that the proposed design methodology is able to outperform non-parameterized design approaches, and it can be easily extended to other types of DL applications.

</details>


### [19] [Fast, Unsupervised Framework for Registration Quality Assessment of Multi-stain Histological Whole Slide Pairs](https://arxiv.org/abs/2602.04046)
*Shikha Dubey,Patricia Raciti,Kristopher Standish,Albert Juan Ramon,Erik Ames Burlingame*

Main category: cs.CV

TL;DR: 提出一种无需GT、快速的WSI配准质量评估框架，结合下采样组织掩膜的全局结构一致性与变形场的局部平滑/连续/真实度指标，能与多专家评估高度相关，适用于大规模数字病理质控。


<details>
  <summary>Details</summary>
Motivation: WSI间（如H&E与IHC）高保真配准对分子整合分析关键，但缺乏GT导致评价困难；现有基于标注地标或强度相似度的方法耗时、易受噪声影响且计算开销大，限制了大规模应用。

Method: 构建无监督RQA框架：1）利用下采样的组织掩膜计算全局结构对应度指标（评估宏观对齐）；2）从配准变形场提取局部平滑性、连续性与形变真实性指标（评估微观合理性）；将二者联合作为质量评分。跨多IHC标记及多专家读片进行相关性验证。

Result: 自动化联合指标与人类专家评分高度相关；在无GT条件下能快速、实时地评估H&E-IHC配准质量，计算资源需求低。

Conclusion: 该无监督、低成本、实时的RQA方法在WSI配准质控中可靠且可扩展，适合大规模数字病理工作流与多标记场景。

Abstract: High-fidelity registration of histopathological whole slide images (WSIs), such as hematoxylin & eosin (H&E) and immunohistochemistry (IHC), is vital for integrated molecular analysis but challenging to evaluate without ground-truth (GT) annotations. Existing WSI-level assessments -- using annotated landmarks or intensity-based similarity metrics -- are often time-consuming, unreliable, and computationally intensive, limiting large-scale applicability. This study proposes a fast, unsupervised framework that jointly employs down-sampled tissue masks- and deformations-based metrics for registration quality assessment (RQA) of registered H&E and IHC WSI pairs. The masks-based metrics measure global structural correspondence, while the deformations-based metrics evaluate local smoothness, continuity, and transformation realism. Validation across multiple IHC markers and multi-expert assessments demonstrate a strong correlation between automated metrics and human evaluations. In the absence of GT, this framework offers reliable, real-time RQA with high fidelity and minimal computational resources, making it suitable for large-scale quality control in digital pathology.

</details>


### [20] [Artifact Removal and Image Restoration in AFM:A Structured Mask-Guided Directional Inpainting Approach](https://arxiv.org/abs/2602.04051)
*Juntao Zhang,Angona Biswas,Jaydeep Rade,Charchit Shukla,Juan Ren,Anwesha Sarkar,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CV

TL;DR: 提出一个轻量、全自动的AFM图像伪影检测与修复流程：先分类判别是否有伪影；对有伪影的图像用自研轻量语义分割生成精确伪影掩膜；按结构方向自适应扩张掩膜；再用方向性邻域插值补洞并局部高斯平滑，实现连续3D表面还原；支持GUI、实时调参与批处理。实验显示在去除伪影同时保留纳米尺度细节。


<details>
  <summary>Details</summary>
Motivation: AFM图像易受环境噪声、扫描缺陷与探针-样品相互作用影响，产生条纹、缺口等伪影，降低定量与形貌解读的可靠性；现有方法要么人工密集、要么通用性差或损伤细节，亟需一个自动、轻量、几何感知且能批处理的解决方案。

Method: 构建两阶段流水线：1) 轻量分类器检测是否含伪影；2) 若有伪影，启用针对AFM数据训练的轻量语义分割网络生成伪影掩膜；对掩膜按结构走向自适应扩张；使用方向性邻域插值进行掩膜内修补以保持3D表面连续；对修复区域进行局部高斯平滑以消除边界；集成到带实时参数调整与批处理的GUI。

Result: 实验表明该系统能有效检测与去除伪影，修复后结构连续性与纳米级细节得到保留；在精度与视觉质量上优于常规非定向插值/平滑方案，同时保持推理轻量、可实时交互与批处理。

Conclusion: 该方法提供了一个几何感知、轻量、全自动的AFM伪影检测与修复框架，兼顾准确性与细节保真，适合高通量与实时应用，为高保真AFM数据解读提供实用工具。

Abstract: Atomic Force Microscopy (AFM) enables high-resolution surface imaging at the nanoscale, yet the output is often degraded by artifacts introduced by environmental noise, scanning imperfections, and tip-sample interactions. To address this challenge, a lightweight and fully automated framework for artifact detection and restoration in AFM image analysis is presented. The pipeline begins with a classification model that determines whether an AFM image contains artifacts. If necessary, a lightweight semantic segmentation network, custom-designed and trained on AFM data, is applied to generate precise artifact masks. These masks are adaptively expanded based on their structural orientation and then inpainted using a directional neighbor-based interpolation strategy to preserve 3D surface continuity. A localized Gaussian smoothing operation is then applied for seamless restoration. The system is integrated into a user-friendly GUI that supports real-time parameter adjustments and batch processing. Experimental results demonstrate the effective artifact removal while preserving nanoscale structural details, providing a robust, geometry-aware solution for high-fidelity AFM data interpretation.

</details>


### [21] [Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal](https://arxiv.org/abs/2602.04053)
*Rio Aguina-Kang,Kevin James Blackburn-Matzen,Thibault Groueix,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: SeeingThroughClutter提出从单张图像中迭代地“移除-重建”前景物体，以获得结构化3D表示；通过VLM编排检测、分割、修复和3D拟合，逐个消解遮挡与杂乱，达到SOTA鲁棒性且无需特定训练。


<details>
  <summary>Details</summary>
Motivation: 传统单图3D重建依赖语义分割、深度估计等中间任务，在强遮挡与杂乱场景下表现不佳；需要一种能在复杂场景中稳定提取并建模多物体的方案。

Method: 提出迭代式对象移除与重建流水线：以VLM为“协调器”依次进行目标检测→分割→前景移除/图像修复→对被移除对象进行3D拟合；每次移除前景后再对剩余场景重复该过程，使后续对象获得更干净的分割信号。无需额外任务特定训练，直接使用基础模型能力。

Result: 在3D-Front与ADE20K上展示出对遮挡与杂乱的强鲁棒性并达成SOTA水平；实验表明逐步移除前景能显著提升后续对象的分割与重建质量。

Conclusion: 将复杂场景分解为一系列可控的“单对象”子任务，结合VLM编排和基础模型能力，可在无需特定训练的情况下实现更可靠的单图多物体3D重建，且方法会随基础模型进步而受益。

Abstract: We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: https://rioak.github.io/seeingthroughclutter/

</details>


### [22] [iSight: Towards expert-AI co-assessment for improved immunohistochemistry staining interpretation](https://arxiv.org/abs/2602.04063)
*Jacob S. Leiby,Jialu Yao,Pan Lu,George Hu,Anna Davidian,Shunsuke Koga,Olivia Leung,Pravin Patel,Isabella Tondi Resta,Rebecca Rojansky,Derek Sung,Eric Yang,Paul J. Zhang,Emma Lundberg,Dokyoon Kim,Serena Yeung-Levy,James Zou,Thomas Montine,Jeffrey Nirschl,Zhi Huang*

Main category: cs.CV

TL;DR: 提出HPA10M超千万张IHC图像数据集与多任务模型iSight，用于自动评估IHC染色（强度、位置、数量、组织类型、良恶性），在多指标上优于现有基础模型，并与病理医师协同提升一致性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有病理AI多聚焦于H&E切片，迁移到IHC受域偏差限制；IHC评估主观性强、跨机构差异大、标注成本高，临床需要可校准、可泛化、能与元数据结合的自动化评估系统与大规模标准化数据资源。

Method: 构建含10,495,672张IHC图像与丰富元数据的HPA10M，覆盖45种正常组织与20类主要癌种；提出iSight多任务学习框架，融合全视野图像视觉特征与组织元数据，通过token级注意力联合预测染色强度、位置、数量、组织类型与恶性状态，并评估校准性能与人机协作效果。

Result: 在保留测试集上，iSight位置预测准确率85.5%，强度76.6%，数量75.7%，较微调的基础模型（PLIP、CONCH）提升2.5–10.2个百分点；校准良好（ECE 0.0150–0.0408）。在包含8名病理医生的用户研究中，iSight在HPA外部图像上单独表现优于初始医生判断（位置79% vs 68%，强度70% vs 57%，数量68% vs 52%）；AI辅助提升医师间一致性（Cohen’s κ：HPA 0.63→0.70；Stanford TMAD 0.74→0.76）。

Conclusion: HPA10M为IHC领域提供大规模标准化数据基座；iSight实现多任务、图像与元数据融合与良好校准，优于现有模型，并在专家协助场景中提升一致性与准确性，具备融入临床工作流以增强IHC评估可靠性与可重复性的潜力。

Abstract: Immunohistochemistry (IHC) provides information on protein expression in tissue sections and is commonly used to support pathology diagnosis and disease triage. While AI models for H\&E-stained slides show promise, their applicability to IHC is limited due to domain-specific variations. Here we introduce HPA10M, a dataset that contains 10,495,672 IHC images from the Human Protein Atlas with comprehensive metadata included, and encompasses 45 normal tissue types and 20 major cancer types. Based on HPA10M, we trained iSight, a multi-task learning framework for automated IHC staining assessment. iSight combines visual features from whole-slide images with tissue metadata through a token-level attention mechanism, simultaneously predicting staining intensity, location, quantity, tissue type, and malignancy status. On held-out data, iSight achieved 85.5\% accuracy for location, 76.6\% for intensity, and 75.7\% for quantity, outperforming fine-tuned foundation models (PLIP, CONCH) by 2.5--10.2\%. In addition, iSight demonstrates well-calibrated predictions with expected calibration errors of 0.0150-0.0408. Furthermore, in a user study with eight pathologists evaluating 200 images from two datasets, iSight outperformed initial pathologist assessments on the held-out HPA dataset (79\% vs 68\% for location, 70\% vs 57\% for intensity, 68\% vs 52\% for quantity). Inter-pathologist agreement also improved after AI assistance in both held-out HPA (Cohen's $κ$ increased from 0.63 to 0.70) and Stanford TMAD datasets (from 0.74 to 0.76), suggesting expert--AI co-assessment can improve IHC interpretation. This work establishes a foundation for AI systems that can improve IHC diagnostic accuracy and highlights the potential for integrating iSight into clinical workflows to enhance the consistency and reliability of IHC assessment.

</details>


### [23] [VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding](https://arxiv.org/abs/2602.04094)
*Junbo Zou,Ziheng Huang,Shengjie Zhang,Liwen Zhang,Weining Shen*

Main category: cs.CV

TL;DR: VideoBrain 通过“语义检索 + 稠密采样”的双代理策略与行为感知奖励，让VLM在长视频中自适应取帧与判断信息是否足够，以更少帧数取得更高任务准确率，并具备跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 长视频理解需要在数千帧中捕获分散信息，受计算预算限制。现有策略要么均匀采样导致信息遗漏，要么一次性关键帧选择，错误不可恢复，且已有代理方法常由仅文本LLM调度视觉工具，缺乏直接感知与信息充分性判断。

Method: 提出端到端的 VideoBrain：VLM直接看帧并学会何时、如何取样。包含两个互补代理：1）CLIP-based 语义代理进行全视频语义检索；2）Uniform 代理在候选时间段内进行稠密时间采样。设计行为感知奖励与数据分类管线，教会模型在何种情形下调用代理真正有益，避免为拿奖励而滥用调用。

Result: +3.5%~+9.0% 超过基线，同时减少30%~40%帧使用量；并在短视频基准上展示良好跨数据集泛化。

Conclusion: 自适应、可恢复的取样策略与行为约束奖励使VLM在长视频理解中更高效、更稳健，兼具性能提升与计算节省，并具有泛化潜力。

Abstract: Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.

</details>


### [24] [DMS2F-HAD: A Dual-branch Mamba-based Spatial-Spectral Fusion Network for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2602.04102)
*Aayushma Pant,Lakpa Tamang,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 提出DMS2F-HAD：基于双分支Mamba的高光谱异常检测模型，线性时序建模分别抽取空间与光谱特征，并通过动态门控融合；在14个基准数据集上达成98.78%平均AUC，推理速度较同类深度方法快4.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有深度方法要么难以捕获长程光谱依赖（CNN），要么计算代价高（Transformer），同时高光谱数据维度高、噪声多且无标注，亟需高效且鲁棒的异常检测模型。

Method: 设计双分支Mamba架构：利用Mamba的线性时间序列建模分别在空间分支与光谱分支中学习异构特征；随后通过动态门控融合机制自适应整合两类特征以提升异常定位。

Result: 在14个HAD基准数据集上取得平均AUC 98.78%的SOTA表现；推理速度较可比深度学习方法提升约4.6倍，体现出效率与精度的兼顾。

Conclusion: DMS2F-HAD兼具高效建模长程依赖与跨模态（空间/光谱）特征融合能力，具备良好的泛化与可扩展性，适合实际高光谱异常检测应用。

Abstract: Hyperspectral anomaly detection (HAD) aims to identify rare and irregular targets in high-dimensional hyperspectral images (HSIs), which are often noisy and unlabelled data. Existing deep learning methods either fail to capture long-range spectral dependencies (e.g., convolutional neural networks) or suffer from high computational cost (e.g., Transformers). To address these challenges, we propose DMS2F-HAD, a novel dual-branch Mamba-based model. Our architecture utilizes Mamba's linear-time modeling to efficiently learn distinct spatial and spectral features in specialized branches, which are then integrated by a dynamic gated fusion mechanism to enhance anomaly localization. Across fourteen benchmark HSI datasets, our proposed DMS2F-HAD not only achieves a state-of-the-art average AUC of 98.78%, but also demonstrates superior efficiency with an inference speed 4.6 times faster than comparable deep learning methods. The results highlight DMS2FHAD's strong generalization and scalability, positioning it as a strong candidate for practical HAD applications.

</details>


### [25] [SuperPoint-E: local features for 3D reconstruction via tracking adaptation in endoscopy](https://arxiv.org/abs/2602.04108)
*O. Leon Barbed,José M. M. Montiel,Pascal Fua,Ana C. Murillo*

Main category: cs.CV

TL;DR: 提出SuperPoint-E，通过“跟踪自适应”监督强化内窥镜视频中的局部特征提取，显著提升SfM重建的密度与稳健性。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频中纹理重复、低对比和形变导致传统局部特征（含原版SuperPoint与COLMAP管线）在检测稀疏、匹配脆弱、跟踪易丢失，进而限制SfM重建的覆盖度与质量。需要面向内窥镜场景专门优化的特征检测与描述方法。

Method: 提出SuperPoint-E：在SuperPoint框架上引入“Tracking Adaptation”监督策略，以跟踪稳健性为导向进行特征检测器与描述子的联合训练/适配；通过真实内窥镜数据进行广泛实验与消融，寻找最佳配置，并评估检测精度、描述子判别性与在SfM中的表现。

Result: 与多种基线相比，SuperPoint-E检测更密集、检测精度更高，特征在视频中更长久存活；描述子更具判别性，使得引导匹配几乎可省略；在真实内窥镜视频的SfM中获得更稠密的3D点云与更长的可重建片段。

Conclusion: 面向内窥镜场景的特征提取适配显著提升SfM效果。SuperPoint-E在检测、描述与重建上全面优于原版SuperPoint与COLMAP的“金标准”管线。

Abstract: In this work, we focus on boosting the feature extraction to improve the performance of Structure-from-Motion (SfM) in endoscopy videos. We present SuperPoint-E, a new local feature extraction method that, using our proposed Tracking Adaptation supervision strategy, significantly improves the quality of feature detection and description in endoscopy. Extensive experimentation on real endoscopy recordings studies our approach's most suitable configuration and evaluates SuperPoint-E feature quality. The comparison with other baselines also shows that our 3D reconstructions are denser and cover more and longer video segments because our detector fires more densely and our features are more likely to survive (i.e. higher detection precision). In addition, our descriptor is more discriminative, making the guided matching step almost redundant. The presented approach brings significant improvements in the 3D reconstructions obtained, via SfM on endoscopy videos, compared to the original SuperPoint and the gold standard SfM COLMAP pipeline.

</details>


### [26] [JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models](https://arxiv.org/abs/2602.04142)
*Hiroshi Sasaki*

Main category: cs.CV

TL;DR: 提出JSynFlow：一个由LLM合成的日文流程图视觉问答数据集，包含业务任务描述、由DSL渲染的流程图及QA对，用于提升VLM在流程图QA上的表现。


<details>
  <summary>Details</summary>
Motivation: 复杂文档（如流程图）的理解是VLM重要能力，但缺乏大规模高质量带标注的流程图-文本数据，人工构建成本高、耗时长，限制了模型在该领域的精确理解与应用。

Method: 设计合成管线：1) 以多种业务岗位为场景生成任务描述；2) 用DSL自动生成可渲染为流程图的结构化代码；3) 渲染得到流程图图像；4) 基于流程内容用LLM生成多样化QA对；5) 组成JSynFlow数据集，并用于微调VLM。

Result: 使用JSynFlow对VLM进行微调，在流程图相关的视觉问答任务上显著提升性能（相较未微调基线）。数据集已在Hugging Face公开。

Conclusion: 合成式数据集能够有效弥补真实标注的缺乏；JSynFlow为日文流程图理解与QA提供可扩展资源，并验证了利用DSL+LLM生成流程图与QA对以提升VLM能力的可行性。

Abstract: Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explanations. However, developing VLMs with precise flowchart understanding requires large-scale datasets of flowchart images and corresponding text, the creation of which is highly time-consuming. To address this challenge, we introduce JSynFlow, a synthesised visual QA dataset for Japanese flowcharts, generated using large language models (LLMs). Our dataset comprises task descriptions for various business occupations, the corresponding flowchart images rendered from domain-specific language (DSL) code, and related QA pairs. This paper details the dataset's synthesis procedure and demonstrates that fine-tuning with JSynFlow significantly improves VLM performance on flowchart-based QA tasks. Our dataset is publicly available at https://huggingface.co/datasets/jri-advtechlab/jsynflow.

</details>


### [27] [Context Determines Optimal Architecture in Materials Segmentation](https://arxiv.org/abs/2602.04154)
*Mingjian Lu,Pawan K. Tripathi,Mark Shteyn,Debargha Ganguly,Roger H. French,Vipin Chaudhary,Yinghui Wu*

Main category: cs.CV

TL;DR: 提出一个跨成像模态的材料图像分割评估框架，比较多种架构并提供OOD检测与反事实解释，用于指导模型选择与可靠性评估。


<details>
  <summary>Details</summary>
Motivation: 现有分割架构多在单一成像模态上评测，导致在实际部署时不同模态下性能差异被掩盖；材料表征社区缺少根据特定成像条件选择架构与评估模型可可信度的工具。

Method: 构建跨模态评估框架，覆盖SEM、AFM、XCT和光学显微等多种材料成像；在7个数据集上，对6种编码器-解码器组合进行系统比较；同时集成分布外检测（OOD）与反事实解释，分析预测受哪些微结构特征驱动。

Result: 不同情境下的最优架构具有系统性差异：UNet在高对比度2D成像中表现最佳，而DeepLabv3+在最困难的案例中更优；框架还能产出可靠性信号与可解释性分析结果。

Conclusion: 该框架为材料图像分割提供实用指南：根据成像模态与任务难度选择合适架构，并通过OOD与反事实工具评估在新样本上的可信度，弥补了材料表征中模型选型与部署可靠性评估的实践缺口。

Abstract: Segmentation architectures are typically benchmarked on single imaging modalities, obscuring deployment-relevant performance variations: an architecture optimal for one modality may underperform on another. We present a cross-modal evaluation framework for materials image segmentation spanning SEM, AFM, XCT, and optical microscopy. Our evaluation of six encoder-decoder combinations across seven datasets reveals that optimal architectures vary systematically by context: UNet excels for high-contrast 2D imaging while DeepLabv3+ is preferred for the hardest cases. The framework also provides deployment feedback via out-of-distribution detection and counterfactual explanations that reveal which microstructural features drive predictions. Together, the architecture guidance, reliability signals, and interpretability tools address a practical gap in materials characterization, where researchers lack tools to select architectures for their specific imaging setup or assess when models can be trusted on new samples.

</details>


### [28] [Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity](https://arxiv.org/abs/2602.04162)
*Chenhe Du,Qing Wu,Xuanyu Tian,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 提出ISCS，在2D扩散模型重建3D医学影像时，通过控制扩散采样中的随机噪声一致性，缓解层间不连续且无需额外损耗或计算成本，提升多任务3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 3D医学成像需要高质量先验，但3D扩散模型训练昂贵且数据稀缺。常用2D先验逐切重建会因扩散采样的随机性导致层间(z轴)不连续。现有通过z轴正则化缓解但需调参且易过平滑。需一种不增加训练/优化负担、能保持层间一致性的策略。

Method: 从扩散采样随机性来源出发，提出Inter-Slice Consistent Stochasticity (ISCS)：在采样过程中对相邻切片的随机噪声成分进行一致性控制，使其采样轨迹对齐。该方法以插件形式嵌入任何基于2D扩散的3D重建流程，无需新增损失或优化步骤，也不增加计算成本。

Result: 在多个医学成像逆问题数据集上，ISCS显著减少层间不连续并提升3D重建性能（定量和可视化均改善），优于依赖z轴正则的基线，避免过平滑。

Conclusion: 控制层间随机性是利用2D扩散先验实现高保真3D医学成像的有效、原则性途径。ISCS简单、即插即用、零额外成本，并广泛适用于2D扩散主导的3D重建管线。

Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distribution with DMs in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the DMs on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter-slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the z-axis, which introduces sensitive hyper-parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter-Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages interslice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps. Importantly, the proposed ISCS is plug-and-play and can be dropped into any 2D trained diffusion based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter-slice stochasticity is a principled and practically attractive route toward high-fidelity 3D medical imaging with 2D diffusion priors. The code is available at: https://github.com/duchenhe/ISCS

</details>


### [29] [Point2Insert: Video Object Insertion via Sparse Point Guidance](https://arxiv.org/abs/2602.04167)
*Yu Zhou,Xiaoyan Yang,Bojia Zi,Lihan Zhang,Ruijie Sun,Weishi Zheng,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Point2Insert 是一种基于稀疏点提示的视频目标插入框架，仅需少量正/负点即可精细控制插入位置，无需密集掩码；通过两阶段训练与蒸馏策略，在视频插入任务上优于强基线并超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖费时的掩码标注、要么基于指令难以实现精准定位。用户与应用需要低成本且位置可控的对象插入方案，能在视频场景中稳定生效。

Method: 提出稀疏点提示（含正/负点）以表达可插入/不可插入区域。训练分两阶段：阶段一训练插入模型，使其在稀疏点或二值掩码条件下在指定区域生成对象；阶段二利用对象移除模型合成成对视频，继续训练以适配视频插入。此外，引入掩码引导插入模型作为教师，对点引导模型进行知识蒸馏，继承其更高的插入成功率与可靠行为。

Result: 在广泛实验中，Point2Insert 在精度与稳健性上稳定超过强基线，并优于参数量大约10倍的模型。

Conclusion: 稀疏点引导结合两阶段训练与教师蒸馏，可在无需繁琐掩码的前提下实现高质量、精细可控的视频目标插入，并具备优越的参数效率与性能。

Abstract: This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\times$10 more parameters.

</details>


### [30] [Partial Ring Scan: Revisiting Scan Order in Vision State Space Models](https://arxiv.org/abs/2602.04170)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin li,Ming-Ching Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: 提出PRISMamba，一种对旋转鲁棒的Vision SSM扫描与通道过滤框架，在ImageNet-1K上以更少FLOPs与更高吞吐超过VMamba，同时在旋转下稳定。


<details>
  <summary>Details</summary>
Motivation: 现有Vision SSM需将2D图像按固定扫描顺序序列化为1D，扫描顺序会改变空间邻接、破坏目标连续性，并在旋转等几何变换下放大退化，但这一因素常被忽视。

Method: 设计“部分环形扫描”（PRIS）：将图像划分为同心环，在每个环内进行顺序无关的聚合，并通过一组短径向SSM在环之间传递上下文；同时引入部分通道过滤，仅将最有信息的通道走循环环路径，其余走轻量残差支路，从而提升效率与鲁棒性。

Result: 在ImageNet-1K上达到84.5% Top-1、3.9G FLOPs、A100上3054 img/s，相比VMamba更高准确率与吞吐、FLOPs更少；在旋转下性能基本保持，而固定路径扫描方法下降约1~2%。

Conclusion: 扫描顺序设计与通道过滤是提升Vision SSM准确率、效率与旋转鲁棒性的关键且被低估的因素；PRISMamba证明了环形扫描与部分通道路由的有效性。

Abstract: State Space Models (SSMs) have emerged as efficient alternatives to attention for vision tasks, offering lineartime sequence processing with competitive accuracy. Vision SSMs, however, require serializing 2D images into 1D token sequences along a predefined scan order, a factor often overlooked. We show that scan order critically affects performance by altering spatial adjacency, fracturing object continuity, and amplifying degradation under geometric transformations such as rotation. We present Partial RIng Scan Mamba (PRISMamba), a rotation-robust traversal that partitions an image into concentric rings, performs order-agnostic aggregation within each ring, and propagates context across rings through a set of short radial SSMs. Efficiency is further improved via partial channel filtering, which routes only the most informative channels through the recurrent ring pathway while keeping the rest on a lightweight residual branch. On ImageNet-1K, PRISMamba achieves 84.5% Top-1 with 3.9G FLOPs and 3,054 img/s on A100, outperforming VMamba in both accuracy and throughput while requiring fewer FLOPs. It also maintains performance under rotation, whereas fixed-path scans drop by 1~2%. These results highlight scan-order design, together with channel filtering, as a crucial, underexplored factor for accuracy, efficiency, and rotation robustness in Vision SSMs. Code will be released upon acceptance.

</details>


### [31] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

Main category: cs.CV

TL;DR: 提出HoloEv-Net用于事件相机动作识别，通过紧凑全息时空表征与频域全局门控，在准确率和效率上显著超越现有方法，并适配边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有EAR方法存在三大问题：基于体素的稠密表示计算冗余、多分支网络结构冗余、以及忽视频谱信息导致对全局运动模式建模不足。

Method: 1) 紧凑全息时空表征（CHSR）：抛弃体素网格，将水平空间线索隐式嵌入Time-Height (T-H) 视图，在2D中保留3D时空上下文，从而同时缓解表示与结构冗余。2) 全局频谱门控（GSG）：利用FFT在频域进行全局token混合，以极小参数开销增强表征能力。整体框架称为HoloEv-Net，包含Base与Small两个规模。

Result: 在THU-EACT-50-CHL、HARDVS、DailyDVS-200上，HoloEv-Net-Base分别超越SOTA 10.29%、1.71%、6.25%。轻量版HoloEv-Net-Small在保持竞争精度的同时，相比重型基线参数降5.4倍、FLOPs降约300倍、延迟降2.4倍。

Conclusion: 将3D时空信息压缩到2D表示并结合频域全局混合，可在事件视频动作识别中兼顾高精度与高效率；方法可扩展且适合边缘部署。

Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.

</details>


### [32] [Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184)
*Angel Martinez-Sanchez,Parthib Roy,Ross Greer*

Main category: cs.CV

TL;DR: 论文在真实世界数据集 doScenes 上，将开放式自然语言指令融入端到端驾驶模型 OpenEMMA，实现指令条件轨迹规划，并显示指令能大幅减少极端失败并细化轨迹对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的驾驶多依赖仿真或受限指令词表，难以泛化到真实场景。需要一个将自由形式、具指代性的乘客语言与真实车辆运动数据对齐的基准，用以检验语言对规划的影响。

Method: 构建并使用 doScenes：把自由文本乘客式指令与 nuScenes 场景及其真值运动对应。将这些指令作为语言提示接入开源 MLLM 驱动的端到端框架 OpenEMMA（输入：前视相机+自车状态；输出：10 步速度-曲率轨迹），形成可复现实验管线；在 849 个标注场景上以 ADE 评估。

Result: 与无指令基线相比，语言条件显著提升鲁棒性，极端失效被抑制，平均 ADE 降低 98.7%；在剔除离群失效后，优质表述的指令仍可带来最高 5.1% 的 ADE 改善。

Conclusion: 自由形式乘客指令在真实世界端到端规划中有效：既能避免大失败，又能细化轨迹对齐。文中分析了对 OpenEMMA 有效的“好指令”特征，并开源评测提示与脚本，确立可复现的指令感知规划基线。

Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning

</details>


### [33] [DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding](https://arxiv.org/abs/2602.04188)
*Ning Zhang,Zhengyu Li,Kwong Weng Loh,Mingxi Xu,Qi Wang,Zhengyu Wen,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: DiMo 是一种离散扩散式的遮蔽建模框架，通过迭代“掩码-填充”令牌细化，统一实现文本生成动作（T2M）、动作生成文本（M2T）与无文本动作到动作（M2M），并通过RVQ提升动作令牌保真度、GRPO提升对齐与可控性，在HumanML3D与KIT-ML上取得高质量与竞争性的双向理解与生成，并支持补全、预测、纠错等零改架构任务。


<details>
  <summary>Details</summary>
Motivation: 现有掩码建模多聚焦于T2M，缺少统一、双向的文本—动作理解与生成能力；自回归方法顺序解码带来延迟与难以权衡质量/时延的问题，且动作离散化保真度与对齐/可控性不足。

Method: 提出DiMo：以离散扩散式遮蔽建模迭代细化动作或文本令牌，统一T2M、M2T、M2M解码范式；通过设置迭代步数在推理中实现质量-时延折中；采用残差向量量化（RVQ）提高动作令牌表达力；引入Group Relative Policy Optimization（GRPO）以强化文本-动作对齐和可控性；在HumanML3D与KIT-ML上训练与评测，并展示补全、预测、caption纠错等能力。

Result: 在HumanML3D与KIT-ML上实现高质量动作生成与有竞争力的双向理解性能；展示在无文本补全、文本引导预测、caption纠错等任务上的有效性；提供项目页定性结果。

Conclusion: DiMo通过迭代遮蔽细化与RVQ、GRPO的结合，将文本—动作的生成与理解统一到一个框架中，兼顾质量与延迟，并具备更强的对齐与可控性，扩展到多种下游任务而无需改动架构。

Abstract: Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.

</details>


### [34] [Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution](https://arxiv.org/abs/2602.04193)
*Hyeonjae Kim,Dongjin Kim,Eugene Jin,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 提出一种基于流匹配的潜在退化空间建模，从单张HR合成真实感LR，用于大规模真实世界超分辨训练，显著提升传统与任意尺度SR表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度SR多在合成退化（如双三次下采样）上表现好，但面对真实图像中的噪声、模糊、压缩伪影等复杂非线性退化效果差；收集真实LR-HR成对数据成本高且尺度受限，难以覆盖多样退化与未知倍率。

Method: 构建潜在退化空间并采用flow matching学习退化分布，从单张HR通过该潜在空间生成带真实伪影、可跨未见退化水平的LR；由此自动合成大规模、覆盖广的真实感LR-HR训练数据集，适配传统固定倍率与任意尺度SR模型训练。

Result: 定量与定性实验表明，合成LR能逼真复现真实退化；基于这些数据训练的传统与任意尺度SR模型在多基准与真实场景上均有稳定、显著的HR重建提升。

Conclusion: 利用流匹配学习潜在退化空间可从HR合成逼真LR，突破数据获取与退化覆盖的瓶颈，为真实世界SR提供通用、可扩展的数据生成方案，显著提升SR效果。

Abstract: While deep learning-based super-resolution (SR) methods have shown impressive outcomes with synthetic degradation scenarios such as bicubic downsampling, they frequently struggle to perform well on real-world images that feature complex, nonlinear degradations like noise, blur, and compression artifacts. Recent efforts to address this issue have involved the painstaking compilation of real low-resolution (LR) and high-resolution (HR) image pairs, usually limited to several specific downscaling factors. To address these challenges, our work introduces a novel framework capable of synthesizing authentic LR images from a single HR image by leveraging the latent degradation space with flow matching. Our approach generates LR images with realistic artifacts at unseen degradation levels, which facilitates the creation of large-scale, real-world SR training datasets. Comprehensive quantitative and qualitative assessments verify that our synthetic LR images accurately replicate real-world degradations. Furthermore, both traditional and arbitrary-scale SR models trained using our datasets consistently yield much better HR outcomes.

</details>


### [35] [VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents](https://arxiv.org/abs/2602.04202)
*Feng Wang,Yichun Shi,Ceyuan Yang,Qiushan Guo,Jingxiang Sun,Alan Yuille,Peng Wang*

Main category: cs.CV

TL;DR: VTok 提出一种统一的视频标记化框架：用一个关键帧保留完整空间特征，其后每帧仅用一个“残差”token 表达相对变化，从而把复杂度从“帧数×每帧token数”降为“帧数+每帧token数”。在视频理解与文生视频上均优于基线，并生成更连贯运动与更强指令跟随。


<details>
  <summary>Details</summary>
Motivation: 现有视听多模态系统常用稀疏采帧并逐帧标记，导致序列过长、时空冗余大，且时间一致性差。需要一种既紧凑又能表达时序变化的通用视频表示，以同时提升视频理解和生成效果与效率。

Method: 将时空表征解耦：选取单个关键帧保留其完整空间token；对后续每帧，仅编码相对关键帧的残差信息为单一残差token，捕捉视角与运动变化。由此，视频token长度从乘积关系变为求和关系。将该标记化用于理解与文生视频模型，评估效率与性能。

Result: 在多项基准上超过采用朴素逐帧标记化的基线：TV-Align 准确率提升约3.4%，VBench 得分提升约1.9%；同时每段视频的token序列更短。生成结果呈现更连贯的运动与更强的文本条件遵循。

Conclusion: VTok 提供了一种紧凑而表达力强的视频标记化范式，通过关键帧空间保留与逐帧残差token实现时空解耦，在理解与生成任务上兼顾性能与效率，值得作为后续视频研究的标准化前端。

Abstract: This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.

</details>


### [36] [AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting](https://arxiv.org/abs/2602.04204)
*Chao Li,Rui Zhang,Siyuan Huang,Xian Zhong,Hongbo Jiang*

Main category: cs.CV

TL;DR: 该论文指出：预测误差受先验质量下界限制，并提出AGMA方法，通过从数据中提取多样行为模式并蒸馏为场景自适应的高斯混合先验，从而提升人群轨迹预测的准确性与多样性，取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 人类轨迹具有多模态特性，但现有方法的固定或学习先验往往与真实分布错位，无法覆盖合理未来的全分布，导致准确率与多样性受限。作者从理论上证明预测误差的下界由先验质量决定，因此高质量先验是性能瓶颈。

Method: 提出AGMA（Adaptive Gaussian Mixture Anchors）。两阶段：1）训练期从数据中挖掘并学习多样化行为模式，构建具有表达力的高斯混合“锚”集合；2）推理期将这些模式蒸馏为与场景条件相关的全局自适应先验，用于引导生成/采样，提升覆盖与匹配。

Result: 在ETH-UCY、Stanford Drone、JRDB三大数据集上，实现最新最优（SOTA）的准确性与多样性指标，显示出较现有方法明显改进。

Conclusion: 高质量、场景自适应的先验是轨迹预测的关键。通过AGMA构建并应用表达力强的高斯混合先验，可有效降低预测误差并增强多样性，验证了先验建模的重要性。

Abstract: Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.

</details>


### [37] [Adaptive 1D Video Diffusion Autoencoder](https://arxiv.org/abs/2602.04220)
*Yao Teng,Minxuan Lin,Xian Liu,Shuai Wang,Xiao Yang,Xihui Liu*

Main category: cs.CV

TL;DR: One-DVA 是一种将视频压缩到一维可变长度潜表示，并用扩散解码还原像素的视频自编码器，解决固定码率、CNN不灵活和确定性解码细节不足的问题，在相同压缩率下重建质量可比3D-CNN VAE，并可自适应达到更高压缩率，同时正则潜分布与微调解码器以利于下游生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频自编码器存在三大痛点：固定码率导致简单视频浪费token；依赖CNN架构限制了可变长度潜表示的建模；确定性解码器难以从强压缩潜向量恢复合理细节，影响重建与下游生成质量。

Method: 提出One-DVA：1) 编码器使用基于查询的视觉Transformer抽取时空特征并输出一维潜表示；2) 通过可变长度dropout机制动态调节潜向量长度，实现自适应压缩；3) 解码器采用像素空间扩散Transformer，以潜表示为条件进行重建；4) 两阶段训练：先重建训练达到与3D-CNN VAE可比的指标，再对潜分布进行正则化并微调解码器以减轻生成过程伪影。

Result: 在相同压缩率下重建指标与3D-CNN VAE相当；由于支持自适应压缩，可在更高压缩率下保持较好质量；经分布正则与解码器微调后，更适配下游潜空间生成并减少伪影。

Conclusion: One-DVA通过Transformer编码+扩散解码与可变长度潜表示，实现自适应视频压缩与高质量重建，兼顾重建性能与下游生成友好性，优于固定码率、确定性解码的传统视频自编码器。

Abstract: Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.

</details>


### [38] [An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation](https://arxiv.org/abs/2602.04227)
*Hanuman Verma,Kiho Im,Pranabesh Maji,Akshansh Gupta*

Main category: cs.CV

TL;DR: 论文提出将直觉模糊逻辑引入UNet（IF-UNet）以提升MRI脑分割的精度与不确定性处理能力，并在IBSR数据集上以Accuracy、Dice、IoU验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统UNet等CNN在脑MRI分割中受部分体素效应与边界模糊影响，难以有效建模组织归属不确定性，需要一种能显式表达不确定性的机制提升分割鲁棒性与可信度。

Method: 构建IF-UNet，在UNet流水线中以直觉模糊集三元（隶属度、非隶属度、犹豫度）表征输入与中间表征，对组织模糊与边界不确定性进行建模；在IBSR数据集上训练与评估，采用Accuracy、Dice、IoU作为指标。

Result: 与基线UNet相比，IF-UNet在IBSR上取得更高的Accuracy、Dice与IoU，定性上边界更连贯、对部分体素区域更稳健。

Conclusion: 将直觉模糊逻辑嵌入UNet能有效处理脑MRI分割中的不确定性与部分体素效应，提升分割质量；该框架具备推广到其他含显著不确定性的医学影像任务的潜力。

Abstract: Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.

</details>


### [39] [SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction](https://arxiv.org/abs/2602.04240)
*Suzeyu Chen,Leheng Li,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出SPOT-Occ：用原型引导的稀疏Transformer解码器，实现高效、准确的摄像头3D占据预测；以两阶段“选择—聚合”替代昂贵的稠密注意力，速度更快且精度更高。


<details>
  <summary>Details</summary>
Motivation: 相机到3D占据预测需同时满足高精度与实时性。稀疏3D表示缓解了编码开销，但给解码带来挑战：如何在稀疏且非均匀分布的体素特征上高效聚合信息，避免计算代价巨大的稠密注意力。

Method: 提出原型驱动的稀疏Transformer解码器。每个查询自适应从体素特征中选择少量最具代表性的“原型”作为关键上下文，分两步进行：1) 引导式特征选择（稀疏原型选择）；2) 以这些原型为中心的聚合（聚焦注意力）。为保证跨层稳定的查询-原型对应关系，引入基于真值掩码的去噪范式提供显式监督与一致性约束。整体模型称为SPOT-Occ。

Result: 在速度上显著优于现有方法，同时精度也有所提升；实现更快的实时3D占据预测。代码已开源。

Conclusion: 原型引导的稀疏解码策略能在不使用稠密注意力的前提下高效整合稀疏体素信息，兼顾速度与准确性，为摄像头端3D占据预测提供了实用解法。

Abstract: Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention.
  In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation.
  To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.

</details>


### [40] [ACIL: Active Class Incremental Learning for Image Classification](https://arxiv.org/abs/2602.04252)
*Aditya R. Bhattacharya,Debanjan Goswami,Shayok Chakraborty*

Main category: cs.CV

TL;DR: 提出ACIL：在类增量学习中结合主动学习的框架，用不确定性+多样性选取少量样本标注与保留，既降标注成本又减遗忘，并在多视觉数据集上验证优于基线。


<details>
  <summary>Details</summary>
Motivation: 增量学习通常假设每一轮所有样本都有标注，带来高昂且浪费的标注成本（后续轮次又无法访问这些样本）。需要一种方法在无法访问历史数据且分布不断变化时，既减少标注量，又缓解灾难性遗忘。主动学习可挑选信息量大的样本以降低人工标注负担，适合与增量学习结合。

Method: 提出ACIL主动增量学习框架：在每一轮，从大量未标注数据中基于模型不确定性与样本多样性联合准则选择少量“代表性/信息性”样本进行标注；被选中的样本作为“典范”在下一轮一并提供给模型，从而在不保存全部历史数据的条件下维持知识。

Result: 在多个视觉数据集上进行广泛实验，ACIL在精度与遗忘指标上优于相关基线，同时显著减少所需标注量。

Conclusion: 通过在类增量学习中引入不确定性+多样性驱动的主动选择，ACIL能大幅降低标注成本并减轻灾难性遗忘，展现出实用潜力。

Abstract: Continual learning (or class incremental learning) is a realistic learning scenario for computer vision systems, where deep neural networks are trained on episodic data, and the data from previous episodes are generally inaccessible to the model. Existing research in this domain has primarily focused on avoiding catastrophic forgetting, which occurs due to the continuously changing class distributions in each episode and the inaccessibility of the data from previous episodes. However, these methods assume that all the training samples in every episode are annotated; this not only incurs a huge annotation cost, but also results in a wastage of annotation effort, since most of the samples in a given episode will not be accessible to the model in subsequent episodes. Active learning algorithms identify the salient and informative samples from large amounts of unlabeled data and are instrumental in reducing the human annotation effort in inducing a deep neural network. In this paper, we propose ACIL, a novel active learning framework for class incremental learning settings. We exploit a criterion based on uncertainty and diversity to identify the exemplar samples that need to be annotated in each episode, and will be appended to the data in the next episode. Such a framework can drastically reduce annotation cost and can also avoid catastrophic forgetting. Our extensive empirical analyses on several vision datasets corroborate the promise and potential of our framework against relevant baselines.

</details>


### [41] [Depth-Guided Metric-Aware Temporal Consistency for Monocular Video Human Mesh Recovery](https://arxiv.org/abs/2602.04257)
*Jiaxin Cen,Xudong Mao,Guanghui Yue,Wei Zhou,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: 该论文提出一种深度引导的人体网格从单目视频恢复框架，通过融合深度几何先验与RGB特征、度量一致的初始化与跨模态时序对齐，提升尺度一致性与时序稳定性，在多基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 单目视频恢复人体网格长期受深度/尺度歧义影响，导致度量不一致（尺度漂移）、时序抖动以及遮挡下的不稳定。现有方法多靠RGB与平滑正则，难以解决深度排序、遮挡和尺度漂移问题，亟需引入可靠的几何/深度先验并实现跨时间的一致估计。

Method: 提出深度引导的三模块框架：1) 深度引导多尺度融合（DGMF）：以置信度感知门控自适应融合深度几何先验与RGB特征，缓解深度排序与遮挡影响；2) 深度引导的度量感知姿态与形状估计（D-MAPS）：通过深度校准的骨骼统计进行尺度一致的初始化，减少尺度漂移；3) 运动-深度对齐细化（MoDAR）：用跨模态注意力在运动动力学与几何线索间建立约束，强化时序一致与几何准确；整体保持计算高效。

Result: 在三项具有挑战的基准上取得更优效果，显著提升在重遮挡场景下的鲁棒性与空间精度，同时兼顾推理效率；（摘要未给出具体数值，但强调全面领先与稳定性提升）。

Conclusion: 将深度先验系统性引入单目视频人体网格恢复，可显著缓解度量与时序难题。三模块协同带来尺度一致、时序稳定且对遮挡鲁棒的估计，且具备实际可用的效率。

Abstract: Monocular video human mesh recovery faces fundamental challenges in maintaining metric consistency and temporal stability due to inherent depth ambiguities and scale uncertainties. While existing methods rely primarily on RGB features and temporal smoothing, they struggle with depth ordering, scale drift, and occlusion-induced instabilities. We propose a comprehensive depth-guided framework that achieves metric-aware temporal consistency through three synergistic components: A Depth-Guided Multi-Scale Fusion module that adaptively integrates geometric priors with RGB features via confidence-aware gating; A Depth-guided Metric-Aware Pose and Shape (D-MAPS) estimator that leverages depth-calibrated bone statistics for scale-consistent initialization; A Motion-Depth Aligned Refinement (MoDAR) module that enforces temporal coherence through cross-modal attention between motion dynamics and geometric cues. Our method achieves superior results on three challenging benchmarks, demonstrating significant improvements in robustness against heavy occlusion and spatial accuracy while maintaining computational efficiency.

</details>


### [42] [Decoupled Hierarchical Distillation for Multimodal Emotion Recognition](https://arxiv.org/abs/2602.04260)
*Yong Li,Yuanzhi Wang,Yi Ding,Shiqing Zhang,Ke Lu,Cuntai Guan*

Main category: cs.CV

TL;DR: 提出DHMD框架，通过层次化蒸馏提升多模态情感识别，对MOSI/MOSEI在ACC与F1上有稳定增益。核心做法：先将各模态特征解耦为同质(模态无关)与异质(模态专属)部分，再在两种空间中做粗/细两级知识蒸馏（图蒸馏+字典匹配），以缓解模态异质性与贡献不均。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别受限于跨模态异质性强、不同模态贡献度可变，导致对齐困难与信息融合低效；需要一种既能区分共享与专属信息、又能自适应地在模态间传递知识的方法。

Method: 1) 特征解耦：用自回归机制将每个模态划分为模态无关(同质)与模态专属(异质)成分。2) 两阶段蒸馏：a) 粗粒度—图蒸馏单元(GD-Unit)在各解耦空间内构建动态图，进行自适应跨模态蒸馏；b) 细粒度—跨模态字典匹配，将语义粒度对齐，获取更判别的表示。3) 层次化蒸馏在同质/异质空间分别进行，实现灵活知识迁移与更好对齐。

Result: 在CMU-MOSI/CMU-MOSEI上相对提升：ACC7 +1.3%/+2.4%，ACC2 +1.3%/+1.9%，F1 +1.9%/+1.8%；可视化显示图边与字典激活在同质/异质空间呈现有意义分布。

Conclusion: 通过解耦特征并在粗/细两级进行跨模态蒸馏，DHMD有效缓解模态异质与贡献差异问题，增强对齐与表示判别性，达到SOTA水平并具备可解释性。

Abstract: Human multimodal emotion recognition (MER) seeks to infer human emotions by integrating information from language, visual, and acoustic modalities. Although existing MER approaches have achieved promising results, they still struggle with inherent multimodal heterogeneities and varying contributions from different modalities. To address these challenges, we propose a novel framework, Decoupled Hierarchical Multimodal Distillation (DHMD). DHMD decouples each modality's features into modality-irrelevant (homogeneous) and modality-exclusive (heterogeneous) components using a self-regression mechanism. The framework employs a two-stage knowledge distillation (KD) strategy: (1) coarse-grained KD via a Graph Distillation Unit (GD-Unit) in each decoupled feature space, where a dynamic graph facilitates adaptive distillation among modalities, and (2) fine-grained KD through a cross-modal dictionary matching mechanism, which aligns semantic granularities across modalities to produce more discriminative MER representations. This hierarchical distillation approach enables flexible knowledge transfer and effectively improves cross-modal feature alignment. Experimental results demonstrate that DHMD consistently outperforms state-of-the-art MER methods, achieving 1.3\%/2.4\% (ACC$_7$), 1.3\%/1.9\% (ACC$_2$) and 1.9\%/1.8\% (F1) relative improvement on CMU-MOSI/CMU-MOSEI dataset, respectively. Meanwhile, visualization results reveal that both the graph edges and dictionary activations in DHMD exhibit meaningful distribution patterns across modality-irrelevant/-exclusive feature spaces.

</details>


### [43] [KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing](https://arxiv.org/abs/2602.04268)
*Siyu Jiang,Feiyang Chen,Xiaojin Zhang,Kun He*

Main category: cs.CV

TL;DR: 提出KVSmooth：一种在推理阶段、免训练的KV缓存平滑方法，通过注意力熵自适应控制，对键值进行EMA平滑，显著降低MLLM幻觉并提升精度与召回。


<details>
  <summary>Details</summary>
Motivation: MLLM在长序列解码中易发生语义漂移，输出逐渐偏离图像事实，产生对象/属性/关系层面的视觉不一致幻觉；现有方法多需再训练或代价高（如对比解码），难以高效部署。

Method: 在推理时对KV-Cache中的key和value施加指数滑动平均（EMA）以抑制不稳定积累；用每个token的注意力分布熵衡量其“sink”程度（不可靠/塌陷程度），据此自适应调整平滑强度，实现注意力熵引导的自适应平滑。无需改模型或再训练，可即插即用。

Result: 在多项实验中显著降低幻觉：CHAIR_S由41.8降至18.2；整体性能提升：F1由77.5升至79.2，且同时提高precision和recall，优于常见仅提升其一的对比方法。

Conclusion: KVSmooth以低开销在推理期稳定位视听对齐，缓解语义漂移，通用有效，兼顾降低幻觉与提升整体性能，适合实际部署。

Abstract: Despite the significant progress of Multimodal Large Language Models (MLLMs) across diverse tasks, hallucination -- corresponding to the generation of visually inconsistent objects, attributes, or relations -- remains a major obstacle to their reliable deployment. Unlike pure language models, MLLMs must ground their generation process in visual inputs. However, existing models often suffer from semantic drift during decoding, causing outputs to diverge from visual facts as the sequence length increases.
  To address this issue, we propose KVSmooth, a training-free and plug-and-play method that mitigates hallucination by performing attention-entropy-guided adaptive smoothing on hidden states. Specifically, KVSmooth applies an exponential moving average (EMA) to both keys and values in the KV-Cache, while dynamically quantifying the sink degree of each token through the entropy of its attention distribution to adaptively adjust the smoothing strength.
  Unlike computationally expensive retraining or contrastive decoding methods, KVSmooth operates efficiently during inference without additional training or model modification. Extensive experiments demonstrate that KVSmooth significantly reduces hallucination ($\mathit{CHAIR}_{S}$ from $41.8 \rightarrow 18.2$) while improving overall performance ($F_1$ score from $77.5 \rightarrow 79.2$), achieving higher precision and recall simultaneously. In contrast, prior methods often improve one at the expense of the other, validating the effectiveness and generality of our approach.

</details>


### [44] [SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization](https://arxiv.org/abs/2602.04271)
*Lifan Wu,Ruijie Zhu,Yubo Ai,Tianzhu Zhang*

Main category: cs.CV

TL;DR: SkeletonGaussian提出从单目视频生成可编辑的动态3D高斯（4D）表示，通过显式骨架驱动的刚性运动与非刚性细化相结合，兼顾质量与可编辑性，优于现有隐式变形方法。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成多用隐式变形场表示运动，难以进行直观控制与编辑，限制了实际应用（如动作重定向、姿态编辑）。需要一种既能高质量生成，又具备可解释、可编辑运动控制的表示与方法。

Method: 1) 从输入单目视频鲁棒提取骨架；2) 以层次关节-骨架结构显式建模稀疏刚性运动，使用线性蒙皮(Linear Blend Skinning)驱动动态3D高斯；3) 采用基于HexPlane的非刚性细化模块补偿局部细节形变；4) 构建可编辑的运动参数接口，实现动作编辑与重定向。

Result: 在质量和可编辑性上优于现有方法：生成的动态3D对象更一致、更细节保真；支持直观的骨架级别编辑（如关节姿态修改、动作重定向），实验与可视化展示了更好的时空一致性与编辑稳定性。

Conclusion: 通过“骨架显式刚性 + HexPlane非刚性细化”的分层表征，SkeletonGaussian实现了可解释、可编辑且高质量的4D生成，提出了面向编辑的动态3D高斯新范式。

Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/

</details>


### [45] [Light Up Your Face: A Physically Consistent Dataset and Diffusion Model for Face Fill-Light Enhancement](https://arxiv.org/abs/2602.04300)
*Jue Gong,Zihan Zhou,Jingkai Wang,Xiaohong Liu,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出面向“面部补光增强”(FFE)的新数据集与扩散模型，实现只给人脸加虚拟补光、保持原场景与背景不变的可控增强。通过物理一致的渲染器生成16万对带6维可分解光参的成对数据，预训练物理感知的光照提示向量，并在扩散模型上训练高效的一步式补光模型，达到高保真、可控、低成本且更好保留背景照明。


<details>
  <summary>Details</summary>
Motivation: 现有“重光照/重打光”方法常重塑整幅图像的光照，易抹杀输入光感或改变背景，造成前景-背景不一致，不符合实际面部补光需求。缺乏大规模、物理一致、可控参数的成对训练数据也限制了可扩展学习与可控性。

Method: 1) 构建LYF-160K：用物理一致的渲染器向人脸场景注入盘状区域补光，六个可分解控制因子（位置、方向、强度、尺寸等），生成16万对“前/后”样本。2) 预训练PALP：把6D光照参数编码为条件token，并加入平面光重建辅助任务，学习物理感知的光提示。3) 基于预训练扩散骨干训练FiLitDiff：一步式、以物理光码为条件的补光扩散模型，实现可控高保真补光，推理成本低。

Result: 在保留测试对上取得较强的主观感知质量与有竞争力的全参考指标，同时明显更好地保持背景原有照明一致性。

Conclusion: LYF-160K与FiLitDiff共同提供了可控、物理一致、成本低的面部补光增强方案，能在不破坏场景与背景的前提下提升人脸曝光；数据与代码将开源。

Abstract: Face fill-light enhancement (FFE) brightens underexposed faces by adding virtual fill light while keeping the original scene illumination and background unchanged. Most face relighting methods aim to reshape overall lighting, which can suppress the input illumination or modify the entire scene, leading to foreground-background inconsistency and mismatching practical FFE needs. To support scalable learning, we introduce LightYourFace-160K (LYF-160K), a large-scale paired dataset built with a physically consistent renderer that injects a disk-shaped area fill light controlled by six disentangled factors, producing 160K before-and-after pairs. We first pretrain a physics-aware lighting prompt (PALP) that embeds the 6D parameters into conditioning tokens, using an auxiliary planar-light reconstruction objective. Building on a pretrained diffusion backbone, we then train a fill-light diffusion (FiLitDiff), an efficient one-step model conditioned on physically grounded lighting codes, enabling controllable and high-fidelity fill lighting at low computational cost. Experiments on held-out paired sets demonstrate strong perceptual quality and competitive full-reference metrics, while better preserving background illumination. The dataset and model will be at https://github.com/gobunu/Light-Up-Your-Face.

</details>


### [46] [Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement](https://arxiv.org/abs/2602.04304)
*Zipeng Zhu,Zhanghao Hu,Qinglin Zhu,Yuxi Hong,Yijun Liu,Jingyong Su,Yulan He,Lin Gui*

Main category: cs.CV

TL;DR: 提出动态选择注意层的LASER，在无需训练的推理中自适应定位与解码，显著提升多种VQA任务表现。


<details>
  <summary>Details</summary>
Motivation: 固定视觉token预算迫使统一分辨率导致细节丢失与语言幻觉；现有注意力增强依赖经验性“魔法层”，在复杂推理上可迁移性差。

Method: 先做层级敏感性分析，发现不同任务在不同层需要视觉重激活；据此提出基于查询的可视激活指标VAQ，度量各层注意力对输入查询的敏感度，选择与当前查询最相关的层；在此基础上设计训练-free推理流程LASER，按任务自适应选层进行视觉定位与答案解码增强。

Result: 在多种VQA基准、覆盖不同复杂度任务上，LASER显著提升准确率，优于静态层选择与既有注意力引导方法。

Conclusion: 视觉落实是动态的：简单识别依赖中层，复杂搜索/推理需深层重激活。基于VAQ的LASER可在推理时自适应选层，稳健提高VQA性能。

Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static "magic layer" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.

</details>


### [47] [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/abs/2602.04317)
*Zihan Lou,Jinlong Fan,Sihan Ma,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: 提出JOintGS：从单目RGB视频在野外重建可动画高保真3D人体，联合优化相机外参、人体姿态与3D高斯表示，实现实时渲染并显著提升鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 3DGS等点渲染方法虽高质实时，但强依赖精准相机与姿态；在野外场景中，COLMAP/HMR2.0等初始化常不准，导致重建劣化。需要一种能在粗初始化下同时校正相机和姿态并提升重建质量的统一框架。

Method: 提出JOintGS统一框架：显式前景（人体）-背景（静态场景）解耦；通过多视一致的静态背景高斯锚定相机外参；改进相机促进人体时序对齐；优化姿态反过来消除静态约束中的动态伪影；加入时间动态模块建模细粒度姿势相关形变；引入残差颜色场处理光照变化；从粗初始（COLMAP/HMR2.0）出发联合迭代优化。

Result: 在NeuMan与EMDB数据集上，较SOTA在NeuMan上PSNR提升约2.1 dB，并保持实时渲染；对噪声初始化更鲁棒，优于基线。

Conclusion: 联合优化相机、姿态与3D高斯并进行前后景解耦，可在不可靠初始化下实现高保真、可动画的人体重建与实时渲染；时间动态与残差色彩进一步提升细节和光照适应性。

Abstract: Reconstructing high-fidelity animatable 3D human avatars from monocular RGB videos remains challenging, particularly in unconstrained in-the-wild scenarios where camera parameters and human poses from off-the-shelf methods (e.g., COLMAP, HMR2.0) are often inaccurate. Splatting (3DGS) advances demonstrate impressive rendering quality and real-time performance, they critically depend on precise camera calibration and pose annotations, limiting their applicability in real-world settings. We present JOintGS, a unified framework that jointly optimizes camera extrinsics, human poses, and 3D Gaussian representations from coarse initialization through a synergistic refinement mechanism. Our key insight is that explicit foreground-background disentanglement enables mutual reinforcement: static background Gaussians anchor camera estimation via multi-view consistency; refined cameras improve human body alignment through accurate temporal correspondence; optimized human poses enhance scene reconstruction by removing dynamic artifacts from static constraints. We further introduce a temporal dynamics module to capture fine-grained pose-dependent deformations and a residual color field to model illumination variations. Extensive experiments on NeuMan and EMDB datasets demonstrate that JOintGS achieves superior reconstruction quality, with 2.1~dB PSNR improvement over state-of-the-art methods on NeuMan dataset, while maintaining real-time rendering. Notably, our method shows significantly enhanced robustness to noisy initialization compared to the baseline.Our source code is available at https://github.com/MiliLab/JOintGS.

</details>


### [48] [Multiview Self-Representation Learning across Heterogeneous Views](https://arxiv.org/abs/2602.04328)
*Jie Chen,Zhu Wang,Chuanbin Liu,Xi Peng*

Main category: cs.CV

TL;DR: 提出MSRL，通过多视角自表示学习在不同预训练模型特征之间学习不变表示，利用信息传递与分配概率一致性机制，在多基准数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 不同预训练模型（目标/架构不同）对同一样本产生的特征分布差异很大，完全无监督迁移情景下要从大规模无标注多视角特征中学习到跨视角不变表示很困难。

Method: 将多个冻结的预训练骨干输出作为异构多视角特征，每个视角之上接入独立线性头；设计基于自表示学习的信息传递机制，在各视角线性输出间进行特征聚合；提出“分配概率分布一致性”约束，利用跨视角互补信息对自表示进行引导，从而在不同线性模型间强制表示不变；并给出信息传递、分配一致性与增量视角的理论分析。

Result: 在多个视觉基准数据集上进行广泛实验，MSRL在无监督迁移场景下稳定且显著优于多种SOTA方法。

Conclusion: 通过多视角自表示与概率一致性约束，可在无需标签的条件下有效对齐不同预训练模型的特征分布，获得跨视角不变表示并带来性能提升。

Abstract: Features of the same sample generated by different pretrained models often exhibit inherently distinct feature distributions because of discrepancies in the model pretraining objectives or architectures. Learning invariant representations from large-scale unlabeled visual data with various pretrained models in a fully unsupervised transfer manner remains a significant challenge. In this paper, we propose a multiview self-representation learning (MSRL) method in which invariant representations are learned by exploiting the self-representation property of features across heterogeneous views. The features are derived from large-scale unlabeled visual data through transfer learning with various pretrained models and are referred to as heterogeneous multiview data. An individual linear model is stacked on top of its corresponding frozen pretrained backbone. We introduce an information-passing mechanism that relies on self-representation learning to support feature aggregation over the outputs of the linear model. Moreover, an assignment probability distribution consistency scheme is presented to guide multiview self-representation learning by exploiting complementary information across different views. Consequently, representation invariance across different linear models is enforced through this scheme. In addition, we provide a theoretical analysis of the information-passing mechanism, the assignment probability distribution consistency and the incremental views. Extensive experiments with multiple benchmark visual datasets demonstrate that the proposed MSRL method consistently outperforms several state-of-the-art approaches.

</details>


### [49] [Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner](https://arxiv.org/abs/2602.04337)
*Qian-Wei Wang,Guanghao Meng,Ren Cai,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出CoFT/CoFT+：通过双模型跨模态协作与正负文本提示，利用无标注数据适配VLM，避免手工阈值与噪声假设，在两阶段训练中先参数高效微调再全面微调，并结合迭代、动量对比和LLM提示，显著优于现有无监督及少样本监督方法。


<details>
  <summary>Details</summary>
Motivation: VLM如CLIP零样本强，但下游适配需要昂贵标注。现有无监督自训练依赖伪标签，存在置信筛选不可靠、确认偏差、低置信样本未充分利用等问题，限制了适配效果与鲁棒性。

Method: 提出协作式微调CoFT：1) 双模型跨模态协作；2) 双提示学习：针对每个样本使用正/负文本提示，显式建模伪标签“干净度”，无需手工阈值或噪声分布假设；负提示同时正则化轻量视觉适配模块以提升噪声鲁棒性；3) 两阶段训练：先在高置信样本上做参数高效微调（PEFT），再用协作过滤后的伪标签进行全量微调。CoFT+在此基础上加入迭代式微调、动量对比学习，以及由LLM生成的提示词以进一步提升。

Result: 在广泛实验中，CoFT/CoFT+在无监督适配任务上持续超过现有自训练/无监督方法，并且在若干设置下超过少样本监督基线。

Conclusion: 通过跨模态协作与正负提示建模伪标签质量，并结合两阶段与对比/迭代策略，可在无标注条件下有效适配VLM，减少对标注与人工阈值的依赖，提升鲁棒性与性能。

Abstract: Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.

</details>


### [50] [Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning](https://arxiv.org/abs/2602.04340)
*Qian-Wei Wang,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出一种基于双提示（dual-prompt）调优的主动学习框架，专为CLIP在小标注预算下的图像分类适配。正提示提升类别判别力，负提示以反向训练方式显式建模预测正确性的概率，作为不确定性信号选择样本，实验在多种微调范式下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习多用熵或聚类等启发式估计不确定性，未从模型内部机制显式建模不确定性；同时在低标注预算下使CLIP有效适配下游任务仍具挑战，需要既提升判别性能又获得稳健的不确定性估计来指导样本标注。

Method: 在CLIP文本分支引入两个可学习提示：1) 正提示（positive prompt）与轻量视觉侧调优配合，提升任务相关文本嵌入的判别性与分类可靠性；2) 负提示（negative prompt）采用“反向”训练策略，用于显式学习预测正确性的概率，从而提供可校准的不确定性度量；基于该不确定性进行主动样本选择，适配于多种微调范式。

Result: 在不同的CLIP微调设定与数据集上，所提方法在相同标注预算下持续优于现有主动学习基线，表现为更高的分类性能与更有效的样本选择。

Conclusion: 双提示不确定性建模为主动CLIP适配提供了统一而稳健的框架：正提示提升可迁移判别能力，负提示提供原则化不确定性信号，从而在有限标注下实现更高效的主动学习与更好下游性能。

Abstract: Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.

</details>


### [51] [Finding NeMO: A Geometry-Aware Representation of Template Views for Few-Shot Perception](https://arxiv.org/abs/2602.04343)
*Sebastian Jung,Leonard Klüpfel,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: NeMO提出一种面向对象的“神经记忆对象”表示，用少量RGB模板视图就能在RGB图像中对未见过的物体进行检测、分割与6DoF位姿估计，并在BOP等基准上达到SOTA/竞品水平。


<details>
  <summary>Details</summary>
Motivation: 现有少样本/零样本的物体感知常需相机标定、密集模板或目标域再训练，难以快速上新新物体；希望以统一网络在多任务（检测、分割、位姿）中实现对新物体的高效适配，减少工程开销并提升可扩展性。

Method: 提出Neural Memory Object（NeMO）：1）编码器以少量RGB模板视图为输入，借助学习到的UDF（隐式有符号/无符号距离场，这里为UDF）生成包含语义与几何信息的稀疏“类点云”对象编码；2）解码器结合该对象编码与查询图像，输出多种致密预测（如检测、分割、6DoF姿态）；无需相机参数、无需在目标数据上再训练。

Result: 在BOP基准的多数据集、多任务上达到具有竞争力乃至SOTA的结果；验证了少样本条件下的泛化与多任务适用性。

Conclusion: 通过将对象信息外包到NeMO并以单一网络覆盖多类感知任务，方法实现了快速对象接入、较高可扩展性与效率，无需再训练或复杂预处理，适合对新物体的交互与部署场景。

Abstract: We present Neural Memory Object (NeMO), a novel object-centric representation that can be used to detect, segment and estimate the 6DoF pose of objects unseen during training using RGB images. Our method consists of an encoder that requires only a few RGB template views depicting an object to generate a sparse object-like point cloud using a learned UDF containing semantic and geometric information. Next, a decoder takes the object encoding together with a query image to generate a variety of dense predictions. Through extensive experiments, we show that our method can be used for few-shot object perception without requiring any camera-specific parameters or retraining on target data. Our proposed concept of outsourcing object information in a NeMO and using a single network for multiple perception tasks enhances interaction with novel objects, improving scalability and efficiency by enabling quick object onboarding without retraining or extensive pre-processing. We report competitive and state-of-the-art results on various datasets and perception tasks of the BOP benchmark, demonstrating the versatility of our approach. https://github.com/DLR-RM/nemo

</details>


### [52] [VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image](https://arxiv.org/abs/2602.04349)
*Teng-Fang Hsiao,Bo-Kai Ruan,Yu-Lun Liu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出VecSet-Edit：首个以VecSet LRM为骨干的直接3D网格编辑管线，仅依赖2D条件即可精准定位并编辑目标区域，结合掩膜引导的token初始化、注意力对齐的token门控、漂移感知的token剪枝与细节保真的纹理烘焙，实现高保真几何与纹理保留，优于体素/多视图方案。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑多依赖高斯Splatting或多视图图像，直接网格编辑不足；如VoxHammer等体素方法分辨率受限、需繁琐3D掩膜。需要一种既能高分辨几何重建又能精定位编辑、且以2D条件驱动的网格编辑方案。

Method: 以VecSet LRM为骨干，利用其token空间性质：不同token子集对应不同几何区域。提出三项关键技术：1) 掩膜引导的Token Seeding：用2D掩膜/图像条件初始化目标相关token；2) 注意力对齐的Token Gating：依据跨视角注意力将无关token抑制以实现区域精定位；3) 漂移感知的Token Pruning：在扩散去噪过程中剔除几何异常漂移token。并加入细节保真的纹理烘焙模块以保留原网格几何与纹理细节。

Result: 在直接网格编辑上实现高保真几何与纹理保留，避免体素分辨率瓶颈与3D掩膜成本；仅用2D条件即可精确定位并编辑目标区域，效果优于现有体素/多视图方法（摘要未给出具体量化指标）。

Conclusion: VecSet-Edit表明利用VecSet LRM的token空间可实现精准、可控、细节保真的网格编辑；所提的token初始化/门控/剪枝与纹理烘焙策略有效缓解扩散漂移与区域定位问题，为直接网格编辑提供新范式。

Abstract: 3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely on voxel-based representations that suffer from limited resolution and necessitate labor-intensive 3D mask. To address these limitations, we propose \textbf{VecSet-Edit}, the first pipeline that leverages the high-fidelity VecSet Large Reconstruction Model (LRM) as a backbone for mesh editing. Our approach is grounded on a analysis of the spatial properties in VecSet tokens, revealing that token subsets govern distinct geometric regions. Based on this insight, we introduce Mask-guided Token Seeding and Attention-aligned Token Gating strategies to precisely localize target regions using only 2D image conditions. Also, considering the difference between VecSet diffusion process versus voxel we design a Drift-aware Token Pruning to reject geometric outliers during the denoising process. Finally, our Detail-preserving Texture Baking module ensures that we not only preserve the geometric details of original mesh but also the textural information. More details can be found in our project page: https://github.com/BlueDyee/VecSet-Edit/tree/main

</details>


### [53] [When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models](https://arxiv.org/abs/2602.04356)
*Jaehyun Kwak,Nam Cao,Boryeong Cho,Segyu Lee,Sumyeong Ahn,Se-Young Yun*

Main category: cs.CV

TL;DR: 提出SAGA：基于注意力的分阶段攻击，通过逐步聚焦高注意力区域，在有限扰动预算下对LVLM实现高成功率且不可感知的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有对LVLM的输入变换攻击（如随机裁剪）虽暗示局部扰动有效，但随机性强、预算利用低，缺乏对“应攻哪里”的系统性指导，难以稳定高效破坏多模态系统安全。

Method: 观察1：区域注意力与对抗损失敏感性正相关；观察2：先攻高注意力区会诱导注意力向后续显著区域结构化再分配。基于此，提出分阶段注意力引导攻击SAGA：在每一阶段，用模型的注意力热力图定位高注意力区域，集中施加微小扰动；随后根据注意力重新分布更新目标区域，递进式迭代，实现在像素预算约束下的局部精细攻击。

Result: 在10个LVLM上实现SOTA攻击成功率，同时扰动高度不可感知；相较全局或随机裁剪式方法，预算使用更高效与更稳定。

Conclusion: 利用LVLM内部注意力信号可作为有效的攻击引导，分阶段聚焦显著区域能显著提升多模态对抗攻击的效率和隐蔽性；SAGA验证了这一点并为后续更精细的注意力驱动攻击提供了框架。

Abstract: Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.

</details>


### [54] [SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration](https://arxiv.org/abs/2602.04361)
*Zekun Li,Ning Wang,Tongxin Bai,Changwang Mei,Peisong Wang,Shuang Qiu,Jian Cheng*

Main category: cs.CV

TL;DR: SparVAR 提出一种面向视觉自回归（VAR）模型的训练无关加速框架，通过预测并复用跨尺度的稀疏注意力模式与局部性结构，在不跳过高分辨率尺度的前提下，大幅降低注意力复杂度与推理时延，实现>5×内核加速与整体1.57×端到端提速，同时几乎不损失高频细节。


<details>
  <summary>Details</summary>
Motivation: 主流 VAR 在每一步需要对历史多尺度的全部token做全注意力，分辨率增大时注意力复杂度近似随尺度四次方增长，导致高延迟。已有加速多通过跳过高分辨率尺度来降时延，但会丢失高频细节，影响图像质量。需要一种既快又保细节、且最好无需再训练的通用加速方法。

Method: 利用 VAR 注意力的三种经验性质：(i) 存在强注意力“汇点”（sink）；(ii) 跨尺度激活相似；(iii) 明显的局部性。具体做法：1) 从较低分辨率的“决策尺度”动态预测高分辨率后续尺度的稀疏注意力模式，并通过高效索引映射机制构造尺度自相似的稀疏注意力；2) 设计跨尺度的局部稀疏注意力；3) 实现高效的分块式稀疏内核，实际前向速度超过 FlashAttention >5×。全流程为训练无关、即插即用。

Result: 在8B模型、1024×1024生成任务上，不跳过末端尺度即可将生成时间压至约1秒；相较采用 FlashAttention 的 VAR 基线，端到端速度提升1.57×，高频细节几乎无损；与尺度跳跃策略结合时，最高可达2.28×加速且视觉质量保持竞争力。

Conclusion: SparVAR 通过跨尺度自相似与局部性的稀疏化策略，在无需再训练的前提下有效降低 VAR 的高分辨率注意力开销，实现显著的推理加速而基本不牺牲图像细节，并可与现有跳尺度方法叠加获得进一步加速。

Abstract: Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at https://github.com/CAS-CLab/SparVAR.

</details>


### [55] [Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture](https://arxiv.org/abs/2602.04381)
*Weihao Gao,Zhuo Deng,Zheng Gong,Lan Ma*

Main category: cs.CV

TL;DR: UltraSeg提出极致压缩（<0.3M参数）的息肉分割模型族，在CPU上实现90FPS且精度接近大型U-Net（保留>94% Dice），可在资源受限环境即刻部署。


<details>
  <summary>Details</summary>
Motivation: 临床一线（基层医院、移动内镜、胶囊机器人）缺乏GPU，现有高精度分割模型难以实时部署；需要在不显著牺牲精度的前提下，极限压缩并CPU原生实时运行的方案。

Method: 提出UltraSeg-108K与UltraSeg-130K两款轻量模型：1）联合优化编码器-解码器通道宽度；2）使用受约束的空洞卷积扩大感受野；3）引入跨层的轻量级特征融合模块；针对单中心与多中心/多模态场景分别调参；整体面向CPU推理优化。

Result: 在七个公共数据集评测，单核CPU可达约90FPS；与31M参数U-Net相比，仅用0.4%参数仍保留>94%的Dice；108K模型适配单中心，130K具更强泛化。

Conclusion: UltraSeg在极致压缩下实现接近SOTA的精度与高实时性，提供临床可行、可复现实用基线与CPU原生解决方案，适用于资源受限的内镜与更广泛微创外科视觉任务；源码公开便于基准与扩展。

Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (<0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains >94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking.

</details>


### [56] [Interactive Spatial-Frequency Fusion Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2602.04405)
*Yixin Zhu,Long Lv,Pingping Zhang,Xuehu Liu,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出ISFM框架，通过交互式空间-频域融合提升多模态图像融合质量，在6个数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MMIF方法虽引入频域信息，但多为简单串并联融合，缺乏空间与频域之间、以及跨模态间的有效交互，难以充分利用低/高频与长程依赖，导致细节与重要信息保真度不足。

Method: 1) 模态特定特征提取器（MSE）：以线性复杂度建模长程依赖，分别从各模态提取特征；2) 多尺度频域融合（MFF）：分解并自适应整合多尺度的低/高频分量，得到鲁棒的频域表征；3) 交互式空-频融合（ISF）：以频域特征引导并增强跨模态的空间特征交互，实现互补信息的充分利用；整体构成ISFM管线。

Result: 在6个MMIF数据集上进行大量实验，定量与定性结果均表明ISFM在多数或全部指标上优于最新SOTA方法。

Conclusion: 通过在多尺度上引入频域信息并与空间特征进行交互式融合，ISFM有效提升了多模态图像融合的细节保留与关键信息表达，具备更好泛化与性能；代码已开源。

Abstract: Multi-Modal Image Fusion (MMIF) aims to combine images from different modalities to produce fused images, retaining texture details and preserving significant information. Recently, some MMIF methods incorporate frequency domain information to enhance spatial features. However, these methods typically rely on simple serial or parallel spatial-frequency fusion without interaction. In this paper, we propose a novel Interactive Spatial-Frequency Fusion Mamba (ISFM) framework for MMIF. Specifically, we begin with a Modality-Specific Extractor (MSE) to extract features from different modalities. It models long-range dependencies across the image with linear computational complexity. To effectively leverage frequency information, we then propose a Multi-scale Frequency Fusion (MFF). It adaptively integrates low-frequency and high-frequency components across multiple scales, enabling robust representations of frequency features. More importantly, we further propose an Interactive Spatial-Frequency Fusion (ISF). It incorporates frequency features to guide spatial features across modalities, enhancing complementary representations. Extensive experiments are conducted on six MMIF datasets. The experimental results demonstrate that our ISFM can achieve better performances than other state-of-the-art methods. The source code is available at https://github.com/Namn23/ISFM.

</details>


### [57] [LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration](https://arxiv.org/abs/2602.04406)
*Jue Gong,Zihan Zhou,Jingkai Wang,Shu Li,Libo Liu,Jianliang Lan,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出LCUDiff：将预训练潜空间扩展至16通道并保持一步生成，显著提升人像/人体修复的细节保真与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有人像修复（尤其人体区域）在扩散式方法中受限于预训练T2I模型的VAE瓶颈，4通道潜空间难以承载高频细节，导致保真度不足与伪影。需要在不牺牲一步推理效率的前提下提升潜表示能力并与现有扩散骨干平滑对接。

Method: 1) 通道扩展：将预训练latent diffusion从4通道升级到16通道，维持一步生成框架。2) VAE微调-通道拆分蒸馏（CSD）：前4通道与原先先验对齐，新增12通道专注编码高频细节。3) 先验保持自适应（PPA）：缓解4通道扩散骨干与16通道潜表示的维度错配，平滑迁移。4) 解码器路由（DeR）：依据样本级修复质量评分进行解码路径路由，提升在多样退化条件下的视觉质量。

Result: 在合成与真实数据上，相比现有方法在轻度退化条件下取得更高保真度与更少伪影，同时保持一步推理效率；定量与定性结果具竞争力。

Conclusion: 通过扩大潜空间并以CSD、PPA与DeR协同，LCUDiff在不增加推理步数的前提下提升人像/人体修复的细节与稳定性，兼顾效率与质量；代码与模型将开源。

Abstract: Existing methods for restoring degraded human-centric images often struggle with insufficient fidelity, particularly in human body restoration (HBR). Recent diffusion-based restoration methods commonly adapt pre-trained text-to-image diffusion models, where the variational autoencoder (VAE) can significantly bottleneck restoration fidelity. We propose LCUDiff, a stable one-step framework that upgrades a pre-trained latent diffusion model from the 4-channel latent space to the 16-channel latent space. For VAE fine-tuning, channel splitting distillation (CSD) is used to keep the first four channels aligned with pre-trained priors while allocating the additional channels to effectively encode high-frequency details. We further design prior-preserving adaptation (PPA) to smoothly bridge the mismatch between 4-channel diffusion backbones and the higher-dimensional 16-channel latent. In addition, we propose a decoder router (DeR) for per-sample decoder routing using restoration-quality score annotations, which improves visual quality across diverse conditions. Experiments on synthetic and real-world datasets show competitive results with higher fidelity and fewer artifacts under mild degradations, while preserving one-step efficiency. The code and model will be at https://github.com/gobunu/LCUDiff.

</details>


### [58] [Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare](https://arxiv.org/abs/2602.04416)
*Aavash Chhetri,Bibek Niroula,Pratik Shrestha,Yash Raj Shrestha,Lesley A Anderson,Prashnna K Gyawali,Loris Bazzani,Binod Bhattarai*

Main category: cs.CV

TL;DR: 提出Med-MMFL：首个医疗领域多模态联邦学习综合基准，覆盖10种模态、4类任务、3种联邦场景，评测6种SOTA算法并开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗联邦学习基准稀缺，多集中于单/双模态与少量任务，缺乏标准化评测，不利于系统性理解与方法比较，尤其在多模态场景下的现实异质性与可复现性不足。

Method: 构建Med-MMFL基准：收集含2–4模态的数据集，涵盖文本、病理图像、ECG、X-ray、放射学报告、多序列MRI等10种模态；设置自然联邦、合成IID与合成非IID三种数据划分；覆盖分割、分类、模态对齐（检索）、VQA四类任务；评测6种代表性FL算法（聚合策略、损失设计与正则化各有侧重）；提供完整数据处理与划分流水线与代码。

Result: 在多数据集、多任务与多场景下系统对比6种SOTA FL方法，揭示不同算法在异质性、任务类型与模态组合下的性能差异与适用性，为MMFL方法选型与改进提供实证依据。

Conclusion: Med-MMFL为医疗多模态联邦学习提供首个全面、可复现实验基准，促进公平比较与方法发展；代码与流程已开源，便于社区扩展与复现。

Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .

</details>


### [59] [TrajVG: 3D Trajectory-Coupled Visual Geometry Learning](https://arxiv.org/abs/2602.04439)
*Xingyu Miao,Weiguang Zhao,Tao Lu,Linning Yu,Mulin Yu,Yang Long,Jiangmiao Pang,Junting Dong*

Main category: cs.CV

TL;DR: TrajVG提出通过显式预测相机坐标系下的3D轨迹来解决多运动视频中前馈多帧重建的错位与重复问题，并以几何一致性和自监督约束实现混合监督训练，全面提升3D跟踪、位姿、点图与视频深度的性能。


<details>
  <summary>Details</summary>
Motivation: 前馈式多帧3D重建在存在多对象/非刚体运动时表现退化：全局参考在多运动下变得歧义、局部点图依赖相对位姿易漂移，导致跨帧错配与结构重复。缺乏可扩展的3D轨迹标注也限制了在野数据训练。

Method: 提出TrajVG：显式预测稀疏3D轨迹（相机坐标系），并与逐帧局部点图及相机相对位姿通过几何一致性进行耦合。(i) 双向轨迹-点图一致性，配合受控梯度流；(ii) 由静态轨迹锚驱动的位姿一致性，抑制动态区域梯度干扰。为可扩展训练，将上述耦合重写为仅依赖伪2D轨迹的自监督目标，实现混合监督。

Result: 在3D跟踪、相机位姿估计、点图重建和视频深度等任务上，相较现有前馈基线取得全面提升。

Conclusion: 显式建模跨帧3D对应为相机坐标3D轨迹，并以几何一致性与自监督混合训练，可在多运动场景中克服对位姿的依赖与漂移问题，显著提高视频3D重建相关任务的鲁棒性与精度。

Abstract: Feed-forward multi-frame 3D reconstruction models often degrade on videos with object motion. Global-reference becomes ambiguous under multiple motions, while the local pointmap relies heavily on estimated relative poses and can drift, causing cross-frame misalignment and duplicated structures. We propose TrajVG, a reconstruction framework that makes cross-frame 3D correspondence an explicit prediction by estimating camera-coordinate 3D trajectories. We couple sparse trajectories, per-frame local point maps, and relative camera poses with geometric consistency objectives: (i) bidirectional trajectory-pointmap consistency with controlled gradient flow, and (ii) a pose consistency objective driven by static track anchors that suppresses gradients from dynamic regions. To scale training to in-the-wild videos where 3D trajectory labels are scarce, we reformulate the same coupling constraints into self-supervised objectives using only pseudo 2D tracks, enabling unified training with mixed supervision. Extensive experiments across 3D tracking, pose estimation, pointmap reconstruction, and video depth show that TrajVG surpasses the current feedforward performance baseline.

</details>


### [60] [SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking](https://arxiv.org/abs/2602.04441)
*Weiguang Zhao,Haoran Xu,Xingyu Miao,Qin Zhao,Rui Zhang,Kaizhu Huang,Ning Gao,Peizhou Cao,Mingze Sun,Mulin Yu,Tao Lu,Linning Xu,Junting Dong,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出SynthVerse，一个大规模多样化的合成数据集与基准，用于提升通用点跟踪的泛化能力，并系统评测现有方法在更广域域移下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有点跟踪研究受限于高质量数据稀缺：数据集多样性不足、轨迹标注不完美，难以支撑对复杂运动、遮挡与视角变化的稳健泛化。

Method: 构建SynthVerse合成数据集，覆盖动画电影风格、具身操作、场景导航、关节物体等此前缺失的域与对象；提供更广的类别覆盖与高质量动态交互。并建立一个多样化点跟踪基准，用于系统评估SOTA方法在更广域移条件下的性能。通过大量实验检验在SynthVerse上训练的效果并分析现有跟踪器的局限。

Result: 在SynthVerse上训练可带来一致的泛化性能提升；在多样化基准上的实验揭示现有点跟踪器在多种设置与域移下的不足。

Conclusion: 合成的、多域多样化的高质量数据与基准能显著提升通用点跟踪的鲁棒性与泛化，并为发现与推动现有方法的改进提供系统化评测平台。

Abstract: Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.

</details>


### [61] [Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search](https://arxiv.org/abs/2602.04454)
*Tianming Liang,Qirui Du,Jian-Fang Hu,Haichao Jiang,Zicheng Lin,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 提出Seg-ReSearch：把外部检索与推理交替融合到语言引导分割中，突破MLLM冻结知识局限；并给出层级奖励训练与新基准OK-VOS，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动分割依赖MLLM内置（冻结）知识，难以应对需要最新信息或专业概念的开放世界场景；需要一种能动态获取外部知识并用于分割推理的方法与训练机制。

Method: 提出Seg-ReSearch框架：在分割过程中允许交替进行外部搜索与多步推理，将检索到的外部知识注入到视觉-语言分割决策；设计层级奖励（从初始引导到渐进激励）以平衡稀疏结果信号与僵硬的逐步监督；构建需要外部知识的视频目标分割基准OK-VOS。

Result: 在OK-VOS及两项现有推理分割基准上显著超越SOTA，取得大幅性能提升（文中未给具体数值，但强调“substantial margin”）。

Conclusion: 通过把外部检索融入推理并用层级奖励训练，Seg-ReSearch突破MLLM知识瓶颈，使语言引导分割能处理动态、开放世界查询；提供新基准与实现代码数据。

Abstract: Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.

</details>


### [62] [Temporal Slowness in Central Vision Drives Semantic Object Learning](https://arxiv.org/abs/2602.04462)
*Timothy Schaumlöffel,Arthur Aubret,Gemma Roig,Jochen Triesch*

Main category: cs.CV

TL;DR: 研究用模拟的人类自我中心视觉流与凝视预测，结合中央视野裁剪与时间缓变（slowness）对比学习，发现二者协同可更好编码物体语义：中央视野强化前景特征，时间缓变（尤其注视微动期间）扩展更广的物体语义。


<details>
  <summary>Details</summary>
Motivation: 人类视觉以中央凹高分辨率加工，并倾向于对时间上相近的输入学习相似表征，可能通过强调缓变信息来形成稳定的物体语义。现有自监督视觉表征多未系统结合“中央视野”与“时间缓变”，亟需检验它们在拟人类视觉经验中的作用机制。

Method: 以Ego4D自我中心视频（约5个月等量）为经验数据；用先进的凝视预测模型生成凝视坐标；基于这些坐标从帧中裁剪中央视野样本；在这些裁剪上训练时间对比式自监督学习（TCL/Time-Contrastive）模型，以实现“slowness”约束；分析不同设置对物体语义各层面表征的影响。

Result: 将中央视野裁剪与时间缓变学习结合，显著提升物体语义表征的多维编码能力；中央视野有利于抽取前景目标的判别性特征；时间缓变（尤其在注视与微眼跳阶段）促进对更广泛的语义信息的捕获与整合。

Conclusion: 中央视野选择性与时间缓变约束是从自然人类视觉经验中形成物体语义表征的关键互补机制；二者协同可更贴近人类视觉的编码方式，并为自监督表征学习设计提供启示。

Abstract: Humans acquire semantic object representations from egocentric visual streams with minimal supervision. Importantly, the visual system processes with high resolution only the center of its field of view and learns similar representations for visual inputs occurring close in time. This emphasizes slowly changing information around gaze locations. This study investigates the role of central vision and slowness learning in the formation of semantic object representations from human-like visual experience. We simulate five months of human-like visual experience using the Ego4D dataset and generate gaze coordinates with a state-of-the-art gaze prediction model. Using these predictions, we extract crops that mimic central vision and train a time-contrastive Self-Supervised Learning model on them. Our results show that combining temporal slowness and central vision improves the encoding of different semantic facets of object representations. Specifically, focusing on central vision strengthens the extraction of foreground object features, while considering temporal slowness, especially during fixational eye movements, allows the model to encode broader semantic information about objects. These findings provide new insights into the mechanisms by which humans may develop semantic object representations from natural visual experience.

</details>


### [63] [SALAD-Pan: Sensor-Agnostic Latent Adaptive Diffusion for Pan-Sharpening](https://arxiv.org/abs/2602.04473)
*Junjie Li,Congyang Ou,Haokui Zhang,Guoting Wei,Shengqin Jiang,Ying Li,Chunhua Shen*

Main category: cs.CV

TL;DR: SALAD-Pan 提出一种面向多传感器的潜空间扩散式全色锐化方法，通过单通道VAE压缩HRMS到潜变量，并在扩散过程中注入光谱物理先验与PAN/MS双向控制，辅以跨光谱注意力以提升光谱一致性，实现更快、更准且具零样本跨传感器泛化的融合。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式全色锐化多在像素空间运行、需为不同MS传感器单独训练，导致推理慢、部署割裂且泛化性差。需要一种统一、效率高且对传感器不敏感的方法。

Method: 1) 训练按波段处理的单通道VAE，将高分辨率多光谱(HRMS)编码为紧凑潜表示，适配不同通道数并奠定加速基础；2) 在扩散骨干中以单向/双向交互控制分别注入光谱物理属性与PAN/MS图像，引导高精度融合；3) 在扩散模型中层加入轻量级跨光谱注意力，强化光谱联系与一致性。

Result: 在GaoFen-2、QuickBird、WorldView-3三数据集上全面优于最先进扩散方法；推理速度提升2-3倍；展现稳健的零样本跨传感器能力。

Conclusion: 潜空间扩散结合物理先验与跨光谱注意力可显著提升全色锐化的准确性与效率，并减少对特定传感器的依赖，实现通用快速的跨传感器融合。

Abstract: Recently, diffusion models bring novel insights for Pan-sharpening and notably boost fusion precision. However, most existing models perform diffusion in the pixel space and train distinct models for different multispectral (MS) imagery, suffering from high latency and sensor-specific limitations. In this paper, we present SALAD-Pan, a sensor-agnostic latent space diffusion method for efficient pansharpening. Specifically, SALAD-Pan trains a band-wise single-channel VAE to encode high-resolution multispectral (HRMS) into compact latent representations, supporting MS images with various channel counts and establishing a basis for acceleration. Then spectral physical properties, along with PAN and MS images, are injected into the diffusion backbone through unidirectional and bidirectional interactive control structures respectively, achieving high-precision fusion in the diffusion process. Finally, a lightweight cross-spectral attention module is added to the central layer of diffusion model, reinforcing spectral connections to boost spectral consistency and further elevate fusion precision. Experimental results on GaoFen-2, QuickBird, and WorldView-3 demonstrate that SALAD-Pan outperforms state-of-the-art diffusion-based methods across all three datasets, attains a 2-3x inference speedup, and exhibits robust zero-shot (cross-sensor) capability.

</details>


### [64] [Vision-aligned Latent Reasoning for Multi-modal Large Language Model](https://arxiv.org/abs/2602.04476)
*Byungwoo Jeon,Yoonwoo Jeong,Hyunseok Lee,Minsu Cho,Jinwoo Shin*

Main category: cs.CV

TL;DR: 论文提出VaLR，一种在每步链式思维前动态生成“与视觉对齐”的潜在token的框架，以缓解长上下文推理中视觉信息被稀释的问题，从而显著提升多步视觉推理与长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: MLLM在多模态理解上进步明显，但在需要长链路、多步推理的任务上表现不佳，根因是长文本生成过程中视觉信息逐步淡化，导致测试时扩展（test-time scaling）收益受限。作者希望在推理过程中持续保留并利用高质量的视觉线索。

Method: 提出Vision-aligned Latent Reasoning (VaLR)。在每个CoT步骤前，先生成一组与视觉编码器对齐的潜在token，作为对后续推理的感知提示。训练时，通过将MLLM中间嵌入与视觉编码器嵌入进行对齐（保持视觉知识）来监督，使这些潜在token在潜在空间中携带并强化关键视觉信息，从而指导随后的语言推理。

Result: 在需要长上下文理解或精细视觉感知的多项基准上均优于现有方法，并展现此前MLLM未出现的良好测试时扩展行为。尤其在VSI-Bench上从33.0%提升到52.9%，相比Qwen2.5-VL有19.9个百分点增益。

Conclusion: 通过在推理各步引入与视觉对齐的潜在token并进行中间表征对齐训练，VaLR有效缓解视觉信息稀释，增强多步视觉推理能力与test-time scaling，取得显著SOTA水平。

Abstract: Despite recent advancements in Multi-modal Large Language Models (MLLMs) on diverse understanding tasks, these models struggle to solve problems which require extensive multi-step reasoning. This is primarily due to the progressive dilution of visual information during long-context generation, which hinders their ability to fully exploit test-time scaling. To address this issue, we introduce Vision-aligned Latent Reasoning (VaLR), a simple, yet effective reasoning framework that dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step, guiding the model to reason based on perceptual cues in the latent space. Specifically, VaLR is trained to preserve visual knowledge during reasoning by aligning intermediate embeddings of MLLM with those from vision encoders. Empirical results demonstrate that VaLR consistently outperforms existing approaches across a wide range of benchmarks requiring long-context understanding or precise visual perception, while exhibiting test-time scaling behavior not observed in prior MLLMs. In particular, VaLR improves the performance significantly from 33.0% to 52.9% on VSI-Bench, achieving a 19.9%p gain over Qwen2.5-VL.

</details>


### [65] [S-MUSt3R: Sliding Multi-view 3D Reconstruction](https://arxiv.org/abs/2602.04517)
*Leonid Antsfeld,Boris Chidlovskii,Yohann Cabon,Vincent Leroy,Jerome Revaud*

Main category: cs.CV

TL;DR: S-MUSt3R 提出一种将 MUSt3R 基础模型扩展到长序列单目 RGB 3D 重建的简洁流水线：把长序列分段、对齐并进行轻量级回环优化，在无需再训练的前提下实现与传统更复杂方法相当的轨迹与重建精度，且可直接输出米制尺度。


<details>
  <summary>Details</summary>
Motivation: 基础模型已能在未标定图像中展现强大的3D感知能力，但受内存约束，难以处理大规模RGB视频流进行3D重建；需要一种可扩展、资源友好的方案。

Method: 基于 MUSt3R 的零训练流水线：1) 将长序列切分为多个段；2) 段间几何/位姿对齐；3) 进行轻量级回环闭合优化，以缓解漂移并保证全局一致性；整体不修改或再训练模型。

Result: 在 TUM、7-Scenes 及自有机器人导航数据集上，能稳定运行于长 RGB 序列，取得与传统复杂系统相当的轨迹与重建精度与一致性。

Conclusion: 通过简单的分段-对齐-回环策略，可将 MUSt3R 的能力扩展到可扩展的单目大规模3D场景重建，且直接在米制空间预测，具备实际应用潜力。

Abstract: The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.

</details>


### [66] [SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking](https://arxiv.org/abs/2602.04525)
*Muhammad Taha Mukhtar,Syed Musa Ali Kazmi,Khola Naseem,Muhammad Ali Chattha,Andreas Dengel,Sheraz Ahmed,Muhammad Naseer Bajwa,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 论文提出用于巴基斯坦拉合尔的新基准数据集及卡拉奇、孟买的配套集（总覆盖约1,869平方公里），并提出一种针对非正式住区遥感分割的半监督框架，通过类别自适应阈值与原型库机制提升少数类与跨域泛化能力，在八座城市多数据集上优于SOTA，尤其在仅用10%标注的跨域测试中取得0.461 mIoU，超过全监督模型的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 城市快速扩张导致低中收入国家大城市出现大量非正式住区，但大规模制图受限于标注稀缺与数据质量差：正式/非正式建筑光谱高度相似（高歧义）且标注噪声大，现有半监督方法易受类不平衡与特征退化影响，跨城跨域泛化弱。

Method: 1) 构建拉合尔自建基准与基于行政边界核验的卡拉奇、孟买数据集，并对数据质量做系统评估；2) 提出半监督语义分割框架：- 类别感知自适应阈值（CAAT），按类别动态调整伪标签置信阈，避免少数类被抑制；- 原型银行（Prototype Bank），利用历史高保真特征原型锚定当前预测以保持语义一致、缓解特征漂移与退化；3) 在五个既有基准扩展实验，覆盖三大洲八城，检验鲁棒性与可迁移性。

Result: 在八城多基准上整体优于现有半监督SOTA；在跨域迁移中，仅用源域10%标注即可在未见地域达到0.461 mIoU，并超过全监督模型的零样本泛化表现。

Conclusion: 专为非正式住区遥感分割设计的半监督框架结合CAAT与原型库能有效缓解类不平衡与特征退化，显著提升跨城市跨洲泛化；所发布的数据集与质量评估为后续研究提供坚实基准。

Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.

</details>


### [67] [OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis](https://arxiv.org/abs/2602.04547)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto*

Main category: cs.CV

TL;DR: OmniRad是一种自监督放射学基础模型，预训练于120万张多模态医学影像，强调可复用表示与跨任务迁移；在分类与分割等多基准上，较现有模型取得小幅但稳定的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前放射学分析需要可泛化到多模态、多任务的预训练表示，以减少对任务特定标注与训练的依赖，并提升跨任务迁移能力与表示质量。

Method: 提出OmniRad：基于自监督学习在120万张医学影像上预训练的视觉编码器。采用“冻结主干+轻量适配器”与“端到端微调”两种下游适配范式，系统评估表示质量与任务特定性能；在分类与分割公开基准上进行验证，并使用潜空间可视化与定性分析评估特征聚类与模态分离。

Result: 在MedMNISTv2分类上，F1最高提升2.05%优于竞品基础模型；在MedSegBench的6个分割数据集上，使用冻结表示时取得平均Dice提升；可视化显示更好的特征聚类与模态相关分离。

Conclusion: OmniRad通过大规模自监督预训练与适配策略，实现跨模态、跨任务的稳定增益，兼顾表示可复用性与下游性能，显示为放射学通用基础模型的可行路径。

Abstract: Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.

</details>


### [68] [Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models](https://arxiv.org/abs/2602.04549)
*Cem Eteke,Enzo Tartaglione*

Main category: cs.CV

TL;DR: NiFi提出一种针对3D Gaussian Splatting的极限压缩与恢复方法，在超低码率下通过一次蒸馏的扩散式复原网络抑制压缩伪影，实现与未压缩3DGS相当的感知质量，并声称可达约1000倍的压缩率（最低至约0.1MB）。


<details>
  <summary>Details</summary>
Motivation: 3DGS以稀疏高斯替代隐式密集表示，实现实时渲染但占用大量存储，限制沉浸式通信等应用。现有3DGS压缩在低码率下出现明显伪影，显著降低感知质量。需要一种在极低码率下仍能保证视觉质量的压缩-复原方案。

Method: 提出NiFi：一种“伪影感知”的扩散模型，并通过one-step distillation（一次步推理）实现高效复原。流程上先进行极端压缩获得强退化的3DGS表示，再用训练好的扩散蒸馏网络进行恢复与伪影抑制，从而在解码端以极低开销重建高感知质量。

Result: 在极低码率（低至0.1 MB）下实现当前最优的感知质量；相较原始3DGS在可比感知质量下可达约1000×的码率改进。

Conclusion: 基于扩散的一步蒸馏复原能有效修补低码率压缩带来的伪影，实现极限3DGS压缩与高感知质量，推动3DGS在带宽/存储受限场景的实际应用。

Abstract: 3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.

</details>


### [69] [Understanding Degradation with Vision Language Model](https://arxiv.org/abs/2602.04565)
*Guanzhou Lan,Chenyi Liao,Yuqi Yang,Qianli Ma,Zhigang Wang,Dong Wang,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: 提出DU-VLM：把图像退化理解重构为分层结构化预测，用自回归下一token统一预测退化类型、参数键与连续物理值，并作为零样本控制器驱动扩散模型做高保真恢复；同时发布110k物理标注数据集，实验显著优于通用基线并具泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM善于文字描述但难以把握图像退化的物理参数与机制，缺乏结构化、可控的退化理解能力，限制了图像恢复等任务。

Method: 将退化理解表述为层级结构化预测：先识别退化类型，再预测参数键，最后量化连续物理值。证明三者可在同一自回归下一token框架下统一，误差由值域量化网格上界。提出DU-VLM：多模态CoT，SFT+基于结构化奖励的RL训练；并用作无微调的扩散模型零样本控制器。构建含物理标注的DU-110k干净-退化对数据集。

Result: 在准确性与鲁棒性上显著超过通用VLM基线；能对未见分布泛化；作为控制器可实现高保真图像恢复且无需微调生成骨干。

Conclusion: 结构化、自回归统一范式能让VLM学习物理可解释的退化理解，并可直接驱动现有扩散模型实现强泛化的图像恢复；数据集与方法为可控低层视觉提供新基线。

Abstract: Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.

</details>


### [70] [PEPR: Privileged Event-based Predictive Regularization for Domain Generalization](https://arxiv.org/abs/2602.04583)
*Gabriele Magrini,Federico Becattini,Niccolò Biondi,Pietro Pala*

Main category: cs.CV

TL;DR: 提出一种在LUPI范式下，利用仅训练期可用的事件相机作为特权信息，通过预测式正则化训练出更鲁棒的单RGB模型，以提升跨域（如昼夜）泛化能力。


<details>
  <summary>Details</summary>
Motivation: RGB模型在真实世界中常遭遇域移（如光照、天气、成像噪声变化）导致性能骤降；事件相机具有高动态范围与时间分辨率、对域变化更稳健。现有跨模态直接对齐会让RGB特征变稀疏、丢语义，因而需要一种既能蒸馏稳健性又保留语义密度的方法。

Method: 在LUPI设定下，引入PEPR：在共享潜在空间中，将问题表述为预测任务，而非直接特征对齐。训练时同时提取RGB与事件特征，用事件分支获得更域不变的潜在表征；以此作为目标，训练RGB编码器去预测事件潜在特征，通过预测误差形成正则化，提升鲁棒性但不强迫RGB特征稀疏化。推理时仅用RGB分支。

Result: 在目标检测与语义分割上，单RGB模型对昼夜与其他域移更稳健，持续优于基于跨模态对齐的基线方法。

Conclusion: 将事件相机作为训练期特权信息，并通过预测式正则而非对齐进行蒸馏，可在不牺牲语义的前提下显著提升单RGB视觉模型的跨域泛化能力。

Abstract: Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.

</details>


### [71] [SalFormer360: a transformer-based saliency estimation model for 360-degree videos](https://arxiv.org/abs/2602.04584)
*Mahmoud Z. A. Wahba,Francesco Barbato,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 提出SalFormer360：基于Transformer（SegFormer编码器+自定义解码器）的360°视频显著性估计模型，引入观看中心偏置，三大数据集上显著优于SOTA（PCC分别提升8.4%、2.5%、18.6%）。


<details>
  <summary>Details</summary>
Motivation: 360°视频在视口预测与沉浸式内容优化等应用中需要准确的显著性估计；现有方法多为2D或对360°适配不足，难以捕捉全景场景的注意力模式，亟需更强的时空建模与对用户观看偏置的建模。

Method: 采用SegFormer作为编码器并微调以适应360°内容，配合自定义解码器进行显著性图重建；在模型中显式注入Viewing Center Bias以反映用户在全景观看中的注意中心倾向；在三大360°显著性数据集上进行训练与评测。

Result: 相较先前SOTA，在PCC指标上：Sport360提升8.4%，PVS-HM提升2.5%，VR-EyeTracking提升18.6%。

Conclusion: Transformer架构结合针对360°内容的适配与观看中心偏置，可显著提升360°视频显著性估计性能；SalFormer360在多个基准上取得领先，适用于视口预测与内容优化等应用。

Abstract: Saliency estimation has received growing attention in recent years due to its importance in a wide range of applications. In the context of 360-degree video, it has been particularly valuable for tasks such as viewport prediction and immersive content optimization. In this paper, we propose SalFormer360, a novel saliency estimation model for 360-degree videos built on a transformer-based architecture. Our approach is based on the combination of an existing encoder architecture, SegFormer, and a custom decoder. The SegFormer model was originally developed for 2D segmentation tasks, and it has been fine-tuned to adapt it to 360-degree content. To further enhance prediction accuracy in our model, we incorporated Viewing Center Bias to reflect user attention in 360-degree environments. Extensive experiments on the three largest benchmark datasets for saliency estimation demonstrate that SalFormer360 outperforms existing state-of-the-art methods. In terms of Pearson Correlation Coefficient, our model achieves 8.4% higher performance on Sport360, 2.5% on PVS-HM, and 18.6% on VR-EyeTracking compared to previous state-of-the-art.

</details>


### [72] [ImmuVis: Hyperconvolutional Foundation Model for Imaging Mass Cytometry](https://arxiv.org/abs/2602.04585)
*Marcin Możejko,Dawid Uchal,Krzysztof Gogolewski,Piotr Kupidura,Szymon Łukasik,Jakub Giezgała,Tomasz Nocoń,Kacper Pietrzyk,Robert Pieniuta,Mateusz Sulimowicz,Michal Orzyłowski,Tomasz Siłkowski,Karol Zagródka,Eike Staub,Ewa Szczurek*

Main category: cs.CV

TL;DR: ImmuVis 是一款针对成像质谱流式(IMC)的高效卷积式基础模型，通过“标记物自适应超卷积”在任意标记子集上无缝工作，并在大规模自监督预训练后，在虚拟染色与下游分类上优于现有方法，且能输出校准的不确定性。


<details>
  <summary>Details</summary>
Motivation: IMC 图像的通道对应生物标记物，实际研究的标记集合在不同数据集间变化，不存在固定通道空间；传统视觉主干假设固定通道数/语义，难以直接迁移。需要一种能随标记集合变化而适配、又高效可扩展的基础模型。

Method: 提出 ImmuVis：用可学习的“标记嵌入”驱动的超网络生成卷积核（marker-adaptive hyperconvolution），使同一模型可处理任意标记子集；在 IMC17M（28 队列、24,405 图像、265 标记、>1700 万 patch）上用自监督遮挡重建进行预训练；目标函数包含异方差似然以提供校准不确定性；与卷积/Transformer 基线和消融对比。

Result: 在虚拟染色与多种下游分类任务上优于 SOTA 与消融；在计算开销上显著低于基于 Transformer 的替代方案；是唯一能输出经校准不确定性的模型。

Conclusion: ImmuVis 作为面向真实世界 IMC 的实用高效基础模型，能在不同标记集合间泛化、性能与效率兼备，并提供不确定性量化，适合大规模空间组织学建模与应用。

Abstract: We present ImmuVis, an efficient convolutional foundation model for imaging mass cytometry (IMC), a high-throughput multiplex imaging technology that handles molecular marker measurements as image channels and enables large-scale spatial tissue profiling. Unlike natural images, multiplex imaging lacks a fixed channel space, as real-world marker sets vary across studies, violating a core assumption of standard vision backbones. To address this, ImmuVis introduces marker-adaptive hyperconvolutions that generate convolutional kernels from learned marker embeddings, enabling a single model to operate on arbitrary measured marker subsets without retraining. We pretrain ImmuVis on the largest to-date dataset, IMC17M (28 cohorts, 24,405 images, 265 markers, over 17M patches), using self-supervised masked reconstruction. ImmuVis outperforms SOTA baselines and ablations in virtual staining and downstream classification tasks at substantially lower compute cost than transformer-based alternatives, and is the sole model that provides calibrated uncertainty via a heteroscedastic likelihood objective. These results position ImmuVis as a practical, efficient foundation model for real-world IMC modeling.

</details>


### [73] [A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction](https://arxiv.org/abs/2602.04624)
*Raúl Jiménez Cruz,César Torres-Huitzil,Marco Franceschetti,Ronny Seiger,Luciano García-Bañuelos,Barbara Weber*

Main category: cs.CV

TL;DR: 提供一个包含11,884张标注图像的静脉采血（放血）训练臂模拟过程数据集，带多类实例分割标注并按训练/验证/测试划分，可用于医疗训练自动化与人机交互研究。


<details>
  <summary>Details</summary>
Motivation: 现有医疗操作场景下的人体-工具交互数据稀缺且标注成本高，尤其是细粒度步骤识别与工具检测所需的像素级标注数据。为推动自动化教学、流程合规检查与工作流分析，需要一个高质量、可复用、与主流检测分割框架兼容的公开数据集。

Method: 在可控环境下录制高清采血操作视频；对视频执行自动人脸匿名化；按帧抽取图像并用结构相似度（SSIM）过滤以降低冗余；对五个医学相关类别（注射器、止血带、消毒巾、手套、训练臂）进行多边形分割标注；以与YOLOv8等兼容的分割格式导出；按70/15/15划分训练、验证、测试集；将数据与标签发布到Zenodo。

Result: 得到包含11,884张图像、五类实例分割标注的高质量数据集，具有低冗余、匿名化处理、标准化格式与清晰的训练/验证/测试拆分；可直接用于目标检测、实例/语义分割与步骤识别等任务。

Conclusion: 该公开数据集为医疗训练自动化和人机交互研究提供了可复用基准，支持工具检测、步骤识别、工作流分析与合规检查，并有助于开发能为学员提供结构化反馈的教育系统。

Abstract: This data article presents a dataset of 11,884 labeled images documenting a simulated blood extraction (phlebotomy) procedure performed on a training arm. Images were extracted from high-definition videos recorded under controlled conditions and curated to reduce redundancy using Structural Similarity Index Measure (SSIM) filtering. An automated face-anonymization step was applied to all videos prior to frame selection. Each image contains polygon annotations for five medically relevant classes: syringe, rubber band, disinfectant wipe, gloves, and training arm. The annotations were exported in a segmentation format compatible with modern object detection frameworks (e.g., YOLOv8), ensuring broad usability. This dataset is partitioned into training (70%), validation (15%), and test (15%) subsets and is designed to advance research in medical training automation and human-object interaction. It enables multiple applications, including phlebotomy tool detection, procedural step recognition, workflow analysis, conformance checking, and the development of educational systems that provide structured feedback to medical trainees. The data and accompanying label files are publicly available on Zenodo.

</details>


### [74] [PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective](https://arxiv.org/abs/2602.04657)
*Haokui Zhang,Congyang Ou,Dawei Yan,Peng Wang,Qingsen Yan,Ying Li,Rong Xiao,Chunhua Shen*

Main category: cs.CV

TL;DR: PIO-FVLM 以“保持推理输出不变”为目标进行视觉token压缩：用层内代理损失产生的梯度显著性重排token，并用NMS选取最重要token，无需训练、兼容FlashAttention，既可独立部署也可与编码器压缩法结合。在 LLaVA-Next-7B 上仅保留11.1%视觉token仍保留97.2%性能，并带来2.67×预填、2.11×推理加速、6.22× FLOPs 与6.05× KV缓存降低。


<details>
  <summary>Details</summary>
Motivation: 现有VLM加速多依赖启发式相似度（视觉-视觉或跨模态）来删减token，导致压缩效果与部署友好性受限。需要一种从推理目标出发、直接面向输出保持性的选取策略，提升压缩率、性能保持与可落地性。

Method: 将视觉token压缩表述为“输出结果不变”约束下的token重要性选择。设计层内代理损失（layer-local proxy loss），对每层计算token级梯度显著性，据此重排token；随后按非极大值抑制（NMS）原则在空间上去冗余，选出最有价值的token。整体为训练免（training-free），兼容FlashAttention；可独立无编码器（encoder-free）部署，或与VisionZip等编码器压缩联合（encoder-involved）。

Result: 在 LLaVA-Next-7B 上，仅保留11.1%视觉token仍保持97.2%原始性能；实现2.67×预填加速、2.11×推理加速、FLOPs降6.22×、KV缓存降6.05×。代码开源于 https://github.com/ocy1/PIO-FVLM。

Conclusion: 面向输出不变性的训练免视觉token选择方法PIO-FVLM，凭借梯度显著性+NMS实现高压缩、高保真与良好工程兼容性，可单独使用或与编码器压缩结合，显著降低计算与存储开销且维持性能。

Abstract: Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\times$ prefill speedup, 2.11$\times$ inference speedup, 6.22$\times$ lower FLOPs, and 6.05$\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.

</details>


### [75] [AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation](https://arxiv.org/abs/2602.04672)
*Jin-Chuan Shi,Binhong Ye,Tao Liu,Junzhe He,Yangjinhui Xu,Xiaoyang Liu,Zeju Li,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: AGILE提出一种从“重建”转向“代理式生成”的单目视频手-物体交互恢复框架：先由VLM引导生成完整可水密网格，再用锚定-跟踪替代SfM，并通过接触感知优化保证物理可行，显著提升几何精度与鲁棒性，产出可仿真资产。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖神经渲染在遮挡下易产生碎片化几何，且强依赖脆弱的SfM初始化，导致在真实场景视频中频繁失败；需要既鲁棒又能产出可仿真、物理可信的资产的方法。

Method: 1) 代理式生成：VLM指导生成模型合成完整、 watertight 的对象网格与高保真纹理，规避视频遮挡带来的信息缺失；2) 锚定-跟踪：用基础模型在单个交互起始帧初始化物体位姿，随后利用生成资产与视频观测的强视觉相似性进行时间传播，完全绕开SfM；3) 接触感知优化：融合语义、几何与交互稳定性约束，提升物理可行性；4) 通过真实到仿真迁移验证。

Result: 在HO3D、DexYCB及野外视频上，相比基线显著提升全局几何精度；在复杂、易失败序列上表现稳定。生成的资产可直接用于仿真，并通过real-to-sim重定向验证。

Conclusion: 将重心从重建转为生成与物理一致性约束，可在单目场景下获得鲁棒、可仿真的手-物体交互结果，优于依赖SfM与神经渲染的传统方法，适合机器人与VR应用。

Abstract: Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.

</details>


### [76] [DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking](https://arxiv.org/abs/2602.04692)
*Sijia Chen,Lijuan Ma,Yanqiu Yu,En Yu,Liman Liu,Wenbing Tao*

Main category: cs.CV

TL;DR: 提出RGBD指引多目标跟踪（DRMOT）新任务与数据集DRSet，并给出多模态LLM引导的深度感知跟踪框架DRTrack，在复杂空间语义与遮挡下实现更稳健的3D感知指代跟踪，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有指代多目标跟踪（RMOT）仅用2D RGB，难以解析诸如“离相机最近的人”等复杂空间语义，且在严重遮挡下难以保持ID一致；缺乏显式3D信息导致检测与关联不稳。

Method: 1) 定义RGB-深度-语言融合的DRMOT任务以实现3D感知指代跟踪；2) 构建DRSet数据集，含187个场景的RGB与深度图、240条语言描述，其中56条含深度相关信息；3) 提出DRTrack：一个由多模态大模型（MLLM）指导的深度指代跟踪框架，联合RGB-D-L输入进行深度感知目标定位，并在轨迹关联中显式利用深度线索提升稳健性。

Result: 在DRSet上进行大量实验，DRTrack在空间语义落地与跟踪稳健性方面取得显著效果，优于基线。

Conclusion: 融合RGB、深度与语言可显著提升指代多目标跟踪的空间语义理解与遮挡鲁棒性；DRSet与DRTrack为DRMOT研究提供基准与有效方案。

Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.

</details>


### [77] [Annotation Free Spacecraft Detection and Segmentation using Vision Language Models](https://arxiv.org/abs/2602.04699)
*Samet Hicsonmez,Jose Sosa,Dan Pineau,Inder Pal Singh,Arunkumar Rathinam,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 该论文提出一个无需人工标注的航天目标检测与分割流程：用预训练VLM在少量无标注真实数据上自动生成伪标签，再用教师-学生蒸馏训练轻量模型，在多个空间数据集上显著提升AP（最高+10）。


<details>
  <summary>Details</summary>
Motivation: 空间场景中目标往往与背景相融、光照变化大、可见性低，人工标注成本高且困难；同时VLM在零样本视觉识别上强，但在航天应用的潜力尚未被充分挖掘。亟需无需大量标注即可检测/分割航天器与轨道目标的方法。

Method: 1) 使用预训练的VLM在一小部分无标注真实数据上自动生成伪标签；2) 将这些伪标签输入教师-学生的标签蒸馏框架，训练轻量级检测/分割模型；3) 通过蒸馏缓解伪标签噪声，以获得比直接零样本VLM推理更好的效果。

Result: 在SPARK-2024、SPEED+和TANGO数据集上的分割任务中，平均精度（AP）稳定提升，最高可达+10点，优于直接使用VLM零样本推理。

Conclusion: 基于VLM伪标签与教师-学生蒸馏的无标注航天目标检测/分割方案有效，可在噪声伪标签下训练出性能更好的轻量模型，显著提升空间场景分割表现，并具备实用价值。

Abstract: Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.

</details>


### [78] [SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation](https://arxiv.org/abs/2602.04712)
*David F. Ramirez,Tim Overman,Kristen Jaskie,Joe Marvin,Andreas Spanias*

Main category: cs.CV

TL;DR: 提出SAR-RAG：将多模态大模型与向量库检索融合，用示例检索增强SAR自动目标识别，显著提升分类与尺寸回归等指标。


<details>
  <summary>Details</summary>
Motivation: SAR图像中不同车辆在视觉上常难以区分，传统ATR易受类间相似与噪声影响；利用已知标签的相似样例可为判别提供上下文，但需要一种可搜索、可比较、可推理的机制来把历史知识注入当前判定流程。

Method: 构建基于视觉上下文的ImageRAG代理：以MLLM为核心推理引擎，配套存有语义嵌入的向量数据库；对输入SAR目标，先用嵌入检索相似的、具有真实标签和尺寸信息的图像样例，将检索到的图像与元信息作为上下文供MLLM进行对比分析与生成，输出类别预测与尺寸回归；并以检索指标和任务指标评估。

Result: 在基线MLLM上附加SAR-RAG“记忆库”后，检索效果更好，目标类别分类准确率提升，车辆尺寸等数值回归误差降低，整体ATR性能改善。

Conclusion: 将示例检索与MLLM推理结合可为SAR ATR提供有用的先验视觉上下文，从而提升分类与度量精度；RAG式代理化框架是提升SAR识别的有效方向。

Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.

</details>


### [79] [How to rewrite the stars: Mapping your orchard over time through constellations of fruits](https://arxiv.org/abs/2602.04722)
*Gonçalo P. Matos,Carlos Santiago,João P. Costeira,Ricardo L. Saldanha,Ernesto M. Morgado*

Main category: cs.CV

TL;DR: 提出以3D质心“星座”匹配为核心的方法，在稀疏点云上实现跨视频同果实跟踪，进而构建果园地图与6DoF定位，用于产量预测与机器人导航/采摘。


<details>
  <summary>Details</summary>
Motivation: 人工用卡尺/树干径向计逐果测量劳动繁重且不可扩展；现有视觉方法可检测/计数/估尺，但缺少跨日期、跨视频对同一果实的可靠匹配，且既有尝试依赖固定起始相机位或GPS等外源信息，限制实用性。

Method: 提出“星座”范式：以果实3D质心构成极稀疏点云，设计相应的稀疏3D描述子，进行局部/全局的星座匹配而非单果匹配，以应对非刚性、遮挡与少纹理场景；由匹配结果构建果园地图，并据此进行6DoF相机位姿定位。

Result: 在具有非刚性、遮挡与少特征的果园视频中，方法可稳定将不同日期视频中的果实进行匹配，实现跨时序跟踪；能据此生成果园地图，并在此地图上实现鲁棒的相机6DoF定位。

Conclusion: 星座式3D质心匹配是解决跨视频同果追踪的有效范式，克服对固定相机起点或GPS的依赖，支持早期产量预测与农业机器人（自主导航、选择性采摘）等应用。

Abstract: Following crop growth through the vegetative cycle allows farmers to predict fruit setting and yield in early stages, but it is a laborious and non-scalable task if performed by a human who has to manually measure fruit sizes with a caliper or dendrometers. In recent years, computer vision has been used to automate several tasks in precision agriculture, such as detecting and counting fruits, and estimating their size. However, the fundamental problem of matching the exact same fruits from one video, collected on a given date, to the fruits visible in another video, collected on a later date, which is needed to track fruits' growth through time, remains to be solved. Few attempts were made, but they either assume that the camera always starts from the same known position and that there are sufficiently distinct features to match, or they used other sources of data like GPS. Here we propose a new paradigm to tackle this problem, based on constellations of 3D centroids, and introduce a descriptor for very sparse 3D point clouds that can be used to match fruits across videos. Matching constellations instead of individual fruits is key to deal with non-rigidity, occlusions and challenging imagery with few distinct visual features to track. The results show that the proposed method can be successfully used to match fruits across videos and through time, and also to build an orchard map and later use it to locate the camera pose in 6DoF, thus providing a method for autonomous navigation of robots in the orchard and for selective fruit picking, for example.

</details>


### [80] [Mitigating Long-Tail Bias via Prompt-Controlled Diffusion Augmentation](https://arxiv.org/abs/2602.04749)
*Buddhi Wijenayake,Nichula Wasalathilake,Roshan Godaliyadda,Vijitha Herath,Parakrama Ekanayake,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出一个可控扩散增强框架，通过显式控制“域（城市/乡村）”与“类别比例”来合成标签-图像对，从而缓解遥感语义分割中的长尾不平衡并提升跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 遥感高分影像语义分割数据存在严重长尾像素不平衡；LoveDA 数据集还存在城市/乡村两域外观差异与类别频次不一致，导致训练难、对少数类与跨域泛化差。需要一种能在保留语义一致性的同时，针对性补齐少数类并控制域差异的数据增强方法。

Method: 两阶段可控扩散合成：
- 阶段A：基于“域感知、掩膜与比例条件”的离散扩散模型生成布局（label mask），满足用户设定的类别比例目标，并遵循学习到的类别共现结构。
- 阶段B：利用 Stable Diffusion + ControlNet，将布局翻译为与指定域一致的逼真影像，得到成对的标签-图像样本。将按比例/域控制的合成数据与真实数据混合训练分割模型。

Result: 在多种分割骨干上稳定提升，提升主要集中在少数类；同时显著改善 Urban 与 Rural 的泛化性能。

Conclusion: 可控扩散数据合成是一种实用手段，可通过精确控制域与语义组成来缓解长尾偏差并提升遥感分割的跨域鲁棒性；代码、模型与合成数据已开源。

Abstract: Semantic segmentation of high-resolution remote-sensing imagery is critical for urban mapping and land-cover monitoring, yet training data typically exhibits severe long-tailed pixel imbalance. In the dataset LoveDA, this challenge is compounded by an explicit Urban/Rural split with distinct appearance and inconsistent class-frequency statistics across domains. We present a prompt-controlled diffusion augmentation framework that synthesizes paired label--image samples with explicit control of both domain and semantic composition. Stage~A uses a domain-aware, masked ratio-conditioned discrete diffusion model to generate layouts that satisfy user-specified class-ratio targets while respecting learned co-occurrence structure. Stage~B translates layouts into photorealistic, domain-consistent images using Stable Diffusion with ControlNet guidance. Mixing the resulting ratio and domain-controlled synthetic pairs with real data yields consistent improvements across multiple segmentation backbones, with gains concentrated on minority classes and improved Urban and Rural generalization, demonstrating controllable augmentation as a practical mechanism to mitigate long-tail bias in remote-sensing segmentation. Source codes, pretrained models, and synthetic datasets are available at \href{https://github.com/Buddhi19/SyntheticGen.git}{Github}

</details>


### [81] [Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention](https://arxiv.org/abs/2602.04789)
*Chengtao Lv,Yumeng Shi,Yushi Huang,Ruihao Gong,Shen Ren,Wenya Wang*

Main category: cs.CV

TL;DR: 提出 Light Forcing：首个面向自回归（AR）视频生成的稀疏注意力方案，通过分块感知增长与分层稀疏注意力，在保持质量的同时显著提速（VBench 84.5，端到端1.2~1.3×；结合FP8与LightVAE达2.3×、19.7 FPS）。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力在双向模型上有效，但直接用于AR视频生成会明显掉点。原因：1）只孤立地看当前chunk的生成，忽略跨chunk依赖；2）未充分利用历史有信息量的上下文。需要一个既高效又能保留AR依赖结构的稀疏方案。

Method: 两大设计：1）Chunk-Aware Growth（分块感知增长）：度量每个chunk的贡献，分配其稀疏度并随生成逐步增稀，让当前chunk能继承早期chunk的知识。2）Hierarchical Sparse Attention（分层稀疏注意力）：从粗到细在帧级与块级两层选择掩码，兼顾全局历史与局部上下文的关键信息，自适应不同注意力模式。

Result: 在多项实验中优于现有稀疏注意：质量更高（如VBench得分84.5），并带来端到端1.2~1.3×加速；与FP8量化和LightVAE联合可达2.3×加速与19.7 FPS（RTX 5090）。

Conclusion: Light Forcing针对AR视频生成定制稀疏注意力，通过分块贡献建模与分层掩码选择，在不显著牺牲质量下实现稳定提速，并可与量化/轻量VAE叠加，适合高效部署场景。

Abstract: Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \textsc{Light Forcing}, the \textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\eg, 84.5 on VBench) and efficiency (\eg, $1.2{\sim}1.3\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \textsc{Light Forcing} further achieves a $2.3\times$ speedup and 19.7\,FPS on an RTX~5090 GPU. Code will be released at \href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.

</details>


### [82] [VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?](https://arxiv.org/abs/2602.04802)
*Qing'an Liu,Juntong Feng,Yuhao Wang,Xinzhe Han,Yujie Cheng,Yue Zhu,Haiwen Diao,Yunzhi Zhuge,Huchuan Lu*

Main category: cs.CV

TL;DR: VISTA-Bench提出系统基准，用统一渲染把同语义的问题以纯文本与“可视化文本”（嵌入图像中的文字）对照，评测VLM在感知、推理到单模态理解上的差异，发现显著模态落差：纯文本强的模型在视觉文字输入上大幅退化，且渲染难度越高差距越大。


<details>
  <summary>Details</summary>
Motivation: 现实应用里文本常以图像中的文字出现，但现有评测多用纯文本查询，无法诊断VLM在视觉化文字上的真实能力与鲁棒性。作者希望用受控条件下的对照实验，量化“语义相同但呈现为像素文字”时性能下降的程度与原因。

Method: 构建VISTA-Bench，覆盖多模态感知、推理到单模态理解的任务；为每个问题生成两种等价形式：纯文本与视觉化文本（受控渲染参数），并系统改变感知难度（字体、分辨率、噪声等），对20+代表性VLM进行大规模评测，比较两种输入的表现差异。

Result: 多数VLM在纯文本上表现良好，但在视觉化文本条件下显著掉分；感知难度增加会进一步放大差距，表明模型对渲染变化敏感，即使语义不变。

Conclusion: 当前VLM缺乏统一的语言表示能力，难以在“token化文本”和“像素文字”之间保持一致理解。VISTA-Bench提供可复现、可控的评测框架，用于诊断该限制并推动在视觉文本理解与跨模态一致性上的改进。

Abstract: Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.

</details>


### [83] [X2HDR: HDR Image Generation in a Perceptually Uniform Space](https://arxiv.org/abs/2602.04814)
*Ronghuan Wu,Wanchao Su,Kede Ma,Jing Liao,Rafał K. Mantiuk*

Main category: cs.CV

TL;DR: 将HDR图像先转换到感知均匀编码（如PU21或PQ），在冻结LDR预训练VAE的前提下，仅在该空间对扩散去噪器进行低秩适配微调，可把现有LDR扩散模型高效扩展到HDR生成与RAW到HDR重建，并在感知保真、文图一致性和有效动态范围上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: HDR显示普及但缺乏大规模HDR训练数据，现有扩散模型多输出LDR；HDR线性RGB与LDR sRGB在强度与颜色统计上差异大，直接处理会导致重建退化，需要一种兼容预训练模型且不从头训练的方法。

Method: 将HDR输入转换到感知均匀编码（如PU21、PQ），实证发现LDR预训练VAE能高保真重建这类编码；据此在感知空间冻结VAE，仅对扩散去噪器进行低秩适配（LoRA）微调，形成统一流程，支持文本到HDR合成与单张RAW到HDR重建。

Result: 在多项实验中，该感知编码适配较先前技术显著提升了感知保真度、文本-图像对齐度以及生成结果的有效动态范围。

Conclusion: 通过在感知均匀空间进行最小化微调，可以高效复用LDR预训练扩散模型完成高质量HDR生成与重建，避免从零训练，并在多方面性能上取得一致改进。

Abstract: High-dynamic-range (HDR) formats and displays are becoming increasingly prevalent, yet state-of-the-art image generators (e.g., Stable Diffusion and FLUX) typically remain limited to low-dynamic-range (LDR) output due to the lack of large-scale HDR training data. In this work, we show that existing pretrained diffusion models can be easily adapted to HDR generation without retraining from scratch. A key challenge is that HDR images are natively represented in linear RGB, whose intensity and color statistics differ substantially from those of sRGB-encoded LDR images. This gap, however, can be effectively bridged by converting HDR inputs into perceptually uniform encodings (e.g., using PU21 or PQ). Empirically, we find that LDR-pretrained variational autoencoders (VAEs) reconstruct PU21-encoded HDR inputs with fidelity comparable to LDR data, whereas linear RGB inputs cause severe degradations. Motivated by this finding, we describe an efficient adaptation strategy that freezes the VAE and finetunes only the denoiser via low-rank adaptation in a perceptually uniform space. This results in a unified computational method that supports both text-to-HDR synthesis and single-image RAW-to-HDR reconstruction. Experiments demonstrate that our perceptually encoded adaptation consistently improves perceptual fidelity, text-image alignment, and effective dynamic range, relative to previous techniques.

</details>


### [84] [XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas](https://arxiv.org/abs/2602.04819)
*Aqsa Sultana,Rayan Afsar,Ahmed Rahu,Surendra P. Singh,Brian Shula,Brandon Combs,Derrick Forchetti,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 提出一种超轻量的状态空间深度学习模型XtraLight‑MedMamba，用于从全幅病理切片中区分将来进展为CRC的低级别管状腺瘤；仅约3.2万参数却达97.18%准确率与0.9767 F1，优于更大规模Transformer/Mamba模型。


<details>
  <summary>Details</summary>
Motivation: 常规结肠镜筛查中对低级别不典型增生的风险分层依赖主观病理判读，难以发现细微形态特征；需要一种能从数字病理WSI中捕捉与恶变相关微弱模式且具高效泛化的模型，以辅助早期精确风险评估。

Method: 提出XtraLight‑MedMamba：以ConvNeXt浅层特征提取器+并行Vision Mamba建模长短程依赖；加入空间-通道注意力桥（SCAB）进行多尺度特征强化；采用固定非负正交分类器（FNOClassifier）以减少参数并提升泛化；在基于后续是否发生CRC而分层的低级别管状腺瘤WSI数据集上训练与评估。

Result: 在该低级别管状腺瘤病例/对照队列上，模型以约3.2万参数取得97.18%准确率与0.9767 F1，显著优于高复杂度的Transformer与传统Mamba基线。

Conclusion: 超轻量状态空间架构结合注意力与正交分类器，可在极低参数量下实现对潜在恶变风险的高精度判别，提示在低级别腺瘤风险分层与临床部署中具备实用价值。

Abstract: Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.

</details>


### [85] [Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization](https://arxiv.org/abs/2602.04820)
*Farzia Hossain,Samanta Ghosh,Shahida Begum,B. M. Shahria Alam,Mohammad Tahmid Noor,Md Parvez Mia,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 提出基于公开数据集（3835张、6类、统一为224×224）的指甲疾病自动分类模型，比较四个CNN（InceptionV3、DenseNet201、EfficientNetV2、ResNet50），InceptionV3最高准确率95.57%；引入对抗训练提升鲁棒性，并用SHAP解释特征，期望辅助临床诊断。


<details>
  <summary>Details</summary>
Motivation: 指甲疾病跨年龄人群常见，老年人更多见，且常被忽视；早期识别有助于揭示潜在健康问题，但不同疾病视觉差异细微、难以区分，亟需自动化、高准确度的计算机辅助诊断。

Method: 基于公开指甲图像数据集（3835张、6类别），统一缩放至224×224；分别训练与评估四个主流CNN架构（InceptionV3、DenseNet201、EfficientNetV2、ResNet50）；通过对抗训练增强模型在含噪/对抗样本上的鲁棒性；利用SHAP对预测进行可解释性分析，突出关键判别区域/特征。

Result: InceptionV3取得最佳准确率95.57%，DenseNet201为94.79%，其余模型略低；对抗训练提升了模型应对困难图像的稳定性（摘要未给出具体数值）；SHAP可视化展示了模型关注的重要区域，支持诊断解释。

Conclusion: 所提系统在公开数据上达到较高精度并具可解释性与鲁棒性，可作为医生的辅助工具，加速并提升指甲疾病诊断的准确性；仍需在更大、更多样的临床数据上验证泛化能力。

Abstract: Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.

</details>


### [86] [LitS: A novel Neighborhood Descriptor for Point Clouds](https://arxiv.org/abs/2602.04838)
*Jonatan B. Bastos,Francisco F. Rivera,Oscar G. Lorenzo,David L. Vilariño,José C. Cabaleiro,Alberto M. Esmorís,Tomás F. Pena*

Main category: cs.CV

TL;DR: 提出LitS，一种用于2D/3D点云的方向性邻域描述子，通过在单位圆上定义分段常数函数来统计各方向锥形邻域的邻居数量，提供鲁棒、可调的局部几何表征，兼具常规与累积两种形式，适应不同密度与噪声。


<details>
  <summary>Details</summary>
Motivation: 点云分析依赖稳定可靠的邻域描述子以刻画局部几何，但现有方法在密度变化、噪声和形状细节捕获方面可能不足；需要一种能在方向维度上细粒度表达局部结构、并在实际问题（噪声/非均匀采样）下仍稳健的描述子。

Method: 构造LitS为单位圆上的分段常数函数：以局部参考系定义方向，将每个方向映射到围绕该方向的锥形区域；LitS在该方向的函数值为该锥域内邻居数量（或其累积版本）。提供两种版本（regular与cumulative）与两个可调参数（如角宽、半径/分辨率），用于适配不同场景与点云特性；比较相邻点的LitS以获得全局结构信息。

Result: LitS能够丰富地表达点的局部邻域分布，对密度变化与噪声具有鲁棒性；在不同类型点云与应用场景中展现了良好的适应性与区分力（摘要层面未给出具体数值或基准）。

Conclusion: LitS是通用且可调的2D/3D点云邻域描述子，通过方向性计数捕获局部几何细节，并能在噪声和非均匀采样下保持稳健；通过比较近邻点的LitS可促进对全局结构的理解。

Abstract: With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS' domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions ('regular' and 'cumulative') and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.

</details>


### [87] [When LLaVA Meets Objects: Token Composition for Vision-Language-Models](https://arxiv.org/abs/2602.04864)
*Soumya Jahagirdar,Walid Bousselham,Anna Kukleva,Hilde Kuehne*

Main category: cs.CV

TL;DR: Mask-LLaVA通过结合全局、局部patch与基于掩码的对象级特征，构建更紧凑的视觉表示，在推理时可动态减少对象token数量，显著降低计算而性能基本保持不变。


<details>
  <summary>Details</summary>
Motivation: 自回归VLM通常依赖大量视觉token，导致训练与推理计算昂贵，尤其在实际应用中推理成本成为瓶颈；需要一种在不牺牲性能的前提下减少视觉token、并能在推理时灵活调节token数的方法。

Method: 提出Mask-LLaVA：利用多层级视觉特征融合——全局图像token、局部patch token与基于掩码的对象token。训练期同时使用全部token以充分学习多层级信息；推理期可按需丢弃（主要是对象掩码token）以控制token数量，无需再训练。

Result: 在多项标准基准上，使用显著更少的视觉token仍达到与LLaVA基线相当的表现，并与当前高效token方法竞争；分析显示多层级特征的结合提升了在少token条件下的学习与表现。

Conclusion: 多层级视觉特征融合能在保持性能的同时显著减少视觉token，并支持测试时动态选择token数，兼顾精度与效率，降低自回归VLM推理成本。

Abstract: Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.

</details>


### [88] [Laminating Representation Autoencoders for Efficient Diffusion](https://arxiv.org/abs/2602.04873)
*Ramón Calvo-González,François Fleuret*

Main category: cs.CV

TL;DR: 提出FlatDINO：把DINOv2的密集patch特征压缩成32个连续token的一维序列，使扩散模型在特征空间生成更高效，同时保持高质量（ImageNet256上DiT-XL用CFG得gFID 1.80），推理FLOPs降8倍、训练每步最高降4.5倍；工作仍在进行中。


<details>
  <summary>Details</summary>
Motivation: 直接在SSL（如DINOv2）patch特征上做扩散能生成高质量图像，但原始特征网格冗余度高，序列很长，导致扩散计算代价大、训练推理都低效，需要一种在不明显损失生成质量的前提下压缩特征表示的方法。

Method: 构建名为FlatDINO的VAE，将二维密集patch网格编码为仅32个连续token的一维潜变量序列，实现序列长度8×缩短与总维度48×压缩；随后在该潜空间上训练DiT-XL扩散变换器，并采用classifier-free guidance评估生成质量与效率。

Result: 在ImageNet 256×256上，基于FlatDINO潜变量训练的DiT-XL取得gFID 1.80（带CFG）；相比直接在未压缩DINOv2特征上扩散，前向推理FLOPs减少8×，训练每步FLOPs最多减少4.5×。

Conclusion: 用FlatDINO对SSL特征进行强压缩可显著降低扩散模型在特征空间生成的计算成本，同时保持甚至提升生成质量；结果为初步且工作仍在进行，尚需更系统的验证与泛化评估。

Abstract: Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.

</details>


### [89] [PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation](https://arxiv.org/abs/2602.04876)
*Jiahao Zhan,Zizhang Li,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: PerpetualWonder提出一种从单张图像出发、可执行长时程动作条件4D场景生成的闭环混合生成模拟器，通过统一表示联通物理状态与视觉原语，并用多视角监督的稳健更新机制，确保外观与动力学共同随生成 refinement 同步更新，最终实现物理可信且视觉一致的多步交互模拟。


<details>
  <summary>Details</summary>
Motivation: 现有方法将物理状态与视觉表示解耦，导致生成后的视觉改进无法回写影响后续交互的物理演化；同时优化存在歧义，缺乏跨视角监督，难以在长时程动作下保持一致性与可扩展性。

Method: 1) 闭环架构：提出“真实”闭环系统，使生成 refinement 可同时作用于动力学与渲染。2) 统一表示：以新的物理-视觉双向耦合表示，将物理状态与视觉原语绑定，形成可相互约束的联合更新。3) 稳健更新机制：汇聚多视角监督，缓解优化歧义，确保对外观与动力学的一致修正。

Result: 在从单张图像出发的长时程、动作条件4D生成任务上，系统能成功模拟复杂多步交互，保持物理合理性与视觉一致性，优于现有方法（摘要未给出具体数值）。

Conclusion: 闭环统一表示与多视角监督是实现从单图像进行长时程动作条件4D生成的关键，使生成器能在交互过程中持续自洽地更新动力学与外观，达成稳定且可信的长序列模拟。

Abstract: We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.

</details>


### [90] [CoWTracker: Tracking by Warping instead of Correlation](https://arxiv.org/abs/2602.04877)
*Zihang Lai,Eldar Insafutdinov,Edgar Sucar,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 提出一种无需代价体、基于特征warp与Transformer联合时空推理的稠密点跟踪器，复杂度更优，性能在多项跟踪与光流基准上达SOTA，并显示跟踪与光流可被统一。


<details>
  <summary>Details</summary>
Motivation: 现有稠密点跟踪多依赖代价体进行跨帧匹配，导致空间分辨率上二次复杂度，限制了可扩展性与效率。希望用更高效的机制在保持或提升精度的同时建立长程对应。

Method: 以“迭代warp”替代显式代价体：根据当前轨迹估计将目标帧特征warp到查询帧，再更新轨迹；配合Transformer对所有轨迹进行联合的时空推理，无需计算特征相关性即可建立长距离对应；整体进行多步细化。

Result: 在TAP-Vid-DAVIS、TAP-Vid-Kinetics、Robo-TAP等稠密跟踪基准上达SOTA；在Sintel、KITTI、Spring光流基准上也具竞争力，部分超过专用方法。

Conclusion: 基于warp的架构在效率与性能上兼具，可统一稠密点跟踪与光流估计，提示未来可用无代价体的时空Transformer+warp范式构建通用运动估计模型。

Abstract: Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.

</details>
