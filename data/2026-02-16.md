<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring](https://arxiv.org/abs/2602.12361)
*Constantino Álvarez Casado,Mohammad Rahman,Sasan Sharifipour,Nhi Nguyen,Manuel Lage Cañellas,Xiaoting Wu,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 该文提出利用面部热红外视频无接触估计三类生理信号：皮电活动（EDA）、心率（HR）与呼吸率（BR），并在公开驾驶数据集SIM1上系统评估。最佳EDA配置在鼻部ROI配合指数滑动平均，平均相关0.40（单场景可达0.89）；BR MAE≈3.1 bpm；HR受7.5 Hz帧率限制，MAE≈13.8 bpm。论文给出基线性能、信号极性变化与延迟特性，并提供设计指导。


<details>
  <summary>Details</summary>
Motivation: 现有可见光远程生理监测可估计HR/BR，但无法获取反映交感兴奋的EDA；而热红外能观测汗腺与皮温变化，或可实现EDA等多模态无接触测量。需要系统刻画从面部热视频中同时提取EDA/HR/BR的可行性、方法与性能上限。

Method: 提出信号处理流水线：1) 面部解剖区域跟踪与空间聚合；2) 分离慢速汗腺（EDA）趋势与较快的心肺成分；3) HR：对多个ROI应用OMIT（正交矩阵图像变换）分解并峰值检测；4) BR：鼻部与脸颊信号求平均后谱峰检测；5) EDA：评估288种配置（不同ROI与滤波/聚合策略），如鼻部ROI+指数滑动平均；在SIM1（31段会话）上统一评估。

Result: 最佳固定EDA配置达与掌部EDA平均相关0.40±0.23，单会话最高0.89；BR估计误差3.1±1.1 bpm；HR估计误差13.8±7.5 bpm（受7.5 Hz帧率限制）。观测到信号极性随会话变化、良好跟踪下热延迟较短，且条件与人群特征影响提取质量。

Conclusion: 面部热红外可在无接触条件下同时估计EDA/HR/BR，但当前性能受相机帧率与条件限制；论文给出基线表现与工程设计建议（选鼻部ROI、考虑极性与延迟、优化跟踪与采样率），为今后改进硬件与算法提供方向。

Abstract: Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \pm 1.1$ bpm, while HR estimation yields $13.8 \pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.

</details>


### [2] [LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens](https://arxiv.org/abs/2602.12370)
*Zekun Li,Sizhe An,Chengcheng Tang,Chuan Guo,Ivan Shugurov,Linguang Zhang,Amy Zhao,Srinath Sridhar,Lingling Tao,Abhay Mittal*

Main category: cs.CV

TL;DR: 提出LLaMo：在不破坏LLM语言能力的前提下统一动作-语言生成与理解，采用模态特定MoT与连续潜空间+流匹配头，实现实时高保真文本转动作与动作转文本，零样本表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM微调用于动作-文本的方案受限于配对数据规模，易遗忘语言能力；且常用离散量化将动作转为token带来抖动伪影，影响质量与流畅性。需要一种既保留语言理解、又能高效适配动作模态并支持高质量生成/理解的统一框架。

Method: 在预训练LLM上增添模态特定的Mixture-of-Transformers（MoT）以实现可扩展多模态适配；将人体动作编码为因果连续潜空间；在decoder-only结构上用轻量级flow-matching头保持下一token预测范式，从而实现>30 FPS的流式动作生成；并进行大规模动作-文本预训练。

Result: 在通用设置下实现高保真文本转动作与动作转文本；尤其在零样本动作生成上表现突出；保持了LLM的语言理解能力并减少离散量化导致的抖动。

Conclusion: LLaMo证明了通过MoT与连续潜空间+流匹配的设计，可以在不牺牲语言能力的情况下统一动作-语言生成与理解，并支持实时、可扩展的多模态适配，迈向通用的动作-语言大模型。

Abstract: Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.

</details>


### [3] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: 论文研究使用CLIP特征进行合成图像检测（SID），提出SynthCLIC数据集以降低语义偏差，并用可解释线性头与文本概念模型分析CLIP学到的线索。结果显示CLIP线性探测器总体有效，但在高质量扩散图像与跨生成器泛化上显著下降，主要依赖高层摄影属性而非低层伪影。


<details>
  <summary>Details</summary>
Motivation: 生成模型逼真度提升使照片可信度受挑战，现有SID方法在新模型与真实场景下泛化差。CLIP在SID上表现强，但不清楚其利用的是可泛化的语义线索还是易失效的伪影/偏差，因而需要一个去语义偏差的数据与可解释分析。

Method: 构建SynthCLIC：由真实照片与高质量扩散模型合成的成对数据，尽量匹配语义以降低主题偏差。基于CLIP图像嵌入训练去相关的可解释线性分类头，并结合文本驱动的概念模型，对判别线索进行分析；在GAN与扩散基准上评测性能与跨家族泛化。

Result: CLIP线性探测器在GAN基准mAP=0.96，在SynthCLIC（高质量扩散）上mAP=0.92；跨生成器家族泛化最低降至mAP=0.37。分析显示模型主要依赖高层摄影学属性（极简风格、镜头炫光、景深分层等），而非生成器特定伪影。

Conclusion: CLIP为SID提供强大基线但泛化不均，尤其在不同生成架构间显著下降。SID需持续更新与更广覆盖的训练数据；应关注高层摄影属性并减少对特定伪影的依赖，CLIP仍是构建更通用、鲁棒SID的有力基础。

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

</details>


### [4] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: 复现研究表明：DragDiffusion 的核心主张基本成立（单一中间时刻的潜变量优化+保身份微调+空间正则可实现精确拖拽编辑），但对少量关键超参（优化时刻与监督特征层级）高度敏感；多时刻优化无益且更耗时。


<details>
  <summary>Details</summary>
Motivation: DragDiffusion 声称通过在扩散过程中的单一中间时刻优化潜变量，并辅以身份保持微调与空间正则，能实现精准的点拖拽式图像编辑。社区需要验证这些结论在公开代码与标准基准（DragBench）下是否稳健可复现，并厘清影响性能的关键超参数与可操作范围。

Method: 使用作者开源实现与 DragBench 基准，系统复现实验与消融：比较不同扩散时刻选择、基于 LoRA 的身份保持微调强度、掩膜正则强度、以及对 UNet 不同层级特征的运动监督；并评估多时刻潜变量联合优化的变体。指标兼顾定性趋势与定量空间精度/代价。

Result: 复现结果与原论文的定性、定量趋势高度一致。性能对少数超参非常敏感：尤其是被优化的扩散时刻与用于运动监督的 UNet 特征层级；其他部件（如 LoRA、掩膜正则）的可用范围更宽。多时刻潜变量优化未带来空间精度提升，却显著增加计算开销。

Conclusion: DragDiffusion 的核心结论得到支持：单时刻潜变量优化在合适超参下可实现精确的点拖拽编辑；但需谨慎选择优化时刻与监督层级以确保稳定复现。多时刻优化不推荐。代码已公开，研究澄清了何种条件下方法可可靠复现。

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

</details>


### [5] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文提出对比RL与监督微调在多模态视觉推理中的具体改进来源，发现RL主要在中后层引入一致的推理时偏移，非均匀提升感知，而是优化视觉到推理的对齐与推理计算。


<details>
  <summary>Details</summary>
Motivation: 端到端基准分数掩盖了RL到底改进了哪些能力，与作为冷启动的监督微调难以区分。需要一种可解释分析来隔离RL带来的具体技能与参数层面变化。

Method: 提出“弗兰肯斯坦”分析框架：1) 因果探测进行功能定位；2) 参数比较表征更新特性；3) 通过模型合并做可迁移性测试。同时用冻结实验检验必要性。

Result: RL在推理时引入一致的分布/表征偏移，主要发生在Transformer的中后层；这些中后层的改进可通过模型合并迁移，且在冻结实验中被证明是获得RL收益的必要部分。

Conclusion: RL对视觉推理的可靠贡献并非统一提升视觉感知，而是系统性地精炼中后层的计算，改善视觉-推理对齐与推理表现；仅用基准分数评估不足以理解多模态推理改进机理。

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

</details>


### [6] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: 提出ZeroDiff++，用扩散模型与测试时自适应/生成机制缓解ZSL中的伪相关与数据稀缺，显著提升三基准表现。


<details>
  <summary>Details</summary>
Motivation: 生成式ZSL常依赖从见类学到的视觉-语义相关再合成未见类特征，但见类样本稀缺与不自适应、全噪声的生成器导致伪相关和与测试分布脱节，限制泛化。作者希望量化并削弱这种伪相关，同时拉近生成特征与真实测试样本分布。

Method: 提出ZeroDiff++：1) 训练期：a) 扩散增广生成多样噪声样本；b) 用监督对比学习构建实例级语义表征；c) 多视角判别器+Wasserstein互学习评估并对齐生成特征。2) 测试期：a) DiffTTA基于伪标签重构做扩散式测试时自适应；b) DiffGen沿扩散去噪轨迹输出“部分合成”特征，将真实与生成数据连通、缓解数据稀缺。并提出两种衡量见类/未见类伪相关的指标。

Result: 在三个ZSL基准上较现有方法取得显著提升；在见类样本稀缺设置下仍保持鲁棒性能。

Conclusion: 通过扩散框架与测试时自适应/部分生成，将生成特征与真实分布更好对齐，降低伪相关、缓解数据稀缺，显著提升ZSL泛化；方法通用且可复现实验代码将开源。

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

</details>


### [7] [MonoLoss: A Training Objective for Interpretable Monosemantic Representations](https://arxiv.org/abs/2602.12403)
*Ali Nasiri-Sarvi,Anh Tien Nguyen,Hassan Rivaz,Dimitris Samaras,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 提出MonoScore单通量算法与MonoLoss训练目标，使稀疏自编码器更高效地学习单义特征，并带来显著加速与更高解释性与下游精度小幅提升。


<details>
  <summary>Details</summary>
Motivation: SAE常学到多义神经元，现有目标对单义分解鼓励不足；评估单义性的主流度量需全样本两两比较，训练/评估代价高，难以作为训练信号使用。

Method: 1) 对近期MonoScore度量进行推导，提出单次线性扫描即可精确计算的算法，将复杂度由O(N^2)降至O(N)。2) 将该度量转化为可微/可用的训练信号，提出Monosemanticity Loss（MonoLoss），直接奖励语义一致的激活；作为即插即用正则，与多种SAE变体（BatchTopK/TopK/JumpReLU）和多种特征骨干（CLIP、SigLIP2、预训练ViT）结合。3) 还将MonoLoss作为辅助正则用于ResNet-50和CLIP-ViT-B/32微调。

Result: - 在OpenImagesV7上，MonoScore评估加速最高达约1200倍、训练阶段加速159倍，且每epoch仅约4%开销。- 在多种编码器与SAE组合下，大多数潜变量的MonoScore提升；类别纯度显著提高，最大从0.152升至0.723。- 在ImageNet-1K上作为辅助正则可带来最高约0.6%准确率提升，并在标准数据集上产生单义激活模式。

Conclusion: 通过将高效MonoScore引入训练（MonoLoss），可高效学习更单义且可解释的表征，并在不显著增加开销的情况下提升下游性能；方法通用、可插拔、代码已开源。

Abstract: Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.

</details>


### [8] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: 提出PathoSpatial：一种将全视野病理图像(WSI)与空间转录组(ST)共配准融合，用于弱监督生存预后建模的可解释端到端框架；通过多层专家与任务引导原型学习，在保持判别力的同时显著增强可解释性，并在TNBC数据集多个生存终点上优于或媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: WSI可通过MIL进行弱监督预后，但仅有形态信息；ST提供原位分子空间信息。随着成对WSI-ST队列在群体尺度增长，如何原则性地融合两种互补的空间信号进行预后仍缺乏成熟策略，尤其需要兼顾可扩展性与可解释性。

Method: 提出PathoSpatial框架：对齐WSI与ST，采用多层专家(multi-level experts)架构；在模态内进行无监督原型发现，同时用任务（生存）目标指导的原型学习进行跨模态聚合；以MIL范式学习空间感知的预后表征，内置可解释模块以支持原型级与分子风险分解的后验解释。

Result: 在配对的三阴性乳腺癌(TNBC)WSI-ST队列上，针对五个生存终点取得稳定、强劲的表现，整体优于或可与领先的单模态和多模态基线匹敌；模型产生的原型解释与分子风险分解提供定量、具生物学依据的解释并指出潜在预后因子。

Conclusion: PathoSpatial验证了在空间组学与病理融合场景下，可扩展且可解释的多模态学习范式的可行性与优势，为今后人群规模的WSI-ST预后模型提供了方法学基础与解释框架。

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

</details>


### [9] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: 论文指出：仅用“手工模板+余弦相似度”生成对抗样本来微调CLIP图像编码器，会误估图文语义相似度，导致鲁棒性被高估。作者提出语义集成攻击与SAFT，用多样且经消幻觉的文本描述生成语义感知对抗样本进行微调，显著提升16个数据集上的零样本对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有做法用单一手工模板（如“A photo of a {label}”）与余弦相似度生成AEs并微调CLIP图像端，然而单图—单模板的余弦相似度无法充分刻画图文对齐的语义，导致生成的AEs在采用更丰富的语义相似度度量时并不具有欺骗性，从而微调后的模型鲁棒性不足。

Method: 1) 指出问题：基于余弦相似度与手工模板生成的AEs在更语义化的相似度下失效。2) 语义集成攻击：用基础模型先生成覆盖核心语义的多样文本描述，再进行去幻觉精炼；以这些描述的平均相似度为目标最小化，得到语义感知AEs。3) SAFT：用上述语义感知AEs对CLIP的图像编码器进行对抗微调。

Result: 在16个数据集上，所提SAFT在零样本对抗鲁棒性上优于现有方法，带来显著提升（定量细节未在摘要中给出）。

Conclusion: 仅依赖手工模板与余弦相似度会产生“伪强”的对抗鲁棒性。通过语义集成与消幻觉得到的语义感知AEs进行微调（SAFT）可以更真实地覆盖语义空间，从而显著提升CLIP零样本对抗鲁棒性。

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

</details>


### [10] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: 提出一种基于优化DenseNet-121的葡萄叶病害分类方法，在多个病害（如细菌性腐烂、霜霉、白粉）上取得约99%的精度与可解释性增强，并兼顾计算效率与可部署性。


<details>
  <summary>Details</summary>
Motivation: 现有基于YOLO等方法的自动化病害检测在计算开销和可解释性上存在不足，限制了在真实葡萄园场景中的应用。需要一种既高精度、又高效率、且具可解释性的叶片病害识别方案，以实现早期、精准诊断并支撑可持续管理。

Method: 采用优化的DenseNet-121作为主干，通过领域特定的预处理（强调叶脉、边缘与病斑等结构特征）和致密连接来增强特征提取；利用迁移学习应对小样本与类别不平衡；通过模型优化减少推理开销；使用Grad-CAM可视化关注区域提升可解释性与信任度；与ResNet18、VGG16、AlexNet、SqueezeNet进行对比实验；进行交叉验证评估泛化。

Result: 所提模型在测试中达到Accuracy 99.27%、F1 99.28%、Specificity 99.71%、Kappa 98.86%，单次推理约9秒；交叉验证平均准确率99.12%，表明在各类别上具有稳健泛化。

Conclusion: 结合高效架构、领域化预处理与可解释性机制的优化DenseNet-121框架，在葡萄叶病害检测上实现高精度、强鲁棒与较低计算成本，适合可扩展和接近实时的部署场景。

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

</details>


### [11] [Human-Like Coarse Object Representations in Vision Models](https://arxiv.org/abs/2602.12486)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 论文探究分割模型的“物体体积表示”与人类直觉物理中的粗粒度体积是否一致，并发现在人力/算力受限的中等模型与早期检查点最对齐。


<details>
  <summary>Details</summary>
Motivation: 人类在直觉物理中倾向用粗糙、体积化、平滑凹陷的“粗身体”来表示物体，以便高效预测；而计算机视觉的分割模型追求像素级精确边界，可能与这种粗表示不一致。尚不清楚：分割模型是否、何时会自发形成“类人粗身体”。

Method: 提出基于“碰撞时间（TTC）”的人类行为范式来评估模型与人类的对齐度，设计比较流程与对齐指标；系统操纵模型资源/容量：训练时长、模型规模、以及通过剪枝改变有效容量，观察这些因素对“类人粗身体”出现的影响。

Result: 无论从训练时长、模型规模还是剪枝程度，人与模型的对齐均呈倒U型：过小/短训/强剪枝模型会欠分割成模糊团块；过大/长训模型则过度分割、产生边界抖动；介于两者之间的“理想体粒度”与人类最吻合。

Conclusion: 类人粗身体更像是资源约束下的涌现，而非特定先验偏置所致。通过选择早期检查点、中等规模架构、轻度剪枝等简单“旋钮”，可诱导对物理高效的表示；这一发现支持在识别细节与物理可供性之间进行资源理性权衡的观点。

Abstract: Humans appear to represent objects for intuitive physics with coarse, volumetric bodies'' that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity'' best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.

</details>


### [12] [Insertion Network for Image Sequence Correspondence](https://arxiv.org/abs/2602.12489)
*Dingjie Su,Weixiang Hong,Benoit M. Dawant,Bennett A. Landman*

Main category: cs.CV

TL;DR: 提出一种通过“插入式”注意力网络为2D图像序列建立对应关系的方法，用于在3D体数据中精确定位2D切片，较传统体位回归显著降低定位误差（8.4mm→5.4mm）。


<details>
  <summary>Details</summary>
Motivation: 临床与计算机视觉任务（诊断、配准、分割）常需从一组2D切片中确定其在3D体积中的解剖覆盖与关键切片位置。现有“身体部位回归”逐片独立预测，忽略序列上下文，限制了定位精度与鲁棒性。

Method: 训练一个“插入网络”：对每个切片编码上下文表征，并用切片到切片的注意力机制建模“将一张来自序列A的切片插入到序列B的正确位置”的过程，从而学习跨序列对应关系。与逐片回归不同，方法利用整段序列的上下文信息。

Result: 在体部CT关键切片定位任务上，相比SOTA身体部位回归基线，监督设定下的平均定位误差由8.4mm降至5.4mm。

Conclusion: 利用序列级上下文和插入式注意力可更准确地建立序列间对应并定位关键切片，优于逐片独立的体位回归方法，对下游配准与分割等流程具有有益的预处理价值。

Abstract: We propose a novel method for establishing correspondence between two sequences of 2D images. One particular application of this technique is slice-level content navigation, where the goal is to localize specific 2D slices within a 3D volume or determine the anatomical coverage of a 3D scan based on its 2D slices. This serves as an important preprocessing step for various diagnostic tasks, as well as for automatic registration and segmentation pipelines. Our approach builds sequence correspondence by training a network to learn how to insert a slice from one sequence into the appropriate position in another. This is achieved by encoding contextual representations of each slice and modeling the insertion process using a slice-to-slice attention mechanism. We apply this method to localize manually labeled key slices in body CT scans and compare its performance to the current state-of-the-art alternative known as body part regression, which predicts anatomical position scores for individual slices. Unlike body part regression, which treats each slice independently, our method leverages contextual information from the entire sequence. Experimental results show that the insertion network reduces slice localization errors in supervised settings from 8.4 mm to 5.4 mm, demonstrating a substantial improvement in accuracy.

</details>


### [13] [Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models](https://arxiv.org/abs/2602.12498)
*Ali Abbasi,Mehdi Taghipour,Rahmatollah Beheshti*

Main category: cs.CV

TL;DR: 提出NAST方法与放射学否定敏感性基准，利用因果追踪引导分层更新，显著提升VLM对否定与肯定陈述的区分且不损伤总体对齐。


<details>
  <summary>Details</summary>
Motivation: 临床影像报告中否定表达普遍且关键，但现有医学VLM常混淆否定与非否定，缺乏系统评测与可用于学习复杂（带位置、严重度等属性）否定的训练数据。

Method: 1) 构建放射学否定敏感性诊断基准，控制临床条件评估极性敏感性；2) 构造具结构化主张与属性级（位置、严重度）否定的数据集；3) 提出否定感知选择性训练（NAST）：用因果追踪效应(CTEs)估计各层对否定处理的因果贡献，并按贡献缩放微调阶段的层级梯度更新，而非使用统一学习率。

Result: 在多个医学VLM上，NAST提升了对肯定/否定陈述的判别能力，同时保持一般视觉-语言对齐性能不下降。

Conclusion: 将可解释性中的因果信号转化为优化规则，可在安全关键的医疗环境中实现针对性的模型适配；所提基准与数据集为系统研究临床否定提供了工具。

Abstract: Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.

</details>


### [14] [Matching of SAR and optical images based on transformation to shared modality](https://arxiv.org/abs/2602.12515)
*Alexey Borisov,Evgeny Myasnikov,Vladislav Myasnikov*

Main category: cs.CV

TL;DR: 提出一种将光学与SAR图像共同映射到共享模态后再进行匹配的方法，并直接利用现成的RoMa/DeDoDe模型实现高质量配准，优于基于风格翻译与传统特征匹配的方案。


<details>
  <summary>Details</summary>
Motivation: 光学与SAR成像物理机理差异巨大，导致跨模态精确配准困难。现有方法或做图像翻译、或做手工/学习特征匹配，常出现失真、退化或泛化差。作者动机是构建一个对两种传感器都“通用”的表示，使跨模态匹配退化为同模态匹配问题，从而复用成熟的自然图像匹配模型。

Method: 将光学与SAR图像映射到一个共享的新模态：1) 保证固定且相同的通道数；2) 变换后、已配准的成对图像在该模态中尽量相似；3) 非退化，能保留原图显著结构。完成变换后，直接训练并应用RoMa（以及可用DeDoDe）在该共享模态上进行匹配，无需为新模态重新预训练自然图像匹配模型。

Result: 在MultiSenGE公开数据集（含光学与SAR）上评估，新方法在配准质量上优于：a) 跨模态图像翻译方法；b) 多种特征匹配算法。同时展现更高的通用性，可直接调用现成的RoMa/DeDoDe模型实现高质量匹配。

Conclusion: 通过将光学与SAR统一到共享模态，使跨模态匹配转化为同模态问题，显著提升配准效果与实用性。该方案无需为新模态重新训练自然图像匹配模型，具有更强的通用性和工程价值。

Abstract: Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.

</details>


### [15] [LiDAR-Anchored Collaborative Distillation for Robust 2D Representations](https://arxiv.org/abs/2602.12524)
*Wonjun Jo,Hyunwoo Ha,Kim Ji-Yeon,Hawook Jeong,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出“协同蒸馏”：用3D LiDAR作为自监督信号来增强2D图像编码器在噪声与恶劣天气下的鲁棒性，同时保持原有能力，并提升3D感知；在多种下游任务与环境中优于对比方法且具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有自监督预训练的2D图像编码器多在晴天清晰场景上表现良好，但在噪声、雨雪、雾等恶劣天气中鲁棒性不足；实际应用（自动驾驶、安防等）需要在多样环境下稳定感知。利用对恶劣条件更稳健的LiDAR特性作为教师，可为2D视觉提供更可靠的监督信号。

Method: 提出Collaborative Distillation：以3D LiDAR为自监督/教师信号，对2D图像编码器进行蒸馏训练，使其学习到由LiDAR提供的稳健几何与深度线索，同时保留原有2D能力。该框架在多条件数据上训练，强调跨模态一致性与鲁棒特征学习，并提升2D编码器的3D感知意识。

Result: 在多种下游任务与多种环境条件（含噪声与恶劣天气）上优于竞争方法，表现出更强的泛化与稳健性；同时在需要3D理解的任务中也有提升。

Conclusion: 跨模态自监督（以LiDAR为教师）能显著增强2D编码器在非理想环境下的鲁棒性与3D感知能力，具有现实落地价值与适应性。

Abstract: As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR's characteristics. This advancement highlights our method's practicality and adaptability in real-world scenarios.

</details>


### [16] [Geometric Stratification for Singular Configurations of the P3P Problem via Local Dual Space](https://arxiv.org/abs/2602.12525)
*Xueying Sun,Zijia Li,Nan Li*

Main category: cs.CV

TL;DR: 论文用代数-计算框架系统刻画了P3P问题的所有奇异构型：当相机中心O的重数μ≥2时O在危险圆柱上；μ≥3时O在与第一Morley三角形或外接圆相关的危险圆柱三条母线上；μ≥4时O在外接圆上并对应无限解。同时刻画了互补构型O′：μ≥2时O′在与危险圆柱相关的风筝形（deltoid）曲面上；μ≥3时O′在其三条尖点曲线之一上。


<details>
  <summary>Details</summary>
Motivation: P3P（Perspective-Three-Point）是计算机视觉中的经典位姿估计问题。其数值不稳定和多解主要来自奇异构型。为提高算法鲁棒性和理解多解结构，需要对奇异情形进行完备、可计算且具几何直观的分类与描述。

Method: 基于局部对偶空间（local dual space）的代数-几何方法：构建描述P3P解的代数簇与相机中心O的重数μ之间的关系；通过符号计算与几何解析，将奇异条件转化为危险圆柱、其母线、以及外接圆等明确的几何实体；进一步对互补构型O′推导其落在与危险圆柱对应的deltoid曲面及其尖点曲线上的条件。

Result: 给出以重数μ为层级的几何分层：μ≥2⇔O在危险圆柱；μ≥3⇔O在与第一Morley三角形或外接圆相关的三条母线之一；μ≥4⇔O在外接圆，对应无限多个P3P解；互补构型O′：μ≥2⇔O′在deltoid曲面上；μ≥3⇔O′在其三条尖点曲线之一。

Conclusion: 提出的局部对偶空间框架实现了对P3P奇异构型的完备几何分层，统一了危险圆柱、母线、外接圆与互补deltoid曲面的关系，解释了多解与无限解的来源，可为P3P求解器的退化检测与数值稳定化提供直接判据。

Abstract: This paper investigates singular configurations of the P3P problem. Using local dual space, a systematic algebraic-computational framework is proposed to give a complete geometric stratification for the P3P singular configurations with respect to the multiplicity $μ$ of the camera center $O$: for $μ\ge 2$, $O$ lies on the ``danger cylinder'', for $μ\ge 3$, $O$ lies on one of three generatrices of the danger cylinder associated with the first Morley triangle or the circumcircle, and for $μ\ge 4$, $O$ lies on the circumcircle which indeed corresponds to infinite P3P solutions. Furthermore, a geometric stratification for the complementary configuration $O^\prime$ associated with a singular configuration $O$ is studied as well: for $μ\ge 2$, $O^\prime$ lies on a deltoidal surface associated with the danger cylinder, and for $μ\ge 3$, $O^\prime$ lies on one of three cuspidal curves of the deltoidal surface.

</details>


### [17] [Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting](https://arxiv.org/abs/2602.12540)
*Haoran Zhu,Anna Choromanska*

Main category: cs.CV

TL;DR: 提出AD-LiST-JEPA：一种基于LiDAR、使用JEPA自监督框架的自动驾驶世界模型，能从未标注数据学习并预测未来时空演化；在占据补全与预测任务上，预训练编码器带来更好表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要能进行长期规划的世界模型，但大规模标注昂贵。JEPA提供了从海量未标注数据学习表示的途径，作者希望将其用于LiDAR以提升时空预测与下游任务表现。

Method: 构建AD-LiST-JEPA：基于JEPA的自监督框架，从LiDAR点云学习时空表示并预测未来状态。通过预训练编码器获取表示，再在下游LiDAR占据补全与预测（OCF）任务上进行评估。

Result: 概念验证实验显示：经过JEPA世界模型预训练的编码器在OCF任务上优于未预训练基线。

Conclusion: JEPA式自监督世界模型可有效从LiDAR学习时空表示，提升自动驾驶中OCF等下游任务性能，验证了该方向的可行性。

Abstract: Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.

</details>


### [18] [PLLM: Pseudo-Labeling Large Language Models for CAD Program Synthesis](https://arxiv.org/abs/2602.12561)
*Yuanbo Li,Dule Shu,Yanying Chen,Matt Klenk,Daniel Ritchie*

Main category: cs.CV

TL;DR: 提出PLLM自训练框架，从无标注3D形状中恢复/合成CAD程序，利用预训练CAD类LLM进行迭代采样、筛选与增强，生成合成形—程对用于微调；在ABC数据集上适配CAD-Recode/DeepCAD，提升几何保真与程序多样性。


<details>
  <summary>Details</summary>
Motivation: 现有CAD程序合成依赖成对的形状-程序监督数据，现实中稀缺；LLM虽有潜力，但缺乏无监督/弱监督方法。需要一种能利用仅有形状数据提升CAD程序生成质量与多样性的学习框架。

Method: 提出PLLM：1) 以预训练具CAD能力的LLM为起点；2) 对无标注3D形状迭代采样候选程序；3) 执行并用几何相似度等指标选择高保真样本；4) 对程序进行数据增强与清洗；5) 将高质量形—程对作为伪标签，用于对LLM微调；6) 循环迭代以逐步提升模型。实验以DeepCAD的CAD-Recode为基座，迁移到ABC数据集。

Result: 在ABC无标注数据上，与原始基线相比，生成程序执行后的几何与目标形状更贴近（更高几何保真度），且生成的程序样式更为多样；报告“持续一致”的改进。

Conclusion: PLLM证明了在缺少形—程配对数据时，借助自训练可有效提升CAD程序合成，兼顾几何精度与程序多样性；为无标注3D形状到CAD程序的学习提供了通用范式。

Abstract: Recovering Computer-Aided Design (CAD) programs from 3D geometries is a widely studied problem. Recent advances in large language models (LLMs) have enabled progress in CAD program synthesis, but existing methods rely on supervised training with paired shape-program data, which is often unavailable. We introduce PLLM, a self-training framework for CAD program synthesis from unlabeled 3D shapes. Given a pre-trained CAD-capable LLM and a shape dataset, PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to construct synthetic program-shape pairs for fine-tuning. We experiment on adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in geometric fidelity and program diversity.

</details>


### [19] [The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving](https://arxiv.org/abs/2602.12563)
*Jiabao Wang,Hongyu Zhou,Yuanbo Yang,Jiahao Shao,Yiyi Liao*

Main category: cs.CV

TL;DR: 论文提出navdream基准，用高保真、像素对齐的风格迁移在几何几乎不变的前提下制造外观分布移位，发现现有自动驾驶规划在纯外观变化下也大幅退化；并提出用冻结的DINOv3提取外观不变特征作为通用感知接口，实现多种规划范式的零样本稳健泛化，无需微调。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶在OOD条件下脆弱，但研究常把“外观变化（天气、光照)”与“结构变化（道路几何）”混为一谈，难以判断失败源于视觉外观还是场景结构。需要一个能在不改变几何的情况下系统施加强外观扰动的基准，以隔离外观因素并评估/提升规划鲁棒性。

Method: 1) 构建navdream基准：利用生成式、像素对齐的风格迁移，对相同场景施加强外观变换（雨、夜间、雾等），确保几何偏差可忽略，从而形成“视觉压力测试”。2) 评测现有多种规划器在该基准下的表现，量化纯外观移位的影响。3) 提出“通用感知接口”：冻结的大型视觉基础模型DINOv3，提取外观不变特征供规划器使用，作为可插拔前端，适配回归式、扩散式、打分式等规划范式，且不做进一步微调。

Result: 实验显示：在几何保持一致但外观OOD时，主流规划算法性能显著下降。采用所提DINOv3特征作为接口后，三类规划器均实现强零样本泛化，对极端外观变化保持稳定性能，明显优于原有感知输入。

Conclusion: 外观与结构因素需要被解耦评测。navdream基准证明外观移位本身就能使规划显著退化；采用外观不变的基础模型特征作为通用接口，可在无需微调的情况下稳健提升多种规划器的OOD鲁棒性。基准与代码将公开。

Abstract: Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.

</details>


### [20] [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](https://arxiv.org/abs/2602.12590)
*Jinze Chen,Wei Zhai,Han Han,Tiankai Ma,Yang Cao,Bin Li,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出一种在保持前向输出不变的前提下，为任意事件分箱（binning）函数在反向传播中合成“弱导数”的框架，实现无偏梯度估计，从而提升事件相机任务的学习效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据常被离散分箱成帧以适配传统视觉管线，但分箱操作不连续，导致梯度在帧级被截断，使得学习依赖于帧级特征或产生有偏梯度，限制端到端训练效率与性能。

Method: 基于分部积分思想，将目标函数提升为泛函形式，在反向传播中得到分箱函数导数的积分表达；通过从采样到的余切向量重建“余切函数”，合成与长程差分一致的弱导数，从而对任意（平滑或非平滑）分箱函数提供无偏梯度，同时保持前向分箱输出不变。

Result: 在简单的基于优化的自运动估计中，实现3.2%更低的RMS误差与1.57×更快的收敛；在复杂任务上，自监督光流EPE降低9.4%，SLAM的RMS误差降低5.1%。

Conclusion: 通过在反向传播阶段合成弱导数实现对任意分箱函数的无偏梯度估计，能显著提升事件视觉在多任务中的优化效率与精度，具有广泛适用性；代码已开源（链接提供）。

Abstract: Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

</details>


### [21] [QuEPT: Quantized Elastic Precision Transformers with One-Shot Calibration for Multi-Bit Switching](https://arxiv.org/abs/2602.12609)
*Ke Xu,Yixin Wang,Zhongcheng Li,Hao Cui,Jinshui Hu,Xingyi Zhang*

Main category: cs.CV

TL;DR: QuEPT是一种针对Transformer/大语言模型的高效弹性精度后训练量化方案：一次小样本校准即可支持多比特宽度（均匀/混合精度）实时切换，并通过MB-ToMe与MB-CLoRA在切换与多比特协同中稳健提准，达到与SOTA PTQ相当或更优表现。


<details>
  <summary>Details</summary>
Motivation: 弹性量化可用一次优化适配多种比特宽度，但在Transformer/LLM上因存储与优化代价高、方法稀缺。需要一种低成本、可多场景复用、切换稳健的后训练量化技术，以减少为不同量化设定重复优化的开销并保持准确率。

Method: 提出QuEPT：在小规模数据上一次性校准，构造分块（block-wise）的多比特误差重建。核心包括：1）可级联的低秩适配器（MB-CLoRA）用于不同比特组的误差补偿与相关性增强，实现对预设多比特宽度的动态适配与统一管理；2）支持均匀量化与混合精度量化间的即时切换，无需重复优化；3）提出多比特Token融合（MB-ToMe），在不同比特宽度产出的特征间进行动态融合，提升切换过程中的鲁棒性与精度。

Result: 在多组实验中，QuEPT在LLM/Transformer任务上达到与现有SOTA后训练量化方法相当或更优的性能，同时显著降低存储与优化成本；可在多种比特配置之间实时切换并保持稳定精度。

Conclusion: 一次校准的QuEPT通过级联低秩适配与多比特特征融合，实现对多比特宽度与量化范式的弹性支持，在保证精度的同时大幅提升部署与切换效率，适合大模型的实际落地。

Abstract: Elastic precision quantization enables multi-bit deployment via a single optimization pass, fitting diverse quantization scenarios.Yet, the high storage and optimization costs associated with the Transformer architecture, research on elastic quantization remains limited, particularly for large language models.This paper proposes QuEPT, an efficient post-training scheme that reconstructs block-wise multi-bit errors with one-shot calibration on a small data slice. It can dynamically adapt to various predefined bit-widths by cascading different low-rank adapters, and supports real-time switching between uniform quantization and mixed precision quantization without repeated optimization. To enhance accuracy and robustness, we introduce Multi-Bit Token Merging (MB-ToMe) to dynamically fuse token features across different bit-widths, improving robustness during bit-width switching. Additionally, we propose Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen correlations between bit-width groups, further improve the overall performance of QuEPT. Extensive experiments demonstrate that QuEPT achieves comparable or better performance to existing state-of-the-art post-training quantization methods.Our code is available at https://github.com/xuke225/QuEPT

</details>


### [22] [Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models](https://arxiv.org/abs/2602.12618)
*Omer Faruk Deniz,Ruiyu Mao,Ruochen Li,Yapeng Tian,Latifur Khan*

Main category: cs.CV

TL;DR: 提出ADSC：利用LLM自带注意力在若干层做统一下采样，逐层压缩视觉token，兼容FlashAttention，在LLaVA-1.5上将FLOPs降53.7%、KV缓存峰值降56.7%，性能保留98.2%，高压缩下仍稳健，优于以往剪枝。


<details>
  <summary>Details</summary>
Motivation: MLLM需要在所有LLM层处理大量视觉token，计算与KV缓存开销大。现有方法要么在LLM前做特定编码/投影侧剪枝，泛化差；要么在LLM内部用启发式重要性分数并改动注意力，且与FlashAttention不兼容，实际落地受限。

Method: 将LLM视为最优压缩引导者，观察到深层逐步传递视觉到文本的信息，提出Attention-Driven Self-Compression：在选定层对视觉token做统一比例下采样，形成信息“瓶颈”，迫使模型把关键信息重组并压缩进保留token。无需额外打分、辅助模块或修改注意力，实现简单且完全兼容FlashAttention。

Result: 在LLaVA-1.5上，FLOPs减少53.7%，峰值KV-cache内存减少56.7%，总体性能保持原模型的98.2%。在多项基准上，效率与准确率均优于既有剪枝方法；当压缩率较高时，启发式方法性能急剧下降，而ADSC保持稳健。

Conclusion: 利用LLM的自注意力与层间信息传递特性进行自压缩，可在不牺牲精度的前提下显著降低MLLM推理成本，并具备实现简洁、通用性强、与高效注意力实现兼容等优势，特别在高压缩场景表现突出。

Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.

</details>


### [23] [ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models](https://arxiv.org/abs/2602.12640)
*Peijie Qiu,Hariharan Ramshankar,Arnau Ramisa,René Vidal,Amit Kumar K C,Vamsi Salaka,Rahul Bhagat*

Main category: cs.CV

TL;DR: 提出ImageRAGTurbo：通过检索增强在极少步（1–4步）扩散采样下保持高保真与对齐，并以高效微调适配器替代昂贵训练。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样迭代多、时延高；少步/一步扩散虽快但常牺牲图像质量与文本对齐，且训练成本高。需要既快又高质、训练高效的方案。

Method: 在推理时先基于文本提示从数据库检索相关图文对，将其作为条件信息；在UNet去噪器的H空间利用检索内容进行编辑与融合：1) 即便不微调，直接用检索内容编辑H空间可提升对齐；2) 进一步在H空间加入可训练适配器，通过跨注意力将检索内容与目标提示融合，实现高效微调。

Result: 在快速文本生成实验中，相比现有少步方法，在不增加时延的情况下生成更高保真、提示对齐更好的一步/少步图像；展示了无需昂贵训练即可获得显著增益。

Conclusion: 检索增强为少步扩散提供了有效上下文，配合H空间适配器与跨注意力，可在保持低延迟的同时提升图像质量与对齐，优于现有方法且训练高效。

Abstract: Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.

</details>


### [24] [Multi-Task Learning with Additive U-Net for Image Denoising and Classification](https://arxiv.org/abs/2602.12649)
*Vikram Lakkavalli,Neelam Sinha*

Main category: cs.CV

TL;DR: 本文提出在U-Net中用门控加性跳连替代拼接跳连，形成AddUNet，用于图像去噪及以去噪为中心的多任务学习；在保持特征维度恒定与模型复杂度不增的前提下，取得与基线相当的重建效果并显著提升训练稳定性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net使用拼接式跳连会扩大通道维度、增加容量与优化难度，尤其在多任务（去噪+分类）联合训练中易造成不稳定的任务干扰与信息泄露；需要一种结构性正则手段，既控制捷径通道容量、稳定编码器-解码器信息流，又不增加参数量，帮助实现更稳健、可扩展的多任务学习。

Method: 将所有跳连由拼接改为“门控加性融合”：对来自编码器与解码器的对应特征施加可学习权重并进行加法融合，保持各层通道数不变，限制捷径带宽；在单任务去噪和去噪-分类多任务设置下训练与评估，分析训练稳定性、重建/分类性能及各层跳连权重的任务分配模式。

Result: 在单任务与多任务场景下，AddUNet的重建质量与基线相当或具竞争力，但训练更稳定；在多任务中，学习到的跳连权重呈现任务感知的再分配：浅层更偏向重建，深层更支持判别；即使分类容量受限，重建仍保持稳健，显示出通过加性融合实现的隐式任务解耦。

Conclusion: 对跳连施加简单的加性与门控约束可作为有效的架构正则器，在不增加模型复杂度的情况下，稳定并扩展以去噪为中心的多任务学习；AddUNet提供了受控信息流与任务解耦的实证依据，可作为U-Net类模型的稳健替代方案。

Abstract: We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.

</details>


### [25] [CBEN -- A Multimodal Machine Learning Dataset for Cloud Robust Remote Sensing Image Understanding](https://arxiv.org/abs/2602.12652)
*Marco Stricker,Masakazu Iwamura,Koichi Kise*

Main category: cs.CV

TL;DR: 提出CloudyBigEarthNet数据集，揭示在云遮挡条件下，多模态（光学+雷达）SOTA方法显著掉分，并通过在含云数据上训练显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 实际业务（如灾害应急）无法丢弃多云影像；云去除预处理尚不成熟且会引入伪影。因此需要“云鲁棒”的多模态方法，并质疑当前数据集与评估中过滤云影像的做法会削弱方法在多云场景的适用性。

Method: 构建包含云遮挡的成对光学与雷达影像数据集CBEN，用于训练与评测；以AP为指标，评测现有光学+雷达SOTA在晴空训练、云天测试下的性能，并将这些方法适配为在含云光学数据上训练，比较前后差异。

Result: SOTA方法在晴空训练、云天测试时AP下降23–33个百分点；在含云光学数据上训练后，于云天测试集相对提升17.2–28.7个百分点。

Conclusion: 排除云影像的训练与评估导致方法在真实多云场景性能显著下降。通过引入含云数据训练并利用雷达辅助，可显著增强云鲁棒性。CBEN与代码公开，期望推动在云遮挡条件下的多模态遥感研究。

Abstract: Clouds are a common phenomenon that distorts optical satellite imagery, which poses a challenge for remote sensing. However, in the literature cloudless analysis is often performed where cloudy images are excluded from machine learning datasets and methods. Such an approach cannot be applied to time sensitive applications, e.g., during natural disasters. A possible solution is to apply cloud removal as a preprocessing step to ensure that cloudfree solutions are not failing under such conditions. But cloud removal methods are still actively researched and suffer from drawbacks, such as generated visual artifacts. Therefore, it is desirable to develop cloud robust methods that are less affected by cloudy weather. Cloud robust methods can be achieved by combining optical data with radar, a modality unaffected by clouds. While many datasets for machine learning combine optical and radar data, most researchers exclude cloudy images. We identify this exclusion from machine learning training and evaluation as a limitation that reduces applicability to cloudy scenarios. To investigate this, we assembled a dataset, named CloudyBigEarthNet (CBEN), of paired optical and radar images with cloud occlusion for training and evaluation. Using average precision (AP) as the evaluation metric, we show that state-of-the-art methods trained on combined clear-sky optical and radar imagery suffer performance drops of 23-33 percentage points when evaluated on cloudy images. We then adapt these methods to cloudy optical data during training, achieving relative improvement of 17.2-28.7 percentage points on cloudy test cases compared with the original approaches. Code and dataset are publicly available at: https://github.com/mstricker13/CBEN

</details>


### [26] [IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models](https://arxiv.org/abs/2602.12659)
*Aarish Shah Mohsin,Mohammed Tayyab Ilyas Khan,Mohammad Nadeem,Shahab Saquib Sohail,Erik Cambria,Jiechao Gao*

Main category: cs.CV

TL;DR: 提出IndicFairFace，一个均衡覆盖印度各邦/联邦属地与性别的14,400张人脸数据集，用于度量并缓解VLM在印度语境下的地理偏见；用CLIP系VLM实证偏见并用迭代零空间投影去偏，在常规检索准确率上平均损失<1.5%。


<details>
  <summary>Details</summary>
Motivation: 现有公平数据集虽改进了全球层面的种族与性别平衡，但将“Indian”视为单一类别，忽视印度28个邦与8个联邦属地的巨大内部多样性，导致表征与地理偏见；需要一个细粒度、伦理来源且均衡的基准来识别与纠正此类偏差。

Method: 构建IndicFairFace：从Wikimedia等开源许可库伦理采集人脸，按邦/属地与性别均衡抽样（总计14,400张）。用该数据集对主流CLIP系VLM做地理偏见量化；随后采用后处理的Iterative Nullspace Projection（INLP）对嵌入进行去偏，并评估对下游检索性能的影响。

Result: 基于IndicFairFace发现主流CLIP系VLM存在显著的印度域内地理偏见；应用INLP后，偏见指标下降，而在多基准上的检索准确率平均下降不足1.5%，显示去偏带来的效能损失有限。

Conclusion: IndicFairFace成为首个用于印度语境下地理偏见研究的VLM基准；结合INLP可在基本保持性能的前提下显著缓解地理偏见，为更细粒度的公平评测与去偏提供路径。

Abstract: Vision-Language Models (VLMs) are known to inherit and amplify societal biases from their web-scale training data with Indian being particularly misrepresented. Existing fairness-aware datasets have significantly improved demographic balance across global race and gender groups, yet they continue to treat Indian as a single monolithic category. The oversimplification ignores the vast intra-national diversity across 28 states and 8 Union Territories of India and leads to representational and geographical bias. To address the limitation, we present IndicFairFace, a novel and balanced face dataset comprising 14,400 images representing geographical diversity of India. Images were sourced ethically from Wikimedia Commons and open-license web repositories and uniformly balanced across states and gender. Using IndicFairFace, we quantify intra-national geographical bias in prominent CLIP-based VLMs and reduce it using post-hoc Iterative Nullspace Projection debiasing approach. We also show that the adopted debiasing approach does not adversely impact the existing embedding space as the average drop in retrieval accuracy on benchmark datasets is less than 1.5 percent. Our work establishes IndicFairFace as the first benchmark to study geographical bias in VLMs for the Indian context.

</details>


### [27] [Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening](https://arxiv.org/abs/2602.12679)
*Wooseok Jeon,Seunghyun Shin,Dongmin Shin,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: 提出一种用于图像到视频生成中的推理阶段蒸馏方法MPD，通过将正向路径的运动残差信息蒸馏给反向路径，缓解双向生成路径不对齐导致的时间不连续和伪影，提升中间帧的时序一致性与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有无训练的推理时采样策略（并行融合或交替双向）常因前后路径各自受其条件帧诱导的运动先验不同而错配，造成时序断裂与伪影。需要一种在不额外训练的前提下，对齐双向路径的运动先验、减少不确定性的办法。

Method: 提出Motion Prior Distillation：在推理阶段从前向路径提取“运动残差”（代表前向条件帧诱导的运动先验偏移），并将其蒸馏/注入到后向路径，使后向去噪过程受前向运动先验约束；同时避免对终端条件路径的去噪以减少路径歧义，从而抑制双向不匹配。

Result: 相较现有并行融合或交替采样的方法，MPD在标准基准上实现更高的时序一致性与更少视觉伪影；用户研究也表明在实际场景中主观质量更优。

Conclusion: MPD是一种简单有效、无需再训练的推理期蒸馏策略，通过传递前向运动先验来对齐双向路径，显著提升I2V中间帧生成的时序连贯性与观感，可作为现有大规模预训练I2V模型的通用增强模块。

Abstract: Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.

</details>


### [28] [Channel-Aware Probing for Multi-Channel Imaging](https://arxiv.org/abs/2602.12696)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: 提出Channel-Aware Probing (CAP)以在多通道成像(MCI)上更好地对冻结的视觉编码器进行探测，核心是对每个通道独立编码并先在通道内池化再跨通道聚合；在三项基准上显著优于默认探测，接近从头微调，并缩小与全量微调的差距。


<details>
  <summary>Details</summary>
Motivation: MCI数据集通道配置不一致，导致固定通道训练与跨数据集复用困难；现有工作多以全量微调评估，冻结编码器的探测策略研究不足；直接移植他域探测策略到MCI效果差，甚至不如从头训练，需设计更适配MCI通道多样性的探测方法。

Method: 提出Channel-Aware Probing (CAP)：(1) Independent Feature Encoding（IFE）：对每个通道单独通过同一编码器（权重共享或独立设定）提取特征，避免通道间相互干扰并适配任意通道组合；(2) Decoupled Pooling（DCP）：先在通道内进行池化以稳定各通道表征，再在通道间进行聚合，使跨通道信息融合受控；整体在冻结预训练编码器下仅训练探测头。

Result: 在三个MCI基准上，CAP相对默认探测协议稳定提升性能；与从头微调的表现相当；相较于在相同MCI预训练权重上进行全量微调，性能差距显著缩小。

Conclusion: 利用通道感知的特征流控制能更有效地挖掘MCI中通道间多样性，CAP为冻结表示的下游利用提供了通用且强大的方案，提升可迁移性并减少对代价高昂的全量微调依赖。

Abstract: Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.

</details>


### [29] [ART3mis: Ray-Based Textual Annotation on 3D Cultural Objects](https://arxiv.org/abs/2602.12725)
*Vasileios Arampatzakis,Vasileios Sevetlidis,Fotis Arnaoutoglou,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,Chairi Kiourt,George Ioannakis,Anestis Koutsoudis,George Pavlidis*

Main category: cs.CV

TL;DR: 提出ART3mis：面向文化遗产领域的通用、交互式3D对象文本标注工具，支持直接在三维表面上对复杂区域进行实时分割与注释，并以JSON保存多区域元数据。


<details>
  <summary>Details</summary>
Motivation: 现有3D可视化多聚焦于展示，缺少对局部区域进行高效标注与元数据附着的通用工具；已有方法多为特定领域/问题定制，难以迁移。文化遗产从业者需要无需3D技术背景即可完成对文物数字副本的精细注释与管理。

Method: 设计并实现ART3mis：用户驱动、直接在三维表面上交互，支持对高细节3D对象的实时处理；提供易用的分割与多区域文本标注功能；将标注以JSON格式存储，便于互操作与集成。

Result: ART3mis能够在实时条件下处理复杂文化遗产3D模型，支持对多个复杂区域进行文本标注与管理，实现跨对象与会话的可保存与交换（JSON）。

Conclusion: ART3mis作为通用且用户友好的3D对象标注工具，填补了文化遗产领域从展示到可编辑注释的空白，降低了非技术用户对三维标注与元数据管理的门槛。

Abstract: Beyond simplistic 3D visualisations, archaeologists, as well as cultural heritage experts and practitioners, need applications with advanced functionalities. Such as the annotation and attachment of metadata onto particular regions of the 3D digital objects. Various approaches have been presented to tackle this challenge, most of which achieve excellent results in the domain of their application. However, they are often confined to that specific domain and particular problem. In this paper, we present ART3mis - a general-purpose, user-friendly, interactive textual annotation tool for 3D objects. Primarily attuned to aid cultural heritage conservators, restorers and curators with no technical skills in 3D imaging and graphics, the tool allows for the easy handling, segmenting and annotating of 3D digital replicas of artefacts. ART3mis applies a user-driven, direct-on-surface approach. It can handle detailed 3D cultural objects in real-time and store textual annotations for multiple complex regions in JSON data format.

</details>


### [30] [VimRAG: Navigating Massive Visual Context in Retrieval-Augmented Generation via Multimodal Memory Graph](https://arxiv.org/abs/2602.12735)
*Qiuchen Wang,Shihang Wang,Yu Zeng,Qiang Zhang,Fanrui Zhang,Zhuoning Guo,Bosi Zhang,Wenxuan Huang,Lin Chen,Zehui Chen,Pengjun Xie,Ruixue Ding*

Main category: cs.CV

TL;DR: VimRAG提出以有向无环图(DAG)组织多模态检索与推理记忆，动态分配高分辨率视觉token，并通过图引导的策略优化实现更细粒度的信用分配，从而在多模态RAG基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RAG依赖线性对话/交互历史，难以处理长上下文与迭代推理，尤其面对信息稀疏但token庞大的图像/视频数据；需要一种能结构化记忆、强调关键信息并高效训练的多模态推理框架。

Method: 1) 将推理过程建模为动态DAG，节点表示agent状态与检索到的多模态证据，形成结构化记忆；2) 提出图调制的视觉记忆编码(GMVME)，依据节点在图中的拓扑重要性，给关键证据分配高分辨率token，对次要线索进行压缩或丢弃；3) 提出图引导的策略优化( GGPO )，通过剪枝与冗余动作关联的记忆节点，将逐步有效性与轨迹级奖励解耦，提升信用分配与训练稳定性。

Result: 在多样的多模态RAG基准上取得一致的SOTA表现，验证所提DAG记忆结构、GMVME与GGPO的有效性。代码开源于https://github.com/Alibaba-NLP/VRAG。

Conclusion: 以DAG为核心的结构化记忆结合图调制视觉编码与图引导策略优化，解决了多模态长上下文与迭代推理中的冗余与信用分配难题，显著提升多模态RAG的效率与效果。

Abstract: Effectively retrieving, reasoning, and understanding multimodal information remains a critical challenge for agentic systems. Traditional Retrieval-augmented Generation (RAG) methods rely on linear interaction histories, which struggle to handle long-context tasks, especially those involving information-sparse yet token-heavy visual data in iterative reasoning scenarios. To bridge this gap, we introduce VimRAG, a framework tailored for multimodal Retrieval-augmented Reasoning across text, images, and videos. Inspired by our systematic study, we model the reasoning process as a dynamic directed acyclic graph that structures the agent states and retrieved multimodal evidence. Building upon this structured memory, we introduce a Graph-Modulated Visual Memory Encoding mechanism, with which the significance of memory nodes is evaluated via their topological position, allowing the model to dynamically allocate high-resolution tokens to pivotal evidence while compressing or discarding trivial clues. To implement this paradigm, we propose a Graph-Guided Policy Optimization strategy. This strategy disentangles step-wise validity from trajectory-level rewards by pruning memory nodes associated with redundant actions, thereby facilitating fine-grained credit assignment. Extensive experiments demonstrate that VimRAG consistently achieves state-of-the-art performance on diverse multimodal RAG benchmarks. The code is available at https://github.com/Alibaba-NLP/VRAG.

</details>


### [31] [SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences](https://arxiv.org/abs/2602.12740)
*Ruipeng Wang,Langkun Zhong,Miaowei Wang*

Main category: cs.CV

TL;DR: SPRig提出在无T-pose的序列网格上，通过跨帧一致性损失微调现有绑定模型，学习对姿态不敏感的rig，并在新提出的置换不变稳定性协议下验证，显著提升时序稳定性并减少伪影。


<details>
  <summary>Details</summary>
Motivation: 现有绑定依赖规范静止姿态（如T-pose），面对来自动物动作捕捉或AIGC/视频重建的序列网格时无法保证跨帧拓扑与语义一致，逐帧处理产生不稳定与伪影，缺乏统一评测标准。

Method: 提出SPRig：作为通用微调框架，在现有rigging模型之上加入跨帧一致性损失，学习姿态不变的绑定；并设计一个对顶点/骨骼排列置换不敏感的稳定性评测协议用于客观比较。

Result: 在多种具有挑战性的序列上获得SOTA时序稳定性，能从缺乏T-pose的序列中产出连贯一致的rig，显著减少传统方法的拓扑不一致与视觉伪影。

Conclusion: 通过跨帧一致性约束微调现有模型，可在无规范姿态条件下实现稳健、姿态不变的rigging；新评测协议为稳定性提供公平基准，代码将公开。

Abstract: State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.

</details>


### [32] [Synthetic Craquelure Generation for Unsupervised Painting Restoration](https://arxiv.org/abs/2602.12742)
*Jana Cuch-Guillén,Antonio Agudo,Raül Pérez-Gonzalo*

Main category: cs.CV

TL;DR: 提出一个无需人工标注的绘画裂纹（裂隙）检测与修复管线：用合成裂纹生成器训练，以形态学检测器+LoRA 适配的 SegFormer 精修裂纹掩码，并用各向异性扩散修复；零样本下优于现有照片修复模型且保留笔触。


<details>
  <summary>Details</summary>
Motivation: 文化遗产修复需要非侵入式数字方法。真实油画裂纹形态复杂、与笔触干扰重叠，且缺少像素级标注，导致监督学习难以落地。需要一种不依赖人工标注、又能准确识别细小分支与渐细裂隙的方案，同时在修复时最大限度保留原始笔触纹理。

Method: 1) 合成数据：构建领域特定裂纹生成器，利用 Bézier 轨迹生成具分支、锥形收束的真实感裂隙；渲染到画作域以产生训练对。2) 检测-精修两阶段：先用经典形态学检测器得到候选裂纹图；再用学习模块（SegFormer 主干 + LoRA 低秩适配）进行掩码细化。3) 检测器引导：将形态学候选图作为空间先验输入网络。4) 训练约束：采用“掩码化混合损失”和 logit 调整，聚焦于候选裂纹区域、缓解类别不平衡与过拟合噪声。5) 修复：用精修掩码引导各向异性扩散（Anisotropic Diffusion）进行图像修补与纹理连续性重建。

Result: 在零样本（无目标实画标注与微调）的评估下，管线在裂纹检测与修复质量上显著优于最先进的照片修复/去裂纹模型，同时更好地保持原作笔触与纹理连续性。

Conclusion: 基于合成裂纹先验与检测器引导的无标注框架，实现高保真裂纹识别与修复，减少人工标注依赖并提升零样本泛化；各向异性扩散在精确掩码引导下能更好地保留画作笔触。方法可推广到其他缺陷检测与文化遗产数字修复场景。

Abstract: Cultural heritage preservation increasingly demands non-invasive digital methods for painting restoration, yet identifying and restoring fine craquelure patterns from complex brushstrokes remains challenging due to scarce pixel-level annotations. We propose a fully annotation-free framework driven by a domain-specific synthetic craquelure generator, which simulates realistic branching and tapered fissure geometry using Bézier trajectories. Our approach couples a classical morphological detector with a learning-based refinement module: a SegFormer backbone adapted via Low-Rank Adaptation (LoRA). Uniquely, we employ a detector-guided strategy, injecting the morphological map as an input spatial prior, while a masked hybrid loss and logit adjustment constrain the training to focus specifically on refining candidate crack regions. The refined masks subsequently guide an Anisotropic Diffusion inpainting stage to reconstruct missing content. Experimental results demonstrate that our pipeline significantly outperforms state-of-the-art photographic restoration models in zero-shot settings, while faithfully preserving the original paint brushwork.

</details>


### [33] [ReBA-Pred-Net: Weakly-Supervised Regional Brain Age Prediction on MRI](https://arxiv.org/abs/2602.12751)
*Shuai Shao,Yan Wang,Shu Jiang,Shiyuan Zhao,Xinzhe Luo,Di Yang,Jiangtao Wang,Yutong Bai,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出ReBA-Pred-Net，一种教师-学生框架，实现细粒度区域脑龄预测；引入两项间接评估指标（HCS与NDC）验证统计与事实一致性；在多种骨干上验证有效。


<details>
  <summary>Details</summary>
Motivation: 全脑龄（WBA）过于粗糙，难以刻画疾病表征及发育/老化的区域选择性变化，缺乏鲁棒、可泛化的区域脑龄（ReBA）模型。

Method: 设计Regional Brain Age Prediction Network（ReBA-Pred-Net）：教师模型产生软ReBA，引导学生模型学习；加入临床先验一致性约束（同一功能域内区域变化应相似）。提出两项间接评估指标：1）Healthy Control Similarity（HCS）评估训练集与未见健康对照在ReBA-GAP分布的一致性；2）Neuro Disease Correlation（NDC）检验临床确诊患者在疾病相关区域的GAP是否升高。跨多种网络骨干进行实验。

Result: 所提方法在多种骨干上均显示较好的统计一致性（高HCS）与事实一致性（显著NDC），证明预测的区域脑龄可信。

Conclusion: ReBA-Pred-Net可生成鲁棒、可泛化的区域脑龄估计；HCS与NDC为评估区域脑龄有效性的有力间接指标，支持疾病研究与发育/老化模式解析。

Abstract: Brain age has become a prominent biomarker of brain health. Yet most prior work targets whole brain age (WBA), a coarse paradigm that struggles to support tasks such as disease characterization and research on development and aging patterns, because relevant changes are typically region-selective rather than brain-wide. Therefore, robust regional brain age (ReBA) estimation is critical, yet a widely generalizable model has yet to be established. In this paper, we propose the Regional Brain Age Prediction Network (ReBA-Pred-Net), a Teacher-Student framework designed for fine-grained brain age estimation. The Teacher produces soft ReBA to guide the Student to yield reliable ReBA estimates with a clinical-prior consistency constraint (regions within the same function should change similarly). For rigorous evaluation, we introduce two indirect metrics: Healthy Control Similarity (HCS), which assesses statistical consistency by testing whether regional brain-age-gap (ReBA minus chronological age) distributions align between training and unseen HC; and Neuro Disease Correlation (NDC), which assesses factual consistency by checking whether clinically confirmed patients show elevated brain-age-gap in disease-associated regions. Experiments across multiple backbones demonstrate the statistical and factual validity of our method.

</details>


### [34] [Towards reconstructing experimental sparse-view X-ray CT data with diffusion models](https://arxiv.org/abs/2602.12755)
*Nelas J. Thomsen,Xinyuan Wang,Felix Lucka,Ezgi Demircan-Tureyen*

Main category: cs.CV

TL;DR: 研究评估扩散模型作为稀视角CT逆问题先验在真实实验数据上的有效性，发现域偏移与前向模型失配对结果有显著且不同影响，并提出分解扩散采样与退火似然策略以缓解伪影并提升效率。


<details>
  <summary>Details</summary>
Motivation: 多数工作只在合成数据上验证，忽略了训练数据分布与真实目标的偏差（域偏移）以及测量物理模型与假设不一致（前向模型失配）。作者希望系统性量化这两类失配如何影响扩散先验在稀视角CT中的实际表现。

Method: 1) 采集一个物理Shepp-Logan样机的真实CT数据；2) 在多种合成数据集上训练扩散先验，控制与实验对象的域偏移程度与多样性；3) 在不同难度的稀视角CT重建任务上，采用Decomposed Diffusion Sampling进行条件采样；4) 研究域偏移与前向模型失配的效应，并引入退火式似然调度以缓解失配。

Result: - 严重域偏移会导致模型崩溃与幻觉，但“多样而广覆盖”的先验优于“匹配但狭窄”的先验；- 前向模型失配会把样本拉离先验流形，产生伪影；- 退火似然调度可减轻伪影并提升计算效率；- 从合成到实验的性能不可直接迁移。

Conclusion: 扩散先验在真实稀视角CT中有效性受域偏移与前向模型失配显著影响。为获得稳健性能，应优先使用多样化训练先验并结合退火似然调度，同时必须在真实基准上验证，避免仅凭合成数据得出结论。

Abstract: Diffusion-based image generators are promising priors for ill-posed inverse problems like sparse-view X-ray Computed Tomography (CT). As most studies consider synthetic data, it is not clear whether training data mismatch (``domain shift'') or forward model mismatch complicate their successful application to experimental data. We measured CT data from a physical phantom resembling the synthetic Shepp-Logan phantom and trained diffusion priors on synthetic image data sets with different degrees of domain shift towards it. Then, we employed the priors in a Decomposed Diffusion Sampling scheme on sparse-view CT data sets with increasing difficulty leading to the experimental data. Our results reveal that domain shift plays a nuanced role: while severe mismatch causes model collapse and hallucinations, diverse priors outperform well-matched but narrow priors. Forward model mismatch pulls the image samples away from the prior manifold, which causes artifacts but can be mitigated with annealed likelihood schedules that also increase computational efficiency. Overall, we demonstrate that performance gains do not immediately translate from synthetic to experimental data, and future development must validate against real-world benchmarks.

</details>


### [35] [Towards complete digital twins in cultural heritage with ART3mis 3D artifacts annotator](https://arxiv.org/abs/2602.12761)
*Dimitrios Karamatskos,Vasileios Arampatzakis,Vasileios Sevetlidis,Stavros Nousias,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,George Pavlidis*

Main category: cs.CV

TL;DR: 提出ART3mis：基于Web的3D对象文本标注工具，支持对3D文物局部区域进行分割、标注与元数据附加，并遵循W3C Web Annotation模型以便共享与复用。


<details>
  <summary>Details</summary>
Motivation: 现有3D可视化工具多侧重展示，缺乏对三维对象特定区域进行细粒度标注与元数据管理的通用、可互操作能力；领域内解决方案各擅其长但难以泛化和互通，难以满足考古与文化遗产工作者的需求。

Method: 设计并实现ART3mis为交互式、功能丰富、易用的Web端3D文本标注系统，支持3D对象的处理、区域分割、局部文本注释与元数据附加，并严格遵循W3C Web Annotation Data Model以实现跨平台通信、分发与重用。

Result: ART3mis实现了对3D文物的细粒度交互标注与元数据附加，在无需深厚3D技术背景的前提下，帮助文保修复与策展人员完成分割与注释并在标准模型下共享信息。

Conclusion: ART3mis填补了3D文化遗产标注工具在通用性与互操作性上的缺口，为非技术用户提供可用、可共享、可复用的三维标注工作流。

Abstract: Archaeologists, as well as specialists and practitioners in cultural heritage, require applications with additional functions, such as the annotation and attachment of metadata to specific regions of the 3D digital artifacts, to go beyond the simplistic three-dimensional (3D) visualization. Different strategies addressed this issue, most of which are excellent in their particular area of application, but their capacity is limited to their design's purpose; they lack generalization and interoperability. This paper introduces ART3mis, a general-purpose, user-friendly, feature-rich, interactive web-based textual annotation tool for 3D objects. Moreover, it enables the communication, distribution, and reuse of information as it complies with the W3C Web Annotation Data Model. It is primarily designed to help cultural heritage conservators, restorers, and curators who lack technical expertise in 3D imaging and graphics, handle, segment, and annotate 3D digital replicas of artifacts with ease.

</details>


### [36] [PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion](https://arxiv.org/abs/2602.12769)
*Hong-Phuc Lai,Phong Nguyen,Anh Tran*

Main category: cs.CV

TL;DR: PixelRush是一种无需微调的高分辨率文本生成图像框架，基于补丁推理，在少步去噪下实现高效4K生成；通过无缝融合与噪声注入缓解拼接伪影与过度平滑，约20秒生成4K图，较现有方法提速10×–35×且保持高保真。


<details>
  <summary>Details</summary>
Motivation: 现有预训练扩散模型受原生训练分辨率限制，直接生成超高分辨率图像质量或效率受限；训练自由（training-free）的提升方法多在去噪过程中多次反演与重生成，计算昂贵（单张4K需数分钟）。需要一种既不再训练、又能在实际时间内高质量生成4K图像的方法。

Method: 在既有的基于图像补丁的推理范式上，取消多轮反演/重生成，改为在少步（low-step）去噪框架中对补丁进行高效去噪；为解决少步拼接导致的边界伪影，引入无缝融合策略；为缓解少步生成带来的过度平滑，引入噪声注入机制。整体为调参/调度式、无需额外训练的推理流程。

Result: 在保持或提升视觉保真度的同时，将4K图像生成时间降至约20秒，相比SOTA训练自由方法实现10×–35×加速。通过大量实验验证了效率与质量的同时提升。

Conclusion: PixelRush在不进行额外训练的前提下，实现了实用级高分辨率文本生成图像：基于补丁的少步去噪、无缝融合与噪声注入共同带来显著的速度提升与高质量输出，表明训练自由的高分辨率生成可以兼顾效率与保真。

Abstract: Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.

</details>


### [37] [Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting](https://arxiv.org/abs/2602.12774)
*Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Qijun Chen,Miaojing Shi*

Main category: cs.CV

TL;DR: 提出WS-COC：首个由多模态大模型驱动、仅用图像级计数监督的类别无关目标计数框架，通过对话分段判断范围、跨图排序优化、全局-局部融合三策略，在多数据集上达SOTA水平并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 全监督计数依赖逐目标点级标注，代价高；现有弱监督多只支持单类（如行人），难以类无关泛化。MLLM具备跨模态理解潜力，但直接微调做精确计数受模态鸿沟与离散回归困难制约，亟需设计能“引导式计数”的训练与推理机制，在低成本监督下实现多场景高精度计数。

Method: WS-COC弱监督框架，核心三策略：1) Divide-and-Discern 对话调优：将计数回归转为区间判定，逐轮对话细分范围直至收敛；2) Compare-and-Rank 排序优化：利用成对/多图比较学习，使模型学会按目标数量进行相对排序，缓解绝对数值学习难点；3) Global-and-Local 融合：结合全局图像级预测与局部区域（密集块）计数并聚合，以提升密集场景鲁棒性。训练与测试阶段均应用这些机制，无需点级标注，仅用图像级计数。

Result: 在FSC-147、CARPK、PUCPR+、ShanghaiTech等数据集上，弱监督下达到或超过多种全监督SOTA的计数精度，展示良好泛化与鲁棒性，同时显著降低标注成本。

Conclusion: 将计数任务转化为对话式区间决策、相对排序学习与全局-局部融合，可有效激发MLLM在类别无关计数上的潜力。WS-COC在多数据集上验证了弱监督可媲美甚至超越全监督，为低成本大规模计数应用提供了可行路径。

Abstract: Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.

</details>


### [38] [GSM-GS: Geometry-Constrained Single and Multi-view Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2602.12796)
*Xiao Ren,Yu Liu,Ning An,Jian Cheng,Xin Qiao,He Kong*

Main category: cs.CV

TL;DR: 提出GSM-GS，通过单视角自适应子区域加权与多视角空间结构细化协同优化，缓解3D Gaussian Splatting在复杂细节重建中的高频丢失，提升几何与渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting训练快、渲染好，但点云无结构与不规则性让常规策略难以精确恢复复杂表面微结构，导致高频细节丢失与多视不一致，需要新的约束与优化机制。

Method: 两路协同优化框架：1) 单视角：利用图像梯度将场景划分为纹理丰富/贫乏子区域；依据深度差异引导的自适应滤波与加权，保留高权重区域；针对不同区域设计双分支约束以强化几何细节。2) 多视角：提出几何引导的跨视点云关联与动态权重采样，在相邻点云帧间构建3D结构法向约束，加强跨视一致性与重建保真。

Result: 在公开数据集上进行大量实验，方法在渲染质量与几何重建上均取得有竞争力表现，表明对高频细节与多视一致性有显著提升。

Conclusion: GSM-GS通过单/多视协同与几何约束有效缓解GS在复杂细节处的不足，实现更高的几何与渲染精度；该策略对基于GS的重建与渲染具有普适的改进潜力。

Abstract: Recently, 3D Gaussian Splatting has emerged as a prominent research direction owing to its ultrarapid training speed and high-fidelity rendering capabilities. However, the unstructured and irregular nature of Gaussian point clouds poses challenges to reconstruction accuracy. This limitation frequently causes high-frequency detail loss in complex surface microstructures when relying solely on routine strategies. To address this limitation, we propose GSM-GS: a synergistic optimization framework integrating single-view adaptive sub-region weighting constraints and multi-view spatial structure refinement. For single-view optimization, we leverage image gradient features to partition scenes into texture-rich and texture-less sub-regions. The reconstruction quality is enhanced through adaptive filtering mechanisms guided by depth discrepancy features. This preserves high-weight regions while implementing a dual-branch constraint strategy tailored to regional texture variations, thereby improving geometric detail characterization. For multi-view optimization, we introduce a geometry-guided cross-view point cloud association method combined with a dynamic weight sampling strategy. This constructs 3D structural normal constraints across adjacent point cloud frames, effectively reinforcing multi-view consistency and reconstruction fidelity. Extensive experiments on public datasets demonstrate that our method achieves both competitive rendering quality and geometric reconstruction. See our interactive project page

</details>


### [39] [Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation](https://arxiv.org/abs/2602.12843)
*Yichen Zhao,Zelin Peng,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: 提出MMRad-IVL-22K，一个面向胸部X光“原生交替视觉-语言推理”的大型数据集，使LVLM在报告生成中可在多轮视觉检视与语言推理间往返，从而显著提升临床准确性与报告质量。


<details>
  <summary>Details</summary>
Motivation: 放射学诊断本质是视觉细察与语言推理的交替过程，而现有医疗LVLM多只做一次看图，之后在纯文本CoT中推理，易幻觉。以边框坐标等“伪视觉”方式仍无法保留纹理、密度等细节，无法真实支撑逐步推理。

Method: 构建MMRad-IVL-22K数据集，含21,994条胸片诊断轨迹，覆盖35个解剖区域；在每一步语言推理旁引入与之配套的视觉证据/视觉理据，实现多轮“看-想-再看”。在闭源与开源LVLM上进行基准评测与微调，对比多模态CoT与文本CoT。

Result: 在先进闭源LVLM上，多模态CoT引导的报告生成在临床准确性和报告质量上优于文本CoT（如RadGraph指标提升约6%）。在7个SOTA开源LVLM上，使用MMRad-IVL-22K微调可带来更一致的推理与更高质量的报告，优于通用及医学专用LVLM。

Conclusion: 高保真、原生交替的视觉-语言证据是构建可靠医疗AI的不可替代要素。MMRad-IVL-22K为胸片交替推理提供了规模化数据支撑，并显著提升LVLM的临床报告生成与一致性。

Abstract: Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.

</details>


### [40] [RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads](https://arxiv.org/abs/2602.12877)
*Vijayasri Iyer,Maahin Rathinagiriswaran,Jyothikamalesh S*

Main category: cs.CV

TL;DR: Roadscapes 提出一个面向自动驾驶场景理解的多任务多模态数据集（约9k印度道路图像），含人工验证框和基于规则的自动生成QA对，并给出基线模型表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在“非结构化”道路环境中需更强的场景理解能力，但现有数据多聚焦于结构化城市道路、单一任务或缺少问答与推理标注；因此需要一个覆盖印度多样路况、支持视觉问答与推理的可扩展数据集。

Method: - 数据采集：在印度多样环境（城市/乡村、高速/辅路/村道、拥堵街道，昼/夜）采集约9,000张图像。
- 标注：提供人工校验的目标边界框。
- 可扩展属性推断：设计基于规则的启发式从元数据/检测结果中推断场景属性。
- QA生成：利用推断属性自动生成多种任务的问答对（对象定位、推理、场景理解）。
- 统计与基线：报告数据统计，并用视觉-语言模型进行图像QA基线评测。

Result: 得到涵盖多场景与光照条件的Roadscapes数据集及其自动生成的多任务QA标注；提供初步基线结果，显示数据可用于评估与推动视觉-语言模型在道路场景理解上的能力。

Conclusion: Roadscapes为非结构化道路环境的多模态场景理解研究提供了新资源，通过人工框与规则生成的QA实现可扩展标注，并在图像问答任务上建立了初步基线，期望促进自动驾驶视觉理解研究。

Abstract: Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.

</details>


### [41] [RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training](https://arxiv.org/abs/2602.12892)
*Yunshuang Nie,Bingqian Lin,Minzhe Niu,Kun Xiang,Jianhua Han,Guowei Huang,Xingyue Quan,Hang Xu,Bokui Chen,Xiaodan Liang*

Main category: cs.CV

TL;DR: RADAR提出一个高效、能力中心的评估框架，用于在不微调的前提下诊断多模态大模型在预训练阶段的感知与推理能力发展，并揭示两者的不对称性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估要么依赖微调与自回归解码，成本高、难以快速诊断；要么使用与预训练目标不匹配或规模有限的基准，且无法解耦量化“感知”和“推理”两类能力的发展。需要一个零样本、与预训练更一致、可持续跟踪能力变化的评估方法。

Method: 提出RADAR框架，含两部分：1) Soft Discrimination Score（SDS）：通过比较模型对正确答案相对干扰项的偏好强度，细粒度、鲁棒地跟踪能力演化，无需微调；2) Multi-Modal Mixture Benchmark（MMMB）：构建15K+样本的大规模混合基准，统一权威数据集并补充新数据，覆盖广泛感知与推理子任务，支持零样本评估。

Result: 用RADAR系统评测不同数据量、模型规模与预训练策略下的MLLM，发现感知与推理能力在预训练中的发展不对称，且该不对称性随多种因素显著变化。SDS能稳定反映能力差异，MMMB能全面覆盖能力维度并弥补现有基准空缺。

Conclusion: 应以解耦视角看待MLLM预训练的能力瓶颈；RADAR为定位与量化这些瓶颈提供高效工具，可指导更有针对性的干预与资源分配，从而更高效地提升MLLM的感知与推理能力。

Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.

</details>


### [42] [Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions](https://arxiv.org/abs/2602.12902)
*Fox Pettersen,Hong Zhu*

Main category: cs.CV

TL;DR: 论文提出用合成恶劣天气/光照增强逐级加剧来测量自动驾驶目标检测模型的失效阈值，并以平均首次失效系数（AFFC）量化鲁棒性；Faster R-CNN最鲁棒，YOLO变体较弱；针对性合成数据训练能提升但存在收益递减与遗忘。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶走向规模化落地，需要明确在多样环境下的安全运行边界。传统基准多在常规条件下评测，难以量化模型在雾、雨、雪与极端光照等不利条件下的抗扰动能力与失效点，因此亟需一种可控、可比较、成本低的鲁棒性评估方法。

Method: 使用七类数据增强算子（雾、雨、雪、暗、亮、眩光、阴影）生成由弱到强的合成劣化序列；对每张基准图像，沿强度轴逐步增加，记录目标检测模型性能首次跌破阈值（或失败）的强度系数；对全部图像取平均得到AFFC作为鲁棒性指标。对四个检测器（YOLOv5s、YOLOv11s、Faster R-CNN、Detectron2）进行评测，并扩展研究在合成逆境数据上进行针对性训练对鲁棒性的影响。

Result: 方法在效率与可行性上得到验证。综合七类不利条件，Faster R-CNN的平均AFFC为71.9%，居首；YOLO变体约43%，较弱。该评估可用于横向比较不同模型在特定不利条件下的鲁棒性强弱谱系。

Conclusion: 基于可控强度的合成不利条件能有效量化并比较目标检测模型的鲁棒阈值。Faster R-CNN整体更稳健；针对性逆境训练可提升鲁棒性，但存在边际收益递减与过度训练导致的遗忘（鲁棒性回落）。建议在开发中引入此评估以设定安全运行阈值，并谨慎设计逆境训练与早停/正则化以缓解遗忘。

Abstract: As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.

</details>


### [43] [Adaptive Scaling with Geometric and Visual Continuity of completed 3D objects](https://arxiv.org/abs/2602.12905)
*Jelle Vermandere,Maarten Bassier,Maarten Vergauwen*

Main category: cs.CV

TL;DR: 提出一种“部件感知缩放”框架，将补全得到的静态SDF转化为可编辑、结构一致的对象，通过自动分割、可控缩放区与平滑插值（含颜色与部件索引）以及重复结构保持策略，实现无伪影、按比例的局部/大尺度形变；在Matterport3D与ShapeNet上优于全局或朴素选择性缩放。


<details>
  <summary>Details</summary>
Motivation: 现有物体补全网络输出的SDF虽能重建几何，但本质静态，难以在不破坏结构的情况下缩放或变形，限制了室内重设计、仿真、内容创作等需要灵活编辑的应用。

Method: 从最先进的补全模型生成的SDF与Texture Field出发：（1）自动部件分割；（2）定义用户可控的缩放区域；（3）对SDF、颜色及部件索引进行平滑插值以实现按比例、无伪影变形；（4）引入基于重复的策略，在大尺度变形时保留重复几何模式。

Result: 在Matterport3D与ShapeNet对象上，方法打破静态SDF的刚性约束，视觉效果优于全局缩放和朴素的选择性缩放，尤其在复杂形状与重复结构上表现更好。

Conclusion: 部件感知缩放能将静态、已补全的SDF变为可编辑对象，兼顾结构一致性与视觉质量，并支持大尺度变形时的重复结构维护，适合灵活的三维内容编辑场景。

Abstract: Object completion networks typically produce static Signed Distance Fields (SDFs) that faithfully reconstruct geometry but cannot be rescaled or deformed without introducing structural distortions. This limitation restricts their use in applications requiring flexible object manipulation, such as indoor redesign, simulation, and digital content creation. We introduce a part-aware scaling framework that transforms these static completed SDFs into editable, structurally coherent objects. Starting from SDFs and Texture Fields generated by state-of-the-art completion models, our method performs automatic part segmentation, defines user-controlled scaling zones, and applies smooth interpolation of SDFs, color, and part indices to enable proportional and artifact-free deformation. We further incorporate a repetition-based strategy to handle large-scale deformations while preserving repeating geometric patterns. Experiments on Matterport3D and ShapeNet objects show that our method overcomes the inherent rigidity of completed SDFs and is visually more appealing than global and naive selective scaling, particularly for complex shapes and repetitive structures.

</details>


### [44] [Reliable Thinking with Images](https://arxiv.org/abs/2602.12916)
*Haobin Li,Yutong Yang,Yijie Lin,Dai Xiang,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: 论文提出“可靠图像思维”（RTWI）以缓解多模态大模型在“图像思维”（TWI）中因错误视觉线索与推理链而产生的噪声积累问题，并通过置信评估、过滤与投票机制显著提升多基准测试表现。


<details>
  <summary>Details</summary>
Motivation: 现有TWI方法假设交错的图像-文本推理链（CoT）是无误的，但真实多模态理解常含噪声与误导性视觉线索，导致“一错再错”的误差传播，显著降低MLLM推理可靠性与最终答案质量。

Method: 提出RTWI：以文本为中心统一评估视觉线索与文本CoT的可靠性，并据此进行鲁棒过滤与投票。核心包括：1）估计每个视觉提示与中间推理步骤的可靠度；2）过滤低可靠度线索/推理片段；3）对保留候选进行投票融合，抑制噪声对最终答案的污染。

Result: 在七个多模态推理基准上进行大量实验，RTWI在存在噪声思维（NT）的情景下显著优于现有TWI方法，验证了其对错误积累的抑制能力与总体性能提升。

Conclusion: TWI的关键瓶颈在于对无误推理链的过度依赖。通过RTWI对视觉和文本中间证据进行可靠性建模并结合过滤与投票，可有效缓解噪声思维带来的性能劣化，提升MLLM多模态推理的稳健性。

Abstract: As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.

</details>


### [45] [EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition](https://arxiv.org/abs/2602.12919)
*Xiao Wang,Xingxing Xiong,Jinfeng Gao,Xufeng Lou,Bo Jiang,Si-bao Chen,Yaowei Wang,Yonghong Tian*

Main category: cs.CV

TL;DR: EPRBench 构建了首个规模化、面向事件流的视觉地点识别基准，含多场景多条件数据与语言标注，并提出利用大模型文本引导的多模态融合方法，显著提升可解释且高精度的VPR性能。


<details>
  <summary>Details</summary>
Motivation: 传统可见光相机在低光、过曝与高速运动下易失效，事件相机具备高动态范围与时间分辨率，但缺少专用VPR数据集与系统评测；同时缺乏将语言先验融入事件感知以提升鲁棒性与可解释性的研究。

Method: 1) 构建EPRBench：含1万段事件序列与6.5万事件帧，覆盖手持与车载、多视角、天气与光照。2) 提供由LLM生成并经人工修订的场景描述，赋能语义感知与语言融合研究。3) 系统复现并评测15种SOTA VPR算法，形成基线。4) 提出新型多模态融合范式：从原始事件流生成文本描述，用其引导空间注意的token选择、跨模态特征融合与多尺度表征学习，并输出可解释的推理过程。

Result: 在EPRBench上，所提框架实现高精度地点识别，并在可解释性与透明度上优于对比方法；同时给出覆盖广泛情境的统一评测基线，验证语言引导对事件VPR的增益。

Conclusion: EPRBench为事件流VPR提供标准化数据与评测，结合LLM的语言引导多模态框架带来性能与可解释性的双提升，为事件相机感知与VPR研究奠定数据与方法基础；代码与数据集将开源。

Abstract: Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [46] [Beyond Benchmarks of IUGC: Rethinking Requirements of Deep Learning Methods for Intrapartum Ultrasound Biometry from Fetal Ultrasound Videos](https://arxiv.org/abs/2602.12922)
*Jieyun Bai,Zihao Zhou,Yitong Tang,Jie Gan,Zhuonan Liang,Jianan Fan,Lisa B. Mcguire,Jillian L. Clarke,Weidong Cai,Jacaueline Spurway,Yubo Tang,Shiye Wang,Wenda Shen,Wangwang Yu,Yihao Li,Philippe Zhang,Weili Jiang,Yongjie Li,Salem Muhsin Ali Binqahal Al Nasim,Arsen Abzhanov,Numan Saeed,Mohammad Yaqub,Zunhui Xian,Hongxing Lin,Libin Lan,Jayroop Ramesh,Valentin Bacher,Mark Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana I. L. Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale,Assanali Serikbey,Jiankai Li,Sung-Liang Chen,Zicheng Hu,Nana Liu,Yian Deng,Wei Hu,Cong Tan,Wenfeng Zhang,Mai Tuyet Nhi,Gregor Koehler,Rapheal Stock,Klaus Maier-Hein,Marawan Elbatel,Xiaomeng Li,Saad Slimani,Victor M. Campello,Benard Ohene-Botwe,Isaac Khobo,Yuxin Huang,Zhenyan Han,Hongying Hou,Di Qiu,Zheng Zheng,Gongning Luo,Dong Ni,Yaosheng Lu,Karim Lekadir,Shuo Li*

Main category: cs.CV

TL;DR: 该论文介绍IUGC挑战：构建多任务产时超声测量框架（标准平面分类+胎头-耻骨联合分割+测量），并发布迄今最大多中心产时超声视频数据集（774段、68106帧），汇总8支队伍方案与基准结果，分析瓶颈与未来方向，结论是仍处早期、需进一步研究方能临床落地。


<details>
  <summary>Details</summary>
Motivation: 产时阶段占母婴死亡和死胎的大比例，低中收入国家缺乏受训超声医师，限制了超声在分娩监测中的应用。需要自动化、可扩展的方法与标准数据集来推动临床可用的产时超声生物测量。

Method: 发起并描述IUGC挑战：设计一个临床导向的多任务自动测量框架，联合标准平面分类、胎头-耻骨联合（FH-PS）分割与生物测量；发布多中心超声视频数据集；收集并系统回顾8个参赛团队的方法，从预处理、数据增强、学习策略、模型结构、后处理五方面评述；基于基准结果进行瓶颈与改进方向分析。

Result: 提供774段视频（68106帧）的大规模数据集与公开基准；参赛方法在多任务融合下取得“鼓舞”的性能，但仍存在显著误差与鲁棒性问题；识别出关键瓶颈并给出潜在改进思路。

Conclusion: 自动产时超声生物测量取得初步进展，但整体尚处早期，距离大规模临床部署仍需更深入研究与优化。数据集与基准全部公开以促进可重复研究与后续发展。

Abstract: A substantial proportion (45\%) of maternal deaths, neonatal deaths, and stillbirths occur during the intrapartum phase, with a particularly high burden in low- and middle-income countries. Intrapartum biometry plays a critical role in monitoring labor progression; however, the routine use of ultrasound in resource-limited settings is hindered by a shortage of trained sonographers. To address this challenge, the Intrapartum Ultrasound Grand Challenge (IUGC), co-hosted with MICCAI 2024, was launched. The IUGC introduces a clinically oriented multi-task automatic measurement framework that integrates standard plane classification, fetal head-pubic symphysis segmentation, and biometry, enabling algorithms to exploit complementary task information for more accurate estimation. Furthermore, the challenge releases the largest multi-center intrapartum ultrasound video dataset to date, comprising 774 videos (68,106 frames) collected from three hospitals, providing a robust foundation for model training and evaluation. In this study, we present a comprehensive overview of the challenge design, review the submissions from eight participating teams, and analyze their methods from five perspectives: preprocessing, data augmentation, learning strategy, model architecture, and post-processing. In addition, we perform a systematic analysis of the benchmark results to identify key bottlenecks, explore potential solutions, and highlight open challenges for future research. Although encouraging performance has been achieved, our findings indicate that the field remains at an early stage, and further in-depth investigation is required before large-scale clinical deployment. All benchmark solutions and the complete dataset have been publicly released to facilitate reproducible research and promote continued advances in automatic intrapartum ultrasound biometry.

</details>


### [47] [Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses](https://arxiv.org/abs/2602.12933)
*Nanna E. Wielenberg,Ilinca Popp,Oliver Blanck,Lucas Zander,Jan C. Peeken,Stephanie E. Combs,Anca-Ligia Grosu,Dimos Baltas,Tobias Fechter*

Main category: cs.CV

TL;DR: 提出一种无需病灶掩膜的可微分深度学习形变配准方法，将有转移灶的病理脑MRI对齐到标准图谱，同时保护转移体积；在209例黑色素瘤脑转移的多中心数据上取得高配准精度，并揭示转移偏好位于灰白质交界和皮层区。


<details>
  <summary>Details</summary>
Motivation: MBM病灶多发且空间异质，解剖结构被破坏，不同中心MRI协议不一致，导致跨队列空间分析和图谱对齐困难。现有方法常需手工/自动病灶掩膜或繁琐预处理，且在病理区域易失真。需要一种既能对齐健康解剖、又能保留肿瘤/转移信息、可在多中心数据上稳健工作的自动化配准框架。

Method: 构建端到端可微的深度学习形变配准网络：1) 相似性度量采用基于解剖标签距离变换的前向模型，弱化因转移导致的缺失对应；2) 引入体积保持（Jacobian约束）正则，限制不合理压缩/拉伸，保护病灶体积与形变可解释性；3) 在三中心MBM数据（n=209）上训练/评估，并将形变场用于将个体影像标准化到解剖、动脉分区与灌注图谱空间；4) 使用DSC、HD、ASSD及Jacobian统计评估配准质量与体积保持。

Result: 多中心均取得高配准精度：DSC 0.89–0.92，HD 6.79–7.60 mm，ASSD 0.63–0.77 mm；在保证病灶体积基本不受形变影响的同时，实现稳健图谱对齐。空间分析显示：MBM在大脑皮层和豆状核（壳核/苍白球复合体中的纹状体部分，文中为putamen）显著过度分布，在白质中不足分布，且稳定聚集于灰白质交界；体积校正后各动脉供血区未见显著富集。

Conclusion: 所提框架无需病灶掩膜与繁琐预处理，即可在病理脑MRI上实现可信图谱配准，支持跨中心、可重复的空间流行病学分析。对MBM案例，其结果验证并细化了既有认识：偏好灰白质交界与皮层区域而非白质；方法与代码公开，具备推广至其他脑肿瘤与神经病理的潜力。

Abstract: Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.
  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.
  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.
  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.

</details>


### [48] [Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation](https://arxiv.org/abs/2602.12936)
*Hongbo Jiang,Jie Li,Xinqi Cai,Tianyu Xie,Yunhang Shen,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出MLLMEmbed-ReID：用单一多模态大模型统一RGB/红外/素描/文本重识别嵌入，并通过低秩蒸馏部署到边缘，云端与边端均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前云-边CM-ReID需维护多种专用模型，体系碎片；MLLM虽具统一潜力，但缺乏将其适配为端到端骨干与面向边缘的有效蒸馏方案。

Method: (1) 云端：将基础MLLM经指令化提示，学习统一嵌入，对齐RGB/IR/素描/文本；采用分层低秩适配（LoRA-SFT）并以整体跨模态对齐目标训练。(2) 边端：提出基于教师特征低秩性的蒸馏——主成分映射损失以保留关键信息，特征关系损失以保持样本间结构，将知识迁移给轻量学生模型。

Result: 云端模型在所有CM-ReID基准上表现最佳；轻量边端学生在多项视觉CM-ReID数据集上达SOTA。

Conclusion: MLLMEmbed-ReID提供了从云到边的统一CM-ReID解决方案：以MLLM为单干、低秩对齐训练、低秩与关系保持蒸馏，使MLLM级跨模态能力在资源受限设备上落地；代码与模型即将开源。

Abstract: Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.

</details>


### [49] [Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding](https://arxiv.org/abs/2602.12957)
*Wenhui Liao,Hongliang Li,Pengyu Xie,Xinyu Cai,Yufan Shen,Yi Xin,Qi Qin,Shenglong Ye,Tianbin Li,Ming Hu,Junjun He,Yihao Liu,Wenhai Wang,Min Dou,Bin Fu,Botian Shi,Yu Qiao,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出一种无需训练的文档解析加速方法：用轻量级草稿模型批量预测未来token，并由更准确的VLM并行验证；同时按版面分区并行解码，最终按阅读顺序拼接。在OmniDocBench等上实现2.42×至4.89×（长文档）无损加速。


<details>
  <summary>Details</summary>
Motivation: VLM端到端文档解析尽管泛化强，但对长文档需自回归生成长token序列，推理延迟大；文档具有极长输出与复杂版面结构，亟需在不牺牲精度下显著加速。

Method: 训练免调度的“草稿-验证”加速框架：以轻量文档解析流水线作为草稿模型，批量预测后续token；高精度VLM并行校验并接受/回退；结合版面先验，将页面划分为相互独立的区域，对各区并行执行相同草稿-验证解码，最后按自然阅读顺序组装输出。

Result: 在OmniDocBench上对dots.ocr实现2.42×的无损加速；在长文档解析任务上最高达4.89×加速，保持精度不降。

Conclusion: 无需额外训练即可在保证准确度的前提下显著降低长文档解析延迟；版面分区并行与草稿-验证解码相辅相成，适用于通用文档解析VLM；代码将开源以促进复现与后续研究。

Abstract: Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.

</details>


### [50] [Detecting Object Tracking Failure via Sequential Hypothesis Testing](https://arxiv.org/abs/2602.12983)
*Alejandro Monroy Muñoz,Rajeev Verma,Alexander Timans*

Main category: cs.CV

TL;DR: 将目标跟踪视为“序贯假设检验”的安全保障框架：在视频流中持续累积证据，快速检测失跟，同时以可控误报率发出告警；无需额外训练、轻量、与模型无关，并有监督/无监督两种实现，在两类模型与四个基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实中的在线目标跟踪系统缺乏形式化安全保证——何时可靠、何时可能失效通常仅依赖启发式置信度阈值，难以权衡延迟、误报和干预成本。需要一种统计上可控、能实时提示失跟并限制误报率的机制，以减少不必要的重标定或人工介入。

Method: 将“是否失跟”建模为序贯假设检验问题，构造满足e-process性质的统计量，随时间累积证据：当证据超过阈值即触发失跟告警，从而在任意停时控制误报（家族式）或长期平均的错误率。方法计算开销小、无需再训练、对跟踪模型透明；给出两种变体：监督式使用真值辅助、无监督式仅用跟踪器内部信号（如置信度、重检测一致性等）。

Result: 在两个常用跟踪模型、四个视频基准上，所提序贯测试能更快检测到失跟，同时在设定的显著性/误报控制下维持低假警；相较启发式阈值，达到更好的告警时效-准确权衡，并减少不必要的干预次数。

Conclusion: 将序贯检验引入实时跟踪，可为系统提供统计上有保证的安全告警：快速识别失效、控制误报、开销小、无需训练、模型无关；适用于实际部署以降低运维与干预成本。

Abstract: Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>


### [51] [MASAR: Motion-Appearance Synergy Refinement for Joint Detection and Trajectory Forecasting](https://arxiv.org/abs/2602.13003)
*Mohammed Amine Bencheikh Lehocine,Julian Schmidt,Frank Moosmann,Dikshant Gupta,Fabian Flohr*

Main category: cs.CV

TL;DR: 提出MASAR：面向端到端自动驾驶的、与任意Transformer类3D检测器兼容的联合3D检测与轨迹预测框架，通过“回看过去以预测未来”的思想，利用外观与运动的时空融合，先预测并用外观引导校正历史轨迹，从而提升未来轨迹预测；在nuScenes上minADE/minFDE提升20%+且检测性能稳健。


<details>
  <summary>Details</summary>
Motivation: 传统流水线以手工设计的3D框框接口衔接感知与预测，信息受限且误差级联；现有端到端方法多依赖短期视觉特征，未充分融合外观与运动、欠缺长时依赖。需要一个能在同一可微框架中联合编码外观与运动、并利用长期时序来改进未来预测的方法。

Method: 提出MASAR：对象为中心的时空编码机制，兼容Transformer式3D检测器。模型联合编码外观(appearance)与运动(motion)特征，采用“先回顾再前瞻”的策略：预测过去轨迹并以外观线索进行细化/校正，从而学习长时依赖；随后在该表示上进行未来轨迹预测，同时保持3D检测端到端训练。框架完全可微并与现有3D检测器无缝集成。

Result: 在nuScenes数据集上，相比基线，未来轨迹的minADE与minFDE均提升20%以上；同时3D检测性能保持稳健（无显著退化）。

Conclusion: 联合外观与运动并显式建模历史轨迹可显著改进未来轨迹预测而不牺牲检测质量；MASAR验证了“回看过去以预测未来”的有效性，并具备与多种Transformer 3D检测器兼容的实用价值。

Abstract: Classical autonomous driving systems connect perception and prediction modules via hand-crafted bounding-box interfaces, limiting information flow and propagating errors to downstream tasks. Recent research aims to develop end-to-end models that jointly address perception and prediction; however, they often fail to fully exploit the synergy between appearance and motion cues, relying mainly on short-term visual features. We follow the idea of "looking backward to look forward", and propose MASAR, a novel fully differentiable framework for joint 3D detection and trajectory forecasting compatible with any transformer-based 3D detector. MASAR employs an object-centric spatio-temporal mechanism that jointly encodes appearance and motion features. By predicting past trajectories and refining them using guidance from appearance cues, MASAR captures long-term temporal dependencies that enhance future trajectory forecasting. Experiments conducted on the nuScenes dataset demonstrate MASAR's effectiveness, showing improvements of over 20% in minADE and minFDE while maintaining robust detection performance. Code and models are available at https://github.com/aminmed/MASAR.

</details>


### [52] [Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions](https://arxiv.org/abs/2602.13013)
*Yunheng Li,Hengrui Zhang,Meng-Hao Guo,Wenzhao Gao,Shaoyong Jia,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出ASID-1M数据集、ASID-Verify数据标注与校验流水线、以及在其上SFT训练的ASID-Captioner，用以提升视频多模态细粒度理解；在多项基准上实现开放源最优并接近Gemini-3-Pro。


<details>
  <summary>Details</summary>
Motivation: 现有视频-指令数据将复杂视听内容压缩为单一且不完整描述，缺乏细粒度组织与可靠标注，导致多模态视频理解（时序、视觉、音频、指令遵循）受限与幻觉问题突出。

Method: 1) 构建ASID-1M：包含百万级结构化、细粒度视听指令标注，覆盖单/多属性监督；2) 设计ASID-Verify：可扩展的数据策划流水线，对描述与视听内容进行自动验证与精炼，保证语义与时间一致性；3) 训练ASID-Captioner：在ASID-1M上进行SFT，面向细粒度字幕、属性描述、问答与时序定位等任务。

Result: 在七个基准（视听字幕、属性级字幕、基于字幕的问答、基于字幕的时序定位）上，ASID-Captioner显著提升细粒度字幕质量，降低幻觉并增强指令遵循；在开源模型中达SOTA，且与Gemini-3-Pro具有竞争力。

Conclusion: 高质量、结构化且经自动一致性校验的视听指令数据能显著提升视频理解模型的细粒度能力与可靠性；ASID-1M与ASID-Verify构成有效数据基础设施，ASID-Captioner验证了其有效性并推动开源SOTA。

Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>


### [53] [Multimodal Classification via Total Correlation Maximization](https://arxiv.org/abs/2602.13015)
*Feng Yu,Xiangyu Wu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: 提出TCMax：通过最大化多模态特征与标签之间的总相关（Total Correlation, TC）的超参数-free损失，缓解多模态学习中的模态竞争并提升分类性能。基于MINE扩展出TCNE以估计TC的下界，进而进行变分优化。实验优于最新的联合与单模态方法。


<details>
  <summary>Details</summary>
Motivation: 多模态联合学习常出现对强势模态过拟合、弱势模态被忽视的“模态竞争”，导致整体性能不如单模态。既有方法多通过权重平衡或联合/单独训练混合来缓解，但缺乏从信息论角度刻画联合与单模态学习关系的系统分析与统一优化目标。

Method: 从信息论出发，将多模态分类表述为最大化“多模态特征与标签之间的总相关”问题。提出：1) TCNE（Total Correlation Neural Estimation），在MINE的基础上构造估计器，给出TC的可训练下界；2) TCMax，无需超参数的损失函数，通过变分下界最大化实现端到端训练；并通过特征对齐来捕获跨模态交互，降低模态竞争。

Result: 在多项基准上进行广泛实验，TCMax在准确率等指标上超过当前最优的联合训练和单模态训练方法，表现稳定，证明了所提信息论目标与估计器在实践中的有效性。

Conclusion: 最大化多模态特征与标签的总相关能系统性缓解模态竞争并提升多模态分类。TCNE为TC提供可优化下界，TCMax以无超参数的方式实现有效训练，实验验证其优越性与通用性。

Abstract: Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.

</details>


### [54] [DynaGuide: A Generalizable Dynamic Guidance Framework for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2602.13020)
*Boujemaa Guermazi,Riadh Ksantini,Naimul Khan*

Main category: cs.CV

TL;DR: DynaGuide 提出一种无监督图像分割框架，通过零样本全局伪标签+轻量CNN局部边界细化、并用动态多项损失联合优化，实现兼顾语义一致性与边界精度的分割；在BSD500/VOC2012/COCO上显著提升mIoU，具备模块化、可插拔与低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有无监督分割难以同时兼顾全局语义结构与精细边界，且目标域缺乏标注；需要一种既能利用强大零样本先验又能校正其粗糙/噪声边界的自适应方法。

Method: 双重引导+动态损失：1) 全局引导：利用DiffSeg、SegFormer等零样本模型生成全局伪标签；2) 局部引导：从零训练的轻量CNN对边界与细节进行细化；3) 动态多项损失：包括特征相似度项、Huber平滑的空间连续性（含对角关系）、与全局伪标签的语义对齐项，并自适应调权；4) 完全无需目标域真值标注，支持多源指导的即插即用集成。

Result: 在BSD500、PASCAL VOC2012、COCO上取得SOTA：mIoU分别提升17.5%、3.1%、11.66%，并展现良好泛化与低计算代价。

Conclusion: DynaGuide 通过融合全局伪标签与局部细化、辅以动态平衡的多项损失，有效缓解无监督分割中的语义-边界权衡，提供可扩展、实用、计算友好的方案，适合真实场景部署；代码已开源，便于复现与扩展。

Abstract: Unsupervised image segmentation is a critical task in computer vision. It enables dense scene understanding without human annotations, which is especially valuable in domains where labelled data is scarce. However, existing methods often struggle to reconcile global semantic structure with fine-grained boundary accuracy. This paper introduces DynaGuide, an adaptive segmentation framework that addresses these challenges through a novel dual-guidance strategy and dynamic loss optimization. Building on our previous work, DynaSeg, DynaGuide combines global pseudo-labels from zero-shot models such as DiffSeg or SegFormer with local boundary refinement using a lightweight CNN trained from scratch. This synergy allows the model to correct coarse or noisy global predictions and produce high-precision segmentations. At the heart of DynaGuide is a multi-component loss that dynamically balances feature similarity, Huber-smoothed spatial continuity, including diagonal relationships, and semantic alignment with the global pseudo-labels. Unlike prior approaches, DynaGuide trains entirely without ground-truth labels in the target domain and supports plug-and-play integration of diverse guidance sources. Extensive experiments on BSD500, PASCAL VOC2012, and COCO demonstrate that DynaGuide achieves state-of-the-art performance, improving mIoU by 17.5% on BSD500, 3.1% on PASCAL VOC2012, and 11.66% on COCO. With its modular design, strong generalization, and minimal computational footprint, DynaGuide offers a scalable and practical solution for unsupervised segmentation in real-world settings. Code available at: https://github.com/RyersonMultimediaLab/DynaGuide

</details>


### [55] [Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels](https://arxiv.org/abs/2602.13022)
*Julius Pesonen,Stefan Rua,Josef Taher,Niko Koivumäki,Xiaowei Yu,Eija Honkavaara*

Main category: cs.CV

TL;DR: 提出用ALS派生伪标签+SAM2增强来训练从RGB/多光谱航片中分割并分离单株树冠的深度模型，无需人工标注且优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 单株树冠分割对城市树木清单与森林健康监测关键，但航片纹理复杂、树冠重叠导致自动分割与实例分离困难，缺乏高质量、领域特定标注数据。

Method: 利用机载激光雷达(ALS)数据生成树冠实例伪标签，结合零样本实例分割模型SAM2对伪标签进行提升/修正，以此训练针对RGB与多光谱图像的实例分割/分离深度学习模型，实现无需人工标注的领域特定监督。

Result: 用ALS+SAM2生成的伪标签训练的模型在单株树冠分割与分离任务上优于面向通用场景发布的现有模型；证明了无人工标注也能获得高性能的领域化模型。

Conclusion: 通过ALS伪标签与SAM2增强可低成本获取高质量、域内训练数据，训练出的光学影像树冠实例分割模型在该任务上优于通用模型，为大规模树冠制图提供实用方案。

Abstract: Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.

</details>


### [56] [FedHENet: A Frugal Federated Learning Framework for Heterogeneous Environments](https://arxiv.org/abs/2602.13024)
*Alejandro Dopico-Castro,Oscar Fontenla-Romero,Bertha Guijarro-Berdiñas,Amparo Alonso-Betanzos,Iván Pérez Digón*

Main category: cs.CV

TL;DR: 提出FedHENet：用预训练特征提取器+单层输出，在一次通信中用同态加密解析聚合，免迭代与超参调优，达到可比精度、更稳、更节能（最高70%）。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习依赖多轮迭代和深网本地训练，代价高、能耗大，且共享梯度仍有隐私泄露风险；需要在隐私合规下减少计算与通信，同时保持精度与稳定性。

Method: 扩展FedHEONN到图像分类：客户端固定预训练特征提取器，仅学习线性输出层；借助同态加密，将各客户端统计或充分信息加密上传，服务器在密文域解析式聚合，单轮得到全局输出层，无本地微调与超参调优。

Result: 在多数据集实验中，对比迭代式FL基线（如FedAvg等）取得相当精度；训练过程更稳定；能源效率提升最高达70%；单轮通信完成训练。

Conclusion: FedHENet在保证隐私的前提下，以一次通信、解析求解的线性层学习替代传统多轮迭代，达到可比精度与更高稳定、能效，并免去超参调优的碳足迹；适合资源受限、对隐私与能耗敏感的场景。

Abstract: Federated Learning (FL) enables collaborative training without centralizing data, essential for privacy compliance in real-world scenarios involving sensitive visual information. Most FL approaches rely on expensive, iterative deep network optimization, which still risks privacy via shared gradients. In this work, we propose FedHENet, extending the FedHEONN framework to image classification. By using a fixed, pre-trained feature extractor and learning only a single output layer, we avoid costly local fine-tuning. This layer is learned by analytically aggregating client knowledge in a single round of communication using homomorphic encryption (HE). Experiments show that FedHENet achieves competitive accuracy compared to iterative FL baselines while demonstrating superior stability performance and up to 70\% better energy efficiency. Crucially, our method is hyperparameter-free, removing the carbon footprint associated with hyperparameter tuning in standard FL. Code available in https://github.com/AlejandroDopico2/FedHENet/

</details>


### [57] [Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis](https://arxiv.org/abs/2602.13028)
*Runzhou Liu,Hailey Weingord,Sejal Mittal,Prakhar Dungarwal,Anusha Nandula,Bo Ni,Samyadeep Basu,Hongjie Chen,Nesreen K. Ahmed,Li Li,Jiayi Zhang,Koustava Goswami,Subhojyoti Mukherjee,Branislav Kveton,Puneet Mathur,Franck Dernoncourt,Yue Zhao,Yu Wang,Ryan A. Rossi,Zhengzhong Tu,Hongru Du*

Main category: cs.CV

TL;DR: 提出一个细粒度的“MLLM 评审员”框架与基准，用12个可解释因素评估图像编辑中的保真、编辑质量与指令一致性，较传统指标更贴近人类判断，能更好区分过度编辑与语义偏差。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑评估指标粒度粗、可解释性弱，偏向“看起来像样”的结果，忽视可控性、编辑局部性和对指令的忠实度；需要一种与人类感知与意图更一致、可扩展且可解释的评估方法与基准。

Method: 1) 将图像编辑评估因子化为覆盖图像保留、编辑质量和指令忠实度的12个细粒度可解释维度；2) 构建“MLLM-as-a-Judge”评审框架，让多模态大模型按这12因子对编辑结果进行评分与说明；3) 发布人类验证的基准，融合人工标注、MLLM 评分、模型输出与传统指标；4) 通过大规模人类实验验证 MLLM 评审与人类在细粒度层面的高度一致性。

Result: 实验证明：MLLM 评审在细粒度对齐上与人类高度相关，可在离线与在线场景稳定工作；传统指标常无法识别过度编辑或语义不精确的问题，作为这些因子的代理较差；所提评审提供更直观、信息量更高的诊断。

Conclusion: 细粒度的 MLLM 评审结合新基准与因子化框架，为研究、比较与改进图像编辑方法提供了可靠、可扩展且更贴近人类感知的评价基础。

Abstract: Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators. We further demonstrate that traditional image editing metrics are often poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, whereas our judges provide more intuitive and informative assessments in both offline and online settings. Together, this work introduces a benchmark, a principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches.

</details>


### [58] [Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images](https://arxiv.org/abs/2602.13041)
*Yuhao Chen,Gautham Vinod,Siddeshwar Raghavan,Talha Ibn Mahmud,Bruce Coburn,Jinge Ma,Fengqing Zhu,Jiangpeng He*

Main category: cs.CV

TL;DR: 提出一个用于真实用餐场景的“隐式尺度”单目多食物图像三维重建基准，用以推动几何化食物分量估计；去除了显式尺度参照，鼓励从情境线索推断尺度；结果显示几何重建方法在准确性与鲁棒性上优于视觉语言基线。


<details>
  <summary>Details</summary>
Motivation: 现有膳食评估多依赖单张图或外观/语言先验，缺乏显式几何推理且易受尺度歧义影响；现实就餐中常见多物体、遮挡与复杂布局，需要能从隐含线索恢复尺度与体积。

Method: 构建一个基准：单目、多食物、无显式物理参照与公制标注；仅提供盘子、餐具等上下文物体作为隐式尺度线索；以三维重建为核心任务，评测体积MAPE与几何L1 Chamfer等；作为MetaFood 2025挑战吸引多团队提交基于重建的方法，并与强VL基线对比。

Result: 几何重建方案总体优于强视觉-语言基线，表现为更高准确性与鲁棒性；最佳方法在体积估计上达到0.21 MAPE，在几何精度上达到5.7的L1 Chamfer Distance。

Conclusion: 将食物分量估计重构为隐式尺度三维重建任务可有效缓解尺度歧义并提升性能；多食物、遮挡与复杂布局条件下，基于几何的重建方法更具优势，基准为后续研究提供了挑战性与现实性的评测平台。

Abstract: We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

</details>


### [59] [Curriculum-DPO++: Direct Preference Optimization via Data and Model Curricula for Text-to-Image Generation](https://arxiv.org/abs/2602.13055)
*Florinel-Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu,Nicu Sebe,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出 Curriculum-DPO++：在文本到图像生成中，将数据难度课程与模型容量课程结合，通过逐步解冻网络层与递增LoRA秩来提升学习效率与最终对齐效果，跨九个基准在文本对齐、美学与人偏好上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF与DPO忽视“偏好难度”差异，统一训练会导致优化次优；已有Curriculum-DPO仅在数据层面排序样本难度，尚未利用模型容量随训练阶段递增的潜力。

Method: 双重课程学习：1) 数据层面：对成对图像按难度排序并从易到难训练，并提出替代的排序策略；2) 模型层面：动态增大去噪网络的学习容量，包括(a) 从部分可训练层开始，随训练推进逐步解冻直至完整架构；(b) LoRA渐进式容量调度，从较小秩开始并逐步提升至基线秩。

Result: 在九个基准上，相比Curriculum-DPO与其他偏好优化方法，Curriculum-DPO++在文本对齐度、美学评分与人工偏好方面均取得更优结果。

Conclusion: 结合数据课程与模型课程的Curriculum-DPO++能更高效地学习复杂偏好，通过逐步提升模型容量与样本难度实现稳定训练与性能提升，成为偏好对齐的更优方案。

Abstract: Direct Preference Optimization (DPO) has been proposed as an effective and efficient alternative to reinforcement learning from human feedback (RLHF). However, neither RLHF nor DPO take into account the fact that learning certain preferences is more difficult than learning other preferences, rendering the optimization process suboptimal. To address this gap in text-to-image generation, we recently proposed Curriculum-DPO, a method that organizes image pairs by difficulty. In this paper, we introduce Curriculum-DPO++, an enhanced method that combines the original data-level curriculum with a novel model-level curriculum. More precisely, we propose to dynamically increase the learning capacity of the denoising network as training advances. We implement this capacity increase via two mechanisms. First, we initialize the model with only a subset of the trainable layers used in the original Curriculum-DPO. As training progresses, we sequentially unfreeze layers until the configuration matches the full baseline architecture. Second, as the fine-tuning is based on Low-Rank Adaptation (LoRA), we implement a progressive schedule for the dimension of the low-rank matrices. Instead of maintaining a fixed capacity, we initialize the low-rank matrices with a dimension significantly smaller than that of the baseline. As training proceeds, we incrementally increase their rank, allowing the capacity to grow until it converges to the same rank value as in Curriculum-DPO. Furthermore, we propose an alternative ranking strategy to the one employed by Curriculum-DPO. Finally, we compare Curriculum-DPO++ against Curriculum-DPO and other state-of-the-art preference optimization approaches on nine benchmarks, outperforming the competing methods in terms of text alignment, aesthetics and human preference. Our code is available at https://github.com/CroitoruAlin/Curriculum-DPO.

</details>


### [60] [A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models](https://arxiv.org/abs/2602.13066)
*Yash Deo,Yan Jia,Toni Lassila,Victoria J Hodge,Alejandro F Frang,Chenghao Qian,Siyuan Kang,Ibrahim Habli*

Main category: cs.CV

TL;DR: 提出用于检测医学影像生成模型训练样本记忆/复刻的逐样本校准指标，基于MRI基础模型特征与多层白化最近邻相似度，映射为有界的ONI与MI分数，能在多数据集上稳健、接近完美地识别重复样本。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型会复制训练数据，医学影像场景中这会引发隐私与合规风险，需有可靠、可跨数据集且逐样本的记忆与重复检测度量，以在模型评估与部署时识别潜在泄露。

Method: 利用MRI基础模型提取多层特征；对各层特征进行白化处理并计算样本与训练集的最近邻相似度；跨层聚合这些相似度；通过校准映射得到有界的Overfit/Novelty Index (ONI) 与 Memorization Index (MI) 两个分数，用于衡量样本是否为训练数据的复刻或过拟合产物。

Result: 在三个MRI数据集上、在受控的重复比例与常见数据增强条件下，所提指标能够稳健区分重复与非重复样本，指标值跨数据集更一致，并在逐样本层面实现近乎完美的重复检测性能。

Conclusion: 基于MRI基础模型特征与多层白化最近邻聚合的ONI/MI指标可作为实用的逐样本记忆检测工具，在医学影像生成应用中有效降低训练数据泄露与隐私风险。

Abstract: Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.

</details>


### [61] [SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery](https://arxiv.org/abs/2602.13067)
*Chunming Li,Shidong Wang,Tong Xin,Haofeng Zhang*

Main category: cs.CV

TL;DR: 提出SIEFormer：以频谱视角重释并增强ViT注意力，用于更难的通用类别发现任务，含隐式（图拉普拉斯+BaF）与显式（傅里叶域MFL）两分支，联合优化并在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: ViT在GCD等开放场景中需更强的特征适应与可解释性；传统注意力缺少对局部/全局频谱结构的明确建模，难以灵活抑制或强化不同频带信息。

Method: 构建双分支SIEFormer：1) 隐式频谱分支以不同图拉普拉斯捕捉token局部结构相关性，并引入可执行带通/带阻的Band-adaptive Filter（BaF）；2) 显式频谱分支的Maneuverable Filtering Layer（MFL）对value特征做FFT→频域以可学习参数调制→IFFT，学习全局依赖；两分支联合优化以增强特征适配性与可解释性。

Result: 在多项图像识别数据集（含GCD设置）取得SOTA表现；消融与可视化验证各组件（拉普拉斯选择、BaF、MFL）的有效性与互补性。

Conclusion: 以频谱分析统一解释并增强ViT注意力，隐式与显式频域建模互补，能够自适应选择/抑制频带，显著提升GCD等任务性能。

Abstract: This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.

</details>


### [62] [Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.13091)
*Declan McIntosh,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: 提出一种“数据折叠”方法，把任何基于单类分类器的图像/视频异常检测器转化为完全无监督方法，并在MVTec AD、ViSA、MVTec LOCO AD上达SOTA。核心思想：假设异常在训练集中少且异质，通过多次独立训练的单类分类器相互筛除潜在异常，进而仅用推定的正常子集训练。


<details>
  <summary>Details</summary>
Motivation: 现实中异常检测常以单类学习训练，但假设训练标签全为正常样本；一旦混入少量异常（标签噪声），性能会恶化。且需要标注成本高、难以覆盖多类异常。作者希望在不改动原有检测器结构与训练流程的情况下，去除对干净正常数据的依赖，实现无监督训练，同时能承接单类方法的进步。

Method: 提出“dataset folding/数据折叠”：将原始训练集拆分为多个重叠/互斥子集，独立训练多个单类分类器实例；利用“异常稀少且多样化”的弱假设，让各实例对不属于其训练子集的数据进行打分；通过交叉筛除（ensemble一致性/不一致性）剔除疑似异常，保留高置信正常样本子集；再用这些子集训练最终模型。无需修改底层检测器，仅算法性选择训练子集即可，适用于图像与视频、各类单类方法。

Result: 在MVTec AD、ViSA、MVTec LOCO AD上，方法作为通用包装器将多种单类异常检测器转为无监督版本，取得新的无监督SOTA；同时首次给出“逻辑意义上的”无监督异常检测器的通用变换路径。

Conclusion: 数据折叠通过多实例交叉筛选，稳健应对训练集含异常的情形，把单类异常检测自然过渡到无监督设定；方法通用、无需改动模型结构，并可直接继承未来单类方法的提升，从而在两种范式间建立可迁移的桥梁。

Abstract: Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.

</details>


### [63] [Realistic Face Reconstruction from Facial Embeddings via Diffusion Models](https://arxiv.org/abs/2602.13168)
*Dong Han,Yong Li,Joachim Denzler*

Main category: cs.CV

TL;DR: 提出FEM框架，用KAN把人脸嵌入映射回高分辨率面孔，结合身份保持扩散模型，对SOTA FR与PPFR实现从嵌入到人脸的攻击；重建人脸可跨系统登录，且对部分/受保护嵌入依然有效，用于评估隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 现有PPFR虽强调隐私与鲁棒性，但缺少从嵌入重建真实高分辨率人脸以量化隐私风险的系统性验证，尤其针对PPFR的实证缺口。

Method: 提出Face Embedding Mapping（FEM）：用Kolmogorov-Arnold Network学习从人脸嵌入到潜空间（或图像空间）的映射，并与预训练的“身份保持”扩散生成模型结合，实现embedding-to-face攻击；在SOTA FR与PPFR上评测，并测试对部分（截断/子集）与受保护（加噪/变换）嵌入的鲁棒性与可迁移性。

Result: FEM可从FR与PPFR嵌入重建出高保真、可辨识的人脸；重建图像在其他真实FR系统中具有可接受的识别/通过率，说明隐私可被跨系统利用；对部分或保护后的嵌入仍能较好重建，展现鲁棒性。

Conclusion: FR与PPFR的嵌入存在显著隐私泄漏风险；FEM既是强力攻击基线，也可作为评估工具帮助检验与提升系统的隐私安全。

Abstract: With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.

</details>


### [64] [LongStream: Long-Sequence Streaming Autoregressive Visual Geometry](https://arxiv.org/abs/2602.13172)
*Chong Cheng,Xianda Chen,Tao Xie,Wei Yin,Weiqiang Ren,Qian Zhang,Xiaoyuang Guo,Hao Wang*

Main category: cs.CV

TL;DR: 提出LongStream，一种用于超长序列流式3D重建的“规范（量纲）解耦”视觉几何模型，通过关键帧相对位姿、正交尺度学习与缓存一致训练+周期刷新，解决注意力衰减、尺度漂移与KV缓存污染，实现公里级、千帧以上稳定的米制重建，达18 FPS并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 自回归流式重建在超长序列中易失败：以首帧为锚导致注意力随距离衰减、外推误差与尺度漂移；Transformer的KV缓存在长时段推理中会引入“注意力水槽”依赖与污染，训练-推理分布不匹配，难以实现稳定的米制重建。

Method: 三点创新：1) 取消首帧锚定，改为关键帧相对位姿预测，将长程外推化为恒定难度的局部配准；2) 正交尺度学习，将几何估计与尺度估计完全解耦，抑制尺度漂移，实现稳定米制；3) 面向超长序列的Transformer缓存策略：缓存一致性训练与周期性缓存刷新，缓解注意力衰减与KV污染，缩小训推差距。

Result: 在超长（千帧、公里级）序列上实现稳定米制三维重建，推理速度18 FPS；综合指标达当前SOTA，显著降低注意力退化与尺度漂移带来的误差。

Conclusion: 通过“锚点解耦+尺度正交+缓存一致与刷新”的整体设计，LongStream在超长流式场景实现稳健、可扩展的米制3D重建，克服了传统自回归方法的注意力衰减、尺度漂移与KV缓存污染等关键瓶颈。

Abstract: Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/

</details>


### [65] [Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace](https://arxiv.org/abs/2602.13176)
*Seth Donahue,J. D. Peiffer,R. Tyler Richardson,Yishan Zhong,Shaun Q. Y. Tan,Benoit Marteau,Stephanie R. Russo,May D. Wang,R. James Cotton,Ross Chafetz*

Main category: cs.CV

TL;DR: 验证用单目相机+AI无标记动作捕捉评估上肢可达工作空间（UERW）的可行性：正前方单目视角与标志点系统高度一致，偏置视角低估范围；提示单摄像头在临床前向工作空间评估中具备实用潜力。


<details>
  <summary>Details</summary>
Motivation: 临床运动分析需要客观、低成本、易部署的方法。传统标记式系统成本高、流程复杂，限制了在常规评估（如UERW）中的应用。作者欲验证AI驱动的单目无标记捕捉能否在特定临床任务中达到可用精度，从而促进临床推广。

Method: 9名健康成人执行标准化UERW任务（VR头显呈现围绕躯干的虚拟球面目标）。同时使用标记式动作捕捉与8台工业相机采集，其中选取两路视频做单目AI无标记重建，分别为正前方与偏置视角。比较各八分体（octant）达到的可达空间百分比，与标记式结果进行一致性分析与偏倚评估。

Result: 正前方单目视角与标记式高度一致，平均偏差为0.61±0.12%每八分体；偏置视角低估可达空间，平均偏差为-5.66±0.45%。前向（前方）工作空间的一致性最佳，表明视角选择对精度影响显著。

Conclusion: 单个正前方相机的AI无标记捕捉可对UERW进行可行、接近标记式的量化评估，尤其适用于前向工作空间；偏置视角不推荐。该研究为UERW任务的单目MMC提供首个验证，降低技术复杂度，有望促进上肢活动度的量化评估在临床的广泛应用。

Abstract: To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.

</details>


### [66] [FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control](https://arxiv.org/abs/2602.13185)
*Mingzhi Sheng,Zekai Gu,Peng Li,Cheng Lin,Hao-Xiang Guo,Ying-Cong Chen,Yuan Liu*

Main category: cs.CV

TL;DR: 论文提出FlexAM：用三维点云化的“运动控制信号”来解耦视频的外观与运动，实现更通用、可控的视频生成与编辑，在I2V/V2V、相机与空间对象编辑等任务上全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成控制多依赖模糊或任务特定信号，难以通用与扩展；核心痛点在于外观（appearance）与运动（motion）未被清晰解耦，导致控制不稳、泛化差。作者认为从基础上实现外观-运动解耦是实现稳健、可扩展控制的关键。

Method: 提出统一框架FlexAM，以新型三维控制信号描述视频动态：将视频运动表示为点云，并引入三项改进——(1) 多频位置编码以区分细粒度运动；(2) 深度感知位置编码提升三维几何一致性；(3) 可调的控制强度机制在精确度与生成质量间灵活权衡。基于该表示，网络在训练/推理中分别建模外观与运动，从而实现多任务控制。

Result: 在图像到视频、视频到视频编辑、相机运动控制、空间对象编辑等多种任务与基准上进行大量实验，FlexAM在控制精度、生成质量与泛化性上均优于对比方法，表现出更稳定、可扩展的控制能力。

Conclusion: 三维点云化的运动控制信号能有效解耦外观与运动，为通用、可控的视频生成提供了统一路径；FlexAM在多任务上取得领先结果，显示该表示与框架具有强泛化与实用价值。

Abstract: Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

</details>


### [67] [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/abs/2602.13191)
*Sayan Deb Sarkar,Rémi Pautrat,Ondrej Miksik,Marc Pollefeys,Iro Armeni,Mahdi Rad,Mihai Dusmanu*

Main category: cs.CV

TL;DR: 提出利用视频编码器的运动向量与残差来替代逐帧全图编码，结合轻量Transformer编码器与对齐预训练，使VideoLM在保持/提升多项视频理解任务性能的同时，大幅降低计算与token开销。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLM受上下文窗口限制依赖稀疏关键帧采样，易遗漏宏观事件与微观细节；逐帧全图处理带来高计算与token成本，影响时延与效率。需要一种既覆盖时序细节又高效的表示方式。

Method: 利用视频编解码原语（运动向量、残差）捕获时序冗余与稀疏性：仅对少量关键帧做全图编码，其余帧以codec原语提取轻量特征。设计轻量Transformer聚合这些原语，并通过预训练将其表征对齐到图像编码器的嵌入空间，随后端到端微调。可调节关键帧与原语密度以权衡效率与性能。

Result: 相较标准VideoLM：首token延迟最高降86%，token使用量最高降93%。在14个多样视频理解基准（QA、时间推理、长视频理解、空间场景理解）上，维持或超过现有性能。

Conclusion: 基于视频codec原语的表征为VideoLM提供了一种高效、可伸缩且时序覆盖更全面的方案，在显著降低计算与token成本的同时保持/提升任务表现，适合长视频与资源受限场景，并可通过密度调节适配不同需求。

Abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>


### [68] [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/abs/2602.13195)
*Aadarsh Sahoo,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 提出“会话式图像分割（CIS）”与数据集/基准ConverSeg，覆盖意图、可供性、功能与安全等推理，并给出结合强分割先验与语言理解的ConverSeg-Net及无监督数据引擎，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往指代式图像定位多聚焦类别与几何（如“最左边的苹果”），忽视功能、可供性与物理/安全等更高层次意图推理（如“哪里能安全放刀”）。需要一个能处理对话式、多维度推理的基准与方法。

Method: 1) 构建CIS任务与ConverSeg基准，涵盖实体、空间关系、意图、可供性、功能、安全与物理推理；2) 设计ConverSeg-Net，将强分割先验与语言理解融合；3) 构建AI驱动的数据引擎，无需人工标注自动生成提示-掩码对；4) 评估现有语言引导分割并与ConverSeg-Net对比。

Result: 现有语言引导分割在CIS上表现不足；用数据引擎训练的ConverSeg-Net在ConverSeg上显著提升，同时在现有语言引导分割基准上保持强劲表现。

Conclusion: CIS与ConverSeg揭示并量化了意图与物理/功能推理在语言引导分割中的缺口；ConverSeg-Net与无监督数据引擎有效弥补该缺口，推进了对话式、意图驱动的像素级理解。

Abstract: Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., "left-most apple") and overlooks functional and physical reasoning (e.g., "where can I safely store the knife?"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/

</details>
