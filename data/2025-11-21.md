<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 110]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment](https://arxiv.org/abs/2511.15831)
*Wei Zhang,Yeying Jin,Xin Li,Yan Zhang,Xiaofeng Cong,Cong Wang,Fengcai Qiao,zhichao Lian*

Main category: cs.CV

TL;DR: UniFit 是一个由多模态大语言模型驱动的通用图像试衣（VTON）框架，通过多模态语义对齐与渐进式训练，支持多服饰与人像到人像等复杂任务并达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的多任务 VTON 框架仍存在两大痛点：文本指令与参考图像间的语义鸿沟，以及复杂场景训练数据稀缺，限制了通用性与性能。

Method: 提出 UniFit：1) MLLM-Guided Semantic Alignment（MGSA）模块，使用 MLLM 与可学习查询融合多模态输入，并施加语义对齐损失以学习跨模态语义关系，为生成过程提供一致且明确的语义引导；2) 两阶段渐进式训练结合自合成（self-synthesis）数据管线，使模型在有限真实数据下习得复杂任务。

Result: 在多种 VTON 任务（含多服饰、模特到模特试穿等）上实现最先进性能，展现更强的任务覆盖面与生成质量。

Conclusion: 利用 MLLM 进行显式跨模态语义对齐并配合自合成驱动的渐进训练，可有效缩小文本-图像语义差距、缓解数据稀缺，构建通用且高性能的 VTON 框架。

Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.

</details>


### [2] [EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3](https://arxiv.org/abs/2511.15833)
*Chengxi Zeng,Yuxuan Jiang,Aaron Zhang*

Main category: cs.CV

TL;DR: EfficientSAM3 通过分阶段蒸馏，将重型 SAM3 的可提示概念分割能力迁移到轻量学生模型，在保证概念级性能的同时显著降低算力与内存需求，适配端侧部署。


<details>
  <summary>Details</summary>
Motivation: SAM3 统一但庞大的架构（共享骨干+DETR 检测器+稠密时空记忆）难以在设备端实时运行，需要一种方法在不显著牺牲 PCS 能力的前提下压缩模型、提升时空检索效率。

Method: 提出渐进式层级蒸馏（PHD），三阶段：1）编码器蒸馏：在 SA-1B 上引入“提示在环”的对齐，逼近教师的图像特征；2）时间记忆蒸馏：以紧凑的 Perceiver 模块替代稠密记忆，在 SA-V 上学习压缩与检索时空特征；3）端到端微调：在官方 SAM3 PCS 数据上微调全流程，保留概念级性能。并提供多种轻量骨干（RepViT、TinyViT、EfficientViT）学生族。

Result: 在常用视频目标分割（VOS）基准上进行评测，与多种相关方法对比，展示出优异的性能-效率权衡，能在设备端实现概念分割与跟踪，并高保真复现教师行为。

Conclusion: PHD 能有效将 SAM3 的 PCS 能力迁移到轻量学生，实现端侧可用的高效概念分割与跟踪；Perceiver 式紧凑记忆是替代稠密记忆的关键。

Abstract: The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.

</details>


### [3] [WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion](https://arxiv.org/abs/2511.15874)
*Sajjad Pakdamansavoji,Yintao Ma,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

TL;DR: 提出一种针对遮挡场景的模型驱动6D位姿估计改进方案，通过聚焦可见区域的动态非均匀采样、多假设推理、迭代精化和遮挡增强训练，实现更稳健与高效的估计；并引入按可见度加权的评测指标，实验在ICBIN提升>5%、BOP提升>2%，推理提速约3倍。


<details>
  <summary>Details</summary>
Motivation: 现有对已见物体可通过逐类微调获得高精度，但对未见物体泛化差；常见的多阶段管线在遮挡下早期检测/分割易错，误差在后续阶段级联放大，影响最终位姿估计。需要一种对遮挡更鲁棒、能缓解顺序依赖误差传播的方法，并改进评测以公平衡量遮挡场景性能。

Method: 在模型驱动位姿估计框架中加入四项关键扩展：1) 动态非均匀致密采样：将计算聚焦于可见区域，降低遮挡引入的噪声；2) 多假设推理：保留多个按置信度排序的候选位姿，避免单一路径脆弱性；3) 迭代精化：多轮细化逐步提升精度；4) 遮挡导向的数据增强：训练阶段加入多种遮挡增强以提升鲁棒与泛化。同时提出按可见度加权的评测指标，更公平评估遮挡条件下的性能。

Result: 在ICBIN基准上准确率提升超过5%，在BOP数据集基准上提升超过2%，同时推理速度约提升3倍。

Conclusion: 通过在采样、推理、精化与训练增强四方面面向遮挡的改进，并引入可见度加权的评测指标，方法在准确性与效率上均优于现有模型驱动6D位姿估计管线，尤其在遮挡场景下更稳健且具更好泛化能力。

Abstract: Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.

</details>


### [4] [Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation](https://arxiv.org/abs/2511.15875)
*Lukas Arzoumanidis,Julius Knechtel,Jan-Henrik Haunert,Youness Dehbi*

Main category: cs.CV

TL;DR: 提出一种通过将历史地图风格迁移到矢量数据上来生成无限量高仿真合成历史地图的数据引导方法，并通过自动深度生成与手动随机退化两种手段注入不确定性，用于提升同质历史地图语义分割的域自适应性能。


<details>
  <summary>Details</summary>
Motivation: 历史地图常缺乏充足且高质量的标注数据，导致深度学习方法难以直接应用。人工标注耗时费力；纯合成数据缺乏足够的真实感和多样性，难以有效学习。因此需要一种既保留历史地图风格与噪声特征、又能大规模生成的数据引导方法，以支撑目标任务（如地类解译）。

Method: 1) 将原始同质历史地图语料库的制图风格迁移到矢量数据上，生成风格一致的合成地图；2) 设计两种不确定性与噪声建模策略：a) 自动的深度生成式方法（深度生成模型对视觉退化与噪声进行建模与注入）；b) 手动的随机退化流程（基于启发式的噪声、模糊、失真等退化仿真）；3) 以这些生成的数据作为训练集，采用Self-Constructing Graph Convolutional Network进行域自适应语义分割实验评估。

Result: 使用风格迁移与退化生成的数据进行训练，可在同质历史地图语料上的域自适应语义分割任务中取得有效提升，验证了数据引导方法的有效性与可用性。

Conclusion: 通过将历史地图风格迁移到矢量数据并注入数据依赖的不确定性，可在无需大量人工标注的情况下构建高质量训练集，从而提升同质历史地图的语义分割表现；自动与手动两套退化策略均有效，为历史地图自动分析提供可扩展的数据增殖途径。

Abstract: The automated analysis of historical documents, particularly maps, has drastically benefited from advances in deep learning and its success across various computer vision applications. However, most deep learning-based methods heavily rely on large amounts of annotated training data, which are typically unavailable for historical maps, especially for those belonging to specific, homogeneous cartographic domains, also known as corpora. Creating high-quality training data suitable for machine learning often takes a significant amount of time and involves extensive manual effort. While synthetic training data can alleviate the scarcity of real-world samples, it often lacks the affinity (realism) and diversity (variation) necessary for effective learning. By transferring the cartographic style of an original historical map corpus onto vector data, we bootstrap an effectively unlimited number of synthetic historical maps suitable for tasks such as land-cover interpretation of a homogeneous historical map corpus. We propose an automatic deep generative approach and a alternative manual stochastic degradation technique to emulate the visual uncertainty and noise, also known as data-dependent uncertainty, commonly observed in historical map scans. To quantitatively evaluate the effectiveness and applicability of our approach, the generated training datasets were employed for domain-adaptive semantic segmentation on a homogeneous map corpus using a Self-Constructing Graph Convolutional Network, enabling a comprehensive assessment of the impact of our data bootstrapping methods.

</details>


### [5] [Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes](https://arxiv.org/abs/2511.15884)
*Yintao Ma,Sajjad Pakdamansavoji,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

TL;DR: 提出Box6D：面向仓储盒类的类别级6D位姿估计方法，利用单幅RGB-D，通过二分搜索估计盒体尺寸、基于类别模板解算位姿，并用深度可行性筛选和早停剔除不合理假设，在真实仓储与公开基准上精度具竞争力，推理时间降约76%。


<details>
  <summary>Details</summary>
Motivation: 现有三类方法各有弊端：模型驱动需精细CAD、泛化差；基于少量参考的无模型方法在遮挡杂乱下易失败；类别级方法虽折中但常忽略环境与对象先验，难落地工业场景。仓储场景中大量规则盒体亟需高效鲁棒的位姿估计以支撑拣选与自动化。

Method: 提出Box6D，针对“盒类”对象：1) 输入单帧RGB-D；2) 通过快速二分搜索估计箱体三维尺寸；3) 使用类别CAD模板（非实例模型）进行位姿求解；4) 引入基于深度的一致性可行性过滤与早停策略，剔除不合理假设并降低计算；

Result: 在真实仓储场景与公开数据集上，Box6D取得与先进方法相当或更优的6D位姿精度，同时将推理时间降低约76%。

Conclusion: 针对仓储盒类对象的类别级位姿估计，通过尺寸搜索+类别模板+深度可行性过滤与早停实现高效且准确的6D位姿，兼顾工业可用性与性能。

Abstract: Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.

</details>


### [6] [RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification](https://arxiv.org/abs/2511.15923)
*Meilong Xu,Di Fu,Jiaxing Zhang,Gong Yu,Jiayu Zheng,Xiaoling Hu,Dongdi Zhao,Feiyang Li,Chao Chen,Yong Cao*

Main category: cs.CV

TL;DR: 提出一种两阶段自我改进范式，用自生成的文本“推理依据”先对VLM进行对齐再进行常规监督微调，在少样本/领域特定视频分类中显著优于直接SFT。


<details>
  <summary>Details</summary>
Motivation: VLM在领域特定的视频分类上受限于数据稀缺与“推理缺口”：复杂时空内容到抽象标签存在语义距离，直接用少量标签难以学到领域逻辑。

Method: 两阶段：1) 无需新增人工标注，先用提示引导VLM为每个视频生成细粒度、领域相关的文本“rationale”，并用这些自生成的解释对模型进行微调，使表示对齐目标领域的时空与语义规律；2) 再进行常规标签级监督微调（SFT），此时模型已具备领域推理能力，从而更高效地学习分类边界。

Result: 在多个数据集上广泛实验显示，相较直接SFT，该方法取得显著性能提升，表明自生成rationale作为中间监督能有效提高领域适配效率。

Conclusion: 利用自生成的领域推理作为中间监督可在无新增标注的前提下缩小VLM的“推理缺口”，提升领域特定视频分析性能；该范式是注释高效、可推广的适配策略。

Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.

</details>


### [7] [Boosting Medical Visual Understanding From Multi-Granular Language Learning](https://arxiv.org/abs/2511.15943)
*Zihan Li,Yiqing Wang,Sina Farsiu,Paul Kinahan*

Main category: cs.CV

TL;DR: 提出MGLL，一种用于多标签与跨粒度对齐的对比学习框架，在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: CLIP等方法主要做单标签、单粒度的图文对齐，难以应对如医学影像这类同时具有多标签与多层级文本标注的复杂场景。需要一种能同时处理多标签和跨粒度语义一致性的预训练方法。

Method: 提出MGLL（Multi-Granular Language Learning）：1）利用结构化多标签监督进行图文对齐；2）融合不同粒度（诊断描述、临床解释等）的文本；3）引入软标签与点对约束增强表示对齐；4）用平滑KL散度实现跨粒度一致性；以即插即用模块形式集成到现有视觉-语言模型中，计算效率可控；在自建的大规模多粒度数据上进行预训练。

Result: 在多个下游数据集上，MGLL优于现有SOTA方法，表现出更好的多标签识别与跨粒度对齐能力。

Conclusion: 多粒度语言学习能有效缓解单粒度对齐的局限，提升复杂领域（如医学影像）的多标签与跨粒度表示学习效果；方法通用、可插拔且高效。

Abstract: Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.

</details>


### [8] [Automated Interpretable 2D Video Extraction from 3D Echocardiography](https://arxiv.org/abs/2511.15946)
*Milos Vukadinovic,Hirotaka Ieki,Yuki Sahasi,David Ouyang,Bryan He*

Main category: cs.CV

TL;DR: 从3D超声体积中自动提取标准2D心超视图：用深度学习视图分类器+基于解剖地标与专家规则的启发式，达到临床可用质量，并保持空间标定与诊断信息；在多医院多评估中表现优异，代码与数据公开。


<details>
  <summary>Details</summary>
Motivation: 临床心超解读长期依赖多个标准2D视图，而3D超声虽能更快、更全面，但医生的工作流与判读经验仍以2D为主。需要一种方法把3D体积自动转化为标准2D视图，既保留3D采集效率，又不改变临床使用习惯，并能可靠支持诊断与测量。

Method: 1) 对3D心脏超声体积进行自动视图选择：训练深度学习视图分类器；2) 结合解剖地标与心脏科医师提供的规则，确定标准2D切面并重建对应视频；3) 对生成的2D视频进行多层验证：盲法专家评阅、下游AI异常检测模型（EchoPrime、PanEcho）以及测量模型（EchoNet-Measurement）的性能评估。

Result: - 盲法专家评估在来自两家医院的1,600段视频上达96%准确率；- 生成的2D视频能被现有AI模型用于异常检测与临床级解剖测量，性能保持；- 证实提取的2D视频保留空间标定和关键诊断特征。

Conclusion: 该方法可把3D心超无缝映射为临床熟悉的标准2D视图，兼具3D采集效率与2D解读可用性，且具备高准确度与临床测量可靠性；公开代码与数据（29例3D心超）促进复现与应用。

Abstract: Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .

</details>


### [9] [Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click](https://arxiv.org/abs/2511.15948)
*Raphael Ruschel,Hardikkumar Prajapati,Awsafur Rahman,B. S. Manjunath*

Main category: cs.CV

TL;DR: Click2Graph将用户的单次视觉提示（点/框）与视频全景分割、时空跟踪与语义关系推理结合，生成时序一致的场景图，实现可交互的PVSG。


<details>
  <summary>Details</summary>
Motivation: 现有VSGG闭环、难以引入人类指导；而可提示分割（如SAM2）可交互但缺乏语义与关系推理。需要一个能把用户交互与时空语义理解统一起来的框架，实现可控、可解释的视频场景理解。

Method: 提出Click2Graph：1）从单次提示对主体进行跨时间分割与跟踪；2）动态交互发现模块（DIDM）基于主体生成条件化的对象提示，自动发掘交互对象；3）语义分类头联合进行实体类别与谓词关系分类；最终输出时序一致的<主体, 客体, 谓词>三元组构成的全景视频场景图。

Result: 在OpenPVSG基准上取得强有力的起点性能，展示了在人类提示指导下进行PVSG的有效性，具备良好的可控性与可解释性。

Conclusion: 通过将人类提示与全景定位和关系推理融合，Click2Graph实现了首个交互式PVSG框架，能够从单次提示生成时序一致、可控且可解释的视频场景图，为用户引导的视频理解奠定了基础。

Abstract: State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.

</details>


### [10] [InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer](https://arxiv.org/abs/2511.15967)
*Muyao Yuan,Yuanhong Zhang,Weizhan Zhang,Lan Ma,Yuan Gao,Jiangyong Ying,Yudeng Xin*

Main category: cs.CV

TL;DR: InfoCLIP 通过信息论目标在微调阶段稳定并继承CLIP的视觉-文本对齐，用于开放词表语义分割：压缩噪声对齐、最大化与预训练对齐知识的互信息，避免过拟合并提升泛化。


<details>
  <summary>Details</summary>
Motivation: 现有将CLIP微调用于语义分割的方法在仅有少量已见类别上训练，易过拟合并破坏预训练的跨模态对齐，导致开放词表泛化能力下降；需要在不损害对齐的前提下，将CLIP的对齐知识有效迁移到像素级任务。

Method: 提出InfoCLIP，从信息论视角进行对齐知识迁移：1) 对预训练CLIP的像素-文本对齐进行“压缩”，以滤除由图文级训练带来的粗粒度局部语义噪声；2) 通过最大化预训练对齐知识与微调模型对齐表示之间的互信息，传递紧凑的局部语义关系；整体作为两个互信息驱动的目标，配合常规分割训练进行微调。

Result: 在多种开放词表分割基准上进行广泛评测，InfoCLIP在保持或提升视觉-语言对齐稳定性的同时，显著优于现有微调方法，展现了在不对称知识迁移场景下的适应性与性能优势。

Conclusion: 基于互信息的对齐压缩与对齐保持可稳定微调CLIP，避免过拟合与对齐退化，从而提升开放词表语义分割的性能与泛化能力；该范式对跨模态任务中的不对称知识迁移具有普适价值。

Abstract: Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.

</details>


### [11] [Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2511.15968)
*Jingru Zhang,Saed Moradi,Ashirbani Saha*

Main category: cs.CV

TL;DR: 提出一种在乳腺超声肿瘤分割的多任务学习中引入一致性正则化的方法，通过可微的BI-RADS启发形态学特征来缓解分割与分类之间的破坏性干扰，在多个外部数据集上显著提升分割泛化（Dice: 0.81/0.66/0.69 vs 0.59/0.56/0.49，p<0.001），并在UDIAT上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多任务学习常受任务间冲突影响，尤其是分割与分类共同训练时会出现负迁移，导致泛化变差。乳腺超声分割在跨中心、跨设备数据上易退化，需要一个机制在保持多任务收益的同时抑制任务干扰、提升外部泛化。

Method: 在多任务（分割+分类）框架中加入一致性正则化：利用BI-RADS启发的、可微的形态学特征（如边界形状、光滑度、轮廓不规则度等）约束分割输出与分类表示在形态学空间的一致性，从而减少任务梯度冲突。训练于BrEaST（波兰），外部验证于UDIAT（西班牙）、BUSI（埃及）、BUS-UCLM（西班牙）。

Result: 相较基线多任务法，所提方法在三个外部数据集的分割Dice显著提升：UDIAT 0.81 vs 0.59、BUSI 0.66 vs 0.56、BUS-UCLM 0.69 vs 0.49（p<0.001），并在UDIAT上达到当前最优。

Conclusion: 将基于BI-RADS的可微形态学一致性正则引入多任务学习能有效缓解任务干扰，显著提升乳腺超声肿瘤分割的跨域泛化，并实现SOTA表现。

Abstract: Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.

</details>


### [12] [UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition](https://arxiv.org/abs/2511.15984)
*Xinyu Nan,Lingtao Mao,Huangyu Dai,Zexin Zheng,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.CV

TL;DR: 提出一个检测引导的生成式框架，用BART按层次生成类别与属性token，显著提升大规模电商场景的细粒度识别与统一推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局相似度，难以区分细粒度类别、捕获类特定属性多样性，尤其在电商大规模多类别多属性场景下表现不足，需要统一处理检测、分类与属性识别的框架。

Method: 以目标检测为前端，对每个检测到的实例提取精炼的ROI特征；在此基础上使用BART生成器，按“由粗到细”的序列生成语义token：先是类别层级（如大类-中类-细类），再生成属性-取值对；同时支持以属性为条件的属性识别（property-conditioned）。

Result: 在大规模私有电商数据集与开源数据集上，优于基于相似度的流水线与多阶段分类系统，在细粒度分类与属性一致性方面取得显著提升。

Conclusion: 检测引导+序列生成的统一框架能更好地建模类别层级与属性结构，提升细粒度识别与统一推理的连贯性，适用于电商等大规模视觉语义理解任务。

Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.

</details>


### [13] [Fairness in Multi-modal Medical Diagnosis with Demonstration Selection](https://arxiv.org/abs/2511.15986)
*Dawei Li,Zijian Gu,Peng Wang,Chuhan Song,Zhen Tan,Mohan Zhang,Tianlong Chen,Yu Tian,Song Wang*

Main category: cs.CV

TL;DR: 论文提出一种在不微调模型的情况下，通过上下文学习改进医疗图像推理公平性的策略。核心是公平感知示例选择（FADS），通过聚类抽样构建在语义相关同时人口统计均衡的示例集，在多个数据集上显著降低性别、种族、族裔差异且不损失准确率。


<details>
  <summary>Details</summary>
Motivation: MLLM 在医疗图像推理上效果好，但不同人群间的偏差仍严重。现有去偏方法依赖大量标注或微调，不适用于基础模型规模和医疗场景的可扩展性需求，因此需要一种轻量、无需微调、数据高效的公平性提升方法。

Method: 将上下文学习用于公平性：分析传统示例选择（DS）如何因人口统计不均衡而引入不公。提出 FADS：先对候选样本做语义聚类，再在每个簇内进行按人口统计属性均衡的抽样，以构建既语义相关又人口均衡的演示示例；用于提示 MLLM 进行推理。

Result: 在多个医疗影像基准上，FADS 相比常规 DS 一致地降低性别、种族、族裔相关的性能差距，同时保持总体准确率。

Conclusion: 公平感知的上下文学习可作为提升医疗图像推理公平性的可扩展、数据高效且无需微调的方案。

Abstract: Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.

</details>


### [14] [Exploiting Inter-Sample Information for Long-tailed Out-of-Distribution Detection](https://arxiv.org/abs/2511.16015)
*Nimeshika Udayangani,Hadi M. Dolatabadi,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

TL;DR: 提出一种基于图表示与GCN的长尾分布场景下OOD检测方法：先以预训练模型特征构图，经高斯化校正激活分布，再用GCN细化特征空间，显著降低FPR并提升尾部类ID分类精度，在CIFAR10/100-LT与ImageNet-LT上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 长尾分布使现有OOD方法在尾部类上误检率高、ID分类差，且预训练模型的激活分布与下游数据不匹配，导致特征空间不适于OOD检测。需要一种能利用样本间关系、并校正分布偏差的方法。

Method: 1) 用预训练模型的特征初始化样本图（节点为样本，边基于特征相似度）；2) 针对预训练与训练数据分布差异，对激活进行高斯化，使其更接近标准正态；3) 以GCN在图上进行特征传播与 refinement，获得更判别的特征空间；4) 在该空间进行长尾OOD检测与ID分类。

Result: 在CIFAR10-LT、CIFAR100-LT、ImageNet-LT三个基准上，相比现有方法显著降低FPR，同时提升长尾类（尾部类）ID分类准确率，达到新的SOTA。

Conclusion: 利用图结构挖掘样本间关系并结合激活高斯化与GCN，可以在长尾场景中显著提升OOD检测与尾部类识别性能，证明了分布校正与图优化对鲁棒泛化的有效性。

Abstract: Detecting out-of-distribution (OOD) data is essential for safe deployment of deep neural networks (DNNs). This problem becomes particularly challenging in the presence of long-tailed in-distribution (ID) datasets, often leading to high false positive rates (FPR) and low tail-class ID classification accuracy. In this paper, we demonstrate that exploiting inter-sample relationships using a graph-based representation can significantly improve OOD detection in long-tailed recognition of vision datasets. To this end, we use the feature space of a pre-trained model to initialize our graph structure. We account for the differences between the activation layer distribution of the pre-training vs. training data, and actively introduce Gaussianization to alleviate any deviations from a standard normal distribution in the activation layers of the pre-trained model. We then refine this initial graph representation using graph convolutional networks (GCNs) to arrive at a feature space suitable for long-tailed OOD detection. This leads us to address the inferior performance observed in ID tail-classes within existing OOD detection methods. Experiments over three benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that our method outperforms the state-of-the-art approaches by a large margin in terms of FPR and tail-class ID classification accuracy.

</details>


### [15] [Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion](https://arxiv.org/abs/2511.16020)
*Dingkun Zhou,Patrick P. K. Chan,Hengxu Wu,Shikang Zheng,Ruiqi Huang,Yuanjie Zhao*

Main category: cs.CV

TL;DR: 该论文提出一种针对监控场景的人体检测模型的可穿戴对抗样本生成框架，通过序列级优化在长视频中持续隐匿行人，数字与物理环境均有效。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴对抗攻击多按帧优化纹理，无法在长时间视频中面对运动、姿态与布料形变时保持稳定隐蔽；实际部署需要自然、可打印、跨视角与跨模型鲁棒的方案以降低监控中的安全与隐私风险。

Method: 1) 将商品服饰图像映射到UV空间，并用调色板+控制点进行紧凑参数化，并通过ICC锁定确保颜色可打印；2) 构建物理真实感的“人-衣”仿真管线，模拟行走、视角、布料动力学和光照变化；3) 以期望-变换(EoT)目标并加入时间权重，在整段序列上优化控制点，最小化检测置信度；4) 采用升华打印制作真实服饰进行物理测试。

Result: 在大量实验中，所生成的衬衫、裤子、帽子的对抗纹理在长序列中保持强而稳定的隐身效果，对多视角具有高鲁棒性，并表现出优于现有方法的跨模型迁移；实体打印的服饰在室内外录制中均实现可靠抑制检测。

Conclusion: 序列级、物理约束的可穿戴对抗纹理优化能够在真实世界中持续规避人体检测，兼顾自然外观、可打印性与跨场景鲁棒性，验证了在监控环境下的实际可行性。

Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.

</details>


### [16] [Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution](https://arxiv.org/abs/2511.16024)
*Xiao He,Zhijun Tu,Kun Cheng,Mingrui Zhu,Jie Hu,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出在真实图像超分(Real-ISR)中，将稀疏MoE思想融入LoRA，构成“Mixture-of-Ranks (MoR)”的单步重建框架：把LoRA的每个秩当作专家，通过CLIP驱动的退化估计与负载均衡策略，动态选择专家，兼顾共享常识与个性化退化适配，在等算力下提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有Real-ISR常以LoRA微调扩散模型，但属于“致密”结构：对复杂、异质的真实退化样本难以自适应，且在相同计算预算下无法实现输入间的知识共享和选择性计算；MoE在大模型中展现了稀疏路由与参数共享优势，值得引入ISR以提升适配性与效率。

Method: 1) 结构：提出Mixture-of-Ranks (MoR)，把LoRA中每个rank视为独立专家；部分固定位置的rank设为共享专家以保留常识特征并减少路由冗余。2) 路由：引入基于CLIP嵌入与预定义正负文本对的退化估计模块，计算相对退化分数，动态指导专家激活。3) 计算自适应：设置zero-expert槽位并提出退化感知的负载均衡损失，依据退化程度动态调整激活专家数量，实现算力按需分配。4) 训练/推理：单步超分，稀疏选择少量rank专家进行组合重建。

Result: 在广泛实验中，该框架表现出有效性与SOTA性能（相较密集Real-ISR和LoRA微调基线在质量与效率上均有提升），并能在等计算预算下更好地处理多样化真实退化。

Conclusion: 将MoE稀疏路由与LoRA秩级专家化结合，能在Real-ISR中实现更灵活的知识重组与退化自适应；通过CLIP引导的退化估计与负载均衡，使专家数与计算按退化难度动态匹配，最终在效率与重建质量上取得领先。

Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.

</details>


### [17] [Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning](https://arxiv.org/abs/2511.16026)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashur,Ahmed Elshinnawy*

Main category: cs.CV

TL;DR: 提出用激光散斑图结合深度学习实现材料识别，用于激光切割过程的监测与控制；在更换激光颜色情况下仍保持高准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 激光切割产生粉尘与烟雾，需实时监测以保障安全与效率。基于散斑的在线感知可区分材料，但以往方法对激光颜色敏感、泛化差，难以在不同设备/工况下可靠使用。

Method: 采集不同材料表面的激光散斑图像，构建数据集；训练卷积神经网络进行材料分类；特别检验模型在更换激光颜色（波长）条件下的识别能力；用训练/验证集准确率与对30种材料的独立测试集F1分数评估。

Result: CNN在训练集准确率98.30%，验证集96.88%；在包含30种材料、3000张新图像的数据上F1=0.9643；在改变激光颜色的情况下仍能保持高分类性能。

Conclusion: 基于散斑图的深度学习分类能为材料感知型激光切割提供鲁棒、准确的在线识别方案，缓解以往对激光颜色敏感的问题，有助于实现安全高效的过程监测与控制。

Abstract: Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.

</details>


### [18] [CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis](https://arxiv.org/abs/2511.16030)
*Zijian Wu,Mingfeng Jiang,Zidian Lin,Ying Song,Hanjie Ma,Qun Wu,Dongping Zhang,Guiyang Pu*

Main category: cs.CV

TL;DR: 提出CuriGS：在稀疏视角下为3D Gaussian Splatting引入“学生视角”的课程式训练，通过逐步增大的姿态扰动、质量评估与晋升机制，稳定扩充有效训练视图，显著提升渲染与几何一致性。


<details>
  <summary>Details</summary>
Motivation: 3DGS在稀疏视角下受监督不足与视角覆盖受限，易过拟合且几何不稳。需要一种既能扩充有效监督、又能保持训练稳定性的机制，以在少视图条件下获得高保真与一致的重建。

Method: 以真实相机位姿为“教师”，在其周围采样多组不同扰动强度的“学生视角”。采用课程学习：从低扰动到高扰动逐级解锁，每轮从当前激活级别随机采样学生辅助训练。对学生样本施加深度相关与互正则化约束；用融合SSIM、LPIPS及图像质量指标的多信号度量评估学生表现。按教师与扰动级别定期保留最佳学生，并将达阈值者晋升到训练集，实现稳定的数据增广。

Result: 在多种合成与真实稀疏视角场景上，相比SOTA基线，CuriGS在渲染保真度与几何一致性两方面均取得更优表现。

Conclusion: 通过课程式学生视角生成、正则与多信号筛选的联合框架，CuriGS有效缓解稀疏监督下的过拟合与覆盖不足问题，为稀疏视角的3DGS重建提供了稳定且高质的训练流程。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/

</details>


### [19] [Crossmodal learning for Crop Canopy Trait Estimation](https://arxiv.org/abs/2511.16031)
*Timilehin T. Ayanlade,Anirudha Powadi,Talukder Z. Jubery,Baskar Ganapathysubramanian,Soumik Sarkar*

Main category: cs.CV

TL;DR: 提出一种跨模态学习方法，用卫星图像生成“类UAV”细节表征，显著提升作物冠层性状（如产量、氮素）预测效果。


<details>
  <summary>Details</summary>
Motivation: 卫星影像覆盖广但分辨率受限，不利于精细到微小地块的现代农业管理；UAV分辨率高但成本与时空覆盖受限。需要一种方法把卫星的广域优势与UAV的细节优势融合，用于作物表型监测与预测。

Method: 收集约共配准的卫星—UAV影像对，来自美国玉米带5地点、84个杂交玉米品种的重复试验小区。训练一个模型学习两种传感模态之间细粒度光谱—空间对应关系，用卫星输入生成具有UAV级细节的表示，用于下游性状估计。

Result: 由卫星输入生成的“UAV样”表示在多项下游任务上稳定优于直接用原始卫星影像，包括产量与氮素预测等。

Conclusion: 跨模态对应学习可有效弥合卫星与UAV在农业监测中的分辨率与细节差距，为高分辨率性状估计与精准农业提供可扩展方案。

Abstract: Recent advances in plant phenotyping have driven widespread adoption of multi sensor platforms for collecting crop canopy reflectance data. This includes the collection of heterogeneous data across multiple platforms, with Unmanned Aerial Vehicles (UAV) seeing significant usage due to their high performance in crop monitoring, forecasting, and prediction tasks. Similarly, satellite missions have been shown to be effective for agriculturally relevant tasks. In contrast to UAVs, such missions are bound to the limitation of spatial resolution, which hinders their effectiveness for modern farming systems focused on micro-plot management. In this work, we propose a cross modal learning strategy that enriches high-resolution satellite imagery with UAV level visual detail for crop canopy trait estimation. Using a dataset of approximately co registered satellite UAV image pairs collected from replicated plots of 84 hybrid maize varieties across five distinct locations in the U.S. Corn Belt, we train a model that learns fine grained spectral spatial correspondences between sensing modalities. Results show that the generated UAV-like representations from satellite inputs consistently outperform real satellite imagery on multiple downstream tasks, including yield and nitrogen prediction, demonstrating the potential of cross-modal correspondence learning to bridge the gap between satellite and UAV sensing in agricultural monitoring.

</details>


### [20] [LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets](https://arxiv.org/abs/2511.16037)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim,Qianru Sun*

Main category: cs.CV

TL;DR: 用LLM生成食物标题与配料，联合图文对齐到共享嵌入空间，再用对齐特征识别，跨域、长尾与细粒度均提升。


<details>
  <summary>Details</summary>
Motivation: 网络抓取训练图像与真实用户拍摄存在显著域偏移；真实食物数据长尾分布严重；不同菜品间差异细微、难以区分，需要同时解决域适配、长尾与细粒度识别难题。

Method: 1) 利用LLM对食物图像进行语义解析，生成标题与配料文本；2) 将多域食物图像与由LLM生成的文本映射到共享嵌入空间，通过对齐学习最大化成对相似度（类似对比学习/跨模态匹配）；3) 使用对齐后的图文特征进行食物类别识别。

Result: 在两个食物数据集上，相比分别针对长尾分布、域适配与细粒度分类的现有方法，该框架实现全面超越。

Conclusion: 引入LLM生成的结构化文本与跨模态对齐，可同时缓解域偏移、长尾与细粒度难题，显著提升食物识别表现。

Abstract: Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.

</details>


### [21] [AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers](https://arxiv.org/abs/2511.16047)
*Boxun Xu,Yu Wang,Zihu Wang,Peng Li*

Main category: cs.CV

TL;DR: 提出AMS-KV：一种面向视觉自回归（VAR）下一尺度预测的自适应KV缓存策略，在不显著损失生成质量下，最高减少84.83%缓存与60.48%注意力时延，并将批大小从128稳定扩展到256。


<details>
  <summary>Details</summary>
Motivation: 下一尺度预测的VAR生成需要跨多尺度长上下文注意力，KV缓存随尺度数急剧膨胀，造成显存与时延瓶颈；现有LLM的KV缓存策略不直接适用于多尺度图像生成，因此亟需一种面向尺度特性的高效KV缓存设计。

Method: 系统分析跨尺度注意力贡献与KV相似性：发现局部（邻近）尺度最关键，最粗尺度少量缓存可稳定多尺度生成；细尺度间在“缓存高效层”KV相似性强，而“缓存高需求层”相似性弱。基于此提出AMS-KV：优先缓存凝缩（最粗）与本地尺度的KV；通过跨尺度相似性度量识别缓存高需求层并针对性分配缓存，提升缓存利用与计算效率。

Result: 在与原生下一尺度VAR对比中，KV缓存占用最多降至15.17%（降幅84.83%），自注意力延迟降低60.48%；在基线VAR-d30于batch=128发生OOM时，AMS-KV可稳定扩展到batch=256并提高吞吐。

Conclusion: 面向下一尺度VAR的尺度自适应KV缓存策略可在保证生成质量的前提下显著压缩内存与时延，提升可扩展性；尺度选择与层级相似性感知是设计高效视觉AR缓存策略的关键。

Abstract: Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.

</details>


### [22] [LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving](https://arxiv.org/abs/2511.16049)
*Pei Liu,Songtao Wang,Lang Zhang,Xingyue Peng,Yuandong Lyu,Jiaxin Deng,Songxin Lu,Weiliang Ma,Xueyang Zhang,Yifei Zhan,XianPeng Lang,Jun Ma*

Main category: cs.CV

TL;DR: LiSTAR提出在激光雷达原生几何上进行可控高保真4D点云生成，显著提升重建、预测与条件生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于笛卡尔体素/图像的方法会引入量化误差，难以保真地表达球面采样的LiDAR；点云时序稀疏导致动态建模困难；同时缺乏高效、可控的4D条件生成机制以满足自动驾驶大规模仿真需求。

Method: 1) 几何：提出混合-柱-球(HCS)表示，贴合传感器射线几何，减少量化伪影；2) 时空建模：Spatio-Temporal Attention with Ray-Centric Transformer(START)，沿射线建模特征随时间的演化，强化时序一致性；3) 可控生成：设计与4D点云对齐的体素布局作为条件输入，并提出离散化的Masked Generative START(MaskSTART)，学习紧凑的token表示，实现高分辨率、布局引导的组合式生成。

Result: 在4D LiDAR重建、预测、条件生成三任务上达到SOTA：生成MMD下降76%，重建IoU提升32%，预测L1中位数降低50%。

Conclusion: 在传感器原生几何与射线中心的时空建模下，LiSTAR实现高保真且可控的4D点云生成，为构建逼真、可控的自动驾驶仿真提供强有力基础。

Abstract: Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.

</details>


### [23] [VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning](https://arxiv.org/abs/2511.16077)
*Zishan Xu,Yifu Guo,Yuquan Lu,Fengyu Yang,Junxin Li*

Main category: cs.CV

TL;DR: 提出VideoSeg-R1，将强化学习引入视频“推理+分割”，通过分解为指代图像分割与视频掩码传播，并用层级文本引导采样、显式推理链与SAM2/XMem传播，实现SOTA的可解释视频推理分割，且自适应控制推理长度以兼顾效率与精度。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理分割依赖监督微调，难以泛化到OOD场景且缺少显式推理过程；需要一种既具泛化能力又可解释、可在复杂视频任务上稳健表现的方法。

Method: 提出VideoSeg-R1：1) 层级文本引导的帧采样器，模拟人类注意，选择关键帧；2) 推理模型输出空间线索与显式推理链，采用强化学习优化推理策略；3) 分割与传播阶段结合SAM2与XMem完成高质量掩码传播；并引入基于任务难度的自适应推理长度控制机制，以权衡效率和精度。

Result: 在多项基准上进行广泛评测，VideoSeg-R1在复杂视频推理与分割任务上取得SOTA性能；显式推理链提升可解释性，自适应推理长度提升效率与准确率。

Conclusion: 将强化学习与解耦架构引入视频推理分割是有效路径；通过文本引导采样、显式推理与强力分割传播模块的协同，VideoSeg-R1实现了对复杂视频场景的强泛化、可解释和高效精确分割。

Abstract: Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.

</details>


### [24] [SpectralTrain: A Universal Framework for Hyperspectral Image Classification](https://arxiv.org/abs/2511.16084)
*Meihua Zhou,Liping Yu,Jiawei Cai,Wai Kin Fung,Ruiguo Hu,Jiarui Zhao,Wenzhuo Liu,Nan Wan*

Main category: cs.CV

TL;DR: 提出SpectralTrain：将课程学习与基于PCA的光谱降采样结合，用于高光谱图像分类的通用高效训练框架，在多数据集上实现2–7倍训练加速，精度仅小幅下降，且与多种架构/优化器兼容。


<details>
  <summary>Details</summary>
Motivation: HSI分类常需处理高维光谱与大规模数据，训练成本高、难以在实际遥感场景部署；现有方法多聚焦模型结构创新，较少从训练策略角度系统降低计算开销并保持泛化。

Method: 在训练早期用PCA对光谱维做降采样，保留最重要主成分，随后按课程学习策略逐步增加光谱通道数（提高光谱复杂度），让模型先学易后难；该流程与具体网络、优化器、损失函数解耦，可直接应用于经典与SOTA骨干；在Indian Pines、Salinas-A与新引入的CloudPatch-7上进行验证。

Result: 在不同空间尺度、光谱特性与任务领域上具备强泛化：训练时间稳定缩短2–7倍，准确率仅有小到中等幅度下降（依赖骨干结构），云分类任务显示训练策略优化能显著提升效率且保持竞争性能。

Conclusion: SpectralTrain通过“PCA降维+课程式逐步增谱”的通用训练框架，有效降低HSI训练计算成本并保持良好精度与跨域泛化，表明优化训练流程可与架构设计互补，具有在气候与遥感应用中的推广潜力。

Abstract: Hyperspectral image (HSI) classification typically involves large-scale data and computationally intensive training, which limits the practical deployment of deep learning models in real-world remote sensing tasks. This study introduces SpectralTrain, a universal, architecture-agnostic training framework that enhances learning efficiency by integrating curriculum learning (CL) with principal component analysis (PCA)-based spectral downsampling. By gradually introducing spectral complexity while preserving essential information, SpectralTrain enables efficient learning of spectral -- spatial patterns at significantly reduced computational costs. The framework is independent of specific architectures, optimizers, or loss functions and is compatible with both classical and state-of-the-art (SOTA) models. Extensive experiments on three benchmark datasets -- Indian Pines, Salinas-A, and the newly introduced CloudPatch-7 -- demonstrate strong generalization across spatial scales, spectral characteristics, and application domains. The results indicate consistent reductions in training time by 2-7x speedups with small-to-moderate accuracy deltas depending on backbone. Its application to cloud classification further reveals potential in climate-related remote sensing, emphasizing training strategy optimization as an effective complement to architectural design in HSI models. Code is available at https://github.com/mh-zhou/SpectralTrain.

</details>


### [25] [Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments](https://arxiv.org/abs/2511.16091)
*Renxiang Xiao,Wei Liu,Yuanfan Zhang,Yushuai Chen,Jinming Chen,Zilu Wang,Liang Hu*

Main category: cs.CV

TL;DR: Rad-GS是一种面向公里级户外环境的4D雷达-相机SLAM系统，采用3D Gaussian作为可微空间表示，融合雷达点云与多普勒信息进行动态图像掩膜、利用不同步图像进行全局纹理优化，并通过全局八叉树与高效Gaussian管理降低噪声与内存，最终在定位与重建上达到与相机/激光雷达方案相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D Gaussian或NeRF式重建多依赖相机或LiDAR，易受光照、遮挡、纹理贫乏影响，且大场景内存与噪声控制困难。4D毫米波雷达具备在恶劣环境鲁棒与携带多普勒速度信息的优势，但如何与图像联合、抑制动态造成的渲染伪影、并在公里级场景中高效存储与优化仍未解决。

Method: 1) 以3D Gaussian为核心表示，联合雷达原始点云及多普勒速度，得到几何增强点云，并驱动同步图像的动态物体掩膜，减少渲染伪影、提升定位精度；2) 将不同步的图像帧用于全局优化3D Gaussian，提升纹理一致性与新视角合成质量；3) 设计全局八叉树结构与有针对性的Gaussian原语管理策略，实现去噪与显著内存压缩，适配大规模场景。

Result: 在大量实验与消融中，Rad-GS在定位与重建精度上与基于相机或LiDAR的传统3D Gaussian方法相当；在大规模户外环境下显著降低内存占用并抑制噪声，提升新视角渲染保真度。真实公里级数据集上的重建结果验证其大场景实用性。

Conclusion: 4D毫米波雷达可与相机有效融合到3D Gaussian管线中，实现鲁棒的公里级户外SLAM与重建；基于多普勒的动态掩膜、不同步图像的全局优化以及八叉树+原语管理的结构化表示，是实现高精度、低噪声、低内存的大规模重建的关键。

Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.

</details>


### [26] [T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs](https://arxiv.org/abs/2511.16107)
*Shao-Jun Xia,Huixin Zhang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 提出T2T-VICL协同管线，使统一视觉-语言模型在跨任务场景下实现视觉类比式的in-context learning，并在多项跨任务基准上达到了领先或次优表现。


<details>
  <summary>Details</summary>
Motivation: 现有VICL多假设视觉提示与目标图像来自同一任务，难以评估和利用VLM在“跨任务”设定（如去噪提示迁移到去雨任务）的能力。需要一种方法与数据集来刻画不同低级视觉任务间的差异，并验证VLM是否能在只给出跨任务示例的情况下完成目标任务。

Method: 1) T2T-VICL：构建“任务到任务”的全协作流程。2) 文本提示生成与选择机制：自动生成并筛选能隐式刻画两类低级视觉任务差异的文本描述，用作跨任务提示；据此构建首个跨任务VICL数据集。3) 推理框架：融合感知质量评分驱动的推理（perceptual score-based reasoning）与传统客观指标，指导VLM在跨任务设定下进行选择与生成，完成目标任务。

Result: 在九个跨任务场景取得顶尖（top-tier）结果，在另外十个场景取得第二梯队（second-tier）表现，证明方法在广泛跨任务设置下有效。

Conclusion: 通过文本化刻画任务差异与评分融合推理，统一VLM可在视觉跨任务的ICL中表现强劲；工作突破了VICL的跨任务边界，并提供了数据与流程为后续研究奠基。

Abstract: In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.

</details>


### [27] [Clustered Error Correction with Grouped 4D Gaussian Splatting](https://arxiv.org/abs/2511.16112)
*Taeho Kang,Jaeyeon Park,Kyungjin Lee,Youngki Lee*

Main category: cs.CV

TL;DR: 提出一种改进4D Gaussian Splatting的动态场景重建方法，通过“椭圆误差聚类+纠错加点”定位并修正动态区域，以及“分组4DGS”强化高斯与动态物体的一致映射，大幅提升时间一致性与感知质量，在Technicolor数据集PSNR提升0.39dB，并公开代码。


<details>
  <summary>Details</summary>
Motivation: 现有4DGS在动态场景中易出现像素对应歧义与动态区域致密化不足，导致时间抖动、遮挡处理不佳与渲染伪影。需要一种既能精准发现并修复动态错误、又能在表示层面保证高斯与真实动态对象一致性的方案。

Method: 1) 椭圆误差聚类与纠错加点：将渲染误差分为缺色与遮挡两类，利用跨视角颜色一致性进行判别；针对缺色通过反投影补点，针对遮挡通过前景分裂与初始化新高斯来修正，并用椭圆误差聚类定位动态区域、指导高斯初始化与优化。2) 分组4D高斯：按对象/运动一致性对高斯进行分组，强化组内一致的映射与优化，使高斯与动态物体边界和运动更匹配，提升时序一致性。

Result: 在Neural 3D Video与Technicolor数据集上取得SOTA的感知渲染质量与更好的时间一致性；在Technicolor Light Field上PSNR提升0.39dB；可视化显示高斯与动态对象对齐更好，纠错模块能识别错误并正确初始化新高斯。

Conclusion: 通过误差类型化纠错与分组4DGS，相较现有方法显著增强动态场景重建的稳健性与时序一致性，兼顾感知质量与定量指标，且方法可落地复现（代码开源）。

Abstract: Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.

</details>


### [28] [Decoupling Complexity from Scale in Latent Diffusion Model](https://arxiv.org/abs/2511.16117)
*Tianxiong Zhong,Xingye Tian,Xuebo Wang,Boyuan Jiang,Xin Tao,Pengfei Wan*

Main category: cs.CV

TL;DR: 提出DCS-LDM，将视觉生成中的“信息复杂度”与“尺度”(分辨率/帧率)解耦，在固定潜空间内支持任意尺度解码与渐进式粗到细生成，性能可比SOTA且更灵活。


<details>
  <summary>Details</summary>
Motivation: 现有潜扩散把更高分辨率/帧率等同于需要更多潜表示，实际决定潜容量的是内容复杂度，尺度只是上界。为避免无谓的计算开销与尺度耦合，需构建与尺度无关、按内容复杂度自适应的潜空间。

Method: 构建分层、与尺度无关的潜空间：用多级token编码样本的结构与细节，分别对应低频结构与高频细节；在固定数量的潜token下，可解码到任意分辨率与帧率。该设计允许按层次调度计算，实现灵活的计算-质量折中，并支持从粗到细的渐进生成。

Result: 在多种任务/尺度上，DCS-LDM与当前SOTA方法性能相当，同时能够在不同分辨率、帧率与质量需求下灵活生成，展现出更好的可伸缩性与效率。

Conclusion: 通过解耦复杂度与尺度、构建分层潜空间，DCS-LDM在保持SOTA级别质量的同时实现统一表示、任意尺度解码与渐进式生成，带来可控的计算-质量权衡与更强的跨尺度适应性。

Abstract: Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.

</details>


### [29] [VTinker: Guided Flow Upsampling and Texture Mapping for High-Resolution Video Frame Interpolation](https://arxiv.org/abs/2511.16124)
*Chenyang Wu,Jiayi Fu,Chun-Le Guo,Shuhao Han,Chongyi Li*

Main category: cs.CV

TL;DR: 提出VTinker：通过引导式光流上采样与纹理映射缓解高分辨率VFI中光流上采样模糊与像素级合成伪影，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 高分辨率视频插帧需要精确运动估计，但大位移与计算开销迫使方法在低分辨率估计光流再上采样，易造成边缘模糊/马赛克；且细粒度运动在低分辨率难以捕捉，导致错配与像素级合成时的重影与不连续。

Method: 提出VTinker管线：1) Guided Flow Upsampling (GFU) 以输入帧为引导，对低分辨率双向光流进行结构感知上采样，增强流场边缘与细节，缓解双线性上采样的模糊。2) Texture Mapping：先基于上采样光流生成中间代理帧（intermediate proxy），以该代理作为线索，从输入帧中选择清晰纹理块并映射到代理上，再通过重建模块融合生成最终插帧。

Result: 在多项基准上取得SOTA表现（文中称“Extensive experiments demonstrate ... state-of-the-art”），代码开源于https://github.com/Wucy0519/VTinker。

Conclusion: 通过GFU提升光流上采样质量、通过Texture Mapping避免像素级融合引发的重影/断裂，VTinker在高分辨率VFI中兼顾精度与观感，达到SOTA。

Abstract: Due to large pixel movement and high computational cost, estimating the motion of high-resolution frames is challenging. Thus, most flow-based Video Frame Interpolation (VFI) methods first predict bidirectional flows at low resolution and then use high-magnification upsampling (e.g., bilinear) to obtain the high-resolution ones. However, this kind of upsampling strategy may cause blur or mosaic at the flows' edges. Additionally, the motion of fine pixels at high resolution cannot be adequately captured in motion estimation at low resolution, which leads to the misalignment of task-oriented flows. With such inaccurate flows, input frames are warped and combined pixel-by-pixel, resulting in ghosting and discontinuities in the interpolated frame. In this study, we propose a novel VFI pipeline, VTinker, which consists of two core components: guided flow upsampling (GFU) and Texture Mapping. After motion estimation at low resolution, GFU introduces input frames as guidance to alleviate the blurring details in bilinear upsampling flows, which makes flows' edges clearer. Subsequently, to avoid pixel-level ghosting and discontinuities, Texture Mapping generates an initial interpolated frame, referred to as the intermediate proxy. The proxy serves as a cue for selecting clear texture blocks from the input frames, which are then mapped onto the proxy to facilitate producing the final interpolated frame via a reconstruction module. Extensive experiments demonstrate that VTinker achieves state-of-the-art performance in VFI. Codes are available at: https://github.com/Wucy0519/VTinker.

</details>


### [30] [How Noise Benefits AI-generated Image Detection](https://arxiv.org/abs/2511.16136)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Kai Zeng,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出PiN-CLIP：在CLIP特征空间注入“正向激励”噪声以抑制捷径、增强取证线索，从而显著提升AI生成图像检测的跨分布泛化；在42模型开放世界数据集上达SOTA，平均准确率提升5.4点。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在跨分布场景表现差，原因在于训练中依赖脆弱的伪相关捷径（shortcut）。作者观察到：在特征空间施加小扰动能削弱捷径主导性，提示可通过可控的特征扰动来引导模型关注稳健取证线索。

Method: 提出PiN-CLIP：1) 以CLIP为基，构建联合训练框架，包含噪声生成器与检测网络；2) 在特征空间引入“正向激励”变分目标，学习能鼓励模型提升判别性的噪声；3) 通过视觉与类别语义特征的跨注意力融合生成噪声；4) 将噪声注入CLIP视觉编码器的特征，微调编码器，使对捷径敏感的方向被抑制、稳定取证线索被放大；5) 联合优化噪声生成器与检测器，获得更鲁棒的伪迹表示。

Result: 在包含42种生成模型的开放世界合成图像数据集上，PiN-CLIP取得新的SOTA，相比现有方法平均准确率提升5.4；在跨模型、跨分布场景中均表现更稳健。

Conclusion: 通过在CLIP特征空间施加语义引导的正向激励噪声并联合优化，可系统性地减少捷径依赖、增强可泛化的取证特征，显著提升AI生成图像检测的鲁棒性与泛化能力。

Abstract: The rapid advancement of generative models has made real and synthetic images increasingly indistinguishable. Although extensive efforts have been devoted to detecting AI-generated images, out-of-distribution generalization remains a persistent challenge. We trace this weakness to spurious shortcuts exploited during training and we also observe that small feature-space perturbations can mitigate shortcut dominance. To address this problem in a more controllable manner, we propose the Positive-Incentive Noise for CLIP (PiN-CLIP), which jointly trains a noise generator and a detection network under a variational positive-incentive principle. Specifically, we construct positive-incentive noise in the feature space via cross-attention fusion of visual and categorical semantic features. During optimization, the noise is injected into the feature space to fine-tune the visual encoder, suppressing shortcut-sensitive directions while amplifying stable forensic cues, thereby enabling the extraction of more robust and generalized artifact representations. Comparative experiments are conducted on an open-world dataset comprising synthetic images generated by 42 distinct generative models. Our method achieves new state-of-the-art performance, with notable improvements of 5.4 in average accuracy over existing approaches.

</details>


### [31] [Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video](https://arxiv.org/abs/2511.16137)
*Li Yu,Yingbo Zhao,Shiyu Wu,Siyue Yu,Moncef Gabbouj,Qingshan Liu*

Main category: cs.CV

TL;DR: 提出一种面向压缩视频质量增强（QECV）的盲/非盲统一方法：用预训练的退化表征学习模块DRL抽取多尺度高维退化表示，并引入层次化终止机制按压缩强度自适应调节去伪影阶段数，在盲设定下显著提升PSNR并降低推理时延。


<details>
  <summary>Details</summary>
Motivation: 现实场景中经转码/传输的压缩视频其量化参数QPs常未知或部分缺失，传统依赖已知QPs且每QP单独训练模型的非盲方法适用性差。现有盲方法用分类产生全局退化向量作通道注意力，缺乏空间细节，难以适应空间位置非均匀的伪影。且现有方法通常在不同QP下使用同构网络，忽视不同压缩强度对计算需求的差异。

Method: 1) 预训练的退化表征学习（DRL）模块：从视频中解耦并提取与内容无关、具有高维和多尺度特性的退化表示，用于精细引导伪影去除（替代仅全局的分类退化向量）。2) 盲/非盲统一：在未知或已知QP场景下均可使用DRL提供的退化引导。3) 层次化终止机制：根据压缩强度（如QP）动态决定去伪影流水线的终止层级，低退化时提前终止以节省计算，高退化时执行更多阶段。

Result: 在盲设定下，QP=22时较SOTA方法PSNR提升从0.31 dB增至0.65 dB（提升约110%）；层次化终止机制使QP=22的平均推理时延约为QP=42的一半。

Conclusion: 通过DRL提供细粒度、空间敏感的退化表征与按退化强度自适应的层次化终止，方法在保持效率的同时显著提升盲QECV表现，并能兼容非盲场景。

Abstract: Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.

</details>


### [32] [Real-Time 3D Object Detection with Inference-Aligned Learning](https://arxiv.org/abs/2511.16140)
*Chenyu Zhao,Xianwei Zheng,Zimin Xia,Linwei Yue,Nan Xue*

Main category: cs.CV

TL;DR: SR3D提出一种面向室内点云的实时三维检测框架，通过空间优先的样本分配与排序感知的自蒸馏，缩小训练与推理（排序选择）之间的鸿沟，并在ScanNet V2与SUN RGB-D上以实时速度取得显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云检测器训练阶段缺乏对空间可靠性与预测排序的关注，而推理时却依赖基于置信度/排序的候选选择（如NMS、Top-K）。这种不一致导致模型学习到的表示与推理行为不对齐，限制最终性能与稳定性。

Method: 1) 空间优先的最优传输（OT）分配：在正负样本指派时动态提升空间定位更准确、几何更可靠的样本权重，使训练目标更贴合真实空间结构。2) 排序感知的自适应自蒸馏：利用教师-学生范式，将排名信息（置信度次序、相对排序）注入学生网络训练，且以自适应方式强调关键排序差异，从而让模型在训练期学习与推理期一致的排名偏好。

Result: 在ScanNet V2与SUN RGB-D两个室内数据集上，SR3D在保持实时推理速度的同时，较现有方法显著提升检测精度（文中强调显著优于先前方法）。

Conclusion: 通过在训练阶段显式建模空间可靠性与排名意识，SR3D有效弥合训练-推理不一致，提升室内点云3D检测的准确性与实用性，并不牺牲实时性。

Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.

</details>


### [33] [A Spatial Semantics and Continuity Perception Attention for Remote Sensing Water Body Change Detection](https://arxiv.org/abs/2511.16143)
*Quanqing Ma,Jiaen Chen,Peng Wang,Yao Zheng,Qingzhan Zhao,Yuchen Zheng*

Main category: cs.CV

TL;DR: 提出HSRW-CD高分辨率遥感水体变化检测数据集与SSCP注意力模块（含MSA、SRGA、CSA），提升水体空间语义与结构连续性建模，可即插即用，在HSRW-CD与Water-CD上验证有效与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有WBCD缺乏高空间分辨率数据集，限制了城镇与乡村等精细定位场景；已有深度方法未充分利用深层特征的空间语义与结构连续性，导致对水体边界与形态变化判别不足。

Method: 1) 构建>3m分辨率HSRW-CD数据集，覆盖多类水体与大量双时相影像；2) 设计SSCP注意力模块，包含：a) MSA增强水体空间语义并为后续提供语义先验；b) SRGA显式捕获全局结构关系，学习水体空间连续性；c) CSA在上述先验引导下进行跨通道自注意力，相似性度量更稳健；模块可插入现有WBCD网络。

Result: 在HSRW-CD与Water-CD数据集上进行大量实验，加入SSCP后模型区分水体变化能力显著提高，表现出良好的泛化性（摘要未给具体数值）。

Conclusion: 高分辨率数据集与SSCP模块共同缓解了数据与特征建模不足的问题；SSCP作为即插即用组件有效提升水体变化检测精度与鲁棒性，代码与数据将开源。

Abstract: Remote sensing Water Body Change Detection (WBCD) aims to detect water body surface changes from bi-temporal images of the same geographic area. Recently, the scarcity of high spatial resolution datasets for WBCD restricts its application in urban and rural regions, which require more accurate positioning. Meanwhile, previous deep learning-based methods fail to comprehensively exploit the spatial semantic and structural information in deep features in the change detection networks. To resolve these concerns, we first propose a new dataset, HSRW-CD, with a spatial resolution higher than 3 meters for WBCD. Specifically, it contains a large number of image pairs, widely covering various water body types. Besides, a Spatial Semantics and Continuity Perception (SSCP) attention module is designed to fully leverage both the spatial semantics and structure of deep features in the WBCD networks, significantly improving the discrimination capability for water body. The proposed SSCP has three components: the Multi-Semantic spatial Attention (MSA), the Structural Relation-aware Global Attention (SRGA), and the Channel-wise Self-Attention (CSA). The MSA enhances the spatial semantics of water body features and provides precise spatial semantic priors for the CSA. Then, the SRGA further extracts spatial structure to learn the spatial continuity of the water body. Finally, the CSA utilizes the spatial semantic and structural priors from the MSA and SRGA to compute the similarity across channels. Specifically designed as a plug-and-play module for water body deep features, the proposed SSCP allows integration into existing WBCD models. Numerous experiments conducted on the proposed HSRW-CD and Water-CD datasets validate the effectiveness and generalization of the SSCP. The code of this work and the HSRW-CD dataset will be accessed at https://github.com/QingMa1/SSCP.

</details>


### [34] [LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM](https://arxiv.org/abs/2511.16144)
*Sibaek Lee,Seongbo Ha,Kyeongsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.CV

TL;DR: LEGO-SLAM在3D高斯喷溅SLAM中引入可适应场景的语言特征蒸馏与紧凑表示，实现实时开放词汇建图与跟踪，并利用语义特征做剪枝和回环检测，达15 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS-SLAM虽可构建逼真地图，但缺乏开放词汇语义理解；直接存储高维语言特征会带来巨大的内存与渲染开销；静态语义模型对新场景适应性差。需要一种既高效又能在线适应的新方法，将语言理解融入实时SLAM。

Method: 提出LEGO-SLAM：1) 场景自适应的编码-解码器，将高维语言嵌入蒸馏到16维紧凑语义特征，在线适配新场景；2) 利用该紧凑特征进行语言引导的高斯剪枝，去除语义冗余以减少高斯数量；3) 复用同一语义特征进行语言驱动的回环检测，无需独立检测模型；4) 结合3DGS实现实时渲染与跟踪。

Result: 在多组实验中，方法在保持或接近的渲染质量与跟踪精度下，实现15 FPS实时性能；通过语义剪枝将高斯数量减少超过60%；同时提供开放词汇语义能力与有效回环检测。

Conclusion: LEGO-SLAM在3DGS-SLAM中成功融入开放词汇语义，兼顾实时性、内存/渲染效率与适应性；通过紧凑特征与语言引导策略，实现高质量建图、精准跟踪和语义回环，无需额外检测模型。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.

</details>


### [35] [Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval](https://arxiv.org/abs/2511.16150)
*Chunxu Liu,Jiyuan Yang,Ruopeng Gao,Yuhan Zhu,Feng Zhu,Rui Zhao,Limin Wang*

Main category: cs.CV

TL;DR: 提出“推理引导嵌入”（RGE）：先让MLLM按指令生成结构化推理，再在推理后提取表示，并用对比学习训练，从而显著提升多模态检索（MMEB上+4.9%）。


<details>
  <summary>Details</summary>
Motivation: 现有把MLLM当嵌入编码器，仅做直接编码，忽视其强生成/推理能力；若将显式推理纳入表征过程，或能嵌入更多上下文条件推断信号，提升表征质量与跨模态对齐。

Method: 设计RGE：1) 指令条件下生成结构化rationale（显式推理步骤）；2) 在推理展开后抽取表示；3) 结合对比学习训练，使带推理的表示在共享空间对齐。核心是把“推理过程”保留并用于表征提取。

Result: 在MMEB多模态检索基准上，相比无推理基线，RGE带来平均+4.9%的性能提升。

Conclusion: 显式引入推理指导的嵌入提取能强化上下文条件推断信号，提升多模态表示与检索效果；MLLM的生成/推理能力不仅可用于回答，也可用于更优表征学习。

Abstract: Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.

</details>


### [36] [Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers](https://arxiv.org/abs/2511.16156)
*Jian Ma,Qirong Peng,Xujie Zhu,Peixing Xie,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: 提出PPCL，一种针对DiT的可插拔结构化剪枝与连续层蒸馏框架，实现50%参数削减，指标下降<3%，仍保持高质量图像生成。


<details>
  <summary>Details</summary>
Motivation: DiT在图像生成上效果卓越但参数量大、计算开销高，难以在资源受限设备部署；需要一种无需为每个剪枝配置单独重训、且能同时做深度与宽度剪枝的通用方案。

Method: 1) 通过线性探测+相似度度量一阶差分趋势分析，识别冗余的层区间；2) 设计可插拔的师生交替蒸馏，将深度（层）与宽度（通道/头）剪枝融合到单阶段训练；3) 跨不同剪枝比率进行灵活知识迁移，避免每个配置单独重训。

Result: 在多种多模态DiT模型上，PPCL将参数规模减少约50%，关键客观指标劣化<3%，仍保持高质量图像生成，并达到更高压缩率。

Conclusion: PPCL为DiT提供了高效、灵活的结构化剪枝与蒸馏方案，适合资源受限场景部署；代码与检查点已开源，便于复现与应用。

Abstract: Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\% reduction in parameter count compared to the full model, with less than 3\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.

</details>


### [37] [Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning](https://arxiv.org/abs/2511.16160)
*Yibin Huang,Wang Xu,Wanyue Zhang,Helu Zhi,Jingjing Huang,Yangbin Xu,Yangang Sun,Conghui Zhu,Tiejun Zhao*

Main category: cs.CV

TL;DR: 提出Video2Layout，将视频转为连续度量化空间布局，用边界坐标替代栅格地图，增强精细空间推理；构建AI2THOR监督数据+强化微调，并发布QVS-Bench分析图像数量与地图/推理准确度关系；模型V2LO-7B在多基准上较栅格方法平均提升4.92%。


<details>
  <summary>Details</summary>
Motivation: 现有多帧视觉到网格认知地图的方法使用离散栅格表示，导致空间关系表述粗糙、量化能力弱，限制MLLM在精细空间推理与真实世界泛化。需要能够直接进行度量计算、减少自然语言空间描述歧义的表示与训练范式。

Method: 提出Video2Layout：1) 使用连续对象边界坐标（大小与相互距离）重建度量化布局；2) 两阶段训练：监督微调，基于AI2THOR生成高质量映射数据学会从视频到边界坐标；随后强化微调提升真实场景泛化；3) 提出QVS-Bench，用于分析输入图像数量与认知地图准确度及空间推理准确度的关系。

Result: 在QVS-Bench及主流空间推理基准上，V2LO-7B较基于栅格地图的对照模型平均提升4.92%，显示连续坐标布局在空间理解与推理上的优势。

Conclusion: 以连续度量边界坐标替代离散栅格，结合监督+强化微调，可显著提升MLLM的空间布局重建与推理能力；QVS-Bench为研究图像数量对空间认知与推理影响提供了系统诊断工具。

Abstract: Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.

</details>


### [38] [Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion](https://arxiv.org/abs/2511.16161)
*Lirui Zhang,Zhengkai Zhao,Zhi Zuo,Pan Gao,Jie Qin*

Main category: cs.CV

TL;DR: 提出Simba：将点云补全中的点级变换回归转化为分布学习，并结合对称先验与扩散模型，辅以分层Mamba上采样，实现更稳健且细节保真的补全，在PCN/ShapeNet/KITTI达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保留输入细节与保持整体结构上难两全；基于回归的局部对称变换虽提升细节，但易过拟合、记忆实例特定变换且对噪声敏感，缺乏可泛化的几何先验与鲁棒性。

Method: 将点级变换回归重塑为分布学习：以扩散模型学习对称变换分布，避免实例记忆并捕获鲁棒几何结构；结合对称先验引导生成；采用分层Mamba架构进行高保真上采样，实现从粗到细的点云重建。

Result: 在PCN、ShapeNet与KITTI基准上取得SOTA性能，表明方法在细节恢复、全局结构一致性及鲁棒性上优于现有方法。

Conclusion: 分布式对称建模+扩散生成、配合Mamba分层上采样，有效缓解回归方法的过拟合与噪声敏感问题，实现同时兼顾细节与全局结构的点云补全。

Abstract: Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.

</details>


### [39] [Layer-wise Noise Guided Selective Wavelet Reconstruction for Robust Medical Image Segmentation](https://arxiv.org/abs/2511.16162)
*Yuting Lu,Ziliang Wang,Weixin Xu,Wei Zhang,Yongqiang Zhao,Yang Yu,Xiaohong Zhang*

Main category: cs.CV

TL;DR: 提出LNG-SWR：在多层注入小噪声学习频率偏置先验，并用先验引导的小波选择性重建抑制噪声敏感频带，增强结构与边界；在CT与超声上提升干净分数并显著提升对PGD和SSAH攻击的鲁棒性，与对抗训练可叠加且无精度牺牲，推理开销低、可扩展。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割在临床部署中面临分布漂移与对抗/随机扰动，主流对抗训练虽能增鲁棒，但引入干净性能下降、训练/调参成本高、维护难，不利于规模化与工程落地。因此需要一种在不显著增加成本的情况下，同时提升干净与鲁棒性能、并可与现有方法兼容的方案。

Method: Layer-wise Noise-Guided Selective Wavelet Reconstruction（LNG-SWR）：1）训练期在多层特征注入小、零均值噪声，促使模型学习“频率偏置先验”，让表示远离对噪声敏感方向；2）基于该先验在输入/特征分支施行选择性小波重建，按频带与方向自适应地抑制噪声敏感子带、强化结构与形状线索，并稳定边界响应，同时保持谱一致性；3）框架与骨干无关、推理期额外开销低，可单独使用或作为对抗训练插件。

Result: 在CT与超声数据集、统一协议下（PGD-L∞/L2与SSAH攻击）：- 干净Dice/IoU均有稳定提升；- 强攻击下性能跌落显著降低；- 与对抗训练结合获得可加成增益；- 结合AT时鲁棒性进一步提升且无干净精度损失。

Conclusion: LNG-SWR以低成本提供频率自适应与边界稳定，既可独立提升鲁棒性，也能作为AT的工程友好型增强插件，在不牺牲干净精度的前提下显著强化医疗分割的对抗与分布漂移稳健性，具备可扩展性与易部署性。

Abstract: Clinical deployment requires segmentation models to stay stable under distribution shifts and perturbations. The mainstream solution is adversarial training (AT) to improve robustness; however, AT often brings a clean--robustness trade-off and high training/tuning cost, which limits scalability and maintainability in medical imaging. We propose \emph{Layer-wise Noise-Guided Selective Wavelet Reconstruction (LNG-SWR)}. During training, we inject small, zero-mean noise at multiple layers to learn a frequency-bias prior that steers representations away from noise-sensitive directions. We then apply prior-guided selective wavelet reconstruction on the input/feature branch to achieve frequency adaptation: suppress noise-sensitive bands, enhance directional structures and shape cues, and stabilize boundary responses while maintaining spectral consistency. The framework is backbone-agnostic and adds low additional inference overhead. It can serve as a plug-in enhancement to AT and also improves robustness without AT. On CT and ultrasound datasets, under a unified protocol with PGD-$L_{\infty}/L_{2}$ and SSAH, LNG-SWR delivers consistent gains on clean Dice/IoU and significantly reduces the performance drop under strong attacks; combining LNG-SWR with AT yields additive gains. When combined with adversarial training, robustness improves further without sacrificing clean accuracy, indicating an engineering-friendly and scalable path to robust segmentation. These results indicate that LNG-SWR provides a simple, effective, and engineering-friendly path to robust medical image segmentation in both adversarial and standard training regimes.

</details>


### [40] [An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs](https://arxiv.org/abs/2511.16163)
*Zhi Luo,Zenghui Yuan,Wenqi Wei,Daizong Liu,Pan Zhou*

Main category: cs.CV

TL;DR: 提出一种针对视觉-语言模型的“冗长文本归纳攻击”（VTIA）：先搜索能诱导LLM冗长输出的对抗提示，再在图像上施加与该提示对齐的细微扰动，使被攻击图像在多款VLM上稳定产生超长、低信息密度的回答，从而显著增加Token开销。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态任务上表现优异，但生成阶段的Token消耗成为效率瓶颈；某些输入会诱导模型输出冗长低密度文本，导致能耗、时延与成本上升。现有方法多通过延迟EOS间接拉长输出，无法稳定、可控地最大化输出长度，缺乏明确的优化目标与通用性。

Method: 两阶段框架：1) 对抗提示搜索（Adversarial Prompt Search）：利用强化学习自动寻找能诱导VLM中LLM组件产生冗长输出的“最恶意”提示嵌入，目标是显式最大化输出Token数；2) 视觉对齐扰动优化（Vision-aligned Perturbation Optimization）：在输入图像上优化细微对抗扰动，使其视觉嵌入与上一步得到的对抗提示嵌入高度相似，从而将“冗长诱导特性”转移到图像上，形成无需文本就能触发啰嗦输出的恶意图像。

Result: 在四个主流VLM上，VTIA显著增加输出长度，同时保持较高成功率、较低查询/优化成本，并在跨模型、跨任务与不同输入上具有较强泛化；相较基线在有效性、效率与稳定性上都有明显优势。

Conclusion: 通过将“最大化输出Token数”设为显式目标，并将对抗提示的效果迁移到图像，VTIA能稳定、可控地诱导VLM产生冗长输出，暴露出VLM在生成效率与鲁棒性方面的安全隐患；建议在部署中加入抗扰鲁棒训练、冗长检测与生成长度治理等防御。

Abstract: With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.

</details>


### [41] [EvoVLA: Self-Evolving Vision-Language-Action Model](https://arxiv.org/abs/2511.16166)
*Zeting Liu,Zida Yang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: EvoVLA提出一套自监督VLA框架，通过对齐阶段奖励、基于位姿的探索与长时记忆，缓解长时序操控中的“阶段幻觉”，在模拟和真实机器人上显著提升成功率与样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA在长时序多阶段任务中会利用粗粒度评估信号“走捷径”，产生阶段完成的幻觉，导致报告的进度高但任务未真正完成；同时探索常基于像素而不稳健，长序列中的内在奖励塑形也易漂移。

Method: 三部分：1) Stage-Aligned Reward（SAR）：利用三元组对比学习，并用Gemini生成的“强难负样本”来对齐视觉表征与阶段进度，抑制视觉捷径与幻觉；2) Pose-Based Object Exploration（POE）：以物体-夹爪的相对位姿为好奇心信号进行探索，替代原始像素驱动，提高可泛化性与稳定性；3) Long-Horizon Memory：选择性上下文保留与门控融合，稳定长回合中的内在奖励塑形。

Result: 在Discoverse-L三项长时序任务上，EvoVLA平均任务成功率69.2%，较最强基线OpenVLA-OFT提升10.2个百分点；样本效率提升至1.5倍；阶段幻觉由38.5%降至14.8%。真实机器人四项操控任务平均成功率54.6%，较OpenVLA-OFT高11个百分点，显示良好的仿真到现实迁移与泛化。

Conclusion: 通过SAR、POE与长时记忆的组合，EvoVLA有效缓解阶段幻觉，提升长时序操控的可靠性、效率与迁移能力；为自监督VLA在复杂多阶段现实任务中落地提供了可行路径。

Abstract: Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.

</details>


### [42] [Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective](https://arxiv.org/abs/2511.16170)
*Jiahao Li,Yang Lu,Yachao Zhang,Yong Xie,Fangyong Wang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出RF-CLIP：一种无需训练的“再聚焦”策略，过滤并重定向CLIP中由维度过度激活引起的分散注意力的无关token，从而提升开放词汇语义分割的像素级多模态对齐与密集预测，达成多项基准SOTA且推理高效。


<details>
  <summary>Details</summary>
Motivation: 现有OVSS依赖CLIP的视觉-语言对齐，但在像素级密集预测上受限，且缺乏从可解释机制角度探查CLIP性能边界；作者观察到CLIP在密集对齐时如“人类走神”，注意力被无关token分散，导致目标区域对齐粒度不足。

Method: 系统剖析CLIP内部注意与激活模式，发现由“维度特异性过度激活”引起的干扰token。提出RF-CLIP：在推理阶段训练无关，检测并过滤这些干扰token，模拟“走神-再聚焦”，将注意力从干扰token引回目标区域，从而细化多模态对齐与像素级预测。

Result: 在8个OVSS基准上取得SOTA性能；同时保持高推理效率（无需额外训练开销）。

Conclusion: 通过识别并缓解CLIP的注意力分散机制，可在不改动训练的情况下显著提升像素级多模态对齐与OVSS性能；再聚焦策略为提升密集预测的通用且高效的途径。

Abstract: Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.

</details>


### [43] [Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight](https://arxiv.org/abs/2511.16175)
*Yi Yang,Xueqi Li,Yiyang Chen,Jin Song,Yihan Wang,Zipeng Xiao,Jiadi Su,You Qiaoben,Pengfei Liu,Zhijie Deng*

Main category: cs.CV

TL;DR: Mantis 提出“解耦视觉前瞻”(DVF)：用DiT头和元查询预测下一视觉状态，同时把VLA主干专注于语言理解与推理；在LIBERO微调后达96.7%成功率，现实机器人上优于π_{0.5}。


<details>
  <summary>Details</summary>
Motivation: 现有VLA要么直接预测高维视觉状态，训练开销大且分散容量；要么强压缩视觉监督，信息瓶颈严重；同时忽视语言监督导致理解与推理弱。需要兼顾视觉前瞻、动作显式学习与语言能力。

Method: 引入Mantis框架：将视觉前瞻从主干解耦，使用元查询+DiT头预测下一视觉状态；通过残差将当前视觉状态喂入DiT；简单的下一状态预测目标促使元查询自动捕获刻画视觉轨迹的潜在动作，进而提升显式动作学习；主干保留语言监督以维持理解和推理。预训练数据包含人类操作视频、机器人示范、图文对。

Result: 在LIBERO基准上微调后达96.7%成功率，收敛快；真实世界评测在指令跟随、未见指令泛化、推理能力上均优于开源VLA强基线π_{0.5}。

Conclusion: DVF的解耦设计在不牺牲语言能力的前提下有效利用视觉监督，提升动作学习与泛化；Mantis在模拟与现实均表现强，代码与权重已开源。

Abstract: Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.

</details>


### [44] [Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.16184)
*Nianchang Huang,Yi Xu,Ruida Xi,Ruida Xi,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出用于无人标注领域自适应的可见-红外行人重识别两阶段框架DSLGA，通过领域共享学习与渐进对齐，显著提升跨域实用场景表现，并提供统一训练测试方案CMDA-XD。


<details>
  <summary>Details</summary>
Motivation: 公开数据与真实场景存在域差异与模态差异，现有VI-ReID方法在实际应用失效；需要在无标注目标域下，将源域知识迁移到目标域且保持精度。

Method: 两阶段DSLGA：1) 预训练阶段引入领域共享学习策略DSLS，挖掘源/目标域可共享信息，缓解跨域模态差异导致的无效预训练；2) 微调阶段提出渐进对齐策略GAS，以“簇到整体”的方式逐步对齐可见与红外数据，处理目标域内的大模态差异；另外构建统一的UDA-VI-ReID训练/测试流程CMDA-XD，便于评测不同模型。

Result: 在多种设置下，大量实验显示该方法显著优于现有UDA域自适应VI-ReID方法，甚至超过部分有监督方法。

Conclusion: 通过领域共享预训练与渐进跨模态对齐，DSLGA有效缓解跨域与跨模态双重差异，提升真实场景VI-ReID的无监督自适应能力；CMDA-XD为评测与应用提供了通用方案。

Abstract: Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.

</details>


### [45] [PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction](https://arxiv.org/abs/2511.16186)
*Deniz Sayin Mercadier,Hieu Le,Yihong Chen,Jiancheng Yang,Udaranga Wickramasinghe,Pascal Fua*

Main category: cs.CV

TL;DR: PrIntMesh提出基于连接模板的器官整体重建框架，联合变形各子结构，显式保持内部边界与拓扑一致，生成平滑无伪影表面，在心脏、海马、肺等任务上比体素/曲面方法更准确、更稳健、更数据高效。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习重建常将器官子结构独立处理，忽视几何与空间约束，导致共享界面不一致、拓扑错误与临床不可用的伪影。需要一种能在统一系统内保持正确拓扑与内部边界、兼顾数据效率的方法。

Method: 以一个连通的器官模板网格为先验，对所有子结构进行联合形变拟合个体解剖；在优化/学习过程中显式保留内部界面（子结构分割边界），并通过几何正则化确保表面光滑、无伪影；框架是模板驱动、拓扑保持，适配多器官（心脏、海马、肺）。

Result: 在多个器官数据上取得高几何精度、拓扑正确性与稳健性，即便训练数据有限或含噪；相较体素与表面方法，能更好重建共享界面并维持结构一致性。

Conclusion: PrIntMesh作为模板与拓扑保持的统一重建框架，提供兼具结构一致性、数据效率与临床适用性的解决方案，优于现有体素/曲面基线，适合在噪声与小样本场景使用。

Abstract: Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.

</details>


### [46] [When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203)
*Yuping Yan,Yuhan Xie,Yinxin Zhang,Lingjuan Lyu,Yaochu Jin*

Main category: cs.CV

TL;DR: 提出VLA-Fool，系统评估具身视觉-语言-行动（VLA）模型在白盒与黑盒下的多模态对抗鲁棒性，发现细微多模态扰动即可显著破坏行为对齐。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦单模态扰动，忽略多模态耦合与跨模态失配对具身推理/决策的关键影响；VLA在真实黑盒、混合模态威胁下鲁棒性缺乏系统研究与基准。

Method: 构建VLA-Fool框架，统一三层对抗攻击：1) 文本：梯度驱动与提示工程扰动；2) 视觉：贴片与噪声扰动；3) 跨模态失配：破坏感知与指令的语义对应；并引入面向VLA的语义空间以自动、语义引导的提示生成。以LIBERO基准、微调OpenVLA开展实验。

Result: 在LIBERO上，微小的文本、视觉与跨模态扰动均可诱发显著行为偏移，揭示当前具身多模态对齐的脆弱性；黑盒与白盒场景均有效。

Conclusion: VLA在现实多模态与黑盒威胁下缺乏鲁棒性；跨模态失配尤其致命。提出的VLA-Fool为评测与攻击提供统一框架与自动语义提示工具，指向未来需加强多模态对齐与对抗防御。

Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.

</details>


### [47] [Unsupervised Image Classification with Adaptive Nearest Neighbor Selection and Cluster Ensembles](https://arxiv.org/abs/2511.16213)
*Melih Baydar,Emre Akbas*

Main category: cs.CV

TL;DR: 提出ICCE：在冻结骨干上训练多头聚类，利用自适应近邻选择与聚类集成生成一致标签，再以其为伪标签训练分类器；在CIFAR10/100与ImageNet上达SOTA（99.3/89/70.4%），首个纯无监督ImageNet>70%。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图像分类多将表示学习与聚类耦合迭代；基础模型兴起后转而仅做聚类但仍有限。多头聚类虽能产生多样划分但结果冲突、稳健性不足。需一种方法整合多头信息、提升无监督分类性能并逼近监督上限。

Method: 1) 使用冻结的预训练骨干提特征；2) 训练多个聚类头（multi-head）并引入自适应最近邻选择，得到一组多样的聚类结果；3) 通过聚类集成（ensemble）将冲突划分整合为一致的共识聚类；4) 以共识聚类作为伪标签，训练一个图像分类器。

Result: 在10个图像分类基准上达SOTA：CIFAR10 99.3%，CIFAR100 89%，ImageNet 70.4%，显著缩小与监督方法差距；据称为首个完全无监督方法在ImageNet上超过70%精度。

Conclusion: 多头聚类结合自适应近邻与聚类集成可在不微调骨干的情况下生成高质量伪标签，进而训练出强大的无监督分类器；该框架在大规模数据上有效并刷新SOTA，表明聚类集成是无监督图像分类的有力途径。

Abstract: Unsupervised image classification, or image clustering, aims to group unlabeled images into semantically meaningful categories. Early methods integrated representation learning and clustering within an iterative framework. However, the rise of foundational models have recently shifted focus solely to clustering, bypassing the representation learning step. In this work, we build upon a recent multi-head clustering approach by introducing adaptive nearest neighbor selection and cluster ensembling strategies to improve clustering performance. Our method, "Image Clustering through Cluster Ensembles" (ICCE), begins with a clustering stage, where we train multiple clustering heads on a frozen backbone, producing diverse image clusterings. We then employ a cluster ensembling technique to consolidate these potentially conflicting results into a unified consensus clustering. Finally, we train an image classifier using the consensus clustering result as pseudo-labels. ICCE achieves state-of-the-art performance on ten image classification benchmarks, achieving 99.3% accuracy on CIFAR10, 89% on CIFAR100, and 70.4% on ImageNet datasets, narrowing the performance gap with supervised methods. To the best of our knowledge, ICCE is the first fully unsupervised image classification method to exceed 70% accuracy on ImageNet.

</details>


### [48] [Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions](https://arxiv.org/abs/2511.16221)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Ruicong Liu,Yoichi Sato*

Main category: cs.CV

TL;DR: 提出MIDA任务与多模态数据集，系统评测12个MLLM显示欺骗识别显著不足；提出SoCoT与DSEM方法显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽具强推理力，但缺乏“读场合/识别欺骗”的社交智能，难以在复杂互动中判断真伪。缺口在于无法有效利用多模态社交线索与建模他人认知状态（所知、所信、意图）。

Method: 1) 构建MIDA任务与带视频-文本同步且逐语句可验证真伪标签的数据集；2) 建立包含12个开闭源MLLM的统一基准评测；3) 失败模式分析揭示多模态落地与社会心智建模缺陷；4) 设计Social Chain-of-Thought(SoCoT)社交推理流程与Dynamic Social Epistemic Memory(DSEM)动态社会知识记忆模块，以增强对他人信念/意图与多模态线索的跟踪和推理。

Result: 在MIDA基准上，包含GPT-4o在内的先进模型可靠性不足，区分真伪显著低于期望。引入SoCoT+DSEM后，模型在该任务上取得可观提升。

Conclusion: 当前MLLM难以进行真正的人类式社交欺骗识别，关键在于多模态社交线索落地与心智建模不足。MIDA基准揭示并量化这一鸿沟；所提SoCoT与DSEM展示了改进方向，为构建更感知且可信的MLLM指明路径。

Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.

</details>


### [49] [SwiTrack: Tri-State Switch for Cross-Modal Object Tracking](https://arxiv.org/abs/2511.16227)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 提出SwiTrack，一种三分支“状态切换”框架用于跨模态目标跟踪（RGB-NIR），通过视觉编码器+NIR门控适配器+一致性轨迹预测，结合动态模板重建与相似度对齐损失，解决不可靠模态下的漂移问题，在基准上以65FPS达SOTA，精度+7.2%，成功率+4.3%。


<details>
  <summary>Details</summary>
Motivation: 现有CMOT多用并行RGB/NIR分支共享骨干，导致模态特征提取不充分、共享空间未校准，一旦输入模态不可靠容易漂移。需要一个能根据模态有效性“切换状态”的机制，充分挖掘RGB与NIR特性，并在无效模态时仍稳健预测目标。

Method: 引入三种“状态/流”：1) RGB流：RGB帧经视觉编码器提特征；2) NIR流：NIR帧经NIR门控适配器并与视觉编码器协同，逐步校准共享潜在空间，增强跨模态表示；3) 无效模态流：一致性轨迹预测模块利用时空线索估计目标运动，降低漂移。配套：动态模板重建用于迭代更新模板特征；相似度对齐损失强化多模态特征一致性。

Result: 在最新RGB-NIR基准上达到SOTA：精度率提升7.2%，成功率提升4.3%，并保持实时65 FPS。

Conclusion: 通过三流状态切换、NIR门控适配与轨迹一致性预测，SwiTrack在不可靠模态和跨模态切换时显著减小目标漂移，实现又快又稳的CMOT，优于现有并行共享骨干方法。

Abstract: Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\% and 4.3\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.

</details>


### [50] [Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs](https://arxiv.org/abs/2511.16264)
*Sinan Mutlu,Georgios F. Angelis,Savas Ozkan,Paul Wisbey,Anastasios Drosou,Mete Ozay*

Main category: cs.CV

TL;DR: 提出一种用于AR/VR稀疏传感器驱动的全身动作重建方法：以带残差的MLP为骨干，并加入“记忆块”用可训练代码向量补全缺失传感器、结合时间上下文；采用多任务学习。实验优于SOTA，且在移动HMD上达72FPS。


<details>
  <summary>Details</summary>
Motivation: 现有AR/VR设备多数只跟踪头和手，导致全身3D重建不完整；需要从有限稀疏传感器推断全身动作，并在移动端保持实时与稳定性。

Method: 以MLP为主干，加入残差连接；提出Memory-Block：用可训练代码向量表示缺失传感器数据，并与之前时刻的稀疏信号融合以强化时序一致性；整体以多任务学习框架训练以获得更稳健的表征。

Result: 在基准上显著降低预测误差，超过SOTA；在移动HMD上实时运行，速度达72 FPS，实现精度-速度更优权衡。

Conclusion: 通过残差MLP+记忆块+多任务学习，可在稀疏输入下实现更准确、时序一致且实时的全身动作重建，适用于移动HMD场景。

Abstract: Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.

</details>


### [51] [TetraSDF: Precise Mesh Extraction with Multi-resolution Tetrahedral Grid](https://arxiv.org/abs/2511.16273)
*Seonghun Oh,Youngjung Uh,Jin-Hwa Kim*

Main category: cs.CV

TL;DR: 提出TetraSDF：结合ReLU MLP与多分辨率四面体位置编码的解析网格提取框架，可精确匹配SDF零等值面，兼具实用效率。


<details>
  <summary>Details</summary>
Motivation: 现有从神经SDF提取网格的方法要么依赖采样（如Marching Cubes/采样查询），导致离散化误差；要么依赖CPWA解析性质，但仅适用于“朴素”ReLU MLP，无法覆盖常用的多分辨率位置编码与更复杂结构。因此需要一种既保留CPWA可解析性、又能处理现代编码器的框架，以获得精确、稳定、无偏的零等值面提取。

Method: 构建TetraSDF：将ReLU MLP与多分辨率四面体（tetrahedral）位置编码组合。利用四面体编码的重心插值保持整体CPWA结构，使网络在编码器诱导的多面体复形内线性，从而可跟踪ReLU线性区域并进行解析求交；提出基于编码器度量的固定解析输入预条件器，减小方向性偏置并稳定训练；在此框架下进行解析网格抽取，得到严格与零等值面一致的网格。

Result: 在多项基准上，TetraSDF的SDF重建精度达到或优于现有基于网格的编码器；解析提取器产生高度自一致的网格，忠实还原学习到的等值面，同时具有可接受的运行时与内存开销。

Conclusion: 四面体位置编码使SDF网络保持全球CPWA，从而实现对零等值面的精确解析提取；结合预条件器可缓解方向偏置并提升稳定性。总体上，TetraSDF在精度、自一致性与效率间取得良好平衡，优于采样式与传统编码器方案。

Abstract: Extracting meshes that exactly match the zero-level set of neural signed distance functions (SDFs) remains challenging. Sampling-based methods introduce discretization error, while continuous piecewise affine (CPWA) analytic approaches apply only to plain ReLU MLPs. We present TetraSDF, a precise analytic meshing framework for SDFs represented by a ReLU MLP composed with a multi-resolution tetrahedral positional encoder. The encoder's barycentric interpolation preserves global CPWA structure, enabling us to track ReLU linear regions within an encoder-induced polyhedral complex. A fixed analytic input preconditioner derived from the encoder's metric further reduces directional bias and stabilizes training. Across multiple benchmarks, TetraSDF matches or surpasses existing grid-based encoders in SDF reconstruction accuracy, and its analytic extractor produces highly self-consistent meshes that remain faithful to the learned isosurfaces, all with practical runtime and memory efficiency.

</details>


### [52] [Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM](https://arxiv.org/abs/2511.16282)
*Gergely Dinya,Péter Halász,András Lőrincz,Kristóf Karacs,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: 提出基于VGGT的快速时空场景理解框架，通过滑动窗口与子图对齐实现近实时、低内存的3D连续更新，并将2D实例语义聚合成3D对象，利用时间戳与实例ID实现时序一致与变化检测；在导航相关数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: VGGT等生成式Transformer在3D场景理解上效果好但内存与延迟高，难以应用于近实时场景（如助残导航）。需要一种既保留VGGT能力又能持续更新3D表示、保证时序一致并检测环境变化的系统。

Method: 以VGGT为核心：1) 采用滑动窗口处理视频流，构建并对齐局部子图（submaps）以缓解显存压力、持续更新3D；2) 利用VGGT的tracking head将多帧2D语义实例掩码融合为3D对象；3) 为实例维护时间戳与ID，实现跨时间的关联与上下文推理，支持变化/新物体检测。

Result: 在通用基准和面向助残导航的自建数据集上实验，系统在接近实时条件下运行稳定，能提供时空一致的3D对象级理解与变化检测，显示出实际可用性。

Conclusion: 通过滑动窗口子图对齐与实例级时序管理，将VGGT扩展为可近实时的时空3D理解框架，适合助残导航等现实应用；证明了在保持性能的同时可显著降低内存并提升时序一致性与环境变化感知能力。

Abstract: We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.

</details>


### [53] [Explainable AI for Diabetic Retinopathy Detection Using Deep Learning with Attention Mechanisms and Fuzzy Logic-Based Interpretability](https://arxiv.org/abs/2511.16294)
*Abishek Karthik,Pandiyaraju V,Sreya Mynampati*

Main category: cs.CV

TL;DR: 提出一种用于杂草检测的混合深度学习框架，融合CNN、ViT、GNN，并结合GAN数据增强与自监督对比预训练，在多基准数据集上取得≈99.33%的各项指标，同时具备可解释性、可适配性与边缘端实时部署能力。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要对田间杂草进行准确的物种级识别，以实现选择性施药、降低除草剂使用并提升可持续的作物管理；现有方法在多变田间环境、类别不均衡和标注数据稀缺等方面存在鲁棒性与泛化不足。

Method: 构建CNN+ViT+GNN的混合架构：CNN提取局部纹理，ViT捕获全局上下文，GNN建模作物-杂草及区域间关系；采用GAN进行数据增强以平衡类别分布；引入自监督对比学习进行预训练，从有限标注中学习更丰富特征；整体支持可解释性分析与边缘端高效推理。

Result: 在多个基准数据集上达到约99.33%的准确率、精确率、召回率和F1值，表明方法在不同田间条件下具有强鲁棒性与泛化能力。

Conclusion: 该框架能联合学习局部、全局与关系特征，具有高可解释性与适配性，可在边缘设备上实时应用于自动化杂草检测，帮助减少除草剂依赖，推动可扩展、可持续的精准农业。

Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment of edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.

</details>


### [54] [Optimizing 3D Gaussian Splattering for Mobile GPUs](https://arxiv.org/abs/2511.16298)
*Md Musfiqur Rahman Sanim,Zhihao Shu,Bahram Afsharmanesh,AmirAli Mirian,Jiexiong Guan,Wei Niu,Bin Ren,Gagan Agrawal*

Main category: cs.CV

TL;DR: 提出Texture3dgs，将3D Gaussian Splatting在移动GPU上进行优化映射，围绕2D纹理缓存友好的排序算法与数据布局，显著提升移动端三维重建速度并降低内存。


<details>
  <summary>Details</summary>
Motivation: 3DGS高效但移动端落地受限：移动GPU依赖2D纹理缓存、带宽与内存受限、排序阶段占主要开销；同时移动部署带来隐私、离线与时延优势，需针对移动体系结构做定制优化。

Method: 围绕移动GPU的2D纹理缓存设计：提出新的排序算法，优化处理流程、数据搬移与2D内存放置；建立纹理缓存成本模型分析算法性质；配合变量布局重构与若干内核级优化，加速3DGS其他步骤。

Result: 在真实端到端评估中，排序阶段最高4.1×加速，整体三维重建1.7×加速，内存占用最多下降1.6×。

Conclusion: 通过纹理缓存感知的排序与数据布局，Texture3dgs有效释放移动GPU潜力，使3DGS在移动端实现更快、更省内存的三维重建，验证了面向架构的专门优化的价值。

Abstract: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\times$ and 1.7$\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.

</details>


### [55] [Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling](https://arxiv.org/abs/2511.16301)
*Minseok Seo,Mark Hamilton,Changick Kim*

Main category: cs.CV

TL;DR: 提出“Upsample Anything”，一种无需训练的测试时优化框架，用各向异性高斯核把低分辨率特征恢复为高分辨率、保边的像素级输出，跨模型与模态通用，在分割、深度和概率图上达SOTA且速度快。


<details>
  <summary>Details</summary>
Motivation: 基础视觉模型多在14x/16x下采样，难以直接用于像素级任务；现有上采样要么依赖数据集再训练，要么隐式优化开销大、泛化差，需要一种轻量、无训练、可泛化的特征上采样方法。

Method: 对每张图在测试时学习一个结合空间与强度（range）线索的各向异性高斯核；该核兼具高斯splatting与联合双边上采样思想，用作通用、保边的卷积/滤波算子，将低分辨率特征（如ViT输出）映射到高分辨率像素网格。优化是无监督、轻量、可跨架构/模态迁移。

Result: 在224x224图像上约0.419秒/张；在语义分割、深度估计，以及深度与概率图上采样任务上达到SOTA精度，并能精确重建高分辨率特征。

Conclusion: 测试时按图自适应学习的各向异性高斯核可作为通用保边上采样算子，弥补基础模型下采样与像素级任务的鸿沟，实现高效、可迁移、无训练的高分辨率恢复。

Abstract: We present \textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\approx0.419 \text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.

</details>


### [56] [Sparse Autoencoders are Topic Models](https://arxiv.org/abs/2511.16309)
*Leander Girrbach,Zeynep Akata*

Main category: cs.CV

TL;DR: 作者将稀疏自编码器（SAE）重新诠释为主题模型，并据此提出SAE-TM，在文本与图像上生成更一致且多样的主题，并可在不重训的情况下灵活合并主题。


<details>
  <summary>Details</summary>
Motivation: 现有对SAE的理解与实用性存在争议，特别是其特征是否可解释、可复用、是否为“可操纵方向”。作者希望给出统一概率视角，阐明SAE学到的结构，并将其用于跨模态的大规模主题分析。

Method: 将LDA扩展到嵌入空间，推导出在该生成模型下SAE目标是MAP估计，从而把SAE特征解释为“主题原子”。据此提出SAE-TM：1）训练SAE以学习可复用的主题原子；2）把原子映射为下游数据上的词分布/成分；3）在不重训的前提下，将原子按需合并为任意数量的主题。

Result: 在文本与图像数据上，SAE-TM较强基线产生更高主题一致性，同时保持主题多样性；还能分析图像数据的主题结构，并追踪日本木版画题材随时间演变。

Conclusion: SAE可自然地视为主题模型，其特征是“主题成分”而非可操纵方向。SAE-TM据此实现跨模态、可复用、可合并的主题发现与演化分析，展示了SAE在大规模主题分析中的实际价值。

Abstract: Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.

</details>


### [57] [BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks](https://arxiv.org/abs/2511.16315)
*Samuel Stevens*

Main category: cs.CV

TL;DR: ImageNet 线性探针精度不再能可靠反映科学影像任务表现。作者推出生态视觉基准 BioBench，覆盖多领域多模态，提供统一评测与便捷 API，显示现有模型在生态任务上的排序与 ImageNet 存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 学界/工业仍以 ImageNet-1K 线性探针 top-1 作为表征质量代理，但在生态与科学影像上该指标相关性弱，导致模型选择与研究方向可能被误导。需要一个更贴近真实生态任务、可复现实验的基准来衡量与推动 AI-for-science。

Method: 构建 BioBench：汇总9个公开、应用驱动的生态视觉任务，覆盖4个生物学界（kingdom）与6种采集模态，总计约310万图像；提供统一 Python API，一键下载、在冻结骨干上训练轻量分类器，并以类均 macro-F1 报告（特定任务含领域指标）；在 A6000 上以 ViT-L 约6小时完成评测。并系统比较46个现代视觉模型，分析 ImageNet 指标与生态任务表现的相关性与排名误差。

Result: 在46个模型上，ImageNet top-1 仅能解释生态任务上约34%的方差，且在75%精度以上的模型中约30%被错误排序。BioBench给出了更具判别力的信号与一致评测结果，并公开了代码、预测与排行榜。

Conclusion: ImageNet 线性探针不再是科学影像/生态视觉的可靠代理。BioBench为生态视觉提供了覆盖广、可复现、成本适中的评测框架，也为构建其他科学领域基准提供了通用范式。

Abstract: ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.

</details>


### [58] [NaTex: Seamless Texture Generation as Latent Color Diffusion](https://arxiv.org/abs/2511.16317)
*Zeqiang Lai,Yunfei Zhao,Zibo Zhao,Xin Yang,Xin Huang,Jingwei Huang,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: NaTex 直接在3D空间生成“原生”纹理颜色，将纹理视作高密度颜色点云；通过几何感知的颜色点云VAE与多控制DiT，从3D数据端到端训练，显著提升跨视角一致性、边界对齐与遮挡区域处理，并在材质生成、纹理润色、部件分割与着色等任务上具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有依赖MVD的“先合成多视图图像再烘焙到2D纹理”的流程存在天然缺陷：遮挡区需补洞、边界处网格与纹理难精确对齐、跨视角内容与色彩一致性差。作者希望绕开2D投影/烘焙瓶颈，直接在3D中建模纹理。

Method: 提出将纹理表述为稠密颜色点云，构建“潜在颜色扩散”框架：1) 几何感知的颜色点云VAE，学习颜色与几何的联合潜在表示；2) 具多重控制的扩散Transformer（DiT），以3D位置嵌入和几何潜变量作为原生几何控制；3) VAE–DiT协同设计，专门的几何分支与颜色VAE紧耦合，确保细粒度表面引导与纹理强对应；全流程基于3D数据从零训练，用于纹理重建与生成。

Result: 在纹理一致性与对齐方面显著优于以往MVD管线；对遮挡、边界和跨视角色彩/内容连贯性问题表现更稳健。实验显示在多种下游任务（材质生成、纹理精修、部件级分割与着色）中具备强泛化，既可零样本也可轻量调优。

Conclusion: 绕开2D烘焙，直接在3D空间进行颜色建模是有效路径。通过几何原生控制与VAE–DiT协同架构，NaTex实现高质量、几何对齐且跨视角一致的纹理生成，并在多任务上展现良好可迁移性。

Abstract: We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.

</details>


### [59] [WWE-UIE: A Wavelet & White Balance Efficient Network for Underwater Image Enhancement](https://arxiv.org/abs/2511.16321)
*Ching-Heng Cheng,Jen-Wei Lee,Chia-Ming Lee,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出WWE-UIE：一个紧凑高效的水下图像增强网络，融合自适应白平衡、基于小波的多频增强和梯度感知边缘保持；在更少参数/FLOPs下达成接近或更优的画质并支持实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有混合（先验+深度）方法效果好但计算开销大，不利于实时与资源受限设备；水下图像存在强烈的波长依赖吸收与散射，导致颜色偏蓝绿、对比度低与边缘模糊，需要既可解释又高效的增强方案。

Method: 设计WWE-UIE，融合三项可解释先验：1) 自适应白平衡以缓解波长依赖的颜色衰减；2) 小波增强块（WEB）进行多带分解，兼顾全局结构与细节纹理；3) 梯度感知模块（SGFB）用可学习门控的Sobel算子保留受散射破坏的边缘；整体为轻量网络以降低参数与FLOPs。

Result: 在多项UIE基准上取得与SOTA相当的重建质量，同时显著减少参数量与计算量，在资源受限平台实现实时推理；消融与可视化证实各组件均有显著贡献。

Conclusion: 将可解释先验与轻量设计结合，可在不牺牲画质的前提下实现高效水下图像增强；WWE-UIE适用于实时与嵌入式场景，并具备可扩展性与可复用组件。

Abstract: Underwater Image Enhancement (UIE) aims to restore visibility and correct color distortions caused by wavelength-dependent absorption and scattering. Recent hybrid approaches, which couple domain priors with modern deep neural architectures, have achieved strong performance but incur high computational cost, limiting their practicality in real-time scenarios. In this work, we propose WWE-UIE, a compact and efficient enhancement network that integrates three interpretable priors. First, adaptive white balance alleviates the strong wavelength-dependent color attenuation, particularly the dominance of blue-green tones. Second, a wavelet-based enhancement block (WEB) performs multi-band decomposition, enabling the network to capture both global structures and fine textures, which are critical for underwater restoration. Third, a gradient-aware module (SGFB) leverages Sobel operators with learnable gating to explicitly preserve edge structures degraded by scattering. Extensive experiments on benchmark datasets demonstrate that WWE-UIE achieves competitive restoration quality with substantially fewer parameters and FLOPs, enabling real-time inference on resource-limited platforms. Ablation studies and visualizations further validate the contribution of each component. The source code is available at https://github.com/chingheng0808/WWE-UIE.

</details>


### [60] [ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery](https://arxiv.org/abs/2511.16322)
*Ching-Heng Cheng,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出ChangeDINO：一个结合轻量主干与冻结DINOv3特征、并配以差分Transformer与可学习形态学后处理的楼宇变化检测模型，在四个基准上以更高IoU与F1超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RSCD方法过度依赖变化标注，未充分利用未变化区域的语义，导致在光照变化、斜视角与小样本场景下鲁棒性不足；需要一个能高效注入强语义先验并显式建模变化的框架。

Method: 构建端到端多尺度Siamese架构：1) 双流特征提取，融合轻量主干与冻结DINOv3迁移的多尺度语义金字塔；2) 空间-光谱差分Transformer解码器，以多尺度绝对差作为变化先验，强化真实楼宇变化抑制干扰；3) 可学习形态学模块在上采样后精炼边界。

Result: 在四个公开基准上，ChangeDINO在IoU与F1上稳定超越近期SOTA；消融实验验证轻量主干+冻结DINOv3、差分Transformer、以及可学习形态学各组件均有显著贡献。

Conclusion: 融合预训练视觉表征与差分先验、并配形态学精炼，可在小数据和复杂成像条件下实现更鲁棒的楼宇变化检测；ChangeDINO为RSCD提供了强基线并具备实际部署潜力。

Abstract: Remote sensing change detection (RSCD) aims to identify surface changes from co-registered bi-temporal images. However, many deep learning-based RSCD methods rely solely on change-map annotations and underuse the semantic information in non-changing regions, which limits robustness under illumination variation, off-nadir views, and scarce labels. This article introduces ChangeDINO, an end-to-end multiscale Siamese framework for optical building change detection. The model fuses a lightweight backbone stream with features transferred from a frozen DINOv3, yielding semantic- and context-rich pyramids even on small datasets. A spatial-spectral differential transformer decoder then exploits multi-scale absolute differences as change priors to highlight true building changes and suppress irrelevant responses. Finally, a learnable morphology module refines the upsampled logits to recover clean boundaries. Experiments on four public benchmarks show that ChangeDINO consistently outperforms recent state-of-the-art methods in IoU and F1, and ablation studies confirm the effectiveness of each component. The source code is available at https://github.com/chingheng0808/ChangeDINO.

</details>


### [61] [Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution with Implicit Representation Networks](https://arxiv.org/abs/2511.16341)
*Yi Ting Tsai,Yu Wei Chen,Hong-Han Shuai,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出ARASFSR：基于隐式表示的任意分辨率/任意倍率人脸超分方法，支持不同输入尺寸与放大倍数，并在定量与可视化上优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有FSR方法通常固定放大倍数，对输入尺寸变化敏感，限制了在实际场景中对不同分辨率与倍率需求的适配性与泛化能力。

Method: 构建隐式表示网络，以2D深度特征、局部相对坐标与放大倍率作为条件，逐像素回归RGB，实现任意倍率超分；引入局部频率估计模块，捕捉高频人脸纹理，缓解频谱偏置；加入全局坐标调制模块，注入人脸结构先验以进行分辨率自适应与全局一致性重建。

Result: 在多种输入分辨率与放大倍率下，ARASFSR在定量指标与主观视觉质量上均优于现有SOTA，表现出更强的鲁棒性与泛化能力。

Conclusion: 隐式表示结合局部频率建模与全局坐标调制，可实现任意分辨率/倍率的人脸超分并提升细节与结构一致性，为实际应用中的灵活超分提供有效方案。

Abstract: Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior facial structure knowledge and achieve resolution adaptation effectively. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales.

</details>


### [62] [Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach](https://arxiv.org/abs/2511.16343)
*Chi-Han Chen,Chieh-Ming Chen,Wen-Huang Cheng,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出用于无人机地形定位的教师-学生框架，通过关键帧选择与更新，在仅标注30%数据下同时提升mIoU与时间一致性，实现稳定定位。


<details>
  <summary>Details</summary>
Motivation: 无人机航拍任务与地面巡检差异大：数据标注成本高、时间一致性难维护、相关数据稀缺且许多技术受有效距离限制。现有做法要么依赖大量全标注数据、要么只选关键数据导致时间一致性差，从而影响空中定位稳定性。

Method: 构建教师-学生架构，结合关键帧选择与关键帧更新算法：以少量全标注关键帧训练教师；教师对非关键帧进行弱监督伪标签与时间一致性知识蒸馏；学生在连续序列中学习mIoU与TC联合目标，迭代用关键帧更新机制强化伪标签质量。

Result: 在仅使用约30%标注数据的情况下，方法同时提升mIoU与时间一致性，能稳定定位地形目标；超越传统仅TC训练或仅关键帧选择方案。提供演示链接。

Conclusion: 全标注并非最佳；结合教师-学生、关键帧选择/更新与TC蒸馏的弱监督方案，在数据稀缺和航拍场景中有效，能以低标注成本实现鲁棒时空一致的地形定位。

Abstract: The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection

</details>


### [63] [CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering](https://arxiv.org/abs/2511.16349)
*Joni Vanherck,Steven Moonen,Brent Zoomers,Kobe Werner,Jeroen Put,Lode Jorissen,Nick Michiels*

Main category: cs.CV

TL;DR: 提出一种将实时相机定位到高精度彩色LiDAR点云中的方法，通过渲染合成视图建立2D-3D对应，并用神经渲染缩小合成-真实域差，实现无漂移、具备公制尺度的跟踪，提供在线与预构建两种实时变体，在ScanNet++上优于现有SLAM。


<details>
  <summary>Details</summary>
Motivation: 传统纯视觉定位/SLAM易出现漂移、尺度歧义，且依赖回环或标志物；而在XR/机器人中需要稳定、全局一致且公制尺度正确的姿态估计。已有将相机与点云对齐的方法常受域差、遮挡与背景伪影影响，导致匹配不稳。本工作动机是利用高精度彩色LiDAR作为全球坐标与尺度基准，并通过缩小合成与真实图像的域差，提升2D-3D匹配的稳健性与实时性。

Method: 预先采集高精度彩色LiDAR点云，实时渲染与当前相机视角一致的合成视图；在合成视图与实时图像之间寻找特征匹配，从而建立图像到点云的2D-3D对应。引入神经渲染技术，减少合成视图中的遮挡与背景伪影，缩小域差，提高匹配质量。提供两种实时管线：1) Online Render and Match：在线渲染并匹配；2) Prebuild and Localize：预构建渲染或特征以加速定位。最终通过PnP/ICP等优化在全局LiDAR坐标中估计姿态。

Result: 在ScanNet++数据集上实现更稳健的定位效果，相比现有SLAM获得更高精度与鲁棒性；实现无漂移、具备正确公制尺度的全球跟踪，并能实时运行。

Conclusion: 将彩色LiDAR点云作为全局、具公制尺度的参考，结合神经渲染生成可匹配的合成视图，可有效提升2D-3D对应质量，带来实时、无漂移、尺度正确的相机定位；在线与预构建两种变体在实际数据上均优于传统视觉SLAM。

Abstract: Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.

</details>


### [64] [Multi-Order Matching Network for Alignment-Free Depth Super-Resolution](https://arxiv.org/abs/2511.16361)
*Zhengxue Wang,Zhiqiang Yan,Yuan Wu,Guangwei Gao,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: 提出MOMNet：一种对RGB与深度非严格对齐场景的鲁棒、无需显式配准的引导深度超分方法；通过多阶匹配与多阶聚合选择并注入与深度一致的RGB信息，取得SOTA性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实中RGB与深度常因传感器分离、标定漂移、振动与温度导致错位，现有方法依赖严格对齐而在失配场景显著退化，亟需对齐无关、能从错位RGB中自适应提取有用信息的方案。

Method: 设计Multi-Order Matching Network (MOMNet)。1) 多阶匹配机制：在零阶/一阶/二阶特征空间联合匹配，分别捕获颜色-深度的强度一致性、边缘/梯度一致性与曲率/结构二阶一致性，从错位RGB中检索与深度最相关的信息。2) 多阶聚合：由多结构检测器组成，以多阶先验作为“提示”，有选择地将RGB特征传递到深度通道，抑制错误迁移。整体为无需显式对齐的端到端框架。

Result: 在广泛实验中优于现有方法，报告SOTA的重建质量，并在不同程度错位的真实场景中表现出显著鲁棒性。

Conclusion: 通过多阶匹配+聚合，MOMNet摆脱严格配准假设，可从错位RGB中自适应利用有效线索，实现更高质量且更稳健的深度超分。

Abstract: Recent guided depth super-resolution methods are premised on the assumption of strictly spatial alignment between depth and RGB, achieving high-quality depth reconstruction. However, in real-world scenarios, the acquisition of strictly aligned RGB-D is hindered by inherent hardware limitations (e.g., physically separate RGB-D sensors) and unavoidable calibration drift induced by mechanical vibrations or temperature variations. Consequently, existing approaches often suffer inevitable performance degradation when applied to misaligned real-world scenes. In this paper, we propose the Multi-Order Matching Network (MOMNet), a novel alignment-free framework that adaptively retrieves and selects the most relevant information from misaligned RGB. Specifically, our method begins with a multi-order matching mechanism, which jointly performs zero-order, first-order, and second-order matching to comprehensively identify RGB information consistent with depth across multi-order feature spaces. To effectively integrate the retrieved RGB and depth, we further introduce a multi-order aggregation composed of multiple structure detectors. This strategy uses multi-order priors as prompts to facilitate the selective feature transfer from RGB to depth. Extensive experiments demonstrate that MOMNet achieves state-of-the-art performance and exhibits outstanding robustness.

</details>


### [65] [DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration](https://arxiv.org/abs/2511.16364)
*Meng-Cheng Shih,Tsai-Ling Huang,Yu-Heng Shih,Hong-Han Shuai,Hsuan-Tung Liu,Yi-Ren Yeh,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出DetailSemNet用于离线签名验证，通过局部结构匹配与细节语义整合器增强细粒度差异，较SOTA显著提升准确率并具备更强泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有OSV多依赖整体特征进行成对比对，忽视细粒度局部差异；同时，Transformer骨干易弱化局部细节，导致验证性能下降。

Method: 1) 提出DetailSemNet，核心是进行两幅签名间的局部结构匹配。2) 设计Detail Semantics Integrator(DSI)，通过特征解耦与再耦合，强化精细局部细节并扩展判别性语义，从而提升局部匹配的有效性。3) 使用基于Transformer的骨干并对其进行结构化改造以保留细节。

Result: 在主流OSV基准上超过近期方法，取得SOTA；在跨数据集测试中保持显著的泛化能力。局部结构匹配带来更高性能与更好可解释性。

Conclusion: 强调细粒度局部结构匹配并结合DSI的细节增强与语义扩展，可显著提升OSV的准确性、可解释性与跨域泛化，适用于实际取证场景。

Abstract: Offline signature verification (OSV) is a frequently utilized technology in forensics. This paper proposes a new model, DetailSemNet, for OSV. Unlike previous methods that rely on holistic features for pair comparisons, our approach underscores the significance of fine-grained differences for robust OSV. We propose to match local structures between two signature images, significantly boosting verification accuracy. Furthermore, we observe that without specific architectural modifications, transformer-based backbones might naturally obscure local details, adversely impacting OSV performance. To address this, we introduce a Detail Semantics Integrator, leveraging feature disentanglement and re-entanglement. This integrator is specifically designed to enhance intricate details while simultaneously expanding discriminative semantics, thereby augmenting the efficacy of local structural matching. We evaluate our method against leading benchmarks in offline signature verification. Our model consistently outperforms recent methods, achieving state-of-the-art results with clear margins. The emphasis on local structure matching not only improves performance but also enhances the model's interpretability, supporting our findings. Additionally, our model demonstrates remarkable generalization capabilities in cross-dataset testing scenarios. The combination of generalizability and interpretability significantly bolsters the potential of DetailSemNet for real-world applications.

</details>


### [66] [CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement](https://arxiv.org/abs/2511.16378)
*Pan Yang,Cheng Deng,Jing Yang,Han Zhao,Yun Liu,Yuling Chen,Xiaoli Ruan,Yanping Chen*

Main category: cs.CV

TL;DR: 提出CAMS，通过多维空间语义解耦提升CLIP式组合零样本学习，对未见属性-对象组合泛化更好，三大数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的CZSL多用全局图像语义表示做属性/对象解耦，但该表示容量有限且难以完全解耦，导致对未见组合的泛化不足。

Method: 1) 设计带门控的跨注意力模块，从CLIP高层编码块中以一组潜在单元获取细粒度语义特征，并自适应抑制背景与无关信息；2) 在提取的语义上进行多空间解耦（Multi-Space Disentanglement），分别学习属性与对象的语义子空间与表示，实现更彻底的解耦；3) 基于解耦后的表示进行组合识别。

Result: 在MIT-States、UT-Zappos、C-GQA三个基准上，在封闭世界与开放世界设定均取得SOTA性能。

Conclusion: 利用门控跨注意力提取细粒度语义并在多空间进行属性-对象解耦，可显著提升CZSL对未见组合的泛化能力。

Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.

</details>


### [67] [End-to-End Motion Capture from Rigid Body Markers with Geodesic Loss](https://arxiv.org/abs/2511.16418)
*Hai Lan,Zongyan Li,Jianmin Hu,Jialing Yang,Houde Dai*

Main category: cs.CV

TL;DR: 提出用刚体标记(RBM)替代传统稠密点标记，并用端到端深度网络在测地损失下直接回归SMPL，实现更简便、更快、仍高精度的人体MoCap。


<details>
  <summary>Details</summary>
Motivation: 传统光学MoCap依赖大量散点标记，存在放置/识别耗时且易混淆的问题，限制了规模化与实用性；需要一种更稀疏、无歧义且可实时的方案，同时保持高精度。

Method: 1) 设计刚体标记(RBM)作为基本单元，单个即可提供无歧义6自由度信息；2) 将RBM的稀疏6DoF观测作为输入，训练端到端深度回归网络直接预测SMPL参数；3) 采用流形感知的测地损失以更好处理旋转等非欧几里得量；4) 主要用AMASS合成数据训练，并与优化式方法对比；5) 使用Vicon实采数据做验证。

Result: 在AMASS合成训练后取得SOTA人体姿态估计精度；与优化式方法精度相当，但计算量降低一个数量级以上；在Vicon真实数据上也表现良好，验证了实用性与鲁棒性。

Conclusion: 将稀疏6DoF的RBM与测地损失结合，可在保证精度的同时显著简化部署与计算，提供适用于图形、VR与生物力学的实时高保真MoCap方案。

Abstract: Marker-based optical motion capture (MoCap), while long regarded as the gold standard for accuracy, faces practical challenges, such as time-consuming preparation and marker identification ambiguity, due to its reliance on dense marker configurations, which fundamentally limit its scalability. To address this, we introduce a novel fundamental unit for MoCap, the Rigid Body Marker (RBM), which provides unambiguous 6-DoF data and drastically simplifies setup. Leveraging this new data modality, we develop a deep-learning-based regression model that directly estimates SMPL parameters under a geodesic loss. This end-to-end approach matches the performance of optimization-based methods while requiring over an order of magnitude less computation. Trained on synthesized data from the AMASS dataset, our end-to-end model achieves state-of-the-art accuracy in body pose estimation. Real-world data captured using a Vicon optical tracking system further demonstrates the practical viability of our approach. Overall, the results show that combining sparse 6-DoF RBM with a manifold-aware geodesic loss yields a practical and high-fidelity solution for real-time MoCap in graphics, virtual reality, and biomechanics.

</details>


### [68] [CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation](https://arxiv.org/abs/2511.16428)
*Samer Abualhanud,Christian Grannemann,Max Mehltretter*

Main category: cs.CV

TL;DR: 提出一种几何引导的自监督环视深度估计方法，通过将多相机视图投影到共享单位圆柱上并进行跨视图特征聚合，得到密集、度量且跨视图一致的深度；在DDAD与nuScenes上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有环视自监督深度估计在重叠视图间缺乏一致性，导致360°多相机系统的3D感知不稳定。需要一种能在跨相机间建立可靠对应并提升度量精度与一致性的方案。

Method: 1) 输入为校准且时间同步的多相机影像，已知内参与相对位姿；2) 先为每张图预测初始深度并回投成3D点；3) 将所有图像的3D点投影到共享的单位圆柱以建立跨视图邻域与对齐；4) 为每张图生成“位置图”，记录像素在圆柱上的投影位置；5) 基于位置图施加显式、非学习的空间注意力，按圆柱距离跨图聚合特征；6) 输出每张图的最终深度；7) 自监督训练。

Result: 在DDAD与nuScenes数据集上，相较现有方法，显著提升跨图深度一致性与整体深度精度，达到新的SOTA或接近SOTA表现。

Conclusion: 将多视图投影到共享圆柱并用几何先验驱动的显式注意力，可有效解决环视自监督深度估计的跨视图不一致问题，获得度量、密集且一致的深度，具有实际部署价值。

Abstract: Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.

</details>


### [69] [Graph Neural Networks for Surgical Scene Segmentation](https://arxiv.org/abs/2511.16430)
*Yihan Li,Nikhil Churamani,Maria Robu,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 将ViT特征与GNN相结合，利用静态kNN+GCNII与动态DGG+GAT两种图结构建模，显著提升腹腔镜胆囊手术场景分割的精度与解剖一致性，尤其对稀有、细薄且安全关键结构效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有深度分割在手术场景中易受遮挡、长程依赖不足与稀有细结构几何刻画不佳的限制，导致关键解剖结构识别不稳，存在手术并发症风险。作者期望通过显式建模区域间空间与语义关系，提升模型对解剖结构的理解与鲁棒性。

Method: 提出两种融合ViT编码与GNN的分割框架：1) 静态图：基于kNN构建固定拓扑，使用GCNII以初始残差与恒等映射稳定长程信息传递；2) 动态图：可微分图生成器DGG自适应学习拓扑，配合GAT进行注意力加权的关系推理。均在Endoscapes-Seg50与CholecSeg8k数据集上评测。

Result: 相较SOTA基线，mIoU提升约7–8%，mDice提升约6%；在细薄、稀有与安全关键结构上产生更具解剖一致性的预测。

Conclusion: 图结构分割方法通过ViT的全局上下文与图式关系推理协同，兼顾性能与解剖一致性，增强可解释性与可靠性，为更安全的腹腔镜与机器人辅助手术提供更精确的关键解剖识别。

Abstract: Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.

</details>


### [70] [Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation](https://arxiv.org/abs/2511.16435)
*Jin Wang,Bingfeng Zhang,Jian Pang,Mengyu Liu,Honglong Chen,Weifeng Liu*

Main category: cs.CV

TL;DR: 提出以语言驱动属性泛化的FSS方法，用LLM生成多属性文本并与视觉特征对齐，减少对支持图像参考的依赖，提升泛化与SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有FSS依赖支持图像提取元参考，但同类内部差异导致指导偏差，尤其对未训练类分割不稳。作者认为支持的关键是提供“无偏”元指导，不必执着于图像参考，因此引入语言属性作为稳健支撑。

Method: 提出LDAG框架：1) 多属性增强（MaE）利用LLM生成目标类别的细粒度属性文本，进行多模态匹配以形成精炼视觉-文本先验；2) 多模态属性对齐（MaA）在文本与视觉之间建立交互以缓解跨模态鸿沟，使属性文本更有效地提升视觉表示。总体通过语言属性替代传统支持图像参考，提供无偏元指导。

Result: 在Few-shot分割基准上明显优于现有方法，达到新的SOTA（具体数值未给出），代码将开源。

Conclusion: 语言驱动的多属性先验能作为更稳健的支持信号；通过MaE和MaA实现从文本到视觉的有效迁移，显著提升FSS对未见类别的泛化能力。

Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.

</details>


### [71] [StreetView-Waste: A Multi-Task Dataset for Urban Waste Management](https://arxiv.org/abs/2511.16440)
*Diogo J. Paulo,João Martins,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 提出StreetView-Waste数据集，面向车载视角下的垃圾与垃圾桶检测、跟踪与溢出分割，并给出基线与两种改进策略；在计数与分割上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有垃圾检测数据多为静态、去情境场景，缺乏对城市真实物流中的垃圾桶跟踪与溢出监测标注，难以支撑基于环卫车图像的实际部署需求。

Method: 构建包含街景垃圾与垃圾桶的StreetView-Waste数据集，支持三项评测：1) 垃圾桶检测；2) 垃圾桶多目标跟踪；3) 垃圾溢出（及散落垃圾）分割。提供SOTA检测/跟踪/分割模型作为基线；并提出两类增强：a) 面向垃圾桶跟踪的启发式策略以提升计数稳定性；b) 模型无关、利用几何先验的多模态框架以细化/约束垃圾（形态不规则）分割。

Result: 微调的检测器能较好检测垃圾桶，但现成跟踪基线在数量估计上表现不佳；所提启发式将平均绝对计数误差降低79.6%。对难以分割的无定形垃圾，引入几何先验的策略在轻量模型上将分割mAP@0.5提升27%。

Conclusion: StreetView-Waste为真实城市场景的垃圾管理感知提供了具有挑战性的基准。启发式跟踪与几何先验分割证明在车载场景下有效，鼓励面向实际环卫应用的检测、跟踪与分割研究。

Abstract: Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.

</details>


### [72] [VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference](https://arxiv.org/abs/2511.16449)
*Ziyan Liu,Yeqiu Chen,Hongyi Cai,Tao Lin,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: 提出VLA-Pruner，一种面向VLA的可插拔视觉token剪枝方法，以双层重要性（语义+动作）准则在计算预算下自适应保留关键视觉token，实现加速且保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有针对VLM的token剪枝仅依据语义显著性（如prefill注意力），忽略VLA模型既要高级语义理解又要低级动作执行的双系统特性，导致对动作生成关键信息的丢失，从而在机器人任务上显著掉点。需要一种既兼顾语义又兼顾动作的剪枝方法，适配连续视频流、实时部署和机器人操作的时序特性。

Method: 提出VLA-Pruner：1) 双层重要性准则——语义层用视觉-语言预填阶段的注意力评估相关性；动作层用动作解码阶段注意力的估计（通过时间平滑/时序连续性近似）评估对动作的关键性。2) 基于该准则的双层token选择策略：在给定算力预算下，自适应选择兼顾语义与动作的重要视觉token，形成紧凑而信息充分的token集合。方法为即插即用、适用于多种VLA架构与多样机器人任务。

Result: 在多种VLA架构和多样机器人操控任务上取得SOTA的加速-性能权衡，相较VLM特定剪枝方法显著减小性能退化，同时提供实时部署友好性。

Conclusion: VLA-Pruner通过结合语义与动作两类注意力并利用任务的时序连续性，解决了仅基于语义剪枝导致的动作信息缺失问题，在保持或提升VLA性能的同时显著提升推理效率，具备跨架构与跨任务的通用性。

Abstract: Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.

</details>


### [73] [LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs](https://arxiv.org/abs/2511.16454)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: 提出LLaVA^3：不做微调、仅用多视角2D图像，通过从中间3D重建提取“全向”对象表示来增强VLM的3D理解；在3D问答与语言指向任务上优于以往2D方案。


<details>
  <summary>Details</summary>
Motivation: 3D场景理解的多模态语言模型受限于稀缺的3D标注/训练数据，而2D数据充足；希望利用现成VLM与2D多视角数据弥补3D数据不足。

Method: 受立体派“多视角合一”启发：对场景做中间的多视角3D重建（例如从多张2D图像），为每个对象构建全向（omnidirectional）的视觉表征，再将这些表征输入现有VLM，使其在不微调的情况下获得3D感知；整体流程为多视角采集→中间3D重建→对象全向表征→喂给VLM进行理解。

Result: 在3D视觉问答（3D VQA）与3D语言定位（grounding）基准上，显著优于此前基于2D的VLM方案。

Conclusion: 通过中间3D重建与全向对象表示，能在不依赖3D训练数据与不微调VLM的前提下，显著提升VLM的3D场景理解能力。

Abstract: Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.

</details>


### [74] [FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry](https://arxiv.org/abs/2511.16471)
*Clemens Pollak,Kersten Diers,Santiago Estrada,David Kügler,Martin Reuter*

Main category: cs.CV

TL;DR: FastSurfer-CC 是一个快速、全自动的胼胝体（含穹隆）形态学分析管线，涵盖从中矢面定位、结构分割、标准化配准到厚度/分区与多种形状指标提取，并在多任务上优于现有工具，且在亨廷顿舞蹈病队列中检测到现有方法未能发现的显著差异。


<details>
  <summary>Details</summary>
Motivation: 胼胝体在衰老与神经疾病研究中至关重要，也是深部脑刺激、再髓鞘化等临床试验的重要靶点/生物标志物。然而，公开可用、覆盖端到端自动流程（从定位到多指标量化）的工具稀缺，阻碍了大规模、可重复的研究与临床应用。

Method: 提出 FastSurfer-CC：1) 自动识别中矢状切片；2) 分割胼胝体与穹隆；3) 自动定位前/后连合以标准化头部姿态；4) 生成厚度剖面与分区；5) 提取8个形状指标用于统计分析。并与现有专用工具在各子任务上进行对比评估。

Result: FastSurfer-CC 在各个单项任务上超越现有专用工具（定位、分割、配准、厚度/分区与形状指标稳健性等）。在亨廷顿病患者与健康对照的比较中，本方法检测到统计学显著差异，而当前最先进方法未能检测到。

Conclusion: FastSurfer-CC 提供了高效、全自动、可标准化的胼胝体形态学分析管线，提升了任务级性能与群体研究的敏感性，显示其在疾病生物标志物评估与临床研究中的应用潜力。

Abstract: The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.

</details>


### [75] [Flow and Depth Assisted Video Prediction with Latent Transformer](https://arxiv.org/abs/2511.16484)
*Eliyas Suleyman,Paul Henderson,Eksan Firkat,Nicolas Pugeault*

Main category: cs.CV

TL;DR: 研究通过显式引入点流(point-flow)与深度图(depth)信息，提升遮挡与背景运动场景下的视频预测效果。基于多物体潜变量Transformer框架，融合几何与运动模态，在合成与真实数据上评测，除外观指标外还引入对目标掩码分布的Wasserstein距离。结果表明加入点流与深度能显著改善遮挡处理与背景运动预测。


<details>
  <summary>Details</summary>
Motivation: 通用视频预测在标准场景表现良好，但对遮挡与复杂背景运动仍脆弱。作者假设若给予模型显式的几何结构(深度)与运动轨迹(点流)先验，可缓解遮挡带来的信息缺失与错配，提升未来帧推断的稳定性与正确性。

Method: 采用标准的多对象潜变量Transformer作为骨干，对未来帧进行建模；在输入/特征层面融合点流与深度模态，使模型获得显式的3D几何与稀疏到稠密的运动指导；在可控环境与真实数据集上进行系统化对比实验；评估除常见外观指标外，还基于对象掩码的Wasserstein距离衡量预测的运动分布准确度。

Result: 在存在遮挡和背景运动的场景中，融合点流与深度的模型优于不使用这些模态的基线；在外观指标与掩码Wasserstein距离上均取得更好分数，特别是在背景运动估计与被遮挡物体的轨迹预测上更为准确与稳定。

Conclusion: 显式的几何(深度)与运动(点流)信息对视频预测至关重要。将其融入多物体Transformer可提高遮挡处理能力并改善背景运动建模；以Wasserstein距离评估对象掩码提供了更敏感的运动质量度量。建议未来工作进一步探索多模态融合与更强的几何先验以应对复杂真实场景。

Abstract: Video prediction is a fundamental task for various downstream applications, including robotics and world modeling. Although general video prediction models have achieved remarkable performance in standard scenarios, occlusion is still an inherent challenge in video prediction. We hypothesize that providing explicit information about motion (via point-flow) and geometric structure (via depth-maps) will enable video prediction models to perform better in situations with occlusion and the background motion. To investigate this, we present the first systematic study dedicated to occluded video prediction. We use a standard multi-object latent transformer architecture to predict future frames, but modify this to incorporate information from depth and point-flow. We evaluate this model in a controlled setting on both synthetic and real-world datasets with not only appearance-based metrics but also Wasserstein distances on object masks, which can effectively measure the motion distribution of the prediction. We find that when the prediction model is assisted with point flow and depth, it performs better in occluded scenarios and predicts more accurate background motion compared to models without the help of these modalities.

</details>


### [76] [Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation](https://arxiv.org/abs/2511.16494)
*Zongcai Tan,Lan Wei,Dandan Zhang*

Main category: cs.CV

TL;DR: 提出一种融合波动光学物理渲染与深度对齐的GAN，用于高保真显微图像合成，显著提升微型机器人位姿估计的数据效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微型光学机器人的高精度位姿估计依赖大量高质量显微图像及标注，但真实数据获取昂贵且困难；现有数字孪生/数据增强方法无法逼真再现衍射、景深等复杂显微成像效应，限制了仿真到真实的迁移。

Method: 构建物理先验驱动的深度生成框架：在GAN中内嵌基于波动光学（含衍射等效应）的物理渲染器，并引入深度（焦深）对齐机制，端到端合成与真实分布一致的显微图像；以合成数据训练CNN位姿估计器。

Result: 相较纯AI生成，SSIM提升35.6%，渲染实时（0.022 s/帧）；用合成数据训练的位姿估计器在俯仰/横滚上达93.9%/91.9%准确率，仅比全真数据训练低5.0%/5.4%；对未见姿态与新型微机器人配置具备良好泛化。

Conclusion: 物理引导的生成学习有效弥合仿真-真实鸿沟，可高效合成高保真显微图像以支持微机器人位姿估计，减少真实数据依赖并提升对新姿态/新配置的鲁棒性。

Abstract: Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.

</details>


### [77] [Acquisition Time-Informed Breast Tumor Segmentation from Dynamic Contrast-Enhanced MRI](https://arxiv.org/abs/2511.16498)
*Rui Wang,Yuexi Du,John Lewin,R. Todd Constable,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 提出一种利用采集相位时间信息，通过FiLM调制特征的DCE-MRI乳腺肿瘤分割方法，在多站点数据上训练并在域内与跨域测试中均提升分割与泛化性能。


<details>
  <summary>Details</summary>
Motivation: DCE-MRI不同受检者与中心的采集协议差异导致即使在同一相位的图像外观也高度可变，增加自动肿瘤分割难度；需要一种能显式利用时间/相位信息来减少域间差异并提升鲁棒性的方案。

Method: 将每张图像的采集时间（相位时间戳）作为条件输入，通过特征级线性调制（FiLM）层对主干网络特征进行通道/层级的仿射变换，实现对不同相位与可变帧数序列的轻量级时序条件化；在大型多站点乳腺DCE-MRI数据上，以多种主干与不同配置训练基线与时间调制模型，并在域内与公开跨域数据集评估。

Result: 与不使用时间信息的基线相比，时间调制模型在肿瘤分割指标上取得更高性能，并在跨域测试中表现出更好的泛化能力；在可变数量相位的情形下亦能有效利用全部图像。

Conclusion: 将相位采集时间通过FiLM融入分割网络可缓解协议与个体差异引起的外观变化，提升DCE-MRI乳腺肿瘤分割准确性与跨域鲁棒性，是一种轻量有效、可扩展到可变帧数研究的策略。

Abstract: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer screening, tumor assessment, and treatment planning and monitoring. The dynamic changes in contrast in different tissues help to highlight the tumor in post-contrast images. However, varying acquisition protocols and individual factors result in large variation in the appearance of tissues, even for images acquired in the same phase (e.g., first post-contrast phase), making automated tumor segmentation challenging. Here, we propose a tumor segmentation method that leverages knowledge of the image acquisition time to modulate model features according to the specific acquisition sequence. We incorporate the acquisition times using feature-wise linear modulation (FiLM) layers, a lightweight method for incorporating temporal information that also allows for capitalizing on the full, variables number of images acquired per imaging study. We trained baseline and different configurations for the time-modulated models with varying backbone architectures on a large public multisite breast DCE-MRI dataset. Evaluation on in-domain images and a public out-of-domain dataset showed that incorporating knowledge of phase acquisition time improved tumor segmentation performance and model generalization.

</details>


### [78] [YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras](https://arxiv.org/abs/2511.16521)
*Fan Yang,Sosuke Yamao,Ikuo Kusajima,Atsunori Moteki,Shoichi Masui,Shan Jiang*

Main category: cs.CV

TL;DR: 提出一种联合室内场景建图与吊顶相机（CMC）注册的方法：用头戴式RGB-D相机移动一次场景，同时让CMC拍摄，通过时间关联将CMC相对位姿对齐到世界坐标，并以因子图联合优化自摄像头、场景布局和CMC位姿；并发布首个相关数据集与基准，实验显示两任务在统一框架下互相提升。


<details>
  <summary>Details</summary>
Motivation: 手工注册CMC到场景布局效率低且成本高；仅依赖视觉定位在存在视觉歧义时效果差。需要一种既高效又稳健的自动化方案，将CMC与场景布局同时估计并相互约束，减少歧义、提升精度。

Method: 1) 让配备头戴式RGB-D相机的移动体一次性遍历场景，同时同步顶部CMC拍摄；2) 自我视角视频生成世界坐标系下的移动体轨迹与场景布局；CMC视频生成伪尺度的轨迹与CMC之间的相对位姿；3) 通过时间戳将各轨迹相关联，把CMC相对位姿对齐到世界坐标，获得初始化；4) 构建定制因子图，联合优化自摄像头位姿、场景布局与CMC位姿；5) 发布新数据集作为协同建图与CMC注册的首个基准。

Result: 在公开的首个协同建图与CMC注册数据集上验证：该方法在统一框架内同时完成两项任务，并且两者通过联合优化彼此提升，较仅做单任务或独立流程取得更稳健与准确的结果。

Conclusion: 联合建图与CMC注册是可行且互利的；通过时间关联初始化与因子图联合优化，可在一次场景遍历中高效完成注册与建图，为后续位置感知应用提供可靠工具和基准数据。

Abstract: Using ceiling-mounted cameras (CMCs) for indoor visual capturing opens up a wide range of applications. However, registering CMCs to the target scene layout presents a challenging task. While manual registration with specialized tools is inefficient and costly, automatic registration with visual localization may yield poor results when visual ambiguity exists. To alleviate these issues, we propose a novel solution for jointly mapping an indoor scene and registering CMCs to the scene layout. Our approach involves equipping a mobile agent with a head-mounted RGB-D camera to traverse the entire scene once and synchronize CMCs to capture this mobile agent. The egocentric videos generate world-coordinate agent trajectories and the scene layout, while the videos of CMCs provide pseudo-scale agent trajectories and CMC relative poses. By correlating all the trajectories with their corresponding timestamps, the CMC relative poses can be aligned to the world-coordinate scene layout. Based on this initialization, a factor graph is customized to enable the joint optimization of ego-camera poses, scene layout, and CMC poses. We also develop a new dataset, setting the first benchmark for collaborative scene mapping and CMC registration (https://sites.google.com/view/yowo/home). Experimental results indicate that our method not only effectively accomplishes two tasks within a unified framework, but also jointly enhances their performance. We thus provide a reliable tool to facilitate downstream position-aware applications.

</details>


### [79] [BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization](https://arxiv.org/abs/2511.16524)
*Rahul Kumar,Vipul Baghel,Sudhanshu Singh,Bikash Kumar Badatya,Shivam Yadav,Babji Srinivasan,Ravi Hegde*

Main category: cs.CV

TL;DR: 他们发布了一个高质量、细粒度标注的拳击出拳视频数据集，用于实时出拳检测与分类研究，旨在促进动作识别在低资源、非受控环境中的发展。


<details>
  <summary>Details</summary>
Motivation: 现有格斗运动的计算机视觉研究受限于数据集稀缺与不稳定：动作高度动态、场景非结构化、拍摄条件多变，导致难以训练鲁棒模型。作者希望通过提供标准化、丰富的标注数据来缓解这一瓶颈。

Method: 从20个YouTube实战视频中筛选并手工切分出6,915段拳击片段，涵盖18名运动员与6类出拳类型。对每段进行人工时间边界划定与类别标注，保留多样的运动风格、机位角度与体型差异，形成用于实时视觉动作识别的基准。

Result: 得到一个规模适中但多样性强的数据集：6,915个高质量出拳片段、6类动作、18名运动员、覆盖多种拍摄和动作变化，标注精确、一致。

Conclusion: 该数据集为拳击出拳检测与分类提供了可靠基准，有望加速实时动作识别、自动化教练与表现评估等应用的发展，尤其在低资源与非受控条件下。

Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.

</details>


### [80] [Contrastive vision-language learning with paraphrasing and negation](https://arxiv.org/abs/2511.16527)
*Kwun Ho Ngan,Saman Sadeghi Afgeh,Joe Townsend,Artur d'Avila Garcez*

Main category: cs.CV

TL;DR: SemCLIP在CLIP框架中同时建模“同义改写靠近、否定远离”，利用LLM生成的原句-同义改写-否定三元组训练，提升对否定语义的鲁棒性，同时基本保持原有检索与下游表现。


<details>
  <summary>Details</summary>
Motivation: CLIP在文本轻微变动（否定）或形式多样（改写）时表现不稳：否定会引起语义反转、改写则保持语义但换表述，现有模型难以同时对二者做出正确的嵌入对齐与分离。

Method: 提出新的对比损失，联合考虑同义改写与否定；利用LLM生成的三元组（原始描述、同义改写、否定描述）进行训练，使模型在嵌入空间中拉近原始与改写、推远否定；在CLIP样式的预训练流程中实现，称为SemCLIP。

Result: 在CC-Neg基准上，以“原句优于否定”检索准确率从68.1%提升至78.1%；在Sugarcrepe++上结果较为混合，但普遍优于仅用否定数据训练的模型；在多项零样本分类任务上，SemCLIP（在Sugarcrepe++预训练）整体优于CLIP。

Conclusion: SemCLIP在不显著牺牲CLIP性能的前提下，显著增强对否定与语义变换的鲁棒性，实现“改写靠近、否定远离”的期望几何结构，对检索与零样本分类均有益。

Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.

</details>


### [81] [Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration](https://arxiv.org/abs/2511.16532)
*Fan Yang,Shigeyuki Odashima,Shoichi Masui,Ikuo Kusajima,Sosuke Yamao,Shan Jiang*

Main category: cs.CV

TL;DR: 提出一种鲁棒的多相机体操运动员跟踪方法，用级联数据关联在可三角化与不可三角化时分别用三角化和射线-平面交替生成3D轨迹候选，利用体操动作多在固定竖直平面内的先验，提升在相机少、遮挡多、检测缺失下的跟踪稳定性；已在世锦赛裁判系统中落地并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 体操比赛场景中相机数量受限、光照/背景/服装变化与遮挡导致跨视角检测不稳定，传统多相机三角化在只有少量视角（常常仅对向两视角）且偶发检测缺失时难以稳定恢复精确的3D轨迹，需要结合领域先验提高鲁棒性以满足裁判打分应用。

Method: 引入体操领域先验：运动员的3D中心在很多动作阶段近似位于预定义竖直平面内。提出级联数据关联（DA）框架：优先在检测充分的多视角条件下做标准三角化生成3D候选；当跨视角检测不足时，利用对向视角的射线与该竖直平面求交，生成共面的3D轨迹候选来补全与约束轨迹；在轨迹层面融合两类候选以减少失败。

Result: 在多组复杂场景实验中方法表现出更高鲁棒性，较现有多相机跟踪方法在检测缺失、遮挡等挑战下取得更优指标；系统已部署于近期体操世锦赛并获得国际体联认可。

Conclusion: 通过将体操动作的平面先验融入多相机跟踪并采用级联DA策略，可在相机受限和检测不稳定条件下显著降低3D轨迹失败率，满足实际裁判系统需求并优于现有方法。

Abstract: We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.

</details>


### [82] [Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation](https://arxiv.org/abs/2511.16535)
*Haytham Ziani*

Main category: cs.CV

TL;DR: 论文比较局部与全局光流方法，重点实现并评估多分辨率版Horn–Schunck，通过双线性插值与延拓提升精度与收敛，在多种图像条件下验证运动估计效果。


<details>
  <summary>Details</summary>
Motivation: 光流估计在运动分析、跟踪与视频理解中关键；局部（Lucas–Kanade）与全局（Horn–Schunck）各有优缺点，缺乏对其在复杂成像条件下以及多分辨率增强策略的系统应用性评估。

Method: 系统回顾并实现局部（Lucas–Kanade）与全局（Horn–Schunck）方法；提出/实现多分辨率Horn–Schunck框架：金字塔自顶向下，使用双线性插值进行下采样与光流延拓（prolongation），在各尺度上迭代优化平滑与数据一致项；比较不同图像条件下的性能。

Result: 多分辨率HS较单尺度HS在精度与收敛速度上均有提升；在多种图像条件（如噪声、纹理稀疏、较大位移）下，结合金字塔与延拓策略能更稳健地恢复运动场；与Lucas–Kanade相比，全局方法在低纹理和噪声场景更鲁棒，但对正则化与参数较敏感。

Conclusion: 局部与全局方法互补；多分辨率与插值/延拓能显著增强HS的实用性。推荐在大位移与复杂场景中采用金字塔HS，并根据纹理与噪声自适应调参或与局部方法混合。

Abstract: This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.

</details>


### [83] [Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution](https://arxiv.org/abs/2511.16541)
*Jaime Álvarez Urueña,David Camacho,Javier Huertas Tato*

Main category: cs.CV

TL;DR: 提出一种两阶段合成图像检测框架：对比学习提取判别嵌入 + 少样本kNN微调，在仅150张/类条件下达91.3%准确率，并显著提升开放集归因AUC与OSCR。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速迭代使合成图像与真图愈发难分；传统周期性重训的检测器难以跟上新模型发布，计算昂贵、部署不便，亟需能跨生成器泛化并以少量样本快速适配的新方法。

Method: 两阶段：1) 以监督式对比学习训练视觉模型，在训练时有意留出部分生成器架构不参与训练，以检验跨生成器泛化，从图像中提取判别性嵌入；2) 在所学嵌入空间上，以少样本k近邻分类器进行检测/归因，仅用少量来自未见生成器的样本进行快速适配。

Result: 在少样本（每类150张）设置下，平均检测准确率91.3%，较现有方法提升5.2个百分点；在来源归因任务的开放集设定中，AUC提升14.70%，OSCR提升4.27%。

Conclusion: 该框架在无需全面重训的情况下，对新出现的生成器具有良好可适配性与泛化能力，为可扩展的合成图像检测与取证归因提供了有效方案。

Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.
  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.
  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\% and 4.27\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.

</details>


### [84] [EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering](https://arxiv.org/abs/2511.16542)
*Pierrick Bournez,Luca Savant Aira,Thibaud Ehret,Gabriele Facciolo*

Main category: cs.CV

TL;DR: EOGS++在卫星遥感中将3D Gaussian Splatting拓展为可直接处理原始高分辨率全色影像的端到端方法，并把光流驱动的束束平差嵌入训练以联合优化相机位姿；结合早停与TSDF后处理，取得更清晰重建与更高几何精度，在IARPA 2016与DFC2019上达SOTA，建筑MAE由1.33降至1.19。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF类方法训练代价高且常依赖外部预处理与独立的位姿优化；初版EOGS虽高效但仍需要外部流程，且在几何精度与噪声控制上有提升空间。需要一个直接面向原始卫星全色数据、训练中自带位姿优化、并进一步提高重建质量与效率的方案。

Method: 在EOGS基础上提出EOGS++：1) 直接对原始高分辨率全色数据进行训练，无需外部预处理；2) 利用光流约束在训练过程中联合进行束束平差（bundle adjustment），内嵌位姿优化，摆脱外部优化工具；3) 加入训练早停策略；4) 采用TSDF后处理以提升表面连贯性与几何精度；整体保持Gaussian Splatting的快速训练优势。

Result: 在IARPA 2016与DFC2019数据集上获得SOTA重建质量与效率，相比原EOGS和其他NeRF系方法有明显优势；建筑区域的平均MAE从1.33降至1.19，重建更锐利、几何更准确。

Conclusion: EOGS++证明了在地球观测场景中，将光流引导的内嵌束束平差与Gaussian Splatting结合的有效性，既维持高效训练，又提升重建质量与几何精度，为卫星影像三维重建提供了更实用的端到端方案。

Abstract: Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models

</details>


### [85] [Progressive Supernet Training for Efficient Visual Autoregressive Modeling](https://arxiv.org/abs/2511.16546)
*Xiaoyue Chen,Yuling Shi,Kaiyuan Li,Huandong Wang,Yong Li,Xiaodong Gu,Xinlei Chen,Mingbao Lin*

Main category: cs.CV

TL;DR: 提出VARiant，在视觉自回归多尺度生成中按尺度自适应减深网络：早期尺度用全深度，后期尺度用子网并权重共享；配合渐进式训练，兼顾质量与效率，实现显著降显存与加速，并支持运行时零成本切换深度。


<details>
  <summary>Details</summary>
Motivation: VAR的“下一尺度”生成虽步数少，但多尺度逐步生成导致KV缓存累积，显存开销大，部署受限；同时观察到尺度-深度的非对称依赖：早期尺度强依赖深度，后期尺度对减深更鲁棒。

Method: 在30层VAR中等距采样得到多种子网（16到2层），早期尺度用全30层，后期尺度改用较浅子网，且与全网共享权重，实现单模型可变深度；为解决权重共享带来的优化冲突，提出渐进式训练策略，在固定训练比下同时提升全网与子网的生成质量。

Result: 在ImageNet上，相比预训练VAR-d30（FID 1.95），VARiant-d16/d8几乎等效（FID 2.05/2.12），显存下降40–65%；VARiant-d2在可接受质量损失（FID 2.97）下实现3.5×加速与80%显存降低。

Conclusion: 利用尺度-深度不对称性与共享权重子网，VARiant在保持单模型的前提下，提供从高质到极致高效的弹性部署与零成本运行时深度切换，突破质量-效率权衡的帕累托边界。

Abstract: Visual Auto-Regressive (VAR) models significantly reduce inference steps through the "next-scale" prediction paradigm. However, progressive multi-scale generation incurs substantial memory overhead due to cumulative KV caching, limiting practical deployment.
  We observe a scale-depth asymmetric dependency in VAR: early scales exhibit extreme sensitivity to network depth, while later scales remain robust to depth reduction. Inspired by this, we propose VARiant: by equidistant sampling, we select multiple subnets ranging from 16 to 2 layers from the original 30-layer VAR-d30 network. Early scales are processed by the full network, while later scales utilize subnet. Subnet and the full network share weights, enabling flexible depth adjustment within a single model.
  However, weight sharing between subnet and the entire network can lead to optimization conflicts. To address this, we propose a progressive training strategy that breaks through the Pareto frontier of generation quality for both subnets and the full network under fixed-ratio training, achieving joint optimality.
  Experiments on ImageNet demonstrate that, compared to the pretrained VAR-d30 (FID 1.95), VARiant-d16 and VARiant-d8 achieve nearly equivalent quality (FID 2.05/2.12) while reducing memory consumption by 40-65%. VARiant-d2 achieves 3.5 times speedup and 80% memory reduction at moderate quality cost (FID 2.97). In terms of deployment, VARiant's single-model architecture supports zero-cost runtime depth switching and provides flexible deployment options from high quality to extreme efficiency, catering to diverse application scenarios.

</details>


### [86] [Lite Any Stereo: Efficient Zero-Shot Stereo Matching](https://arxiv.org/abs/2511.16555)
*Junpeng Jing,Weixun Luo,Ye Mao,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: 提出Lite Any Stereo：超轻量立体匹配模型，兼具高效与强零样本泛化，训练于百万级数据，通过紧凑骨干+混合代价聚合+三阶段训练，跨四个真实基准夺冠，计算量<1%却达/超SOTA准确度。


<details>
  <summary>Details</summary>
Motivation: 现有立体匹配方法追求精度导致模型庞大，社区普遍认为高效小模型缺乏零样本能力；需要打破“高效=泛化差”的认知，在保持极低算力成本下实现强泛化、跨数据集零样本迁移。

Method: 1) 设计紧凑且可扩展的表达性骨干网络；2) 提出混合（hybrid）代价聚合模块以高效建模匹配关系；3) 基于百万级数据的三阶段训练策略，专门处理从模拟到真实的域间差异（sim-to-real）。

Result: 在四个常用真实世界基准上排名第一；在无需先验的SOTA精确方法面前，达到或超越其精度，同时计算成本低于1%。

Conclusion: 超轻量模型也可实现强零样本泛化与高精度，通过合适的网络设计与大规模、分阶段训练能跨域稳健泛化，为高效立体匹配设定了新基准。

Abstract: Recent advances in stereo matching have focused on accuracy, often at the cost of significantly increased model size. Traditionally, the community has regarded efficient models as incapable of zero-shot ability due to their limited capacity. In this paper, we introduce Lite Any Stereo, a stereo depth estimation framework that achieves strong zero-shot generalization while remaining highly efficient. To this end, we design a compact yet expressive backbone to ensure scalability, along with a carefully crafted hybrid cost aggregation module. We further propose a three-stage training strategy on million-scale data to effectively bridge the sim-to-real gap. Together, these components demonstrate that an ultra-light model can deliver strong generalization, ranking 1st across four widely used real-world benchmarks. Remarkably, our model attains accuracy comparable to or exceeding state-of-the-art non-prior-based accurate methods while requiring less than 1% computational cost, setting a new standard for efficient stereo matching.

</details>


### [87] [NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening](https://arxiv.org/abs/2511.16566)
*Misaal Khan,Mayank Vatsa,Kuldeep Singh,Richa Singh*

Main category: cs.CV

TL;DR: NutriScreener提出一种结合CLIP视觉嵌入、检索增强与多姿态图注意力的系统，实现从儿童图像进行营养不良检测与人体测量预测，兼顾泛化与类不平衡问题；在多数据集与临床评估中表现出较高准确性与可部署性，特别在使用人口统计匹配知识库时显著提升召回与降低RMSE。


<details>
  <summary>Details</summary>
Motivation: 现有营养不良筛查依赖人工测量，流程繁琐、可扩展性差，导致早期发现与干预受限；同时，真实场景下数据分布多样、正负样本不平衡，模型泛化不足。

Method: 提出NutriScreener：1) 用CLIP获取图像视觉表征；2) 检索增强：基于类别提升与人口统计匹配的知识库检索相关先验；3) 多姿态图注意力网络整合多视角/多姿态关键点和上下文；4) 结合上下文感知进行营养状态分类并预测人体测量（身高、臂围等）；并在跨数据集设置下训练/评估。

Result: 在AnthroVision等共2141名儿童上训练测试，并在ARAN与自建CampusPose等跨洲数据上评估：召回0.79、AUC 0.82，人体测量RMSE显著降低；使用人口统计匹配知识库时，跨数据集召回最高提升25%，RMSE最高降低3.5 cm。医生主观评估准确性4.3/5、效率4.6/5。

Conclusion: NutriScreener在低资源环境中提供可扩展、准确的早期营养不良筛查与测量方案，检索增强与多姿态图注意力有效提升泛化和类不平衡下的表现，具备部署就绪度。

Abstract: Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.

</details>


### [88] [POMA-3D: The Point Map Way to 3D Scene Understanding](https://arxiv.org/abs/2511.16567)
*Ye Mao,Weixun Luo,Ranran Huang,Junpeng Jing,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: POMA-3D 提出首个基于点图（point maps）的自监督3D表示学习框架，借助2D基础模型先验与多视角几何一致性，作为强大3D骨干支持多种任务，仅用几何输入即可泛化。


<details>
  <summary>Details</summary>
Motivation: 3D表示学习缺乏大规模预训练先验与数据，且3D数据格式与2D基础模型不兼容；需要在保留全局几何的同时，能无缝利用2D模型能力，并解决多视角下的几何一致性问题。

Method: 1) 点图表示：将显式3D坐标编码到2D网格，兼容2D模型输入且保留全局3D几何。2) 视图-场景对齐：将2D先验迁移到3D，通过对齐机制将多视图信息汇聚成场景级表示。3) POMA-JEPA：联合嵌入-预测架构，跨多视角约束点图特征的几何一致性。4) ScenePoint 数据集：从6.5K室内RGB-D场景与100万2D图像场景构建用于大规模预训练。

Result: 在多项下游任务上作为通用或专用3D骨干表现强劲，包括3D问答、具身导航、场景检索与具身定位；仅使用3D坐标输入即取得优异性能。

Conclusion: 点图为3D场景理解提供了新的自监督路径，能有效嫁接2D基础模型先验并缓解3D数据与预训练匮乏问题，展现出强泛化与任务适应性。

Abstract: In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/

</details>


### [89] [Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks](https://arxiv.org/abs/2511.16574)
*Nirjhor Datta,Md. Golam Rabiul Alam*

Main category: cs.CV

TL;DR: 提出“Erase to Retain”框架，用教师-学生蒸馏与LoRA低秩子空间更新，在不完全重训下对医疗分割进行可控遗忘：在指定“忘记集”上强力反对教师预测以抹除特定病灶/类别表征，随后仅调头部轻恢复保持总体性能。实验证明在ISIC与CHASE上可显著降低忘记集性能同时保持保留集/验证集表现；在ISIC分类上亦呈现相似趋势。


<details>
  <summary>Details</summary>
Motivation: 隐私合规（被遗忘权）、伦理部署与持续数据修订要求模型能选择性“忘记”特定样本或结构，而不牺牲对非敏感解剖的理解且无需代价高昂的全量重训。现有方法要么遗忘不彻底、要么破坏整体性能，缺少可控、可逆、低开销的医疗影像遗忘方案。

Method: 教师-学生蒸馏框架：在解码器低秩子空间引入LoRA模块，仅更新低秩分量以约束改动范围。两阶段：1）强遗忘阶段：在指定forget subset上对LoRA进行对抗式优化，使学生与教师在高置信区域产生矛盾，迫使抹除病灶/类别表征；2）温和恢复阶段：冻结主体，仅对输出头进行有监督微调，恢复在保留数据上的泛化。目标是擦除局部/类别表征同时保留全局解剖理解。

Result: ISIC分割：忘记集IoU从0.875降至0.509，同时保留与验证集IoU维持在0.647–0.677区间。跨域CHASE：持续降低忘记集IoU且保留/验证集性能稳定。ISIC分类：忘记集准确率由87.0%降至64.1%，保留集准确率由83.9%升至90.6%。

Conclusion: 基于LoRA的子空间遗忘在医疗影像中实现可控、可逆、代价低的选择性遗忘，能在“该忘的忘、该保的保”的前提下维持实用性能，为负责任的模型更新与隐私合规提供可行途径。

Abstract: The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement.
  For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent.
  These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.

</details>


### [90] [TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding](https://arxiv.org/abs/2511.16595)
*Boshen Xu,Zihan Xiao,Jiaze Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Qin Jin*

Main category: cs.CV

TL;DR: TimeViper 提出一种混合 Mamba-Transformer 的视觉-语言模型，利用跨模态令牌转移与压缩（TransV）在保持理解能力的同时大幅减少冗余视觉令牌，从而高效处理超长视频（>10k帧），在多基准上达到接近或领先SOTA，并给出对混合层注意力行为的可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 长视频理解需要在极长时间尺度上高效处理视觉序列，但纯Transformer在计算与内存上代价高昂，且多模态堆叠中存在视觉信息在LLM深层逐步转移到文本令牌、造成大量视觉令牌冗余的问题。

Method: 1) 采用混合骨干：将状态空间模型（Mamba）的长程高效建模与Transformer的注意力表达力结合；2) 通过实证揭示“视觉到文本的信息聚合”现象；3) 设计TransV模块，将视觉令牌的信息转移并压缩进指令/文本令牌，减少冗余并保持多模态对齐与理解能力；4) 分析Mamba与Transformer层的注意力/动态行为以提升可解释性。

Result: 在多项长视频基准上，TimeViper可处理小时级视频（>10,000帧），同时达到与SOTA相当或更优的性能；实验显示视觉令牌显著冗余，TransV能有效压缩且不损精度；对混合架构层级行为的可视化与分析提供了新见解。

Conclusion: 混合Mamba-Transformer结合TransV的令牌转移压缩是长视频理解的有效路径，既提升效率、扩展可处理帧数，又保持竞争性能，并为混合模型的可解释性与进一步压缩提供基础。

Abstract: We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.

</details>


### [91] [Generative AI for Enhanced Wildfire Detection: Bridging the Synthetic-Real Domain Gap](https://arxiv.org/abs/2511.16617)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 用生成式AI合成带标注的烟雾数据集，并结合无监督域自适应与风格迁移、GAN、抠图等方法，缩小合成与真实差异，提升野火烟羽分割与早期检测效果。


<details>
  <summary>Details</summary>
Motivation: 野火早期发现依赖对烟羽的及时识别，但真实、带像素级标注的烟雾数据稀缺，限制了深度网络在定位/分割任务上的表现，需要通过数据增广与跨域技术缓解数据瓶颈。

Method: 1) 利用生成式AI合成大规模、带精确标注的烟雾数据集；2) 采用无监督域自适应策略，将在合成域训练的模型迁移到真实域用于烟羽分割；3) 融合风格迁移、GAN与图像抠图，提高合成数据的逼真度并缩小域差异。

Result: 在合成到真实的迁移场景中，上述策略能有效缩小性能差距，分割与检测精度提升（摘要未给出具体数值）。

Conclusion: 通过高质量合成数据与域自适应的联合，可显著改善野火烟雾分割的泛化能力，为可扩展、准确的早期野火检测提供可行路径。

Abstract: The early detection of wildfires is a critical environmental challenge, with timely identification of smoke plumes being key to mitigating large-scale damage. While deep neural networks have proven highly effective for localization tasks, the scarcity of large, annotated datasets for smoke detection limits their potential. In response, we leverage generative AI techniques to address this data limitation by synthesizing a comprehensive, annotated smoke dataset. We then explore unsupervised domain adaptation methods for smoke plume segmentation, analyzing their effectiveness in closing the gap between synthetic and real-world data. To further refine performance, we integrate advanced generative approaches such as style transfer, Generative Adversarial Networks (GANs), and image matting. These methods aim to enhance the realism of synthetic data and bridge the domain disparity, paving the way for more accurate and scalable wildfire detection models.

</details>


### [92] [SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking](https://arxiv.org/abs/2511.16618)
*Haofeng Liu,Ziyue Wang,Sudhanshu Mishra,Mingqi Gao,Guanyi Qin,Chang Han Low,Alex Y. W. Kong,Yueming Jin*

Main category: cs.CV

TL;DR: 提出SA-SV外科交互式视频分割基准与SAM2S模型，在外科场景实现更稳定长时跟踪和更强零样本泛化；在J&F上显著超过SAM2与其微调版，并保持实时速度。


<details>
  <summary>Details</summary>
Motivation: 现有iVOS如SAM2虽具通用与交互优势，但在外科领域存在域间差异、遮挡多、器械细长快速运动、光照体液干扰，以及长时跟踪易漂移、跨数据标注不一致等问题，缺乏大规模、实例级时空标注基准限制了算法发展与评测。

Method: 1) 构建SA-SV：覆盖8类手术、61k帧、1.6k实例级“masklet”时空标注，支持长时跟踪与零样本评测。2) 提出SAM2S：在SAM2基础上引入三项改进——(a) DiveMem多样化可训练记忆模块，提升长时稳健跟踪；(b) 时序语义学习，增强器械语义与运动建模；(c) 面向多源数据的歧义鲁棒学习，缓解标注不一致。并在SA-SV上进行微调。

Result: 在SA-SV上微调可使SAM2平均J&F提升12.99；SAM2S进一步达到80.42的平均J&F，较原始SAM2和微调版分别提升17.10和4.11，同时推理速度达68 FPS，并具备良好的零样本泛化。

Conclusion: 提供了规模化、实例级时空标注的外科iVOS基准SA-SV，并在其上通过SAM2S实现对SAM2在外科场景的系统增强，兼顾精度、长时稳健性、实时性与泛化，为计算机辅助手术中的目标分割与跟踪提供强有力的基础设施和模型方案。

Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.

</details>


### [93] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 针对LVIS长尾分布目标检测，基于Faster R-CNN改进BAGS并结合度量学习与kNN推理，mAP提升到24.5%，优于24.0%。


<details>
  <summary>Details</summary>
Motivation: 现实数据呈长尾分布，常见类样本多、稀有类样本少，导致检测模型偏向头部类别，尾部类别性能显著下降。现有方法多在COCO等均衡数据上优化，难以应对LVIS这类极端类别不均衡情形，亟需提升尾类检测能力并在总体mAP上取得突破。

Method: 1) 以两阶段Faster R-CNN为骨干；2) 在Balanced Group Softmax(BAGS)基础上做改进以缓解类别不平衡；3) 提出尾类特征可能在头类特征空间中形成更小更密集簇的假设，引入度量学习，使类间分离、类内紧致；4) 推理阶段采用kNN对特征嵌入进行分类以强化稀有类判别。

Result: 在LVISv1（1203类、16.4万图像）上取得SOTA，mAP达24.5%，超过此前24.0%的基线；度量学习与kNN对尾类分类尤其有效。

Conclusion: 改进的BAGS结合度量学习与kNN推理能够有效缓解长尾检测中的类别偏置，提升尾部类别与整体检测性能；在复杂长尾数据集上具备通用价值。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [94] [Adaptive Guided Upsampling for Low-light Image Enhancement](https://arxiv.org/abs/2511.16623)
*Angela Vivian Dcosta,Chunbo Song,Rafael Radkowski*

Main category: cs.CV

TL;DR: 提出自适应引导上采样（AGU），用于低光照图像的实时放大，同时降噪与提锐，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于引导图像的方法在低光照场景中受限：目标图像噪声高、亮度低，无法提供足够的结构与细节供引导，导致结果改善有限。

Method: 以少量成对的低光/高亮图像为样本，进行多参数（多目标）优化，学习低光特征与亮图特征之间的关联；在推理时以引导方式将学到的亮图特性（如去噪、锐化、细节）转移到低质、低分辨率输入上，实现上采样与质量增强。

Result: 在低光放大任务上实现实时推理，生成高质量图像；实验表明在该用例上优于当前最先进方法。

Conclusion: AGU通过学习跨域特征映射并进行自适应引导，上采样同时兼顾降噪与锐化，适用于低光实时增强，性能与质量均优于SOTA。

Abstract: We introduce Adaptive Guided Upsampling (AGU), an efficient method for upscaling low-light images capable of optimizing multiple image quality characteristics at the same time, such as reducing noise and increasing sharpness. It is based on a guided image method, which transfers image characteristics from a guidance image to the target image. Using state-of-the-art guided methods, low-light images lack sufficient characteristics for this purpose due to their high noise level and low brightness, rendering suboptimal/not significantly improved images in the process. We solve this problem with multi-parameter optimization, learning the association between multiple low-light and bright image characteristics. Our proposed machine learning method learns these characteristics from a few sample images-pairs. AGU can render high-quality images in real time using low-quality, low-resolution input; our experiments demonstrate that it is superior to state-of-the-art methods in the addressed low-light use case.

</details>


### [95] [SAM 3D: 3Dfy Anything in Images](https://arxiv.org/abs/2511.16624)
*SAM 3D Team,Xingyu Chen,Fu-Jen Chu,Pierre Gleize,Kevin J Liang,Alexander Sax,Hao Tang,Weiyao Wang,Michelle Guo,Thibaut Hardin,Xiang Li,Aohan Lin,Jiawei Liu,Ziqi Ma,Anushka Sagar,Bowen Song,Xiaodong Wang,Jianing Yang,Bowen Zhang,Piotr Dollár,Georgia Gkioxari,Matt Feiszli,Jitendra Malik*

Main category: cs.CV

TL;DR: SAM 3D 是一个从单张图像生成带几何、纹理与布局的三维对象重建模型，通过大规模人机协同标注与多阶段训练，显著提升在自然场景中的重建质量，并在人工偏好测试中以至少5:1胜出。


<details>
  <summary>Details</summary>
Motivation: 在真实自然图像中，遮挡、杂乱和上下文依赖使三维重建困难，现有方法受限于缺乏大规模、贴近真实的带纹理与姿态标注数据（“3D 数据壁垒”）。作者希望构建可在复杂场景中稳健工作的单视图三维重建模型。

Method: 提出人-模型协同的标注流水线，生成大规模、具视觉对齐的对象形状、纹理与位姿数据；采用“合成预训练 + 真实对齐”的多阶段训练框架，实现从合成到实景的域迁移与对齐；模型直接从单张图像预测几何、纹理与布局。

Result: 在真实世界对象与场景上，相比近期方法取得显著提升；在人类偏好测试中至少以5:1的比例获胜；还将发布代码、权重、在线演示与新的在野三维重建基准。

Conclusion: 通过大规模人机协同数据与多阶段训练，SAM 3D 打破三维数据瓶颈，显著改善自然场景下的单视图三维重建，并为社区提供可复现资源与更具挑战的评测基准。

Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.

</details>


### [96] [SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction](https://arxiv.org/abs/2511.16635)
*Guolin Huang,Wenting Chen,Jiaqi Yang,Xinheng Lyu,Xiaoling Luo,Sen Yang,Xiaohan Xing,Linlin Shen*

Main category: cs.CV

TL;DR: 提出SurvAgent：一种分层CoT增强的多智能体系统，用于多模态（病理WSI+基因）生存预测，通过案例库构建与多专家推理两阶段，实现可解释、可检索、可复用的经验学习，在5个TCGA队列上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床生存分析需要高准确性与高可解释性，但现有方法对临床友好的透明度不足；既有可解释病理代理在生存预测上存在三大短板：难以融合多模态、ROI探索不足、无法利用历史经验。

Method: 两阶段框架：1）WSI-Gene CoT案例库构建：对WSI进行分层层级分析（低倍筛查、跨模态相似性感知补丁挖掘、置信度感知补丁挖掘），对基因按六类功能进行分层分析；两者均生成带链式推理的结构化报告并入库用于经验学习。2）基于二分的多专家智能体推理：通过RAG检索相似案例，融合多模态报告与专家预测，采用渐进式区间细化进行生存期推断。

Result: 在5个TCGA队列上，SurvAgent在性能上优于传统方法、商用多模态大语言模型和医疗智能体；实现更强可解释性与可迁移性。

Conclusion: SurvAgent为精准肿瘤学中的可解释生存预测提供新范式：多模态融合+层级CoT+案例检索的多智能体体系，兼顾性能与临床可解释性。

Abstract: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.

</details>


### [97] [TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming](https://arxiv.org/abs/2511.16642)
*Zeyuan Yin,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出TRIM，通过时间轨迹裁剪与实例掩码去噪，加速3D高斯扩散模型推理，同时保持甚至提升生成质量，并支持推理时可扩展采样。


<details>
  <summary>Details</summary>
Motivation: 3D高斯扩散模型因包含海量高斯基元，单步去噪和去噪后处理都很耗时，导致生成慢、沿采样轨迹扩展性差。需要一种在不重新训练或不牺牲质量的情况下，提高推理效率的方法。

Method: 提出后训练(post-training)框架TRIM，包括两部分：1) 轨迹裁剪(Temporal Trajectory Reduction)：用轻量选择器模型对来自多噪声采样的潜在高斯基元进行质量潜力评估，及早选择高质量候选并丢弃其余，减少后续去噪轨迹数；2) 实例掩码去噪(Spatial Instance Mask Denoising)：通过实例/前景掩码过滤冗余背景区域，对可学习高斯基元进行剪枝，从而在每个去噪步减少计算。支持推理时扩展采样而不进行昂贵的端到端扩展。

Result: 在多项实验中，TRIM在不降低(甚至提升)生成质量的前提下显著缩短3D生成时间，并降低每步计算量；展示了优于基线的效率—质量折中。

Conclusion: TRIM作为一个后训练加速策略，通过时间与空间双重“修剪”有效提升3D高斯扩散模型的推理效率和可扩展性，适用于推理阶段扩展而无需昂贵端到端训练。

Abstract: Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{https://github.com/zeyuanyin/TRIM}{link}$.

</details>


### [98] [Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision](https://arxiv.org/abs/2511.16650)
*Shuyu Cao,Chongshou Li,Jie Xu,Tianrui Li,Na Zhao*

Main category: cs.CV

TL;DR: 提出用于3D层级语义分割的新框架：主干3DHS分支+辅助判别分支，采用“后期解耦”多解码器、粗到细引导与一致性，以及基于语义原型的双分支互监督，以缓解跨层级冲突与类别不平衡，取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D场景需要多粒度、多层级的语义理解。现有3DHS方法存在两大痛点：1) 多标签共享参数的训练在跨层级优化时产生冲突，导致某些层级欠拟合/过拟合；2) 多层级普遍存在类别不平衡，模型容易被多数类主导，少数类性能较差。

Method: 设计由主3DHS分支与辅助判别分支组成的框架：1) 后期解耦的多解码器结构，为各层级设置独立解码器，同时通过粗到细的层级引导与一致性约束缓解跨层级冲突；2) 基于语义原型的双分支监督机制，学习类别级判别性点云特征，主辅分支进行互监督，以提升类别不平衡下的辨识能力。核心组件可插拔。

Result: 在多数据集与多骨干网络上取得SOTA的3DHS性能；实验显示所提组件对现有方法也具有普适增益。

Conclusion: 后期解耦的多解码器与语义原型双分支互监督可有效缓解跨层级冲突与类别不平衡，显著提升3D层级语义分割的鲁棒性与精度，且具备良好通用性和可插拔性。

Abstract: 3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.

</details>


### [99] [Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation](https://arxiv.org/abs/2511.16653)
*Md. Samiul Alim,Sharjil Khan,Amrijit Biswas,Fuad Rahman,Shafin Rahman,Nabeel Mohammed*

Main category: cs.CV

TL;DR: 提出一种教师引导的一次性全局非结构化剪枝框架，将知识蒸馏融入重要性分数估计，用教师梯度信号选保关键权重，实现高稀疏、低精度损失、训练计算更省，并在多数据集上优于EPG/EPSD、接近或超过COLT但更高效。


<details>
  <summary>Details</summary>
Motivation: 非结构化剪枝虽有效，但常需多轮“训练-剪枝-再训练”，计算开销大；现有方法多在剪枝后用KD补救，未在评分阶段利用教师信息，难以精准保留对任务与知识迁移都关键的参数。

Method: 提出教师引导的重要性分数估计：在计算权重重要性时引入教师模型提供的梯度/蒸馏信号，从而衡量参数对任务损失与教师知识对齐的共同贡献；据此执行一次性全局剪枝，删除冗余权重；随后进行稀疏感知的再训练，可选带/不带KD，且不恢复已剪除连接。

Result: 在CIFAR-10/100与TinyImageNet上，方法在高稀疏率下保持较小性能下降；在高稀疏情形下优于EPG与EPSD；相较迭代方案COLT，达到相近或更好精度且计算更低。

Conclusion: 将KD与重要性评估深度融合的一次性全局剪枝，能在显著压缩模型的同时保持性能，并降低训练与部署成本，适合资源受限环境。

Abstract: Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.

</details>


### [100] [Solving Spatial Supersensing Without Spatial Supersensing](https://arxiv.org/abs/2511.16655)
*Vishaal Udandarao,Shyamgopal Karthik,Surabhi S. Nath,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.CV

TL;DR: 论文对Cambrian-S在“空间超感知”视频世界模型上的贡献与局限进行复审：提出极简基线NoSense即可几乎解出VSR，且对VSC做“视频重复”检验使Cambrian-S性能崩溃，说明基准与推断策略都被捷径所主导。


<details>
  <summary>Details</summary>
Motivation: Cambrian-S声称通过新基准（VSR/VSC）与定制推断提升“空间超感知”。作者动机是检验：这些基准是否真在测量空间超感知与跨时整合能力；Cambrian-S的推断收益是否源自鲁棒感知还是利用了数据/基准中的捷径。

Method: 1) 设立极简基线NoSense：舍弃时序结构，仅用SigLIP的bag-of-words表示来做VSR。2) 设计sanity check——VSC-Repeat：将每段视频自连接1-5次，不改变独特物体数，检测模型是否保持计数不变、是否能识别重复场景。3) 比较Cambrian-S定制推断在原始与Repeat条件下的性能。

Result: NoSense在VSR上达到≈95%准确率（即便4小时视频），显示无需空间认知/世界建模即可高分；在VSC-Repeat中，Cambrian-S平均相对准确率从42%降至0%，表明其推断强依赖“房间不再被重复访问”的捷径。

Conclusion: (i) 当前VSI-Super基准（尤其VSR/VSC）不足以可靠衡量空间超感知；(ii) Cambrian-S的预测式推断配方更多是在利用基准捷径而非实现稳健的空间超感知。作者附录包含Cambrian-S团队回应；代码已开源以便复现。

Abstract: Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity

</details>


### [101] [PartUV: Part-Based UV Unwrapping of 3D Meshes](https://arxiv.org/abs/2511.16659)
*Zhaoning Wang,Xinyue Wei,Ruoxi Shi,Xiaoshuai Zhang,Hao Su,Minghua Liu*

Main category: cs.CV

TL;DR: PartUV是一种面向部件的UV展开管线，针对噪声大、非流形等不良条件的AI生成网格，生成更少且与语义部件对齐的图块，同时保持低失真，并在稳健性、效率和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有UV展开方法在AI生成的嘈杂、起伏和条件不佳的网格上表现不稳，常产生大量破碎图块和次优边界，导致伪影并影响后续任务。因此需要一种能在复杂、退化网格上仍可生成少量、语义对齐且低失真的图块的方案。

Method: 以学习型部件分解PartField为基础，采用自顶向下的递归框架，将高层语义部件划分与几何启发式相结合：在保证每个图块失真低于用户阈值的前提下，最小化图块数量。集成并扩展参数化与打包算法，针对非流形与退化网格提供专门处理，并进行大规模并行化以提升效率；同时支持多贴图瓦片的部件级打包。

Result: 在四个多样数据集（人造物、CAD、AI生成、Common Shapes）上评测，PartUV在图块数量与接缝长度上优于现有工具与近期神经方法，失真可比，且在挑战性网格上成功率高，并解锁部件特定多瓦片打包等新应用。

Conclusion: 将语义部件分解与几何约束结合的PartUV能在困难网格上稳定地产生更少、对齐部件且低失真的UV图块，兼顾质量与效率，为复杂与AI生成网格的UV展开提供实用方案。

Abstract: UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.

</details>


### [102] [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](https://arxiv.org/abs/2511.16662)
*Eddie Pokming Sheung,Qihao Liu,Wufei Ma,Prakhar Kaushik,Jianwen Xie,Alan Yuille*

Main category: cs.CV

TL;DR: TriDiff-4D提出基于扩散的triplane重定姿管线，先生成规范空间3D头像与动作，再用第二个扩散模型驱动动画，实现长时、可控且高保真4D头像，显著提升时空一致性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成方法存在时序与几何不一致、感知伪影、动作不稳定、计算开销高、对动态控制有限等痛点，难以满足文本到4D头像的实际需求。

Method: 提出TriDiff-4D：1) 文本驱动先生成规范空间的3D头像与对应骨架动作序列；2) 采用“扩散式triplane重定姿”将规范头像按动作逐帧动画化；3) 自回归地逐帧生成任意长度4D序列，每帧仅需一次扩散推理；4) 从大规模3D与动作数据中显式学习3D结构与运动先验，实现骨架驱动与高时序一致性。

Result: 在实验中，相比现有方法显著提升外观保真度、3D几何准确性、时序一致性与动作精度；通过去掉优化环节，将生成时间从数小时降至数秒，尤其在复杂动作场景下优势明显。

Conclusion: TriDiff-4D提供一种高效、可控、可扩展的文本到4D头像生成方案，兼顾质量与速度并支持超长时序生成，为4D生成的实际应用落地奠定基础。

Abstract: With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.

</details>


### [103] [SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation](https://arxiv.org/abs/2511.16666)
*Zhenyuan Qin,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

TL;DR: 提出SceneDesigner，实现对多对象9D姿态（位置、尺度、朝向）的可控生成，借助CNOCS表示、两阶段RL微调与解耦采样，显著提升可控性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有可控图像生成对多对象9D姿态的同时控制不足，常出现可控性有限、质量下降、对象混淆与数据不均衡导致的长尾姿态表现差的问题。

Method: 在预训练基础模型上接入分支网络；提出从相机视角编码9D姿态信息的新表示CNOCS，具备强几何可解释性以提升训练稳定性；构建ObjectPose9D数据集并采用两阶段训练：先常规训练，再在重采样平衡数据上以奖励为目标进行强化学习微调；推理时提出解耦对象采样以缓解多对象场景中对象不足与概念混淆；支持个性化权重以实现参考主体的定制化姿态控制。

Result: 在多项定性与定量评测上优于现有方法，在多对象9D姿态的可控性和生成质量方面取得显著提升。

Conclusion: SceneDesigner通过CNOCS表示、RL两阶段训练与解耦采样，实现精确、灵活的多对象9DoF姿态操控，并在多数据源数据集上验证有效性，优于现有方法；代码已开源。

Abstract: Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.

</details>


### [104] [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/abs/2511.16668)
*Yang Luo,Xuanlei Zhao,Baijiong Lin,Lingting Zhu,Liyao Tang,Yuqi Liu,Ying-Cong Chen,Shengju Qian,Xin Wang,Yang You*

Main category: cs.CV

TL;DR: 提出V-ReasonBench，用于系统评测生成式视频模型在四类视频推理任务上的能力，并实证比较多种SOTA模型与图像模型，揭示维度差异与幻觉模式。


<details>
  <summary>Details</summary>
Motivation: 生成式视频模型（如Veo-3）展现出零样本推理迹象，但缺乏统一、可复现实证评估框架；现有评测多聚焦感知/字幕，难以量化结构化推理、空间、模式与物理等维度。

Method: 构建V-ReasonBench：由合成与真实图像序列组成，覆盖四大推理维度（结构化问题求解、空间认知、基于模式的推断、物理动态），设计可核验答案、可扩展、无歧义的任务；对六个SOTA视频模型进行维度化评测，并与强图像模型对比；分析常见幻觉行为；研究视频时长对“Chain-of-Frames”推理的影响。

Result: 六个视频模型在四个维度上表现差异显著，结构化、空间、模式、物理推理能力各有所长短；与强图像模型相比，视频模型并非全面占优；观察到系统性的幻觉模式；视频时长对链式逐帧推理存在可测影响。

Conclusion: V-ReasonBench提供统一、可复现的基准来量化视频推理，促进更可靠、符合人类期望的推理型视频模型发展。

Abstract: Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.

</details>


### [105] [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669)
*Junhao Cheng,Liang Hou,Xin Tao,Jing Liao*

Main category: cs.CV

TL;DR: 提出VNEP任务：用视频而非文本来回答“下一步事件预测”，并给出强化学习对齐VLM与VDM的模型VANS与数据集VANS-Data-100K，达成SOTA的预测与可视化效果。


<details>
  <summary>Details</summary>
Motivation: 语言模型在实际应用广泛，但视频生成多停留在娱乐；很多程序性与预测性问题仅用文本难以表达与教学（例如打领带）。因此将“下一事件预测”从文本回答扩展为视频回答，可更直观、定制化地支持学习与创意探索。

Method: 定义VNEP任务；提出VANS框架，用强化学习将视觉语言模型(VLM)与视频扩散模型(VDM)在共同奖励下联合对齐。核心为Joint-GRPO：以共享奖励同时优化VLM生成既准确又便于可视化的字幕/指令，及VDM生成与字幕及输入视觉上下文一致的视频。构建专用数据集VANS-Data-100K以支撑训练与评测。

Result: 在程序性与预测性基准上，VANS在视频事件预测与可视化质量上均达SOTA；模型能输出语义与视觉一致、对指令条件敏感的视频。

Conclusion: 将NEP从“说”扩展到“演示”的VNEP显著提升了程序学习与预测表达的直观性。VANS通过联合强化学习有效对齐VLM与VDM，实现高质量的视频回答，并以专用数据集推动该方向发展。

Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.

</details>


### [106] [Learning to Think Fast and Slow for Visual Language Models](https://arxiv.org/abs/2511.16670)
*Chenyu Lin,Cheng Chi,Jinlin Wu,Sharon Li,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 提出DualMindVLM：用简单RL让视觉语言模型按任务难度在“快思-慢想”间自动切换，兼顾精度与token效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理型VLM偏好冗长链式推理，计算与token成本高；人类则会按问题难度分配思考资源（简单快答、复杂深思）。作者希望让VLM具备类似双系统思维，提高效率而不降性能。

Method: 两阶段简单RL：1) 利用预训练VLM对不同问题自然产生的回答长度差异，将样本自动标注为快思或慢想（依据输出长度）。2) 采用GRPO训练，引入思维模式标签，促使模型学习双模推理并在推断时根据难度切换。

Result: DualMindVLM显著优于基座模型，在多项视觉推理任务上达到与SOTA相当的性能，同时保持极高的token效率（更短推理链、计算成本更低）。

Conclusion: 无需复杂标注或规则奖励，通过长度驱动标签+GRPO即可让VLM学会按难度自适应地快慢思维，获得SOTA级精度与更佳效率。

Abstract: When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.

</details>


### [107] [Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation](https://arxiv.org/abs/2511.16671)
*Ziyu Guo,Renrui Zhang,Hongyu Li,Manyuan Zhang,Xinyan Chen,Sifan Wang,Yan Feng,Peng Pei,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出TwiG：在视觉生成过程中交错插入文本推理（边生成边思考）的框架，提升上下文一致性与语义丰富度；探索零样本提示、SFT（TwiG-50K）与RL（TwiG-GRPO）三种训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在生成前做规划或在生成后做修正，缺少在生成过程中与视觉状态动态交互的“在线”多模态推理，导致局部一致性、全局语义与细节控制不足。

Method: 设计Thinking-while-Generating框架：在逐步生成视觉内容的同时插入可交互的文本推理回合，用于指导接下来区域与反思已合成区域；实现三种策略以揭示该交错机制的效果与取舍——(1) 零样本提示；(2) 基于自建TwiG-50K数据的有监督微调；(3) 基于定制的TwiG-GRPO进行强化学习优化。

Result: 交错推理与生成的动态耦合带来更具上下文意识与语义丰富的可视化结果；三种训练策略从无监督到SFT再到RL逐步提升了生成质量（摘要层面未给出定量，但展示了潜力）。

Conclusion: TwiG首次将“思考-生成”交替耦合到视觉生成流程中，证明在线多模态交互的价值；为后续在数据、算法（如RL目标）、与系统化评测上的研究打开方向；代码将开源。

Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.

</details>


### [108] [EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards](https://arxiv.org/abs/2511.16672)
*Omkat Thawakar,Shravan Venkatraman,Ritesh Thawkar,Abdelrahman Shaker,Hisham Cholakkal,Rao Muhammad Anwer,Salman Khan,Fahad Khan*

Main category: cs.CV

TL;DR: 提出EvoLMM：从单一LMM派生“提问者+解题者”两代理，自举生成视觉问题并用一致性自奖励学习，在无标注/无外部奖励下提升多模态数学推理，基于Qwen2.5-VL在ChartQA/MathVista/MathVision约+3%。


<details>
  <summary>Details</summary>
Motivation: 当前LMM虽具强推理感知，但训练依赖人工标注或外部验证奖励，限制自治与可扩展性。作者希望在完全无监督、无人工信号条件下提升推理能力，降低成本并提高可扩展性。

Method: 提出自进化框架EvoLMM：从同一基座创建两个协作代理——Proposer生成多样且与图像相关的问题；Solver在内部一致性原则下求解，并通过自奖励（无需真值或人工判断）形成闭环训练。动态反馈同时鼓励高信息量问题生成与结构化推理优化，仅使用原始训练图像。

Result: 以Qwen2.5-VL为基座，在多模态数学推理基准（ChartQA、MathVista、MathVision）上取得稳定提升，最高约3%。代码与模型开源。

Conclusion: EvoLMM在完全无监督设定下实现自我改进，简洁有效，可作为自改进LMM研究的强基线，展示了无需标注/外部奖励也能提升多模态推理的可行性。

Abstract: Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\sim$3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.

</details>


### [109] [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](https://arxiv.org/abs/2511.16673)
*Jing Wen,Alexander G. Schwing,Shenlong Wang*

Main category: cs.CV

TL;DR: 提出 NoPo-Avatar：从单/少视图图像重建可动画3D人体，无需相机/人体姿态输入；在无真值姿态的现实场景优于现有方法，在有真值姿态的实验室场景表现相当。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA在测试时依赖精确的人体与相机姿态作为条件，然而实际应用中姿态估计常含噪声，导致重建质量显著下降；需要一种对姿态误差不敏感、仅依赖图像的重建框架。

Method: 移除测试阶段对人体姿态与相机位姿的依赖，设计仅以图像为输入的重建流程（NoPo-Avatar）。通过学习式策略在多数据集上直接从外观线索恢复可动画头像/身体形状与姿态驱动参数，避免误差传播。

Result: 在THuman2.0、XHuman、HuGe100K等具有挑战性的数据集上：在无真值姿态（实际应用）条件下显著优于现有基线；在有真值姿态（实验室）条件下取得与现有方法可比的效果。

Conclusion: 去姿态依赖的重建在噪声姿态场景更稳健、适用范围更广；NoPo-Avatar在实践中更好，在受控环境下不逊色，验证了仅依赖图像的可动画3D人体重建的可行性与优势。

Abstract: We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).

</details>


### [110] [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](https://arxiv.org/abs/2511.16674)
*George Cazenavette,Antonio Torralba,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 他们提出“线性梯度匹配”（Linear Gradient Matching, LGM），在固定的大型自监督特征提取器上蒸馏出一小批合成图像，使在线性探针训练时，其梯度与真实数据诱导的梯度匹配，从而用极少数据达到或超越用大量真实数据线性评估的效果，并具有跨预训练模型的泛化与可解释性价值。


<details>
  <summary>Details</summary>
Motivation: 现有数据蒸馏主要面向从随机初始化训练整个模型，但现实中视觉系统多依赖大规模自监督预训练并只在线性头上做下游学习。如何为“线性探针”场景专门蒸馏数据、在固定特征上高效训练且具跨模型泛化，尚缺系统方法。

Method: 提出线性梯度匹配：优化少量合成图像，使其经预训练特征提取器后的特征，在线性分类器上的梯度近似真实数据产生的梯度。通过最小化合成-真实梯度差，得到用于线性评估的蒸馏数据；并在不同预训练主干（如DINO、CLIP）间验证跨模型泛化。

Result: 蒸馏得到的合成数据在线性探针任务上优于所有真实图像基线；还能跨预训练模型泛化，例如用DINO蒸馏的数据训练CLIP线性探针也具竞争力；在细粒度分类上表现尤为突出。

Conclusion: 为线性探针场景量身定制的数据蒸馏可通过梯度匹配实现高效且可泛化的合成数据，既提升性能，亦支持可解释性分析，如评估模型表征空间相似度与对对抗数据中虚假相关的敏感性。

Abstract: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.

</details>
