<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 179]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 论文通过系统实验刻画了文本到视频扩散模型在去噪时间步上“运动 vs 外观”的分工：前期主导运动与布局，后期主导外观细节，并据此提出仅在前期时间步进行训练与推理的简单配方，实现强运动迁移。


<details>
  <summary>Details</summary>
Motivation: 实践中有经验性做法：早期步影响运动/布局，晚期步影响外观，但缺乏系统量化与可操作边界。需要将运动与外观在时序上的竞争关系进行定量分离，以指导更高效的运动定制与编辑。

Method: 提出用“在指定时间步范围内注入新条件时外观编辑增益与运动保持损失之间的权衡”作为运动编码的代理指标。进行大规模量化实验，跨多种架构，沿去噪轨迹扫描时间步，绘制运动-外观竞争曲线，确定运动占优与外观占优的边界。基于此，只在运动占优的早期时间步进行训练与推理，实现一轮(one-shot)运动定制，无需去偏模块或特定损失。

Result: 在多种模型与数据上稳定观察到早期“运动主导”、后期“外观主导”的两阶段特性，并给出可操作的时间步边界。基于时间步约束的配方在运动迁移任务中取得强性能，且实现更简洁，无需额外去偏或专门目标函数。

Conclusion: 将经验性启发转化为可量化的时空解耦原则：沿扩散时间步存在清晰的运动-外观分治。利用该边界，仅在早期步进行训练/推理即可实现高质量运动迁移，且可即插即用地整合到现有运动迁移与编辑方法中。

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [2] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

TL;DR: 提出一套结合3D CNN与LSTM的实时ASL词级识别系统，基于视频流提取时空特征并建模序列依赖，在多数据集训练后取得0.71–0.99的F1，被部署于AWS与OAK-D边缘设备，实现可落地的无障碍应用。


<details>
  <summary>Details</summary>
Motivation: 弥合全球约7000万聋人与听力障碍者在沟通上的鸿沟，需要一种能在现实环境中实时、准确识别ASL手语的系统。

Method: 采用混合深度学习架构：3D CNN提取视频帧的时空特征，随后以LSTM捕捉手势的序列依赖；使用WLASL（2000词）、ASL-LEX（约2700词）与100个专家标注的ASL子集进行训练；并设计实时视频流处理流水线；在AWS上部署并支持OAK-D边缘推理。

Result: 在不同手语类别上获得F1=0.71–0.99的性能区间，实现实时推理能力，并完成云端与边缘部署。

Conclusion: 3D CNN+LSTM架构能有效识别ASL词级手势并适用于实际场景；系统在多源数据集上表现稳健，具备可扩展的部署路径，为无障碍沟通应用提供了实用方案。

Abstract: This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

</details>


### [3] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 提出AI增强的局部线性嵌入（LLE）方法，用于医疗计费与转录高维数据处理，显著提升准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 医疗计费与转录涉及高维、复杂数据，传统流程易出错且效率低；需要能降维并保持结构信息的算法，结合AI以自动化与提升质量。

Method: 将AI技术与LLE相结合：在高维医疗数据上进行非线性流形降维，配合自动化计费与转录流程；给出数学模型并在真实医疗场景中实验评估。

Result: 实验显示数据处理准确度与操作效率显著提升，相比基线在计费与转录任务中减少错误、加快流程。

Conclusion: AI增强LLE在医疗数据分析与业务流程自动化上有效，可作为进一步扩展到更广泛医疗应用的基础。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [4] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: VISTA 通过“感知-推理”解耦与显式信息瓶颈，让冻结的视觉传感器仅回答客观、短小的感知查询，文本LLM负责分解问题、规划查询并汇总视觉事实，再用强化学习对这一受控接口进行对齐训练，从而减少捷径依赖、提升鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 端到端VLM在视觉问答中常利用数据偏差/虚假相关而非真实因果证据，且微调后更易走捷径；需要一种能约束感知与推理交互、并可对“以证据为依据”的推理进行奖励对齐的训练范式。

Method: 提出VISTA框架：设定一个显式信息瓶颈的模块化接口。冻结的VLM作为“传感器”，仅响应简短、客观的感知查询；文本LLM作为“推理器”，将问题分解、规划多步查询并用自然语言汇总视觉事实。把该接口建模成奖励对齐的环境，用GRPO强化学习在仅641条多步问题上训练。使用Qwen2.5-VL与Llama3.2-Vision作为可互换传感器，评估跨传感器可迁移性与失败恢复能力。

Result: 在SpuriVerse对真实世界伪相关的鲁棒性测试上显著提升：Qwen-2.5-VL-7B提升+16.29%，Llama-3.2-Vision-11B提升+6.77%；在MMVP与平衡版SeedBench上保持有竞争力。VISTA可跨未见传感器迁移，并能识别与纠正感知失败。人类评估显示其推理轨迹更中性、较少依赖伪属性、且更明确落地于视觉证据，相比端到端VLM基线更可解释。

Conclusion: 通过以受控、可奖励的文本接口解耦感知与推理，VISTA减少了端到端VLM对伪相关的依赖、提升鲁棒性与可解释性，并具有跨传感器迁移与失败恢复能力，证明小规模RL训练即可带来显著稳健性收益。

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [5] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: 论文提出SAMM2D双编码器模型，在RSNA颅内动脉瘤数据集上取得AUC 0.686，相比临床基线提升32%，且发现强预训练下任何数据增广都会显著降性能。通过阈值校准可达95%灵敏度并优于平均放射科医师，经济模型估计每千名筛查患者节省约1390万美元。可视化显示模型关注血管相关区域，提示强预训练可能比复杂增广更重要。


<details>
  <summary>Details</summary>
Motivation: 动脉瘤检测困难，因病灶形态细微、类别极度不平衡、标注稀缺。医学影像低数据场景常依赖数据增广提升泛化，但其在强预训练特征下的真实效用尚不清楚。

Method: 提出SAMM2D：双编码器框架，使用强ImageNet预训练骨干；在RSNA动脉瘤数据集上系统评估，包括六种增广方案的消融实验；采用阈值校准以达到高灵敏度；使用Grad-CAM评估注意区域与专家标注的重叠（IoU）。

Result: 整体AUC=0.686，较临床基线提升32%；在六种增广设定下，未增广基线均优于增广版本1.75–2.23个百分点（p<0.01）；阈值校准后灵敏度95%，超过平均放射科医师；经济分析预测每1000名筛查患者节省约$13.9M；Grad-CAM中85%真阳性聚焦于相关血管，IoU达62%。

Conclusion: 在低数据医学影像中，强预训练已蕴含稳健不变性，额外增广可能冗余甚至破坏特征流形；优先采用强预训练与阈值校准，可获得临床有意义的提升与经济价值，复杂增广管线的收益有限。

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [6] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: 提出HookMIL：用可学习的“Hook Token”在弱监督病理WSI的MIL中高效聚合上下文，线性复杂度、可多模态初始化，并通过多样性与Hook间通信提升性能与可解释性，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统MIL丢失上下文；Transformer-MIL虽强但计算复杂度二次、冗余大。病理WSI需要既保留上下文又高效的框架，并能利用视觉-文本、空间转录组等先验以加速收敛、提升表示和可解释性。

Method: 设计HookMIL：引入少量可学习Hook Token作为上下文汇聚器，与实例（patch）进行双向注意力交互，整体为线性复杂度；Hook Token可从三类先验初始化：（i）关键patch视觉特征，（ii）病理视觉-语言模型文本嵌入，（iii）空间转录组-视觉模型的空间特征；加入Hook Diversity Loss促使不同Hook关注不同病理模式；提供Hook与Hook间通信以精炼上下文并去冗余；保持端到端训练。

Result: 在四个公开病理数据集上达到SOTA，同时计算更高效（避免Transformer二次复杂度、减少冗余），并带来更好的可解释性与更快收敛。

Conclusion: HookMIL通过小量可学习Hook Token的上下文聚合与多模态先验初始化，在效率、准确性和可解释性之间取得平衡，优于现有MIL与Transformer变体，且具有良好的扩展性。

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [7] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: 提出Tiny-YOLOSAM：用YOLOv12生成框提示配合TinySAM，并在未覆盖区域稀疏点采样，替代密集“segment-everything”，在COCO val2017显著提高覆盖与速度。


<details>
  <summary>Details</summary>
Motivation: SAM与TinySAM零样本分割质量高，但在全场景“segment-everything”模式下需要大量提示，延迟高、不实用；需要更快且保持高覆盖/质量的方案。

Method: 1) 复现TinySAM在COCO val2017的基线（与报告AP差0.03%）；2) 提出混合流水线：用YOLOv12检测显著前景并将检测框作为TinySAM的框提示；3) 对YOLO掩膜未覆盖区域进行稀疏点提示采样；4) 组合得到全场景分割。

Result: 在COCO val2017上，类无关覆盖与质量大幅提升：AR从16.4%→77.1%，mIoU从19.2%→67.8%；同时端到端推理时间从49.20秒/图降至10.39秒/图（M1 Pro CPU上约4.7倍加速）。

Conclusion: 检测器引导的框提示+针对性稀疏点采样可有效替代密集提示，实现实用的全场景分割，兼顾高覆盖/质量与显著速度提升。

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [8] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: 提出一种用于糖网病可解释诊断的多模态VLM少样本方法：按视网膜象限量化病灶、生成配对Grad-CAM热图（OCT与眼底），从而给出分类依据而非仅标注病灶位置。使用3000张眼底与1000张OCT数据集，提升实用性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有DR筛查缺医少护，人工病灶标注成本高，许多方法只基于单一模态且仅给出病灶高亮，难以满足临床对“分类推理过程”的需求。需要一种能用自然语言量化病灶、结合多模态并提供可视化依据的系统。

Method: 构建多模态可解释模型：以VLM进行少样本学习；在眼底图上按象限统计病灶分布，模拟眼科医生推理；同时在眼底与OCT上生成配对Grad-CAM，展示神经元权重与贡献区域；输出对DR严重度的自然语言解释与量化描述。数据集包含3000眼底与1000OCT。

Result: 模型能够在两种成像（眼底与OCT）上产生一致的热图与文本解释，定量指出各象限病灶分布，并据此进行DR严重度分类，较现有单模态、仅定位病灶的方法更具解释性与适用性。

Conclusion: 该方法以少样本多模态VLM与象限量化策略，提供面向临床的可解释DR诊断框架，兼顾定量与视觉解释，适用于筛查、治疗与科研场景，缓解人工标注与单模态局限，潜在改善患者结局。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [9] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: 提出TCFormer：仅5M参数、弱监督、面向边缘设备的人群计数Transformer，兼顾精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有人群计数依赖点级标注和重型骨干，标注成本高、推理开销大，难以在资源受限环境部署。

Method: 1) 采用高效ViT作特征提取，利用全局上下文获取语义丰富的拥挤特征；2) 设计可学习的密度加权平均（LDWA）模块，按预测密度对局部token动态重加权，实现区域特征自适应调制；3) 引入密度等级分类损失，将密度离散为若干等级，与全局计数回归联合优化；整体仅使用图像级总人数弱监督训练。

Result: 在ShanghaiTech A/B、UCF-QNRF、NWPU四个基准上获得与主流方法相当的精度，同时参数量仅约5M，推理内存与计算成本显著降低，性价比优于同类。

Conclusion: TCFormer在弱监督条件下，通过全局上下文建模、密度加权聚合与密度等级正则，实现小模型高精度的人群计数，适合在边缘设备部署。

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

</details>


### [10] [A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability](https://arxiv.org/abs/2512.22205)
*Md. Ismiel Hossen Abir,Awolad Hossain*

Main category: cs.CV

TL;DR: 提出定制CNN自动分类疟疾血涂片图像，准确率96%，并用SHAP/LIME/显著性图提升可解释性，优于/可比多种主流架构，适合资源受限地区快速诊断。


<details>
  <summary>Details</summary>
Motivation: 传统显微镜诊断对专家依赖高、敏感性低、在资源匮乏地区难以推广，亟需自动、准确、可解释且可在边缘环境部署的计算机辅助诊断方法。

Method: 构建并训练一个定制CNN对红细胞图像进行二分类（感染/未感染），并与ResNet50、VGG16、MobileNetV2、DenseNet121进行性能对比；使用SHAP、LIME和显著性图解释模型判别依据。

Result: 定制CNN达到96%准确率，两个类别precision与recall均>0.95；与多种主流架构相比具备竞争力。

Conclusion: 深度学习结合可解释AI可实现快速、准确且可理解的疟疾诊断，尤其适合资源受限环境的部署与应用。

Abstract: Malaria remains a prevalent health concern in regions with tropical and subtropical climates. The cause of malaria is the Plasmodium parasite, which is transmitted through the bites of infected female Anopheles mosquitoes. Traditional diagnostic methods, such as microscopic blood smear analysis, are low in sensitivity, depend on expert judgment, and require resources that may not be available in remote settings. To overcome these limitations, this study proposes a deep learning-based approach utilizing a custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. This study also compares the custom CNN with established deep learning architectures, including ResNet50, VGG16, MobileNetV2, and DenseNet121. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied. The proposed system shows how deep learning can provide quick, accurate and understandable malaria diagnosis, especially in areas with limited resources.

</details>


### [11] [Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2512.22214)
*Naichuan Zheng,Xiahai Lun,Weiyi Li,Yuchen Du*

Main category: cs.CV

TL;DR: 提出Signal-SGN++：结合拓扑自适应与时-频脉冲动力学的骨架动作识别模型，在保持能效的同时获得接近/优于GCN的精度。


<details>
  <summary>Details</summary>
Motivation: GCN善于捕捉骨架拓扑但计算密集、能耗高；SNN能量高效但难以同时建模人体动作的时频与拓扑耦合依赖。需要一种既能量高效又能捕捉时空-频谱与结构关系的框架。

Method: 构建拓扑感知的脉冲图框架Signal-SGN++。主干由1D Spiking Graph Convolution（1D-SGC）与Frequency Spiking Convolution（FSC）联合提取时空与频谱特征；在主干中嵌入Topology-Shift Self-Attention（TSSA）以在学习到的骨架拓扑之间自适应路由注意力、提升图级敏感性且不增复杂度；增设Multi-Scale Wavelet Transform Fusion（MWTF）辅助分支，将脉冲特征分解为多分辨率时频表示，并通过Topology-Aware Time-Frequency Fusion（TATF）引入结构先验以保持拓扑一致的频谱融合。

Result: 在大规模基准上，Signal-SGN++在显著降低能耗的前提下，超过现有SNN方法，并在精度-效率权衡上对比最先进GCN取得有竞争力甚至更优的表现。

Conclusion: 通过将拓扑自适应注意力与时频脉冲建模相结合，Signal-SGN++有效弥合GCN高能耗与SNN表达受限的鸿沟，提供了更优的准确率与能效折中，适用于低功耗动作识别场景。

Abstract: Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.

</details>


### [12] [VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition](https://arxiv.org/abs/2512.22217)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Fadi Dornaika,Cosimo Distante,Abdenour Hadid*

Main category: cs.CV

TL;DR: 提出VLM-PAR：在冻结SigLIP 2多语视觉-语言编码器上，通过轻量级跨模态对齐与融合，显著提升行人属性识别在多基准（PA100K/PETA/Market-1501）的准确率与均衡性，刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: PAR需预测服饰颜色、性别、配饰等细粒度属性，但普遍面临类别极度不均衡、属性间复杂相关性、以及跨域泛化差等难题。现有方法在鲁棒性与泛化上不足，亟需利用大规模视觉-语言预训练的语义先验并进行针对性跨模态细化。

Method: 构建模块化的视觉-语言框架VLM-PAR：以冻结的SigLIP 2多语编码器提取图像与文本提示嵌入；通过紧凑的交叉注意力融合模块对视觉特征进行精炼，使图像与提示嵌入对齐；利用提示引导的多标签判别实现属性预测。

Result: 在高度不均衡的PA100K上取得显著精度提升并达成新的SOTA；同时在PETA与Market-1501上实现平均准确率的大幅提升，显示良好的跨数据集泛化。

Conclusion: 大规模视觉-语言预训练与目标化的跨模态特征细化相结合，能够有效缓解类别不均衡与泛化难题，显著提升行人属性识别性能。

Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.

</details>


### [13] [Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark](https://arxiv.org/abs/2512.22218)
*Hieu Minh Nguyen,Tam Le-Thanh Dang,Kiet Van Nguyen*

Main category: cs.CV

TL;DR: 提出ViSignVQA：首个面向越南语标牌理解的大规模VQA数据集（10,762张图像、25,573问答），并基于OCR与越南语预训练模型建立基线与多智能体框架，显著提升文本型VQA性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的标牌文本对VQA至关重要，但低资源语言（如越南语）缺乏专门数据与评测基准；现有通用VQA模型对双语、口语化表达及复杂版式等场景文本适配不足。

Method: 1) 构建ViSignVQA数据集，覆盖越南标牌的语言与视觉多样性（双语、非正式措辞、颜色与布局等）。2) 将SwinTextSpotter作为越南语OCR，ViT5作为语言模型，适配SOTA VQA（BLIP-2、LaTr、PreSTU、SaL），并将OCR文本拼接到问题作为上下文。3) 设计结合感知与推理代理、并用GPT-4的多代理VQA框架，通过多数投票融合。

Result: 在基线中，加入OCR上下文可使F1最高提升至原来的约3.09倍（提升209%）。多代理框架通过多数投票在该数据集上达到75.98%准确率。

Conclusion: 专门面向越南场景文本的多模态数据与OCR集成策略显著增强文本型VQA能力。ViSignVQA为低资源语言建立了首个大规模基准，促进OCR+VQA模型的发展与评测，并强调领域特定资源的重要性。

Abstract: Understanding signboard text in natural scenes is essential for real-world applications of Visual Question Answering (VQA), yet remains underexplored, particularly in low-resource languages. We introduce ViSignVQA, the first large-scale Vietnamese dataset designed for signboard-oriented VQA, which comprises 10,762 images and 25,573 question-answer pairs. The dataset captures the diverse linguistic, cultural, and visual characteristics of Vietnamese signboards, including bilingual text, informal phrasing, and visual elements such as color and layout. To benchmark this task, we adapted state-of-the-art VQA models (e.g., BLIP-2, LaTr, PreSTU, and SaL) by integrating a Vietnamese OCR model (SwinTextSpotter) and a Vietnamese pretrained language model (ViT5). The experimental results highlight the significant role of the OCR-enhanced context, with F1-score improvements of up to 209% when the OCR text is appended to questions. Additionally, we propose a multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving 75.98% accuracy via majority voting. Our study presents the first large-scale multimodal dataset for Vietnamese signboard understanding. This underscores the importance of domain-specific resources in enhancing text-based VQA for low-resource languages. ViSignVQA serves as a benchmark capturing real-world scene text characteristics and supporting the development and evaluation of OCR-integrated VQA models in Vietnamese.

</details>


### [14] [On Extending Semantic Abstraction for Efficient Search of Hidden Objects](https://arxiv.org/abs/2512.22220)
*Tasha Pais,Nikhilesh Belulkar*

Main category: cs.CV

TL;DR: 利用2D多模态视觉语言模型的相关性激活作为“抽象对象”表示，结合历史摆放先验，实现对被遮挡隐藏物体的3D定位与补全，显著快于随机搜索。


<details>
  <summary>Details</summary>
Motivation: 家庭机器人常需寻找被遮挡的遗失物体，直接依赖VLM难以识别遮挡目标；若能把VLM的相关性图视为对象存在与位置的置信度，并结合环境中物体常见位置的历史先验，就可能更高效地进行隐藏物体搜索与定位。

Method: 提出语义抽象框架：将2D VLM的相关性/注意力热力图当作“抽象对象”表示；在隐藏物体场景中，融合多视角相关性图与场景几何，学习3D定位与形状补全；同时引入历史数据（物体常出现的位置分布）作为搜索先验，从而将无结构搜索变为有指导的推断。

Result: 模型可在首次尝试中准确预测并给出隐藏物体的完整3D位置与形状，相比天真的随机搜索显著更快、更准确。

Conclusion: 语义抽象的扩展使机器人能利用VLM相关性与历史先验高效定位被遮挡物体，为家庭机器人快速寻找遗失物体提供了可行途径。

Abstract: Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as "abstract object" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.

</details>


### [15] [VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs](https://arxiv.org/abs/2512.22226)
*Naishan Zheng,Jie Huang,Qingpei Guo,Feng Zhao*

Main category: cs.CV

TL;DR: VideoScaffold提出面向流式长视频理解的动态表示框架，通过自适应事件切分与分层整合，在保证细粒度语义的同时随时长扩展到高层事件推理，显著提升MLLM在离线与流式任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 长视频含有大量跨帧冗余且需要时间一致性表示。现有静态策略（稀疏采样、压缩、聚类）多为离线优化，应用于连续视频时易导致片段化或过度压缩，无法平滑从细粒度到抽象层次推理。

Method: 构建动态表示框架VideoScaffold，包含：1) 弹性尺度事件分割（EES）：基于预测引导的自适应分割，动态细化事件边界与粒度；2) 分层事件整合（HEC）：将语义相关片段逐级聚合为多层抽象。两者联动，使模型随视频推进从帧级理解过渡到事件级推理；框架可即插即用扩展图像型MLLM到连续视频。

Result: 在离线和流式视频理解基准上取得SOTA；在长时长、连续流场景中较现有静态方法更稳健，兼顾细粒度语义与高层抽象。

Conclusion: 动态、分层的事件表示是长视频流式理解的关键。VideoScaffold通过EES与HEC实现粒度自适应与层次聚合，兼容现有MLLM并在多基准上验证有效，适合拓展到更广泛的视频理解应用。

Abstract: Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.

</details>


### [16] [KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation](https://arxiv.org/abs/2512.22228)
*HaoNan Tang*

Main category: cs.CV

TL;DR: 提出KAN增强的FPN-Stem用于ViT前端，多尺度融合后用KAN卷积替换3x3平滑层，COCO上较ViTPose-S最高提升+2.0 AP。


<details>
  <summary>Details</summary>
Motivation: ViT在密集预测（如姿态估计）中前端常用简单patch划分，难以处理多尺度并在初期造成信息不可逆损失；现有“加注意力”式改造收效有限，亟需改进前端的多尺度特征融合质量。

Method: 保留经典FPN“上采样+相加”融合流程，但将末端的线性3x3平滑卷积替换为具更强非线性建模能力的KAN卷积层，以自适应校正多尺度融合带来的伪影；通过消融验证定位瓶颈在后融合非线性平滑而非注意力模块。

Result: 在COCO上，所提KAN-FPN-Stem相较轻量级ViTPose-S获得最高+2.0 AP的显著提升。

Conclusion: ViT前端性能瓶颈更多在“特征融合”而非“特征细化”；引入KAN算子替换FPN平滑层可有效提升多尺度融合质量，提供即插即用的高性能前端模块。

Abstract: Vision Transformers (ViT) have demonstrated significant promise in dense prediction tasks such as pose estimation. However, their performance is frequently constrained by the overly simplistic front-end designs employed in models like ViTPose. This naive patchification mechanism struggles to effectively handle multi-scale variations and results in irreversible information loss during the initial feature extraction phase. To overcome this limitation, we introduce a novel KAN-enhanced FPN-Stem architecture. Through rigorous ablation studies, we first identified that the true bottleneck for performance improvement lies not in plug-and-play attention modules (e.g., CBAM), but in the post-fusion non-linear smoothing step within the FPN. Guided by this insight, our core innovation is to retain the classic "upsample-and-add" fusion stream of the FPN, but replace its terminal, standard linear 3x3 smoothing convolution with a powerful KAN-based convolutional layer. Leveraging its superior non-linear modeling capabilities, this KAN-based layer adaptively learns and rectifies the "artifacts" generated during the multi-scale fusion process. Extensive experiments on the COCO dataset demonstrate that our KAN-FPN-Stem achieves a significant performance boost of up to +2.0 AP over the lightweight ViTPose-S baseline. This work not only delivers a plug-and-play, high-performance module but, more importantly, reveals that: the performance bottleneck in ViT front-end often lies not in 'feature refinement' (Attention), but in the quality of 'feature fusion' (Fusion). Furthermore, it provides an effective path to address this bottleneck through the introduction of the KAN operator.

</details>


### [17] [Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction](https://arxiv.org/abs/2512.22237)
*Mengxiao Geng,Ran Hong,Xiaoling Xu,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出MiG-DM：结合患者元信息与投影/图像双域协同的扩散模型，显著提升低剂量PET重建质量并保生理细节，在UDPET与临床多剂量数据上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET可降辐射，但易受噪声、对比度降低与生理细节丢失影响；现有方法忽视投影域物理先验与患者特异元信息，限制了功能—语义相关性的挖掘与跨模态对齐。

Method: 构建MiG-DM：1) 元信息编码模块将患者特征、剂量信息与半定量参数转为语义提示，进行文本—图像跨模态对齐并指导扩散过程；2) 跨域协同架构同时处理投影域与图像域；其中投影域引入正弦图(sinogram)适配器，以等效于全局图像域滤波的卷积来捕获全局物理结构；3) 将跨模态先验与双域特征融合，驱动扩散重建高质量PET。

Result: 在UDPET公共集与不同剂量的临床数据上，MiG-DM在图像质量与生理细节保持方面均优于现有SOTA（定量和视觉评估）。

Conclusion: 跨域（投影+图像）与跨模态（元信息提示）协同的扩散重建能有效缓解低剂量PET的噪声与对比损失，提升细节与功能语义一致性，具备临床适用潜力。

Abstract: Low-dose PET imaging is crucial for reducing patient radiation exposure but faces challenges like noise interference, reduced contrast, and difficulty in preserving physiological details. Existing methods often neglect both projection-domain physics knowledge and patient-specific meta-information, which are critical for functional-semantic correlation mining. In this study, we introduce a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates comprehensive cross-modal priors to generate high-quality PET images. Specifically, a meta-information encoding module transforms clinical parameters into semantic prompts by considering patient characteristics, dose-related information, and semi-quantitative parameters, enabling cross-modal alignment between textual meta-information and image reconstruction. Additionally, the cross-domain architecture combines projection-domain and image-domain processing. In the projection domain, a specialized sinogram adapter captures global physical structures through convolution operations equivalent to global image-domain filtering. Experiments on the UDPET public dataset and clinical datasets with varying dose levels demonstrate that MiG-DM outperforms state-of-the-art methods in enhancing PET image quality and preserving physiological details.

</details>


### [18] [Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture](https://arxiv.org/abs/2512.22239)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: 提出一种混合知识蒸馏框架，打造轻量学生CNN，在稻米品种与多种植物病叶数据集上以极低计算/参数实现与或优于教师与主流模型的精度。


<details>
  <summary>Details</summary>
Motivation: 边缘设备算力与存储受限，农业场景需要在高准确率与低计算成本间权衡；现有轻量模型或单一蒸馏策略难以同时兼顾精度、效率与跨数据集泛化。

Method: 设计结合倒残差块与致密连接的定制学生网络；以ResNet18为教师，采用多目标训练：硬标签监督、特征层蒸馏、响应层蒸馏与自蒸馏联合优化；在稻米种子九类别数据集训练，并在四个植物病叶数据集（稻、马铃薯、咖啡、玉米）验证泛化。

Result: 在稻米种子分类上学生模型98.56%准确率，仅比ResNet18教师低0.09%；计算量0.68 GFLOPs、参数约1.07M，相比教师降约2.7倍计算、>10倍参数；相对DenseNet121参数减少>6倍、相对ViT减少>80倍，同时精度可比或更优；在多病叶数据集上表现稳定提升。

Conclusion: 混合蒸馏+轻量结构可在硬件受限农业应用中以极低资源实现接近或超越重型模型的识别性能，具有良好鲁棒性、效率与部署潜力。

Abstract: Deploying deep learning models on resource-constrained edge devices remains a major challenge in smart agriculture due to the trade-off between computational efficiency and recognition accuracy. To address this challenge, this study proposes a hybrid knowledge distillation framework for developing a lightweight yet high-performance convolutional neural network. The proposed approach designs a customized student model that combines inverted residual blocks with dense connectivity and trains it under the guidance of a ResNet18 teacher network using a multi-objective strategy that integrates hard-label supervision, feature-level distillation, response-level distillation, and self-distillation. Experiments are conducted on a rice seed variety identification dataset containing nine varieties and further extended to four plant leaf disease datasets, including rice, potato, coffee, and corn, to evaluate generalization capability. On the rice seed variety classification task, the distilled student model achieves an accuracy of 98.56%, which is only 0.09% lower than the teacher model (98.65%), while requiring only 0.68 GFLOPs and approximately 1.07 million parameters. This corresponds to a reduction of about 2.7 times in computational cost and more than 10 times in model size compared with the ResNet18 teacher model. In addition, compared with representative pretrained models, the proposed student reduces the number of parameters by more than 6 times relative to DenseNet121 and by over 80 times compared with the Vision Transformer (ViT) architecture, while maintaining comparable or superior classification accuracy. Consistent performance gains across multiple plant leaf disease datasets further demonstrate the robustness, efficiency, and strong deployment potential of the proposed framework for hardware-limited smart agriculture systems.

</details>


### [19] [Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions](https://arxiv.org/abs/2512.22263)
*Aahan Sachdeva,Dhanvinkumar Ganeshkumar,James E. Gallagher,Tyler Treat,Edward J. Oughton*

Main category: cs.CV

TL;DR: 提出一种自适应多比率RGB-LWIR视频融合与模型切换框架，在不同照度下动态选择最佳目标检测模型，显著提升应急机器人视觉可靠性。


<details>
  <summary>Details</summary>
Motivation: 应急与灾害场景中光照变化剧烈：RGB在弱光下失效，热成像虽稳健但缺乏颜色与纹理。单一模态难以兼顾性能与鲁棒性，需开发跨照度、跨模态的自适应检测方案。

Method: 采集并标注2.2万+图像，按照度分为无光(<10 lux)、微光(10–1000 lux)、亮光(>1000 lux)。将对齐的RGB与LWIR帧按11种比例(从100/0到0/100，步长10%)进行像素级线性融合；针对每个照度与比例训练33个YOLO系列模型。运行时根据照度条件动态选择最优融合比与对应检测模型。

Result: 在亮光下，80/20(RGB/LWIR)模型mConf=92.8%；微光下，90/10模型mConf=92.0%；均显著优于YOLOv5n与YOLOv11n基线。无光下最佳为40/60，mConf=71.0%，虽高于基线但差异未达显著。总体表明跨比率融合在各照度下提升检测置信度与稳定性。

Conclusion: 多比例RGB-LWIR融合与自适应模型选择能在全照度范围提升目标检测表现，特别在弱光与复杂环境中增强应急机器人自主感知。但无光条件仍有改进空间，提示需更强红外特征建模与时序/对齐优化。

Abstract: Autonomous robotic platforms are playing a growing role across the emergency services sector, supporting missions such as search and rescue operations in disaster zones and reconnaissance. However, traditional red-green-blue (RGB) detection pipelines struggle in low-light environments, and thermal-based systems lack color and texture information. To overcome these limitations, we present an adaptive framework that fuses RGB and long-wave infrared (LWIR) video streams at multiple fusion ratios and dynamically selects the optimal detection model for each illumination condition. We trained 33 You Only Look Once (YOLO) models on over 22,000 annotated images spanning three light levels: no-light (<10 lux), dim-light (10-1000 lux), and full-light (>1000 lux). To integrate both modalities, fusion was performed by blending aligned RGB and LWIR frames at eleven ratios, from full RGB (100/0) to full LWIR (0/100) in 10% increments. Evaluation showed that the best full-light model (80/20 RGB-LWIR) and dim-light model (90/10 fusion) achieved 92.8% and 92.0% mean confidence; both significantly outperformed the YOLOv5 nano (YOLOv5n) and YOLOv11 nano (YOLOv11n) baselines. Under no-light conditions, the top 40/60 fusion reached 71.0%, exceeding baselines though not statistically significant. Adaptive RGB-LWIR fusion improved detection confidence and reliability across all illumination conditions, enhancing autonomous robotic vision performance.

</details>


### [20] [Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models](https://arxiv.org/abs/2512.22272)
*Antara Titikhsha,Om Kulkarni,Dharun Muthaiah*

Main category: cs.CV

TL;DR: 用一个小型“人类感知嵌入”（HPE）教师作为外部判别器，在不重新训练大模型的情况下向扩散生成过程注入几何约束梯度，能将形状与风格解耦并显著提升几何一致性与语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型质感细节强，但常受文本风格牵引而忽视严格几何约束，体现出模型与人类感知的语义落差。作者希望在无需专门再训练的前提下，引入人类对形状敏感性的指导，强化几何控制。

Method: 训练一个基于THINGS三元组数据的人类感知嵌入（HPE）教师，用以度量/判别物体形状相似性；在扩散（或流匹配/变换器）生成的潜空间采样过程中，注入来自教师的梯度作为外部引导，从而将几何与风格进行可控分离。方法在三类架构上评测：Stable Diffusion v1.5（U-Net）、SiT-XL/2（flow-matching）、PixArt-Σ（DiT）。

Result: 外部HPE引导实现零样本地将复杂3D形状（如伊姆斯椅）迁移到冲突材质（如粉色金属）上；在flow模型中若不持续引导会回漂到默认轨迹；总体语义对齐较无引导基线提升约80%。

Conclusion: 小型教师模型可稳定引导大型生成系统，实现更强的几何可控性与风格-形状解耦，扩展文本到图像合成的创作空间。

Abstract: Text-to-image diffusion models generate highly detailed textures, yet they often rely on surface appearance and fail to follow strict geometric constraints, particularly when those constraints conflict with the style implied by the text prompt. This reflects a broader semantic gap between human perception and current generative models. We investigate whether geometric understanding can be introduced without specialized training by using lightweight, off-the-shelf discriminators as external guidance signals. We propose a Human Perception Embedding (HPE) teacher trained on the THINGS triplet dataset, which captures human sensitivity to object shape. By injecting gradients from this teacher into the latent diffusion process, we show that geometry and style can be separated in a controllable manner. We evaluate this approach across three architectures: Stable Diffusion v1.5 with a U-Net backbone, the flow-matching model SiT-XL/2, and the diffusion transformer PixArt-Σ. Our experiments reveal that flow models tend to drift back toward their default trajectories without continuous guidance, and we demonstrate zero-shot transfer of complex three-dimensional shapes, such as an Eames chair, onto conflicting materials such as pink metal. This guided generation improves semantic alignment by about 80 percent compared to unguided baselines. Overall, our results show that small teacher models can reliably guide large generative systems, enabling stronger geometric control and broadening the creative range of text-to-image synthesis.

</details>


### [21] [GeCo: A Differentiable Geometric Consistency Metric for Video Generation](https://arxiv.org/abs/2512.22274)
*Leslie Gu,Junhwa Hur,Charles Herrmann,Fangneng Zhan,Todd Zickler,Deqing Sun,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 提出GeCo，一种以几何为基础的度量，用于在静态场景中联合检测几何形变与遮挡不一致伪影；通过融合残差运动与深度先验，生成可解释的稠密一致性图，并用于评测视频生成模型与作为训练外指导损失以减少形变伪影。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成与编辑模型在静态场景中常出现几何形变和遮挡不一致等伪影，缺乏统一、可解释、可量化的检测指标来诊断与改进这些问题。

Method: 构建几何约束：将估计的残差运动场与深度先验融合，形成几何一致性度量；输出稠密一致性（或不一致性）地图以定位形变与遮挡伪影；据此对多种视频生成模型进行系统基准测试；并将该度量以无需训练的损失项形式接入生成过程，充当指导以抑制形变。

Result: GeCo生成的稠密一致性图能够直观揭示几何形变和遮挡不一致；在对近期视频生成模型的基准中揭示了普遍的失效模式；作为训练外的指导损失能在生成时降低形变伪影。

Conclusion: 以几何为基础的GeCo既是有效的诊断度量，也是实用的生成指导：可解释、可泛化，能系统评测并在无需额外训练的情况下缓解视频生成中的几何伪影。

Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.

</details>


### [22] [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency](https://arxiv.org/abs/2512.22275)
*Dingyu Wang,Zimu Yuan,Jiajun Liu,Shanggui Liu,Nan Zhou,Tianxing Xu,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 提出B&J骨与关节多任务基准，发现模型在多模态开放任务上显著失效，临床胜任力不足。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准多为考试题或精心设计小案例，难以反映临床真实、需要跨文本与影像整合的推理过程；需有更全面、贴近临床路径的评测来检验“临床推理”而非单纯做题能力。

Method: 构建Bones & Joints（B&J）基准：从真实骨科与运动医学病例中抽取1,245道题，覆盖7个贴近临床推理链的任务（知识回忆、文本解读、影像解读、诊断生成、治疗方案制定与理由说明等）。以专家标注为真值，系统评估11个视觉-语言模型（VLMs）与6个大语言模型（LLMs），比较不同任务与模型类型表现。

Result: 任务间存在显著性能鸿沟：在结构化选择题上，最强模型可>90%正确；但在需多模态整合的开放式任务上，准确率仅~60%。VLM在医学影像解读薄弱，常被文本线索“牵着走”，即便视觉证据相反也会出现严重幻觉。面向医学微调的模型并未稳定优于通用模型。

Conclusion: 当下AI尚不具备复杂多模态临床推理胜任力，安全落地宜限于辅助性的文本类场景。要在核心临床任务上突破，需在多模态融合与视觉理解方面实现基础性技术进展。

Abstract: Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.

</details>


### [23] [FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound](https://arxiv.org/abs/2512.22278)
*Hussain Alasmawi,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 提出Fetal-Gauge：首个大规模胎儿超声VQA基准（4.2万图像、9.3万问答），覆盖解剖面判别、结构定位、胎位评估、视图合规与临床诊断；系统评测多款通用/医疗VLM，最佳仅55%准确率，远未达临床标准。


<details>
  <summary>Details</summary>
Motivation: 产前超声需求激增与技师短缺造成监测瓶颈；VLM可统一处理图像与文本、多任务一体化，但缺少标准化基准，且超声数据获取难、操作者依赖强，导致模型发展受限。

Method: 构建Fetal-Gauge基准：收集并标注大规模胎儿超声图像，设计多任务VQA问题集（解剖平面识别、解剖结构可视化定位、胎儿朝向、视图符合度、临床诊断）；统一评测协议，对多种SOTA通用与医用VLM进行系统评测与误差分析。

Result: 在Fetal-Gauge上，现有VLM整体表现不佳，最佳模型准确率约55%，显著低于临床可用水平；任务层面暴露出在解剖理解、定位与合规判定上的多重不足。

Conclusion: Fetal-Gauge为胎儿超声多模态学习建立首个严格评测平台，揭示当前VLM显著短板，呼吁面向超声的领域自适应架构与专门训练策略；数据集将在论文接收后公开，旨在推动产前护理AI与可及性。

Abstract: The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality's challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark will be publicly available once the paper gets accepted.

</details>


### [24] [A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation](https://arxiv.org/abs/2512.22294)
*Philip Xu,David Elizondo,Raouf Hamzaoui*

Main category: cs.CV

TL;DR: Uni4D 是一个统一框架，面向开放词汇的大规模3D检索与可控4D生成，通过在文本、3D模型与图像三模态上的三级对齐来实现高质量检索与时序一致的4D资产生成。


<details>
  <summary>Details</summary>
Motivation: 当前多模态研究中，文本-图像、文本-3D、3D-图像的跨模态对齐仍不充分，难以在开放词汇条件下高效检索3D内容并生成时序一致、可控的4D（随时间变化的3D）。需要一个统一框架提升语义对齐与跨模态一致性，从而支持大规模检索与动态资产生成。

Method: 基于 Align3D-130 数据集，提出：1) 文本到3D的多头注意力检索与搜索模型，优化语义对齐；2) 三级对齐模块：精确的文本→3D检索、多视角3D→图像对齐、图像→文本对齐；3) 在此基础上进行受控的4D生成，强调时间一致性。

Result: 在实验中，Uni4D 在3D检索的准确率与质量上取得显著提升，并能生成高质量、时序一致且可控的4D资产。

Conclusion: Uni4D 通过结构化的三层跨模态对齐，有效统一了开放词汇3D检索与可控4D生成，推动了动态多模态理解与应用落地。

Abstract: We introduce Uni4D, a unified framework for large scale open vocabulary 3D retrieval and controlled 4D generation based on structured three level alignment across text, 3D models, and image modalities. Built upon the Align3D 130 dataset, Uni4D employs a 3D text multi head attention and search model to optimize text to 3D retrieval through improved semantic alignment. The framework further strengthens cross modal alignment through three components: precise text to 3D retrieval, multi view 3D to image alignment, and image to text alignment for generating temporally consistent 4D assets. Experimental results demonstrate that Uni4D achieves high quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding and practical applications.

</details>


### [25] [Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors](https://arxiv.org/abs/2512.22295)
*Tian Guo,Hui Yuan,Philip Xu,David Elizondo*

Main category: cs.CV

TL;DR: SirenPose是一种结合正弦网络(SIREN)的周期激活与关键点几何先验的损失函数，用于提高动态三维场景重建的时空一致性与运动建模精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在快速运动、多目标场景中难以同时保持关键点预测的准确性与跨时间的一致性，导致重建抖动、漂移与形变不稳定。需要一种能把物理/几何约束融入学习过程、提升时空一致性的机制。

Method: 提出SirenPose损失：利用SIREN的周期激活特性捕捉高频运动与精细细节，同时引入由关键点结构出发的几何先验与受物理启发的约束（如跨帧一致性、速度/加速度平滑、骨架长度保持等），在空间与时间维度上强制关键点预测一致。并扩充训练集至60万标注实例以增强泛化。

Result: 在动态3D重建任务上，相比现有方法，采用SirenPose训练的模型在时空一致性指标上有显著提升，尤其在快速运动与复杂场景变化中表现更优。

Conclusion: 通过将SIREN的周期表征能力与关键点几何和物理一致性约束相结合，SirenPose有效缓解了动态场景中的时空不一致问题，提升了多目标、快动作场景的重建稳定性与精度。

Abstract: We propose SirenPose, a novel loss function that combines the periodic activation properties of sinusoidal representation networks with geometric priors derived from keypoint structures to improve the accuracy of dynamic 3D scene reconstruction. Existing approaches often struggle to maintain motion modeling accuracy and spatiotemporal consistency in fast moving and multi target scenes. By introducing physics inspired constraint mechanisms, SirenPose enforces coherent keypoint predictions across both spatial and temporal dimensions. We further expand the training dataset to 600,000 annotated instances to support robust learning. Experimental results demonstrate that models trained with SirenPose achieve significant improvements in spatiotemporal consistency metrics compared to prior methods, showing superior performance in handling rapid motion and complex scene changes.

</details>


### [26] [Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware](https://arxiv.org/abs/2512.22298)
*Vesal Ahsani,Babak Hossein Khalaj*

Main category: cs.CV

TL;DR: 提出单摄像头车内驾驶员行为识别系统，在树莓派5（仅CPU）与Coral Edge TPU上实现低时延、低成本、实时17类行为监测，通过轻量视觉模型+混淆意识标签设计+持续置信触发的时间决策头，实现16–25 FPS与稳定告警。


<details>
  <summary>Details</summary>
Motivation: DMS需要在受限算力/功耗/成本下低时延地识别分心与困倦相关行为；现有方案要么算力要求高、要么易误报且告警不稳定，难以在廉价边缘设备上可靠落地。

Method: 构建单摄像头端侧管线：1) 紧凑的逐帧视觉模型（INT8量化）；2) 考虑混淆因素的标签设计，降低外观相似类的误报；3) 时间决策头，仅当预测“高置信且持续”时触发告警。覆盖17类行为；使用多源授权数据训练评估，并在真实车内测试；在Raspberry Pi 5与Coral TPU上优化部署。

Result: 在Raspberry Pi 5上实现约16 FPS（逐帧<60 ms），在Coral Edge TPU上约25 FPS；实现实时监控与稳定告警，跨驾驶员/车辆/光照条件表现稳健。

Conclusion: 所提系统在低成本硬件上实现可靠的车内人状态感知，可作为上游输入支撑以人为中心的车辆智能与新型“代理化”车辆应用。

Abstract: In-cabin Driver Monitoring Systems (DMS) must recognize distraction- and drowsiness-related behaviors with low latency under strict constraints on compute, power, and cost. We present a single-camera in-cabin driver behavior recognition system designed for deployment on two low-cost edge platforms: Raspberry Pi 5 (CPU-only) and Google Coral Edge TPU. The proposed pipeline combines (i) a compact per-frame vision model, (ii) a confounder-aware label design to reduce visually similar false positives, and (iii) a temporal decision head that triggers alerts only when predictions are both confident and sustained. The system covers 17 behavior classes, including multiple phone-use modes, eating/drinking, smoking, reaching behind, gaze/attention shifts, passenger interaction, grooming, control-panel interaction, yawning, and eyes-closed sleep. Training and evaluation use licensed datasets spanning diverse drivers, vehicles, and lighting conditions (details in Section 6), and we further validate runtime behavior in real in-vehicle tests. The optimized deployments achieve about 16 FPS on Raspberry Pi 5 with INT8 inference (per-frame latency under 60 ms) and about 25 FPS on Coral Edge TPU, enabling real-time monitoring and stable alert generation on inexpensive hardware. Finally, we discuss how reliable in-cabin human-state perception can serve as an upstream input for human-centered vehicle intelligence, including emerging agentic vehicle concepts.

</details>


### [27] [Attack-Aware Deepfake Detection under Counter-Forensic Manipulations](https://arxiv.org/abs/2512.22303)
*Noor Fatima,Hasan Faraz Khan,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出一种在真实部署下具鲁棒性、可靠校准并可解释的攻防感知深伪与图像取证检测器。方法用“双流+适配器+弱监督热图”，并结合红队训练（最坏K对抗取证）与测试时随机防御聚合，达成高AUC、低校准误差、强弱定位与可靠弃判。


<details>
  <summary>Details</summary>
Motivation: 现有深伪/取证检测对压缩、重采样、再颗粒化等对抗性后处理脆弱，概率不校准且可解释性差，难以在社交平台与监控等现实环境稳定部署。需要一个能抵御多样攻击、输出可信概率并给出局部篡改证据的实用基线。

Method: 两流架构：一条语义流用预训练骨干提取内容表征；一条取证流提取残差与细微痕迹。通过轻量残差适配器融合并分类；浅层FPN式头在弱监督下产生篡改热图。红队训练：每批次最坏K对抗取证变换（JPEG重对齐/重压缩、重采样形变、去噪-再增粒、缝隙平滑、小幅色彩/伽马变化、社交App转码）。测试时防御：低成本抖动（尺寸与裁剪相位、轻微伽马、JPEG相位）并聚合预测。热图用人脸框约束集中于脸部，无需像素级标注。

Result: 在标准深伪数据集与监控风格（弱光、强压缩）拆分上，报告干净与受攻下性能：AUC、最坏情形准确率、可靠性与弃判质量、弱定位评分。结果显示：跨攻击近乎完美排序、低校准误差、弃判风险小、在再增粒攻下退化受控。

Conclusion: 该方法建立了可模块化、数据高效、可部署的攻防感知检测基线，能提供校准概率与可行动热图，在现实攻击与数据分布下保持稳健表现。

Abstract: This work presents an attack-aware deepfake and image-forensics detector designed for robustness, well-calibrated probabilities, and transparent evidence under realistic deployment conditions. The method combines red-team training with randomized test-time defense in a two-stream architecture, where one stream encodes semantic content using a pretrained backbone and the other extracts forensic residuals, fused via a lightweight residual adapter for classification, while a shallow Feature Pyramid Network style head produces tamper heatmaps under weak supervision. Red-team training applies worst-of-K counter-forensics per batch, including JPEG realign and recompress, resampling warps, denoise-to-regrain operations, seam smoothing, small color and gamma shifts, and social-app transcodes, while test-time defense injects low-cost jitters such as resize and crop phase changes, mild gamma variation, and JPEG phase shifts with aggregated predictions. Heatmaps are guided to concentrate within face regions using face-box masks without strict pixel-level annotations. Evaluation on existing benchmarks, including standard deepfake datasets and a surveillance-style split with low light and heavy compression, reports clean and attacked performance, AUC, worst-case accuracy, reliability, abstention quality, and weak-localization scores. Results demonstrate near-perfect ranking across attacks, low calibration error, minimal abstention risk, and controlled degradation under regrain, establishing a modular, data-efficient, and practically deployable baseline for attack-aware detection with calibrated probabilities and actionable heatmaps.

</details>


### [28] [PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation](https://arxiv.org/abs/2512.22304)
*Darrin Bright,Rakshith Raj,Kanchan Keisham*

Main category: cs.CV

TL;DR: PortionNet通过跨模态蒸馏，用点云几何知识训练，推理仅需RGB图像，实现单图食物体积与能量估计的SOTA并具良好跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 单张RGB图像缺乏3D信息导致营养估计困难；依赖深度传感器的方法在多数手机上不可用，亟需无需专用硬件也能进行几何推理的方案。

Method: 提出PortionNet：跨模态知识蒸馏框架。训练时引入点云分支作为教师，设计轻量适配器网络在RGB分支中模仿点云表示，实现伪3D推理；采用双模态/双模式训练，推理阶段仅用RGB图像输出体积与能量估计。

Result: 在MetaFood3D上，体积与能量估计均达到SOTA；在SimpleFood45上进行跨数据集评测，能量估计表现优异，显示出良好泛化能力。

Conclusion: 通过在训练阶段学习点云几何表征，PortionNet在无需深度传感器的情况下实现准确食物体积与能量估计，并具备强泛化与实用性。

Abstract: Accurate food nutrition estimation from single images is challenging due to the loss of 3D information. While depth-based methods provide reliable geometry, they remain inaccessible on most smartphones because of depth-sensor requirements. To overcome this challenge, we propose PortionNet, a novel cross-modal knowledge distillation framework that learns geometric features from point clouds during training while requiring only RGB images at inference. Our approach employs a dual-mode training strategy where a lightweight adapter network mimics point cloud representations, enabling pseudo-3D reasoning without any specialized hardware requirements. PortionNet achieves state-of-the-art performance on MetaFood3D, outperforming all previous methods in both volume and energy estimation. Cross-dataset evaluation on SimpleFood45 further demonstrates strong generalization in energy estimation.

</details>


### [29] [MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation](https://arxiv.org/abs/2512.22310)
*Run Ling,Ke Cao,Jian Lu,Ao Ma,Haowei Liu,Runze He,Changwei Wang,Rongtao Xu,Yihua Shao,Zhanjie Zhang,Peng Wu,Guibing Guo,Wei Feng,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Xingwei Wang*

Main category: cs.CV

TL;DR: MoFu 提出一个统一框架，用于从文本与多张参考图生成多主体视频，核心解决“尺度不一致”和“排列敏感”两大问题，通过LLM引导的尺度感知调制、基于FFT的傅里叶特征融合，以及联合的尺度-排列稳定性损失，并构建专门基准，实验显示在尺度自然性、主体保真度和总体画质上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多主体视频生成中，主体大小常不自然（尺度不一致），且参考图片顺序会影响结果（排列敏感），导致主体变形与不稳定；现有方法缺乏对尺度线索的显式建模与对参考顺序的不变性，评测基准也不足。

Method: 1) Scale-Aware Modulation（SMO）：利用LLM从文本提示中抽取隐式尺度线索，对生成特征进行调制以保持统一尺度；2) Fourier Fusion：对多参考特征做快速傅里叶变换，融合频域信息形成顺序无关的统一表征；3) Scale-Permutation Stability Loss：联合约束尺度一致与排列不变；4) 构建含受控尺度与排列变化的评测基准。

Result: 在所建基准与多种既有数据上，MoFu在主体尺度自然性、身份/外观保真和整体视觉质量等指标上显著超越现有方法，且对参考顺序更稳健。

Conclusion: 将LLM引导的尺度调制与频域融合相结合，可有效解决多主体视频生成的尺度不一致与排列敏感问题；配合新损失与基准，MoFu实现更稳定、更高保真度的生成。

Abstract: Multi-subject video generation aims to synthesize videos from textual prompts and multiple reference images, ensuring that each subject preserves natural scale and visual fidelity. However, current methods face two challenges: scale inconsistency, where variations in subject size lead to unnatural generation, and permutation sensitivity, where the order of reference inputs causes subject distortion. In this paper, we propose MoFu, a unified framework that tackles both challenges. For scale inconsistency, we introduce Scale-Aware Modulation (SMO), an LLM-guided module that extracts implicit scale cues from the prompt and modulates features to ensure consistent subject sizes. To address permutation sensitivity, we present a simple yet effective Fourier Fusion strategy that processes the frequency information of reference features via the Fast Fourier Transform to produce a unified representation. Besides, we design a Scale-Permutation Stability Loss to jointly encourage scale-consistent and permutation-invariant generation. To further evaluate these challenges, we establish a dedicated benchmark with controlled variations in subject scale and reference permutation. Extensive experiments demonstrate that MoFu significantly outperforms existing methods in preserving natural scale, subject fidelity, and overall visual quality.

</details>


### [30] [VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning](https://arxiv.org/abs/2512.22315)
*Yang Ding,Yizhen Zhang,Xin Lai,Ruihang Chu,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出VideoZoomer：让MLLM在长视频理解中通过“时间变焦”动态聚焦关键片段，从粗到细多轮交互采样，结合SFT+RL训练，显著提升长视频推理性能与效率。


<details>
  <summary>Details</summary>
Motivation: 长视频理解受限于MLLM上下文窗口，小样本或静态均匀采样易遗漏关键信息且无法在推理中纠偏，需要一种能在推理过程中动态调整视觉关注、递进获取证据的机制。

Method: 设计代理式框架VideoZoomer：先以低帧率全局浏览，再通过“时间变焦”工具在自主选择的时间点获取高帧率片段，多轮交互逐步细化证据；训练包含两阶段：1）冷启动SFT，基于蒸馏的示例与反思轨迹数据；2）强化学习进一步优化代理策略。

Result: 基于7B模型展现多样且复杂的推理模式，在多项长视频理解/推理基准上取得强劲表现，稳定超越开源模型，部分任务可与商用闭源系统匹敌；在更低帧预算下保持更高效率。

Conclusion: 动态时域聚焦与代理式多轮推理能有效突破上下文限制，提升长视频理解的准确性与效率；该范式具备可扩展性与实际应用价值。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.

</details>


### [31] [SpotEdit: Selective Region Editing in Diffusion Transformers](https://arxiv.org/abs/2512.22323)
*Zhibin Qin,Zhenxiong Tan,Zeqing Wang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: SpotEdit通过在扩散变换器编辑中只更新被修改区域、复用稳定区域特征，实现更高效且更保真的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有扩散编辑方法在每个时间步对所有token统一去噪，尽管大多数编辑仅涉及小区域，导致计算冗余并可能破坏未改区域质量。作者质疑是否有必要在编辑时重生成全部区域。

Method: 提出无需训练的框架SpotEdit：1) SpotSelector基于感知相似性检测稳定（未变）区域，并在这些区域跳过计算，直接复用条件图像特征；2) SpotFusion通过动态融合机制，将复用的稳定特征与被编辑token自适应混合，确保上下文一致与编辑质量。

Result: 在保持未改区域高保真度的同时，显著减少对无关区域的计算与去噪开销，实现高效且精确的编辑（文摘层面未给出具体数值）。

Conclusion: 选择性更新与特征复用能避免不必要重生成，兼顾效率与质量；SpotEdit为扩散编辑提供了训练免且可泛化的实用方案。

Abstract: Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.

</details>


### [32] [DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models](https://arxiv.org/abs/2512.22324)
*Jianrong Zhang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 提出DeMoGen：用能量式扩散模型进行运动分解学习，从整体动作中自动发现并解耦语义原子，支持重新组合生成多样新动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法多做“正向”合成（文本→整体动作或拼装概念）而缺乏“逆向”能力：将复杂整体动作分解为可复用、语义明确的运动原语，且常需原语级标注数据。

Method: 提出基于能量的扩散模型的“组合式训练”范式，直接建模多个运动概念的组合分布，无需单个概念的真值动作。设计三种训练变体以促进分解：(1) DeMoGen-Exp：用分解后的文本提示显式训练；(2) DeMoGen-OSS：正交自监督分解，鼓励概念间表示正交；(3) DeMoGen-SC：在原始与分解文本嵌入间施加语义一致性约束。并构建文本分解数据集辅助训练。

Result: 模型能从复杂序列中解耦可复用运动原语，并将学到的概念灵活重组，生成多样且新颖、超出训练分布的动作。在无需单概念真值的情况下实现有效分解与合成。

Conclusion: DeMoGen实现从整体动作到语义原语的无标注分解学习，三种训练策略提升可分解性；所学概念可泛化重组以生成新动作。并提供文本分解数据集以推进文本到动作的组合式生成研究。

Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.

</details>


### [33] [The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma](https://arxiv.org/abs/2512.22331)
*Mariya Miteva,Maria Nisheva-Pavlova*

Main category: cs.CV

TL;DR: 提出一种基于多视角VAE的潜在表示学习框架，将T1Gd与FLAIR放射组学特征在潜在空间融合，用于GBM中MGMT启动子甲基化状态分类。


<details>
  <summary>Details</summary>
Motivation: MGMT启动子甲基化是GBM重要的预后与治疗生物标志物；现有单模态或早期特征拼接的放射组学方法存在特征冗余高、难以保留模态特异信息等问题，需要更有效的多模态融合策略以提升非侵入性分子表型推断。

Method: 从T1Gd与FLAIR MRI提取放射组学特征；为每个模态分别构建概率编码器（VAE），在各自潜在空间表示的基础上进行紧凑潜在层融合，既保留模态特异结构又实现信息互补；将融合得到的潜在嵌入作为下游分类器输入，预测MGMT甲基化状态。

Result: 经融合后的潜在表示可用于MGMT甲基化分类，显示出有效的多模态整合能力（摘要未给具体数值）。

Conclusion: 多视角VAE在紧凑潜在空间实现对T1Gd与FLAIR特征的有效融合，缓解特征冗余并保留模态特异信息，从而提升GBM中MGMT甲基化的非侵入性预测潜力。

Abstract: Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.

</details>


### [34] [Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides](https://arxiv.org/abs/2512.22335)
*Olaide N. Oyelade,Oliver Hoxey,Yulia Humrye*

Main category: cs.CV

TL;DR: 提出端到端视觉Transformer管线，联合H&E与IHC全视野切片，实现肿瘤定位与HER2四分类（0/1+/2+/3+）及阴/阳判定，并提供像素级定位，实验在小型私有数据上取得较高准确与特异度。


<details>
  <summary>Details</summary>
Motivation: 传统依赖IHC的HER2评分流程需要精确的表达水平判定，低高表达均难，且H&E与IHC联合分析复杂；现有深度学习多无法给出像素级HER2定位，限制临床解释与可用性。

Method: 构建端到端ViT系统：1) 对H&E WSI切片进行patch级处理以定位肿瘤；2) 提出新颖映射函数，将H&E中恶性区域对应到配对IHC WSI的相关区域；3) 将临床启发的HER2评分机制嵌入模型，实现像素级四分类（0/1+/2+/3+）与阴/阳判定；4) 以WSI及其patch作为输入进行训练与评估。

Result: 在13例H&E/IHC配对私有WSI上进行实验：肿瘤定位得到良好分类准确率；HER2四分类整体准确率0.94、特异度0.933；方法在WSI patch层面与病理医生可比。

Conclusion: 联合H&E与IHC的端到端ViT框架可实现可解释的像素级HER2评分与阴/阳判定，效果良好，显示该策略在临床工作流中具有可用性与潜力。

Abstract: The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring

</details>


### [35] [Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data](https://arxiv.org/abs/2512.22349)
*Alaa Alahmadi,Mohamed Hasan*

Main category: cs.CV

TL;DR: 利用感知启发的伪彩色编码ECG时序特征，在极少样本条件下提升深度模型的数据效率与可解释性，验证于药物诱导性长QT综合征检测。


<details>
  <summary>Details</summary>
Motivation: 深度网络在生理信号判读中常需大规模数据且可解释性差，不利于临床可靠性与与人类推理一致；ECG中LQTS正例稀缺、形态异质，是检验小样本泛化与可解释性的理想难题。

Method: 将临床关键时序特征（如QT间期）通过结构化伪彩色编码到ECG图像；在一拍与10秒节律两种粒度生成数据，采用原型网络与ResNet-18进行一/少样本学习；进行可解释性分析并评估多心搏聚合对性能的影响。

Result: 伪彩色引导模型从1或5个样本即可学习到判别且可解释的特征；注意力聚焦于临床相关ECG成分，抑制无关信号；多心搏聚合进一步提升性能，表现出与人类感知平均一致的趋势。

Conclusion: 以人类感知为导向的编码将临床要点嵌入表示空间，可在复杂生理数据上同时提升数据效率、可解释性与因果一致性，为医疗AI的可靠部署提供路径。

Abstract: Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data.
  We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.

</details>


### [36] [VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement](https://arxiv.org/abs/2512.22351)
*Zhengfei Kuang,Rui Lin,Long Zhao,Gordon Wetzstein,Saining Xie,Sanghyun Woo*

Main category: cs.CV

TL;DR: 提出一套面向3D场景物体摆放任务的MLLM系统：用MCP式API替代脆弱的代码编辑、引入视觉工具形成感知—行动闭环、采用规划-执行-验证多智能体协作；在25个复杂任务上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要在2D视觉-语言上表现突出，但在需要精确空间推理与可控编辑的3D场景操控上表现不足：视觉落地不稳、缺少3D状态感知、迭代编辑易出错且难恢复。

Method: 1) 以MCP风格的函数级API承载编辑指令，降低从自然语言到精确3D操作的脆弱映射；2) 为MLLM增配专用视觉/空间工具，获取场景状态、空间关系并验证动作结果，构建感知反馈闭环；3) 设计规划-执行-验证的多代理协作框架，支持多步指令与错误恢复。

Result: 在25个复杂3D物体摆放任务上，相比现有基线取得显著性能提升（摘要未给出具体数值），展示出更强的多步规划、空间理解与鲁棒编辑能力。

Conclusion: 函数级API+视觉工具反馈+多代理协作的组合，有效提升MLLM在3D场景操控中的可控性与稳健性，缩小语言指令与精确3D操作之间的鸿沟；方法具有通用性，适合拓展到更广泛的3D操作任务。

Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io

</details>


### [37] [Self-Evaluation Unlocks Any-Step Text-to-Image Generation](https://arxiv.org/abs/2512.22374)
*Xin Yu,Xiaojuan Qi,Zhengqi Li,Kai Zhang,Richard Zhang,Zhe Lin,Eli Shechtman,Tianyu Wang,Yotam Nitzan*

Main category: cs.CV

TL;DR: Self-E 是一种从零训练的文本到图像模型，将流匹配的局部学习与基于自身评分的自评估全局匹配结合，实现任意步数推理：少步数也高质，多步数持续提升，无需预训练教师。


<details>
  <summary>Details</summary>
Motivation: 现有扩散/流模型依赖局部监督，需要多步推理；蒸馏方法虽能少步数但需教师模型且受限。需要一种既能少步高质、又能扩展到多步、且从零训练的统一框架。

Method: 在流匹配式学习的同时，引入“自评估”机制：模型用当前的score估计对自身生成样本打分，作为动态自教师，进行全局匹配信号的学习。结合即时局部学习（flow matching）与自驱动的全局对齐，实现任意步推理能力。无需预训练教师。

Result: 在大规模文本到图像基准上，Self-E 在少步生成上显著领先；在50步时与最新的流匹配SOTA相当。其性能随推理步数单调提升，既能超快少步生成，也支持高质量长轨迹采样。

Conclusion: Self-E 实现了首个从零训练的任意步文本到图像模型，将局部与全局学习桥接，提供高效、可扩展、统一的生成框架。

Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.

</details>


### [38] [iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI](https://arxiv.org/abs/2512.22392)
*Himanshu Naidu,Yuxiang Zhang,Sachin Mehta,Anat Caspi*

Main category: cs.CV

TL;DR: 提出一款在iPhone/iPad上进行实时人行道要素采集的应用iOSPointMapper，融合语义分割、LiDAR深度与GPS/IMU，实现隐私友好的特征检测、定位与人工校验，并将匿名数据接入TDEI，提升行人基础设施数据的覆盖与质量。


<details>
  <summary>Details</summary>
Motivation: 现有人行道数据收集昂贵、割裂、难扩展，影响无障碍与包容性的步行基础设施建设，需要一种低成本、可扩展且隐私友好的地面实测方案。

Method: 在移动端实现：1) 设备端语义分割识别交通标志/信号灯/杆等要素；2) 利用LiDAR进行深度估计，结合GPS/IMU进行空间配准与定位；3) 提供用户引导的标注界面对检测结果进行提交前校验；4) 将匿名化数据上传并无缝融入TDEI多模态交通数据集；5) 对特征检测与空间映射性能进行评估。

Result: 实验/评估显示应用在要素检测和空间映射方面具有良好性能，表明该系统可有效提升行人地图数据的质量与覆盖。

Conclusion: iOSPointMapper提供了可扩展、以用户为中心且注重隐私的行人数据采集途径，有望弥补行人基础设施关键数据缺口并促进更包容的出行环境建设。

Abstract: Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale. We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads. The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles. To ensure transparency and improve data quality, iOSPointMapper incorporates a user-guided annotation interface for validating system outputs before submission. Collected data is anonymized and transmitted to the Transportation Data Exchange Initiative (TDEI), where it integrates seamlessly with broader multimodal transportation datasets. Detailed evaluations of the system's feature detection and spatial mapping performance reveal the application's potential for enhanced pedestrian mapping. Together, these capabilities offer a scalable and user-centered approach to closing critical data gaps in pedestrian

</details>


### [39] [DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization](https://arxiv.org/abs/2512.22406)
*Hansang Lee,Chaelin Lee,Nieun Seo,Joon Seok Lim,Helen Hong*

Main category: cs.CV

TL;DR: DeFloMat 用条件流匹配将扩散式检测的多步随机采样改为确定性流的少步ODE推理，在MRE克罗恩病检测上以3步达到更高AP与更好召回与稳定性，显著降低时延。


<details>
  <summary>Details</summary>
Motivation: 扩散式目标检测（如DiffusionDet）虽准确，但需要大量采样步数，推理延迟高，难以用于时间敏感的临床应用（如MRE中的克罗恩病检测）。需要一种既保留生成式优势又显著加速的检测范式。

Method: 提出DeFloMat：用条件流匹配（CFM）与条件最优传输（OT）理论近似Rectified Flow，学习从噪声到目标框的确定性流场；推理阶段用简单的常微分方程（ODE）求解器，在极少步数内完成生成式检测；对比扩散的随机多步去噪路径，改为直接、稳定的确定性轨迹。

Result: 在具有挑战性的MRE临床数据集上，DeFloMat在仅3个推理步即可达成43.32% AP_{10:50}，超过DiffusionDet在4步的31.03%（约1.4倍提升）；同时在少步设置下具有更好的定位特性，召回率与稳定性更优。

Conclusion: 通过将扩散的随机过程替换为基于条件OT的确定性流，DeFloMat在极少步实现高精度与高稳定的目标定位，兼顾生成式准确性与临床时效性，为快速可靠的临床目标检测树立新基准。

Abstract: We propose DeFloMat (Detection with Flow Matching), a novel generative object detection framework that addresses the critical latency bottleneck of diffusion-based detectors, such as DiffusionDet, by integrating Conditional Flow Matching (CFM). Diffusion models achieve high accuracy by formulating detection as a multi-step stochastic denoising process, but their reliance on numerous sampling steps ($T \gg 60$) makes them impractical for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE). DeFloMat replaces this slow stochastic path with a highly direct, deterministic flow field derived from Conditional Optimal Transport (OT) theory, specifically approximating the Rectified Flow. This shift enables fast inference via a simple Ordinary Differential Equation (ODE) solver. We demonstrate the superiority of DeFloMat on a challenging MRE clinical dataset. Crucially, DeFloMat achieves state-of-the-art accuracy ($43.32\% \text{ } AP_{10:50}$) in only $3$ inference steps, which represents a $1.4\times$ performance improvement over DiffusionDet's maximum converged performance ($31.03\% \text{ } AP_{10:50}$ at $4$ steps). Furthermore, our deterministic flow significantly enhances localization characteristics, yielding superior Recall and stability in the few-step regime. DeFloMat resolves the trade-off between generative accuracy and clinical efficiency, setting a new standard for stable and rapid object localization.

</details>


### [40] [Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy](https://arxiv.org/abs/2512.22423)
*Amil Khan,Matheus Palhares Viana,Suraj Mishra,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出Bright-4B，一种40亿参数的基础模型，可直接从3D明场显微堆栈进行亚细胞结构分割，无需荧光或繁重后处理，并在多数据集上优于CNN/Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 3D明场显微镜可快速、无损地观察细胞形态，但稳健的体素级分割通常仍依赖荧光标记或大量后处理；缺乏能直接从明场体数据进行高保真分割的通用方法。

Method: 构建在单位超球面上学习的40亿参数模型Bright-4B：1) Native Sparse Attention，硬件友好地融合局部、粗尺度与选择性全局上下文；2) 深-宽残差HyperConnections稳定表示流；3) 软MoE实现自适应容量；4) 可插拔的各向异性patch嵌入，匹配共聚焦PSF与轴向稀疏以实现几何保真的3D分词。直接以3D明场体输入，端到端输出亚细胞器分割。

Result: 在多个共聚焦数据集上，仅用明场数据即可生成形态精确的核、线粒体等细胞器分割，跨深度与细胞类型保持细节，并一致优于当代CNN与Transformer基线。无需荧光、辅助通道或手工后处理。

Conclusion: Bright-4B填补了无标记3D明场分割的空白，提供可泛化、精细的亚细胞分割能力，将开源代码、预训练权重与下游微调模型，助推大规模、无标记的3D细胞图谱构建。

Abstract: Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.

</details>


### [41] [FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning](https://arxiv.org/abs/2512.22425)
*Ujunwa Mgboh,Rafi Ibn Sultan,Joshua Kim,Kundan Thind,Dongxiao Zhu*

Main category: cs.CV

TL;DR: 提出FluenceFormer：一个结合解剖信息与束几何、端到端回归放疗流强图的两阶段Transformer框架，并以物理约束的FAR损失提升可实现性与结构一致性；在前列腺IMRT数据上优于CNN与单阶段方法，能量误差降至4.5%，结构一致性显著提升。


<details>
  <summary>Details</summary>
Motivation: 流强图预测是自动化放疗计划的核心，但从体积解剖到束强度调制是病态逆问题；卷积方法难以建模长程依赖，易产生结构不一致或物理不可实现的计划。需要能显式融合几何与全局上下文、并受物理原则约束的模型。

Method: 提出FluenceFormer，一个主干无关的两阶段Transformer。阶段1：仅基于解剖输入预测全局剂量先验；阶段2：将该先验与显式束几何融合，回归物理校准的流强图。核心是FAR损失，融合体素级拟合、梯度平滑、结构一致性与按束能量守恒等物理约束。框架可搭配Swin UNETR、UNETR、nnFormer、MedFormer等主干。

Result: 在前列腺IMRT数据集上评估，多种Transformer主干均可用，其中Swin UNETR版本性能最佳；相较基准CNN与单阶段方法，能量误差降至4.5%，结构保真度显著提升（p<0.05）。

Conclusion: 将全局剂量先验与束几何条件化的两阶段Transformer，加上FAR物理感知损失，可显著提升流强回归的物理可实现性与结构一致性，具有跨主干的通用性并在IMRT任务上达到SOTA。

Abstract: Fluence map prediction is central to automated radiotherapy planning but remains an ill-posed inverse problem due to the complex relationship between volumetric anatomy and beam-intensity modulation. Convolutional methods in prior work often struggle to capture long-range dependencies, which can lead to structurally inconsistent or physically unrealizable plans. We introduce \textbf{FluenceFormer}, a backbone-agnostic transformer framework for direct, geometry-aware fluence regression. The model uses a unified two-stage design: Stage~1 predicts a global dose prior from anatomical inputs, and Stage~2 conditions this prior on explicit beam geometry to regress physically calibrated fluence maps. Central to the approach is the \textbf{Fluence-Aware Regression (FAR)} loss, a physics-informed objective that integrates voxel-level fidelity, gradient smoothness, structural consistency, and beam-wise energy conservation. We evaluate the generality of the framework across multiple transformer backbones, including Swin UNETR, UNETR, nnFormer, and MedFormer, using a prostate IMRT dataset. FluenceFormer with Swin UNETR achieves the strongest performance among the evaluated models and improves over existing benchmark CNN and single-stage methods, reducing Energy Error to $\mathbf{4.5\%}$ and yielding statistically significant gains in structural fidelity ($p < 0.05$).

</details>


### [42] [EmoCtrl: Controllable Emotional Image Content Generation](https://arxiv.org/abs/2512.22437)
*Jingyuan Yang,Weibin Luo,Hui Huang*

Main category: cs.CV

TL;DR: 提出C-EICG任务与方法EmoCtrl：在保持文本内容忠实的同时可控地生成目标情感的图像；通过文本与视觉情感增强模块和带情感标注的数据集，显著提升情感表达且不牺牲内容一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型要么强调内容一致性但缺乏情感意识，要么强调情感而导致内容失真。需要一种能在内容忠实与情感可控之间取得平衡的生成框架。

Method: 构建含内容、情感与情感提示的标注数据集，建立抽象情感到可视线索的桥接。提出EmoCtrl：引入文本情感增强模块（用描述性语义补充情感词）与视觉情感增强模块（注入感知线索）；学习“情感token”，通过训练使其与基础文生图模型协同，兼顾内容一致性与情感表达；通过消融与可视化分析其互补性。

Result: 在定量与定性评测中，EmoCtrl同时实现高内容忠实度与强情感控制，较现有方法在多方面指标上更优；用户研究显示更符合人类偏好；情感token在创意应用中具备良好泛化与鲁棒性。

Conclusion: EmoCtrl有效弥合内容忠实与情感表达的权衡，通过文本与视觉双通道增强与可学习情感token实现可控情感图像生成，并在客观指标与主观评测上均优于现有方法，具备推广潜力。

Abstract: An image conveys meaning through both its visual content and emotional tone, jointly shaping human perception. We introduce Controllable Emotional Image Content Generation (C-EICG), which aims to generate images that remain faithful to a given content description while expressing a target emotion. Existing text-to-image models ensure content consistency but lack emotional awareness, whereas emotion-driven models generate affective results at the cost of content distortion. To address this gap, we propose EmoCtrl, supported by a dataset annotated with content, emotion, and affective prompts, bridging abstract emotions to visual cues. EmoCtrl incorporates textual and visual emotion enhancement modules that enrich affective expression via descriptive semantics and perceptual cues. The learned emotion tokens exhibit complementary effects, as demonstrated through ablations and visualizations. Quantatitive and qualatitive experiments demonstrate that EmoCtrl achieves faithful content and expressive emotion control, outperforming existing methods across multiple aspects. User studies confirm EmoCtrl's strong alignment with human preference. Moreover, EmoCtrl generalizes well to creative applications, further demonstrating the robustness and adaptability of the learned emotion tokens.

</details>


### [43] [SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems](https://arxiv.org/abs/2512.22439)
*Khalfalla Awedat,Mohamed Abidalrekab,Gurcan Comert,Mustafa Ayad*

Main category: cs.CV

TL;DR: 提出SuperiorGAT，一种基于图注意力的框架，通过梁感知图建模与门控残差融合+前馈精炼，在不加深网络的前提下重建稀疏LiDAR点云缺失的垂直信息；在KITTI上模拟每四条垂直束丢失的情形，优于PointNet与更深GAT基线，误差更低、几何一致性更好。


<details>
  <summary>Details</summary>
Motivation: 车载LiDAR受固定垂直束分辨率限制，且因遮挡等导致束丢失，造成点云稀疏与垂直信息缺失，影响感知精度与几何完整性；希望在不增加传感器硬件或显著计算开销的情况下提升“有效分辨率”。

Method: 将LiDAR扫描建模为“梁感知”图结构，利用图注意力网络进行特征传播；引入门控残差融合以稳定/选择性融合多层特征，并配合前馈精炼模块提升重建质量，从而在不增加网络深度的条件下恢复缺失的高度信息；评估时通过结构化丢弃（每四条垂直束移除）模拟束缺失。

Result: 在KITTI多场景（行人、道路、校园、城市）上，相比PointNet类模型与更深GAT基线，SuperiorGAT取得更低的重建误差和更好的几何一致性；X-Z投影定性显示结构保持良好且垂直畸变小。

Conclusion: 通过架构级改进（梁感知图+门控残差融合+前馈精炼），可在不增加网络深度与硬件成本的前提下，有效重建LiDAR垂直信息、提升几何一致性，为提升LiDAR“分辨率”的计算高效路径提供了证据。

Abstract: LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model's ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.

</details>


### [44] [LECalib: Line-Based Event Camera Calibration](https://arxiv.org/abs/2512.22441)
*Zibin Liu,Banglei Guana,Yang Shanga,Zhenbao Yu,Yifei Bian,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于几何直线的事件相机标定方法，直接从事件流检测线，先由事件-直线模型估计初值，再用非线性优化精炼，适用于平面与非平面线，单目与双目均验证，兼具速度与准确性并免去人工标定板。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机标定依赖闪烁图案、重建强度图像或事件特征，操作繁琐、耗时，并需人工放置标定物，难以应对快速变化场景。需要一种无需专用标定板、可快速、鲁棒的标定方法。

Method: - 从事件流中直接检测环境中的几何直线（如门窗、盒子边缘）。
- 构建事件-直线的成像/约束模型，据此对相机内外参生成初始估计，适配平面与非平面直线集合。
- 采用非线性优化（如最小化重投影或几何误差）联合优化相机参数（单目或双目）。
- 在仿真与真实数据上评估。

Result: 在仿真与实测中，该方法实现了可行且准确的标定效果；对单目与双目事件相机均有效。相较需要人工标定物和耗时处理的现有方法，效率更高、部署更灵活。源码已开源。

Conclusion: 基于环境几何直线的事件相机标定框架能在无专用标定板、快速变化场景中实现高效准确的内外参估计，并可推广至多种设置（单目/双目、平面/非平面线），具有实用价值。

Abstract: Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.

</details>


### [45] [Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework](https://arxiv.org/abs/2512.22447)
*Zhicheng Zhao,Yuancheng Xu,Andong Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出QDFNet，通过质量感知的动态融合在光学与SAR缺失/退化情况下实现更稳健的目标检测。核心是参考token驱动的质量评估与带正交约束的自适应融合，实验在SpaceNet6-OTD与OGSOD-2.0显著优于SOTA，尤其在模态缺失时。


<details>
  <summary>Details</summary>
Motivation: 现实中光学与SAR获取不同步、配准困难，常导致配对数据错位、缺失或局部退化。现有方法对随机模态缺失鲁棒性不足，且缺乏可保证融合稳定增益的机制。需要一种能动态评估模态质量并在不可靠区域抑制错误传播的融合检测框架。

Method: 提出质量感知动态融合网络QDFNet：1) 动态模态质量评估（DMQA）：引入可学习参考token，迭代与多模态特征交互，对区域级特征可靠性打分，定位退化区域，为后续融合提供质量引导；2) 正交约束归一化融合（OCNF）：对不同模态特征施加正交约束以保持独立性，并依据可靠性分数自适应调整融合权重，抑制不可靠特征传播；整体用于多尺度目标检测。

Result: 在SpaceNet6-OTD与OGSOD-2.0上，QDFNet较多种SOTA方法取得更优检测性能；在部分模态腐蚀或缺失场景下优势更显著，体现强鲁棒性与稳定融合增益。

Conclusion: 通过参考token驱动的质量评估与带正交约束的动态融合，QDFNet在光学-SAR目标检测中有效应对异步、配准误差与模态缺失问题，确保融合持续带来性能提升，并在多基准上验证了优越性。

Abstract: Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.

</details>


### [46] [SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues](https://arxiv.org/abs/2512.22449)
*Md Abu Obaida Zishan,Annajiat Alim Rasel*

Main category: cs.CV

TL;DR: SonoVision 是一款离线运行的智能手机应用，利用 EfficientDet-D2 进行目标检测，并通过左右声道正弦音提示目标方位，帮助视障人士独立定位日常物体。


<details>
  <summary>Details</summary>
Motivation: 视障人士在日常环境中定位物体困难且伴随安全风险，现有方案常依赖他人、在线服务或额外硬件，缺少便携、低成本、可离线的定位反馈方式。

Method: 在手机端用 Flutter 开发前端，后台集成 EfficientDet-D2 进行物体检测；根据检测框相对屏幕中心的水平位置，将提示音路由到左/右耳（两侧）或双耳（正前方），以简单空间听觉线索引导使用者；提示音采用正弦波，通过耳机/头戴设备输出；系统可完全离线运行。

Result: 原型系统实现并开源，能够在本地检测并以左右声道/双耳提示物体方位，支持无网络环境下使用；摘要未给出量化精度、延迟或用户试验数据。

Conclusion: SonoVision 以纯声学左右声道提示结合本地目标检测，为视障人士提供低依赖、可离线的日常物体定位辅助；未来应补充用户研究、性能评测和更细粒度的空间提示以验证与提升实用性。

Abstract: Locating objects for the visually impaired is a significant challenge and is something no one can get used to over time. However, this hinders their independence and could push them towards risky and dangerous scenarios. Hence, in the spirit of making the visually challenged more self-sufficient, we present SonoVision, a smart-phone application that helps them find everyday objects using sound cues through earphones/headphones. This simply means, if an object is on the right or left side of a user, the app makes a sinusoidal sound in a user's respective ear through ear/headphones. However, to indicate objects located directly in front, both the left and right earphones are rung simultaneously. These sound cues could easily help a visually impaired individual locate objects with the help of their smartphones and reduce the reliance on people in their surroundings, consequently making them more independent. This application is made with the flutter development platform and uses the Efficientdet-D2 model for object detection in the backend. We believe the app will significantly assist the visually impaired in a safe and user-friendly manner with its capacity to work completely offline. Our application can be accessed here https://github.com/MohammedZ666/SonoVision.git.

</details>


### [47] [SAM 3D for 3D Object Reconstruction from Remote Sensing Images](https://arxiv.org/abs/2512.22452)
*Junsheng Yao,Lichao Mou,Qingyu Li*

Main category: cs.CV

TL;DR: 评估通用图像到3D模型（SAM 3D）在单目遥感建筑重建上的表现，并与TRELLIS对比；结果显示SAM 3D在屋顶几何一致性与边界清晰度上更优，并提出面向城市场景的“分割-重建-组合”流程及其局限与未来方向。


<details>
  <summary>Details</summary>
Motivation: 单目遥感3D建筑重建对大规模城市建模关键，但现有方法依赖任务特化架构与大量监督，缺乏可泛化的基础模型方案与系统性评估。

Method: 在NYC Urban Dataset样本上，将SAM 3D与TRELLIS对比评测；采用FID与基于CLIP的CMMD作为感知与跨模态分布差异指标；并将SAM 3D扩展为“segment-reconstruct-compose”的城市场景重建管线。

Result: SAM 3D较TRELLIS生成更连贯的屋顶几何与更锐利的建筑边界；在城市场景层面，通过分割-重建-组合流程展示了可行性。

Conclusion: 基础模型可有效用于单目遥感3D建筑与城市场景重建，具备实际部署价值；但仍存在局限，需引入场景级结构先验与进一步研究以提升鲁棒性与一致性。

Abstract: Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.

</details>


### [48] [Comparing Object Detection Models for Electrical Substation Component Mapping](https://arxiv.org/abs/2512.22454)
*Haley Mody,Namish Bansal,Dennies Kiprono Bor,Edward J. Oughton*

Main category: cs.CV

TL;DR: 论文比较三种目标检测模型（YOLOv8、YOLOv11、RF-DETR）在美国变电站图像上的组件识别表现，用于自动化、可扩展的变电站组件制图与脆弱性评估。


<details>
  <summary>Details</summary>
Motivation: 变电站关键资产易受自然灾害与GIC等多种威胁，人工制图耗时费力；为开展大规模脆弱性量化与风险缓解，需要高效、可靠的自动化组件识别与映射方法。

Method: 构建并手工标注美国变电站图像数据集；训练三类检测模型（YOLOv8、YOLOv11、RF-DETR）；从检测准确率、精度（precision）与效率（推理速度/资源开销）等维度进行对比评估；总结各模型优势与局限，并将最佳或合适模型用于全国范围组件映射。

Result: 三种模型均能识别多类变电站组件，性能存在权衡：不同模型在精度、召回与推理效率上各有优势与短板；论文明确指出哪一模型在大规模、可靠映射方面更具可行性，并展示了在美国范围的组件映射效果。

Conclusion: 计算机视觉可有效支持变电站组件自动化识别与制图；根据任务需求选择在准确性与效率间权衡最优的模型，可为电网韧性评估与风险缓解提供数据基础与应用示范。

Abstract: Electrical substations are a significant component of an electrical grid. Indeed, the assets at these substations (e.g., transformers) are prone to disruption from many hazards, including hurricanes, flooding, earthquakes, and geomagnetically induced currents (GICs). As electrical grids are considered critical national infrastructure, any failure can have significant economic and public safety implications. To help prevent and mitigate these failures, it is thus essential that we identify key substation components to quantify vulnerability. Unfortunately, traditional manual mapping of substation infrastructure is time-consuming and labor-intensive. Therefore, an autonomous solution utilizing computer vision models is preferable, as it allows for greater convenience and efficiency. In this research paper, we train and compare the outputs of 3 models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US substation images. Each model is evaluated for detection accuracy, precision, and efficiency. We present the key strengths and limitations of each model, identifying which provides reliable and large-scale substation component mapping. Additionally, we utilize these models to effectively map the various substation components in the United States, showcasing a use case for machine learning in substation mapping.

</details>


### [49] [Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing](https://arxiv.org/abs/2512.22464)
*Sukhyun Jeong,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 提出PGR^2M：在可解释姿态码的基础上，用RVQ学习残差码，实现文本驱动的3D动作生成与编辑，兼顾全局结构与细粒度时间动态，优于CoMo与近期扩散/离散化基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于姿态码（如CoMo）的逐帧离散表示难以表达细微的时间动态与高频细节，导致重建保真度与局部可控性下降；同时需要在可解释控制、语义对齐与结构保持的前提下提升生成与编辑质量。

Method: 提出PGR^2M混合表示：用姿态引导的RVQ tokenizer将动作分解为（1）姿态潜码，表示粗全局结构；（2）残差潜码，刻画细粒度时间变化。引入残差dropout以防模型过度依赖残差，保持姿态码的语义与可编辑性。建模方面，基础Transformer自回归地从文本预测姿态码；精炼Transformer在文本、姿态码与量化阶段条件下预测残差码。

Result: 在人体动作数据集HumanML3D与KIT-ML上，相比CoMo及近期扩散与tokenization方法，PGR^2M在FID与重建指标上更优；用户研究显示其支持直观、结构保持的运动编辑。

Conclusion: 通过姿态码+RVQ残差码的混合与两阶段Transformer预测，并配合残差dropout，PGR^2M在不牺牲可解释与可编辑性的情况下，显著提升文本到动作的生成与编辑质量。

Abstract: Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.

</details>


### [50] [Event-based high temporal resolution measurement of shock wave motion field](https://arxiv.org/abs/2512.22474)
*Taihang Lei,Banglei Guan,Minzu Liang,Pengju Sun,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于多事件相机的冲击波运动参数测量与三维重建方法，利用极坐标编码、ROI自适应提取与迭代斜率分析提取波前事件，建立事件—光学成像几何与运动参数模型，实现多角度测量、运动场重建与当量反演，速度测量误差最低0.06%、最高5.20%。


<details>
  <summary>Details</summary>
Motivation: 传统光学/传感器在冲击波快速且不均匀传播、强动态范围和测试条件不稳定下难以兼顾高时空分辨与高精度。需要一种能在强亮度变化、超高速场景中稳定获取冲击波运动参数并实现三维重建与当量评估的方法。

Method: - 采用多事件相机，利用其高时间分辨与高动态范围。
- 建立极坐标事件编码以揭示传播模式，并通过事件偏移计算实现自适应ROI提取。
- 用迭代斜率分析（基于速度变化连续性）从事件流中提取冲击波前事件。
- 基于事件成像模型推导事件几何与冲击波运动参数关系，构建3D重建模型。
- 多角度融合实现运动场重建与爆炸当量反演。

Result: 与压力传感器与经验公式对比，速度测量误差范围0.06%—5.20%；实现了高时空分辨的多角度冲击波测量、运动场重建与当量反演。

Conclusion: 多事件相机框架能在复杂条件下高精度测量冲击波运动并进行三维重建，兼具高时空分辨与鲁棒性，为功率场测试和损伤评估提供有效工具。

Abstract: Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.

</details>


### [51] [Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection](https://arxiv.org/abs/2512.22483)
*Zihan Liu,Xiangning Ren,Dezhang Kong,Yipeng Zhang,Meng Han*

Main category: cs.CV

TL;DR: 提出一种面向红外小目标检测的半监督两阶段范式：先用仅10%标注数据与包含物理先验的层级MoE适配器对SAM蒸馏成专家教师（Scalpel-SAM），再用其生成伪标签训练轻量下游模型，在极少标注下达到或超越全监督性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标(IR-SOT)标注成本高且数据稀缺；现有使用SAM的方案存在域间差异、难以注入物理先验、结构臃肿与部署困难等问题，需一种既能利用少量标注又能适配部署的体系。

Method: 设计层级MoE Adapter（由四个可解释的白盒神经算子构成），作为核心模块；提出两阶段流程：1) 先验引导的知识蒸馏：用10%全标注数据+MoE适配器，将通用SAM蒸馏为更适配红外域的专家教师Scalpel-SAM；2) 面向部署的知识迁移：用Scalpel-SAM在大量未标注红外数据上生成伪标签，训练轻量高效的下游检测/跟踪模型。

Result: 在极少标注条件下，所提范式训练的下游模型性能可与全监督训练相当或更优；验证其有效缩小域间差异并提升红外小目标检测/跟踪性能与效率。

Conclusion: 通过引入具物理先验的层级MoE适配器与两阶段蒸馏-迁移流程，系统性缓解红外小目标数据稀缺与部署难题，是首个以SAM为教师、面向IR-SOT的半监督范式，并在低标注成本下实现接近/超越全监督表现。

Abstract: Infrared small object detection urgently requires semi-supervised paradigms due to the high cost of annotation. However, existing methods like SAM face significant challenges of domain gaps, inability of encoding physical priors, and inherent architectural complexity. To address this, we designed a Hierarchical MoE Adapter consisting of four white-box neural operators. Building upon this core component, we propose a two-stage paradigm for knowledge distillation and transfer: (1) Prior-Guided Knowledge Distillation, where we use our MoE adapter and 10% of available fully supervised data to distill SAM into an expert teacher (Scalpel-SAM); and (2) Deployment-Oriented Knowledge Transfer, where we use Scalpel-SAM to generate pseudo labels for training lightweight and efficient downstream models. Experiments demonstrate that with minimal annotations, our paradigm enables downstream models to achieve performance comparable to, or even surpassing, their fully supervised counterparts. To our knowledge, this is the first semi-supervised paradigm that systematically addresses the data scarcity issue in IR-SOT using SAM as the teacher model.

</details>


### [52] [Tracking by Predicting 3-D Gaussians Over Time](https://arxiv.org/abs/2512.22489)
*Tanish Baranwal,Himanshu Gaurav Singh,Jathushan Rajasegaran,Jitendra Malik*

Main category: cs.CV

TL;DR: 提出Video-GMAE：把视频编码为随时间运动的高斯斑点，用自监督掩码自编码学习；这个表征天然符合“视频是动态三维场景投影”的归纳偏置，预训练后直接在图像平面读取高斯轨迹即可零样本跟踪，效果接近SOTA；少量微调后在Kinetics与Kubric上显著超越现有自监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督视频表征多基于2D像素或patch，缺少与三维动态场景一致的归纳偏置，难以自然地产生时空一致性与跟踪能力；需要一种既能高效学习又能对物体与运动形成稳定表征的方法。

Method: 设计Video Gaussian Masked Autoencoders：将视频表示为一组随时间演化的高斯splats；采用掩码自编码预训练，通过重建被遮挡内容来学习；网络学习到每个高斯的参数与随时间的轨迹，并将这些轨迹投影到图像平面以作下游零样本跟踪；随后在小规模数据上微调评估下游任务。

Result: 零样本跟踪性能与当前最优方法相当；在下游视频任务上，经小规模微调，Kinetics上提升34.6%，Kubric上提升13.1%，优于现有自监督视频表征方法；代码与项目页已开放。

Conclusion: 以时变高斯表示视频为核心的自监督学习在没有显式监督的情况下自然产生跟踪能力，并在多个数据集上带来显著收益，说明将视频视为动态三维场景投影的归纳偏置是有效的且可推广。

Abstract: We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.

</details>


### [53] [SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration](https://arxiv.org/abs/2512.22503)
*Xin Chen,Kang Luo,Yangyi Xiao,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出SCAFusion，用于月面机器人小而不规则目标的多模态3D检测；在保持参数/算力几乎不变下，通过多种对齐与注意力改进，大幅提升nuScenes与月面仿真小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 地外（月面）环境中，需要可靠检测如陨石碎片/岩块等小型不规则物体以保障自主导航。但现有面向地面自动驾驶的多模态3D感知方法在异域环境中表现欠佳，主要因为跨模态特征对齐差、模态协同有限、对小目标不敏感。

Method: 在BEVFusion之上提出SCAFusion，包含：(1) Cognitive Adapter：高效调优相机主干；(2) Contrastive Alignment Module：对比式对齐相机与LiDAR特征；(3) Camera Auxiliary Training Branch：辅助训练以增强视觉表征；(4) Section-aware Coordinate Attention：面向小而不规则目标的坐标注意力细粒度建模。整体几乎不增加参数/计算。

Result: 在nuScenes验证集上达成69.7% mAP与72.1% NDS，较基线+5.0% mAP、+2.7% NDS；在Isaac Sim构建的月面仿真中达90.93% mAP，较基线+11.5%，对小型“流星样”障碍物检测提升显著。

Conclusion: SCAFusion通过跨模态对齐、辅助视觉训练与面向小目标的分段坐标注意力，在几乎不增加开销的前提下显著提升月面小型不规则目标3D检测，为月面机器人导航提供更稳健的多模态感知方案。

Abstract: Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.

</details>


### [54] [DreamOmni3: Scribble-based Editing and Generation](https://arxiv.org/abs/2512.22525)
*Bin Xia,Bohao Peng,Jiyang Liu,Sitong Wu,Jingyao Li,Junjia Huang,Xu Zhao,Yitong Wang,Ruihang Chu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出DreamOmni3：在统一生成/编辑框架中加入“涂鸦/手绘笔划+文本/多模态指令”的交互，解决文本难以精确定位与细粒度控制的问题，并通过联合输入两张图（原图与涂鸦图）、同一索引与位置编码实现精确区域对齐，在多个新基准上取得领先。


<details>
  <summary>Details</summary>
Motivation: 仅依赖文本的图像编辑/生成很难表达用户的具体编辑位置与细粒度视觉约束；二值掩码在多处涂鸦、多图融合、复杂指令情形下不够灵活与鲁棒。需要一种更贴近GUI交互、可用自由手绘标注的统一模型与数据。

Method: 1) 数据合成：围绕“涂鸦驱动”的两大类任务构建训练数据。a) 涂鸦编辑含四子任务：涂鸦+文本、涂鸦+多模态指令、图像融合、涂鸦修饰；b) 涂鸦生成含三子任务：涂鸦+文本、涂鸦+多模态指令、涂鸦生成。基于DreamOmni2提取可编辑区域，叠加手绘框/圆/涂鸦/裁剪图，形成配对样本。2) 框架设计：摒弃单一二值mask，采用“联合输入”——将原图与涂鸦图同时送入模型，用不同颜色编码区分区域，并对两图使用相同index与position encoding，使模型精准对齐与定位涂鸦区域，支持多涂鸦、多图与复杂指令。3) 构建完整基准用于评测。

Result: 在所建立的多任务基准上表现突出，优于现有统一生成/编辑方法；能够更好地精准定位与执行复杂编辑。

Conclusion: DreamOmni3用涂鸦引导+联合输入方案，提升了统一生成/编辑的可控性与精确度，适配GUI实用场景；数据与代码将开源，期望推动该方向研究。

Abstract: Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.

</details>


### [55] [CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation](https://arxiv.org/abs/2512.22536)
*Qinglin Zeng,Kaitong Cai,Ruiqi Chen,Qinhan Lv,Keze Wang*

Main category: cs.CV

TL;DR: CoAgent提出一个“规划-合成-验证”的闭环多代理框架，用于长视频的连贯生成，显著降低身份漂移与场景不一致问题。


<details>
  <summary>Details</summary>
Motivation: 开域文本到视频生成常把镜头逐个独立生成，导致角色外观漂移、场景与时间逻辑不稳、叙事节奏紊乱，缺乏跨镜头的全局一致性控制与自动纠错机制。

Method: 引入协作式闭环流程：1) Storyboard Planner将用户提示、风格与节奏约束分解为结构化分镜计划（实体、空间关系、时间线索）；2) Global Context Manager维持实体级记忆，确保跨镜头外观与身份一致；3) Synthesis Module在Visual Consistency Controller指导下逐镜生成；4) Verifier Agent用视觉-语言推理评估中间结果并选择性重生成；5) 节奏感知编辑器对节奏与转场进行后期对齐。

Result: 在长视频生成任务上，实验显示该框架在叙事连贯性、视觉一致性与整体叙事质量方面均显著优于现有方法。

Conclusion: 多代理闭环与全局上下文记忆结合的计划—合成—验证流水线，有效缓解开域视频生成中的身份漂移与一致性问题，为长形式视频的可控与稳健生成提供了通用范式。

Abstract: Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.

</details>


### [56] [Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains](https://arxiv.org/abs/2512.22545)
*Jesen Zhang,Ningyuan Liu,Kaitong Cai,Sidi Liu,Jing Yang,Ziliang Chen,Xiaofei Sun,Keze Wang*

Main category: cs.CV

TL;DR: 提出SR-MCR：利用模型自身输出的过程信号（语义一致、词汇忠实、非冗余、视觉落地、步骤一致）构造归一化可靠性加权奖励，结合无判别器的GRPO与置信度退火，轻量且免标注地对多模态LLM进行过程对齐，显著提升答案准确率与推理连贯性，在7B规模达SOTA（81.4%）。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM常出现“流利但不可靠”的推理：步骤间不连贯、视觉依据不足。现有对齐多关注最终答案，忽略中间推理的可靠性与可解释性，导致幻觉与跳步。需要一种不依赖人工标注、可细粒度约束推理过程的训练方法。

Method: 提出SR-MCR（Self-Referential Multi-Cue Reasoning）：从模型自身生成中提取五类自指信号——语义对齐、词汇忠实、非冗余、视觉落地、步骤一致性；将其整合为归一化、可靠性加权的过程级奖励。采用无评论家（critic-free）的GRPO目标，并引入基于置信度的“冷却”机制抑制过度自信或平庸解。整体为轻量、免标注的对齐框架，基于Qwen2.5-VL进行训练。

Result: 在多项视觉基准上同时提升答案准确率与推理连贯性；在同规模开源模型中，SR-MCR-7B取得平均准确率81.4%的SOTA。消融实验证明五个奖励项与冷却模块各自带来独立增益，训练稳定性提高。

Conclusion: 通过利用自指过程信号并以可靠性加权的方式进行过程对齐，SR-MCR在无需额外标注与判别器的条件下有效提升多模态LLM的可依赖推理与准确性，提供了一条稳定、可扩展的对齐新范式。

Abstract: Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.

</details>


### [57] [ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization](https://arxiv.org/abs/2512.22570)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Arefin Ittesafun Abian,Yan Zhang,Mirjam Jonkman,Sami Azam*

Main category: cs.CV

TL;DR: 提出ReFRM3D与基于多特征肿瘤标记的分类器，用多模MRI实现更高效的胶质瘤分割与分类，在BraTS2019/2020/2021上取得高DSC。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤致死率高、诊断复杂；现有方法面临影像异质性大、算力利用不足、分割与分类效率与精度不足的问题，需要新的模型与流程提升分割精度与分类鲁棒性与效率。

Method: 基于3D U-Net提出Radiomics增强的融合残差多参数3D网络ReFRM3D：引入多尺度特征融合、混合上采样、扩展残差跳连；利用多参数MRI训练分割；在分割ROI上提取放射组学特征，构建多特征肿瘤标记分类器进行分类。

Result: 在BraTS2019/2020/2021上分割显著提升：DSC（WT/ET/TC）分别为2019: 94.04/92.68/93.64%，2020: 94.09/92.91/93.84%，2021: 93.70/90.36/92.13%。

Conclusion: ReFRM3D结合放射组学与多参数MRI可有效提升胶质瘤分割与下游分类性能，具有跨数据集的稳定性和高效率潜力。

Abstract: Gliomas are among the most aggressive cancers, characterized by high mortality rates and complex diagnostic processes. Existing studies on glioma diagnosis and classification often describe issues such as high variability in imaging data, inadequate optimization of computational resources, and inefficient segmentation and classification of gliomas. To address these challenges, we propose novel techniques utilizing multi-parametric MRI data to enhance tumor segmentation and classification efficiency. Our work introduces the first-ever radiomics-enhanced fused residual multiparametric 3D network (ReFRM3D) for brain tumor characterization, which is based on a 3D U-Net architecture and features multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. Additionally, we propose a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented regions. Experimental results demonstrate significant improvements in segmentation performance across the BraTS2019, BraTS2020, and BraTS2021 datasets, achieving high Dice Similarity Coefficients (DSC) of 94.04%, 92.68%, and 93.64% for whole tumor (WT), enhancing tumor (ET), and tumor core (TC) respectively in BraTS2019; 94.09%, 92.91%, and 93.84% in BraTS2020; and 93.70%, 90.36%, and 92.13% in BraTS2021.

</details>


### [58] [KV-Tracker: Real-Time Pose Tracking with Transformers](https://arxiv.org/abs/2512.22581)
*Marwan Taher,Ignacio Alzugaray,Kirill Mazur,Xin Kong,Andrew J. Davison*

Main category: cs.CV

TL;DR: 提出KV-Tracker：通过缓存多视图网络全局自注意力的KV对，实现单目视频实时6DoF位姿跟踪与在线重建，推理最高提速约15×，无需重训且适配多模型，性能强且帧率最高约27FPS。


<details>
  <summary>Details</summary>
Motivation: 多视图3D几何网络提供强先验但速度慢，难以在线应用；需要一种在保持精度与鲁棒性的同时显著加速的方法，以支持实时位姿跟踪与重建，并避免漂移与遗忘。

Method: 1) 采用π^3架构的全双向注意力，从单目视频快速选择并维护关键帧以构建地图；2) 将全局自注意力模块的键值对缓存，作为唯一场景表示用于在线跟踪；3) 该缓存策略与具体模型无关，可直接用于现成多视图网络而无需再训练。

Result: 在TUM RGB-D、7-Scenes、Arctic、OnePose上验证：在保持强精度与鲁棒性的同时，将推理加速至最高约15×，在线帧率可达约27 FPS，支持场景级跟踪和更具挑战的即席目标跟踪与重建（无深度或目标先验）。

Conclusion: 缓存全局自注意力KV作为场景表示可在不牺牲精度的情况下显著加速多视图3D网络，实现通用、无重训、实时的6DoF跟踪与在线重建，并适用于不同离线多视图模型。

Abstract: Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\sim}27$ FPS.

</details>


### [59] [PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment](https://arxiv.org/abs/2512.22602)
*Bin Wang,Yang Xu,Huan Zhao,Hao Zhang,Zixing Zhang*

Main category: cs.CV

TL;DR: 提出PTalker：一种通过风格解耦与三重跨模态对齐，实现个性化且高拟真的语音驱动三维说话人头部动画的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然嘴形同步精度提高，但忽视个人说话风格（语速、表情幅度、韵律等）导致缺乏个性化与真实感，需要能同时维护风格与提升唇同步的统一框架。

Method: 1) 风格-内容解耦：对音频与面部运动序列施加解耦约束，将其编码到独立的style/content空间以保留个体说话风格。2) 三层模态对齐提升唇同步：- 空间对齐：用图注意力网络建模3D网格顶点邻接；- 时间对齐：跨注意力同步语音与网格的时序依赖；- 特征对齐：Top-k双向对比学习与KL散度约束，增强语音与网格表征一致性。

Result: 在公开数据集上的主客观评测均优于SOTA，生成更真实且能匹配身份特定说话风格的3D说话人动画，唇形同步精度更高。

Conclusion: PTalker通过风格解耦与三重对齐，兼顾个性化风格与唇同步精度，显著提升3D说话头部生成的真实感与个体一致性；代码与视频已开放。

Abstract: Speech-driven 3D talking head generation aims to produce lifelike facial animations precisely synchronized with speech. While considerable progress has been made in achieving high lip-synchronization accuracy, existing methods largely overlook the intricate nuances of individual speaking styles, which limits personalization and realism. In this work, we present a novel framework for personalized 3D talking head animation, namely "PTalker". This framework preserves speaking style through style disentanglement from audio and facial motion sequences and enhances lip-synchronization accuracy through a three-level alignment mechanism between audio and mesh modalities. Specifically, to effectively disentangle style and content, we design disentanglement constraints that encode driven audio and motion sequences into distinct style and content spaces to enhance speaking style representation. To improve lip-synchronization accuracy, we adopt a modality alignment mechanism incorporating three aspects: spatial alignment using Graph Attention Networks to capture vertex connectivity in the 3D mesh structure, temporal alignment using cross-attention to capture and synchronize temporal dependencies, and feature alignment by top-k bidirectional contrastive losses and KL divergence constraints to ensure consistency between speech and mesh modalities. Extensive qualitative and quantitative experiments on public datasets demonstrate that PTalker effectively generates realistic, stylized 3D talking heads that accurately match identity-specific speaking styles, outperforming state-of-the-art methods. The source code and supplementary videos are available at: PTalker.

</details>


### [60] [Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer](https://arxiv.org/abs/2512.22612)
*Dafeng Zhang,Yongqi Song,Shizhuo Liu*

Main category: cs.CV

TL;DR: 提出一种预测驱动的Top-K Jaccard与稀疏差分Transformer（SDT），在面部聚类中提高相似度测量纯度与鲁棒性，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有以Jaccard替代余弦距离的相似度度量虽然提升了一定精度，但引入了大量无关邻居节点，Jaccard判别力不足，影响聚类。需要一种既能控制邻居纯度又能鲁棒建模中心-邻居关系的方案。

Method: 1) 预测驱动的Top-K Jaccard：根据模型预测确定近邻K值，仅保留更纯的邻居以计算Jaccard；2) 用Transformer建模中心节点与Top-K附近邻居的关系，进一步提升相似度估计可靠性；3) 针对原生Transformer易受无关关系干扰的问题，提出稀疏差分Transformer（SDT），通过稀疏化与差分机制抑制噪声、强调关键信息。

Result: 在MS-Celeb-1M等多数据集上进行大量实验，所提方法优于现有方法，达到SOTA。

Conclusion: 通过预测驱动的邻居选择与SDT的抗噪关系建模，提升了Jaccard相似度的判别力与聚类鲁棒性，显著改进面部聚类性能。

Abstract: The method used to measure relationships between face embeddings plays a crucial role in determining the performance of face clustering. Existing methods employ the Jaccard similarity coefficient instead of the cosine distance to enhance the measurement accuracy. However, these methods introduce too many irrelevant nodes, producing Jaccard coefficients with limited discriminative power and adversely affecting clustering performance. To address this issue, we propose a prediction-driven Top-K Jaccard similarity coefficient that enhances the purity of neighboring nodes, thereby improving the reliability of similarity measurements. Nevertheless, accurately predicting the optimal number of neighbors (Top-K) remains challenging, leading to suboptimal clustering results. To overcome this limitation, we develop a Transformer-based prediction model that examines the relationships between the central node and its neighboring nodes near the Top-K to further enhance the reliability of similarity estimation. However, vanilla Transformer, when applied to predict relationships between nodes, often introduces noise due to their overemphasis on irrelevant feature relationships. To address these challenges, we propose a Sparse Differential Transformer (SDT), instead of the vanilla Transformer, to eliminate noise and enhance the model's anti-noise capabilities. Extensive experiments on multiple datasets, such as MS-Celeb-1M, demonstrate that our approach achieves state-of-the-art (SOTA) performance, outperforming existing methods and providing a more robust solution for face clustering.

</details>


### [61] [Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone](https://arxiv.org/abs/2512.22615)
*Jiacheng Ye,Shansan Gong,Jiahui Gao,Junming Fan,Shuang Wu,Wei Bi,Haoli Bai,Lifeng Shang,Lingpeng Kong*

Main category: cs.CV

TL;DR: 作者提出基于扩散式大语言模型的视觉-语言与视觉-语言-行动体系：Dream-VL 与 Dream-VLA。相较传统自回归VLM，它们在视觉规划与机器人控制中表现更优，收敛更快，并在多个机器人基准上刷新或追平开源最优。


<details>
  <summary>Details</summary>
Motivation: 自回归VLM按序生成，难以高效处理需要全局一致性与并行决策的复杂视觉规划、动态控制与动作分块；研究者希望利用扩散式（本质上双向、可并行）语言模型作为骨干，缓解顺序瓶颈并提升下游机器人任务表现与训练效率。

Method: 1) 构建Dream-VL：以扩散式LLM为骨干的开源dVLM，在开放数据上预训练，评测与主流AR-VLM对比；2) 基于Dream-VL持续预训练形成Dream-VLA：在开放机器人数据上做视觉-语言-行动建模；3) 利用扩散骨干的双向性实现动作chunking与并行生成；4) 在多基准上系统评测，并与AR基线（含π_0、GR00T-N1）对比。

Result: Dream-VL在多项VLM基准上达到当前dVLM最优，整体与顶级开源AR-VLM相当，尤其在视觉规划任务上更优。Dream-VLA在机器人基准上取得SOTA：LIBERO平均成功率97.2%，SimplerEnv-Bridge总体71.4%，SimplerEnv-Fractal总体60.5%，超过π_0与GR00T-N1；同时在不同训练目标上，dVLM下游任务整体优于AR基线，并表现出显著更快的微调收敛。

Conclusion: 扩散式LLM作为VLM/VLA骨干在规划与控制场景具备天然优势：双向、并行与动作分块友好，带来更强性能与更高训练效率。作者开源Dream-VL与Dream-VLA，期望推动社区研究。

Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

</details>


### [62] [Rethinking Memory Design in SAM-Based Visual Object Tracking](https://arxiv.org/abs/2512.22624)
*Mohamad Alansari,Muzammal Naseer,Hasan Al Marzouqi,Naoufel Werghi,Sajid Javed*

Main category: cs.CV

TL;DR: 研究系统性分析并统一设计SAM系目标跟踪中的“记忆”，在SAM2与SAM3上重现对比不同记忆策略，提出将记忆显式分解为短期表观与长期干扰消解的混合框架，跨10个基准验证在遮挡、复杂运动与干扰场景下更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM2的跟踪方法普遍依赖记忆来提升稳健性，但各自为战：记忆何取舍、如何迁移到更强的SAM3并无清晰设计原则与系统评估。需要统一视角厘清记忆策略差异、可迁移性与通用设计准则。

Method: 1) 选取具代表性的SAM2跟踪器，剖析其共性与差异，发现主要差异集中在短期记忆帧选择策略，而对象表征基本一致；2) 将这些记忆机制在SAM3框架中忠实重现，在十个多样化基准上进行大规模、可控对比，隔离骨干强度影响；3) 基于实证提出统一的混合记忆框架，将记忆显式分解为：短期表观记忆（快速适应目标外观变化）与长期干扰消解记忆（对抗遮挡/相似干扰），并以模块化方式整合既有策略。

Result: 跨SAM2与SAM3骨干，所提混合记忆框架在长时间遮挡、复杂运动以及强干扰场景中持续带来更高鲁棒性与性能提升；验证了短期与长期记忆分解的有效性和多策略可并用的可迁移性。

Conclusion: 记忆设计是SAM系跟踪器性能的关键。通过系统化评估与在SAM3中的可迁移复现，论文提出的显式分解与模块化融合原则，提供了通用而有效的记忆设计路径，且在多基准上证实其鲁棒性收益；代码开源、结果仍有更新可能。

Abstract: \noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}

</details>


### [63] [Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion](https://arxiv.org/abs/2512.22626)
*Yuming Gu,Yizhi Wang,Yining Hong,Yipeng Gao,Hao Jiang,Angtian Wang,Bo Liu,Nathaniel S. Dennler,Zhengfei Kuang,Hao Li,Gordon Wetzstein,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出Envision：以目标图像为约束的扩散式视觉规划框架，先生成任务一致的目标图，再用首末帧条件视频扩散在起始与目标间插值，得到物理可行、对齐目标的轨迹，显著提升对齐度、空间一致性与物体保真，并可直接用于机器人控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频扩散的视觉规划多为前向预测，只依赖初始观测，缺乏对目标的显式建模，易出现空间漂移与目标不对齐，难以为具身操作提供可靠视觉计划。

Method: 两阶段框架：1）Goal Imagery Model：从场景与指令中通过区域感知的交叉注意力，定位任务相关区域并合成一致的目标图像；2）Env-Goal Video Model：基于首末帧条件的视频扩散模型（FL2V），以上述目标图与初始观测为边界条件进行时域插值，生成平滑、物理可行、朝向目标演化的视频轨迹。

Result: 在物体操作与图像编辑基准上，优于基线，在目标对齐、空间一致性与物体保持方面表现更好，生成的视觉计划能直接支持下游机器人规划与控制。

Conclusion: 显式引入目标图像约束并采用首末帧条件的视频扩散插值，可有效缓解前向预测的漂移与错位问题，产出可执行、与目标一致的视觉计划，为具身智能中的操作任务提供可靠的视觉引导。

Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.

</details>


### [64] [FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution](https://arxiv.org/abs/2512.22647)
*Yidi Liu,Zihao Fan,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出FinPercep-RM细粒度感知奖励模型与协同进化课程学习（CCL），缓解ISR中传统IQA全局分数导致的奖励黑客与训练不稳问题，兼顾全局质量与局部真实感，并在多种ISR与RLHF设置下验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统IQA作为RLHF奖励仅给出全局分数，难以感知局部/细微失真，易被ISR模型“钻空子”获得高分却产生伪影（奖励黑客），导致感知质量与优化目标错位。

Method: 1) 设计Encoder-Decoder架构的FinPercep-RM，同时输出全局质量分与像素级感知劣化图以定位量化局部缺陷；并构建包含真实SR模型多样细微失真的FGR-30k数据集进行训练。2) 提出协同进化课程学习（CCL）：奖励模型复杂度逐步提升；ISR策略学习先用简单全局奖励快速收敛，再逐步过渡到复杂、细粒度奖励，稳定训练并抑制奖励黑客。

Result: 在多种ISR模型与RLHF设置上，FinPercep-RM与CCL带来更高的全局质量与更真实的局部视觉效果，相比传统IQA奖励显著减少伪影与奖励黑客现象，训练更稳定。

Conclusion: 细粒度奖励与课程协同进化可有效对齐ISR的感知目标，兼顾稳定性与鲁棒性；FGR-30k与FinPercep-RM为面向局部质量对齐的RLHF提供了通用基座。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity allows ISR models to produce perceptually undesirable artifacts that yield spurious high scores, misaligning optimization objectives with perceptual quality and results in reward hacking. To address this, we propose a Fine-grained Perceptual Reward Model (FinPercep-RM) based on an Encoder-Decoder architecture. While providing a global quality score, it also generates a Perceptual Degradation Map that spatially localizes and quantifies local defects. We specifically introduce the FGR-30k dataset to train this model, consisting of diverse and subtle distortions from real-world super-resolution models. Despite the success of the FinPercep-RM model, its complexity introduces significant challenges in generator policy learning, leading to training instability. To address this, we propose a Co-evolutionary Curriculum Learning (CCL) mechanism, where both the reward model and the ISR model undergo synchronized curricula. The reward model progressively increases in complexity, while the ISR model starts with a simpler global reward for rapid convergence, gradually transitioning to the more complex model outputs. This easy-to-hard strategy enables stable training while suppressing reward hacking. Experiments validates the effectiveness of our method across ISR models in both global quality and local realism on RLHF methods.

</details>


### [65] [Visual Autoregressive Modelling for Monocular Depth Estimation](https://arxiv.org/abs/2512.22653)
*Amir El-Ghoussani,André Kaup,Nassir Navab,Gustavo Carneiro,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 提出一种基于视觉自回归(Vision Autoregressive, VAR)先验的单目深度估计方法，采用规模化T2I VAR模型并加入按尺度条件上采样与无分类器引导，在10个固定自回归阶段推理、仅用7.4万合成样本微调即可达到室内SOTA与户外强表现，展现相较扩散模型在数据可扩展性和3D任务适配上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计常依赖扩散先验或大量标注/合成数据，训练与推理成本高、对数据规模敏感。作者希望探索除扩散外的另一类生成式几何先验——自回归模型，以期在更少数据、更稳定推理阶段下获得可比甚至更优的深度估计效果，并验证VAR在3D视觉任务中的适配性与可扩展性。

Method: 将大规模文本到图像的视觉自回归模型改造为深度估计器：1) 以VAR为基础，设计尺度级(Scale-wise)条件上采样机制，使模型在多尺度上逐步细化深度；2) 采用Classifier-Free Guidance增强条件控制；3) 采用固定10阶段的自回归推理流程；4) 仅用7.4万合成数据进行微调。

Result: 在受限训练条件(仅7.4万合成样本)下，室内数据集达到SOTA；在户外数据集上也取得强竞争力；推理阶段固定为10步，效率和稳定性较好。

Conclusion: 视觉自回归先验可作为与扩散先验互补的几何感知生成模型家族，能在较小数据规模下实现强性能，并对3D视觉任务具有良好适配性。

Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".

</details>


### [66] [Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos](https://arxiv.org/abs/2512.22657)
*Shravan Saranyan,Pramit Saha*

Main category: cs.CV

TL;DR: 研究比较多种深度学习架构（3D Inception、双流、CNN-RNN）在超声心动图视频上估计LVEF，发现改进版3D Inception表现最佳（RMSE 6.79%），并讨论过拟合与超参数敏感性。


<details>
  <summary>Details</summary>
Motivation: LVEF是评估心功能的重要指标，人工读片耗时且存在观察者差异。深度学习有望自动、稳定、接近专家水平地估计LVEF，需系统比较不同视频模型与训练策略的有效性。

Method: 基于EchoNet-Dynamic数据集（10,030例超声视频），实现并对比3D Inception、双流与CNN-RNN；系统调研架构改动与多模态/多分支融合；调参包括卷积核大小、归一化策略等，并评估泛化与过拟合。

Result: 改进的3D Inception取得最佳总体性能，RMSE 6.79%。各架构普遍存在过拟合倾向，更小更简单的模型泛化更好；性能对超参数（尤其卷积核尺寸与归一化）高度敏感。

Conclusion: 在LVEF估计中，优化的3D Inception最优，但需谨慎防止过拟合并重视超参数选择。所获关于架构与训练的见解对更广泛的视频分析任务亦有借鉴意义。

Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.

</details>


### [67] [Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains](https://arxiv.org/abs/2512.22664)
*Qiankun Li,Feng He,Huabao Chen,Xin Ning,Kun Wang,Zengfu Wang*

Main category: cs.CV

TL;DR: 提出CLAdapter：一种带聚类注意力的适配器，在小样本/专域下高效利用大规模预训练视觉模型的表征，统一支持CNN/Transformer与2D/3D任务，在10个跨科学领域数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT、ConvNeXt等在LAION、ImageNet-21K、Kinetics等大规模数据上预训练，许多专门科学领域数据有限、分布差异大，直接迁移表现受限，需要一种既能保留大模型知识又能针对小数据场景个性化适配的方法。

Method: 提出Cluster Attention Adapter（CLAdapter）。核心思想：在特征空间引入聚类中心与注意力机制，计算样本特征与聚类分布的相关性，利用相应的变换矩阵对特征进行个性化增强/变换；从而为不同特征子集学习差异化表征。接口统一，易插拔，适配CNN/Transformer，支持2D与3D。

Result: 在涵盖通用、多媒体、生物、医学、工业、农业、环境、地理、材料、OOD与3D等10个数据集上，CLAdapter微调后取得SOTA，优于现有迁移/适配方法。

Conclusion: CLAdapter能有效把大规模预训练知识自适应地迁移到数据受限的多样科学下游任务，具有通用可插拔、架构无关、2D/3D统一的优势，显著释放基础视觉模型潜力。

Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.

</details>


### [68] [INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading](https://arxiv.org/abs/2512.22666)
*Mert Ikinci,Luna Toma,Karin U. Loeffler,Leticia Ussem,Daniela Süsskind,Julia M. Weller,Yousef Yeganeh,Martina C. Herwig-Carl,Shadi Albarqouni*

Main category: cs.CV

TL;DR: 提出INTERACT-CMIL多头深度学习框架，利用共享特征、部分监督与跨任务一致性损失，联合预测5个CMIL病理轴，在多中心数据上较CNN与基础模型显著提升（宏F1最高+55.1%）。


<details>
  <summary>Details</summary>
Motivation: CMIL分级对治疗与黑色素瘤风险预测关键，但形态学线索细微、诊断标准相互关联，人工一致性与可重复性不足，缺少标准化的计算基准。

Method: 构建多头网络同时预测WHO4、WHO5、水平/垂直扩展及细胞学异型性。采用共享特征学习；组合式部分监督以利用不完全标注；引入任务间依赖损失（Inter-Dependence Loss）强制各轴预测一致。以三家大学医院486个专家标注活检补丁构成多中心数据集进行训练与评估，并与CNN与大模型（FM）基线比较。

Result: 在多中心数据上，相较CNN与FM基线取得一致改进：宏F1在WHO4提高最高55.1%，在垂直扩展提高25.0%，其余轴亦有提升；输出在多轴上具备一致性与可解释性，并与专家分级对齐。

Conclusion: INTERACT-CMIL为CMIL提供可重复、可解释的多准则联合预测框架，改进分级准确性并促进数字眼科病理标准化；数据集与方法可作为后续研究的计算基准。

Abstract: Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.

</details>


### [69] [CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation](https://arxiv.org/abs/2512.22681)
*ZhenQi Chen,TsaiChing Ni,YuanFu Yang*

Main category: cs.CV

TL;DR: CritiFusion是一种无需再训练的推理阶段插件框架，通过语义评论与频域融合提升文生图的一致性与细节，显著提高人偏好与审美评分，效果可比奖励优化SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽具高保真，但对复杂提示词的语义对齐不足，生成细节与结构常偏离用户意图，需要一种通用、可与现有模型兼容、无需再训练的对齐与细化方法。

Method: 提出CritiFusion框架，含两大模块：1) CritiCore：调用视觉-语言模型与多LLM，对提示与中间生成进行高层语义审阅与上下文扩展，形成语义反馈以引导后续扩散采样；2) SpecFusion：在频域对中间生成状态进行融合，用谱域的低频注入保证粗结构，同时保留高频细节。整体作为推理时的插件，与现有扩散骨干无缝衔接且无需训练。

Result: 在标准基准上，人类一致性指标与视觉质量均显著提升；在人类偏好分与美学评估上持续优于基线，达到与SOTA奖励优化方法相当的水平；定性结果显示更佳的细节、真实感与提示遵循度。

Conclusion: 语义评论+频域对齐的推理时组合能有效强化文生图的提示遵循与视觉质量，具有通用性、无训练开销、可作为现有扩散模型的提升插件。

Abstract: Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.

</details>


### [70] [Autoregressive Flow Matching for Motion Prediction](https://arxiv.org/abs/2512.22688)
*Johnathan Xie,Stefan Stojanov,Cristobal Eyzaguirre,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出ARFM（自回归流匹配）用于概率建模连续序列，基于多样视频数据预测长时程未来点轨迹；在新建的人体与机器人运动预测基准上，ARFM能生成复杂运动，并将其预测轨迹用于下游人机动作预测显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测模型多在窄分布上训练，泛化和复杂运动建模有限；尽管大规模视频预测在视觉质量上突出，但难以精准刻画复杂运动动态。作者受视频生成可扩展性的启发，寻求一种可扩展、能精确建模连续时间序列运动的不确定性的通用方法。

Method: 提出自回归流匹配（ARFM）：将流匹配的连续概率流建模与自回归时序建模结合，对连续序列（点轨迹）进行条件生成。模型在多样视频数据上训练，学习从历史观测到未来点轨迹的分布；推理时递归生成长时程未来轨迹。并构建评测基准，覆盖人类与机器人运动预测任务。

Result: ARFM能预测复杂、长时程的点轨迹；在新基准上取得优异表现。将其预测的未来轨迹作为条件，能显著提升机器人动作预测与人体运动预测等下游任务的效果。

Conclusion: ARFM提供了一种可扩展的连续序列概率建模框架，能更准确地捕捉复杂运动并在多类下游任务中带来实质收益；公开代码与模型支持复现与进一步研究。

Abstract: Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.

</details>


### [71] [Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors](https://arxiv.org/abs/2512.22689)
*Salvador Rodriguez-Sanz,Monica Hernandez*

Main category: cs.CV

TL;DR: 提出一种基于Neural ODE的多模态微分同胚配准方法，结合结构描述子与局部互信息，实例级无训练需求，在多数据集上优于多种SOTA，并兼顾大/小形变、尺度与正则鲁棒性及效率。


<details>
  <summary>Details</summary>
Motivation: 现有非刚性配准在精度、形变模型复杂度与正则化之间存在权衡，且多假设同模态强度相关，限制多模态适用性；学习式方法依赖大量训练数据，且对未见模态泛化差。

Method: 以Neural ODE实现连续深度的微分同胚形变场建模，使用结构描述子作为模态无关度量，并引入局部互信息；提出三种变体：基于图像的结构描述子、基于特征的结构描述子、以及结合非结构相似性的局部互信息；采用实例级优化，无需大规模训练。

Result: 在多种跨数据集实验（包含大/小形变、多模态）中，定性与定量均优于针对多模态或大形变的SOTA基线；在不同显式正则强度下保持低误差；支持多尺度配准；相较面向大形变的方法具更高效率。

Conclusion: Neural ODE驱动的多模态微分同胚配准框架在无需大量训练数据的前提下，实现对未见模态的强泛化、对正则与尺度的鲁棒性，并在精度与效率上优于现有方法。

Abstract: This work proposes a multimodal diffeomorphic registration method using Neural Ordinary Differential Equations (Neural ODEs). Nonrigid registration algorithms exhibit tradeoffs between their accuracy, the computational complexity of their deformation model, and its proper regularization. In addition, they also assume intensity correlation in anatomically homologous regions of interest among image pairs, limiting their applicability to the monomodal setting. Unlike learning-based models, we propose an instance-specific framework that is not subject to high scan requirements for training and does not suffer performance degradation at inference time on modalities unseen during training. Our method exploits the potential of continuous-depth networks in the Neural ODE paradigm with structural descriptors, widely adopted as modality-agnostic metric models which exploit self-similarities on parameterized neighborhood geometries. We propose three different variants that integrate image-based or feature-based structural descriptors and nonstructural image similarities computed by local mutual information. We conduct extensive evaluations on different experiments formed by scan dataset combinations and show surpassing qualitative and quantitative results compared to state-of-the-art baselines adequate for large or small deformations, and specific of multimodal registration. Lastly, we also demonstrate the underlying robustness of the proposed framework to varying levels of explicit regularization while maintaining low error, its suitability for registration at varying scales, and its efficiency with respect to other methods targeted to large-deformation registration.

</details>


### [72] [SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis](https://arxiv.org/abs/2512.22706)
*Paul Dobre,Jackson Cooper,Xin Wang,Hongzhou Yang*

Main category: cs.CV

TL;DR: SCPainter提出一个统一框架，把3D高斯点渲（3D Gaussian Splat）车体资产与场景点云投影到新视角，并用扩散模型条件生成高质量图像，从而同时实现真实的3D资产插入与新视角合成，提升自动驾驶仿真的数据多样性与真实性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要覆盖长尾场景的多样高质量训练数据。现有方法要么能重建并插入3D资产但缺乏真实感（光照/阴影不匹配），要么能做NVS但与资产插入割裂，难以在统一框架下支持交互与新场景生成。

Method: 构建统一管线：1) 用3D高斯点渲表征动态车辆资产，场景用3D点云；2) 将资产与场景共同投影到目标新视角，得到条件信息；3) 以该条件引导扩散模型，生成外观一致、光照更真实的图像，实现同时的资产插入与NVS。

Result: 在Waymo Open Dataset上展示了该框架能在新视角下生成高保真画面，并实现逼真的车辆资产插入，定性/定量显示较强的合成质量与多样化能力。

Conclusion: 将3D GS资产、场景点云与扩散生成有效融合，可统一解决资产插入与NVS问题，为构建多样、写实的自动驾驶仿真数据提供实用工具。

Abstract: 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.

</details>


### [73] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Olivier X. Miguel,Kevin Dick,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 该研究将大量无标注超声图像的自监督预训练模型（USF‑MAE）微调用于孕早期颈部囊性淋巴管瘤的二分类检测，在相同数据与评测设置下显著优于DenseNet‑169基线（ROC‑AUC 0.98 vs 0.94），并通过Score‑CAM显示出临床相关的关注区域；统计学检验差异显著。


<details>
  <summary>Details</summary>
Motivation: 颈部囊性淋巴管瘤在产前超声中提示高染色体异常和不良结局，需早筛且一致性强的自动检测。然而标注数据稀缺限制了传统监督深度学习性能，促使探索利用海量无标注超声数据进行自监督预训练以提升下游任务的准确性与鲁棒性。

Method: 采用在37万余张无标注超声图像上以掩码自编码（MAE）方式预训练的超声自监督基础模型USF‑MAE，并在目标任务（正常 vs 囊性淋巴管瘤）上微调；与DenseNet‑169在同一精选数据集、预处理流程和4折交叉验证协议下对比，评估指标包含准确率、敏感度、特异度与ROC‑AUC；以Score‑CAM定性解释关注区域，并用配对Wilcoxon符号秩检验评估统计显著性。

Result: USF‑MAE在全部指标上优于基线：准确率0.96（vs 0.93）、敏感度0.94（vs 0.92）、特异度0.98（vs 0.94）、ROC‑AUC 0.98（vs 0.94）。Score‑CAM热图聚焦胎儿颈部的期望区域；配对Wilcoxon检验p=0.0057，差异具有统计学意义。

Conclusion: 超声领域自监督预训练能在小标注集的孕早期囊性淋巴管瘤检测中显著提升性能与可解释性，优于传统从头或迁移学习的监督模型，支持其在可扩展早筛方案中的应用潜力。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [74] [Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation](https://arxiv.org/abs/2512.22745)
*Yongzhen Hu,Yihui Yang,Haotong Lin,Yifan Wang,Junting Dong,Yifu Deng,Xinyu Zhu,Fan Jia,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 提出Freetime FeatureGS与流式特征学习，摆脱视频级分割依赖，实现更稳健的分解4D重建，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分解4D重建通常把视频分割结果通过可微渲染提升到4D，但视频分割质量不稳定，导致重建不可靠。需一种对分割噪声鲁棒、无需视频级一致分割的方案。

Method: 以可线性运动的高斯原语集表示动态场景（Freetime FeatureGS），每个原语携带可学习特征并可随时间在邻域移动。使用对比损失：若投影落在同一2D实例分割则拉近特征，否则拉远。原语可跨时间移动使特征学习自然延时扩展，实现4D分割。训练时按时间顺序采样观测，进行流式特征传播，帮助跳出优化局部极小。

Result: 在多个数据集上，相比近期方法，重建质量（分解准确度与几何/外观保真度）大幅提升。

Conclusion: 通过Freetime FeatureGS与时序对比特征学习及流式训练，无需视频级分割即可实现稳健的分解4D重建，效果显著优于现有方法。

Abstract: This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.

</details>


### [75] [TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts](https://arxiv.org/abs/2512.22748)
*Hao Zhang,Mengsi Lyu,Bo Huang,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 提出一种针对长上下文、多图像场景的自适应视觉token剪枝方法，在保持性能的同时显著减少视觉token数量。


<details>
  <summary>Details</summary>
Motivation: LMM在多模态任务上有效，但视觉token数量随图像与上下文增长而激增，导致推理成本高。现有剪枝方法多忽略长上下文、跨多图像的冗余结构，无法在复杂场景中高效分配token预算并保持性能。

Method: 将冗余分解为图内（intra-image）与图间（inter-image）两类，通过“图内多样性”和“图间差异”进行量化，进而实现动态预算分配。两阶段流程：1）图内阶段：为每张图分配内容感知的token预算，并贪心选择其最具代表性的token；2）图间阶段：在全局范围做多样性过滤生成候选池，然后采用兼顾多样性与文本对齐的Pareto选择策略，得到最终保留token集合。

Result: 在长上下文设置和多图像输入的基准上进行大量实验证明：在显著减少视觉token数量的同时，模型性能基本保持不变或下降极小。

Conclusion: 通过图内/图间冗余建模与自适应预算分配的两阶段剪枝框架，可在长上下文、多图像场景下显著压缩视觉token而保持强性能，为LMM高效推理提供有效方案。

Abstract: Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.

</details>


### [76] [Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers](https://arxiv.org/abs/2512.22760)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 提出基于希尔伯特曲线重排的邻域感知Token精简方法，通过保留局部连续性来提升ViT的精度-效率折中。包含邻域感知剪枝(NAP)与相邻相似度合并(MAT)，在多项实验中达到了SOTA的效率与准确率平衡。


<details>
  <summary>Details</summary>
Motivation: 现有ViT的Token合并/剪枝往往忽视空间连续性与邻居关系，导致局部上下文丢失与计算冗余。需要一种能在1D序列处理保持2D邻域结构的方案，以减少冗余同时维持特征质量。

Method: 利用希尔伯特空间填充曲线将2D特征映射到1D序列以最大化邻域保持性；在此序列上：1) 邻域感知剪枝(NAP)：依据重要性评分并结合相邻依赖选择性保留Token，避免破坏局部结构；2) 相邻相似度合并(MAT)：按相邻Token相似度进行局部聚合以降低冗余。两者可单独或联合应用于ViT各层。

Result: 在多个视觉识别基准上，相比现有剪枝/合并方法取得更优的精度-效率折中，达到或刷新SOTA；在相同FLOPs或推理时延下获得更高准确率，或在相同精度下降幅度下实现更大计算/Token减少。

Conclusion: 保持空间连续性与邻域结构对ViT的Token精简至关重要。以希尔伯特曲线为基础的邻域感知策略能在不显著牺牲精度的情况下显著降低计算成本，为ViT架构优化提供了有效思路与实践路径。

Abstract: Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.

</details>


### [77] [Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2512.22771)
*Yiqian Li,Wen Jiang,Kostas Daniilidis*

Main category: cs.CV

TL;DR: 提出一种基于Fisher信息的主动视角选择算法，用于多摄像头数据中挑选最具信息增益的帧，从而同时提升动态场景渲染与语义分割效果，优于随机与不确定性启发式。


<details>
  <summary>Details</summary>
Motivation: 具身智能在语义与动态理解中存在大量冗余数据；现有基于启发式或随机的帧/视角选择缺乏原则性，难以兼顾语义推理与动态建模，需要一种能量化“信息性”的选择机制以高效训练。

Method: 将视角/帧选择表述为主动学习问题：以Fisher信息衡量候选视角对模型参数的增益，参数同时覆盖语义高斯表示与形变（deformation）网络。根据该信息度量从多摄像头序列中优先选取帧，统一处理静态与动态数据训练。

Result: 在大规模静态图像与动态视频数据集（多摄像头设置）上进行帧选择实验，结果显示渲染质量与语义分割性能稳定提升，明显优于随机选择和基于不确定性的启发式基线。

Conclusion: Fisher信息驱动的主动学习视角选择为联合语义与动态建模提供了原则化方案，能有效减少冗余并提升下游渲染与分割表现，相比现有启发式具有更一致与更强的性能。

Abstract: Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.

</details>


### [78] [Plug In, Grade Right: Psychology-Inspired AGIQA](https://arxiv.org/abs/2512.22780)
*Zhicheng Liao,Baoliang Chen,Hanwei Zhu,Lingyu Zhu,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 论文提出一种面向AGIQA的“算术分级反应模型”模块（AGQG），通过分离“图像能力”和“难度等级”，并以单调算术方式生成难度，从而得到单峰、可解释的质量分布，缓解文本-图像共享空间中的语义漂移问题；该模块可插拔，能稳健提升多种SOTA框架，并跨自然/屏显场景泛化良好。


<details>
  <summary>Details</summary>
Motivation: 现有AGIQA多依赖文本描述与图像嵌入的多等级相似度聚合。但实际相似度分布常呈多峰：同一图像可能同时接近“excellent”和“poor”，偏离“good”。这种“语义漂移”源于文本嵌入与其语义标签不一致，破坏了共享空间学习的可靠性，需一种更稳健、可解释的分级质量建模方式。

Method: 借鉴心理测量学的分级反应模型（GRM），将图像质量视为“图像能力”对不同“难度阈值”的通过概率。设计两分支质量分级模块：能力分支估计图像能力，难度分支构造多级难度阈值。通过算术（线性递增）方式生成并约束难度，保证阈值单调，诱导质量分布单峰且可解释。该Arithmetic GRM based Quality Grading（AGQG）作为即插即用模块嵌入多种AGIQA框架。

Result: 在多种最先进AGIQA框架上集成AGQG后均获得一致性能提升；同时在自然图像与屏幕内容图像质量评估任务上表现出良好的跨域泛化。

Conclusion: 以心理测量学为基础的算术GRM分级建模能有效抑制由文本-图像相似度多峰带来的语义漂移，提供单峰、可解释的质量估计；作为可插拔模块，AGQG提升SOTA并具备跨场景泛化潜力，值得作为未来IQA模型的关键组件。

Abstract: Existing AGIQA models typically estimate image quality by measuring and aggregating the similarities between image embeddings and text embeddings derived from multi-grade quality descriptions. Although effective, we observe that such similarity distributions across grades usually exhibit multimodal patterns. For instance, an image embedding may show high similarity to both "excellent" and "poor" grade descriptions while deviating from the "good" one. We refer to this phenomenon as "semantic drift", where semantic inconsistencies between text embeddings and their intended descriptions undermine the reliability of text-image shared-space learning. To mitigate this issue, we draw inspiration from psychometrics and propose an improved Graded Response Model (GRM) for AGIQA. The GRM is a classical assessment model that categorizes a subject's ability across grades using test items with various difficulty levels. This paradigm aligns remarkably well with human quality rating, where image quality can be interpreted as an image's ability to meet various quality grades. Building on this philosophy, we design a two-branch quality grading module: one branch estimates image ability while the other constructs multiple difficulty levels. To ensure monotonicity in difficulty levels, we further model difficulty generation in an arithmetic manner, which inherently enforces a unimodal and interpretable quality distribution. Our Arithmetic GRM based Quality Grading (AGQG) module enjoys a plug-and-play advantage, consistently improving performance when integrated into various state-of-the-art AGIQA frameworks. Moreover, it also generalizes effectively to both natural and screen content image quality assessment, revealing its potential as a key component in future IQA models.

</details>


### [79] [Parallel Diffusion Solver via Residual Dirichlet Policy Optimization](https://arxiv.org/abs/2512.22796)
*Ruoyu Wang,Ziyu Li,Beier Zhu,Liangyu Yuan,Hanwang Zhang,Xun Yang,Xiaojun Chang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出EPD-Solver：在每一步并行评估多条梯度方向以降低扩散模型采样的截断误差，在低步数下保持高画质且可并行加速；并通过蒸馏与参数高效的RL细化；可作为插件增强现有ODE采样器。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样需要多步顺序去噪，现有利用高阶/显式ODE求解器在低步数下会因高曲率段无法准确跟踪而产生累计截断误差，导致画质显著下降；需要一种既低延迟又能更精确逼近轨迹积分解的方法。

Method: 提出Ensemble Parallel Direction（EPD）求解器：在每个时间步对多个独立噪声/方向并行计算网络梯度，利用向量值函数的中值定理在低维流形上的几何约束，组合这些方向以更好逼近积分；两阶段优化：1) 蒸馏少量可学习参数以拟合教师轨迹；2) 将求解器参数化为Dirichlet随机策略，进行参数高效的强化学习微调，仅在求解器低维空间内优化，缓解reward hacking。另提供EPD-Plugin，可无缝嵌入现有ODE采样器。

Result: 在低延迟（少步数）采样下显著降低截断误差，生成质量更高；并行多梯度不增加串行时延；RL微调在复杂T2I任务上进一步提升指标与感知质量，优于传统微调骨干网络的方法；可泛化增强多种现有ODE采样器。

Conclusion: 通过并行多方向梯度与几何动机的ODE求解器设计，EPD在不牺牲延迟的前提下提升低步数扩散采样质量；结合蒸馏与参数高效RL细化，方法通用、可插拔且鲁棒，适用于T2I等复杂生成场景。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.

</details>


### [80] [VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM](https://arxiv.org/abs/2512.22799)
*Jingchao Wang,Kaiwen Zhou,Zhijian Wu,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: 提出VPTracker：基于多模态大语言模型的全局视觉-语言跟踪框架，通过位置感知视觉提示在全图搜索中抑制干扰并增强稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言跟踪多采用局部搜索，易在视角变化、遮挡、快速运动时丢失目标且产生漂移；全局搜索虽更稳健，但会引入与目标相似的干扰，需要一种既能全局推理又能抑制干扰的方法。

Method: 以MLLM作为推理核心，进行全图全局定位；引入位置感知的视觉提示（location-aware visual prompting），依据上一帧目标位置构建区域级提示，优先在该区域内进行识别，必要时再进行全局推理，从而在保留全局能力的同时抑制干扰。

Result: 在多种具有挑战性的场景下（视角变化、遮挡、快速运动、相似干扰物）显著提升跟踪稳定性与目标消歧能力；整体性能优于现有方法。

Conclusion: 将MLLM引入全局视觉-语言跟踪并结合位置感知提示，可兼顾全局稳健性与抗干扰性，开启MLLM在视觉跟踪中的新方向；代码已开源。

Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.

</details>


### [81] [Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation](https://arxiv.org/abs/2512.22800)
*Bin Liu,Wenyan Tian,Huangxin Fu,Zizheng Li,Zhifen He,Bo Li*

Main category: cs.CV

TL;DR: 提出一种结合3D高斯与tri-plane表示的高效三维重建方法，在稀疏切片下仍能保持结构连续与语义一致，并显著提升重建效率与质量。


<details>
  <summary>Details</summary>
Motivation: 传统三维重建在稀疏切片条件下易出现结构不连贯、细节丢失且计算开销大，难以满足临床精度与效率需求，亟需兼顾效率与解剖一致性的重建方案。

Method: 将3D Gaussian表示（高效渲染与几何表达）与tri-plane表示（在低采样下的几何与语义约束）融合：利用高斯点云进行连续体表达与快速渲染，引入tri-plane作为全局/局部特征先验与正则，增强跨切片的结构连续性与语义稳定性；在US、MRI等多模态数据上训练与评估，针对稀疏切片优化。

Result: 在超声与MRI等多模态数据上，在稀疏数据条件下生成高质量、解剖一致、语义稳定的三维结果，并显著提升重建效率（推测为较传统方法更快，具体数值未给出）。

Conclusion: 融合3D高斯与tri-plane的重建框架在稀疏切片下兼顾效率与精度，为医疗影像三维可视化与临床分析提供了高效可靠的新途径。

Abstract: 3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.

</details>


### [82] [Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image](https://arxiv.org/abs/2512.22801)
*Po-Chih Wu*

Main category: cs.CV

TL;DR: 论文评估开放词汇目标检测在低质量图像下的鲁棒性，并发布一个仿真低质图像数据集；结果显示轻度退化影响小、重度退化性能陡降，OWLv2最稳健，OWL-ViT、GroundingDINO、Detic降幅明显。


<details>
  <summary>Details</summary>
Motivation: 开放词汇检测应能像人类一样在开放世界识别目标，但真实场景常伴随噪声、模糊、压缩等低质成像。现有工作多在高质数据上评测，缺乏系统性低质量场景的基准与分析，因此需要构建数据集并量化各模型鲁棒性差异。

Method: 构建一个模拟真实世界低质图像的新数据集，涵盖多种退化类型与强度等级；在该数据集上对主流开放词汇检测模型（如OWLv2、OWL-ViT、GroundingDINO、Detic）进行系统评测，比较不同退化强度下的mAP变化。

Result: 在低强度退化下，各模型mAP基本不降；在高强度退化下，所有模型性能显著下滑。OWLv2在各退化类型与强度上更稳健，OWL-ViT、GroundingDINO、Detic在高强度退化下跌幅较大。

Conclusion: 开放词汇检测对重度图像退化仍不鲁棒，OWLv2相对最强。所提出数据集与代码将公开，为后续鲁棒性研究与方法改进提供基准。

Abstract: Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.

</details>


### [83] [EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation](https://arxiv.org/abs/2512.22808)
*Libo Zhang,Zekun Li,Tianyu Li,Zeyu Cao,Rui Xu,Xiaoxiao Long,Wenjia Wang,Jingbo Wang,Yuan Liu,Wenping Wang,Daquan Zhou,Taku Komura,Zhiyang Dou*

Main category: cs.CV

TL;DR: 提出EgoReAct，一个严格因果、实时、3D对齐的人体反应生成框架，并构建空间对齐的自摄（egocentric）视频-反应数据集HRD；通过VQ-VAE压缩动作潜空间与GPT式自回归生成，结合度量深度与头部动态实现更强空间落地，显著提升真实感、空间一致性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有自摄视频到人体反应生成存在两大痛点：1）数据不足且视频与动作空间不对齐（如动态反应却配固定机位视频，导致学习偏差）；2）模型需同时满足严格因果（仅用过去帧）与精确3D空间对齐，现有方法难兼得。

Method: 1）构建HRD：采集并校准自摄视频与对应反应动作，确保3D空间对齐。2）模型EgoReAct：先用VQ-VAE将反应动作离散化为紧致潜码本；再以GPT式自回归Transformer从视觉输入预测动作code序列，实现严格因果、实时生成。3）在生成过程中显式引入3D动态特征（度量深度、头部动态）以增强空间对齐/落地。

Result: 在大量实验中，相比现有方法，EgoReAct在真实感、空间一致性与生成效率上显著更优，同时保持严格因果。

Conclusion: HRD解决了训练数据的空间错配与匮乏，EgoReAct通过VQ-VAE+GPT并融合3D动态特征，实现实时、严格因果、3D对齐的人体反应生成。代码、模型与数据将在论文接收后开放。

Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.

</details>


### [84] [Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild](https://arxiv.org/abs/2512.22819)
*Hualie Jiang,Ziyang Song,Zhiqiang Lou,Rui Xu,Minglang Tan*

Main category: cs.CV

TL;DR: 提出DA360，将Depth Anything V2适配到全景（360°）深度估计：通过从ViT主干学习shift参数把尺度与偏移不变的输出变为仅尺度不变，并在DPT解码器中用环形填充去接缝；在室内/新建户外数据集Metropolis上均显著优于基线与SOTA（相对误差室内>50%、户外>10%降低，相比PanDA约30%提升）。


<details>
  <summary>Details</summary>
Motivation: 全景深度估计可完整表征环境结构，利于机器人与AR/VR。但相对透视图像，开域零样本泛化差，数据稀缺；因此希望把透视域强大的深度模型能力迁移到全景域并处理球面连续性与接缝问题。

Method: 在Depth Anything V2基础上提出DA360：1）从ViT主干学习一个shift参数，将原本尺度与偏移不变的深度输出转换为仅尺度不变，从而直接生成尺度一致、可重建良好3D点云的估计；2）在DPT解码器中引入环形（circular）填充，消除全景拼接接缝并保持球面连续性；3）在标准室内基准与新建的户外Metropolis数据集上进行零样本评测。

Result: 相较基础模型，在室内与户外基准上相对深度误差分别降低>50%与>10%。相较强健的全景方法PanDA，在三个测试集上相对误差约降低30%，在零样本全景深度估计上达成新的SOTA。

Conclusion: 通过学习shift与环形填充的适配，DA360有效将透视域深度能力迁移至全景域，生成空间连贯、可直接用于3D重建的深度，并在室内外数据上取得显著且稳定的SOTA性能。

Abstract: Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model's scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.

</details>


### [85] [KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution](https://arxiv.org/abs/2512.22822)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 提出一种基于Kolmogorov–Arnold定理的可解释神经算子KANO，用B样条加性结构拟合退化的频谱曲线，提升单幅超分辨的物理可解释性，并系统比较MLP与KAN在复杂序列拟合上的差异。


<details>
  <summary>Details</summary>
Motivation: 单图超分辨退化过程高度非线性、物理交互复杂且充满不确定性，现有可解释方法仍依赖黑盒深网刻画潜变量，退化机理不可控、难以物理解释。需要一种结构透明、参数可解释的表示来直接拟合并控制退化频谱特性。

Method: 受KAT启发，构建Kolmogorov–Arnold Neural Operator（KANO）：以有限个B样条基函数的加性组合近似连续频谱曲线；在分段定义的区间内学习样条形状参数，以捕捉局部线性趋势与非线性拐点的峰谷结构。将KANO嵌入SR退化建模与拟合流程，并从理论与实验上对比MLP与KAN（含KANO/KAN结构）在复杂序列拟合的表现。

Result: KANO能准确拟合退化频谱的关键特征，使SR过程可解释、可控；在自然图像、航拍与遥感数据上验证有效性。比较实验揭示MLP与KAN在表达能力、稳定性与对非线性结构（峰谷/拐点）刻画上的差异与互补。

Conclusion: 基于KAT的KANO为SR退化建模提供了透明、物理可解释的算子框架，既提升拟合能力又增强可控性；对MLP与KAN的系统比较为今后构建可解释的SR模型提供了方法学指导。

Abstract: The highly nonlinear degradation process, complex physical interactions, and various sources of uncertainty render single-image Super-resolution (SR) a particularly challenging task. Existing interpretable SR approaches, whether based on prior learning or deep unfolding optimization frameworks, typically rely on black-box deep networks to model latent variables, which leaves the degradation process largely unknown and uncontrollable. Inspired by the Kolmogorov-Arnold theorem (KAT), we for the first time propose a novel interpretable operator, termed Kolmogorov-Arnold Neural Operator (KANO), with the application to image SR. KANO provides a transparent and structured representation of the latent degradation fitting process. Specifically, we employ an additive structure composed of a finite number of B-spline functions to approximate continuous spectral curves in a piecewise fashion. By learning and optimizing the shape parameters of these spline functions within defined intervals, our KANO accurately captures key spectral characteristics, such as local linear trends and the peak-valley structures at nonlinear inflection points, thereby endowing SR results with physical interpretability. Furthermore, through theoretical modeling and experimental evaluations across natural images, aerial photographs, and satellite remote sensing data, we systematically compare multilayer perceptrons (MLPs) and Kolmogorov-Arnold networks (KANs) in handling complex sequence fitting tasks. This comparative study elucidates the respective advantages and limitations of these models in characterizing intricate degradation mechanisms, offering valuable insights for the development of interpretable SR techniques.

</details>


### [86] [3D Scene Change Modeling With Consistent Multi-View Aggregation](https://arxiv.org/abs/2512.22830)
*Zirui Zhou,Junfeng Ni,Shujie Zhang,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出SCaR-3D：利用稠视角“变前”与稀视角“变后”图像，基于SDF差分+多视角投票裁剪，实现对象级3D变化检测与持续重建；并发布可控合成数据集CCS3D；实验显示精度与效率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D变化检测结果常空间不一致，且难以显式区分变化前/后的场景状态，影响监控、探测与持续重建的可靠性与可用性。

Method: 1) 签距函数(SDF)驱动的2D差分，生成候选变化；2) 跨视角聚合：投票与剪枝，借助3DGS一致性稳健区分前/后状态；3) 持续重建策略：仅选择性更新动态区域，保持未变区域；4) 构建CCS3D合成数据集，支持多种3D变化类型与可控评测。

Result: 在多项实验中实现高精度与高效率的对象级变化检测与持续重建，优于现有方法。

Conclusion: SCaR-3D能稳健分离变前/变后状态并高效更新动态区域；CCS3D为可控评测提供基准，整体方法在准确性与效率上均领先。

Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.

</details>


### [87] [A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences](https://arxiv.org/abs/2512.22833)
*Zhenbao Yu,Shirong Ye,Ronghe Jin,Shunkun Liang,Zibin Liu,Huiyun Zhang,Banglei Guan*

Main category: cs.CV

TL;DR: 利用已知重力方向的两幅图像中2个仿射对应，联合估计3自由度相对位姿与单一未知焦距；通过从AC建立约束并化简为仅含焦距与相对转角的四个方程，使用多项式特征值法求解，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实际多传感器系统（车载、手机、无人机）中IMU可提供垂直方向，减少相机相对位姿自由度；同时在弱纹理或少点场景中可从仿射对应获得更强局部几何信息。现有最小解法对未知焦距及位姿联合估计的效率/稳健性不足，或需更多对应点；因此需一个仅用极少对应（2个AC）即可在已知垂直方向下稳健求解的高效最小解算器。

Method: - 设定：双目、内参已知但焦距未知，IMU给出垂直方向，使相对位姿由5DOF降至3DOF（平移2+绕垂直轴旋转1）。- 从两对仿射对应构建约束，将本征几何与局部仿射传播关系结合。- 利用“存在非平凡解”的条件，将方程组化简为四个仅含焦距f与相对旋转角θ的多项式方程。- 采用多项式特征值（polynomial eigenvalue）方法联立求解f与θ，再恢复平移。

Result: 在合成与真实数据上评测：在精度、鲁棒性与稳定性方面均优于现有最先进求解器；在噪声与不同基线/视角条件下依旧表现更好。

Conclusion: 结合IMU提供的垂直方向与两条仿射对应，可以最小化地联合求解未知焦距与3DOF相对位姿；所提多项式特征值求解器在效率与精度上优于SOTA，适用于实际多传感器视觉应用。

Abstract: In this paper, we aim to estimate the relative pose and focal length between two views with known intrinsic parameters except for an unknown focal length from two affine correspondences (ACs). Cameras are commonly used in combination with inertial measurement units (IMUs) in applications such as self-driving cars, smartphones, and unmanned aerial vehicles. The vertical direction of camera views can be obtained by IMU measurements. The relative pose between two cameras is reduced from 5DOF to 3DOF. We propose a new solver to estimate the 3DOF relative pose and focal length. First, we establish constraint equations from two affine correspondences when the vertical direction is known. Then, based on the properties of the equation system with nontrivial solutions, four equations can be derived. These four equations only involve two parameters: the focal length and the relative rotation angle. Finally, the polynomial eigenvalue method is utilized to solve the problem of focal length and relative rotation angle. The proposed solver is evaluated using synthetic and real-world datasets. The results show that our solver performs better than the existing state-of-the-art solvers.

</details>


### [88] [ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning](https://arxiv.org/abs/2512.22854)
*Bangya Liu,Xinyu Gong,Zelin Zhao,Ziyang Song,Yulei Lu,Suhui Wu,Jun Zhang,Suman Banerjee,Hao Zhang*

Main category: cs.CV

TL;DR: ByteLoom 是一个基于 DiT 的 HOI 视频生成框架，结合简化的人体条件与3D物体输入，通过RCM-cache实现跨视角几何一致、可控6DoF物体运动，并用渐进式课程学习缓解手部网格依赖与数据稀缺，生成保持身份与物体几何的平滑操控视频。


<details>
  <summary>Details</summary>
Motivation: 现有HOI视频生成方法难以在多视角下保持物体几何一致，且对精细手部网格标注依赖高；同时，HOI数据集稀缺限制了模型泛化与交互遮挡建模能力。

Method: 提出ByteLoom：1) 采用Diffusion Transformer作为生成主干，输入简化的人体条件与3D物体；2) 设计RCM-cache机制，用相对坐标图RCM作为统一表示，在时序中缓存并传播，精确控制物体6-DoF与跨视角几何一致性；3) 课程学习式训练，逐步增强从合成/现有数据到真实HOI的能力，降低对手部网格的依赖。

Result: 在多项实验中，方法在保持人物身份、动作平滑性方面表现优异，同时显著提升物体多视角几何一致与可操控性，减少遮挡处理对手部网格标注的需求。

Conclusion: RCM-cache与课程学习使DiT框架能以更少手部标注与更强多视角一致性生成真实HOI视频，兼顾可控6DoF物体操控与稳定的人物外观与运动。

Abstract: Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.

</details>


### [89] [MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments](https://arxiv.org/abs/2512.22867)
*Zhuonan Liu,Xinyu Zhang,Zishuo Wang,Tomohito Kawabata,Xuesu Xiao,Ling Xiao*

Main category: cs.CV

TL;DR: MUSON 是一个面向社会化导航的多模态、短时域数据集，提供结构化五步链式推理标注与均衡的离散动作空间，用于提升在动态行人和物理约束下的安全、可解释决策；在其基准上，小型多模态模型 Qwen2.5‑VL‑3B 达到最高决策准确率 0.8625。


<details>
  <summary>Details</summary>
Motivation: 现有社会化导航数据集缺乏显式推理监督，且动作分布长尾，导致模型难以学习安全关键行为并给出可解释决策。

Method: 构建 MUSON 数据集：跨多样校园室内外场景采集短时域导航片段；为每条样本提供“五步链式推理”标注（感知→预测→推理→动作→解释），显式刻画静态物理约束；设计理性均衡的离散动作空间；并与 SNEI 对比，提供一致的推理、动作与解释标注；用多种小型视觉语言模型进行基准评测。

Result: 在 MUSON 上，多个 SOTA 小型 VLM 得到评测，其中 Qwen2.5‑VL‑3B 取得最高决策准确率 0.8625，显示 MUSON 对社会化合规导航的有效训练与评测价值。

Conclusion: MUSON 通过结构化推理监督与均衡动作空间缓解了以往数据集的长尾与缺乏可解释性问题，成为可复用的社会化导航基准，并促进安全、可解释的短时域导航决策研究与模型发展。

Abstract: Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON

</details>


### [90] [Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs](https://arxiv.org/abs/2512.22872)
*Ziyu Zhou,Haozhe Luo,Mohammad Reza Hosseinzadeh Taher,Jiaxuan Pang,Xiaowei Ding,Michael B. Gotway,Jianming Liang*

Main category: cs.CV

TL;DR: 提出Lamps：基于胸部X光的多视角解剖自监督预训练，利用解剖的一致性、连贯性与层级性，显著提升跨数据集鲁棒与迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像SSL多受NLP/视觉范式启发，忽视医学影像的真正“基座”是人体解剖结构的稳定性与层级性，导致对解剖表达学习不足、泛化与临床可用性受限。

Method: 构建名为Lamps的自监督框架，在大规模胸部X光上预训练；将解剖的一致性（跨视图/投影/个体的稳定特征）、连贯性（区域与整体的关联）与层级性（器官—区域—子结构）转化为监督信号，进行多视角对比/重建/层级约束联合优化。

Result: 在10个数据集上经微调与“涌现属性”分析评估，相较10个基线取得更强鲁棒性、迁移性与临床潜力。

Conclusion: 从解剖多视角学习能为医学影像奠定更贴合领域本质的基础表征，Lamps展示了构建解剖对齐的医学基础模型的有效路径。

Abstract: Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

</details>


### [91] [Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples](https://arxiv.org/abs/2512.22874)
*Weiwei Li,Junzhuo Liu,Yuanyuan Ren,Yuchen Zheng,Yahao Liu,Wen Li*

Main category: cs.CV

TL;DR: 提出一种数据导向的管线，通过识别→中和→消除→更新四步，缓解深度学习中的虚假相关；在图像与NLP基准上显著提升最差组准确率（>20% 相比ERM）。


<details>
  <summary>Details</summary>
Motivation: 深度模型常学到与类别标签偶然相关但与任务无关的虚假特征，现有方法依赖显式标注或基于简化假设筛除偏置，难以应对真实数据中复杂多变的虚假相关，导致泛化失败，尤其在最弱群体上表现不佳。

Method: 数据导向的流程：1) 识别：利用受虚假特征影响的样本在表征空间更分散的现象，检测潜在虚假特征及其样本；2) 中和：基于简单分组策略构造“偏置不变”表示，抵消虚假特征影响；3) 消除：学习特征变换，使模型表征对齐该偏置不变表示，从而去除虚假特征；4) 更新：将学得的特征变换集成到分类器，得到去偏模型。

Result: 在图像与NLP去偏基准上，相比ERM，最差组准确率提升超过20%，显示对群体鲁棒性显著增强；开源代码与模型检查点已发布。

Conclusion: 无需显式标注或强假设，依托表征分布差异来发现并消除虚假特征，形成可操作的去偏管线，显著提升最差组性能与整体鲁棒性。

Abstract: Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfactory performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spurious features based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spurious features by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing .

</details>


### [92] [M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models](https://arxiv.org/abs/2512.22877)
*Ju-Hsuan Weng,Jia-Wei Liao,Cheng-Fu Chou,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出M-ErasureBench多模态概念擦除评测与推理时鲁棒增强方法IRECE；现有方法对文本擦除有效，但对学习嵌入与反演潜变量易失效；IRECE通过跨注意力定位并在去噪中扰动相关潜变量，显著降低在白盒场景的概念重现率，同时保持画质。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能生成有害或受版权保护的内容，现有“擦除”方法主要针对文本提示，忽视了图像编辑、个性化等场景中的其他输入模态（如学习嵌入、潜变量反演），这些成为攻击面，导致被擦除概念重新出现。需要一个覆盖多模态的系统性评测与能在这些模态下仍然有效的防御。

Method: 1) 构建M-ErasureBench：在三种输入模态（文本、学习嵌入、反演潜变量）上评测擦除方法，并区分对白盒/黑盒访问以形成五种评测场景；以概念重现率（CRR）等指标量化。
2) 提出IRECE：推理时即插即用模块，利用跨注意力定位目标概念的相关位置/通道，并在扩散去噪过程中对关联潜变量施加有针对性的扰动，从而抑制概念重现，同时保持视觉质量。

Result: 基准显示：现有方法对文本提示擦除表现强，但在学习嵌入与潜变量反演下大多失败，白盒场景CRR>90%。引入IRECE后，在最困难的白盒潜变量反演场景CRR最多降低约40%，且图像质量基本不受损。

Conclusion: M-ErasureBench首次系统评测超越文本模态的概念擦除鲁棒性，揭示现有方法对非文本输入的显著脆弱性；IRECE作为推理时的可插拔增强，能恢复多模态鲁棒性并兼顾画质，为构建更可靠的防护型生成模型提供了实用工具。

Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.

</details>


### [93] [SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation](https://arxiv.org/abs/2512.22878)
*Hasan Faraz Khan,Noor Fatima,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出SwinTF3D：一种轻量级视觉-语言融合的文本引导3D医学分割模型，在BTCV上以更低计算开销取得与主流方法相当的Dice/IoU，并具备良好泛化与可交互性。


<details>
  <summary>Details</summary>
Motivation: 现有3D器官分割多依赖大规模标注和纯视觉学习，缺乏语义理解，难以适应用户可变的文本描述和新域/新任务；需要一种能将自然语言语义与体数据对齐、支持交互式目标指定且高效的框架。

Method: 构建SwinTF3D：以Transformer式体素视觉编码器提取体数据特征；使用紧凑文本编码器获得语义嵌入；通过高效的多模态融合机制对齐语言提示与空间体素特征，实现文本引导的3D分割；整体追求轻量化与低计算开销。

Result: 在BTCV数据集上，多个器官的Dice与IoU达到与常规Transformer分割网络相竞争的水平；在未见数据上保持较好泛化；计算效率显著提升。

Conclusion: SwinTF3D将视觉感知与语言理解统一，为交互式、文本驱动的3D医学分割提供可解释且资源高效的范式，适用于更自适应的临床影像场景。

Abstract: The recent integration of artificial intelligence into medical imaging has driven remarkable advances in automated organ segmentation. However, most existing 3D segmentation frameworks rely exclusively on visual learning from large annotated datasets restricting their adaptability to new domains and clinical tasks. The lack of semantic understanding in these models makes them ineffective in addressing flexible, user-defined segmentation objectives. To overcome these limitations, we propose SwinTF3D, a lightweight multimodal fusion approach that unifies visual and linguistic representations for text-guided 3D medical image segmentation. The model employs a transformer-based visual encoder to extract volumetric features and integrates them with a compact text encoder via an efficient fusion mechanism. This design allows the system to understand natural-language prompts and correctly align semantic cues with their corresponding spatial structures in medical volumes, while producing accurate, context-aware segmentation results with low computational overhead. Extensive experiments on the BTCV dataset demonstrate that SwinTF3D achieves competitive Dice and IoU scores across multiple organs, despite its compact architecture. The model generalizes well to unseen data and offers significant efficiency gains compared to conventional transformer-based segmentation networks. Bridging visual perception with linguistic understanding, SwinTF3D establishes a practical and interpretable paradigm for interactive, text-driven 3D medical image segmentation, opening perspectives for more adaptive and resource-efficient solutions in clinical imaging.

</details>


### [94] [Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance](https://arxiv.org/abs/2512.22881)
*Haosen Li,Wenshuo Chen,Shaofeng Liang,Lei Wang,Haozhe Jia,Yutao Yue*

Main category: cs.CV

TL;DR: 提出Guided Path Sampling（GPS）用于迭代式细化扩散模型，替代CFG的外推为流形内插，保证路径稳定并显著提升画质与提示对齐。


<details>
  <summary>Details</summary>
Motivation: 现有迭代细化（去噪-反演循环）在结合无分类器引导（CFG）时不稳定，因为CFG外推使采样路径偏离数据流形，误差被放大，削弱细化效果。需要一种在迭代细化中既能注入语义又保持路径稳定的方法。

Method: 提出GPS：将CFG的外推改为受约束的流形内插，理论上保证采样路径留在数据流形上；给出误差从无界放大转为有界的稳定性证明；设计最优调度策略，随时间步动态调节引导强度，使语义注入与从粗到细的生成过程对齐；在SDXL和Hunyuan-DiT等骨干上实现并评测。

Result: 与现有方法相比，在感知质量与复杂提示遵循上均有提升：SDXL上ImageReward 0.79、HPS v2 0.2995；在GenEval上整体语义对齐准确率57.45%。

Conclusion: 迭代细化要有效，路径稳定性是前提。GPS以流形内插和动态引导调度提供稳定且高效的细化框架，理论与实证均显示其优于基于CFG的方案。

Abstract: Iterative refinement methods based on a denoising-inversion cycle are powerful tools for enhancing the quality and control of diffusion models. However, their effectiveness is critically limited when combined with standard Classifier-Free Guidance (CFG). We identify a fundamental limitation: CFG's extrapolative nature systematically pushes the sampling path off the data manifold, causing the approximation error to diverge and undermining the refinement process. To address this, we propose Guided Path Sampling (GPS), a new paradigm for iterative refinement. GPS replaces unstable extrapolation with a principled, manifold-constrained interpolation, ensuring the sampling path remains on the data manifold. We theoretically prove that this correction transforms the error series from unbounded amplification to strictly bounded, guaranteeing stability. Furthermore, we devise an optimal scheduling strategy that dynamically adjusts guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process. Extensive experiments on modern backbones like SDXL and Hunyuan-DiT show that GPS outperforms existing methods in both perceptual quality and complex prompt adherence. For instance, GPS achieves a superior ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, while improving overall semantic alignment accuracy on GenEval to 57.45%. Our work establishes that path stability is a prerequisite for effective iterative refinement, and GPS provides a robust framework to achieve it.

</details>


### [95] [Hash Grid Feature Pruning](https://arxiv.org/abs/2512.22882)
*Yangzhi Ma,Bojun Liu,Jie Li,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: 提出一种基于高斯点云分布的哈希网格特征裁剪方法，仅编码有效特征，在不降质的前提下降低存储与码率，CTC下平均码率降低约8%。


<details>
  <summary>Details</summary>
Motivation: 哈希网格在高斯splatting的隐式场学习中常用于熵模型或帧间预测，但3D高斯分布稀疏且不均匀，造成大量网格特征从未被访问，导致冗余存储与传输开销，影响率失真表现。

Method: 根据输入高斯splat的空间坐标，识别哈希网格中从未被访问或无效的索引/单元，对多层哈希表的特征向量进行稀疏性检测与裁剪，仅保留有效特征参与编码与传输；推断时通过映射表恢复索引以保持功能等价。

Result: 在标准化组织定义的CTC条件下，相比基线方法平均码率降低约8%，且模型性能（失真/质量）不受影响，整体率失真性能提升。

Conclusion: 利用数据分布感知的特征裁剪能有效减少哈希网格冗余并提升压缩效率，且不牺牲精度，为高斯splatting场景下的隐式表征与编码提供了简单高效的改进。

Abstract: Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.

</details>


### [96] [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/abs/2512.22905)
*Kai Liu,Jungang Li,Yuchong Sun,Shengqiong Wu,Jianzhang Gao,Daoan Zhang,Wei Zhang,Sheng Jin,Sicheng Yu,Geng Zhan,Jiayi Ji,Fan Zhou,Liang Zheng,Shuicheng Yan,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: JavisGPT 提出首个统一的音频-视频（JAV）理解与生成的多模态大模型，采用编码器-LLM-解码器架构与同步融合机制，并通过三阶段训练与20万+指令数据支撑，显著提升时序同步与复杂场景表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型多聚焦于视觉或语音单一模态，缺乏对音视频联合理解与生成的统一框架，尤其在需要时序一致性与跨模态同步的复杂任务上表现不足；同时缺少大规模高质量的音视频-文本指令数据支撑此类能力。

Method: 采用简洁的 encoder-LLM-decoder 统一架构：引入 SyncFusion 模块进行时空维度的音视频对齐与融合；设计具备同步感知的可学习查询，桥接到预训练的 JAV-DiT 生成器，实现从指令到音视频生成的端到端联动。训练上采用三阶段：多模态预训练→音视频微调→大规模指令微调；并构建 JavisInst-Omni（20万+ GPT-4o 标注的音视频-文本对话）。

Result: 在多项音视频理解与生成基准上取得优于现有 MLLM 的效果，尤其在复杂与需要严格时间同步的场景中表现突出。

Conclusion: 统一的JAV理解与生成框架结合同步感知融合与分阶段训练，配合大规模高质指令数据，可显著提升时序一致与复杂场景下的多模态能力。

Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.

</details>


### [97] [ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939)
*Qihang Peng,Xuesong Chen,Chenye Yang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: ColaVLA提出一种将VLM推理从文本域迁移到统一潜在空间，并配合分层并行轨迹解码器的E2E自动驾驶框架，实现高效、因果一致的轨迹生成，在nuScenes上达SOTA并兼顾效率与稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM驱动的E2E驾驶存在三大痛点：文本离散推理与连续控制不匹配；自回归链式思维解码延迟高；规划器因果性不足或并行性差，难以实时部署。需要一种既保留VLM泛化与可解释性，又能高效低延迟地产生安全轨迹的方法。

Method: 提出ColaVLA框架：1) Cognitive Latent Reasoner（认知潜在推理器），通过自车自适应选择，将场景理解压缩为紧凑的、决策导向的“元动作”嵌入，仅需两次VLM前向；2) Hierarchical Parallel Planner（分层并行规划器），在单次前向中生成多尺度、因果一致的轨迹，采用层级并行解码以替代自回归文本推理。整体将跨模态推理转移到统一潜在空间并与动作解码紧耦合。

Result: 在nuScenes基准上，ColaVLA在开环与闭环评测中均取得SOTA，同时展现更低延迟与更强鲁棒性（相较现有VLM规划器减少推理次数，提高实时性与稳定性）。

Conclusion: 将VLM的推理从文本转为潜在表示并配合层级并行规划，可兼顾VLM的泛化/可解释性与实时、因果一致的轨迹生成，为E2E自动驾驶提供高效可靠的新范式。

Abstract: Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.

</details>


### [98] [Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects](https://arxiv.org/abs/2512.22949)
*Zhicheng Zhao,Xuanang Fan,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出DRMNet，通过密度图先验自适应聚焦遥感图像中的高密小目标区域，结合密度引导的局部-全局交互与频域融合，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 高分遥感图像中小目标密集、相互遮挡严重、像素占比极小，现有检测器均匀分配算力，难以对密集区域自适应加强，导致特征学习无效与注意力开销高。

Method: 1) 密度生成分支DGB：学习目标分布，输出可量化密度图作为显式空间先验；2) 密集区域聚焦模块DAFM：用密度图定位高密区域，替代全局注意的高开销，在被选区内进行高效的局部-全局交互；3) 双滤波融合模块DFFM：对多尺度特征做离散余弦变换，分解为高/低频，基于密度图进行跨注意力融合，强化互补、抑制背景干扰。

Result: 在AI-TOD与DTOD数据集上超越SOTA，尤其在目标高度密集与严重遮挡场景取得更大优势。

Conclusion: 将密度图作为显式先验，驱动“哪里看”和“怎么看”：先用密度指引关注区域，再以频域与跨注意力增强多尺度互补，既提升准确率又降低注意力计算负担，适合处理密集小目标检测。

Abstract: High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.

</details>


### [99] [CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision](https://arxiv.org/abs/2512.22969)
*Behnam Raoufi,Hossein Sharify,Mohamad Mahdee Ramezanee,Khosrow Hajsadeghi,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: 提出CLIP-Joint-Detect：在目标检测中加入CLIP式对比视觉-语言监督，与常规检测损失联合端到端训练，在VOC与COCO上对Faster R-CNN与YOLOv11均带来稳定显著提升且保持实时性。


<details>
  <summary>Details</summary>
Motivation: 传统检测器使用交叉熵分类，易受类别不平衡与标签噪声影响；希望借助对比学习与文本语义先验增强分类鲁棒性与判别性。

Method: 在任意检测器内新增轻量并行头，将区域/网格特征映射到CLIP嵌入空间；引入可学习的类别文本嵌入，与视觉特征通过InfoNCE对比损失对齐，并辅以辅助交叉熵；与标准检测损失（如回归、置信度等）联合端到端优化；适用于两阶段和单阶段结构。

Result: 在Pascal VOC 2007+2012上用Faster R-CNN、在MS COCO 2017上用YOLOv11验证，获得一致且显著的mAP提升，同时保持实时推理速度；消融显示联合优化与可学习文本嵌入是关键增益来源。

Conclusion: 将CLIP式对比视觉-语言监督与检测器联合训练能在封闭集合检测中稳健提升性能且不牺牲速度，方法简单、架构无关、易于集成。

Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.

</details>


### [100] [Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection](https://arxiv.org/abs/2512.22972)
*Runwei Guan,Jianan Liu,Shaofeng Liang,Fangqiang Ding,Shanliang Yao,Xiaokai Bai,Daizong Liu,Tao Huang,Guoqiang Mao,Hui Xiong*

Main category: cs.CV

TL;DR: 提出WRCFormer：将原始4D雷达cube与相机多视角特征融合的3D检测框架，用小波注意力FPN增强稀疏信号、几何引导的两阶段查询式渐进融合，K-Radar上SOTA，整体+2.4%，冻雨/雨夹雪场景+1.6%。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达价格低、全天候，但点云稀疏、语义弱；点云化带来信息丢失，直接用原始雷达cube计算量大。需要一种既保留雷达原始信息又可高效与相机互补的融合检测方法。

Method: 1) 将原始4D雷达cube解耦并构建多视角表示；2) 设计小波注意力模块作为小波FPN的基本单元，提升雷达与图像的多尺度表达；3) 提出几何引导的两阶段、查询式、模态无关渐进融合机制，高效整合两模态多视角特征；4) 用于3D目标检测。

Result: 在K-Radar基准上达到SOTA，整体场景mAP约提升2.4%，在冻雨/雨夹雪等恶劣天气场景提升约1.6%，展现更强鲁棒性。

Conclusion: 保留并高效利用原始雷达信息、结合相机互补性的WRCFormer能够显著提升3D检测性能，尤其在恶劣天气下更鲁棒。

Abstract: 4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.

</details>


### [101] [YOLO-IOD: Towards Real Time Incremental Object Detection](https://arxiv.org/abs/2512.22973)
*Shizhou Zhang,Xueqiang Lv,Yinghui Xing,Qirui Wu,Di Xu,Chen Zhao,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出YOLO-IOD，将增量学习融入YOLO实时检测；通过伪标注冲突感知、重要卷积核选择和跨阶段不对称蒸馏，缓解前景/背景混淆、参数干扰和蒸馏错配；并构建无泄漏的LoCo COCO基准，实验显示更优性能与更小遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有增量目标检测多基于Faster R-CNN或DETR，不适配实时YOLO；YOLO在增量场景中出现灾难性遗忘，其根因包括前景-背景混淆、参数相互干扰、跨阶段蒸馏对齐不良，缺少针对YOLO特性的系统性解决方案与现实无泄漏基准。

Method: 提出YOLO-IOD，基于预训练YOLO-World，采用分阶段、参数高效微调：1) 冲突感知伪标注精炼（CPR），利用伪标注置信和潜在未来类提示缓解前景/背景混淆；2) 基于重要度的卷积核选择（IKS），在当前阶段仅更新与当前任务最相关的卷积核，减少参数干扰；3) 跨阶段不对称知识蒸馏（CAKD），让学生特征同时经先前与当前教师检测头，实现对已知与新类的不对称蒸馏对齐。另提出LoCo COCO基准，避免跨阶段数据泄漏。

Result: 在常规与LoCo COCO基准上，相较现有方法实现更高mAP与更小遗忘（未给出具体数值），同时保持实时性与参数高效。

Conclusion: YOLO-IOD有效将增量学习引入YOLO实时检测，通过CPR、IKS、CAKD协同解决三类知识冲突，在更严格的无泄漏基准上验证了其优越性与稳定性。

Abstract: Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.

</details>


### [102] [RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance](https://arxiv.org/abs/2512.22974)
*Chunyuan Chen,Yunuo Cai,Shujuan Li,Weiyun Liang,Bin Wang,Jing Xu*

Main category: cs.CV

TL;DR: 提出ReamCamo：基于外延生成（out-painting）的统一伪装图像生成框架，通过布局控制与多模态条件提升伪装真实感与语义一致性，并引入前景-背景分布散度指标评估伪装质量。


<details>
  <summary>Details</summary>
Motivation: 现有伪装图像生成方法要么伪装弱、与真实差距大，要么背景杂乱且与前景语义不一致，导致用于COD训练的合成数据质量不足。

Method: 1) 统一的外延生成框架ReamCamo：对给定前景进行外扩生成背景；2) 显式布局控制，约束全局结构以提升前景与生成背景的语义一致性；3) 多模态文本-视觉条件：细粒度文本任务描述+面向纹理的背景检索共同引导生成，强化视觉保真与真实感；4) 提出前景-背景分布散度度量，用于量化伪装有效性。

Result: 在多组实验与可视化中，生成图像的伪装强度、语义一致性与视觉保真度均优于现有方法；所提指标能够有效区分不同方法的伪装质量。

Conclusion: ReamCamo通过布局控制与多模态条件提升CIG的真实感与可用性，并提供新的量化评估指标，为COD训练提供更高质量的数据。

Abstract: Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.

</details>


### [103] [PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects](https://arxiv.org/abs/2512.22979)
*Huiming Yang,Linglin Liao,Fei Ding,Sibo Wang,Zijian Zeng*

Main category: cs.CV

TL;DR: 提出PoseStreamer：面向高速运动与弱光场景的事件相机+RGB多模态6DoF位姿估计框架，借助历史姿态记忆、目标中心2D跟踪与沿光线几何滤波，并发布高速运动多模态数据集MoCapCube6D；在无需模板的设定下，对未见物体取得更高准确率与强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有6DoF位姿估计在高速/低光条件下因RGB运动模糊而退化；虽然事件相机具高时间分辨率，但现有方法在高速物体场景仍表现欠佳，缺乏面向这类极端动态场景的鲁棒多模态方案与标准数据集。

Method: 构建多模态框架PoseStreamer：1) 自适应姿态记忆队列（Adaptive Pose Memory Queue）利用历史朝向线索加强时序一致性；2) 以目标为中心的2D跟踪器提供强2D先验，提高3D中心召回；3) 沿相机光线的Ray Pose Filter进行几何约束与精炼；并引入新数据集MoCapCube6D用于快速运动基准。

Result: 在高速运动情形的广泛实验中，PoseStreamer精度优于现有方法；作为无模板方法，对未见运动物体也保持强泛化能力。

Conclusion: 结合事件相机的时间优势与多阶段时空约束，PoseStreamer在高速、弱光等极端动态场景下实现稳健且精确的6DoF位姿估计，并通过新数据集推动该方向的基准评测。

Abstract: Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.

</details>


### [104] [Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation](https://arxiv.org/abs/2512.22981)
*Linglin Liao,Qichuan Geng,Yu Liu*

Main category: cs.CV

TL;DR: 提出SSA框架，通过对称最优传输对齐与复合方向性引导，提升文本引导下医学图像分割对多类型文本（位置/描述/诊断）的理解，显著改善含空间关系约束的病灶分割并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导医学分割方法难以同时处理诊断与描述文本，难以将文本与图像区域精准对应；且多忽略位置约束，导致分割越界（如“左下肺”却覆盖双肺）。

Method: 1) 空间感知的对称对齐：引入对称最优传输（bi-directional OT）在文本多种表达（位置、描述、诊断）与图像区域间建立细粒度双向对应，增强相关性抑制无关性。2) 复合方向性引导：从文本显式构建区域级引导掩膜，将空间约束融入分割过程，实现定位与语义协同。

Result: 在公共基准上取得SOTA，尤其在含空间关系约束的病灶分割任务中显著提升定位精度与分割质量。

Conclusion: 通过双向细粒度跨模态对齐与显式空间引导，SSA有效缓解多文本类型融合与空间约束缺失的问题，显著提升文本引导医学分割的准确性与稳健性。

Abstract: Text-guided Medical Image Segmentation has shown considerable promise for medical image segmentation, with rich clinical text serving as an effective supplement for scarce data. However, current methods have two key bottlenecks. On one hand, they struggle to process diagnostic and descriptive texts simultaneously, making it difficult to identify lesions and establish associations with image regions. On the other hand, existing approaches focus on lesions description and fail to capture positional constraints, leading to critical deviations. Specifically, with the text "in the left lower lung", the segmentation results may incorrectly cover both sides of the lung. To address the limitations, we propose the Spatial-aware Symmetric Alignment (SSA) framework to enhance the capacity of referring hybrid medical texts consisting of locational, descriptive, and diagnostic information. Specifically, we propose symmetric optimal transport alignment mechanism to strengthen the associations between image regions and multiple relevant expressions, which establishes bi-directional fine-grained multimodal correspondences. In addition, we devise a composite directional guidance strategy that explicitly introduces spatial constraints in the text by constructing region-level guidance masks. Extensive experiments on public benchmarks demonstrate that SSA achieves state-of-the-art (SOTA) performance, particularly in accurately segmenting lesions characterized by spatial relational constraints.

</details>


### [105] [Reverse Personalization](https://arxiv.org/abs/2512.22984)
*Han-Wei Kung,Tuomas Varanka,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出一种“反向个性化”框架，用扩散反演与身份引导条件分支，实现无文本提示的人脸匿名化，同时可控保留/修改属性，兼顾去身份、属性保持与图像质量，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的身份移除依赖预训练模型对主体覆盖充分，或需为特定身份微调；且匿名化方法常缺乏对面部属性（如年龄、表情、姿态等）的可控性，难以在隐私保护与可用性之间取得平衡。

Method: 1) 分析扩散模型中的身份生成机制；2) 提出conditional diffusion inversion，直接在图像空间进行反演与编辑而无需文本提示；3) 设计identity-guided conditioning分支，使方法泛化到训练数据外的主体；4) 在该框架下实现属性可控的人脸匿名化（可调面部属性同时移除身份特征）。

Result: 在去身份、属性保持与图像质量三方面取得更优权衡，达到SOTA；可在未出现在预训练中的主体上泛化；代码与数据开源。

Conclusion: 反向个性化框架通过条件扩散反演与身份引导分支，实现无需文本提示的属性可控匿名化，兼顾隐私与视觉质量，并具有良好泛化性。

Abstract: Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .

</details>


### [106] [A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection](https://arxiv.org/abs/2512.22990)
*Soham Dutta,Soham Banerjee,Sneha Mahata,Anindya Sen,Sayantani Datta*

Main category: cs.CV

TL;DR: 提出一套仅用RGB相机的低成本无人机果园智能管线：ResNet50检叶病、VGG16判新鲜度、YOLOv8检定位苹果，离线运行于ESP32-CAM与树莓派，精度与实时性兼顾。


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统多各自为政（病害、质量、产量分离处理），且依赖昂贵多光谱传感器，难以在实际果园大规模推广。需要一种在低成本硬件上统一完成关键任务、且可离线运行的方案。

Method: 构建RGB-only的端到端管线：使用ResNet50进行叶片病害分类，VGG16进行苹果新鲜度判别，YOLOv8用于实时苹果目标检测与定位；部署在ESP32-CAM与Raspberry Pi上，保证全离线推理与现场应用。

Result: 叶病分类准确率98.9%；新鲜度分类准确率97.4%；苹果检测F1=0.857；系统可在低成本硬件上实现实时与准确的任务融合。

Conclusion: 该统一框架为多光谱UAV方案提供可及、可扩展的替代路径，降低成本并提升部署便捷性，支持精准农业在实际果园的落地应用。

Abstract: Apple orchards require timely disease detection, fruit quality assessment, and yield estimation, yet existing UAV-based systems address such tasks in isolation and often rely on costly multispectral sensors. This paper presents a unified, low-cost RGB-only UAV-based orchard intelligent pipeline integrating ResNet50 for leaf disease detection, VGG 16 for apple freshness determination, and YOLOv8 for real-time apple detection and localization. The system runs on an ESP32-CAM and Raspberry Pi, providing fully offline on-site inference without cloud support. Experiments demonstrate 98.9% accuracy for leaf disease classification, 97.4% accuracy for freshness classification, and 0.857 F1 score for apple detection. The framework provides an accessible and scalable alternative to multispectral UAV solutions, supporting practical precision agriculture on affordable hardware.

</details>


### [107] [OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding](https://arxiv.org/abs/2512.23020)
*Wenyuan Huang,Zhao Wang,Zhou Wei,Ting Huang,Fang Zhao,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: OpenGround提出一种零样本、开放世界的3D视觉指代框架，通过主动认知式推理逐步扩展VLM的认知范围，摆脱固定对象查询表限制；在多个数据集上表现优异并新建OpenTarget用于评测。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉指代方法依赖预定义对象查询表（OLT）向VLM提问，导致无法处理未定义或新颖目标，限制开放世界场景中的适用性。需要一种能在无固定类别前提下理解语言并定位新目标的机制。

Method: 提出OpenGround框架，核心为Active Cognition-based Reasoning（ACR）模块：通过类似人类的“认知任务链”，主动感知目标、按上下文逐步推理相关对象，动态更新并扩展OLT，从而让VLM在推理过程中增广其可识别类别集合；系统既能处理已有类别也能处理开放世界类别。并构建OpenTarget数据集（>7000对对象-描述）用于评估。

Result: 在Nr3D上达竞争性性能，在ScanRefer上达SOTA；在新建的OpenTarget数据集上相较基线提升17.6%。

Conclusion: 通过ACR动态扩展VLM的知识/对象范畴，OpenGround实现开放世界零样本3D指代并在多数据集上验证有效，证明摆脱固定OLT的主动认知推理可显著提升开放场景下的定位能力。

Abstract: 3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at [this https URL](https://why-102.github.io/openground.io/).

</details>


### [108] [With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs](https://arxiv.org/abs/2512.23024)
*Ciprian Constantinescu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出Geo-Semantic Contextual Graph（GSCG）从单张图像构建上下文图，并用图分类器融合目标、邻域与全局信息，实现可解释的上下文感知物体分类；在COCO上显著优于无上下文与强基线。


<details>
  <summary>Details</summary>
Motivation: 人类识别物体高度依赖场景上下文（空间关系、材质、共现），而现有算法多在孤立区域上分类，忽略上下文且不可解释，导致性能受限。

Method: 从单目图像联合度量深度估计与全景+材质分割，构建包含几何、色彩、材质属性为节点、空间关系为边的GSCG；再用专门的图分类器聚合目标、邻居及全局场景特征进行分类；进行消融与与多种基线比较。

Result: 在COCO 2017 train/val上，具备上下文的模型分类准确率73.4%，显著高于无上下文版本（低至38.4%）；超过微调ResNet（最高53.5%）和多模态LLM Llama 4 Scout（最高42.3%）。

Conclusion: 显式、可解释的上下文结构对于物体识别至关重要；通过GSCG与图分类器能显著提升性能，优于传统CNN与强多模态基线。

Abstract: Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.

</details>


### [109] [An Architecture-Led Hybrid Report on Body Language Detection Project](https://arxiv.org/abs/2512.23028)
*Thomson Tong,Diba Darooneh*

Main category: cs.CV

TL;DR: 对两种VLM（Qwen2.5-VL-7B-Instruct与Llama-4-Scout-17B-16E-Instruct）进行架构导向分析，并映射到一个视频到输出物的实际管线；强调共享多模态基础、各自架构足以支撑工程选择的要点，以及模型行为与系统约束的关系与风险。


<details>
  <summary>Details</summary>
Motivation: 在实际工程中需要将视频帧中的人物检测、属性（默认情绪）标注与边框生成统一到一个可验证、可落地的管线上。不同VLM的架构差异会影响输出格式、可控性与可靠性，因此需要从架构角度解释为何这些模型适合、有哪些陷阱，以及如何设计防护与评估。

Method: 1) 概述两模型的共享基础：视觉分词、Transformer注意力、指令跟随。2) 针对每个模型给出足以支撑工程选择的架构层面描述，避免无根据细节。3) 将模型行为映射到BodyLanguageDetection管线：采样帧—提示词检测人—生成像素级边框与属性—按预定义schema校验—可选地渲染标注视频。4) 指出结构化输出与语义正确性的差异、schema验证的局限、ID为单帧局部、交互式单帧默认自由文本等关键约束。

Result: 两模型在多模态基础上都能产出结构化边框与属性描述，但：结构校验能通过而语义仍可能错误；schema验证无法保证几何正确性；人物ID不跨帧；交互式单帧分析常返回自由文本而非严格JSON。这些观察解释了为何需要更强的提示词与后处理策略。

Conclusion: 在工程实践中应以架构认知指导接口与评估设计：对输出实施结构与语义的双层校验；在需要跨帧一致性时引入跟踪或外部ID对齐；为自由文本场景制定严格的提示约束与后处理；明确声明系统能力与边界以形成可辩护的结论与稳健的评测方案。

Abstract: This report provides an architecture-led analysis of two modern vision-language models (VLMs), Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct, and explains how their architectural properties map to a practical video-to-artifact pipeline implemented in the BodyLanguageDetection repository [1]. The system samples video frames, prompts a VLM to detect visible people and generate pixel-space bounding boxes with prompt-conditioned attributes (emotion by default), validates output structure using a predefined schema, and optionally renders an annotated video. We first summarize the shared multimodal foundation (visual tokenization, Transformer attention, and instruction following), then describe each architecture at a level sufficient to justify engineering choices without speculative internals. Finally, we connect model behavior to system constraints: structured outputs can be syntactically valid while semantically incorrect, schema validation is structural (not geometric correctness), person identifiers are frame-local in the current prompting contract, and interactive single-frame analysis returns free-form text rather than schema-enforced JSON. These distinctions are critical for writing defensible claims, designing robust interfaces, and planning evaluation.

</details>


### [110] [Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion](https://arxiv.org/abs/2512.23035)
*Yi Zhou,Xuechao Zou,Shun Zhang,Kai Li,Shiying Wang,Jingming Chen,Congyan Lang,Tengfei Cao,Pin Tao,Yuanchun Shi*

Main category: cs.CV

TL;DR: 提出Co2S：结合CLIP与DINOv3先验的稳定半监督遥感分割框架，缓解伪标签漂移并在六个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 半监督遥感分割减标注成本但易受确认偏差导致的伪标签漂移影响，误差在训练中累积，需要稳健的先验与结构抑制漂移、提升语义一致性与细节精度。

Method: 构建CLIP与DINOv3初始化的异构双学生ViT架构；引入显式-隐式语义共引导机制，利用文本嵌入与可学习查询提供类级显式与隐式指导；设计全局-局部特征协同融合，将CLIP的全局语境与DINOv3的局部细节有效融合，用于生成更可靠的伪标签与分割结果。

Result: 在六个主流数据集、不同划分协议与场景下均取得领先性能，显示鲁棒性与泛化性提升。

Conclusion: 融合VLM与自监督先验的异构双学生与语义共引导、全局-局部融合，可显著缓解伪标签漂移并提升半监督RS分割精度与稳定性，具备广泛适用性。

Abstract: Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.

</details>


### [111] [3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds](https://arxiv.org/abs/2512.23042)
*Ryousuke Yamada,Kohsuke Ide,Yoshihiro Fukuhara,Hirokatsu Kataoka,Gilles Puy,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: 论文提出LAM3C框架，从无标注视频生成的点云中自监督学习3D表示，并在无需真实3D扫描数据的情况下，在室内语义/实例分割上超越以往方法。


<details>
  <summary>Details</summary>
Motivation: 3D自监督学习需要大量高质量3D扫描，但采集昂贵且费时。作者想验证：是否能仅靠无标注视频（无真实3D传感器）生成的点云来学习有效的3D表示，从而降低数据成本并扩大可用数据规模。

Method: 1) 提出LAM3C：拉普拉斯感知的多层级3D聚类结合Sinkhorn-Knopp的自监督框架，用于从视频生成的点云学习特征；2) 构建RoomTours数据集：从网络收集室内漫游视频（如房产带看），用现成前馈重建模型生成49,219个场景点云；3) 设计噪声正则化损失：通过局部几何平滑与在含噪点云下的特征稳定性约束，提升训练稳定性；4) 多层级聚类对齐与分配采用Sinkhorn以获得鲁棒伪标签。

Result: 在不使用任何真实3D扫描的前提下，在室内语义分割与实例分割基准上，性能超过既有3D自监督方法。

Conclusion: 无标注视频可作为丰富而低成本的数据源，用视频生成点云并配合LAM3C的噪声鲁棒与多层级聚类机制，可学得强大的3D表示，缓解对昂贵3D扫描数据的依赖。

Abstract: Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.

</details>


### [112] [Video-BrowseComp: Benchmarking Agentic Video Research on Open Web](https://arxiv.org/abs/2512.23044)
*Zhengyang Liang,Yan Shu,Xiangrui Liu,Minghao Qin,Kaixin Liang,Paolo Rota,Nicu Sebe,Zheng Liu,Lizi Liao*

Main category: cs.CV

TL;DR: 该论文提出并发布Video-BrowseComp基准，专为开放网络环境下的“视频检索+推理”型智能体而设，要求模型主动在视频时间轴中查证证据、与外部网页信息交叉验证，避免仅凭文本搜索作答。评测显示即使是强大的检索增强模型（如GPT‑5.1+Search）也仅15.24%准确率，暴露当前模型过度依赖文本代理、缺乏视觉时序落地能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准多为被动感知：向模型提供剪辑片段，不需外部检索，无法衡量智能体在开放互联网上进行主动视频分析与事实核验的能力。现实任务常涉及沿时间轴定位关键片段、跨来源交叉比对与验证外部主张，现有评测存在模态与任务鸿沟。

Method: 构建Video-BrowseComp数据集（210个问题），设计题目使答案必须依赖“时间性的视觉证据”，并需在网络环境中浏览与检索以验证外部声明；题型覆盖元数据稀缺（体育、游戏）与元数据丰富（电视剧等）场景，强制避免仅靠文本搜索得出答案。用多种SOTA（含检索增强）模型在该基准上评估，并分析其错误模式。

Result: 总体准确率低，代表性模型GPT‑5.1（带搜索）仅15.24%。模型在元数据丰富领域表现尚可，但在需要强视觉时序定位与外部核验的动态场景（如体育、游戏）表现显著崩塌，显示其依赖文本代理、缺少真实视频落地与时间轴操作能力。

Conclusion: Video-BrowseComp首次系统评测开放网页环境下的“代理式视频研究”，从被动感知推进到主动视频推理。当前主流模型在该任务上存在关键瓶颈：缺乏时间性视觉证据对齐与跨源验证能力。该基准为后续在视频时间轴交互、视觉检索与多源一致性验证方面的方法研究提供挑战与方向。

Abstract: The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.

</details>


### [113] [MedSAM-based lung masking for multi-label chest X-ray classification](https://arxiv.org/abs/2512.23089)
*Brayden Miao,Zain Rehman,Xin Miao,Siming Liu,Jianjie Wang*

Main category: cs.CV

TL;DR: 提出用MedSAM做肺区分割引导的胸片多标签分类；“松/紧”掩膜对不同网络与任务表现存在权衡：原图ResNet50总体现最好，松掩膜提升正常筛查，紧掩膜降异常识别但训练更高效。结论：掩膜应作为可调空间先验匹配骨干与临床目标。


<details>
  <summary>Details</summary>
Motivation: CXR自动解读受弱病灶信号、数据偏差、空间监督有限等问题制约。借助医学分割基础模型可引入解剖先验，期望提升稳健性与可解释性。

Method: 构建“分割引导-分类”流水线：用公开肺部图像-掩膜数据微调MedSAM，先提取肺区；在经筛选的NIH CXR子集上训练/评估多标签CNN（五类异常：肿块、结节、肺炎、肺水肿、纤维化；正常通过派生评分）。比较原图、松掩膜、紧掩膜输入在不同骨干（如ResNet50）下的效果。

Result: MedSAM在多种成像条件下生成解剖合理的肺掩膜。表现与任务和架构相关：ResNet50在原图上异常区分最佳；松掩膜宏AUROC近似原图但显著提升“No Finding”判别；紧掩膜普遍降低异常分类但提升训练效率；松掩膜保留肺门/边缘上下文，部分缓解性能下降。

Conclusion: 肺区掩膜不应一刀切，而是作为可控的空间先验，根据骨干网络与临床目标（异常细分 vs 正常筛查、训练效率）选择“松/紧”程度，以平衡可解释性、稳健性与性能。

Abstract: Chest X-ray (CXR) imaging is widely used for screening and diagnosing pulmonary abnormalities, yet automated interpretation remains challenging due to weak disease signals, dataset bias, and limited spatial supervision. Foundation models for medical image segmentation (MedSAM) provide an opportunity to introduce anatomically grounded priors that may improve robustness and interpretability in CXR analysis. We propose a segmentation-guided CXR classification pipeline that integrates MedSAM as a lung region extraction module prior to multi-label abnormality classification. MedSAM is fine-tuned using a public image-mask dataset from Airlangga University Hospital. We then apply it to a curated subset of the public NIH CXR dataset to train and evaluate deep convolutional neural networks for multi-label prediction of five abnormalities (Mass, Nodule, Pneumonia, Edema, and Fibrosis), with the normal case (No Finding) evaluated via a derived score. Experiments show that MedSAM produces anatomically plausible lung masks across diverse imaging conditions. We find that masking effects are both task-dependent and architecture-dependent. ResNet50 trained on original images achieves the strongest overall abnormality discrimination, while loose lung masking yields comparable macro AUROC but significantly improves No Finding discrimination, indicating a trade-off between abnormality-specific classification and normal case screening. Tight masking consistently reduces abnormality level performance but improves training efficiency. Loose masking partially mitigates this degradation by preserving perihilar and peripheral context. These results suggest that lung masking should be treated as a controllable spatial prior selected to match the backbone and clinical objective, rather than applied uniformly.

</details>


### [114] [PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion](https://arxiv.org/abs/2512.23130)
*Jian Wang,Sixing Rong,Jiarui Xing,Yuling Xu,Weide Liu*

Main category: cs.CV

TL;DR: PathoSyn提出一种将病理效应视为稳定解剖基上的可加“偏差”的统一生成框架，用偏差空间扩散模型生成局部病灶变化，同时保持全局结构，显著优于全局/掩膜式基线。


<details>
  <summary>Details</summary>
Motivation: 现有MRI合成方法在全像素域工作或依赖二值掩膜，易出现特征缠结、解剖结构破坏与边界伪影，难以在低数据下生成解剖保真且具病灶异质性的图像。需要一种既保证全局解剖连续性又能灵活建模局部病理变化的可解释生成范式。

Method: 将影像生成分解为：1) 确定性的解剖重建（稳定解剖流形）；2) 随机的病理偏差建模。核心是偏差空间扩散模型（Deviation-Space Diffusion），学习条件病理残差的分布，只改变局部强度而不破坏全局结构。辅以缝合感知融合策略与推理期稳定化模块，抑制拼接边界伪影并提升病灶内部异质性与空间一致性。

Result: 在肿瘤影像基准上，PathoSyn在感知真实感与解剖保真度上显著优于整体扩散与掩膜条件基线；可生成病人特异的高保真合成数据，并实现可解释的反事实疾病进展建模。

Conclusion: PathoSyn以数学上规范的“解剖+偏差”分解与偏差空间扩散实现结构保真的病理合成，适用于低数据场景下的鲁棒算法开发、精准干预规划与临床决策系统基准测试；代码将公开。

Abstract: We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

</details>


### [115] [Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations](https://arxiv.org/abs/2512.23142)
*Mingzhen Shao,Sarang Joshi*

Main category: cs.CV

TL;DR: 论文提出UniReg，一个将特征提取与形变估计解耦的通用深度可变形配准框架，证明深度配准对域移不敏感的关键在于依赖局部特征而非全局外观；即便仅在单数据集训练，也能在跨域/多模态上与传统优化方法相当。


<details>
  <summary>Details</summary>
Motivation: 学习式图像配准被认为对域移敏感，通常依赖大规模多样训练来提升鲁棒性，但缺乏机制解释。作者怀疑鲁棒性的本质来自局部特征的一致性而非全局外观，并希望验证、隔离并利用这一机制。

Method: 提出UniReg：使用固定、预训练的特征提取器获取局部表征，再用UNet形变网络进行估计，实现特征提取与形变估计解耦；通过跨域、跨模态实验分析鲁棒性来源，并比较传统CNN在模态迁移下的失败原因（早期卷积层的偏置）。

Result: 即使只在单一数据集训练，UniReg在跨域与多模态配准中表现稳健，性能可与优化式方法相当；分析显示常规CNN在模态迁移失败源于数据集诱导的早期卷积偏置。

Conclusion: 学习式可变形配准的域移免疫性来自域不变的局部特征一致性；应设计能保留域不变局部特征的骨干网络，实现无需大规模多域训练也具鲁棒性的配准。

Abstract: Deep learning has advanced deformable image registration, surpassing traditional optimization-based methods in both accuracy and efficiency. However, learning-based models are widely believed to be sensitive to domain shift, with robustness typically pursued through large and diverse training datasets, without explaining the underlying mechanisms. In this work, we show that domain-shift immunity is an inherent property of deep deformable registration models, arising from their reliance on local feature representations rather than global appearance for deformation estimation. To isolate and validate this mechanism, we introduce UniReg, a universal registration framework that decouples feature extraction from deformation estimation using fixed, pre-trained feature extractors and a UNet-based deformation network. Despite training on a single dataset, UniReg exhibits robust cross-domain and multi-modal performance comparable to optimization-based methods. Our analysis further reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers. These findings identify local feature consistency as the key driver of robustness in learning-based deformable registration and motivate backbone designs that preserve domain-invariant local features.

</details>


### [116] [GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2512.23147)
*Jingyu Li,Xiaolong Zhao,Zhe Liu,Wenxiao Wu,Li Zhang*

Main category: cs.CV

TL;DR: 提出GeoTeacher用于半监督3D目标检测，通过几何关系监督与体素级增强提升学生模型对几何的敏感度，在ONCE与Waymo上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SS3D方法多依赖高质量伪标签或教师-学生一致性，但在标注稀缺时模型对目标几何不敏感，难以学习物体结构与定位关键信息，需要专门机制从未标注数据中强化几何感知。

Method: 1) 几何关系监督：基于关键点的几何关系监督模块，将教师模型的物体几何知识（关键点及其关系）迁移给学生，强化对几何结构与相对关系的理解。2) 体素级数据增强：在体素层面对对象几何进行多样化增强，并引入距离衰减机制以保护远距目标的完整性。3) 可插件化：GeoTeacher可与不同的半监督3D检测框架组合以提升性能。

Result: 在ONCE和Waymo数据集上进行大量实验，方法有效且具泛化性，取得新的SOTA表现。

Conclusion: 面向半监督3D检测，几何关系监督与体素级增强能显著提升学生模型的几何敏感度与检测性能；方法通用可与现有SS3D方法结合并达到SOTA。

Abstract: Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher

</details>


### [117] [REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation](https://arxiv.org/abs/2512.23169)
*Fulin Shi,Wenyi Xiao,Bin Chen,Liang Din,Leilei Gan*

Main category: cs.CV

TL;DR: 提出REVEALER：一种用于文本-图像元素级对齐评估的统一框架，通过强化学习引导的视觉推理实现可解释、细粒度、效率高且SOTA的对齐评估。


<details>
  <summary>Details</summary>
Motivation: 现有T2I对齐评估多为粗粒度或静态QA，缺乏元素级可解释性，且与人类偏好不一致，难以可靠反映模型表现。

Method: 构建“定位-推理-结论”的结构化范式，利用MLLM进行显式语义元素定位与可解释判定；以GRPO优化，奖励由结构化格式、定位（grounding）准确度与对齐忠实度组成；统一在多基准上评估并与强商用与监督式基线比较，同时关注推理效率。

Result: 在EvalMuse-40K、RichHF、MHaluBench、GenAI-Bench四个基准上达SOTA，稳定优于强商用模型与监督学习基线，并较现有迭代式视觉推理方法具有更高推理效率。

Conclusion: REVEALER能在元素级实现可解释、准确且高效的文本-图像对齐评估，优于既有方法，显示强化引导的视觉推理与结构化范式在T2I评估中的有效性。

Abstract: Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured "grounding-reasoning-conclusion" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.

</details>


### [118] [GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection](https://arxiv.org/abs/2512.23176)
*Yi Zhang,Yi Wang,Lei Yao,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 提出GVSynergy-Det：通过高斯-体素协同表示学习，在仅RGB条件下实现更精确的3D检测，无需深度或稠密3D监督，在ScanNetV2与ARKitScenes达SOTA。


<details>
  <summary>Details</summary>
Motivation: 图像式3D检测想摆脱点云等昂贵传感器，但现有方法要么依赖稠密3D监督才能高精度，要么无监督时几何提取不足、定位不准。作者观察到连续高斯与离散体素在几何表达上互补，有望同时捕获细节与空间结构，从而提升检测。

Method: 提出双表示架构：1) 将可泛化的Gaussian Splatting适配于检测，提取细粒度几何特征；2) 设计跨表示增强机制，用高斯场的几何细节丰富体素特征；3) 通过可学习融合，直接利用两种表示的特征进行3D目标定位，避免逐场景优化与仅将高斯用于深度正则的做法；全流程仅用RGB训练，无深度/点云/TSDF监督。

Result: 在室内基准ScanNetV2与ARKitScenes上显著优于现有方法，达到SOTA性能；尤其在无任何深度或稠密3D几何监督条件下仍保持高精度定位。

Conclusion: 高斯与体素表示在几何上互补，协同学习与可学习融合能在仅RGB条件下显著提升3D检测精度，避免昂贵监督与耗时优化，具备通用性与效率优势。

Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

</details>


### [119] [GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation](https://arxiv.org/abs/2512.23180)
*Tianchen Deng,Xuefeng Chen,Yi Chen,Qu Chen,Yuyao Xu,Lijin Yang,Le Xu,Yu Zhang,Bo Zhang,Wuxiong Huang,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出基于3D高斯表示的统一驾驶世界模型，实现3D理解与多模态生成的早期跨模态对齐与任务感知采样，在nuScenes与NuInteract上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型缺乏真正的3D场景理解与推理能力，只能条件生成；且用点云/BEV表达3D信息难以将文本与真实3D场景精确对齐，影响理解与生成质量。

Method: 以3D Gaussian作为场景表示：在每个高斯元中嵌入丰富语言特征，实现文本与3D的早期对齐；提出任务感知、语言引导的采样策略，去除冗余高斯并将紧凑准确的3D token注入LLM；设计双条件多模态生成模型，以视觉-语言模型提取的高层语言条件与低层图像条件联合引导生成。

Result: 在nuScenes和NuInteract上进行了全面实验，方法在理解与生成任务均达到SOTA表现。

Conclusion: 3D高斯驱动的统一DWM可同时进行3D场景理解与多模态生成，通过早期跨模态对齐与任务感知采样提升效率与精度，验证效果优于现有方法，并将开源代码。

Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.

</details>


### [120] [ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis](https://arxiv.org/abs/2512.23196)
*Maisha Haque,Israt Jahan Ayshi,Sadaf M. Anis,Nahian Tasnim,Mithila Moontaha,Md. Sabbir Ahmed,Muhammad Iqbal Hossain,Mohammad Zavid Parvez,Subrata Chakraborty,Biswajeet Pradhan,Biswajit Banik*

Main category: cs.CV

TL;DR: 提出ForCM方法：将OBIA与多种深度学习分割网络（UNet/UNet++/ResUNet/AttentionUNet/ResNet50-SegNet）结合，用Sentinel‑2多光谱影像在亚马逊进行森林覆盖制图；最佳为AttentionUNet‑OBIA与ResUNet‑OBIA，整体精度达95.64%与94.54%，优于传统OBIA（92.91%）；展示了基于QGIS等免费工具实现高精度制图的可行性。


<details>
  <summary>Details</summary>
Motivation: 森林覆盖精确制图对全球环境监测与保护至关重要，但单一OBIA或DL在高分辨率、复杂景观（如亚马逊雨林）中存在精度或泛化不足；需要系统评估并融合OBIA的对象层次信息与DL的表征能力，提高制图精度与可用性，并验证在免费工具生态内的可行性。

Method: 使用Sentinel‑2 Level‑2A多光谱（两组三波段与一组四波段）构建数据集；比较多种语义分割网络（UNet、UNet++、ResUNet、AttentionUNet、ResNet50‑SegNet）；筛选表现最佳的DL模型，与OBIA流程耦合（先由DL生成概率/分割图，再基于对象级分割与规则优化）形成ForCM；与传统OBIA基线对比评估。

Result: AttentionUNet‑OBIA整体精度95.64%，ResUNet‑OBIA 94.54%，均显著优于仅用OBIA的92.91%；证明DL+OBIA协同能提升森林覆盖映射精度；免费、易用的软件（如QGIS）在一定限制下可支持高精度制图。

Conclusion: DL与OBIA融合是提升森林覆盖制图精度的有效路径；在亚马逊场景下，AttentionUNet/ResUNet结合OBIA优于传统OBIA；免费工具链具有实用潜力，为大范围生态监测与保护提供支持。

Abstract: This research proposes "ForCM", a novel approach to forest cover mapping that combines Object-Based Image Analysis (OBIA) with Deep Learning (DL) using multispectral Sentinel-2 imagery. The study explores several DL models, including UNet, UNet++, ResUNet, AttentionUNet, and ResNet50-Segnet, applied to high-resolution Sentinel-2 Level 2A satellite images of the Amazon Rainforest. The datasets comprise three collections: two sets of three-band imagery and one set of four-band imagery. After evaluation, the most effective DL models are individually integrated with the OBIA technique to enhance mapping accuracy. The originality of this work lies in evaluating different deep learning models combined with OBIA and comparing them with traditional OBIA methods. The results show that the proposed ForCM method improves forest cover mapping, achieving overall accuracies of 94.54 percent with ResUNet-OBIA and 95.64 percent with AttentionUNet-OBIA, compared to 92.91 percent using traditional OBIA. This research also demonstrates the potential of free and user-friendly tools such as QGIS for accurate mapping within their limitations, supporting global environmental monitoring and conservation efforts.

</details>


### [121] [Exploring Syn-to-Real Domain Adaptation for Military Target Detection](https://arxiv.org/abs/2512.23208)
*Jongoh Jeong,Youngjin Oh,Gyeongrae Nam,Jeongeun Lee,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出用Unreal Engine生成高拟真的RGB合成数据，在多域军事目标检测任务上进行“合成到真实”迁移评估，发现带少量监督提示的DA方法优于无/半监督方法，并总结当前挑战。


<details>
  <summary>Details</summary>
Motivation: 军事指挥侦察需可靠目标检测，但现有域自适应检测多局限自然/自动驾驶且为单一源-目标域；军事场景多域复杂，且SAR虽鲁棒但成本高、处理慢；RGB廉价但缺乏军事数据集，限制其应用。

Method: 利用Unreal Engine生成军事目标的高保真RGB合成数据，构建合成训练集与网络抓取的真实军事目标验证集；在该数据对上系统评测多种最先进域自适应检测方法，按监督强度（无监督、半监督、带最小提示如类别）分类对比。

Result: 在所提合成-真实数据对上，使用图像最小提示（如目标类别）的DA方法显著优于无监督或半监督DA方法；为多域军事目标检测提供了基准结果。

Conclusion: RGB合成数据可缓解军事数据稀缺并用于跨域训练，但当前完全无监督/弱监督DA仍不足；引入轻量监督能带来明显收益。仍存在多域差异大、真实域稀缺等挑战，有待进一步研究。

Abstract: Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.

</details>


### [122] [Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks](https://arxiv.org/abs/2512.23210)
*Changgyoon Oh,Jongoh Jeong,Jegyeong Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出在扩散模型中自适应选择并融合最有用的扩散时间步特征，以少样本条件下提升密集预测任务在未见任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有将扩散模型用于判别/密集预测的做法通常从多步马尔可夫过程的中间特征中“拍脑袋”选取时间步，并用任务特定解码器执行单任务推断；这种启发式选择易偏向特定任务并造成次优。作者希望在通用与少样本场景中，自动发现对任意未见任务最有判别力的时间步特征。

Method: 提出两模块：1) 任务感知时间步选择（TTS），根据时间步级损失和相似度分数自适应挑选最合适的扩散时间步；2) 时间步特征整合（TFC），将被选中的时间步特征进行整合以增强密集预测性能。并配合参数高效的微调适配器，在少量支持样本条件下进行端到端训练/适配。

Result: 在Taskonomy大规模数据集的多种密集预测任务上进行实证，尤其在通用与few-shot设定中，相较基线取得优越性能，显示可学习的时间步整合带来显著收益。

Conclusion: 自适应时间步选择与特征整合能有效释放扩散模型多时间步表示的判别潜力，在少样本密集预测与未见任务迁移中优于启发式选择，并以参数高效方式实现。

Abstract: Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.

</details>


### [123] [AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding](https://arxiv.org/abs/2512.23215)
*Jongoh Jeong,Taek-Jin Song,Jong-Hwan Kim,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出AVOID数据集：在仿真环境下采集、覆盖多种恶劣天气与昼夜条件，包含小型突发道路障碍，并配套语义/深度/激光雷达与航点；基于实时网络进行障碍检测基准与多任务消融（分割、深度、航点）。


<details>
  <summary>Details</summary>
Motivation: 现有道路数据集通常只覆盖正常或单一不良场景，且缺少与其他类别同域采集的“意外小障碍”样本，影响在复杂条件下实时稳定检测的能力；为支持多任务感知并统一域差，亟需一个兼具恶劣条件与丰富传感标注的新数据集。

Method: 在仿真平台系统采集多天气、多时段的行驶图像；在每条路径布设意外小障碍；为每张图像同步生成语义分割图、深度图、原始与语义化LiDAR以及航点；以高性能实时网络进行障碍检测基准评测；构建一个联合语义分割/深度/航点预测的多任务网络并进行消融实验。

Result: 给出在AVOID上多种实时障碍检测模型的基准结果；多任务网络的消融显示在联合学习设置下对相关任务的性能影响（细节未在摘要中给出具体数值）。

Conclusion: AVOID为不利视觉条件下的小型突发障碍实时检测提供了统一、丰富且多模态的基准；其多任务与多传感标注支持更广泛的视觉感知研究，基准与消融验证了其研究价值。

Abstract: Understanding road scenes for visual perception remains crucial for intelligent self-driving cars. In particular, it is desirable to detect unexpected small road hazards reliably in real-time, especially under varying adverse conditions (e.g., weather and daylight). However, existing road driving datasets provide large-scale images acquired in either normal or adverse scenarios only, and often do not contain the road obstacles captured in the same visual domain as for the other classes. To address this, we introduce a new dataset called AVOID, the Adverse Visual Conditions Dataset, for real-time obstacle detection collected in a simulated environment. AVOID consists of a large set of unexpected road obstacles located along each path captured under various weather and time conditions. Each image is coupled with the corresponding semantic and depth maps, raw and semantic LiDAR data, and waypoints, thereby supporting most visual perception tasks. We benchmark the results on high-performing real-time networks for the obstacle detection task, and also propose and conduct ablation studies using a comprehensive multi-task network for semantic segmentation, depth and waypoint prediction tasks.

</details>


### [124] [MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?](https://arxiv.org/abs/2512.23219)
*Shiqi Dai,Zizhi Ma,Zhicong Luo,Xuesong Yang,Yibin Huang,Wanyue Zhang,Chi Chen,Zonghao Guo,Wang Xu,Yufei Sun,Maosong Sun*

Main category: cs.CV

TL;DR: 提出MM-UAVBench：面向低空无人机场景，对多模态大模型（MLLM）在感知、认知、规划三维能力进行统一评测的基准，含19子任务、5.7K人工标注问题，基于真实UAV数据。结果显示现有MLLM在低空复杂视觉与认知需求下表现不佳，暴露空间偏置与多视角理解等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准很少覆盖低空UAV独有挑战；UAV评测多聚焦单一任务（定位、导航），缺乏对MLLM通用智能的统一量化与对比，阻碍模型在真实低空智能中的落地。

Method: 构建综合基准MM-UAVBench：围绕感知、认知、规划三大能力设计19个子任务，基于公开数据集的真实UAV视觉数据，人工标注超过5.7K问答；对16个开源/闭源MLLM进行系统实验与误差分析，挖掘低空场景关键难点。

Result: 多数现有MLLM在低空多模态任务上显著失准，适应性与稳健性不足；实验揭示关键瓶颈包括空间偏置（如高度/视角引起的误判）与多视角理解薄弱，影响感知到规划的全链路表现。

Conclusion: MM-UAVBench填补低空UAV场景对MLLM通用智能评测的空白，可作为标准化对比平台；当前模型仍需在空间理解、多视角融合与任务规划等方面改进，以实现可靠的低空UAV智能应用。

Abstract: While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.

</details>


### [125] [Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information](https://arxiv.org/abs/2512.23221)
*Youngchae Kwon,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 提出 Holi-DETR，通过显式融合三类上下文（共现、物件间相对空间、人-物关键点关系）来整体检测穿搭图中的时尚单品，在DETR与Co-DETR上显著提升AP（+3.6pp/+1.1pp）。


<details>
  <summary>Details</summary>
Motivation: 时尚单品外观高度多样且细分类别相似，独立检测易混淆；穿搭元素存在语义/空间共性与人与衣物的结构关系，若能系统利用上下文可缓解歧义。

Method: 在DETR框架中设计整体式检测器Holi-DETR，显式建模三种上下文：1) 类别共现关系；2) 物件间相对位置与尺度；3) 物件与人体关键点的空间关系；将异质上下文融入Transformer的编码/解码过程以联合推理多个目标。

Result: 在标准评测中，相比原始DETR平均精度提升3.6个百分点，相比Co-DETR提升1.1个百分点。

Conclusion: 整体式利用多源上下文能有效缓解时尚检测歧义，Holi-DETR可作为DETR家族可扩展的上下文增强模块，带来稳定AP增益。

Abstract: Fashion item detection is challenging due to the ambiguities introduced by the highly diverse appearances of fashion items and the similarities among item subcategories. To address this challenge, we propose a novel Holistic Detection Transformer (Holi-DETR) that detects fashion items in outfit images holistically, by leveraging contextual information. Fashion items often have meaningful relationships as they are combined to create specific styles. Unlike conventional detectors that detect each item independently, Holi-DETR detects multiple items while reducing ambiguities by leveraging three distinct types of contextual information: (1) the co-occurrence relationship between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. %Holi-DETR explicitly incorporates three types of contextual information: (1) the co-occurrence probability between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. To this end, we propose a novel architecture that integrates these three types of heterogeneous contextual information into the Detection Transformer (DETR) and its subsequent models. In experiments, the proposed methods improved the performance of the vanilla DETR and the more recently developed Co-DETR by 3.6 percent points (pp) and 1.1 pp, respectively, in terms of average precision (AP).

</details>


### [126] [Bridging Your Imagination with Audio-Video Generation via a Unified Director](https://arxiv.org/abs/2512.23222)
*Jiaxu Zhang,Tianshu Hu,Yuan Zhang,Zenan Li,Linjie Luo,Guosheng Lin,Xin Chen*

Main category: cs.CV

TL;DR: UniMAGE将剧本撰写与关键镜头生成统一到一个“导演模型”中，用混合Transformer和“先交错、后解耦”的训练范式，既理解长文本叙事又输出一致的关键帧，达成开源SOTA的逻辑连贯脚本与视觉一致关键帧生成。


<details>
  <summary>Details</summary>
Motivation: 现有流程把LLM写剧本与图像模型做镜头设计割裂处理，导致叙事逻辑与视觉连贯性难以协同；作者认为导演思维需要推理与想象的统一，需一个端到端桥接用户提示到可制作多镜头影片的系统。

Method: 构建统一的Mixture-of-Transformers框架，兼顾文本与图像生成；提出“先交错、后解耦”的训练：1) 交错概念学习，用交错的图文数据增强对剧本—画面的联想与理解；2) 解耦专家学习，将剧本撰写与关键帧生成分别由不同专家子模块学习，提升叙事灵活性与创意；最终输出结构化剧本与关键帧以驱动现有音视频生成。

Result: 在大量实验中，相比开源方法达到SOTA：能生成逻辑连贯的长上下文多镜头视频脚本，并产出视觉一致的关键帧图像。

Conclusion: 统一导演模型可有效对齐叙事推理与视觉想象，通过交错学习建立跨模态理解，再以解耦专家增强创造性与一致性，从而显著提升从提示到影片的端到端生产质量。

Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.

</details>


### [127] [Anomaly Detection by Effectively Leveraging Synthetic Images](https://arxiv.org/abs/2512.23227)
*Sungho Kang,Hyunkyu Park,Yeonho Lee,Hanbyul Lee,Mijoo Jeong,YeongHyeon Park,Injae Lee,Juneho Yi*

Main category: cs.CV

TL;DR: 提出一种高效利用合成缺陷图像以提升无监督异常检测的方法：先用规则合成做预训练，再用检索筛选过的高质量生成缺陷图像做微调，在MVTec AD上验证有效。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷样本稀缺，导致无监督仅用正常图像的方法受限；现有合成策略要么低成本但不逼真（规则法），要么高质量但昂贵（生成模型）。需要一种在成本与质量之间取得平衡并能显著提升检测性能的方案。

Method: 利用预训练的文本引导图像到图像翻译模型生成缺陷图像，并结合图像检索模型评估其与真实正常图像的相似度，过滤无关或不合理输出以提升相关性与质量；训练上采用两阶段：1）用大量规则合成数据预训练异常检测模型；2）用较小规模的高质量生成缺陷图像进行微调。

Result: 在MVTec AD数据集上，所提框架显著提升异常检测性能，相比仅规则合成或仅生成模型方案，以更低数据与计算成本取得更优结果。

Conclusion: 通过检索筛选的文本引导生成缺陷图像与两阶段训练策略，可以在成本可控的前提下有效提升无监督异常检测效果；该方法兼顾数据质量与效率，适合实际工业场景的数据扩充与训练流程。

Abstract: Anomaly detection plays a vital role in industrial manufacturing. Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied. Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution. In this work, we focus on a strategy to effectively leverage synthetic images to maximize the anomaly detection performance. Previous synthesis strategies are broadly categorized into two groups, presenting a clear trade-off. Rule-based synthesis, such as injecting noise or pasting patches, is cost-effective but often fails to produce realistic defect images. On the other hand, generative model-based synthesis can create high-quality defect images but requires substantial cost. To address this problem, we propose a novel framework that leverages a pre-trained text-guided image-to-image translation model and image retrieval model to efficiently generate synthetic defect images. Specifically, the image retrieval model assesses the similarity of the generated images to real normal images and filters out irrelevant outputs, thereby enhancing the quality and relevance of the generated defect images. To effectively leverage synthetic images, we also introduce a two stage training strategy. In this strategy, the model is first pre-trained on a large volume of images from rule-based synthesis and then fine-tuned on a smaller set of high-quality images. This method significantly reduces the cost for data collection while improving the anomaly detection performance. Experiments on the MVTec AD dataset demonstrate the effectiveness of our approach.

</details>


### [128] [SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems](https://arxiv.org/abs/2512.23232)
*Minwoo Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 提出SGPS，用SURE梯度与PCA噪声估计来纠正扩散模型逆问题采样轨迹的偏差，在<100 NFE下仍保持高质量重建，低步数显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在逆问题中很强，但现有“扩散采样↔数据一致性”交替方法因噪声累积与轨迹偏离，需数百/上千步才能达到高质量重建；需要在低NFE下仍保持精度。

Method: 在采样的早中期引入基于SURE的无偏风险估计梯度更新，结合PCA估计噪声水平，在线纠正采样轨迹的偏差，降低噪声诱发误差；与数据一致性配合，实现更准确的后验采样，减少误差累积与所需NFE。

Result: 在多种逆问题基准上，SGPS在低NFE设定下一致优于现有方法，并在<100 NFE即可达到高重建质量。

Conclusion: 通过SURE引导与PCA噪声估计纠偏，可显著缓解扩散逆问题采样中的误差累积，使得以远少于传统步数的NFE实现高质量重建。

Abstract: Diffusion models have emerged as powerful learned priors for solving inverse problems. However, current iterative solving approaches which alternate between diffusion sampling and data consistency steps typically require hundreds or thousands of steps to achieve high quality reconstruction due to accumulated errors. We address this challenge with SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations using Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA based noise estimation. By mitigating noise induced errors during the critical early and middle sampling stages, SGPS enables more accurate posterior sampling and reduces error accumulation. This allows our method to maintain high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs). Our extensive evaluation across diverse inverse problems demonstrates that SGPS consistently outperforms existing methods at low NFE counts.

</details>


### [129] [Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network](https://arxiv.org/abs/2512.23234)
*Dongsheng Li,Chaobo Chen,Siling Wang,Song Gao*

Main category: cs.CV

TL;DR: 提出PEG-DRNet用于红外气体泄漏检测，结合物理启发的气体传输单元、鲁棒边缘先验与自适应稀疏路由聚合，在IIG与LangGas上以更少计算实现更高AP，尤其提升小目标与弱边界羽流检测。


<details>
  <summary>Details</summary>
Motivation: 红外气体羽流通常微弱、半透明、边界弥散且目标小，导致现有CNN/Transformer检测器在对比度低与跨尺度特征融合方面表现欠佳，需要兼顾物理过程建模与边缘信息以增强检测鲁棒性与效率。

Method: 1) Gas Block：扩散-对流建模单元，局部分支捕获短程变化，大核分支捕获长程传播；通过边缘门控可学习融合平衡细节与全局。2) AGPEO：从多方向梯度与相位一致性响应计算稳健边缘先验；3) MSEPM：将先验转为多尺度层级边缘特征以强化边界；4) CASR-PAN：内容自适应稀疏路由聚合，利用边缘与内容线索进行跨尺度信息选择性传播与调制，降低冗余、提升判别性。

Result: 在IIG数据集上整体AP 29.8%、AP50 84.3%、小目标AP 25.3%，相对RT-DETR-R18分别提升+3.0%、+6.5%、+5.3%，模型仅43.7 GFLOPs、14.9M参数；在IIG与LangGas上较现有CNN与Transformer检测器在AP与AP50上均更优。

Conclusion: PEG-DRNet通过物理-边缘混合设计与自适应稀疏路由，实现精度-效率最佳平衡，显著改善弱对比、小目标与模糊边界的红外气体泄漏检测，可作为工业与环境监测的高效方案。

Abstract: Detecting infrared gas leaks is critical for environmental monitoring and industrial safety, yet remains difficult because plumes are faint, small, semitransparent, and have weak, diffuse boundaries. We present physics-edge hybrid gas dynamic routing network (PEG-DRNet). First, we introduce the Gas Block, a diffusion-convection unit modeling gas transport: a local branch captures short-range variations, while a large-kernel branch captures long-range propagation. An edge-gated learnable fusion module balances local detail and global context, strengthening weak-contrast plume and contour cues. Second, we propose the adaptive gradient and phase edge operator (AGPEO), computing reliable edge priors from multi-directional gradients and phase-consistent responses. These are transformed by a multi-scale edge perception module (MSEPM) into hierarchical edge features that reinforce boundaries. Finally, the content-adaptive sparse routing path aggregation network (CASR-PAN), with adaptive information modulation modules for fusion and self, selectively propagates informative features across scales based on edge and content cues, improving cross-scale discriminability while reducing redundancy. Experiments on the IIG dataset show that PEG-DRNet achieves an overall AP of 29.8\%, an AP$_{50}$ of 84.3\%, and a small-object AP of 25.3\%, surpassing the RT-DETR-R18 baseline by 3.0\%, 6.5\%, and 5.3\%, respectively, while requiring only 43.7 Gflops and 14.9 M parameters. The proposed PEG-DRNet achieves superior overall performance with the best balance of accuracy and computational efficiency, outperforming existing CNN and Transformer detectors in AP and AP$_{50}$ on the IIG and LangGas dataset.

</details>


### [130] [RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models](https://arxiv.org/abs/2512.23239)
*Fan Wei,Runmin Dong,Yushan Lai,Yixiang Yang,Zhaoyang Luo,Jinxiao Zhang,Miao Yang,Shuai Yuan,Jiyao Zhao,Bin Luo,Haohuan Fu*

Main category: cs.CV

TL;DR: 提出一种无训练、两阶段的数据剪枝方法，用于遥感扩散生成基础模型，在高剪枝率下仍能选出高质量子集，加速收敛并提升生成与下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有RS扩散基础模型需大规模、全球代表性数据，但存在冗余、噪声与类不平衡，导致训练低效与难以收敛；现有做法多为简单合并分类数据或粗粒度去重，忽视生成建模的分布需求与RS图像异质性。

Method: 训练自由的两阶段剪枝：1) 基于熵的判据快速剔除低信息样本；2) 以RS场景分类数据为参考基准，进行场景感知聚类与分层采样，在大规模无标注数据上兼顾聚类效果与计算成本；最终在聚类层面均匀性与样本代表性之间权衡，进行细粒度选择，在高剪枝率下保持整体多样性与代表性。

Result: 在裁剪85%的训练数据后，仍显著提升扩散模型的收敛速度与生成质量；并在超分辨率、语义图像合成等下游任务上持续达到或刷新SOTA性能。

Conclusion: 数据剪枝范式能在不额外训练代价下，构建更高效、更具代表性的训练子集，促进RS扩散基础模型快速收敛，提升生成与多下游任务表现，为RS生成基础模型开发提供实用指导。

Abstract: Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.

</details>


### [131] [Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism](https://arxiv.org/abs/2512.23243)
*Siyu Zhang,Ying Chen,Lianlei Shan,Runhe Qiu*

Main category: cs.CV

TL;DR: 提出一个用于遥感多模态融合的视觉-语言模型框架，包含动态分辨率输入策略与多尺度对齐机制，在RS-GPT4V上显著提升图像描述与跨模态检索的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 单一数据源与固定分辨率方法难以兼顾效率与细节，单尺度对齐缺少语义层级，导致语义错配与粒度不均，限制遥感智能解译效果。

Method: 构建VLM框架，包含：1) 动态分辨率输入策略（DRIS），依据图像内容复杂度自适应进行粗到细的分辨率与算力分配，保留关键细节、减少冗余计算；2) 多尺度视觉-语言对齐机制（MS-VLAM），在目标、局部区域、全局三层进行跨模态对齐，系统性捕获语义一致性、缓解粒度失衡。

Result: 在RS-GPT4V数据集上，图像描述（BLEU-4、CIDEr）与跨模态检索（R@10）均优于传统方法，同时具备更高计算效率。

Conclusion: 动态分辨率与多层对齐的VLM有效提升遥感多模态理解的精度与效率，为工程化的智能遥感解译提供方法论与技术指引。

Abstract: Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.

</details>


### [132] [ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2512.23244)
*Xingwei Ma,Shiyang Feng,Bo Zhang,Bin Wang*

Main category: cs.CV

TL;DR: 提出ViLaCD-R1：先用多图像推理器产生粗变更掩膜，再由掩膜引导解码器细化，显著提升遥感变化检测的语义识别、定位与鲁棒性，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统像素/编码-解码方法难以捕获高层语义，易受非语义扰动影响；现有多模态与VLM方法虽有语义优势，但定位不准、边界粗糙、可解释性有限。

Method: 两阶段框架：1) 多图像推理器（MIR）基于VLM，对双时相图像块进行监督微调+强化学习训练，直接输出粗变更掩膜；2) 掩膜引导解码器（MGD）融合双时相图像特征与粗掩膜，预测精细二值变化图。

Result: 在多个人工与真实复杂场景RSCD基准上，显著提升语义变化识别与定位能力，强抑制非语义变化，整体精度达SOTA。

Conclusion: 结合VLM的高层语义推理与掩膜引导的精细解码，可同时解决语义理解、空间定位与边界刻画难题，为复杂场景下的遥感变化检测提供高精度、可扩展方案。

Abstract: Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.

</details>


### [133] [ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation](https://arxiv.org/abs/2512.23245)
*Shin seong Kim,Minjung Shin,Hyunin Cho,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出ASemconsist框架，在文本到图像扩散中通过选择性文本嵌入修改与自适应特征共享，兼顾角色身份一致性与逐图提示对齐，并引入CQS统一评测，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散在多场景序列生成时，很难同时保证角色身份一致与每张图的提示对齐，方法常在两者间权衡；需要一种能显式控制身份语义且不牺牲对齐的机制，并有统一评测度量两者失衡。

Method: 1) 选择性文本嵌入修改：对与身份相关的文本嵌入进行精细操控，实现显式语义控制，避免破坏其他提示词对齐。2) 基于FLUX的padding embedding分析：将padding嵌入重用为“语义容器”，承载和隔离身份语义，以减少对主提示的干扰。3) 自适应特征共享：自动评估文本歧义度，仅对含身份歧义的提示施加跨图约束，其他情况保持独立生成以保留对齐。4) 统一评测CQS：将身份保持与逐图对齐融合成单一指标，并显式刻画两者间的失衡。

Result: 在多场景角色一致性生成任务上取得SOTA，既提升身份一致性又保持或提升逐图提示对齐，较以往方法减少两者之间的权衡。

Conclusion: 通过对文本嵌入与padding语义的可控改造及自适应约束，ASemconsist打破身份一致性与提示对齐的传统权衡，并用CQS提供更全面的评测。

Abstract: Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist

</details>


### [134] [Contour Information Aware 2D Gaussian Splatting for Image Representation](https://arxiv.org/abs/2512.23255)
*Masaya Takabe,Hiroshi Watanabe,Sujun Hong,Tomohiro Ikai,Zheming Fan,Ryo Ishimoto,Kakeru Sugimoto,Ruri Imichi*

Main category: cs.CV

TL;DR: 提出在2D高斯点渲染中引入分割先验与区域约束，避免跨边界混合，在极少高斯数下显著提升边缘重建质量，同时保持实时与低内存。


<details>
  <summary>Details</summary>
Motivation: 现有2D Gaussian Splatting虽解码快、存储紧凑，但在高压缩（高斯数量少）时边缘模糊、轮廓不清，缺乏对物体边界的显式感知。需要一种轮廓敏感的表征以保留清晰边界。

Method: 引入“轮廓信息感知”的2DGS：利用对象分割先验，在光栅化时将每个高斯约束在其对应的分割区域内，防止跨边界混合；并提出训练warm-up方案以稳定优化并加速收敛。

Result: 在合成色卡与DAVIS数据集上，相比现有2DGS，在物体边缘区域的重建质量更高，优势在高压缩（极少高斯）场景尤为明显；同时保持实时渲染与低内存。

Conclusion: 将分割先验纳入2DGS并进行区域约束，可在不牺牲效率的情况下显著改善边缘与轮廓重建，尤其适用于高压缩应用。

Abstract: Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.

</details>


### [135] [Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization](https://arxiv.org/abs/2512.23258)
*Tong Shao,Yusen Fu,Guoying Sun,Jingde Kong,Zhuotao Tian,Jingyong Su*

Main category: cs.CV

TL;DR: 提出CEM（累计误差最小化）作为适配插件，利用动态规划在给定加速预算下优化缓存策略，显著提升DiT加速生成的保真度且零额外开销。


<details>
  <summary>Details</summary>
Motivation: DiT推理迭代去噪缓慢；无训练缓存加速虽快但误差大。现有方法用剪枝/预测等纠错，但采用固定缓存策略，无法适应去噪过程中误差的时变与区间差异，限制了纠错潜力。

Method: 1) 预定义“误差先验”，刻画时间步与缓存间隔共同影响下模型对加速的敏感性；2) 以累计误差近似为目标，构建动态规划求解最优（或近优）缓存策略，使全程缓存误差最小；3) 将该策略作为插件嵌入现有纠错和量化框架，支持任意加速预算，无需额外计算开销。

Result: 在9个生成模型/量化设定、3类任务上，CEM显著提升已有加速模型的生成保真度；在FLUX.1-dev、PixArt-α、StableDiffusion1.5、Hunyuan上甚至超过原始未加速生成表现。

Conclusion: CEM作为与模型无关的、可泛化的保真度优化插件，可在不增加开销的前提下，通过动态规划优化缓存策略，有效降低加速引入的累计误差，广泛提升DiT系加速生成质量。

Abstract: Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.

</details>


### [136] [YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection](https://arxiv.org/abs/2512.23273)
*Xu Lin,Jinlong Peng,Zhenye Gan,Jiawen Zhu,Jun Liu*

Main category: cs.CV

TL;DR: 提出YOLO-Master：在YOLO范式中引入实例自适应稀疏MoE计算，以场景复杂度动态分配算力，兼顾精度与速度；在COCO上达42.4% AP、1.62ms推理，优于YOLOv13-N（+0.8 mAP、+17.8%速度）。


<details>
  <summary>Details</summary>
Motivation: 现有RTOD多采用YOLO式静态致密计算，对所有输入一视同仁，导致在简单场景上过度计算、复杂场景上算力不足，既浪费算力又限制检测性能，尤其在稠密困难场景效果欠佳。

Method: 设计YOLO-Master框架，核心为高效稀疏专家混合（ES-MoE）模块：1) 轻量级动态路由网络按实例与场景复杂度选择少量专家；2) 通过多样性增强目标引导专家在训练中形成互补专长；3) 推理时仅激活最相关专家，实现自适应稀疏计算，减少开销并提升检测。

Result: 在五个大规模基准上全面优于现有方法；COCO上达42.4% AP、1.62ms延迟，相比YOLOv13-N提升0.8 mAP且推理加速17.8%；在稠密复杂场景收益更显著，同时在常规输入保持实时与高效。

Conclusion: 实例条件的自适应稀疏计算能有效缓解YOLO静态计算的错配问题，YOLO-Master通过ES-MoE与动态路由在不增加或减少总体开销的同时提升精度与速度，尤其改善复杂场景检测表现，并保持实时性。

Abstract: Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.

</details>


### [137] [Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition](https://arxiv.org/abs/2512.23291)
*Arman Martirosyan,Shahane Tigranyan,Maria Razzhivina,Artak Aslanyan,Nazgul Salikhova,Ilya Makarov,Andrey Savchenko,Aram Avetisyan*

Main category: cs.CV

TL;DR: 提出两个多模态框架：用于微手势分类的RGB+3D骨架融合（MViTv2-S与2s-AGCN，经跨模态token融合）；用于基于行为的情感预测的面部+上下文融合（SwinFace与MViTv2-S，经InterFusion），在iMiGUE/MiGA 2025挑战情感任务获第二名。


<details>
  <summary>Details</summary>
Motivation: 微手势与基于行为的情感识别需捕捉极其细微的时空模式，单一模态难以充分建模；现有方法对跨模态细粒度融合与情境/面部—姿态互补的利用仍不足。

Method: 1) 微手势：提取视频表征（MViTv2-S）与3D骨架表征（2s-AGCN），通过Cross-Modal Token Fusion融合以结合空间外观与姿态动力学。2) 情感：提取面部表征（SwinFace）与上下文表征（MViTv2-S），通过InterFusion模块融合以捕捉表情与身体/环境线索；二分类训练于iMiGUE。

Result: 在iMiGUE数据集上进行实验；在MiGA 2025挑战的基于行为的情感预测赛道取得第二名，显示方法具有较强的鲁棒性与准确度。

Conclusion: 跨模态表示与专门设计的融合模块（Cross-Modal Token Fusion、InterFusion）能有效提升微手势分类与情感预测性能；结合外观、姿态与面部-情境信息可更好捕捉细粒度人类行为线索。

Abstract: Micro-gesture recognition and behavior-based emotion prediction are both highly challenging tasks that require modeling subtle, fine-grained human behaviors, primarily leveraging video and skeletal pose data. In this work, we present two multimodal frameworks designed to tackle both problems on the iMiGUE dataset. For micro-gesture classification, we explore the complementary strengths of RGB and 3D pose-based representations to capture nuanced spatio-temporal patterns. To comprehensively represent gestures, video, and skeletal embeddings are extracted using MViTv2-S and 2s-AGCN, respectively. Then, they are integrated through a Cross-Modal Token Fusion module to combine spatial and pose information. For emotion recognition, our framework extends to behavior-based emotion prediction, a binary classification task identifying emotional states based on visual cues. We leverage facial and contextual embeddings extracted using SwinFace and MViTv2-S models and fuse them through an InterFusion module designed to capture emotional expressions and body gestures. Experiments conducted on the iMiGUE dataset, within the scope of the MiGA 2025 Challenge, demonstrate the robust performance and accuracy of our method in the behavior-based emotion prediction task, where our approach secured 2nd place.

</details>


### [138] [MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images](https://arxiv.org/abs/2512.23304)
*Md. Sazzadul Islam Prottasha,Nabil Walid Rafi*

Main category: cs.CV

TL;DR: 论文比较了专用开源多模态医疗模型MedGemma与专有GPT-4在六类疾病影像诊断上的表现，MedGemma经LoRA微调后平均准确率80.37%，优于未调参的GPT-4的69.58%，且在癌症、肺炎等高风险任务上灵敏度更高，强调领域微调可减少幻觉并提升证据驱动推理。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像中，多模态LLM可利用大规模临床知识进行解释与分类，但不同架构与是否进行领域微调对临床性能影响尚不明晰。作者希望评估专用开源模型与通用专有模型在实际诊断任务中的差异，特别是对高风险病种的敏感性与幻觉控制。

Method: 选取MedGemma-4b-it并通过LoRA进行领域微调；以未专门微调的GPT-4多模态作为对照。在六种疾病分类任务上进行测试，使用混淆矩阵与分类报告进行定量分析，比较准确率与敏感性等指标。

Result: MedGemma（经LoRA微调）取得平均测试准确率80.37%，显著优于未微调GPT-4的69.58%；在癌症、肺炎等高风险任务上表现出更高的敏感性。

Conclusion: 领域特定微调对临床应用至关重要，可减少幻觉并提升证据驱动的医学推理能力；MedGemma作为开源并经微调的模型在复杂医疗影像诊断中具有优势。

Abstract: Multimodal Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a transformative approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large multimodal model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.

</details>


### [139] [CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation](https://arxiv.org/abs/2512.23333)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Zhengtao Yao,Weitao Jia,Xiaodong Ge,Jingqun Tang,Benlei Cui,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 提出CME-CAD范式，用多专家协作+强化学习生成高精度、可编辑且满足约束的CAD代码，并发布含17299实例的CADExpert基准。


<details>
  <summary>Details</summary>
Motivation: 传统从草图/文本/图像生成3D模型方法常得到近似、不可编辑的结果，难以满足工业级精度与约束需求；且依赖重标注，难以规模化。因此需要能直接生成可执行、可编辑、满足几何/尺寸约束的CAD代码的方法与数据。

Method: 提出异构协作多专家强化学习（CME-CAD）：1）多专家微调（MEFT），整合不同模型/专家的互补能力与CoT过程；2）多专家强化学习（MERL），以约束兼容性、可编辑性、精度等为奖励信号，促进协作学习与代码质量提升；并构建CADExpert数据集，含正投影+精确尺寸标注、专家CoT、可执行CADQuery代码与渲染3D。

Result: CME-CAD能生成更准确、满足约束、完全可编辑的CAD模型，相较现有草图到3D方法在精度和可编辑性上有明显提升（摘要未给具体数值）。同时提供了17299样本的公开基准促进研究。

Conclusion: 通过多专家协同与强化学习，显著提升CAD代码生成的可编辑性与精度；CADExpert为社区提供统一数据和评测基础，推动工业级CAD自动化。

Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.

</details>


### [140] [Visual Language Hypothesis](https://arxiv.org/abs/2512.23335)
*Xiu Li*

Main category: cs.CV

TL;DR: 论文从结构与拓扑视角审视视觉表征：语义等价类形成商空间，干扰因素位于纤维，语义抽象需要非同胚的判别性监督与可发生拓扑变化的“扩张-折叠”表征机制。


<details>
  <summary>Details</summary>
Motivation: 解释为何大规模判别/多模态模型在有监督或对比信号下才能获得稳健的语义不变性，并用拓扑学框架统一“迁移性、抽象性与语义等价”的经验现象。

Method: 提出一个假设：视觉理解依赖离散语义语言，观测空间可视为纤维丛，纤维承载干扰变化，基空间为语义商空间。由此推导两点：1) 语义商X/G非X之子流形，需非同胚的判别性目标（标签、跨实例识别、多模态对齐）来实现语义不变；2) 近似商结构对模型架构提出要求——需要支持拓扑变化的“先扩张后收缩（expand-and-snap）”过程以形成离散语义区。

Result: 得到两条理论结论：仅靠平滑变形/自监督重构难以获得真正语义不变；实现语义抽象需外部语义信号与具备拓扑改变能力的表示机制，从而解释了大型判别与多模态模型的经验规律。

Conclusion: 语义抽象是拓扑意义上的商化过程，不能由连续同胚变换完成。有效的视觉表征学习应结合判别性监督/对齐信号与可执行“扩张-折叠”的架构，以匹配视觉语义的纤维丛结构。

Abstract: We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.

</details>


### [141] [CountGD++: Generalized Prompting for Open-World Counting](https://arxiv.org/abs/2512.23351)
*Niki Amini-Naieni,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出CountGD++与新型提示方式，通过文本/视觉指定要计数与不要计数的对象、伪示例自动标注、以及来自自然/合成外部图像的视觉示例输入，并作为LLM视觉专家代理，显著提升开放世界多模态计数的准确性、效率与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有自动计数方法在对象指定上受限：视觉示例需在图像内手动标注，且无法表达“不计哪些”。这限制了灵活性与准确性，尤其在开放世界、多模态场景中。

Method: 1) 扩展计数提示：允许用文本与/或视觉示例同时指定要计数与不要计数的对象；2) 提出“伪示例”机制：在推理时自动生成并标注视觉示例，降低人工标注；3) 扩展模型接口：接受来自自然或合成外部图像的视觉示例；4) 将新计数模型CountGD++作为LLM的视觉专家代理集成，实现代理式推理与控制。

Result: 在多数据集上取得显著的准确性、效率与泛化提升；新提示能力减少误计与漏计，伪示例在无需人工标注下维持或提升性能；作为LLM代理时进一步增强任务表现。

Conclusion: 通过更灵活的多模态提示（包含排除项）与伪示例自动化，CountGD++实现开放世界计数的精度、效率与可泛化性提升，并为LLM代理提供强有力的视觉计数能力。

Abstract: The flexibility and accuracy of methods for automatically counting objects in images and videos are limited by the way the object can be specified. While existing methods allow users to describe the target object with text and visual examples, the visual examples must be manually annotated inside the image, and there is no way to specify what not to count. To address these gaps, we introduce novel capabilities that expand how the target object can be specified. Specifically, we extend the prompt to enable what not to count to be described with text and/or visual examples, introduce the concept of `pseudo-exemplars' that automate the annotation of visual examples at inference, and extend counting models to accept visual examples from both natural and synthetic external images. We also use our new counting model, CountGD++, as a vision expert agent for an LLM. Together, these contributions expand the prompt flexibility of multi-modal open-world counting and lead to significant improvements in accuracy, efficiency, and generalization across multiple datasets. Code is available at https://github.com/niki-amini-naieni/CountGDPlusPlus.

</details>


### [142] [SpatialMosaic: A Multiview VLM Dataset for Partial Visibility](https://arxiv.org/abs/2512.23365)
*Kanghee Lee,Injae Lee,Minseok Kwak,Kwonyoung Ryu,Jungi Hong,Jaesik Park*

Main category: cs.CV

TL;DR: 提出SpatialMosaic：面向多视角空间推理的数据集（200万QA）、基准（100万QA/6任务）与模型框架（将3D重建作为几何编码器融入VLM），解决遮挡、可见性不全、低重叠等真实场景难题，显著提升MLLM在多视角空间推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM/ VLM在3D理解多依赖显式重建或离线管线，限制可扩展性；直接从多视图学空间推理虽有探索，但对真实环境中的部分可见、遮挡、低重叠等碎片化线索场景欠考虑，缺乏大规模训练与严格评测资源。

Method: 1) 构建可扩展的多视图数据生成与标注流水线，自动合成真实感强的空间推理QA；2) 发布SpatialMosaic数据集（200万QA）与SpatialMosaic-Bench基准（100万QA、6类任务），专注多视角、具挑战性的空间推理；3) 提出SpatialMosaicVLM：在VLM中引入3D重建模型作为几何编码器（hybrid框架），融合几何与语义进行鲁棒空间推理。

Result: 在多种严格、现实的多视角设置下，使用所提数据与任务训练/评测可显著提升空间推理性能；实验验证了数据生成流水线能构造真实且多样的QA对，并且混合几何编码器框架在困难条件下更稳健。

Conclusion: SpatialMosaic提供了成体系的数据、基准与模型方案，弥补了多视角空间推理在真实复杂场景中的训练与评测缺口；将3D重建作为几何先验融入VLM能有效增强多视角空间理解的鲁棒性。代码与数据集即将开源。

Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.

</details>


### [143] [MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning](https://arxiv.org/abs/2512.23369)
*Shuyuan Lin,Mengtin Lo,Haosheng Chen,Yanjie Liang,Qiangqiang Wu*

Main category: cs.CV

TL;DR: 提出MGCA-Net，通过上下文几何注意与跨阶段多图共识，提升两视图匹配的鲁棒性与精度，在YFCC100M与SUN3D上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有两视图对应学习方法在局部几何建模不足、跨阶段信息优化薄弱，难以准确刻画匹配对的几何约束，导致离群点剔除与位姿估计鲁棒性不够。

Method: 构建MGCA-Net，含两核心模块：1) CGA（Contextual Geometric Attention）：自适应注意力融合空间位置与外观特征，联合建模局部-全局几何关系；2) CSMGC（Cross-Stage Multi-Graph Consensus）：利用跨阶段稀疏图网络建立几何共识，在不同阶段间传递与校正几何信息，提升一致性与鲁棒性。

Result: 在YFCC100M与SUN3D数据集上，MGCA-Net在离群点剔除与相机位姿估计任务显著优于现有SOTA（文中宣称的定量结果）。

Conclusion: 融合上下文几何注意与跨阶段多图共识能更好地捕捉几何约束并保持阶段间一致性，从而显著提升两视图匹配、离群点剔除与位姿估计性能；代码已开源。

Abstract: Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.

</details>


### [144] [NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization](https://arxiv.org/abs/2512.23374)
*Yifei Li,Haoyuan He,Yu Zheng,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: NeXT-IMDL提出一个大规模、系统化诊断基准，用于更真实地评估图像篡改检测与定位(IMDL)模型的泛化能力，揭示现有方法在跨维度真实场景下的脆弱性与性能崩塌。


<details>
  <summary>Details</summary>
Motivation: 现有IMDL研究常用“跨数据集”评估，但这种简化做法掩盖了模型面对多样化AIGC编辑的脆弱性，造成进展被高估。需要一个能细粒度刻画不同编辑维度并严格测试泛化边界的基准。

Method: 构建NeXT-IMDL基准，将AIGC操纵沿四个基本轴分类：编辑模型、操纵类型、内容语义、伪造粒度；据此设计五种严格的跨维度评测协议，系统测试模型在不同泛化场景下的表现。对11个代表性模型进行大规模实验。

Result: 在各自原始设定中模型表现良好，但在NeXT-IMDL的跨维度协议下普遍出现系统性失败与显著性能下降，暴露出当前方法对真实多样场景的泛化不足。

Conclusion: NeXT-IMDL作为诊断工具揭示现有IMDL模型的泛化瓶颈，推动研究从“在单一基准上好看”转向“在真实多维变化中稳健”，为构建下一代鲁棒IMDL模型提供方向与评测标准。

Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.

</details>


### [145] [SoulX-LiveTalk Technical Report](https://arxiv.org/abs/2512.23379)
*Le Shen,Qiao Qian,Tan Yu,Ke Zhou,Tianhang Yu,Yu Zhan,Zhenjie Wang,Ming Tao,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: 提出SoulX-LiveTalk：14B扩散式头像生成系统，兼顾高保真与实时性。通过双向蒸馏保持块内双向注意力、引入多步回溯自纠错保证无限时长稳定，并配套全栈推理加速（混合序列并行、并行VAE、内核优化）。在保持视觉细节与动作连贯的同时，实现0.87秒启动时延与32 FPS实时吞吐。


<details>
  <summary>Details</summary>
Motivation: 现有实时音驱数字人生成在算力与低时延之间权衡：为满足流式推理常牺牲视觉保真（改为单向注意或缩小模型），导致动作连贯性差、细节损失；同时长时生成易累积误差崩溃。需要在大模型规模上同时满足高保真、低延迟、稳定的无限时长生成。

Method: 1) 自校正双向蒸馏：在视频块内保留双向注意，通过蒸馏兼顾流式需求与时空相关性，提升运动一致性与细节。2) 多步回溯自纠错：在线监测并回溯多步，自动修复累积误差，避免长时坍塌。3) 全栈加速：混合序列并行、并行VAE、内核级优化，实现大模型流式推理的吞吐与时延目标。

Result: 在14B参数规模下，实现0.87秒子秒级启动时延与32 FPS实时吞吐；相比单向/小模型方案，显著提升视觉细节与动作连贯性，并在无限时长生成中保持稳定。

Conclusion: SoulX-LiveTalk在不牺牲保真的情况下达成实时、稳定的音驱数字人生成，树立14B级扩散模型流式推理的新基准；关键在于块内双向注意的蒸馏、自纠错机制与系统级推理加速的协同。

Abstract: Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.

</details>


### [146] [SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation](https://arxiv.org/abs/2512.23411)
*Xiaolan Li,Wanquan Liu,Pengcheng Li,Pengyu Jie,Chenqiang Gao*

Main category: cs.CV

TL;DR: 提出SOFTooth：将冻结的2D SAM语义嵌入与3D点云特征融合，结合中心引导与顺序感知匹配，实现牙齿实例分割SOTA，尤其提升第三磨牙等少数类。


<details>
  <summary>Details</summary>
Motivation: 纯3D方法在拥挤牙弓、牙龈边界模糊、缺失牙与第三磨牙等复杂情况下易出现边界泄漏、中心漂移与ID不一致；直接把2D大模型用到3D临床流程不现实，但其边界感知语义很强，需要一种可迁移的2D→3D机制。

Method: 提出语义增强、顺序感知的2D-3D融合框架SOFTooth：1) 点级残差门控，将咬合视角的SAM特征注入3D点特征，细化牙龈与牙齿边界及牙间分隔；2) 中心引导的掩膜细化，约束实例掩膜与几何质心一致性，缓解中心漂移；3) 顺序感知的匈牙利匹配，将解剖学牙位顺序与中心距离融入相似度分配，在缺牙或拥挤情况下确保标签一致。无需显式2D掩膜监督与2D微调。

Result: 在3DTeethSeg'22上取得SOTA的整体准确率与mIoU，尤其在含第三磨牙案例上有显著提升。

Conclusion: 冻结的2D语义可有效转移至3D牙齿实例分割；通过语义注入、中心一致性与顺序匹配，可缓解边界泄漏、中心漂移与ID不一致，在临床复杂场景中更稳健。

Abstract: Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.

</details>


### [147] [Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment](https://arxiv.org/abs/2512.23413)
*Henglin Liu,Nisha Huang,Chang Liu,Jiangpeng Yan,Huijuan Huang,Jixuan Ying,Tong-Yee Lee,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出RAD数据集与ArtQuant框架，缓解审美评估中的数据稀缺/不均衡与模型碎片化问题；结合LLM进行长文本美学描述建模，并给出熵最小化的理论依据，SOTA且训练更省。


<details>
  <summary>Details</summary>
Motivation: AIGC需要与人类审美对齐的量化评估，但审美涵盖感知、认知、情感，难以建模；现有数据集昂贵且维度偏窄，方法上要么多分支割裂属性，要么多模态对比学习难处理长文本描述。

Method: 1) 用迭代生成-精炼流程构建70k规模、多维结构化的Refined Aesthetic Description（RAD）数据集，低成本可扩展；2) 提出ArtQuant：通过“联合描述生成”耦合各审美维度，采用LLM解码器更好建模长文本语义；3) 给出理论分析，说明数据语义充分性与生成范式共同最小化预测熵。

Result: 在多个数据集上达成SOTA，同时只需传统训练epoch的约33%。

Conclusion: RAD数据与ArtQuant模型相辅相成，缩小艺术图像与审美判断的认知差距；代码与数据将开源，促进后续研究。

Abstract: The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD's semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.

</details>


### [148] [DriveLaW:Unifying Planning and Video Generation in a Latent Driving World](https://arxiv.org/abs/2512.23421)
*Tianze Xia,Yongkang Li,Lijun Zhou,Jingfeng Yao,Kaixin Xiong,Haiyang Sun,Bing Wang,Kun Ma,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DriveLaW 将视频生成与轨迹规划统一起来：用视频生成器的潜在表征直接驱动扩散式规划器，在预测未来视频的一致潜在空间中生成可执行轨迹，分别在视频预测与规划基准上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型虽在统一框架下运行，但“世界预测”和“运动规划”仍然解耦，导致预测与规划不一致、长尾场景泛化受限。需要一个能在同一潜在空间中同时服务高保真未来生成与可靠规划的统一范式。

Method: 提出 DriveLaW：1) DriveLaW-Video 世界模型，学习表达力强的潜在表示并进行高保真视频预测；2) DriveLaW-Act 扩散式规划器，直接以 DriveLaW-Video 的潜在表征为条件生成一致的轨迹；3) 三阶段渐进式训练同时优化两者，使潜在表征既适合视频生成也适合规划。

Result: 在视频预测上，FID 提升33.3%，FVD 提升1.8%（相对最佳方法）；在规划上，于 NAVSIM 基准取得新的最优成绩，显示统一范式在两项任务上均带来提升。

Conclusion: 将世界模型的视频潜在表征无缝注入规划器，可在同一潜在空间中实现高保真未来预测与可靠轨迹规划，显著提升一致性与性能，为端到端自动驾驶提供更统一有效的方向。

Abstract: World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.

</details>


### [149] [Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision](https://arxiv.org/abs/2512.23426)
*Dohyun Kim,Seungwoo Lyu,Seung Wook Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出DDSPO：在扩散模型中直接在每个去噪时间步进行偏好优化，用更少监督提升文本对齐与美学质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的生成模型在文本到图像任务上表现出色，但难以细腻对齐用户意图并保持一致美学质量。现有偏好训练（如DDPO）依赖昂贵且噪声大的人工标注，且多只对最终样本施加信号，监督稀疏。

Method: 提出Direct Diffusion Score Preference Optimization（DDSPO）。当存在胜/负策略时，直接在每个时间步将它们的打分（score）差异作为监督，提供致密的过渡级信号。实用上，以一个预训练参考模型自动产生偏好：对同一提示与其语义退化版本分别生成图像，构造“胜者/败者”，避免人工标注与显式奖励建模。

Result: 实验证明DDSPO在文本图像对齐和视觉质量上优于或匹配现有偏好方法，同时监督成本显著降低。

Conclusion: 在扩散模型中进行分时步、得分空间的偏好优化可提升生成质量与对齐度，并以自动构造的偏好信号减少对人工标注和奖励模型的依赖。

Abstract: Diffusion models have achieved impressive results in generative tasks such as text-to-image synthesis, yet they often struggle to fully align outputs with nuanced user intent and maintain consistent aesthetic quality. Existing preference-based training methods like Diffusion Direct Preference Optimization help address these issues but rely on costly and potentially noisy human-labeled datasets. In this work, we introduce Direct Diffusion Score Preference Optimization (DDSPO), which directly derives per-timestep supervision from winning and losing policies when such policies are available. Unlike prior methods that operate solely on final samples, DDSPO provides dense, transition-level signals across the denoising trajectory. In practice, we avoid reliance on labeled data by automatically generating preference signals using a pretrained reference model: we contrast its outputs when conditioned on original prompts versus semantically degraded variants. This practical strategy enables effective score-space preference supervision without explicit reward modeling or manual annotations. Empirical results demonstrate that DDSPO improves text-image alignment and visual quality, outperforming or matching existing preference-based methods while requiring significantly less supervision. Our implementation is available at: https://dohyun-as.github.io/DDSPO

</details>


### [150] [Towards Integrating Uncertainty for Domain-Agnostic Segmentation](https://arxiv.org/abs/2512.23427)
*Jesse Brouwers,Xiaoyan Xing,Alexander Timans*

Main category: cs.CV

TL;DR: 提出UncertSAM基准，评估并利用不确定性提升SAM在复杂场景下的健壮性。


<details>
  <summary>Details</summary>
Motivation: SAM在零样本分割上很强，但遇到域移、知识受限（如阴影、透明、伪装等）时性能易退化。作者想验证：不确定性量化是否能以与任务和领域无关的方式缓解这些问题、提升可泛化性。

Method: (1) 构建UncertSAM基准：包含8个强调困难视觉条件的数据集；(2) 针对SAM应用一系列轻量、事后（post-hoc）的不确定性估计方法；(3) 设计初步的不确定性引导的预测后处理/细化步骤；(4) 重点方法为最后一层的拉普拉斯近似来估计参数不确定性并得到像素级不确定性。

Result: 最后一层拉普拉斯近似产生的不确定性与分割误差有较好相关性，能提供有意义的信号；不确定性引导的结果细化带来初步但有限的收益。

Conclusion: 在分割基础模型中显式引入不确定性有潜力提升跨域、与领域无关的鲁棒性；UncertSAM基准与代码公开，为后续研究提供评测与复现基础。

Abstract: Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.

</details>


### [151] [Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification](https://arxiv.org/abs/2512.23436)
*Mustafa Demetgul,Sanja Lazarova Molnar*

Main category: cs.CV

TL;DR: 提出一套结合车载相机与加速度数据的实时道路路面状态监测系统，评测多种CNN（AlexNet/LeNet/VGG/ResNet），在五类路面上达成>95%分类准确，并辅以模糊逻辑按天气/昼夜条件选择传感源。


<details>
  <summary>Details</summary>
Motivation: 传统道路状态监测成本高、测量周期长且不系统；自动驾驶与主动安全需要实时、低成本的路面感知。

Method: 用手机摄像头采集校园周边道路图像，并采集车辆加速度数据；将加速度序列映射成“图像”参与训练；比较AlexNet、LeNet、VGG、ResNet在五类路面（沥青、破损沥青、碎石、破损碎石、人行道/铺装）上的分类表现；提出基于天气与昼夜条件的模糊逻辑，用于在相机与加速度源之间进行传感器选择或融合。

Result: 相机与加速度两种途径均可用于分类；在五类路面任务上总体准确率超过95%；不同CNN在精度上有差异（未给出具体数值），相机在光照/天气不佳时可由加速度补充。

Conclusion: 多源数据+深度学习能实现低成本、实时的路面分类；模糊逻辑根据环境条件自适应选择传感器，提升鲁棒性。建议在更广域、更复杂环境与更详尽指标上进一步验证。

Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.

</details>


### [152] [RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction](https://arxiv.org/abs/2512.23437)
*Shuhong Liu,Chenyu Bao,Ziteng Cui,Yun Liu,Xuangeng Chu,Lin Gu,Marcos V. Conde,Ryo Umagami,Tomohiro Hashimoto,Zijian Hu,Tianhan Xu,Yuan Gan,Yusuke Kurose,Tatsuya Harada*

Main category: cs.CV

TL;DR: RealX3D 是一个面向真实场景的多视图视觉复原与三维重建基准，系统采集并对齐多种物理退化下的LQ/GT数据，并揭示现有方法在真实复杂退化中的显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有多视图复原与3D重建方法多在合成或受限退化上评测，难以反映真实世界复杂物理退化（光照、散射、遮挡、模糊）下的鲁棒性与上限。因此需要一个统一、可量化、可复现实验的真实采集基准来客观评估与推动方法发展。

Method: 提出 RealX3D：以统一采集协议在多种退化家族与多个强度等级下获取像素对齐的低质量/真值多视角；每个场景提供高分辨率、RAW图像与稠密激光扫描，据此构建世界尺度网格与度量深度。随后对比评测优化式与前馈式多种代表方法，量化退化对复原与重建质量的影响。

Result: 在各种物理退化条件下，广泛方法的三维重建质量显著下降，表明多视图流程在真实复杂环境中脆弱；基准提供了细粒度的跨退化、跨强度的对比结果。

Conclusion: 真实物理退化对当前多视图复原与重建管线影响巨大，现有方法鲁棒性不足。RealX3D 为研究社区提供了标准化、可扩展的数据与评测框架，促进更鲁棒方法的研发。

Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.

</details>


### [153] [CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models](https://arxiv.org/abs/2512.23453)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: 提出CoFi-Dec：一种无需训练的解码框架，通过自反馈与由粗到细的视觉条件融合，显著降低LVLM幻觉。


<details>
  <summary>Details</summary>
Motivation: LVLM尽管强大，但常生成与图像不一致的幻觉，影响可靠性；需要无需改动模型、可泛化的推理/解码方法来提升忠实度。

Method: 1) 粗到细视图：基于原图生成粗粒度与细粒度两个条件下的中间文本回答；2) 文转图：将这两段文本用文生图模型合成为多层级“视觉假设”；3) 多条件预测：在这些视觉条件下让LVLM产生分布；4) 基于Wasserstein的融合：将多条件预测分布在几何上一致化，形成稳定的解码轨迹，统一高层语义与细粒度对齐。全流程训练free、模型无关。

Result: 在6个以幻觉为重点的基准上，相比现有解码策略，显著降低实体级与语义级幻觉，鲁棒性与忠实度提升。

Conclusion: CoFi-Dec通过自反馈+多层视觉假设+Wasserstein融合，在不训练与模型无关的前提下，有效缓解LVLM幻觉，适用于广泛模型与场景。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.

</details>


### [154] [Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin](https://arxiv.org/abs/2512.23454)
*Kayathri Vigneswaran,Hugo Retief,Jai Clifford Holmes,Mariangel Garcia Andarcia,Hansaka Tennakoon*

Main category: cs.CV

TL;DR: 该研究提出一套将视觉水线检测、YOLOv8刻度间距估计与多模态大模型（GPT‑4o、Gemini 2.0 Flash）结合的自动化河道水尺读数框架，实验证明在优质图像下可达到厘米级误差与较高相关度，并强调几何元数据对LLM读数准确性的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统水位监测依赖人工或固定传感器，易受环境与人工误差影响，难以连续、精确、低成本地覆盖；需要一种可扩展、自动化且对环境鲁棒的图像化数字读数方案。

Method: 构建流水线：图像预处理与标注→基于视觉的水线检测→YOLOv8提取水尺刻度间距与几何标定→利用多模态大模型结合几何元数据进行数字读数；并评估不同图像质量与是否提供几何元数据对读数性能的影响。

Result: 水线检测精度94.24%，F1为83.64%；刻度间距检测为后续数值读取提供准确几何标定；引入刻度几何元数据显著提升LLM读数精度，其中Gemini阶段2在优质图像下达到MAE 5.43 cm、RMSE 8.58 cm、R²=0.84；图像质量下降会显著恶化LLM表现。

Conclusion: 融合视觉几何信息与多模态大模型能实现可扩展、效率高且可靠的自动水尺读数，对实时数字化水位监测与水资源管理具有潜力；鲁棒性依赖图像质量，几何元数据是关键增强要素。

Abstract: Accurate and continuous monitoring of river water levels is essential for flood forecasting, water resource management, and ecological protection. Traditional hydrological observation methods are often limited by manual measurement errors and environmental constraints. This study presents a hybrid framework integrating vision based waterline detection, YOLOv8 pose scale extraction, and large multimodal language models (GPT 4o and Gemini 2.0 Flash) for automated river gauge plate reading. The methodology involves sequential stages of image preprocessing, annotation, waterline detection, scale gap estimation, and numeric reading extraction. Experiments demonstrate that waterline detection achieved high precision of 94.24 percent and an F1 score of 83.64 percent, while scale gap detection provided accurate geometric calibration for subsequent reading extraction. Incorporating scale gap metadata substantially improved the predictive performance of LLMs, with Gemini Stage 2 achieving the highest accuracy, with a mean absolute error of 5.43 cm, root mean square error of 8.58 cm, and R squared of 0.84 under optimal image conditions. Results highlight the sensitivity of LLMs to image quality, with degraded images producing higher errors, and underscore the importance of combining geometric metadata with multimodal artificial intelligence for robust water level estimation. Overall, the proposed approach offers a scalable, efficient, and reliable solution for automated hydrological monitoring, demonstrating potential for real time river gauge digitization and improved water resource management.

</details>


### [155] [Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators](https://arxiv.org/abs/2512.23463)
*Bohan Xiao,Peiyong Wang,Qisheng He,Ming Dong*

Main category: cs.CV

TL;DR: 提出Dual-approx Bridge：基于布朗桥动力学并配对前/反向近似器的去噪生成模型，实现确定性I2I（含超分）中极低方差且高保真输出，显著优于现有随机和确定性基线。


<details>
  <summary>Details</summary>
Motivation: 传统扩散/随机I2I方法输出有方差，难以保证与GT一致；而确定性方法虽追求高保真，但可能受限于模型表达与收敛稳定性。作者希望借助布朗桥（起止分布已知、路径受约束）构造既稳定又接近GT的确定性生成过程，提升质量与一致性。

Method: 以布朗桥噪声过程连接输入与目标域，设计“去噪布朗桥”框架：同时训练两个神经网络近似器，分别拟合前向（从输入向目标注入噪声/演化）与反向（从噪声状态去噪到目标）动态；通过联合学习与一致性约束，减少采样方差，使推理时几乎无随机性，输出逼近GT。方法在图像生成与超分任务上评估。

Result: 在多个基准上，Dual-approx Bridge在图像质量指标（如感知质量、保真度）和与GT一致性上均优于随机与确定性基线；生成稳定、方差极小。

Conclusion: 利用布朗桥动力学与双近似器可实现几乎确定性的高保真I2I翻译，兼顾质量与一致性，提供优于现有方法的通用框架；代码与项目页公开以供复现与扩展。

Abstract: Image-to-Image (I2I) translation involves converting an image from one domain to another. Deterministic I2I translation, such as in image super-resolution, extends this concept by guaranteeing that each input generates a consistent and predictable output, closely matching the ground truth (GT) with high fidelity. In this paper, we propose a denoising Brownian bridge model with dual approximators (Dual-approx Bridge), a novel generative model that exploits the Brownian bridge dynamics and two neural network-based approximators (one for forward and one for reverse process) to produce faithful output with negligible variance and high image quality in I2I translations. Our extensive experiments on benchmark datasets including image generation and super-resolution demonstrate the consistent and superior performance of Dual-approx Bridge in terms of image quality and faithfulness to GT when compared to both stochastic and deterministic baselines. Project page and code: https://github.com/bohan95/dual-app-bridge

</details>


### [156] [HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation](https://arxiv.org/abs/2512.23464)
*Yuxin Wen,Qing Shuai,Di Kang,Jing Li,Cheng Wen,Yue Qian,Ningxin Jiao,Changhai Chen,Weijie Chen,Yiran Wang,Jinkun Guo,Dongyue An,Han Liu,Yanyu Tong,Chao Zhang,Qing Guo,Juan Chen,Qiao Zhang,Youyi Zhang,Zihao Yao,Cheng Zhang,Hong Duan,Xiaoping Wu,Qi Chen,Fei Cheng,Liang Dong,Peng He,Hao Zhang,Jiaxin Lin,Chao Zhang,Zhongyi Fan,Yifan Li,Zhichao Hu,Yuhong Liu,Linus,Jie Jiang,Xiaolong Li,Linchao Bao*

Main category: cs.CV

TL;DR: HY-Motion 1.0 是一套基于DiT与flow matching、扩展到10亿级参数的文本到3D人体动作生成模型，结合大规模预训练、精选微调与人类/奖励模型强化学习，实现指令对齐与高质量动作，覆盖200+类别并开源。


<details>
  <summary>Details</summary>
Motivation: 当前开源动作生成在指令跟随、可扩展性与数据清洗方面不足；缺少将DiT与flow matching在动作领域扩展到超大规模并实现可靠对齐的系统性范式，阻碍商业化落地。

Method: 提出完整训练范式：1) 3000+小时大规模预训练；2) 400小时高质精选数据微调；3) 基于人类反馈与奖励模型的强化学习对齐；配套严谨的数据处理流水线（清洗与描述生成/标注）；采用DiT式flow matching架构并扩展至十亿级参数。

Result: 在指令遵循与动作质量上显著优于现有开源基线；覆盖6大类、200+动作类别，展现SOTA的生成能力与广域覆盖。

Conclusion: HY-Motion 1.0 证明了将DiT+flow matching扩展至十亿参数并结合分阶段训练与RLHF可显著提升文本到动作生成的对齐度与质量；开源有望推动研究与商业化进程。

Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

</details>


### [157] [MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration](https://arxiv.org/abs/2512.23472)
*Shuyuan Lin,Wenwu Peng,Junjie Huang,Qiang Qi,Miaohui Wang,Jian Weng*

Main category: cs.CV

TL;DR: 提出MCI-Net，通过跨域上下文聚合提升点云配准鲁棒性与判别力，3DMatch上达96.4%注册召回。


<details>
  <summary>Details</summary>
Motivation: 现有深度方法多依赖欧氏邻域提特征，难以捕获点云的隐式语义与全局结构一致性，导致配准鲁棒性与判别性不足。

Method: 1) 图邻域聚合：构建全局图以刻画点云整体结构关系；2) 渐进式上下文交互：先域内特征解耦，再进行跨域上下文交互以增强判别性；3) 动态内点选择：利用多次位姿估计的残差信息动态优化内点权重，提升精度与稳健性。

Result: 在室内RGB-D与室外LiDAR数据上均显著优于SOTA；在3DMatch达到最高96.4%的注册召回。

Conclusion: 跨域上下文整合与动态内点加权能有效提升点云特征表征与配准性能，方法在多场景下稳定且精度高，并具有开源实现。

Abstract: Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\% on 3DMatch. Source code is available at http://www.linshuyuan.com.

</details>


### [158] [SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context](https://arxiv.org/abs/2512.23473)
*Shuyuan Lin,Hailiang Liao,Qiang Qi,Junjie Huang,Taotao Lai,Jian Weng*

Main category: cs.CV

TL;DR: 提出SC-Net，通过空间与通道双向上下文整合，缓解CNN在大视差场景中的全局建模不足与过平滑问题，并在YFCC100M与SUN3D上于位姿估计与离群点剔除任务优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 通用CNN骨干用于两视图对应学习时，难以充分聚合全局上下文，且在大视差场景中易导致稠密运动场过平滑，从而影响相对位姿估计与鲁棒匹配质量。

Method: 设计SC-Net，包含三大模块：1) 自适应聚焦正则化（AFR）增强位置感知与对伪运动样本的鲁棒性，生成更可靠的初始运动场；2) 双向（空间-通道）场调整（BFA）同时建模长程依赖并在空间与通道间交互，细化运动场；3) 位置感知恢复（PAR）从细化场中恢复运动向量，保证一致性与精度。

Result: 在YFCC100M与SUN3D上进行相对位姿估计与离群点剔除实验，SC-Net整体性能优于现有最先进方法（未给出具体数值）。

Conclusion: 通过空间与通道双向上下文整合与位置感知建模，SC-Net有效克服CNN骨干全局建模不足与过平滑问题，提升两视图对应学习中的运动场质量与下游任务表现。

Abstract: Recent research has focused on using convolutional neural networks (CNNs) as the backbones in two-view correspondence learning, demonstrating significant superiority over methods based on multilayer perceptrons. However, CNN backbones that are not tailored to specific tasks may fail to effectively aggregate global context and oversmooth dense motion fields in scenes with large disparity. To address these problems, we propose a novel network named SC-Net, which effectively integrates bilateral context from both spatial and channel perspectives. Specifically, we design an adaptive focused regularization module (AFR) to enhance the model's position-awareness and robustness against spurious motion samples, thereby facilitating the generation of a more accurate motion field. We then propose a bilateral field adjustment module (BFA) to refine the motion field by simultaneously modeling long-range relationships and facilitating interaction across spatial and channel dimensions. Finally, we recover the motion vectors from the refined field using a position-aware recovery module (PAR) that ensures consistency and precision. Extensive experiments demonstrate that SC-Net outperforms state-of-the-art methods in relative pose estimation and outlier removal tasks on YFCC100M and SUN3D datasets. Source code is available at http://www.linshuyuan.com.

</details>


### [159] [TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding](https://arxiv.org/abs/2512.23483)
*Zongsheng Cao,Yangfan He,Anran Liu,Feng Chen,Zepeng Wang,Jun Xie*

Main category: cs.CV

TL;DR: TV-RAG是一种无需训练的长视频检索与推理框架，通过时间衰减检索与基于熵的关键帧采样，解决LVLM在长时序理解与多模态对齐上的不足，在多个长视频基准上优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型视频语言模型面对长视频时：时序可见范围有限，难以捕捉跨长时间的细粒度语义变化；主流文本检索依赖词面重合，忽视画面、音频、字幕间的时间依赖与语义互补，导致检索与推理不稳健。

Method: 提出无需训练的TV-RAG架构，包含两大机制：（1）时间衰减检索模块：在相似度计算中显式注入时间偏移，对文本查询依据真实多模态上下文进行排序；（2）熵加权关键帧采样：按时间均匀但以信息熵加权选取信息密集帧，降低冗余、保留代表性。将时序与语义信号编织为可插拔的双层推理流程，可直接嫁接到任意LVLM。

Result: 在Video-MME、MLVU、LongVideoBench等长视频基准上，TV-RAG在多项指标上持续超越多数主流基线，表现稳定且成本低。

Conclusion: TV-RAG通过时间对齐与熵引导语义选帧，显著增强LVLM的长视频理解与检索能力，提供轻量、经济、无需再训练的升级路径，并在标准基准上验证其有效性。

Abstract: Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.

</details>


### [160] [Multi-label Classification with Panoptic Context Aggregation Networks](https://arxiv.org/abs/2512.23486)
*Mingyuan Jiu,Hailong Zhu,Wenchuan Wei,Hichem Sahbi,Rongrong Ji,Mingliang Xu*

Main category: cs.CV

TL;DR: 提出PanCAN：一种通过跨尺度特征聚合与多阶几何上下文建模来提升多标签图像分类的方法，基于随机游走与注意力机制层级整合不同尺度的邻域关系，跨数据集取得SOTA或有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉上下文方法多局限于基本几何关系或局部特征，忽视对象间跨尺度、跨阶的上下文交互，导致对复杂场景理解不足，难以学习高判别力的图像表示。

Method: 构建Deep Panoptic Context Aggregation Network（PanCAN）：在高维Hilbert空间中进行跨尺度特征聚合。每个尺度上用“随机游走+注意力”学习多阶邻域关系；多尺度模块级联，将细粒度尺度的显著锚点选出，并通过注意力动态融合其邻域特征，实现多阶与跨尺度上下文的联合建模。

Result: 在NUS-WIDE、PASCAL VOC2007、MS-COCO多标签分类上进行广泛实验，定量与定性评估均显示优于或匹配当下SOTA方法，显著提升多标签分类性能。

Conclusion: 通过将随机游走与注意力结合实现的多阶、跨尺度上下文融合，PanCAN在复杂场景理解与多标签分类中效果显著，证明跨尺度上下文建模对视觉识别的重要性。

Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.

</details>


### [161] [IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation](https://arxiv.org/abs/2512.23519)
*Donghao Zhou,Jingyu Lin,Guibao Shen,Quande Liu,Jialin Gao,Lihao Liu,Lan Du,Cunjian Chen,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出IdentityStory框架，在人类角色为中心的多图故事生成中，实现跨图像的人脸与角色一致性，并支持多角色协同与无限长度扩展，实验在ConsiStory-Human上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的故事生成虽能保持角色一致，但在人类角色场景中，细粒度的人脸特征稳定、跨多图多角色协同以及可扩展性仍不足。需要一种方法在复杂人脸细节、多角色交互和长序列生成中维持身份一致性。

Method: “驯化”具身份保持能力的生成器，提出两大组件：1) 迭代身份发现（Iterative Identity Discovery），从序列中提取并聚合连贯的角色身份表示；2) 重新去噪身份注入（Re-denoising Identity Injection），在保证场景语义与上下文的前提下，通过再去噪过程将已发现的身份嵌入到各帧图像中，实现稳定的人脸/身份一致性，并支持多角色组合。

Result: 在ConsiStory-Human基准上，IdentityStory在脸部一致性等指标显著领先现有方法，能稳定处理多角色组合与跨图像一致性；展示了对无限长度故事生成与动态角色组合的潜力。

Conclusion: IdentityStory有效解决人类中心故事生成中的身份一致性与多角色协调难题。通过迭代身份抽取与再去噪式身份注入，方法在基准上取得领先，并具备良好的扩展性与应用前景（如长序列生成、角色灵活编排）。

Abstract: Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.

</details>


### [162] [Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution](https://arxiv.org/abs/2512.23532)
*Hexin Zhang,Dong Li,Jie Huang,Bingzhou Wang,Xueyang Fu,Zhengjun Zha*

Main category: cs.CV

TL;DR: 提出IAFS，一种无需训练的扩散超分辨率推理时缩放框架，通过迭代校正结构与自适应频率融合，同时提升感知细节与结构保真，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式超分方法在感知高频细节与低频结构保真之间难以兼顾；推理时扩展计算虽可在理论上改善，但现有奖励驱动的粒子优化会过度平滑，最优路径搜索又损失结构一致性，因而需要新的推理时策略。

Method: 提出IAFS（Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering）：1) 迭代式精炼，逐步检测并纠正与结构相关的偏差，实现结构一致性；2) 频率感知的粒子融合，自适应地将高频感知线索与低频结构信息融合，以实现更平衡的重建；3) 完全训练自由，可直接用于多种基于扩散的SR模型的推理阶段。

Result: 在多种扩散式SR模型和数据集上进行大量实验，IAFS在感知与保真两方面同时提升，缓解了二者的冲突，并稳定优于现有推理时缩放方法。

Conclusion: IAFS通过迭代校正和自适应频率融合，在不改动训练的前提下有效平衡感知质量与结构保真，为扩散式SR的推理时提升提供了通用且更优的方案。

Abstract: Diffusion models have become a leading paradigm for image super-resolution (SR), but existing methods struggle to guarantee both the high-frequency perceptual quality and the low-frequency structural fidelity of generated images. Although inference-time scaling can theoretically improve this trade-off by allocating more computation, existing strategies remain suboptimal: reward-driven particle optimization often causes perceptual over-smoothing, while optimal-path search tends to lose structural consistency. To overcome these difficulties, we propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free framework that jointly leverages iterative refinement and frequency-aware particle fusion. IAFS addresses the challenge of balancing perceptual quality and structural fidelity by progressively refining the generated image through iterative correction of structural deviations. Simultaneously, it ensures effective frequency fusion by adaptively integrating high-frequency perceptual cues with low-frequency structural information, allowing for a more accurate and balanced reconstruction across different image details. Extensive experiments across multiple diffusion-based SR models show that IAFS effectively resolves the perception-fidelity conflict, yielding consistently improved perceptual detail and structural accuracy, and outperforming existing inference-time scaling methods.

</details>


### [163] [AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization](https://arxiv.org/abs/2512.23537)
*Binhe Yu,Zhen Wang,Kexin Li,Yuqian Yuan,Wenqiao Zhang,Long Chen,Juncheng Li,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: AnyMS提出一种无需训练的多主体定制生成框架，通过文本、主体图像与布局三种条件，并行采用全局/局部双层注意力解耦与预训练图像适配器，实现文本对齐、身份保持与布局控制的兼顾，达到SOTA并可扩展到更多主体与复杂组合。


<details>
  <summary>Details</summary>
Motivation: 现有布局引导的多主体生成在三目标（文本对齐、身份保持、布局控制）之间难以权衡，易出现主体缺失/冲突；且依赖额外训练或适配器微调，降低可扩展性与效率。需要一种训练-free的方法在统一框架内稳健融合三种条件并提升可扩展性。

Method: 提出AnyMS：输入文本+主体图像+显式布局。核心是自底向上的“双层注意力解耦”：1) 全局解耦，将跨注意力中来自文本与视觉条件的通道分离，避免相互干扰以保证文本对齐；2) 局部解耦，将每个主体的注意力限制在其布局区域内，避免主体混淆与位置冲突，强化身份与布局约束。同时使用预训练图像适配器抽取与扩散模型对齐的主体特征，无需主体学习或适配器调参。

Result: 在多主体合成任务上实现SOTA，能处理复杂组合并可扩展至更多主体；在身份一致性、布局遵循与文本对齐方面优于现有方法。

Conclusion: 无需训练的AnyMS通过全局/局部注意力解耦与预训练适配器高效融合多条件，在三目标间取得平衡，兼具性能与可扩展性，适用于复杂、多主体的布局引导合成。

Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.

</details>


### [164] [PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis](https://arxiv.org/abs/2512.23545)
*Shengyi Hua,Jianfeng Wu,Tianle Shen,Kangzhe Hu,Zhongzhen Huang,Shujuan Ni,Zhihong Zhang,Yuan Li,Zhe Wang,Xiaofan Zhang*

Main category: cs.CV

TL;DR: 提出PathFound，一个面向病理诊断的“能动型”多模态模型，通过分阶段的证据寻访（初步诊断→取证→终判）动态更新预测，较静态一次性推理显著提高诊断准确率并达SOTA，能发现细微核特征与局部侵袭。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型多采用一次性静态推理，无法在诊断不确定时主动回看切片、定向获取更多证据；这与临床反复观察、追加检查的流程不一致，限制了模型在模糊病例上的可靠性与可解释性。

Method: 提出PathFound：将病理视觉基础模型、视觉-语言模型与基于强化学习训练的推理/决策模块集成，设计“初步诊断—证据寻访—最终决策”三阶段代理式工作流；模型可在WSI上主动选择关注区域、发起信息获取动作，并基于新证据迭代更新诊断。该策略可附着于多种大模型进行统一训练与推理。

Result: 在多种大规模多模态模型上应用该证据寻访策略均带来一致的诊断准确率提升；PathFound在多种临床场景中达成SOTA，并显示出对细微核学特征与局部浸润等难点证据的强检测与利用能力。

Conclusion: 将临床式的“主动取证—迭代推理”引入计算病理能显著增强诊断性能与细粒度证据发现能力；PathFound验证了该范式的有效性，并为病理AI提供通用、可扩展的能动推理框架。

Abstract: Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.

</details>


### [165] [PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation](https://arxiv.org/abs/2512.23546)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: PurifyGen是一种无需训练、保持原模型权重不变的文本到图像安全生成方法，通过对提示词进行细粒度“净化”，显著降低不安全内容生成，同时尽量保留原意与画质。


<details>
  <summary>Details</summary>
Motivation: 现有T2I安全手段如黑名单匹配和有害分类要么易被绕过、要么依赖大规模标注与额外训练，泛化性与可维护性差。需要一种轻量、可插拔、具理论依据且对模型与数据零侵入的安全方案。

Method: 提出双阶段提示净化：1) 安全评估：计算提示中每个token相对“毒性/清洁”概念嵌入的互补语义距离，实现无需显式关键词匹配的细粒度风险识别；2) 双空间变换：对被判定为风险的token嵌入，投影到毒性概念矩阵的零空间以移除有害语义，同时对齐到清洁概念的值域空间以强化安全语义；并采用逐token替换策略，仅替换风险token，减小对无害内容的扰动。方法为训练自由、保留原权重、可即插即用。

Result: 在五个数据集上，PurifyGen显著降低不安全内容生成，优于当前无训练方法，并与需训练的方法表现相当或更好，且对未见提示与不同模型具有良好泛化。

Conclusion: PurifyGen通过基于概念嵌入的风险评估与双空间投影，实现对提示中有害语义的定向消除与安全语义增强，在无需训练和不改动权重的前提下提升T2I安全性与可用性，具备即插即用与跨模型迁移优势。

Abstract: Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.

</details>


### [166] [RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature](https://arxiv.org/abs/2512.23565)
*Hanzheng Li,Xi Fang,Yixuan Li,Chaozheng Huang,Junjie Wang,Xi Wang,Hongzhe Bai,Bojun Hao,Shenyu Lin,Huiqi Liang,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: 提出RxnBench，一个评估MLLM在化学反应理解上的基准，涵盖图像级与全文级问答，发现模型能抓文本却难以做深层化学推理与结构识别，推理增强模型更强但全文任务准确率不足50%，呼吁更好的视觉编码与推理引擎。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在通用视觉语言任务上表现突出，但化学文献中的反应示意图、机制箭头、条件标注等图形化语言密集且专业，模型对真实PDF中的跨模态理解能力缺乏系统评估；需要一个标准化基准揭示能力短板并推动面向化学的MLLM发展。

Method: 构建多层级评测RxnBench：1) 单图问答（SF-QA），从305个精挑反应示意图中抽取1,525个细粒度问题，考察视觉识别与机理推理；2) 全文问答（FD-QA），覆盖108篇论文，需要综合正文、反应图、表格进行跨模态信息整合。对多种MLLM进行系统评测，并比较是否采用推理时增强（inference-time reasoning）的差异。

Result: MLLM普遍能提取显式文本信息，但在精确结构识别、反应机理逻辑与跨图表整合上表现不佳；采用推理时增强的模型显著优于标准架构，但在FD-QA任务上无一超过50%准确率，显示明显能力缺口。

Conclusion: 当前通用MLLM不足以胜任真实化学文献的反应理解；需要面向化学的领域特化视觉编码器与更强的推理引擎，以推动自治AI化学家的实现。

Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.

</details>


### [167] [ThinkGen: Generalized Thinking for Visual Generation](https://arxiv.org/abs/2512.23568)
*Siyu Jiao,Yiheng Lin,Yujie Zhong,Qi She,Wei Zhou,Xiaohan Lan,Zilong Huang,Fei Yu,Yingchen Yu,Yunqing Zhao,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: ThinkGen 提出一个“先思考再生成”的多模态图像生成框架：用MLLM做CoT思维分解生成精细指令，再由DiT据此合成高质图像，并通过可分离的GRPO强化学习交替优化两模块，在多数据与多场景下取得SOTA。


<details>
  <summary>Details</summary>
Motivation: CoT在理解任务上已被证实有效，但在图像生成等生成任务中的泛化与适配性不足，现有方法多为场景定制，难以统一扩展。需要一种通用机制把MLLM的推理优势转化为可控、可泛化的生成能力。

Method: 提出ThinkGen的解耦框架：预训练MLLM负责根据用户意图进行CoT推理与细化指令；Diffusion Transformer(DiT)依据这些“思维驱动”的指令生成图像。训练上提出SepGRPO：对MLLM与DiT采用可分离、交替的GRPO强化学习范式，可在多样数据上联合训练，强化CoT对不同生成场景的指导作用。

Result: 在多个图像生成基准上达到稳健的SOTA表现，显示出跨场景泛化与鲁棒性；实验证明利用CoT生成的中间指令可以显著提升生成质量和任务对齐度。

Conclusion: 将MLLM的CoT显式引入图像生成，通过解耦架构与可分离RL训练，实现对多种生成场景的统一建模与性能提升，提供了可扩展的“思考驱动”生成范式。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen

</details>


### [168] [Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2512.23569)
*Zhaoming Kong,Xiaowei Yang,Jiahuan Zhang*

Main category: cs.CV

TL;DR: 提出一种结合Haar变换与张量SVD（t-SVD）的高效图像去噪方法Haar-tSVD，建立了PCA与Haar变换在循环结构下的理论联系，具备一步并行、无需学习局部字典、速度与性能兼顾，并可与深度网络融合在高噪声下进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 海量成像设备带来的图像数据使高效、有效的去噪需求凸显；现有方法在速度、鲁棒性和对局部/全局相关性的兼顾方面存在权衡，亟需一种无需复杂学习、可并行且稳健的去噪框架。

Method: 从理论上把PCA与在循环表示下的Haar变换建立联系；提出统一的t-SVD投影框架并结合Haar变换以同时捕获全局与局部补丁相关性；设计一步式、可并行的plug-and-play去噪流程，无需学习局部基；基于循环结构特征值分析引入自适应噪声估计；在高噪声情形下，根据Haar-PCA关系将深度神经网络与Haar-tSVD融合。

Result: 在多种去噪数据集上的实验表明，该方法在去噪效率与效果上表现良好，验证了其快速性、有效性和鲁棒性。

Conclusion: Haar-tSVD通过Haar变换与t-SVD的统一框架高效建模图像相关性，在无需学习局部字典的前提下实现一步并行去噪，并借助自适应噪声估计与深度融合在强噪声下进一步提升性能。

Abstract: The proliferation of imaging devices and countless image data generated every day impose an increasingly high demand on efficient and effective image denoising. In this paper, we establish a theoretical connection between principal component analysis (PCA) and the Haar transform under circulant representation, and present a computationally simple denoising algorithm. The proposed method, termed Haar-tSVD, exploits a unified tensor singular value decomposition (t-SVD) projection combined with Haar transform to efficiently capture global and local patch correlations. Haar-tSVD operates as a one-step, parallelizable plug-and-play denoiser that eliminates the need for learning local bases, thereby striking a balance between denoising speed and performance. Besides, an adaptive noise estimation scheme is introduced to improve robustness according to eigenvalue analysis of the circulant structure. To further enhance the performance under severe noise conditions, we integrate deep neural networks with Haar-tSVD based on the established Haar-PCA relationship. Experimental results on various denoising datasets demonstrate the efficiency and effectiveness of proposed method for noise removal. Our code is publicly available at https://github.com/ZhaomingKong/Haar-tSVD.

</details>


### [169] [ProGuard: Towards Proactive Multimodal Safeguard](https://arxiv.org/abs/2512.23573)
*Shaohan Yu,Lijun Li,Chenyang Si,Lu Sheng,Jing Shao*

Main category: cs.CV

TL;DR: ProGuard 是一个无需改模型、以强化学习训练的多模态前摄式安全守卫，能在文本、图像及图文输入上识别并简述分布外（OOD）安全风险，在二分类与风险细粒度归类上优于现有开源方案，并显著提升 OOD 检测与描述能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态安全防护多为“反应式”，需改模型或针对已知风险，难以覆盖快速涌现的分布外风险；同时存在模态偏置与跨模态一致性差的问题。

Method: 1) 构建含87K样本的多模态均衡数据集，给出二分类与基于分层多模态安全分类法的风险类别标注；2) 以纯强化学习训练视觉-语言基座模型，目标为高效、简洁的推理与描述；3) 设计近似前摄场景的 OOD 安全类别推断任务；4) 在RL中加入基于同义词库的相似度奖励，鼓励对未见不安全类别生成简洁准确的描述。

Result: 在安全二分类上可比闭源大模型；在不安全内容细粒度分类上显著优于开源守卫；在前摄能力上，OOD 风险检测提升52.6%，OOD 风险描述提升64.8%。

Conclusion: 通过数据集与RL目标的联合设计，ProGuard 无需模型改动即可实现跨模态一致、对分布外风险敏感的前摄式安全审核，在检测与描述 OOD 风险方面展现了强竞争力。

Abstract: The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.

</details>


### [170] [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/abs/2512.23576)
*Ethan Chern,Zhulin Hu,Bohao Tang,Jiadi Su,Steffi Chern,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 提出一种面向多模态条件（文本/图像/音频）的实时交互视频扩散生成方法，通过改进在策略蒸馏流程与条件质量控制，实现与全步双向扩散相当画质、但20倍更低时延与推理成本，并集成音频语言模型与长视频技术构建LiveTalk系统，在多轮交互一致性与质量上优于Sora2/Veo3，实现真正实时的人机多模态互动。


<details>
  <summary>Details</summary>
Motivation: 传统扩散视频生成需对全序列进行双向注意力迭代去噪，导致高时延，难以实时交互；现有蒸馏多集中于文生视频，缺乏对多模态条件与交互场景的优化，且在on-policy蒸馏下出现闪烁、黑帧、质量下降等问题。目标是实现对文本/图像/音频条件的低延时、高质量实时视频生成并支撑多轮交互。

Method: 以自回归蒸馏为核心，针对Self Forcing在多模态条件下的失效点，改进：1) 条件输入质量控制与增强（音频、图像、文本对齐与清洗）；2) on-policy优化的初始化与训练日程（schedule）设计，缓解训练漂移与累积误差；3) 将模型蒸馏为可逐帧/短片段前向、低步数采样的单向生成器。随后与音频语言模型和Anchor-Heavy Identity Sinks长视频推理技术集成，形成LiveTalk系统。

Result: 在HDTF、AVSpeech、CelebV-HQ等多模态头像视频基准上，蒸馏模型以约20倍更低推理成本与时延达到与同等或更大规模全步双向扩散基线相当的视觉质量。系统级评测表明，LiveTalk在多轮视频连贯性与内容质量上优于Sora2与Veo3，并将响应时延从1–2分钟降低到实时生成。

Conclusion: 通过改进在策略蒸馏与条件质量管理，可将双向扩散转化为高质量、低延迟的自回归视频生成器，支持多模态条件与多轮交互。所提出的LiveTalk系统实现了实时、连贯的人机多模态视频互动，并在基准与系统评测中达到或超越现有SOTA。

Abstract: Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.

</details>


### [171] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://arxiv.org/abs/2512.23592)
*Damiano Marsili,Aditya Mehta,Ryan Y. Lin,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 他们提出TWIN：一个包含56.1万对相似图像配对的训练语料，要求模型判断两图是否为同一对象，用于提升VLM的细粒度感知；并推出FGVQA基准（1.2万条）评估跨域细粒度问答/检索。用TWIN微调后，在FGVQA上最高提升19.3%，且不损伤通用VQA表现；数据规模与标注扩展带来持续收益。


<details>
  <summary>Details</summary>
Motivation: 现有VLM侧重粗粒度类别识别，易受视觉偏差影响，忽视微小差异；训练语料也偏重“猫还是狗”式的泛化识别，缺乏逼迫模型关注细节的任务与数据。

Method: 构建TWIN：跨场景、视角、外观的相似图像对，任务是判断是否为同一对象，迫使模型捕捉细微线索；将多域细粒度识别/检索数据重构为FGVQA评测套件；对现有开源VLM进行TWIN微调并在FGVQA与常规VQA上评估，同时做数据规模与标注量化分析。

Result: 在FGVQA上，未微调的VLM表现不佳；经TWIN微调后，细粒度识别能力显著提升，最高提高19.3%；在通用VQA基准上无显著性能下降；扩展TWIN规模与对象标注带来单调提升。

Conclusion: 细粒度、成对判别任务的数据（TWIN）可显著增强VLM的感知精度且不牺牲通用能力；规模是关键。TWIN适合作为开源VLM训练语料的可插拔补充，推动未来模型的细节感知。

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/

</details>


### [172] [Detection Fire in Camera RGB-NIR](https://arxiv.org/abs/2512.23594)
*Nguyen Truong Khai,Luong Duc Vinh*

Main category: cs.CV

TL;DR: 提出NIR数据集+两阶段检测+Patched-YOLO，显著降低夜间红外误检并提升小目标RGB火焰检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有红外夜视火灾检测虽有较高mAP，但数据集构建不足，明亮人造光常被误判为火源，夜间场景鲁棒性差，小远目标检测不佳。

Method: 1) 扩充并增强NIR与分类数据集；2) 两阶段管线：YOLOv11负责候选检测，EfficientNetV2-B0做二次分类，专门抑制人造光误报；3) 提出Patched-YOLO：对RGB图像进行基于patch的处理以强化小/远目标检出。

Result: 两阶段YOLOv11+EfficientNetV2-B0在夜间火焰检测上优于既有方法（相较YOLOv7/RT-DETR/YOLOv9等），显著降低人造光误检；Patched-YOLO在RGB小目标场景中提升检测性能。

Conclusion: 通过数据集增强、两阶段检测与patch化策略，提升夜间红外与RGB火焰检测的准确性与鲁棒性，尤其减少夜间人造光误报并强化小目标识别。

Abstract: Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.
  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.

</details>


### [173] [Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging](https://arxiv.org/abs/2512.23597)
*Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi*

Main category: cs.CV

TL;DR: 提出一套可扩展的残差特征聚合（SRFA）框架，用于CT多模态数据上胰腺肿瘤早期检测，在分割、特征提取、特征选择、分类与超参优化多环节叠加改进，报告>96%准确率，优于传统CNN与Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤在CT上对比度低、边界模糊、个体间解剖差异大，导致早期检测困难；需要一个既能增强细微线索显著性、又能在多模态数据上具备良好泛化能力的端到端系统。

Method: 提出SRFA框架：1) 预处理+MAGRes-UNet进行胰腺结构增强与ROI分割；2) 以带残差特征存储的DenseNet-121进行多层级深度特征提取与聚合；3) 采用混合HHO-BA元启发式进行特征选择；4) 分类器为ViT与EfficientNet-B3的混合模型以兼顾全局注意与高效表征；5) 用SSA与GWO双重优化机制调参以提升鲁棒性并抑制过拟合。

Result: 在实验中取得96.23%准确率、95.58% F1、94.83%特异性，显著优于传统CNN与当代Transformer基线。

Conclusion: SRFA在胰腺肿瘤早期检测任务上表现出色，能增强细微病灶可辨性并具备较强泛化能力，显示为临床早筛的有前景工具。

Abstract: The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.

</details>


### [174] [Memorization in 3D Shape Generation: An Empirical Study](https://arxiv.org/abs/2512.23628)
*Shu Pu,Boya Zeng,Kaichen Zhou,Mengyu Wang,Zhuang Liu*

Main category: cs.CV

TL;DR: 本文提出用于量化3D生成模型“背诵/记忆”训练数据的评估框架，并用其系统分析数据与模型设计如何影响记忆；结果揭示若干导致或缓解记忆的关键因素，并给出简单有效的减记忆策略而不损害生成质量。


<details>
  <summary>Details</summary>
Motivation: 3D生成模型在形状合成上取得进展，但其是否、以及在何种程度上“记忆”训练形状仍不清楚。缺乏对记忆的量化理解会带来隐私/数据泄露风险，并限制生成多样性与泛化能力。因此需要一个系统的方法来度量与分析3D生成模型的记忆行为。

Method: 提出一套评估框架，能够量化3D生成模型的记忆程度；先对现有方法进行测评；再以一个可控的Vecset（向量集合）扩散模型为试验平台，通过控制变量实验系统研究数据模态、数据多样性、条件粒度、指导尺度(guidance scale)、Vecset长度、旋转增强等因素对记忆的影响。

Result: （1）数据侧：记忆程度随数据模态不同而变化，数据多样性越高、条件越细粒度，记忆越强。（2）模型侧：记忆在中等指导尺度达到峰值；延长Vecset长度与加入简单的旋转增强可显著降低记忆。（3）框架成功衡量了多种现有3D生成方法的记忆水平。

Conclusion: 该工作提供了对3D生成模型记忆行为的实证理解，并提出无需牺牲生成质量即可缓解记忆的简易策略（如更长Vecset与旋转增强）。代码公开，有助于后续研究与更安全多样的3D生成。

Abstract: Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.

</details>


### [175] [Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception](https://arxiv.org/abs/2512.23635)
*Xiaoyu Li,Peidong Li,Xian Wu,Long Shi,Dedong Liu,Yitao Wu,Jiajia Fu,Dixiao Cui,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: 提出HAT模块：为E2E自动驾驶时空对齐提供多假设自适应解码，无显式监督；在nuScenes上显著提升检测与跟踪，并增强鲁棒性与规划安全。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的跨帧对齐常假设统一的简化物理运动模型（如匀速）并偏重语义特征，导致不同类别/状态/帧间运动与外观变化下对齐次优；需要一种能结合多种显式运动先验与语义信息、并让每个目标自适应选择最佳对齐的机制。

Method: 提出HAT（Hypothesis-Aware spatio-Temporal alignment）：1）为历史目标用多种显式运动模型生成空间锚点与含运动信息的特征候选；2）利用缓存的目标查询中语义与运动线索进行多假设解码，从候选中自适应选择最优对齐提案，无需额外监督；3）与3D检测/跟踪器或E2E AD方法模块化集成。

Result: 在nuScenes上对多种3D时序检测与跟踪基线稳定增益；与DETR3D结合在测试集达到46.0% AMOTA的SOTA；在对象中心E2E AD中提升+1.3% mAP、+3.1% AMOTA，并将碰撞率降低32%；在语义扰动数据集nuScenes-C上表现更稳健。

Conclusion: 显式多假设运动建模与语义联合的自适应对齐优于仅靠注意力/单一物理模型，可广泛增强E2E感知与规划的准确性与鲁棒性。

Abstract: Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.

</details>


### [176] [OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding](https://arxiv.org/abs/2512.23646)
*Keda Tao,Wenjie Du,Bohan Yu,Weiqiang Wang,Jian Liu,Huan Wang*

Main category: cs.CV

TL;DR: 提出OmniAgent：一个由音频引导、可主动感知与动态调度工具的多模态智能体，相较以往静态流程与密集字幕方法，能以粗到细的音频引导感知聚焦关键时空片段，显著提升音视频理解，SOTA，提升10%-20%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有“全模态”大模型虽可统一音视频输入，但在精细跨模态理解与对齐上不足，常依赖被动生成与固定流程，难以聚焦任务相关线索，导致多模态推理与对齐效果有限。

Method: 构建OmniAgent，核心是“粗到细”的音频引导感知范式：先利用音频线索进行时间段定位与粗粒度注意，然后在相关片段上触发/编排专用工具进行细粒度视觉与多模态分析；通过动态规划自主演化工具调用与感知注意，避免密集逐帧标注与僵化流水线，从被动响应转向主动多模态询问。

Result: 在三个音视频理解基准上达成SOTA，相较领先的开源与闭源模型提高约10%-20%准确率，显示在多模态对齐与细粒度推理上的明显优势。

Conclusion: 音频先导的主动感知与工具动态编排能有效提升多模态对齐与精细推理效率与准确度；从被动生成转向主动查询是提升音视频理解的关键路径。

Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

</details>


### [177] [IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition](https://arxiv.org/abs/2512.23667)
*Kang Du,Yirui Guan,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出IDT：一个基于Transformer的多视图内在图像分解框架，单次前向即可输出一致的漫反射、漫反射阴影和高光分量，比扩散式方法在多视图一致性和成分可分离性上更好。


<details>
  <summary>Details</summary>
Motivation: 单视图扩散方法虽强，但在多视图扩展时常出现视角不一致，且生成式迭代采样开销大、控制性差；需要一种能够在多视图下保持一致、结构可解释、物理可控的分解方法。

Method: 设计Intrinsic Decomposition Transformer（IDT），用多头注意力联合推理多张输入图像，基于物理成像模型显式分解为漫反射（diffuse reflectance）、漫反射阴影（diffuse shading）和镜面阴影（specular shading），将朗伯与非朗伯传输分离；采用前馈方式一次性预测，无需迭代生成采样。

Result: 在合成与真实数据集上，得到更洁净的漫反射、更一致的漫反射阴影与更独立的镜面成分；多视图一致性显著优于既有内在分解方法。

Conclusion: 基于Transformer的前馈多视图内在分解在一致性、解释性与可控性方面优于扩散式方法，为多视图下的材质与光照分析提供了有效方案。

Abstract: Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.

</details>


### [178] [Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation](https://arxiv.org/abs/2512.23705)
*Shaocong Xu,Songlin Wei,Qizhe Wei,Zheng Geng,Hong Li,Licheng Shen,Qianpu Sun,Shu Han,Bin Ma,Bohan Li,Chongjie Ye,Yuhang Zheng,Nan Wang,Saining Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 他们用视频扩散模型的先验来做透明/反射场景的深度与法线估计，在合成视频数据集TransPhy3D上训练出轻量LoRA视频到视频译码器DKT，实现对真实与合成基准的零样本SOTA，并在机器人抓取中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 传统深度感知在透明/反射物体上失败（折射、反射、透射破坏立体、ToF与判别式单目假设），导致深度空洞和时间不稳定。与此同时，现代视频扩散模型能合成逼真的透明现象，暗示其已内化光学规律，因此可以将其生成先验转用于稳健的时序一致感知。

Method: 构建TransPhy3D：用Blender/Cycles物理渲染11k段透明/反射场景视频，提供RGB+深度+法线，含多类真实与程序化资产与玻璃/塑料/金属材质，并用OptiX去噪。以大型视频扩散模型为基底，在DiT骨干中拼接RGB与（含噪）深度潜变量，通过轻量LoRA适配器训练视频到视频译码器，联合TransPhy3D与现有逐帧合成集共同训练，预测任意长度视频的时序一致深度/法线。还提供1.3B紧凑版本，约0.17秒/帧。

Result: 提出的DKT在透明相关基准上零样本达到SOTA：ClearPose、DREDS（已知/未知类别）和TransPhy3D-Test，相比强图像/视频基线提升精度与时间一致性；法线变体在ClearPose上取得最佳视频法线估计；集成到抓取系统后，在半透明、反射和漫反射表面上均提升抓取成功率，超过以往估计器。

Conclusion: 视频扩散模型的生成先验可高效且无需标签地迁移为对透明现象鲁棒、时序一致的深度/法线感知；“扩散懂透明”。该方向为现实世界机器人操作等应用提供了可靠感知基础。

Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.

</details>


### [179] [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/abs/2512.23709)
*Hau-Shiang Shiu,Chin-Yang Lin,Zhixiang Wang,Chi-Wei Hsiao,Po-Fan Yu,Yu-Chih Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Stream-DiffVSR：仅用历史帧、四步蒸馏扩散、时序引导与轻量解码器，实现低时延在线视频超分，4090上720p每帧0.328秒，感知质量优于现有扩散与在线SOTA，同时初始延迟从>4600秒降至0.328秒。


<details>
  <summary>Details</summary>
Motivation: 扩散式VSR有强感知质量，但依赖未来帧且多步去噪推理昂贵，不适合低时延在线场景；需要一种仅用过去帧、高效率且时序一致性良好的方法。

Method: 1) 因果条件扩散框架，仅利用过去帧；2) 四步蒸馏去噪器，实现快速推理；3) 自回归时序引导（ARTG），在潜空间去噪过程中注入运动对齐线索；4) 轻量时序感知解码器与时间处理模块（TPM），提升细节与时序一致性。

Result: 在RTX4090上处理720p单帧耗时0.328秒；相较在线SOTA TMP，LPIPS提升0.095，延迟降低>130倍；相较扩散式方法，显著领先；扩散VSR中最低时延，初始延迟从>4600秒降至0.328秒。

Conclusion: Stream-DiffVSR在仅依赖历史帧的前提下兼顾高感知质量与极低时延，首次使扩散式VSR可用于低时延在线部署。

Abstract: Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/

</details>
