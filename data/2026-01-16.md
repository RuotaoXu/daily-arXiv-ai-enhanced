<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

TL;DR: 提出一个端到端流程，用FGSM生成针对人脸身份分类的对抗补丁，并用扩散模型反向扩散做平滑与亮度自适应以提高不可感知性；将补丁应用到人脸测试规避识别，同时用ViT-GPT2生成对抗图像的身份语义描述；最后以分类变化、描述变化、以及人脸验证/表情识别脆弱性评估效果，并用感知哈希与分割检测对抗样本，SSIM达0.95。


<details>
  <summary>Details</summary>
Motivation: 在人脸生物识别系统中，物理与数字对抗补丁可导致身份规避或冒充，给取证与安全测试带来挑战。需要一个统一流程既能生成高隐蔽性对抗补丁，又能量化其对识别与解释模型的影响，并提供检测与取证分析手段。

Method: 1) 用FGSM对身份分类器生成对抗噪声并构造补丁；2) 通过扩散模型的反向扩散，结合高斯平滑与自适应亮度校正，提升补丁的不可感知性与“合成补丁规避”能力；3) 将补丁叠加在人脸图像上测试识别规避，同时用ViT-GPT2对对抗图像生成身份语义描述；4) 评估身份分类变更、图像字幕变化、对人脸验证与表情识别的影响；5) 用感知哈希与分割对对抗补丁/样本进行检测分析；6) 用SSIM量化相似度。

Result: 对抗补丁在保持自然外观的同时能有效影响身份分类与描述生成；系统可揭示人脸验证与表情识别在对抗条件下的脆弱性；检测模块（感知哈希+分割）能有效发现对抗补丁/样本，整体图像与原图的SSIM可达0.95。

Conclusion: 该端到端管线兼顾生成、隐蔽化、评估与检测，对人脸识别对抗攻防与取证具有实用价值；扩散模型增强的补丁更难察觉但仍可被感知哈希与分割辅助检测；为身份规避与安全测试提供可复现框架与多指标评估。

Abstract: This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

</details>


### [2] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 提出LCF3D，一种结合RGB相机与LiDAR的级联+后期融合框架，通过2D-3D匹配抑制LiDAR误检并用RGB未匹配框生成新3D棱锥提案以补漏，提升跨域与小目标类（行人/骑行者等）的3D检测表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要精确的3D目标定位，单一LiDAR或RGB在复杂场景下各有不足；多传感器融合虽常见，但如何在不同域与不同传感器配置下稳定地提升检测、特别是对易漏检/易误检的小目标，仍具挑战。

Method: 构建LCF3D：在RGB上运行2D目标检测器，在LiDAR上运行3D检测器；1) 后期融合（late fusion）：将LiDAR 3D检测与图像2D检测进行匹配，过滤未匹配的LiDAR结果以降低误检；2) 级联融合（cascade fusion）：对未匹配的RGB检测生成对应3D视锥（frustum）提案，补充LiDAR漏检；整体遵循多模态互补原则以修正LiDAR网络误差。

Result: 在KITTI和nuScenes上，相较纯LiDAR方法显著提升，尤其是行人、骑行者、摩托车、自行车等困难类别；同时在训练与测试传感器配置不一致的跨域场景中表现出更好的泛化能力。

Conclusion: 通过将2D-3D后期匹配抑误与级联视锥提案补漏相结合，LCF3D有效提升3D检测的精度与泛化，适用于异构传感器配置的自动驾驶场景；代码已开源。

Abstract: Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

</details>


### [3] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

TL;DR: 本文比较DenseNet121与EfficientNet‑B0在儿童胸片肺炎自动检测上的表现，统一预处理与训练设定下，EfficientNet‑B0在准确率、F1与MCC上优于DenseNet121，且两者召回率均>0.99；Grad‑CAM与LIME显示模型关注肺部相关区域。结论：EfficientNet‑B0更均衡且计算更高效，适合临床部署，并通过可解释性增强可信度。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎负担沉重，亟需准确高效的影像辅助诊断工具。深度学习在胸片分析表现突出，但不同SOTA CNN在儿科胸片上的比较与可解释性验证仍需系统评估，以指导临床可用模型的选择。

Method: 使用5863张公开儿科胸片，统一做归一化、缩放与数据增强；在相同训练设定下微调ImageNet预训练的DenseNet121与EfficientNet‑B0；以准确率、F1、MCC与召回率评估；并用Grad‑CAM与LIME可视化判别区域以提供可解释性。

Result: EfficientNet‑B0：准确率84.6%、F1=0.8899、MCC=0.6849；DenseNet121：准确率79.7%、F1=0.8597、MCC=0.5852；两者召回率均>0.99。可解释性可视化显示关注临床相关肺部区域且一致性良好。

Conclusion: 在儿科肺炎胸片检测中，EfficientNet‑B0在性能与计算效率上更均衡，较DenseNet121更适合临床部署；结合Grad‑CAM与LIME提升了模型透明度与临床信任度。

Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

</details>


### [4] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

TL;DR: 提出NanoSD：从Stable Diffusion 1.5蒸馏而来的全流程轻量扩散基础模型，通过同时瘦身U-Net与VAE并保持潜空间结构，实现在移动端NPU上实时推理，同时在多种图像复原与生成任务上达到SOTA与良好可部署性。


<details>
  <summary>Details</summary>
Motivation: 现有轻量化扩散模型多仅压缩U-Net或缩短扩散步数，破坏潜在流形与生成先验，导致泛化差、只能面向单任务且在真实硬件上效率有限；需要一种既保留强生成先验又在边缘设备上高效的通用基础模型。

Method: 对SD1.5进行“全管线”协同设计：网络外科手术、特征级生成蒸馏、结构化架构缩放，联合作用于U‑Net与VAE编解码器；维持潜空间与特征路由的平衡以保留生成先验，并针对硬件延迟进行架构权衡，形成不同精度‑延迟‑规模帕累托前沿的模型族。

Result: 得到130M–315M参数的NanoSD家族，可在移动级NPU上实现至低约20ms的实时推理；分析表明仅靠参数减少并不等同于硬件高效，架构平衡、特征路由与潜空间保持共同决定端侧延迟。作为可替换骨干，NanoSD在超分、去模糊、人脸复原、单目深度估计等任务上在感知质量与可部署性上均优于以往轻量扩散模型。

Conclusion: NanoSD作为通用扩散基础模型，兼顾生成先验与端侧效率，提供沿精度‑延迟‑尺寸前沿的多点选择，适用于边缘设备上的实时视觉生成与复原，并为设计高效扩散模型提供了关于架构与潜空间保持的实践准则。

Abstract: Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

</details>


### [5] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

TL;DR: 提出UniHash双分支统一哈希框架，结合点式与对式训练，通过互学习与SM-MoH实现跨分支知识迁移，在Seen/Unseen检索均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单一训练范式（点式/对式）各有优劣：点式对已见类精度高但泛化差，对式对未见类泛化好但已见类受限；缺乏同时兼顾的统一方案。

Method: 设计双分支：中心/点式分支与对式分支；引入互学习损失对齐两分支哈希表示；提出Split-Merge Mixture of Hash Experts (SM-MoH)模块进行表示的分裂-合并专家混合以增强跨分支信息交换；总体形成可双向知识迁移的哈码学习；并给出理论分析。

Result: 在CIFAR-10、MSCOCO、ImageNet上，对已见与未见类别检索均获得持续、稳定的SOTA表现。

Conclusion: 统一点式与对式优势的UniHash能在已见与未见场景取得更均衡与领先的检索效果，方法具有理论与实证支持。

Abstract: Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

</details>


### [6] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 提出ViSIL指标，用VLM推断衡量视频摘要的信息损失，实现跨模态摘要（文本与关键帧等）的统一评估，并与人类/VLM在VQA上的表现显著相关；利用ViSIL挑选摘要可在不增负载下将VQA准确率提升约7%。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态摘要用于压缩视频并支持高效检索与生成，但BLEU/ROUGE等文本相似度指标无法衡量“文本段落 vs 关键帧序列”等跨模态的信息覆盖度，缺乏统一度量来比较不同结构的多模态摘要质量。

Method: 提出Video Summary Information Loss (ViSIL)：基于信息论的度量，利用视觉-语言模型（VLM）对视频与其摘要进行推断，估计摘要未覆盖的视频信息量（信息损失）。通过衡量损失，实现对不同摘要形式（文本、关键帧、混合）的统一比较，并用于选择摘要以最小化损失与计算成本的权衡。

Result: ViSIL分数与人类评估以及VLM在视频问答（VQA）任务上的表现显著相关；基于ViSIL进行摘要选择可构建Pareto前沿，在不增加处理开销的前提下，相比仅文本摘要将VQA准确率提升约7%。

Conclusion: ViSIL为多模态视频摘要提供了跨结构的一体化质量度量，能有效预测与实际任务（VQA）相关的有用性，并指导摘要选取以在信息保真与效率间实现更优权衡。

Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

</details>


### [7] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

TL;DR: 提出TuneCLIP：在不开销重训的情况下，用自监督数据对开源CLIP权重进行后训练，解决直接微调易退化的问题，并在多项基准上取得稳定提升。


<details>
  <summary>Details</summary>
Motivation: 开源CLIP模型广泛用于多模态任务，但想进一步提升通常需在海量数据上从头训练，成本高昂。直接用常规自监督/对比学习流程从现有权重继续训练常会导致性能下降。作者希望找到一种低成本、自监督的后训练方法，在不针对单一下游任务的前提下，提升模型在多任务/分布外上的通用表现。

Method: 提出TuneCLIP两阶段框架：1）Warm-up阶段恢复优化器统计量（如动量、方差等）以减少从冷启动继续训练带来的偏置，依据理论分析设计；2）Fine-tuning阶段采用新的对比损失，降低将“假负样本”（本应为正但被当作负的样本对）受到的惩罚，从而缓解退化。

Result: 在不同架构与规模的开源CLIP上稳定提升：例如对SigLIP(ViT-B/16)在ImageNet及OOD基准最高+2.5%，在DataComp基准+1.2%。

Conclusion: TuneCLIP能以自监督、低成本的方式对开源CLIP进行后训练，解决常规继续训练导致的退化，显著提升多任务与分布外泛化，为高效后预训练适配提供了强基线。

Abstract: CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

</details>


### [8] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

TL;DR: VibrantSR 从10 m分辨率的Sentinel-2季节合成影像生成0.5 m林冠高（CHM），在西部美国22个生态区上以空间独立验证达成4.39 m MAE（对≥2 m林冠），优于多项卫星基准；尽管基于航片的VibrantVS更准（2.71 m MAE），VibrantSR以稳定、全球可得的时序实现大陆尺度的运营化森林监测与碳核算。


<details>
  <summary>Details</summary>
Motivation: 航片或LiDAR支撑的高分辨率CHM更新不频、成本高、覆盖不均；需要一种依赖全球可用、频繁更新的卫星数据，在保持可用精度下实现大范围、常态化的林冠高度估计，以支撑森林监测与碳核算。

Method: 提出生成式超分辨率框架VibrantSR：从10 m Sentinel-2季节合成影像推断0.5 m CHM。采用空间上不相交的训练/验证划分，并与卫星与航片基准进行对比。另有基于航片的变体VibrantVS作为上限性能参考。

Result: 在美国西部22个EPA Level 3生态区，针对≥2 m林冠，VibrantSR达到4.39 m MAE，优于Meta（4.83 m）、LANDFIRE（5.96 m）和ETH（7.05 m）。VibrantVS的MAE为2.71 m，精度更高但依赖航片。

Conclusion: VibrantSR在精度略逊于航片方案的前提下，借助全球可得的Sentinel-2季节合成，实现季节至年度节奏的稳定、可扩展CHM估计，适合大陆尺度的运营化森林监测与碳核算，无需昂贵且稀疏的航片获取。

Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

</details>


### [9] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: 提出MedVL-SAM2：统一的3D医学多模态模型，兼具报告生成、VQA与多范式分割（语义/指代/交互），通过SAM2式体素分割模块实现细粒度三维定位与推理，并在大规模3D CT图文对预训练后，联合语言理解与分割目标优化，取得多任务SOTA与可靠三维视觉定位。


<details>
  <summary>Details</summary>
Motivation: 当前医学VLM在图像级文本任务强，但难以在单一框架内实现细粒度视觉定位与三维体素级空间推理；临床与研究亟需既能高层语义推理又能精确3D定位/分割的通用模型。

Method: 构建统一架构，将图像级推理与像素级感知融合；引入基于SAM2的体素分割模块以支持多粒度空间推理与多种提示（语言、点、框）；两阶段训练：1）用大规模3D CT图文对预训练，使体积视觉特征与放射学语言嵌入对齐；2）在综合3D CT分割数据上联合优化语言理解与分割目标，实现多任务协同。

Result: 在报告生成、VQA及多种3D分割任务上达到SOTA；表现出可靠的3D视觉指向能力、可控的交互式分割与稳健的跨模态推理。

Conclusion: 高层语义推理与精确3D定位可在统一3D医学VLM中共同实现；MedVL-SAM2验证了多任务联合与SAM2体素分割的有效性，提供可泛化的三维跨模态理解与分割框架。

Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

</details>


### [10] [Transition Matching Distillation for Fast Video Generation](https://arxiv.org/abs/2601.09881)
*Weili Nie,Julius Berner,Nanye Ma,Chao Liu,Saining Xie,Arash Vahdat*

Main category: cs.CV

TL;DR: 提出TMD，将多步视频扩散去噪轨迹蒸馏为少步概率转移过程，每步用轻量条件flow实现，保持画质同时极大加速生成。


<details>
  <summary>Details</summary>
Motivation: 大型视频扩散/流模型生成效果强，但需要多步采样，难以满足实时交互等低延迟场景；现有蒸馏方案在速度/质量间权衡不佳。

Method: 将扩散去噪轨迹匹配为少量“外层转移”+“内层flow更新”的概率转移过程。把原扩散骨干拆为：1) 主干(早期大部分层)提取语义表征；2) flow head(末端少量层)在每个外层转移内执行多次条件flow更新。步骤：在预训练视频扩散模型上添加并适配flow head为条件flow映射；在每个外层转移中展开flow head多次更新，并对学生模型做分布匹配蒸馏，使其少步轨迹逼近教师多步轨迹。

Result: 在Wan2.1 1.3B与14B文生视频模型上，TMD在相近推理成本下优于现有蒸馏方法，提供速度与视觉质量的更优折中，提升画面保真与文本遵从。

Conclusion: TMD通过“转移匹配+条件flow头”的结构化蒸馏，有效将多步扩散压缩为少步生成，兼顾高质量与低延迟，适合实时与交互式视频应用。

Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd

</details>


### [11] [OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport](https://arxiv.org/abs/2601.09952)
*Zhihua Zhao,Guoqiang Li,Chen Min,Kangping Lu*

Main category: cs.CV

TL;DR: 提出OT-Drive：将RGB与法向融合建模为最优传输问题，以语义锚点约束特征到共享流形，实现在OOD场景下稳健的可通行区域分割；在ORFD与跨数据集任务显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的可通行区域分割在非结构化与分布外(OOD)场景性能显著退化，影响自动驾驶规划与决策；需要能在少量训练数据下具备强泛化的多模态融合方法。

Method: 1) 设计Scene Anchor Generator (SAG)，将场景分解为天气、昼夜、道路类型的联合分布，生成可泛化的语义锚点；2) 设计基于最优传输(OT)的多模态融合模块(OT Fusion)，把RGB与表面法向特征对齐并运输到由语义锚点定义的流形上，实现鲁棒融合与分割；整体将多模态融合表述为分布运输问题。

Result: 在ORFD的OOD场景上获得95.16% mIoU，较现有方法提升6.35%；跨数据集迁移任务达89.79% mIoU，超过基线13.99%。

Conclusion: OT-Drive通过语义锚点+最优传输的多模态对齐，有效提升OOD下的可通行区域分割与跨数据集泛化能力，并在有限训练数据条件下具备实用性与效率优势。

Abstract: Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.

</details>


### [12] [The Spatial Blindspot of Vision-Language Models](https://arxiv.org/abs/2601.09954)
*Nahid Alam,Leema Krishna Murali,Siddhant Bharadwaj,Patrick Liu,Timothy Chung,Drishti Sharma,Akshata A,Kranthi Kiran,Wesley Tam,Bala Krishna S Vegesna*

Main category: cs.CV

TL;DR: 论文指出现有VLM因CLIP式1D补丁序列和训练目标忽视空间结构，导致空间关系推理薄弱；通过尝试非对比式图像目标与2D位置编码，能显著提升多基准上的空间推理表现。


<details>
  <summary>Details</summary>
Motivation: 许多需要空间落地（如机器人、具身智能）的应用受限于VLM对空间关系理解不足；现有CLIP范式在编码器与位置表示上忽略图像的二维结构，是设计中的缺维与性能瓶颈。

Method: 系统研究两类改动：(i) 采用替代的图像编码训练目标（非单一对比学习，如更能保留结构/密集预测相关的目标）；(ii) 在架构中引入或改进2D位置编码而非将图像展平为1D序列；并在多项空间推理基准上进行对比实验。

Result: 采用替代训练目标与2D位置编码的模型在多个空间推理基准上取得优于标准CLIP风格编码器的表现，显示更强的空间关系理解能力。

Conclusion: 空间意识是VLM设计中缺失的重要维度；通过调整图像编码训练目标与引入2D位置编码，可缓解空间推理短板，促进在需要空间落地的应用中的表现提升。

Abstract: Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.

</details>


### [13] [DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models](https://arxiv.org/abs/2601.09981)
*Yulin He,Wei Chen,Zhikang Jian,Tianhang Guo,Wenjuan Zhou,Minglong Li*

Main category: cs.CV

TL;DR: 论文提出DR^2Seg自奖励框架，通过两阶段推理与自校验描述，抑制多模态大模型在推理分割中的“过度思考”，在无需额外思维标注的前提下同时提升推理效率与分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割需理解复杂文本并定位目标，但MLLM常生成冗长推理链，干扰目标定位、降低效率与精度；缺乏无需额外监督即可引导“更短、更准”推理的机制。

Method: 采用两阶段rollout：1) 多模态推理阶段，模型生成自包含的目标描述（明确被指物的关键属性/关系）；2) 指代分割阶段，用该描述替换原复杂查询以验证自包含性与可执行性。基于此设计两个自奖励信号：强化面向目标的推理、惩罚冗余/偏题思考，从而在无外部思维监督下优化模型。适配不同规模MLLM与分割模型。

Result: 在多种尺度的MLLM与多套分割器上，DR^2Seg稳定提升推理效率（更短/更快的思维过程）与总体分割性能，相较基线取得一致性增益。

Conclusion: 通过自包含描述与自奖励约束，DR^2Seg有效缓解过度思考，兼顾效率与精度，为推理分割提供通用、无额外监督的提升途径，可广泛集成于不同模型架构。

Abstract: Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.

</details>


### [14] [DW-DGAT: Dynamically Weighted Dual Graph Attention Network for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2601.10001)
*Chengjia Liang,Zhenjiong Wang,Chao Chen,Ruizhi Zhang,Songxi Liang,Hai Xie,Haijun Lei,Zhongwei Huang*

Main category: cs.CV

TL;DR: 提出DW-DGAT，一种动态加权的双图注意力网络，通过多结构数据融合、基于脑区与样本关系的双重注意力，以及类权重+稳健损失，解决多模态高维、异质性与类别不平衡， 在PPMI与ADNI上达SOTA用于早期PD/AD诊断。


<details>
  <summary>Details</summary>
Motivation: 早诊对PD/AD至关重要，但现实数据存在：多指标多结构（向量、矩阵、图等）高维、影像与表型异质、类别不平衡（早期患者稀少）。现有方法难以同时高效融合多结构、兼顾微观（脑区特征）与宏观（样本间关系）信息，并对不平衡鲁棒。

Method: 1) 通用融合策略：将三种结构形式的多指标数据进行一致性对齐与融合；2) 双图注意力：构建脑区层面的图（微观）与样本间关系图（宏观），采用注意力机制分别学习并联合表示；3) 动态类权重+两种稳定损失：自适应生成类别权重，结合如加权交叉熵与对比/焦点/ALS等稳健损失缓解不平衡。整体形成DW-DGAT端到端训练。

Result: 在PPMI（PD）与ADNI（AD）数据集上，经严格实验对比，DW-DGAT取得SOTA分类/早诊性能（如更高的准确率、AUC、敏感性），优于现有多模态融合与图学习方法；同时在不平衡场景和异质数据上表现稳定。

Conclusion: DW-DGAT有效融合多结构多指标数据，并联合挖掘脑区与样本关系的多尺度特征，结合动态类权重与稳健损失缓解类别不平衡，实现对PD/AD早期诊断的SOTA表现，具有潜在临床应用价值。

Abstract: Parkinson's disease (PD) and Alzheimer's disease (AD) are the two most prevalent and incurable neurodegenerative diseases (NDs) worldwide, for which early diagnosis is critical to delay their progression. However, the high dimensionality of multi-metric data with diverse structural forms, the heterogeneity of neuroimaging and phenotypic data, and class imbalance collectively pose significant challenges to early ND diagnosis. To address these challenges, we propose a dynamically weighted dual graph attention network (DW-DGAT) that integrates: (1) a general-purpose data fusion strategy to merge three structural forms of multi-metric data; (2) a dual graph attention architecture based on brain regions and inter-sample relationships to extract both micro- and macro-level features; and (3) a class weight generation mechanism combined with two stable and effective loss functions to mitigate class imbalance. Rigorous experiments, based on the Parkinson Progression Marker Initiative (PPMI) and Alzhermer's Disease Neuroimaging Initiative (ADNI) studies, demonstrate the state-of-the-art performance of our approach.

</details>


### [15] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: 该论文提出VERHallu基准，系统评测视频大模型在事件关系（因果、时间、子事件）上的幻觉，并给出对抗性与反直觉场景；发现现有VideoLLM依赖先验、忽视帧级线索与周边子事件；提出关键帧传播(KFP)在中间层重分配帧级注意力，提升多事件理解，减少关系幻觉且不增推理时延。


<details>
  <summary>Details</summary>
Motivation: 现有对VideoLLM幻觉研究多集中于“有没有”的层面（物体/事件/场景存在性），忽略“事件之间如何关联”的关系层面。现实视频包含密集事件与复杂因果/时间/组成关系，模型常因训练分布偏差与帧级利用不足而产生关系误判，影响问答、检索与推理可靠性，亟需系统评测与改进方法。

Method: 1) 构建VERHallu：覆盖因果、时间、子事件三类关系；三项任务（关系分类、QA、反事实QA）；设计反直觉视频与人标候选，显式区分视觉-语言与纯语言偏置。2) 分析SOTA VideoLLM在密集事件关系上的失败模式（依赖先验、忽视帧级线索与子事件）。3) 提出KFP：在中间层对关键帧的注意力进行传播与重分配，强化多事件线索聚合与关系推理，保持推理速度。

Result: 在VERHallu上，现有SOTA模型在三类关系与三项任务中均显著失分，表现出对先验的依赖与子事件忽略。采用KFP后，事件关系理解显著提升，关系幻觉明显降低，同时推理延迟基本不变。

Conclusion: 事件关系幻觉是VideoLLM的重要但被忽视的问题。VERHallu为其提供全面评测，揭示当前模型的帧级与子事件利用不足。KFP通过关键帧注意力传播有效缓解该问题，在不牺牲速度的前提下提升多事件关系推理能力。

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [16] [Disentangled Concept Representation for Text-to-image Person Re-identification](https://arxiv.org/abs/2601.10053)
*Giyeol Kim,Chanho Eom*

Main category: cs.CV

TL;DR: 提出DiCo框架，通过共享的slot（部位级锚点）和其内的多概念块，实现文本-图像在部位与属性层面的分解式对齐，提升细粒度检索与可解释性，在三大数据集上达SOTA竞争性能。


<details>
  <summary>Details</summary>
Motivation: TIReID需要从文本检索人像图，但图文模态差距大，且要区分细粒度属性（颜色、纹理、版型等），现有方法难以同时保证跨模态对齐与可解释的细粒度对应。

Method: 设计DiCo的分层解耦对齐：1）共享slot表示作为跨模态的“部位级”锚点；2）每个slot再分解为多个概念块（颜色、纹理、形状等），实现属性级解耦；3）在图文两端将对应部位-属性进行对齐，从而兼顾稳定的部位对应与属性的可分离性。

Result: 在CUHK-PEDES、ICFG-PEDES、RSTPReid三数据集上取得与SOTA相当或更优的检索指标，并展示了基于slot与block的解释性可视化与更细粒度的检索效果。

Conclusion: 分层的slot+概念块表示可有效缩小图文模态差距、实现细粒度跨模态对齐，并提升TIReID性能与可解释性。

Abstract: Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.

</details>


### [17] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 提出首个物理真实感的合成水下事件相机光流基准数据集，基于光线追踪RGBD序列与视频转事件流程，提供事件流、稠密光流、深度与相机运动，并系统评测现有方法，建立水下事件视觉的新基线。


<details>
  <summary>Details</summary>
Motivation: 水下成像受波长依赖衰减、强散射、浑浊模糊与非均匀照明影响，标准相机难以获取真实运动；缺乏同时具备真实水下光学与精确光流标注的事件相机数据集，阻碍算法研究与评测。

Method: 以物理真实感光线追踪渲染生成RGBD水下视频；通过现代视频转事件管线合成逼真的事件数据流；输出配套的稠密真值光流、深度与相机位姿；并对学习式与模型式光流算法在水下条件下的表现进行基准评测。

Result: 获得首个含真实感水下光学效应与精确标注的事件光流数据集；实验显示水下光传输对事件生成与光流精度有显著影响，并量化了多种方法的性能差异。

Conclusion: 该数据集为水下事件视觉提供统一评测平台与新基线，将推动后续算法设计、鲁棒性分析与性能提升；代码与数据已公开。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [18] [CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation](https://arxiv.org/abs/2601.10061)
*Chengzhuo Tong,Mingkun Chang,Shenglong Zhang,Yuran Wang,Cheng Liang,Zhizheng Zhao,Ruichuan An,Bohan Zeng,Yang Shi,Yifan Dai,Ziming Zhao,Guanbin Li,Pengfei Wan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出CoF-T2I，将视频模型的Chain-of-Frame（逐帧）推理引入文本到图像生成，通过逐步可视化细化生成，利用中间帧作为显式推理步骤，最终帧为结果。在自建CoF-Evol-Instruct数据集和逐帧独立编码的支持下，方法在GenEval得分0.86、Imagine-Bench 7.468，优于基线视频模型。


<details>
  <summary>Details</summary>
Motivation: 虽然视频模型已展现逐帧视觉推理（CoF）并在解迷、视觉谜题等任务中有效，但在T2I中缺少明确的推理起点与可解释的中间状态，使其潜力未被挖掘。需要一种能把从语义到美学的生成过程显式化、可监督、可解释的方法以提升T2I质量和可控性。

Method: 提出CoF-T2I：将T2I建模为视频式“从粗到细”的逐帧演化过程。1) 设计渐进式视觉细化流程，把每一帧当作显式推理/生成步骤，最终帧为输出图。2) 构建CoF-Evol-Instruct数据集，包含从语义到美学的逐步演化轨迹，用于训练显式的生成链。3) 为减少运动伪影并提高图像质量，对每一帧采用独立编码操作（独立时空处理/解耦），在保留连贯性的同时避免不必要的跨帧运动。

Result: 在挑战性基准上取得强竞争力：GenEval 0.86，Imagine-Bench 7.468，显著优于底座视频模型；同时减少运动伪影、提升一致性与可解释性。

Conclusion: 将视频模型的逐帧推理机制引入T2I是有效路径。通过显式中间帧的推理轨迹与逐帧独立编码，CoF-T2I在质量与评测上均表现突出，显示视频模型可推动高质量、可解释的文本到图像生成。

Abstract: Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.

</details>


### [19] [ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology](https://arxiv.org/abs/2601.10073)
*Hyun Do Jung,Jungwon Choi,Hwiyoung Kim*

Main category: cs.CV

TL;DR: ReaMIL是在病理全视野MIL上加一个轻量选择头，用预算化“充分性”目标，在不降AUC的前提下用更少、更紧凑的切片证据支撑预测，并提供证据效率指标（MSK、AUKC、连通性）。


<details>
  <summary>Details</summary>
Motivation: 现有WSI的MIL模型虽有高准确率，但缺乏对“需要多少、哪些证据即可自信预测”的定量把握，证据常分散且不够可解释；希望在保持性能的同时，得到小而连贯、可量化的证据集与诊断指标。

Method: 在强MIL骨干上添加软门控的选择头，对每个tile输出保留概率；训练时采用“预算化充分性”目标：在稀疏预算（限定选中tile数K）的约束下，仅用被保留的证据使真类概率≥阈值τ，通过hinge损实现；端到端训练，无需额外标注，并可生成切片级覆盖图。

Result: 在TCGA-NSCLC、TCGA-BRCA与PANDA数据集上，AUC与基线持平或略优；NSCLC上AUC=0.983，τ=0.90时平均最小充分K(MSK)≈8.2，AUKC≈0.864，显示少量tile即可快速提升并稳定信心；同时给出证据效率与紧凑性指标。

Conclusion: ReaMIL在不牺牲分类性能的前提下，产生小而连续的证据集，提升可解释性与证据效率评估；方法简单、无额外监督、易集成到标准MIL训练流程，适合WSI实际应用与行为评估。

Abstract: We introduce ReaMIL (Reasoning- and Evidence-Aware MIL), a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective: a hinge loss that enforces the true-class probability to be $\geq τ$ using only the kept evidence, under a sparsity budget on the number of selected tiles. The budgeted-sufficiency objective yields small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK) $\approx 8.2$ tiles at $τ= 0.90$ and AUKC $\approx 0.864$, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays. We report accuracy alongside MSK, AUKC, and contiguity for rigorous evaluation of model behavior on WSIs.

</details>


### [20] [Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting](https://arxiv.org/abs/2601.10075)
*Zhendong Wang,Lebin Zhou,Jingchuan Xiao,Rongduo Han,Nam Ling,Cihan Ruan*

Main category: cs.CV

TL;DR: 论文提出一种将后印象派“夸张结构、抑制细节”的艺术原则引入3D高斯溅射的风格迁移方法，通过从2D画作提取流场并回推到3D，驱动几何而非仅纹理，实现与场景拓扑一致的笔触化几何变形；并通过亮度-结构解耦与VLM审美评测，获得更具艺术真实性的结果。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移多把几何当作刚性支架，仅投射表面纹理，违背后印象派以几何夸张为主的表达方式；需要一种能在无网格前提下，将2D绘画“运动/流向”转化为3D几何抽象的机制，并以审美而非像素误差衡量效果。

Method: 提出面向3D Gaussian Splatting的“流引导几何平流”框架：从2D绘画提取方向性流场；通过投影-回传将流场传播至3D，对高斯原语进行整形，使其与流向对齐形成笔触；采用网格无关的投影式引导以适配场景拓扑；引入亮度-结构解耦，分离几何变形与颜色优化，降低强烈形变下的伪影；最后用“VLM-评审”框架，以大模型的审美判断替代像素级指标进行评价。

Result: 方法在无网格条件下实现由绘画运动直接驱动的结构性几何变形，生成与场景拓扑一致、流向对齐的笔触化3D表现，同时减少颜色/结构耦合带来的伪影；评测显示相较传统纹理投射法在艺术真实性上更优（由VLM判定）。

Conclusion: 几何应成为艺术风格迁移的核心表达通道。通过流场引导的3DGS几何整形与亮度-结构解耦，本方法在无网格条件下有效实现后印象派式的结构夸张，并以审美导向的VLM评测更贴近主观风格判定。

Abstract: In 1888, Vincent van Gogh wrote, "I am seeking exaggeration in the essential." This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.
  We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.
  Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.

</details>


### [21] [Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks](https://arxiv.org/abs/2601.10090)
*Mingzhuo Li,Guang Li,Linfeng Ye,Jiafeng Mao,Takahiro Ogawa,Konstantinos N. Plataniotis,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出一种难度引导采样（DGS）与难度感知引导（DAG），将“样本难度”融入数据集蒸馏，以缩小蒸馏目标与下游分类任务之间的目标鸿沟，从而提升蒸馏数据训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏多依赖原始数据的通用特征，忽视与具体下游任务（如图像分类）相关的“任务难度”信息，导致蒸馏目标与实际训练需求不匹配，影响下游性能。

Method: 1) 定义并度量样本“难度”，据此构建目标难度分布；2) 在现有蒸馏方法生成的图像池上，使用后处理式的插件模块DGS按目标难度分布进行采样，得到最终精炼数据集；3) 在生成阶段引入难度感知引导（DAG），使生成过程受难度信号影响；方法可与主流蒸馏算法兼容。

Result: 在多种设置与数据集上进行大量实验，DGS与DAG显著提升下游分类性能，验证难度信号在数据蒸馏中的有效性与泛化潜力。

Conclusion: 通过在数据蒸馏中显式引入与任务相关的难度分布，并在采样与生成两阶段施加难度约束，可有效缩小目标鸿沟、提升下游表现；难度概念对更广泛的下游任务亦具潜力。

Abstract: In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.

</details>


### [22] [V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation](https://arxiv.org/abs/2601.10094)
*Han Wang,Yi Yang,Jingyuan Hu,Minfeng Zhu,Wei Chen*

Main category: cs.CV

TL;DR: V-Zero提出一个仅用无标注图像的自我改进后训练框架，通过“提问者-求解者”的共进化循环提升VLM推理能力，并在Qwen2.5-VL-7B-Instruct上实现无人工标注的稳健增益。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理强化需大量人工标注数据，成本高且难扩展；需要一种无需人工标注、可持续自我提升的训练范式。

Method: 构建两个协作但对抗性的角色：Questioner生成高质量、具有挑战性的视觉问题；Solver回答并进行推理。Questioner用双轨迹推理奖励（直觉猜测 vs. 逐步推理结果对比）来学习生成更难更好的问题；Solver通过对自身多样化采样答案进行多数投票形成伪标签训练。两者使用GRPO进行迭代更新，形成共进化闭环。

Result: 在无需任何人工标注的前提下，对Qwen2.5-VL-7B-Instruct带来一致增益：视觉数学推理提升约+1.7分，通用视觉中心任务提升约+2.6分。

Conclusion: 自监督的后训练框架可有效提升多模态模型推理能力，证明了利用仅无标注图像与角色共进化即可获得显著性能改进，具备通用性与可扩展潜力；代码开源以促进复现与进一步研究。

Abstract: Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero

</details>


### [23] [InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery](https://arxiv.org/abs/2601.10098)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出InfoSculpt：基于信息瓶颈的GCD方法，通过双重条件互信息目标在类别级与实例级同时压缩噪声、保留类别信息，避免伪标签/两阶段聚类局限，并在8个基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: GCD需在大规模无标注数据中同时识别已知与新类别，现实开放环境关键却困难。现有方法多依赖伪标签或两阶段聚类，缺乏从原理上区分“类别本质信号”与“实例特异噪声”的机制，导致表示纠缠、鲁棒性差。

Method: 从信息论角度重构GCD，基于信息瓶颈原则，提出InfoSculpt：最小化双重条件互信息(CMI)。1) 类别级CMI在有标注数据上学习紧致、判别的已知类别表示；2) 实例级CMI在全部数据上压缩由数据增强引入的噪声，提炼不变特征。二者在不同尺度协同，塑造保留类别信息且去除实例噪声的潜在空间。

Result: 在8个基准数据集上进行广泛实验，验证该信息论框架的有效性（文中摘要未给出具体数值，但声称取得显著性能）。

Conclusion: 通过双重CMI的信息瓶颈优化，InfoSculpt能系统性地解耦类别信息与实例噪声，避免伪标签/两阶段聚类依赖，得到鲁棒、可分离的表示，在多基准上表现优越。

Abstract: Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.

</details>


### [24] [FlowAct-R1: Towards Interactive Humanoid Video Generation](https://arxiv.org/abs/2601.10103)
*Lizhen Wang,Yongming Zhu,Zhipeng Ge,Youwei Zheng,Longhao Zhang,Tianshu Hu,Shiyang Qin,Mingshuang Luo,Jiaxu Zhang,Xin Chen,Yulong Wang,Zerong Zheng,Jianwen Jiang,Chao Liang,Weifeng Chen,Xing Wang,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: FlowAct-R1 提出一个面向实时交互的人形视频生成框架，基于MMDiT并行-流式合成，结合分块扩散强制与自强制策略，兼顾长时一致性与低延迟，实现480p 25fps、约1.5s首帧延迟，并提供细粒度全身控制与良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视频合成在高保真与实时互动之间存在权衡：长时生成易积累误差、交互延迟高、难以在连续输入条件下保持时间一致且控制细腻。需要一种既低延迟又长时稳定、可全身精细控制的交互式人形视频生成方案。

Method: 在MMDiT架构上实现流式生成：1) 提出分块（chunkwise）扩散强制，将视频分段迭代并在段间施加一致性约束，缓解误差累积；2) 设计自强制（self-forcing）变体，进一步稳定长时依赖；3) 高效蒸馏与系统级优化（推理管线、缓存与并行）以降低TTFF与提升吞吐；4) 提供全身细粒度与整体控制，使代理在多行为状态间自然切换。

Result: 达到480p下25fps的稳定实时生成，首帧时间约1.5秒；在交互场景中呈现高行为生动性与感知真实感；在多样角色风格上具有稳健泛化与长时一致性。

Conclusion: FlowAct-R1在实时交互人形视频生成中实现了低延迟、长时稳定与可控性的综合突破，为连续互动场景提供高保真且响应迅速的合成能力。

Abstract: Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.

</details>


### [25] [MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers](https://arxiv.org/abs/2601.10104)
*Chenyue Zhou,Jiayi Tuo,Shitong Qin,Wei Dai,Mingxuan Wang,Ziwei Zhao,Duoyang Li,Shiyang Su,Yanxi Lu,Yanbiao Ma*

Main category: cs.CV

TL;DR: 提出MathDoc数据集与评测框架，用于真实高中数学试卷的文档级结构化题目抽取，并强调对不可辨识样本的“拒答”能力评估；实验发现现有SOTA多模态大模型在拒绝不清晰输入上显著失效。


<details>
  <summary>Details</summary>
Motivation: 现实考试卷面存在涂改、遮挡、扫描噪声等严重视觉噪声，现有基准多基于干净文档或通用版面分析，忽视数学题目的结构完整性与模型对不完整输入的主动拒答能力。因此需要一个面向真实试卷、强调可靠性的基准。

Method: 构建MathDoc基准：收集并精修3,609道来源于真实高中数学试卷的问题，保留真实伪影并显式包含不可识别样本；提出多维评测框架，包含题干准确性（结构化抽取正确性）、视觉相似度（版面/内容还原度）、与拒绝能力（对不可读输入的拒答率）。对多款SOTA MLLMs（如Qwen3-VL、Gemini-2.5-Pro）进行端到端抽取实验。

Result: 端到端大模型在抽取准确性上表现较强，但在不可辨识样本上普遍未能拒答，反而自信地产生无效输出，显示鲁棒性与可靠性不足。

Conclusion: MathDoc填补了在退化文档条件下评估结构化数学题抽取与拒答能力的空白，揭示现有MLLMs在“拒绝不确定输入”方面的系统性缺陷，为后续提升模型可靠性提供标准化评测平台。

Abstract: The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \textbf{3,609} carefully curated questions with real-world artifacts and explicitly includes unrecognizable samples to evaluate active refusal behavior. We propose a multi-dimensional evaluation framework covering stem accuracy, visual similarity, and refusal capability. Experiments on SOTA MLLMs, including Qwen3-VL and Gemini-2.5-Pro, show that although end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions. Our project repository is available at \href{https://github.com/winnk123/papers/tree/master}{GitHub repository}

</details>


### [26] [Enhancing Visual In-Context Learning by Multi-Faceted Fusion](https://arxiv.org/abs/2601.10107)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出一种用于视觉上下文学习的多组合协同融合框架：从多条高质量提示中构建三路上下文分支，并通过MULTI-VQGAN联合解释多源信息，跨任务表现优于检索-提示与单一融合方法。


<details>
  <summary>Details</summary>
Motivation: 现有VICL多采用“检索-再提示”，只选单一最佳视觉提示，丢失其他上下文信息；即便将top-K提示简单融合为单一表示，也压缩多样信号，限制推理能力。需要一种能保留并协同利用多来源上下文的融合方式。

Method: 提出多组合协同融合：从若干优质提示中构建三条互补的上下文表示分支，每条分支由不同提示组合整合而成；设计MULTI-VQGAN架构，联合接收三路指导信号并进行协同解码/重建，以更好利用多源上下文信息。

Result: 在前景分割、单目标检测、图像上色等多样任务上进行大量实验，显示所提方法具有较强的跨任务泛化与更有效的上下文融合能力，预测更鲁棒、准确，优于主流方法。

Conclusion: 多组合协同融合与MULTI-VQGAN能充分挖掘多提示上下文的互补性，相较单一提示或简单top-K融合，能显著提升VICL在多任务上的性能与稳健性。

Abstract: Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.

</details>


### [27] [Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL](https://arxiv.org/abs/2601.10117)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Shifu Yan,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出一种端到端视觉类比(视觉上下文学习)框架，通过多示例融合与排列特定适配，实现少样本任务快速迁移，在分割、检测、上色上优于现有并具跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VICL方法只选最相似示例，忽略其他高质量提示的互补信息；且未利用不同提示排列所蕴含的结构化先验，导致适配不充分。

Method: 1) 自适应融合模块：从多条提示中聚合关键模式与标注，形成更精确的上下文提示；2) 排列特定的轻量MLP：将布局/排列先验与主模型解耦，以最小改动适配不同提示排列；3) 双向微调：交换查询与提示角色，迫使模型从融合上下文重建原提示，促进融合模块与修复模型协同。端到端训练。

Result: 在前景分割、单目标检测、图像上色三项任务上取得更优性能，并在跨任务泛化上表现强。

Conclusion: 多提示融合和排列先验建模能显著提升VICL的少样本适配与泛化；双向微调进一步增强模块协同，从而在多任务上稳定超越现有方法。

Abstract: Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.
  We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.

</details>


### [28] [VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2601.10124)
*Sicheng Yang,Zhaohu Xing,Lei Zhu*

Main category: cs.CV

TL;DR: 提出VQ-Seg，用向量量化替代dropout做一致性学习的特征扰动，通过量化索引打乱的可控扰动、双分支重建+分割共享后量化空间、以及引入基础模型引导的PFA，缓解量化信息损失；在自建828例肺癌CT数据与公开基准上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学分割的特征扰动多依赖dropout，需手工调参且对超参敏感，容易导致欠/过正则；希望提供一种可控、稳定且无需精细调整的扰动机制，同时提升半监督分割在真实大规模肺癌CT上的效果。

Method: 1) 以向量量化离散化特征空间，提出量化扰动模块QPM：在codebook索引层面对空间位置进行shuffle以产生可控扰动，替代dropout。2) 设计双分支架构：后量化特征同时服务于图像重建与分割，使重建任务缓解量化带来的信息损失。3) 提出后VQ特征适配器PFA，引入基础模型(FM)的高层语义指导，补充量化损失的语义信息。4) 构建828例中央型肺癌CT的LC数据集并用于评测。

Result: 在自建LC数据集及多个公开基准上进行广泛实验，VQ-Seg在分割性能上整体优于最新方法，显示更强的一致性正则与泛化能力。

Conclusion: 用VQ离散表示结合QPM提供可控、稳定的特征扰动，配合重建分支与FM引导缓解量化损失，能在半监督医学分割中替代dropout并取得SOTA表现；方法具有实用性且在大型肺癌CT数据上验证有效。

Abstract: Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.

</details>


### [29] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: 论文提出LaViT，用对齐“潜在视觉思维”（注意力与视觉语义轨迹）而非静态嵌入来改进多模态推理蒸馏，显著提升视觉落地与复杂推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态蒸馏常只对齐教师的文本输出或静态特征，忽视了在推理过程中模型对图像的动态关注，导致学生模型用语言先验“对上答案”却看错地方（感知鸿沟）。

Method: 提出LaViT：在文本生成前，让学生自回归地重构教师的视觉语义与注意力轨迹（潜在视觉思维对齐），并引入课程式感官门控以逐步开放视觉线索、抑制捷径；相比对齐静态嵌入，更强调时序的视觉注意与语义状态。

Result: 在多项复杂视觉推理与落地评测上显著提升，最高提升+16.9%；一个3B参数模型可超过更大的开源模型并优于如GPT-4o等专有系统。

Conclusion: 对齐动态的潜在视觉思维能有效弥合蒸馏中的感知鸿沟，增强视觉落地与推理能力，且在小模型上具备高效与强竞争力。

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [30] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出视频异常推理(VAR)新任务与大规模数据集，并基于PerCoAct-CoT与新训练策略(Anomaly-Aware Group RPO)构建Vad-R1-Plus，实现分阶段、风险感知、决策导向的异常理解，实验优于现有开源与商用基线。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频异常领域多停留在定位或事后描述，缺乏显式推理、风险意识与决策解释，难以满足实际安防/监控等应用对因果解释与行动建议的需求。

Method: 1) 定义视频异常推理(VAR)任务，要求在回答问题前进行分阶段推理：感知(视觉事实)→认知(因果解释)→行动(风险/决策)。2) 构建包含8641视频与5万+样本的大型数据集，标注遵循Perception-Cognition-Action的链式思维(PerCoAct-CoT)，覆盖不同推理深度与问题类型。3) 训练策略：提出Anomaly-Aware Group Relative Policy Optimization，在弱监督下提升推理可靠性。4) 模型：端到端MLLM Vad-R1-Plus，支持自适应分层推理与风险感知决策。

Result: 在VAR基准上进行大量实验，Vad-R1-Plus在多项指标上显著超过开源与商用MLLM基线，验证所提任务、数据与方法对提升推理能力的有效性与稳健性。

Conclusion: 通过新任务、数据集、链式标注与优化策略，推动MLLM在视频异常从描述走向结构化、多阶段、面向决策的推理；Vad-R1-Plus展现更可靠的风险感知与推理能力，为实际异常监测与处置提供更强支持。

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [31] [RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2601.10168)
*Yue Chang,Rufeng Chen,Zhaofan Zhang,Yi Chen,Sihong Xie*

Main category: cs.CV

TL;DR: 提出RAG-3DSG，用不确定性引导与检索增强来提升开放词汇3D场景图的节点描述准确率，并以动态下采样映射大幅加速跨图像聚合；在Replica上显著提升准确率并将建图时间降至约1/3。


<details>
  <summary>Details</summary>
Motivation: 开放词汇3D场景图对机器人任务有价值，但现有方法因视角受限、遮挡、表面点密度冗余，导致物体级识别准确率低、处理慢。

Method: 1) 重拍引导的不确定性估计：通过重新拍摄/合成视角评估节点不确定性，缓解跨视角聚合噪声；2) 以低不确定性对象为锚进行对象级检索增强生成（RAG），改进节点语义标注/字幕；3) 动态下采样-映射策略：自适应粒度地降低表面点密度，加速跨图像对象聚合。

Result: 在Replica数据集上，节点描述（captioning）准确率显著提升；与基础版本相比，映射/建图时间减少约三分之二。

Conclusion: RAG-3DSG在开放词汇3DSG中同时提升准确率与效率，通过不确定性驱动的RAG与动态下采样实现稳健且快速的跨视角对象聚合，适合用于机器人场景理解与下游任务。

Abstract: Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.

</details>


### [32] [From Physical Degradation Models to Task-Aware All-in-One Image Restoration](https://arxiv.org/abs/2601.10192)
*Hu Gao,Xiaoning Lei,Xichen Xu,Xingjian Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出OPIR：通过预测任务感知的逆退化算子，以两阶段、含不确定性感知图的紧凑架构，实现高效的一体化图像复原，并在多任务与单任务场景均具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有一体化图像复原方法多依赖提示或大模型，额外模块导致系统复杂、速度慢，难以实时应用；需要一种既能适配多种退化任务、又高效可靠的统一框架。

Method: 从物理退化建模出发，学习预测“任务感知的逆退化算子”。两阶段流程：阶段1用所预测的逆算子生成初始复原图，并输出“不确定性感知图”以标注难以重建的区域；阶段2在不确定性引导下进一步细化复原。两阶段均复用同一逆算子预测网络，并在算子预测后引入任务感知参数以适配不同退化。通过加速该逆算子的卷积计算实现整体高效。

Result: 在多种数据集与任务上进行广泛实验，OPIR在一体化复原上取得优于现有方法的性能，同时在对应单任务（task-aligned）设定中仍具强竞争力与高效率。

Conclusion: 以逆退化算子为核心的OPIR在保持架构紧凑与高效的同时，实现可靠、可泛化的一体化图像复原；不确定性引导与加速算子卷积是性能与效率的关键。

Abstract: All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.

</details>


### [33] [ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation](https://arxiv.org/abs/2601.10200)
*Kim Youwang,Lee Hyoseok,Subin Park,Gerard Pons-Moll,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: ELITE提出一种基于高斯表示的人头化身生成框架，通过学习化初始化与测试时生成式自适应，在单目视频下高效生成可动画高保真头像，并在野外场景泛化良好，速度比2D生成先验方法快约60倍。


<details>
  <summary>Details</summary>
Motivation: 单目视频缺乏深度与遮挡信息，现有方法用3D先验或2D生成先验补偿：3D先验泛化差，2D先验耗时且易身份幻觉。需要一种结合两者优势、既高保真又高效且能在野外稳健泛化的方案。

Method: 1) 提出Mesh2Gaussian先验模型（MGPM），将网格快速初始化为高斯头像，实现快速前馈建模；2) 测试时生成式自适应：以真实与合成图像为监督，缩小域差；3) 引入渲染引导的单步扩散增强器，仅一步去噪、以高斯渲染为锚，恢复缺失细节，避免全扩散链条的慢与幻觉。

Result: 在具有挑战的表情与野外场景中，视觉质量优于现有方法；相较2D生成先验方案，合成速度提升约60倍。

Conclusion: 融合3D与2D先验的互补性，通过快速MGPM初始化与高效的测试时单步扩散增强，实现高保真、可动画且强泛化的人头头像生成，并显著提升效率与稳定性。

Abstract: We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.

</details>


### [34] [Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation](https://arxiv.org/abs/2601.10214)
*Dong-Yu Chen,Yixin Guo,Shuojin Yang,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: DepthDirector提出一种利用深度视频作为几何引导的再渲染框架，实现对给定视频在新相机轨迹下的精确相机控制与内容一致生成，结合双流条件与LoRA适配器，并构建多机位合成数据集，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通过3D表示扭曲实现相机控制的方法未充分利用视频扩散模型的3D先验，易陷入“修补陷阱”，导致主体不一致与画质下降，缺乏既能精确控制相机又能保持视频内容一致的方案。

Method: 1) 从显式3D表示渲染目标视角下的深度序列；2) 设计“视角-内容”双流条件：将源视频与目标视角的扭曲深度序列共同注入预训练VDM，作为几何与外观的联合引导；3) 采用轻量LoRA式视频扩散适配器训练，以保留VDM先验；4) 使用UE5构建多机位同步数据集MultiCam-WarpData用于训练与评估。

Result: 在相机可控性和视觉质量上显著优于现有方法，能在改变相机轨迹时保持动态场景与主体一致性并减少修补伪影。

Conclusion: 通过深度引导与双流条件，DepthDirector充分激发VDM的3D理解，实现精确相机控制与一致内容再渲染；LoRA适配器与大规模多机位数据集进一步提升性能与泛化。

Abstract: Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.

</details>


### [35] [Optimizing Multimodal LLMs for Egocentric Video Understanding: A Solution for the HD-EPIC VQA Challenge](https://arxiv.org/abs/2601.10228)
*Sicheng Yang,Yukai Huang,Shitong Sun,Weitong Cai,Jiankang Deng,Jifei Song,Zhensong Zhang*

Main category: cs.CV

TL;DR: 提出一套端到端管线（预处理→专用微调→时间链式思维提示→稳健后处理）来提升MLLM在HD-EPIC VQA上的表现，达41.6%准确率，说明复杂视频理解需整体优化。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在复杂视频问答中受限：问题/选项含糊、长时序推理薄弱、输出不规范，导致在如HD-EPIC VQA等基准上表现不佳。

Method: 1) 预处理：对查询与选项进行清洗、消歧与标准化；2) 模型：基于领域数据对Qwen2.5-VL进行微调；3) 推理：提出Temporal Chain-of-Thought（T-CoT）以多步时序推理；4) 后处理：对生成答案进行鲁棒标准化与匹配。

Result: 在HD-EPIC VQA上取得41.6%准确率。代码与微调模型开源于GitHub。

Conclusion: 整体式管线优化（从数据/提示到输出）是攻克高难度视频理解任务的关键；时间链式思维与领域微调能明显提升MLLM在长时序视频QA中的表现。

Abstract: Multimodal Large Language Models (MLLMs) struggle with complex video QA benchmarks like HD-EPIC VQA due to ambiguous queries/options, poor long-range temporal reasoning, and non-standardized outputs. We propose a framework integrating query/choice pre-processing, domain-specific Qwen2.5-VL fine-tuning, a novel Temporal Chain-of-Thought (T-CoT) prompting for multi-step reasoning, and robust post-processing. This system achieves 41.6% accuracy on HD-EPIC VQA, highlighting the need for holistic pipeline optimization in demanding video understanding. Our code, fine-tuned models are available at https://github.com/YoungSeng/Egocentric-Co-Pilot.

</details>


### [36] [Attend to what I say: Highlighting relevant content on slides](https://arxiv.org/abs/2601.10244)
*Megha Mariam K M,C. V. Jawahar*

Main category: cs.CV

TL;DR: 提出一种根据演讲者叙述自动在幻灯片上高亮最相关区域的方法，以降低听看切换带来的认知负荷并提升理解；并提供代码与数据集。


<details>
  <summary>Details</summary>
Motivation: 现场/视频报告中听众需在听讲与浏览整页幻灯片间频繁切换，难以迅速定位与当前讲述对应的关键信息，尤其在内容密集、节奏较快的场景；需要能理解多模态（文本、图形、版式）的系统来同步语言与视觉注意。

Method: 从演讲者的口语内容出发，解析语音转文本并与幻灯片中的文本与图形元素进行匹配，自动定位与当前叙述最相关的区域并进行高亮；作者比较多种实现方案（如不同对齐/匹配策略）并分析成功与失败案例。

Result: 证明该自动高亮策略在多媒体文档（教育视频、会议报告等）中能更好地将听觉与视觉信息同步，展示了有效性与局限；发布了实现代码与数据集以便复现与进一步研究。

Conclusion: 自动基于叙述高亮幻灯片关键区域可降低认知负担、提升内容获取，是多媒体理解的重要方向；方法与资源为后续研究与应用奠定基础。

Abstract: Imagine sitting in a presentation, trying to follow the speaker while simultaneously scanning the slides for relevant information. While the entire slide is visible, identifying the relevant regions can be challenging. As you focus on one part of the slide, the speaker moves on to a new sentence, leaving you scrambling to catch up visually. This constant back-and-forth creates a disconnect between what is being said and the most important visual elements, making it hard to absorb key details, especially in fast-paced or content-heavy presentations such as conference talks. This requires an understanding of slides, including text, graphics, and layout. We introduce a method that automatically identifies and highlights the most relevant slide regions based on the speaker's narrative. By analyzing spoken content and matching it with textual or graphical elements in the slides, our approach ensures better synchronization between what listeners hear and what they need to attend to. We explore different ways of solving this problem and assess their success and failure cases. Analyzing multimedia documents is emerging as a key requirement for seamless understanding of content-rich videos, such as educational videos and conference talks, by reducing cognitive strain and improving comprehension. Code and dataset are available at: https://github.com/meghamariamkm2002/Slide_Highlight

</details>


### [37] [DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset](https://arxiv.org/abs/2601.10305)
*Hengyu Shen,Tiancheng Gu,Bin Qin,Lan Wu,Yuling Wu,Shuo Tan,Zelong Sun,Jun Wang,Nan Wu,Xiang An,Weidong Cai,Ziyong Feng,Kaicheng Yang*

Main category: cs.CV

TL;DR: 提出并开源一个高质量中文跨模态数据集 DanQing（1亿图文对），用于提升中文视觉-语言预训练效果。基于更严格的数据筛选与最新（2024-2025）网页数据，持续预训练 SigLIP2 后在多项中文下游任务上显著领先。


<details>
  <summary>Details</summary>
Motivation: 中文视觉-语言预训练落后于英文的主要瓶颈是高质量中文图文数据稀缺，现有大规模公开数据（如 COYO、LAION）以英文为主，导致 CLIP/SigLIP 等在中文场景表现不足。

Method: 从 Common Crawl 抽取中文网页，构建严谨的数据管线进行图文对生成与清洗；采用更严格的质量筛选策略（相较现有数据集）；以 2024-2025 的新鲜数据为主，反映最新语义趋势；将数据用于对 SigLIP2 进行持续预训练并进行系统评测。

Result: 使用 DanQing 对 SigLIP2 持续预训练后，在中文零样本分类、跨模态检索以及基于多模态大模型（LMM）的评测上持续、全面优于使用现有数据集的对照。

Conclusion: DanQing 数据集（1亿中文图文对）能显著提升中文视觉-语言模型的表现，并因覆盖最新语义趋势而具更强实用性；计划以 CC-BY 4.0 许可开源，推动中文 VLP 研究。

Abstract: Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.

</details>


### [38] [Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313)
*Peng-Fei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: 提出HRA，一种用于视觉-语言预训练模型的层次化通用对抗攻击框架，分别在样本层与优化层细化UAP，并在图像与文本模态上联合施攻，显著提升跨任务、跨模型与跨数据集的通用攻击效果与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有VLP对抗攻击多为样本特定，扩展到大规模数据或新场景时计算开销大、迁移性差、易陷入局部最优，缺乏在多模态（图像-文本）上的通用扰动方案。

Method: 提出层次化精炼的通用扰动学习：1) 图像模态：将对抗样本解耦为干净图像与扰动分别处理；引入ScMix数据增强以丰富视觉上下文、提升UAP全局与局部效用、减少伪相关依赖；在优化层面利用历史与“未来”梯度的时间层次信息，优化路径避开局部最优并稳定UAP学习。 2) 文本模态：结合句内与句间的重要性度量，选出全局影响力词汇，作为通用文本扰动。

Result: 在多种下游任务、不同VLP模型与多个数据集上进行大量实验，所提通用多模态攻击在攻击强度、稳定性与泛化上均优于现有方法。

Conclusion: HRA通过在样本与优化两个层次的精炼、以及图像与文本双模态的协同通用扰动，突破了传统样本特定攻击的局限，提供了高效、可迁移、稳定的VLP通用攻击方案。

Abstract: Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.

</details>


### [39] [ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding](https://arxiv.org/abs/2601.10323)
*Xueyun Tian,Wei Li,Bingbing Xu,Heng Dong,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CV

TL;DR: ROMA是一种面向实时的全能多模态（音频/视觉/文本）助手，统一支持主动与被动交互；通过将连续音频与离散视频帧对齐为同步单元、引入“说话头”实现触发与生成解耦、配合流式课程学习与统一评测套件，达到在主动任务SOTA、被动任务具竞争力的效果。


<details>
  <summary>Details</summary>
Motivation: 现有流式视听理解方案要么模态支持不全、要么缺乏自主主动监测与响应，导致无法在真实场景中稳定进行在线理解与交互；评测也分裂，难以客观比较。

Method: 1) 输入建模：将连续音频与离散视频帧同步成多模态单元，缓解时间粒度不匹配。2) 在线决策：设计轻量“speak head”，将“何时开始回应”的触发与具体内容生成解耦，避免任务冲突并保证精确触发。3) 训练：构建定制流式数据集，采用两阶段课程学习（先适应流式格式，再优化主动响应性）。4) 评测：重组多种基准为统一套件，覆盖主动（告警、旁白）与被动（问答）两类设置。

Result: 在12个基准上实验显示：ROMA在主动任务（如告警、叙述）达到SOTA，在被动问答任务上具备竞争力，体现了其实时全模态理解的稳健性。

Conclusion: ROMA通过同步多模态单元、触发-生成解耦与针对流式的训练策略，实现统一的实时主动/被动交互，显著提升流式视听理解与评测的一致性与性能。

Abstract: Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.

</details>


### [40] [SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition](https://arxiv.org/abs/2601.10324)
*Yiming Zhang,Weibo Qin,Yuntian Liu,Feng Wang*

Main category: cs.CV

TL;DR: 提出SRAW空间重加权对抗形变攻击：通过对前景/背景分配不同预算的可学习空间形变，生成对SAR-ATR模型高转移性且难以察觉的对抗样本，显著降低SOTA性能。


<details>
  <summary>Details</summary>
Motivation: SAR图像稀疏、DNN易受对抗样本影响且过度依赖背景。现有SAR-ATR攻击往往需要可感知的噪声/形变才能有效，难以兼顾隐蔽性与有效性，并缺乏对前景与背景差异化约束。

Method: 提出Space-Reweighted Adversarial Warping（SRAW）：以空间形变而非像素加噪为主，优化一个形变场，对前景与背景设定重加权预算与约束，控制形变幅度与分布，实现在保持视觉不可察觉的同时最大化分类器损失；支持黑/白盒并强调跨模型转移。

Result: 在多种SOTA SAR-ATR模型上，SRAW显著降低识别精度；与现有方法相比，具更高隐蔽性（更低可感知度指标）与更强的对抗迁移能力；在前景/背景 reweighting 下，鲁棒性退化更明显。

Conclusion: 空间重加权形变是攻击SAR-ATR的有效且隐蔽途径；针对前景/背景分配差异化预算可提升攻击效率与迁移性，为评估与提升SAR-ATR鲁棒性提供新基线与挑战。

Abstract: Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.

</details>


### [41] [Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders](https://arxiv.org/abs/2601.10332)
*Siqi Kou,Jiachun Jin,Zetong Zhou,Ye Ma,Yugang Wang,Quan Chen,Peng Jiang,Xiao Yang,Jun Zhu,Kai Yu,Zhijie Deng*

Main category: cs.CV

TL;DR: 本文提出“先思考再生成”(T2G)范式：让LLM文本编码器先进行推理与重写，再将重写状态用于扩散模型条件，从而提升事实一致性与语义对齐，并显著改善视觉质量，在推理型生成/编辑基准上获得接近GPT-4的WISE 0.79分。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型多将LLM当作静态文本编码器，缺乏对提示语的推理、补全与世界知识调用能力，导致生成结果往往流于字面匹配，事实性与复杂指令理解不足。

Method: 提出T2G两阶段：1) 轻量监督微调激活LLM编码器的“思考-重写”模式；2) 使用Dual-GRPO联合优化文本编码器与扩散骨干。文本端用基于图像的奖励强化，使其更好地推理、回忆世界知识并输出重写提示；扩散端被优化以提升语义一致和视觉连贯。最终用重写后的提示状态作为扩散条件生成图像。

Result: 在多项需要推理的图像生成与编辑基准上，模型在事实一致性、语义对齐和视觉真实感方面显著优于基线，WISE得分达0.79，接近GPT-4。

Conclusion: 将LLM的推理显式注入T2I流程，通过“先思考再生成”和Dual-GRPO的联合强化，可有效提升复杂语义理解与表达，朝向集推理、表达与示范于一体的下一代统一模型迈进一步。

Abstract: Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.

</details>


### [42] [An analytic theory of convolutional neural network inverse problems solvers](https://arxiv.org/abs/2601.10334)
*Minh Hai Nguyen,Quoc Bao Do,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

TL;DR: 提出LE-MMSE：在平移等变与局部感受野约束下的MMSE解析形式，用以解释CNN在成像逆问题中的行为，并与多数据集/架构实验高度吻合。


<details>
  <summary>Details</summary>
Motivation: 监督式CNN在成像逆问题上效果极佳，但理论可解释性不足，被当作黑箱。作者希望以统计最优估计视角（MMSE）并结合CNN的两大归纳偏置（平移等变、局部性）来建立可解析的理论框架，解释网络为何有效及其适用边界。

Method: 在经验训练分布下，将MMSE估计器加入函数约束：1) 平移等变（卷积结构）；2) 有限感受野（局部性），得到局部-等变MMSE（LE-MMSE）。导出可解析的闭式/可计算公式。随后在多类逆问题（去噪、修复、反卷积）、多数据集（FFHQ、CIFAR-10、FashionMNIST）和多种架构（U-Net、ResNet、PatchMLP）上比较LE-MMSE与训练CNN输出的一致性。

Result: LE-MMSE与实际训练的CNN输出在PSNR上高度匹配（≥约25 dB），表明该解析估计器能准确刻画网络行为。实验还揭示：物理先验显式融入的估计器（physics-aware）与不融入的（physics-agnostic）在表现与偏差上的差异；训练补丁分布中高密度区域对估计器有主导作用；数据量、补丁大小等因素显著影响性能与泛化。

Conclusion: 将CNN的结构偏置形式化为受限MMSE可得到可解释且可计算的估计器，能在多场景下贴合真实网络输出，为理解与设计成像逆问题中的深度模型提供理论依据，并指向通过控制数据与补丁分布、物理先验和感受野设置来优化性能的策略。

Abstract: Supervised convolutional neural networks (CNNs) are widely used to solve imaging inverse problems, achieving state-of-the-art performance in numerous applications. However, despite their empirical success, these methods are poorly understood from a theoretical perspective and often treated as black boxes. To bridge this gap, we analyze trained neural networks through the lens of the Minimum Mean Square Error (MMSE) estimator, incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality via finite receptive fields. Under the empirical training distribution, we derive an analytic, interpretable, and tractable formula for this constrained variant, termed Local-Equivariant MMSE (LE-MMSE). Through extensive numerical experiments across various inverse problems (denoising, inpainting, deconvolution), datasets (FFHQ, CIFAR-10, FashionMNIST), and architectures (U-Net, ResNet, PatchMLP), we demonstrate that our theory matches the neural networks outputs (PSNR $\gtrsim25$dB). Furthermore, we provide insights into the differences between \emph{physics-aware} and \emph{physics-agnostic} estimators, the impact of high-density regions in the training (patch) distribution, and the influence of other factors (dataset size, patch size, etc).

</details>


### [43] [Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs](https://arxiv.org/abs/2601.10369)
*Ningyu Sun,Zhaolin Cai,Zitong Xu,Peihang Chen,Huiyu Duan,Yichao Yan,Xiongkuo Min,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出HPE-Bench基准与基于层选择的多模态大模型框架，统一评估文本驱动人体姿态编辑的真伪与多维质量，显著提升检测与回归表现。


<details>
  <summary>Details</summary>
Motivation: 当前文本驱动的人体姿态编辑常出现结构失真与伪影；现有指标将真伪检测与质量评估割裂，难以细粒度定位姿态相关不一致性，需要统一且细粒度的评测与模型。

Method: 1) 构建HPE-Bench：包含来自17个SOTA模型的1700个标准化样本，提供真伪标签与多维质量分数；2) 提出基于多模态大语言模型的统一框架：采用对比式LoRA微调，并引入层敏感性分析（LSA）以选择最优特征层进行姿态评估，实现层选择的MLLM；3) 在同一框架下同时进行真伪检测与多维质量回归。

Result: 所提框架在真实性检测与多维质量回归上均优于现有方法；HPE-Bench为姿态编辑的一致性与质量提供了可比、细粒度评测。

Conclusion: HPE-Bench填补了姿态编辑评测空白，所提层选择MLLM统一了取证与质量评估，提升了对姿态相关异常与质量维度的识别与量化能力。

Abstract: Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.

</details>


### [44] [Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement](https://arxiv.org/abs/2601.10373)
*Yichong Xia,Yimin Zhou,Jinpeng Wang,Bin Chen*

Main category: cs.CV

TL;DR: 提出DiffCR：结合频率感知跳跃估计与一致性估计，实现扩散先验驱动的极低码率高保真图像压缩，且解码两步、速度提升>10×、显著节码。


<details>
  <summary>Details</summary>
Motivation: 现有扩散先验图像压缩在极低码率下视觉质量佳，但存在两大痛点：1) 采样慢导致编码/解码效率低；2) 训练割裂（先验与压缩侧不对齐）造成比特分配与重建次优。需要一个既高效又端到端协同的框架。

Method: 提出DiffCR框架：1) 频率感知跳跃估计（FaSE）模块，在预训练潜空间扩散模型的ε-pred先验基础上，通过频率解耦注意力（FDA）对不同时间步的压缩潜变量进行对齐与细化，显式建模高/低频成分；2) 轻量一致性估计器，学习保持扩散采样的语义轨迹，使解码可用两步完成（跳过多数采样步骤）；3) 不更新主干扩散模型，仅在侧模块上训练，实现与压缩管线协同。

Result: 在不微调主扩散骨干的情况下，相比SOTA扩散压缩方法取得显著改进：BD-rate 降低27.2%（LPIPS度量）与65.1%（PSNR度量），解码速度提升超过10×。

Conclusion: 通过将频率解耦的先验细化与一致性引导结合，DiffCR在极低码率下实现高保真、快速解码与更优比特利用，证明无需改动扩散骨干即可兼得质量与效率。

Abstract: Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \textbf{Diff}usion-based Image Compression via \textbf{C}onsistency Prior \textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $ε$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\% BD-rate (LPIPS) and 65.1\% BD-rate (PSNR)) and over $10\times$ speed-up compared to SOTA diffusion-based compression baselines.

</details>


### [45] [Global Context Compression with Interleaved Vision-Text Transformation](https://arxiv.org/abs/2601.10378)
*Dian Jiao,Jiaxin Duan,Shuai Zhao,Jiabing Leng,Yiran Zhang,Feng Huang*

Main category: cs.CV

TL;DR: VIST2提出用视觉编码全局压缩上下文，让Transformer在预填与推理阶段都依赖视觉token，从而以更少token完成长文本建模与生成，加速并省显存/FLOPs。


<details>
  <summary>Details</summary>
Motivation: 现有将文本部分转为图像进行prefill的方案只能在预填阶段省token，推理时仍需逐token耗费注意力计算，无法全程降本。需要一种在全局上下文层面压缩、同时在prefill与逐步生成阶段都能减少token与计算的方法。

Method: 提出VIST2：将输入文本分块，并将每块渲染为“草图式”图像，与原文本块交错输入Transformer；模型在预测下一个文本token时只依赖其之前的视觉token（不再用之前的文本token）。训练采用多阶段：1）课程式预训练进行“光学语言建模”（从视觉草图学习语言分布）；2）模态交错的指令微调。缩放模型规模0.6B到8B，系统探索训练配方与超参。

Result: 在4×上下文压缩下，VIST2在长文本写作任务上较基线显著提升：首token生成速度提升约3×，显存占用减少77%，FLOPs减少74%。

Conclusion: 全局上下文视觉压缩可在prefill与推理两阶段同时降本提效。VIST2验证了以视觉token承载历史上下文的可行性与高效性，为长上下文生成提供新的模型设计路径；代码与数据将开源以促进研究。

Abstract: Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.

</details>


### [46] [Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer](https://arxiv.org/abs/2601.10386)
*Filippo Ruffini,Camillo Maria Caruso,Claudia Tacconi,Lorenzo Nibid,Francesca Miccolis,Marta Lovino,Carlo Greco,Edy Ippolito,Michele Fiore,Alessio Cortellini,Bruno Beomonte Zobel,Giuseppe Perrone,Bruno Vincenzi,Claudio Marrocco,Alessandro Bria,Elisa Ficarra,Sara Ramella,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 提出一种对缺失模态鲁棒的多模态生存预测框架，结合CT、病理WSI与临床变量，利用基础模型提特征与缺失感知编码，实现中间层融合，能在模态不完整下训练/推断，优于单模态及早/晚融合；WSI+临床最佳（C-index 0.733），模型会自适应下调信息较弱的CT。


<details>
  <summary>Details</summary>
Motivation: 现实NSCLC数据规模小、模态缺失普遍，常需删样本或激进插补，限制MDL在生存预测中的临床可用性。需要一种能在模态不完备下仍能有效融合、提升预后预测的方案。

Method: 采用基础模型分别从CT与WSI提取表征，结合结构化临床变量；设计缺失感知编码器与中间层多模态融合机制，使各模态在存在缺失时仍可参与；训练端与推断端均无需剔除患者。比较早/中/晚融合与单模态；分析模态重要性与自适应加权。

Result: 中间融合在各设定下稳定优于单模态与早/晚融合；WSI+临床融合达最佳C-index 0.733；模型能自动降低信息较弱（如CT）模态的权重，从而提升总体生存预测性能。

Conclusion: 缺失感知的中间层多模态生存建模在不完整模态环境下有效、鲁棒且可用，避免删样本/强插补，改进NSCLC生存预测；自适应权重机制有助于在模态质量不均时保持性能。

Abstract: Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.

</details>


### [47] [Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy](https://arxiv.org/abs/2601.10392)
*Hassan Eshkiki,Sarah Costa,Mostafa Mohammadpour,Farinaz Tanhaei,Christopher H. George,Fabio Caraffini*

Main category: cs.CV

TL;DR: 提出一种将时间序列荧光显微视频融合为单帧高质量复合图像的计算框架。在心肌细胞复杂数据集上，能在保留生物学信息的同时抑噪、稳定时变信号、统一可视化；较以往方法平均提升细胞计数44%。


<details>
  <summary>Details</summary>
Motivation: 活细胞荧光显微成像常受噪声、时间波动和振荡信号可视化不一致的限制，影响标注和下游分割等分析。需要一种既能综合多帧信息又不丢失生物学内容的稳健表示方式。

Method: 提出一个可解释的多步管线，融合来自不同计算机视觉领域的技术，将时间分辨的多帧栈整合为单幅复合图像。通过对111种配置进行评估，在动态、异质、形态复杂的心肌细胞2D单层视频上测试，确保在保留单帧信息的同时提升整体质量与可视一致性。

Result: 在具有挑战的数据集上生成的复合图像既保真又增强细节，相比既有方法，平均细胞计数提高44%，并改善信号可视化与稳定性。

Conclusion: 该管线有效地把多时刻图像融合为高质量2D表示，便于人工标注与自动分割，并可推广到其他需多时序堆栈融合的成像领域。

Abstract: Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.

</details>


### [48] [Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation](https://arxiv.org/abs/2601.10449)
*Clementine Grethen,Nicolas Menga,Roland Brochard,Geraldine Morin,Simone Gasparini,Jeremy Lebreton,Manuel Sanchez Gestido*

Main category: cs.CV

TL;DR: 提出Lunar-G2R：从月面DEM直接预测空间可变BRDF参数，通过可微渲染与轨道影像监督训练，无需多视图/受控光照/专用采集，显著降低光度误差并提升PSNR/SSIM，首次从地形几何直接推断空间可变反射特性。


<details>
  <summary>Details</summary>
Motivation: 现有登月渲染与视觉导航常用简化或全局统一的BRDF，参数难以估计且无法刻画局部反射差异，导致真实感欠佳并制约基于外观的感知与导航。因此需要一种能在缺乏复杂采集条件下，从可得的几何数据中恢复空间变化反射率的方法。

Method: 以DEM为输入，使用U-Net预测每个像素的BRDF参数；训练阶段利用可微渲染器，在已知光照与观测几何下将预测反射率与DEM渲染成影像，并与真实轨道影像做光度损失最小化，端到端学习几何到反射率的映射；测试时仅需DEM即可输出SVBRDF参数。

Result: 在Tycho坑地理留出区域测试，较SOTA基线光度误差降低38%，同时PSNR/SSIM与感知相似度更高，能够捕获传统空间均匀模型缺失的细尺度反射变化。

Conclusion: Lunar-G2R无需多视图或专用硬件，即可从地形几何直接推断空间可变反射模型，推动高保真月面渲染与基于视觉的导航；据称为首个从几何直接推断SVBRDF的行星表面方法。

Abstract: We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.

</details>


### [49] [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://arxiv.org/abs/2601.10477)
*Yu Wang,Yi Wang,Rui Dai,Yujie Wang,Kaikui Liu,Xiangxiang Chu,Yansheng Li*

Main category: cs.CV

TL;DR: 提出SocioSeg数据集与SocioReasoner框架，实现对卫星图像中社会语义实体（如学校、公园）的分割，通过视觉-语言推理与强化学习显著优于现有方法，并具备强零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型对物理属性明确的对象表现良好，但难以处理由社会定义的类别（如学校、医院、园区等），这些类别在下游城市计算与规划中至关重要，需要结合地理影像与语义线索进行推理。

Method: 构建包含卫星影像、数字地图以及按层级组织的社会语义像素级标注的数据集SocioSeg；提出SocioReasoner：一个模拟人类标注流程的视觉-语言推理框架，采用跨模态识别与多阶段推理，并用强化学习优化该不可微流程以激发VLM的推理能力。

Result: 在多个基准上超过SOTA分割模型；展现出强零样本泛化能力，能对未见过的社会语义类别进行有效分割。

Conclusion: 通过数据与方法双驱动，将VLM推理引入社会语义分割，可有效识别卫星图像中的社会定义实体，提升泛化与实用性；数据与代码开源以促进后续研究。

Abstract: As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.

</details>


### [50] [mergetune: Continued fine-tuning of vision-language models](https://arxiv.org/abs/2601.10497)
*Wenqing Wang,Da Li,Xiatian Zhu,Josef Kittler*

Main category: cs.CV

TL;DR: 提出MERGETUNE：一种在已适配后继续微调以恢复CLIP等预训练知识的方法，通过线性模态连通性寻找同时连到零样本和已微调解的“继续模型”，无需架构改动与大规模回放，显著提升基新泛化与稳健性。


<details>
  <summary>Details</summary>
Motivation: VLM（如CLIP）在下游适配（如CoOp）时易灾难性遗忘预训练知识。现有方法多在适配阶段抑制遗忘，但遗忘仍难避免；因此需要在“适配完成后”恢复被遗忘的预训练能力。

Method: 提出继续微调（CFT）范式与模型无关的MERGETUNE策略：在已微调模型的可训练参数上继续优化，寻找一个“继续模型”，使其到零样本模型（CLIP）与已微调模型（如CoOp）各自存在低损连接路径（线性模态连通性，LMC）。通过利用损失景观几何，该继续模型隐式合并两解以恢复预训练知识。为避免标准LMC需预训练数据回放的问题，提出二阶近似替代对零样本模型的约束，从而不需大规模回放数据。

Result: 在基-新类别泛化上，使CoOp的调和平均提升+5.6%。在鲁棒微调评测中，MERGETUNE得到的LMC合并模型优于集成基线且推理成本更低；与零样本模型再集成可进一步取得SOTA。并报告在跨数据集传输（如DTD、EuroSAT）上优于CLIP。

Conclusion: 通过CFT与LMC引导的MERGETUNE，可在不增参、无需大规模回放的条件下，将零样本与已微调解合并，显著缓解并“事后”修复遗忘，提升泛化与鲁棒性，具有通用、后处理式适用性。

Abstract: Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.

</details>


### [51] [SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction](https://arxiv.org/abs/2601.10512)
*Kanak Mazumder,Fabian B. Flohr*

Main category: cs.CV

TL;DR: SatMap提出将卫星图像作为全局先验，与多相机BEV特征融合，在线直接输出矢量化HD地图；在nuScenes上显著超越相机-only与相机+激光雷达融合基线，尤其在长距与恶劣天气下受益明显。


<details>
  <summary>Details</summary>
Motivation: 纯相机在线构图易受深度歧义与遮挡影响，精度与稳健性不足；而仅依赖车载传感器缺乏全球语义与远距信息。将具车道级纹理与语义的卫星图作为先验，可缓解遮挡、提升远距与复杂环境下的制图质量，服务端到端预测与规划。

Method: 提出SatMap：1) 从卫星BEV影像提取车道级语义与纹理，作为全局先验；2) 融合多视角相机特征至BEV表示；3) 设计跨模态对齐与融合模块，将卫星先验与相机BEV对齐；4) 端到端直接预测矢量化HD地图（道路/车道要素），供下游模块使用。

Result: 在nuScenes上，相比相机-only基线mAP提升34.8%，相较相机+LiDAR融合基线提升8.5%；在长距离与恶劣天气评测中保持优势，显示卫星先验的有效性。

Conclusion: 融合卫星BEV先验能有效缓解相机深度与遮挡问题，提升在线矢量化HD制图精度与鲁棒性，适用于端到端AD的预测与规划。

Abstract: Online high-definition (HD) map construction is an essential part of a safe and robust end-to-end autonomous driving (AD) pipeline. Onboard camera-based approaches suffer from limited depth perception and degraded accuracy due to occlusion. In this work, we propose SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. Our method leverages lane-level semantics and texture from satellite imagery captured from a Bird's Eye View (BEV) perspective as a global prior, effectively mitigating depth ambiguity and occlusion. In our experiments on the nuScenes dataset, SatMap achieves 34.8% mAP performance improvement over the camera-only baseline and 8.5% mAP improvement over the camera-LiDAR fusion baseline. Moreover, we evaluate our model in long-range and adverse weather conditions to demonstrate the advantages of using a satellite prior map. Source code will be available at https://iv.ee.hm.edu/satmap/.

</details>


### [52] [BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition](https://arxiv.org/abs/2601.10521)
*Max A. Buettner,Kanak Mazumder,Luca Koecher,Mario Finkbeiner,Sebastian Niebler,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出FUSE-Bike感知平台与BikeActions多模态数据集，从骑行者视角采集并标注VRU（尤其自行车场景）动作，给出基线模型与开源资源。


<details>
  <summary>Details</summary>
Motivation: 现有AD/机器人主要从车端视角研究行人过街预测，忽视密集共享空间中VRU交互；缺少从骑行者视角的高保真近距数据与标准基线。

Method: 构建FUSE-Bike平台（双激光雷达、相机、GNSS）在骑行者视角采集数据；整理并标注BikeActions数据集（852样本，5类动作，多模态）；基于公开划分评测图卷积与Transformer模型，形成基线。

Result: 获得一个面向VRU动作理解的数据集与工具链；给出多种SOTA模型在该数据上的首个性能基线；完整开源数据、硬件设计与基准代码。

Conclusion: 从骑行者视角的多模态感知与动作数据能补全VRU意图预测研究版图，为密集共享空间的安全交互提供基础；所释出平台与基线为后续方法改进与公平对比提供标准。

Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.

</details>


### [53] [SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery](https://arxiv.org/abs/2601.10535)
*Chong Liu,Luxuan Fu,Yang Jia,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: 提出SVII-3D框架，用稀疏影像也能实现城市资产的鲁棒识别、分米级三维定位与细粒度状态诊断，显著提高识别精度并降低定位误差，实现低成本高保真数字孪生与资产清单构建。


<details>
  <summary>Details</summary>
Motivation: 智慧城市与设施全生命周期管理需要自动化、高精度的数字孪生与资产盘点，但现有基于低成本稀疏影像的方法在鲁棒性、定位精度与细粒度状态理解方面不足。

Method: 1) 用LoRA微调的开集检测器结合空间注意力匹配网络，跨稀疏视角稳健关联观测；2) 引入几何引导的细化机制，纠正结构误差，实现分米级3D定位；3) 融合多模态提示的视觉-语言模型Agent，对设备进行自动化细粒度运行状态诊断。

Result: 实验显示该方法显著提高资产识别准确率并减小定位误差，能够在稀疏感知条件下获得精确的3D定位和状态判断。

Conclusion: SVII-3D为基础设施高保真数字化提供可扩展、低成本方案，有效弥合稀疏感知与自动化智能运维之间的差距。

Abstract: The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.

</details>


### [54] [Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning](https://arxiv.org/abs/2601.10537)
*Oscar H. Ramírez-Agudelo,Akshay N. Shewatkar,Edoardo Milana,Roland C. Aydin,Kai Franke*

Main category: cs.CV

TL;DR: 提出用深度学习（FFA-Net、AECR-Net）增强烟雾/雾霾环境下的模拟指针表计图像可读性；基于Unreal Engine合成超1.4万张数据集训练，AECR-Net更稳健；雾霾上PSNR≈43 dB、SSIM≈0.98，烟雾上效果较弱但仍可用于后续自动读表。


<details>
  <summary>Details</summary>
Motivation: 在灾害或紧急场景中，烟雾/雾霾导致监测设施（如压力/流量等模拟表计）可视性下降，妨碍第一响应者获取关键信息；缺乏现成基准数据集也制约了算法研究。

Method: 1) 使用Unreal Engine合成带轻到重雾霾与烟雾退化的模拟表计图像数据集（>14,000张），按8/1/1划分训练/验证/测试；2) 选用两种去雾深度网络FFA-Net与AECR-Net进行端到端训练；3) 用PSNR、SSIM评估，比较两模型在雾霾与烟雾上的表现；4) 对增强图像进行后续自动读表测试。

Result: 在合成雾霾集上，达到SSIM≈0.98、PSNR≈43 dB，达SOTA水平；AECR-Net整体优于FFA-Net且更稳健。合成烟雾集结果较差（因烟雾不均匀且密度高、模型为去雾而非去烟设计），但仍能显著提升可视性并支持后续自动读表。

Conclusion: 深度学习去雾架构可显著提升烟雾/雾霾环境下模拟表计图像质量与机器可读性；AECR-Net优于FFA-Net。烟雾场景仍具挑战，需针对性“去烟”建模与更贴近真实的数据以进一步提升。

Abstract: Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\% train, 10\% validation, and 10\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges

</details>


### [55] [Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure](https://arxiv.org/abs/2601.10551)
*Luxuan Fu,Chong Liu,Bisheng Yang,Zhen Dong*

Main category: cs.CV

TL;DR: 提出将VLM适配为城市道路侧设施分析代理：用Grounding DINO开放词汇微调做鲁棒定位，Qwen-VL经LoRA做属性推理，并以双模态RAG检索行业标准与视觉样例以减少幻觉与确保合规；在新数据集上达58.9 mAP与95.5%属性准确率。


<details>
  <summary>Details</summary>
Motivation: 通用模型难以识别道路基础设施的细粒度属性与遵循工程规范；现有大视觉语言模型虽擅长开放域识别，但对复杂设施状态与标准合规性解释不可靠，导致落地受限。

Method: 提出领域自适配框架：1) 以开放词汇微调的Grounding DINO实现多类资产的少监督稳健定位；2) 采用LoRA对Qwen-VL进行领域属性与语义推理适配；3) 设计双模态RAG，在推理时动态检索权威行业文本标准与视觉示例以约束推理、减少幻觉。

Result: 在新建的城市道路场景数据集上，目标检测mAP为58.9，属性识别准确率95.5%，表现稳健。

Conclusion: 通过数据高效微调与知识增强推理，将VLM转化为面向基础设施感知的专业代理，兼顾定位与属性合规推理，提升可靠性，适合智能基础设施监测场景。

Abstract: Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.

</details>


### [56] [Inference-time Physics Alignment of Video Generative Models with Latent World Models](https://arxiv.org/abs/2601.10553)
*Jianhao Yuan,Xiaofeng Zhang,Felix Friedrich,Nicolas Beltran-Velez,Melissa Hall,Reyhane Askari-Hemmat,Xiaochuang Han,Nicolas Ballas,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: 提出WMReward，将视频生成的物理合理性视为推理时对齐问题，用潜在世界模型(VJEPA-2)作为奖励在去噪轨迹中搜索与引导，显著提升多种条件下视频的物理可信度，并在ICCV 2025 PhysicsIQ竞赛中夺冠。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型视觉质量高但常违背基本物理规律。除预训练物理知识不足外，作者发现推理策略欠佳同样导致物理不合理，因此希望在不重新训练大模型的情况下，于推理阶段提升物理一致性。

Method: 构建WMReward：以具备强物理先验的潜在世界模型(VJEPA-2)作为奖励函数，对扩散式视频生成的多条候选去噪轨迹进行并行搜索与引导(steer)，在测试时用更多计算探索并选择物理更合理的轨迹，从而实现推理时对齐。适用于图像条件、多帧条件与文本条件的生成。

Result: 在多种设定下显著提升物理合理性，获得人类偏好验证；在ICCV 2025 Perception Test PhysicsIQ Challenge上取得62.64%最终分，较此前SOTA提升7.42%，排名第一。

Conclusion: 潜在世界模型可作为通用的物理奖励，在无需重新训练的前提下，通过推理时对齐有效提升视频生成的物理可信度；该思路具有可扩展性，不局限于具体模型或参数化。

Abstract: State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.

</details>


### [57] [DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery](https://arxiv.org/abs/2601.10554)
*Constantin Selzer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出DeepUrban无人机视角的密集城市交通数据集，补足现有基准在高密度场景的缺失；与nuScenes结合可显著提升SOTA轨迹预测与规划（ADE/FDE最高提升44.1%/44.3%）。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶基准缺乏密集交通场景，难以研究复杂交互与泛化能力，因此需要一个专注于城市密集路口、支持预测与规划评测的新数据集。

Method: 与工业伙伴合作采集约100米高空无人机在城市路口的高分辨率图像，构建含3D交通目标、地图与场景信息的数据集DeepUrban；在此数据上评测SOTA预测与规划方法，并进行与nuScenes联合训练/评测的泛化实验。

Result: 将DeepUrban与nuScenes结合可显著提升车辆轨迹预测与规划精度，ADE/FDE最高分别提升44.1%与44.3%。

Conclusion: DeepUrban为密集城市场景提供高质量数据与地图/场景标注，能增强现有基准并提升方法在复杂交互下的性能与泛化能力；资源与更多信息见项目网站。

Abstract: The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban

</details>


### [58] [Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation](https://arxiv.org/abs/2601.10577)
*Serena Grazia De Benedictis,Amedeo Altavilla,Nicoletta Del Buono*

Main category: cs.CV

TL;DR: 提出一种基于数字拓扑与Jordan曲线定理的“拓扑感知”分割一致性判定：若分割掩码可导出满足β0=β1=1的数字4-曲线（或其补集恰分成两个8连通分量），则认为结构上连贯。作为无监督、严格的结构评估标准，弥补常见像素/区域/边界指标无法反映全局形状与连通性的缺陷。


<details>
  <summary>Details</summary>
Motivation: 常用分割评估指标（像素、区域、边界）对细小边界误差、孔洞、碎片化不敏感，可能给出高分但破坏对象的全局形状与连通性，尤其在医学影像等需保持拓扑正确性的场景。因此需要能判定“是否真正把图像分成有意义的内外区域”的拓扑一致性标准。

Method: 以Jordan曲线定理在数字平面中的对应为基础：从二值掩码中提取候选数字4-曲线；用数字拓扑与同调（Betti数）验证其拓扑有效性。具体标准：候选曲线形成数字4-曲线且β0=β1=1；等价条件为掩码的补集正好分裂为两个8连通分量。

Result: 得到一个数学上严格、无需监督的结构一致性判定框架，可作为对现有指标的补充，用以检测掩码是否保持全局形状与连通性，并能区分虽然指标高但拓扑错误的结果。

Conclusion: 数字Jordan理论结合同调不变量可为分割评估提供拓扑正确性标准。该框架在需保持结构连贯性的应用（如医学图像、目标轮廓）中比传统指标更具判别力，可作为评测或后处理的可靠替代/补充。

Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.

</details>


### [59] [Adversarial Evasion Attacks on Computer Vision using SHAP Values](https://arxiv.org/abs/2601.10587)
*Frank Mollard,Marcus Becker,Florian Roehrbein*

Main category: cs.CV

TL;DR: 论文提出一种利用SHAP值的白盒对抗攻击方法，并与FGSM比较，显示在梯度隐藏场景下SHAP攻击更易诱发误分类且更稳健。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击常依赖梯度信息，遇到梯度遮蔽/隐藏、非光滑性或防御技巧时效果下降；需要一种在推理阶段能精确度量输入特征重要性的攻击机制，以生成更隐蔽且更有效的扰动。

Method: 在白盒设定下，利用SHAP值评估各输入像素/区域对模型输出（置信度/分类）的边际贡献，按重要性方向与幅度施加最小可感扰动，实现降低置信度或诱导误分类；并与Fast Gradient Sign Method进行对比实验，尤其考察梯度隐藏场景。

Result: 实验证明：SHAP驱动的扰动在相似或更小幅度的噪声下更能降低模型置信度并产生误分类；在存在梯度隐藏/非平滑等情况时，相比FGSM，SHAP攻击成功率更高、稳定性更强。

Conclusion: 基于SHAP的重要性估计可作为有效的白盒对抗攻击手段，具备较强的隐蔽性与鲁棒性，尤其在梯度隐藏情境下优于FGSM；提示需要针对解释性驱动攻击的防御策略。

Abstract: The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.

</details>


### [60] [Action100M: A Large-scale Video Action Dataset](https://arxiv.org/abs/2601.10592)
*Delong Chen,Tejaswi Kasarla,Yejin Bang,Mustafa Shukor,Willy Chung,Jade Yu,Allen Bolourchi,Theo Moutakanni,Pascale Fung*

Main category: cs.CV

TL;DR: 提出Action100M：从120万教学视频自动构建的开词表动作数据集，含约一亿个时间定位片段与结构化标注，训练VL-JEPA在多基准上零样本表现强并随数据扩大持续提升。


<details>
  <summary>Details</summary>
Motivation: 视觉系统要在物理世界中推理“谁在做什么”的动作，但现有视频数据集规模小、词表封闭、领域窄，难以支撑通用动作理解与世界建模。需要一个大规模、开放词表、覆盖广泛领域、且具时间定位与丰富语义的动作数据集，并且能以自动化流水线构建以扩展。

Method: 构建自动流水线：1) 使用V-JEPA 2 的嵌入进行层级时间分割，获得多粒度片段；2) 生成多层级帧与片段描述，组织为“Tree-of-Captions”；3) 以大型推理模型GPT-OSS-120B在多轮Self-Refine中聚合证据，产出结构化标注（简/详动作、主体、简/详字幕）。在此数据上训练VL-JEPA并评估。

Result: 得到Action100M：来自120万网络教学视频（总时长约14.6年），规模约一亿个时间定位片段，具开放词表动作监督与丰富描述。将VL-JEPA在Action100M上训练，展示稳定的数据扩展收益，并在多种动作识别基准上实现强零样本性能。

Conclusion: Action100M成为视频理解与世界建模的可扩展新基石：自动化、开放词表、时间定位、语义丰富；在此之上训练的模型随数据规模持续提升，并在下游基准上具备强零样本泛化。

Abstract: Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.

</details>


### [61] [RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation](https://arxiv.org/abs/2601.10606)
*Peng Chen,Xiaobao Wei,Yi Yang,Naiming Yao,Hui Chen,Feng Tian*

Main category: cs.CV

TL;DR: RSATalker用3D高斯点云驱动的人脸复现，兼顾多轮对话中的真实感与社交关系建模，达成高效渲染与社会感知的SOTA。


<details>
  <summary>Details</summary>
Motivation: VR社交中多轮对话需要既真实又高效的说话人头像生成。现有3D网格方法能处理双人对话但质感差，2D大模型外观自然但算力昂贵；3DGS高效逼真但只支持单说话人且忽视社交关系。因此需要一种既利用3DGS效率/真实感，又能编码社交关系并支持多轮对话的框架。

Method: 提出RSATalker：(1) 以语音驱动网格级3D面部运动；(2) 将3D高斯绑定到网格面片以高保真渲染2D头像；(3) 设计“社会感知模块”，通过可学习query机制将血缘/非血缘、平等/不平等等关系编码为高层嵌入；(4) 三阶段训练范式；(5) 构建含语音-网格-图像三元组及社会关系标注的RSATalker数据集。

Result: 在真实感与社会感知两方面达到SOTA，支持多轮对话生成；实验广泛验证有效性。

Conclusion: RSATalker首次将3DGS用于具社会意识的多轮对话头像生成，兼顾效率与逼真度，并通过新数据集与训练策略实现SOTA表现，代码与数据将开源。

Abstract: Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.

</details>


### [62] [Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding](https://arxiv.org/abs/2601.10611)
*Christopher Clark,Jieyu Zhang,Zixian Ma,Jae Sung Park,Mohammadreza Salehi,Rohun Tripathi,Sangho Lee,Zhongzheng Ren,Chris Dongjoo Kim,Yinuo Yang,Vincent Shao,Yue Yang,Weikai Huang,Ziqi Gao,Taira Anderson,Jianrui Zhang,Jitesh Jain,George Stoica,Winson Han,Ali Farhadi,Ranjay Krishna*

Main category: cs.CV

TL;DR: Molmo2 是一套开源视频-语言模型家族，基于全新自建视频与多图数据集与高效训练配方，实现领先的点选/跟踪式视觉定位能力，并在多项基准上超越同类开源与部分闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前最强的VLM多为闭源；开源强模型常依赖闭源模型生成的合成数据或不公开训练数据/配方，限制了社区可复现与迭代。此外，许多下游任务需要像素级定位（指点与跟踪），而现有闭源/开源模型普遍欠缺这类“grounding”能力。

Method: 1) 构建7个视频数据集与2个多图数据集：含高细粒度视频字幕预训练集、自由问答微调集、复杂查询的目标跟踪集、创新的视频指点数据集，均不依赖闭源VLM；2) 训练配方：高效packing与message-tree编码；3) 模型改进：视觉token的双向注意力与新颖的token加权策略；4) 规模：最佳8B参数模型。

Result: 在短视频、计数、描述任务上同类开源可用权重与数据模型中最佳，并在长视频上具竞争力；在video grounding上显著优于开源如Qwen3-VL（视频计数准确率35.5 vs 29.6），并在部分任务上超过闭源如Gemini 3 Pro（视频指点F1：38.4 vs 20.0；视频跟踪J&F：56.2 vs 41.1）。

Conclusion: Molmo2 在无需闭源数据的前提下，通过新数据与训练策略，显著提升了视频与多图的点驱动定位与理解能力，为开源社区提供可复现的强基线，并在部分grounding任务上达到或超过闭源水平。

Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).

</details>


### [63] [CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos](https://arxiv.org/abs/2601.10632)
*Chengfeng Zhao,Jiazhi Shu,Yubo Zhao,Tianyu Huang,Jiahao Lu,Zekai Gu,Chengwei Ren,Zhiyang Dou,Qing Shuai,Yuan Liu*

Main category: cs.CV

TL;DR: 提出CoMoVi：在单一扩散反噪循环内协同生成3D人体运动与2D视频，通过共享先验与跨模态注意力实现双向增强，并提供新数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 3D人体动作生成缺少强泛化的视觉先验，2D视频生成缺少结构一致性与可物理性。两者本质耦合：3D提供结构与一致性，预训练视频模型提供强泛化与丰富外观。因此需要在生成过程中显式耦合而非后处理。

Method: 1) 设计可从预训练视频扩散模型继承先验的有效2D人体运动表示；2) 构建双分支协同视频扩散模型，同步生成动作与视频，包含互特征交互与3D-2D跨注意力；3) 在单一扩散去噪循环中联合采样；4) 构建大规模带文本与动作标注的人体视频数据集CoMoVi。

Result: 在3D人体动作生成与视频生成任务上进行了大量实验，显示所提框架在可行性、连贯性与多样性方面优于对比方法（摘要未给出具体数值）。

Conclusion: 3D动作与2D视频生成应被协同建模。CoMoVi通过双分支VDM与跨模态交互，在单一去噪循环中实现同步生成，并借助新数据集显著提升了两类任务的性能。

Abstract: In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.

</details>


### [64] [CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning](https://arxiv.org/abs/2601.10649)
*Darshan Singh,Arsha Nagrani,Kawshik Manikantan,Harman Singh,Dinesh Tewari,Tobias Weyand,Cordelia Schmid,Anelia Angelova,Shachi Dave*

Main category: cs.CV

TL;DR: 提出CURVE，一个多文化多语言视频理解评测基准，覆盖18地文化视频，提供母语编写的复杂问答与多步推理痕迹，并利用推理证据图的迭代策略定位细粒度错误。现有SOTA视频大模型在CURVE上远低于人类，主要因对文化视觉元素感知不足。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解评测偏西方语料与英语，存在文化与语言偏置，难以真实评估模型的跨文化推理能力。

Method: 构建CURVE数据集：从18个全球地区采集具有地域文化特征的视频，由母语 annotators 人工编写高质量复杂问题、答案与多步推理链；不依赖自动翻译。基于这些推理链生成证据图，并提出迭代式利用证据图来定位和分析模型推理过程中的细粒度错误。

Result: 在CURVE上评测显示，最先进的视频-LLM表现显著落后于人类基线，错误多由对文化相关视觉要素的识别与理解失败引起。

Conclusion: 跨文化多语言视频推理仍是重大挑战。CURVE为更公平、贴近真实情境的评测提供基准与分析工具，能驱动模型在文化感知与推理上的系统性改进。

Abstract: Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural

</details>


### [65] [A continental-scale dataset of ground beetles with high-resolution images and validated morphological trait measurements](https://arxiv.org/abs/2601.10687)
*S M Rayeed,Mridul Khurana,Alyson East,Isadora E. Fluck,Elizabeth G. Campolongo,Samuel Stevens,Iuliia Zarubiieva,Scott C. Lowe,Michael W. Denslow,Evan D. Donoso,Jiaman Wu,Michelle Ramirez,Benjamin Baiser,Charles V. Stewart,Paula Mabee,Tanya Berger-Wolf,Anuj Karpatne,Hilmar Lapp,Robert P. Guralnick,Graham W. Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 他们构建并公开了一个多模态数据集：对来自全美30个NEON站点的13,200余只地甲（Carabidae）进行高分辨率成像，并从图像中提取鞘翅长度与宽度，精度达亚毫米级，用于推动AI自动性状提取与生物多样性研究。


<details>
  <summary>Details</summary>
Motivation: 无脊椎动物在生态学中重要，但全球性状数据库严重偏向脊椎动物与植物；地甲是关键生物指示物，但NEON的大量标本主要以实体存在，难以开展广域、可重复的计算分析。

Method: 对实体标本进行高分辨率数字成像；基于图像的性状测量流程提取鞘翅长度和宽度；用人工量测进行验证，评估数字性状提取的精度。

Result: 获得覆盖美国本土与夏威夷30站点、超过13,200个体的图像与配套性状数据；数字测量与人工测量高度一致，性状提取达到亚毫米精度。

Conclusion: 该数据集弥补无脊椎性状数据不足，支持AI驱动的自动物种鉴定与性状研究，促进大尺度生物多样性监测与保护应用。

Abstract: Despite the ecological significance of invertebrates, global trait databases remain heavily biased toward vertebrates and plants, limiting comprehensive ecological analyses of high-diversity groups like ground beetles. Ground beetles (Coleoptera: Carabidae) serve as critical bioindicators of ecosystem health, providing valuable insights into biodiversity shifts driven by environmental changes. While the National Ecological Observatory Network (NEON) maintains an extensive collection of carabid specimens from across the United States, these primarily exist as physical collections, restricting widespread research access and large-scale analysis. To address these gaps, we present a multimodal dataset digitizing over 13,200 NEON carabids from 30 sites spanning the continental US and Hawaii through high-resolution imaging, enabling broader access and computational analysis. The dataset includes digitally measured elytra length and width of each specimen, establishing a foundation for automated trait extraction using AI. Validated against manual measurements, our digital trait extraction achieves sub-millimeter precision, ensuring reliability for ecological and computational studies. By addressing invertebrate under-representation in trait databases, this work supports AI-driven tools for automated species identification and trait-based research, fostering advancements in biodiversity monitoring and conservation.

</details>


### [66] [See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection](https://arxiv.org/abs/2601.10707)
*Amir Mallak,Erfan Aasi,Shiva Sreeram,Tsun-Hsuan Wang,Daniela Rus,Alaa Maalouf*

Main category: cs.CV

TL;DR: 论文提出随机补丁选择（SPS）以缓解基础模型补丁特征的冗余与相关性，通过随机掩蔽部分补丁描述符、保留空间布局，促使策略对补丁存活不变，从而提升端到端自动驾驶在OOD上的鲁棒性与效率；在闭环仿真中平均提升6.2%，最高20.4%，并且推理2.4倍更快，还可零调参迁移到真实车辆。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶近来用基础模型（如BLIP2）的补丁对齐特征，但注意力导致单个补丁隐含全局上下文，特征高度冗余与跨补丁强相关，训练策略易过拟合虚假相关，OOD泛化受损。

Method: 提出SPS：对每帧随机掩蔽一定比例的补丁描述符，不将其输入策略模型，同时保留剩余补丁的原始空间布局；由此让策略在不同随机但语义完整的视图上学习，迫使决策对具体存活token不变。并以PCA和跨补丁相似度量化冗余（90%方差被17/64主成分解释）。进行掩蔽率与特征重组的消融实验。

Result: 在各种OOD场景下，SPS优于SOTA：闭环仿真平均+6.2%，最高+20.4%，推理速度提升2.4倍；9个系统的训练/评估中有8个超过先前SOTA；同一策略无调参可迁移至真实车辆。

Conclusion: 减少冗余补丁输入并施加随机视图一致性可提升端到端自动驾驶策略的鲁棒性、泛化与效率；SPS简单通用、在OOD显著收益，并具备从仿真到真实的零调参可迁移性。

Abstract: Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.

</details>


### [67] [From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion](https://arxiv.org/abs/2601.10710)
*Cheng Chen,Yuyu Guo,Pengpeng Zeng,Jingkuan Song,Peng Di,Hang Yu,Lianli Gao*

Main category: cs.CV

TL;DR: 提出一种名为CLI的轻量框架，通过跨层注入打破VLM中视觉到语言的单一瓶颈，实现LLM对全层级视觉特征的按需访问，从而显著提升多模态理解与推理。


<details>
  <summary>Details</summary>
Motivation: 现有VLM通常只在视觉编码器的最终输出与LLM输入之间建立单一、静态、非对称连接，导致局部细节与全局语义难以被同时对齐与利用，限制了推理能力，需要一种能让LLM动态利用多层视觉表示的机制。

Method: 提出Cross-Layer Injection (CLI)，包含两部分：1) Adaptive Multi-Projection (AMP) 将来自视觉编码器不同层的特征进行对齐与投影；2) Adaptive Gating Fusion (AGF) 根据LLM解码的上下文动态选择并注入最相关的视觉信息，形成多对多的跨层桥接。该方法参数高效且可插拔，并集成到LLaVA-OneVision与LLaVA-1.5中进行验证。

Result: 在18个多样化基准上取得显著性能提升，显示CLI在多模态理解和推理方面优于仅依赖单层特征的基线。

Conclusion: CLI作为可扩展范式，为LLM提供对完整视觉层级的按需访问能力，缓解视觉瓶颈并提升多模态对齐与推理效果。

Abstract: Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.

</details>


### [68] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: Alterbute是一种基于扩散模型的图像对象内在属性编辑方法，在保持对象身份与场景上下文的同时，灵活修改颜色、材质、纹理与形状。通过宽松训练目标与视觉命名实体（VNE）监督，实现可扩展、身份保真的编辑，并在相关任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖无监督先验，往往破坏对象身份一致性；要么使用过于严格的监督，限制对颜色、材质、形状等内在属性的有意义变化。因此需要一种既能保持身份与场景，又能大幅调整内在属性的编辑方法。

Method: 1) 训练阶段采用“放宽”的目标：模型接收身份参考图、目标内在属性的文本描述、以及定义外在上下文（背景图与对象掩码）的输入，允许同时改变内外在属性以学习对比信号；2) 推理阶段通过复用原始背景与掩码，约束外在改变，使模型只改变内在属性；3) 引入视觉命名实体（VNE）作为细粒度身份标签（如“Porsche 911 Carrera”），利用视觉-语言模型从大规模公开数据集中自动抽取VNE与内在属性描述，实现可扩展、身份保真的监督。

Result: Alterbute在身份保真的对象内在属性编辑任务上优于现有方法，能稳定地修改颜色、纹理、材质与形状，同时保留对象的感知身份和场景一致性。

Conclusion: 通过放宽训练目标与VNE驱动的可扩展监督，Alterbute在保证身份与上下文一致的前提下，实现高质量、可控的对象内在属性编辑，并在基准上取得领先表现。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


### [69] [WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments](https://arxiv.org/abs/2601.10716)
*Xuweiyi Chen,Wentao Zhou,Zezhou Cheng*

Main category: cs.CV

TL;DR: WildRayZer是一种自监督动态场景新视角合成框架：用静态渲染器解释刚体背景、用残差发现瞬态（运动）区域，蒸馏运动估计并在训练中屏蔽瞬态以专注跨视角背景补全；并发布两套动态数据集；单次前向推理在去除瞬态与整体画质上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 静态NVS依赖多视图一致性，遇到动态内容会出现重影、虚假几何、位姿不稳。现实视频普遍存在相机和物体同时运动，缺少可规模化的数据与方法来在动态环境中稳健进行NVS。

Method: 提出分析-合成测试流程：1）仅相机位姿的静态渲染器拟合刚体结构；2）用渲染残差定位瞬态区域，生成伪运动掩码；3）以此蒸馏训练运动估计器；4）在编码与损失中用掩码进行token屏蔽与梯度门控，使监督聚焦于跨视角的背景完成；5）构建D-RE10K与D-RE10K-iPhone数据集以进行大规模训练与评测。

Result: 在D-RE10K及iPhone基准上，WildRayZer以单次前向推理在瞬态区域去除与整帧NVS质量上，均稳定超过优化式与前馈式基线；提升消除鬼影、几何错觉与位姿不稳问题。

Conclusion: 通过将动态因素视为静态渲染残差并以掩码与梯度门控隔离其影响，WildRayZer实现对动态场景的鲁棒NVS；所构建数据集支撑了可规模化训练与评测，显示该思路在真实世界动态视频中有效。

Abstract: We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.

</details>
