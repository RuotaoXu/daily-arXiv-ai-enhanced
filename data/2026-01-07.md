<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data](https://arxiv.org/abs/2601.02392)
*Mo Chen*

Main category: cs.CV

TL;DR: 提出Dense-MAE：用自监督遮挡重建预训练Dense-Unet，以在CTA中更好去除冠脉钙化引起的膨胀伪影；在少样本下显著提升补全与狭窄评估精度。


<details>
  <summary>Details</summary>
Motivation: CTA中冠脉钙化导致膨胀伪影，影响管腔狭窄诊断。现有基于DCNN的去伪影/补全方法（如Dense-Unet）需大量标注数据，医学场景标注稀缺，需无标注的预训练策略以学习血管拓扑高层特征并提升小样本表现。

Method: 受3D点云MAE启发，提出Dense-MAE：对体数据的血管腔体进行随机3D patch遮挡，训练Dense-Unet重建缺失几何；通过自监督重建任务学习高层语义/拓扑表征。随后将该预训练权重用于钙化去除（inpainting）下游网络初始化。

Result: 在临床CTA数据上，使用MAE预训练的权重初始化钙化去除网络，相比从零训练，在补全准确度和狭窄度估计上都有提升，尤其在小样本（few-shot）设置下提升显著。

Conclusion: 自监督的3D遮挡-重建预训练能让Dense-Unet学到动脉拓扑的鲁棒表示，缓解标注稀缺问题，从而在CTA钙化去除与狭窄评估任务中取得更优性能，特别适合少样本场景。

Abstract: Coronary calcification creates blooming artifacts in Computed Tomography Angiography (CTA), severely hampering the diagnosis of lumen stenosis. While Deep Convolutional Neural Networks (DCNNs) like Dense-Unet have shown promise in removing these artifacts via inpainting, they often require large labeled datasets which are scarce in the medical domain. Inspired by recent advancements in Masked Autoencoders (MAE) for 3D point clouds, we propose \textbf{Dense-MAE}, a novel self-supervised learning framework for volumetric medical data. We introduce a pre-training strategy that randomly masks 3D patches of the vessel lumen and trains the Dense-Unet to reconstruct the missing geometry. This forces the encoder to learn high-level latent features of arterial topology without human annotation. Experimental results on clinical CTA datasets demonstrate that initializing the Calcium Removal network with our MAE-based weights significantly improves inpainting accuracy and stenosis estimation compared to training from scratch, specifically in few-shot scenarios.

</details>


### [2] [MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion](https://arxiv.org/abs/2601.02414)
*Jichao Zhu,Jun Yu*

Main category: cs.CV

TL;DR: 提出MIAR框架，通过跨模态特征交互生成全局表示token，并结合对比学习与归一化进行对齐，在CMU-MOSI/MOSEI上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往MER侧重于简单模态融合，忽视不同模态分布差异与贡献不均，且对文本特征泛化弱，导致在多模态场景下性能受限。

Method: 提出Modality Interaction and Alignment Representation (MIAR)。核心：1) 跨模态特征交互模块，将各模态上下文特征交互后，为每个模态生成若干“全局表示token”，这些token编码“该模态从其他模态中提取的信息”；文中提到共四个token用于表征不同模态间的信息抽取关系。2) 对齐策略：采用对比学习与归一化，使不同模态的全局表示在表示空间中对齐，缓解分布差异并考虑模态贡献差异。3) 在CMU-MOSI与CMU-MOSEI上进行训练与评估。

Result: 在CMU-MOSI与CMU-MOSEI数据集上，MIAR在关键指标上超过当前多模态情感识别的SOTA方法。

Conclusion: 通过显式的模态交互与对齐，MIAR有效缓解模态分布差异与贡献不均问题，提升泛化与性能，适用于多数据集的多模态情感识别。

Abstract: Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.

</details>


### [3] [Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion](https://arxiv.org/abs/2601.02415)
*Wangyuan Zhu,Jun Yu*

Main category: cs.CV

TL;DR: 提出一种用于多模态情感分析的框架：多通道单模态特征提取 + 对称互促（SMP）跨模态融合（交叉注意力+自注意力）+ 单/跨模态特征整合；在两套基准数据上验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析存在两大痛点：1) 单模态特征提取不足，表达能力不够；2) 融合时多关注“共性/一致性”，忽视模态间“差异性”，导致融合不充分。需要既增强单模态表征又更有效地交互与融合不同模态信息。

Method: (1) 单模态阶段：视觉与语音各自采用“双通道/多通道”特征提取，以获得更全面、互补的单模态表示。 (2) 跨模态阶段：提出对称互促（SMP）融合——由对称的跨模态注意力（相互汲取有用信息）和自注意力（建模上下文与内部结构）组成，促进双向信息交互与增强。 (3) 融合阶段：将强化后的单模态特征与SMP输出的跨模态特征进行整合，显式利用互补性并兼顾模态差异。

Result: 在两个公开基准数据集上进行实验，提出的方法在效果上优于对比方法，验证了有效性与优越性（具体指标与提升幅度未在摘要中给出）。

Conclusion: 通过多通道单模态表征与SMP对称互促融合机制，能够更充分地交换与整合跨模态信息，同时保留模态差异性，从而提升多模态情感识别性能；实验支撑其有效性与先进性。

Abstract: Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.

</details>


### [4] [Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning](https://arxiv.org/abs/2601.02422)
*Wenting Lu,Didi Zhu,Tao Shen,Donglin Zhu,Ayong Ye,Chao Wu*

Main category: cs.CV

TL;DR: 提出CoCoT框架，通过动态多区域定位与关系感知推理，缓解跨模态CoT对单一区域依赖与语义碎片化问题，构建CoCoT-70K数据集，并在多基准上显著提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态Chain-of-Thought推理常只关注粗粒度单一区域，忽略图像中相互关联的关键信息，且步进式推理间缺乏语义连贯，导致复杂视觉推理表现不佳。

Method: 1) 动态多区域定位：依据问题自适应发现最相关的多个图像区域；2) 关系感知推理：在多区域间迭代对齐视觉线索，建构连贯、可解释的跨模态思维链；并据此标注与生成包含多区域标注和结构化推理链的CoCoT-70K数据集。

Result: 在六个具有挑战的基准上，相比LLaVA-1.5平均提升15.4%准确率、相比Qwen2-VL提升4.0%；显示对复杂视觉推理的显著增益。

Conclusion: 协同跨模态思维结合动态多区域与关系建模可有效缓解单区域偏置与语义碎片化，显著提升多模态推理效果；相关数据与代码已开源。

Abstract: Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.

</details>


### [5] [NitroGen: An Open Foundation Model for Generalist Gaming Agents](https://arxiv.org/abs/2601.02427)
*Loïc Magne,Anas Awadalla,Guanzhi Wang,Yinzhen Xu,Joshua Belofsky,Fengyuan Hu,Joohwan Kim,Ludwig Schmidt,Georgia Gkioxari,Jan Kautz,Yisong Yue,Yejin Choi,Yuke Zhu,Linxi "Jim" Fan*

Main category: cs.CV

TL;DR: NitroGen 是一个视觉-动作通用基础模型，基于超4万小时、覆盖千余款游戏的视频-动作数据，使用大规模行为克隆训练，并在跨游戏基准上展示强泛化，迁移到未见游戏时显著提升任务成功率（相对提升最高达52%）。


<details>
  <summary>Details</summary>
Motivation: 现有游戏智能体多依赖单游戏或小规模数据，泛化与迁移差；缺乏统一的大规模视频-动作数据、跨游戏评测基准与可复用的通用模型，限制了通用具身智能体的发展。

Method: 提出三要素：1）自动从公开游戏视频中提取玩家动作，构建互联网规模的视频-动作数据集；2）构建可衡量跨游戏泛化的多游戏基准环境；3）采用统一的视觉-动作架构，以大规模行为克隆进行训练。

Result: 模型在多种类型游戏（3D动作战斗、2D平台高精度操作、程序化世界探索）上表现强劲；在未见游戏上的迁移有效，相比从零训练的模型，任务成功率相对提升最高达52%。同时开源数据集、评测套件与权重。

Conclusion: 互联网规模的视频-动作数据与统一行为克隆训练可支撑跨游戏泛化的通用视觉-动作智能体；NitroGen 为通用具身智能研究提供了实证与资源基础。

Abstract: We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.

</details>


### [6] [TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers](https://arxiv.org/abs/2601.02437)
*Zhibo Wang,Zuoyuan Zhang,Xiaoyi Pang,Qile Zhang,Xuanyi Hao,Shuguo Zhuo,Peng Sun*

Main category: cs.CV

TL;DR: 提出TAP-ViTs：在不接触设备本地原始数据的前提下，为异构设备生成任务自适应剪枝的ViT模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ViT算力与内存开销大，阻碍在移动/边缘设备部署。现有剪枝要么产出单一模型，忽略设备异构与任务差异；要么需用设备本地数据微调，受资源与隐私限制不可行。需要一种既隐私友好又能针对设备任务与预算定制的剪枝框架。

Method: 1) 隐私友好的任务表征：每台设备用轻量GMM拟合其私有数据分布，只上传GMM参数；云端据此从公共数据中选择与分布一致的样本，构建每设备的代理metric dataset。2) 剪枝策略：提出“双粒度”重要性评估——组合神经元重要性与自适应层重要性联合度量，在给定计算预算下对ViT进行细粒度、任务感知的结构化剪枝，生成设备特定的剪枝模型。

Result: 在多种ViT骨干与数据集上，TAP-ViTs在相近压缩率下稳定优于SOTA剪枝方法，体现更好的精度-压缩权衡与跨设备适配能力。

Conclusion: 通过GMM参数化的隐私保留任务表征与双粒度重要性评估，TAP-ViTs实现了无需访问本地原始数据的设备定制化ViT剪枝，适合隐私约束与资源受限的移动计算场景。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a wide range of vision tasks, yet their substantial computational and memory demands hinder efficient deployment on resource-constrained mobile and edge devices. Pruning has emerged as a promising direction for reducing ViT complexity. However, existing approaches either (i) produce a single pruned model shared across all devices, ignoring device heterogeneity, or (ii) rely on fine-tuning with device-local data, which is often infeasible due to limited on-device resources and strict privacy constraints. As a result, current methods fall short of enabling task-customized ViT pruning in privacy-preserving mobile computing settings. This paper introduces TAP-ViTs, a novel task-adaptive pruning framework that generates device-specific pruned ViT models without requiring access to any raw local data. Specifically, to infer device-level task characteristics under privacy constraints, we propose a Gaussian Mixture Model (GMM)-based metric dataset construction mechanism. Each device fits a lightweight GMM to approximate its private data distribution and uploads only the GMM parameters. Using these parameters, the cloud selects distribution-consistent samples from public data to construct a task-representative metric dataset for each device. Based on this proxy dataset, we further develop a dual-granularity importance evaluation-based pruning strategy that jointly measures composite neuron importance and adaptive layer importance, enabling fine-grained, task-aware pruning tailored to each device's computational budget. Extensive experiments across multiple ViT backbones and datasets demonstrate that TAP-ViTs consistently outperforms state-of-the-art pruning methods under comparable compression ratios.

</details>


### [7] [Understanding Pure Textual Reasoning for Blind Image Quality Assessment](https://arxiv.org/abs/2601.02441)
*Yuan Li,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文从信息流角度检验文本推理在无参考图像质量评价（BIQA）中的作用，通过对比现有模型与三种范式（CoT、自一致、自动编码器）学习图像-文本-分数关系，发现仅用文本预测显著退化；CoT几乎无增益；自一致显著缩小图像与文本条件预测差距（PLCC/SRCC差仅0.02/0.03）；自动编码器效果一般但指明优化方向。


<details>
  <summary>Details</summary>
Motivation: 尽管文本推理在BIQA中流行，但尚不清楚文本信息如何、以及在多大程度上表征与质量分数相关的图像内容。为量化文本在质量预测中的贡献并揭示改进路径，作者从信息流的角度系统评估文本在图像-文本-分数关系中的作用。

Method: 设计并对比三类学习范式以刻画图像-文本-分数关系：1) Chain-of-Thought（逐步推理）；2) Self-Consistency（多次采样、投票/聚合的自一致推理）；3) 类自动编码器（以文本为中间表征进行重构/回归）。将这些范式与现有BIQA模型进行对照实验，分别测试仅文本、图像+文本条件下的预测，并以PLCC/SRCC衡量差距。

Result: 仅以文本为条件进行质量分数预测时，现有模型性能显著下降；CoT对整体BIQA指标提升有限；Self-Consistency显著缩小图像与文本条件预测的性能差距，使PLCC/SRCC差分别缩小至约0.02/0.03；类自动编码器在缩差方面不如自一致，但显示出潜在的优化空间。

Conclusion: 文本单独难以充分承载与质量分数相关的信息；简单引入CoT不足以改进BIQA；通过自一致的多样化采样与聚合可更好地对齐文本与图像条件下的预测；自动编码器式中间表征值得进一步探索。研究为改进BIQA中文本推理及更广泛高层视觉任务提供了可操作的思路。

Abstract: Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.

</details>


### [8] [Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative](https://arxiv.org/abs/2601.02443)
*Li Wang,Xi Chen,XiangWen Deng,HuaHui Yi,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 论文评估多模态大模型在膝骨关节炎X光分类中的有效性，发现单独的视觉编码器在诊断分类上优于完整MLLM；LLM微调无显著增益；小而平衡的数据经LoRA微调胜过大而不平衡数据。结论：MLLM更适合解释与生成报告，分类应优先优化视觉编码器与数据质量/平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在医学VQA与报告生成上表现出色，但这种生成与解释能力并未可靠迁移到疾病特异的分类任务。膝骨关节炎影响数亿人，然而在医学MLLM基准中仍被低估，需要系统评估各模块与训练策略对临床分类准确性的影响。

Method: 围绕膝OA放射影像分类，进行系统性消融：分别替换/比较视觉编码器、模态连接器与LLM，并在不同训练策略下评估（提示式 vs. 微调、LLM微调、LoRA微调）。比较单独视觉编码器分类器与完整MLLM流水线的诊断准确率；分析小而类别平衡（500张）与大而类别不平衡（5778张）数据的训练效果。

Result: (1) 仅用训练好的视觉编码器即可超过完整MLLM在分类准确率上的表现；(2) 对LLM进行微调相较提示工程并无显著提升；(3) 在小规模且类别平衡的数据上进行LoRA微调优于在更大但类别不平衡的数据上训练。

Conclusion: 在医疗图像的特定疾病分类中，LLM更适合作为解释与报告生成器，而非主要分类器；对需要高确定性的诊断任务，MLLM架构并不理想。应优先优化视觉编码器并重视数据集的平衡与质量，以提升临床可用性。

Abstract: Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.

</details>


### [9] [A Spatio-Temporal Deep Learning Approach For High-Resolution Gridded Monsoon Prediction](https://arxiv.org/abs/2601.02445)
*Parashjyoti Borah,Sanghamitra Sarkar,Ranjan Phukan*

Main category: cs.CV

TL;DR: 提出用深度学习把印度夏季风降水的季节/逐月格点预报建模为“视频到图像”问题：用1–5月多变量再分析场序列（ERA5）作为多通道时空输入，经CNN学习到6–9月逐月与整季格点降水（IMD）的映射，85年数据训练，能给出高分辨率、月度与季节尺度的空间化预报。


<details>
  <summary>Details</summary>
Motivation: 传统长时段季风预测多给单一区域平均量，缺少空间细节，难以支撑地区层面的水资源与农业决策；需要能在季节提前量上提供格点级、逐月的预报方法。

Method: 将1–5月的大气与海洋多变量场视作多通道时序“图像序列”，构造成视频式张量输入；以CNN为核心的时空架构学习从前汛期场到目标季（6–9月）高分辨率降水格点场的映射；预测目标包括四个单月和整季平均；使用ERA5作预测因子、IMD降水作标签，样本期约85年。

Result: 模型能生成面向每个汛期月份以及整季的空间化降水预报，相较传统仅有单一区域平均的输出，实现了具有区分度的格点级预测能力。

Conclusion: 将季风预测表述为计算机视觉的时空学习任务可行且有效，能为印度季风提供逐月与季节的高分辨率空间预报，提升区域资源管理与决策的实用性。

Abstract: The Indian Summer Monsoon (ISM) is a critical climate phenomenon, fundamentally impacting the agriculture, economy, and water security of over a billion people. Traditional long-range forecasting, whether statistical or dynamical, has predominantly focused on predicting a single, spatially-averaged seasonal value, lacking the spatial detail essential for regional-level resource management. To address this gap, we introduce a novel deep learning framework that reframes gridded monsoon prediction as a spatio-temporal computer vision task. We treat multi-variable, pre-monsoon atmospheric and oceanic fields as a sequence of multi-channel images, effectively creating a video-like input tensor. Using 85 years of ERA5 reanalysis data for predictors and IMD rainfall data for targets, we employ a Convolutional Neural Network (CNN)-based architecture to learn the complex mapping from the five-month pre-monsoon period (January-May) to a high-resolution gridded rainfall pattern for the subsequent monsoon season. Our framework successfully produces distinct forecasts for each of the four monsoon months (June-September) as well as the total seasonal average, demonstrating its utility for both intra-seasonal and seasonal outlooks.

</details>


### [10] [Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis](https://arxiv.org/abs/2601.02447)
*Bennet Kahrs,Julia Andresen,Fenja Falta,Monty Santarossa,Heinz Handels,Timo Kepp*

Main category: cs.CV

TL;DR: 提出基于隐式神经表示（INR）的分辨率无关框架，解决OCT体数据各向异性与2D方法不一致的问题，实现跨B-scan插值和通用视网膜图谱构建，从而实现致密3D分析。


<details>
  <summary>Details</summary>
Motivation: 临床OCT体积常因B-scan间距大而各向异性严重，导致2D方法在相邻切片间不一致、3D表面不规则；传统CNN受训练分辨率限制，难以泛化到不同采集协议。需一种能跨分辨率、处理稀疏采样并保持3D一致性的表示与分析方法。

Method: 利用隐式神经表示以坐标为输入、输出连续场：1) 结合en-face模态信息进行B-scan间插值，补全稀疏切片间结构；2) 构建分辨率无关的视网膜INR图谱，通过群体级训练提升可泛化性，并可对未见个体进行预测。两套框架均采用可泛化INR，实现跨个体、跨协议的连续3D表示与推断。

Result: 在稀疏采样OCT体数据上，INR框架提升了视网膜形状与层结构的3D一致性与连续性，能在大B-scan间距下进行致密体积分析，并对未见数据维持稳健预测。

Conclusion: 基于INR的分辨率无关框架有效克服OCT各向异性与2D方法不一致性，支持跨协议、跨分辨率的致密3D评估，为视网膜结构与病理的体积化分析提供新途径。

Abstract: Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.

</details>


### [11] [PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding](https://arxiv.org/abs/2601.02457)
*Souhail Hadgi,Bingchen Gong,Ramana Sundararaman,Emery Pierson,Lei Li,Peter Wonka,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 提出一个仅编码器的3D点云模型，直接学习与语言对齐的patch级特征，通过两阶段预训练（2D视觉特征蒸馏+与部件级文本对齐）实现零样本快速部件分割，显著优于基于渲染和前馈的方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D基础模型在检索/分类等全局任务上强，但在局部部件级推理上迁移差。多视角渲染+文本查询的方法虽有效，但推理成本高、依赖LLM提示工程且未利用固有3D几何。需要一种直接从点云获得语言对齐的局部特征、同时高效推理的方案。

Method: 构建一个点云Transformer编码器并进行两阶段预训练：1) 将DINOv2等2D视觉编码器的致密特征蒸馏到3D patch上；2) 通过多正样本对比学习将这些patch嵌入与部件级文本嵌入对齐。训练数据来自“多视角SAM区域+VLM字幕”生成的部件标注3D形状数据引擎。推理为单次前向、无需多视角渲染。

Result: 模型实现零样本3D部件分割，单次推理、速度快；在多个3D部件分割基准上显著优于以渲染为中心的管线和其他前馈方法。

Conclusion: 通过2D到3D特征蒸馏与部件文本对齐，可在不依赖多视角渲染和繁琐提示工程的情况下获得语言对齐的3D局部表示，实现高效且强大的零样本3D部件分割。

Abstract: Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/

</details>


### [12] [CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking](https://arxiv.org/abs/2601.02521)
*Amirreza Parvahan,Mohammad Hoseyni,Javad Khoramdel,Amirhossein Nikoofard*

Main category: cs.CV

TL;DR: 将3D CT体数据视作“视频”，用轻量级2D检测器逐切片检测，并用跨切片追踪与一致性过滤来近似3D语境，从而在边缘设备上实现更高精度、低开销的ICH检测。


<details>
  <summary>Details</summary>
Motivation: 3D CNN对内存与算力要求高，不利于边缘设备与实时救治场景（如移动卒中单元、远程门诊）。需要兼顾3D上下文信息与2D模型的效率，提升切片级检测的稳定性与临床可用性。

Method: 1) 将体CT序列重构为沿z轴的“视频流”。2) 基于YOLO Nano（在v8/v10/v11/v12中以mAP@50择优）进行切片级检测。3) 用ByteTrack在z轴上进行目标关联，施加解剖连续性约束。4) 为缓解视频跟踪起始阶段不稳定，引入混合推理策略与时空一致性滤波，抑制瞬态噪声并区分真实病灶。

Result: 在独立测试集上，相比纯2D基线，Precision由0.703提升至0.779，且保持高敏感性；以较小计算代价近似获得3D上下文效果。

Conclusion: 视频化视角使2D检测器具备近似3D的时空一致性与稳健性，在资源受限环境下实现实时、可扩展的ICH优先分诊与检测。

Abstract: Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.

</details>


### [13] [MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark](https://arxiv.org/abs/2601.02536)
*Shaden Shaar,Bradon Thymes,Sirawut Chaixanien,Claire Cardie,Bharath Hariharan*

Main category: cs.CV

TL;DR: 提出MovieRecapsQA：一个基于YouTube电影复盘视频的开放式多模态VideoQA基准，含约8.2K开放问答及可核验的文本“事实”，用于细粒度评测MLLM在跨视觉与对白的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA基准难以覆盖电影等真实世界视频中的复杂多模态推理，且多为非开放式，因为自由回答的评测困难；缺少可显式核验答案的上下文支撑。

Method: 利用电影复盘视频的两种同步模态（视觉复盘视频与文本复盘摘要），结合电影字幕，自动生成约8.2K开放式问答，并随每题提供必要的文本“事实”以进行无参考评测；同时按视频粒度（复盘片段、电影片段）与问题类别（按模态与类型）组织数据。用该基准评测7个前沿多模态大模型。

Result: 实验显示：1) 纯视觉问题最难；2) 只要文本可用，模型倾向依赖文本；3) 从视频中抽取事实仍困难；4) 在依赖视频的问题上，闭源与开源模型表现相近。

Conclusion: MovieRecapsQA为首个提供显式文本上下文并支持开放式回答的多模态VideoQA基准，可用于更细粒度地分析与评估MLLM的跨模态理解与事实提取能力，揭示当前模型在视频依赖推理上的瓶颈。

Abstract: Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.

</details>


### [14] [Shallow- and Deep-fake Image Manipulation Localization Using Vision Mamba and Guided Graph Neural Network](https://arxiv.org/abs/2601.02566)
*Junbin Zhang,Hamid Reza Tohidypour,Yixiao Wang,Panos Nasiopoulos*

Main category: cs.CV

TL;DR: 提出一种面向浅伪与深伪图像的统一篡改定位方法：以Vision Mamba提取边界敏感特征，并引入引导式图神经网络(G-GNN)强化真伪像素分离，较SOTA精度更高。


<details>
  <summary>Details</summary>
Motivation: 现有研究多各自针对浅伪图像或深伪视频，缺少同时适用于两类伪造的统一定位方案；且需要更精细地区分真实与被篡改像素边界以降低误检与漏检。

Method: 使用Vision Mamba网络获取能清晰刻画篡改与未篡改区域边界的特征图；在此基础上设计Guided Graph Neural Network（G-GNN）模块，以引导信息在图结构中传播，放大真伪像素间的差异，实现更精准的像素级分割。

Result: 在评测中，相较于其他当前最先进方法，本方法在定位准确率上取得更高的推理表现（具体指标未在摘要给出）。

Conclusion: 深度网络结合Vision Mamba与G-GNN可在浅伪与深伪图像上实现统一且更精确的篡改定位，验证了该思路的可行性与有效性。

Abstract: Image manipulation localization is a critical research task, given that forged images may have a significant societal impact of various aspects. Such image manipulations can be produced using traditional image editing tools (known as "shallowfakes") or advanced artificial intelligence techniques ("deepfakes"). While numerous studies have focused on image manipulation localization on either shallowfake images or deepfake videos, few approaches address both cases. In this paper, we explore the feasibility of using a deep learning network to localize manipulations in both shallow- and deep-fake images, and proposed a solution for such purpose. To precisely differentiate between authentic and manipulated pixels, we leverage the Vision Mamba network to extract feature maps that clearly describe the boundaries between tampered and untouched regions. To further enhance this separation, we propose a novel Guided Graph Neural Network (G-GNN) module that amplifies the distinction between manipulated and authentic pixels. Our evaluation results show that our proposed method achieved higher inference accuracy compared to other state-of-the-art methods.

</details>


### [15] [DreamLoop: Controllable Cinemagraph Generation from a Single Photograph](https://arxiv.org/abs/2601.02646)
*Aniruddha Mahapatra,Long Mai,Cusuh Ham,Feng Liu*

Main category: cs.CV

TL;DR: DreamLoop 是一个从单张照片生成可控循环动图（cinemagraph）的新框架，利用对通用视频扩散模型的“时间桥接”和“运动条件”训练，无需专门的cinemagraph数据即可实现无缝循环、静态背景与用户可控运动路径，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像动画方法只支持简单、低频运动且局限于水、烟等纹理重复的狭窄领域；通用视频扩散模型又未针对无缝循环与可控运动的cinemagraph约束，且缺乏专门数据。因此需要一种既能遵循cinemagraph特性、又能在通用场景中实现可控复杂运动的方案。

Method: 以通用视频扩散模型为基础，引入两项训练目标：1）时间桥接（temporal bridging），使模型学会在给定起止帧条件下生成中间帧并实现首尾一致；2）运动条件（motion conditioning），通过静态轨迹维持背景静止，并接受用户指定的目标对象运动路径与时序，从而实现可控动画。在推理时，将输入图像同时作为首帧与尾帧条件以强制无缝循环，并用静态轨迹锁定静态区，按用户路径生成运动。

Result: 在不使用专门的cinemagraph训练数据的情况下，能在通用场景中生成高质量、复杂且与用户意图一致的循环动图，支持灵活直观的控制，并在主观与客观评估上优于现有方法。

Conclusion: DreamLoop首次实现针对通用场景的可控cinemagraph生成：通过时间桥接与运动条件化适配视频扩散模型，实现无缝循环、静态背景与用户可控运动路径，效果与可控性均领先于既有方法。

Abstract: Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.

</details>


### [16] [GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images](https://arxiv.org/abs/2601.02709)
*Shuman He,Xiehua Li,Xioaju Yang,Yang Xiong,Keqin Li*

Main category: cs.CV

TL;DR: 提出基于去除绿色通道并重建的检测范式（GRRE），通过比较重建误差有效区分真实与生成图像，并在多种与未见模型上表现出强泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测器在遇到新型/未见生成模型时准确率显著下降；需要一种对跨模型、后处理和扰动更稳健的通用取证方法。

Method: 观察到从真实图像中移除绿色（G）通道并进行重建，其重建误差分布与AI生成图像显著不同；据此提出G-channel Removed Reconstruction Error（GRRE）：对图像去除G通道→进行重建（具体重建器未在摘要中细化）→计算重建误差→用该误差作为判别依据进行检测与分类。

Result: 在多种生成模型（含未见模型）上稳定获得高检测准确率；相较现有方法，对多种扰动与后处理保持更强鲁棒性，并展现更优的跨模型泛化能力。

Conclusion: 基于通道移除的重建误差可作为强有力的取证信号；GRRE为区分真实与生成图像提供了简单有效且具泛化与鲁棒性的检测方案。

Abstract: The rapid progress of generative models, particularly diffusion models and GANs, has greatly increased the difficulty of distinguishing synthetic images from real ones. Although numerous detection methods have been proposed, their accuracy often degrades when applied to images generated by novel or unseen generative models, highlighting the challenge of achieving strong generalization. To address this challenge, we introduce a novel detection paradigm based on channel removal reconstruction. Specifically, we observe that when the green (G) channel is removed from real images and reconstructed, the resulting reconstruction errors differ significantly from those of AI-generated images. Building upon this insight, we propose G-channel Removed Reconstruction Error (GRRE), a simple yet effective method that exploits this discrepancy for robust AI-generated image detection. Extensive experiments demonstrate that GRRE consistently achieves high detection accuracy across multiple generative models, including those unseen during training. Compared with existing approaches, GRRE not only maintains strong robustness against various perturbations and post-processing operations but also exhibits superior cross-model generalization. These results highlight the potential of channel-removal-based reconstruction as a powerful forensic tool for safeguarding image authenticity in the era of generative AI.

</details>


### [17] [CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos](https://arxiv.org/abs/2601.02716)
*Taeyeon Kim,Youngju Na,Jumin Lee,Minhyuk Sung,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: CAMO提出一种无需类别模板与显式3D监督、从单目2D视频直接将动作迁移到任意3D网格的通用方法，通过形态参数化的可动3D高斯点与稠密语义对应联合优化形状与姿态，缓解形状-姿态歧义，在多类别与随拍视频中实现更准确、高效且连贯的运动迁移。


<details>
  <summary>Details</summary>
Motivation: 现有2D到3D运动迁移依赖类别特定的参数化模板或需要3D监督，且在形状多样、姿态模糊的情况下表现不稳健。需要一种不依赖模板、可泛化到多类别、并能缓解形状-姿态歧义的方案。

Method: 提出CAMO：以形态（morphology）参数化的关节化3D Gaussian Splatting表示目标对象，并结合稠密语义对应，将形状与姿态在同一优化框架中联合适配，从单目2D视频驱动目标网格的运动，无需预定义模板或显式3D监督。

Result: 在多类别与日常视频场景中，相比现有方法获得更高的运动精度、更好的效率与视觉连贯性，定性与定量结果均显示明显优势。

Conclusion: CAMO能以类无关的方式可靠地把单目2D视频中的动作迁移到不同形状的3D资产上，通过联合优化的3D高斯关节模型与语义对应有效消除形状-姿态歧义，推动了跨类别运动迁移的实用性与性能上限。

Abstract: Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.

</details>


### [18] [Robust Mesh Saliency GT Acquisition in VR via View Cone Sampling and Geometric Smoothing](https://arxiv.org/abs/2601.02721)
*Guoquan Zheng,Jie Hao,Huiyu Duan,Yongming Han,Liang Yuan,Dong Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出用于VR中3D网格显著性标注的稳健框架：视域锥采样与流形-欧氏混合扩散，避免纹理诱导与拓扑泄漏，获得更符合人类感知的显著性GT。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格显著性GT获取多沿用2D图像思路与单射线采样/欧氏平滑，忽视3D拓扑与2D阵列差异，导致在复杂拓扑下触发纹理注意、跨缝隙信号泄漏与“拓扑短路”，影响VR人眼跟踪与显著性研究的可靠性。

Method: 1) 视域锥采样（VCS）：用高斯分布的射线束模拟人眼中央凹感受野，相比单射线对复杂拓扑更鲁棒；2) 混合流形-欧氏受限扩散（HCD）：在扩散过程中融合流形测地约束与欧氏尺度，沿真实表面拓扑传播并抑制跨缝隙/别名效应，实现拓扑一致的显著性传播。

Result: 通过VCS提高采样稳健性并减少纹理诱导；HCD在扩散阶段避免拓扑短路与混叠，使得获得的3D注意/显著性分布与真实人类感知更加一致，显著提升GT质量与鲁棒性。

Conclusion: 结合VCS与HCD的框架为3D网格显著性提供高保真、拓扑一致的GT获取范式，作为更准确稳健的基线，改进VR人眼跟踪与3D显著性研究。

Abstract: Reliable 3D mesh saliency ground truth (GT) is essential for human-centric visual modeling in virtual reality (VR). However, current 3D mesh saliency GT acquisition methods are generally consistent with 2D image methods, ignoring the differences between 3D geometry topology and 2D image array. Current VR eye-tracking pipelines rely on single ray sampling and Euclidean smoothing, triggering texture attention and signal leakage across gaps. This paper proposes a robust framework to address these limitations. We first introduce a view cone sampling (VCS) strategy, which simulates the human foveal receptive field via Gaussian-distributed ray bundles to improve sampling robustness for complex topologies. Furthermore, a hybrid Manifold-Euclidean constrained diffusion (HCD) algorithm is developed, fusing manifold geodesic constraints with Euclidean scales to ensure topologically-consistent saliency propagation. By mitigating "topological short-circuits" and aliasing, our framework provides a high-fidelity 3D attention acquisition paradigm that aligns with natural human perception, offering a more accurate and robust baseline for 3D mesh saliency research.

</details>


### [19] [Foreground-Aware Dataset Distillation via Dynamic Patch Selection](https://arxiv.org/abs/2601.02727)
*Longzhen Li,Guang Li,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出一种前景感知的数据集蒸馏方法，通过类别自适应阈值与双路径动态补丁选择（最优补丁或整图缩放）保留主体信息，减少背景冗余，显著提升跨架构与不同图像构成下的蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 优化式数据集蒸馏计算/内存代价高，且易生成噪声样、泛化差；非优化方法虽用真实补丁缓解成本，但僵化的补丁选择会丢失主体（foreground）关键信息，需要一种既高效又能保留主体语义的补丁选择策略。

Method: 利用Grounded SAM2检测并分割前景，计算每张图的前景占比，据此得到类别层面的补丁决策阈值；在该阈值引导下，对每张图采用双路径：若前景不占主导，生成多个候选补丁并选择信息量最大者；若前景占主导，则直接对整图做缩放作为蒸馏样本，从而内容自适应地保留主体、压缩背景。

Result: 在多基准数据集上相较现有优化式与非优化式方法均有持续提升，得到更具信息性与代表性的蒸馏数据；并在不同网络结构与图像构成下表现出更强鲁棒性。

Conclusion: 前景感知与类别自适应阈值结合的动态补丁选择能有效缓解以往方法的信息丢失与不真实问题，在保证效率的同时提升蒸馏数据质量与跨架构泛化能力。

Abstract: In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.

</details>


### [20] [HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps](https://arxiv.org/abs/2601.02730)
*Xuchang Zhong,Xu Cao,Jinke Feng,Hao Fang*

Main category: cs.CV

TL;DR: 提出一种同质变换(单应性)引导的图像-SD地图精定位网络：将车载视角特征投影到BEV并与SD地图语义对齐，利用单应关系进行特征融合与可行位姿约束，相比注意力融合与直接3DoF回归显著提升训练效率与定位精度；支持跨分辨率输入，在nuScenes上大幅优于SOTA。


<details>
  <summary>Details</summary>
Motivation: SD地图成本低、可扩展，但现有基于回归的方法忽略几何先验，导致训练低效、精度有限。需要一种兼顾几何约束与语义匹配的视觉定位方法，以在SD地图上实现高精度、可扩展的自动驾驶定位。

Method: 1) 构造满足单应约束的输入对：将车载地面视角特征投影至BEV，与SD地图特征进行语义对齐；2) 用单应关系引导特征融合，并将位姿输出限制在几何可行区域；3) 统一BEV语义推理与单应学习，从而进行图像到地图的细粒度定位；4) 显式建模单应变换，天然支持跨分辨率输入。

Result: 在nuScenes上进行大量实验，所提方法在定位精度上显著优于现有SOTA，同时训练效率更高；提供代码与预训练模型以促进复现与后续研究。

Conclusion: 通过将单应几何先验与BEV语义推理融合，可在SD地图上实现更高效、更高精度的视觉定位，并具备跨分辨率的灵活性，开辟了图像到地图定位的新范式。

Abstract: Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.

</details>


### [21] [Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench](https://arxiv.org/abs/2601.02737)
*Zanting Ye,Xiaolong Niu,Xuanbin Wu,Xu Han,Shengyuan Liu,Jing Hao,Zhihao Peng,Hao Sun,Jieqin Lv,Fanghu Wang,Yanchao Huang,Hubing Wu,Yixuan Yuan,Habib Zaidi,Arman Rahmim,Yefeng Zheng,Lijun Lu*

Main category: cs.CV

TL;DR: 提出PET-Bench功能影像基准，揭示MLLM在功能感知上的缺陷与CoT幻觉陷阱，并通过AVA微调策略显著提升诊断准确度。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在解剖影像任务上表现出色，但在功能影像（如PET）中的能力未知；作者发现视觉编码器依赖形态先验，无法独立解码示踪剂生物分布，带来安全风险，需系统评测与改进。

Method: 1) 构建PET-Bench：来自9732例多站点、多示踪剂PET研究，生成52308个分层问答；2) 系统评测19个SOTA MLLM，分析标准CoT提示在PET任务中的失效机制；3) 提出Atomic Visual Alignment（AVA）：先对低层次功能感知进行对齐学习，再进行高层诊断推理的简单微调流程。

Result: 在PET-Bench上发现“CoT幻觉陷阱”：标准CoT会使语言生成脱离视觉证据，产生看似专业但事实不扎实的诊断。采用AVA后，弥合功能感知鸿沟，将CoT从幻觉源转化为稳健推理工具，最高提升诊断准确度达14.83%。

Conclusion: 功能影像中的核心瓶颈是低层功能感知与高层推理的错位。通过PET-Bench揭示问题并用AVA进行原子级视觉对齐，可显著提升MLLM在PET任务中的可信度与准确性，对医疗AI安全具有重要意义。

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.

</details>


### [22] [D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images](https://arxiv.org/abs/2601.02747)
*Zixiao Wen,Zhen Yang,Xianjie Bao,Lei Zhang,Xiantai Xiang,Wenshuai Li,Yuhan Liu*

Main category: cs.CV

TL;DR: 提出D^3R-DETR：一种融合空间与频域信息、以密度引导的Tiny目标检测DETR，提升收敛与匹配精度，在AI-TOD-v2上达SOTA。


<details>
  <summary>Details</summary>
Motivation: Tiny目标在遥感场景中承载关键信息，但像素极少、密度变化大，主流Transformer/DETR存在收敛慢与查询-目标匹配不准的问题，需要一种能利用低层细节并稳健建模目标密度的方案。

Method: 构建D^3R-DETR，核心是“Dual-Domain Density Refinement”：将空间域与频域信息融合以细化低层特征，并用这些细节生成更准确的目标密度图；密度图作为先验指导DETR的查询定位与匹配，改善小目标的发现与回归；整体为DETR框架上的密度引导模块与特征精炼策略。

Result: 在AI-TOD-v2数据集上进行大量实验，D^3R-DETR在tiny对象检测上优于各现有SOTA方法，表现为更高的检测精度与更好的定位。

Conclusion: 密度先验与空间-频域融合可有效缓解DETR在小目标检测中的收敛与匹配问题，D^3R-DETR取得SOTA，并验证了低层细节与密度图对微小目标定位的价值。

Abstract: Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.

</details>


### [23] [Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups](https://arxiv.org/abs/2601.02759)
*Hyungtae Lim,Minkyun Seo,Luca Carlone,Jaesik Park*

Main category: cs.CV

TL;DR: BUFFER-X是一种无需训练的点云配准框架，面向零样本泛化：自适应超参、替换学习式关键点、尺度归一化，并通过分层多尺度匹配实现稳健配准；还提供更高效的BUFFER-X-Lite，计算时间降43%且精度基本不损失。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习点云配准在跨数据集/跨传感器零样本泛化差：1) 依赖固定手工超参（体素、搜索半径），难适配不同尺度；2) 学习式关键点跨域迁移弱；3) 使用绝对坐标放大不同数据集间的尺度不匹配。需要一种无需重训/调参、可广域泛化的方法。

Method: 提出BUFFER-X：1) 几何自举自动估计超参数；2) 分布感知的最远点采样（替代学习式关键点检测）；3) 基于patch的坐标归一化保证尺度一致；并采用层级多尺度匹配（局部-中尺度-全局感受野）提取鲁棒对应。提供BUFFER-X-Lite，通过早退策略与快速位姿求解器将总时延降43%。

Result: 在覆盖物体级、室内、室外与跨传感器（异构LiDAR）的12个数据集上，无需手动调参或先验知识即可实现强零样本泛化；BUFFER-X-Lite在保持精度的同时将计算时间相对BUFFER-X降低约43%。

Conclusion: 训练-free与自适应设计使BUFFER-X在多域点云配准中实现稳定零样本泛化；多尺度匹配与尺度归一化是关键。Lite版本进一步提升效率而不显著牺牲精度。

Abstract: Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.

</details>


### [24] [AnyDepth: Depth Estimation Made Easy](https://arxiv.org/abs/2601.02760)
*Zeyu Ren,Zeyu Zhang,Wukai Li,Qingxiang Liu,Hao Tang*

Main category: cs.CV

TL;DR: 提出AnyDepth：基于DINOv3编码器与简化解码器SDT的零样本单目深度估计框架，并配合数据质量筛选策略，在五个基准上以更少参数优于DPT。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计在零样本泛化上受限：依赖大规模数据与复杂解码器（如DPT），导致计算开销大、训练数据质量参差影响效果与效率。

Method: 1) 使用DINOv3作为视觉编码器以获取高质量稠密特征；2) 设计Simple Depth Transformer（SDT）作为轻量Transformer解码器，采用单路径特征融合与上采样，避免跨尺度复杂融合；3) 提出基于质量的样本筛选策略，剔除有害样本，缩小数据规模并提升训练质量。

Result: 在五个数据集基准上，所提框架在精度上超过DPT，同时参数量减少约85%-89%，计算开销更低；具备更好的零样本泛化与效率。

Conclusion: 精简的解码器设计与数据质量控制相结合，可在保持甚至提升精度的同时显著降低参数与计算成本，实现高效、可泛化的零样本单目深度估计。

Abstract: Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.

</details>


### [25] [ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration](https://arxiv.org/abs/2601.02763)
*Xu Zhang,Huan Zhang,Guoli Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: ClearAIR是一个受人类视觉感知启发的全能图像复原框架，通过“整体评估—区域与任务识别—细节重建”的层级粗到细流程，结合MLLM-IQA全局质评、语义引导的区域降解建模和自监督的内部线索复用，实现对复杂复合降解的更精确与细致恢复，优于多种合成与真实数据集上的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有All-in-One图像复原方法过度依赖降解特定表征，面对真实复杂复合降解时易出现过度平滑与伪影，缺乏跨模态理解的整体质量评估与对区域异质降解的精细处理能力。

Method: 提出ClearAIR框架，分三层：1) 全局阶段：利用基于多模态大模型的IQA进行整体质量与复合降解评估，利用跨模态理解更准确刻画复杂降解；2) 区域与任务阶段：语义交叉注意力与语义引导单元产生粗语义提示，并以此引导降解感知模块隐式捕获区域特异降解特征，实现局部精准复原；3) 细节阶段：内部线索复用机制以自监督方式挖掘并重用图像内部冗余与纹理线索，增强高频细节恢复。

Result: 在多种合成与真实世界数据集上取得更优的定量与定性表现（文中称“优于现有方法”），在复杂复合降解场景中减少过度平滑与伪影并提升细节质量。

Conclusion: 受HVP启发的分层粗到细复原范式、MLLM驱动的全局质评、语义引导的区域降解建模及自监督内部线索复用的组合，有效提升AiOIR对复杂真实降解的适应性与细节还原能力，具有通用性与扩展潜力。

Abstract: All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.

</details>


### [26] [AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs](https://arxiv.org/abs/2601.02771)
*Boyu Chang,Qi Wang,Xi Guo,Zhixiong Nan,Yazhou Yao,Tianfei Zhou*

Main category: cs.CV

TL;DR: 提出AbductiveMLLM，通过语言推理(REASONER)与图像想象(IMAGINER)的双模协同，显著提升MLLM在视觉溯因推理(VAR)上的表现，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型虽擅长通用推理，但在面对不完整视觉观测下的溯因推理仍弱于人类。认知科学提示人类在溯因中会交替使用语言与图像化思维，启发用双模机制提升模型的因果一致与情境落地能力。

Method: 设计两大组件并端到端联合训练：1) REASONER（语言域）：用“盲”LLM先广泛生成候选解释，再依据跨模态因果对齐剔除与视觉不一致的假设，并将保留假设作为面向因果的先验注入MLLM以引导其推理；2) IMAGINER（图像域）：将输入视频与REASONER输出的语义嵌入一起作为条件，驱动文本到图像扩散模型“想象”与解释相符的场景，增强MLLM的情境与视觉锚定。

Result: 在标准VAR基准上优于传统方法与先进MLLM，稳定取得SOTA。

Conclusion: 语言与图像双模溯因的协同可显著弥补MLLM在视觉溯因中的短板；通过因果对齐的先验与可视化想象强化上下文落地，实现更可靠的解释性推理。

Abstract: Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.

</details>


### [27] [EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework](https://arxiv.org/abs/2601.02783)
*Junjue Wang,Yanfei Zhong,Zihang Chen,Zhuo Zheng,Ailong Ma,Liangpei Zhang*

Main category: cs.CV

TL;DR: 论文提出一个用于地球视觉-语言理解与生成的渐进式框架：数据集EarthVLSet与模型EarthVLNet。通过先做土地覆盖分割得到像素级语义，再用具备对象感知的LLM进行关系推理与答案生成，并引入数值差异损失以适配不同对象统计。实验在分割、多选VQA、开放式VQA三项基准上优于对比，并总结三点经验：分割特征稳定提升VQA，且跨数据集有效；多选更依赖视觉编码器；开放式需更强视觉与语言模块。


<details>
  <summary>Details</summary>
Motivation: 传统遥感/地球视觉在目标识别上进展显著，但对对象间关系与综合语义理解探索不足，限制了如城市规划等高层决策应用。缺少同时涵盖图像-掩膜-文本、并支持多任务（分割与VQA）的标准基准与方法。

Method: 1) 数据集EarthVLSet：10.9k亚米级遥感图像+土地覆盖掩膜+约76.15万文本对，包含多选与开放式VQA。2) 模型EarthVLNet：对象中心的两阶段/渐进式管线——先进行土地覆盖语义分割，产出像素级语义与对象实例提示；再以对象感知的LLM执行关系推理与知识汇总，生成答案。3) 优化：提出数值差异损失，动态施加差异惩罚以处理不同对象统计偏差。

Result: 在三类基准（语义分割、多选VQA、开放式VQA）上取得优于现有方法的表现；并通过消融与跨数据集实验验证分割特征对VQA的稳定增益、多选VQA对视觉编码器更敏感、开放式VQA需更强的视觉与语言组件。

Conclusion: 将“图像-掩膜-文本”联结的渐进式地球视觉-语言框架能有效提升关系推理与综合理解，提供面向城市规划等应用的有价值基准与方法；未来应强化分割-VQA耦合、提升视觉编码器能力并发展更强的语言解码器以优化开放式任务。

Abstract: Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.

</details>


### [28] [DreamStyle: A Unified Framework for Video Stylization](https://arxiv.org/abs/2601.02785)
*Mengtian Li,Jinshu Chen,Songtao Zhao,Wanquan Feng,Pengqi Tu,Qian He*

Main category: cs.CV

TL;DR: DreamStyle提出统一的视频风格化框架，支持文本、风格图像和首帧三种条件，并通过高质量数据与LoRA的token特定上行矩阵训练，显著提升风格一致性与时序稳定性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法通常只支持单一风格条件，限制适用性；同时缺乏高质量成对数据导致风格不一致与时间闪烁，需要一个统一且稳健的方案与数据管线。

Method: 在基础I2V模型上构建统一框架，支持三类条件：(1) 文本引导；(2) 风格图像引导；(3) 首帧引导。提出数据策划管线获取高质量成对视频数据；采用LoRA进行微调，并为不同条件token设计特定的上行(up)矩阵以降低条件间混淆。

Result: 在三项视频风格化任务上进行定性及定量评估，显示在风格一致性与视频质量方面优于现有方法，能稳定处理长视频与多样风格条件。

Conclusion: DreamStyle实现了对多种风格条件的统一支持，并通过数据与训练细节改进缓解风格漂移与时间闪烁，提供更高质量、适用范围更广的视频风格化。

Abstract: Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.

</details>


### [29] [Textile IR: A Bidirectional Intermediate Representation for Physics-Aware Fashion CAD](https://arxiv.org/abs/2601.02792)
*Petteri Teikari,Neliana Fuenmayor*

Main category: cs.CV

TL;DR: Textile IR提出一种可双向联通的中间表示，将可制造的服装版型CAD、基于物理的仿真、以及生命周期评估（LCA）整合在同一架构中，并用七层“验证阶梯”贯通从快速语法检查到昂贵物理验证，实现不确定性显式追踪与双向反馈。


<details>
  <summary>Details</summary>
Motivation: 现有工具各自为政：版型软件能保证可缝制性却不了解悬垂与力学，物理仿真能预测行为却不能自动修正版型，LCA难与设计迭代联动且不确定性被忽视，导致设计—制造—可持续性之间割裂、迭代成本高、结论不可靠。

Method: 提出Textile IR作为语义粘合层：以场景图结构编码服装（版片、缝线、材料、约束）；定义三域（制造、物理、LCA）的约束满足问题；构建七层验证阶梯（如版片闭合、缝合兼容、材质映射、悬垂仿真、应力分析等）；支持双向反馈（仿真失败→版型修改建议，材料替换→LCA即时更新）；对测试误差、仿真近似、数据库缺口进行不确定性传播并给出置信界。

Result: 证明IR能让AI将服装当作结构化程序操控而非像素；实现跨工具联动与在线一致性检查；在例子中能即时反映材料替换对可持续性与力学的影响，并通过验证阶梯减少昂贵全物理仿真的调用频率。

Conclusion: Textile IR使工程约束可感知、可操作且立即生效，从而让设计师同时权衡可制造性、可持续性与美学，降低返工与实体打样成本；作者提出六项未来研究重点并讨论在中小企业中的落地要点。

Abstract: We introduce Textile IR, a bidirectional intermediate representation that connects manufacturing-valid CAD, physics-based simulation, and lifecycle assessment for fashion design. Unlike existing siloed tools where pattern software guarantees sewable outputs but understands nothing about drape, and physics simulation predicts behaviour but cannot automatically fix patterns, Textile IR provides the semantic glue for integration through a seven-layer Verification Ladder -- from cheap syntactic checks (pattern closure, seam compatibility) to expensive physics validation (drape simulation, stress analysis). The architecture enables bidirectional feedback: simulation failures suggest pattern modifications; material substitutions update sustainability estimates in real time; uncertainty propagates across the pipeline with explicit confidence bounds. We formalise fashion engineering as constraint satisfaction over three domains and demonstrate how Textile IR's scene-graph representation enables AI systems to manipulate garments as structured programs rather than pixel arrays. The framework addresses the compound uncertainty problem: when measurement errors in material testing, simulation approximations, and LCA database gaps combine, sustainability claims become unreliable without explicit uncertainty tracking. We propose six research priorities and discuss deployment considerations for fashion SMEs where integrated workflows reduce specialised engineering requirements. Key contribution: a formal representation that makes engineering constraints perceptible, manipulable, and immediately consequential -- enabling designers to navigate sustainability, manufacturability, and aesthetic tradeoffs simultaneously rather than discovering conflicts after costly physical prototyping.

</details>


### [30] [StableDPT: Temporal Stable Monocular Video Depth Estimation](https://arxiv.org/abs/2601.02793)
*Ivan Sobko,Hayko Riemenschneider,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: 提出StableDPT：在任意单帧MDE模型上加上轻量时序模块，用跨帧关键帧交叉注意力与新推理策略，实现更稳、更准、更快的视频深度估计。


<details>
  <summary>Details</summary>
Motivation: 单帧MDE直接用于视频会产生时间不稳定与闪烁；现有视频方法常依赖重叠滑窗，带来尺度错配与冗余计算。需要一种既能引入全局时序上下文又高效可训练/推理的通用适配方案。

Method: 以ViT编码器+改进DPT头为基底，在头部引入时序层：对全序列采样关键帧，通过高效交叉注意力融合多帧信息，学习全局上下文与跨帧关系；并提出新的视频推理策略，支持任意长度输入，避免重叠窗口导致的尺度不对齐与重复计算；可在单GPU几天内训练，适配任意SOTA图像深度模型。

Result: 在多基准数据集上获得更高时间一致性、与SOTA相当或更优的精度；在真实场景推理中实现约2倍速度提升。

Conclusion: 将时序交叉注意力集成到DPT头并配合无重叠的新推理策略，可在不更换底座模型的前提下显著提升视频深度估计的稳定性、准确性与效率。

Abstract: Applying single image Monocular Depth Estimation (MDE) models to video sequences introduces significant temporal instability and flickering artifacts. We propose a novel approach that adapts any state-of-the-art image-based (depth) estimation model for video processing by integrating a new temporal module - trainable on a single GPU in a few days. Our architecture StableDPT builds upon an off-the-shelf Vision Transformer (ViT) encoder and enhances the Dense Prediction Transformer (DPT) head. The core of our contribution lies in the temporal layers within the head, which use an efficient cross-attention mechanism to integrate information from keyframes sampled across the entire video sequence. This allows the model to capture global context and inter-frame relationships leading to more accurate and temporally stable depth predictions. Furthermore, we propose a novel inference strategy for processing videos of arbitrary length avoiding the scale misalignment and redundant computations associated with overlapping windows used in other methods. Evaluations on multiple benchmark datasets demonstrate improved temporal consistency, competitive state-of-the-art performance and on top 2x faster processing in real-world scenarios.

</details>


### [31] [Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining](https://arxiv.org/abs/2601.02806)
*Mingzhou Jiang,Jiaying Zhou,Nan Zeng,Mickael Li,Qijie Tang,Chao He,Huazhu Fu,Honghui He*

Main category: cs.CV

TL;DR: 提出一种拓扑感知的H&E到IHC虚拟染色框架，通过图对比学习与拓扑扰动（TACM）提升结构一致性，并以节点重要性约束的病理匹配（TCPM）对齐阳性区域，缓解弱配准与形变问题，在多基准与多任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 临床IHC染色信息丰富但昂贵耗时，且与H&E相邻切片配对存在错位与局部形变，导致监督学习困难。需要一种既能抵御空间不对齐又能保持病理关键区域一致性的虚拟染色方法，以提升临床可用性。

Method: 提出拓扑感知框架，包含两大机制：1）TACM：利用图对比学习与拓扑扰动，学习在空间错位情况下仍稳定的匹配模式，强化结构（拓扑）一致性；2）TCPM：依据图节点重要性（如病理显著性）进行约束与匹配，对齐病理阳性区域，提升病理一致性。整体在弱配对数据上训练，实现H&E→IHC生成。

Result: 在两个基准、四个染色任务上，所提方法在生成质量与临床相关性上均超过现有最先进方法，表现出更好的结构保持与病理区域对齐。

Conclusion: 拓扑感知的一致性与病理约束共同缓解弱配准带来的学习困难，可更稳健地从H&E生成IHC，兼顾结构与病理一致性，具备更高临床应用潜力。

Abstract: Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.

</details>


### [32] [SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models](https://arxiv.org/abs/2601.02825)
*Ruiyang Zhang,Dongzhan Zhou,Zhedong Zheng*

Main category: cs.CV

TL;DR: 提出SketchThinker-R1，通过奖励“草图式”推理，让多模态大模型在保持精度的同时将推理token成本降低约64%。


<details>
  <summary>Details</summary>
Motivation: 长链式推理虽有效但计算代价高（token和时延）。人类常用“草图式”简明、目标导向的推理，聚焦关键信息、效率高。作者希望把这种高效认知引入多模态大模型，减少推理冗余而不损失答案准确性。

Method: 三阶段：1) Sketch-Mode Cold Start：将常规长推理转化为草图式推理样本，对基座多模态模型进行微调，赋予初始草图式能力；2) 训练SketchJudge奖励模型：专门评估思维过程，对草图式推理给更高分；3) 在SketchJudge监督下进行Sketch-Thinking强化学习，进一步泛化草图式推理。

Result: 在四个基准上，推理token成本平均降低超过64%，同时最终答案准确率不下降。定性分析显示模型推理更聚焦关键线索。

Conclusion: 通过面向草图式推理的奖励与RL训练，可显著压缩思维token并保持准确性，展示了将人类高效推理范式引入多模态模型的有效性。

Abstract: Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.

</details>


### [33] [DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection](https://arxiv.org/abs/2601.02831)
*Yuetong Li,Qing Zhang,Yilin Zhao,Gongyang Li,Zeming Liu*

Main category: cs.CV

TL;DR: 论文提出DGA-Net，通过“深度提示（depth prompting）”将SAM适配到伪装目标检测，实现密集深度引导，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: COD中伪装目标与背景相似，单靠RGB难以辨别；深度几何能提供补充线索。但现有基于SAM的改造多用稀疏点/框提示，无法充分利用深度的全局、密集信息，且特征层级传递易衰减，导致分割不稳定。

Method: 构建DGA-Net，以密集“深度提示”适配SAM：1）提出跨模态图增强CGE，将RGB语义与深度几何在异构图中融合，生成统一引导信号作为密集提示；2）设计锚点引导细化AGR，构建全局锚并打通非局部通路，将深层指导直接广播至浅层，缓解层级信息衰减，获得一致、精确分割。

Result: 在定量与定性实验上，DGA-Net整体优于当前SOTA COD方法（文中宣称在多项指标上取得领先）。

Conclusion: 密集深度提示结合跨模态图融合与锚点引导的非局部传播，能有效提升SAM在伪装目标检测中的分割精度与稳定性，优于依赖稀疏提示的方法。

Abstract: To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.

</details>


### [34] [Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection](https://arxiv.org/abs/2601.02837)
*Yuteng Liu,Duanni Meng,Maoxun Yuan,Xingxing Wei*

Main category: cs.CV

TL;DR: 论文提出SEF-DETR用于红外小目标检测，针对DETR在低信噪比与复杂杂波下自注意力被背景主导的问题，提出频域引导筛选、动态嵌入增强与可靠性-一致性融合三模块，显著提升查询初始化与定位精度，并在三套数据集上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有DETR依赖全局自注意力，红外小目标在频繁背景干扰、尺寸小、SNR低的场景中，其目标相关特征在注意力中被背景强特征淹没，导致查询初始化不可靠、定位不准，需一种能抑制背景并增强目标相关嵌入的机制。

Method: 提出SEF-DETR，包括：1）FPS：利用局部补丁的傅里叶频谱构建目标相关密度图，筛除背景主导特征；2）DEE：以目标感知方式动态增强多尺度表示，强化目标嵌入；3）RCF：通过空间-频率一致性与可靠性约束，进一步优化object queries的表示与融合。整体在DETR框架中改进查询初始化与解码。

Result: 在三个公共IRSTD数据集上进行大量实验，SEF-DETR在检测精度等指标上优于现有最先进方法，表现稳健且高效。

Conclusion: 通过频域引导筛选、动态多尺度增强与可靠性-一致性融合，SEF-DETR有效缓解自注意力下背景压制目标的问题，提升红外小目标检测中的查询初始化与定位质量，实现对复杂杂波下小目标的稳健检测。

Abstract: Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.

</details>


### [35] [Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion](https://arxiv.org/abs/2601.02881)
*Jakob Lønborg Christensen,Morten Rieger Hannemose,Anders Bjorholm Dahl,Vedrana Andersen Dahl*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的通用图像分割框架，摆脱掩码范式，整体直接生成整幅分割；通过位置感知调色板与2D格雷码、tanh末层、sigmoid加权和x-prediction等设计提升离散分割性能，虽未超越SOTA掩码法，但缩小差距并带来不确定性建模等新能力。


<details>
  <summary>Details</summary>
Motivation: 现有分割多依赖掩码/实例化管线与特定先验，难以统一处理不同类型分割并缺少对多解/不确定性的原则性描述；作者希望用单一扩散生成式建模直接输出全图分割，实现“不可知/任务无关”的统一框架。

Method: 将扩散模型用于离散标签图生成：1) 使用位置感知调色板并采用二维格雷码排序以稳定和提升像素类别编码；2) 采用最终tanh激活以适配离散数据重建；3) 系统比较扩散目标与损失权重，发现sigmoid加权在不同预测目标下更优，最终采用x-prediction；4) 端到端从零训练，不依赖掩码解码器。

Result: 模型尚未超越领先的掩码式架构，但显著缩小性能差距；同时展现独特能力，如对分割歧义的原则性建模。

Conclusion: 基于扩散的整体分割是可行的，并通过位置感知调色板+2D格雷码、tanh末层、sigmoid损失权重和x-prediction等关键改动获得稳健改进；结合大规模预训练或可提示条件，有望达到具有竞争力的表现。

Abstract: This paper introduces a diffusion-based framework for universal image segmentation, making agnostic segmentation possible without depending on mask-based frameworks and instead predicting the full segmentation in a holistic manner. We present several key adaptations to diffusion models, which are important in this discrete setting. Notably, we show that a location-aware palette with our 2D gray code ordering improves performance. Adding a final tanh activation function is crucial for discrete data. On optimizing diffusion parameters, the sigmoid loss weighting consistently outperforms alternatives, regardless of the prediction type used, and we settle on x-prediction. While our current model does not yet surpass leading mask-based architectures, it narrows the performance gap and introduces unique capabilities, such as principled ambiguity modeling, that these models lack. All models were trained from scratch, and we believe that combining our proposed improvements with large-scale pretraining or promptable conditioning could lead to competitive models.

</details>


### [36] [TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors](https://arxiv.org/abs/2601.02908)
*Wei-Yuan Cheng,Kai-Po Chang,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 提出TA-Prompting：用可学习时间锚点精准定位视频事件、并以提示方式让VideoLLMs进行时序感知生成；结合事件一致性采样在推理时选取连贯且与视频相符的字幕，显著提升密集视频描述与时序理解任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs虽能生成细致描述，但难以在未裁剪视频中精准划定事件边界，导致字幕缺乏时序落地（grounding）。需要一种机制提升事件定位与跨模态对齐，使生成结果在时间维度更准确且连贯。

Method: 1) Temporal Anchors：学习一组时间锚点以精准定位事件边界，并将这些锚点作为提示，引导LLM进行“时间感知”的事件理解与生成；2) 事件一致性采样（inference）：面对不定数量事件，基于跨事件的语义连贯性及与视频的跨模态相似度，选择最终输出的事件字幕序列。

Result: 在基准数据集上，相较于SOTA VideoLLMs，在密集视频描述与多种时序理解任务（如moment retrieval、TemporalQA）上取得更优表现。

Conclusion: 通过时间锚点与一致性采样，将精准事件定位与LLM生成有效结合，提升了视频字幕的时序落地性与整体质量，并具备通用的时序理解增强能力。

Abstract: Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.

</details>


### [37] [Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning](https://arxiv.org/abs/2601.02918)
*Guoqiang Liang,Jianyi Wang,Zhonghua Wu,Shangchen Zhou*

Main category: cs.CV

TL;DR: 提出Zoom-IQA：一种具不确定性感知、区域推理与迭代精炼能力的VLM式图像质量评估模型，通过SFT+RL两阶段训练（含KL-Coverage正则与渐进重采样），在鲁棒性、可解释性与泛化上优于现有方法，并可助力图像复原等下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有IQA方法要么只给分数缺少解释，要么仅有低层次描述缺乏精确评分；基于VLM的方案虽可联合生成描述与分数，但视觉-文本线索融合不充分，导致推理不可靠。需要一种能显式模拟人类评估关键认知过程（不确定性意识、关注关键区域、逐步修正）的模型。

Method: 提出Zoom-IQA与两阶段训练管线：1) 在自建的GR-IQA数据集上进行SFT，学习将质量评估与关键区域定位（grounding）对齐；2) 采用RL进行动态策略探索，引入KL-Coverage正则以抑制推理与评分的多样性坍缩，并用渐进重采样策略缓解标注偏差，实现不确定性感知、区域级推理和迭代式回答精炼。

Result: 在多项实验中，Zoom-IQA在鲁棒性、可解释性、泛化能力方面显著优于现有VLM-IQA与传统IQA方法；在图像复原等下游任务中表现出更好的指导与评估效果。

Conclusion: 通过将区域级依据、RL驱动的策略探索与正则化/重采样机制结合，Zoom-IQA实现了更可靠的视觉-语言推理与评分，提升IQA的可信度与可用性，并能迁移到实际下游应用。

Abstract: Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.

</details>


### [38] [DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification](https://arxiv.org/abs/2601.02924)
*Aihua Zheng,Ya Gao,Shihao Li,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出DCG-ReID，通过动态置信解耦加上两种情景化融合（协同与引导）来适配多模态车辆ReID中模态质量分布的差异，解决同类一致性与跨模态差异的冲突，在三大基准上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 多模态车辆ReID需要同时利用RGB/NIR/TIR的互补信息，但各模态固有差异导致质量分布不确定：当模态质量均衡与不均衡时，最佳融合需求相互冲突（既要类内一致，又要抑制跨模态异质性）。现有方法用单一融合策略处理所有数据，难以兼顾两种情形并解耦冲突。

Method: 构建DCG-ReID框架，核心为动态置信解耦加权（DCDW）：依据模态交互得到的“模态置信度”，对三模态贡献进行动态加权，从而在融合层面实现样本级解耦。在此基础上设计两种情景化融合策略：1) 协同融合模块（CFM）用于质量分布均衡时，挖掘两两模态的一致性与共享判别特征，提升类内一致性；2) 引导融合模块（GFM）用于质量分布不均衡时，放大量纲优势模态的判别力，同时引导辅助模态挖掘互补信息并缓解跨模态分歧，提升联合判别。

Result: 在WMVeID863、MSVR310、RGBNT100三项多模态ReID基准上进行广泛实验，DCG-ReID优于现有方法（摘要未给出具体数值），验证了方法有效性。

Conclusion: 通过DCDW实现基于置信的解耦，再用CFM与GFM分别适配均衡与不均衡场景，能化解类内一致性与跨模态异质性的冲突，显著提升多模态车辆ReID性能；代码将开源。

Abstract: Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.

</details>


### [39] [PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding](https://arxiv.org/abs/2601.02927)
*Iñaki Erregue,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: PrismVAU是一种轻量实时视频异常理解系统，通过一个现成的多模态大模型同时完成异常评分、解释与提示优化，避免昂贵标注与复杂流水线，并在标准基准上取得有竞争力表现与可解释结果。


<details>
  <summary>Details</summary>
Motivation: 传统VAD只检测/定位异常，无法解释语境；现有VAU依赖微调MLLM或外部模块（字幕器等），导致需要密集标注、复杂训练与高推理成本，难以落地。

Method: 两阶段：1）粗粒度异常评分：利用与文本锚点的相似度计算帧级异常分数；2）MLLM精炼：通过系统与用户提示对异常进行语境化解释与细化。文本锚点与提示均通过弱监督的自动提示工程（APE）优化。仅用一个现成MLLM，无指令微调、无帧级标注、无外部密集处理。

Result: 在标准VAD基准上实现竞争性的检测性能，并能给出可解释的异常说明，同时维持低开销、实时性与简洁部署。

Conclusion: PrismVAU在不增加训练与推理复杂度的前提下，实现了VAU的检测与解释一体化，为实际场景提供高效、可解释、可部署的解决方案。

Abstract: Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.

</details>


### [40] [HybridSolarNet: A Lightweight and Explainable EfficientNet-CBAM Architecture for Real-Time Solar Panel Fault Detection](https://arxiv.org/abs/2601.02928)
*Md. Asif Hossain,G M Mota-Tahrin Tayef,Nabil Subhan*

Main category: cs.CV

TL;DR: 提出HybridSolarNet：EfficientNet‑B0+CBAM，配合focal loss与余弦退火，在Kaggle太阳能面板故障数据上以严格“先划分后增强”防泄漏，5折平均准确率92.37%、F1=0.9226，模型仅16.3MB、推理54.9 FPS，适合UAV实时部署，Grad‑CAM显示关注有效区域。


<details>
  <summary>Details</summary>
Motivation: 人工巡检光伏板成本高且易出错；现有深度学习方法要么参数庞大不适合边缘端，要么因数据划分与训练策略不当导致精度估计偏差。需要一种轻量且评估无偏、可实时部署于无人机的故障检测模型。

Method: 构建HybridSolarNet：以EfficientNet‑B0为骨干，插入CBAM提升通道与空间注意；采用“先划分后数据增强”的严格协议避免数据泄漏；训练中使用focal loss处理类别不平衡，并使用cosine annealing学习率调度；进行5折分层交叉验证与消融实验；用Grad‑CAM做可视化。

Result: 在Kaggle太阳能面板竞赛数据上，5折平均准确率92.37%±0.41、F1=0.9226±0.39；相比VGG19等基线更优；添加CBAM带来约+1.53%提升，focal loss改善长尾类别识别；模型大小16.3MB（约小32倍），GPU推理54.9 FPS；Grad‑CAM聚焦于真实故障区域。

Conclusion: HybridSolarNet兼具高准确、轻量与实时性，评估流程避免性能高估，适合UAV端部署；注意力与不平衡学习策略是性能提升关键。

Abstract: Manual inspections for solar panel systems are a tedious, costly, and error-prone task, making it desirable for Unmanned Aerial Vehicle (UAV) based monitoring. Though deep learning models have excellent fault detection capabilities, almost all methods either are too large and heavy for edge computing devices or involve biased estimation of accuracy due to ineffective learning techniques. We propose a new solar panel fault detection model called HybridSolarNet. It integrates EfficientNet-B0 with Convolutional Block Attention Module (CBAM). We implemented it on the Kaggle Solar Panel Images competition dataset with a tight split-before-augmentation protocol. It avoids leakage in accuracy estimation. We introduced focal loss and cosine annealing. Ablation analysis validates that accuracy boosts due to added benefits from CBAM (+1.53%) and that there are benefits from recognition of classes with imbalanced samples via focal loss. Overall average accuracy on 5-fold stratified cross-validation experiments on the given competition dataset topped 92.37% +/- 0.41 and an F1-score of 0.9226 +/- 0.39 compared to baselines like VGG19, requiring merely 16.3 MB storage, i.e., 32 times less. Its inference speed measured at 54.9 FPS with GPU support makes it a successful candidate for real-time UAV implementation. Moreover, visualization obtained from Grad-CAM illustrates that HybridSolarNet focuses on actual locations instead of irrelevant ones.

</details>


### [41] [VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on](https://arxiv.org/abs/2601.02945)
*Xinyi Wei,Sijing Wu,Zitong Xu,Yunhao Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出VTONQA数据集，用于评估虚拟试衣图像在“服装贴合度、人体兼容性、整体质量”三维度的主观质量，并对现有VTON与IQA方法做基准，暴露其不足，为感知一致的评价与模型改进提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣生成常出现衣物形变与人体不一致等伪影，缺乏可靠、细粒度且与人类感知对齐的质量评测标准与数据，难以比较模型优劣与指导改进。

Method: 构建首个面向VTON的多维质量评测数据集VTONQA：收集11个代表性VTON模型生成的8132张图像；按服装贴合度、人体兼容性、整体质量三维度收集24396条主观评分（MOS）。基于该数据集同时对VTON模型与多种IQA指标进行系统性基准测试。

Result: 实验基准显示：现有VTON方法在三维度上均存在明显缺陷；通用IQA指标与VTON质量知觉相关性有限，难以全面反映VTON特有问题；VTONQA能有效区分模型质量差异并揭示评价盲点。

Conclusion: VTONQA为VTON领域提供了首个多维、感知对齐的评测基准，可作为后续质量评估方法与VTON模型改进的坚实基础，促进更可靠的虚拟试衣生成与客观评价。

Abstract: With the rapid development of e-commerce and digital fashion, image-based virtual try-on (VTON) has attracted increasing attention. However, existing VTON models often suffer from artifacts such as garment distortion and body inconsistency, highlighting the need for reliable quality evaluation of VTON-generated images. To this end, we construct VTONQA, the first multi-dimensional quality assessment dataset specifically designed for VTON, which contains 8,132 images generated by 11 representative VTON models, along with 24,396 mean opinion scores (MOSs) across three evaluation dimensions (i.e., clothing fit, body compatibility, and overall quality). Based on VTONQA, we benchmark both VTON models and a diverse set of image quality assessment (IQA) metrics, revealing the limitations of existing methods and highlighting the value of the proposed dataset. We believe that the VTONQA dataset and corresponding benchmarks will provide a solid foundation for perceptually aligned evaluation, benefiting both the development of quality assessment methods and the advancement of VTON models.

</details>


### [42] [LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing](https://arxiv.org/abs/2601.02987)
*Wingwa Fu,Takayuki Okatani*

Main category: cs.CV

TL;DR: 提出LAMS-Edit：在扩散模型的真实图像编辑中，将反演过程与编辑生成过程的中间状态（潜变量与注意力图）按步交替混合，以兼顾内容保真与编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有T2I编辑难以在“保留原始内容与结构”与“落实编辑指令”之间取得平衡，尤其在真实图像（需反演）的场景中更难稳定。需要一种在编辑时充分利用反演得到的中间信息的方法。

Method: 提出LAMS（Latent and Attention Mixing with Schedulers）：在每个扩散步，将反演轨迹与编辑轨迹的潜空间表示和注意力图按权重插值融合，权重由调度器控制；并与Prompt-to-Prompt结合形成LAMS-Edit，支持基于区域掩码的精确编辑与通过LoRA实现风格迁移。

Result: 大量实验表明，LAMS-Edit在保持内容结构的同时，能更好地执行编辑指令；在真实图像编辑、局部编辑和风格迁移任务中取得优于或竞争的效果。

Conclusion: 利用反演中间状态与编辑过程联合指导的混合策略，可显著改善扩散模型的真实图像编辑，兼顾内容保真与编辑力度，且框架可拓展并易与P2P、LoRA等方法集成。

Abstract: Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.

</details>


### [43] [ULS+: Data-driven Model Adaptation Enhances Lesion Segmentation](https://arxiv.org/abs/2601.02988)
*Rianne Weber,Niels Rocholl,Max de Grauw,Mathias Prokop,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: ULS+ 是在通用病灶分割（ULS）基础上融合更多公开数据并缩小输入尺寸的改进模型，取得更高精度、更快推理，并在ULS23测试榜单居首。


<details>
  <summary>Details</summary>
Motivation: 利用近年来新增的公开CT病灶数据提升全身病灶分割的泛化与鲁棒性，并改进交互式（点击点）VOI分割在临床中的可用性与稳定性。

Method: 在原ULS框架上：1) 融合多项新公开数据集进行联合训练；2) 调整输入为更小的图像尺寸以加快推理；3) 在ULS23测试集与Longitudinal-CT子集上，以Dice与对点击点偏移的鲁棒性为指标对比ULS与ULS+。

Result: ULS+在所有比较中显著优于ULS，推理更快、Dice更高、对点击点位置更稳健；并在ULS23挑战测试阶段排行榜名列第一。

Conclusion: 数据驱动的持续更新与临床验证循环使ULS+成为更稳健、具临床相关性的全身病灶分割基线与框架。

Abstract: In this study, we present ULS+, an enhanced version of the Universal Lesion Segmentation (ULS) model. The original ULS model segments lesions across the whole body in CT scans given volumes of interest (VOIs) centered around a click-point. Since its release, several new public datasets have become available that can further improve model performance. ULS+ incorporates these additional datasets and uses smaller input image sizes, resulting in higher accuracy and faster inference.
  We compared ULS and ULS+ using the Dice score and robustness to click-point location on the ULS23 Challenge test data and a subset of the Longitudinal-CT dataset. In all comparisons, ULS+ significantly outperformed ULS. Additionally, ULS+ ranks first on the ULS23 Challenge test-phase leaderboard. By maintaining a cycle of data-driven updates and clinical validation, ULS+ establishes a foundation for robust and clinically relevant lesion segmentation models.

</details>


### [44] [Towards Faithful Reasoning in Comics for Small MLLMs](https://arxiv.org/abs/2601.02991)
*Chengcheng Feng,Haojie Yin,Yucheng Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 提出一种针对漫画VQA的推理框架，结合模块化CoT与基于GRPO的强化微调及结构化奖励，在5个基准上以3B模型超越SOTA，并作为插件为多种MLLM带来平均12.1%的提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在漫画VQA上表现不佳，原因在于漫画的符号抽象、叙事逻辑与幽默特性不同于常规VQA。直接使用标准CoT在小模型和资源受限条件下会适得其反，存在状态纠缠、伪迁移与探索低效等问题，需要一种能生成更忠实、可迁移推理链的方法。

Method: 提出“漫画推理框架”：1) 模块化CoT生成，将整体推理拆分为可控子模块（如角色识别、情节因果、幽默/反讽解析等），降低状态耦合与伪关联；2) 采用GRPO（Group Relative Policy Optimization）进行强化微调，以相对优势信号稳定训练；3) 设计结构化奖励，覆盖正确感知、叙事一致性、幽默解析与最终答案一致性，促进高质量可迁移推理链；并作为插件接入不同MLLM。

Result: 在漫画VQA、迷因理解、时评漫画阐释等幽默/抽象视觉推理任务上，3B模型在5个困难基准上超越SOTA；作为插件接入多种MLLM，平均额外提升12.1%。

Conclusion: 标准CoT在CVQA中会因状态纠缠、伪迁移和探索低效而降性能，尤其在小模型。通过模块化CoT+GRPO强化微调+结构化奖励，可生成更忠实、可迁移的推理链，显著提升漫画及更广泛幽默/抽象视觉推理任务表现。

Abstract: Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\mathbf{12.1\%}$ across different MLLMs.

</details>


### [45] [Towards Efficient 3D Object Detection for Vehicle-Infrastructure Collaboration via Risk-Intent Selection](https://arxiv.org/abs/2601.03001)
*Li Wang,Boqi Li,Hang Chen,Xingjian Wu,Yichen Wang,Jiewen Tan,Xinyu Zhang,Huaping Liu*

Main category: cs.CV

TL;DR: 提出RiSe：通过基于风险与意图的选择性融合，在车路协同感知中仅传输高交互区域的高保真特征，把通信量降至全量特征的0.71%，同时保持SOTA检测精度。


<details>
  <summary>Details</summary>
Motivation: 车路协同感知能缓解遮挡，但中间特征共享仍存在冗余：大量来自非关键背景区域的特征占用带宽。现有方法多用空间压缩或静态置信图，难以针对动态交互风险自适应筛选，从而在带宽与性能间形成瓶颈。

Method: 提出RiSe框架，将“可见性优先”转为“风险优先”。1) 通过潜势场-轨迹相关模型（PTCM）量化运动体间的动力学与交互风险。2) 设计基于自车运动先验的意图驱动区域预测模块（IDAPM），在BEV上前瞻性预测决策关键区域。3) 基于上述两个模块进行语义选择性融合，仅对高交互/高风险区域传输高保真特征，对其他区域降采样或抑制，实现特征去噪与带宽控制。

Result: 在DeepAccident数据集上，通信量仅为全量特征共享的0.71%，仍保持SOTA级别的目标检测精度，展现出在带宽效率与感知性能之间的优越Pareto前沿。

Conclusion: 风险与意图驱动的选择性特征传输能显著降低带宽而不牺牲检测性能，为VICP提供了更高效的中间融合范式，并证明针对高交互区域聚焦是实现高性价比协同感知的有效途径。

Abstract: Vehicle-Infrastructure Collaborative Perception (VICP) is pivotal for resolving occlusion in autonomous driving, yet the trade-off between communication bandwidth and feature redundancy remains a critical bottleneck. While intermediate fusion mitigates data volume compared to raw sharing, existing frameworks typically rely on spatial compression or static confidence maps, which inefficiently transmit spatially redundant features from non-critical background regions. To address this, we propose Risk-intent Selective detection (RiSe), an interaction-aware framework that shifts the paradigm from identifying visible regions to prioritizing risk-critical ones. Specifically, we introduce a Potential Field-Trajectory Correlation Model (PTCM) grounded in potential field theory to quantitatively assess kinematic risks. Complementing this, an Intention-Driven Area Prediction Module (IDAPM) leverages ego-motion priors to proactively predict and filter key Bird's-Eye-View (BEV) areas essential for decision-making. By integrating these components, RiSe implements a semantic-selective fusion scheme that transmits high-fidelity features only from high-interaction regions, effectively acting as a feature denoiser. Extensive experiments on the DeepAccident dataset demonstrate that our method reduces communication volume to 0.71\% of full feature sharing while maintaining state-of-the-art detection accuracy, establishing a competitive Pareto frontier between bandwidth efficiency and perception performance.

</details>


### [46] [ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios](https://arxiv.org/abs/2601.03011)
*Yihan Wei,Shenghai Yuan,Tianchen Deng,Boyang Lou,Enwen Hu*

Main category: cs.CV

TL;DR: ReCCur提出一个低算力、多代理递归管线，把嘈杂网页图像转化为可审计的细粒度角落案例标签，用于在资源受限下提升异常/稀有场景的数据纯度与可分性。


<details>
  <summary>Details</summary>
Motivation: 角落案例（稀有/极端场景）常导致真实世界失败，但难以规模化获取与标注：网页数据嘈杂、标签脆弱、边缘部署难以大规模重训。需要一种低成本方法，从开放网络中持续挖掘并净化此类数据，且标签需可解释、可审计。

Method: 三阶段递归框架：1）数据获取与过滤：用VLM扩展领域词汇，爬取网页，基于图像-描述-关键词三模态一致性与少量人工抽检筛选候选；2）混合专家蒸馏：结合CLIP/DINOv2/BEiT等编码器进行kNN投票，采用双置信激活与不确定性采样，迭代收敛到高精度集合；3）区域证据的VLM对抗式标注：提议器生成多粒度区域与语义线索，验证器做全局与局部链式一致性校验，产出可解释标签并闭环迭代。

Result: 在真实角落案例（如洪水浸泡车辆检测）上，于消费级GPU运行，数据纯度与类别可分性稳步提升，且仅需极少人工监督，形成可用于下游训练与评估的实用数据基座。

Conclusion: ReCCur能在资源受限环境下，从嘈杂网络图像递归提纯角落案例并生成可审计细粒度标签，为下游鲁棒性训练/评测提供高质量数据；代码与数据将开源。

Abstract: Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.

</details>


### [47] [SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection](https://arxiv.org/abs/2601.03024)
*Kim Jun-Seong,Tae-Hyun Oh,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: SA-ResGS 是用于主动场景重建中的下一最佳视角（NBV）选择的框架，通过自增强点云与残差监督稳定并提升不确定性估计与其监督效能，从而改进重建质量与视角选择稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有 3D Gaussian Splatting 在稀疏、宽基线采样下不确定性估计不稳、视角选择偏置，且对“贡献弱/监督不足”的高不确定性高斯难以有效学习，导致场景覆盖不足与NBV规划不可靠。

Method: 1) 自增强点云（SA-Points）：在训练视角与外推栅格化视图之间做三角化，得到可用于快速估计场景覆盖的点云；2) 物理约束的视角选择：基于 SA-Points 进行覆盖度评估，选择能提升有效覆盖的下一视角；3) 不确定性感知残差监督（首个面向3DGS的残差策略）：结合不确定性过滤与受dropout/困难样本挖掘启发的采样，针对不确定性高、贡献弱的高斯放大梯度信号，缓解欠监督；4) 约束视角选择+残差监督共同作用，隐式消偏不确定性。

Result: 在主动视角选择任务上，相较SOTA基线，SA-ResGS在重建质量和视角选择稳健性上均有提升，并提供更稳定、有效的不确定性估计与覆盖评估。

Conclusion: SA-ResGS 通过自增强覆盖评估与不确定性感知残差监督，缓解稀疏与宽基线条件下的不确定性偏置与欠监督问题，实现更高质量、更稳健的主动重建与NBV规划。

Abstract: We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.

</details>


### [48] [Flow Matching and Diffusion Models via PointNet for Generating Fluid Fields on Irregular Geometries](https://arxiv.org/abs/2601.03030)
*Ali Kashefi*

Main category: cs.CV

TL;DR: 提出Flow Matching PointNet与Diffusion PointNet，两种将PointNet融入生成式流匹配与扩散模型的框架，直接在点云上预测不规则几何的流场，较GNN扩散法无高频噪声且架构更简洁，并在绕圆柱稳态不可压测试中在场变量与受力预测上更准、对不完整几何更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有在不规则几何上预测流场的方法常依赖像素化或图神经网络：像素化引入分辨率与别扭几何投影问题，GNN扩散模型易出现高频噪声且需额外条件网络，增加复杂性与不稳定性。需要一种能直接处理点云、生成式重构物理场、结构简洁且稳健的方案。

Method: 将PointNet作为几何条件编码器与场生成器的统一骨干：1) Flow Matching PointNet：采用flow matching生成框架，从标准高斯噪声通过反向常微分方程生成物理场，训练以匹配概率流。2) Diffusion PointNet：采用扩散模型，从噪声经反向扩散过程重建场。两者直接在计算域点云（如有限体积网格顶点）上操作，避免栅格化；无需额外中间网络即可条件化几何。

Result: 在稳态不可压绕圆柱数据集（改变截面形状与朝向）上，与相同参数量的vanilla PointNet及GNN扩散基线相比，两框架在速度、压力、升力与阻力预测更准确；生成结果无高频噪声伪影；对不完整几何输入更鲁棒。

Conclusion: 基于PointNet的生成式几何深度学习可在点云上高质量重建流场，较像素化与GNN扩散方法更简洁、稳定与精确；Flow Matching与Diffusion两种范式均有效，适合处理不规则与缺失几何的流体问题。

Abstract: We present two novel generative geometric deep learning frameworks, termed Flow Matching PointNet and Diffusion PointNet, for predicting fluid flow variables on irregular geometries by incorporating PointNet into flow matching and diffusion models, respectively. In these frameworks, a reverse generative process reconstructs physical fields from standard Gaussian noise conditioned on unseen geometries. The proposed approaches operate directly on point-cloud representations of computational domains (e.g., grid vertices of finite-volume meshes) and therefore avoid the limitations of pixelation used to project geometries onto uniform lattices. In contrast to graph neural network-based diffusion models, Flow Matching PointNet and Diffusion PointNet do not exhibit high-frequency noise artifacts in the predicted fields. Moreover, unlike such approaches, which require auxiliary intermediate networks to condition geometry, the proposed frameworks rely solely on PointNet, resulting in a simple and unified architecture. The performance of the proposed frameworks is evaluated on steady incompressible flow past a cylinder, using a geometric dataset constructed by varying the cylinder's cross-sectional shape and orientation across samples. The results demonstrate that Flow Matching PointNet and Diffusion PointNet achieve more accurate predictions of velocity and pressure fields, as well as lift and drag forces, and exhibit greater robustness to incomplete geometries compared to a vanilla PointNet with the same number of trainable parameters.

</details>


### [49] [Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion](https://arxiv.org/abs/2601.03046)
*Han Zhang,Yanwei Wang,Fang Li,Hongjun Wang*

Main category: cs.CV

TL;DR: 提出DFRCP作为YOLOv11的即插即用升级，通过动态模糊鲁棒金字塔提升在相机抖动引起的运动模糊下的检测精度；在自建模糊测试集上较基线提升约10.4%，并通过CUDA并行内核实现边缘侧可部署的高效推理。


<details>
  <summary>Details</summary>
Motivation: 运动模糊导致重影，显著破坏边缘与结构，现有方法要么把模糊当噪声压制而丢失判别信息，要么进行全图去模糊带来高延迟且不利于资源受限设备部署，亟需一种在不显著增加开销的情况下提升模糊鲁棒性的检测头/特征层方案。

Method: 在YOLOv11的特征金字塔中加入DFRCP：1) 融合大尺度与中尺度特征并保留原生表示；2) 设计Dynamic Robust Switch单元，自适应注入“模糊特征”以增强在抖动下的全局感知；3) 模糊特征通过对多尺度特征进行旋转与非线性插值生成，并用“透明卷积”学习性地在原始与模糊线索间权衡融合；4) 实现CUDA并行旋转与插值内核，避免边界溢出并获得>400×加速。训练使用约3500张小麦害虫损伤私有数据集，三倍增强（全图均匀运动模糊与目标框内旋转模糊）。

Result: 在带模糊的测试集上，YOLOv11+DFRCP较YOLOv11基线准确率提高约10.4%，训练时间仅有温和增加；实现层面通过CUDA内核显著加速，支持边缘设备部署，减少采集后人工过滤需求。

Conclusion: DFRCP作为轻量、可插拔的金字塔增强模块，能在不进行完整去模糊的情况下显著提升运动模糊场景下的目标检测鲁棒性，并具备高效的工程可落地性。

Abstract: Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.

</details>


### [50] [On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning](https://arxiv.org/abs/2601.03048)
*Siyi Lyu,Quan Liu,Feng Yan*

Main category: cs.CV

TL;DR: 论文指出：恒深度ViT在本质上缺乏表示非可解空间群结构（如SO(3)）所需的逻辑深度，因此在心智旋转等空间推理任务上会系统性失效。理论上将空间理解形式化为群同态学习，并给出复杂度下界与上界，实验以潜变量探针验证结构崩塌。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT在语义识别强，但在需要组合性空间推理（如三维旋转）时表现不佳。通常归因于数据/训练不足，作者提出更根本的原因可能是架构的“电路复杂度”限制，需建立复杂度层面的理论边界。

Method: 1) 将“空间理解”刻画为从图像序列到潜空间的群同态映射，要求保持底层变换群的代数结构；2) 对非可解群（如SO(3)）证明维持结构保持嵌入的计算需求至少与词问题等价，为NC^1完备；3) 证明多头注意力的恒深度ViT（在多项式精度下）受限于TC^0；4) 在TC^0 ⊊ NC^1的标准猜想下，给出恒深度ViT无法高效捕获非可解空间结构的复杂度边界；5) 通过潜空间探针实验，观察到在非可解任务上随组合深度增大ViT表示出现结构性坍塌。

Result: 理论：建立了非可解群空间推理需要至少NC^1复杂度，而恒深度ViT仅达TC^0，从而给出不可表达性边界。实证：在非可解群相关任务上，ViT的潜表示无法保持群结构，同态性指标随组合深度增加显著退化。

Conclusion: 恒深度ViT在逻辑深度上不足以学习非可解群的空间结构（如SO(3)心智旋转），即使扩大数据或精度也难以弥补；需要增加计算深度、改变架构或引入显式群结构（如等变/群卷积、可微群算子）以跨越该复杂度鸿沟。

Abstract: Vision Transformers (ViTs) excel in semantic recognition but exhibit systematic failures in spatial reasoning tasks such as mental rotation. While often attributed to data scale, we propose that this limitation arises from the intrinsic circuit complexity of the architecture. We formalize spatial understanding as learning a Group Homomorphism: mapping image sequences to a latent space that preserves the algebraic structure of the underlying transformation group. We demonstrate that for non-solvable groups (e.g., the 3D rotation group $\mathrm{SO}(3)$), maintaining such a structure-preserving embedding is computationally lower-bounded by the Word Problem, which is $\mathsf{NC^1}$-complete. In contrast, we prove that constant-depth ViTs with polynomial precision are strictly bounded by $\mathsf{TC^0}$. Under the conjecture $\mathsf{TC^0} \subsetneq \mathsf{NC^1}$, we establish a complexity boundary: constant-depth ViTs fundamentally lack the logical depth to efficiently capture non-solvable spatial structures. We validate this complexity gap via latent-space probing, demonstrating that ViT representations suffer a structural collapse on non-solvable tasks as compositional depth increases.

</details>


### [51] [IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation](https://arxiv.org/abs/2601.03054)
*Yankai Jiang,Qiaoru Li,Binlu Xu,Haoran Sun,Chao Ding,Junting Dong,Yuxiang Cai,Xuhong Zhang,Jianwei Yin*

Main category: cs.CV

TL;DR: 提出IBISAgent：将医学图像分割重构为多步视觉决策的代理式MLLM，通过文本点击与工具调用迭代细化掩码，无需改架构，配合两阶段训练（SFT冷启动+细粒度奖励的强化学习）在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLM做像素级理解受限：1）依赖隐式分割token并需同时微调MLLM与像素解码器，易灾难性遗忘、泛化差；2）多为单步推理，缺乏迭代细化能力，性能受限。需要一种能在不改架构的前提下实现可迭代、可泛化的分割方法。

Method: 提出IBISAgent，将分割视作以视觉为中心的多步决策过程。模型可交替输出推理与文本化点击动作，调用外部分割工具生成/更新掩码；在被掩膜的图像特征上继续多步视觉推理，从而支持掩码迭代优化与像素级视觉推理能力。训练采用两阶段：1）冷启动监督微调，使模型学会基本动作与对齐；2）基于细粒度、定制化奖励的代理式强化学习，提升在复杂医学指代与推理分割任务中的鲁棒性与策略质量。无需修改MLLM架构与引入专用像素解码器。

Result: 在多项医学分割与referring/推理分割基准上，IBISAgent稳定超越开源与闭源SOTA，生成高质量掩码并展现更强的跨域泛化与鲁棒性。

Conclusion: 把分割问题代理化与多步化，结合工具调用与迭代视觉推理，可在不改架构的前提下显著提升医学像素级理解；两阶段训练进一步增强策略与泛化。代码、数据与模型将开放。

Abstract: Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model's robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.

</details>


### [52] [Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding](https://arxiv.org/abs/2601.03056)
*Zhen Wang,Jiaojiao Zhao,Qilong Wang,Yongfeng Dong,Wenlong Yu*

Main category: cs.CV

TL;DR: 提出CFSG，通过在概念与特征空间中显式拆分“共性/特异/干扰”三部分，并引入自适应比例与显式配重，在细粒度域泛化中显著提升鲁棒性与可解释性，较基线+9.87%、较SOTA+3.08%。


<details>
  <summary>Details</summary>
Motivation: 细粒度任务类间差异细微、类内差异显著，域移下模型过度依赖脆弱细粒度线索，关键特征被抑制导致性能骤降；认知研究表明人类会结合共性与特异属性来判别细粒度类别，而现有DL方法未有效建模这种机制。

Method: 提出CFSG：在概念空间与特征空间同时进行结构化拆解为共性、特异、干扰三部分；设计自适应机制动态调整三部分比例以应对不同强度的分布偏移；在最终预测时对各“概念-特征”对应组件显式赋权融合；并通过可解释性分析验证结构化知识的整合与从特征到概念的结构化涌现。

Result: 在三个单源基准上，平均较基线提升9.87%，较现有SOTA平均再提升3.08%；可解释性结果显示模型学习到多粒度的共性/特异结构，并降低干扰影响。

Conclusion: 在细粒度域泛化中，同时对概念与特征进行三分结构化并自适应配比与赋权，可显著增强跨域鲁棒性与可解释性；特征结构化有助于概念结构化的形成。

Abstract: Fine-Grained Domain Generalization (FGDG) presents greater challenges than conventional domain generalization due to the subtle inter-class differences and relatively pronounced intra-class variations inherent in fine-grained recognition tasks. Under domain shifts, the model becomes overly sensitive to fine-grained cues, leading to the suppression of critical features and a significant drop in performance. Cognitive studies suggest that humans classify objects by leveraging both common and specific attributes, enabling accurate differentiation between fine-grained categories. However, current deep learning models have yet to incorporate this mechanism effectively. Inspired by this mechanism, we propose Concept-Feature Structuralized Generalization (CFSG). This model explicitly disentangles both the concept and feature spaces into three structured components: common, specific, and confounding segments. To mitigate the adverse effects of varying degrees of distribution shift, we introduce an adaptive mechanism that dynamically adjusts the proportions of common, specific, and confounding components. In the final prediction, explicit weights are assigned to each pair of components. Extensive experiments on three single-source benchmark datasets demonstrate that CFSG achieves an average performance improvement of 9.87% over baseline models and outperforms existing state-of-the-art methods by an average of 3.08%. Additionally, explainability analysis validates that CFSG effectively integrates multi-granularity structured knowledge and confirms that feature structuralization facilitates the emergence of concept structuralization.

</details>


### [53] [Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA](https://arxiv.org/abs/2601.03073)
*Tong Wu,Thanet Markchom*

Main category: cs.CV

TL;DR: 提出一个针对卡通风格图像VQA的多智能体LLM框架，包括视觉、语言与批评三代理，并在Pororo与Simpsons数据集上评估其贡献与协同效果。


<details>
  <summary>Details</summary>
Motivation: 标准在自然图像上训练的LLM/多模态模型难以处理卡通图像的夸张抽象与叙事语境，导致VQA表现受限；需要一种能融合视觉线索与叙事上下文、并具备可解释推理的方案。

Method: 设计多智能体架构：视觉代理提取并解释卡通视觉线索，语言代理建模叙事与语义推理，批评代理对中间推理与答案进行审查与纠偏；三者协作完成结构化推理。对Pororo与Simpsons两个卡通VQA数据集进行系统评测与消融，分析各代理对最终预测的贡献。

Result: 在两个数据集上进行了实验，报告了分解式分析与消融结果，展示各代理在提高答案准确性与推理可靠性中的作用，并揭示了多智能体在卡通VQA中的行为特征。

Conclusion: 多智能体LLM框架能更好地融合卡通图像的视觉夸张与叙事上下文，实现更可靠的VQA与可解释推理；对理解多智能体在多模态推理中的协同机制提供了实证依据。

Abstract: Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.

</details>


### [54] [LesionTABE: Equitable AI for Skin Lesion Detection](https://arxiv.org/abs/2601.03090)
*Rocio Mexia Diaz,Yasmin Greenway,Petru Manescu*

Main category: cs.CV

TL;DR: 提出LesionTABE，一个结合对抗去偏与皮肤科领域基础模型嵌入的公平性框架，在多数据集和多疾病类型上显著提升公平性（>25%）并同时提高总体诊断准确度。


<details>
  <summary>Details</summary>
Motivation: 皮肤科AI在深色皮肤上性能较差，阻碍临床落地；需要一种能在不牺牲准确度的前提下缓解肤色偏差、促进公平与可用性的方案。

Method: 构建LesionTABE：以皮肤科专用基础模型的特征嵌入为表示空间，并引入对抗式去偏训练，使模型在保持诊断能力的同时去除与肤色相关的可辨信息；在多数据集（涵盖恶性与炎症性病变）上与ResNet-152及现有去偏方法比较。

Result: 相较ResNet-152基线，公平性指标提升超过25%，并优于其他去偏方法；同时总体诊断准确率也得到提升。

Conclusion: 基础模型结合对抗去偏可有效降低肤色相关偏差，并在保持或提升准确度的同时改善公平性，朝向公平可用的皮肤科临床AI迈进一步。

Abstract: Bias remains a major barrier to the clinical adoption of AI in dermatology, as diagnostic models underperform on darker skin tones. We present LesionTABE, a fairness-centric framework that couples adversarial debiasing with dermatology-specific foundation model embeddings. Evaluated across multiple datasets covering both malignant and inflammatory conditions, LesionTABE achieves over a 25\% improvement in fairness metrics compared to a ResNet-152 baseline, outperforming existing debiasing methods while simultaneously enhancing overall diagnostic accuracy. These results highlight the potential of foundation model debiasing as a step towards equitable clinical AI adoption.

</details>


### [55] [Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs](https://arxiv.org/abs/2601.03100)
*Chenchen Lin,Sanbao Su,Rachel Luo,Yuxiao Chen,Yan Wang,Marco Pavone,Fei Miao*

Main category: cs.CV

TL;DR: 提出TGIF：一种按文本查询自适应融合视觉编码器多层特征的轻量模块，提升多模态大模型的视觉扎根，减少幻觉，并在多项基准上带来稳定增益。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多只用视觉编码器的末层特征，忽略层间层级信息，导致模型倾向语言先验而非图像证据，出现视觉未扎根的幻觉。已有缓解方法多在文本侧或使用静态多层融合，无法根据查询动态利用不同层的视觉线索。

Method: 提出Text-Guided Inter-layer Fusion (TGIF)。将视觉编码器各层视作深度方向的“专家”，在不更新编码器权重的前提下，通过文本提示（query）条件化，预测对各层视觉特征的融合权重，进行外部、直接、轻量的跨层特征融合；集成到LLaVA-1.5-7B中，几乎不增加计算与参数。

Result: 在幻觉、OCR、VQA基准上取得一致提升，同时在ScienceQA、GQA、MMBench上保持或小幅提升性能，显示出更强的视觉证据利用能力与更低的幻觉率。

Conclusion: 对查询敏感、层级感知的多层视觉特征融合可有效增强视觉扎根、减少幻觉；TGIF以低开销、免改编码器的方式为现代MLLM提供通用增益。

Abstract: Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise "experts" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.

</details>


### [56] [LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition](https://arxiv.org/abs/2601.03124)
*B. M. Shahria Alam,Md. Nasim Ahmed*

Main category: cs.CV

TL;DR: 论文提出一个葡萄叶病害分类系统，基于Xception与InceptionV3迁移学习，在9,032张四分类数据集上取得最高96.23%准确率，并通过对抗训练提升鲁棒性、用Grad-CAM增强可解释性，最终以Streamlit部署含热力图与置信度的网页应用。


<details>
  <summary>Details</summary>
Motivation: 病害会显著降低作物产量和品质，农民需要快速、可靠、可解释的诊断工具；针对葡萄叶病害，现有方法在鲁棒性与可解释性、以及端到端应用落地方面仍有改进空间。

Method: - 数据：9,032张葡萄叶影像，四分类（三种病害+健康），按70/20/10划分训练/验证/测试；进行严格预处理。
- 模型：采用预训练的InceptionV3与Xception进行迁移学习与微调。
- 鲁棒性：引入对抗训练以提升对扰动的稳健性。
- 可解释性：集成Grad-CAM生成热力图以定位病灶区域并验证模型决策。
- 部署：使用Streamlit构建网页应用，提供预测结果、置信度与可视化热力图。

Result: Xception在测试上达到96.23%准确率，优于InceptionV3；系统能够输出热力图并在网页端交互展示。

Conclusion: Xception是该任务上更优选，结合对抗训练与Grad-CAM可实现更鲁棒、可解释的葡萄叶病害识别，并已通过Streamlit实现实用化部署。

Abstract: Plant disease diagnosis is essential to farmers' management choices because plant diseases frequently lower crop yield and product quality. For harvests to flourish and agricultural productivity to boost, grape leaf disease detection is important. The plant disease dataset contains grape leaf diseases total of 9,032 images of four classes, among them three classes are leaf diseases, and the other one is healthy leaves. After rigorous pre-processing dataset was split (70% training, 20% validation, 10% testing), and two pre-trained models were deployed: InceptionV3 and Xception. Xception shows a promising result of 96.23% accuracy, which is remarkable than InceptionV3. Adversarial Training is used for robustness, along with more transparency. Grad-CAM is integrated to confirm the leaf disease. Finally deployed a web application using Streamlit with a heatmap visualization and prediction with confidence level for robust grape leaf disease classification.

</details>


### [57] [Unified Thinker: A General Reasoning Modular Core for Image Generation](https://arxiv.org/abs/2601.03127)
*Sashuai Zhou,Qiang Zhou,Jijin Hu,Hanqing Yang,Yue Cao,Junpeng Ma,Yinchao Ma,Jun Song,Tiezheng Ge,Cheng Yu,Bo Zheng,Zhou Zhao*

Main category: cs.CV

TL;DR: 提出Unified Thinker：把“思考(规划)”与“生成”解耦的统一推理核心，通过结构化规划接口与强化学习，以像素级反馈优化，显著提升文本生成图像与图像编辑中的推理与执行一致性。


<details>
  <summary>Details</summary>
Motivation: 开源生成模型在遵循复杂逻辑指令和多约束合成上存在“推理—执行”鸿沟；封闭系统展示了强推理驱动的生成能力，说明仅提升视觉生成器不足，需要可执行、可验证的规划来直接驱动生成流程。

Method: 提出任务无关的Unified Thinker架构：1) 将Thinker与Generator解耦，作为可插拔的统一规划核心；2) 两阶段训练：先为Thinker构建结构化规划接口（将高层意图分解为可执行、可验证的计划步骤），再用强化学习以像素级反馈对其策略对齐，鼓励视觉正确性优先于文本表象；3) 可无缝接入多种生成器与工作流。

Result: 在文本生成图像与图像编辑任务上，Unified Thinker显著提升复杂指令的逻辑一致性与生成质量，证明规划驱动能有效缩小推理—执行差距。

Conclusion: 通过把推理从图像生成器中解耦并以像素反馈强化，Unified Thinker实现可执行规划驱动的通用图像生成，能模块化升级推理能力并在多任务中带来显著收益。

Abstract: Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.

</details>


### [58] [LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images](https://arxiv.org/abs/2601.03163)
*Matěj Pekár,Vít Musil,Rudolf Nenutil,Petr Holub,Tomáš Brázdil*

Main category: cs.CV

TL;DR: 提出LSP-DETR：端到端、线性复杂度的轻量Transformer，用星形多边形表示细胞核，实现无需后处理的实例分割，在PanNuke和MoNuSeg上高效且泛化强，速度比次快SOTA快5倍以上。


<details>
  <summary>Details</summary>
Motivation: 全视野病理切片为吉像素级，传统基于小块（patch）的实例分割既丢失上下文又需昂贵后处理来分离重叠核，效率与可扩展性受限，迫切需要能在大图上高效、可扩展、端到端的核实例分割方法。

Method: 设计LSP-DETR框架：使用计算复杂度线性的轻量Transformer，允许处理更大尺寸图像；将细胞核表示为星凸多边形，并引入径向距离损失（radial distance loss），使重叠核的实例分割在训练中自然涌现，无需显式重叠标注与手工后处理。端到端检测与分割统一。

Result: 在PanNuke与MoNuSeg数据集上验证，跨组织类型具有良好泛化与精度；与主流方法相比在效率上显著领先，推理速度超过次快领先方法5倍以上。

Conclusion: LSP-DETR在不增加计算成本的前提下显著扩大可处理图像尺度，利用星凸多边形与径向距离损失实现无需后处理的核实例分割，兼具精度、泛化性与高效率，适合大规模全视野病理分析。

Abstract: Precise and scalable instance segmentation of cell nuclei is essential for computational pathology, yet gigapixel Whole-Slide Images pose major computational challenges. Existing approaches rely on patch-based processing and costly post-processing for instance separation, sacrificing context and efficiency. We introduce LSP-DETR (Local Star Polygon DEtection TRansformer), a fully end-to-end framework that uses a lightweight transformer with linear complexity to process substantially larger images without additional computational cost. Nuclei are represented as star-convex polygons, and a novel radial distance loss function allows the segmentation of overlapping nuclei to emerge naturally, without requiring explicit overlap annotations or handcrafted post-processing. Evaluations on PanNuke and MoNuSeg show strong generalization across tissues and state-of-the-art efficiency, with LSP-DETR being over five times faster than the next-fastest leading method. Code and models are available at https://github.com/RationAI/lsp-detr.

</details>


### [59] [DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation](https://arxiv.org/abs/2601.03178)
*Jiajun jiao,Haowei Zhu,Puyuan Yang,Jianghui Wang,Ji Liu,Ziqiong Liu,Dong Li,Yuejian Fang,Junhai Yong,Bin Wang,Emad Barsoum*

Main category: cs.CV

TL;DR: 论文提出一个由大语言模型驱动的框架，用于自动生成与评测扩散模型加速代码，包括评测基准DiffBench和智能体DiffAgent。通过自动规划、代码生成、调试与遗传算法反馈闭环，针对任意扩散模型组合多种加速技术，显著优于通用LLMs在生成有效加速策略上的表现。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像/视频生成上表现出色，但多步推理导致计算开销大、部署受限。现有加速技术众多且组合复杂，缺乏系统化、自动化的方法来为具体模型与场景找到最优加速方案与实现代码。

Method: 1) DiffBench：构建涵盖多种扩散架构、优化组合与部署场景的三阶段自动化评测流水线，实现端到端性能与正确性评估；2) DiffAgent：采用闭环代理，包含规划与调试模块迭代优化代码生成模块产出，并以遗传算法从执行环境中提取性能反馈，指导后续代码与策略改进，从而自动产出最优加速策略与实现。

Result: 实验显示：DiffBench能全面评测生成代码；DiffAgent在生成有效的扩散加速策略方面显著优于现有LLMs，带来更优的性能提升（推理加速）与稳健性。

Conclusion: 基于LLM的自动化框架能有效整合与搜索多种加速技术，为任意扩散模型生成高质量加速代码与策略；DiffBench提供了可靠的评测基础，DiffAgent在实际加速效果上具有显著优势。

Abstract: Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.

</details>


### [60] [AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2601.03191)
*Anees Ur Rehman Hashmi,Numan Saeed,Christoph Lippert*

Main category: cs.CV

TL;DR: AnatomiX 是一个专为胸部 X 光解读而设计的多任务多模态大语言模型，通过显式的解剖学定位与特征提取，显著提升了解剖推理与空间对应，较现有方法在多项“有定位约束”的任务上提升超过25%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医学大模型在胸片上的空间推理与解剖理解薄弱；现有 grounding 技术虽能提升总体表现，却常无法建立真实的解剖对应，导致医学场景中的解剖误判。需要一种面向解剖对齐的框架来弥补这一缺口。

Method: 受放射科工作流启发的两阶段框架：1）检测并识别关键解剖结构，提取其视觉与空间特征；2）利用大语言模型结合这些显式的解剖特征执行多种下游任务（短语定位、报告生成、VQA、图像理解）。模型为多任务多模态训练，强调“解剖对齐”的表示学习。

Result: 在多个基准上取得领先的解剖推理表现；在解剖定位（anatomy grounding）、短语定位、带定位约束的诊断与描述任务上，相比现有方法提升超过25%。

Conclusion: 显式的解剖结构建模与两阶段工作流能显著强化胸片解读中的空间与解剖推理，改善多类下游任务的准确性与可解释性；AnatomiX 提供了通用的、可扩展的解剖对齐范式，并已开源代码与预训练模型。

Abstract: Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix

</details>


### [61] [UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision](https://arxiv.org/abs/2601.03193)
*Ruiyan Han,Zhen Fang,XinYu Sun,Yuchen Ma,Ziheng Wang,Yu Zeng,Zehui Chen,Lin Chen,Wenxuan Huang,Wei-Jie Xu,Yi Cao,Feng Zhao*

Main category: cs.CV

TL;DR: 提出UniCorn自我改进框架，缓解统一多模态模型“理解强、生成弱”的断裂（称为传导性失语）。通过将同一模型分成提议者-求解者-评审者三角色自博弈，并进行认知模式重构，把潜在理解蒸馏为显式生成信号；并引入文本→图像→文本闭环一致性基准UniCycle。实验在六个生成基准上大幅超越基础模型，多个榜单达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型虽能准确理解跨模态输入，但难以将理解转化为高保真、可控的生成，表现为“理解-生成脱耦”。缺少无需外部数据与教师的自监督方式来修补这一断裂并确保多模态一致性。

Method: 单一UMM被拆分为三角色：Proposer生成候选与计划，Solver依据计划执行生成与改写，Judge评估一致性与质量并提供反馈。通过自博弈产生高质量交互数据，并以“认知模式重构”将潜在对齐信号显化为可训练的生成控制信号；训练完全自监督。另提出UniCycle：以文本→图像→文本的闭环一致性作为评测。

Result: 在六个通用图像生成基准全面提升：TIIF 73.8、DPG 86.8、CompBench 88.5达SOTA；WISE提升+5.0，OneIG提升+6.5；UniCycle上表现最佳，同时保持原有理解能力。

Conclusion: UniCorn无需外部数据与教师即可显著提升统一多模态模型的文本到图像生成质量与可控性，并保持理解稳健；闭环一致性评测验证了多模态一致性恢复，展示了全自监督统一多模态智能的可扩展性。

Abstract: While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.

</details>


### [62] [LTX-2: Efficient Joint Audio-Visual Foundation Model](https://arxiv.org/abs/2601.03233)
*Yoav HaCohen,Benny Brazowski,Nisan Chiprut,Yaki Bitterman,Andrew Kvochko,Avishai Berkowitz,Daniel Shalem,Daphna Lifschitz,Dudu Moshe,Eitan Porat,Eitan Richardson,Guy Shiran,Itay Chachy,Jonathan Chetboun,Michael Finkelson,Michael Kupchick,Nir Zabari,Nitzan Guetta,Noa Kotler,Ofir Bibi,Ori Gordon,Poriya Panet,Roi Benita,Shahar Armon,Victor Kulikov,Yaron Inger,Yonatan Shiftan,Zeev Melumian,Zeev Farbman*

Main category: cs.CV

TL;DR: LTX-2 是一个开源、统一生成“视频+音频”的扩散模型，采用双流Transformer（视频14B、音频5B）与双向跨模态注意力和共享时间步条件，实现高质量且时间同步的视听生成，在开源系统中达到了SOTA质量与指令遵循，并接近商用模型但成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频模型缺乏声音，导致语义、情感、氛围线索缺失；同时，独立的音视频生成会带来不同步与控制困难。作者希望构建一个单一基础模型，统一、同步地生成高质量视听内容，提升对复杂提示的理解与对齐，并降低训练/推理成本。

Method: 提出LTX-2：不对称双流Transformer架构，视频流14B、音频流5B，通过双向音视频跨注意力连接；引入时间位置嵌入与跨模态AdaLN实现共享timestep条件；使用多语种文本编码器增强提示理解；设计modality-aware classifier-free guidance（modality-CFG）以改进视听对齐与可控性；更大容量分配给视频以提高效率与质量。

Result: 模型能生成语音之外的丰富、连贯环境与拟音，角色、场景、风格、情绪均能随画面变化同步；在开源体系内达到SOTA的视听质量与提示遵循度，并在更低算力与更短推理时间下接近专有模型的效果；权重与代码均公开。

Conclusion: 统一视听扩散模型在质量、同步性与可控性方面可行且高效；通过不对称双流与modality-CFG等设计，LTX-2在开源范畴达到了领先水平，并以较低成本逼近商用系统，具有实用与研究价值。

Abstract: Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.

</details>


### [63] [A Versatile Multimodal Agent for Multimedia Content Generation](https://arxiv.org/abs/2601.03250)
*Daoan Zhang,Wenlin Yao,Xiaoyang Wang,Yebowen Hu,Jiebo Luo,Dong Yu*

Main category: cs.CV

TL;DR: 提出MultiMedia-Agent，通过工具库、数据生成管线与偏好对齐指标，结合技能习得理论与两阶段相关策略（自相关与模型偏好相关）进行计划优化，并用三阶段训练（基础/成功计划微调+偏好优化）实现端到端多模态内容生成，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC模型多为单点能力，难以在真实场景中端到端处理涉及视频、音频、文本等多模态的复杂创作流程。需要一种能够编排多工具、跨模态协作并与人类偏好对齐的系统化方案。

Method: 构建MultiMedia-Agent，包括：1）数据生成管线；2）内容创作工具库；3）偏好对齐评估指标。引入“技能习得理论”指导数据筛选与训练。提出两阶段计划相关策略：自相关（自检自修）与模型偏好相关（依据偏好反馈重排计划）。训练采用三阶段：基础微调、成功计划微调、偏好优化（类似DPO/对比学习）。利用生成的计划对Agent进行监督与偏好式训练。

Result: 对比实验表明所提相关策略与三阶段训练有效，MultiMedia-Agent在多媒体内容生成质量与偏好匹配度上优于当前代表性模型。

Conclusion: 面向真实应用的多模态创作，基于Agent的编排加上计划优化与偏好对齐训练可实现更好的端到端生成；MultiMedia-Agent验证了该范式的有效性。

Abstract: With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.

</details>


### [64] [InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields](https://arxiv.org/abs/2601.03252)
*Hao Yu,Haotong Lin,Jiawei Wang,Jiaxin Li,Yida Wang,Xueyang Zhang,Yue Wang,Xiaowei Zhou,Ruizhen Hu,Sida Peng*

Main category: cs.CV

TL;DR: InfiniDepth 用隐式神经场表示深度，结合局部隐式解码器，实现可在连续坐标查询的任意分辨率、细节丰富的深度估计，并在合成与真实数据上达SOTA，尤其细节区域更强，还改进大视角新视点合成质量。


<details>
  <summary>Details</summary>
Motivation: 栅格化深度预测受固定像素网格限制，难以无缝扩展到任意输出分辨率，且细节重建受限；需要一种能够在连续图像平面上精确查询深度、提升几何细节与可扩展性的表示。

Method: 提出将深度表示为神经隐式场，用一个简单高效的局部隐式解码器，对任意2D连续坐标进行查询输出深度；并构建一个来自5款游戏、覆盖多样场景与丰富几何/外观细节的高质量4K合成基准，用于系统评估。

Result: 在相对与绝对（metric）深度任务上，InfiniDepth在合成与真实世界基准均达SOTA，尤其在细节区域表现突出；在大视角变化的新视点合成任务中，减少空洞与伪影，生成质量更高。

Conclusion: 用隐式场表示并通过局部隐式解码器进行连续坐标查询，可突破离散网格的分辨率限制，实现任意分辨率、细节更佳的深度估计，并对下游新视点合成具有实际益处。

Abstract: Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.

</details>


### [65] [Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training](https://arxiv.org/abs/2601.03256)
*Hexiao Lu,Xiaokun Sun,Zeyu Cai,Hao Guo,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: Muses 是一种无需训练、端到端前向生成的奇幻3D生物生成方法，以3D骨架为核心，完成结构设计、体素装配与图像引导外观建模，达到高保真且与文本一致的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖部件级优化、手工拼装或2D图像生成，难以处理复杂部件关系与跨域生成，常导致3D资产不真实或不连贯。研究动机是引入更基础且通用的结构表示来稳健地组织与组合复杂形体。

Method: 以3D骨架为中心的结构化流水线：1) 通过图约束推理生成具备连贯布局与尺度的创意骨架；2) 在结构化潜空间中进行体素级装配，将不同对象的区域融合；3) 在骨架条件下进行图像引导的外观建模，生成风格一致的纹理。全流程无需额外训练，采用前向推理。

Result: 在视觉逼真度、与文本描述的对齐度方面达到SOTA，并展示了灵活3D对象编辑的潜力。

Conclusion: 利用骨架这一生物形态的基础表示，可将3D内容生成形式化为“设计-组合-生成”的结构感知流程，显著提升奇幻生物类3D生成的连贯性、可控性与质量。

Abstract: We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [66] [Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis](https://arxiv.org/abs/2601.02409)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh*

Main category: eess.IV

TL;DR: 提出EGxFSL与xGAL双框架：用放射科专家ROI指导可解释的少样本学习，并以“预测不确定性+注意力错配”共同驱动主动样本获取；在多医学影像数据集上显著优于无解释指导的基线，并在低标注场景下以更少样本达更高准确率，且可跨模态泛化。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI受限于标注稀缺与缺乏可解释性，影响临床落地。现有少样本学习缓解数据不足但不透明，主动学习提高标注效率却忽视“为何选这些样本”。需要一个将专家知识嵌入训练、并让可解释性反过来指导样本选择的统一方案。

Method: 1) EGxFSL：在原型网络/原型分类框架上，引入由放射科医生标注的ROI作为空间监督，将Grad-CAM生成的注意力图与ROI通过Dice损失对齐，与分类损失联合优化，使模型在少样本条件下学到“对哪里看”的可解释表征。2) xGAL：主动学习循环中同时考虑预测不确定性与注意力与ROI的错配程度（attention misalignment），优先采集既难判又看错位置的样本；与EGxFSL形成闭环——解释性信号既用于训练也用于样本选择。

Result: 在BraTS MRI、VinDr-CXR、SIIM-COVID-19上取得92%、76%、62%准确率，全面优于无解释指导的基线；在严重数据受限下，仅用680张样本达到76%准确率，随机采样仅57%；Grad-CAM显示模型关注诊断相关区域；在乳腺超声上验证了跨模态泛化。

Conclusion: 用专家引导的可解释监督与可解释性驱动的主动采样相融合，可在标注稀缺条件下同时提升性能与可解释性，并以更低标注成本获得更好结果，具备跨数据集与跨模态的推广潜力。

Abstract: Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.

</details>
