<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 56]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Bi-Orthogonal Factor Decomposition for Vision Transformers](https://arxiv.org/abs/2601.05328)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 提出BFD方法解析ViT注意力在“位置与内容”信息上的交换：先用ANOVA分解token表征为正交的“位置/内容”因子，再对QK^T做SVD找出双正交通信模式。结果显示注意力以内容为主，并存在头与奇异模的功能专化；DINOv2较监督模型更强调内容-位置耦合并在中层实现更好的形状处理。


<details>
  <summary>Details</summary>
Motivation: 现有注意力图只能显示权重集中位置，无法区分注意力在位置与语义内容之间到底交换了什么信息，缺乏可解释的因子层面理解。需要一种原则化工具来分离并量化“位置、内容及其耦合”对注意力通信的贡献。

Method: 提出Bi-orthogonal Factor Decomposition（BFD）：(1) 用ANOVA式统计分解将token激活拆为正交的“位置因子、内容因子及交互”组件，确保因子隔离；(2) 对查询-键交互矩阵QK^T做SVD，得到双正交的通信模，分析各模如何承载位置/内容因素及其耦合能量分布；在多模型（含DINOv2与监督ViT）与多层多头上应用并验证分解正确性。

Result: 三大现象：1) 注意力主要通过内容通道工作：内容-内容能量占主导，其次是内容-位置耦合；DINOv2在内容-位置上分配更多能量且模式谱更丰富。2) 机制专化：注意力头分化为内容-内容、内容-位置、位置-位置三类；单个头内的奇异模式也呈类似专化。3) DINOv2的整体形状感知优势源于中间层：既保留位置结构又语义上下文增益。

Conclusion: BFD提供了一个可解释框架，明确了注意力在位置与语义因子间的通信路径与强度，解释了不同训练范式（如DINOv2）为何在形状与上下文建模上更强，并为设计与诊断ViT注意力头与层的角色提供实证与可操作线索。

Abstract: Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.
  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.

</details>


### [2] [Coding the Visual World: From Image to Simulation Using Vision Language Models](https://arxiv.org/abs/2601.05344)
*Sagi Eppel*

Main category: cs.CV

TL;DR: 论文提出Im2Sim方法：给VLM一张现实系统图像，让其描述并生成可运行的仿真代码，运行后与原图比较，从而评估VLM对系统的理解。结果显示VLM能把握高层系统机制但难以还原细节。


<details>
  <summary>Details</summary>
Motivation: 理解是否等同于能构建可运行的世界心智模型？在视觉领域，若模型能从图像中识别系统与机制并生成能再现其统计与结构特性的仿真，就能更直接检验“理解”。

Method: 给VLM输入现实系统图像（城市、云、植被、物理波等），要求其：1）用自然语言描述系统与关键机制；2）编写生成式仿真代码；3）执行代码生成合成图像；4）与原图比较。跨多领域评测并分析生成模型与图像的一致性。

Result: 领先VLM（GPT、Gemini）能构建多层抽象的系统模型并生成合理仿真，体现对复杂多组分系统的高层理解；但在微观纹理、精细布局与低层模式匹配上能力有限，难以复刻细节。

Conclusion: VLM呈现“高层理解强、低层感知弱”的非对称性：能抓住宏观机制与结构，却难以还原细粒度细节；Im2Sim提供了一种检验与分析VLM视觉系统理解的新框架。

Abstract: The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.

</details>


### [3] [STResNet & STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs](https://arxiv.org/abs/2601.05364)
*Sudhakar Sah,Ravish Kumar*

Main category: cs.CV

TL;DR: 提出STResNet（分类）与STYOLO（检测）两大轻量模型族，在微控器/NPUs等受限设备上同时优化精度、延迟与内存，较同级SOTA（如MobileNetV1、ShuffleNetV2、YOLOv5n、YOLOX-Nano）取得更高精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有轻量网络多以牺牲精度换取低延迟，难以在资源受限硬件上兼顾精度、效率与内存占用，限制了边缘部署的应用范围。

Method: 设计两类面向边缘设备的模型族：用于分类的STResNet（含Nano/Tiny/Milli等变体）与用于检测的STYOLO；在架构设计与训练流程上联合优化精度、计算量与参数规模，并在ImageNet-1K与MS COCO上评测；部分设置中以STResNetMilli作为检测骨干，并结合Ultralytics训练环境。

Result: STResNet系列在≤4M参数预算内取得有竞争力的ImageNet-1K精度：STResNetMilli以约3M参数达70.0% Top-1，优于同等复杂度的MobileNetV1与ShuffleNetV2。检测方面，STYOLOMicro/Milli在COCO上分别达30.5%与33.6% mAP，均在精度与效率上超过YOLOv5n与YOLOX-Nano。

Conclusion: STResNet与STYOLO在资源受限平台上实现更优的精度-效率-内存折中，为边缘端分类与检测提供更高实用性的基础模型；作为骨干接入现有训练框架亦具可行性。

Abstract: Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.

</details>


### [4] [MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments](https://arxiv.org/abs/2601.05368)
*Svitlana Morkva,Maximum Wilder-Smith,Michael Oechsle,Alessio Tonioni,Marco Hutter,Vaishakh Patil*

Main category: cs.CV

TL;DR: MOSAIC-GS 提出一种面向单目视频的显式高保真动态场景重建方法，结合多种几何线索与刚性约束先估计三维运动，再进行基于高斯喷溅的光度优化；将场景拆分为静态/动态并用时变多项-傅里叶曲线编码轨迹，实现更快训练与实时渲染，质量接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目动态重建缺乏多视角约束，导致几何与时间一致性难以恢复；现有方法常依赖外观驱动的运动推断，容易产生歧义且训练/渲染开销大。需要一种能利用多种几何先验、显式建模运动、同时高效训练与实时渲染的方案。

Method: 以高斯喷溅为表征，融合深度、光流、动态分割、点跟踪等几何线索，并施加基于刚性的运动约束，在初始化阶段估计初始三维场景动态；随后进行光度优化。场景分解为静态与动态部分；动态高斯各自携带轨迹，以随时间变化的Poly-Fourier曲线紧凑编码非刚性运动，实现参数高效的运动表示与快速优化/渲染。

Result: 在标准单目动态场景基准上，与现有方法相比实现明显更快的优化与实时渲染速度，同时在重建质量上达到或接近当前最优水平。

Conclusion: 通过多几何线索与显式运动先验的初始化，加上静/动态分解和Poly-Fourier轨迹编码，MOSAIC-GS在保证重建质量的同时显著提升训练与渲染效率，适用于单目动态场景的高效重建。

Abstract: We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.

</details>


### [5] [Ensemble of radiomics and ConvNeXt for breast cancer diagnosis](https://arxiv.org/abs/2601.05373)
*Jorge Alberto Garza-Abdala,Gerardo Alejandro Fumagal-González,Beatriz A. Bosques-Palomo,Mario Alexis Monsivais Molina,Daly Avedano,Servando Cardona-Huerta,José Gerardo Tamez-Pena*

Main category: cs.CV

TL;DR: 比较放射组学、深度学习与集成法在乳腺X线筛查中的癌症检测，集成法AUC最高0.87。


<details>
  <summary>Details</summary>
Motivation: 提升乳腺癌早筛准确性与泛化性，评估不同方法（放射组学、DL、及其组合）在跨数据集上的实际表现。

Method: 使用两个独立数据集：RSNA 2023（11,913例）与TecSalud墨西哥队列（19,400例）。在RSNA上训练ConvNeXtV1-small并在TecSalud上验证；在TecSalud上开发放射组学模型并采用逐年留一验证；采用一致的校准与融合策略构建集成模型，组合DL与放射组学预测。

Result: AUC：集成0.87 > DL(ConvNeXtV1-small) 0.83 > 放射组学0.80；集成在不同数据与验证方案中表现最优且稳定。

Conclusion: 将DL与放射组学进行集成能显著提升乳腺X线筛查的癌症检出性能，优于单一方法。

Abstract: Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.

</details>


### [6] [EdgeLDR: Quaternion Low-Displacement Rank Neural Networks for Edge-Efficient Deep Learning](https://arxiv.org/abs/2601.05379)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

TL;DR: 提出EdgeLDR：把四元数通道耦合与分块循环结构结合，并用复数伴随表示实现FFT高效推理；在CNN/Transformer中验证，获得高压缩比、低延迟与竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 边缘设备受限于稠密线性算子的访存与计算成本。四元数网络虽参数高效但权重仍稠密；结构化矩阵可加速却多在实域。需要一种既利用四元数通道混合、又引入结构化权重、并能高效求值的方案。

Method: 提出EdgeLDR框架：将四元数块循环（block-circulant）线性与卷积层结合；通过复数伴随表示将四元数运算映射为复域，使块循环可用FFT快速乘法；给出参考实现，并比较FFT实现与时域朴素四元数循环乘法；将该层集成入紧凑CNN与Transformer并在多数据集评测。

Result: FFT评估相较朴素实现获得显著加速，且随块大小增大延迟基本稳定，使更大压缩因子可行；在CIFAR-10/100、SVHN及高光谱数据集上，EdgeLDR在显著参数压缩下保持有竞争力的精度，并报告CPU/GPU时延。

Conclusion: EdgeLDR将四元数通道混合与块循环结构有效融合，借助FFT实现实际可用的加速与压缩，在多任务中以较低延迟提供高压缩且精度竞争力的模型。

Abstract: Deploying deep neural networks on edge devices is often limited by the memory traffic and compute cost of dense linear operators. While quaternion neural networks improve parameter efficiency by coupling multiple channels through Hamilton products, they typically retain unstructured dense weights; conversely, structured matrices enable fast computation but are usually applied in the real domain. This paper introduces EdgeLDR, a practical framework for quaternion block-circulant linear and convolutional layers that combines quaternion channel mixing with block-circulant parameter structure and enables FFT-based evaluation through the complex adjoint representation. We present reference implementations of EdgeLDR layers and compare FFT-based computation against a naive spatial-domain realization of quaternion circulant products. FFT evaluation yields large empirical speedups over the naive implementation and keeps latency stable as block size increases, making larger compression factors computationally viable. We further integrate EdgeLDR layers into compact CNN and Transformer backbones and evaluate accuracy-compression trade-offs on 32x32 RGB classification (CIFAR-10/100, SVHN) and hyperspectral image classification (Houston 2013, Pavia University), reporting parameter counts and CPU/GPU latency. The results show that EdgeLDR layers provide significant compression with competitive accuracy.

</details>


### [7] [Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation](https://arxiv.org/abs/2601.05394)
*Yuang Shi,Simone Gasparini,Géraldine Morin,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: 提出将3D高斯分成“素描高斯”（高频边界）与“贴片高斯”（低频平滑），实现分层渐进式流式传输，并通过层次自适应聚类与质量驱动细化在3DGS上泛化到任意场景，显著优于均匀剪枝。


<details>
  <summary>Details</summary>
Motivation: 观察到不同高斯在3D重建中承担类似艺术创作的不同角色：部分强调边缘轮廓（高频），部分覆盖大面积平滑区域（低频）。为支持在受限带宽/资源设备上的高保真渲染，需要一种结构感知、可分层传输且参数高效的表示。

Method: 在3D Gaussian Splatting(3DGS)上提出层次自适应分类：1) 基于多准则密度聚类，将高斯按空间分布、频率/梯度等指标分为素描与贴片两类；2) 引入自适应、质量驱动的细化策略，在保证重建质量的同时优化参数编码；3) 利用这种语义分层实现先传骨架（素描高斯）再逐步细化体积（贴片高斯）的渐进流式传输与渲染。

Result: 在多样场景（人工与自然）中，相同模型大小下较均匀剪枝基线提升：PSNR最高+1.74 dB、SSIM+6.7%、LPIPS降低41.4%；室内场景可在仅保留0.5%的原模型参数量下维持视觉质量。

Conclusion: 结构感知的混合高斯表示与自适应分类框架在保证高保真度的同时显著压缩与加速，实现对带宽受限与资源有限设备友好的存储、流式与渲染方案，优于传统均匀剪枝。

Abstract: We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.
  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.

</details>


### [8] [Multi-task Cross-modal Learning for Chest X-ray Image Retrieval](https://arxiv.org/abs/2601.05399)
*Zhaohui Liang,Sivaramakrishnan Rajaraman,Niccolo Marini,Zhiyun Xue,Sameer Antani*

Main category: cs.CV

TL;DR: 论文提出对BiomedCLIP进行多任务微调，通过加入MLP投影头与复合损失（异常检测BCE、监督式对比损失、CLIP对齐损失）来改进胸片图文检索，在图到文与文到图两向检索上取得更平衡、临床更有意义的表现，并在t-SNE中呈现更清晰的正常/异常语义簇。


<details>
  <summary>Details</summary>
Motivation: 通用的CLIP与BiomedCLIP虽具强跨模态嵌入，但未针对细粒度医疗检索（如以胸片检索临床相关报告）优化，导致对临床关键差异与诊断敏感性不足。

Method: 以BiomedCLIP为骨干，新增轻量MLP投影头，采用多任务复合损失：1) 正常/异常二分类的BCE以捕捉临床重要差异；2) 监督式对比损失增强类内一致性、区分类间边界；3) 维持跨模态对齐的CLIP损失；共同微调以适配CXR检索。

Result: 微调后模型在图到文、文到图检索均优于预训练BiomedCLIP与通用CLIP，且更均衡、更符合临床意义；t-SNE显示正常与异常样本形成更清晰语义聚类，提示诊断敏感性提升。

Conclusion: 面向领域的多任务学习能显著提升生物医学跨模态检索的效果与临床相关性；在CXR场景下，通过分类+对比+对齐的联合优化，可在保持跨模态一致性的同时强化病理判别能力。

Abstract: CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.

</details>


### [9] [Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization](https://arxiv.org/abs/2601.05432)
*Yuxiang Ji,Yong Wang,Ziyu Ma,Yiming Hu,Hailang Huang,Xuecai Hu,Guanhua Chen,Liaoni Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出“带地图思考”的图像地理定位方法：将LVLM变成在地图中的智能体，先用强化学习提升探索-决策，再用并行测试时扩展多路径搜索；并发布真实世界基准MAPBench，显著提升Acc@500m。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM虽有世界知识与推理能力，但忽视人类常用的“看图配地图”策略；同时，地理定位需要在大空间中探索多候选路径，现有方法采样效率与决策质量不足，且缺乏贴近真实分布的基准数据。

Method: 1) 设计“agent-in-the-map”闭环：模型在地图环境中迭代感知—规划—行动—定位，显式使用地图线索。2) 两阶段优化：a) 代理式强化学习训练，增强探索与决策效率；b) 并行测试时扩展（TTS），同时探索多条候选路径后再做最终预测。3) 构建全真实图片的MAPBench用于训练与评测。

Result: 在MAPBench等评测上，方法在多项指标上优于开源和闭源模型；相对带Google Search/Map加持的Gemini-3-Pro，Acc@500m由8.0%提升至22.1%。

Conclusion: 将地图交互融入LVLM并配合RL与并行TTS，能显著提升大尺度地理定位；MAPBench为更真实、更新的评测提供了数据支撑。

Abstract: The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\% to 22.1\% compared to \textit{Gemini-3-Pro} with Google Search/Map grounded mode.

</details>


### [10] [TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection](https://arxiv.org/abs/2601.05446)
*Hongyang Xie,Hongyang He,Victor Sanchez*

Main category: cs.CV

TL;DR: 提出TAPM-Net，通过显式建模小目标引发的特征扰动在空间中的扩散轨迹，提高红外小目标检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有CNN/ViT虽有效，但难以刻画小目标在多层特征空间中引发的方向性、层间扰动传播，这一“轨迹/扩散”线索对区分目标与结构化噪声至关重要。

Method: 提出由两部分组成的TAPM-Net：1) 扰动引导路径模块（PGM）从多层特征构建扰动能量场，提取沿梯度的特征轨迹；2) 轨迹感知状态块（TASB）基于Mamba状态空间单元，沿轨迹进行各向异性、上下文敏感的状态传播，并引入速度约束扩散与词级/句级语义对齐的特征融合，实现低计算量下的全局一致性。

Result: 在NUAA-SIRST与IRSTD-1K数据集上达到SOTA性能（文中宣称优于现有注意力方法，计算成本较低）。

Conclusion: 通过显式建模目标诱发扰动的空间扩散轨迹，实现各向异性传播与语义对齐融合，TAPM-Net在ISTD任务上兼顾精度与效率，优于现有注意力范式。

Abstract: Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.

</details>


### [11] [ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout Transformers in Key Information Extraction](https://arxiv.org/abs/2601.05470)
*Tingwei Xie,Jinxin He,Yonghong Song*

Main category: cs.CV

TL;DR: 提出ROAP管线，通过显式阅读顺序建模与削弱视觉噪声，提升Layout Transformer在VrDU任务上的注意力分配与性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态Transformer在文档理解中缺乏对逻辑阅读顺序的显式建模，且视觉token干扰导致对文本语义的关注被稀释，限制了性能与鲁棒性。

Method: 在不改动预训练主干的前提下，提出轻量、架构无关的ROAP管线：(1) 使用Adaptive-XY-Gap（AXG-Tree）从复杂版面中鲁棒提取层级化阅读序列；(2) 将阅读顺序融入注意力，设计Reading-Order-Aware Relative Position Bias（RO-RPB）；(3) 引入Textual-Token Sub-block Attention Prior（TT-Prior），自适应抑制视觉噪声并强化细粒度文本-文本交互。

Result: 在FUNSD与CORD基准上，ROAP为代表性主干（如LayoutLMv3、GeoLayoutLM）带来一致性能提升。

Conclusion: 显式建模阅读逻辑与调控模态干扰对鲁棒文档理解至关重要；ROAP作为可扩展的无侵入式方案适用于复杂版面分析，并能普适增强现有Layout Transformer。

Abstract: The efficacy of Multimodal Transformers in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout Transformers without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.

</details>


### [12] [Multi-Image Super Resolution Framework for Detection and Analysis of Plant Roots](https://arxiv.org/abs/2601.05482)
*Shubham Agarwal,Ofek Nourian,Michael Sidorov,Sharon Chemweno,Ofer Hadar,Naftali Lazarovitch,Jhonathan E. Ephrath*

Main category: cs.CV

TL;DR: 提出一种用于地下植物根系成像的多视角成像+深度学习多图像超分(MISR)框架，利用空间冗余提升根系可见性与细节；在合成数据集上验证，较SOTA超分略优（BRISQUE降2.3%，CLIP-IQA持平），并提升根毛计数与密度等性状估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 地下根系图像受遮挡、土壤湿度变化、低对比度等影响，传统视觉方法难以获得清晰细节，限制了对根系性状（营养吸收、健康、生态相互作用等）的研究与量化。因此需要一种能在复杂地下环境下增强根系可见性与结构保真的成像与重建方法。

Method: 设计一个获取多重重叠视角的地下成像系统，并提出深度学习的多图像超分辨率（MISR）算法，利用跨视角的空间冗余重建高分辨图像；构建包含关键环境因素（遮挡、湿度、低对比等）的合成数据集用于训练与评估；通过客观无参图像质量指标（BRISQUE、CLIP-IQA）与下游根系性状估计任务进行评测。

Result: 在合成数据上，方法优于现有超分基线：BRISQUE降低2.3%，在CLIP-IQA上保持不变；重建图像具有更好的结构保真和可视清晰度，从而改进根毛计数与根毛密度等性状的估计效果。

Conclusion: 多视角地下成像结合MISR能在挑战性土壤条件下提升根系图像质量并促进性状量化，为农业与生态研究中的自动化根系成像与表型分析提供了可行方向。

Abstract: Understanding plant root systems is critical for advancing research in soil-plant interactions, nutrient uptake, and overall plant health. However, accurate imaging of roots in subterranean environments remains a persistent challenge due to adverse conditions such as occlusion, varying soil moisture, and inherently low contrast, which limit the effectiveness of conventional vision-based approaches. In this work, we propose a novel underground imaging system that captures multiple overlapping views of plant roots and integrates a deep learning-based Multi-Image Super Resolution (MISR) framework designed to enhance root visibility and detail. To train and evaluate our approach, we construct a synthetic dataset that simulates realistic underground imaging scenarios, incorporating key environmental factors that affect image quality. Our proposed MISR algorithm leverages spatial redundancy across views to reconstruct high-resolution images with improved structural fidelity and visual clarity. Quantitative evaluations show that our approach outperforms state-of-the-art super resolution baselines, achieving a 2.3 percent reduction in BRISQUE, indicating improved image quality with the same CLIP-IQA score, thereby enabling enhanced phenotypic analysis of root systems. This, in turn, facilitates accurate estimation of critical root traits, including root hair count and root hair density. The proposed framework presents a promising direction for robust automatic underground plant root imaging and trait quantification for agricultural and ecological research.

</details>


### [13] [Hippocampal Atrophy Patterns Across the Alzheimer's Disease Spectrum: A Voxel-Based Morphometry Analysis](https://arxiv.org/abs/2601.05494)
*Trishna Niraula*

Main category: cs.CV

TL;DR: 本研究使用CAT12/SPM12的体素形态学分析，对ADNI基线T1MRI（CN=90，MCI=129，AD=30）发现AD组相较CN与MCI有显著海马萎缩（效应量d=2.03与1.61），海马体积对MCI转AD的预测中等（AUC=0.66），APOE4分层未见显著横断面影响。结果强调内侧颞叶退变是AD进展关键特征。


<details>
  <summary>Details</summary>
Motivation: AD与MCI以内侧颞叶灰质丢失为特征，但需量化不同诊断阶段的萎缩差异、评估海马体积的预测价值，并探查APOE4对结构表型的影响，以为早期诊断与风险分层提供生物标志物证据。

Method: 纳入ADNI基线T1加权MRI共249例，采用CAT12/SPM12的VBM流程。以诊断分组为主要自变量，年龄与颅内容积为协变量，GLM检验全脑灰质体积差异。阈值设定为体素水平p<0.001，簇水平FWE校正p<0.05。比较组间海马体积差异，计算Cohen's d；以海马体积预测MCI转AD，报告AUC；按APOE4携带状态分层分析。

Result: AD相较CN与MCI呈显著海马萎缩（d=2.03与1.61）；海马体积对MCI转AD具有中等预测力（AUC=0.66）；APOE4分层未见对横断面海马体积的显著主效应；整体图谱支持内侧颞叶优先受累。

Conclusion: 海马萎缩是AD进展的显著影像学标志，可作为MCI转归风险评估的中等效力生物标志物；APOE4对横断面海马体积影响不明显。需在更大样本与纵向设计中检验其预测增益并与多模态标志物整合。

Abstract: Alzheimer's disease (AD) and mild cognitive impairment (MCI) are associated with progressive gray matter loss, particularly in medial temporal structures. In this study, CAT12/SPM12 voxel-based morphometry was applied to baseline T1-weighted MRI scans from 249 ADNI participants (CN = 90, MCI = 129, AD = 30). Gray matter volume was analyzed using a general linear model, with the diagnostic group as primary predictor and age and total intracranial volume as covariates. Statistical maps were thresholded at p < 0.001 (voxelwise) and corrected for multiple comparisons at the cluster level using family-wise error (FWE) correction (p < 0.05). Significant hippocampal atrophy was observed in AD relative to CN and MCI (Cohen's d = 2.03 and 1.61, respectively). Hippocampal volume demonstrated moderate predictive value for conversion from MCI to AD (AUC = 0.66). Stratification by APOE4 status did not reveal significant genetic effects on cross-sectional hippocampal volume. These results support medial temporal degeneration as a key feature of AD progression and provide insights into predictive biomarkers and genetic influences.

</details>


### [14] [MMViR: A Multi-Modal and Multi-Granularity Representation for Long-range Video Understanding](https://arxiv.org/abs/2601.05495)
*Zizhong Li,Haopeng Zhang,Jiawei Zhang*

Main category: cs.CV

TL;DR: MMViR提出一种多模态、多粒度的长视频结构化表示：通过关键转折点分段，并构建三层描述以结合全局叙事与细粒度视觉细节，实现高效检索与理解。在长视频问答、摘要与检索上显著优于现有方法，同时将延迟降低至原来的45.4%。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM难以处理分钟到小时级长视频：直接编码成本高，纯视频转文本会冗余或碎片化，难以捕捉长程依赖与复杂事件结构。需要一种既高效又结构化的表示来支持检索与下游任务。

Method: 提出MMViR：1) 检测视频关键转折点进行分段；2) 构建三层结构化表示，将全局叙事层与细粒度视觉细节层耦合；3) 设计基于查询的高效检索机制。该表示面向多模态输入，适配多场景。

Result: 在长视频问答、摘要和检索三类任务上，MMViR优于最强基线；在小时级视频理解上提升19.67%，处理延迟降至45.4%。

Conclusion: 多粒度结构化表示能够在保证细节与全局一致性的同时，显著提升长视频理解的准确性与效率，并具备良好的跨场景泛化能力。

Abstract: Long videos, ranging from minutes to hours, present significant challenges for current Multi-modal Large Language Models (MLLMs) due to their complex events, diverse scenes, and long-range dependencies. Direct encoding of such videos is computationally too expensive, while simple video-to-text conversion often results in redundant or fragmented content. To address these limitations, we introduce MMViR, a novel multi-modal, multi-grained structured representation for long video understanding. MMViR identifies key turning points to segment the video and constructs a three-level description that couples global narratives with fine-grained visual details. This design supports efficient query-based retrieval and generalizes well across various scenarios. Extensive evaluations across three tasks, including QA, summarization, and retrieval, show that MMViR outperforms the prior strongest method, achieving a 19.67% improvement in hour-long video understanding while reducing processing latency to 45.4% of the original.

</details>


### [15] [Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification](https://arxiv.org/abs/2601.05498)
*Samuel E. Johnny,Bernes L. Atabonfack,Israel Alagbe,Assane Gueye*

Main category: cs.CV

TL;DR: 提出一个基于SAM视觉编码器特征的多任务网络，同时进行乳腺超声病灶分割与良恶性分类，采用无提示、全监督适配，并用掩膜引导注意力提升分类，实验在PRECISE 2025数据集上取得DSC 0.887与92.3%准确率，位列榜单前列。


<details>
  <summary>Details</summary>
Motivation: BUS图像对比度低、斑点噪声重、病灶形态多样，导致精准分割与诊断困难。现有SAM多依赖提示、迁移到医疗场景受限；同时，分割与分类往往分开做，难以互相促进。作者希望利用SAM强表征并通过分割引导提升分类鲁棒性。

Method: 采用SAM视觉编码器提取高维特征，进行无提示、全监督适配。分割分支：用轻量卷积头或UNet式解码器进行像素级分割。分类分支：引入掩膜引导注意力（由分割掩膜提供关注区域），抑制背景伪影并强调病灶相关特征。多任务联合训练。

Result: 在PRECISE 2025乳腺超声数据集上（按类别8:2划分训练/测试），方法取得分割DSC=0.887、分类准确率=92.3%，在PRECISE挑战榜单中名列前茅。

Conclusion: SAM表征结合分割引导学习可显著提升BUS中的病灶勾画与诊断预测。无提示、全监督的适配方式有效，掩膜引导注意力有助于抑制背景噪声并提升分类性能。

Abstract: Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.

</details>


### [16] [Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors](https://arxiv.org/abs/2601.05508)
*Fuwen Luo,Zihao Wan,Ziyue Wang,Yaluo Liu,Pau Tong Lin Xu,Xuanjia Qiao,Xiaolong Wang,Peng Li,Yang Liu*

Main category: cs.CV

TL;DR: 提出HieroSA：让多模态大模型从字符位图自动推断笔画级结构，转为可解释的线段表示，跨语言泛化、无需手工先验与数据，助力表意文字结构与语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs把字符当token，MLLMs把字符当像素网格，均忽视笔画与构形逻辑；现有结构分析方法脚本依赖强、人工成本高，需要一种通用、自动、可泛化的笔画结构提取框架。

Method: 设计HieroSA框架：将现代表意文字与古代象形文字的位图转换为归一化坐标系中的显式线段（笔画）表示；在MLLM中嵌入该表示，实现无需手工标注的数据驱动结构推断；以通用可解释的线段图替代像素/字符token输入，支持跨语种迁移。

Result: 大量实验显示，HieroSA能有效捕获字符内部结构与语义，无需语言特定先验；在多种脚本与任务上获得稳健表现，优于仅用像素或文本token的基线。

Conclusion: HieroSA为表意文字笔画级结构建模提供通用可解释方案，可作为音位学/字位学（graphematics）分析工具，推动对古今象形/表意文字的更深理解；代码已开源。

Abstract: Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and Multimodal LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It transforms modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.

</details>


### [17] [GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting](https://arxiv.org/abs/2601.05511)
*Xuan Cheng,Jiahao Rao,Chengyang Li,Wenhao Wang,Weilin Chen,Lvqing Yang*

Main category: cs.CV

TL;DR: GaussianSwap提出用3D高斯点渲(3D Gaussian Splatting)驱动的面部头像来做视频换脸：从目标视频重建可控制的3D头像，再将源图像的身份特征迁移到该头像，并渲回视频，获得高保真、可交互、时序一致的换脸结果。


<details>
  <summary>Details</summary>
Motivation: 传统视频换脸多在像素域直接生成，结果只是无结构的像素，缺乏可编辑性与动画控制，且时序与身份保持受限。需要一种既能高质量换脸又能得到可控制3D表示的方法，以支持交互与稳定的视频生成。

Method: 1) 预处理目标视频：提取FLAME参数、相机位姿与分割掩码；2) 将3D高斯点云与FLAME骨架/表情参数绑定，跨帧驱动实现可控动态表情；3) 身份保持：构建由三种SOTA人脸识别模型融合的复合身份嵌入，用于头像微调；4) 渲染：把换好身份的头像渲到背景帧，生成换脸视频。

Result: 实验表明在身份保持、视觉清晰度与时序一致性上优于现有方法，并实现以往难以实现的交互式应用（如可编辑、动画控制的换脸头像）。

Conclusion: 将换脸从像素视频生成转向可控3D头像生成，实现高保真、稳定且可交互的换脸；证明3D高斯点渲结合FLAME与复合身份嵌入是有效路径。

Abstract: We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar. Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats. The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation. Our work introduces a paradigm shift from conventional pixel-based video generation to the creation of high-fidelity avatar with swapped faces. The framework first preprocesses target video to extract FLAME parameters, camera poses and segmentation masks, and then rigs 3D Gaussian splats to the FLAME model across frames, enabling dynamic facial control. To ensure identity preserving, we propose an compound identity embedding constructed from three state-of-the-art face recognition models for avatar finetuning. Finally, we render the face-swapped avatar on the background frames to obtain the face-swapped video. Experimental results demonstrate that GaussianSwap achieves superior identity preservation, visual clarity and temporal consistency, while enabling previously unattainable interactive applications.

</details>


### [18] [SAS-VPReID: A Scale-Adaptive Framework with Shape Priors for Video-based Person Re-Identification at Extreme Far Distances](https://arxiv.org/abs/2601.05535)
*Qiwei Yang,Pingping Zhang,Yuhao Wang,Zijing Gong*

Main category: cs.CV

TL;DR: 提出SAS-VPReID：在极远距离、低分辨率与视角变化下的视频行人重识别框架，结合记忆增强视觉骨干、多粒度时序建模与先验正则的形态动态，显著提升判别力并在VReID-XFD夺冠。


<details>
  <summary>Details</summary>
Motivation: 极远距离视频行人重识别受到分辨率极低、视角剧变与外观噪声影响，现有方法难以有效利用细粒度外观与时序结构信息，缺乏对跨尺度运动线索与人体形态结构动态的鲁棒建模。

Method: 构建SAS-VPReID包含三模块：1) 记忆增强视觉骨干（MEVB）：以CLIP视觉编码器为基底，引入多代理记忆库以增强判别特征；2) 多粒度时序建模（MGTM）：在多时间粒度上构建序列并自适应强调不同尺度的运动线索；3) 先验正则的形态动态（PRSD）：引入人体形状/骨架先验，对时序中的身体结构变化进行建模与正则化以抑制噪声。

Result: 在VReID-XFD基准上逐模块消融验证其有效性，完整框架在挑战榜单排名第一。

Conclusion: 通过结合记忆增强特征提取、跨尺度时序建模与形态先验约束，SAS-VPReID在极远距离场景显著提升视频行人重识别性能并取得SOTA表现。

Abstract: Video-based Person Re-IDentification (VPReID) aims to retrieve the same person from videos captured by non-overlapping cameras. At extreme far distances, VPReID is highly challenging due to severe resolution degradation, drastic viewpoint variation and inevitable appearance noise. To address these issues, we propose a Scale-Adaptive framework with Shape Priors for VPReID, named SAS-VPReID. The framework is built upon three complementary modules. First, we deploy a Memory-Enhanced Visual Backbone (MEVB) to extract discriminative feature representations, which leverages the CLIP vision encoder and multi-proxy memory. Second, we propose a Multi-Granularity Temporal Modeling (MGTM) to construct sequences at multiple temporal granularities and adaptively emphasize motion cues across scales. Third, we incorporate Prior-Regularized Shape Dynamics (PRSD) to capture body structure dynamics. With these modules, our framework can obtain more discriminative feature representations. Experiments on the VReID-XFD benchmark demonstrate the effectiveness of each module and our final framework ranks the first on the VReID-XFD challenge leaderboard. The source code is available at https://github.com/YangQiWei3/SAS-VPReID.

</details>


### [19] [DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion](https://arxiv.org/abs/2601.05538)
*Yiming Sun,Zifan Ye,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出DIFF-MF：一种差异驱动的通道-空间状态空间模型，用于红外-可见等多模态图像融合；通过差异图引导特征提取，并在通道与空间两维做跨模态交互与融合，实现线性复杂度下的全局依赖建模，实验在驾驶与低空无人机数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于状态空间模型的融合方法虽然高效，但常出现两类偏差：要么过度突出红外强度导致可见细节丢失，要么保留可见结构却削弱热目标显著性。需要一种同时兼顾红外目标与可见细节的高效融合框架。

Method: 核心是“差异驱动”的通道-空间双重交换机制：1) 先计算跨模态特征差异图以指导后续特征提取与融合；2) 通道维度：采用跨注意力的双状态空间建模（channel-exchange），实现通道交互与自适应重加权；3) 空间维度：用跨模态状态空间扫描（spatial-exchange）进行全面空间融合；4) 通过状态空间框架捕获全局依赖，同时保持线性计算复杂度。

Result: 在驾驶场景与低空无人机数据集上，所提方法在视觉质量和定量指标上均优于现有方法（未给出具体数值，但声称全面领先）。

Conclusion: 利用差异图引导并结合通道与空间两维的跨模态状态空间融合，可在复杂场景中高效整合互补信息，兼顾红外目标显著性与可见结构细节，达到优于现有方法的融合效果。

Abstract: Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.

</details>


### [20] [MoGen: A Unified Collaborative Framework for Controllable Multi-Object Image Generation](https://arxiv.org/abs/2601.05546)
*Yanfeng Li,Yue Sun,Keren Fu,Sio-Kei Im,Xiaoming Liu,Guangtao Zhai,Xiaohong Liu,Tao Tan*

Main category: cs.CV

TL;DR: MoGen提出一种无需刚性外部控制信号即可实现多目标文本生成图像的框架，通过语义-区域对齐与自适应多模态引导，提升数量一致性与细粒度可控性，并在质量与灵活性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多目标图像生成难以将语言描述中的局部语义与图像区域精确对齐，造成目标数量不一致、属性串扰；主流方法依赖外部控制（布局图、分割图等），输入刚性、资源要求高，限制用户可用性与约束灵活性。

Method: 提出MoGen，包括两大核心模块：1) RSA（Regional Semantic Anchor）在生成过程中将文本中的短语单元精确锚定到对应图像区域，保证多目标数量与语义一致；2) AMG（Adaptive Multi-modal Guidance）自适应解析并融合多源控制信号（文本、布局、草图、分割、参考图等）的任意组合，形成结构化意图，选择性约束场景布局与对象属性，实现动态细粒度控制。

Result: 在多对象生成基准上，MoGen在图像质量、目标数量一致性与属性控制的细粒度上显著优于现有方法；同时在可访问性与控制灵活性方面表现更佳。代码已开源。

Conclusion: 通过RSA实现短语-区域精确对齐、通过AMG实现对多源控制信号的自适应融合，MoGen在保持高生成质量的同时实现数量可控与细粒度属性控制，兼顾易用性与灵活性，优于主流依赖强外部控制的方案。

Abstract: Existing multi-object image generation methods face difficulties in achieving precise alignment between localized image generation regions and their corresponding semantics based on language descriptions, frequently resulting in inconsistent object quantities and attribute aliasing. To mitigate this limitation, mainstream approaches typically rely on external control signals to explicitly constrain the spatial layout, local semantic and visual attributes of images. However, this strong dependency makes the input format rigid, rendering it incompatible with the heterogeneous resource conditions of users and diverse constraint requirements. To address these challenges, we propose MoGen, a user-friendly multi-object image generation method. First, we design a Regional Semantic Anchor (RSA) module that precisely anchors phrase units in language descriptions to their corresponding image regions during the generation process, enabling text-to-image generation that follows quantity specifications for multiple objects. Building upon this foundation, we further introduce an Adaptive Multi-modal Guidance (AMG) module, which adaptively parses and integrates various combinations of multi-source control signals to formulate corresponding structured intent. This intent subsequently guides selective constraints on scene layouts and object attributes, achieving dynamic fine-grained control. Experimental results demonstrate that MoGen significantly outperforms existing methods in generation quality, quantity consistency, and fine-grained control, while exhibiting superior accessibility and control flexibility. Code is available at: https://github.com/Tear-kitty/MoGen/tree/master.

</details>


### [21] [VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck](https://arxiv.org/abs/2601.05547)
*Feiran Zhang,Yixin Wu,Zhenghua Wang,Xiaohua Wang,Changze Lv,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.CV

TL;DR: 提出VIB-Probe：利用变分信息瓶颈从VLM内部注意力中提取与真实性相关信号，既能检测也能缓解幻觉，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态任务上表现强，但易产生与图像不符的语言幻觉。现有检测多依赖输出logits或外部校验，忽视内部机制，难以定位因果来源并在推理时干预。作者希望从模型内部注意力层面找到与“真实生成”最相关的信号，并在不引入外部工具的情况下同时实现检测与缓解。

Method: 基于VIB理论设计VIB-Probe：对跨层跨注意力头的内部表示进行编码，通过信息瓶颈压缩去除语义噪声与句法纠缠，只保留与是否幻觉相关的判别信息；利用探针的梯度/归因识别对幻觉具有强因果影响的注意力头；在推理时对这些头进行干预（如重加权、抑制或引导），以达到缓解幻觉的目的。

Result: 在多个多样化基准上，VIB-Probe在幻觉检测与缓解两个任务上均显著超越现有基线（未给出具体数字，但声称在广泛实验中稳健领先）。

Conclusion: 内部注意力头蕴含与真实性相关的关键信号。通过VIB压缩可从高维纠缠表示中提取稳健特征，并据此进行因果定位与推理时干预，从而有效降低VLM幻觉；方法通用且代码将开源。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal tasks, but remain susceptible to hallucinations, where generated text deviates from the underlying visual content. Existing hallucination detection methods primarily rely on output logits or external verification tools, often overlooking their internal mechanisms. In this work, we investigate the outputs of internal attention heads, postulating that specific heads carry the primary signals for truthful generation.However, directly probing these high-dimensional states is challenging due to the entanglement of visual-linguistic syntax and noise. To address this, we propose VIB-Probe, a novel hallucination detection and mitigation framework leveraging the Variational Information Bottleneck (VIB) theory. Our method extracts discriminative patterns across layers and heads while filtering out semantic nuisances through the information bottleneck principle. Furthermore, by leveraging the gradients of our VIB probe, we identify attention heads with strong causal influence on hallucinations and introduce an inference-time intervention strategy for hallucination mitigation. Extensive experiments across diverse benchmarks demonstrate that VIB-Probe significantly outperforms existing baselines in both settings. Our code will be made publicly available.

</details>


### [22] [One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection](https://arxiv.org/abs/2601.05552)
*Bin-Bin Gao,Chengjie Wang*

Main category: cs.CV

TL;DR: UniADet提出一种极其简单的通用视觉异常检测框架，通过完全去耦分类与分割、以及跨层特征，去除语言编码器和复杂适配/训练，参数仅0.002M，能灵活适配多种基础模型，在14个工业与医疗基准上零/小样本显著超越SOTA，甚至首次超过全监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的异常检测方法依赖繁琐的提示工程、适配模块和训练策略，限制了通用性与部署简易性；作者重审VL模型在AD中的本质作用，质疑语言编码器在通用AD中的必要性，寻求一个更简单、可泛化且高效的统一方案。

Method: 1) 机制观察：语言编码器主要用于生成异常分类/分割的决策权重，但对通用AD并非必要。2) 去耦设计：完全去耦分类与分割两任务，并对跨层（层级）特征分别学习独立权重，实现任务与层级双重去耦。3) 轻量参数化：仅学习少量独立权重（约0.002M），适配多种视觉基础模型，无需复杂训练或微调。

Result: 在14个真实世界AD数据集（含工业与医疗）上，UniADet在零样本与小样本设置下大幅超过现有SOTA，并且首次在多基准上达到或超过全监督（full-shot）方法的性能。

Conclusion: 去除语言编码器、采用任务与层级完全去耦的极简权重学习即可实现强大的通用异常检测能力；该方案简单、通用、参数高效，并在多领域基准上实现显著领先，展示了以更少假设和更低复杂度解决通用AD的可行性。

Abstract: Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.

</details>


### [23] [Semi-Supervised Facial Expression Recognition based on Dynamic Threshold and Negative Learning](https://arxiv.org/abs/2601.05556)
*Zhongpeng Cai,Jun Yu,Wei Xu,Tianyu Liu,Jianqing Sun,Jiaen Liang*

Main category: cs.CV

TL;DR: 提出一种结合动态阈值调整（DTA）与选择性负学习（SNL）的半监督人脸表情识别方法，通过局部注意力增强与特征图随机丢弃提升鲁棒表征，利用动态阈值适配伪标签置信度，并从低置信未标注样本的互补标签中挖掘信息，最终在RAF-DB与AffectNet上达SOTA，少用标注仍优于全监督。


<details>
  <summary>Details</summary>
Motivation: 表情识别标注成本高、标注规模受限，导致监督方法泛化不足；现有半监督方法易受伪标签噪声与过度自信影响，且对局部面部区域的依赖可能导致过拟合，需要一种既能稳健利用未标注数据、又能抑制局部过拟合和噪声伪标签的算法。

Method: 1) 特征提取阶段引入局部注意力增强与特征图随机丢弃，突出局部判别区域同时防止对单一区域过拟合。2) 动态阈值调整（DTA）：根据训练过程中的分布与不确定性自适应设定伪标签置信阈值，决定未标注样本的正学习纳入与否。3) 选择性负学习（SNL）：对低置信未标注样本，不采用正伪标签，而从互补标签（非目标类别）中学习，选择性施加“排除”约束以挖掘有用信息并降低噪声影响。

Result: 在RAF-DB与AffectNet两大基准上取得SOTA；即使未使用全部训练数据，也优于对应的全监督方法，显示出更高的数据效率与鲁棒性。

Conclusion: 结合DTA与SNL的半监督框架能有效利用未标注样本、缓解伪标签噪声与局部过拟合问题，从而在表情识别上实现高精度与数据效率；该策略有望推广至其他噪声敏感的视觉分类任务。

Abstract: Facial expression recognition is a key task in human-computer interaction and affective computing. However, acquiring a large amount of labeled facial expression data is often costly. Therefore, it is particularly important to design a semi-supervised facial expression recognition algorithm that makes full use of both labeled and unlabeled data. In this paper, we propose a semi-supervised facial expression recognition algorithm based on Dynamic Threshold Adjustment (DTA) and Selective Negative Learning (SNL). Initially, we designed strategies for local attention enhancement and random dropout of feature maps during feature extraction, which strengthen the representation of local features while ensuring the model does not overfit to any specific local area. Furthermore, this study introduces a dynamic thresholding method to adapt to the requirements of the semi-supervised learning framework for facial expression recognition tasks, and through a selective negative learning strategy, it fully utilizes unlabeled samples with low confidence by mining useful expression information from complementary labels, achieving impressive results. We have achieved state-of-the-art performance on the RAF-DB and AffectNet datasets. Our method surpasses fully supervised methods even without using the entire dataset, which proves the effectiveness of our approach.

</details>


### [24] [What's Left Unsaid? Detecting and Correcting Misleading Omissions in Multimodal News Previews](https://arxiv.org/abs/2601.05563)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Dayang Li,Herun Wan,Wei Zhou,Min-Yen Kan*

Main category: cs.CV

TL;DR: 论文提出MM-Misleading基准与OMGuard方法，专注于检测和纠正社交媒体新闻预览（图像+标题）通过“省略关键信息”造成的误导。新基准揭示开源多模态大模型对这种“遗漏型误导”有明显盲区；OMGuard将解释感知微调与基于理由的纠偏结合，使8B模型检测性能追平235B，并在端到端改写上显著更强；分析显示误导多源于局部叙事缺口，需要视觉介入场景下的纠偏。


<details>
  <summary>Details</summary>
Motivation: 社交媒体预览虽事实正确，但通过省略上下文引发理解漂移，导致读者判断偏离全文。此类隐蔽危害较难发现、研究不足，缺少针对性的评测与方法。

Method: 提出一条多阶段管线，分离并模拟“预览理解”与“上下文理解”，据此构建MM-Misleading基准；系统评测开源LVLM检测遗漏型误导的能力。提出OMGuard：1) 解释感知微调（提升误导检测）；2) 基于理由的误导内容纠正（以显式推理引导标题改写），并考虑图像与文本的联动。

Result: 基准实验显示开源LVLM对遗漏型误导存在明显盲点。OMGuard使8B模型在检测准确率上达到235B LVLM水平，并在端到端纠正上显著优于基线。

Conclusion: 遗漏型误导主要来自局部叙事缺失（如缺背景）而非全局框架改变；在图像驱动场景中，仅文本纠正无效，需引入视觉层面的干预。OMGuard与新基准为识别与纠正此类隐蔽误导提供有效路径。

Abstract: Even when factually correct, social-media news previews (image-headline pairs) can induce interpretation drift: by selectively omitting crucial context, they lead readers to form judgments that diverge from what the full article conveys. This covert harm is harder to detect than explicit misinformation yet remains underexplored. To address this gap, we develop a multi-stage pipeline that disentangles and simulates preview-based versus context-based understanding, enabling construction of the MM-Misleading benchmark. Using this benchmark, we systematically evaluate open-source LVLMs and uncover pronounced blind spots to omission-based misleadingness detection. We further propose OMGuard, which integrates (1) Interpretation-Aware Fine-Tuning, which used to improve multimodal misleadingness detection and (2) Rationale-Guided Misleading Content Correction, which uses explicit rationales to guide headline rewriting and reduce misleading impressions. Experiments show that OMGuard lifts an 8B model's detection accuracy to match a 235B LVLM and delivers markedly stronger end-to-end correction. Further analysis reveals that misleadingness typically stems from local narrative shifts (e.g., missing background) rather than global frame changes, and identifies image-driven scenarios where text-only correction fails, highlighting the necessity of visual interventions.

</details>


### [25] [Towards Generalized Multi-Image Editing for Unified Multimodal Models](https://arxiv.org/abs/2601.05572)
*Pengcheng Xu,Peng Tang,Donghao Luo,Xiaobin Hu,Weichu Cui,Qingdong He,Zhennan Chen,Jiangning Zhang,Charles Ling,Boyu Wang*

Main category: cs.CV

TL;DR: 提出一种可扩展的多图像编辑框架，解决UMMs在多参考图像间的身份区分与细节一致性问题，通过可学习的潜在分隔符与正弦索引编码显式标注图片身份，并构建高保真逆向数据集做训练评测，显著提升语义一致性、视觉保真与跨图整合能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在处理多张参考图像时，难以保持视觉一致、清楚区分不同图片的细节并进行跨图参考，尤其在输入数量可变时泛化能力不足。

Method: 1) 可学习的潜在分隔符：在潜空间为每张参考图像注入可学习分隔标记，使条件信号解耦、避免混淆。2) 正弦索引编码：为同一图像的视觉token分配连续的正弦索引嵌入，显式提供图像身份并可对可变数量输入进行泛化与外推。3) 构建高保真基准：采用“逆向数据集构造”保证无伪影、可实现的目标，用于训练与评测。

Result: 在多图像编辑任务上，相较以往基线在语义一致性、视觉保真度和跨图整合上均有明显提升，表现出更好的身份区分与可变输入数的泛化能力。

Conclusion: 显式身份建模（潜在分隔符+正弦索引编码）与高保真数据基准相结合，可有效提升UMMs的多图像编辑与跨图一致性与泛化，验证了方法的有效性与可扩展性。

Abstract: Unified Multimodal Models (UMMs) integrate multimodal understanding and generation, yet they are limited to maintaining visual consistency and disambiguating visual cues when referencing details across multiple input images. In this work, we propose a scalable multi-image editing framework for UMMs that explicitly distinguishes image identities and generalizes to variable input counts. Algorithmically, we introduce two innovations: 1) The learnable latent separators explicitly differentiate each reference image in the latent space, enabling accurate and disentangled conditioning. 2) The sinusoidal index encoding assigns visual tokens from the same image a continuous sinusoidal index embedding, which provides explicit image identity while allowing generalization and extrapolation on a variable number of inputs. To facilitate training and evaluation, we establish a high-fidelity benchmark using an inverse dataset construction methodology to guarantee artifact-free, achievable outputs. Experiments show clear improvements in semantic consistency, visual fidelity, and cross-image integration over prior baselines on diverse multi-image editing tasks, validating our advantages on consistency and generalization ability.

</details>


### [26] [Orient Anything V2: Unifying Orientation and Rotation Understanding](https://arxiv.org/abs/2601.05573)
*Zehan Wang,Ziang Zhang,Jiayang Xu,Jialei Wang,Tianyu Pang,Chao Du,HengShuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: Orient Anything V2 是一个统一处理单/双图像中物体三维朝向与相对旋转的基础模型，支持旋转对称物体与相对旋转估计，并在多项基准上零样本达SOTA。


<details>
  <summary>Details</summary>
Motivation: V1 仅以“唯一前脸”定义朝向，难以覆盖多样旋转对称物体与相对旋转需求；实际应用（如6DoF位姿与对称识别）需要能刻画多前脸/周期性解与多帧相对旋转的模型与数据。

Method: 四项关键创新：1) 通过生成式模型合成可扩展3D资产，保证大覆盖与均衡分布；2) 模型闭环标注系统，为每个物体鲁棒识别0~N个有效前脸；3) 引入对称感知的周期分布拟合目标，覆盖所有合理前向，显式建模旋转对称性；4) 多帧架构直接预测相对旋转。

Result: 在11个常用基准上实现零样本SOTA，覆盖朝向估计、6DoF位姿估计和物体对称识别；展现强泛化能力。

Conclusion: V2 有效解决旋转对称与相对旋转建模难题，显著提升零样本与跨任务泛化，拓宽朝向估计在下游中的适用性。

Abstract: This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.

</details>


### [27] [Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection](https://arxiv.org/abs/2601.05580)
*Hanyi Wang,Jun Lan,Yaoyu Kang,Huijia Zhu,Weiqiang Wang,Zhuosheng Zhang,Shilin Wang*

Main category: cs.CV

TL;DR: 提出一个三阶段“域持续学习”框架，针对不断演化的生成模型，实现检测器的持续适配与泛化；离线可迁移检测+在线小样本持续学习（增强链+K-FAC防遗忘）+线性模式连通插值，构建27模型时间序列基准，实验显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: AI生成图像被恶意滥用，检测方法对未知生成模型泛化差，生成技术快速迭代导致检测器易失效，现实应用需要具备持续适配与抗遗忘能力。

Method: 三阶段框架：1) 离线阶段采用参数高效微调，训练具备强泛化的可迁移检测器；2) 持续学习阶段将未知模型数据流纳入训练，设计“复杂度递增”的数据增强链以小样本高效学习，并用K-FAC近似Hessian来正则化，缓解灾难性遗忘；3) 融合阶段通过基于线性模式连通性的线性插值，在不同生成模型间捕捉共有结构，进一步提升鲁棒性。另构建涵盖GAN、Deepfake、扩散模型的27模型按时间顺序的真实场景基准。

Result: 离线检测器在mAP上较最佳基线提升+5.51%；持续学习策略平均准确率92.20%，整体优于现有最先进方法。

Conclusion: 该三阶段域持续学习框架能够在生成模型持续演化的现实环境中保持高效、稳定的检测性能，兼具小样本适应、抗遗忘与跨模型泛化能力。

Abstract: The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.

</details>


### [28] [GS-DMSR: Dynamic Sensitive Multi-scale Manifold Enhancement for Accelerated High-Quality 3D Gaussian Splatting](https://arxiv.org/abs/2601.05584)
*Nengbo Lu,Minghua Pan,Shaohua Sun,Yizhou Liang*

Main category: cs.CV

TL;DR: 提出GS-DMSR用于3D动态场景重建，通过自适应梯度聚焦与多尺度流形增强，在复杂形变场景中兼顾收敛速度与渲染质量，达至最高96 FPS并降低存储与训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有3D动态场景重建在复杂运动/形变条件下难以同时实现快速收敛与高渲染质量，训练时间长、存储开销大，需要新的优化机制与表征方式提升效率与稳定性。

Method: 1) 动态高斯属性演化的定量分析，构建自适应梯度聚焦机制：根据高斯模型在不同运动状态下的重要性差异，动态识别显著性并施加差异化优化策略，以加速关键区域收敛。2) 多尺度流形增强模块：结合隐式非线性解码器与显式形变场的协同优化，在多尺度上建模复杂形变，提高表达与拟合效率。

Result: 在合成数据集上实现最高96 FPS渲染帧率，同时显著降低存储开销与训练时间；保持较高的重建/渲染质量（摘要未给出具体数值与基线对比）。

Conclusion: GS-DMSR在复杂动态场景中有效平衡了收敛速度与渲染质量，通过自适应梯度聚焦与多尺度流形增强实现快速高效的建模与渲染，并带来更低的训练与存储成本。

Abstract: In the field of 3D dynamic scene reconstruction, how to balance model convergence rate and rendering quality has long been a critical challenge that urgently needs to be addressed, particularly in high-precision modeling of scenes with complex dynamic motions. To tackle this issue, this study proposes the GS-DMSR method. By quantitatively analyzing the dynamic evolution process of Gaussian attributes, this mechanism achieves adaptive gradient focusing, enabling it to dynamically identify significant differences in the motion states of Gaussian models. It then applies differentiated optimization strategies to Gaussian models with varying degrees of significance, thereby significantly improving the model convergence rate. Additionally, this research integrates a multi-scale manifold enhancement module, which leverages the collaborative optimization of an implicit nonlinear decoder and an explicit deformation field to enhance the modeling efficiency for complex deformation scenes. Experimental results demonstrate that this method achieves a frame rate of up to 96 FPS on synthetic datasets, while effectively reducing both storage overhead and training time.Our code and data are available at https://anonymous.4open.science/r/GS-DMSR-2212.

</details>


### [29] [Quantifying and Inducing Shape Bias in CNNs via Max-Pool Dilation](https://arxiv.org/abs/2601.05599)
*Takito Sawada,Akinori Iwata,Masahiro Okuda*

Main category: cs.CV

TL;DR: 提出一种衡量数据集“形状-纹理”倾向的度量，并据此用极小代价让CNN更偏向形状，从而在素描/插画等形状主导数据集提升分类效果，尤其在小样本场景。


<details>
  <summary>Details</summary>
Motivation: CNN 因卷积局部性天然偏好纹理，对自然图像有利，但在形状占主导的插画/素描等数据上表现欠佳。已有的形状偏置方法缺乏量化标准来判定何时需要形状化改造，实践中难以决策。

Method: 1) 度量：对每张图像计算亮度通道与其 L0 平滑版之间的 SSIM，汇总为数据集层面的“形状-纹理平衡”指标（亮度与平滑后越相似，越偏形状）。2) 适配：冻结卷积权重，仅修改最大池化的膨胀(dilation)以扩大感受野、促进形状偏置；仅训练最后的分类层，实现高效迁移。

Result: 在多种形状主导数据集上分类准确率稳定提升，尤其是低数据量情境下，相比全面微调成本更低、效果更稳。

Conclusion: 该度量可判定数据集是否需要形状偏置；基于池化膨胀的轻量适配无需改动卷积权重即可提升形状数据的性能，为小样本与跨域迁移提供实用方案。

Abstract: Convolutional Neural Networks (CNNs) are known to exhibit a strong texture bias, favoring local patterns over global shape information--a tendency inherent to their convolutional architecture. While this bias is beneficial for texture-rich natural images, it often degrades performance on shape-dominant data such as illustrations and sketches. Although prior work has proposed shape-biased models to mitigate this issue, these approaches lack a quantitative metric for identifying which datasets would actually benefit from such modifications. To address this gap, we propose a data-driven metric that quantifies the shape-texture balance of a dataset by computing the Structural Similarity Index (SSIM) between each image's luminance channel and its L0-smoothed counterpart. Building on this metric, we further introduce a computationally efficient adaptation method that promotes shape bias by modifying the dilation of max-pooling operations while keeping convolutional weights frozen. Experimental results show that this approach consistently improves classification accuracy on shape-dominant datasets, particularly in low-data regimes where full fine-tuning is impractical, requiring training only the final classification layer.

</details>


### [30] [SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes](https://arxiv.org/abs/2601.05600)
*Chuhan Wang,Xintong Li,Jennifer Yuntong Zhang,Junda Wu,Chengkai Huang,Lina Yao,Julian McAuley,Jingbo Shang*

Main category: cs.CV

TL;DR: 提出SceneAlign：利用场景图进行可控结构干预，生成与语言一致但视觉上错误的“硬负样本”推理链，通过偏好优化提升多模态模型的视觉对齐与忠实推理，在7个基准上提高准确率与忠实度。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在复杂视觉场景推理中常出现不忠实问题（幻觉实体、关系错位、跳步、过度描述），根源是偏好方法常让模型依赖语言先验而非真实视觉证据。需要一种能在训练中显式约束每步推理的视觉落地性的方法。

Method: 引入SceneAlign：把图像转为场景图，定位对推理关键的节点与关系；设计四类针对性结构干预，模拟常见失配（如实体替换、关系扰动、步骤缺失、细节夸张），生成与语言流畅但视觉事实错误的负向“理据”（rationales）；将正确/错误理据构成对比对，使用Direct Preference Optimization进行偏好学习，促使模型在细粒度结构上对齐视觉证据。

Result: 在7个视觉推理基准上，SceneAlign稳定提升答案准确率与推理忠实度，相比依赖文本扰动或答案条件化理据的方法更有效；能减少幻觉、错误对齐与跳步等错误类型。

Conclusion: 以场景图为媒介的结构化、可控干预可生成高质量对比数据，通过DPO训练能显著强化多模态模型的视觉落地与结构忠实推理，是提升多模态推理可靠性的有效对齐范式。

Abstract: Multimodal large language models often struggle with faithful reasoning in complex visual scenes, where intricate entities and relations require precise visual grounding at each step. This reasoning unfaithfulness frequently manifests as hallucinated entities, mis-grounded relations, skipped steps, and over-specified reasoning. Existing preference-based approaches, typically relying on textual perturbations or answer-conditioned rationales, fail to address this challenge as they allow models to exploit language priors to bypass visual grounding. To address this, we propose SceneAlign, a framework that leverages scene graphs as structured visual information to perform controllable structural interventions. By identifying reasoning-critical nodes and perturbing them through four targeted strategies that mimic typical grounding failures, SceneAlign constructs hard negative rationales that remain linguistically plausible but are grounded in inaccurate visual facts. These contrastive pairs are used in Direct Preference Optimization to steer models toward fine-grained, structure-faithful reasoning. Across seven visual reasoning benchmarks, SceneAlign consistently improves answer accuracy and reasoning faithfulness, highlighting the effectiveness of grounding-aware alignment for multimodal reasoning.

</details>


### [31] [Learning Geometric Invariance for Gait Recognition](https://arxiv.org/abs/2601.05604)
*Zengbin Wang,Junjie Li,Saihui Hou,Xu Liu,Chunshui Cao,Yongzhen Huang,Muyi Sun,Siye Wang,Man Zhang*

Main category: cs.CV

TL;DR: 论文提出从几何不变性的角度做步态识别：把跨视角/换装等条件差异近似看作几何变换组合，通过显式建模反射、旋转、缩放三类变换并学习其不变特征，实现跨条件身份判别。


<details>
  <summary>Details</summary>
Motivation: 主流方法多靠数据驱动隐式对齐不同步态条件，缺乏对条件间内在联系的显式建模，泛化受限。作者认为多种步态条件差异可抽象为几何变换，如果能获得对这些变换的不变性，则自然得到身份不变表示。

Method: 提出RRS-Gait框架：1) 将常见条件差异建模为三种几何变换（Reflect、Rotate、Scale）；2) 设计可根据变换自适应调整的卷积核，实现近似特征等变；3) 分别对反射/旋转/缩放等变特征做全局池化，得到对应不变表征；4) 以不变表征进行身份识别训练与推理。

Result: 在Gait3D、GREW、CCPG、SUSTech1K四个主流数据集上取得优于现有方法的性能，在多种跨条件（跨视角、换装等）场景下均显示出显著提升。

Conclusion: 显式把步态条件差异归结为几何变换并学习反射-旋转-缩放不变性是有效路径；RRS-Gait验证了该思路的可行性与优越性，为后续扩展到更多变换与更强等变/不变建模提供了方向。

Abstract: The goal of gait recognition is to extract identity-invariant features of an individual under various gait conditions, e.g., cross-view and cross-clothing. Most gait models strive to implicitly learn the common traits across different gait conditions in a data-driven manner to pull different gait conditions closer for recognition. However, relatively few studies have explicitly explored the inherent relations between different gait conditions. For this purpose, we attempt to establish connections among different gait conditions and propose a new perspective to achieve gait recognition: variations in different gait conditions can be approximately viewed as a combination of geometric transformations. In this case, all we need is to determine the types of geometric transformations and achieve geometric invariance, then identity invariance naturally follows. As an initial attempt, we explore three common geometric transformations (i.e., Reflect, Rotate, and Scale) and design a $\mathcal{R}$eflect-$\mathcal{R}$otate-$\mathcal{S}$cale invariance learning framework, named ${\mathcal{RRS}}$-Gait. Specifically, it first flexibly adjusts the convolution kernel based on the specific geometric transformations to achieve approximate feature equivariance. Then these three equivariant-aware features are respectively fed into a global pooling operation for final invariance-aware learning. Extensive experiments on four popular gait datasets (Gait3D, GREW, CCPG, SUSTech1K) show superior performance across various gait conditions.

</details>


### [32] [LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction](https://arxiv.org/abs/2601.05611)
*Chengen Xie,Bin Sun,Tianyu Li,Junjie Wu,Zhihui Hao,XianPeng Lang,Hongyang Li*

Main category: cs.CV

TL;DR: 提出LatentVLA：无需语言标注、以潜变量动作学习提升VLA在长尾驾驶场景泛化与实时性的框架，SOTA于NAVSIM（PDMS 92.4），在nuScenes零样本表现强。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶在大规模数据上对常见场景有效，但对长尾稀有情形表现差。现有VLA虽引入视觉-语言知识，但存在：轨迹离散化导致数值不精确；依赖语言标注带来偏置与成本；多步链式推理计算重、难以实时部署。

Method: 提出LatentVLA：以自监督的潜在动作预测训练VLA，无需语言注释，从未标注轨迹中学习丰富驾驶表征；通过知识蒸馏，将VLA的泛化能力迁移到高效的纯视觉网络，实现鲁棒与实时兼顾。

Result: 在NAVSIM基准上取得新的SOTA，PDMS=92.4；在nuScenes上展现强零样本泛化。

Conclusion: 去语言化的潜在动作学习结合知识蒸馏，可缓解长尾场景、数值精度与实时性问题，兼顾泛化与效率，适合实际部署。

Abstract: End-to-end autonomous driving models trained on largescale datasets perform well in common scenarios but struggle with rare, long-tail situations due to limited scenario diversity. Recent Vision-Language-Action (VLA) models leverage broad knowledge from pre-trained visionlanguage models to address this limitation, yet face critical challenges: (1) numerical imprecision in trajectory prediction due to discrete tokenization, (2) heavy reliance on language annotations that introduce linguistic bias and annotation burden, and (3) computational inefficiency from multi-step chain-of-thought reasoning hinders real-time deployment. We propose LatentVLA, a novel framework that employs self-supervised latent action prediction to train VLA models without language annotations, eliminating linguistic bias while learning rich driving representations from unlabeled trajectory data. Through knowledge distillation, LatentVLA transfers the generalization capabilities of VLA models to efficient vision-based networks, achieving both robust performance and real-time efficiency. LatentVLA establishes a new state-of-the-art on the NAVSIM benchmark with a PDMS score of 92.4 and demonstrates strong zeroshot generalization on the nuScenes benchmark.

</details>


### [33] [Compressing image encoders via latent distillation](https://arxiv.org/abs/2601.05639)
*Caroline Mazini Rodrigues,Nicolas Keriven,Thomas Maugey*

Main category: cs.CV

TL;DR: 提出通过简化知识蒸馏来压缩图像压缩网络的编码器，以更少数据与更短训练得到轻量编码器，同时保持重建质量与统计保真度，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像压缩模型虽重建质量高，但在硬件受限场景下因模型复杂、参数重、训练开销大而难以部署；需要在不牺牲性能的前提下降低编码端的复杂度与训练需求。

Method: 对原始重型模型的潜在空间进行近似：用简化的知识蒸馏策略，只蒸馏编码器，让轻量编码器在较少数据与较短训练中匹配教师模型的潜变量分布/表示；在两种架构上进行实现与评估，并与使用原始损失直接训练的轻量编码器比较。

Result: 在两个图像压缩架构上，蒸馏得到的轻量编码器在重建质量与统计保真度上优于用原始损失直接训练的轻量模型，同时显著降低训练数据量与训练时间。

Conclusion: 通过对潜在空间的简化蒸馏，可将重型图像压缩模型的编码器部分压缩为轻量版本，在资源受限环境中更实用，并能在保持甚至提升重建与统计一致性的情况下减少训练成本。

Abstract: Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.

</details>


### [34] [SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving](https://arxiv.org/abs/2601.05640)
*Jingyu Li,Junjie Wu,Dongnan Hu,Xiangkai Huang,Bin Sun,Zhihui Hao,Xianpeng Lang,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: SGDrive通过引入“场景-体体-目标”的层级知识结构，增强VLM在自动驾驶中的时空理解与规划能力，在NAVSIM上以纯相机方案取得SOTA（PDMS与EPDMS）。


<details>
  <summary>Details</summary>
Motivation: 通用VLM缺乏面向自动驾驶的三维时空推理与结构化表示，难以可靠捕捉几何关系、场景语义与运动模式，导致规划不稳健。

Method: 在预训练VLM骨干上，引入“Scene→Agent→Goal”层级：先建模全局场景与语境，再关注安全关键体体与行为，最后生成短期目标，从而形成紧凑而全面的结构化时空表示用于轨迹规划。

Result: 在NAVSIM基准上，SGDrive在纯相机设置下于PDMS与EPDMS两项指标均达SOTA表现，优于现有方法。

Conclusion: 将通用VLM以驾驶知识层级进行结构化可显著提升端到端自动驾驶的规划效果，验证层级知识对适配VLM至驾驶任务的有效性。

Abstract: Recent end-to-end autonomous driving approaches have leveraged Vision-Language Models (VLMs) to enhance planning capabilities in complex driving scenarios. However, VLMs are inherently trained as generalist models, lacking specialized understanding of driving-specific reasoning in 3D space and time. When applied to autonomous driving, these models struggle to establish structured spatial-temporal representations that capture geometric relationships, scene context, and motion patterns critical for safe trajectory planning. To address these limitations, we propose SGDrive, a novel framework that explicitly structures the VLM's representation learning around driving-specific knowledge hierarchies. Built upon a pre-trained VLM backbone, SGDrive decomposes driving understanding into a scene-agent-goal hierarchy that mirrors human driving cognition: drivers first perceive the overall environment (scene context), then attend to safety-critical agents and their behaviors, and finally formulate short-term goals before executing actions. This hierarchical decomposition provides the structured spatial-temporal representation that generalist VLMs lack, integrating multi-level information into a compact yet comprehensive format for trajectory planning. Extensive experiments on the NAVSIM benchmark demonstrate that SGDrive achieves state-of-the-art performance among camera-only methods on both PDMS and EPDMS, validating the effectiveness of hierarchical knowledge structuring for adapting generalist VLMs to autonomous driving.

</details>


### [35] [SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More](https://arxiv.org/abs/2601.05688)
*Muye Huang,Lingling Zhang,Yifei Li,Yaqiang Wu,Jun Liu*

Main category: cs.CV

TL;DR: 提出SketchVL与FinePO算法，通过在推理中绘图标注并用细粒度过程奖励进行信用分配，显著提升图表与多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在图表理解中难以进行精确视觉推理，强化学习训练普遍存在信用分配粗糙的问题（仅轨迹级优势估计，无法区分单次推理步骤的对错）。

Method: 构建MLLM——SketchVL，在推理过程中把中间推理步骤以可视化标记绘制到图像上，并将带标注图像回馈模型形成多步闭环推理；训练时引入FinePO强化学习算法，依托细粒度过程奖励模型FinePRM，对轨迹内的每一次绘图动作逐步打分，从而对正确/错误token进行精细奖励与惩罚。

Result: 在图表数据集、自然图像数据集与数学任务上，相比基座模型平均提升7.23%，模型学会在步骤层面对齐FinePRM信号。

Conclusion: 细粒度过程级信用分配结合可视化中间步骤可显著增强MLLM的复杂视觉推理与图表理解能力，为训练强推理多模态模型提供新范式。

Abstract: Charts are high-density visual carriers of complex data and medium for information extraction and analysis. Due to the need for precise and complex visual reasoning, automated chart understanding poses a significant challenge to existing Multimodal Large Language Models (MLLMs). Many MLLMs trained with reinforcement learning (RL) face the challenge of credit assignment. Their advantage estimation, typically performed at the trajectory level, cannot distinguish between correct and incorrect reasoning steps within a single generated response. To address this limitation, we introduce SketchVL, a novel MLLM that optimized with FinePO, a new RL algorithm designed for fine-grained credit assignment within each trajectory. SketchVL's methodology involves drawing its intermediate reasoning steps as markers on the image and feeding the annotated image back to itself, creating a robust, multi-step reasoning process. During training, the FinePO algorithm leverages a Fine-grained Process Reward Model (FinePRM) to score each drawing action within a trajectory, thereby precisely assigning credit for each step. This mechanism allows FinePO to more strongly reward correct tokens when a trajectory is globally successful, and more heavily penalize incorrect tokens when the trajectory is globally suboptimal, thus achieving fine-grained reinforcement signals. Experiments show that SketchVL learns to align its step-level behavior with the FinePRM, achieving an average performance gain of 7.23\% over its base model across chart datasets, natural image datasets, and mathematics, providing a promising new direction for training powerful reasoning models.

</details>


### [36] [Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation](https://arxiv.org/abs/2601.05722)
*Jin Wang,Jianxiang Lu,Comi Chen,Guangzheng Xu,Haoyu Yang,Peng Chen,Na Zhang,Yifan Xu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: RCM提出一种面向单图像到多视角/3D角色的扩散式图像转视频框架，通过姿态归一化与多视角条件，实现可控、高分辨率的环绕视角视频与更优的3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 单图生成高质量3D角色困难，人体复杂姿态与自遮挡导致跨视角不一致、漂移和细节缺失；现有扩散式NVS方法在分辨率、视角一致性与相机控制上受限。

Method: 提出RCM（Rotate your Character Model）：(1) 将任意复杂姿态的角色映射到规范（canonical）姿态以实现跨视角一致；(2) 基于图像到视频的扩散模型生成整圈轨道视频，分辨率达1024×1024；(3) 引入可控相机观测位姿，支持由初始相机姿态决定的观察位置；(4) 多视角条件，最多接收4张输入图像，适配不同用户场景；并以此驱动后续3D重建。

Result: 在NVS与3D角色生成两方面均优于SOTA，生成的环绕视频质量更高、一致性更强，且支持更高分辨率与可控相机。

Conclusion: 通过姿态归一化与多视角可控的扩散式图像转视频流程，RCM显著提升单图到多视角与3D角色生成的质量与可控性，为复杂姿态与自遮挡场景提供有效解决方案。

Abstract: Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.

</details>


### [37] [TAGRPO: Boosting GRPO on Image-to-Video Generation with Direct Trajectory Alignment](https://arxiv.org/abs/2601.05729)
*Jin Wang,Jianxiang Lu,Guangzheng Xu,Comi Chen,Haoyu Yang,Linqing Wang,Peng Chen,Mingtao Chen,Zhichao Hu,Longhuang Wu,Shuai Shao,Qinglin Lu,Ping Luo*

Main category: cs.CV

TL;DR: 提出TAGRPO：在I2V模型中基于对比思想的GRPO后训练框架，通过相同初始噪声生成的rollout对齐高奖励轨迹、远离低奖励，并配合记忆库提升多样性与效率，显著优于DanceGRPO。


<details>
  <summary>Details</summary>
Motivation: 现有将GRPO用于流匹配的成功多在文生图/文生视频，但直接套用到图生视频(I2V)时奖励一致性提升不稳定，效果有限，需要更稳健的后训练方法。

Method: 1) 观察：从相同初始噪声出发生成的多条视频rollout对优化更具指导性；2) 设计：在中间潜变量上施加新的GRPO对比损失，使模型向高奖励轨迹对齐、同时与低奖励轨迹拉开距离；3) 工程：引入rollout视频记忆库，复用多样样本以降低计算和提高多样性。

Result: 在I2V任务上，相比DanceGRPO取得显著性能提升（奖励和生成质量更优），训练更稳定、计算更高效。

Conclusion: TAGRPO是一个简单有效、受对比学习启发的I2V后训练框架，通过同噪声rollout对比和记忆库机制，显著提升I2V模型的奖励与生成效果，优于现有GRPO变体。

Abstract: Recent studies have demonstrated the efficacy of integrating Group Relative Policy Optimization (GRPO) into flow matching models, particularly for text-to-image and text-to-video generation. However, we find that directly applying these techniques to image-to-video (I2V) models often fails to yield consistent reward improvements. To address this limitation, we present TAGRPO, a robust post-training framework for I2V models inspired by contrastive learning. Our approach is grounded in the observation that rollout videos generated from identical initial noise provide superior guidance for optimization. Leveraging this insight, we propose a novel GRPO loss applied to intermediate latents, encouraging direct alignment with high-reward trajectories while maximizing distance from low-reward counterparts. Furthermore, we introduce a memory bank for rollout videos to enhance diversity and reduce computational overhead. Despite its simplicity, TAGRPO achieves significant improvements over DanceGRPO in I2V generation.

</details>


### [38] [FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time](https://arxiv.org/abs/2601.05738)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出FeatureSLAM：将3D Gaussian Splatting用于实时SLAM，联合高效跟踪与具语义的逼真建图；通过与视觉基础模型对齐的稠密特征光栅化，实现开放集、自由视角分割，并提升跟踪与建图精度。


<details>
  <summary>Details</summary>
Motivation: 现有语义SLAM多依赖预定义类别标签，限制开放集与下游任务能力；同时高保真建图与实时性难以兼得。需要一种既能实时跟踪、又能融合强语义与高质量渲染的统一框架，以提升SLAM稳定性、地图保真度，并支持更灵活的语义交互。

Method: 基于3DGS的实时SLAM：在新视图合成管线中引入与视觉基础模型对齐的稠密特征光栅化，将语义特征直接嵌入3D高斯表示。利用这些特征辅助相机位姿跟踪与地图优化，实现光照真实的RGB/深度渲染与语义/语言掩膜生成；系统保持实时性、计算开销可控。

Result: 在标准基准上达到与SOTA相当的实时跟踪速度，同时提升稳定性与地图质量；相较固定类别的SLAM基线，姿态误差降低约9%，建图精度提升约8%；语义与语言掩膜效果与离线3DGS模型相当，并具备自由视角、开放集分割能力。

Conclusion: 将特征嵌入的3DGS与SLAM实时统一可同时增强跟踪与建图，并解锁开放集、自由视角的语义应用；在不显著增加计算成本的前提下，获得更稳健的位姿估计与更高保真地图与渲染。

Abstract: We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\% lower pose error and 8\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.

</details>


### [39] [ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers](https://arxiv.org/abs/2601.05741)
*Guray Ozgur,Eduarda Caldeira,Tahar Chettaoui,Jan Niklas Kolf,Marco Huber,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出ViTNT-FIQA：基于ViT中间层patch嵌入演化稳定性的训练免微调人脸图像质量评估方法，只需一次前向传播，在多个基准上达到与SOTA竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 现有FIQA方法多只利用最终层特征，或训练免方法依赖多次前向/反向传播，计算开销大、适配性差。作者希望在不修改模型、不做训练的前提下，从中间层动态中提取质量信号，提升效率与通用性。

Method: 使用任意预训练ViT人脸识别模型，对输入图像进行一次前向传播；在各个Transformer block之间，提取并L2归一化patch嵌入，计算相邻block间的欧氏距离，衡量特征演化的“稳定性”；将全图patch及层间距离进行聚合，得到图像级质量分数。高质量图像在层间呈现稳定、渐进的特征精炼；退化图像表现为更不稳定、剧烈的变化。

Result: 在受控退化的合成质量标注数据上验证演化稳定性与质量的相关性；在八个基准（LFW、AgeDB-30、CFP-FP、CALFW、Adience、CPLFW、XQLFW、IJB-C）上，达到与SOTA竞争的识别性能与质量评估效果，同时显著降低计算成本。

Conclusion: ViTNT-FIQA以一次前向传播、无反传与无架构改动，即可为任意预训练ViT人脸模型提供准确、高效的质量评估，展现出强通用性与实用价值。

Abstract: Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.

</details>


### [40] [FlyPose: Towards Robust Human Pose Estimation From Aerial Views](https://arxiv.org/abs/2601.05747)
*Hassaan Farooq,Marvin Brenner,Peter St\ütz*

Main category: cs.CV

TL;DR: 提出FlyPose：轻量级、顶视角（top-down）的无人机人类检测与2D姿态估计管线，多数据集训练，实时部署于Jetson Orin与在飞四旋翼，显著提升检测与姿态mAP，并发布小而难的航拍姿态数据集FlyPose-104。


<details>
  <summary>Details</summary>
Motivation: 无人机在人群附近执行投递、监测、救灾和巡检，需要从高空视角可靠感知人的位置与动作。然而航拍存在低分辨率、俯视大倾角、遮挡等难点，且应用需实时，现有方法在该视角与算力约束下表现不足。

Method: 构建FlyPose轻量级顶视管线：（1）多数据集联合训练以提升跨域泛化；（2）首先进行行人检测，再做2D关键点姿态估计；（3）模型与预处理优化以实现边缘端实时推理；在Jetson Orin AGX上端到端约20 ms；并在真实飞行中机载部署。

Result: 在人检：在Manipal-UAV、VisDrone、HIT-UAV及自建数据集上平均提升6.8 mAP；在2D姿态：在UAV-Human上提升16.3 mAP；推理延迟约20 ms（含预处理），可在四旋翼飞行中稳定运行。

Conclusion: FlyPose在航拍场景实现高效、鲁棒的人体检测与2D姿态估计，兼顾精度与实时性；发布FlyPose-104数据集以促进航拍姿态研究与评测。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.

</details>


### [41] [Adaptive Disentangled Representation Learning for Incomplete Multi-View Multi-Label Classification](https://arxiv.org/abs/2601.05785)
*Quanjiang Li,Zhiming Liu,Tianxiang Xu,Tingjin Luo,Chenping Hou*

Main category: cs.CV

TL;DR: 提出一种用于多视角多标签学习的自适应解耦表示学习方法（ADRL），在特征缺失与标注不完整场景下，通过跨模态邻域传播完成视图、随机掩码强化重建、标签分布传播建模语义、互信息目标促进共享一致与专属解耦、原型特异特征选择与伪标签引导的视图融合，获得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实多视角多标签任务常同时存在视图特征缺失与标签不完整，当前方法在特征恢复、表示解耦和标签语义建模方面存在局限，需要一种统一框架兼顾鲁棒视图补全、标签依赖刻画与可训练的理论支撑。

Method: 提出ADRＬ：1）基于邻域感知的跨模态特征亲和传播实现鲁棒视图补全，并用随机掩码策略增强重建；2）通过类别级关联在标签分布间传播，细化分布参数以捕获相互依赖的标签原型；3）构建互信息目标，增强共享表示一致性、抑制视图特异表示与他模态的信息重叠，并给出可训练的下界；4）通过标签嵌入与视图表示的独立交互实现原型特异的特征选择；5）为每个类别生成伪标签，利用伪标签空间的结构特性在视图融合时实现判别性权衡。

Result: 在公开数据集和真实应用上进行了大量实验，ADRL取得优于现有方法的性能，展现出在特征缺失与不完全标注条件下的鲁棒性与泛化能力。

Conclusion: ADRL在统一框架下兼顾视图补全、表示解耦与标签语义建模，通过互信息驱动的一致性/解耦目标与伪标签引导的融合策略，显著提升多视角多标签学习的表现，且具备理论可训练界限与实践有效性。

Abstract: Multi-view multi-label learning frequently suffers from simultaneous feature absence and incomplete annotations, due to challenges in data acquisition and cost-intensive supervision. To tackle the complex yet highly practical problem while overcoming the existing limitations of feature recovery, representation disentanglement, and label semantics modeling, we propose an Adaptive Disentangled Representation Learning method (ADRL). ADRL achieves robust view completion by propagating feature-level affinity across modalities with neighborhood awareness, and reinforces reconstruction effectiveness by leveraging a stochastic masking strategy. Through disseminating category-level association across label distributions, ADRL refines distribution parameters for capturing interdependent label prototypes. Besides, we formulate a mutual-information-based objective to promote consistency among shared representations and suppress information overlap between view-specific representation and other modalities. Theoretically, we derive the tractable bounds to train the dual-channel network. Moreover, ADRL performs prototype-specific feature selection by enabling independent interactions between label embeddings and view representations, accompanied by the generation of pseudo-labels for each category. The structural characteristics of the pseudo-label space are then exploited to guide a discriminative trade-off during view fusion. Finally, extensive experiments on public datasets and real-world applications demonstrate the superior performance of ADRL.

</details>


### [42] [SceneFoundry: Generating Interactive Infinite 3D Worlds](https://arxiv.org/abs/2601.05810)
*ChunTeng Chen,YiChen Hsu,YiWen Liu,WeiFang Sun,TsaiChing Ni,ChunYi Lee,Min Sun,YuanFu Yang*

Main category: cs.CV

TL;DR: SceneFoundry提出一种语言引导的扩散框架，从文本生成包含可动关节家具、可交互、可导航的公寓级三维环境，用于机器人训练。通过LLM规划户型，扩散后验采样填充带关节的资产，并用可微约束确保数量、碰撞与可行走空间，生成结构合理、语义连贯、可交互的场景。


<details>
  <summary>Details</summary>
Motivation: 现有生成3D室内方法难以表达真实室内的功能复杂性，尤其是含可动关节物体（如门、柜、抽屉），导致对机器人操作与导航训练不足，需要一种既大规模又物理可用、可交互的三维环境生成方法。

Method: 提出SceneFoundry：以自然语言为输入，(1) LLM模块生成和控制户型与布局先验；(2) 基于扩散的后验采样从大规模3D资产库高效选择与布置带关节的物体；(3) 设计可微指导函数，对物体数量、关节/几何碰撞、可行走空间等进行约束，确保物理可用性与功能交互性。

Result: 在多种场景类型与条件下，生成的环境在结构有效性、语义一致性与功能交互性上表现出色，包含可操作的关节部件并适配机器人导航；实验显示该框架可稳定、大规模地生成训练所需的室内世界。

Conclusion: SceneFoundry能从文本自动构建公寓级、功能可交互且物理可行的3D环境，为可扩展的具身智能与机器人学习提供高质量训练场景。

Abstract: The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.

</details>


### [43] [Boosting Latent Diffusion Models via Disentangled Representation Alignment](https://arxiv.org/abs/2601.05823)
*John Page,Xuesong Niu,Kai Wu,Kun Gai*

Main category: cs.CV

TL;DR: 提出Send-VAE：通过与预训练视觉基础模型(VFM)的语义层级对齐，学习更可解缠的潜空间，从而提升下游生成模型的训练效率和质量，在ImageNet 256x256上达SOTA FID（1.21/1.75）。


<details>
  <summary>Details</summary>
Motivation: 现有LDM在VAE压缩潜空间中生成，但通常将VAE与LDM都对齐到同一VFM语义目标，忽视两者需求差异：LDM需要高层语义保持；VAE更需要语义可解缠以结构化编码属性级信息。统一对齐目标限制了VAE对生成友好的表示能力。

Method: 提出Semantic disentangled VAE (Send-VAE)。通过一个非线性映射器将VAE潜变量映射到与VFM语义层级对齐的空间，使VAE学习到属性级别解缠的表示，同时保留与高层语义的关联；对解缠程度以线性探测做评估，并用该VAE潜空间训练流式Transformer（SiT）。

Result: 线性探测显示更强的属性预测能力且与生成质量提升相关；在ImageNet 256x256上，基于Send-VAE训练的SiT显著加速训练，并取得SOTA：使用/不使用无分类器引导的FID分别为1.21和1.75。

Conclusion: VAE与LDM需要不同的对齐目标。通过将VAE潜空间与VFM的语义层级对齐、强化属性级解缠，可同时提升表示质量与生成性能，并加速后续生成模型训练。

Abstract: Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.

</details>


### [44] [GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras](https://arxiv.org/abs/2601.05839)
*Weimin Liu,Wenjun Wang,Joshua H. Meng*

Main category: cs.CV

TL;DR: 提出GeoSurDepth：以几何一致性为核心的环视自监督深度估计方法，结合基础模型提供的几何先验与特征增强、基于空间扭转载体的2D-3D提升与多时空视合成监督、以及自适应联合运动学习，在DDAD与nuScenes上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有环视深度估计多依赖光度一致性约束，较少显式利用单目与环视场景中丰富的几何结构，导致在纹理稀缺、动态物体、遮挡等情况下不稳定。需要一个以几何一致性为核心的框架，提升鲁棒性与精度，作为激光雷达的有力替代。

Method: 1) 以几何一致性为主导：用基础模型（foundation models）作为伪几何先验与特征增强，约束3D空间表面法向一致，并在2D层面正则化物体与纹理一致的深度。2) 新的视图合成流程：通过空间扭转重建稠密深度实现2D-3D提升，跨时间、空间及时空联合进行光度监督，弥补单视图重建的不足。3) 自适应联合运动学习：根据空间几何线索自适应地强调信息量更高的区域，改进相机/场景运动推断。

Result: 在DDAD与nuScenes环视数据集上取得SOTA性能，显著优于已有自监督多视深度方法，验证了几何一致性主导学习与多时空监督的有效性。

Conclusion: 几何一致性与几何连贯性是环视自监督深度估计的关键。通过基础模型先验、视图合成中的2D-3D提升与自适应运动学习，GeoSurDepth在真实自动驾驶数据上实现鲁棒、高精度深度估计，为以视觉替代部分激光雷达感知提供可行路径。

Abstract: Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.

</details>


### [45] [Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals](https://arxiv.org/abs/2601.05848)
*Nate Gillman,Yinghua Zhou,Zitian Tang,Evan Luo,Arjan Chakravarthy,Daksh Aggarwal,Michael Freeman,Charles Herrmann,Chen Sun*

Main category: cs.CV

TL;DR: 提出“Goal Force”框架：用显式力向量与中间动力学来设定视频生成模型的目标，使其像隐式物理引擎一样进行物理一致的规划与推理，并在零样本下泛化到复杂真实场景。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型能预测未来，但难以精确设定目标：文本过于抽象难以表达物理细节，目标图像在动态任务中难以给出且不可行。需要一种能直接编码物理意图（力、接触、因果链）的目标表示方式。

Method: 构建合成“因果原语”数据集（如弹性碰撞、多米诺效果等），用显式的力向量与中间动力学标注，训练视频生成模型学习在时空中传播力与因果关系；在推理时，用户通过力场/力矢量与阶段性约束来设定目标，模型据此生成满足物理约束的视频轨迹。

Result: 尽管仅在简单物理数据上训练，模型在零样本下对复杂真实任务（如工具操控、多物体因果链）表现出强泛化能力，能生成精确、物理一致的未来视频以支持规划。

Conclusion: 将视频生成扎根于基本物理交互，使模型涌现为隐式神经物理模拟器，可进行精细、物理感知的规划而无需外部物理引擎。作者开源数据、代码、权重与交互式演示。

Abstract: Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.

</details>


### [46] [Kidney Cancer Detection Using 3D-Based Latent Diffusion Models](https://arxiv.org/abs/2601.05852)
*Jen Dusseljee,Sarah de Boer,Alessa Hering*

Main category: cs.CV

TL;DR: 提出一种基于潜空间扩散的3D肾脏异常检测流程，在仅有病例级伪标签的弱监督下直接处理体数据，虽未超越监督基线但验证了可行性并指出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为切片级、依赖精细标注且对复杂腹部解剖的建模受限。为降低标注成本并更好地利用体数据，需要探索能在弱监督下进行3D异常检测的生成式方法。

Method: 构建将VQ-GAN的潜空间表示与扩散模型结合的3D管线：在潜空间中用DDPM/DDIM进行重建与生成，直接对体数据操作；训练时仅使用病例级伪标签进行弱监督以进行异常定位（通过重建误差/一致性等信号进行检测与定位）。与当前最优的监督分割/检测模型进行基准对比。

Result: 方法能在仅病例级监督下实现可行的3D异常检测与初步定位，但定量性能尚未达到监督基线；实验显示潜空间重建质量与定位精度相关，揭示了影响因素和改进潜力。

Conclusion: 3D潜扩散在弱监督肾脏异常检测上可行且有前景，尽管尚不及监督方法，但为降低标注依赖提供了路径；后续应提高重建保真度与定位能力以缩小与监督基线的差距。

Abstract: In this work, we present a novel latent diffusion-based pipeline for 3D kidney anomaly detection on contrast-enhanced abdominal CT. The method combines Denoising Diffusion Probabilistic Models (DDPMs), Denoising Diffusion Implicit Models (DDIMs), and Vector-Quantized Generative Adversarial Networks (VQ-GANs). Unlike prior slice-wise approaches, our method operates directly on an image volume and leverages weak supervision with only case-level pseudo-labels. We benchmark our approach against state-of-the-art supervised segmentation and detection models. This study demonstrates the feasibility and promise of 3D latent diffusion for weakly supervised anomaly detection. While the current results do not yet match supervised baselines, they reveal key directions for improving reconstruction fidelity and lesion localization. Our findings provide an important step toward annotation-efficient, generative modeling of complex abdominal anatomy.

</details>


### [47] [LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting](https://arxiv.org/abs/2601.05853)
*Yinghan Xu,John Dingliana*

Main category: cs.CV

TL;DR: 提出一种将任意姿态的人体分解为可动画的多层3D头像（身体与服装分离）的框架，结合2D高斯表示与扩散模型SDS补全遮挡，实现更高保真重建与真实试穿。


<details>
  <summary>Details</summary>
Motivation: 单层重建会把衣物“绑定”到单一身份，难以复用；现有多层方法在遮挡区域重建不佳，影响服装/身体分离与重组及虚拟试穿质量。

Method: - 将每一层（身体、服装）编码为2D Gaussians以获得精确几何与逼真渲染；
- 用预训练2D扩散模型通过SDS对被遮挡区域进行补全；
- 三阶段训练：1）单层重建得到规范空间中的粗糙服装；2）多层联合训练同时恢复内层身体与外层服装细节；3）细化以提升渲染与几何质量；
- 在新视角与姿态下支持层分解与重组。

Result: 在4D-Dress与THuman2.0两数据集上，渲染质量、层分解与重组效果均优于SOTA；实现逼真的跨视角、跨姿态虚拟试穿。

Conclusion: 多层2D高斯表示结合SDS补全与分阶段训练，有效解决单层绑定与遮挡重建难题，推动高保真、可动画3D人体资产的实用化；代码已开源。

Abstract: We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS

</details>


### [48] [Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation](https://arxiv.org/abs/2601.05855)
*Kaiwen Huang,Yizhe Zhang,Yi Zhou,Tianyang Xu,Tao Zhou*

Main category: cs.CV

TL;DR: 提出BCSI半监督医学图像分割框架：用语义-空间强扰动与一致性学习提升鲁棒性；通过通道选择路由器与双向通道交互在标注/未标注间筛选并强化关键信息，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分割依赖Mean Teacher或双流一致性，存在伪标签误差累积、结构复杂、标注与未标注流交互不足、噪声传播等问题，限制了在标注稀缺场景下的性能与稳定性。

Method: 1) 语义-空间扰动（SSP）：对同一图像施加两种强增强，弱增强生成的伪标签用于无监督监督；再在两种强增强预测间施加一致性约束，提升鲁棒性与稳定性。2) 通道选择路由器（CR）：在标注/未标注信息交互时，动态筛选最相关通道，仅激活高相关特征，抑制噪声与无关干扰。3) 双向通道交互（BCI）：在两数据流间进行双向语义补充与重要通道增强，强化表示。整体构成BCSI框架并适配3D医学分割。

Result: 在多种3D医学分割基准数据集上，该方法取得较现有半监督方法更优的分割性能（文中声称全面超越SOTA），表现为更高的指标与更强的稳定性/鲁棒性。

Conclusion: 通过强扰动一致性与通道级可选路由的双向交互，有效缓解误差传播与结构复杂度导致的问题，在低标注条件下实现稳健、精确的3D医学图像分割，优于现有半监督方案。

Abstract: Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.

</details>


### [49] [Phase4DFD: Multi-Domain Phase-Aware Attention for Deepfake Detection](https://arxiv.org/abs/2601.05861)
*Zhen-Xin Lin,Shang-Kuan Chen*

Main category: cs.CV

TL;DR: 提出Phase4DFD：在深度伪造检测中显式建模“相位-幅值”交互，用相位感知注意力引导频域特征，结合RGB、FFT幅度与LBP，基于轻量BNext-M主干，取得SOTA性能与低开销。


<details>
  <summary>Details</summary>
Motivation: 现有频域方法多依赖谱幅值，忽视相位；而相位包含由合成过程引入的不连续性与错位信息，可能更能揭示伪造伪影。需要一种显式利用相位并与幅值协同的框架。

Method: 1) 输入层多模态：RGB + FFT幅度 + LBP；2) 设计相位感知注意力模块，在主干特征提取前，利用相位不连续性引导关注最具操控指示性的频域模式，实现相位-幅值交互；3) 主干采用高效BNext-M；4) 可选通道-空间注意力作语义细化；5) 端到端训练与消融验证。

Result: 在CIFAKE与DFFD数据集上优于主流空间域与频域检测器，同时保持较低计算开销；消融研究显示显式相位建模提供与幅值互补且非冗余的信息，显著提升检测性能。

Conclusion: 相位信息在深伪检测中至关重要。通过输入级相位感知注意力与多域表征的结合，Phase4DFD在有效性与效率上达成平衡，并为后续频域相位建模提供可行路径。

Abstract: Recent deepfake detection methods have increasingly explored frequency domain representations to reveal manipulation artifacts that are difficult to detect in the spatial domain. However, most existing approaches rely primarily on spectral magnitude, implicitly under exploring the role of phase information. In this work, we propose Phase4DFD, a phase aware frequency domain deepfake detection framework that explicitly models phase magnitude interactions via a learnable attention mechanism. Our approach augments standard RGB input with Fast Fourier Transform (FFT) magnitude and local binary pattern (LBP) representations to expose subtle synthesis artifacts that remain indistinguishable under spatial analysis alone. Crucially, we introduce an input level phase aware attention module that uses phase discontinuities commonly introduced by synthetic generation to guide the model toward frequency patterns that are most indicative of manipulation before backbone feature extraction. The attended multi domain representation is processed by an efficient BNext M backbone, with optional channel spatial attention applied for semantic feature refinement. Extensive experiments on the CIFAKE and DFFD datasets demonstrate that our proposed model Phase4DFD outperforms state of the art spatial and frequency-based detectors while maintaining low computational overhead. Comprehensive ablation studies further confirm that explicit phase modeling provides complementary and non-redundant information beyond magnitude-only frequency representations.

</details>


### [50] [Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens](https://arxiv.org/abs/2601.05927)
*Yohann Perron,Vladyslav Sydorov,Christophe Pottier,Loic Landrieu*

Main category: cs.CV

TL;DR: 提出一种在超高分辨率图像分割中兼顾全局与局部的Transformer多尺度框架：并行处理高分辨率小裁剪与低分辨率大裁剪，通过少量可学习“中继token”在两分支间传递与聚合特征，几乎不增参数，显著提升mIoU。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么滑窗导致缺乏全局上下文，要么下采样丢失细节。需要一种既保留局部精细结构又具备全局感知的简单通用方法，并能无缝接入主流Vision Transformer骨干。

Method: 构建双分支多尺度Transformer：局部分支以高分辨率小裁剪捕捉细节，全局分支以低分辨率大裁剪获取上下文；引入少量可学习的“relay tokens”在两分支间进行特征聚合与信息传递；模块可直接插入ViT/Swin，参数增加<2%。

Result: 在Archaeoscape、URUR、Gleason三大超高分辨率分割基准及Cityscapes上均有稳定收益，最高可达相对mIoU提升15%；提供代码与预训练模型。

Conclusion: 显式多尺度推理与中继token机制能在不显著增加模型复杂度的前提下同时保留局部细节与全局语义，适用于标准Transformer骨干并在多数据集上验证有效。

Abstract: Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .

</details>


### [51] [Performance of a Deep Learning-Based Segmentation Model for Pancreatic Tumors on Public Endoscopic Ultrasound Datasets](https://arxiv.org/abs/2601.05937)
*Pankaj Gupta,Priya Mudgil,Niharika Dutta,Kartik Bose,Nitish Kumar,Anupam Kumar,Jimil Shah,Vaneet Jearth,Jayanta Samanta,Vishal Sharma,Harshal Mandavdhare,Surinder Rana,Saroj K Sinha,Usha Dutta*

Main category: cs.CV

TL;DR: 研究评估一种基于Vision Transformer的EUS胰腺肿瘤分割模型，在多数据集上训练并外部验证，取得中等偏高的分割性能（DSC约0.65，IoU约0.61），但存在多发错误分割与数据异质性问题，需进一步标准化与前瞻性研究。


<details>
  <summary>Details</summary>
Motivation: EUS对胰腺癌诊断依赖操作者经验，主观性强、可重复性差；需要自动、客观、可泛化的分割工具辅助肿瘤识别与量化，以提升诊断一致性与效率。

Method: 采用USFM框架并以Vision Transformer为骨干的深度分割模型。数据：来自两个公开数据集的17,367张EUS图像进行5折交叉验证训练/验证；另有第三个公开数据集350张由放射科医师手工标注图像作独立外部测试。预处理：灰度化、裁剪、统一至512×512。评价指标：DSC、IoU、敏感度、特异度、准确率，并报告置信区间与错误类型（多重预测）。

Result: 5折交叉验证：平均DSC 0.651±0.738，IoU 0.579±0.658，敏感度69.8%，特异度98.8%，准确率97.5%。外部验证：DSC 0.657（95%CI 0.634–0.769），IoU 0.614（95%CI 0.590–0.689），敏感度71.8%，特异度97.7%。约9.7%病例出现错误的多重肿瘤预测。

Conclusion: ViT驱动的EUS肿瘤分割模型在内部与外部数据上表现稳定，可作为辅助工具的潜力明确；但受限于数据集异质性、外部验证规模有限及假阳性多发预测，需要更大规模、多中心、标准化标注与前瞻性研究以提升鲁棒性与临床可用性。

Abstract: Background: Pancreatic cancer is one of the most aggressive cancers, with poor survival rates. Endoscopic ultrasound (EUS) is a key diagnostic modality, but its effectiveness is constrained by operator subjectivity. This study evaluates a Vision Transformer-based deep learning segmentation model for pancreatic tumors. Methods: A segmentation model using the USFM framework with a Vision Transformer backbone was trained and validated with 17,367 EUS images (from two public datasets) in 5-fold cross-validation. The model was tested on an independent dataset of 350 EUS images from another public dataset, manually segmented by radiologists. Preprocessing included grayscale conversion, cropping, and resizing to 512x512 pixels. Metrics included Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity, specificity, and accuracy. Results: In 5-fold cross-validation, the model achieved a mean DSC of 0.651 +/- 0.738, IoU of 0.579 +/- 0.658, sensitivity of 69.8%, specificity of 98.8%, and accuracy of 97.5%. For the external validation set, the model achieved a DSC of 0.657 (95% CI: 0.634-0.769), IoU of 0.614 (95% CI: 0.590-0.689), sensitivity of 71.8%, and specificity of 97.7%. Results were consistent, but 9.7% of cases exhibited erroneous multiple predictions. Conclusions: The Vision Transformer-based model demonstrated strong performance for pancreatic tumor segmentation in EUS images. However, dataset heterogeneity and limited external validation highlight the need for further refinement, standardization, and prospective studies.

</details>


### [52] [Context-Aware Decoding for Faithful Vision-Language Generation](https://arxiv.org/abs/2601.05939)
*Mehrdad Fazli,Bowen Wei,Ziwei Zhu*

Main category: cs.CV

TL;DR: 论文提出一种无需再训练的低开销方法，通过在解码过程中注入“上下文嵌入”来降低多模态大模型的幻觉；基于层级生成动力学分析发现真实与幻觉token在“承诺深度”上存在显著差异，并据此设计CEI，在多个基准和模型上显著降低幻觉率。


<details>
  <summary>Details</summary>
Motivation: LVLM在开放式任务（图像描述、视觉推理）中易产生与视觉输入不一致的回答（幻觉），现有方法要么代价高（再训练、重打分），要么通用性差。作者希望从机制层面理解幻觉在解码各层是如何形成的，并据此提出一种训练无关、可扩展的缓解方案。

Method: 1) 使用Logit Lens逐层观察解码器层生成的下一token分布，比较“真实”与“幻觉”token的概率累积轨迹，提出“承诺深度差距”：真实token更早在候选中累积概率并稳定。2) 基于此，提出Context Embedding Injection（CEI）：在解码时利用最后一个输入token的隐藏状态（上下文嵌入）作为持续的视觉-语境锚点，向后续层注入以稳定对视觉证据的依赖；提供静态与动态变体（动态地调节注入强度）。

Result: 在CHAIR、AMBER、MMHal-Bench基准（最长512 token）上，CEI在三个LVLM上均优于SOTA基线，动态CEI获得最低总体幻觉率。

Conclusion: 幻觉与真实生成在层级承诺进程上存在系统性差异；利用上下文嵌入作为解码期间的锚点可训练无关地降低幻觉，具有轻量、可扩展、跨模型通用的优点。

Abstract: Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.

</details>


### [53] [WaveRNet: Wavelet-Guided Frequency Learning for Multi-Source Domain-Generalized Retinal Vessel Segmentation](https://arxiv.org/abs/2601.05942)
*Chanchan Wang,Yuanfang Wang,Qing Xu,Guanxin Chen*

Main category: cs.CV

TL;DR: 提出WaveRNet：结合小波频域建模与SAM以应对视网膜血管分割的跨域泛化与细结构保真问题，实现SOTA泛化表现。


<details>
  <summary>Details</summary>
Motivation: 跨域视网膜血管分割受照明不均、对比度变化导致的域偏移影响严重，且细小血管易在上采样中丢失。现有基于SAM的方法多做简单适配器微调，忽视频域（可提供域不变表征），并存在细节恢复不足，导致泛化性受限。

Method: 构建WaveRNet框架：1）Spectral-guided Domain Modulator（SDM）：将小波分解与可学习域token结合，分离鲁棒的低频结构与高频边界并生成域特异特征；2）Frequency-Adaptive Domain Fusion（FADF）：利用小波频域相似度在测试时进行智能域选择与软权融合；3）Hierarchical Mask-Prompt Refiner（HMPR）：层级式掩码-提示细化与长程依赖建模，缓解SAM直接上采样导致的细节丢失。

Result: 在四个公开数据集、Leave-One-Domain-Out设定下，取得当前最优的跨域泛化性能，细小血管保真度更高。

Conclusion: 频域引导的域建模与自适应融合，加上层级细化，可显著提升SAM在视网膜血管分割的跨域鲁棒性与细节保持；WaveRNet实现SOTA且代码已开源。

Abstract: Domain-generalized retinal vessel segmentation is critical for automated ophthalmic diagnosis, yet faces significant challenges from domain shift induced by non-uniform illumination and varying contrast, compounded by the difficulty of preserving fine vessel structures. While the Segment Anything Model (SAM) exhibits remarkable zero-shot capabilities, existing SAM-based methods rely on simple adapter fine-tuning while overlooking frequency-domain information that encodes domain-invariant features, resulting in degraded generalization under illumination and contrast variations. Furthermore, SAM's direct upsampling inevitably loses fine vessel details. To address these limitations, we propose WaveRNet, a wavelet-guided frequency learning framework for robust multi-source domain-generalized retinal vessel segmentation. Specifically, we devise a Spectral-guided Domain Modulator (SDM) that integrates wavelet decomposition with learnable domain tokens, enabling the separation of illumination-robust low-frequency structures from high-frequency vessel boundaries while facilitating domain-specific feature generation. Furthermore, we introduce a Frequency-Adaptive Domain Fusion (FADF) module that performs intelligent test-time domain selection through wavelet-based frequency similarity and soft-weighted fusion. Finally, we present a Hierarchical Mask-Prompt Refiner (HMPR) that overcomes SAM's upsampling limitation through coarse-to-fine refinement with long-range dependency modeling. Extensive experiments under the Leave-One-Domain-Out protocol on four public retinal datasets demonstrate that WaveRNet achieves state-of-the-art generalization performance. The source code is available at https://github.com/Chanchan-Wang/WaveRNet.

</details>


### [54] [VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction](https://arxiv.org/abs/2601.05966)
*Longbin Ji,Xiaoxiong Liu,Junyuan Shang,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CV

TL;DR: VideoAR提出首个大规模视觉自回归(VAR)视频生成框架，通过多尺度下一帧预测结合自回归建模，实现高质量且更高效的视频生成，并显著缩小与扩散模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成主要依赖扩散/flow-matching模型，虽然质量高但计算昂贵、难以扩展；已有自回归方法难以兼顾时空建模与长程一致性。作者希望提供一种更可扩展、高效且时序稳定的自回归替代方案。

Method: 提出VideoAR：1) 通过3D多尺度tokenizer高效编码时空信息；2) 空间-时间解耦：帧内使用VAR建模，帧间采用因果的下一帧预测；3) 为缓解误差累积并提升长期一致性，设计多尺度Temporal RoPE、跨帧误差纠正(Cross-Frame Error Correction)与随机帧遮盖(Random Frame Mask)；4) 多阶段预训练，从低到高分辨率与时长逐步对齐空间与时间学习。

Result: 在UCF-101上FVD从99.5降至88.6，推理步数减少>10倍；在VBench上得分81.74，与体量大一数量级的扩散模型相竞争，刷新自回归类SOTA。

Conclusion: VideoAR证明自回归范式在视频生成中可通过多尺度下一帧预测与时空解耦实现高效、可扩展且时序一致的生成，显著缩小与扩散模型的差距，为后续视频生成研究提供了可扩展基础。

Abstract: Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.

</details>


### [55] [Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation](https://arxiv.org/abs/2601.05981)
*Yinsong Wang,Xinzhe Luo,Siyi Du,Chen Qin*

Main category: cs.CV

TL;DR: 提出AC-CAR：一种对比度无关的可形变多对比度图像配准框架，通过随机卷积对比度增强与自适应条件调制，实现对未见对比度的泛化，并提供对比度无关的不确定性估计，实验优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统配准迭代优化慢；学习式方法虽快但对训练时见到的成像对比度依赖强，跨对比泛化差。多对比度（如MRI不同序列）存在复杂非线性强度映射，需能在未见对比下仍稳健配准并量化不确定性。

Method: 1) 随机卷积对比度增强：在训练中用随机卷积核改变图像外观，模拟多样对比度，逼迫网络学习对比度不变表征；2) 自适应条件特征调制（ACFM）：利用条件信号自适应调制特征并加入对比度不变的潜在正则，使不同对比度的特征一致；3) 对比度无关的不确定性：引入方差网络，复用对比度无关的编码器，输出配准不确定性估计以提升可信度；4) 整体为端到端学习的可形变配准框架。

Result: 在多数据集和未见对比度设置下，AC-CAR在配准精度上优于多种基线（传统与学习法），并在外观分布移位时保持更强的鲁棒性与泛化能力；同时能提供合理的不确定性估计。

Conclusion: 随机卷积增强+ACFM使网络学到对比度不变特征，从而实现对任意对比度的快速、准确、可解释（带不确定性）的配准；该策略提升了跨对比度泛化，是多对比度医学图像配准的有效方案。

Abstract: Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.

</details>


### [56] [Deepfake detectors are DUMB: A benchmark to assess adversarial training robustness under transferability constraints](https://arxiv.org/abs/2601.05986)
*Adrian Serrano,Erwan Umlil,Ronan Thomas*

Main category: cs.CV

TL;DR: 研究在现实条件下对深度伪造检测的对抗鲁棒性：在转移攻击和跨数据集下评估对抗训练是否真正有效。


<details>
  <summary>Details</summary>
Motivation: 实际部署的深伪检测会遭遇对手添加细微扰动，且攻击者常对模型与数据了解有限、分布不匹配；现有防御（如对抗训练）在这类现实限制下的有效性缺乏系统研究。

Method: 将DUMB/DUMBer方法论扩展到深伪检测：在“数据来源、模型结构、类别平衡”维度控制变量，设置转移性受限的攻击与跨数据集配置。从攻击者与防御者视角，系统评测5个SOTA检测器（RECCE、SRM、XCeption、UCF、SPSL），3种攻击（PGD、FGSM、FPBA），2个数据集（FaceForensics++、Celeb-DF-V2），比较不同对抗训练策略在内/外分布条件下的表现。

Result: 对抗训练在同分布（in-distribution）下显著提升鲁棒性；但在跨数据集（分布失配）时，某些对抗训练策略会削弱鲁棒性。转移性受限、数据不匹配情境下的鲁棒表现高度依赖具体策略与配置。

Conclusion: 现实应用中需采用情境感知、数据/模型/平衡联合考虑的防御策略；单纯对抗训练并非通用解，在跨数据集与有限知识攻击下可能适得其反。

Abstract: Deepfake detection systems deployed in real-world environments are subject to adversaries capable of crafting imperceptible perturbations that degrade model performance. While adversarial training is a widely adopted defense, its effectiveness under realistic conditions -- where attackers operate with limited knowledge and mismatched data distributions - remains underexplored. In this work, we extend the DUMB -- Dataset soUrces, Model architecture and Balance - and DUMBer methodology to deepfake detection. We evaluate detectors robustness against adversarial attacks under transferability constraints and cross-dataset configuration to extract real-world insights. Our study spans five state-of-the-art detectors (RECCE, SRM, XCeption, UCF, SPSL), three attacks (PGD, FGSM, FPBA), and two datasets (FaceForensics++ and Celeb-DF-V2). We analyze both attacker and defender perspectives mapping results to mismatch scenarios. Experiments show that adversarial training strategies reinforce robustness in the in-distribution cases but can also degrade it under cross-dataset configuration depending on the strategy adopted. These findings highlight the need for case-aware defense strategies in real-world applications exposed to adversarial attacks.

</details>
