<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model](https://arxiv.org/abs/2509.02659)
*Zilong Guo,Yi Luo,Long Sha,Dongxu Wang,Panqu Wang,Chenyang Xu,Yi Yang*

Main category: cs.CV

TL;DR: 将多模态视觉语言模型（VLM）融入端到端自动驾驶框架，仅用单目相机取得榜单最佳的相机-only性能，显示VLM对端到端驾驶有效。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶多依赖模块化DNN，尚不清楚强大的LLM/VLM能否提升端到端驾驶；验证视觉为主、参数共享的端到端设计结合知识型VLM是否能带来性能与通用性优势。

Method: 构建结合VLM的端到端驾驶架构：以单目相机输入，通过多模态视觉语言模型进行高层语义理解与决策辅助，再由端到端控制头输出驾驶指令；强调“知识型”VLM在感知-推理-控制链路中的作用与联合训练/适配。

Result: 在公开排行榜中取得相机-only方法的最佳成绩，显示在仅用单摄像头情况下也能达到强竞争力；整体性能优于传统模块化DNN端到端方案。

Conclusion: 知识丰富的VLM与端到端设计相结合能显著提升自动驾驶效果；纯视觉方案具有可行性与效率优势，为端到端自动驾驶提供了新的强基线与发展方向。

Abstract: End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.

</details>


### [2] [PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?](https://arxiv.org/abs/2509.02807)
*Mennatullah Siam*

Main category: cs.CV

TL;DR: 提出并检验一个“运动中心”的视频视觉指代/分割评估框架，证明现有基准常可被单帧外观线索破解，并给出新基准MoCentric-Bench与简单适配策略，在像素级时空指代上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频MLLM在问答/描述上进展显著，但像素级视觉定位（分割）方面对“运动-语言”对齐能力缺乏系统检验。许多数据集的运动描述可被单帧外观推断，无需真正的时间推理，导致对模型运动理解的高估。作者希望构建能真正迫使模型利用运动信息（真假运动辨别、顺序理解）的评测与方法。

Method: 1) 系统分析现有基准的缺陷，证明单帧足以完成不少运动指代表达。2) 设计四种“运动中心”的探测（probing）技术，专门用于视觉指代：用于区分真/假运动与理解运动顺序。3) 构建MoCentric-Bench，使语言-视觉对齐必须依赖运动而非静态外观。4) 建立强力的单图像基线，并提出简单的运动中心适配策略（对视频MLLM进行轻量改造/训练），用于像素级分割与时空定位。

Result: 在MoCentric-Bench上，作者的单图像基线已能与或超过以往方法；进一步引入运动中心适配后，在新基准上取得SOTA。实验表明：现有视频MLLM在真实运动理解与顺序把握上存在不足，单帧偏置显著。

Conclusion: 当前视频MLLM的像素级时空指代常被静态外观主导，未充分利用运动。MoCentric-Bench与所提探测与适配方法能更准确评估并提升模型对运动-语言交互的建模，推动更强的密集时空定位与像素级理解。

Abstract: Multi-modal large language models (MLLMs) have shown impressive
generalization across tasks using images and text modalities. While their
extension to video has enabled tasks such as video question answering and video
captioning, their pixel-level visual grounding abilities are less studied. In
this work, we raise the pertinent question of whether motion is used in
pixel-level visual grounding and whether video MLLMs can segment objects based
on natural language expressions describing their motion patterns. We identify
the shortcomings in the current benchmarks, where we show that a single frame
can often suffice for capturing the motion referring expression without any
temporal reasoning. To address this, we introduce four motion-centric probing
techniques, particularly designed for the visual grounding task, to study video
MLLMs' ability to identify true motion from a fake one and their ability to
grasp the motion order. Consequently, we provide a motion-centric benchmark,
MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging
the interaction between motion and language rather than being dominated by
static appearance cues emphasized in existing visual grounding datasets. We
further establish strong single-image baselines that are on par with or
outperform prior methods. Finally, we explore simple motion-centric adaptation
techniques that provide state-of-the-art performance on our MoCentric-Bench.
Our motion-centric benchmark, evaluation and findings challenge future models
to improve dense spatiotemporal grounding and pixel-level understanding within
videos. Code and datasets will be made publicly available at
https://github.com/MSiam/PixFoundation-2.0.git.

</details>


### [3] [Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach](https://arxiv.org/abs/2509.02851)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Colon cancer also known as Colorectal cancer, is one of the most malignant
types of cancer worldwide. Early-stage detection of colon cancer is highly
crucial to prevent its deterioration. This research presents a hybrid
multi-scale deep learning architecture that synergizes capsule networks, graph
attention mechanisms, transformer modules, and residual learning to advance
colon cancer classification on the Lung and Colon Cancer Histopathological
Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the
HG-TNet model that introduces a hybrid architecture that joins strength points
in transformers and convolutional neural networks to capture multi-scale
features in histopathological images. Mainly, a transformer branch extracts
global contextual bonds by partitioning the image into patches by
convolution-based patch embedding and then processing these patches through a
transformer encoder. Analogously, a dedicated CNN branch captures fine-grained,
local details through successive Incorporation these diverse features, combined
with a self-supervised rotation prediction objective, produce a robust
diagnostic representation that surpasses standard architectures in performance.
Results show better performance not only in accuracy or loss function but also
in these algorithms by utilizing capsule networks to preserve spatial orders
and realize how each element individually combines and forms whole structures.

</details>


### [4] [PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis](https://arxiv.org/abs/2509.02898)
*Armin Saadat,Nima Hashemi,Hooman Vaseli,Michael Y. Tsang,Christina Luong,Michiel Van de Panne,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 提出一种利用强化学习进行主动视频采集的框架，在超声心动图用于主动脉瓣狭窄诊断中，仅用约一半视频即可达到约80.6%准确率，提升效率与可及性。


<details>
  <summary>Details</summary>
Motivation: AS 诊断依赖回声心动图，但设备与专家资源有限，尤其在欠发达地区；POCUS 可及但受操作者经验与视图选择限制。需要一种能在资源受限下自动挑选最有信息量视图、减少冗余采集同时维持诊断性能的方法。

Method: 构建一个强化学习驱动的主动特征/视频获取框架：模型在采集过程中逐步决定是否获取新的超声视图或停止，依据当前已观测视频对诊断不确定性与信息增益进行权衡；与固定视图集对比，强调个体化、动态选择最有信息量的视频以优化准确率与效率。

Result: 在 2,572 名患者数据上评估，采用该主动采集策略达到 80.6% 的AS分类准确率，同时仅使用完整检查所需视频的 47%，显著减少采集量与工作量。

Conclusion: 主动视频获取在AS超声评估中可在不显著牺牲性能的前提下提升效率、可扩展性与个体化潜力，有助于在资源受限环境推广；代码已开源（PRECISE-AS）。

Abstract: Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of
the aortic valve, leading to impaired blood flow. Despite its high prevalence,
access to echocardiography (echo), the gold-standard diagnostic tool, is often
limited due to resource constraints, particularly in rural and underserved
areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative
but is restricted by operator expertise and the challenge of selecting the most
relevant imaging views. To address this, we propose a reinforcement learning
(RL)-driven active video acquisition framework that dynamically selects each
patient's most informative echo videos. Unlike traditional methods that rely on
a fixed set of videos, our approach continuously evaluates whether additional
imaging is needed, optimizing both accuracy and efficiency. Tested on data from
2,572 patients, our method achieves 80.6% classification accuracy while using
only 47% of the echo videos compared to a full acquisition. These results
demonstrate the potential of active feature acquisition to enhance AS
diagnosis, making echocardiographic assessments more efficient, scalable, and
personalized. Our source code is available at:
https://github.com/Armin-Saadat/PRECISE-AS.

</details>


### [5] [LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research](https://arxiv.org/abs/2509.02902)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: LiGuard 是一个面向激光雷达（LiDAR）研究与应用的开源框架，提供统一的数据I/O、预/后处理、常用算法与可视化，以及可交互的算法流水线管理，从而加速开发、减少重复造轮子并便于复用与共享。


<details>
  <summary>Details</summary>
Motivation: 当前激光雷达在自动驾驶与智能交通中应用迅速，但研究者常为各自细分任务重复实现相似流程（数据读写、预/后处理、多阶段算法），且数据或算法的小改动会引发大规模代码重构，影响研发效率与复现性。

Method: 提出 LiGuard 框架：1）内置标准化数据I/O、预/后处理与常用算法模块；2）支持交互式增删与重排自定义算法并调参，形成可配置流水线；3）提供分类、检测、分割、跟踪等任务的结果可视化；4）以结构化目录生成项目代码与组件，便于分享与复用。

Result: 通过若干案例研究展示 LiGuard 在多类激光雷达任务中的有效性，表明其可快速搭建流程、灵活调整算法并直观呈现结果，减少重复开发工作量。

Conclusion: LiGuard 能作为通用的LiDAR研究与工程支撑平台，提升开发效率、可复用性与可视化能力，缓解代码碎片化与维护成本高的问题。

Abstract: There is a growing interest in the development of lidar-based autonomous
mobility and Intelligent Transportation Systems (ITS). To operate and research
on lidar data, researchers often develop code specific to application niche.
This approach leads to duplication of efforts across studies that, in many
cases, share multiple methodological steps such as data input/output (I/O),
pre/post processing, and common algorithms in multi-stage solutions. Moreover,
slight changes in data, algorithms, and/or research focus may force major
revisions in the code. To address these challenges, we present LiGuard, an
open-source software framework that allows researchers to: 1) rapidly develop
code for their lidar-based projects by providing built-in support for data I/O,
pre/post processing, and commonly used algorithms, 2) interactively
add/remove/reorder custom algorithms and adjust their parameters, and 3)
visualize results for classification, detection, segmentation, and tracking
tasks. Moreover, because it creates all the code files in structured
directories, it allows easy sharing of entire projects or even the individual
components to be reused by other researchers. The effectiveness of LiGuard is
demonstrated via case studies.

</details>


### [6] [PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2509.02903)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: LiDAR-based perception in intelligent transportation systems (ITS), for tasks
such as object detection, tracking, and semantic and instance segmentation, is
predominantly solved by deep neural network models which often require
large-scale labeled datasets during training to achieve generalization.
However, creating these datasets is costly. time consuming and require human
labor before the datasets are ready for training models. This hinders
scalability of the LiDAR-based perception systems in ITS. Sim2Real learning
offers scalable alternative, however, its effectiveness is dependent on the
fidelity of the source simulation(s) to real-world, in terms of environment
structure, actor dynamics, and sensor emulations. In response, this paper
introduces a rigorous and reproducible methodology for creating large-scale,
high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).
The proposed workflow outlines the steps, tools, and best practices for
digitally replicating real-world environments, encompassing static geometry
modeling, road infrastructure replication, and dynamic traffic scenario
generation. Leveraging open-source and readily available resources such as
satellite imagery and OpenStreetMap data, alongside specific sensor
configurations, this paper provides practical, detailed guidance for
constructing robust synthetic environments. These environments subsequently
facilitate scalable, cost-effective, and diverse dataset generation, forming a
reliable foundation for robust Sim2Real learning.

</details>


### [7] [High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception](https://arxiv.org/abs/2509.02904)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 提出高保真数字孪生(HiFi DT)用于LiDAR感知的Sim2Real迁移，可在仅用仿真数据训练时，在真实数据上超越用真实数据训练的模型4.8%，并通过多种距离度量证明分布对齐提升。


<details>
  <summary>Details</summary>
Motivation: 现有在仿真中训练的LiDAR感知模型因分布偏移在现实中表现欠佳，收集和标注真实数据昂贵且不具可扩展性，需一种可缩小Sim2Real鸿沟、又具成本效益的方案。

Method: 构建高保真数字孪生框架：引入真实世界的背景几何、车道级道路拓扑、传感器规格与安装位姿；形式化Sim2Real域适配问题；给出系统化环境构建流程以生成“域内”的合成数据。使用现成3D检测器仅以HiFi DT合成数据训练，在真实数据上测试；通过CD、MMD、EMD、FD等在原始输入与潜特征层面评估合成—真实分布对齐度。

Result: DT合成数据训练的检测器在真实数据上比等价的“真实数据训练”基线高4.8%；对齐度指标表明HiFi DT显著减小合成与真实的分布差距，并在多样场景上提升泛化。

Conclusion: 高保真数字孪生能有效缩小LiDAR感知的Sim2Real差距，甚至在某些情况下优于直接用真实数据训练；数字孪生是实现面向ITS的可靠、可扩展仿真驱动感知的重要途径。

Abstract: Sim2Real domain transfer offers a cost-effective and scalable approach for
developing LiDAR-based perception (e.g., object detection, tracking,
segmentation) in Intelligent Transportation Systems (ITS). However, perception
models trained in simulation often under perform on real-world data due to
distributional shifts. To address this Sim2Real gap, this paper proposes a
high-fidelity digital twin (HiFi DT) framework that incorporates real-world
background geometry, lane-level road topology, and sensor-specific
specifications and placement. We formalize the domain adaptation challenge
underlying Sim2Real learning and present a systematic method for constructing
simulation environments that yield in-domain synthetic data. An off-the-shelf
3D object detector is trained on HiFi DT-generated synthetic data and evaluated
on real data. Our experiments show that the DT-trained model outperforms the
equivalent model trained on real data by 4.8%. To understand this gain, we
quantify distributional alignment between synthetic and real data using
multiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy
(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both
raw-input and latent-feature levels. Results demonstrate that HiFi DTs
substantially reduce domain shift and improve generalization across diverse
evaluation scenarios. These findings underscore the significant role of digital
twins in enabling reliable, simulation-based LiDAR perception for real-world
ITS applications.

</details>


### [8] [Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach](https://arxiv.org/abs/2509.02918)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.

</details>


### [9] [A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images](https://arxiv.org/abs/2509.02928)
*Zhicheng Tang,Jinwen Tang,Yi Shang*

Main category: cs.CV

TL;DR: 提出DDR-Net，在RetinaNet框架上通过数据驱动的特征层与锚框选择和新采样策略，大幅提升小目标（航拍）检测，少样本下也有效，显著优于RetinaNet及同类方法。


<details>
  <summary>Details</summary>
Motivation: 航拍场景中小目标（鸟类、行人、车辆等）检测对环保监测、城市管理、安防与应急具有关键价值，但现有方法在小目标、数据有限、锚框与特征层选择不匹配时表现受限、训练代价高。

Method: 基于RetinaNet提出DDR-Net：1）数据驱动的特征图选择机制，自动确定最适合小目标的FPN层；2）数据驱动的锚框估计，自适配目标尺度与长宽比分布；3）创新的采样策略，在有限数据情形下提升正负样本质量与训练稳定性；整体保持RetinaNet精度与效率。

Result: 在多种航拍鸟类图像数据集上，DDR-Net在检测小目标上显著优于RetinaNet及其他当代模型（文中宣称更高的精度/召回或mAP），同时降低数据采集与训练时间成本，在小样本条件下仍具竞争力。

Conclusion: DDR-Net以数据驱动的特征与锚框自适应及新采样策略，有效提升航拍小目标检测并减少训练与标注成本，具备在环保、交通、安全、农业与考古等领域的广泛应用潜力。

Abstract: In the realm of aerial imaging, the ability to detect small objects is
pivotal for a myriad of applications, encompassing environmental surveillance,
urban design, and crisis management. Leveraging RetinaNet, this work unveils
DDR-Net: a data-driven, deep-learning model devised to enhance the detection of
diminutive objects. DDR-Net introduces novel, data-driven techniques to
autonomously ascertain optimal feature maps and anchor estimations, cultivating
a tailored and proficient training process while maintaining precision.
Additionally, this paper presents an innovative sampling technique to bolster
model efficacy under limited data training constraints. The model's enhanced
detection capabilities support critical applications including wildlife and
habitat monitoring, traffic flow optimization, and public safety improvements
through accurate identification of small objects like vehicles and pedestrians.
DDR-Net significantly reduces the cost and time required for data collection
and training, offering efficient performance even with limited data. Empirical
assessments over assorted aerial avian imagery datasets demonstrate that
DDR-Net markedly surpasses RetinaNet and alternative contemporary models. These
innovations advance current aerial image analysis technologies and promise
wide-ranging impacts across multiple sectors including agriculture, security,
and archaeology.

</details>


### [10] [STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images](https://arxiv.org/abs/2509.02952)
*Zeyu Liu,Shengwei Ding*

Main category: cs.CV

TL;DR: 提出STAR：一个针对连续切片WSI的快速、稳健、刚性配准开源框架，兼容多染色（H&E、特染、IHC），在ANHIR/ACROBAT数据集上实现分钟级稳定对齐，具备预处理、分层相关、可变核尺度与质控；作为可复现基线，助力临床与大规模成对数据构建。


<details>
  <summary>Details</summary>
Motivation: 现有WSI配准多依赖复杂的可形变或深度学习方法，计算昂贵、复现性差；而在大量连续切片场景中，刚性配准已足够但工具欠缺。需要一个轻量、稳健、易复现的多染色刚性配准框架，降低临床与AI数据构建门槛。

Method: 提出STAR框架：1) 染色条件化预处理；2) 分层粗到细的互相关/相关性搜索策略；3) 自适应核尺度调节以适配不同组织与分辨率；4) 内置质量控制模块；实现多WSI的刚性（旋转/平移/尺度）对齐，适配H&E、特染和IHC等多染色。

Result: 在ANHIR 2019与ACROBAT 2022多器官、多扫描条件数据上，STAR以每张数分钟完成稳定刚性配准，对跨染色差异与部分组织重叠具备鲁棒性；同时提供H&E-IHC对齐、多IHC面板构建与失败模式案例分析。

Conclusion: STAR作为开源、轻量且可复现的刚性WSI配准基线，显著降低临床应用与大规模成对数据准备门槛，为虚拟染色与生物标志物预测等计算病理工作流提供可靠支撑；同时明确其适用范围与局限（如非刚性形变场景）。

Abstract: Registration of serial whole-slide histopathological images (WSIs) is
critical for enabling direct comparison across diverse stains and for preparing
paired datasets in artificial intelligence (AI) workflows such as virtual
staining and biomarker prediction. While existing methods often rely on complex
deformable or deep learning approaches that are computationally intensive and
difficult to reproduce, lightweight rigid frameworks-sufficient for many
consecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial
Tissue Alignment for Rigid registration), a fast and robust open-source
framework for multi-WSI alignment. STAR integrates stain-conditioned
preprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive
kernel scaling, and built-in quality control, achieving reliable rigid
registration across heterogeneous tissue types and staining protocols,
including hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,
PASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).
Evaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs
and scanning conditions, STAR consistently produced stable alignments within
minutes per slide, demonstrating robustness to cross-stain variability and
partial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC
alignment, construction of multi-IHC panels, and typical failure modes,
underscoring both utility and limitations. Released as an open and lightweight
tool, STAR provides a reproducible baseline that lowers the barrier for
clinical adoption and enables large-scale paired data preparation for
next-generation computational pathology.

</details>


### [11] [Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability](https://arxiv.org/abs/2509.02962)
*Shuai Jiang,Yunfeng Ma,Jingyu Zhou,Yuan Bian,Yaonan Wang,Min Liu*

Main category: cs.CV

TL;DR: 提出一种在模态缺失场景下的多模态工业表面缺陷检测方法：通过跨模态提示与对称对比学习，利用文本作为桥接，实现RGB与3D的鲁棒融合；在高缺失率(0.7)下显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 实际工业检测中RGB与3D传感器可能不稳定，常出现单/多模态缺失，导致传统融合模型在模式迁移与信息缺口上表现不佳，鲁棒性不足。作者希望在动态模态缺失下仍保持高检测/定位性能。

Method: 1) 跨模态提示学习：
- 跨模态一致性提示：约束RGB与3D共享一致的语义表征。
- 模态特异提示：为不同输入模式定制适配提示，提升各自表征能力。
- 缺失感知提示：在模态缺失时注入补偿信息，缓解信息空洞。
2) 对称对比学习：以文本为桥，将RGB、3D与成对“正/反”二元文本语义进行三模态对比预训练，促进两视觉模态间可对齐、可替代的表示学习。

Result: 在总缺失率0.7条件下，I-AUROC达73.83%，P-AUROC达93.05%，分别较SOTA提升3.84%与5.58%；对不同缺失类型与缺失率均有稳定优势。

Conclusion: 通过提示驱动的跨模态一致性与文本桥接的对称对比学习，可在极端模态缺失下保持鲁棒的缺陷检测与定位能力，优于现有方法；代码将开源。

Abstract: Multimodal industrial surface defect detection (MISDD) aims to identify and
locate defect in industrial products by fusing RGB and 3D modalities. This
article focuses on modality-missing problems caused by uncertain sensors
availability in MISDD. In this context, the fusion of multiple modalities
encounters several troubles, including learning mode transformation and
information vacancy. To this end, we first propose cross-modal prompt learning,
which includes: i) the cross-modal consistency prompt serves the establishment
of information consistency of dual visual modalities; ii) the modality-specific
prompt is inserted to adapt different input patterns; iii) the missing-aware
prompt is attached to compensate for the information vacancy caused by dynamic
modalities-missing. In addition, we propose symmetric contrastive learning,
which utilizes text modality as a bridge for fusion of dual vision modalities.
Specifically, a paired antithetical text prompt is designed to generate binary
text semantics, and triple-modal contrastive pre-training is offered to
accomplish multimodal learning. Experiment results show that our proposed
method achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7
for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%
respectively), and outperforms existing approaches to varying degrees under
different missing types and rates. The source code will be available at
https://github.com/SvyJ/MISDD-MM.

</details>


### [12] [EdgeAttNet: Towards Barb-Aware Filament Segmentation](https://arxiv.org/abs/2509.02964)
*Victor Solomon,Piet Martens,Jingyu Liu,Rafal Angryk*

Main category: cs.CV

TL;DR: 提出EdgeAttNet：在U-Net瓶颈引入由输入学习得到的边缘图，并线性变换Attention的Q/K以嵌入结构先验，从而更好分割H-alpha日冕暗条及其barbs；在MAGFILO上精度更高、对barbs识别更佳且推理更快，参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有H-alpha细丝分割难以同时捕捉长程依赖与精细边界，尤其对决定CME行为的重要细丝barbs识别不足；需要一种既保留U-Net空间细节又增强长程关系的模型。

Method: 以U-Net为骨架，在瓶颈处加入自注意力模块，并从输入图像端到端学习一个可训练的“边缘图”；用该边缘图线性变换注意力的Key/Query，使注意力更关注边界与barbs；该结构作为显式结构先验，提升空间敏感性且减少可训练参数；整体端到端训练。

Result: 在MAGFILO数据集上，较U-Net与U-Net+Transformer基线取得更高分割精度，barbs识别显著提升；同时推理更快、参数更少，适合部署。

Conclusion: 显式将学习到的边缘先验注入注意力计算能有效提升日面细丝及barbs的分割质量与效率；EdgeAttNet为相关天文影像分割提供了高效、结构感知的方案。

Abstract: Accurate segmentation of solar filaments in H-alpha observations is critical
for determining filament chirality, a key factor in the behavior of Coronal
Mass Ejections (CMEs). However, existing methods often fail to capture
fine-scale filament structures, particularly barbs, due to a limited ability to
model long-range dependencies and spatial detail.
  We propose EdgeAttNet, a segmentation architecture built on a U-Net backbone
by introducing a novel, learnable edge map derived directly from the input
image. This edge map is incorporated into the model by linearly transforming
the attention Key and Query matrices with the edge information, thereby guiding
the self-attention mechanism at the network's bottleneck to more effectively
capture filament boundaries and barbs. By explicitly integrating this
structural prior into the attention computations, EdgeAttNet enhances spatial
sensitivity and segmentation accuracy while reducing the number of trainable
parameters.
  Trained end-to-end, EdgeAttNet outperforms U-Net and other U-Net-based
transformer baselines on the MAGFILO dataset. It achieves higher segmentation
accuracy and significantly better recognition of filament barbs, with faster
inference performance suitable for practical deployment.

</details>


### [13] [KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models](https://arxiv.org/abs/2509.02966)
*Yujin Wang,Tianyi Wang,Quanfeng Liu,Wenxian Fan,Junfeng Jiao,Christian Claudel,Yunbing Yan,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: 提出KEPT：结合检索增强与CoT规划的知识增强VLM，实现前视视频到自车短期轨迹预测，nuScenes上达SOTA并兼顾精度、碰撞率与推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在自动驾驶中缺乏对场景动态与驾驶知识的有效落地，难以在短时轨迹预测中保证准确与安全；需要一种既能理解时空信息又能显式纳入规划约束、并可高效部署的方案。

Method: 1) 模型：KEPT，以前视连续帧直接预测轨迹。2) 表征：TFSF视频编码器，融合时间频率与空间特征，自监督训练+硬负样本挖掘。3) 检索：k-means聚类+HNSW索引，返回场景对齐的示例（Top-K），子毫秒延迟。4) 提示：将检索先验嵌入CoT提示，包含显式规划约束。5) 训练：三阶段微调，使语言头依次对齐：度量空间线索→物理可行运动→带时间条件的前视规划。6) 评估：nuScenes开放环协议（NoAvg、TemAvg+轻量ego状态）。

Result: - NoAvg：平均L2=0.70m，碰撞率=0.21%。- TemAvg（含轻量ego状态）：平均L2=0.31m，碰撞率=0.07%。- 消融：三阶段微调均有增益；Top-2示例在精度-安全间最佳；检索索引达亚毫秒延迟。

Conclusion: 检索增强+CoT引导的VLM能以数据高效、可解释方式提升短期轨迹预测的精度与安全，并具备工程可部署性；KEPT在nuScenes达SOTA，验证该路线的潜力。

Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable
autonomous driving, yet existing vision-language models (VLMs) often fail to
effectively ground their reasoning in scene dynamics and domain knowledge. To
address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM
framework that predicts ego trajectories directly from consecutive front-view
driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video
encoder, trained via self-supervised learning with hard-negative mining, with a
scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.
Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit
planning constraints, while a triple-stage fine-tuning schedule incrementally
aligns the language head to metric spatial cues, physically feasible motion,
and temporally conditioned front-view planning. Evaluated on nuScenes dataset,
KEPT achieves state-of-the-art performance across open-loop protocols: under
NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg
with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision
rate. Ablation studies show that all three fine-tuning stages contribute
complementary benefits, and that using Top-2 retrieved exemplars yields the
best accuracy-safety trade-off. The k-means-clustered HNSW index delivers
sub-millisecond retrieval latency, supporting practical deployment. These
results indicate that retrieval-augmented, CoT-guided VLMs offer a promising,
data-efficient pathway toward interpretable and trustworthy autonomous driving.

</details>


### [14] [VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results](https://arxiv.org/abs/2509.02969)
*Dasong Li,Sizhuo Ma,Hang Hua,Wenjie Li,Jian Wang,Chris Wei Zhou,Fengbin Guan,Xin Li,Zihao Yu,Yiting Lu,Ru-Ling Liao,Yan Ye,Zhibo Chen,Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai,Erjia Xiao,Lingfeng Zhang,Zhenjie Su,Hao Cheng,Yu Liu,Renjing Xu,Long Chen,Xiaoshuai Hao,Zhenpeng Zeng,Jianqin Wu,Xuxu Wang,Qian Yu,Bo Hu,Weiwei Wang,Pinxin Liu,Yunlong Tang,Luchuan Song,Jinxi He,Jiaru Wu,Hanjia Lyu*

Main category: cs.CV

TL;DR: VQualA 2025 挑战赛摘要：面向短视频的多模态用户参与度（热度）预测，提供含真实交互指标的新UGC数据集，鼓励鲁棒建模；97名参赛者、15份有效测试提交，推动该领域进展。


<details>
  <summary>Details</summary>
Motivation: 短视频UGC在社交平台爆发，影响力与传播效果与“参与度”密切相关，但其受多因素（视觉、音频、创作者元数据等）影响且噪声大、分布偏移强，缺少统一数据与评测框架来系统建模与比较方法，因此需要一个标准化挑战与数据集促进研究。

Method: 组织ICCV 2025配套挑战：构建新的短视频UGC数据集，提供视觉、音频、创作者元数据等多模态特征，并以真实用户交互衍生的参与度指标作为标签；开放评测基准与提交通道，鼓励选手探索多模态融合与鲁棒建模策略。

Result: 吸引97名参与者、收到15份有效测试提交；参赛方法覆盖多模态特征利用与融合，在数据与评测驱动下推动参与度预测性能与实践策略的积累。

Conclusion: 该挑战为短视频参与度预测提供了标准数据与评测平台，验证了多模态建模的价值并促进社区交流与进步，为后续在真实场景中的UGC热度预测研究奠定基础。

Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement
Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge
focuses on understanding and modeling the popularity of user-generated content
(UGC) short videos on social media platforms. To support this goal, the
challenge uses a new short-form UGC dataset featuring engagement metrics
derived from real-world user interactions. This objective of the Challenge is
to promote robust modeling strategies that capture the complex factors
influencing user engagement. Participants explored a variety of multi-modal
features, including visual content, audio, and metadata provided by creators.
The challenge attracted 97 participants and received 15 valid test submissions,
contributing significantly to progress in short-form UGC video engagement
prediction.

</details>


### [15] [InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System](https://arxiv.org/abs/2509.02973)
*Xianbao Hou,Yonghao He,Zeyd Boukhers,John See,Hu Su,Wei Sui,Cong Yang*

Main category: cs.CV

TL;DR: 提出InstaDA：一个无需训练的双代理系统（文本代理与图像代理）协同LLM与扩散模型，为实例分割数据集做自动增广；在LVIS上较基线显著提升AP，并优于SOTA DiverGen。


<details>
  <summary>Details</summary>
Motivation: 实例分割标注昂贵且类别长尾严重；现有Copy-Paste+扩散方法多未充分联动LLM与扩散模型，且未有效挖掘/复用已有训练数据的丰富信息与分布。

Method: Dual-Agent、training-free：
- T-Agent（文本代理）：让LLM与扩散模型闭环协作。提出Prompt Rethink机制：依据生成图像迭代反思并优化文本提示，使后续生成更符合多样性与任务需求，提高图像利用率与提示质量。
- I-Agent（图像代理）：以训练图像为条件生成新实例，扩展总体数据分布并缓解类不均衡。
两代理均独立、自动化运行，便于实际部署。

Result: 在LVIS 1.0 val：相较基线，box AP +4.0、mask AP +3.3；相较SOTA DiverGen，box AP +0.3、mask AP +0.1；在常见类box AP +0.7，mask AP常见类+0.2、频繁类+0.5。

Conclusion: InstaDA通过LLM-扩散深度协作与训练图像条件生成，显著提升实例分割数据增广效果，兼顾实用性（训练免、自动化）与性能，并在LVIS上超越现有最佳方法。

Abstract: Acquiring high-quality instance segmentation data is challenging due to the
labor-intensive nature of the annotation process and significant class
imbalances within datasets. Recent studies have utilized the integration of
Copy-Paste and diffusion models to create more diverse datasets. However, these
studies often lack deep collaboration between large language models (LLMs) and
diffusion models, and underutilize the rich information within the existing
training data. To address these limitations, we propose InstaDA, a novel,
training-free Dual-Agent system designed to augment instance segmentation
datasets. First, we introduce a Text-Agent (T-Agent) that enhances data
diversity through collaboration between LLMs and diffusion models. This agent
features a novel Prompt Rethink mechanism, which iteratively refines prompts
based on the generated images. This process not only fosters collaboration but
also increases image utilization and optimizes the prompts themselves.
Additionally, we present an Image-Agent (I-Agent) aimed at enriching the
overall data distribution. This agent augments the training set by generating
new instances conditioned on the training images. To ensure practicality and
efficiency, both agents operate as independent and automated workflows,
enhancing usability. Experiments conducted on the LVIS 1.0 validation set
indicate that InstaDA achieves significant improvements, with an increase of
+4.0 in box average precision (AP) and +3.3 in mask AP compared to the
baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in
box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common
categories and mask AP gains of +0.2 on common categories and +0.5 on frequent
categories.

</details>


### [16] [SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation](https://arxiv.org/abs/2509.02993)
*Chao Fan,Xibin Jia,Anqi Xiao,Hongyuan Yu,Zhenghan Yang,Dawei Yang,Hui Xu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of
medical objects using only a few labeled images. Prototype-based methods have
made significant progress in addressing FSMIS. However, they typically generate
a single global prototype for the support image to match with the query image,
overlooking intra-class variations. To address this issue, we propose a
Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce
a Multi-level Prototype Generation (MPG) module, which enables
multi-granularity measurement between the support and query images by
simultaneously generating a global prototype and an adaptive number of local
prototypes. Additionally, we observe that not all local prototypes in the
support image are beneficial for matching, especially when there are
substantial discrepancies between the support and query images. To alleviate
this issue, we propose a Query-guided Local Prototype Enhancement (QLPE)
module, which adaptively refines support prototypes by incorporating guidance
from the query image, thus mitigating the negative effects of such
discrepancies. Extensive experiments on three public medical datasets
demonstrate that SPENet outperforms existing state-of-the-art methods,
achieving superior performance.

</details>


### [17] [SOPSeg: Prompt-based Small Object Instance Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2509.03002)
*Chenhao Wang,Yingrui Ji,Yu Meng,Yunjian Zhang,Yao Zhu*

Main category: cs.CV

TL;DR: 提出SOPSeg：面向遥感小目标实例分割的提示式框架，通过区域自适应放大与边缘引导解码，结合面向OBB的提示机制，大幅提升小目标分割精度，并发布基于SODA-A构建的小目标实例分割数据集。


<details>
  <summary>Details</summary>
Motivation: 遥感领域对小目标抽取在城市、环境与灾害等应用中关键，但现有研究重检测轻分割，缺乏专门数据集；像素级标注代价高且技术难，通用模型SAM在小目标上因1/16特征分辨率过粗导致细节丢失、性能明显退化。需要一种既提升小目标分割质量又能降低数据构建成本的方法。

Method: 1) 区域自适应放大（region-adaptive magnification）：针对候选/提示区域动态放大以保留细粒度细节；2) 定制解码器：融合边缘预测并进行逐步细化（progressive refinement），强化边界刻画能力；3) 新型提示机制：面向遥感常用的旋转/定向边界框（OBB）设计提示，使提示与对象几何形态更匹配；4) 以提示式框架兼容零样本/少样本设置，利于高效数据构建。

Result: SOPSeg在遥感小目标实例分割上优于现有方法（包含SAM等零样本/提示模型），在边界与细节保真度上有明显提升，并显著提升数据构建效率。

Conclusion: SOPSeg有效弥补小目标实例分割研究与数据缺口，通过面向OBB的提示、区域放大与边缘细化实现更精准的小目标分割；作者基于SODA-A构建并将开源小目标实例分割数据集与模型，促进后续研究。

Abstract: Extracting small objects from remote sensing imagery plays a vital role in
various applications, including urban planning, environmental monitoring, and
disaster management. While current research primarily focuses on small object
detection, instance segmentation for small objects remains underexplored, with
no dedicated datasets available. This gap stems from the technical challenges
and high costs of pixel-level annotation for small objects. While the Segment
Anything Model (SAM) demonstrates impressive zero-shot generalization, its
performance on small-object segmentation deteriorates significantly, largely
due to the coarse 1/16 feature resolution that causes severe loss of fine
spatial details. To this end, we propose SOPSeg, a prompt-based framework
specifically designed for small object segmentation in remote sensing imagery.
It incorporates a region-adaptive magnification strategy to preserve
fine-grained details, and employs a customized decoder that integrates edge
prediction and progressive refinement for accurate boundary delineation.
Moreover, we introduce a novel prompting mechanism tailored to the oriented
bounding boxes widely adopted in remote sensing applications. SOPSeg
outperforms existing methods in small object segmentation and facilitates
efficient dataset construction for remote sensing tasks. We further construct a
comprehensive small object instance segmentation dataset based on SODA-A, and
will release both the model and dataset to support future research.

</details>


### [18] [Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers](https://arxiv.org/abs/2509.03006)
*Tzuhsuan Huang,Cheng Yu Yeo,Tsai-Ling Huang,Hong-Han Shuai,Wen-Huang Cheng,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 论文研究后处理深度数字水印，提出在训练阶段引入“攻击网络集成(ensemble attack network)”以提升鲁棒性。通过在时域(空间域)用CNN、在频域用Transformer的组合模拟多样攻击，显著提升多种基线方法在WAVES基准上的平均比特准确率，尤其对再生成攻击(Regeneration Attack)使StegaStamp提升18.743%。


<details>
  <summary>Details</summary>
Motivation: 现有工作多为生成中嵌入(in-processing)的水印，受限于需访问模型内部、难以适配不同生成器，也难以为每张图像定制水印。后处理(post-processing)更灵活，但对各种现实攻击(压缩、噪声、裁剪、再生成等)鲁棒性不足。因此作者希望在不依赖生成模型细节的前提下，系统提升后处理水印的抗攻击能力。

Method: 采用对抗式训练思想：在训练水印嵌入/提取网络时，引入一个可学习的“攻击网络集成”，由多种结构与域的攻击子网组成，用以近似/覆盖真实世界中的破坏性操作。具体地：1) 空间域(时域)使用CNN型攻击器，2) 频域使用Transformer型攻击器，并探究不同组合(空间/频域×CNN/Transformer)对鲁棒性的影响。最终发现“空间域-CNN + 频域-Transformer”的组合最优。训练时水印模型需在该集成攻击后仍能正确解码，从而学到鲁棒表征。

Result: 在WAVES基准上，以平均比特准确率为指标，所提方法在多种压力测试下显著优于基线后处理水印方法。尤其在WAVES定义的再生成攻击(Regeneration Attack)上，使StegaStamp提升18.743%的平均比特准确率。

Conclusion: 后处理水印可通过在训练中引入跨域、异构架构的攻击网络集成显著提升鲁棒性。最佳组合为“空间域CNN + 频域Transformer”。方法对多类攻击均有效，并在标准基准上系统性提升，代码已开源。

Abstract: Recent studies on deep watermarking have predominantly focused on
in-processing watermarking, which integrates the watermarking process into
image generation. However, post-processing watermarking, which embeds
watermarks after image generation, offers more flexibility. It can be applied
to outputs from any generative model (e.g. GANs, diffusion models) without
needing access to the model's internal structure. It also allows users to embed
unique watermarks into individual images. Therefore, this study focuses on
post-processing watermarking and enhances its robustness by incorporating an
ensemble attack network during training. We construct various versions of
attack networks using CNN and Transformer in both spatial and frequency domains
to investigate how each combination influences the robustness of the
watermarking model. Our results demonstrate that combining a CNN-based attack
network in the spatial domain with a Transformer-based attack network in the
frequency domain yields the highest robustness in watermarking models.
Extensive evaluation on the WAVES benchmark, using average bit accuracy as the
metric, demonstrates that our ensemble attack network significantly enhances
the robustness of baseline watermarking methods under various stress tests. In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%. The code is released
at:https://github.com/aiiu-lab/DeepRobustWatermark.

</details>


### [19] [Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations](https://arxiv.org/abs/2509.03011)
*Alexis Ivan Lopez Escamilla,Gilberto Ochoa,Sharib Al*

Main category: cs.CV

TL;DR: 提出用于溃疡性结肠炎（UC）的病灶感知图像描述模型：将ResNet特征、Grad-CAM热力图、CBAM注意力与T5解码器融合，并用临床元数据作为自然语言提示，生成结构化描述与MES分类/病灶标签，优于基线。


<details>
  <summary>Details</summary>
Motivation: 内镜报告常缺乏标准化与可解释性；传统图像字幕模型对病灶关注不足，且难融合临床量表（如MES）与病灶属性，导致描述与分级不一致、可用性差。需要一个既能聚焦病灶、又能输出临床对齐的可解释描述与分类的系统。

Method: 多模态融合：1) 视觉主干用ResNet抽取特征；2) Grad-CAM生成病灶关注热图；3) 通过CBAM增强通道与空间注意力；4) 文本端用T5解码器生成描述；5) 将临床元数据（MES 0-3、血管纹理、出血、红斑、脆性、溃疡）转为自然语言prompt注入解码器；6) 输出结构化描述，同时给出MES分类与病灶标签。

Result: 与基线模型相比，图像描述质量与MES分类准确率均提升；生成的文本与临床实践更一致，具可解释性（病灶热图与注意力对齐）。

Conclusion: 病灶感知与临床提示融合的图像字幕框架能改进UC内镜报告的可靠性与可解释性，提供结构化描述与准确MES分级，优于现有基线，适合临床对齐的自动化报告场景。

Abstract: We present a lesion-aware image captioning framework for ulcerative colitis
(UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and
CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3,
vascular pattern, bleeding, erythema, friability, ulceration) is injected as
natural-language prompts to guide caption generation. The system produces
structured, interpretable descriptions aligned with clinical practice and
provides MES classification and lesion tags. Compared with baselines, our
approach improves caption quality and MES classification accuracy, supporting
reliable endoscopic reporting.

</details>


### [20] [Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens](https://arxiv.org/abs/2509.03025)
*Sohee Kim,Soohyun Ryu,Joonhyung Park,Eunho Yang*

Main category: cs.CV

TL;DR: 论文发现并利用LVLM内部“看不见”的神经元信号，来检测并纠正文本概念是否未被图像支撑，从而减少把文本误当作图像内容的错误。


<details>
  <summary>Details</summary>
Motivation: LVLM常把纯文本中提到但图像中不存在的概念误判为图像中存在，导致幻觉式回答。需要一种可解释且可迁移的方法在模型内部识别并抑制这类“视觉缺失”误判。

Method: 在模型FFN层中识别一组“视觉缺失感知”(VA)神经元，它们在文本概念缺乏图像证据时呈现特定激活模式。基于该模式训练/构建一个检测模块，逐token判断该文本是否被视觉证据支撑；在生成时用该检测指导：1) 重新诠释问题提示；或2) 对被判定为缺失的token进行替换/干预，从而修正输出。

Result: 大量实验表明：该检测与干预策略显著降低LVLM将文本误认为图像内容的倾向；方法对多种LVLM具有良好的泛化性。

Conclusion: LVLM内部存在可分离的VA神经元信号，可用于判定文本是否有视觉落地；利用该信号的检测与生成时干预能有效缓解视觉幻觉，并可跨模型泛化。

Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses
by jointly interpreting visual and textual inputs. However, our finding reveals
they often mistakenly perceive text inputs lacking visual evidence as being
part of the image, leading to erroneous responses. In light of this finding, we
probe whether LVLMs possess an internal capability to determine if textual
concepts are grounded in the image, and discover a specific subset of
Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons,
that consistently signal the visual absence through a distinctive activation
pattern. Leveraging these patterns, we develop a detection module that
systematically classifies whether an input token is visually grounded. Guided
by its prediction, we propose a method to refine the outputs by reinterpreting
question prompts or replacing the detected absent tokens during generation.
Extensive experiments show that our method effectively mitigates the models'
tendency to falsely presume the visual presence of text input and its
generality across various LVLMs.

</details>


### [21] [Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification](https://arxiv.org/abs/2509.03032)
*Kaicong Huang,Talha Azfar,Jack M. Reilly,Thomas Guggisberg,Ruimin Ke*

Main category: cs.CV

TL;DR: 提出一种双分支跨模态ReID框架，同时建模前景与背景，并通过语义内对齐与语义间对抗提升判别性，在多数据集上达到或超过SOTA。


<details>
  <summary>Details</summary>
Motivation: 视觉单模态方法需昂贵标注且对遮挡敏感；CLIP等多模态方案虽引入语义先验，但只关注前景，忽略背景的有用信息与抑制需求。受人类感知启发，作者认为背景语义同样关键：理解并抑制背景干扰有助于突出身份相关特征。

Method: 构建前景-背景双分支跨模态特征提取管线：图像与文本（语义提示）分别在前景支路与背景支路中建模。提出两种学习策略：1) 语义内（intra-semantic）对齐：将同一语义域内的视觉-文本特征进行对齐，分别对前景与背景做跨模态匹配；2) 语义间（inter-semantic）对抗：对前景与背景特征施加相似度惩罚/对抗损失，鼓励它们彼此可分、减少混淆，从而抑制噪声背景、强化身份相关区域注意。整体端到端训练。

Result: 在两个全身（holistic）与两个遮挡（occluded）ReID基准上进行全面实验，性能达到或超过当前SOTA，体现方法的有效性与通用性。

Conclusion: 联合建模前景与背景并在跨模态空间中进行内对齐与间对抗，能显著提升ReID对复杂背景与遮挡的鲁棒性，减少对昂贵标注的依赖，并提供可泛化的性能提升。

Abstract: Person re-identification faces two core challenges: precisely locating the
foreground target while suppressing background noise and extracting
fine-grained features from the target region. Numerous visual-only approaches
address these issues by partitioning an image and applying attention modules,
yet they rely on costly manual annotations and struggle with complex
occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic
cues to guide visual understanding. However, they focus solely on foreground
information, but overlook the potential value of background cues. Inspired by
human perception, we argue that background semantics are as important as the
foreground semantics in ReID, as humans tend to eliminate background
distractions while focusing on target appearance. Therefore, this paper
proposes an end-to-end framework that jointly models foreground and background
information within a dual-branch cross-modal feature extraction pipeline. To
help the network distinguish between the two domains, we propose an
intra-semantic alignment and inter-semantic adversarial learning strategy.
Specifically, we align visual and textual features that share the same
semantics across domains, while simultaneously penalizing similarity between
foreground and background features to enhance the network's discriminative
power. This strategy drives the model to actively suppress noisy background
regions and enhance attention toward identity-relevant foreground cues.
Comprehensive experiments on two holistic and two occluded ReID benchmarks
demonstrate the effectiveness and generality of the proposed method, with
results that match or surpass those of current state-of-the-art approaches.

</details>


### [22] [MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model](https://arxiv.org/abs/2509.03041)
*Pengyang Yu,Haoquan Wang,Gerard Marks,Tahar Kechadi,Laurence T. Yang,Sahraoui Dhelim,Nyothiri Aung*

Main category: cs.CV

TL;DR: 提出MedLiteNet：轻量级CNN-Transformer混合模型，用于皮肤镜图像分割，在小样本下以更低计算量实现高精度与清晰边界。


<details>
  <summary>Details</summary>
Motivation: 现有CNN受限于感受野，难以捕获长程依赖；ViT虽能建模全局上下文，但计算复杂度高、参数大，不适合皮肤科常见的小样本医疗数据。需要一种既能高效获取全局/多尺度信息又计算友好的分割模型。

Method: 设计MedLiteNet编码器：1) 采用深度可分离的Mobile Inverted Bottleneck（MBConv）层级式堆叠以降低计算；2) 在瓶颈层引入跨尺度token-mixing单元，促进不同分辨率间的信息交互与多尺度上下文聚合；3) 融合边界感知自注意力模块，显式强化病灶边缘与轮廓细节；整体是CNN与Transformer的轻量混合结构。

Result: 在皮肤镜分割任务中实现高精度与更清晰的病灶边界，同时显著降低参数规模与计算复杂度，适配小样本场景。（摘要未给出具体数字）

Conclusion: 轻量级混合架构与跨尺度/边界增强机制可在小样本皮肤镜分割中兼顾全局建模与效率，缓解CNN感受野不足和纯Transformer高复杂度的问题。

Abstract: Accurate skin-lesion segmentation remains a key technical challenge for
computer-aided diagnosis of skin cancer. Convolutional neural networks, while
effective, are constrained by limited receptive fields and thus struggle to
model long-range dependencies. Vision Transformers capture global context, yet
their quadratic complexity and large parameter budgets hinder use on the
small-sample medical datasets common in dermatology. We introduce the
MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic
segmentation that achieves high precision through hierarchical feature
extraction and multi-scale context aggregation. The encoder stacks depth-wise
Mobile Inverted Bottleneck blocks to curb computation, inserts a
bottleneck-level cross-scale token-mixing unit to exchange information between
resolutions, and embeds a boundary-aware self-attention module to sharpen
lesion contours.

</details>


### [23] [DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks](https://arxiv.org/abs/2509.03044)
*Chengjie Huang,Jiafeng Yan,Jing Li,Lu Bai*

Main category: cs.CV

TL;DR: 提出“动态条件双扩散桥”训练范式，用以在病态多任务（如去雾与可见-红外融合）中解耦扩散与条件生成，并引入随时间演化的动态条件，提升学习稳定性与多任务关联建模，达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有条件扩散多依赖静态条件与监督数据，难以在数据稀缺且任务相关性复杂的病态多任务场景中有效学习。构造数据分布路径的方式限制了任务间固有相关性的利用；静态条件无法适应多任务学习中随时间/噪声步进而变化的特性。

Method: 提出动态条件双扩散桥（DCDB）训练范式：1) 解耦扩散过程与条件生成过程，减弱对监督数据的依赖；2) 利用相同噪声进度表生成“动态条件”，随时间步逐步调整其统计特征，内嵌时间相关信息，降低网络学习难度；3) 在单步去噪下推导不同条件形式的学习目标，并通过网络注意力权重分析对比动态条件的优势。框架通用于多任务，尤其病态任务。

Result: 在去雾与可见-红外图像融合两类病态多任务公共数据集上，多项指标取得最佳性能（SOTA）；注意力分析显示动态条件带来更合理的关注分布与收敛特性。代码已开源。

Conclusion: 动态条件+双扩散桥的解耦式训练能够更好利用任务间关联、缓解监督与数据不足问题，并在病态多任务上显著优于基线；该范式具备通用性，可推广至其他多任务视觉问题。

Abstract: Conditional diffusion models have made impressive progress in the field of
image processing, but the characteristics of constructing data distribution
pathways make it difficult to exploit the intrinsic correlation between tasks
in multi-task scenarios, which is even worse in ill-posed tasks with a lack of
training data. In addition, traditional static condition control makes it
difficult for networks to learn in multi-task scenarios with its dynamically
evolving characteristics. To address these challenges, we propose a dynamic
conditional double diffusion bridge training paradigm to build a general
framework for ill-posed multi-tasks. Firstly, this paradigm decouples the
diffusion and condition generation processes, avoiding the dependence of the
diffusion model on supervised data in ill-posed tasks. Secondly, generated by
the same noise schedule, dynamic conditions are used to gradually adjust their
statistical characteristics, naturally embed time-related information, and
reduce the difficulty of network learning. We analyze the learning objectives
of the network under different conditional forms in the single-step denoising
process and compare the changes in its attention weights in the network,
demonstrating the superiority of our dynamic conditions. Taking dehazing and
visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve
the best performance in multiple indicators on public datasets. The code has
been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.

</details>


### [24] [Isolated Bangla Handwritten Character Classification using Transfer Learning](https://arxiv.org/abs/2509.03061)
*Abdul Karim,S M Rafiuddin,Jahidul Islam Razin,Tahira Alam*

Main category: cs.CV

TL;DR: 提出一种结合3DCNN、ResNet、MobileNet并利用迁移学习的端到端模型，对Bangla手写字符（含基础与复合，共84类）进行分类，在Bangla Lekha Isolated数据集上达到训练99.82%、测试99.46%的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Bangla文字具有50个基本字符与大量复合字符，形态复杂、类间相似度高，给手写体识别带来挑战。现有Bangla字符识别虽有进展，但在复合字符覆盖、端到端统一建模以及优化稳定性（如梯度消失）方面仍存在不足，亟需更高精度且鲁棒的模型。

Method: 采用迁移学习框架，整合深度神经网络组件：3D卷积用于更丰富的特征提取（可能包含通道/形态变化的体素式建模），ResNet用于缓解梯度消失并加深网络，MobileNet引入轻量化与深度可分离卷积以提升效率。在Bangla Lekha Isolated（166,105样本、84类）上进行端到端训练与评估，并与多种SOTA基线比较。

Result: 在该数据集上取得训练集99.82%、测试集99.46%的分类准确率；对比现有Bangla手写字符识别基线显示性能更优。

Conclusion: 多模型融合与迁移学习的端到端方案能有效识别Bangla基础与复合手写字符，并在标准数据集上刷新精度，证明其在复杂文字脚本识别中的有效性；同时通过ResNet缓解梯度消失、MobileNet提升效率。

Abstract: Bangla language consists of fifty distinct characters and many compound
characters. Several notable studies have been performed to recognize Bangla
characters, both handwritten and optical. Our approach uses transfer learning
to classify the basic, distinct, as well as compound Bangla handwritten
characters while avoiding the vanishing gradient problem. Deep Neural Network
techniques such as 3D Convolutional Neural Network (3DCNN), Residual Neural
Network (ResNet), and MobileNet are applied to generate an end-to-end
classification of all possible standard formations of handwritten characters in
the Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105
Bangla character image samples categorized into 84 distinct classes, is used
for this classification model. The model achieved 99.82% accuracy on training
data and 99.46% accuracy on test data. Comparisons with various
state-of-the-art benchmarks of Bangla handwritten character classification show
that the proposed model achieves better accuracy in classifying the data.

</details>


### [25] [High Cursive Complex Character Recognition using GAN External Classifier](https://arxiv.org/abs/2509.03062)
*S M Rafiuddin*

Main category: cs.CV

TL;DR: 提出一种结合外部分类器与GAN的ADA-GAN，用对抗生成的手写体样本（经判别器置信度阈值筛选并加入对抗扰动）进行数据增广，对复杂/连笔手写字符分类更鲁棒，优于常规CNN，且CNN随字符复杂度上升准确率明显下降。


<details>
  <summary>Details</summary>
Motivation: 手写字符尤其连笔、复杂字体的形状变化大、连笔多、噪声高，传统或标准CNN在复杂度提高时准确率显著下降，亟需一种对复杂与连笔形态更鲁棒的分类与增广方法。

Method: 构建一个包含外部分类器与GAN的框架：生成器产生伪手写字符图像；对生成样本施加对抗性扰动；用判别器评估样本并以置信度阈值筛选高质量样本；将通过筛选的样本用于扩充训练集；最终用外部分类器进行分类。整体方法称为ADA-GAN。

Result: 实验表明：随着字符复杂度增加，标准CNN分类准确率下降明显；而ADA-GAN在连笔与复杂字符上保持更高、更加稳定的准确率，展现出鲁棒性优势。

Conclusion: 通过对抗生成与置信度筛选驱动的数据增广策略，ADA-GAN能有效缓解复杂与连笔手写字符导致的性能退化问题，相比常规CNN更稳健、效果更好。

Abstract: Handwritten characters can be trickier to classify due to their complex and
cursive nature compared to simple and non-cursive characters. We present an
external classifier along with a Generative Adversarial Network that can
classify highly cursive and complex characters. The generator network produces
fake handwritten character images, which are then used to augment the training
data after adding adversarially perturbed noise and achieving a confidence
score above a threshold with the discriminator network. The results show that
the accuracy of convolutional neural networks decreases as character complexity
increases, but our proposed model, ADA-GAN, remains more robust and effective
for both cursive and complex characters.

</details>


### [26] [TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis](https://arxiv.org/abs/2509.03095)
*Clément Hervé,Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.CV

TL;DR: 把通用3D生成模型（TRELLIS）的表面特征迁移到医学场景，用于动脉瘤的分类、分割和血流预测，显著提升准确率/F1/分割质量，并将仿真误差降至约15%。


<details>
  <summary>Details</summary>
Motivation: 医学动脉瘤任务受限于稀缺的高质量标注3D数据，传统几何特征（点法向/网格描述符）表达能力有限。通用大规模3D生成模型在非医疗数据上学到的几何潜在表示可能具备可迁移性，可用于弥补医学数据不足、提升下游性能。

Method: 提出跨域特征迁移：用TRELLIS（在大规模非医疗3D数据上训练的生成模型）提取的表面几何嵌入替换标准几何输入（法向/描述符），并将其接入三类下游网络：1) Intra3D数据集上动脉瘤/健康血管二分类；2) 3D网格上的动脉瘤与血管区域分割；3) 在AnXplore数据集上用图神经网络预测随时间变化的血流场。比较包含/不包含TRELLIS特征的模型表现。

Result: 引入TRELLIS表面特征在三项任务上均显著优于SOTA基线：分类准确率与F1提升、分割质量提升；在血流时变预测中，仿真误差降低约15%。

Conclusion: 从通用3D生成模型迁移几何表示到医学任务可行且有效，能在有限医学标注数据下系统提升性能，显示出广泛的跨域3D表示迁移潜力。

Abstract: Intracranial aneurysms pose a significant clinical risk yet are difficult to
detect, delineate and model due to limited annotated 3D data. We propose a
cross-domain feature-transfer approach that leverages the latent geometric
embeddings learned by TRELLIS, a generative model trained on large-scale
non-medical 3D datasets, to augment neural networks for aneurysm analysis. By
replacing conventional point normals or mesh descriptors with TRELLIS surface
features, we systematically enhance three downstream tasks: (i) classifying
aneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting
aneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving
blood-flow fields using a graph neural network on the AnXplore dataset. Our
experiments show that the inclusion of these features yields strong gains in
accuracy, F1-score and segmentation quality over state-of-the-art baselines,
and reduces simulation error by 15\%. These results illustrate the broader
potential of transferring 3D representations from general-purpose generative
models to specialized medical tasks.

</details>


### [27] [Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods](https://arxiv.org/abs/2509.03108)
*Shota Iwamatsu,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出一种针对人脸活体检测的隐蔽后门投毒攻击：把伪造人脸的特征嵌入到真实人脸图像中、外观几乎不变，从而让特定欺骗样本在训练后被误判为“活体”，实验证明现有方法易受此威胁。


<details>
  <summary>Details</summary>
Motivation: 活体检测常用深度学习且依赖大量数据，若训练集被注入恶意样本，模型可能对某类攻击产生系统性误判；作者要揭示并量化这种在面部反欺骗场景中的后门风险。

Method: 设计一种后门投毒流程：从目标伪造人脸提取特征，嵌入到真实人脸图像，使图像外观几乎不变但携带“触发”特征；将这些带触发的样本以“活体”标签混入训练集，训练后模型学到当出现该特征时输出“活体”，从而让相应伪造攻击在推理阶段绕过检测。

Result: 在公开数据集上验证：带有嵌入特征的伪造样本能稳定逃避主流活体检测模型，显著提高目标攻击的通过率，显示现有系统对该类后门投毒缺乏鲁棒性。

Conclusion: 面部反欺骗系统存在现实可行的后门投毒面；即便视觉不可察觉的特征注入也能导致严重误判。需引入数据与训练阶段的防护（数据清洗、后门检测、鲁棒训练等）以缓解风险。

Abstract: Face recognition systems are robust against environmental changes and noise,
and thus may be vulnerable to illegal authentication attempts using user face
photos, such as spoofing attacks. To prevent such spoofing attacks, it is
crucial to discriminate whether the input image is a live user image or a
spoofed image prior to the face recognition process. Most existing spoofing
attack detection methods utilize deep learning, which necessitates a
substantial amount of training data. Consequently, if malicious data is
injected into a portion of the training dataset, a specific spoofing attack may
be erroneously classified as live, leading to false positives.In this paper, we
propose a novel backdoor poisoning attack method to demonstrate the latent
threat of backdoor poisoning within face anti-spoofing detection. The proposed
method enables certain spoofing attacks to bypass detection by embedding
features extracted from the spoofing attack's face image into a live face image
without inducing any perceptible visual alterations.Through experiments
conducted on public datasets, we demonstrate that the proposed method
constitutes a realistic threat to existing spoofing attack detection systems.

</details>


### [28] [Information transmission: Inferring change area from change moment in time series remote sensing images](https://arxiv.org/abs/2509.03112)
*Jialu Li,Chen Wu,Meiqi Hu*

Main category: cs.CV

TL;DR: 提出CAIM-Net，通过“先判时后判域”的一体化框架，在时间序列遥感中同时定位变化时刻并推断变化区域，保证二者一致性；使用差异特征提取/增强、粗到细的时刻提取，以及基于多尺度时域CAM的区域推断，提升精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常将变化区域检测与变化时刻识别割裂处理，导致结果不一致、互相制约信息未充分利用。由于知道变化时刻可推断发生变化的像素集合，作者动机是构建一个利用时间-空间内在联系、由“时刻→区域”推理的一体化模型。

Method: 提出CAIM-Net，包括三步：1）差异提取与增强：采用批维堆叠的轻量编码器快速提取时间序列差异特征，并用边界增强卷积强化边界与变化信号；2）粗变化时刻提取：对增强差异特征做时空相关分析，利用两种不同策略估计粗变化时刻；3）细变化时刻提取与区域推断：引入多尺度时间域CAM，提升粗候选时刻的权重，获得细粒度时刻；再依据“发生变化的像素在该时刻必有响应”的原理，将加权的变化时刻映射为变化区域。

Result: 在抽象中未给出具体数据，但声称通过上述流程可快速且鲁棒地抽取差异，稳定定位变化时刻，并由时刻推断区域，从而提升时空一致性与总体性能。

Conclusion: CAIM-Net实现了从变化时刻到变化区域的因果式推断，统一了时间与空间的变化检测，缓解了任务割裂带来的不一致问题；关键在于轻量差异编码、边界增强、粗到细时刻定位与多尺度时间CAM融合。

Abstract: Time series change detection is a critical task for exploring ecosystem
dynamics using time series remote sensing images, because it can simultaneously
indicate where and when change occur. While deep learning has shown excellent
performance in this domain, it continues to approach change area detection and
change moment identification as distinct tasks. Given that change area can be
inferred from change moment, we propose a time series change detection network,
named CAIM-Net (Change Area Inference from Moment Network), to ensure
consistency between change area and change moment results. CAIM-Net infers
change area from change moment based on the intrinsic relationship between time
series analysis and spatial change detection. The CAIM-Net comprises three key
steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,
and Fine Change Moment Extraction and Change Area Inference. In the Difference
Extraction and Enhancement, a lightweight encoder with batch dimension stacking
is designed to rapidly extract difference features. Subsequently, boundary
enhancement convolution is applied to amplify these difference features. In the
Coarse Change Moment Extraction, the enhanced difference features from the
first step are used to spatiotemporal correlation analysis, and then two
distinct methods are employed to determine coarse change moments. In the Fine
Change Moment Extraction and Change Area Inference, a multiscale temporal Class
Activation Mapping (CAM) module first increases the weight of the
change-occurring moment from coarse change moments. Then the weighted change
moment is used to infer change area based on the fact that pixels with the
change moment must have undergone a change.

</details>


### [29] [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
*Shan Wang,Maying Shen,Nadine Chang,Chuong Nguyen,Hongdong Li,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 提出一种基于梯度自反思的影响估计与对比解码方法，在不额外训练和资源的前提下同时缓解多模态大模型的文本-视觉偏置与共现偏置，显著降低幻觉（LLaVA-QA90准确率最高提升92%）。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型易产生幻觉，核心成因包括：1）文本-视觉偏置（过度依赖文本/历史输出）；2）共现偏置（训练数据中的对象共现统计）。现有方法多靠启发式、固定策略，忽视实例间偏置强度的差异，且常需额外模型或微调，代价高、泛化差。

Method: 1）梯度自反思：基于梯度贡献估计不同类型token（视觉、提示词、先前输出）对当前决策的影响度。2）对象视觉token识别：利用影响度定位与目标对象相关的视觉token。3）影响感知的对比解码：在解码时根据各token影响度构建对比目标，抑制过强的文本/共现信号，强化与对象相关的视觉证据；无需额外数据、微调或辅助模型。

Result: 在多项基准上显著降低幻觉，特别是在LLaVA-QA90上准确率最高提升92%；相比现有启发式或需要额外资源的方法，在稳定性与开销上更优。

Conclusion: 通过精细化估计不同信息源对生成的影响，并将其纳入对比解码，可自适应地按实例强度缓解文本-视觉与共现双重偏置，无需额外训练即可有效抑制多模态幻觉。

Abstract: Hallucinations in multimodal large language model are caused by the
text-visual bias and the co-occurrence bias. The former reflects an
over-reliance on text information in the decision-making process, while the
latter arises from the statistical object-pairing patterns abstracted from the
training data. Existing mitigation methods heuristically address these biases
without understanding the fluctuating bias level across the instances. We first
propose estimating the influence of respective token types (visual, prompt, and
previous outputs) using a gradient-based self-reflection method. The estimated
token influence further enables the detection of object-related visual tokens
and their integration into an influence-aware contrastive decoding framework to
mitigate both types of biases simultaneously. Our method operates without the
need for additional resources, such as costly fine-tuning, extra models, or
data statistics. Extensive experiments show it effectively reduces
hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.

</details>


### [30] [Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge](https://arxiv.org/abs/2509.03114)
*Miao Xu,Xiangyu Zhu,Xusheng Liang,Zidu Wang,Jinlin Wu,Zhen Lei*

Main category: cs.CV

TL;DR: 提出Gravity-Field Based Diffusion Bridge(GravityDB)，将手-物体交互建模为引力吸引过程，生成无穿模、稳定抓握且含真实手部形变的交互；并用文本语义引导引力场，提升交互的语义合理性。实验在多数据集验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有重建或手-物体位姿估计只能给出粗交互状态，复杂几何导致穿插或接触间隙；真实手表面在交互中会发生显著形变，既难捕捉也难表示。需要能同时避免穿模、保证稳定接触/抓握，并刻画手部形变的方法，并最好具备语义可控性。

Method: 将手-物体互动视为“吸引驱动”的生成过程：构造基于“重力场”的势能/力场，使手部表面（可形变）在扩散桥（Diffusion Bridge）框架下逐步演化至与刚体物体形成物理可行、接触合理的状态。通过场的设计避免穿插、鼓励稳定接触并允许手面形变；同时引入文本语义信息调节引力场的分布，选择更语义相关的接触区域。

Result: 在多个数据集上定性与定量实验显示，相比现有方法，本法显著减少穿插与空隙，提升抓握稳定性，并更好地复现实际手部形变；在相关指标上取得优于或具竞争力的成绩。

Conclusion: GravityDB通过引力场+扩散桥的建模，提供了物理合理、语义可控且可变形的手-物体交互生成方案，克服穿模与接触不稳等问题；为更真实的交互重建与估计提供新途径。

Abstract: Existing reconstruction or hand-object pose estimation methods are capable of
producing coarse interaction states. However, due to the complex and diverse
geometry of both human hands and objects, these approaches often suffer from
interpenetration or leave noticeable gaps in regions that are supposed to be in
contact. Moreover, the surface of a real human hand undergoes non-negligible
deformations during interaction, which are difficult to capture and represent
with previous methods. To tackle these challenges, we formulate hand-object
interaction as an attraction-driven process and propose a Gravity-Field Based
Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand
surface and rigid objects. Our approach effectively resolves the aforementioned
issues by generating physically plausible interactions that are free of
interpenetration, ensure stable grasping, and capture realistic hand
deformations. Furthermore, we incorporate semantic information from textual
descriptions to guide the construction of the gravitational field, enabling
more semantically meaningful interaction regions. Extensive qualitative and
quantitative experiments on multiple datasets demonstrate the effectiveness of
our method.

</details>


### [31] [Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation](https://arxiv.org/abs/2509.03141)
*Mattia Litrico,Francesco Guarnera,Mario Valerio Giuffrida,Daniele Ravì,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 提出TADM-3D：一个融合时间感知的3D扩散模型，用于从基线MRI生成随访MRI，显式建模时间间隔与结构变化，并通过“回溯正则”提升时间一致性；在OASIS-3训练、NACC外部验证。


<details>
  <summary>Details</summary>
Motivation: 临床需要能生成真实、面向未来的脑MRI以评估预后与疾病进展。但现有方法：1) 难以显式建模时间间隔与脑结构变化（特别在年龄分布不均数据上）；2) 多为时间点间插值，临床意义有限；3) 依赖2D切片，忽略3D解剖上下文，影响纵向预测准确性。

Method: 提出3D Temporally-Aware Diffusion Model (TADM-3D)。关键设计：1) 时间感知：引入预训练的脑龄估计器(BAE)作为条件/指导信号，使扩散过程生成与基线到随访的年龄差一致的MRI。2) Back-In-Time Regularisation (BITR)：在训练中进行双向预测（前向：基线→随访；反向：随访→基线），以约束模型的时间一致性与可逆性，从而提升对时间变化的建模。3) 全3D体数据建模，捕获完整解剖上下文。

Result: 在OASIS-3上训练与评估，外部NACC数据上验证泛化；结果显示较现有方法在纵向预测的真实性、时间一致性与临床相关性指标上取得更优表现（摘要未给具体数值）。

Conclusion: TADM-3D能更准确地生成反映未来进展的3D脑MRI；通过BAE引导与BITR正则显式建模时间与结构变化关系，提升时间一致性与泛化能力；方法具有跨数据集的可迁移性，代码将公开。

Abstract: Generating realistic MRIs to accurately predict future changes in the
structure of brain is an invaluable tool for clinicians in assessing clinical
outcomes and analysing the disease progression at the patient level. However,
current existing methods present some limitations: (i) some approaches fail to
explicitly capture the relationship between structural changes and time
intervals, especially when trained on age-imbalanced datasets; (ii) others rely
only on scan interpolation, which lack clinical utility, as they generate
intermediate images between timepoints rather than future pathological
progression; and (iii) most approaches rely on 2D slice-based architectures,
thereby disregarding full 3D anatomical context, which is essential for
accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion
Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To
better model the relationship between time interval and brain changes, TADM-3D
uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in
the generation of MRIs that accurately reflect the expected age difference
between baseline and generated follow-up scans. Additionally, to further
improve the temporal awareness of TADM-3D, we propose the Back-In-Time
Regularisation (BITR), by training TADM-3D to predict bidirectionally from the
baseline to follow-up (forward), as well as from the follow-up to baseline
(backward). Although predicting past scans has limited clinical applications,
this regularisation helps the model generate temporally more accurate scans. We
train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the
generalisation performance on an external test set from the NACC dataset. The
code will be available upon acceptance.

</details>


### [32] [Preserving instance continuity and length in segmentation through connectivity-aware loss computation](https://arxiv.org/abs/2509.03154)
*Karol Szustakowski,Luk Frank,Julia Esser,Jan Gründemann,Marie Piraud*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: In many biomedical segmentation tasks, the preservation of elongated
structure continuity and length is more important than voxel-wise accuracy. We
propose two novel loss functions, Negative Centerline Loss and Simplified
Topology Loss, that, applied to Convolutional Neural Networks (CNNs), help
preserve connectivity of output instances. Moreover, we discuss characteristics
of experiment design, such as downscaling and spacing correction, that help
obtain continuous segmentation masks. We evaluate our approach on a 3D
light-sheet fluorescence microscopy dataset of axon initial segments (AIS), a
task prone to discontinuity due to signal dropout. Compared to standard CNNs
and existing topology-aware losses, our methods reduce the number of
segmentation discontinuities per instance, particularly in regions with missing
input signal, resulting in improved instance length calculation in downstream
applications. Our findings demonstrate that structural priors embedded in the
loss design can significantly enhance the reliability of segmentation for
biological applications.

</details>


### [33] [Count2Density: Crowd Density Estimation without Location-level Annotations](https://arxiv.org/abs/2509.03170)
*Mattia Litrico,Feng Chen,Michael Pound,Sotirios A Tsaftaris,Sebastiano Battiato,Mario Valerio Giuffrida*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Crowd density estimation is a well-known computer vision task aimed at
estimating the density distribution of people in an image. The main challenge
in this domain is the reliance on fine-grained location-level annotations,
(i.e. points placed on top of each individual) to train deep networks.
Collecting such detailed annotations is both tedious, time-consuming, and poses
a significant barrier to scalability for real-world applications. To alleviate
this burden, we present Count2Density: a novel pipeline designed to predict
meaningful density maps containing quantitative spatial information using only
count-level annotations (i.e., total number of people) during training. To
achieve this, Count2Density generates pseudo-density maps leveraging past
predictions stored in a Historical Map Bank, thereby reducing confirmation
bias. This bank is initialised using an unsupervised saliency estimator to
provide an initial spatial prior and is iteratively updated with an EMA of
predicted density maps. These pseudo-density maps are obtained by sampling
locations from estimated crowd areas using a hypergeometric distribution, with
the number of samplings determined by the count-level annotations. To further
enhance the spatial awareness of the model, we add a self-supervised
contrastive spatial regulariser to encourage similar feature representations
within crowded regions while maximising dissimilarity with background regions.
Experimental results demonstrate that our approach significantly outperforms
cross-domain adaptation methods and achieves better results than recent
state-of-the-art approaches in semi-supervised settings across several
datasets. Additional analyses validate the effectiveness of each individual
component of our pipeline, confirming the ability of Count2Density to
effectively retrieve spatial information from count-level annotations and
enabling accurate subregion counting.

</details>


### [34] [AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain](https://arxiv.org/abs/2509.03179)
*Alma M. Liezenga,Stefan Wijnja,Puck de Haan,Niels W. T. Brink,Jip J. van Stijn,Yori Kamphuis,Klamer Schutte*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Poisoning attacks pose an increasing threat to the security and robustness of
Artificial Intelligence systems in the military domain. The widespread use of
open-source datasets and pretrained models exacerbates this risk. Despite the
severity of this threat, there is limited research on the application and
detection of poisoning attacks on object detection systems. This is especially
problematic in the military domain, where attacks can have grave consequences.
In this work, we both investigate the effect of poisoning attacks on military
object detectors in practice, and the best approach to detect these attacks. To
support this research, we create a small, custom dataset featuring military
vehicles: MilCivVeh. We explore the vulnerability of military object detectors
for poisoning attacks by implementing a modified version of the BadDet attack:
a patch-based poisoning attack. We then assess its impact, finding that while a
positive attack success rate is achievable, it requires a substantial portion
of the data to be poisoned -- raising questions about its practical
applicability. To address the detection challenge, we test both specialized
poisoning detection methods and anomaly detection methods from the visual
industrial inspection domain. Since our research shows that both classes of
methods are lacking, we introduce our own patch detection method: AutoDetect, a
simple, fast, and lightweight autoencoder-based method. Our method shows
promising results in separating clean from poisoned samples using the
reconstruction error of image slices, outperforming existing methods, while
being less time- and memory-intensive. We urge that the availability of large,
representative datasets in the military domain is a prerequisite to further
evaluate risks of poisoning attacks and opportunities patch detection.

</details>


### [35] [PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising](https://arxiv.org/abs/2509.03185)
*Debopom Sutradhar,Ripon Kumar Debnath,Mohaimenul Azam Khan Raiaan,Yan Zhang,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出PPORLD-EDNetLDCT：用PPO强化学习驱动的编码器-解码器式低剂量CT去噪框架，基于图像质量反馈实时优化去噪策略，在多个数据集上较传统与DL方法显著提升PSNR/SSIM，并在COVID-19分类任务上提高准确率。


<details>
  <summary>Details</summary>
Motivation: LDCT为降低辐射剂量必然带来噪声增多与细节退化；传统迭代或监督学习去噪常出现过平滑、细节丢失或泛化差。需要一种可依据图像质量动态自适应、在不同噪声水平与结构细节间权衡的去噪策略。

Method: 构建RL+编码器-解码器的去噪框架：以PPO为核心的后验策略优化，在自定义Gym环境中训练；代理根据图像质量反馈（奖励）动态调整去噪策略/参数，实现“在线”或实时优化。网络骨干为EDNet（Encoder–Decoder），RL控制去噪过程或模块选择/强度；以PSNR/SSIM/RMSE等指标指导学习。

Result: 在自有低剂量CT图像与投影数据集上取得PSNR 41.87、SSIM 0.9814、RMSE 0.00236；在NIH-AAPM-Mayo挑战数据集上PSNR 41.52、SSIM 0.9723、RMSE 0.0051。与传统和其他DL去噪方法相比均有优势。进一步在COVID-19 LDCT分类任务中，使用该方法处理的图像使分类准确率提升至94%，较无RL去噪提高4%。

Conclusion: PPO驱动的自适应去噪策略与ED架构结合，可在保持细节的同时显著降噪，并在下游诊断任务中转化为可观的性能增益；方法为更安全（低剂量）且高质量的CT成像提供了可行路径。

Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation
exposure, but it often leads to increased noise and reduced image quality.
Traditional denoising methods, such as iterative optimization or supervised
learning, often fail to preserve image quality. To address these challenges, we
introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with
Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in
which an advanced posterior policy optimization (PPO) algorithm is used to
optimize denoising policies in real time, based on image quality feedback,
trained via a custom gym environment. The experimental results on the low dose
CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT
model outperforms traditional denoising techniques and other DL-based methods,
achieving a peak signal-to-noise ratio of 41.87, a structural similarity index
measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in
NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of
41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality
of denoising using a classification task in the COVID-19 LDCT dataset, where
the images processed by our method improved the classification accuracy to
94\%, achieving 4\% higher accuracy compared to denoising without RL-based
denoising. This method offers a promising solution for safer and more accurate
LDCT imaging.

</details>


### [36] [AIVA: An AI-based Virtual Companion for Emotion-aware Interaction](https://arxiv.org/abs/2509.03212)
*Chenxi Li*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved
natural language understanding and generation, enhancing Human-Computer
Interaction (HCI). However, LLMs are limited to unimodal text processing and
lack the ability to interpret emotional cues from non-verbal signals, hindering
more immersive and empathetic interactions. This work explores integrating
multimodal sentiment perception into LLMs to create emotion-aware agents. We
propose \ours, an AI-based virtual companion that captures multimodal sentiment
cues, enabling emotionally aligned and animated HCI. \ours introduces a
Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion
transformer and supervised contrastive learning to provide emotional cues.
Additionally, we develop an emotion-aware prompt engineering strategy for
generating empathetic responses and integrate a Text-to-Speech (TTS) system and
animated avatar module for expressive interactions. \ours provides a framework
for emotion-aware agents with applications in companion robotics, social care,
mental health, and human-centered AI.

</details>


### [37] [RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion](https://arxiv.org/abs/2509.03214)
*Junhao Jia,Yifei Sun,Yunyou Liu,Cheng Yang,Changmiao Wang,Feiwei Qin,Yong Peng,Wenwen Min*

Main category: cs.CV

TL;DR: 提出RTGMFF框架：把ROI级别fMRI信息自动转成文本并与频率-空间视觉特征融合，通过语义对齐实现更强诊断性能；在ADHD-200与ABIDE上显著提升准确率、灵敏度、特异性与AUC。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI诊断面临三难题：低信噪比与被试差异大；主流CNN/Transformer对频率信息感知不足；缺少文本注释来解释区域激活与连接。作者希望用“可生成的ROI文本+频率空间融合+跨模态对齐”提升鲁棒诊断并增强可解释性。

Method: RTGMFF包含三模块：1) ROI驱动文本生成：将激活、功能连接、年龄、性别等压缩为确定性文本token，避免随机性；2) 混合频率-空间编码：波レット-mamba（层级小波+状态空间模型）分支捕获频域结构，跨尺度Transformer捕获长程空间依赖；3) 自适应语义对齐：将ROI文本序列与视觉特征映射到共享嵌入，用带正则的余弦相似损使两模态对齐、缩小模态差距。

Result: 在ADHD-200与ABIDE基准上，相比SOTA方法取得更高诊断准确率，并在灵敏度、特异性与AUC上都有显著提升（具体数值未在摘要中给出）。

Conclusion: 将确定性ROI文本与频率-空间多模态融合结合的RTGMFF，在低信噪与个体差异场景下更稳健，既提升诊断性能也增加可解释性；该框架可推广到其他脑影像任务。

Abstract: Functional magnetic resonance imaging (fMRI) is a powerful tool for probing
brain function, yet reliable clinical diagnosis is hampered by low
signal-to-noise ratios, inter-subject variability, and the limited frequency
awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI
datasets lack textual annotations that could contextualize regional activation
and connectivity patterns. We introduce RTGMFF, a framework that unifies
automatic ROI-level text generation with multimodal feature fusion for
brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven
fMRI text generation deterministically condenses each subject's activation,
connectivity, age, and sex into reproducible text tokens; (ii) Hybrid
frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a
cross-scale Transformer encoder to capture frequency-domain structure alongside
long-range spatial dependencies; and (iii) Adaptive semantic alignment module
embeds the ROI token sequence and visual features in a shared space, using a
regularized cosine-similarity loss to narrow the modality gap. Extensive
experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses
current methods in diagnostic accuracy, achieving notable gains in sensitivity,
specificity, and area under the ROC curve. Code is available at
https://github.com/BeistMedAI/RTGMFF.

</details>


### [38] [LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking](https://arxiv.org/abs/2509.03221)
*Jing Zhang,Siying Tao,Jiao Li,Tianhe Wang,Junchen Wu,Ruqian Hao,Xiaohui Du,Ruirong Tan,Rui Li*

Main category: cs.CV

TL;DR: 提出LGBP-OrgaNet：一种无需荧光标记的自动类器官分割与跟踪方法，结合CNN与Transformer，并用可学习高斯带通融合与双向交叉融合解码，多尺度融合提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统荧光标记会破坏类器官结构，且手工分析效率低、可重复性差；需要一种非破坏、自动化且高精度的分割与跟踪方法来支持肿瘤研究与药筛等应用。

Method: 构建深度学习框架LGBP-OrgaNet：双分支（CNN与Transformer）提取互补特征；设计Learnable Gaussian Band Pass Fusion(可学习高斯带通融合)模块在编码阶段融合两分支信息；在解码端引入Bidirectional Cross Fusion Block实现多尺度特征双向交叉融合，并通过渐进式拼接与上采样完成重建，同时实现分割、跟踪与定量分析。

Result: 在类器官分割数据集（称为SROrga）上取得“令人满意”的分割精度与鲁棒性；在跟踪与定量任务上表现稳定（摘要未给出具体指标）。

Conclusion: LGBP-OrgaNet实现了对类器官的无损、自动化高精度分割与跟踪，多尺度与跨分支融合设计有效；为类器官研究（如肿瘤治疗与药物筛选）提供实用工具。

Abstract: Organoids replicate organ structure and function, playing a crucial role in
fields such as tumor treatment and drug screening. Their shape and size can
indicate their developmental status, but traditional fluorescence labeling
methods risk compromising their structure. Therefore, this paper proposes an
automated, non-destructive approach to organoid segmentation and tracking. We
introduced the LGBP-OrgaNet, a deep learning-based system proficient in
accurately segmenting, tracking, and quantifying organoids. The model leverages
complementary information extracted from CNN and Transformer modules and
introduces the innovative feature fusion module, Learnable Gaussian Band Pass
Fusion, to merge data from two branches. Additionally, in the decoder, the
model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,
and finally completes the decoding through progressive concatenation and
upsampling. SROrga demonstrates satisfactory segmentation accuracy and
robustness on organoids segmentation datasets, providing a potent tool for
organoid research.

</details>


### [39] [PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR](https://arxiv.org/abs/2509.03262)
*Fabio F. Oberweger,Michael Schwingshackl,Vanessa Staderini*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We present PI3DETR, an end-to-end framework that directly predicts 3D
parametric curve instances from raw point clouds, avoiding the intermediate
representations and multi-stage processing common in prior work. Extending
3DETR, our model introduces a geometry-aware matching strategy and specialized
loss functions that enable unified detection of differently parameterized curve
types, including cubic B\'ezier curves, line segments, circles, and arcs, in a
single forward pass. Optional post-processing steps further refine predictions
without adding complexity. This streamlined design improves robustness to noise
and varying sampling densities, addressing critical challenges in real world
LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC
dataset and generalizes effectively to real sensor data, offering a simple yet
powerful solution for 3D edge and curve estimation.

</details>


### [40] [SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model](https://arxiv.org/abs/2509.03267)
*Hongxu Yang,Edina Timko,Levente Lippenszky,Vanda Czipczer,Lehel Ferenczi*

Main category: cs.CV

TL;DR: 提出SynBT：一种用于乳腺MRI的3D掩码条件扩散生成框架，先用patch-to-volume自编码器把大FOV高分辨率体数据压缩到潜空间，再在潜空间里根据肿瘤掩码合成逼真的肿瘤；合成数据用于训练常见分割网络，在大数据集上Dice提升约2–3%。


<details>
  <summary>Details</summary>
Motivation: 现有合成肿瘤方法多基于小patch，难以在大视野（FOV）的MRI中生成大体积肿瘤，导致对分割任务的增益有限。需要一种既能处理高分辨率大体数据、又能控制位置与形状的高质量肿瘤合成方法，以更好提升分割性能。

Method: 提出SynBT：1) 设计patch-to-volume自编码器，将高分辨率3D MRI压缩到紧凑潜表示，同时保持大FOV体素分辨率信息；2) 在该潜空间上训练掩码条件扩散模型，根据选定乳腺组织区域与肿瘤掩码生成逼真的肿瘤信号；3) 将生成结果解码回图像空间，构造合成带肿瘤的MRI用于下游分割模型训练/增强。

Result: 在大型公共乳腺对比增强MRI数据集上，使用合成肿瘤数据增强后，常见分割模型的Dice提升约2–3%，显示生成肿瘤质量高且对分割有实质帮助。

Conclusion: 面向大FOV乳腺MRI的SynBT能够高质量、可控地合成乳腺肿瘤，作为数据增强显著提升分割性能；方法适合处理大体积3D医学图像，优于基于小patch的传统生成策略。

Abstract: Synthetic tumors in medical images offer controllable characteristics that
facilitate the training of machine learning models, leading to an improved
segmentation performance. However, the existing methods of tumor synthesis
yield suboptimal performances when tumor occupies a large spatial volume, such
as breast tumor segmentation in MRI with a large field-of-view (FOV), while
commonly used tumor generation methods are based on small patches. In this
paper, we propose a 3D medical diffusion model, called SynBT, to generate
high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed
model consists of a patch-to-volume autoencoder, which is able to compress the
high-resolution MRIs into compact latent space, while preserving the resolution
of volumes with large FOV. Using the obtained latent space feature vector, a
mask-conditioned diffusion model is used to synthesize breast tumors within
selected regions of breast tissue, resulting in realistic tumor appearances. We
evaluated the proposed method for a tumor segmentation task, which demonstrated
the proposed high-quality tumor synthesis method can facilitate the common
segmentation models with performance improvement of 2-3% Dice Score on a large
public dataset, and therefore provides benefits for tumor segmentation in MRI
images.

</details>


### [41] [PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection](https://arxiv.org/abs/2509.03277)
*Qihang Zhou,Shibo He,Jiangtao Yan,Wenchao Meng,Jiming Chen*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization
capabilities to identify 3D anomalies across unseen objects of highly diverse
class semantics. To this end, we propose a unified framework to comprehensively
detect and segment 3D anomalies by leveraging both point- and pixel-level
information. We first design PointAD, which leverages point-pixel
correspondence to represent 3D anomalies through their associated rendering
pixel representations. This approach is referred to as implicit 3D
representation, as it focuses solely on rendering pixel anomalies but neglects
the inherent spatial relationships within point clouds. Then, we propose
PointAD+ to further broaden the interpretation of 3D anomalies by introducing
explicit 3D representation, emphasizing spatial abnormality to uncover abnormal
spatial relationships. Hence, we propose G-aggregation to involve geometry
information to enable the aggregated point representations spatially aware. To
simultaneously capture rendering and spatial abnormality, PointAD+ proposes
hierarchical representation learning, incorporating implicit and explicit
anomaly semantics into hierarchical text prompts: rendering prompts for the
rendering layer and geometry prompts for the geometry layer. A cross-hierarchy
contrastive alignment is further introduced to promote the interaction between
the rendering and geometry layers, facilitating mutual anomaly learning.
Finally, PointAD+ integrates anomaly semantics from both layers to capture the
generalized anomaly semantics. During the test, PointAD+ can integrate RGB
information in a plug-and-play manner and further improve its detection
performance. Extensive experiments demonstrate the superiority of PointAD+ in
ZS 3D anomaly detection across unseen objects with highly diverse class
semantics, achieving a holistic understanding of abnormality.

</details>


### [42] [Empowering Lightweight MLLMs with Reasoning via Long CoT SFT](https://arxiv.org/abs/2509.03321)
*Linyu Ou*

Main category: cs.CV

TL;DR: 探究<7B轻量级多模态LLM>的推理增强：先用长链式思维(CoT)做有监督微调大幅提升，其后再做可验证奖励的RL还能进一步增益；结论是“长CoT的SFT是轻量级MLLM推理能力的前置关键步骤”。


<details>
  <summary>Details</summary>
Motivation: 现有“可验证奖励的强化学习(RLVR/RLAIF等)”已提升大模型推理，但对小参数多模态模型(≤7B)是否同样有效尚不明确；同时，长CoT数据对小模型是否必要、与RL的先后关系与叠加收益也缺少系统验证。

Method: - 选取<7B的MLLM作为基座；
- 构建/收集含长链式思维标注的多模态推理数据集；
- 阶段一：仅用长CoT进行SFT，评估对推理任务的影响；
- 阶段二：在SFT后再进行RL（基于可验证奖励的信号）以进一步优化；
- 在多项多模态推理基准上对比：无CoT、短CoT、长CoT、仅RL、SFT→RL等方案。

Result: - 长CoT的SFT显著提升轻量级MLLM的推理性能，优于无CoT或短CoT；
- 在完成长CoT SFT后，再进行RL可带来额外但次级的增益；
- 若跳过SFT直接RL，收益有限或不稳定。

Conclusion: 对轻量级MLLM，建立推理能力的关键路径是：先进行基于长CoT的SFT以打牢“过程式推理”表征，再叠加RL做细化；长CoT SFT是必要前置条件，RL是锦上添花。

Abstract: While Reinforcement Learning with Verifiable Rewards has enhanced the
reasoning of large-scale language models (LLMs), its efficacy for lightweight
multimodal language models (MLLMs) with fewer than seven billion parameters
remains underexplored. This paper investigates the role of long
Chain-of-Thought (long CoT) data in enhancing the reasoning abilities of such
MLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT
data significantly improves MLLM reasoning. Furthermore, we observe that after
this initial SFT phase, MLLMs can achieve additional performance gains through
a subsequent RL stage. We conclude that a SFT stage with long CoT data is a
critical prerequisite for developing the reasoning capabilities of lightweight
MLLMs.

</details>


### [43] [Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions](https://arxiv.org/abs/2509.03323)
*Xizhe Zhang,Jiayang Zhu*

Main category: cs.CV

TL;DR: 提出一种混合CNN-Transformer的星形胶质细胞检测器，利用热图引导的查询锚点与轻量Transformer做全局上下文建模，在ALDH1L1与GFAP染色数据上优于Faster R-CNN、YOLOv11与DETR，FROC显示更高灵敏度且更少误检。


<details>
  <summary>Details</summary>
Motivation: 星形胶质细胞在多种神经疾病中形态和密度改变显著，但其分支结构复杂、染色差异大，导致传统或端到端检测方法在小体积、低对比度、密集聚集区域上易漏检和误检，亟需一种既能抓住局部细节又能利用全局上下文的鲁棒检测方案。

Method: 1) 采用混合架构：CNN负责局部细节与纹理提取，轻量Transformer负责长程依赖与全局上下文推理；2) 热图引导的查询机制：通过生成空间热图来定位潜在小而弱的细胞，作为Transformer查询的空间锚点，从而提升对微弱目标的关注；3) 在推理与训练中融合密集区域判别能力与小目标召回；4) 在ALDH1L1与GFAP两种染色情况的数据集上进行评测。

Result: 相较Faster R-CNN、YOLOv11、DETR，该方法在两种染色数据上均取得更高灵敏度与更少假阳性；FROC曲线与指标验证其在不同阈值/工作点下的稳定优势，尤其在小目标与密集聚类场景。

Conclusion: 混合CNN-Transformer加热图引导查询能够有效提升星形胶质细胞的自动检测鲁棒性，适用于染色差异与结构复杂场景；该框架为计算病理学中更高级别的细胞分析与下游定量研究提供可靠基础。

Abstract: Astrocytes are critical glial cells whose altered morphology and density are
hallmarks of many neurological disorders. However, their intricate branching
and stain dependent variability make automated detection of histological images
a highly challenging task. To address these challenges, we propose a hybrid CNN
Transformer detector that combines local feature extraction with global
contextual reasoning. A heatmap guided query mechanism generates spatially
grounded anchors for small and faint astrocytes, while a lightweight
Transformer module improves discrimination in dense clusters. Evaluated on
ALDH1L1 and GFAP stained astrocyte datasets, the model consistently
outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with
fewer false positives, as confirmed by FROC analysis. These results highlight
the potential of hybrid CNN Transformer architectures for robust astrocyte
detection and provide a foundation for advanced computational pathology tools.

</details>


### [44] [InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds](https://arxiv.org/abs/2509.03324)
*Yixiong Jing,Cheng Zhang,Haibing Wu,Guangming Wang,Olaf Wysocki,Brian Sheil*

Main category: cs.CV

TL;DR: InfraDiffusion提出一种零样本流程：将砌体点云投影成深度图，用改造的DDNM进行恢复增强，再用SAM做砖块级分割，在低光环境下显著提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 低光环境（如隧道）难以获取高分辨率RGB图像，传统砖级缺陷分割依赖图像；点云虽不受光照影响，但稀疏、无结构且噪声大，难以做精细分割。因此需要一种不依赖额外训练、能把点云变得“可分割”的方法。

Method: 1) 用虚拟相机将砌体点云投影为多视角深度图；2) 采用并改造DDNM（扩散去噪零空间模型）对深度图进行恢复与增强，提升视觉清晰度与几何一致性；3) 在零样本设定下，将增强后的深度图输入SAM进行砖级（如掉块、砂浆流失）分割；无需任务特定训练。

Result: 在砌体桥梁与隧道点云数据集上，增强后的深度图用于SAM取得显著更好的砖级分割表现（定量与可视化均提升），证明该流程有效。

Conclusion: 通过将点云投影+扩散恢复，再配合通用分割模型，可在无任务训练下实现细粒度砖级分割，适合低光基础设施巡检；方法泛化性强，具备实用潜力与开源可复现性。

Abstract: Point clouds are widely used for infrastructure monitoring by providing
geometric information, where segmentation is required for downstream tasks such
as defect detection. Existing research has automated semantic segmentation of
structural components, while brick-level segmentation (identifying defects such
as spalling and mortar loss) has been primarily conducted from RGB images.
However, acquiring high-resolution images is impractical in low-light
environments like masonry tunnels. Point clouds, though robust to dim lighting,
are typically unstructured, sparse, and noisy, limiting fine-grained
segmentation. We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific
training, InfraDiffusion enhances visual clarity and geometric consistency of
depth maps. Experiments on masonry bridge and tunnel point cloud datasets show
significant improvements in brick-level segmentation using the Segment Anything
Model (SAM), underscoring its potential for automated inspection of masonry
assets. Our code and data is available at
https://github.com/Jingyixiong/InfraDiffusion-official-implement.

</details>


### [45] [Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing](https://arxiv.org/abs/2509.03376)
*Hui Chen,Liangyu Liu,Xianchao Xiu,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出T-CAGU：用Transformer抓全局、内容自适应图神经网络保局部，并融合多阶传播与图残差，提升高光谱解混的精度与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习HU方法难以兼顾远程依赖与局部边界一致性，且对噪声与图结构设定敏感，导致细节丢失和鲁棒性不足。

Method: 构建Transformer引导的内容自适应图解混框架：1) Transformer建模光谱-空间的全局依赖；2) 内容自适应GNN根据特征自动态学习图结构以强化局部关系；3) 融合多阶传播以综合不同邻接尺度信息并提升抗噪；4) 引入图残差机制，保留全局信息并稳定训练。

Result: 在多个基准上优于当前SOTA，能更好地保留边界细节与长程交互，同时对噪声更鲁棒（代码开源）。

Conclusion: 联合Transformer与内容自适应图学习、配合多阶传播与图残差，可在HU中兼顾全局与局部信息，提升准确性与稳定性，并具备更强的泛化与实用潜力。

Abstract: Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote
sensing images into a set of endmembers and their corresponding abundances.
Despite significant progress in this field using deep learning, most methods
fail to simultaneously characterize global dependencies and local consistency,
making it difficult to preserve both long-range interactions and boundary
details. This letter proposes a novel transformer-guided content-adaptive graph
unmixing framework (T-CAGU), which overcomes these challenges by employing a
transformer to capture global dependencies and introducing a content-adaptive
graph neural network to enhance local relationships. Unlike previous work,
T-CAGU integrates multiple propagation orders to dynamically learn the graph
structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a
graph residual mechanism to preserve global information and stabilize training.
Experimental results demonstrate its superiority over the state-of-the-art
methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.

</details>


### [46] [TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers](https://arxiv.org/abs/2509.03379)
*Guoxin Wang,Qingyuan Wang,Binhua Huang,Shaowu Chen,Deepu John*

Main category: cs.CV

TL;DR: 提出TinyDrop：一个训练免调、可插拔的ViT推理加速方案，通过轻量视觉模型在线估计token重要性并丢弃低重要性token，在保持分类精度的同时将FLOPs最高降低80%。


<details>
  <summary>Details</summary>
Motivation: ViT在图像分类上表现强，但因对所有图像token做注意力计算而推理代价高。现有加速方法常需改结构或再训练，影响通用性与部署便捷性，亟需一种无需改动主模型、能在不同ViT上通用的高效推理方案。

Method: 提出TinyDrop框架：在大ViT推理时引入一个轻量级指导模型，动态估计各token的重要性；当大模型需要执行注意力时，选择性丢弃低重要性token，减少参与注意力计算的token数量。方法为训练免调、即插即用、不改主模型结构，适配多种ViT架构。

Result: 在标准图像分类基准上，大幅降低计算开销（FLOPs最高降至原来的20%）且准确率几乎不降，显示出良好的泛化与实用价值。

Conclusion: TinyDrop在不牺牲精度的前提下显著降低大型ViT推理成本，具备即插即用、训练免调和跨架构适配的优势，适合在实际场景中部署高效的ViT分类模型。

Abstract: Vision Transformers (ViTs) achieve strong performance in image classification
but incur high computational costs from processing all image tokens. To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model. The guidance model estimates the importance of tokens while
performing inference, thereby selectively discarding low-importance tokens if
large vit models need to perform attention calculations. The framework operates
plug-and-play, requires no architectural modifications, and is compatible with
diverse ViT architectures. Evaluations on standard image classification
benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs
with minimal accuracy degradation, highlighting its generalization capability
and practical utility for efficient ViT-based classification.

</details>


### [47] [Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation](https://arxiv.org/abs/2509.03385)
*Reina Ishikawa,Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Evaluating concept customization is challenging, as it requires a
comprehensive assessment of fidelity to generative prompts and concept images.
Moreover, evaluating multiple concepts is considerably more difficult than
evaluating a single concept, as it demands detailed assessment not only for
each individual concept but also for the interactions among concepts. While
humans can intuitively assess generated images, existing metrics often provide
either overly narrow or overly generalized evaluations, resulting in
misalignment with human preference. To address this, we propose Decomposed GPT
Score (D-GPTScore), a novel human-aligned evaluation method that decomposes
evaluation criteria into finer aspects and incorporates aspect-wise assessments
using Multimodal Large Language Model (MLLM). Additionally, we release Human
Preference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark
dataset containing both single- and multi-concept tasks, enabling stage-wise
evaluation across a wide difficulty range -- from individual actions to
multi-person interactions. Our method significantly outperforms existing
approaches on this benchmark, exhibiting higher correlation with human
preferences. This work establishes a new standard for evaluating concept
customization and highlights key challenges for future research. The benchmark
and associated materials are available at
https://github.com/ReinaIshikawa/D-GPTScore.

</details>


### [48] [Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping](https://arxiv.org/abs/2509.03408)
*Mohammed Amer,Mohamed A. Suliman,Tu Bui,Nuria Garcia,Serban Georgescu*

Main category: cs.CV

TL;DR: 提出可扩展、松耦合多模态框架，将WSI（双表征：图像+图结构）、CNV与临床记录融合，用于乳腺癌分子分型；新融合策略+双重WSI显著优于SOTA，且便于增减模态、迁移至其他癌种。


<details>
  <summary>Details</summary>
Motivation: 临床环境中可用模态因机构与患者差异而不一致，现有多模态方法耦合度高、扩展性差；乳腺癌分子分型对个体化治疗关键，整合影像、基因组与临床信息有望提升分型准确性与泛化性。

Method: 提出可伸缩的松耦合多模态框架：1）WSI“双重表征”——传统基于图像的切块特征与基于图的WSI表示结合；2）设计无需为每次新增/缺失模态重训的模块化管线；3）提出新的多模态融合策略以适应不同模态可用性场景；4）与CNV、临床数据联合训练/推理。

Result: 在乳腺癌分型任务上，双重WSI表征+CNV+临床记录结合的新融合策略在多种模态条件下均显著优于现有SOTA，显示稳健与泛化能力；双重WSI本身带来明显性能增益。

Conclusion: 所提框架在准确性、可扩展性与实用性上具有优势：能无缝增减模态、减少重训练成本，适用于其他癌种与更多模态；双重WSI与新融合策略是性能提升的关键。

Abstract: Healthcare applications are inherently multimodal, benefiting greatly from
the integration of diverse data sources. However, the modalities available in
clinical settings can vary across different locations and patients. A key area
that stands to gain from multimodal integration is breast cancer molecular
subtyping, an important clinical task that can facilitate personalized
treatment and improve patient prognosis. In this work, we propose a scalable
and loosely-coupled multimodal framework that seamlessly integrates data from
various modalities, including copy number variation (CNV), clinical records,
and histopathology images, to enhance breast cancer subtyping. While our
primary focus is on breast cancer, our framework is designed to easily
accommodate additional modalities, offering the flexibility to scale up or down
with minimal overhead without requiring re-training of existing modalities,
making it applicable to other types of cancers as well. We introduce a
dual-based representation for whole slide images (WSIs), combining traditional
image-based and graph-based WSI representations. This novel dual approach
results in significant performance improvements. Moreover, we present a new
multimodal fusion strategy, demonstrating its ability to enhance performance
across a range of multimodal conditions. Our comprehensive results show that
integrating our dual-based WSI representation with CNV and clinical health
records, along with our pipeline and fusion strategy, outperforms
state-of-the-art methods in breast cancer subtyping.

</details>


### [49] [Time-Scaling State-Space Models for Dense Video Captioning](https://arxiv.org/abs/2509.03426)
*AJ Piergiovanni,Ganesh Satish Mallya,Dahun Kim,Anelia Angelova*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Dense video captioning is a challenging video understanding task which aims
to simultaneously segment the video into a sequence of meaningful consecutive
events and to generate detailed captions to accurately describe each event.
Existing methods often encounter difficulties when working with the long videos
associated with dense video captioning, due to the computational complexity and
memory limitations. Furthermore, traditional approaches require the entire
video as input, in order to produce an answer, which precludes online
processing of the video. We address these challenges by time-scaling
State-Space Models (SSMs) to even longer sequences than before. Our approach,
State-Space Models with Transfer State, combines both the long-sequence and
recurrent properties of SSMs and addresses the main limitation of SSMs which
are otherwise not able to sustain their state for very long contexts,
effectively scaling SSMs further in time. The proposed model is particularly
suitable for generating captions on-the-fly, in an online or streaming manner,
without having to wait for the full video to be processed, which is more
beneficial in practice. When applied to dense video captioning, our approach
scales well with video lengths and uses 7x fewer FLOPs.

</details>


### [50] [Decoding Visual Neural Representations by Multimodal with Dynamic Balancing](https://arxiv.org/abs/2509.03433)
*Kaili sun,Xingyu Miao,Bing Zhai,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: 提出一个融合EEG、图像和文本的多模态解码框架，通过引入文本语义、适配器、动态模态平衡与随机扰动正则，提高从低信噪比EEG解码视觉表征的准确率；在ThingsEEG上Top-1/Top-5分别提升2.0%/4.7%。


<details>
  <summary>Details</summary>
Motivation: EEG信号低信噪比、语义信息稀疏，导致将EEG对齐到视觉表征困难；仅用图像-EEG对齐易受噪声和高维不稳定影响，且多模态融合时文本可能造成模态贡献失衡，需要既稳定表征又动态平衡模态贡献的方法。

Method: 1) 在EEG-图像解码中引入文本模态，构建共享多模态空间，用类别文本标签提供显式语义锚点；2) 设计适配器模块，缓解高维预训练视觉/文本表征的不稳定，同时促进跨模态对齐与融合；3) 提出模态一致性动态平衡（MCDB），根据训练动态自适应调整EEG/图像/文本的权重，解决文本主导问题；4) 提出随机扰动正则（SPR），在模态优化过程中注入动态高斯噪声，提升对语义扰动的泛化；5) 在ThingsEEG上评估，与SOTA比较。

Result: 在ThingsEEG数据集上，Top-1准确率提升2.0个百分点、Top-5提升4.7个百分点，超过先前SOTA。

Conclusion: 引入文本语义与适配器可稳健对齐EEG-图像-文本表征；MCDB缓解模态失衡，SPR提升泛化，整体有效提升EEG解码视觉语义的性能。

Abstract: In this work, we propose an innovative framework that integrates EEG, image,
and text data, aiming to decode visual neural representations from low
signal-to-noise ratio EEG signals. Specifically, we introduce text modality to
enhance the semantic correspondence between EEG signals and visual content.
With the explicit semantic labels provided by text, image and EEG features of
the same category can be more closely aligned with the corresponding text
representations in a shared multimodal space. To fully utilize pre-trained
visual and textual representations, we propose an adapter module that
alleviates the instability of high-dimensional representation while
facilitating the alignment and fusion of cross-modal features. Additionally, to
alleviate the imbalance in multimodal feature contributions introduced by the
textual representations, we propose a Modal Consistency Dynamic Balance (MCDB)
strategy that dynamically adjusts the contribution weights of each modality. We
further propose a stochastic perturbation regularization (SPR) term to enhance
the generalization ability of semantic perturbation-based models by introducing
dynamic Gaussian noise in the modality optimization process. The evaluation
results on the ThingsEEG dataset show that our method surpasses previous
state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by
2.0\% and 4.7\% respectively.

</details>


### [51] [Joint Training of Image Generator and Detector for Road Defect Detection](https://arxiv.org/abs/2509.03465)
*Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Road defect detection is important for road authorities to reduce the vehicle
damage caused by road defects. Considering the practical scenarios where the
defect detectors are typically deployed on edge devices with limited memory and
computational resource, we aim at performing road defect detection without
using ensemble-based methods or test-time augmentation (TTA). To this end, we
propose to Jointly Train the image Generator and Detector for road defect
detection (dubbed as JTGD). We design the dual discriminators for the
generative model to enforce both the synthesized defect patches and overall
images to look plausible. The synthesized image quality is improved by our
proposed CLIP-based Fr\'echet Inception Distance loss. The generative model in
JTGD is trained jointly with the detector to encourage the generative model to
synthesize harder examples for the detector. Since harder synthesized images of
better quality caused by the aforesaid design are used in the data
augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road
defect detection benchmark across various countries under the condition of no
ensemble and TTA. JTGD only uses less than 20% of the number of parameters
compared with the competing baseline, which makes it more suitable for
deployment on edge devices in practice.

</details>


### [52] [Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA](https://arxiv.org/abs/2509.03494)
*Yahya Benmahane,Mohammed El Hassouni*

Main category: cs.CV

TL;DR: 提出一种参数高效的无参考图像质量评估（NR-IQA）方法：在像素空间学习可加到图像上的视觉提示，冻结mPLUG-Owl2，仅训练≤60万参数（<0.01%），在多数据集与多失真下达到与全量微调和专用NR-IQA模型相当的性能（KADID-10k上SRCC=0.93）。


<details>
  <summary>Details</summary>
Motivation: MLLM全量微调成本高、参数量大且对低层次视觉任务适配低效；NR-IQA需要泛化到多种失真（合成、真实、AI生成）并保持效率。作者希望在不改动/微调大模型权重的前提下，让通用MLLM具备强NR-IQA能力，降低计算和存储开销。

Method: 冻结mPLUG-Owl2，训练小规模像素级视觉提示（visual prompts），在推理时将提示与输入图像逐像素相加；统一使用文本查询“Rate the technical quality of the image.”引导模型输出质量评分。只优化提示参数（≤600K），不更新基座模型；在KADID-10k、KonIQ-10k、AGIQA-3k上评估，覆盖合成、真实和AI生成失真。

Result: 在不微调基座权重的情况下，取得与全量微调和专用NR-IQA方法相竞争的表现：KADID-10k上SRCC=0.93；跨多失真类型与数据集保持稳健。训练参数量<0.01%基模规模，显著节省计算与存储。

Conclusion: 首次将像素空间视觉提示用于NR-IQA，证明了在低层视觉任务中通过极少参数即可高效适配MLLM。该方法在多数据集上表现强、成本低、部署友好，具备实用价值；代码已开源。

Abstract: In this paper, we propose a novel parameter-efficient adaptation method for
No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized
in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models
(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base
model), while keeping the underlying model fully frozen. During inference,
these visual prompts are combined with images via addition and processed by
mPLUG-Owl2 with the textual query "Rate the technical quality of the image."
Evaluations across distortion types (synthetic, realistic, AI-generated) on
KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against
full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on
KADID-10k. To our knowledge, this is the first work to leverage pixel-space
visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level
vision tasks. The source code is publicly available at https: // github. com/
yahya-ben/ mplug2-vp-for-nriqa .

</details>


### [53] [OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation](https://arxiv.org/abs/2509.03498)
*Han Li,Xinyu Peng,Yaoming Wang,Zelin Peng,Xin Chen,Rongxiang Weng,Jingang Wang,Xunliang Cai,Wenrui Dai,Hongkai Xiong*

Main category: cs.CV

TL;DR: OneCAT 提出纯解码器式统一多模态模型，用单一自回归目标与模态专属MoE，在不依赖ViT/视觉分词器的前提下统一理解、生成与编辑，并通过多尺度视觉AR在LLM内实现高效高分辨率推理，性能全面领先现有开源方案。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型常依赖外部视觉编码器（如ViT）与扩散式生成，推理链条长、效率低、难以支持动态分辨率与多任务统一；作者希望用纯AR与单一骨干统一理解/生成/编辑，同时提升高分辨率效率与端到端一致性。

Method: 1) 架构：纯decoder-only Transformer，取消外部ViT与视觉tokenizer；2) 模态专属MoE：为不同模态设计专家路由，在同一AR训练目标下共享骨干、分治特化；3) 单一自回归训练：统一理解、生成、编辑为序列建模任务；4) 多尺度视觉AR：在LLM内部进行多尺度视觉token建模，减少生成步数并原生支持动态分辨率；5) 推理端到端：不依赖外部组件，提升高分辨率吞吐。

Result: 在多模态生成、编辑、理解等基准上超越现有开源统一多模态模型；在高分辨率场景显著减少解码步数与推理开销，同时保持或达到SOTA质量。

Conclusion: 纯自回归建模可作为统一多模态智能的充分且优雅基础；通过模态专属MoE与多尺度视觉AR，OneCAT在效率与性能上树立新标杆，实现端到端、动态分辨率、统一任务处理。

Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.

</details>


### [54] [DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video](https://arxiv.org/abs/2509.03499)
*Kevin Barnard,Elaine Liu,Kristine Walz,Brian Schlining,Nancy Jacobsen Stout,Lonny Lundsten*

Main category: cs.CV

TL;DR: 提出首个用于深海视频的多目标跟踪（MOT）基准数据集与评测流程，覆盖中层与底栖四段视频，结合多种检测器与跟踪器，用 HOTA 指标统一评估并开放数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有MOT/检测评测多来自陆地/常规场景，缺乏深海视频的公开基准，难以客观比较MBARI与FathomNet等模型及各类跟踪器的性能、并指导优化。

Method: 构建四段代表深海中层与底栖环境的基准视频；收集人类标注的‘测试’数据；将多种MBARI对象检测模型与一个FathomNet单类检测器结合多种跟踪器进行评估；采用HOTA指标同时衡量检测、定位与身份关联；提供可复现实验流水线与示例Python笔记本以计算指标并生成更多基准视频。

Result: 完成并公开一个新的深海MOT基准；在该基准上系统评测多种检测器+跟踪器组合，得到可比的性能结果（摘要未给出具体数值），验证HOTA在该场景下的适用性。

Conclusion: 该工作为深海视频MOT提供首个公开评测平台与工具链，使研究者能一致地比较与优化检测/跟踪模型，并可扩展生成更多基准数据以推动该领域发展。

Abstract: Benchmarking multi-object tracking and object detection model performance is
an essential step in machine learning model development, as it allows
researchers to evaluate model detection and tracker performance on
human-generated 'test' data, facilitating consistent comparisons between models
and trackers and aiding performance optimization. In this study, a novel
benchmark video dataset was developed and used to assess the performance of
several Monterey Bay Aquarium Research Institute object detection models and a
FathomNet single-class object detection model together with several trackers.
The dataset consists of four video sequences representing midwater and benthic
deep-sea habitats. Performance was evaluated using Higher Order Tracking
Accuracy, a metric that balances detection, localization, and association
accuracy. To the best of our knowledge, this is the first publicly available
benchmark for multi-object tracking in deep-sea video footage. We provide the
benchmark data, a clearly documented workflow for generating additional
benchmark videos, as well as example Python notebooks for computing metrics.

</details>


### [55] [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501)
*Honglu Zhou,Xiangyu Peng,Shrikant Kendre,Michael S. Ryoo,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Next-generation AI companions must go beyond general video understanding to
resolve spatial and temporal references in dynamic, real-world environments.
Existing Video Large Language Models (Video LLMs), while capable of
coarse-level comprehension, struggle with fine-grained, spatiotemporal
reasoning, especially when user queries rely on time-based event references for
temporal anchoring, or gestural cues for spatial anchoring to clarify object
references and positions. To bridge this critical gap, we introduce Strefer, a
synthetic instruction data generation framework designed to equip Video LLMs
with spatiotemporal referring and reasoning capabilities. Strefer produces
diverse instruction-tuning data using a data engine that pseudo-annotates
temporally dense, fine-grained video metadata, capturing rich spatial and
temporal information in a structured manner, including subjects, objects, their
locations as masklets, and their action descriptions and timelines. Our
approach enhances the ability of Video LLMs to interpret spatial and temporal
references, fostering more versatile, space-time-aware reasoning essential for
real-world AI companions. Without using proprietary models, costly human
annotation, or the need to annotate large volumes of new videos, experimental
evaluations show that models trained with data produced by Strefer outperform
baselines on tasks requiring spatial and temporal disambiguation. Additionally,
these models exhibit enhanced space-time-aware reasoning, establishing a new
foundation for perceptually grounded, instruction-tuned Video LLMs.

</details>


### [56] [A comprehensive Persian offline handwritten database for investigating the effects of heritability and family relationships on handwriting](https://arxiv.org/abs/2509.03510)
*Abbas Zohrevand,Javad Sadri,Zahra Imani*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: This paper introduces a comprehensive database for research and investigation
on the effects of inheritance on handwriting. A database has been created that
can be used to answer questions such as: Is there a genetic component to
handwriting? Is handwriting inherited? Do family relationships affect
handwriting? Varieties of samples of handwritten components such as: digits,
letters, shapes and free paragraphs of 210 families including (grandparents,
parents, uncles, aunts, siblings, cousins, nephews and nieces) have been
collected using specially designed forms, and family relationships of all
writers are captured. To the best of our knowledge, no such database is
presently available. Based on comparisons and investigation of features of
handwritings of family members, similarities among their features and writing
styles are detected. Our database is freely available to the pattern
recognition community and hope it will pave the way for investigations on the
effects of inheritance and family relationships on handwritings.

</details>


### [57] [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](https://arxiv.org/abs/2509.03516)
*Ouxiang Li,Yuan Wang,Xinting Hu,Huijuan Huang,Rui Chen,Jiarong Ou,Xin Tao,Pengfei Wan,Fuli Feng*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Text-to-image (T2I) generation aims to synthesize images from textual
prompts, which jointly specify what must be shown and imply what can be
inferred, thereby corresponding to two core capabilities: composition and
reasoning. However, with the emerging advances of T2I models in reasoning
beyond composition, existing benchmarks reveal clear limitations in providing
comprehensive evaluations across and within these capabilities. Meanwhile,
these advances also enable models to handle more complex prompts, whereas
current benchmarks remain limited to low scene density and simplified
one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a
comprehensive and complex benchmark that evaluates both composition and
reasoning capabilities of T2I models. To ensure comprehensiveness, we structure
composition around scene graph elements (instance, attribute, and relation) and
reasoning around the philosophical framework of inference (deductive,
inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To
increase complexity, driven by the inherent complexities of real-world
scenarios, we curate each prompt with high compositional density for
composition and multi-step inference for reasoning. We also pair each prompt
with a checklist that specifies individual yes/no questions to assess each
intended element independently to facilitate fine-grained and reliable
evaluation. In statistics, our benchmark comprises 1,080 challenging prompts
and around 13,500 checklist questions. Experiments across 27 current T2I models
reveal that their composition capability still remains limited in complex
high-density scenarios, while the reasoning capability lags even further behind
as a critical bottleneck, with all models struggling to infer implicit elements
from prompts. Our project page: https://t2i-corebench.github.io/.

</details>
