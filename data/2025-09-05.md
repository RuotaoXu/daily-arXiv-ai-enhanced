<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 提出GFP框架，用高层语义特征预测替代MAE式低层关节点重建，通过协同学习与受限优化生成多层时空监督，显著提效并提升下游表现。


<details>
  <summary>Details</summary>
Motivation: 现有骨骼动作MAE多重建原始关节或近似，计算冗余、语义弱，且常依赖离线特征，限制了效率与表示力。需要一种能直接学习更高层语义、同时避免离线依赖与塌缩的自监督方案。

Method: 提出General Feature Prediction (GFP)：以“预测高层特征”取代重建低层坐标。设计轻量级目标生成网络，在线动态产生从局部运动到全局语义的多层时空监督信号；采用协同学习框架与受限优化约束，促进目标多样性并抑制模型塌缩；无需预先计算的离线特征。

Result: 在NTU RGB+D 60/120与PKU-MMD上，较标准掩码骨骼建模训练加速约6.2倍，同时在多种下游任务上达到SOTA，显示更优表示质量与效率。

Conclusion: 高层特征预测替代低层重建能减少冗余、提升语义与效率；动态目标生成与受限优化确保多样且稳定的自监督信号，使GFP在骨骼动作识别自监督中实现高效与SOTA表现。

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [2] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Dimitri Androutsos,Susan Done,April Khademi*

Main category: cs.CV

TL;DR: 将有丝分裂检测视为像素级分割问题，采用带对比学习与域对抗的UNet为骨干，并用教师-学生策略生成多类伪掩码，同时联合一个多尺度CNN完成非典型有丝分裂分类；在初测集上Track1 F1=0.766，Track2 平衡准确率=0.841，体现了统一分割+分类框架对域移和类不平衡的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 病理学家计数有丝分裂既耗时又主观，AI虽可自动化但容易受域移（器官/物种/染色差异）影响，同时有丝分裂极少导致严重类不平衡。需要一种既能提升跨域泛化、又能缓解稀有类检测难题的方法。

Method: 将检测建模为像素级分割，采用UNet骨干并融入两种域泛化机制：对比表示学习与域对抗训练；通过教师-学生框架为标注的有丝分裂、困难负样本及正常细胞核生成像素级伪掩码，增强判别性与鲁棒性；在此基础上构建多尺度CNN分类器，利用分割模型的特征并以多任务学习同时优化检测与非典型有丝分裂分类。

Result: 在初步测试集：Track 1（检测）F1=0.7660；Track 2（分类）平衡准确率=0.8414，显示方法有效。

Conclusion: 统一的分割-分类多任务教师学生框架，结合对比学习和域对抗，可在类不平衡与域移条件下提升有丝分裂检测与非典型分类的鲁棒性与性能。

Abstract: Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [3] [Multi Attribute Bias Mitigation via Representation Learning](https://arxiv.org/abs/2509.03616)
*Rajeev Ranjan Dwivedi,Ankur Kumar,Vinod K Kurmi*

Main category: cs.CV

TL;DR: 提出GMBM，一个两阶段、训练期用组标签、测试期无需额外信息的通用多偏差缓解框架；先显式学习并定位多种偏差，再通过梯度抑制微调共同去除这些偏差；并提出SBA指标在分布失衡/shift下更稳健地衡量偏差放大。实验在多数据集上显著提升最弱组准确率并降低偏差。


<details>
  <summary>Details</summary>
Motivation: 现实图像存在多重重叠偏差（纹理、水印、性别化妆、场景-物体共现等），当前视觉模型易依赖这些“捷径”，影响鲁棒性与公平性。逐一处理单一偏差往往适得其反：减弱一种会放大另一种。此外，现有偏差评估在子群不平衡和训练-测试分布偏移下失效，需要更稳健的度量。

Method: 提出Generalized Multi Bias Mitigation (GMBM) 两阶段框架：1) ABIL（Adaptive Bias Integrated Learning）：为每个已知属性训练偏差编码器，并与主干融合，使分类器显式识别这些偏差方向；2) Gradient Suppression Fine-Tuning：在微调时抑制并修剪主干梯度中的这些偏差方向，使最终单一紧凑网络忽略先前识别的所有捷径。另引入测试期指标SBA（Scaled Bias Amplification），在不平衡与分布移位下分离模型放大偏差与数据分布差异。训练只需组标签，推理时无额外开销。

Result: 在FB CMNIST、CelebA、COCO上：提升最弱子群准确率；将多属性偏差放大约减半；在偏差复杂度和分布移位加剧时，SBA达新低；总体较现有方法实现更好的鲁棒性与公平性权衡。

Conclusion: GMBM作为首个端到端、实用的多偏差缓解方案，通过“先学偏差、再抑制梯度中的偏差方向”实现对多种重叠偏差的同时消除，并配合SBA提供更可靠的偏差评估。该方法在多数据集验证有效，训练期依赖组标签但推理无负担，具有推广与落地潜力。

Abstract: Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/

</details>


### [4] [Lightweight image segmentation for echocardiography](https://arxiv.org/abs/2509.03631)
*Anders Kjelsrud,Lasse Løvstakken,Erik Smistad,Håvard Dalen,Gilles Van De Vyver*

Main category: cs.CV

TL;DR: 提出一种轻量级U-Net，通过系统性消融研究精简nnU-Net关键要素，在心超左心室相关分割上以2M参数达成与nnU-Net等效精度，但体量小16倍、推理快4倍，跨数据集泛化可比。


<details>
  <summary>Details</summary>
Motivation: 临床需要在超声心动图中实时、准确地分割左心室及相关结构，以自动计算体积、射血分数等指标。现有强基线nnU-Net虽精度高，但模型庞大且推理慢，不利于实时应用，因此需要找出影响性能的关键因素并据此设计更轻、更快的模型。

Method: 进行逐步消融实验，依次评估：数据增强方案（对比简单仿射与复杂增强）、网络结构改动（含深度监督等）、损失函数、后处理等；据此提炼出有效组件，构建参数约2M的轻量U-Net，并与nnU-Net在CAMUS数据集上对比，同时做跨数据集验证。

Result: 发现简单仿射增强与深度监督对性能贡献最大；复杂增强与更大模型容量收益有限。所提轻量U-Net在CAMUS上对LV/MYO/LA的Dice为0.93/0.85/0.89，与nnU-Net的0.93/0.86/0.89差异无统计学意义（p>0.05）。模型仅2M参数（vs 33M），推理1.35ms/帧（vs 5.40ms），尺寸小16倍、速度快约4倍；在内部311例数据上也表现相当，显示良好泛化。

Conclusion: 通过聚焦最具影响力的组件（简单仿射增强与深度监督），可在显著压缩模型规模与提升速度的同时维持与nnU-Net等效的分割性能，适用于临床实时心超应用，并具备跨数据集的泛化能力。

Abstract: Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.

</details>


### [5] [treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds](https://arxiv.org/abs/2509.03633)
*Josafat-Mattias Burmeister,Andreas Tockner,Stefan Reder,Markus Engel,Rico Richter,Jan-Peter Mund,Jürgen Döllner*

Main category: cs.CV

TL;DR: 提出修订版的treeX无监督算法，用于从近程激光扫描点云中实例级分割单株树，结合基于聚类的树干检测与区域生长冠层分割，提供TLS/PLS与ULS两套参数预设；在六个公开数据集与六种开源方法对比中，显著提速并提高准确度，尤其在地面数据上F1提升0.11–0.49，在ULS上达0.58且原版失败；方法适合资源受限场景与半自动标注，代码开源于pointtree。


<details>
  <summary>Details</summary>
Motivation: 深度学习分割树木实例虽有效但需要大量标注数据与算力，资源成本高；原始treeX针对PLS设计，泛化与效率仍有改进空间。需要一种无需监督、可在多平台（TLS/PLS/ULS）上稳健运行、且资源友好的方法。

Method: 提出修订版treeX：1) 无监督流程；2) 以聚类为核心的树干检测（stem detection）；3) 区域生长进行树冠扩展/分割；4) 提供两组参数预设分别适配地面平台（TLS/PLS）与空基平台（ULS）；5) 在六个公共数据集上与六个开源基线（含原treeX与DL方法）对比评测；强调运行效率优化。

Result: 相较原treeX，在地面数据上实例级检测F1提升+0.11至+0.49且运行时间更短；ULS数据上本方法预设F1=0.58，原算法无法产生正确实例；在TLS/PLS上与近期开源方法（含深度学习）达到相近精度。

Conclusion: 修订版treeX在点云树木实例分割中兼顾效率与精度，特别适用于树干可见度与点密度足够的场景；可作为深度学习的资源高效替代方案，亦可用于生成半自动标注；已开源为pointtree Python包，利于社区采用。

Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.

</details>


### [6] [Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding](https://arxiv.org/abs/2509.03635)
*Hongpei Zheng,Lintao Xiang,Qijun Yang,Qian Lin,Hujun Yin*

Main category: cs.CV

TL;DR: Reg3D提出在指令微调中引入几何重建监督，通过双重监督（输入与学习目标均含3D信息）与双编码器架构，利用对象级与帧级重建任务强化几何一致性，从而显著提升3D场景理解能力，并在多项基准上取得大幅改进。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在2D理解上表现突出，但迁移到3D场景理解受限；主流方法多依赖纯文本监督，缺乏几何约束，导致空间表示与推理能力不足。需要一种能直接利用并学习3D几何结构的训练范式。

Method: 提出Reg3D：在指令微调阶段引入“重建式几何监督”。采用双监督与双编码器设计：1) 输入侧注入3D几何信息；2) 学习目标侧以对象级与帧级重建任务为显式目标；通过几何一致性约束，促使模型从数据中恢复底层几何结构而非仅做语义描述。

Result: 在ScanQA、Scan2Cap、ScanRefer与SQA3D等多项3D任务基准上取得显著性能提升，验证了方法对3D空间理解与推理能力的增强。

Conclusion: 将几何重建纳入指令微调、对3D信息进行输入与目标双向监督，是提升LMM三维场景理解的有效范式。Reg3D建立了面向空间感知多模态模型的新训练范式。

Abstract: The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.

</details>


### [7] [QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception](https://arxiv.org/abs/2509.03704)
*Seth Z. Zhao,Huizhi Zhang,Zhaowei Li,Juntong Peng,Anthony Chui,Zewei Zhou,Zonglin Meng,Hao Xiang,Zhiyu Huang,Fujia Wang,Ran Tian,Chenfeng Xu,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: 提出QuantV2X：首个端到端全量化的多模态多车协同感知系统，在低比特约束下保持精度，同时显著降低计算与通信成本，实现3.2×系统时延降低与mAP30提升+9.5，并具备更好可扩展性与部署性。


<details>
  <summary>Details</summary>
Motivation: 以往V2X协同感知研究多追求精度提升，忽视系统层面的效率、时延与实际部署问题；主流采用全精度模型，计算与带宽开销高，不适合资源受限与实时需求。需要一种既高效又可部署、兼顾精度的解决方案。

Method: 提出QuantV2X：将统一的端到端量化策略同时应用于神经网络模型与跨车通信的中间特征/消息表征，实现低比特（全量化）下的多模态、多智能体中间融合；在系统设计上面向部署优化，兼顾计算、带宽与内存预算，并验证可扩展到更大模型。

Result: 在部署导向指标上，相比全精度基线，系统级时延降低3.2倍，mAP30提升+9.5；在低比特条件下仍达可比于全精度的准确率；同时在相同内存预算下可容纳更大、更强的模型。

Conclusion: 全量化的多智能体中间融合协同感知在真实部署中可行且具优势；QuantV2X兼顾精度、时延与资源效率，具备更强的可扩展性，并将开源以促进该领域研究。

Abstract: Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.

</details>


### [8] [Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns](https://arxiv.org/abs/2509.03729)
*Bandita Bharadwaj,Ankur Mishra,Saurav Bharadwaj*

Main category: cs.CV

TL;DR: 比较ResNet50、MobileNetV2、EfficientNetB0在基于叶脉纹理的植物物种分类上的效果，EfficientNetB0综合表现最佳（测试准确率94.67%、F1>94.6%），MobileNetV2兼顾轻量与泛化，ResNet50存在过拟合。


<details>
  <summary>Details</summary>
Motivation: 叶脉（脉序）是具有高分类学价值的形态特征，但传统基于人工特征或人工识别的流程效率低、可扩展性差。深度学习在图像分类上表现突出，有望构建可扩展、自动化、准确的植物分类工具，尤其在物种识别与数字化分类学场景中。

Method: 使用Swedish Leaf Dataset（15类，每类75张，共1125张）作为数据集，分别训练ResNet50、MobileNetV2与EfficientNetB0，采用标准训练/测试划分与评价指标（准确率、精确率、召回率、F1），比较三种模型在训练与测试阶段的性能与泛化表现。

Result: ResNet50：训练准确率94.11%，测试准确率88.45%，F1=87.82%，出现过拟合；MobileNetV2：测试准确率93.34%，F1=93.23%，泛化更好、适合轻量实时场景；EfficientNetB0：测试准确率94.67%，Precision/Recall/F1均>94.6%，在叶脉分类上最稳健。

Conclusion: 深度学习对基于叶脉的植物分类有效，其中EfficientNetB0在精度与鲁棒性上领先；MobileNetV2在资源受限与实时应用中具优势；ResNet50需改进以缓解过拟合。结果支持以EfficientNetB0为核心构建可扩展、准确的自动化植物分类系统。

Abstract: This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.

</details>


### [9] [LayoutGKN: Graph Similarity Learning of Floor Plans](https://arxiv.org/abs/2509.03737)
*Casper van Engelenburg,Jan van Gemert,Seyran Khademi*

Main category: cs.CV

TL;DR: 提出LayoutGKN，用可微图核在最终节点嵌入上进行跨图比较，达到与图匹配网络相当或更优的相似度效果，同时显著加速推理。代码与数据开源。


<details>
  <summary>Details</summary>
Motivation: 楼层平面图常用图结构表示，许多任务（检索、聚类、可视化）需要快速准确比较图。现有最强方法（图匹配网络）依赖代价高的跨图节点级交互，推理慢，限制了大规模应用。

Method: 提出LayoutGKN：采用联合嵌入架构，将跨图节点级交互延后到最后一步；通过一个可微分的图核作为距离函数作用于学习到的最终节点嵌入，从而避免中间层的昂贵跨图交互；端到端训练以优化相似度度量。

Result: 在楼层平面图相似度任务上，LayoutGKN的相似性评估与或优于图匹配网络，同时显著提升推理速度（抽象中未给具体倍数）。

Conclusion: 推迟跨图交互并用可微图核在最终嵌入上进行比较，可在保持或提升精度的同时大幅提升效率；该方法适用于需要快速图比较的应用，并已开源。

Abstract: Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.

</details>


### [10] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: 提出CLIP-SVD：仅微调CLIP参数矩阵的奇异值，实现跨模态、参数高效的领域自适应，在少样本下于多项自然与生物医学数据集达SOTA，同时保持泛化并提供可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 现有VLM（如CLIP）在新细粒度领域适配困难：依赖提示工程；完全微调成本高；现有轻量化方法（软提示、适配器等）可能引入外部模块、影响稳定性与预训练知识保留，限制适配质量。

Method: 对CLIP权重做SVD分解，仅学习奇异值（不改动奇异向量/不插入新模块），相当于在预训练基底上重标定各方向的重要性；跨多模态（图像、文本）统一应用；参数开销仅约0.04%，并结合自然语言分析评估适配动态与有效性。

Result: 在11个自然与10个生物医学数据集上，少样本分类精度超过现有方法；在准确率与泛化上均优于提示调优/适配器等；更好保留原始CLIP的通用能力。

Conclusion: SVD仅调奇异值可实现稳定、参数高效且可解释的VLM领域适配，达成SOTA并保持泛化；为不引入额外模块的多模态适配提供新范式。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [11] [STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification](https://arxiv.org/abs/2509.03754)
*Zongsen Qiu*

Main category: cs.CV

TL;DR: 提出STA-Net：结合训练免NAS(DeepMAD)骨干与形-纹理解耦注意(STAM)，在边缘设备上实现高效植物病害识别；401K参数/51.1M FLOPs，在CCMT上Acc 89.00%、F1 88.96%，显著优于基线与通用注意模块。


<details>
  <summary>Details</summary>
Motivation: 轻量化模型难以在边缘端兼顾精度与算力限制；通用注意机制对植物病斑的不规则形状与复杂纹理刻画不足，导致微小病理特征识别欠佳。

Method: 两部分：1) 采用训练免NAS方法DeepMAD搜索高效骨干，降低参数和FLOPs以适配边缘设备；2) 提出STAM注意模块，将注意力解耦为形状与纹理两支：形状支路用DCNv4捕获不规则病斑形变与边界；纹理支路用Gabor滤波组建模方向性与频率选择的纹理；融合两支输出得到判别特征；整体构成STA-Net。

Result: 在CCMT植物病害数据集上，STA-Net仅401K参数与51.1M FLOPs，即达Acc 89.00%、F1 88.96%；消融表明加入STAM显著提升，相较基线与常规注意模块更优。

Conclusion: 将领域知识注入注意力并进行形-纹理解耦能显著增强轻量模型对细粒度病理特征的捕获，适合边缘部署；所提STA-Net在效率与精度间取得良好平衡，具备推广潜力并已开源。

Abstract: Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.

</details>


### [12] [SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2509.03786)
*Xinxin Wang,Han Sun,Ningzhong Liu,Huiyu Zhou,Yinan Yao*

Main category: cs.CV

TL;DR: 提出UCOD任务与DeepCamo数据集，并构建SLENet（含GAE、LGB、MSSD）以提升水下伪装目标检测的定位与增强能力，在新数据集与多种COD基准上超过SOTA且具良好泛化。


<details>
  <summary>Details</summary>
Motivation: 水下伪装目标难以检测，受光学畸变、浑浊度、复杂生物纹理与形态影响，现有COD方法与数据对该场景覆盖不足，影响海洋生态监测与保护，需要专门任务、数据与方法。

Method: 1) 引入DeepCamo数据集并基准测试现有COD模型以识别瓶颈；2) 提出SLENet：包含Gamma-Asymmetric Enhancement（GAE）用于多尺度特征增强与对比度/光照校正；Localization Guidance Branch（LGB）生成富全局语义的定位图；Multi-Scale Supervised Decoder（MSSD）在定位图引导下进行多尺度监督解码，提升边界与细节；整体端到端训练。

Result: 在DeepCamo及三个人工场景COD基准上，SLENet在多项指标上优于SOTA，显示更高的检测精度与鲁棒性，并对更广泛的COD任务具有良好泛化。

Conclusion: 面向UCOD的DeepCamo与SLENet有效缓解水下成像与目标伪装带来的难题；GAE+LGB+MSSD的协同设计提升定位与外观增强，从而改进预测质量；方法在不同数据集上表现稳定，具有推广潜力。

Abstract: Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.

</details>


### [13] [Fitting Image Diffusion Models on Video Datasets](https://arxiv.org/abs/2509.03794)
*Juhun Lee,Simon S. Woo*

Main category: cs.CV

TL;DR: 提出一种利用视频时间归纳偏置来改进图像扩散模型训练的简单策略，无需改架构，能加速收敛、降低FID并提升多样性，并从优化角度解释其通过降低梯度方差而加速训练。


<details>
  <summary>Details</summary>
Motivation: 静态图像独立采样的训练无法充分利用现实世界的时间连续性，导致收敛慢、分布覆盖不足与泛化受限；特别是在手-物交互等场景，细微时序变化蕴含语义信息，单帧学习难以捕捉。

Method: 在不改动扩散模型架构与训练管线的前提下，引入一种基于连续视频帧的训练策略，利用时间上的稠密一致性作为归纳偏置进行正则化与数据组织，从而在训练中鼓励模型学习有意义的时间变化（具体形式为时序正则以降低梯度方差）。

Result: 在HandCo数据集上，相较标准训练，收敛速度提升超过2倍；训练与验证分布上的FID更低；生成多样性提升，模型更能捕获手指关节等细微时序变化。

Conclusion: 利用视频的时间归纳偏置作为正则可在不改架构的情况下显著提升扩散模型训练效率与质量；优化分析表明其通过降低梯度方差促进更快收敛。

Abstract: Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.

</details>


### [14] [MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting](https://arxiv.org/abs/2509.03800)
*Yuheng Li,Yenho Chen,Yuxiang Lai,Jike Zhong,Vanessa Wildman,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MedVista3D提出多尺度、语义增强的3D CT视听-语言预训练框架，同时实现局部精准检测与全局体积级推理，并通过规范化报告语义以提升图文对齐，在多项任务上达SOTA并具备良好迁移性。


<details>
  <summary>Details</summary>
Motivation: 临床中放射学误诊常见，源于：1) 局部病灶漏读与注意盲视；2) 对整卷3D影像的全局理解不足；3) 放射科报告语言噪声大、变异性强。现有3D视听-语言模型难以同时满足“局部-全局空间推理+稳健语义对齐”的需求。

Method: 提出MedVista3D预训练框架：1) 多尺度、局部-全局图文对齐，学习细粒度表征并保持全卷上下文；2) 使用语言模型重写原始放射报告以减少噪声与变异；3) 构建“放射学语义匹配库”（Radiology Semantic Matching Bank），实现语义感知的图文对齐与监督。

Result: 在零样本疾病分类、报告检索、医学VQA上获得SOTA；并能有效迁移至器官分割与预后预测等下游任务。

Conclusion: 通过多尺度图文对齐与语义规范化，MedVista3D兼顾局部检测与全局推理，缓解报告噪声问题，显著提升3D CT多任务性能与迁移能力；代码与数据将开源。

Abstract: Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.

</details>


### [15] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: 提出CaPL：一种通过视觉粒化与因果推理引导的文本提示学习方法，强化CLIP在细粒度识别上的表现，在15个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的提示学习在细粒度数据集上能力有限，难以捕捉类别间细微差异；需要一种机制将视觉中可解释、可组合的细节对齐到文本提示，以提升区分度。

Method: CaPL包含两大模块：（1）属性解缠模块：利用布朗桥扩散模型将视觉特征分解为“非个体化属性”（类间共享）与“个体化属性”（类特异）；（2）粒化学习模块：将上述属性整合为视觉粒（granules），并在两种因果推理策略下学习，使文本提示能针对粒级差异进行对齐与区分。

Result: 在15个数据集上进行广泛实验，CaPL较现有提示学习方法取得显著提升，尤其在细粒度识别任务上效果突出。

Conclusion: 通过因果引导的视觉粒化与属性解缠，能为CLIP学习到更具判别性的文本提示，有效弥补细粒度识别中的性能短板。

Abstract: Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.

</details>


### [16] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

TL;DR: 提出基于事件相机的湍流缓解(EGTM)框架，用“事件-幸运”洞见从事件流中提取可靠无湍流引导，实现时间域幸运融合；发布首个真实事件驱动湍流数据集；在质量与效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统TM依赖多帧同步图像寻找“幸运”无畸变片段，但受限于低帧率与粗粒度动态，需要庞大模型且计算/存储低效。事件相机高时间分辨率和稀疏异步特性有望从根本上缓解该瓶颈。

Method: 1) 提出“事件-幸运”洞见：湍流畸变与事件流在时空上的分布呈反相关，可利用事件的稀疏分布指示稳定区域；2) 设计EGTM框架：从显式但噪声较大的湍流事件中提取像素级可靠、无湍流的引导，进行时间域幸运融合以还原清晰图像；3) 构建首个真实场景事件驱动TM数据采集系统与数据集。

Result: 在作者自建真实EGTM数据集上，实现比SOTA TM方法更小模型(×710)、更低时延(×214)、更低复杂度(×224)，同时在画质上达到SOTA：PSNR提升+0.94、SSIM提升+0.08。

Conclusion: 引入事件模态可显著提升湍流缓解的效率与效果；“事件-幸运”指导下的EGTM能以极低资源开销实现高质量恢复，并得到真实数据集支持。

Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [17] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 提出FocusMamba：一种对RGB与事件两模态进行自适应协同稀疏化与高效融合的目标检测框架，在DSEC-Det与PKU-DAVIS-SOD上兼顾更高精度与更低计算。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-Event方法对图像背景与事件的非事件区域等低信息区域一视同仁地处理，造成特征提取和融合阶段的冗余计算与性能瓶颈；已有稀疏化方案多用固定token数/阈值，无法适应样本复杂度差异，导致信息性token保留不足或冗余过多。

Method: 提出FocusMamba框架：1) 事件引导的多模态稀疏化（EGMS），利用事件相机对场景变化的感知，跨模态判别并自适应丢弃各自低信息区域，实现协同稀疏；2) 跨模态聚焦融合（CMFF），在稀疏结果基础上对两模态的互补特征进行高效对齐与整合，突出关键信息区域。

Result: 在DSEC-Det与PKU-DAVIS-SOD数据集上，FocusMamba在准确性与效率上均优于现有方法（文中宣称SOTA），验证了自适应稀疏化与聚焦融合的有效性。

Conclusion: 自适应、事件引导的协同稀疏与跨模态聚焦融合可以同时降低计算开销并提升检测性能，为RGB-Event多模态检测提供了更好的精度-效率权衡；代码将开源。

Abstract: Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [18] [SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition](https://arxiv.org/abs/2509.03873)
*Jiajun Song,Xiaoou Liu*

Main category: cs.CV

TL;DR: 提出CZSFR任务与SalientFusion方法，含SalientFormer与DebiasAT两模块，解决背景冗余、主副食角色混淆与语义偏置问题；在新建CZSFood-90/164与通用CZSL数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统食物识别在新菜品激增场景下难以覆盖全部类别，需零样本能力。食物天然具备“菜系-配料”的可组合属性，适配CZSL范式，但存在背景干扰、主副食角色混淆、单一属性语义偏置等痛点，促使提出专门的组合零样本食物识别任务与方法。

Method: 提出CZSFR任务与SalientFusion框架：1) SalientFormer：通过显著性与上下文感知机制过滤背景冗余，并引入深度(距离/几何)特征以区分主食与配菜的角色关系；2) DebiasAT：以提示(提示词/文本嵌入)与视觉特征对齐来消减单属性语义偏置，进行更稳健的属性-对象(菜系-食材/菜品)组合建模。另构建两套基准数据集CZSFood-90与CZSFood-164。

Result: 在自建CZSFood-90与CZSFood-164上取得SOTA表现，并在主流泛用CZSL数据集上也达到最优或极具竞争力的结果。

Conclusion: CZSFR将食物领域与CZSL的可组合结构对齐；SalientFusion通过显著性过滤、深度辅助的角色消歧与对齐去偏，有效缓解三大挑战，显著提升未见组合的识别能力，并具备跨数据集的泛化优势；代码已开源。

Abstract: Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.

</details>


### [19] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 这是一篇关于“人体动作视频生成”的系统性综述，按五个关键阶段（输入—动作规划—动作视频生成—精炼—输出）梳理超过十个子任务，覆盖视觉/文本/音频三模态，评述200+论文并首次系统讨论大语言模型在该领域的潜力与作用，提供里程碑工作与资源清单。


<details>
  <summary>Details</summary>
Motivation: 现有综述多零散围绕单一方法或子任务，缺乏对完整生成流程的统一视角与阶段化分析；同时，LLM 在跨模态理解与协同中的潜能尚未被系统梳理。作者旨在填补这两方面的缺口，帮助研究者把握全景、趋势与应用前景。

Method: 以“生成流水线”作为组织框架：将人体动作视频生成划分为输入、动作规划、视频生成、结果精炼、输出五阶段；按视觉/文本/音频三大模态综述各阶段代表方法与技术路线，汇总200+文献与里程碑成果；讨论大语言模型在指令理解、跨模态对齐、长时序规划与评价环节的作用与机会，并给出资源仓库。

Result: 产出一份结构化综述：1）明确定义10+子任务与其在流水线中的位置；2）对三模态最新方法与趋势进行比较与归纳；3）罗列关键突破性工作与开源资源；4）总结LLM 赋能的可能方向与初步实践。

Conclusion: 人体动作视频生成正快速演进，三模态融合与端到端质量提升是趋势；以阶段化流水线统一问题有助于方法比较与系统优化；LLM 有望在跨模态条件理解、动作脚本/约束生成、长时序一致性与自动评测方面发挥作用。该综述与仓库旨在作为研究入门与系统参考，推动数字人应用落地。

Abstract: Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [20] [OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction](https://arxiv.org/abs/2509.03887)
*Bu Jin,Songen Gu,Xiaotao Hu,Yupeng Zheng,Xiaoyang Guo,Qian Zhang,Xiaoxiao Long,Wei Yin*

Main category: cs.CV

TL;DR: 提出OccTENS：一种面向占据（occupancy）世界模型的生成方法，兼顾可控性、高保真、长时预测与高效推理。以“时间上的下一尺度预测”将时序建模拆分为空间逐尺度生成与时间逐场景预测，并以TensFormer统一处理时空关系与位姿控制，实验优于SOTA、速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归的占据生成虽然能同时预测车辆运动与未来占据，但存在推理低效、长时生成退化、难以控制（特别是位姿/自车运动）的问题。占据世界模型需要细粒度3D几何与动态演化的精准表达，较视觉生成更具挑战，亟需新的时空建模范式以提升质量、稳定性与可控性。

Method: 将占据世界模型重构为Temporal Next-Scale Prediction (TENS) 任务：把时序问题分解为空间“按尺度”生成与时间“按场景”递进预测；设计TensFormer以灵活、可扩展地处理时序因果与空间关系；提出“整体位姿聚合”策略，将占据与自车位姿统一为一个序列进行建模，从而实现位姿层面的可控生成。

Result: 在基准实验中，OccTENS在占据质量指标上优于现有SOTA，并显著缩短推理时间，实现更高效的长时预测和更稳定的时序表现，同时支持对自车位姿的可控生成。

Conclusion: TENS范式与TensFormer有效解决了AR方法的低效、长时退化与不可控问题；OccTENS在质量与效率上均达到新的水平，并提供了更强的位姿可控性，适合长时、可控的3D占据世界建模。

Abstract: In this paper, we propose OccTENS, a generative occupancy world model that
enables controllable, high-fidelity long-term occupancy generation while
maintaining computational efficiency. Different from visual generation, the
occupancy world model must capture the fine-grained 3D geometry and dynamic
evolution of the 3D scenes, posing great challenges for the generative models.
Recent approaches based on autoregression (AR) have demonstrated the potential
to predict vehicle movement and future occupancy scenes simultaneously from
historical observations, but they typically suffer from \textbf{inefficiency},
\textbf{temporal degradation} in long-term generation and \textbf{lack of
controllability}. To holistically address these issues, we reformulate the
occupancy world model as a temporal next-scale prediction (TENS) task, which
decomposes the temporal sequence modeling problem into the modeling of spatial
scale-by-scale generation and temporal scene-by-scene prediction. With a
\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and
spatial relationships of occupancy sequences in a flexible and scalable way. To
enhance the pose controllability, we further propose a holistic pose
aggregation strategy, which features a unified sequence modeling for occupancy
and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art
method with both higher occupancy quality and faster inference time.

</details>


### [21] [Weakly-Supervised Learning of Dense Functional Correspondences](https://arxiv.org/abs/2509.03893)
*Stefan Stojanov,Linan Zhao,Yunzhi Zhang,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出一种弱监督方法学习跨类别的稠密“功能对应”，借助视觉-语言模型对多视角图像伪标注功能部位，并结合像素级对比学习蒸馏功能与空间信息，形成能在不同物体类别间建立稠密对应的模型，且在新建的合成与真实数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统稠密对应通常假设同类或外观相近，跨类别时外形差异大难以匹配；但实现相同功能的部件在形状/外观上往往相似，功能可作为匹配先验。现有自监督表示与VLM虽强，但难直接得到跨类、像素级功能对应。

Method: 1) 定义“稠密功能对应”的概念，以实现相同功能的部位为对齐目标；2) 利用视觉-语言模型对多视角图像进行功能部件伪标注（弱监督来源）；3) 将伪标注与像素级对比学习结合，进行稠密表示学习，蒸馏功能语义与空间几何；4) 训练得到可预测跨类稠密功能对应的模型；5) 构建合成与真实评测数据集作为基准。

Result: 在新策划的合成与真实数据集上，与基于通用自监督图像表征和基于VLM的基线相比，该方法在功能对应预测上取得更优性能。

Conclusion: 功能先验与VLM伪标注相结合、配合稠密对比学习，能有效学习跨类别的稠密功能对应；所建基准验证了方法的优势，相比现成自监督与VLM方案更适合功能驱动的稠密匹配任务。

Abstract: Establishing dense correspondences across image pairs is essential for tasks
such as shape reconstruction and robot manipulation. In the challenging setting
of matching across different categories, the function of an object, i.e., the
effect that an object can cause on other objects, can guide how correspondences
should be established. This is because object parts that enable specific
functions often share similarities in shape and appearance. We derive the
definition of dense functional correspondence based on this observation and
propose a weakly-supervised learning paradigm to tackle the prediction task.
The main insight behind our approach is that we can leverage vision-language
models to pseudo-label multi-view images to obtain functional parts. We then
integrate this with dense contrastive learning from pixel correspondences to
distill both functional and spatial knowledge into a new model that can
establish dense functional correspondence. Further, we curate synthetic and
real evaluation datasets as task benchmarks. Our results demonstrate the
advantages of our approach over baseline solutions consisting of off-the-shelf
self-supervised image representations and grounded vision language models.

</details>


### [22] [Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model](https://arxiv.org/abs/2509.03895)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: 提出Attn-Adapter，一种在线少样本学习框架，无需微调CLIP即可通过双重注意力动态适配，提升跨类别/跨数据集性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有对比式视觉-语言模型（如CLIP）在零样本表现优异，但在少样本场景常依赖离线提示学习微调，计算开销大且易过拟合，限制了实用性与泛化能力。

Method: 在CLIP上引入双注意力适配器：1) Memory Attn-Adapter：利用支持集样本对类别文本嵌入进行注意力聚合与精炼，注入数据集特定信息；2) Local-Global Attn-Adapter：将图像的局部与全局特征通过注意力融合以增强图像嵌入。两者作为轻量可插拔模块，实现在线、自适应的少样本更新，无需重训基座模型。

Result: 在跨类别与跨数据集的少样本识别上优于当前SOTA，同时推理高效，可在不同规模的CLIP骨干上平滑扩展。

Conclusion: Attn-Adapter通过内存注意力与局部-全局注意力的组合，有效缓解少样本过拟合与计算开销问题，实现对CLIP的在线快速适配并提升通用化与效率。

Abstract: Contrastive vision-language models excel in zero-shot image recognition but
face challenges in few-shot scenarios due to computationally intensive offline
fine-tuning using prompt learning, which risks overfitting. To overcome these
limitations, we propose Attn-Adapter, a novel online few-shot learning
framework that enhances CLIP's adaptability via a dual attention mechanism. Our
design incorporates dataset-specific information through two components: the
Memory Attn-Adapter, which refines category embeddings using support examples,
and the Local-Global Attn-Adapter, which enriches image embeddings by
integrating local and global features. This architecture enables dynamic
adaptation from a few labeled samples without retraining the base model.
Attn-Adapter outperforms state-of-the-art methods in cross-category and
cross-dataset generalization, maintaining efficient inference and scaling
across CLIP backbones.

</details>


### [23] [SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation](https://arxiv.org/abs/2509.03897)
*Xiaofu Chen,Israfel Salazar,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 提出SPECS（Specificity-Enhanced CLIPScore），一种面向长图像描述、无参考的表示相似度评价指标，兼顾效率与与人类评价的高相关性。


<details>
  <summary>Details</summary>
Motivation: 现有长描述评测困境：n-gram 指标高效但语义不可靠；RS 指标（如基于表示的相似度）早期算力昂贵、当下与人评相关性仍低；LLM 评测与人评高度相关但成本过高，不适合迭代开发。需要一种既高相关又高效、可在开发循环中频繁使用的指标。

Method: 在 CLIP 基础上引入强调“特异性”的新训练/优化目标：对描述中的正确细节给予奖励、对错误细节施以惩罚，从而形成无参考、面向长描述的 RS 指标 SPECS（Specificity-Enhanced CLIPScore）。

Result: SPECS 与开源 LLM 评测在与人类判断的相关性上相当，但计算成本显著更低，适合在模型训练/检查点评估中频繁使用。

Conclusion: SPECS 作为实用替代方案，解决长图像描述评估的效率与可靠性矛盾：在保持与人评相当相关性的同时大幅降低开销，利于迭代开发；代码已开源。

Abstract: As interest grows in generating long, detailed image captions, standard
evaluation metrics become increasingly unreliable. N-gram-based metrics though
efficient, fail to capture semantic correctness. Representational Similarity
(RS) metrics, designed to address this, initially saw limited use due to high
computational costs, while today, despite advances in hardware, they remain
unpopular due to low correlation to human judgments. Meanwhile, metrics based
on large language models (LLMs) show strong correlation with human judgments,
but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS
metric tailored to long image captioning. SPECS modifies CLIP with a new
objective that emphasizes specificity: rewarding correct details and penalizing
incorrect ones. We show that SPECS matches the performance of open-source
LLM-based metrics in correlation to human judgments, while being far more
efficient. This makes it a practical alternative for iterative checkpoint
evaluation during image captioning model development.Our code can be found at
https://github.com/mbzuai-nlp/SPECS.

</details>


### [24] [A Generative Foundation Model for Chest Radiography](https://arxiv.org/abs/2509.03903)
*Yuanfeng Ji,Dan Lin,Xiyue Wang,Lu Zhang,Wenhui Zhou,Chongjian Ge,Ruihang Chu,Xiaoli Yang,Junhan Zhao,Junsong Chen,Xiangde Luo,Sen Yang,Jin Fang,Ping Luo,Ruijiang Li*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.

</details>


### [25] [LMVC: An End-to-End Learned Multiview Video Coding Framework](https://arxiv.org/abs/2509.03922)
*Xihua Sheng,Yingwen Zhang,Long Xu,Shiqi Wang*

Main category: cs.CV

TL;DR: 提出端到端学习的多视角视频编码（LMVC）框架，通过利用独立视角的运动与内容特征，改进依赖视角的编码与熵模型，实现随机访问与向后兼容，并显著优于MV-HEVC。


<details>
  <summary>Details</summary>
Motivation: 多视角视频是体积视频/沉浸式3D重建的重要来源，但数据量巨大，传统编解码在效率与访问灵活性上受限；深度学习视频编码进展主要集中在单视角/双目，通用多视角仍欠缺系统性方案。

Method: 1) 端到端LMVC框架，保证随机访问与向后兼容。2) 运动侧：提出基于特征的跨视角运动向量预测，将依赖视角的运动编码条件化在已解码的独立视角运动特征上；并训练跨视角运动熵模型以学习先验。3) 内容侧：提出无视差估计的跨视角上下文预测模块，从已解码的独立视角内容特征预测依赖视角的上下文；并配套跨视角上下文熵模型以捕获先验。

Result: 在标准数据集上，所提LMVC在压缩效率上大幅优于传统MV-HEVC参考软件（定量未给出，描述为“by a large margin”）。

Conclusion: 有效利用跨视角运动与内容相关性可显著提升多视角视频端到端学习编码的压缩性能；该LMVC框架在保持随机访问与兼容性的同时建立了强基线，值得后续研究拓展。

Abstract: Multiview video is a key data source for volumetric video, enabling immersive
3D scene reconstruction but posing significant challenges in storage and
transmission due to its massive data volume. Recently, deep learning-based
end-to-end video coding has achieved great success, yet most focus on
single-view or stereo videos, leaving general multiview scenarios
underexplored. This paper proposes an end-to-end learned multiview video coding
(LMVC) framework that ensures random access and backward compatibility while
enhancing compression efficiency. Our key innovation lies in effectively
leveraging independent-view motion and content information to enhance
dependent-view compression. Specifically, to exploit the inter-view motion
correlation, we propose a feature-based inter-view motion vector prediction
method that conditions dependent-view motion encoding on decoded
independent-view motion features, along with an inter-view motion entropy model
that learns inter-view motion priors. To exploit the inter-view content
correlation, we propose a disparity-free inter-view context prediction module
that predicts inter-view contexts from decoded independent-view content
features, combined with an inter-view contextual entropy model that captures
inter-view context priors. Experimental results show that our proposed LMVC
framework outperforms the reference software of the traditional MV-HEVC
standard by a large margin, establishing a strong baseline for future research
in this field.

</details>


### [26] [TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes](https://arxiv.org/abs/2509.03938)
*Minghui Zhang,Yaoyu Liu,Junyang Wu,Xin You,Hanxiao Zhang,Junjun He,Yun Gu*

Main category: cs.CV

TL;DR: TopoSculpt提出一个用于3D管状解剖结构的全局拓扑细化框架，通过整体区域建模、Betti数与完整性联合约束（TIB）以及配合持续同调的课程式细化，从粗到细纠正几何与拓扑错误，在气道与Willis环数据上显著降低β0错误并提升树长与分支检出率。


<details>
  <summary>Details</summary>
Motivation: 现有分割/重建方法多依赖体素重叠度量，难以反映和保证全局拓扑正确性；已有的拓扑感知损失与持续同调约束多为patch级，无法在推理阶段保障全局连通与形态完整，且难以有效修正细粒度分支错误。因此需要一个能在全局尺度上建模并可逐步纠错的拓扑细化方法。

Method: 提出TopoSculpt：1) 全局整体区域建模，捕捉完整空间上下文；2) 首次提出Topological Integrity Betti（TIB）约束，同时引入Betti数先验与全局完整性约束，确保连通分量与空腔等拓扑指标符合期望；3) 以持续同调为指导的课程式（coarse-to-fine）细化流程，递进地修复从粗大结构到细小分支的错误。

Result: 在气道与Willis环数据集上，几何与拓扑指标显著改善：气道β0错误由69.00降至3.40，CoW由1.65降至0.30；树长与分支检出率提升近10%，显示对关键拓扑错误的有效纠正。

Conclusion: TopoSculpt通过全局建模、TIB约束与持续同调驱动的课程式细化，实现对复杂3D管状解剖结构的高保真拓扑修复，显著提升连通性与分支完整性，优于以往依赖体素重叠与局部拓扑约束的方法。

Abstract: Medical tubular anatomical structures are inherently three-dimensional
conduits with lumens, enclosing walls, and complex branching topologies.
Accurate reconstruction of their geometry and topology is crucial for
applications such as bronchoscopic navigation and cerebral arterial
connectivity assessment. Existing methods often rely on voxel-wise overlap
measures, which fail to capture topological correctness and completeness.
Although topology-aware losses and persistent homology constraints have shown
promise, they are usually applied patch-wise and cannot guarantee global
preservation or correct geometric errors at inference. To address these
limitations, we propose a novel TopoSculpt, a framework for topological
refinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a
holistic whole-region modeling strategy to capture full spatial context, (ii)
first introduces a Topological Integrity Betti (TIB) constraint that jointly
enforces Betti number priors and global integrity, and (iii) employs a
curriculum refinement scheme with persistent homology to progressively correct
errors from coarse to fine scales. Extensive experiments on challenging
pulmonary airway and Circle of Willis datasets demonstrate substantial
improvements in both geometry and topology. For instance, $\beta_{0}$ errors
are reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on
the CoW dataset, with Tree length detected and branch detected rates improving
by nearly 10\%. These results highlight the effectiveness of TopoSculpt in
correcting critical topological errors and advancing the high-fidelity modeling
of complex 3D tubular anatomy. The project homepage is available at:
https://github.com/Puzzled-Hui/TopoSculpt.

</details>


### [27] [Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture](https://arxiv.org/abs/2509.03950)
*Alvaro Aranibar Roque,Helga Sebastian*

Main category: cs.CV

TL;DR: 提出一种基于U-Net（EfficientNet-B4编码器）的胸片气胸自动分割模型，在独立数据集PTX-498上取得IoU 0.7008、Dice 0.8241，显示良好定位能力，可辅助放射科医师。


<details>
  <summary>Details</summary>
Motivation: 气胸若未及时发现可危及生命；胸片为首选检查但小体积气胸征象微弱、易漏诊，需自动化、稳定的计算机辅助检测/分割工具提升敏感性与效率。

Method: 采用U-Net结构，编码器为EfficientNet-B4；在SIIM-ACR数据集上训练，使用数据增强；损失函数为二元交叉熵与Dice组合；评估在独立的PTX-498数据集上进行。

Result: 在PTX-498上获得IoU 0.7008、Dice 0.8241，表明模型对气胸区域具较高分割精度和一致性。

Conclusion: 该深度学习分割管线能较准确地定位气胸，具有临床辅助价值以支持放射科诊断，但仍需更多外部验证与前瞻性评估以确认泛化与稳健性。

Abstract: Pneumothorax, the abnormal accumulation of air in the pleural space, can be
life-threatening if undetected. Chest X-rays are the first-line diagnostic
tool, but small cases may be subtle. We propose an automated deep-learning
pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax
regions. Trained on the SIIM-ACR dataset with data augmentation and a combined
binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and
Dice score of 0.8241 on the independent PTX-498 dataset. These results
demonstrate that the model can accurately localize pneumothoraces and support
radiologists.

</details>


### [28] [ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection](https://arxiv.org/abs/2509.03951)
*Zhu Wenjie,Zhang Yabin,Xin Jin,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出ANTS：利用多模态大模型生成并自适应融合两类负文本空间，显著提升近/远域OOD检测，ImageNet上FPR95降4.2%，方法零训练零样本、可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有用负标签提升OOD检测的方法对OOD图像理解不足，负空间不精确；且近域OOD中易出现假负标签，导致性能显著下降。需要一种能准确刻画OOD分布并减少假负的负空间构建方式。

Method: 以MLLM为核心：1) 先识别可能的OOD图像作为负图；2) 让MLLM对这些负图进行描述，生成细粒度、表达力强的负文本（负句子）以刻画远域OOD分布；3) 面向近域OOD，先在ID类别中找与负图视觉相似的子集，再让MLLM基于推理生成与该子集“视觉相似但为负”的定制化负标签，降低假负；4) 设计自适应加权评分，自主平衡两类负文本空间（远域与近域），无需任务先验；整体无需训练、零样本。

Result: 在ImageNet基准上，FPR95降低4.2%，达到新的SOTA；方法具备训练-free与zero-shot特性，适合大规模部署。

Conclusion: 通过MLLM构建自适应负文本空间并动态融合，既提升远域OOD识别，又缓解近域假负问题，实现不同OOD场景下的稳健检测与可扩展性。

Abstract: The introduction of negative labels (NLs) has proven effective in enhancing
Out-of-Distribution (OOD) detection. However, existing methods often lack an
understanding of OOD images, making it difficult to construct an accurate
negative space. In addition, the presence of false negative labels
significantly degrades their near-OOD performance. To address these issues, we
propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the
understanding and reasoning capabilities of multimodal large language models
(MLLMs). Specifically, we identify images likely to be OOD samples as negative
images and prompt the MLLM to describe these images, generating expressive
negative sentences that precisely characterize the OOD distribution and enhance
far-OOD detection. For the near-OOD setting, where OOD samples resemble the
in-distribution (ID) subset, we first identify the subset of ID classes that
are visually similar to negative images and then leverage the reasoning
capability of MLLMs to generate visually similar negative labels tailored to
this subset, effectively reducing false negatives and improving near-OOD
detection. To balance these two types of negative textual spaces, we design an
adaptive weighted score that enables the method to handle different OOD task
settings (near-OOD and far-OOD) without relying on task-specific prior
knowledge, making it highly adaptable in open environments. On the ImageNet
benchmark, our ANTS significantly reduces the FPR95 by 4.2\%, establishing a
new state-of-the-art. Furthermore, our method is training-free and zero-shot,
enabling high scalability.

</details>


### [29] [Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection](https://arxiv.org/abs/2509.03961)
*Yijun Zhou,Yikui Zhai,Zilu Ying,Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Xiaolin Tian,Xudong Jia,Hongsheng Zhang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 提出MMChange：结合图像与文本的多模态遥感变化检测方法，通过图像特征精炼、文本差异增强与图文融合显著提升准确性与鲁棒性，三套RSCD基准均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 单一图像模态的RSCD方法在光照变化与噪声干扰下易退化，且难以充分建模语义变化与具备良好泛化，亟需引入额外模态提升表征与鲁棒性。

Method: 1) 图像特征精炼(IFR)：突出关键区域，抑制环境噪声；2) 利用视觉语言模型对双时相图像生成语义描述；3) 文本差异增强(TDE)：对语义描述进行细粒度差异建模，引导关注“有意义”的变化；4) 图文特征融合(ITFF)：跨模态深度融合以缓解模态异构性。

Result: 在LEVIRCD、WHUCD、SYSUCD三数据集上，MMChange在多项指标上持续超过现有SOTA，显示更高精度与更强鲁棒性。

Conclusion: 多模态（图像+文本）结合、配合IFR/TDE/ITFF模块能有效提升RSCD在复杂场景下的准确性与泛化能力，方法经多基准验证有效；代码已开源。

Abstract: Although deep learning has advanced remote sensing change detection (RSCD),
most methods rely solely on image modality, limiting feature representation,
change pattern modeling, and generalization especially under illumination and
noise disturbances. To address this, we propose MMChange, a multimodal RSCD
method that combines image and text modalities to enhance accuracy and
robustness. An Image Feature Refinement (IFR) module is introduced to highlight
key regions and suppress environmental noise. To overcome the semantic
limitations of image features, we employ a vision language model (VLM) to
generate semantic descriptions of bitemporal images. A Textual Difference
Enhancement (TDE) module then captures fine grained semantic shifts, guiding
the model toward meaningful changes. To bridge the heterogeneity between
modalities, we design an Image Text Feature Fusion (ITFF) module that enables
deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and
SYSUCD demonstrate that MMChange consistently surpasses state of the art
methods across multiple metrics, validating its effectiveness for multimodal
RSCD. Code is available at: https://github.com/yikuizhai/MMChange.

</details>


### [30] [SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification](https://arxiv.org/abs/2509.03973)
*Yu Bai,Zitong Yu,Haowen Tian,Xijing Wang,Shuo Yan,Lin Wang,Honglin Li,Xitong Ling,Bo Zhang,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: cs.CV

TL;DR: 提出SAC-MIL用于WSI分类：以坐标为位置编码、用MLP实现线性复杂度的全实例相关建模，解决长度外推与部署难题，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有WSI多实例学习常用基于序列索引的位置编码与Transformer相关建模：1) 位置编码不反映真实空间关系；2) 训练/测试序列长度不一致导致外推差；3) Transformer全局注意力复杂度高且依赖自定义CUDA，部署困难。因此需要一种能显式利用空间信息、具备长度外推能力、计算/工程更友好的方法。

Method: - 空间位置编码：用实例在切片中的二维坐标编码空间关系，替代按实例顺序的索引编码，并设计可长度外推的编码方式。
- SAC块：基于MLP的全实例相关建模模块，在序列长度上实现线性时间复杂度，避免注意力的二次复杂度；简单结构无需自定义CUDA，便于部署。
- 将上述组件组成SAC-MIL框架用于WSI分类。

Result: 在CAMELYON-16、TCGA-LUNG、TCGA-BRAC三个数据集上取得SOTA性能（具体指标未在摘要中给出），显示了方法的有效性与泛化性。

Conclusion: 引入空间感知的位置编码与MLP式全实例相关建模，可在保持或提升性能的同时降低复杂度并提升可部署性，且具备对不同长度序列的外推能力。代码将在录用后开源。

Abstract: We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for
performing WSI classification. SAC-MIL consists of a positional encoding module
to encode position information and a SAC block to perform full instance
correlations. The positional encoding module utilizes the instance coordinates
within the slide to encode the spatial relationships instead of the instance
index in the input WSI sequence. The positional encoding module can also handle
the length extrapolation issue where the training and testing sequences have
different lengths. The SAC block is an MLP-based method that performs full
instance correlation in linear time complexity with respect to the sequence
length. Due to the simple structure of MLP, it is easy to deploy since it does
not require custom CUDA kernels, compared to Transformer-based methods for WSI
classification. SAC-MIL has achieved state-of-the-art performance on the
CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon
acceptance.

</details>


### [31] [Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training](https://arxiv.org/abs/2509.03975)
*Daniel Sobotka,Alexander Herold,Matthias Perkonigg,Lucian Beer,Nina Bastati,Alina Sablatnig,Ahmed Ba-Ssalamah,Georg Langs*

Main category: cs.CV

TL;DR: 提出一种多任务学习框架，在无对比增强的肝脏MRI中分割血管；训练期利用仅在训练可用的对比增强MRI（含/不含标注）作为辅助，提高在推理期仅用无增强图像的分割精度，尤其在标注稀缺时效果显著；并在脑肿瘤分割上验证可迁移性。


<details>
  <summary>Details</summary>
Motivation: 临床上肝脏弥漫性疾病的评估依赖血管重塑的计算分析，但现有血管分割多数依靠对比增强MRI；然而增强序列并不总被采集，而无增强图像更常见且难以分割，并需要大量标注数据。因此需要一种在无增强条件下仍能高精度分割、且降低标注需求的方法。

Method: 构建多任务学习框架，联合利用配对的无增强与对比增强MRI数据进行训练：包含带/不带血管标注的样本。通过共享特征表示和任务结构，将对比增强模态作为训练期仅可用的辅助信息（privileged/auxiliary modality），以蒸馏或协同学习方式提升无增强分割器的表示能力。推理阶段只输入无增强图像。

Result: 引入辅助的对比增强数据在没有推理期可用性的前提下，显著提升无增强肝脏MRI血管分割精度；当标注样本很少时增益最大。跨域实验（脑肿瘤分割）同样观察到性能提升，证明方法的通用性。

Conclusion: 训练期可用但推理期不可用的“辅助信息模态”能有效增强分割模型，对降低标注依赖与跨域任务均有帮助；该策略为在资源受限或成像协议不统一的环境下进行高质量医学影像分割提供了实用方案。

Abstract: Liver vessel segmentation in magnetic resonance imaging data is important for
the computational analysis of vascular remodelling, associated with a wide
spectrum of diffuse liver diseases. Existing approaches rely on contrast
enhanced imaging data, but the necessary dedicated imaging sequences are not
uniformly acquired. Images without contrast enhancement are acquired more
frequently, but vessel segmentation is challenging, and requires large-scale
annotated data. We propose a multi-task learning framework to segment vessels
in liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data
available only during training to reduce the need for annotated training
examples. Our approach draws on paired native and contrast enhanced data with
and without vessel annotations for model training. Results show that auxiliary
data improves the accuracy of vessel segmentation, even if they are not
available during inference. The advantage is most pronounced if only few
annotations are available for training, since the feature representation
benefits from the shared task structure. A validation of this approach to
augment a model for brain tumor segmentation confirms its benefits across
different domains. An auxiliary informative imaging modality can augment expert
annotations even if it is only available during training.

</details>


### [32] [Promptception: How Sensitive Are Large Multimodal Models to Prompts?](https://arxiv.org/abs/2509.03986)
*Mohamed Insaf Ismithdeen,Muhammad Uzair Khattak,Salman Khan*

Main category: cs.CV

TL;DR: 提出Promptception框架系统评估LMM在多选题中的提示敏感性，发现提示表述的细微变化可致准确率最高约15%的波动；专有模型更敏感、对语义对齐更紧，开源模型更稳但对复杂表述吃力，并据此给出分别适配的提示原则以提升评测公平与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LMM在MCQA任务上常以“精选提示”的最佳结果汇报，提示设计缺乏系统性与可复现性；微小措辞/结构改变导致大幅性能波动，影响透明、公平评估。需要一个可控、全面的基准方法来量化提示敏感性并指导更稳健的评测与使用。

Method: 构建Promptception框架：61种提示类型，覆盖15类与6个超类，分别刻意操纵提示措辞与结构维度；在3个MCQA基准（MMStar、MMMU-Pro、MVBench）上，对10个LMM（含轻量开源与专有模型如GPT-4o、Gemini 1.5 Pro）进行系统测试，量化不同提示对准确率的影响，并对比专有与开源模型的敏感性特征。

Result: 不同提示导致的性能波动可达约15%（取决于模型与提示）；专有模型对提示语义更“对齐”，因此对措辞更敏感；开源模型对提示扰动较稳，但在细腻、复杂表述下性能下降明显。由此归纳出不同模型族群的提示设计差异与规律。

Conclusion: 提示敏感性是LMM MCQA评估的关键变量；应采用系统化、多样化提示集合进行公平评估，并依据模型类型分别应用提示原则，以获得更鲁棒、可复现的结果。

Abstract: Despite the success of Large Multimodal Models (LMMs) in recent years, prompt
design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly
understood. We show that even minor variations in prompt phrasing and structure
can lead to accuracy deviations of up to 15% for certain prompts and models.
This variability poses a challenge for transparent and fair LMM evaluation, as
models often report their best-case performance using carefully selected
prompts. To address this, we introduce Promptception, a systematic framework
for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,
spanning 15 categories and 6 supercategories, each targeting specific aspects
of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight
open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:
MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit
greater sensitivity to prompt phrasing, reflecting tighter alignment with
instruction semantics, while open-source models are steadier but struggle with
nuanced and complex phrasing. Based on this analysis, we propose Prompting
Principles tailored to proprietary and open-source LMMs, enabling more robust
and fair model evaluation.

</details>


### [33] [SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation](https://arxiv.org/abs/2509.03999)
*Han Huang,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen*

Main category: cs.CV

TL;DR: 提出SliceSemOcc：基于垂直切片的多模态3D语义占用预测框架，结合全局/局部高度切片与GL融合，并引入保留高度分辨率的SEAttention3D，对不同高度层自适应加权，在nuScenes两个基准显著提升mIoU，尤其小目标类提升明显。


<details>
  <summary>Details</summary>
Motivation: BEV方法仅在2D平面建模，难以捕获垂直方向语义变化；现有占用方法常在体素处理中忽略高度轴，且SENet式通道注意力对各高度层加权一致，无法突出不同高度的关键信息，限制了3D理解精度。

Method: 1) 垂直切片特征提取：沿高度轴构建全局与局部vertical slices，分别获取整体上下文与细粒度空间细节的体素特征；2) 全局-局部融合模块(GL fusion)：自适应融合两类切片特征，兼顾细节与全局语义；3) SEAttention3D：以高度维为分组进行平均池化，保留height-wise分辨率，为每个高度层生成动态通道注意力权重，而非统一加权；4) 多模态输入与3D体素网格预测，输出语义占用（voxel-wise）结果。

Result: 在nuScenes-SurroundOcc与nuScenes-OpenOccupancy上显著提升mIoU，并在多数小物体类别上有更为突出的性能增益；消融实验验证垂直切片、GL融合和SEAttention3D各组件的有效性。

Conclusion: 显式建模高度维的重要性被验证：通过垂直切片与保留高度分辨率的注意力机制，SliceSemOcc更好地捕捉3D空间与语义变化，提升3D语义占用预测的整体性能与小目标识别能力。

Abstract: Driven by autonomous driving's demands for precise 3D perception, 3D semantic
occupancy prediction has become a pivotal research topic. Unlike
bird's-eye-view (BEV) methods, which restrict scene representation to a 2D
plane, occupancy prediction leverages a complete 3D voxel grid to model spatial
structures in all dimensions, thereby capturing semantic variations along the
vertical axis. However, most existing approaches overlook height-axis
information when processing voxel features. And conventional SENet-style
channel attention assigns uniform weight across all height layers, limiting
their ability to emphasize features at different heights. To address these
limitations, we propose SliceSemOcc, a novel vertical slice based multimodal
framework for 3D semantic occupancy representation. Specifically, we extract
voxel features along the height-axis using both global and local vertical
slices. Then, a global local fusion module adaptively reconciles fine-grained
spatial details with holistic contextual information. Furthermore, we propose
the SEAttention3D module, which preserves height-wise resolution through
average pooling and assigns dynamic channel attention weights to each height
layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy
datasets verify that our method significantly enhances mean IoU, achieving
especially pronounced gains on most small-object categories. Detailed ablation
studies further validate the effectiveness of the proposed SliceSemOcc
framework.

</details>


### [34] [Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding](https://arxiv.org/abs/2509.04009)
*Solha Kang,Esla Timothy Anzaku,Wesley De Neve,Arnout Van Messem,Joris Vankerschaver,Francois Rameau,Utku Ozbulak*

Main category: cs.CV

TL;DR: 提出一种检测视觉Transformer中虚假相关（spurious correlations）的方法，并在ImageNet与乳腺肿块分类中验证其有效性，展示训练范式与类别层面的虚假信号差异，并发布易受虚假信号影响的图像清单。


<details>
  <summary>Details</summary>
Motivation: 神经网络易借助与任务偶然相关但非因果的信号做出预测，损害模型可信度与泛化性；尤其在视觉任务中，颜色偏差、小文本等细微信号常被模型利用，需要系统地检测与缓解。

Method: 针对ViT（含监督与自监督训练）提出新的虚假相关检测方法，通过在ImageNet上大规模实验评估模型对非因果线索的依赖；分析同架构不同训练方式的影响，并对具体类别的可检测虚假信号进行定位与解释；提供受影响图像清单；并开展乳腺侵袭性肿块分类案例研究。

Result: 方法能有效识别ViT中的虚假相关；同一架构下，训练方法显著影响对虚假信号的依赖；ImageNet中某些类别包含易被模型利用的明显虚假线索，已系统整理并公开。

Conclusion: 应在模型开发与评测中纳入虚假相关检测；在使用ImageNet等基准时需谨慎对待已知含虚假信号的样本；自监督/监督训练选择会影响鲁棒性；该方法在医疗影像等现实场景中同样适用。

Abstract: Due to their powerful feature association capabilities, neural network-based
computer vision models have the ability to detect and exploit unintended
patterns within the data, potentially leading to correct predictions based on
incorrect or unintended but statistically relevant signals. These clues may
vary from simple color aberrations to small texts within the image. In
situations where these unintended signals align with the predictive task,
models can mistakenly link these features with the task and rely on them for
making predictions. This phenomenon is referred to as spurious correlations,
where patterns appear to be associated with the task but are actually
coincidental. As a result, detection and mitigation of spurious correlations
have become crucial tasks for building trustworthy, reliable, and generalizable
machine learning models. In this work, we present a novel method to detect
spurious correlations in vision transformers, a type of neural network
architecture that gained significant popularity in recent years. Using both
supervised and self-supervised trained models, we present large-scale
experiments on the ImageNet dataset demonstrating the ability of the proposed
method to identify spurious correlations. We also find that, even if the same
architecture is used, the training methodology has a significant impact on the
model's reliance on spurious correlations. Furthermore, we show that certain
classes in the ImageNet dataset contain spurious signals that are easily
detected by the models and discuss the underlying reasons for those spurious
signals. In light of our findings, we provide an exhaustive list of the
aforementioned images and call for caution in their use in future research
efforts. Lastly, we present a case study investigating spurious signals in
invasive breast mass classification, grounding our work in real-world
scenarios.

</details>


### [35] [Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning](https://arxiv.org/abs/2509.04023)
*Shiku Kaito,Shinnosuke Matsuo,Daiki Suehiro,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出多类多数标签学习（LML）：包的标签由包内实例的多数类决定；用计数网络预测多数类，并引入多数比例增强模块（MPEM）通过移除少数类实例提升学习；在4个数据集上优于传统MIL，消融验证各模块有效。


<details>
  <summary>Details</summary>
Motivation: 现实任务常只有包级弱监督，且包标签常由“多数实例”主导，如病理分割、投票预测、情感分析、环境监测。传统MIL多基于“存在至少一个正例”（max/OR假设）或二分类假设，难以处理多类且“多数决定”的情形，因此需要新的问题设定与模型。

Method: 1) 定义LML：包标签=包内实例的多数类。2) 计数网络（Counting Network）：对实例进行分类打分并在包内计数各类实例数，以计数结果的多数类匹配包级监督进行训练。3) 经验分析：发现多数类占比高的包更利于学习。4) 多数比例增强模块（MPEM）：通过识别并移除/抑制包内少数类实例，提升多数类占比，从而加强监督信号。5) 实验与消融：在四个数据集与传统MIL比较，评估整体系及去除各模块的影响。

Result: 在四个数据集上，所提方法整体性能优于传统MIL基线；计数网络有效学习包内类分布，MPEM进一步提升准确率/指标；消融实验显示每个模块均有正向贡献。

Conclusion: LML为多类MIL提供了新的多数规则设定；计数式训练与多数比例增强有效地将包级多数标签转化为实例级学习信号，实证上优于现有方法。代码已开源。

Abstract: The paper proposes a novel multi-class Multiple-Instance Learning (MIL)
problem called Learning from Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag-level label. The goal of LML is to
train a classification model that estimates the class of each instance using
the majority label. This problem is valuable in a variety of applications,
including pathology image segmentation, political voting prediction, customer
sentiment analysis, and environmental monitoring. To solve LML, we propose a
Counting Network trained to produce bag-level majority labels, estimated by
counting the number of instances in each class. Furthermore, analysis
experiments on the characteristics of LML revealed that bags with a high
proportion of the majority class facilitate learning. Based on this result, we
developed a Majority Proportion Enhancement Module (MPEM) that increases the
proportion of the majority class by removing minority class instances within
the bags. Experiments demonstrate the superiority of the proposed method on
four datasets compared to conventional MIL methods. Moreover, ablation studies
confirmed the effectiveness of each module. The code is available at
\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.

</details>


### [36] [Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on "Phytium + Cambricon"](https://arxiv.org/abs/2509.04043)
*Yuchen Zhu,Longxiang Yin,Kai Zhao*

Main category: cs.CV

TL;DR: 提出基于飞腾处理器+寒武纪加速卡的异构架构，结合轻量YOLOv5s与DeepSORT，实现无人机监控中的毫秒级检测-跟踪-反馈闭环，1080p下单帧50–100 ms延迟，识别准确率>98.5%。


<details>
  <summary>Details</summary>
Motivation: 传统视频监控在复杂动态场景中响应延迟>200 ms，受限于自动识别算法深度特征提取不足与计算架构效率瓶颈，难以满足实时性与精度需求，尤其在无人机等高速目标监测中。

Method: 硬件：采用飞腾FT-2000/4处理器与寒武纪MLU220加速卡协同，多卡并行提升算力。软件：将轻量化YOLOv5s检测网络与DeepSORT级联跟踪融合，构建“检测-跟踪-反馈”闭环控制链，实现实时目标检测与稳定跟踪。

Result: 在1920×1080视频流中，系统实现稳定单帧综合处理延迟50–100 ms，多尺度目标识别准确率>98.5%，兼具低时延与高精度。

Conclusion: 所提异构计算与算法融合方案显著优于传统相机系统的实时性，满足复杂场景下无人机监控需求，并验证了国产芯片在视频智能监控领域的可行性与应用价值。

Abstract: In the frontier research and application of current video surveillance
technology, traditional camera systems exhibit significant limitations of
response delay exceeding 200 ms in dynamic scenarios due to the insufficient
deep feature extraction capability of automatic recognition algorithms and the
efficiency bottleneck of computing architectures, failing to meet the real-time
requirements in complex scenes. To address this issue, this study proposes a
heterogeneous computing architecture based on Phytium processors and Cambricon
accelerator cards, constructing a UAV tracking and gazing system with
millisecond-level response capability. At the hardware level, the system adopts
a collaborative computing architecture of Phytium FT-2000/4 processors and
MLU220 accelerator cards, enhancing computing power through multi-card
parallelism. At the software level, it innovatively integrates a lightweight
YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming
a closed-loop control chain of "detection-tracking-feedback". Experimental
results demonstrate that the system achieves a stable single-frame
comprehensive processing delay of 50-100 ms in 1920*1080 resolution video
stream processing, with a multi-scale target recognition accuracy of over
98.5%, featuring both low latency and high precision. This study provides an
innovative solution for UAV monitoring and the application of domestic chips.

</details>


### [37] [A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification](https://arxiv.org/abs/2509.04050)
*Quang-Huy Che,Le-Chuong Nguyen,Gia-Nghia Tran,Dinh-Duy Phan,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: 提出一种高效的行人重识别重排序方法K-nearest Weighted Fusion (KWF)，通过聚合K个邻居特征生成多视角特征，无需微调或标注，显著提升Rank@1与mAP并具备更高效率。


<details>
  <summary>Details</summary>
Motivation: 单视角特征易受视角偏差、姿态变化与遮挡影响，导致初始检索排序不稳。多视角表征可缓解视角偏差，但现有重排序多依赖复杂后处理或监督信息，难以在大规模数据上高效应用。

Method: 从初始检索结果中，在无监督设置下为每个查询/候选选择K个最近邻，采用加权融合策略（探索多种权重选择方案）将邻居特征聚合为该目标的多视角特征；随后基于新特征重新计算相似度并对前M候选进行重排序。方法不需模型微调或额外标注。

Result: 在Market1501、MSMT17、Occluded-DukeMTMC上评估，重排序显著提升Rank@1与mAP；在MSMT17与Occluded-DukeMTMC上，Rank@1分别较初始提升9.8%与22.0%。计算效率优于现有重排序方法，适合大规模应用。

Conclusion: KWF通过邻域特征加权融合构建多视角表征，有效减轻视角偏差并提升重识别重排序性能，在保持无监督与高效率的同时取得大幅精度收益，具备实际部署价值。

Abstract: In person re-identification, re-ranking is a crucial step to enhance the
overall accuracy by refining the initial ranking of retrieved results. Previous
studies have mainly focused on features from single-view images, which can
cause view bias and issues like pose variation, viewpoint changes, and
occlusions. Using multi-view features to present a person can help reduce view
bias. In this work, we present an efficient re-ranking method that generates
multi-view features by aggregating neighbors' features using K-nearest Weighted
Fusion (KWF) method. Specifically, we hypothesize that features extracted from
re-identification models are highly similar when representing the same
identity. Thus, we select K neighboring features in an unsupervised manner to
generate multi-view features. Additionally, this study explores the weight
selection strategies during feature aggregation, allowing us to identify an
effective strategy. Our re-ranking approach does not require model fine-tuning
or extra annotations, making it applicable to large-scale datasets. We evaluate
our method on the person re-identification datasets Market1501, MSMT17, and
Occluded-DukeMTMC. The results show that our method significantly improves
Rank@1 and mAP when re-ranking the top M candidates from the initial ranking
results. Specifically, compared to the initial results, our re-ranking method
achieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:
MSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach
demonstrates substantial enhancements in computational efficiency compared to
other re-ranking methods.

</details>


### [38] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出结合双向文本融合（BiT）与类别感知时间图（CATS）的弱监督音视视频解析方法，缓解伪标签噪声放大与注意力扩散问题，在LLP与UnAV-100上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有AVVP方法要么依赖注意力架构但把噪声伪标签当作可靠监督，要么生成更丰富伪标签却被无差别注意力在时间上扩散，导致初始误差被反复放大。需要一种既能净化语义线索又能精确时序传播的方案。

Method: 引入两模块：1）Bi-Directional Text Fusion（BiT）对音频与视觉特征进行语义注入与动态校准，定位并净化更干净、更丰富的语义线索；2）Category-Aware Temporal Graph（CATS）在时间维度进行类别感知的语义传播与连接，实现精确的跨时间信息扩散。两者结合，融合两类研究方向的优势与互补性。

Result: 在LLP与UnAV-100两个基准数据集的多个关键指标上达到SOTA表现（具体数值未在摘要中给出）。

Conclusion: 通过BiT净化与CATS精确传播的联合框架，有效抑制伪标签噪声放大与注意力扩散，显著提升弱监督AVVP的时序定位与分类性能。

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [39] [TriLiteNet: Lightweight Model for Multi-Task Visual Perception](https://arxiv.org/abs/2509.04092)
*Quang-Huy Che,Duc-Khai Lam*

Main category: cs.CV

TL;DR: TriLiteNet是一种面向ADAS的轻量级多任务感知网络，在BDD100K上同时完成车辆检测、可行驶区域分割和车道线分割，以极低参数量与算力实现接近主流的精度与低延迟/低功耗推理。


<details>
  <summary>Details</summary>
Motivation: ADAS/自动驾驶需要多传感任务的实时、低延迟推理，但现有多任务模型常在精度、计算开销与部署可行性之间难以兼顾。作者希望提供一个可在嵌入式设备上部署的统一感知模型，兼顾性能、效率与可扩展性。

Method: 提出TriLiteNet多任务架构，面向全景驾驶感知，包含基础版（~2.35M参数，7.72 GFLOPs）与超小型版（~0.14M参数）两种配置。模型以单一网络同时输出车辆检测、可行驶区域分割与车道线分割结果，并针对嵌入式部署优化参数/算力与延迟/功耗。

Result: 在BDD100K上，TriLiteNet_base实现：车辆检测Recall 85.6%；可行驶区域分割mIoU 92.4%；车道线分割Acc 82.3%；仅2.35M参数、7.72 GFLOPs。Tiny版仅0.14M参数，提供最低计算开销的多任务能力。实测在嵌入式设备上具备低延迟与合理功耗。

Conclusion: TriLiteNet在性能、计算效率与可部署性之间取得平衡，适合真实场景ADAS/自动驾驶应用，并提供从tiny到base的可扩展配置与开源实现。

Abstract: Efficient perception models are essential for Advanced Driver Assistance
Systems (ADAS), as these applications require rapid processing and response to
ensure safety and effectiveness in real-world environments. To address the
real-time execution needs of such perception models, this study introduces the
TriLiteNet model. This model can simultaneously manage multiple tasks related
to panoramic driving perception. TriLiteNet is designed to optimize performance
while maintaining low computational costs. Experimental results on the BDD100k
dataset demonstrate that the model achieves competitive performance across
three key tasks: vehicle detection, drivable area segmentation, and lane line
segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of
85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for
drivable area segmentation, and an Acc of 82.3% for lane line segmentation with
only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed
model includes a tiny configuration with just 0.14M parameters, which provides
a multi-task solution with minimal computational demand. Evaluated for latency
and power consumption on embedded devices, TriLiteNet in both configurations
shows low latency and reasonable power during inference. By balancing
performance, computational efficiency, and scalability, TriLiteNet offers a
practical and deployable solution for real-world autonomous driving
applications. Code is available at https://github.com/chequanghuy/TriLiteNet.

</details>


### [40] [DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset](https://arxiv.org/abs/2509.04117)
*Mustafa Sakhai,Kaung Sithu,Min Khant Soe Oke,Maciej Wielgosz*

Main category: cs.CV

TL;DR: 提出DVS-PedX事件相机行人数据集，含合成与真实两源、配套RGB与事件帧及标签；基线SNN显示可用性但存在显著仿真到现实差距。


<details>
  <summary>Details</summary>
Motivation: 事件相机具低延迟与高动态范围，适合移动与恶劣环境下的行人安全与过街意图预测。然而缺乏针对该任务、覆盖多天气光照且兼顾合成与真实的数据集，评估与推动算法（尤其SNN与事件驱动方法）发展受限。

Method: 构建DVS-PedX数据集：1) 在CARLA中生成“接近-过街”场景的合成事件流，系统性变化天气与光照；2) 将真实JAAD行车记录仪视频经v2e转换为事件流，保留自然行为与背景。每序列提供配对RGB帧、33ms累积的事件帧、逐帧过街/非过街标签，另附原始AEDAT2.0/4.0与AVI事件视频及元数据。以SpikingJelly实现基线SNN，检验可用性并分析域差异。

Result: 基线SNN在数据集上可运行并完成检测/意图判别，但表现显示明显的仿真到现实性能落差（sim-to-real gap），提示当前模型难以直接泛化。

Conclusion: DVS-PedX为事件相机行人检测与过街意图研究提供标准化基准和多模态资源，揭示并量化域迁移挑战，鼓励开展域自适应与RGB-事件融合等方法以提升在恶劣环境中的安全感知能力。

Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.

</details>


### [41] [TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering](https://arxiv.org/abs/2509.04123)
*Ayan Banerjee,Josep Lladós,Umapada Pal,Anjan Dutta*

Main category: cs.CV

TL;DR: 提出TaleDiffusion，用迭代式多角色生成与一系列注意力/后处理机制，实现跨帧人物一致、交互可控与对白准确渲染，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到故事可视化在多角色跨帧一致性、角色交互控制与对白渲染上表现不佳，常产生伪影与对白错配，导致叙事割裂。作者希望在保证画面一致性与对白准确的同时，提升视觉质量与叙事连贯性。

Method: 1) 以LLM进行情节分解：基于提示学习生成逐帧描述、角色细节与对白。2) 有界注意力的逐框/逐框-角色掩膜（per-box mask）控制：限定角色交互范围，抑制伪影。3) 身份一致的自注意力：在扩散模型中对同一角色跨帧共享身份表征，保障外观一致。4) 区域感知的交叉注意力：提升物体/角色在指定位置的精准放置。5) 对白渲染与分配：以气泡形式渲染，并用CLIPSeg将对白准确关联到对应角色。

Result: 在一致性、噪声控制与对白渲染准确性方面优于现有方法，实验表明能生成更连贯且更少伪影的多角色故事画面。

Conclusion: TaleDiffusion通过LLM驱动的情节分解、受限注意力与身份一致机制，结合区域感知注意力与CLIPSeg后处理，实现多角色故事可视化的跨帧一致与对白准确分配，整体性能优于现有基线。

Abstract: Text-to-story visualization is challenging due to the need for consistent
interaction among multiple characters across frames. Existing methods struggle
with character consistency, leading to artifact generation and inaccurate
dialogue rendering, which results in disjointed storytelling. In response, we
introduce TaleDiffusion, a novel framework for generating multi-character
stories with an iterative process, maintaining character consistency, and
accurate dialogue assignment via postprocessing. Given a story, we use a
pre-trained LLM to generate per-frame descriptions, character details, and
dialogues via in-context learning, followed by a bounded attention-based
per-box mask technique to control character interactions and minimize
artifacts. We then apply an identity-consistent self-attention mechanism to
ensure character consistency across frames and region-aware cross-attention for
precise object placement. Dialogues are also rendered as bubbles and assigned
to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion
outperforms existing methods in consistency, noise reduction, and dialogue
rendering.

</details>


### [42] [MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation](https://arxiv.org/abs/2509.04126)
*Yuan Zhao,Liu Lin*

Main category: cs.CV

TL;DR: 提出MEPG框架，用位置/风格感知LLM分解提示为空间坐标与风格指令，并用多专家扩散在局部与全局区域动态路由专家模型，显著提升复杂多要素提示下的画质与风格多样性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型对复杂、多元素提示理解与布局控制不足，且风格多样性受限；需要能够准确解析空间布局与风格需求，并在生成中有针对性地调用不同能力的模型。

Method: 提出两部分：1) PSA（Position-Style-Aware）模块：经监督微调的LLM将输入提示分解为精确的空间坐标与风格编码的语义指令；2) MED（Multi-Expert Diffusion）模块：在局部与全局层面进行跨区域生成，利用注意力门控进行动态专家路由，为每个空间子区域选择合适的专家（写实、风格化等）。框架支持专家模型的轻量集成/替换，并配套交互界面进行实时布局编辑与逐区风格选择。

Result: 在相同主干模型下，MEPG在图像质量与风格多样性上显著优于基线。

Conclusion: 通过位置/风格感知的提示解析与跨区域多专家扩散协作，MEPG有效解决复杂提示与风格受限问题，具备可扩展与交互友好的特性，并在实验中取得明显性能提升。

Abstract: Text-to-image diffusion models have achieved remarkable image quality, but
they still struggle with complex, multiele ment prompts, and limited stylistic
diversity. To address these limitations, we propose a Multi-Expert Planning and
Gen eration Framework (MEPG) that synergistically integrates position- and
style-aware large language models (LLMs) with spatial-semantic expert modules.
The framework comprises two core components: (1) a Position-Style-Aware (PSA)
module that utilizes a supervised fine-tuned LLM to decom pose input prompts
into precise spatial coordinates and style encoded semantic instructions; and
(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera
tion through dynamic expert routing across both local regions and global areas.
During the generation process for each lo cal region, specialized models (e.g.,
realism experts, styliza tion specialists) are selectively activated for each
spatial par tition via attention-based gating mechanisms. The architec ture
supports lightweight integration and replacement of ex pert models, providing
strong extensibility. Additionally, an interactive interface enables real-time
spatial layout editing and per-region style selection from a portfolio of
experts. Ex periments show that MEPG significantly outperforms base line models
with the same backbone in both image quality
  and style diversity.

</details>


### [43] [Revisiting Simple Baselines for In-The-Wild Deepfake Detection](https://arxiv.org/abs/2509.04150)
*Orlando Castaneda,Kevin So-Tang,Kshitij Gurung*

Main category: cs.CV

TL;DR: 改进超参数后的简单预训练视觉骨干微调法，在Deepfake-Eval-2024上将准确率提升到81%，逼近商业检测器（82%），显著超越此前同类开源基线（61–69%）。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测多在可控数据集上评估，缺乏对“野外”场景的实用性验证；最新基准Deepfake-Eval-2024显示开源模型落后商业方案，亟需检验简单可复现方法能否在真实条件下缩小差距。

Method: 复现并改进Ojha等提出的通用方法：将标准预训练视觉骨干进行适配与微调用于深伪检测，系统调优训练超参数（如学习率、数据增广、正则化、训练时长、采样策略等），在Deepfake-Eval-2024上评测，同时分析精度、算力成本与可解释性权衡。

Result: 在相同基准上，改进的超参数设定使该简单方法达到81%准确率，较先前报告的同基线提升约18个百分点，并接近商业检测器的82%。

Conclusion: 通过细致的超参数调优，开源且简单的预训练骨干微调方案即可在“野外”基准上与商业检测器竞争，具有良好的实用前景；部署需在准确率、计算开销与可解释性之间做权衡。

Abstract: The widespread adoption of synthetic media demands accessible deepfake
detectors and realistic benchmarks. While most existing research evaluates
deepfake detectors on highly controlled datasets, we focus on the recently
released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on
Deepfake-Eval-2024 showed that three finetuned open-source models achieve
accuracies between 61% and 69%, significantly lagging behind the leading
commercial deepfake detector with 82% accuracy. Our work revisits one of these
baseline approaches, originally introduced by Ojha et al., which adapts
standard pretrained vision backbones to produce generalizable deepfake
detectors. We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors. We
discuss tradeoffs in accuracy, computational costs, and interpretability,
focusing on how practical these deepfake detectors might be when deployed in
real-world settings. Our code can be found at
https://github.com/Deepfake-Detection-KKO/deepfake-detection.

</details>


### [44] [YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components](https://arxiv.org/abs/2509.04156)
*Serhii Svystun,Pavlo Radiuk,Oleksandr Melnychenko,Oleg Savenko,Anatoliy Sachenko*

Main category: cs.CV

TL;DR: 利用多光谱（可见光+热成像）数据，将通用YOLOv8与热成像专用模型集成，并通过边界框融合提升风电场部件缺陷检测，mAP@0.5=0.93、F1=0.90，优于单一YOLOv8。


<details>
  <summary>Details</summary>
Motivation: 风电场叶片、塔筒等关键部件需高可靠巡检；UAV可高效采集多光谱数据，但单一通道/单模型对不同类型（视觉/热）缺陷的鲁棒性不足，需提高检测精度与稳定性。

Method: 构建由通用YOLOv8（可见光）与热成像专用模型组成的集成检测器；对两模型输出采用精细的边界框融合算法（多源预测融合），实现多光谱信息互补；在多模态数据集上评估。

Result: 集成方法达到mAP@0.5=0.93、F1=0.90，优于仅用YOLOv8（mAP@0.5=0.91），表明多模型+多光谱融合带来稳定增益。

Conclusion: 多YOLO架构与可见-热成像融合能更可靠地发现视觉与热异常，优于单模型方案，适用于风电巡检等多场景缺陷检测。

Abstract: Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.

</details>


### [45] [VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision](https://arxiv.org/abs/2509.04180)
*Safouane El Ghazouali,Umberto Michelucci*

Main category: cs.CV

TL;DR: VisioFirm是一款开源、浏览器可用的AI辅助图像标注工具，集成CLIP、Ultralytics、Grounding DINO与SAM等基础模型，通过低阈值高召回的预标注、过滤与交互编辑，支持多种标注类型与格式导出，并可离线运行，实测可将人工标注工作量降低约90%且保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统图像标注高度依赖人工，流程繁琐、成本高、难以扩展到大规模数据集。现有工具对复杂任务（如OBB、实例分割）支持有限或需要大量手动干预；同时，通用/自定义类别的自动化能力与易用性不足。

Method: 提出VisioFirm：在Web端集成多模型的混合管线。- 检索/过滤：用CLIP进行图像-文本关联与组件聚类，辅助消歧。- 检测与零样本：常见类别用Ultralytics预训练检测器，自定义类别用Grounding DINO零样本检测；采用低置信度阈值以最大化召回。- 冗余抑制：基于IoU图进行冗余检测抑制。- 分割：浏览器侧WebGPU加速的Segment Anything支持即时报seg。- 交互编辑：提供BBox、旋转BBox与多边形工具。- 导出与可用性：支持YOLO/COCO/VOC/CSV，多模型缓存后可离线运行。

Result: 在COCO类目测试中，初始预测“大多正确”；基于多数据集的基准评测显示，相比纯手工流程，人工标注工作量最高可减少约90%，同时保持高标注精度。

Conclusion: VisioFirm通过“高召回自动预标注+人机交互精修+浏览器侧加速”策略，实现对常见与自定义类别的高效标注，在准确性和效率上显著优于传统纯手工工具，适合大规模数据集的实际生产使用。

Abstract: AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.

</details>


### [46] [DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval](https://arxiv.org/abs/2509.04193)
*Ruohong Yang,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: 提出DUDE，用文本到图像生成模型实现特征解耦，并通过域内到跨域的互邻居逐步对齐，实现无监督跨域图像检索的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有UCIR方法对整图做跨域特征对齐，目标物体特征与域风格纠缠，导致受域间差异影响，检索不稳健。需要将物体语义与域风格分离并更稳健地进行跨域对齐。

Method: 1) 利用文本到图像生成模型进行特征解耦：从图像中分离出与类别语义相关的物体特征，去除域特定风格。2) 渐进式对齐：先在域内基于互为近邻建立可靠匹配，再将这些互邻居跨域对齐，逐步扩展，实现稳定的特征对齐。

Result: 在3个基准、覆盖13个域上达到最新最优性能（SOTA），显著优于现有UCIR方法。代码将公开。

Conclusion: 通过生成模型驱动的语义-风格解耦与渐进式互邻居对齐，DUDE有效缩小域间差异，实现更可靠的无监督跨域检索，并在多个数据集上验证其优越性。

Abstract: Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of
the same category across diverse domains without relying on annotations.
Existing UCIR methods, which align cross-domain features for the entire image,
often struggle with the domain gap, as the object features critical for
retrieval are frequently entangled with domain-specific styles. To address this
challenge, we propose DUDE, a novel UCIR method building upon feature
disentanglement. In brief, DUDE leverages a text-to-image generative model to
disentangle object features from domain-specific styles, thus facilitating
semantical image retrieval. To further achieve reliable alignment of the
disentangled object features, DUDE aligns mutual neighbors from within domains
to across domains in a progressive manner. Extensive experiments demonstrate
that DUDE achieves state-of-the-art performance across three benchmark datasets
over 13 domains. The code will be released.

</details>


### [47] [Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding](https://arxiv.org/abs/2509.04243)
*Wanfu Wang,Qipeng Huang,Guangquan Xue,Xiaobo Liang,Juntao Li*

Main category: cs.CV

TL;DR: 提出LASER框架，通过自进化多步感知与区域质量评估，提升VLM在GUI grounding中的精确坐标预测与性能，达成7B规模SoTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽具视觉-语言推理能力，但在高分辨率、多元素交互界面的GUI grounding中，难以聚焦合适区域并进行有效多步感知，尤其在需要放大检索与复杂场景下。

Method: 提出LASER：1) 采用Monte Carlo质量估计生成多样候选感知轨迹/区域；2) 结合IoU为核心的区域质量评估，联合考虑准确性与多样性，构造高质量偏好数据；3) 通过偏好优化引导模型聚焦与指令相关关键区域；4) 自适应分配多步推理/感知步数以匹配任务复杂度，实现精确坐标预测。

Result: 在ScreenSpot Pro与ScreenSpot-v2上均取得一致性能提升；在GTA1-7B上微调后，于ScreenSpot-Pro达到55.7分，刷新7B规模模型SoTA。

Conclusion: LASER有效赋能VLM进行多步主动感知与精确GUI定位，通过MC+IoU的偏好数据构建策略提升准确性与泛化性，验证了在高分辨率、复杂交互场景中的可行性与优势。

Abstract: Vision Language Models (VLMs) have recently achieved significant progress in
bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model
introduced a zoom-in search strategy that effectively elicits active perception
capabilities in VLMs, improving downstream task performance. However, enabling
VLMs to reason effectively over appropriate image regions remains a core
challenge in GUI grounding, particularly under high-resolution inputs and
complex multi-element visual interactions. In this work, we propose LASER, a
self-evolving framework that progressively endows VLMs with multi-step
perception capabilities, enabling precise coordinate prediction. Specifically,
our approach integrate Monte Carlo quality estimation with
Intersection-over-Union (IoU)-based region quality evaluation to jointly
encourage both accuracy and diversity in constructing high-quality preference
data. This combination explicitly guides the model to focus on
instruction-relevant key regions while adaptively allocating reasoning steps
based on task complexity. Comprehensive experiments on the ScreenSpot Pro and
ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating
the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER
achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new
state-of-the-art (SoTA) among 7B-scale models.

</details>


### [48] [Differential Morphological Profile Neural Networks for Semantic Segmentation](https://arxiv.org/abs/2509.04268)
*David Huangal,J. Alex Hurt*

Main category: cs.CV

TL;DR: 将差分形态学剖面（DMP）这一多尺度形状特征融入先进语义分割网络，用直接输入与双流混合两种策略，在iSAID上评估不同DMP参数；结果显示混合DMP方案稳定优于直接输入，并可在mIoU、F1、召回上超过无DMP基线。


<details>
  <summary>Details</summary>
Motivation: 遥感俯视图像与地面视角照片差异显著（尺度变化极端、前景-背景失衡、图像超大），现有SOTA分割网络多为地面场景优化，难以充分利用形状信息。既有工作表明DMP能为遥感提供关键形状线索，但主要用于分类/检测，缺少在语义分割中的系统验证。

Method: 将DMP特征与三种主流分割架构（卷积与Transformer）融合：1）直接输入：修改输入干端，使网络接受RGB+DMP多通道；2）混合（双流）：分别编码RGB与DMP特征后在中后期融合。系统评测不同DMP差分与结构元素形状/尺度设置，在iSAID数据集上比较。

Result: 非DMP基线通常优于“直接输入”DMP变体；但“混合DMP”在各架构上稳定优于“直接输入”，并且在mIoU、F1、Recall上可超过无DMP模型。

Conclusion: 在遥感语义分割中，单纯把DMP当额外通道并不稳健；采用双流混合以保留并有效融合形状与外观信息更有效，能带来可靠增益。选择合适的DMP差分与结构元素进一步提升性能。

Abstract: Semantic segmentation of overhead remote sensing imagery enables applications
in mapping, urban planning, and disaster response. State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes. We explore the incorporation of the differential morphological profile
(DMP), a multi-scale shape extraction method based on grayscale morphology,
into modern segmentation networks. Prior studies have shown that the DMP can
provide critical shape information to Deep Neural Networks to enable superior
detection and classification performance in overhead imagery. In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures. We utilize both direct input,
which adapts the input stem of feature extraction architectures to accept DMP
channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP
encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP
differentials and structuring element shapes to more effectively provide shape
information to the model. Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.

</details>


### [49] [TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models](https://arxiv.org/abs/2509.04269)
*Yuxin Gong,Se-in Jang,Wei Shao,Yi Su,Kuang Gong*

Main category: cs.CV

TL;DR: 提出一种文本引导的3D扩散模型，用MRI与血浆p-tau217作为条件来合成3D tau PET，实现逼真的、阶段可控的阿尔茨海默病tau成像，支持数据增广与疾病进程模拟。


<details>
  <summary>Details</summary>
Motivation: tau PET对AD诊断与监测重要但昂贵且可及性差；MRI与血浆生物标志物易获取且互补，但单独不足以直接可视化tau。需要一种能利用可及模态来替代或补充tau PET的生成方法。

Method: 构建文本引导的3D扩散模型：以MRI提供解剖结构约束；将血浆p-tau217数值映射为文本提示引导生成；在ADNI的AV1451 tau PET数据上训练与评估，实现条件化3D PET合成。

Result: 模型可在不同疾病阶段生成逼真且具有临床意义的3D tau PET影像；在多种设定下实现数据增广，并能依据不同p-tau217与认知状态条件生成对应的PET分布。

Conclusion: 该框架为tau PET提供低成本、非侵入的替代/补充手段，可用于数据扩充与疾病进程模拟，潜在提升临床与研究场景中对AD tau病理的可视化与评估能力。

Abstract: Accurate quantification of tau pathology via tau positron emission tomography
(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).
However, the high cost and limited availability of tau PET restrict its
widespread use. In contrast, structural magnetic resonance imaging (MRI) and
plasma-based biomarkers provide non-invasive and widely available complementary
information related to brain anatomy and disease progression. In this work, we
propose a text-guided 3D diffusion model for 3D tau PET image synthesis,
leveraging multimodal conditions from both structural MRI and plasma
measurement. Specifically, the textual prompt is from the plasma p-tau217
measurement, which is a key indicator of AD progression, while MRI provides
anatomical structure constraints. The proposed framework is trained and
evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that
our approach can generate realistic, clinically meaningful 3D tau PET across a
range of disease stages. The proposed framework can help perform tau PET data
augmentation under different settings, provide a non-invasive, cost-effective
alternative for visualizing tau pathology, and support the simulation of
disease progression under varying plasma biomarker levels and cognitive
conditions.

</details>


### [50] [Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2509.04273)
*Junying Meng,Gangxuan Zhou,Jun Liu,Weihong Guo*

Main category: cs.CV

TL;DR: 提出一种半监督医学图像分割框架，将显式体素体积先验与基于变分模型的空间正则（阈值动力学）嵌入主干网络，并通过图像尺度与数据集尺度的Wasserstein距离约束对无标注样本进行体积一致性与分布匹配，显著提升ACDC、PROMISE12与大腿肌肉MRI上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分割多数缺乏对特征提取的有效方法学约束，忽视了来自数据的关键先验（如目标区域体积比例），导致无标注数据监督信号弱、预测不稳定，泛化受限。作者希望将稳健的先验与正则项显式注入网络，提升无标注情形下的结构一致性与统计一致性。

Method: 1) 在图像尺度引入强显式体积先验：训练一个体积回归网络预测每个无标注图像的类别体积分布；通过Wasserstein距离约束，使分割结果的类别比例匹配回归预测。2) 在数据集尺度引入弱隐式体积先验：以标注集的体积分布为参考，对无标注集的预测体积分布施加Wasserstein距离损失，实现分布级匹配。3) 融合基于变分模型的阈值动力学（Threshold Dynamics）空间正则，促使分割边界更平滑、区域更连贯。4) 以上先验与正则项作为可微损失与模块无缝集成至主干分割网络。

Result: 在ACDC 2017、PROMISE12与大腿肌肉MRI数据集上，所提方法优于现有方法（摘要未列具体数值），体现出更高的分割精度与稳健性，尤其在无标注样本占比较大时表现突出。

Conclusion: 通过将体积先验与变分正则显式整合到半监督分割框架，并利用图像尺度与数据集尺度的Wasserstein一致性约束，可有效提升医学图像分割性能与稳定性。该策略通用、可与不同主干网络结合，适用于多数据集场景。

Abstract: Despite signi cant progress in semi-supervised medical image segmentation,
most existing segmentation networks overlook e ective methodological guidance
for feature extraction and important prior information from
  datasets. In this paper, we develop a semi-supervised medical image
segmentation framework that e ectively integrates spatial regularization
methods and volume priors. Speci cally, our approach integrates a strong
explicit volume prior at the image scale and Threshold Dynamics spatial
regularization, both derived from variational models, into the backbone
segmentation network. The target region volumes for each unlabeled image are
estimated by a regression network, which e ectively regularizes the backbone
segmentation network through an image-scale Wasserstein distance constraint,
ensuring that the class ratios in the segmentation results for each unlabeled
image match those predicted by the regression network. Additionally, we design
a dataset-scale Wasserstein distance loss function based on a weak implicit
volume prior, which enforces that the volume distribution predicted for the
unlabeled dataset is similar to that of labeled dataset. Experimental results
on the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset
show the superiority of the proposed method.

</details>


### [51] [PAOLI: Pose-free Articulated Object Learning from Sparse-view Images](https://arxiv.org/abs/2509.04276)
*Jianning Deng,Kartic Subr,Hakan Bilen*

Main category: cs.CV

TL;DR: 提出一种无需相机位姿监督、仅用极少视角（每种构型约4视图）的自监督框架，重建可动对象并学习跨姿态稠密对应与运动分解，最终在基准与真实数据上取得高精度细节表示。


<details>
  <summary>Details</summary>
Motivation: 现有可动体重建方法常依赖稠密多视角与真值相机位姿，限制了在真实场景与数据稀缺条件下的适用性。需要在稀疏视角、无位姿监督下学习对象的几何、外观与运动学表示。

Method: (1) 对每个关节姿态独立进行稀疏视角3D重建；(2) 基于此学习跨姿态的形变场以建立稠密对应；(3) 采用逐步解耦策略区分静态与运动部件，从而区分相机与物体运动；(4) 通过自监督联合优化几何、外观与运动学，使用跨视角与跨姿态一致性损失。

Result: 在标准基准与真实数据上，方法在极弱监督（少视角、无相机位姿）条件下仍能获得准确且细节丰富的可动体表示，优于或可比于依赖更强假设的现有方法。

Conclusion: 自监督、稀疏视角与无位姿的设定下，提出的分阶段解耦与形变场联合优化策略能稳健地区分相机与物体运动，并重建高质量的可动体表示，显著放宽输入假设与数据需求。

Abstract: We present a novel self-supervised framework for learning articulated object
representations from sparse-view, unposed images. Unlike prior methods that
require dense multi-view observations and ground-truth camera poses, our
approach operates with as few as four views per articulation and no camera
supervision. To address the inherent challenges, we first reconstruct each
articulation independently using recent advances in sparse-view 3D
reconstruction, then learn a deformation field that establishes dense
correspondences across poses. A progressive disentanglement strategy further
separates static from moving parts, enabling robust separation of camera and
object motion. Finally, we jointly optimize geometry, appearance, and
kinematics with a self-supervised loss that enforces cross-view and cross-pose
consistency. Experiments on the standard benchmark and real-world examples
demonstrate that our method produces accurate and detailed articulated object
representations under significantly weaker input assumptions than existing
approaches.

</details>


### [52] [Noisy Label Refinement with Semantically Reliable Synthetic Images](https://arxiv.org/abs/2509.04298)
*Yingxuan Li,Jiafeng Mao,Yusuke Matsui*

Main category: cs.CV

TL;DR: 用合成图像作“标注锚点”来纠正语义噪声，显著提升多数据集分类精度，并可与现有鲁棒训练方法叠加取得更大增益。


<details>
  <summary>Details</summary>
Motivation: 真实图像分类数据集中常见语义噪声（外观相近类之间的错标）会严重破坏监督学习效果；而先进文本到图像模型能提供高质量、标注可靠的合成图像，但直接用来训练存在域间差异与多样性不足。作者希望利用合成图像的“可靠标签”优势来清洗/纠正噪声数据，而非直接替代真实数据训练。

Method: 生成与各类别对应的高质量合成图像，将其作为可靠参考；度量真实样本与这些参考之间的相似性或一致性，定位疑似错标样本；对疑似样本进行纠正（重标或筛除/重加权），再在净化后的数据上训练分类器。该流程与现有噪声鲁棒训练方法正交，可作为前置或并行模块集成。

Result: 在多项基准中（如 CIFAR-10/100、ImageNet-100）与不同噪声设置（尤其是语义噪声）下，均显著提升分类精度；与最先进噪声鲁棒方法结合时，于 70% 语义噪声下 CIFAR-10 提升约 30%，CIFAR-100 提升约 11%，在真实世界噪声的 ImageNet-100 上提升约 24%。

Conclusion: 将合成图像作为“可靠参考”来识别并修正噪声标注是一条有效、通用且与现有方法互补的路径，能在高噪声尤其是语义噪声场景中显著提升分类性能。

Abstract: Semantic noise in image classification datasets, where visually similar
categories are frequently mislabeled, poses a significant challenge to
conventional supervised learning approaches. In this paper, we explore the
potential of using synthetic images generated by advanced text-to-image models
to address this issue. Although these high-quality synthetic images come with
reliable labels, their direct application in training is limited by domain gaps
and diversity constraints. Unlike conventional approaches, we propose a novel
method that leverages synthetic images as reliable reference points to identify
and correct mislabeled samples in noisy datasets. Extensive experiments across
multiple benchmark datasets show that our approach significantly improves
classification accuracy under various noise conditions, especially in
challenging scenarios with semantic label noise. Additionally, since our method
is orthogonal to existing noise-robust learning techniques, when combined with
state-of-the-art noise-robust training methods, it achieves superior
performance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100
under 70% semantic noise, and by 24% on ImageNet-100 under real-world noise
conditions.

</details>


### [53] [Efficient Odd-One-Out Anomaly Detection](https://arxiv.org/abs/2509.04326)
*Silvio Chito,Paolo Rabino,Tatiana Tommasi*

Main category: cs.CV

TL;DR: 提出一种高效的“异类检测（odd-one-out）”模型：在保持与SOTA相当性能下，将参数减少约1/3、训练时间缩短约3倍，并给出一个MLLM基线以揭示其在结构化视觉推理上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在多对象场景中做异类检测需要跨视角空间推理与关系推理，且需在不同类别与布局间泛化；当前方法复杂、训练开销大，缺乏高效而稳健的方案与对MLLM在该任务上的系统性评估。

Method: 基于DINO（自监督视觉骨干）的高效架构设计：在模型与训练策略上做参数与计算的削减，显著降低参数量与训练时长；并构建/采用多视角、多对象的场景输入，执行空间与关系推理。与此同时，构建一个多模态大语言模型（MLLM）作为对照基线，用于分析其在结构化视觉推理中的表现与瓶颈。

Result: 与当前SOTA相比，参数减少约三分之一，训练时间缩短约三倍，同时性能保持竞争力（接近SOTA）。此外，MLLM基线显示出在结构化视觉推理与跨视角关系理解方面的明显不足。

Conclusion: 高效的DINO-based方案能在显著降低资源消耗的同时维持异类检测的高水平表现；而MLLM目前仍难以胜任需要精细结构与关系推理的视觉任务。项目页提供更多细节与资源。

Abstract: The recently introduced odd-one-out anomaly detection task involves
identifying the odd-looking instances within a multi-object scene. This problem
presents several challenges for modern deep learning models, demanding spatial
reasoning across multiple views and relational reasoning to understand context
and generalize across varying object categories and layouts. We argue that
these challenges must be addressed with efficiency in mind. To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance. Our experimental
evaluation also introduces a Multimodal Large Language Model baseline,
providing insights into its current limitations in structured visual reasoning
tasks. The project page can be found at
https://silviochito.github.io/EfficientOddOneOut/

</details>


### [54] [GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization](https://arxiv.org/abs/2509.04334)
*Pengyue Jia,Yingyi Zhang,Xiangyu Zhao,Yixuan Li*

Main category: cs.CV

TL;DR: 提出GeoArena，一个用于全球图像地理定位的开放评测平台，避免数据泄漏并用人类成对偏好取代仅基于坐标的指标，提供更真实、隐私友好的人本基准与排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有评测存在两大问题：1）数据泄漏——LVLM常在包含测试集的数据上预训练，导致评估结果高估模型实际地理定位能力；2）指标单一且隐私风险——仅用精确坐标评估忽略推理过程，且需要用户级位置信息引发隐私担忧。

Method: 构建GeoArena开放平台：允许用户上传真实世界图像扩充评测语料；采用人类成对比较投票以判断哪个模型输出更符合人类期望；在线运行两个月收集上千条投票；据此进行分析并生成不同LVLM的排行榜。

Result: 平台收集到大量成对投票数据，形成了跨模型的人类偏好评测结果和排行榜，并对模型在全球地理定位任务中的表现进行了细粒度分析。

Conclusion: GeoArena提供了更公平、真实且以人为中心的LVLM地理定位评测方案，缓解数据泄漏与隐私问题，并为未来模型改进与比较提供可靠基准。

Abstract: Image geolocalization aims to predict the geographic location of images
captured anywhere on Earth, but its global nature presents significant
challenges. Current evaluation methodologies suffer from two major limitations.
First, data leakage: advanced approaches often rely on large vision-language
models (LVLMs) to predict image locations, yet these models are frequently
pretrained on the test datasets, compromising the accuracy of evaluating a
model's actual geolocalization capability. Second, existing metrics primarily
rely on exact geographic coordinates to assess predictions, which not only
neglects the reasoning process but also raises privacy concerns when user-level
location data is required. To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking. GeoArena
enables users to upload in-the-wild images for a more diverse evaluation
corpus, and it leverages pairwise human judgments to determine which model
output better aligns with human expectations. Our platform has been deployed
online for two months, during which we collected over thousands voting records.
Based on this data, we conduct a detailed analysis and establish a leaderboard
of different LVLMs on the image geolocalization task.

</details>


### [55] [From Editor to Dense Geometry Estimator](https://arxiv.org/abs/2509.04338)
*JiYuan Wang,Chunyu Lin,Lei Sun,Rongying Liu,Lang Nie,Mingxing Li,Kang Liao,Xiangxiang Chu,Yao Zhao*

Main category: cs.CV

TL;DR: 提出FE2E：用基于DiT的图像编辑扩散模型替代T2I生成模型作为基础，针对稠密几何（深度与法线）估计进行微调，借助一致速度目标与对数量化，零样本跨数据集显著提升，优于使用百倍数据训练的DepthAnything。


<details>
  <summary>Details</summary>
Motivation: 稠密预测本质是图像到图像任务，现有方法多利用T2I生成模型的视觉先验，但编辑模型更贴近编辑/变换输入图像的范式，或更适合微调。作者系统比较编辑器与生成器在稠密几何估计中的微调行为，发现编辑模型有更强结构先验与更稳定收敛。

Method: 1) 基于Diffusion Transformer的高级编辑模型作为骨干；2) 将原编辑器的flow matching损失重构为“consistent velocity”训练目标，使之适配确定性的稠密几何任务；3) 采用对数量化，解决编辑器原生BFloat16与任务高精度需求的冲突；4) 利用DiT全局注意力，在一次前向中联合估计深度与法线，使两者监督互相增强；5) 不扩大训练数据规模进行训练与评估。

Result: 在多数据集的零样本单目深度与法线估计上取得显著提升：ETH3D上超过35%的性能增益；整体性能超过使用约100倍数据训练的DepthAnything系列。

Conclusion: 图像编辑扩散模型作为稠密预测的基础优于T2I生成模型。通过一致速度目标、对数量化与全局注意力的联合估计，FE2E在无需扩充数据的前提下实现稳定训练与显著SOTA提升，验证了编辑模型内在结构先验的价值。

Abstract: Leveraging visual priors from pre-trained text-to-image (T2I) generative
models has shown success in dense prediction. However, dense prediction is
inherently an image-to-image task, suggesting that image editing models, rather
than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning
behaviors of both editors and generators for dense geometry estimation. Our
findings show that editing models possess inherent structural priors, which
enable them to converge more stably by ``refining" their innate features, and
ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction. Specifically, to tailor the
editor for this deterministic task, we reformulate the editor's original flow
matching loss into the ``consistent velocity" training objective. And we use
logarithmic quantization to resolve the precision conflict between the editor's
native BFloat16 format and the high precision demand of our tasks.
Additionally, we leverage the DiT's global attention for a cost-free joint
estimation of depth and normals in a single forward pass, enabling their
supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance
improvements in zero-shot monocular depth and normal estimation across multiple
datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100$\times$ data.
The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.

</details>


### [56] [MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition](https://arxiv.org/abs/2509.04344)
*Feng-Qi Cui,Zhen Lin,Xinlong Rao,Anyang Tong,Shiyao Li,Fei Wang,Changlin Chen,Bin Liu*

Main category: cs.CV

TL;DR: 提出MICACL：结合图增强实例交互、加权实例聚合与多尺度类感知对比学习，提升动态表情识别在长尾分布与时空建模下的性能与泛化。


<details>
  <summary>Details</summary>
Motivation: DFER受两大痛点制约：1) 类别长尾导致模型偏置；2) 时空特征建模复杂，难以捕捉帧间与实例间依赖。现有深度方法未能同时有效处理二者，造成诱导偏差与鲁棒性不足。

Method: 构建多实例学习框架MICACL，包括：1) GEIIM：通过自适应邻接矩阵与多尺度卷积建模相邻实例间复杂时空关系（图增强交互）；2) WIAN：依据实例重要性动态赋权，实现更优的实例级特征聚合；3) MCCL：多尺度、类别感知的对比学习，重平衡主尾类别训练，缓解长尾。

Result: 在DFEW与FERV39k等in-the-wild数据集上达到SOTA，表现出更强鲁棒性与泛化能力。

Conclusion: 通过将图增强的时空依赖建模、动态加权聚合与类感知对比学习整合于MICACL，有效缓解长尾偏置并提升DFER性能与泛化，优于现有方法。

Abstract: Dynamic facial expression recognition (DFER) faces significant challenges due
to long-tailed category distributions and complexity of spatio-temporal feature
modeling. While existing deep learning-based methods have improved DFER
performance, they often fail to address these issues, resulting in severe model
induction bias. To overcome these limitations, we propose a novel
multi-instance learning framework called MICACL, which integrates
spatio-temporal dependency modeling and long-tailed contrastive learning
optimization. Specifically, we design the Graph-Enhanced Instance Interaction
Module (GEIIM) to capture intricate spatio-temporal between adjacent instances
relationships through adaptive adjacency matrices and multiscale convolutions.
To enhance instance-level feature aggregation, we develop the Weighted Instance
Aggregation Network (WIAN), which dynamically assigns weights based on instance
importance. Furthermore, we introduce a Multiscale Category-aware Contrastive
Learning (MCCL) strategy to balance training between major and minor
categories. Extensive experiments on in-the-wild datasets (i.e., DFEW and
FERV39k) demonstrate that MICACL achieves state-of-the-art performance with
superior robustness and generalization.

</details>


### [57] [Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage](https://arxiv.org/abs/2509.04370)
*Dor Cohen,Inga Efrosman,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: 提出一条从执法记录仪视频自动生成全景摘要图的视觉管线，以便快速态势感知与事后回顾。


<details>
  <summary>Details</summary>
Motivation: 紧急响应中无法在有限时间内回看冗长视频，但又需要对现场环境快速、直观地掌握全局。需要将连续视频浓缩为易于解读的视觉摘要。

Method: 使用单目SLAM估计相机轨迹并重建环境空间布局；沿轨迹对相机位姿聚类以发现关键视点；从每簇选代表帧；利用多帧拼接将这些帧融合为空间一致的全景图，从而得到场景摘要。

Result: 生成的全景摘要图能在保持空间一致性的同时覆盖关键视点，帮助快速理解复杂环境并提升决策与复盘效率。

Conclusion: 该管线可将穿戴式摄像机视频有效转化为信息密集的全景图片摘要，改善态势感知与事件审查流程。

Abstract: First responders widely adopt body-worn cameras to document incident scenes
and support post-event analysis. However, reviewing lengthy video footage is
impractical in time-critical situations. Effective situational awareness
demands a concise visual summary that can be quickly interpreted. This work
presents a computer vision pipeline that transforms body-camera footage into
informative panoramic images summarizing the incident scene. Our method
leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate
camera trajectories and reconstruct the spatial layout of the environment. Key
viewpoints are identified by clustering camera poses along the trajectory, and
representative frames from each cluster are selected. These frames are fused
into spatially coherent panoramic images using multi-frame stitching
techniques. The resulting summaries enable rapid understanding of complex
environments and facilitate efficient decision-making and incident review.

</details>


### [58] [AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval for Text-Based Person Anomaly Search](https://arxiv.org/abs/2509.04376)
*Hao Ju,Hu Zhang,Zhedong Zheng*

Main category: cs.CV

TL;DR: 提出AnomalyLMM，用于零样本文本描述的人体异常检索，通过粗到细管线与无训练适配策略，提升跨模态细粒度对齐与稀样本异常识别，PAB上Recall@1提升0.96%。


<details>
  <summary>Details</summary>
Motivation: 公共安全需求增长，需根据自然语言描述从视频/图像中检索异常行为个体。现有问题：文本-视觉异常细粒度对齐难、真实场景异常样本稀缺。LMM虽强于多模态理解，但在检索任务受限于生成-判别域间差距与缺乏高效部署适配。

Method: 提出AnomalyLMM框架：1) 粗到细（coarse-to-fine）流程，将LMM的生成性世界知识与检索式异常检测对接；2) 无训练适配“食谱”：包括遮蔽式跨模态提示（masked cross-modal prompting）、行为显著性预测（behavioral saliency prediction）、知识感知重排序（knowledge-aware re-ranking），实现零样本关注细微异常线索；并进行可解释对齐分析。

Result: 在唯一公开基准PAB数据集上进行严格评测，覆盖跌倒、碰撞、被击中等多类真实异常场景；相较强竞争基线，Recall@1提升+0.96%，并在质性分析中展示了文本异常与视觉行为的可解释对齐。

Conclusion: AnomalyLMM首次将LMM用于文本驱动的人体异常检索，通过粗到细管线与训练免适配，有效缓解生成-检索域差距与稀样本问题，取得小幅但稳定的性能提升并具备可解释性；代码与模型将开源以推动后续研究。

Abstract: With growing public safety demands, text-based person anomaly search has
emerged as a critical task, aiming to retrieve individuals with abnormal
behaviors via natural language descriptions. Unlike conventional person search,
this task presents two unique challenges: (1) fine-grained cross-modal
alignment between textual anomalies and visual behaviors, and (2) anomaly
recognition under sparse real-world samples. While Large Multi-modal Models
(LMMs) excel in multi-modal understanding, their potential for fine-grained
anomaly retrieval remains underexplored, hindered by: (1) a domain gap between
generative knowledge and discriminative retrieval, and (2) the absence of
efficient adaptation strategies for deployment. In this work, we propose
AnomalyLMM, the first framework that harnesses LMMs for text-based person
anomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline
integrating LMMs to bridge generative world knowledge with retrieval-centric
anomaly detection; (2) A training-free adaptation cookbook featuring masked
cross-modal prompting, behavioral saliency prediction, and knowledge-aware
re-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study
to explore LMMs for this task, we conduct a rigorous evaluation on the PAB
dataset, the only publicly available benchmark for text-based person anomaly
search, with its curated real-world anomalies covering diverse scenarios (e.g.,
falling, collision, and being hit). Experiments show the effectiveness of the
proposed method, surpassing the competitive baseline by +0.96% Recall@1
accuracy. Notably, our method reveals interpretable alignment between textual
anomalies and visual behaviors, validated via qualitative analysis. Our code
and models will be released for future research.

</details>


### [59] [Aesthetic Image Captioning with Saliency Enhanced MLLMs](https://arxiv.org/abs/2509.04378)
*Yilin Tao,Jiashui Huang,Huaze Xu,Ling Shao*

Main category: cs.CV

TL;DR: 提出ASE-MLLM，将“审美显著性”显式注入多模态大模型，用IASM提取审美显著性并与图像特征通过跨注意力融合，实现更聚焦审美内容的图像审美描述，取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像审美研究多停留在评分预测，少用于审美描述；基于MLLM的AIC多仅做微调，未让模型聚焦目标审美要点，导致描述泛化与准确性受限。

Method: 构建端到端框架ASE-MLLM：1) 设计图像审美显著性模块IASM，高效提取审美显著性特征； 2) 设计IAS-ViT作为图像编码器，通过跨注意力将审美显著性与原始视觉特征融合； 3) 将融合后的表征输入MLLM以生成审美描述。

Result: 在主流AIC数据集上，较传统方法与通用MLLM显著提升，达成SOTA（多项指标优于对比基线）。

Conclusion: 首次将图像审美显著性显式整合进MLLM以服务AIC，证明了显著性引导能有效提升审美描述质量，为审美理解与生成融合提供新范式。

Abstract: Aesthetic Image Captioning (AIC) aims to generate textual descriptions of
image aesthetics, becoming a key research direction in the field of
computational aesthetics. In recent years, pretrained Multimodal Large Language
Models (MLLMs) have advanced rapidly, leading to a significant increase in
image aesthetics research that integrates both visual and textual modalities.
However, most existing studies on image aesthetics primarily focus on
predicting aesthetic ratings and have shown limited application in AIC.
Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods
without specifically adapting MLLMs to focus on target aesthetic content. To
address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal
Large Language Model (ASE-MLLM), an end-to-end framework that explicitly
incorporates aesthetic saliency into MLLMs. Within this framework, we introduce
the Image Aesthetic Saliency Module (IASM), which efficiently and effectively
extracts aesthetic saliency features from images. Additionally, we design
IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency
features with original image features via a cross-attention mechanism. To the
best of our knowledge, ASE-MLLM is the first framework to integrate image
aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments
demonstrated that our approach significantly outperformed traditional methods
and generic MLLMs on current mainstream AIC benchmarks, achieving
state-of-the-art (SOTA) performance.

</details>


### [60] [SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer](https://arxiv.org/abs/2509.04379)
*Jimin Xu,Bosheng Qin,Tao Jin,Zhou Zhao,Zhenhui Ye,Jun Yu,Fei Wu*

Main category: cs.CV

TL;DR: 提出一个两阶段3D风格迁移管线：先用2D扩散先验生成多关键视角的风格化渲染，再将其一致地投射到3D表示；通过跨视角注意力与实例级风格迁移两项设计，实现高水平风格语义与实例一致性，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法虽能在NeRF/3D Gaussians上保持三维一致，但难以从参考风格图中抽取高层风格语义，且结果结构性差、物体边界与实例难分。作者希望利用强大的2D扩散先验来提升风格语义表达与实例级清晰度。

Method: 两阶段流程：1) 扩散先验生成关键视角的风格化图像；在UNet最后上采样块加入跨视角注意力，使多视角特征交互，保证风格忠实与实例级一致。2) 实例级风格迁移：利用关键视角间的实例一致性，将其蒸馏/投射到3D表示（如NeRF或3D Gaussian），实现结构化的3D风格化。

Result: 在前向场景到360度复杂场景的多数据集上，定性与定量均优于当前SOTA；生成结果具备更清晰的结构分离、更高的风格语义保真与跨视角实例一致性。

Conclusion: 通过融合2D扩散先验的跨视角对齐与实例级迁移，方法实现更有结构感、艺术性与三维一致的风格化，广泛场景下优于现有方法。

Abstract: Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.

</details>


### [61] [Learning neural representations for X-ray ptychography reconstruction with unknown probes](https://arxiv.org/abs/2509.04402)
*Tingyou Li,Zixin Xu,Zirui Gao,Hanfei Yan,Xiaojing Huang,Jizhou Li*

Main category: cs.CV

TL;DR: 提出PtyINR：一种自监督的隐式神经表示方法，可同时重建X射线ptychography中的物体与探针，在无先验探针表征、低信号条件下仍能实现高保真重建，并优于传统迭代与深度学习方法。


<details>
  <summary>Details</summary>
Motivation: X射线ptychography具备纳米级分辨率，但在探针未知时的成像重建困难，尤其在低剂量/高速实验的低信噪比条件下，现有迭代与学习方法表现欠佳，限制了技术应用与推广。

Method: 将样品（object）与照明探针（probe）同时参数化为连续隐式神经表示（INR），构建物理约束驱动的自监督端到端重建框架，直接从原始衍射图进行优化，无需探针预表征；利用物理一致的前向模型和损失函数实现联合反演。

Result: 在模拟与实测数据上，相比传统迭代和现有深度学习方法，PtyINR获得更高的重建质量与更强鲁棒性，尤其在低信号条件下表现突出。

Conclusion: PtyINR作为通用、物理知情的自监督框架，有效解决了ptychography中的物体-探针联合恢复问题，并可推广到更广泛的依赖探针的计算显微成像逆问题。

Abstract: X-ray ptychography provides exceptional nanoscale resolution and is widely
applied in materials science, biology, and nanotechnology. However, its full
potential is constrained by the critical challenge of accurately reconstructing
images when the illuminating probe is unknown. Conventional iterative methods
and deep learning approaches are often suboptimal, particularly under the
low-signal conditions inherent to low-dose and high-speed experiments. These
limitations compromise reconstruction fidelity and restrict the broader
adoption of the technique. In this work, we introduce the Ptychographic
Implicit Neural Representation (PtyINR), a self-supervised framework that
simultaneously addresses the object and probe recovery problem. By
parameterizing both as continuous neural representations, PtyINR performs
end-to-end reconstruction directly from raw diffraction patterns without
requiring any pre-characterization of the probe. Extensive evaluations
demonstrate that PtyINR achieves superior reconstruction quality on both
simulated and experimental data, with remarkable robustness under challenging
low-signal conditions. Furthermore, PtyINR offers a generalizable,
physics-informed framework for addressing probe-dependent inverse problems,
making it applicable to a wide range of computational microscopy problems.

</details>


### [62] [Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios](https://arxiv.org/abs/2509.04403)
*Jingen Qu,Lijun Li,Bo Zhang,Yichen Yan,Jing Shao*

Main category: cs.CV

TL;DR: 提出一种以图像为起点的自适应数据集构建方法，面向真实多模态安全（RMS）场景，自动生成3.5万图文对及引导式响应，并提出统一评测范式：用安全裁判模型微调并在其他安全数据集上评估；实验显示该方法可扩展且有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态安全数据集多为“风险导向”，难覆盖日益复杂的真实RMS场景，且缺乏统一评估指标，导致方法有效性难以比较与验证。

Method: 图像导向的自适应数据构建流水线：以图像为起点，自动生成匹配文本与安全引导式响应，形成RMS图文对；并提出标准化评测：微调一个“安全裁判”模型，并用其在其他安全数据集上进行评估，以度量数据集质量与泛化。

Result: 构建了约35k张图像-文本-引导响应的RMS数据集；在多种任务上的广泛实验显示，所提图像导向流水线具备良好可扩展性与有效性，评测范式能够比较不同安全数据集的质量。

Conclusion: 图像导向、自适应的数据集构建为真实多模态安全数据提供了新的思路，并配套统一评测范式；实验证明该方法可扩展且有效，有助于推进MLLM安全数据集的构建与评估。

Abstract: Multimodal large language models (MLLMs) are rapidly evolving, presenting
increasingly complex safety challenges. However, current dataset construction
methods, which are risk-oriented, fail to cover the growing complexity of
real-world multimodal safety scenarios (RMS). And due to the lack of a unified
evaluation metric, their overall effectiveness remains unproven. This paper
introduces a novel image-oriented self-adaptive dataset construction method for
RMS, which starts with images and end constructing paired text and guidance
responses. Using the image-oriented method, we automatically generate an RMS
dataset comprising 35k image-text pairs with guidance responses. Additionally,
we introduce a standardized safety dataset evaluation metric: fine-tuning a
safety judge model and evaluating its capabilities on other safety
datasets.Extensive experiments on various tasks demonstrate the effectiveness
of the proposed image-oriented pipeline. The results confirm the scalability
and effectiveness of the image-oriented approach, offering a new perspective
for the construction of real-world multimodal safety datasets.

</details>


### [63] [Few-step Flow for 3D Generation via Marginal-Data Transport Distillation](https://arxiv.org/abs/2509.04406)
*Zanwei Zhou,Taoran Yi,Jiemin Fang,Chen Yang,Lingxi Xie,Xinggang Wang,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: 提出MDT-dist，将3D流模型蒸馏为少步推理，借助等价的速度层面与分布层面目标（VM与VD）绕过难以实现的传输积分，显著减少采样步数至1–2步，同时保持高质量与几何一致性，超越现有CM蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 3D流式生成在推理时需数十步采样，推理耗时高。2D领域的少步蒸馏（如CM）已显著加速，但在3D上尚未充分探索，需要一种能在复杂3D任务中稳定高效蒸馏的框架。

Method: 以“边缘数据传输”（Marginal-Data Transport, MDT）为主目标，对预训练流模型进行蒸馏。由于直接学习需对速度场积分且不可行，提出两种可优化替代目标：1) 速度匹配（VM）：学生稳定匹配教师速度场，但存在偏置梯度；2) 速度蒸馏（VD）：在已学得速度场基础上进行概率密度蒸馏，从分布层面修正与强化优化。整体将难以处理的传输级别问题等价转化为速度级与分布级学习。

Result: 在TRELLIS 3D生成框架上，将每个flow transformer的采样步数由25降至1或2步，在A800上实现0.68s（1步×2）与0.94s（2步×2）延迟，分别获得9.0×与6.5×加速，同时保持高视觉与几何保真度，显著优于现有CM蒸馏基线。

Conclusion: MDT-dist能在不牺牲质量的前提下，将3D流模型高效蒸馏为少步推理模型。通过VM与VD的组合，规避不可行的传输积分，并在3D生成中取得领先的速度-质量折中，推动TRELLIS在少步3D生成上达成更优表现。

Abstract: Flow-based 3D generation models typically require dozens of sampling steps
during inference. Though few-step distillation methods, particularly
Consistency Models (CMs), have achieved substantial advancements in
accelerating 2D diffusion models, they remain under-explored for more complex
3D generation tasks. In this study, we propose a novel framework, MDT-dist, for
few-step 3D flow distillation. Our approach is built upon a primary objective:
distilling the pretrained model to learn the Marginal-Data Transport. Directly
learning this objective needs to integrate the velocity fields, while this
integral is intractable to be implemented. Therefore, we propose two
optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),
to equivalently convert the optimization target from the transport level to the
velocity and the distribution level respectively. Velocity Matching (VM) learns
to stably match the velocity fields between the student and the teacher, but
inevitably provides biased gradient estimates. Velocity Distillation (VD)
further enhances the optimization process by leveraging the learned velocity
fields to perform probability density distillation. When evaluated on the
pioneer 3D generation framework TRELLIS, our method reduces sampling steps of
each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s
(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high
visual and geometric fidelity. Extensive experiments demonstrate that our
method significantly outperforms existing CM distillation methods, and enables
TRELLIS to achieve superior performance in few-step 3D generation.

</details>


### [64] [Durian: Dual Reference-guided Portrait Animation with Attribute Transfer](https://arxiv.org/abs/2509.04434)
*Hyunsoo Cha,Byungjun Kim,Hanbyul Joo*

Main category: cs.CV

TL;DR: Durian是一种零样本的人像动画视频生成方法，可将参考图像中的面部属性高保真、时空一致地迁移到目标肖像，并支持一次生成多属性组合，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨帧属性一致性、空间对齐和多属性组合方面存在限制，且常需成对/三元监督或额外训练。作者希望在无显式三元监督下，实现对任意参考-目标组合的稳健属性迁移与动画生成。

Method: 提出双参考网络，在扩散模型去噪过程中同时注入目标肖像与属性参考的空间特征。训练采用自重建：从同一肖像视频采样两帧，一帧作属性参考、一帧作目标，其余帧在条件输入与对应掩码下被重建。为覆盖不同空间尺度的属性，使用基于关键点条件的图像生成进行掩码扩展；并对属性/肖像图像施加空间与外观增强以缓解位姿与外观不对齐。

Result: 在肖像动画与属性迁移任务上达到最先进效果；在不额外训练的情况下，双参考设计支持单次生成过程中的多属性组合；模型对多样属性与真实场景参考组合具有良好泛化。

Conclusion: Durian通过双参考注入、自重建训练、掩码扩展与多重增强，实现零样本、高保真、时空一致的人像属性迁移与动画生成，并以单次推理支持多属性组合，性能达SOTA。

Abstract: We present Durian, the first method for generating portrait animation videos
with facial attribute transfer from a given reference image to a target
portrait in a zero-shot manner. To enable high-fidelity and spatially
consistent attribute transfer across frames, we introduce dual reference
networks that inject spatial features from both the portrait and attribute
images into the denoising process of a diffusion model. We train the model
using a self-reconstruction formulation, where two frames are sampled from the
same portrait video: one is treated as the attribute reference and the other as
the target portrait, and the remaining frames are reconstructed conditioned on
these inputs and their corresponding masks. To support the transfer of
attributes with varying spatial extent, we propose a mask expansion strategy
using keypoint-conditioned image generation for training. In addition, we
further augment the attribute and portrait images with spatial and
appearance-level transformations to improve robustness to positional
misalignment between them. These strategies allow the model to effectively
generalize across diverse attributes and in-the-wild reference combinations,
despite being trained without explicit triplet supervision. Durian achieves
state-of-the-art performance on portrait animation with attribute transfer, and
notably, its dual reference design enables multi-attribute composition in a
single generation pass without additional training.

</details>


### [65] [From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform](https://arxiv.org/abs/2509.04437)
*Benjamin El-Zein,Dominik Eckert,Andreas Fieselmann,Christopher Syben,Ludwig Ritschl,Steffen Kappler,Sebastian Stober*

Main category: cs.CV

TL;DR: 提出一种结合可微分霍夫变换的几何约束深度分割方法，用于自动检测X线图像中多边形整形限束（遮挡）边界与ROI中心，生成线约束掩膜，显著提升散射条件下的边界重建准确性。


<details>
  <summary>Details</summary>
Motivation: X线限束通过将曝光限制在兴趣区域来降低患者辐射剂量，但由于散射导致的边缘模糊，基于图像的限束阴影检测难以稳健完成。已有方法往往忽略“限束边界呈多边形直线结构”的先验，导致在噪声和遮挡下性能下降。

Method: 引入一个融合可微分霍夫变换（Differentiable Hough Transform, DHT）的深度网络：1）以分割为主干，同时内嵌可微直线检测模块以显式回归多边形边界；2）学习并利用ROI中心信息；3）在推理阶段，将分割输出与线检测与中心位置联合约束，生成满足直线/多边形几何先验的精炼掩膜。方法不限定固定边数，尽管应用中通常≤4条边。

Result: 在多样化真实X线测试集上对限束区域实现稳健重建，报告的中位数Hausdorff距离为4.3–5.0 mm，优于在散射与遮挡条件下的传统纯分割基线（隐含对比）。

Conclusion: 通过把几何先验（多边形直线边）以可微分霍夫变换形式融入网络，并在推理时进行线约束掩膜重建，可显著提升限束阴影检测的稳健性与精度；方法具备推广性，并不受边数严格限制。

Abstract: Collimation in X-ray imaging restricts exposure to the region-of-interest
(ROI) and minimizes the radiation dose applied to the patient. The detection of
collimator shadows is an essential image-based preprocessing step in digital
radiography posing a challenge when edges get obscured by scattered X-ray
radiation. Regardless, the prior knowledge that collimation forms
polygonal-shaped shadows is evident. For this reason, we introduce a deep
learning-based segmentation that is inherently constrained to its geometry. We
achieve this by incorporating a differentiable Hough transform-based network to
detect the collimation borders and enhance its capability to extract the
information about the ROI center. During inference, we combine the information
of both tasks to enable the generation of refined, line-constrained
segmentation masks. We demonstrate robust reconstruction of collimated regions
achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real
Xray images. While this application involves at most four shadow borders, our
method is not fundamentally limited by a specific number of edges.

</details>


### [66] [The Telephone Game: Evaluating Semantic Drift in Unified Models](https://arxiv.org/abs/2509.04438)
*Sabbir Mollah,Rohit Gupta,Sirnam Swetha,Qingyang Liu,Ahnaf Munir,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出UCF-UM循环一致性评估框架，交替I2T与T2I多轮以量化语义漂移，并给出三项指标（MCD、SDR、MGG）与新数据集ND400，显示单趟强的统一模型在跨模态稳定性上差异显著，循环一致性应成为评估必要补充。


<details>
  <summary>Details</summary>
Motivation: 统一模型同时做理解与生成，但现有评测割裂：I2T与T2I分别用单趟指标，无法判断“能理解是否也能生成”“跨模态往返是否保持语义”。需要一个能度量跨模态往返过程中语义保持/漂移的框架与指标。

Method: 提出UCF-UM循环评估协议：在同一模型上交替I2T与T2I多轮，形成文本-图像-文本…的循环，量化语义变化。定义三指标：1) MCD（Mean Cumulative Drift）：基于嵌入的整体语义流失累计量；2) SDR（Semantic Drift Rate）：语义随轮次衰减速率；3) MGG（Multi-Generation GenEval）：扩展GenEval的多轮对象级合规得分。为避免COCO过拟合，构建ND400基准（取样自NoCaps与DOCCI）。在七个近期统一模型上评测。

Result: UCF-UM揭示模型间跨模态稳定性差异大：如BAGEL能在多次往返中较好保持语义，而Vila-u虽单趟分数高但漂移快。三指标量化了这种差异，并与传统单趟指标互补。

Conclusion: 循环一致性是统一模型评估的关键维度，单靠传统I2T/T2I指标不足。UCF-UM与ND400提供了可复现、可量化的工具来评估跨模态稳定性与共享表示的强度，指导模型选择与改进。

Abstract: Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research. While UMs can also
support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus
on the core cross-modal pair T2I and I2T, as consistency between understanding
and generation is critical for downstream use. Existing evaluations consider
these capabilities in isolation: FID and GenEval for T2I, and benchmarks such
as MME, MMBench for I2T. These single-pass metrics do not reveal whether a
model that understands a concept can also render it, nor whether meaning is
preserved when cycling between image and text modalities. To address this, we
introduce the Unified Consistency Framework for Unified Models (UCF-UM), a
cyclic evaluation protocol that alternates I2T and T2I over multiple
generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean
Cumulative Drift (MCD), an embedding-based measure of overall semantic loss;
(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)
Multi-Generation GenEval (MGG), an object-level compliance score extending
GenEval. To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models. UCF-UM reveals substantial variation in
cross-modal stability: some models like BAGEL maintain semantics over many
alternations, whereas others like Vila-u drift quickly despite strong
single-pass scores. Our results highlight cyclic consistency as a necessary
complement to standard I2T and T2I evaluations, and provide practical metrics
to consistently assess unified model's cross-modal stability and strength of
their shared representations. Code:
https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models

</details>


### [67] [One Flight Over the Gap: A Survey from Perspective to Panoramic Vision](https://arxiv.org/abs/2509.04444)
*Xin Lin,Xian Ge,Dizhe Zhang,Zhaoliang Wan,Xianshun Wang,Xiangtai Li,Wenjie Jiang,Bo Du,Dacheng Tao,Ming-Hsuan Yang,Lu Qi*

Main category: cs.CV

TL;DR: 这是一篇关于全景视觉（ODIs）领域的系统综述，聚焦“从透视到全景”的适配问题，梳理成像与投影基础，总结三大域适配挑战，横向分析跨任务方法策略，并按任务类型将研究分为四大类，同时提出数据、模型与应用层面的开放问题与未来方向。


<details>
  <summary>Details</summary>
Motivation: ODI提供360度视场，对VR/AR、自动驾驶与具身智能等场景感知至关重要，但其几何投影、空间采样与边界连续性与常规透视图像差异显著，直接迁移透视方法效果欠佳，迫切需要系统梳理与适配策略指导研究与应用。

Method: 1) 回顾全景成像管线与多种投影（尤其ERP），阐释与透视图像的结构差异；2) 提炼三大域适配挑战：极区几何畸变、ERP非均匀采样、经度方向周期性边界；3) 从300+文献中覆盖20+任务：一方面按“挑战—策略”做跨方法分析；另一方面做跨任务对比并构建四大类别（质量增强/评估、视觉理解、多模态理解、视觉生成）；4) 讨论数据、模型、应用层的开放问题与未来研究方向；提供项目页与资源。

Result: 形成一套系统化的全景视觉问题版图与适配策略谱系，对各任务常见技术路径进行归纳对比，给出方法设计与评估的通用考虑（如投影变换、采样重加权、边界处理、极区稳健性等），并整理了代表性任务与文献资源。

Conclusion: 全景视觉与透视视觉存在本质差异，需要专门的域适配与方法设计。综述为研究者提供了结构化参考与实践指南，同时指出在数据规模与标注、模型架构与归纳偏置、跨模态与生成、以及下游应用落地等方面仍有大量空间，呼吁社区在标准基准、统一评价与更鲁棒的几何感知模型上持续推进。

Abstract: Driven by the demand for spatial intelligence and holistic scene perception,
omnidirectional images (ODIs), which provide a complete 360\textdegree{} field
of view, are receiving growing attention across diverse applications such as
virtual reality, autonomous driving, and embodied robotics. Despite their
unique characteristics, ODIs exhibit remarkable differences from perspective
images in geometric projection, spatial distribution, and boundary continuity,
making it challenging for direct domain adaption from perspective methods. This
survey reviews recent panoramic vision techniques with a particular emphasis on
the perspective-to-panorama adaptation. We first revisit the panoramic imaging
pipeline and projection methods to build the prior knowledge required for
analyzing the structural disparities. Then, we summarize three challenges of
domain adaptation: severe geometric distortions near the poles, non-uniform
sampling in Equirectangular Projection (ERP), and periodic boundary continuity.
Building on this, we cover 20+ representative tasks drawn from more than 300
research papers in two dimensions. On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks. On the other hand, we conduct a cross-task
comparison and classify panoramic vision into four major categories: visual
quality enhancement and assessment, visual understanding, multimodal
understanding, and visual generation. In addition, we discuss open challenges
and future directions in data, models, and applications that will drive the
advancement of panoramic vision research. We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies. Our project page is
https://insta360-research-team.github.io/Survey-of-Panorama

</details>


### [68] [Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.04446)
*Kiymet Akdemir,Jing Shi,Kushal Kafle,Brian Price,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出Plot'n Polish：零样本故事可视化框架，兼顾跨帧一致性与细粒度可控编辑，支持生成后精修与多层次修改。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型虽能生成多样细致图像，但在故事可视化中难以在多帧间保持角色/风格/叙事一致，同时缺乏对生成后进行细粒度或粗粒度编辑的灵活能力。实际创作需要在不破坏一致性的前提下进行局部与整体的反复打磨。

Method: 提出名为Plot'n Polish的零样本框架：在不额外训练的前提下，为故事序列提供一致生成与多层级控制/编辑。方法核心是将故事情节（Plot）与后期精修（Polish）解耦，允许对不同粒度（角色、场景、风格、物体细节）进行条件控制与跨帧一致传播，并支持生成后编辑保持一致性。

Result: 实现了在多帧故事可视化中对细/粗编辑的灵活控制，同时维持视觉与叙事一致性，相比现有方法更易于在真实创作中迭代打磨。

Conclusion: Plot'n Polish在不需额外训练的零样本设定下，为故事可视化提供了可控、一致且可后期编辑的解决方案，缓解了现有方法在跨帧一致性与可编辑性上的不足。

Abstract: Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.

</details>


### [69] [TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection](https://arxiv.org/abs/2509.04448)
*Zehong Yan,Peng Qi,Wynne Hsu,Mong Li Lee*

Main category: cs.CV

TL;DR: 提出TRUST-VL统一可解释多模态谣言/失真检测模型，配合Question-Aware Visual Amplifier与198K结构化推理数据集TRUST-Instruct，在域内与零样本基准上达SOTA并具可解释性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态谣言包含文本、图像及跨模态失真，受生成式AI推波助澜，现有方法多各自为政、专注单一失真类型，导致跨场景与未见任务泛化差、可解释性不足。作者观察到不同失真任务共享通用推理能力但又需任务特定技能，动机是通过统一训练促进知识共享与提升泛化和可解释。

Method: 构建统一视觉-语言模型TRUST-VL，并加入Question-Aware Visual Amplifier(QAVA)以根据问题/任务信号放大与提取任务相关视觉特征；同时构建TRUST-Instruct包含约198K条、与人类事实核查流程对齐的结构化推理链指令数据，进行多任务联合指令微调，实现可解释的链式推理与多失真类型统一检测。

Result: 在多种域内与零样本基准上取得SOTA表现；实验显示在不同失真类型间的联合训练提升了对未见场景的泛化能力，并提供强解释性（通过结构化推理链）。

Conclusion: 统一建模与指令化、带推理链的数据结合任务感知视觉放大模块，可在多模态失真检测中兼顾精度、泛化与可解释性；证明跨失真类型的知识共享有效，适合应对由生成式AI带来的新型误导场景。

Abstract: Multimodal misinformation, encompassing textual, visual, and cross-modal
distortions, poses an increasing societal threat that is amplified by
generative AI. Existing methods typically focus on a single type of distortion
and struggle to generalize to unseen scenarios. In this work, we observe that
different distortion types share common reasoning capabilities while also
requiring task-specific skills. We hypothesize that joint training across
distortion types facilitates knowledge sharing and enhances the model's ability
to generalize. To this end, we introduce TRUST-VL, a unified and explainable
vision-language model for general multimodal misinformation detection. TRUST-VL
incorporates a novel Question-Aware Visual Amplifier module, designed to
extract task-specific visual features. To support training, we also construct
TRUST-Instruct, a large-scale instruction dataset containing 198K samples
featuring structured reasoning chains aligned with human fact-checking
workflows. Extensive experiments on both in-domain and zero-shot benchmarks
demonstrate that TRUST-VL achieves state-of-the-art performance, while also
offering strong generalization and interpretability.

</details>


### [70] [Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview](https://arxiv.org/abs/2509.04450)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 提出VFR视频生成模型，可自回归地逐段生成任意时长的虚拟试衣视频，兼顾相邻片段的平滑与全局时间一致性，通过前缀视频与360°锚点视频实现；可生成分钟级长视频。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣或长视频生成需要大量算力与长视频数据，难以生成任意时长且在长时间尺度上保持一致性；需要一种高效、可扩展且在局部与全局时序上稳定的方法。

Method: 将长视频生成建模为自回归的段落式生成：每次生成下一个视频片段。为保证局部平滑性，使用前缀视频作为条件；为保持全局一致性，引入覆盖全身外观信息的360°锚点视频作为全局约束。整体框架在虚拟试衣场景下统一实现。

Result: 在多种动作下，能生成分钟级的虚拟试衣长视频，展示了良好的相邻片段平滑过渡与全局时间一致性。

Conclusion: VFR是首个可生成分钟级、兼顾局部平滑与全局一致性的长时虚拟试衣视频生成框架，证明了段落式自回归生成结合前缀与锚点约束的有效性。

Abstract: We introduce the Virtual Fitting Room (VFR), a novel video generative model
that produces arbitrarily long virtual try-on videos. Our VFR models long video
generation tasks as an auto-regressive, segment-by-segment generation process,
eliminating the need for resource-intensive generation and lengthy video data,
while providing the flexibility to generate videos of arbitrary length. The key
challenges of this task are twofold: ensuring local smoothness between adjacent
segments and maintaining global temporal consistency across different segments.
To address these challenges, we propose our VFR framework, which ensures
smoothness through a prefix video condition and enforces consistency with the
anchor video -- a 360-degree video that comprehensively captures the human's
wholebody appearance. Our VFR generates minute-scale virtual try-on videos with
both local smoothness and global temporal consistency under various motions,
making it a pioneering work in long virtual try-on video generation.

</details>
