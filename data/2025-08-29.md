<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出CHAIR-DPO：用图像字幕领域的CHAIR反幻觉指标直接作为偏好信号，配合DPO微调现成多模态大模型，从而显著降低视觉幻觉，无需复杂或专有数据生成管线。


<details>
  <summary>Details</summary>
Motivation: MLLM在多任务上表现强，但常发生视觉幻觉（生成与图像不符的内容）。现有对齐方法多依赖复杂、昂贵、甚至专有模型生成的偏好数据，成本高、可复现性差。作者希望用一个简单、开源、可自动化的度量来直接驱动对齐，降低幻觉。

Method: 把CHAIR指标（衡量图像字幕中“未在图像出现的对象被提及”的比例）扩展为偏好打分：对同一图像与问题生成的两段回答，用CHAIR判定哪段更少幻觉（winner）与更多幻觉（loser）。以这些自动标注的偏好对构建训练对，使用Direct Preference Optimization（DPO）微调现成的MLLM，无需奖励模型或强化学习。方法命名为CHAIR-DPO。

Result: 在多个多模态幻觉基准上，CHAIR-DPO显著减少幻觉回答比例；相较未对齐或使用复杂合成偏好数据的方法，取得更好的或相当的性能。代码与模型开源。

Conclusion: 利用CHAIR作为自动化偏好信号并用DPO微调，是一种简单、可复现、无需专有资源的对齐途径，能有效抑制MLLM的视觉幻觉。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 提出将Stable Diffusion（尤其SD3）的多模态生成与感知能力引入图像取证，实现无需大量像素级标注也能更准确地定位伪造区域；通过把高频伪造残差作为显式模态注入潜空间，保留语义、提升定位，较SOTA提升最高12%，并对未见真实场景/文档伪造有良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有伪造定位严重依赖昂贵的像素级标注且难以跟上新型多模态生成方法（如SD）带来的复杂篡改，亟需一种能利用强大生成模型感知能力、减少标注依赖并提升泛化与精度的新框架。

Method: 1) 理论上证明SD的多模态架构可被伪造相关信息调控，使其内生地产生伪造定位输出；2) 具体采用Stable Diffusion 3（SD3）框架：将图像的伪造残差（经高通滤波得到的高频信号）视为显式模态，在训练中与图像一同注入潜空间进行多模态融合；3) 设计保持SD3原有潜特征以保留丰富语义信息，从而提升定位精度与鲁棒性。

Result: 在多个主流伪造定位基准上，相比现有SOTA最高提升约12%；在未参与训练的真实文档伪造与自然场景伪造任务上仍表现强劲，显示良好零样本/跨域泛化能力。

Conclusion: 将SD3的多模态潜空间与显式伪造残差模态融合，可在不破坏语义表征的前提下显著提升伪造定位效果与泛化；表明生成式多模态大模型可作为取证定位的有效骨干，减轻对昂贵标注的依赖。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 用多模态大模型的可解释推理与“可量化皮损属性”(如面积)结合，并通过微调让MLLM嵌入预测这些属性；在SLICE-3D上以按属性的图像检索验证可行性。


<details>
  <summary>Details</summary>
Motivation: AI能诊断皮肤病但缺乏可解释性；临床落地需要能将预测与可理解的、与病灶外观相关的量化属性绑定。MLLM具备自然语言推理交互潜力，而量化属性被证实与恶性预测相关，因此将二者结合以提升可解释性与实用性。

Method: 对MLLM进行微调，使其从皮损图像中回归或分类若干定量视觉属性；把这些属性作为锚点来“对齐/扎根”其嵌入空间；通过基于属性的内容检索（CBIR）在SLICE-3D数据集上验证嵌入的属性敏感性与可控性。

Result: 微调后的MLLM嵌入能够较好地预测并反映目标属性，在以属性为条件的检索任务中取得有效的相似样本匹配，显示嵌入空间已被属性化、可用于属性导向的CBIR。

Conclusion: MLLM的嵌入可以与临床相关的量化属性对齐，从而为图像诊断提供可解释的中间表征与检索工具；该方向有望将自然语言解释与可量化概念结合，提升皮肤病AI的临床可用性。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 提出一个统一的ViT框架，把监督、 self-supervised 和重建目标整合，用更少标签在AMR任务上取得强表现。


<details>
  <summary>Details</summary>
Motivation: AMR在认知无线电、频谱监测与安全通信中很关键，但现有方法依赖大量标注或复杂多阶段训练，导致可扩展性与泛化受限。

Method: 构建一个由ViT编码器、轻量卷积解码器与线性分类器组成的模型：预训练阶段联合自监督与重建任务（将增强后的信号重建回原始I/Q，以锚定对细粒度结构的表征），促进判别特征学习；微调阶段使用部分标签进行监督分类。

Result: 在RML2018.01A数据集上，在低标签场景下优于监督的CNN与ViT基线；仅用15–20%标注即可接近ResNet的准确率；在不同SNR下保持稳健表现。

Conclusion: 该统一框架简单通用、标注高效，能在AMR任务中实现强泛化与鲁棒性，适合低标注与多SNR环境。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 提出InfinityHuman，用粗到细框架与姿态引导精炼器，结合手部奖励，解决音频驱动人体生成中的长时高分辨率一致性与手势自然度问题，在EMTD与HDTF上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人像视频存在长时生成易累积误差（身份漂移、色偏、场景不稳）和手部动作建模差（失真、与音频不对齐）。需要一种能稳定外观、提升唇形与手势语义准确性的方案。

Method: 1) 粗到细两阶段：先生成与音频同步的中间表征，再逐步精炼为高分辨率长视频。2) 姿态引导精炼器：使用解耦的稳定姿态序列与首帧作为视觉锚，降低漂移并提升唇同步。3) 手部专项奖励：以高质量手部动作数据训练的奖励机制，提升手势语义与真实感。

Result: 在EMTD与HDTF数据集上于视频质量、身份保持、手部准确性、唇形同步等指标达到SOTA；消融实验验证各模块有效。

Conclusion: 通过姿态解耦与首帧锚定抑制长时退化，并以手部奖励增强手势表现，InfinityHuman实现在高分辨率、长时音驱动人像生成的稳定性与真实感，并具备可复现实验与公开代码。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: 提出ATMS-KD，将自适应温度调度与混合样本增强结合，用大教师(MobileNetV3-L)向轻量ResNet学生蒸馏，在摩洛哥达马斯克玫瑰成熟度数据集上，各学生均超96.7%验证准确率，最佳紧凑模型达97.11%，推理72.19ms，优于11种基线，知识保留率>99%。


<details>
  <summary>Details</summary>
Motivation: 农业场景算力受限且环境复杂，需要在边缘设备上部署小模型，同时保持高准确率与鲁棒性。现有蒸馏方法对学生容量敏感、温度与数据增强设置固定，难以在真实农业影像中高效迁移教师知识。

Method: 提出ATMS-KD框架：1) 自适应温度调度(AT)：根据训练阶段或不确定性动态调整蒸馏温度，平衡软目标的信息量与稳定性；2) 混合样本(Mix-sample)增强：在输入或特征层进行样本混合(类似MixUp/CutMix思想)以丰富分布、强化蒸馏信号；3) 教师为MobileNetV3-L(5.7M)，学生为三种轻量残差CNN(1.3M/2.4M/3.8M)；4) 统一蒸馏损失(交叉熵+温度化KL)并与混合增强对齐。

Result: 在达马斯克玫瑰成熟度分类数据集上，相比直接训练的95–96%验证准确率，所有学生用ATMS-KD均>96.7%；紧凑型学生达97.11%，以最低推理延迟72.19ms领先11种主流蒸馏方法，知识保留率>99%，表明对学生容量不敏感且传递充分。

Conclusion: ATMS-KD能在资源受限农业环境中实现高效知识迁移，使小型CNN在保持极低延迟的同时获得接近教师的性能，并在真实田间条件下优于既有蒸馏技术；具有良好可扩展性与部署价值。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 提出将微结构图像与专家文本知识通过定制与混合的视觉-语言表示（VLRs）统一编码，实现无重训的零样本缺陷判定；在增材制造金属基复合材料数据上验证，结合CLIP与FLAVA并用Z分数归一化和正负样本相似度评分，提升可解释性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 工业中先进材料的快速可靠定级仍是瓶颈，尤其是非传统增材制造产生的异质微结构。通用嵌入在专业领域敏感性不足，且频繁任务特定重训成本高、可追溯性差。因此需要将专家表征知识与微结构数据在同一语义空间对齐，实现可扩展、可解释、少样本/零样本的质量判定。

Method: - 架构：混合VLR，将深度语义分割特征与预训练多模态模型（CLIP、FLAVA）融合。- 表示：基于相似度的定制表征，使用专家标注图像与文本的正/负参考集，编码视觉与语言。- 推理：对未见微结构进行零样本分类，采用“净相似度”评分（正相似度减负相似度），并对单模态与跨模态相似度做Z分数归一化以对齐分布。- 对比：评估CLIP与FLAVA在视觉敏感度与文本对齐上的差异，并在混合框架中协同使用。- 人在回路：提供透明可追溯的打分与依据，无需任务特定重训。

Result: 在增材制造金属基复合材料数据集上，系统可区分合格与缺陷样本，跨多种表征准则均有效。FLAVA在视觉敏感度更高，CLIP在文本准则对齐更稳定。Z分数归一化提升了各模态与跨模态分数的可比性与分类性能。

Conclusion: 定制相似度的混合VLR能在无重训条件下，将微结构数据与专家知识语义互操作，实现可扩展、可解释的材料资格认定；方法支持域适配与人在回路决策，为工程信息学中的可扩展质量鉴定提供新路径。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 提出并系统评估一种基于MedNeXt-L-k5的3D分割网络用于脑PVS自动分割：在同质T2w数据上Dice达0.88（WM，达人际一致性水平），T1w显著较低；跨站点泛化中等；整体未优于nnU-Net，说明Transformer式全局注意力并非必需。


<details>
  <summary>Details</summary>
Motivation: PVS是小血管病、阿尔茨海默病、中风与老年性神经退行的关键影像生物标志物。人工分割耗时、主观性强，现有自动方法性能与泛化有限，尤其跨不同MRI序列与扫描中心时。需要一种在多数据源上可靠且高效的自动分割方案，并明确注意力/Transformer结构是否对PVS分割必要。

Method: 改造Transformer启发的3D编解码卷积网络MedNeXt-L-k5，分别训练两套模型：1) 同质的HCP-Aging 200例T2w；2) 异质的7研究、6台扫描仪共40例T1w。采用5折交叉验证与留一站点交叉验证评估，指标包括体素级Dice与簇级Dice，分别在白质（WM）与基底节（BG）区域报告。并与nnU-Net对比。

Result: 在HCP-Aging T2w上：WM体素级Dice 0.88±0.06（与该数据集的人际一致性相当，且为目前文献最高）。在同数据集T1w上：WM Dice显著下降至0.58±0.09。跨站点LOSOCV（T1w异质数据）：WM体素Dice 0.38±0.16、BG 0.35±0.12；簇级Dice WM 0.61±0.19、BG 0.62±0.21。整体性能未超过nnU-Net。

Conclusion: MedNeXt-L-k5能在T2w同质数据上实现高精度PVS分割，但在T1w及跨站点泛化下性能下降。与nnU-Net相比无优势，提示获得高精度PVS分割并不依赖Transformer式全局注意力；数据同质性与序列选择（T2w优于T1w）对性能影响更大。该方法为多序列PVS自动分割提供高效基线，但实际部署需关注域泛化与数据标准化。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: 提出一个“训练免+反馈驱动”的自适应框架，把CLIP的最终输出（补丁级对应关系）反向用于中间注意力，从而增强开放词汇分割的定位与语义一致性，在多种方法、骨干与注意力类型上稳定提升多个基准的表现。


<details>
  <summary>Details</summary>
Motivation: CLIP具备强视觉-文本对齐，但做开放词汇分割时定位弱。现有方法在中间注意力层面做空间一致性增强，但这些改动在后续投影等操作中会被削弱，且中间注意力与文本表示缺乏直接交互，造成语义不一致，限制CLIP潜力。

Method: 提出训练免的反馈自适应：利用最终输出的补丁级预测作为更强的空间一致性先验，将其回灌到中间注意力以校正内部表示。核心组件包括：1）注意力隔离，避免引入干扰；2）基于置信度的剪枝，进行稀疏化适配，聚焦高可信区域；3）适配集成，综合多次/多层的反馈信号。以插件形式无缝接入多种SOTA开放词汇分割方法与不同骨干。

Result: 在四种SOTA方法、三种ViT骨干（B/L/H）以及多种注意力类型（Q-K、自注意、以及与MAE、SAM、DINO结合的Proxy注意力）上均有效；在八个基准上持续提升性能。

Conclusion: 通过把最终预测作为反馈信号来对齐内部注意力与输出语义，可在无需训练的前提下显著改进CLIP系开放词汇分割的定位与语义一致性，具有通用、可插拔、可扩展的优势。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 提出一种探针框架，逐层用线性分类器分析MLLM在视觉与文本处理中的功能分工，揭示早-中-晚层的阶段性结构在多模型中一致但层位随基座LLM变化而迁移。


<details>
  <summary>Details</summary>
Motivation: MLLM在多种视觉-语言任务上表现强，但其内部逐层如何处理视觉与文本尚不清楚；缺乏统一、轻量、可对比的手段来解释不同模型与训练设置下的表征动态。

Method: 构建逐层探针：在每一层提取基于标准化“锚问题”的token嵌入，训练线性分类器预测细粒度视觉类别（如犬种）。设计三类受控提示变体以定位层功能：1) 词汇变体（表层改变）；2) 语义否定变体（翻转视觉概念）；3) 输出格式变体（保持推理但改答案格式）。在LLaVA-1.5、LLaVA-Next-LLaMA-3、Qwen2-VL上系统评估。

Result: 发现一致的三阶段结构：早层侧重视觉对齐与指称落地（visual grounding），中层进行词汇整合与语义推理，末层进行任务/输出格式化。此结构对视觉分词、指令微调数据与预训练语料变化具有稳健性；但各阶段对应的具体层数位置会随基座LLM架构显著迁移。

Conclusion: 提出一种轻量、模型无关的逐层分析方法，统一刻画MLLM的层级组织与表征动态；为理解与调优不同基座架构下的多模态对齐、推理与输出设计提供依据。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 论文提出SLiCS：在CLIP等视觉-语言共同嵌入空间中，通过带组结构的稀疏非负字典学习，将场景内容按概念子空间可分解，从而实现更精确的概念过滤检索与条件生成，并在CLIP、TiTok、DINOv2等嵌入上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言嵌入（如CLIP）虽具语义信息，但各概念在同一潜在空间中纠缠，难以对复杂场景进行按概念分离和精确控制，限制了检索、编辑与下游任务的可解释性与可控性。作者希望找到一种可解释、可控、与多标签监督一致的方式，将嵌入解耦为多个概念特定成分，以实现更精准的检索与生成。

Method: 提出稀疏线性概念子空间（SLiCS）：
- 建模：将图像（或多模态）嵌入表示为字典原子按组（概念）划分后的非负稀疏线性组合；每个概念对应一组原子，其组激活与多标签监督对齐。
- 学习：监督的组结构字典学习，采用新型交替优化算法，保证收敛；系数非负且稀疏，鼓励可解释的概念分解。
- 文本协同：利用文本共嵌入，寻找与概念原子组最匹配的词向量来获得可语义解释的描述；在无监督情形下，借助标签文本嵌入对训练图像做zero-shot多标签赋值，再进行字典学习。
- 应用：将分解得到的概念成分用于概念过滤图像检索与image-to-prompt的条件生成；方法可泛化到TiTok高压缩自编码器嵌入和DINOv2自监督嵌入。

Result: 在定量与定性实验中，SLiCS在多种嵌入（CLIP、TiTok、DINOv2）上实现更高精度的概念过滤检索；分解产生的概念子空间带来更精确的条件控制与更可解释的语义描述。

Conclusion: 通过组结构、非负、稀疏的字典学习，将视觉-语言嵌入可解耦为多个概念子空间，提升检索与条件生成的精度与可解释性；算法收敛有保证，且能借助文本共嵌入实现监督与零样本设置下的统一框架。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出MedFoundationHub：一个离线本地、隐私友好的医疗视觉-语言模型（VLM）GUI工具包，可无编程部署与评估多款开源VLM；在病理任务实测中发现模型存在偏题作答、模糊推理与术语不一致等缺陷。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM虽能用于报告生成、医生助手与不确定性评估，但在医院环境中存在PHI泄露、数据外泄与网络攻击风险；即便研究场景也需合规与安全防护。因此需要一个既易用又能在本地离线、安全部署与评测多模型的解决方案。

Method: 构建MedFoundationHub：- 提供图形界面，医生可手动选择与调用不同模型；- 工程侧支持即插即用部署，集成Hugging Face开源模型；- 通过Docker编排实现操作系统无关、隐私保护的本地推理；- 仅需单卡NVIDIA A6000离线工作站即可运行；- 组织董事会认证病理学家在结肠与肾脏病例上评估5个SOTA VLM（MedGemma3-4B、Qwen2-VL-7B-Instruct、Qwen2.5-VL-7B-Instruct、LLaVA-1.5-7B/13B），共获得1015次临床评分事件。

Result: 工具包成功在本地离线环境中部署并运行多款VLM；专家评估显示，现有模型在病理任务上的常见问题包括：回答偏题、推理描述模糊、病理术语使用不一致，整体可靠性仍不足。

Conclusion: MedFoundationHub在资源可及与隐私安全方面可行，为医疗VLM的安全落地与比较评测提供了通用平台；但当前模型在临床病理应用上仍存在显著局限，需继续改进准确性、术语一致性与可解释性。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 提出BIM：在多任务密集预测中，用双向交互扫描与多尺度扫描，将Mamba线性序列建模适配为高效的跨任务交互，兼顾低复杂度与高效果，在NYUD-V2与PASCAL-Context上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测需要充分的跨任务信息交互，但充分交互通常带来高计算开销，现有方法在交互完整性与效率之间权衡，限制了性能与可扩展性。

Method: 基于Mamba序列建模框架设计BIM：1) BI-Scan（双向交互扫描）将任务特定表示在交互时组织为双向序列，融合“task-first”和“position-first”两种扫描模式于统一线性复杂度架构，实现高效保留关键跨任务信息；2) MS-Scan（多尺度扫描）进行多粒度场景建模，满足不同任务的粒度需求并强化细致的跨任务特征交互。整体保持线性复杂度。

Result: 在NYUD-V2与PASCAL-Context两个挑战性基准上，BIM较现有SOTA取得更优结果（摘要未给出具体分数，但宣称全面领先）。

Conclusion: 通过将双向与多尺度扫描机制引入Mamba，BIM在不显著增加计算的情况下实现更充分的跨任务交互，兼顾效率与效果，取得SOTA性能。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 提出无训练的音频引导视觉编辑框架，利用强零样本多模态编码器并通过空间对齐、噪声分支与自适应patch选择处理多文本+多音频复杂编辑，实验显示优于仅文本方法。


<details>
  <summary>Details</summary>
Motivation: 仅靠文本提示的扩散模型在复杂编辑情景下表达不足；现有音频引导方法需专门数据对齐训练，泛化差，难以应对真实世界的多模态、多指令编辑。

Method: 1) 使用预训练、零样本能力强的多模态编码器，将多样音频嵌入引入编辑流程；2) 通过缓解音频编码空间与扩散模型提示编码空间的不匹配，实现无额外训练的对齐；3) 设计“独立噪声分支”机制与“自适应patch选择”，支持多文本+多音频的复杂指令，并在编辑时局部控制与组合。

Result: 在多种编辑任务上进行系统实验，显示该框架能有效融合音频中的丰富信息，处理复杂场景，在文本方法失效时仍能成功编辑，性能更优。

Conclusion: 音频作为补充提示能显著提升复杂视觉编辑；通过预训练多模态对齐与分支/patch策略，可在无需再训练的前提下实现强泛化和可控编辑。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 提出AEVLP框架：用GPR鲁棒损失+ DAMP动态伪标签策略，在单阳性多标签学习（SPML）场景下稳健利用多源伪标签，四个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多标签学习常需全量标注，成本高。SPML只给每图一个正标签，余下未标注，若当作未知/负样本会造成误差；直接用伪标签容易引入噪声。缺少一种既能融合多种伪标签、又能抑制噪声的学习机制。

Method: 1) GPR Loss：通用伪标签鲁棒损失，可同时接收多源/多质量伪标签（含正负与不确定），通过重加权与边界控制减小噪声伪标签的影响，稳定训练。2) DAMP：动态增强的多焦点伪标签生成策略，结合视觉-语言模型与数据增强，逐步更新伪标签，关注多区域/多语义，提高召回并控制噪声。3) 组成AEVLP框架：以视觉-语言伪标签为基础，配合GPR Loss与DAMP的自适应迭代训练。

Result: 在四个基准数据集上显著优于现有方法，报告为SOTA（具体指标未给出，推测mAP/F1等显著提升），显示对噪声与缺失标注的鲁棒性。

Conclusion: GPR Loss + DAMP组成的AEVLP能在SPML下有效利用多源伪标签并抑制噪声，显著提升多标签分类性能；方法通用、可高效扩展到大规模与不同数据集。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 提出延时脉冲与时间依赖IF（tdIF）神经元，使SNN在极低步长（≤5）下显著提升目标检测与车道线检测性能，能耗与传统IF相当，达到SOTA的ANN→SNN转换效果。


<details>
  <summary>Details</summary>
Motivation: 现有ANN→SNN转换在分类上已很好，但在视觉检测等需要精细时空特征的任务上效果不佳，尤其在超低时间步下受异质放电导致的残余膜电位与频率编码局限影响，难以获得精确、低延迟检测。

Method: 1) 延时-脉冲策略：缓解由异质放电模式引起的残余膜电位积累问题。2) 提出时间依赖的Integrate-and-Fire（tdIF）神经元：使膜电位积分与放电行为随时间步次序自适应调整，使脉冲具备显式时间属性而非仅靠频率编码；同时保持与传统IF近似的能耗。方法以ANN→SNN转换为基础，面向检测任务优化。

Result: 在目标检测与车道线检测两大任务上进行广泛实验，低时间步（≤5）即可获得更精细特征表达；相较现有转换方法取得更高精度，达到SOTA，同时保持超低延迟与能耗水平。

Conclusion: 通过延时-脉冲与tdIF神经元，SNN在检测类任务中实现高精度、低步长与低能耗三者兼得，克服频率编码与残余膜电位带来的瓶颈，为在神经形态硬件上进行高效视觉检测提供了可行路径。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: 提出DUP-MCRNet用于RGB-D+边缘显著性目标检测，通过动态不确定性传播与多模态协同推理，提升小结构与边缘清晰度，在复杂背景下更鲁棒，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SOD在复杂场景易丢细节、边缘模糊、单模态融合不足，对遮挡、弱纹理、干扰背景下识别不稳。需要一种能动态评估不确定性并充分利用多模态互补的信息来增强鲁棒性与边缘质量。

Method: 1) 动态不确定性图卷积DUGC：基于空间语义距离构建稀疏图，在层间传播不确定性，结合通道自适应交互以增强小结构和边缘区域检测。2) 多模态协同融合MCF：对RGB、深度、边缘注意图使用可学习模态门控权重做加权融合，动态调节模态重要性，抑制冗余/干扰，强化跨模态一致性与互补。3) 训练优化：多尺度BCE+IoU损失、跨尺度一致性约束与不确定性引导监督，兼顾像素级与区域级性能。

Result: 在多数主流基准上整体优于各类SOD方法，尤其在边缘清晰度与复杂背景鲁棒性上表现突出。

Conclusion: 动态不确定性传播结合可学习多模态门控能有效缓解细节丢失与模糊，增强跨模态一致性，显著提升RGB-D+边缘SOD性能；方法通用，可为复杂场景显著性检测提供参考。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: 提出MSMVD：把每个视角的多尺度特征逐尺度投影到BEV，构建多尺度BEV特征，再用FPN融合，从而显著提升多视角行人检测的尺度鲁棒性，在GMVD上MODA+4.5。


<details>
  <summary>Details</summary>
Motivation: 现有端到端MVPD方法通常仅用单尺度或不显式利用多尺度信息，导致当行人在各视角中始终很小/很大，或不同视角间尺度差异巨大时检测性能下降。需要一种能显式建模并融合多尺度跨视角信息到BEV的机制。

Method: 1) 对每个摄像头视图提取多尺度图像特征；2) 逐尺度将这些特征通过几何投影/变换映射到BEV空间，得到对应的多尺度BEV特征，使其继承各自尺度的判别性；3) 使用FPN在BEV域对多尺度特征进行自顶向下/横向融合，整合跨尺度与跨视角信息；4) 在融合后的BEV上进行行人检测训练与预测，端到端优化。

Result: 在GMVD数据集上，利用多尺度图像到多尺度BEV的建模显著提升检测指标，相较于此前SOTA，MODA提高4.5个百分点；消融显示多尺度BEV与FPN融合均有贡献。

Conclusion: 显式从多视图多尺度图像特征构建并融合多尺度BEV能有效解决尺度不一致问题，提升MVPD鲁棒性与精度；该思路为多视角到BEV任务提供通用的多尺度建模范式。

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 提出SFMFNet，一种轻量级、具空间-频域感知与多尺度融合的实时深伪检测网络，在多基准上实现精度-效率平衡与较强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测虽在基准上高准确，但模型庞大、算力开销大，难以在视频会议/社媒等实时场景部署；需要既快又准且可泛化的方案。

Method: 提出Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet)：1) 空间-频率混合感知模块，用门控机制联合建模空间纹理与频域伪迹；2) token-selective跨层注意力，实现高效多层特征交互；3) 残差增强的blur pooling，下采样时保留关键语义线索；整体为轻量化实时架构。

Result: 在多个深伪基准数据集上验证，兼顾精度与效率，推理速度满足实时应用，并展现良好的跨数据集泛化能力。

Conclusion: SFMFNet在不显著增加计算成本的情况下提升对细微操纵的敏感性，实现实时、准确、可泛化的深伪检测，具有实际部署价值。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [22] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出一种将双模型权重选择与自蒸馏（SKD）结合的轻量级医学图像分类方法，在算力受限场景下逼近大模型效果。


<details>
  <summary>Details</summary>
Motivation: 医疗实际部署受算力/内存限制，难以使用大模型；需要轻量模型在不显著增加计算成本的情况下，尽量保留大模型知识与性能。

Method: 1) 从大型预训练模型出发，为两个轻量模型选择初始化权重（双模型权重选择），实现有效知识迁移；2) 对这两个已选权重的模型进行自知识蒸馏（SKD），允许更广的初始权重组合且不显著增加计算；3) 最后在目标分类任务上微调。

Result: 在胸部X光、肺部CT、脑部MRI三类公开数据上进行大量实验，所提方法在性能与鲁棒性上优于现有方法。

Conclusion: 双模型权重选择+SKD能在保持计算效率的同时增强轻量模型的表示与泛化，缓解传统轻量化方法信息丢失问题，适合资源受限的医疗影像分类部署。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [23] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 提出一种用于LiDAR点云的快速高效压缩方法，通过几何再致密化与跨尺度特征传播获得紧凑特征，实现更优上下文建模与实时编码/解码（12-bit量化下26 FPS），在KITTI上达SOTA压缩率。


<details>
  <summary>Details</summary>
Motivation: 高精度LiDAR点云体量大，存储与传输开销高。现有八叉树/体素等从密到疏的预测编码在极度稀疏几何细节处上下文建模效率低、计算慢，限制压缩率与速度。需要一种既能提升上下文建模又兼顾实时性的特征表示。

Method: 提出两模块轻量框架：1) 几何再致密化模块（Geometry Re-Densification）：将已编码的稀疏几何在更致密尺度上重建/采样以提取更丰富特征，再将特征再稀疏化供预测编码，既避免在极稀疏细节上做高代价计算，又保持轻量预测头。2) 跨尺度特征传播模块（Cross-scale Feature Propagation）：利用多分辨率层级的占用线索引导层级间特征传播与共享，减少重复特征提取，并为前一模块提供更充足上下文。两模块结合形成紧凑特征表示，提升上下文建模效率并加速编码流程。

Result: 在KITTI数据集上达到SOTA压缩率；在12-bit量化条件下，编码和解码均可实时运行，速度达26 FPS。

Conclusion: 通过再致密化与跨尺度传播构建紧凑特征表征，有效缓解极稀疏几何下的上下文建模难题，显著提升点云压缩的效率与速度，实现SOTA性能与实时处理。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [24] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 论文提出利用大规模视频数据中的“常识先验”缓解3D生成数据稀缺问题：构建首个带多视角级标注的视频数据集Droplet3D-4M，并训练支持图像与密集文本输入的生成模型Droplet3D，在空间一致性与语义契合度上优于现有方法，且具备向场景级扩展的潜力。


<details>
  <summary>Details</summary>
Motivation: 3D生成受数据量与多视角一致性约束，互联网原生3D数据远少于文本/图像/视频，导致难以遵循“规模定律”。而视频天然包含多视角与丰富语义，可作为替代监督信号，提升3D资产生成的泛化与可控性。

Method: 1) 数据：构建Droplet3D-4M大规模视频数据集，提供多视角级别标注以编码空间一致性与语义信息。2) 模型：训练Droplet3D生成模型，支持图像条件与密集文本条件输入；通过从视频中提取多视角一致性与语义先验，引导3D资产生成；强调可扩展到场景级。3) 训练与评测：在多任务/多模态设定下进行联合训练并与主流3D方法比较。

Result: 实验显示，Droplet3D在空间一致性（多视角一致）与语义合理性（更贴合文本提示）方面取得显著提升；相较主流3D方案具备更强的泛化性与可扩展性，初步验证了场景级生成潜力。

Conclusion: 视频中的常识先验（多视角与丰富语义）是解决3D数据稀缺与泛化瓶颈的有效路径。Droplet3D-4M与Droplet3D共同证明：借助视频监督可显著提升3D资产生成质量并推动向场景级应用扩展；相关数据、代码与模型已开源。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [25] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [26] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [27] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 提出首个统一多模态（手语、唇动、音频）到口语文本的框架，兼容多种输入组合，在SLT/VSR/ASR/AVSR上达SOTA或持平，并证实将唇动作为独立模态可显著提升SLT。


<details>
  <summary>Details</summary>
Motivation: ASR依赖音频，无法服务聋人/重听群体；现有手语翻译(SLT)与可视语音识别(VSR)各自为政，未充分探索多模态融合及唇动在手语中的非手势线索作用，缺少能统一处理多种模态组合的通用框架。

Method: 提出统一、模态无关的体系结构：能接收并组合手语视频、唇动视频与音频，多路编码与对齐融合设计；明确将唇动作为独立模态建模，探索跨模态协同；在同一框架下训练与推理，支持多任务（SLT/VSR/ASR/AVSR）与不同模态缺失情形。

Result: 在SLT、VSR、ASR、AVSR四项任务上达到或超过专用SOTA模型表现；消融/分析显示：显式引入唇动模态显著提升SLT效果，表明唇动作为非手势线索对理解手语有关键贡献。

Conclusion: 统一多模态到文本的框架既具实用性（适配不同可用模态）又具性能优势；唇动应被作为独立信息源纳入手语理解，未来可拓展更丰富非手动特征与更广数据以进一步提升鲁棒性与可达性。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [28] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 提出Video-MTR：一种用于长视频问答/理解的强化学习多轮推理框架，通过迭代选段与理解，结合轨迹级+回合级的门控双层奖励，在无需外部VLM的端到端训练下，提升准确率与效率并刷新多基准SOTA。


<details>
  <summary>Details</summary>
Motivation: 长视频包含长时序依赖与多事件，现有方法多为单轮静态推理或依赖外部VLM，存在推理不充分、训练非端到端、复杂度高与性能受限等问题，需要一种能在模型内部进行动态、多轮、可训练的关键片段选择与问题理解的机制。

Method: - 框架：Video-MTR（Multi-Turn Reasoning）。
- 流程：多轮迭代地根据当前问题与已看过的片段，选择下一批关键视频片段并更新理解，然后继续推理，直至给出答案。
- 选择策略：基于强化学习的片段选择与推理策略，逐轮细化。
- 奖励：门控双层奖励（bi-level）——
  1) 轨迹级（trajectory-level）基于最终答案正确性；
  2) 回合级（turn-level）突出帧-查询相关性，指导每轮的片段选择与理解。
- 训练：端到端，无需外部VLM。

Result: 在VideoMME、MLVU、EgoSchema等长视频理解基准上，较现有方法在准确率与效率上均有提升，达到或推动SOTA。

Conclusion: 多轮、强化、可解释（有中间推理过程约束）的选段与理解机制能更好地处理长视频的长程依赖与多事件；门控双层奖励有效优化了片段选择与问答质量，使端到端方法在准确性与效率上均优于依赖外部VLM的方案。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [29] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 论文提出DUO，用于单目3D目标检测在推理时自适应，联合优化语义与几何双重不确定性，通过凸化focal loss与无监督版本、以及语义感知的法向场约束，实现互补循环，显著提升跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测在真实域移（环境/传感器变化）下性能显著下降，现有TTA虽利用不确定性与泛化相关性，但忽视M3OD中同时存在的语义不确定性（类别模糊）和几何不确定性（空间定位不稳），亟需能同时处理二者的推理期自适应框架。

Method: 提出Dual Uncertainty Optimization (DUO) 流程：1) 从凸优化角度重释focal loss，构建新的凸结构并推导无监督版本，实现无需标签的“不确定性加权”，对高不确定样本进行平衡学习；2) 设计语义感知的法向场（normal field）约束，在语义线索清晰区域保持几何一致性，抑制3D表示不稳；3) 两分支形成闭环：几何增强促进语义分类，语义稳健反哺空间理解；整体在测试时对目标域数据自适应优化，无需源域标签。

Result: 在多数据集与多种域移场景下，DUO优于现有TTA方法，表现出更强的鲁棒性与泛化能力（定量细节未给出，但“广泛实验”显示全面领先）。

Conclusion: 联合最小化语义与几何不确定性是提升M3OD测试时自适应的关键；通过凸化focal loss的无监督权重与语义感知几何约束的互补机制，DUO在跨域与噪声条件下实现更稳健的单目3D检测。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [30] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 提出CaddieSet数据集：从单次挥杆视频分阶段提取关节信息，并配套球的多种轨迹/结果数据与15个专家定义指标；在多种基线下验证能预测球轨迹，且可解释模型与高尔夫知识一致。


<details>
  <summary>Details</summary>
Motivation: 现有用深度学习提升高尔夫击球精度的研究，缺乏将挥杆姿态与球飞行轨迹之间的定量关联，导致难以给出可操作的姿态改进建议。需要一个既含姿态（关节）又含球结果、并可解释的基准数据与方法。

Method: 构建CaddieSet：1) 以计算机视觉法将单次挥杆视频切分为8个挥杆阶段；2) 提取运动员关节关键点/骨架序列；3) 基于专家知识定义15个影响挥杆的关键指标（姿态与时序特征）；4) 以这些关节与指标为输入，训练多种基线（含可解释模型）预测球的轨迹/结果；5) 以可解释分析验证指标与领域知识一致性。

Result: 在多个基线任务上，CaddieSet可用于从关节与指标预测球轨迹，性能可观；可解释模型给出的特征重要性与高尔夫专家共识相符，能产生定量的一致反馈。

Conclusion: CaddieSet填补了挥杆姿态与球轨迹之间的定量桥梁，既支持预测也支持解释与反馈，为学术研究与产业（训练、教练辅助、设备评估）提供新资源与洞见。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [31] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [32] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [33] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [34] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 用3D高斯泼洒(3DGS)作为可微渲染器，通过新视角颜色损失反传到相机参数以微调标定，从而在无真实外参/内参真值下提升新视角合成质量；在3DGS参考数据集上仅靠重标定即可平均提升约0.4 dB PSNR，但微调较耗时，适合对画质要求高的基准场景（如Mip-NeRF 360）。


<details>
  <summary>Details</summary>
Motivation: 新视角合成对相机标定极其敏感，1像素级标定误差就会显著劣化重建质量。真实场景缺乏标定真值，通常以合成质量间接评估标定优劣，因此需要一种能够在无真值下自动提升标定并直接以合成质量为目标的办法。

Method: 利用3DGS作为可微模型：以新视角渲染的颜色重建误差为损失，对相机内外参进行梯度反传与联合微调（可能在固定或同时更新3DGS参数的设置下），实现“以画质监督的标定校正”。

Result: 在3DGS参考数据集上，仅对相机标定进行微调（不依赖其他复杂改动）即可带来平均约+0.4 dB的PSNR提升。

Conclusion: 用新视角损失反传微调相机标定是有效的，能稳定带来画质提升；但计算开销较大，是否采用取决于训练时长与应用需求。在以画质为首要目标的基准场景（如Mip‑NeRF 360）更值得投入。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [35] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 提出用于多模态医学影像分割的“主动+序列”域自适应框架，通过动态选择最有价值的目标域样本标注与训练，显著提升NPC/GBM肿瘤体积分割性能，超越现有ADA方法。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割需要大量标注，但标注成本高。现有主动域自适应多挑选“远离源域”的样本，易负迁移，且常需访问源数据；同时，多模态（如CT/MRI等）情景下的查询策略未被充分研究。

Method: 提出一个主动且序列化的域自适应框架：在迭代过程中根据“信息量（不确定性）+代表性（覆盖目标域分布、减冗余）”的综合指标，从多模态目标域中选择样本进行标注与训练；不依赖源数据访问，强调动态更新与多模态一致性；作为ADA的查询策略专门面向多模态数据。

Result: 在多种鼻咽癌与胶质母细胞瘤的肿瘤体积（GTV）分割任务上验证，所提方法在分割指标上显著优于现有最先进ADA基线，体现更好的精度与样本利用效率。

Conclusion: 动态、多模态感知的主动序列域自适应能在有限标注下有效提升跨域医学分割，降低负迁移与样本冗余，实践中无需源数据访问，具有较强应用价值；代码已开源（mmActS）。

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [36] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 提出一种在无监督3D目标检测中进行“数据级融合”的框架，利用视觉大模型做2D实例分割与深度估计，将RGB与LiDAR在早期双向对齐并用局部/全局滤波与动态自进化策略迭代优化伪框，在nuScenes上显著超越SOTA（mAP 28.4%）。


<details>
  <summary>Details</summary>
Motivation: 3D目标检测通常需昂贵的人工3D标注。现有无监督方法多在标签层面简单融合由LiDAR与RGB各自产生的伪框，但忽视两模态在数据层的互补性，导致伪框质量有限。

Method: - 数据级早期融合：用视觉基础模型在图像上做实例分割与深度估计；双向融合——(1) 将2D类别标签映射到投影后的真实LiDAR点以获得语义；(2) 将2D像素通过深度投影到3D以补 densify 点云。
- 去噪：局部半径滤波抑制深度估计误差；全局统计滤波移除分割导致的离群点。
- 动态自进化：在致密表示下迭代细化伪框（位置与尺寸等），逐轮提升定位精度，并用其训练无监督检测器。

Result: 在nuScenes验证集上，所训练检测器达28.4% mAP，显著优于此前SOTA的无监督3D检测方法。

Conclusion: 数据级的双向融合结合局部/全局滤波与动态自进化能有效提升无监督3D检测中的伪框质量与定位精度，证明早期跨模态融合优于简单的标签级融合。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [37] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 用深度学习从手机全身照估计BMI，构建并清洗大规模WayBED数据集（≈8.5万图像），在自测与跨数据集上取得当前最低或可比MAPE，并实现移动端部署与开源。


<details>
  <summary>Details</summary>
Motivation: 传统身高体重测量在远程医疗、应急环境中可能不可行；现有视觉BMI估计受限于小数据集与图像质量问题，影响泛化与实用落地。

Method: 收集WayBED数据集（84,963图像/25,353人）；提出自动图像过滤（姿态聚类+人体检测）去除异常姿态与不完整视野，保留71,322优质样本；基于深度学习的全身图像BMI回归模型；跨数据集评测、微调；在Android上用CLAID框架部署；代码与部署包开源。

Result: 在WayBED留出测试集上MAPE=7.9%（据称为文献最低）；在未见过的VisualBodyToBMI上MAPE=13%（与SOTA可比）；对VBTB微调后MAPE=8.56%（该数据集当前最低）。

Conclusion: 大规模、自动清洗的数据与全身图像深度模型能显著提升BMI估计精度与泛化；方法可在移动端实时部署，具备实际应用潜力，并通过开源促进复现与扩展。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [38] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [39] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出CLAB：在视频目标检测中加入对比学习辅助分支与动态损失权重，训练期增强特征、推理期零开销，提升VID上mAP达SOTA（CNN范式）。


<details>
  <summary>Details</summary>
Motivation: 视频中存在运动模糊、遮挡、形变等退化，使检测比静态图更难。以往方法依赖特征聚合与复杂后处理，带来较高推理开销。作者希望在不增加推理复杂度的情况下，提高对退化的鲁棒性与检测精度。

Method: 1) 对比学习辅助分支：在主干网络上增加辅助分支，使用对比损失（正负样本对）强化时空鲁棒的特征表征；训练时生效，推理时移除。2) 动态损失加权：训练前期加大辅助分支损失权重以学稳健特征，后期逐步将权重转向检测损失，确保最终优化检测性能。3) 无需额外后处理或多帧聚合，保持推理零额外开销。

Result: 在ImageNet VID上，ResNet-101达84.0% mAP，ResNeXt-101达85.2% mAP；消融实验显示两项组件（对比辅助分支与动态加权）均有稳定增益。相较同类CNN方法，无需额外后处理仍达SOTA。

Conclusion: CLAB以简单的训练期对比辅助与动态权重策略，在不增加推理成本的前提下显著提升视频目标检测，适合作为通用、易集成的训练范式，特别对退化场景更鲁棒。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [40] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 论文研究并防御针对多模态模型（如CLIP）的“排版/文字注入”攻击：在图片中嵌入文字诱导模型误判或越狱。作者在CLIP视觉编码器中定位到专门传递文字信息的注意力头，并提出在推理时选择性“切断”这一路径，以几乎不损失常规准确率的代价显著提升对排版攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态系统容易被图像中的可读文本操纵（目标误分类、生成恶意内容、越狱）。现有防御多依赖微调或牺牲较多性能/泛化。需要一种无需训练、可直接部署、对文字注入强鲁棒且尽量保持常规视觉任务性能的方法。

Method: 对CLIP视觉编码器进行因果分析：定位层后半段中“专司文字提取与传递”的注意力头（typographic heads/circuit），这些头将字符区域信息汇聚到CLS。基于此，在推理阶段对该回路进行选择性消融（ablation），即屏蔽特定注意力头的输出，不改动权重、无需微调，形成“dyslexic CLIP”。

Result: 在ImageNet-100的排版攻击变体上准确率最高提升19.6%；在标准ImageNet-100上准确率下降<1%。与依赖微调的SOTA防御相比，此训练免疫方法在鲁棒性上具有竞争力。

Conclusion: CLIP中存在可解释的“文字处理回路”，对排版攻击至关重要。通过定位并消融这些注意力头，可显著提升对文字注入攻击的鲁棒性，且几乎不损害常规性能。作者开源了一系列“dyslexic CLIP”作为可直接替换的安全优先模型，适用于对文字操纵风险敏感的应用场景。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [41] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 提出GLaRE：用图神经网络在面部关键点上进行区域级嵌入的表情识别方法，借助层次粗化得到商图以保结构降复杂度，在AffectNet达64.89%、FERG达94.24%，优于若干基线；消融显示区域级嵌入有效。


<details>
  <summary>Details</summary>
Motivation: 传统FER易受遮挡、表情多样性影响且可解释性弱；需要一种能显式建模关键点间关系、同时具备结构化与可解释性的模型。GNN在关系建模与结构学习方面具优势，适合处理面部关键点图。

Method: 1) 用3D人脸对齐提取面部关键点；2) 基于关键点构图并通过分层粗化构造商图（quotient graph），在保留空间结构的同时降维降复杂度；3) 在该图上进行区域级嵌入与GNN学习，实现可解释的情感分类；4) 通过消融评估区域级嵌入的贡献。

Result: 在AffectNet上准确率64.89%，在FERG上94.24%，均优于若干现有基线；消融实验表明商图产生的区域级嵌入显著提升预测性能。

Conclusion: GLaRE利用商图进行区域级图嵌入，兼顾结构性与可解释性，在大型与合成数据集上均取得竞争性表现，验证了以关键点区域关系建模提升FER的有效性。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [42] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [43] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出UTA-Sign：在低照环境下，利用热成像与事件相机互补，进行无监督的视频增强以更好地刻画交通标志与车牌/路障等，提升感知与检测效果。


<details>
  <summary>Details</summary>
Motivation: 热成像在弱光下鲁棒，但对材质相近的标志物（如标牌、车牌、反光路障）对比度低、语义难辨；事件相机对光强变化敏感、适于高速低光，却存在非均匀采样与稀疏性。两者互补但单独使用都有盲区，需一种融合策略来持续且准确地呈现标志细节，降低自动驾驶安全风险。

Method: 提出UTA-Sign无监督热-事件视频增强框架：1) 双重boost机制融合热帧与事件信号；2) 以热帧提供稳定的运动线索/时间参考，对齐并重采样不均匀的事件流；3) 由事件信号向热帧注入细微的标志纹理与边缘信息，补全热成像对标志的“盲点”；4) 全流程无需标注，面向车牌、路障指示等目标，生成更一致的标志时序表示。

Result: 在真实道路数据集上验证：生成的交通标志“素描/描边”质量更高；在下游感知层面的检测精度提升（相较基线/单模态方法）。

Conclusion: 热-事件互补融合在低照交通场景中有效；双重boost实现时序一致与细节增强，能无监督地改善标志可见性并提升检测性能，具备在夜间自动驾驶中的实用潜力。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [44] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [45] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 提出Diverse-T2M：在保证文本一致性的同时显著提升文本到3D人体动作生成的多样性，通过在Transformer框架中显式建模不确定性（噪声与潜空间随机采样）实现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作方法虽能生成高质量、与文本匹配的动作，但样本多样性不足，难以覆盖同一语义下的多种合理动作。需要一种既保留语义一致性又提升生成多样性的机制。

Method: 1) 噪声即多样性载体：在Transformer式生成流程中引入/利用噪声信号，作为多样性信息的携带者，从而显式建模不确定性；2) 潜空间构建与采样：将文本投影到连续潜空间而非一对一刚性映射，并通过潜空间采样器进行随机抽样，把随机性注入生成过程，提高输出多样性与不确定性；整体方法命名为Diverse-T2M。

Result: 在HumanML3D与KIT-ML数据集上，显著提升生成多样性指标，同时在文本一致性（语义对齐）方面达到SOTA水平。

Conclusion: 通过把不确定性以噪声与潜空间随机采样的形式融入文本到动作生成管线，可在不牺牲语义一致性的前提下，显著提高3D人体动作生成的多样性；方法简单有效，具有通用性与可扩展性。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [46] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 提出一种利用3D打印体模与优化求解的3D IVUS标定与体积重建方法，实现术中IVUS与术前CT的精确配准，用于肝手术导航；在猪肝体内实验中达到亚毫米级标定误差和约3–6 mm的配准误差。


<details>
  <summary>Details</summary>
Motivation: 术中US视野窄、结构复杂，难与术前CT对齐，影响肝手术导航。若能重建整肝3D影像并与CT配准，可显著提升导航与定位精度。3D IVUS具备围绕血管扫查获取全器官信息的潜力，但需要高精度探头-追踪器-成像几何标定与稳定的体积重建流程。

Method: 设计基于3D打印体模（phantom）的几何标定流程：使用外部追踪的IVUS探头，建立成像坐标到追踪坐标的外参与内参模型；通过优化最小化重建一致性/重投影/几何约束误差以求解标定参数；据此对连续IVUS帧进行位姿融合与体素集成，重建3D IVUS体数据；最后以此体数据与术前CT进行基于强度/形状的配准，完成跨模态对齐。

Result: 在体内猪肝实验中，标定误差0.88–1.80 mm；3D IVUS与CT配准误差3.40–5.71 mm；实现稳定、可重复的体积重建与对齐。

Conclusion: 所提方法能可靠、准确地标定IVUS并进行3D体积重建，支持术中IVUS与术前CT的配准，提升肝手术导航精度与可行性；具备向临床转化的潜力。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [47] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 提出PI-GenMFI：融合物理约束的扩散生成模型，用于生成磁场成像（MFI）缺陷样本（如电源短路），以缓解数据稀缺并提升缺陷定位ML训练，实验与专家评估显示优于VAE/常规扩散等SOTA，定量定性指标均有改进。


<details>
  <summary>Details</summary>
Motivation: 半导体缺陷检测需要高效定位ROI；X射线虽可靠但大规模扫描成本高、耗时和内存占用大。MFI更高效但数据受保密限制稀缺，制约基于ML的缺陷定位模型训练。因此需要能合成高质量、物理一致的MFI数据来补充训练集。

Method: 提出物理约束的扩散生成框架PI-GenMFI：在扩散模型中显式融入两项与MFI相关的物理信息/约束（例如与磁场/电流分布一致性、边界/守恒约束等），面向常见缺陷类型（电源短路）生成MFI图像；并与VAE和通用扩散模型进行对比。

Result: 生成的MFI样本通过领域专家评估达到较高可信度；在图像生成与信号处理相关的多种定性与定量指标上表现优于对比SOTA模型，显示更好质量与物理一致性，可用于训练下游缺陷定位算法。

Conclusion: PI-GenMFI能够在数据匮乏场景下生成物理合理的MFI样本，缓解数据瓶颈并有望优化基于MFI的缺陷定位流程；相对VAE/常规扩散方法更具优势。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [48] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 提出一种针对分割推理(SI)的更强数据重建攻击：用带有逐级特征优化(PFO)的GAN生成器从中间特征重建高保真图像，并通过L1球约束稳定优化，显著提升在高分辨率、OOD与更深模型上的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 分割推理将模型拆分到端侧与云端，降低时延并保护隐私，但中间特征可能被攻击者用来重建原始输入，造成隐私泄露。现有重建攻击多只适用于浅层模型、未充分利用语义先验，跨数据集/架构的泛化差，重建质量受限。

Method: 提出基于GAN的攻击框架：将生成器分解为分层/层级块，进行渐进式特征优化(PFO)，逐步细化中间表征以提升语义保真度；在重建过程中加入L1-ball约束以稳定优化并提升图像真实感。

Result: 大量实验显示，相比已有攻击方法，新方法在高分辨率、分布外(OOD)场景以及更深、更复杂DNN上均有显著优势，重建质量更高、泛化更强。

Conclusion: SI存在更严重的隐私风险：即便在深模型、复杂与跨分布条件下，中间特征仍可被高质量重建。所提PFO+L1约束的GAN攻击显著提升攻击能力，提示需要更强的防护/加密与分割点选择策略。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [49] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 提出EmoCAST：一个扩散式情感说话人像生成框架，通过文本可控地生成表情生动、语音同步的说话视频，并在新构建的数据集与训练策略加持下达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有情感说话头像合成方法在三方面不足：1) 控制灵活性差（难以精确按文本/情感控制）；2) 动作不自然、唇形与音频耦合不佳；3) 表情质量有限。加之现有数据多为实验室环境采集，域内单一，限制模型泛化与实用性。

Method: 提出扩散模型框架EmoCAST，包含两大关键模块：1) 文本引导的解耦情感模块（decoupled emotive module），将情感提示注入外观建模，增强空间情感理解；2) 情感音频注意力模块（emotive audio attention），显式建模受控情感与驱动音频之间的交互，产出情感感知特征指导面部运动合成。此外，构建带有丰富情感文本描述的新数据集，并提出两种训练策略：情感感知采样（emotion-aware sampling）与渐进式功能训练（progressive functional training），以强化细腻表情与唇形同步。

Result: 在真实感、情感表达丰富度/准确性与音画同步方面达到SOTA。实验显示新数据集与训练策略显著提升细节表情捕捉与唇形对齐。

Conclusion: EmoCAST通过文本驱动的情感注入与音频-情感交互建模，结合新数据与训练策略，显著提升情感说话头像的可控性、自然度与同步性，具备更强的实际应用潜力；代码/项目页已公开。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [50] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 基于SwinUNETR的乳腺MRI多中心AI诊断模型，在ODELIA挑战赛中获第二名；通过乳腺区域掩膜、强数据增广与集成提升鲁棒性与泛化，适用于高风险/致密乳腺人群的早筛辅助。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早检至关重要，但在致密乳腺或高风险人群中，传统钼靶敏感性不足；MRI更敏感，但解读负担重、主观性强、跨中心差异大。需要能在多中心、多厂商、多磁场强度数据上泛化的AI模型，辅助诊断与分级。

Method: 构建以SwinUNETR为核心的深度学习框架：1）乳腺区域掩膜以聚焦病灶相关区域、降低背景噪声；2）大规模/多样化数据增广以模拟扫描差异、提升鲁棒性；3）集成学习融合多模型或多折结果以稳定预测。数据来自6个欧洲中心、1.5T和3T、多厂商，标签为左右乳三分类（无病灶/良性/恶性）。

Result: 在ODELIA多中心挑战数据集（511例）上，方法获得排行榜第二名，显示出良好泛化与稳健性（具体分数未给出）。

Conclusion: SwinUNETR结合掩膜、增广与集成可有效提升多中心乳腺MRI三分类性能，具有临床辅助解读潜力；代码已开源，便于复现与扩展。

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [51] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: 提出AvatarBack，通过生成后视伪图与自适应空间对齐，补全Gaussian Splatting头像的后脑信息，显著提升后部几何与外观，同时不损伤正面质量，保持可动画性。


<details>
  <summary>Details</summary>
Motivation: 基于Gaussian Splatting的人头复原多依赖正面视角，导致后脑区域信息缺失，出现几何不一致、结构模糊和现实感下降，限制头像保真度与可用性。

Method: 1) Subject-specific Generator (SSG)：利用生成先验，从稀疏正面输入合成与身份一致、可信的后视伪图，提供多视角监督；2) Adaptive Spatial Alignment (ASA)：为合成视图与3D高斯表示之间学习可训练的变换矩阵，在训练中优化，解决姿态与坐标不匹配，实现精确几何对齐；整体为即插即用框架以显式建模后脑区域。

Result: 在NeRSemble与K-hairstyle上，通过几何、光度与基于GPT-4o的感知指标评测，显著提升后脑重建质量；同时保持正面保真，动态驱动下视觉真实一致，完全可动画。

Conclusion: AvatarBack能有效补全并对齐后脑区域，在不牺牲正面质量的前提下提升整体头像一致性与真实感；框架通用、可插拔，适合提升Gaussian Splatting头像在多动作场景中的实用性。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [52] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 用基础模型微调并与传统人脸识别嵌入融合，可显著提升历史绘画中人物身份识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 艺术史研究需要识别画作中的人物（sitter），但人工方法主观且受数据稀缺与风格差异限制。传统人脸识别在照片上表现好，却因域偏移、风格与绘画技法差异、作者意图等在绘画上失效，亟需更鲁棒的方法。

Method: 尝试将大规模预训练的基础模型（foundation models）进行针对艺术人脸的微调；同时将其特征嵌入与常规人脸识别网络的嵌入进行融合（多模/多源特征集成），以缓解域偏移与类内差异。

Result: 在历史绘画人脸识别任务上，相较现有最先进方法取得“显著”性能提升；尤其在传统方法无效的风格变异、画工差异等场景中更稳健。

Conclusion: 基础模型具备跨域迁移与强表征能力，经过微调并与传统模型融合，能有效弥合照片与绘画间的识别鸿沟，为艺术作品中人物识别提供更可靠的自动化手段。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [53] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti提出一个文本引导的涂鸦生成框架，在极端风格化下保持人脸身份可辨。先做风格迁移，再用带显式身份嵌入的自注意力机制校正人脸特征，并通过CLIP引导的提示扩展实现无关键点的姿态控制。实验与部署显示兼顾审美与身份一致性。


<details>
  <summary>Details</summary>
Motivation: 极端风格（如高对比、抽象的涂鸦）会轻易破坏眼鼻口等关键部位，导致人物不可识别，影响个人与文化真实性。现有方法在强风格化和身份保真之间难以平衡，且姿态控制常依赖关键点，限制创作自由。

Method: 1) 以LoRA微调的预训练扩散Transformer进行涂鸦风格迁移；2) 设计“人脸一致”自注意力：在注意力层中注入显式身份嵌入，约束生成保持目标面部特征；3) 无关键点姿态定制：利用CLIP引导的提示扩展实现重定姿态同时维持面部连贯；4) 采用“先风格、后身份”的流水线，并对该范式进行形式化论证与实验验证。

Result: 与反向流程相比，“先风格、后身份”显著降低属性漂移；在定量评估中，人脸特征一致性具竞争力，审美与人偏好得分达SOTA；定性展示与在Cruilla音乐节的实际部署表明方法具备现实创作影响力。

Conclusion: CraftGraffiti提供了在强风格化场景下维护身份的系统化方案，将风格自由与可识别性兼顾，推动“尊重身份”的AI艺术创作实践。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [54] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [55] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 提出S-HArM数据集与基准，聚焦AI生成图像的“意图感知”分类（幽默/讽刺、艺术、虚假信息）；比较多种训练与建模策略，发现保留视觉语境的合成数据与模型更具泛化，但总体难度高、性能仍有限。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检测多关注“真假/出圈”而忽视生成内容的意图；现实场景中区分幽默、艺术与误导对治理与审核重要，缺少公开数据与系统评测。

Method: 1) 构建S-HArM：从Twitter/X与Reddit收集9,576组图文并标注三类意图。2) 用Stable Diffusion基于三种提示策略（图像引导、文本描述引导、图文联合引导）合成大规模训练数据。3) 系统评测：比较模态融合、对比学习、重建网络、注意力机制与大规模视觉语言模型等方法，分析不同数据与策略的泛化。

Result: 基于图像引导和图文联合引导生成的数据能更好泛化到“野外”内容，因其更好保留视觉上下文；相较之下，仅文本描述引导效果较差。尽管如此，各类模型整体表现仍受限。

Conclusion: 意图识别远比内容真伪检测更具挑战，需要保留视觉语境的数据与更针对性的架构；当前方法在真实场景下仍难以可靠，提示未来应面向意图建模设计专门的多模态方法与数据。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [56] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 提出MobileCLIP2，通过更强教师集成与改进的多模态强化蒸馏，在保持移动端低延迟与小模型规模下，刷新ImageNet-1k零样本精度；提供模型与数据生成代码以便复现与扩展。


<details>
  <summary>Details</summary>
Motivation: 在移动/边缘设备上，希望获得CLIP级别的零样本理解能力，但受限于时延和参数规模。现有MobileCLIP虽有效，但教师与数据生成管线仍有提升空间，特别是在教师质量、caption多样性与对比蒸馏细节上，以进一步提高精度-延迟权衡。

Method: 改进MobileCLIP的多模态强化训练：1) 使用在DFN数据上训练的更优CLIP教师集成；2) 使用在DFN上训练并在多种高质量图文数据上微调的caption生成教师，增强caption多样性与质量；3) 通过消融研究调整对比蒸馏温度、验证多教师合成caption的互补性与可加性；据此训练新的MobileCLIP2系列。

Result: MobileCLIP2在低延迟（3–15ms、50–150M参数）下达成SOTA零样本表现：MobileCLIP2-B较MobileCLIP-B在ImageNet-1k零样本提升2.2%；MobileCLIP2-S4与SigLIP-SO400M/14在ImageNet-1k零样本精度相当，但参数量约为其1/2；相较DFN ViT-L/14以2.5倍更低延迟取得更优结果。

Conclusion: 更强教师与更优数据生成/蒸馏细节显著提升移动端图文模型的零样本能力，且改进具有可加性与可扩展性。作者公开了预训练模型与分布式数据生成工具，便于构建任意教师参与的强化数据集与复现实验。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [57] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 提出一种可变比特率的动态神经视频压缩框架，通过可切换编码路径与率控代理实现精确率控，同时用联合路径优化保证不同路径的协同训练；在HEVC/UVG上优于SOTA（BD-Rate -14.8%、BD-PSNR +0.47dB），平均码率误差1.66%，实现率-失真-复杂度联合优化。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩在固定设定下效果好，但在实际场景需要按约束（如带宽、算力、码率预算）动态调码率与复杂度；学习式编解码器往往难以精确率控，导致码率偏差大、RD 性能受损。

Method: 1) 动态路径自编码器（DRA）：网络内置多条“编码路径”，每条仅占用部分计算量并对应不同的RD权衡，支持按需切换实现可变比特率与可变复杂度；2) 率控代理（RCA）：在线估计各路径的码率并在运行时选择/调整DRA路径以逼近目标码率；3) 联合路径优化（Joint-Routes Optimization）：训练期对多条路径协同优化，覆盖广泛码率区间同时不牺牲整体RD。

Result: 在HEVC与UVG数据集上，相比SOTA实现平均BD-Rate降低14.8%，BD-PSNR提升0.47 dB；在目标码率跟踪上平均误差1.66%；同时通过路径切换降低计算量以达成RDCO。

Conclusion: 该框架在多比特率/受限码率应用中实现精确率控与优异RD表现，并兼顾计算复杂度；为实际部署场景提供可落地的动态神经视频编解码方案，代码已开源。

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [58] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: 提出CardioMorphNet：一种基于递归贝叶斯深度学习的3D心脏形状引导配准方法，仅用分割形状递归配准（非强度相似度），在UK Biobank上优于现有方法，并提供运动不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 现有CMR动态图像配准多依赖强度相似度损失，易忽视心脏解剖结构，导致运动估计不准；同时缺乏对时空依赖和不确定性的系统建模。需要一种能聚焦解剖区域、利用时序信息、并量化不确定性的心脏运动估计方法。

Method: 提出CardioMorphNet：1) 使用递归变分自编码器建模整个心动周期的时空依赖；2) 设计两个后验模型，分别用于双心室分割与运动场估计；3) 基于贝叶斯推断导出损失函数，通过递归注册分割图（而非强度相似度）来聚焦解剖区域；4) 利用连续SAX体数据与时空特征进行3D形状引导的可形变配准；5) 通过贝叶斯建模输出运动场不确定性图。

Result: 在UK Biobank数据集上，通过将变形后的掩膜与真实掩膜比对，CardioMorphNet在心脏运动估计上优于多种SOTA方法；在概率模型对比中，其在心脏区域的运动场不确定性更低，显示预测更有信心。

Conclusion: CardioMorphNet能够在无需强度相似度损失的情况下，利用解剖形状与时空依赖实现更准确的心脏运动估计，并提供可信的不确定性评估，优于现有方法。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [59] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [60] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出Pref-GRPO：用成对偏好奖励替代点值奖励的GRPO训练，缓解T2I中的奖励黑客；并发布统一细粒度评测基准UniGenBench，验证方法有效。


<details>
  <summary>Details</summary>
Motivation: GRPO类强化学习提升T2I生成，但常用点值奖励模型在归一化后会放大小分差，诱发“奖励黑客”，模型为微小得分而过度优化，训练不稳。同时，现有T2I基准评测粗糙，难以全面衡量模型。

Method: 1) Pref-GRPO：将优化目标由分数最大化转为偏好拟合。对同组生成图像进行成对比较，使用偏好RM计算胜率作为奖励信号，实现更稳健的优势估计与更新；2) UniGenBench：统一基准，600个提示，覆盖5大主题20子主题，定义10个一级与27个二级语义一致性标准，借助多模态大模型进行构建与评测。

Result: 实验显示：Pref-GRPO能区分细微画质差异，提供稳定优势，显著缓解奖励黑客；UniGenBench揭示开源与闭源T2I模型的优劣，并在该基准上验证Pref-GRPO有效。

Conclusion: 用偏好胜率替代点值分数可稳定GRPO训练并减轻奖励黑客；UniGenBench为T2I提供细粒度统一评测框架，促进更全面的模型比较与方法发展。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [61] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 提出C3-GS，一种无需逐场景优化的可泛化Gaussian Splatting方法，通过三类约束（上下文感知、跨维度、跨尺度）改进特征融合，在稀疏视角下仍能重建更准确几何并实现更高质量新视图合成，达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有可泛化Gaussian Splatting多用前馈网络预测每像素高斯参数，但缺乏判别性、跨视角一致的特征表示，尤其在稀疏视角下难以构建准确几何，导致渲染质量受限。

Method: 提出C3-GS框架：在统一渲染管线中嵌入三个轻量模块，引入（1）上下文感知（context-aware）、（2）跨维度（cross-dimension）、（3）跨尺度（cross-scale）约束，强化特征学习与融合；无需额外监督，直接预测用于高斯渲染的参数，支持前馈快速推理。

Result: 在多个基准数据集上进行大量实验，C3-GS在渲染质量与泛化能力上达到或超过现有方法的最先进水平，尤其在稀疏输入视角下表现显著提升。

Conclusion: 通过将三类约束以轻量模块方式融入GS渲染流程，C3-GS有效提升多视角一致性与几何重建能力，实现无需逐场景优化的高保真新视图合成，并具备强泛化。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [62] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出SeqVLM：一种零样本3D视觉指代方法，利用多视角真实场景图像与空间信息进行目标定位，通过3D实例提议、语义筛选、多视图投影与动态调度调用VLM，实现更强的空间推理与上下文保真，在ScanRefer与Nr3D上取得SOTA（Acc@0.25分别55.6%/53.2%）。


<details>
  <summary>Details</summary>
Motivation: 现有零样本3DVG方法多依赖单视角定位，导致空间推理受限；同时在从点云到图像或语言对齐时，容易丢失上下文或细节，且直接调用大型VLM成本高。需要一种兼顾空间关系、细节保真与计算效率的零样本方案。

Method: 1) 用3D语义分割网络生成3D实例提议；2) 语义过滤保留与文本相关的候选；3) 提议引导的多视图投影：将候选投影到多帧真实场景图像上，显式保留空间关系与上下文细节；4) 动态调度机制：迭代式构造“序列-查询”提示词，分批调用VLM进行跨模态推理，识别与文本描述匹配的目标，同时控制计算负载。

Result: 在ScanRefer与Nr3D基准上达到零样本SOTA：Acc@0.25分别为55.6%与53.2%，较此前最佳零样本方法提升4.0与5.2个百分点。

Conclusion: SeqVLM通过多视图投影与动态VLM调度，有效缓解单视角空间推理不足与上下文丢失问题，在不进行场景特定训练的前提下提升3DVG泛化能力与实用性，并以较低计算开销实现SOTA表现。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [63] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 评估CLIP及其变体在军事场景遮挡与低信噪条件下的鲁棒性：自建18类军用车辆数据集，用NAUC随遮挡比例衡量；Transformer优于CNN；细碎分散遮挡更致命；线性探针在≈35%遮挡急剧掉点；微调骨干后拐点推迟到>60%；建议进行遮挡感知训练与探究补丁级敏感性与结构韧性。


<details>
  <summary>Details</summary>
Motivation: 国防应用常缺标注数据，VLM如CLIP可零样本识别，但其在真实军事环境中常见的遮挡与低SNR下鲁棒性尚不清楚。明确这些边界可指导模型选择、训练策略与部署可靠性。

Method: 构建包含18类军用车辆的图像数据集；设计多种遮挡模式（细粒度分散 vs 大块连续）与不同遮挡比例；比较CLIP的Transformer与CNN变体，并考察零样本、线性探针与骨干微调设置；以随遮挡比例的Normalized AUC（NAUC）作为综合鲁棒性指标。

Result: 1) Transformer版CLIP在各遮挡条件下稳定优于CNN版；2) 相同遮挡比例下，细碎、分散的补丁级遮挡对性能打击更大；3) 线性探针虽提升无遮挡精度，但在≈35%遮挡时性能陡降；4) 对骨干进行微调可显著推迟性能崩塌点至>60%遮挡。

Conclusion: 针对遮挡的专门数据增广与训练方案对提升鲁棒性至关重要；应进一步研究补丁级敏感性与架构层面的抗遮挡机制，以便在复杂军事场景中可靠部署CLIP。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [64] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 提出SKGE-Swin：在端到端自动驾驶中，用带跳跃阶段的Swin Transformer实现像素到像素的上下文建模，在CARLA对抗场景中取得更高Driving Score，并通过消融验证各组件贡献。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶需要从原始像素中捕获远距离依赖与多尺度上下文，同时保持早期关键信息，现有方法对全局建模或特征保真不足，且在更具挑战的对抗场景中鲁棒性有限。

Method: 构建SKGE-Swin架构：以Swin Transformer为骨干，利用其SW-MSA实现滑动窗口的多头自注意力，结合“skip-stage”机制在不同网络层级进行跨阶段特征融合，增强多尺度与长程依赖表征；在CARLA平台设计对抗性驾驶场景进行训练/评测；并做消融实验评估Swin与跳连等模块的作用。

Result: 在CARLA对抗场景基准上，SKGE-Swin获得高于既有方法的Driving Score；消融显示引入Swin与跨阶段跳连均能带来显著增益（具体数值未在摘要中给出）。

Conclusion: 通过Swin的全局-局部上下文建模与跨阶段信息保留，SKGE-Swin更好理解复杂路况，实现优于以往的端到端驾驶性能；组件级分析支持其设计有效性。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [65] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 论文是一篇综述，聚焦“视频中抽象概念理解”的长期挑战，梳理相关任务与数据集，并倡议结合多模态基础模型与以往经验，避免重复造轮子。


<details>
  <summary>Details</summary>
Motivation: 尽管机器已能较好识别视频中的具体可见实体（物体、动作、事件、场景），但对“正义、自由、团结”等抽象概念的识别仍乏力；而高层抽象理解对于与人类价值与推理对齐至关重要。

Method: 作为调查研究：系统回顾与归纳视频抽象概念理解相关的任务设定与数据集演进，分析历史周期性尝试与技术工具迭代，并将这些努力置于当下多模态基础模型的语境中进行对比与总结。

Result: 总结了现有任务和数据集版图，指出社区在不同时代借助当时技术反复攻克该问题的趋势，揭示抽象理解需要多层语义推理与上下文建模，且当代基础模型提供了新的契机。

Conclusion: 视频抽象概念理解仍是重要未解难题；应借鉴数十年的研究积累，与多模态基础模型结合，系统推进高层语义与价值对齐的自动化理解，避免重复造轮子。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [66] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出一种面向皮肤病变分类的全类激活概率图评估方法（Global Class Activation Probabilistic Map Evaluation, GCAPME）与SafeML集成，统一、像素级、概率化地可视化所有类别的诊断证据，并对潜在误诊发出预警；在ISIC数据集上结合MobileNetV2与ViT验证，旨在提升可解释性与诊断安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管皮肤病变分类模型精度很高，甚至超过皮肤科医生，但临床对AI仍不信任。现有可解释方法存在可靠性问题：LIME类方法一致性差，CAM类方法只关注单一或少数类别，忽视全类别证据，可能导致误导性解释与误诊风险。

Method: 提出GCAPME：对所有类别的激活概率图进行概率化、像素级分析与统一可视化，展示每个像素对各类的支持度分布，从而全景化呈现模型决策过程；并引入SafeML框架，对可解释性与输出进行安全评估，检测潜在错误诊断并向医生/患者发出警告。实验在ISIC数据集上，以MobileNetV2与Vision Transformer作为骨干模型进行评估。

Result: 在ISIC数据集的实验表明，该方法能够同时可视化多类证据、提升对模型决策过程的可解释性，并更有效地发现潜在误诊（通过SafeML预警）；相较LIME/CAM类基线解释方法，表现出更高的稳定性与可靠性（摘要未给出具体数值）。

Conclusion: GCAPME统一、概率化地评估与可视化多类别激活证据，结合SafeML可提升皮肤病变AI诊断的可信度与安全性；适用于不同架构（MobileNetV2、ViT），有望降低临床误诊风险。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [67] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: 研究比较扩散分类器与判别式VLM（CLIP、ViLT）在组合泛化上的表现：对象-属性绑定较好，但关系推理在GZSL上普遍失败；CLIP在关系概念表示过于相似或导致混淆。


<details>
  <summary>Details</summary>
Motivation: 自然语言与视觉语义的关键在于可组合性，但现有VLM常被指“词袋式”表示，难以正确绑定对象与属性/关系（如把红立方体与蓝圆柱误配）。扩散模型在生成与零样本分类中崭露头角，作者想验证其是否比判别式模型在组合泛化上更强。

Method: 比较三种模型：生成式的Diffusion Classifier与判别式的CLIP、ViLT；评估对象-属性、对象-关系的绑定能力，在零样本（ZSL）与广义零样本（GZSL）两种设置下测试；并分析CLIP嵌入空间中关系概念（如left/right）的可分性。

Result: Diffusion Classifier与ViLT在概念绑定（对象-属性）任务上表现良好；但在关系型GZSL任务上三者都显著困难。CLIP嵌入显示“left”“right”等关系概念表示过于相似，可能导致混淆。

Conclusion: 生成式扩散分类器在绑定任务上不逊于甚至可比判别式模型，但关系推理仍是VLM共同短板，尤其在GZSL中；问题或源于关系概念在表示空间的可分性不足。

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [68] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 提出一种基于surfel与SE(3)等变特征的点云配准回归模型，通过等变卷积与跨注意力，稳健预测源-目标扫描的相对位姿，较SOTA更鲁棒于噪声与大旋转。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准方法多忽略点的朝向与不确定性，对噪声和大幅旋转（如正交变换）敏感，往往需要大量带变换增强的训练数据。需要一种天然对位姿变化鲁棒、能显式建模方向信息的方案。

Method: 1) 以虚拟透视相机参数从LiDAR点云初始化surfel；2) 设计SE(3)等变卷积编码器，学习同时包含位置与旋转的等变特征；3) 通过跨注意力计算源-目标相似性；4) 全连接解码器回归相对位姿；5) 使用非线性Huber损失进行训练。

Result: 在室内与室外数据集上，相较现有SOTA方法，该模型在真实点云扫描配准任务中表现更优，且对噪声与大旋转更稳健。

Conclusion: 显式建模surfel朝向与SE(3)等变特征，可在无需大量旋转增强的情况下实现更鲁棒、更准确的点云位姿回归；架构通用，可推广至多场景3D重建与遥感/文保应用。

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [69] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: 提出DVCTNet：通过全景X光（全局视图）与单牙裁剪图（局部视图）的双视图协同训练，并用门控跨视图注意力融合，显著提升龋齿检测精度，SOTA。


<details>
  <summary>Details</summary>
Motivation: 龋齿在全景片上对比度微弱、形态多样，现有检测方法易漏检、误检；临床上牙医会先全局筛查再逐牙细看，启发将全局与局部信息系统性结合。

Method: 1) 自动牙齿检测生成两种视图：全局（全景片）与局部（裁剪牙齿图）; 2) 分别预训练两个视觉基础模型：全局模型作为检测骨干产生日志候选框与全局特征，局部模型从匹配的牙齿补丁提取细粒度特征；3) 提出Gated Cross-View Attention模块，动态融合双视图特征；4) 将融合特征回注到检测管线进行最终龋齿检测；5) 在公共数据集与新构建高精度数据集上评测。

Result: 在两个数据集上均优于现有SOTA方法（更高检测性能，具体指标未在摘要中给出），展示更强的鲁棒性与泛化能力。

Conclusion: 双视图协同训练与门控跨视图注意力有效缓解全景片对比度低与形态多样的挑战，显著提升龋齿检测准确度，具备临床应用潜力；代码与数据集已开源。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [70] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: 提出FusionCounting：将可见光-红外图像融合与人群计数统一为多任务框架，通过密度估计提供语义与监督，配合动态损权与对抗训练，兼顾融合质量与计数精度，并在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VIF多聚焦于图像质量，或借助语义分割/目标检测引导，但分割标注昂贵、检测在高密度人群中受遮挡与重叠框影响；同时RGB-T人群计数兴起却未与VIF结合。需要一种在密集场景中标注成本低、鲁棒且可提升下游的联合方法。

Method: 设计多任务学习框架FusionCounting：以可见光与红外输入，同时进行图像融合与人群计数（密度图回归）。- 利用密度信息为融合提供语义与任务驱动的监督，实现互利学习；- 引入动态损失权重，自适应平衡融合与计数任务并加速收敛；- 融入对抗训练，提升对对抗扰动的鲁棒性与稳定性。

Result: 在公开数据集上，FusionCounting同时提升了融合图像质量指标，并在人群计数任务上取得更优的性能，相较现有方法具有优势。

Conclusion: 将人群计数纳入VIF的统一框架是有效的：密度监督在密集场景下标注成本低、语义明确，配合动态损权与对抗训练，可同时改进融合表现与计数精度与鲁棒性。

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [71] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 用LoRA微调预训练视觉语言模型（VLM），通过精心设计的指令与语义关键点描述，实现外科手术器械的2D关键点估计，在小样本下快速收敛并优于CNN/Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 传统CNN/Transformer在小规模医学数据集上易过拟合，难以获得稳健的关键点估计。希望利用大规模预训练VLM的泛化能力，通过少量数据与低开销微调，提升手术器械关键点定位的准确性与样本效率，并为后续3D位姿估计奠基。

Method: - 采用预训练VLM作为视觉-语义对齐骨干。
- 通过精心设计的prompt构建指令调优数据集，将关键点的语义描述与图像特征对齐。
- 使用LoRA进行低秩参数适配，仅训练少量增量参数，进行短周期（两轮）微调。
- 输出2D关键点位置预测，并与传统CNN/Transformer基线比较。

Result: 仅用两轮微调，适配后的VLM在2D关键点检测上超过基线模型，显示出更好的泛化与样本效率。

Conclusion: 基于LoRA的VLM指令调优在小样本医疗场景中有效提升2D关键点估计表现，并为扩展至3D手术手与器械位姿估计提供可行路径。

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [72] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [73] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: 提出PathMR：一种面向病理图像的细胞级多模态视觉推理框架，同时生成专家级文字解释与细胞分布（分割/计数）预测；在PathGen与新构建的GADVR数据集上，文本、分割与跨模态对齐均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习病理诊断虽提升效率但缺乏可解释性与可追溯依据。多模态视觉推理可同时给出像素级定位与语义解释，但现有方法在病理场景下尚欠细粒度（细胞级）推理与稳定对齐能力。作者希望通过统一框架提供可定位（细胞/病灶级）与可读（专家风格叙述）的诊断依据，促进临床可信度与落地。

Method: 构建PathMR框架：输入病理图像与文本查询，联合输出（1）专家风格诊断文本，（2）细胞分布与分割掩码。核心包括：多模态编码（视觉编码器提取细胞级特征；文本编码器处理查询）；跨模态对齐与推理模块（如注意力/对比学习）以实现查询引导的区域定位；细胞级分割/计数头与文本生成解码器协同训练；使用跨模态一致性与因果/指向性损失以强化“看哪里、说什么”的一致。并在公共PathGen与自建GADVR数据集上训练/评估。

Result: 在PathGen与GADVR上，PathMR在三方面显著优于SOTA：文本生成质量（如BLEU/ROUGE/CIDEr等）、分割/细胞分布预测精度（如Dice/IoU/计数误差）、跨模态对齐指标（如指向性/定位一致性）。

Conclusion: PathMR实现了细胞级可解释多模态推理，在文本解释、像素/细胞级定位及跨模态一致性上均领先，显示其提升AI病理诊断可解释性的潜力；代码将开源，利于复现与扩展。

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [74] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: 该研究开发并验证了一个基于双模态成像（自体荧光与二次谐波生成SHG）的深度学习框架，用于早期胰腺导管腺癌（PDAC）检测；在40例患者样本上评估多种CNN与ViT架构，最终采用冻结预训练层+类别加权的改进ResNet，实现>90%癌检出准确率，显示出临床部署潜力。


<details>
  <summary>Details</summary>
Motivation: PDAC 5年生存率<10%，主要因晚期发现；手工图像分析效率低且主观性强。需要在小样本、类别不平衡的现实条件下，利用多模态显微成像自动、早期、可靠地区分正常、纤维化与癌变组织，以提升诊断效率与一致性。

Method: 收集40位患者的双模态显微图像（自体荧光+SHG），构建分类任务（正常/纤维化/癌变）。系统比较六种深度学习架构（传统CNN与ViT），并针对医学影像常见难题采取：预训练迁移学习、部分层冻结、类别权重训练（处理不平衡）、以及模型与超参的系统实验筛选。最终确定改进ResNet为最佳方案。

Result: 在测试中，改进ResNet（冻结预训练层+类别加权）在癌症检测上取得>90%准确率，优于人工分析方法；在小样本与类别不平衡情境下表现稳健。

Conclusion: 提出了一个鲁棒的PDAC自动检测流程，可辅助病理医生并具临床落地潜力；方法论对小样本医学影像的深度学习应用具有可迁移性，并为扩展至其他癌种奠定基础。

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [75] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 该论文以“反事实”作为统一视角，提出一系列面向视觉分类与文本到图像生成模型的解释、审计与去偏方法，既能发现与量化偏差，也能在不重训练或少量微调的情况下缓解交叉偏见，提升公平性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 深度模型常依赖虚假相关与不可解释线索，导致不公平与脆弱性。现有方法要么缺乏因果性、可扩展性或概念层面解释。作者希望用反事实干预在“语义保持”的前提下改变特定属性，既能诊断模型对受保护属性（性别、种族、年龄）的依赖，也能在分类与生成两类任务中提供可操作的去偏策略。

Method: 分两部分：1) 分类器：提出CAVLI，将LIME的局部归因与TCAV的概念向量结合，给出概念级重要性与局部热力图，并定义“概念依赖分数”衡量对无关线索（如背景）的依赖；提出ASAC，构造在保持语义的同时对受保护属性进行对抗式扰动的反事实样本，并配合课程式学习对偏置模型进行微调，兼顾公平与准确。2) 文生图：提出TIBET，通过在提示词中系统性变化身份相关词汇，规模化评估与因果审计TTI模型的提示敏感偏见；提出BiasConnect，建立因果图以捕捉并诊断交叉（多属性交互）偏见；提出InterMit，基于因果敏感度分数与用户定义的公平目标、以模块化且免训练方式缓解交叉偏见。

Result: 在分类任务中，CAVLI能定量揭示模型对不相关概念（如背景）的依赖；ASAC通过对抗式反事实与课程学习显著改善公平性，同时保持或提升准确率并避免刻板化伪影。在文生图任务中，TIBET能系统揭示不同身份词对生成结果的因果影响；BiasConnect发现并解释多属性交互导致的偏差；InterMit无需训练即可有效降低交叉偏见，并可适配不同公平目标。

Conclusion: 反事实提供了兼具可解释、因果与可扩展的统一框架，既适用于判别式也适用于生成式模型。论文提出的工具链（CAVLI、ASAC、TIBET、BiasConnect、InterMit）能够更可靠地评估与缓解社会偏差，为构建公平、稳健与负责任的AI系统提供了原则化方法。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [76] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: 该综述系统梳理“统一式感知”（Unified Perception）在自动驾驶中的范式与方法，提出早期/后期/完全统一三类框架与一套分类维度，并总结架构、训练、数据与开源状况，指出未来方向，旨在提升鲁棒性、上下文推理与效率，同时保持可解释输出。


<details>
  <summary>Details</summary>
Motivation: 传统模块化流水线（检测-跟踪-预测）虽可解释，但存在误差累积、跨任务信息利用不足与协同弱的问题。端到端或统一式方法可在共享表示与联合优化下缓解上述不足，行业与学术界亟需一份系统化全景综述来厘清概念、方法谱系与实践要点。

Method: 提出“任务整合、跟踪表述、表示流动”三大分类维度，定义早期统一（共享特征、后续分头）、后期统一（各子任务先独立后融合）与完全统一（端到端共享表示与联合训练）三种范式；对现有方法在架构、训练策略、数据集与开源情况进行系统回顾与对比，并归纳优缺点与适用场景。

Result: 构建首个统一式感知的系统性框架与术语体系，整合分散研究，形成对方法设计空间的清晰地图；总结主流方法在鲁棒性、上下文推理、效率与可解释性上的取舍，并给出现状盘点（数据与开源）。

Conclusion: 统一式感知有望在保持可解释性的同时，提升跨任务协同、鲁棒性与效率；该综述为后续研究提供了结构化参照与研究议程，指向更通用、更可解释、更强泛化的自动驾驶感知系统。

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [77] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: 提出Dino U-Net：将冻结的DINOv3视觉基础模型作为编码器，通过适配器融合语义与空间信息，并用保真感知投影模块（FAPM）降维，构建端到端医学图像分割网络；在7个公开数据集上达SOTA，且随骨干规模（至7B）提升而稳健增益，参数效率高。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉基础模型在自然图像上学到强大表征，但如何高保真、有效地迁移到对细粒度空间信息要求高的医学图像分割仍具挑战：语义与低层细节融合、降维过程中的信息损失、参数与计算开销。

Method: 1) 采用冻结的DINOv3作为编码器骨干，避免过拟合并利用其密集预训练特征；2) 设计专用Adapter，将高层语义与底层空间细节进行多尺度融合；3) 提出保真感知投影模块FAPM，在降维至解码器前对特征进行保真约束与精炼；4) 构建U-Net式解码器进行逐级上采样与跳连；5) 评估不同规模DINOv3（含至7B参数）以验证可扩展性。

Result: 在7个多模态公开医学分割数据集上取得SOTA，稳定优于以往方法；性能随骨干规模增加而持续提升；在参数数量与计算成本方面表现为高效迁移（参数效率高）。

Conclusion: 密集预训练的一般视觉基础模型（DINOv3）可在冻结条件下，通过适配融合与保真投影，有效移植到医学分割，带来显著、可扩展且参数高效的精度提升。代码已开源。

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [78] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: 提出用ConvNeXtBase深度集成识别肿瘤组织中的非典型有丝分裂像（AMF），在MIDOG25预测试集上获得84.02%平衡准确率；规则精炼模块提升特异度但牺牲灵敏度与总体表现。


<details>
  <summary>Details</summary>
Motivation: AMF与肿瘤分级密切相关，但与正常有丝分裂像（NMF）区分困难；人工标注既耗时又主观，亟需自动化、稳健的AMF分类方法以辅助病理诊断与一致性。

Method: 使用AUCMEDI框架训练多个ConvNeXtBase分类器构成深度集成，并在其预测之后加入规则驱动的后处理（RBR）以进行结果精炼；在MIDOG25的预测试集上评估，关注平衡准确率、灵敏度和特异度。

Result: 集成模型在MIDOG25预测试集上达到84.02%的平衡准确率；加入RBR后，特异度上升，但灵敏度下降，整体性能被拉低。

Conclusion: 深度集成对AMF分类有效；规则精炼可定向提升特异度，但存在降低灵敏度与总体性能的权衡，仍需改进与更系统的设计与验证。

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [79] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: 提出COMETH：一种用于多视角人类姿态融合与跟踪的轻量级实时算法，通过凸优化逆运动学与状态观测器，在边缘分布式场景下提升空间与时间一致性，兼顾精度、带宽与计算成本，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业5.0场景需要对人类活动进行监测以保障工效学与安全。多相机集中式方案虽准但带宽计算开销大，不利于实时扩展；边缘分布式可减负但因资源受限带来精度下降、时空不一致。

Method: COMETH的三大核心：1) 将人体运动学与生物力学约束融入以提升关节定位精度；2) 使用凸优化的逆运动学进行多视角空间融合；3) 引入状态观测器（滤波/估计器）提升时间一致性。整体为轻量、可实时、适配边缘设备的融合管线。

Result: 在公共与工业数据集上，COMETH在定位、检测与跟踪精度均超过SOTA，同时具备更好的可扩展性与实时性（低带宽与低算力需求）。

Conclusion: COMETH实现了准确、可扩展的多人/多人类运动跟踪，适合工业与安全关键应用；凸优化+生物/运动学约束+状态观测的组合有效缓解边缘分布式带来的精度与一致性问题。

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [80] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [81] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: 提出 E-ConvNeXt：将 CSP 机制与一系列结构优化引入 ConvNeXt，在显著降复杂度（最高约80%）的同时保持/提升精度，ImageNet 与下游检测均展现更优精度-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有高性能网络（如 ConvNeXt）多为高算力场景设计，参数量与计算复杂，限制在轻量级与边缘设备的应用，需要在不显著牺牲精度的前提下降低复杂度、提升效率与可迁移性。

Method: 以 ConvNeXt 为基座，提出 E-ConvNeXt：1) 将 CSPNet 融合入 ConvNeXt 并重构阶段连接，降低冗余计算与参数（宣称最多降80%复杂度）；2) 优化 Stem（输入端）与 Block 结构以提升特征表达与运行效率；3) 用通道注意力替代 Layer Scale，以更高效稳定的特征重标定机制提升效果。提供多种复杂度配置（mini/small）。

Result: 在 ImageNet 分类上：E-ConvNeXt-mini 0.9 GFLOPs 达到 78.3% Top-1；E-ConvNeXt-small 3.1 GFLOPs 达到 81.9% Top-1，显示较优精度-效率折中。迁移到目标检测任务时亦表现出较好的泛化（定性结论，未给出具体数值）。

Conclusion: 通过 CSP 融合与结构优化，E-ConvNeXt 在保持（甚至提升）精度的同时显著降低计算与参数，适合轻量级/资源受限场景，并在分类与检测中展示良好泛化与实用价值。

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [82] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [83] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出利用网页规模伪造图像与自动标注来缓解数据稀缺，构建超大像素级数据集 MIMLv2，并配套训练策略与模型（Web-IML），在多基准显著超越 SOTA。


<details>
  <summary>Details</summary>
Motivation: 图像篡改局部定位难，关键瓶颈是像素级高质量标注数据昂贵且稀缺；现有手工数据集规模很小（如 IMD20），限制了模型泛化与实用性。

Method: 1) 数据与自动标注：提出 CAAA v2，从“受限图像操纵定位”辅助任务自动生成高精度像素级伪造掩码；2) 质量控制：提出 QES 质量评估指标，过滤低可信度自动标注；3) 数据集：据此构建大规模多样的 MIMLv2，包含 246,212 张手工伪造图像与像素掩码；4) 训练增强：提出 Object Jitter，合成高质量操纵伪迹以增强训练；5) 模型：设计 Web-IML，能有效利用网页规模监督进行图像操纵定位。

Result: 在多个真实世界伪造基准上显著提升：引入网页监督后，Web-IML 实现31%的性能提升；相较此前 SOTA（TruFor）在平均 IoU 上提升24.1 点。

Conclusion: 通过网页可得的手工伪造数据与自动标注框架（CAAA v2+QES），配合 Object Jitter 和专门模型 Web-IML，显著缓解了数据稀缺并大幅提升操纵区域定位精度；MIMLv2 规模达现有手工数据集的120倍以上，代码与数据将开源。

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [84] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: 提出ExpertSim：面向ALICE零度量能器（ZDC）的生成式仿真框架，采用“生成专家混合”（Mixture-of-Generative-Experts）架构，以专门化专家分治复杂数据分布，实现比传统蒙特卡洛更快且更准确的探测器响应模拟。


<details>
  <summary>Details</summary>
Motivation: LHC探测器响应仿真依赖蒙特卡洛，计算代价高、占用CERN计算网格资源；同时不同模拟条件导致数据分布差异大，通用生成模型难以覆盖。因此需要一种既高效又能适配多模态/分段分布的仿真方法。

Method: 构建Mixture-of-Generative-Experts体系：将数据空间划分为若干子分布，由多个生成式“专家”分别学习其子集（可能通过门控/路由机制进行样本分配）；每个专家专注ZDC响应的特定方面，实现更细粒度的建模与生成；整体作为深度学习仿真器替代部分MC过程。

Result: 在ZDC场景下，相比传统蒙特卡洛方法显著提速，同时提升生成精度（更贴合真实/高保真响应分布）；对多样化数据分布具有更强适应性。

Conclusion: ExpertSim证明了专家混合生成框架在高能物理探测器仿真中的有效性：在保持或提升精度的同时大幅加速，缓解计算资源压力，适合作为CERN高效率仿真的候选方案；代码已开源以便复现与扩展。

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [85] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 提出一种可解释、模块化的Causal-Why VideoQA框架：先抽取自然语言因果链，再基于因果链作答；在三大数据集上超越SOTA，并提升可解释性、信任与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有Causal-Why VideoQA模型将视频理解、因果推理与答案生成紧耦合，形成黑盒，难以解释，且易依赖浅层启发式，尤其在高阶因果推理任务上表现欠佳。

Method: 提出两阶段模块化架构：1) Causal Chain Extractor (CCE) 从视频与问题对中生成自然语言因果链（可解释中间表征）；2) Causal Chain-Driven Answerer (CCDA) 以因果链为依据生成答案。为解决缺乏标注因果推理轨迹的问题，利用大语言模型从现有数据集可扩展地自动合成高质量因果链；并提出面向因果性的字幕评测指标CauCo。

Result: 在三个大规模基准上优于现有SOTA；同时在可解释性、用户信任与跨域泛化方面取得明显提升；CCE可作为可复用的因果推理引擎。

Conclusion: 通过将因果推理与答案生成解耦并引入自然语言因果链，本方法实现了更透明、可解释且具备更强泛化能力的Causal-Why VideoQA，且配套的自动链生成与新评测指标完善了训练与评估生态。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [86] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: 提出POSE，一种用于大型视频扩散模型的一步蒸馏框架，通过两阶段流程显著减少采样步数，实现单步高质量视频生成，并在条件生成中提升语义与时序一致性；在VBench-I2V上显著优于现有加速方法并将推理时延降至原来的1/100。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散加速多沿用图像方法，无法建模跨帧时序一致性，且缺乏对大规模视频模型的一步蒸馏，导致长序列和大模型采样效率低、速度慢。

Method: POSE的两阶段蒸馏：1) 稳定预热（stability priming）：从高到低信噪比区间逐步适配一步生成器的高质量轨迹，稳定对抗式蒸馏，优化流轨迹端点附近的一步映射质量。2) 统一对抗平衡（unified adversarial equilibrium）：在高斯噪声空间内进行自对抗训练，推动训练朝向纳什均衡，生成更接近真实视频的一步结果。对于条件视频生成，引入3) 条件对抗一致性（conditional adversarial consistency），同时提高条件帧与生成帧的语义一致性与帧间一致性。

Result: 在VBench-I2V上，相比其他加速方法在语义对齐、时间一致性与帧质量上平均提升7.15%；在保持竞争性效果的同时，将预训练模型推理时延从1000秒降至10秒（约100倍加速）。

Conclusion: POSE有效将大规模视频扩散模型蒸馏为单步生成器，兼顾时序与语义一致性，显著提升采样效率并维持高质量视频生成，为长序列与大模型的视频生成提供实用的加速方案。

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [87] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 面向成批相似提示词的文本到图像扩散生成，利用早期去噪的共享结构，在不改模型的前提下共享前几步计算，显著降算力并提升画质。


<details>
  <summary>Details</summary>
Motivation: 单次生成的加速已被大量研究，但在大批量、相关提示词场景中存在跨样本冗余，造成高成本与碳排放；作者希望利用扩散过程“由粗到细”的特性，跨提示共享早期计算以降低总体成本。

Method: 1) 以语义相似度对提示词进行聚类（针对以图像嵌入作为条件的扩散模型）；2) 对同簇提示在扩散早期步骤共享计算结果，仅在后期细化步骤分化；3) 借助 UnClip 的文本到图像先验，优化扩散步数的分配与切分点；4) 训练免修改、可无缝插入现有流水线。

Result: 在基于图像嵌入条件的扩散模型上，实验表明：总计算量显著下降，同时生成质量有提升；方法可随提示集规模扩展，跨簇共享带来更高的吞吐效率。

Conclusion: 跨提示共享早期去噪能有效压缩大规模文生图的计算与成本，并可提升质量；方法易集成、可扩展，并减轻环境与经济负担。

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [88] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [89] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 提出一种多任务学习方法，利用与主分类任务相关的辅助任务来提升非典型有丝分裂体识别在跨域下的稳健性，并在三个数据集上做了初步验证，结果有前景。


<details>
  <summary>Details</summary>
Motivation: 病理图像中识别非典型有丝分裂体对评估肿瘤侵袭性很关键，但既有机器学习模型在域迁移/域偏移下性能显著下降，需要提高跨域泛化能力。

Method: 采用多任务学习框架，引入与主分类任务（识别非典型有丝分裂体）相关的辅助任务，旨在引导模型关注目标实例本身并忽略图像中随域变化的背景；方法以MIDOG挑战Track 2为场景进行设计与提交。

Result: 在三个数据集（MIDOG 2025 Atypical Training Set、Ami-Br、MIDOG25初测集）上的初步评估显示该方法取得了有希望的性能提升（具体数值未给出）。

Conclusion: 多任务学习通过利用辅助任务可缓解域偏移对非典型有丝分裂体识别的影响，具备跨域泛化潜力，但仍需更全面的实验与定量结果来证实。

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [90] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [91] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [92] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出CogVLA：通过指令驱动的路由与稀疏化，在保持/提升VLA性能的同时显著降低训练与推理开销，达成LIBERO与真实机器人SOTA并开源。


<details>
  <summary>Details</summary>
Motivation: 当前基于大规模VLM的VLA需要大量后训练与高计算开销，限制可扩展与部署；希望借鉴人类多模态协调，以更高效的方式在视觉、语言与动作间对齐并路由信息。

Method: 三阶段渐进式架构与认知对齐：1) EFA-Routing：将指令通过FiLM注入视觉编码器，按指令选择性聚合与压缩双流视觉token，得到指令感知的紧凑表征；2) LFP-Routing：在LLM侧用FiLM并基于动作意图修剪与指令无关的视觉对齐token，实现token级稀疏；3) V-L-A Coupled Attention（CAtten）：因果V-L注意力结合双向动作并行解码，保证压缩后仍能生成准确连贯动作。

Result: 在LIBERO基准与真实机器人任务上取得SOTA：成功率分别97.4%与70.0%；相较OpenVLA训练成本降2.5倍、推理时延降2.8倍。

Conclusion: 指令驱动的路由与稀疏化可在不牺牲、甚至提升性能的同时显著降低VLA的计算成本；CogVLA验证了认知对齐的架构设计对多模态感知-语言-动作耦合的有效性，并具备实用部署价值。

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [93] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 提出HydraFake真实场景导向数据集与Veritas多模态LLM检测器，通过层级化泛化评测与模式感知推理训练，显著提升对未知模型、未知伪造技术和新域的鲁棒检测，并提供可解释输出。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测基准与工业现实脱节：训练源单一、测试图像质量低，导致模型在真实复杂场景与新型伪造上的失效，需要更贴近实务的评测与具备推理能力的检测器。

Method: 1) 数据集HydraFake：汇集多样深伪技术与野外伪造，设计层级化泛化测试；训练/评测协议覆盖未见模型架构、涌现伪造技术与新数据域。2) 模型Veritas：基于多模态大语言模型，引入“模式感知推理”，包含规划与自我反思等关键推理模式，模拟取证流程；提出两阶段训练，将深伪推理能力无缝注入现有MLLM；强调透明、可核验的检测输出。

Result: 在HydraFake上，传统检测器虽能跨模型泛化，但在未见伪造类型与新域上表现不佳；Veritas在多种OOD情形实现显著性能提升，并产出更可解释、可信的结论。

Conclusion: 通过更贴合实务的HydraFake与具备模式化推理能力的Veritas，可有效弥合学术与工业应用间的鸿沟，提升对真实世界未知伪造与域迁移的鲁棒检测与可解释性。

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [94] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [95] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 提出首个数据驱动的多视角3D点追踪器，利用少量相机（如4台）在动态场景中在线鲁棒追踪任意点，显著缓解单目歧义与遮挡问题，并在真实基准上取得厘米级误差。


<details>
  <summary>Details</summary>
Motivation: 现有单目点追踪受深度歧义与遮挡影响严重；多相机方法往往需要大量相机（>20）与繁琐的逐序列优化，难以在线使用。需要一种能以实际可行的相机数量、端到端前馈、可在线运行、对遮挡稳健的3D点追踪方案。

Method: - 输入：已知相机位姿与多视角深度（传感器或估计）。
- 融合：将多视角特征融合到统一点云表示。
- 匹配：基于k近邻的相关性搜索，结合Transformer更新模块，估计长距离3D对应关系（能跨遮挡）。
- 训练：在5K条Kubric合成多视序列上训练；推理端到端前馈，无需逐序列优化。
- 适配：可处理1–8视角、不同视角布局与24–150帧视频。

Result: 在Panoptic Studio与DexYCB上分别达成3.1 cm与2.0 cm的中位轨迹误差；对视角数量、机位多样性与序列长度具备良好泛化与稳健性。

Conclusion: 以少量相机实现在线、稳健、准确的3D点追踪，缓解遮挡与深度歧义，设定多视角3D追踪的新基准；开源模型与数据以促进研究与实际应用。

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [96] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward提出用“单一奖励模型”的统一RL框架，驱动多任务生成（以掩码引导图像编辑为例），在不同评测标准下提升生成质量；由此训练的Seedream 3.0 Fill无需任务特定SFT且在多维指标上超越商用与开源对手。


<details>
  <summary>Details</summary>
Motivation: 现有掩码编辑子任务（补全、扩展、去除、文字渲染）虽然共享条件形式，但数据分布与评价标准差异大，通常需分别做SFT，带来泛化差、成本高、难统一优化的问题。作者希望用一个可区分胜负的VLM奖励，跨任务、跨指标统一优化生成模型。

Method: 1) 设计OneReward：以单个VLM作为生成奖励模型，输入同一任务下的候选对比，判别优劣，形成可用于RL的奖励信号；2) 将其应用于掩码引导图像编辑多子任务，直接在预训练基础模型上做多任务RL，无需任务特定SFT；3) 实现Seedream 3.0 Fill，统一训练一个编辑模型；4) 以多维评价（多任务、多标准）验证。

Result: 统一的Seedream 3.0 Fill在多种编辑任务与评测维度上，稳定优于商用（Ideogram、Photoshop）与开源（FLUX Fill [Pro]）方案。

Conclusion: 一个VLM奖励即可在多任务与多评价标准下提供一致优化信号，使掩码编辑模型无需任务特定SFT仍能取得SOTA级表现；方法具备通用性，有望推广到其他多任务生成场景。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [97] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: Dress&Dance 是一个视频扩散框架，可在1152×720、24FPS下从单张用户照与参考动作视频生成5秒高质量虚拟试穿视频，支持上装、下装、连体以及上下装同时试穿。核心是CondNet，多模态（文本/图像/视频）注意力条件网络，配合分阶段、异构数据训练，提升服装贴合与动作保真，效果优于现有开源与商用方案。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿多聚焦静态图像或短、低分辨率视频，难以在高分辨率和长序列中同时保证衣物对齐（注册）与动作保真，且多模态条件（用户照、服装图、文字、参考视频）融合不充分。缺少可在有限视频数据下有效利用大量图像数据的训练范式。

Method: 提出视频扩散生成框架Dress&Dance，并设计CondNet作为统一条件网络：通过注意力机制整合文本、图像（用户、服装）、参考视频条件，指导扩散模型生成。训练采用多阶段渐进策略，将稀缺的视频数据与规模更大、更易获取的图像数据结合，以提升服装注册与运动一致性；支持单次前向同时试穿上下装。

Result: 在5秒、24FPS、1152×720设置下，从单张用户图与参考视频生成高质量虚拟试穿视频。相较现有开源与商用方案，在画质、服装贴合与动作保真上取得更佳表现；实现灵活多类服装与多模态条件输入。

Conclusion: CondNet驱动的多模态条件融合与渐进式、异构数据训练，使Dress&Dance在高分辨率虚拟试穿视频生成上实现质量和灵活性双提升，证明了在有限视频数据下利用图像数据强化视频生成的有效性。

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 提出一套在黑盒与米色盒（已知方法类别但未知细节）场景下高度有效的图像水印擦除攻击，在NeurIPS 2024“Erasing the Invisible”挑战中获胜；综合VAE自适应规避、CIELAB颜色对比恢复，以及基于聚类的扩散重绘与ChatGPT语义先验，最终以≈95.7%成功率去除水印，且对图像质量影响极小。


<details>
  <summary>Details</summary>
Motivation: 现有内容水印常被用于认证与版权保护，但其在不同程度对手知识下的对抗鲁棒性仍不明确；需要系统性压力测试并推动更强鲁棒水印方法的发展。

Method: - Beige-box：构建自适应的VAE规避攻击，结合测试时优化（test-time optimization）以最小化水印可检测性，同时在CIELAB空间进行颜色与对比度恢复，确保感知质量。
- Black-box：先对图像进行基于空间/频域伪影的聚类；随后对各簇采用图像到图像扩散模型（controlled noise injection）进行受控噪声注入的重绘，并利用由ChatGPT生成的描述作为语义先验来引导扩散；对每簇参数进行优化以提升去水印效果。

Result: 在挑战赛评测中实现近乎完美的去水印成功率（95.7%），同时对残留图像质量影响可以忽略。

Conclusion: 所提多场景自适应攻击显著削弱了现有图像水印的有效性，提示当前方法在对抗性鲁棒性方面存在明显缺陷；作者呼吁社区研发更健壮的水印方案。

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>
