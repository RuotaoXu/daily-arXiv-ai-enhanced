<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: ReT-2 提出统一多模态检索模型，支持图文混合查询与跨图文文档检索，通过多层表示与含LSTM门控的递归Transformer融合跨层与跨模态信息，在M2KR与M-BEIR等基准上达SOTA，并在RAG中提升下游VQA/检索问答表现，同时推理更快、显存更省。


<details>
  <summary>Details</summary>
Motivation: 现有方法多需针对具体任务微调，且通常只支持单一模态的查询或文档，难以应对真实应用中复杂的多模态检索需求；同时需要在保证效果的前提下降低推理延迟与内存占用，以便在LLM/RAG场景广泛落地。

Method: 提出ReT-2：1) 统一的多模态检索框架，支持图+文组合查询与图文共存文档库；2) 多层表示学习，捕捉细粒度视觉与文本特征；3) 递归Transformer架构，引入受LSTM启发的门控机制，动态整合跨层与跨模态信息；4) 在不同检索配置下训练/评估，并可无缝接入RAG流水线。

Result: 在M2KR与M-BEIR等多种检索设置下，ReT-2稳定取得SOTA；相较先前方法拥有更快的推理速度与更低的显存/内存消耗；接入RAG后，在Encyclopedic-VQA与InfoSeek数据集上进一步提升下游性能。

Conclusion: ReT-2是面向复杂多模态检索的统一模型，通过递归Transformer与门控融合实现高效准确的跨模态匹配，兼顾性能与资源效率，并能为RAG应用带来显著增益；代码与模型已开源，具备实际落地潜力。

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [2] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: 提出用视觉扩散模型(VDM)特征+Transformer汇聚，实现跨物种、视角与场景的动作识别强泛化，并通过在扩散早期时间步提取更语义化的特征取得三项基准SOTA。


<details>
  <summary>Details</summary>
Motivation: 人类能在巨大上下文与视角变化下稳定识别动作（物种差异、第一/第三人称、真实/影视），而现有深度模型在这类分布偏移上的泛化不足，需寻找更鲁棒的表征。

Method: 利用视觉扩散模型生成的中间特征作为视频帧表征，通过Transformer进行时空聚合。关键设计是选择扩散过程的较早时间步的条件特征，以强调语义信息而非像素细节，从而提升跨域泛化能力；在多个跨域设置上训练/评估。

Result: 在跨物种、跨视角、跨录制场景三类泛化基准上均取得新的SOTA性能，优于现有深度学习方法。

Conclusion: 基于VDM早期语义特征并用Transformer聚合能显著提升动作识别的跨域鲁棒性，使机器在复杂分布变化下更接近人类级泛化；提供项目页与代码以复现与扩展。

Abstract: Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [3] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: 提出PromptGuard框架，核心为VulnGuard Prompt：以数据驱动的对比学习、少样例、伦理推理与角色自适应提示相结合，面向脆弱群体预防有害内容生成，并给出多目标优化与信息论分析的理论证明与收敛性论证。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全主要依赖事后过滤或通用对齐，无法在生成源头主动阻断对弱势群体的有害、偏见与误导内容。需要一个可模块化、可证明、面向特定人群风险的主动防护方法。

Method: 提出PromptGuard六模块体系：输入分类、VulnGuard提示、伦理原则整合、外部工具交互、输出验证、用户-系统交互。关键的VulnGuard Prompt采用混合技术：从GitHub精选库提取的少样例、伦理链式思考、角色自适应提示，并以真实世界数据进行对比学习。框架以多目标优化建模，给出熵界与帕累托最优分析、收敛性与脆弱性的信息论形式化。

Result: 理论上证明可实现约25-30%的“分析性”危害降低（基于熵界与Pareto分析）；给出收敛与脆弱性评估的数学形式化以及使用GitHub数据集的理论验证流程。未报告大规模实证结果，仅提供系统化的理论与形式化验证框架。

Conclusion: PromptGuard提供了一个针对特定弱势人群的主动式提示防护方案，结合对比学习与伦理推理，具备可证明的（理论层面）危害降低潜力，并奠定进一步实证研究的数学基础。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [4] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 提出Beta-SOD：用双Beta混合建模嵌入余弦相似度的异常检测，结合孪生网络与多重相似度损失，在含噪Re-ID中稳健去噪并提升性能。


<details>
  <summary>Details</summary>
Motivation: Re-ID标签噪声常见且会显著伤害性能；现有方法对噪声敏感且缺少理论稳健的去噪机制，需要一种能在训练中识别并抑制噪声样本的统计化方法。

Method: 将Re-ID重构为监督的图像相似度学习，采用孪生网络，联合使用BCE、对比损失与余弦嵌入损失。核心提出Beta-SOD：以两成分Beta混合模型拟合嵌入对的余弦相似度分布，基于该分布进行统计异常检测以识别噪声对；并给出双Beta混合的可辨识性结果，保证学习问题良定。

Result: 在行人(CUHK03、Market-1501)与车辆(VeRi-776)Re-ID上，10–30%噪声范围内优于SOTA；在去噪与Re-ID性能上均取得提升，显示更强鲁棒性与泛化性。

Conclusion: 统计式相似度分布建模与孪生相似度学习互补，可在噪声标签下稳健训练Re-ID。Beta-SOD提供理论可辨识性与实践性能双重优势，并具备跨领域适用性。

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [5] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

TL;DR: 提出SFD-Mamba2Net，在ICA冠脉造影中通过结构先验+状态空间长程依赖+频域细节增强，显著提升血管分割与狭窄检测表现。


<details>
  <summary>Details</summary>
Motivation: ICA因对比度低、噪声高、血管细且结构复杂，导致现有方法难以精准分割和识别狭窄，影响CAD临床评估。需要能突出细长管状结构、抑制背景干扰，并兼顾全局依赖与细节的模型。

Method: 构建端到端SFD-Mamba2Net：编码器嵌入CASE模块，利用多尺度响应与曲率感知强化细长血管、抑制背景、引导注意；解码器引入PHFP模块，采用多层小波分解逐步增强高频细节并融合低频全局结构；整体结合状态空间模型以建模长程依赖。

Result: 在ICA数据上，SFD-Mamba2Net在8项分割指标上均优于SOTA；在狭窄检测上获得最高的真阳性率和阳性预测值，显示更可靠的检出与精准性。

Conclusion: 多尺度结构先验与频域细节增强、长程依赖建模的协同可显著提升ICA冠脉分割与狭窄检测性能，具有临床应用潜力。

Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [6] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

TL;DR: 提出一个全自动从术前增强/非增强MRI预测结直肠肝转移(CRLM)术后生存的框架：用提示可控的分割流程（含SAMONAI零样本3D提示传播）得到肝/肿瘤/脾分割，再以多实例自编码SurvAMINN提取肿瘤放射组学特征做生存分析，在227例数据上C-index较临床/基因标志提升>10%。


<details>
  <summary>Details</summary>
Motivation: CRLM手术是唯一潜在治愈手段，但预后差异大，现有基于有限临床或分子特征的模型预测力不足，尤其多灶病例。需要一种既准确又省标注、可解释的术前影像学方法提升个体化预后评估。

Method: 两阶段框架：1) 分割管线：利用提示型基础模型补全缺失标签，提出SAMONAI零样本3D点提示传播，借助SAM从单点提示生成3D ROI，提高肝/肿瘤/脾分割精度与效率；2) 放射组学管线：对比前后对比增强MRI的预测分割，逐肿瘤提取特征，使用SurvAMINN（自编码器+多实例生存网络）在删失数据上联合学习降维与风险预测，强调最具侵袭性的肿瘤贡献。

Result: 在227例机构数据集上，框架的生存预测优于现有临床与基因生物标志物，C-index提升超过10%，并具备自动化、少标注和可解释性优势。

Conclusion: 将自动分割与基于多实例的放射组学生存分析集成，可在CRLM实现更准确、省标注、可解释的术后结局预测，显示出临床转化潜力。

Abstract: Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [7] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 提出CompCon：一种用于比较两文本到图像模型视觉表征差异的进化搜索方法，自动发现“某模型更常产生而另一个较少/不产出”的视觉属性及其由哪些提示概念触发，并构建ID2数据集验证与基线对比，实证揭示主流模型在情感场景与职业刻画上的分歧与潜在偏差。


<details>
  <summary>Details</summary>
Motivation: 不同生成模型在相同文本提示下常呈现不同视觉风格、语义偏向与隐含偏见，但缺乏系统方法去定位“哪些视觉属性”“在何种提示语境下”产生差异；现有依赖人工或通用LLM/VLM评估的方法效率低、可解释性弱、难以规模化。

Method: 提出CompCon（Comparing Concepts）：以进化搜索为核心，在两模型间生成对照样本，自动挖掘一方更高流行度的视觉属性，并反向关联触发这些属性的提示概念；构建自动数据生成流水线，得到包含60个输入依赖差异的ID2数据集；与多种LLM/VLM基线对比评估发现能力。

Result: CompCon能系统性发现两模型在视觉属性上的分歧及其触发提示，如PixArt在“孤独”相关提示中偏向生成“湿漉街道”景象，Stable Diffusion 3.5在媒体职业相关提示中对非裔美国人呈现特定描绘；在ID2数据集上优于若干基线。

Conclusion: CompCon为比较生成模型表征差异提供了可扩展、可解释的自动化框架，可用于审计与诊断模型偏差与风格差异；ID2数据集与代码为后续研究提供基准与工具。

Abstract: In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [8] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

TL;DR: 提出利用U-Net深度学习从UAS影像中检测并校正云影与太阳耀斑（sun glint），以提升水质参数反演的可靠性。


<details>
  <summary>Details</summary>
Motivation: UAS影像具备高时空/光谱灵活性且可在多云条件下获取，但常受云影与水面耀斑干扰，严重影响水环境遥感（尤其水质参数估算）的准确性，因此需要自动、鲁棒的方法识别并纠正这些干扰区域。

Method: 在像素级从UAS影像提取训练数据，训练U-Net分割模型，以区分云影、太阳耀斑与无障碍区域；通过多种测试集评估指标筛选最优训练设置；在此基础上构建图像校正流程，用于恢复受云影与耀斑影响的区域。

Result: 模型能有效分割并提取云影与太阳耀斑区域；根据评估指标选出最佳模型与训练配置，形成高质量的影像校正模型，能够对受影响区域进行恢复。

Conclusion: 基于U-Net的检测与校正方法可显著减轻UAS水体影像中云影与耀斑的影响，提升水质参数反演可靠性与影像质量。

Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [9] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

TL;DR: 提出CoSwin，将Swin Transformer的分层移窗注意力与可学习的局部卷积特征增强模块融合，兼顾局部细节与全局语义；在多小样本数据集上较Swin显著提升精度，验证局部-全局融合的有效性。


<details>
  <summary>Details</summary>
Motivation: ViT在小数据集上偏重全局上下文而缺乏局部性与平移等归纳偏置，导致局部特征提取不足与泛化受限。需要一种能在Transformer中补齐卷积式局部归纳偏置的方法。

Method: 在每个注意力块中插入可学习的局部特征增强模块（类似卷积）与Swin的分层移窗自注意力并行/融合，实现局部细节与全局语义的联合建模；整体为特征融合架构CoSwin。

Result: 在CIFAR-10/100、MNIST、SVHN、Tiny ImageNet上全面优于SOTA卷积与Transformer模型；相对Swin基线提升：CIFAR-10 +2.17%、CIFAR-100 +4.92%、MNIST +0.10%、SVHN +0.26%、Tiny ImageNet +4.47%。

Conclusion: 在小规模视觉任务中，引入卷积式局部归纳偏置并与Transformer全局注意力融合，能有效提升泛化与鲁棒性；CoSwin验证了局部-全局特征融合的价值，并公开代码与预训练权重。

Abstract: Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [10] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

TL;DR: iMatcher 是一个端到端可微的点云特征匹配框架，通过局部图嵌入初始化匹配得分、双向最近邻重定位、以及全局几何一致性学习，输出几何一致的匹配概率矩阵，并在多数据集上显著提升刚体配准的内点率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准中，特征匹配往往缺乏对局部与全局几何一致性的统一建模，导致内点率低、对噪声与部分重叠敏感；需要一个可微、可学习的机制来直接预测几何一致的匹配概率，提高配准鲁棒性与精度。

Method: 提出 iMatcher：1) 局部图嵌入模块对点特征进行聚合，初始化匹配分数矩阵；2) 通过3D空间的双向（源到目标、目标到源）最近邻搜索进行重定位，细化分数矩阵；3) 将成对点特征堆叠，输入全局几何一致性学习模块，输出点级匹配概率（置信度）矩阵；整体为端到端可微。

Result: 在KITTI、KITTI-360、3DMatch等真实数据集及TUD-L位姿估计、MVP-RG局部-局部匹配任务上，显著提升刚体配准表现：内点率SOTA，KITTI达95%-97%，KITTI-360达94%-97%，3DMatch最高81.1%，体现出在室外/室内、多任务中的鲁棒性。

Conclusion: 通过联合建模局部与全局几何一致性并端到端学习，iMatcher可预测几何一致的匹配概率矩阵，显著提高特征匹配质量与配准鲁棒性，在多基准上达SOTA性能，适用于多样场景与任务。

Abstract: This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [11] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 提出UltrON：一种利用B超声学特征、基于occupancy的隐式表示方法，在弱监督下实现更稳健的3D重建，缓解遮挡与稀疏标注问题，并通过新颖损失处理视角依赖。


<details>
  <summary>Details</summary>
Motivation: 自由手超声需要操作者在脑中将多帧2D合成为3D形状，现有隐式表示多依赖精确分割标注且忽视B模强度中的声学信息；同时超声具有视角依赖与声影遮挡，导致现有SDF/occupancy方法重建不稳健。

Method: 采用occupancy隐式表示，提出UltrON框架：1) 从B模图像中自动提取声学特征（无需额外标注）以引导几何一致性；2) 设计新型损失函数，显式补偿B模成像的视角依赖，使多视角超声可用于occupancy优化；3) 在弱监督（稀疏/不精确注释）下进行优化并利用声学属性促进跨个体同解剖结构的泛化。

Result: 在多视角自由手超声数据上，UltrON相较依赖精确标注的SDF/以往隐式方法，更能缓解遮挡与标注稀缺带来的退化，实现更准确、几何一致的3D重建；对同一解剖结构的不同个体具有更好的泛化能力。代码和数据集将开源。

Conclusion: 基于occupancy并融合声学特征与视角补偿损失的UltrON，在弱监督情境下有效提升3D超声重建的准确性与稳健性，降低对精确分割标注的依赖，为临床友好的形状重建提供新途径。

Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [12] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: 提出一种基于隐式神经表示（INR）与学习到的潜在代码的模型，用于从心肌标记MRI自动估计左心室连续位移与应变；在UK Biobank上比三种DL基线更准且大幅更快。


<details>
  <summary>Details</summary>
Motivation: 心肌标记MRI可量化心肌位移与应变，但自动化、准确、可扩展的追踪仍具挑战：传统或深度方法常需推理时优化、精度/速度权衡、以及跨病例泛化受限。需要一种既准确又无需推理期优化、可在大规模数据上高效运行的方案。

Method: 使用隐式神经表示（INR）建模连续的左心室位移场；通过学习的潜在代码对INR进行条件化，从而在推理时直接预测位移而不需逐病例优化。与三种深度学习基线对比评估。

Result: 在452个UK Biobank测试病例上，位移跟踪RMSE为2.14 mm，为最佳；全局环向应变误差2.86%、径向应变误差6.42%，综合最低；相较最准确基线，推理速度约快380倍。

Conclusion: INR条件化模型在心肌标记MRI中可实现高精度、无需推理期优化且极具速度优势的位移与应变估计，适用于大规模CMR数据集的应变分析。

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [13] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

TL;DR: 提出E-MLNet，通过对OEM引入动态加权，利用闭集分类器预测为每个目标样本聚焦最相关的一对多分类器，从而更好区分已知与未知；在Office-31、Office-Home、VisDA-2017、ImageCLEF等基准上优于MLNet，H-score更高且鲁棒性更强。


<details>
  <summary>Details</summary>
Motivation: 现有UniDA方法如MLNet用一组一对多分类器并通过OEM自适应，但默认等权对待所有分类器，导致学习信号被稀释、边界不聚焦，影响已知/未知判别与迁移效率。

Method: 在MLNet框架上，引入“增强互学习”E-MLNet：用闭集分类器对目标样本的预测作为引导，给各一对多分类器分配动态权重，聚焦与样本最相关的类别决策边界；将该权重融入OEM目标，强化相关分类器的适应、抑制无关分类器的影响，从而更清晰地区分已知/未知。

Result: 在Office-31、Office-Home、VisDA-2017、ImageCLEF四个基准上进行大量实验：在VisDA与ImageCLEF上取得最高平均H-score；在Open-Partial设置31个任务中胜过MLNet 22次，在Open-Set设置31个任务中胜过19次，展现更强鲁棒性与总体性能提升。

Conclusion: 对OEM引入基于闭集预测的动态加权能将适应聚焦于相关类边界，提升已知/未知分离与整体迁移表现；E-MLNet在多个基准与设置上系统性优于MLNet，验证了该聚焦式适应策略的有效性与鲁棒性。

Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [14] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: 提出COCO-Urdu：基于MS COCO的乌尔都语大规模图文数据集（59k图像/319k字幕），并发布结合翻译质量与视觉一致性的混合质量评估与迭代修正管线，以缓解多模态研究中的语言偏置。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语用户众多但在多模态/视觉-语言研究中严重欠服务，缺乏大规模高质量数据导致模型对高资源语言偏置严重，限制了乌尔都语能力发展。

Method: 从MS COCO按分层抽样保留原始分布，使用SeamlessM4T v2翻译字幕；构建混合质量评估框架：COMET-Kiwi评估翻译质量，CLIP相似度评估视觉对齐，BERTScore+回译评估语义一致性；对低分字幕用开源大语言模型迭代修正；最终在BLEU、SacreBLEU、chrF上进行基准测试。

Result: 得到包含59,000张图像和319,000条乌尔都语字幕的数据集；在多种自动指标上表现稳定且强；数据与质量评估管线将公开。

Conclusion: COCO-Urdu是目前最大的公开乌尔都语图像字幕数据集；所发布的数据与质量评估流程为更包容的视觉-语言系统奠定基础，并有望减少多模态研究中的语言偏置。

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [15] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

TL;DR: 提出VoxelFormer，一种轻量级Transformer，通过多受试者训练从fMRI解码视觉信息，并与CLIP图像嵌入对齐；在7T NSD上以更少参数取得有竞争力的检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI视觉解码多依赖单受试者特定训练，扩展性和部署性差；需要一种能跨受试者共享、参数高效且与视觉语义空间对齐的方法。

Method: 设计VoxelFormer，包括两部分：1) Token Merging Transformer（ToMer）对高维稀疏体素进行高效压缩，减少序列长度与计算；2) 查询驱动的Q-Former，将压缩后的体素序列映射为固定长度表示，并与CLIP图像嵌入空间对齐，支持多受试者联合训练。

Result: 在7T Natural Scenes Dataset上，VoxelFormer以显著更少的参数达到对训练包含受试者的图像检索任务上的有竞争力表现。

Conclusion: 基于token合并与查询式Transformer的设计能实现参数高效的多受试者fMRI视觉解码，为可扩展和实用的神经解码提供了有前景的方向。

Abstract: Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [16] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: 提出PCGM，一个结合概率图先验与3D扩散模型的框架，用由因果/解剖约束生成的空间掩膜引导Counterfactual MRI合成，能保留细微解剖差异并复现实证中的疾病皮层效应。


<details>
  <summary>Details</summary>
Motivation: MRI获取昂贵且肉眼难以分辨微小形态差异，现有反事实生成模型缺乏解剖归纳偏置，优化全局外观而非医学关键的局部细微变化，导致生成解剖不可信。需要一种方法在生成时显式保留体素级细粒度变异。

Method: 构建Probabilistic Causal Graph Model：以概率图模块编码解剖/因果约束，产出标示细微差异区域的空间二值掩膜；采用3D版ControlNet将掩膜条件注入到新设计的反事实去噪UNet；再用3D扩散解码器将编码转为高质量脑MRI。整体为受掩膜约束的扩散式反事实生成。

Result: 在多数据集上较多个基线生成更高质量的结构MRI；更重要的是，从PCGM生成的反事实中提取的脑测量指标能够复现文献报道的疾病对皮层区域的细微影响。

Conclusion: 引入体素级解剖先验并以掩膜条件化的扩散模型进行反事实合成，可产生解剖可信、细节保真的3D脑MRI；证明了合成MRI可用于研究细微形态差异，推动了神经影像学中反事实生成的可靠应用。

Abstract: 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [17] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

TL;DR: 提出Med3DInsight：用2D多模态大模型（MLLM）增强3D医学图像理解，通过切片感知Transformer连接3D编码器与2D MLLM，并用部分最优传输做鲁棒对齐，无需人工标注，在CT/MRI分割和分类上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学SSL（卷积或Transformer）在语义层面理解不足；2D MLLM具备强文本理解与跨模态对齐能力，但直接用于3D体数据存在维度鸿沟与噪声文本问题。因此需要一种能将3D体数据有效映射到2D MLLM语义空间、且对LLM生成噪声鲁棒的预训练框架。

Method: 设计Med3DInsight：1）3D图像编码器提取体数据特征；2）Plane-slice-aware Transformer将体数据按切片/平面建模并与2D MLLM接口对齐，实现体到平面语义桥接；3）引入部分最优传输（Partial OT）用于图像-文本对齐，容忍LLM生成文本的噪声与不完全匹配；4）在无标注条件下进行可扩展多模态预训练。

Result: 在多个公开CT/MRI数据集的两类下游任务（分割与分类）上取得SOTA，优于现有SSL方法；模型可无缝集成到现有3D医学网络并提升性能。

Conclusion: 将3D编码与2D MLLM通过切片感知Transformer和Partial OT对齐，实现无需人工标注的可扩展3D医学多模态表征学习，显著提升分割与分类性能，并具备良好兼容性。

Abstract: Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [18] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

TL;DR: 提出在骨架GCN基础上引入固定物体场景信息并采用多任务学习，以提升人-物交互识别；在自采公共环境数据上达99.25%准确率，比仅用骨架的基线提升2.75%。


<details>
  <summary>Details</summary>
Motivation: 仅用人体骨架的GCN对人-物交互识别效果不佳，原因在于缺乏对固定场景物体的有效表征及相应学习架构。实际环境中（如ATM、自助闸机）动作理解需要结合人与固定物体的关系，因此需要把场景/交互区域信息纳入模型以提升识别。

Method: 构建利用固定物体（交互区域）信息的多任务学习框架：在骨架GCN的基础上，加入表示环境中固定对象/交互区域的特征通道或掩膜，并通过多任务目标（如动作分类+交互区域相关任务）联合训练，从而强化对人-物交互的判别能力。

Result: 在作者自建的公共环境真实数据集（包含与固定物体交互类如ATM/闸机操作，以及非交互类如走路/站立）上评估：多任务+交互区域信息模型的动作识别准确率达99.25%，较只用骨架的基线提升2.75%。

Conclusion: 将固定物体场景信息与多任务学习结合，显著提升了基于骨架的动作识别在人-物交互场景的性能，验证了场景/交互区域对动作理解的重要性。

Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [19] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

TL;DR: 提出IRDFusion跨模态对比与筛选的特征融合框架，通过迭代关系图与差分反馈，强化目标显著结构、抑制共享背景噪声，在FLIR、LLVIP、M3FD上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱目标检测在特征融合时常保留无关背景/噪声，导致跨模态对齐差、判别力不足，影响弱光、复杂场景下的感知性能。需要一种能自适应强化目标相关互补信息、抑制共模干扰的融合机制。

Method: 构建IRDFusion：由两大模块迭代耦合形成。1) 互信息/关系建模的“互特征细化模块”(MFRM)，同时在模态内与模态间建模关系，提升跨模态对齐与区分性；2) 受差分放大器启发的“差分特征反馈模块”(DFFM)，计算跨模态差分作为引导信号回馈给MFRM，促进融合互补、抑制共模噪声；通过迭代的关系图-差分反馈逐步放大显著关系信号并抑噪。

Result: 在FLIR、LLVIP、M3FD等多光谱检测数据集上，IRDFusion取得SOTA并在多种复杂场景下稳定优于现有方法。

Conclusion: IRDFusion通过MFRM与DFFM的迭代差分反馈，有效提升跨模态对齐与判别力，抑制背景噪声，实现高质量融合与显著性能提升，具有鲁棒性与通用性；代码将开源。

Abstract: Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [20] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: 提出SQAP-VLA：首个结构化、免训练的VLA推理加速框架，同时实现量化与token裁剪，兼顾效率与性能，达1.93×加速并提升平均成功率至多4.5%。


<details>
  <summary>Details</summary>
Motivation: VLA模型在具身智能上表现强，但计算与内存开销大，现有仅做量化或裁剪且彼此不兼容，无法整体提升效率，亟需能协同两者的通用方法。

Method: 共设计量化与token裁剪流程：在激进量化的前提下提出“量化感知的token裁剪准则”，并改进量化器以提升裁剪有效性；全流程结构化、免训练，直接用于现成VLA模型推理。

Result: 在标准VLA模型上显著提升计算效率与推理速度，实现约1.93×加速；同时保持核心性能，甚至平均成功率最高提升4.5%，优于原模型与以往单一手段。

Conclusion: 通过协同设计量化与裁剪，SQAP-VLA打破二者不兼容瓶颈，在不训练的前提下实现VLA推理端的系统性加速与性能保持/提升，为实际部署提供可行路径。

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [21] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: 提出S-BEVLoc：一种无需真值位姿、基于鸟瞰图(BEV)的自监督LiDAR全局定位框架，在KITTI与NCLT上实现最先进的地点识别、回环与全局定位效果。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR全局定位多依赖GPS或SLAM里程计提供的高精度真值位姿进行监督训练，采集成本高且不易扩展；需要一种去真值、可扩展的训练方法。

Method: 将单幅BEV图像分割为以关键点为中心的BEV小块，利用其已知地理距离构造训练三元组；用CNN提取局部特征，NetVLAD聚合生成全局描述子；提出SoftCos损失强化从三元组中学习；整体形成自监督框架S-BEVLoc。

Result: 在KITTI与NCLT大规模数据集上，S-BEVLoc在地点识别、回环检测和全局定位任务上达到SOTA表现，同时训练与部署具备高可扩展性。

Conclusion: 通过BEV表征与距离驱动的自监督三元组学习，S-BEVLoc在无需真值位姿的情况下实现高精度全局定位，减少标注成本并提升可扩展性，对大规模SLAM中的回环与重定位具有实际价值。

Abstract: LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [22] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

TL;DR: 提出FPI-Det数据集，聚焦移动设备使用检测，含2.29万张图并标注人脸与手机，在多场景、尺度与遮挡下评测YOLO与DETR并给出基线与细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 通用目标检测基准难以刻画“人-设备”细粒度交互；在安全监控、生产力评估与注意力管理中，需判断是否在用手机，这不仅是物体检测，还需理解人与设备（脸、手、手机）关系，现有数据与评测不足以支撑此类行为语境推理。

Method: 构建FPI-Det数据集：22,879张图像，覆盖职场、教育、交通、公共等场景；同步标注人脸与手机，包含极端尺度变化、频繁遮挡与多样采集条件。选取代表性YOLO与DETR系列检测器进行训练与评测；从目标尺寸、遮挡程度与环境场景等维度做基线与误差分析。

Result: 给出YOLO与DETR在FPI-Det上的基线性能，并报告不同尺寸、遮挡等级与场景下的表现差异（整体上受小目标与遮挡显著影响）；提供代码与数据以复现。

Conclusion: FPI-Det填补人-设备细粒度交互检测的基准空白，为“是否使用手机”这类行为感知任务提供了更贴近真实应用的评测平台；现有通用检测器在小尺度与遮挡条件下仍有明显改进空间。

Abstract: The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [23] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

TL;DR: 提出ZeroPlantSeg：无需训练即可从顶视图图像将多叶重叠的莲座型植物分割为完整个体，结合基础分割模型与视觉-语言模型，跨物种与跨域表现优于现有零样本与部分有监督方法。


<details>
  <summary>Details</summary>
Motivation: 零样本基础分割模型能提取叶片实例，但难以将多个重叠叶片聚合为单个植物个体；传统层级（叶→植株）分割依赖物种特定、标注代价高的数据集。作者希望消除标注需求并提升跨物种、跨场景泛化。

Method: 构建ZeroPlantSeg流水线：1) 使用基础分割模型在顶视图作物图像中零样本提取叶片实例；2) 引入视觉-语言模型，根据植物结构先验与语义关系对叶片进行推理与聚合，得到植株级实例；无需额外训练。

Result: 在包含多物种、不同生长阶段与拍摄环境的数据集上评估，ZeroPlantSeg优于现有零样本方法，并在跨域测试中超过有监督训练的模型。

Conclusion: 通过将基础分割与视觉-语言推理结合，可在无标注与无训练条件下解决植物的层级实例分割，获得强泛化与跨域优势；代码已开源。

Abstract: Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [24] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

TL;DR: 提出WebPerson数据集与GA-DMS框架，利用MLLM过滤与自动标注5M人物图文对，并通过梯度-注意力引导的双重掩蔽与掩码词预测，提升CLIP在人物表征的细粒度对齐与抗噪性能，达成多基准SOTA。


<details>
  <summary>Details</summary>
Motivation: CLIP虽强，但用于人物表征面临两难：缺乏大规模人物导向的图文数据；全局对比学习对局部判别特征不敏感且易受噪声文本干扰，影响细粒度匹配。

Method: 数据与模型双线提升：1) 数据端：构建噪声鲁棒的数据管线，借助MLLM的in-context能力自动过滤与生成描述，得到5M高质量人物图文对数据集WebPerson。2) 模型端：提出GA-DMS框架——以梯度-注意力相似度自适应掩蔽噪声文本token（Gradient-Attention Guided），并加入Dual-Masking与Masked Token Prediction目标，强制模型预测信息性词，以加强细粒度语义。

Result: 在多项人物相关基准上取得SOTA表现（相较既有方法在检索/重识别等任务上显著提升）。

Conclusion: 通过高质量人物图文数据与梯度-注意力引导的双掩蔽训练目标，缓解CLIP在人物表征中的噪声与细粒度对齐问题，显著提升跨模态匹配与细节表征能力。

Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [25] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: ALL-PET 是一个在投影域工作的低资源、低样本 PET 基础模型，基于潜在扩散模型（LDM），通过Radon掩膜增强、正负掩膜几何约束和“透明医疗注意力”三项创新，在仅500样本下实现高质量正弦图生成并跨多任务泛化。


<details>
  <summary>Details</summary>
Motivation: PET 基础模型受限于标注数据稀缺与算力不足；传统图像域方法难以充分利用几何物理先验且对数据规模敏感，因此需要一种在投影域高效学习、可泛化且资源友好的方法。

Method: 基于LDM，在正弦图（投影域）直接建模：1) Radon 掩膜增强策略（RMAS）将随机图像域掩膜投影到正弦图，配合动态多掩膜（DMM）机制，生成20万+结构多样样本；2) 正/负掩膜约束，注入严格几何一致性，降低参数同时保持生成质量；3) 透明医疗注意力（TMA），基于粗分割得到的病灶关注图（含高/低代谢区），按PET几何投影到正弦图进行无参、物理一致的注意力引导，且支持临床可调ROI。

Result: 在仅500个样本训练下，ALL-PET 生成的正弦图质量与使用大规模数据训练的模型相当，显著提升泛化；在低剂量重建、衰减校正、延迟帧预测、示踪剂分离等任务中表现稳健；整机显存占用<24GB，效率高。

Conclusion: ALL-PET 证明了在投影域利用物理几何先验与掩膜增强的LDM可在低资源、低样本条件下实现高质量PET建模，并具备跨任务泛化与临床可解释、可控的注意力引导能力。

Abstract: Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [26] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

TL;DR: 比较ANN与基于PH（立方复形+有符号欧氏距离变换）的拓扑估计在噪声下的鲁棒性；在1个合成与2个真实数据集上，ANN在预测2D二值图像Betti数时在噪声下优于该PH流水线。


<details>
  <summary>Details</summary>
Motivation: 传统PH提供稳定的拓扑不变量，但对结构噪声需复杂预处理；实践中希望在含噪图像中稳健估计Betti数。作者探究是否可用监督学习（ANN）从数据中学习上下文与几何先验，从而在噪声条件下胜过常用的PH+SEDT方案。

Method: 搭建监督ANN预测2D二值图像的Betti数；对比基准为PH流水线：构造立方复形，基于SEDT计算持久同调；在一个合成和两个真实世界数据集上，系统评测不同噪声水平下的性能与鲁棒性。

Result: 在引入噪声的条件下，ANN在Betti数预测准确率/误差等指标上优于PH+SEDT方案；优势归因于ANN可学习上下文与几何先验，从而抵消结构噪声影响。

Conclusion: 在结构噪声场景中，用ANN进行拓扑量（Betti数）估计是对PH方法的有力替代方案；尽管该方向仍在发展，但显示出更强噪声鲁棒性与实用潜力。

Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [27] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出OSIM，一种面向对象的3D场景评价指标，利用检测与特征表征度量“objectness”，与人类感知更一致，并用于重新评估重建与生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景质量指标多关注整体图像/场景相似度，常与人类感知不一致。基于神经心理学，人类理解场景依赖对单个“对象”的关注，因此需要对象级、与人类感知更一致的评价方式。

Method: 引入OSIM：借助预训练的目标检测模型与其中间特征，对场景中的每个检测到的对象提取表征，并计算对象级的“objectness”相似度，再聚合为场景评分。包含对象检测、特征匹配与相似度度量的流程；并在标准化实验设置下应用于多种3D重建与生成结果。

Result: 用户研究表明，OSIM与人类主观偏好的一致性优于现有指标；对OSIM特性进行了多角度分析；在统一设置下对近期3D重建与生成方法重新评估，揭示更清晰的性能差异与进展。

Conclusion: 对象为中心的评价更契合人类感知。OSIM通过检测与特征相似度刻画对象级质量，优于传统整体指标，并为3D重建/生成领域提供更可靠的比较基准和分析工具。

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [28] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 该综述以“数据集驱动”的新视角梳理视频理解：数据集的运动复杂度、时间跨度、层级结构与多模态丰富性，如何作为归纳偏置压力塑造从双流/3D-CNN到序列、Transformer与多模态基础模型的演进，并据此给出模型设计与可扩展性的实践建议，最终提出统一数据集—归纳偏置—架构的框架与路线图。


<details>
  <summary>Details</summary>
Motivation: 以往综述多按任务或模型家族分类，忽视数据集结构性特征对架构演化的主导作用。作者希望解释为何与如何“数据集属性”在推动视频模型从早期到当下基础模型的里程碑式转变，并提供可操作的设计指导。

Method: 提出数据集四类压力/归纳偏置（运动复杂度、时间跨度、层级组成、多模态性），将历史关键模型（双流、3D CNN、序列模型、Transformer、多模态基础模型）重新解读为对这些压力的回应，并据此总结对齐模型设计与数据集不变性的实务原则与折中。

Result: 形成一个把数据集属性、归纳偏置与模型架构关联起来的综合框架；对现有里程碑方法给出统一的因果式解读；产出面向实践的模型—数据集匹配与可扩展性平衡建议。

Conclusion: 视频理解应以数据集诱导的归纳偏置为核心来设计与选择模型。统一框架既回顾了演进脉络，也为通用视频理解提供了可遵循的处方式路线图与设计指南。

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [29] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: OCELOT 2023挑战提出并验证：将细胞检测与组织分割的多尺度、重叠标注结合，显著提升细胞检测性能；最佳方案较仅细胞模型在测试集F1提升约7.99。


<details>
  <summary>Details</summary>
Motivation: 病理医生诊断依赖在不同放大倍率间切换以理解细胞与组织之间的关联，而现有深度学习细胞检测模型难以学习跨尺度、跨结构的相互语义。关键瓶颈是缺乏同时包含多尺度、重叠的细胞与组织标注数据集，因此发起挑战以检验“细胞-组织交互”对达到类人水平的重要性并推动研究。

Method: 构建包含六个器官、从TCGA H&E切片中提取的673对细胞检测与组织分割重叠标注的数据集，划分为训练/验证/测试；组织社区竞赛，比较参赛者采用的多尺度与细胞-组织关系建模策略，与不利用组织信息的细胞基线模型进行对比评测（指标如F1）。

Result: 多种参赛方法通过融合细胞与组织语义、建模跨尺度关系，测试集相对基线最高提升F1约7.99，显著优于传统仅细胞检测方法，证明引入多尺度语义与细胞-组织交互的价值。

Conclusion: 验证了细胞-组织交互与多尺度信息对细胞检测至关重要；公开的OCELOT 2023数据与对比分析为后续方法提供了基准与方向，鼓励在细胞检测中系统性整合组织级语义与多尺度学习。

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [30] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: 提出RT-DETR++，在RT-DETR的编码器中引入通道门控注意力的上/下采样与CSP-PAC特征融合，以更好处理无人机图像中密集小目标与尺度变化，在不增加复杂度的前提下实现更高精度和实时性。


<details>
  <summary>Details</summary>
Motivation: UAV图像目标检测常见小目标密集、尺度变化大与遮挡，现有实时检测模型在特征传递与多尺度融合上易丢失细节或引入误差，难以兼顾精度与速度。

Method: 改进RT-DETR编码器的“neck”：1) 设计通道门控注意力的上采样/下采样(AU/AD)双路径机制，减少跨层传播误差并保留细节；2) 在特征融合中引入CSP-PAC，利用并行空洞卷积同时建模局部与上下文信息，促进多尺度特征整合；整体保持实时推理速度与计算复杂度。

Result: 在UAV检测任务上，改进的neck在小目标与密集场景检测上优于基线，保持实时速度；报告称无额外计算复杂度增量。

Conclusion: 通过改进特征编码与融合（AU/AD与CSP-PAC），RT-DETR++在不牺牲速度的情况下提升了密集小目标检测性能，为实时检测系统的neck设计提供有效思路。

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [31] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: 提出一个免训练、聚焦知识的KB-VQA框架，通过精炼查询、知识片段抽取与选择性集成，降低冗余与噪声，提升答案准确性，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有KB-VQA方法将检索到的大量外部知识直接拼接给模型，忽视其中的大量冗余与不相关内容，容易引入噪声，削弱模型判断与推理，影响回答准确度。需要一种在保留关键信息的同时抑制冗余的方法。

Method: 1) 低噪声知识检索：从图像-问题对中归纳出要点，形成更精炼的检索查询，以获取高相关知识；2) 知识聚焦：利用大模型对已检索知识进行再加工，识别并抽取对回答有益的片段，减少冗余；3) 选择性知识集成：当模型对答案置信度不足时才引入外部知识，以降低无关知识对决策的干扰；整体为免训练框架。

Result: 在多个KB-VQA基准上进行广泛实验，所提框架在准确率等指标上超过当前SOTA方法，显示其在降低噪声与冗余方面的有效性。

Conclusion: 通过聚焦与选择性使用外部知识，免训练框架能够获得更准确、关键信息，显著缓解冗余带来的噪声影响，从而提升KB-VQA性能。

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [32] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

TL;DR: 提出CWSSNet，将3D光谱-空间特征与小波卷积融合，用于ZY1F高光谱图像地物分类，在江西余干县取得较高mIoU/mAcc/mF1和类间IoU，且小样本下性能稳健、训练时间增加有限。


<details>
  <summary>Details</summary>
Motivation: 高光谱影像带来丰富光谱信息有助于精细分类，但存在高维、波段多与光谱混合导致的特征冗余与性能瓶颈。需要一种能有效利用光谱-空间信息、减少冗余并提高分类鲁棒性的框架。

Method: 以ZY1F卫星高光谱数据为源，针对研究区（江西省上饶市余干县）提出CWSSNet：将3D光谱-空间特征提取与小波域多波段分解和卷积相结合；通过多尺度卷积注意力模块整合多模态信息；在小波域进行多尺度分解+卷积以缓解冗余并提升判别性。

Result: 在余干县实验中，CWSSNet取得mIoU 74.50%、mAcc 82.73%、mF1 84.94；在水体、植被、裸地等类别上获得最高IoU；当训练集比例为70%时，训练时间增加有限且效果接近最优。

Conclusion: CWSSNet通过引入小波域多尺度分解与3D光谱-空间特征融合，突破传统方法分类瓶颈，表现出较强鲁棒性与小样本可用性，适用于精细地物分类场景。

Abstract: Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [33] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: 提出RRDataset，一个用于评估AI生成图像检测在真实世界条件下鲁棒性的基准，覆盖场景泛化、网络传输与再数字化三方面；在其上对17个检测器与10个VLM做基准，并开展大规模人类实验，结果显示现有方法在真实条件下明显受限，提示应借鉴人类适应性以改进算法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测常在理想化或单一场景数据上评测，难以反映社交平台传播、再拍再扫等真实世界扰动；缺乏覆盖关键社会语境与多重退化因素的系统评估基准。

Method: 构建Real-World Robustness Dataset（RRDataset）：1) 收集七大社会场景的高质量图像以测试场景泛化；2) 通过跨多社交平台多轮转发，考察网络传输导致的压缩与处理对检测器的影响；3) 设计四种再数字化流程（如打印-再拍、屏摄等）评估鲁棒性。随后对17个检测器与10个VLM进行基准测试，并组织192名参与者的人类少样本学习实验。

Result: 在RRDataset上，多数现有检测器与VLM在多平台传输与再数字化条件下性能显著下降，且在跨场景泛化方面存在明显短板；人类在少样本提示后展示出较强适应性与稳健性。

Conclusion: 当前检测方法对真实世界扰动不鲁棒；应结合人类适应性启示，面向多场景、多退化因素设计更稳健的检测策略与训练评估流程，RRDataset为此提供了标准化测试基准。

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [34] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: 提出Dark-ISP，一个轻量、可自适应的可微ISP插件，直接在低光环境下处理Bayer RAW并端到端联合目标检测训练，利用线性/非线性模块分解与自增强机制，在三套RAW数据上以更少参数超越现有RGB与RAW方法。


<details>
  <summary>Details</summary>
Motivation: 低光条件下目标检测因噪声大、动态范围受限与传统ISP失真而困难。现有方法要么使用RAW→RGB过程中信息丢失的管线，要么设计繁复的网络，难以部署与端到端优化。需要一种既轻量又能与检测任务联合优化、充分利用RAW信息的ISP方案。

Method: 将传统ISP解构为顺序级联的两类可微子模块：线性（传感器标定相关，如白平衡/增益/去马赛克前的线性操作）与非线性（色调映射等），并通过任务驱动损失（检测目标）端到端训练。每个模块引入内容自适应机制与物理先验，自动完成与检测目标对齐的RAW→RGB转换。利用ISP天然的级联结构提出Self-Boost机制，促进子模块间协同优化，从而在轻量参数下提升鲁棒性与性能。

Result: 在三个RAW图像数据集上进行广泛实验，Dark-ISP在低光目标检测上优于最先进的基于RGB与基于RAW的方法，同时使用的参数量更少，表现出色。

Conclusion: 通过将ISP模块可微化并与检测任务联合学习，且引入自适应与物理先验，以及自增强的级联协同，Dark-ISP在低光检测中实现了性能与效率的兼得，证明了面向任务的RAW域ISP设计的有效性。

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [35] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

TL;DR: VQualA 2025 挑战总结：面向多图像视觉质量比较的开放域推理评测，为指令调优多模态大模型（LMMs）提供系统基准与协议；通过2AFC与MCQ等方式评估细粒度质量判断能力，吸引约百名参赛者，五个模型表现突出，推动可解释、与人类一致的质量评估研究。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在视觉质量评估上的能力尚未系统检验，尤其是跨多图像、开放式、细粒度的质量比较与解释。缺乏统一基准与全面协议来衡量模型在人类一致性与可解释性方面的表现。

Method: 构建包含数千个从粗到细的视觉质量比较任务的数据集，覆盖单图、成对与多图组场景；设计整体化评测协议，包括2AFC二选偏好与多项选择题（MCQ）；组织挑战赛收集广泛参赛模型，重点评估指令调优LMM在质量判断与推理上的能力。

Result: 约100名参与者提交，五个模型在任务上展示出新兴能力，能够进行较准确的质量判断与比较；证明指令调优LMM在视觉质量评估中的潜力。

Conclusion: 该挑战为开放域视觉质量推理建立了新基准与评测流程，推动可解释、与人类一致的质量评估系统研究，为后续方法改进与标准化比较提供了平台。

Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [36] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

TL;DR: MGTraj提出多粒度（从粗到细）目标引导的人体轨迹预测框架，用递归Transformer在各粒度逐步细化并共享权重，辅以速度预测，实验在ETH/UCY与SDD上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有目标引导轨迹预测多为“两阶段”：先预测终点（粗粒度），再逐帧补全全轨迹（细粒度），忽略了中间时间粒度的信息利用，导致不确定性大、泛化与效率受限。已有多尺度表示能捕捉不同动态，但难以无缝融入目标引导框架，需一种能在多粒度间有效交互与递进优化的方法。

Method: 提出MGTraj：1) 以多粒度时间分辨率从粗到细生成轨迹proposal；2) 在每个粒度上使用Transformer式递归精炼网络（RRN）编码历史与当前proposal并输出增量修正；3) 跨粒度特征融合采用权重共享以保持一致性与参数效率；4) 以速度预测为辅助任务，提升时序动态建模；5) 递归地由粗到细细化，最终输出高分辨率未来轨迹。

Result: 在ETH/UCY与Stanford Drone Dataset上，相较基线与现有目标引导方法，MGTraj在主流指标（如ADE/FDE）上取得显著提升，报告为SOTA表现。

Conclusion: 多粒度、递归精炼的目标引导框架能更充分利用中间时间粒度信息，降低不确定性并提升预测精度；共享权重与速度辅助任务进一步增强了建模能力与效率。MGTraj验证了在多数据集上的有效性，值得在机器人导航与自动驾驶中推广。

Abstract: Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [37] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: Medverse 提出一个通用的3D医学影像“上下文内学习”（ICL）模型，覆盖分割、变换、增强等多任务与多器官/模态，采用逐尺度自回归与块级跨注意力，兼顾全局解剖理解与高保真全分辨率输出，并在多未见数据集上显著优于现有ICL基线。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像ICL模型存在两大缺陷：无法同时获得高保真预测与全局解剖一致性；缺乏在多任务、多解剖区域、跨机构统一训练的一体化模型，导致ICL潜力未被充分挖掘。

Method: 1) 训练数据：22个数据集，涵盖多器官、多模态、多中心，任务包含分割、图像变换与增强。2) 框架：下一尺度（next-scale）自回归ICL，从粗到细逐级细化，生成一致的全分辨率三维体数据，并具备多尺度解剖感知。3) 模块：提出块级跨注意力（blockwise cross-attention），在保持空间稀疏性以控制计算量的同时，实现上下文与目标输入的长程交互。

Result: 在广泛的外部留出数据集（含未见的临床中心、器官、物种、成像模态）上评测，Medverse在各项任务上显著超过现有ICL基线，展现更好的泛化与一致性。

Conclusion: Medverse建立了一个覆盖多任务与多模态的通用3D医学影像ICL范式，通过逐尺度自回归与块级跨注意力实现高保真且具全局解剖理解的预测，并在广泛外部测试中验证有效；代码与模型将公开。

Abstract: In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [38] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 提出CoAtNeXt混合架构，用ConvNeXtV2替换CoAtNet的MBConv并加入CBAM注意力，在两大公开胃组织病理数据集上显著优于10个CNN与10个ViT基线，二分类与多分类均达SOTA级性能。


<details>
  <summary>Details</summary>
Motivation: 传统病理诊断全靠人工镜检，工作量大、主观性强、易漏检，且缺乏标准化流程，迫切需要稳定、自动、高效的组织图像分类方法以提高早期胃病诊断的准确性与一致性。

Method: 在CoAtNet框架上进行结构改造：将MBConv替换为增强的ConvNeXtV2块以加强卷积特征表征；融合CBAM实现通道与空间注意力以改善局部特征提取；通过模型尺度调整在算力与性能间平衡。用HMU-GC-HE-30K（8类）与GasHisSDB（二分类）评估，并与10个CNN与10个ViT比较。

Result: 在HMU-GC-HE-30K上：Acc 96.47%、Prec 96.60%、Recall 96.47%、F1 96.45%、AUC 99.89%；在GasHisSDB上：Acc 98.29%、Prec 98.07%、Recall 98.41%、F1 98.23%、AUC 99.90%。整体优于所有对比CNN与ViT，超越既有文献。

Conclusion: CoAtNeXt对胃组织病理图像的二分类与多分类均表现稳健，具有辅助病理医生提升诊断准确性并降低工作量的潜力。

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [39] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

TL;DR: 提出MMOral口腔全景片多模态指令数据集与评测基准，并在其上微调得到OralGPT；现有LVLM在口腔放射学任务表现不足，专门数据与SFT可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 通用LVLM虽在医学任务上有进展，但在牙科尤其是全景X片解读方面缺乏专门数据与评测，现有基准无法覆盖该成像的复杂解剖与细微病灶特征，导致模型难以泛化与评估。

Method: 1) 构建MMOral：包含20,563幅全景X片与约130万条指令跟随实例，覆盖属性抽取、报告生成、VQA、图像对话等任务；2) 设计MMOral-Bench：围绕牙科5个关键诊断维度的综合评测套件；3) 系统评测64个LVLM；4) 基于Qwen2.5-VL-7B进行单轮SFT，得到OralGPT。

Result: 在MMOral-Bench上，即使最强的GPT-4o准确率仅41.45%，显示该领域难度大与现有模型局限；使用MMOral对Qwen2.5-VL-7B进行一次epoch的SFT后，OralGPT性能显著提升，报告提升幅度为24.73%。

Conclusion: 牙科全景片解读需要专门数据与基准；MMOral与MMOral-Bench填补空白，并表明针对性SFT能显著提升LVLM在牙科任务的表现，为智能牙科和临床更具影响力的多模态AI奠定基础。

Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [40] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: 提出DATE框架，通过时间戳注入与语义引导的相似度采样，显式建模长视频中的绝对时间与关键事件，显著提升MLLM在小时级视频上的时间理解与事件定位，7B模型部分超过72B。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM长视频理解普遍用均匀采样与隐式位置编码，难以处理长程依赖，导致时间顺序不清与关键信息丢失，尤其在绝对时间推理与事件定位上表现不佳。

Method: 提出Dynamic Absolute Time Enhancement (DATE)，含两部分：1) Timestamp Injection Mechanism (TIM)：在视频帧嵌入间插入文本化时间戳令牌，构建连续的时间参照系，显式编码绝对时间。2) Temporal-Aware Similarity Sampling (TASS)：将采样视作视觉-语言检索的两阶段过程：先将查询扩写为描述性caption以增强与视觉特征对齐，再用相似度驱动且具时间正则的贪心策略，兼顾语义相关性与时间覆盖，选取关键事件帧/片段。

Result: 在小时级长视频基准上，7B与72B规模模型均取得SOTA的绝对时间理解与关键事件定位性能；其中7B模型在若干基准上超越多款72B模型。

Conclusion: 显式的时间戳注入与语义引导的时序感知采样能有效缓解长程依赖问题，提升MLLM对绝对时间与事件的精确建模，带来显著性能提升并具参数效率优势。

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [41] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: 提出PSP-Seg：通过逐步剪枝实现动态高效的3D医学图像分割，在保持nnU-Net相当精度下显著降低显存、训练时间与参数量。


<details>
  <summary>Details</summary>
Motivation: 3D医学图像分割资源与时间开销巨大，现有效模型多为静态、人工设计，难以在不同任务间自适应与在性能-效率间权衡。

Method: 从冗余大模型出发，结合块级（block-wise）剪枝与功能解耦损失（functional decoupling loss），迭代识别并移除冗余模块，形成渐进式（progressive）剪枝框架PSP-Seg；同时提供轻量变体PSP-Seg-S。

Result: 在5个公共数据集上，与7个SOTA和6个高效分割模型对比，PSP-Seg-S在精度上与nnU-Net相当，同时显存降低42-45%、训练时间减少29-48%、参数量减少83-87%。

Conclusion: PSP-Seg实现了在保持高性能的同时显著提升资源效率，具备在临床环境中广泛部署的潜力。

Abstract: 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [42] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: 提出一种用于图表理解的自适应推理框架“Code-as-Thought (CaT)”，让VLM在可验证代码推理与直接视觉推理之间动态选择，并用双重奖励的强化学习训练选择策略，显著提升多基准鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解方法要么依赖外部工具导致脆弱受限，要么微调只会单一路径（如文本CoT），其中文本中间步骤难以验证，限制了基于事实准确性的强化学习信号使用。因此需要一种既可验证又能适应不同题目的推理表示与选择机制。

Method: 提出“视觉可编程性”概念：判断给定图表-问题对更适合用代码（符号表示）还是直接视觉推理。实现为一个自适应两路径框架：1) CaT路径将图表视觉信息转为可验证的符号/代码格式进行程序化计算；2) 直接视觉路径进行端到端视觉推理。通过强化学习训练一个选择策略，采用双重奖励：数据准确性奖励（抑制数值幻觉、对齐事实）+ 决策奖励（鼓励正确选择路径，避免单一模式）。

Result: 在多种图表理解基准上取得强健且优异的表现，显示出相较既有方法的稳健性与泛化能力。固定“只用代码”的实现对复杂图表会失败，而自适应选择可显著缓解。

Conclusion: VLM不仅能学会推理，还能学会“如何推理”。通过学习何时使用可验证代码推理与何时进行直接视觉推理的动态选择，可提升图表理解的准确性与鲁棒性。

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [43] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 提出在U-Net中并行加入“模态无关”输入通道与“模态特定”通道，并用合成MRI模态的增强来训练，使模型既保留已见模态性能，又能在推理时利用未见或任意组合模态进行脑病灶分割。


<details>
  <summary>Details</summary>
Motivation: 多模态脑MRI分割常受限于固定模态配置；现实临床中模态种类与可用性不一，且可能出现训练中未见的模态。现有能泛化到未见模态的方法往往牺牲模态特异信息，导致分割性能下降。需要一种既能利用已见模态的判别性，又能处理未见模态与任意组合的通用模型。

Method: 在U-Net架构中加入一条模态无关输入通道/路径，与模态特定通道并行。为训练模态无关分支，设计图像增强策略，合成“人工MRI模态”：对病灶与健康组织施加差异化外观变换，制造逼真的跨组织对比同时保持解剖一致性。训练后，推理阶段可接受训练见过的模态、未见模态或其异质组合。

Result: 在8个MRI数据库、涵盖5类脑病变（卒中、肿瘤、TBI、多发性硬化、白质高信号）与8种模态（T1、T1C、T2、PD、SWI、DWI、ADC、FLAIR）上评估。方法在已见模态上保持有效分割性能，同时能处理未见模态并提升分割效果，相比仅限固定模态或牺牲模态特异性的方案更稳健。

Conclusion: 通过对U-Net进行简单且实用的结构改动，并配合针对病灶/健康差异的合成模态增强，可实现对任意可用MRI模态的鲁棒脑病灶分割：既不放弃已见模态的判别力，又能泛化到未见模态与任意模态组合。

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [44] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出一个面向嵌入式（embedding-based）目标检测器的模型无关开集检测框架，通过在嵌入空间熵建模、谱归一化与温度缩放，提高未知目标拒识与抗腐蚀鲁棒性，在AOT航拍基准与实飞实验中优于YOLO基线，AUROC相对提升最高约10%，且背景拒绝不损精度。


<details>
  <summary>Details</summary>
Motivation: 闭集检测器在域移与飞行数据腐蚀下显著退化，影响空对空无人机感知的安全与可靠性，迫切需要可识别/拒绝未知目标并具鲁棒性的开集检测方法。

Method: 面向嵌入式检测器，构建模型无关的开集检测框架：在嵌入空间进行语义不确定性熵建模；引入谱归一化稳定特征并抑制过度自信；采用温度缩放增强开集可分性；并加入背景拒绝策略，整体在不改动主干架构的前提下实现未知目标拒识与抗腐蚀鲁棒性。

Result: 在AOT航拍基准与大量实飞测试中，较YOLO等基线持续提升开集性能；消融实验证明各组件有效，AUROC相对提升最高约10%；背景拒绝进一步提升鲁棒性且不降低检测精度。

Conclusion: 该框架能在动态空对空环境中实现可靠的开集检测与鲁棒感知，适用于安全关键的UAV自主系统。

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [45] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

TL;DR: 提出SlotSAR：无掩码标注下，将SAR图像中的目标表征从复杂杂波中解缠的对象中心学习框架，融合高/低层特征并用多级slot注意力提升目标分离与识别，达SOTA且保留结构细节。


<details>
  <summary>Details</summary>
Motivation: SAR图像含复杂地物回波与斑点噪声，杂波常与目标在强度与纹理上相似，导致模型学到纠缠或伪特征，影响任何分类器的判别能力，因此需要一种能在无掩码的情况下将目标与背景表征解耦的学习方法。

Method: 1) 特征提取：从SARATR-X获得高层语义特征；从小波散射网络获得低层散射/纹理特征，形成互补多层表示。2) 多级slot注意力：将高低层特征融合到slot表示中，增强各slot的区分性，实现对象中心学习与目标-背景解缠；无需掩码监督。

Result: 在SAR数据上实现SOTA，相比现有OCL方法更好地保留结构细节，目标表征更清晰、鲁棒。

Conclusion: 融合语义与物理散射特征并通过多级slot注意力的OCL可有效从SAR杂波中解耦出目标表示，提升识别性能与结构保真；方法在无掩码标注下也能取得领先效果。

Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [46] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: 提出MatCha——首个面向材料表征图像理解的基准，含1500道高难度问题；评测显示主流多模态大模型在高层次专业与复杂视觉任务上显著落后于人类，常规提示工程难以弥补。


<details>
  <summary>Details</summary>
Motivation: 材料科学中的表征图像是连接加工—组织—性能关系的关键，但现有多模态大模型在真实表征图像理解上的能力尚不明确，缺乏系统评测基准来反映实际科研挑战与需求。

Method: 构建MatCha基准：覆盖材料研究四个关键阶段、21类真实任务，包含1500个需要专家知识的问题；用多种最先进的多模态大模型进行系统评测，并尝试few-shot与Chain-of-Thought提示以观察性能变化。

Result: 与人类专家相比，当前MLLM在MatCha上存在显著性能差距；在需要更高专业知识和更复杂视觉感知的问题上性能下降明显；few-shot与CoT对提升效果有限。

Conclusion: 现有MLLM对真实材料表征场景的适应性不足；MatCha为社区提供了标准化评测平台，期待推动在新材料发现与自主科研智能体等方向的后续研究与改进。

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [47] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

TL;DR: 提出PHCP框架，在无监督、少样本的推理阶段自训练适配器以对齐异构车辆间特征，实现协同感知而无需联合训练，性能接近需全数据训练的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实车联网中不同厂商与模型导致协同感知存在严重的域差异；现有方法需在推理前进行联合微调或预存多模型，工程上难以扩展与维护，亟需在不依赖标注和联合训练的前提下直接在推理时解决异构域对齐。

Method: 将异构协同感知建模为少样本无监督域自适应问题：在推理阶段对来自协作方的特征通过自训练的适配器进行动态对齐。核心是在线/渐进式（progressive）自训练，使用少量无标注数据与伪标签、特征一致性或对齐损失，使适配器在运行中逐步校准不同模型输出的表示，无需访问源域标注或进行离线联合训练。

Result: 在OPV2V数据集多种异构场景下取得强性能，用少量无标注数据即可达到与全数据训练的SOTA相当的效果，验证了推理期自适应的有效性与泛化性。

Conclusion: PHCP证明了在协同感知中可于推理阶段进行渐进式无监督对齐，避免联合训练的成本与部署复杂度，同时在异构环境下维持接近SOTA的性能，具备实际落地潜力。

Abstract: Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [48] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

TL;DR: 评估多种双编码VLM在ImageNet上的语言引导与纯视觉分类，揭示二者互补；提出基于按类精度的零学习融合策略，提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM以图文对齐支持零样本分类，但其“脱离文本”的纯视觉推理能力与与语言提示的互补性尚未系统评估；同时实践中受提示工程、类多样性、kNN参数与参考集规模等因素影响的性能缺乏统一分析。

Method: 在ImageNet-1k及其更正标签版本上，系统对比一系列双编码VLM（含SigLIP2、RADIOv2.5等）的两种推理范式：1）语言引导（文本提示+图文相似度）；2）视觉仅用（基于视觉嵌入的kNN/相似度检索）。逐项考察提示设计、类别多样性、k邻居数、参考集规模等因素；据此提出不需训练、按类别精度的融合：对每个类别依据其在验证/参考集上的精度权重，在文本与视觉分数间加权。

Result: 发现不同类别在文本提示与视觉相似度上表现偏好不同，二者具有显著互补性；通过按类精度的简单融合，在标准设置下较单一路径获得更高的分类准确率（在ImageNet-1k及校正标签集上均有提升）。

Conclusion: VLM的语言与视觉路径各擅所长，合并可超越单一路径；无需训练的基于按类精度的融合方法简单有效，提供了在零样本/近零样本场景下改进VLM图像识别的实用范式。

Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>


### [49] [Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM](https://arxiv.org/abs/2509.09324)
*Hui Li,Yi You,Qiqi Chen,Bingfeng Zhang,George Q. Huang*

Main category: cs.CV

TL;DR: 提出BUG工作流：利用大型多模态模型，将“聊天+图像入提示”用于自动生成并细粒度定制服装设计，并发布FashionEdit数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI虽能快速产出时装概念图，但普通用户缺乏专业术语与细节描述能力，导致文本不确定性，难以实现细粒度定制与可控编辑。

Method: 提出Better Understanding Generation（BUG）工作流：结合大型多模态模型（LMM），通过“image-into-prompt”将图像线索融入对话式提示，自动完成从创意到精细化定制的服装设计与编辑，全流程尽量减少人工参与。并构建FashionEdit数据集，模拟真实服装设计流程，用于训练与评估。

Result: 在FashionEdit上进行评测，从生成相似度、用户满意度与图像质量三方面展示优于现有方法的表现（摘要未给出具体指标数值）。

Conclusion: BUG工作流能降低服装设计门槛，提升细粒度定制的可控性与用户创造力；发布数据集与代码，促进后续研究与应用。

Abstract: Generative AI evolves the execution of complex workflows in industry, where
the large multimodal model empowers fashion design in the garment industry.
Current generation AI models magically transform brainstorming into fancy
designs easily, but the fine-grained customization still suffers from text
uncertainty without professional background knowledge from end-users. Thus, we
propose the Better Understanding Generation (BUG) workflow with LMM to
automatically create and fine-grain customize the cloth designs from chat with
image-into-prompt. Our framework unleashes users' creative potential beyond
words and also lowers the barriers of clothing design/editing without further
human involvement. To prove the effectiveness of our model, we propose a new
FashionEdit dataset that simulates the real-world clothing design workflow,
evaluated from generation similarity, user satisfaction, and quality. The code
and dataset: https://github.com/detectiveli/FashionEdit.

</details>


### [50] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 研究将手术技能评估（SSA）表述为小样本学习任务，系统比较自监督预训练来源与域相似度对下游SSA性能的影响；结果表明小而域相关的数据优于大而不匹配的数据，且加入手术过程特定数据可进一步提升表现。


<details>
  <summary>Details</summary>
Motivation: SSA标注稀缺且昂贵，需要专家一致性，限制了强健模型的训练。小样本学习可减轻标注依赖，但高度依赖有效预训练。尽管预训练在其它手术视觉任务中常见，其在SSA上的系统性研究不足，因此需要量化不同预训练来源、域差异与过程特定数据对迁移的影响。

Method: 将SSA设定为FSL任务，在公开机器人手术数据上新增OSATS评分作为监督；采用自监督预训练，跨三种few-shot设定（1/2/5-shot）评估多种预训练来源；通过度量域相似度与域间差距，分析预训练数据规模、相似性与是否包含手术过程特定数据对下游迁移的作用。

Result: 小而域相关的预训练数据优于大规模但不匹配的数据；在1/2/5-shot下分别取得60.16%、66.03%、73.65%准确率。将过程特定数据与域相关外部数据联合预训练，平均准确率提升+1.22%，F1提升+2.28%；而与不相似的大规模来源结合会导致性能下降。

Conclusion: 在SSA的FSL场景中，预训练的域相关性比数据规模更关键；适度地将过程特定数据融入与任务相近的预训练集合能稳定增益，而依赖不相似的大规模来源可能适得其反。

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [51] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: 提出一种基于外部观测的驾驶行为分类系统，利用计算机视觉（YOLO检测、目标跟踪、横向位移与车道位置监测）识别分心与受损驾驶的迹象，并在多样视频数据上验证其可靠性与适应性。


<details>
  <summary>Details</summary>
Motivation: 交通事故中人为失误（尤其是分心、受损驾驶）是主要原因；现有依赖车联网的方案对非联网车辆覆盖不足，亟需一种仅凭视觉即可对道路上其他车辆进行行为评估的系统。

Method: 构建外部相机视角的行为分类框架：1) 使用YOLO进行车辆检测；2) 进行实时多目标跟踪；3) 自定义车道估计算法与车道位置监测；4) 计算车辆横向位移与轨迹波动特征；5) 将过度横向移动、轨迹不稳等作为不安全行为指示。

Result: 在多样道路与环境条件的视频数据集上进行了实验，系统可稳定检测不安全驾驶模式，表现出良好的可靠性与跨场景适应性。

Conclusion: 视觉驱动、无需车联网的信息即可对非联网车辆进行分心/受损驾驶迹象的识别，框架在不同环境下具备鲁棒性与推广潜力。

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [52] [Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors](https://arxiv.org/abs/2509.09352)
*Xiaodong Wang,Zijun He,Xin Yuan*

Main category: cs.CV

TL;DR: 提出一种针对复杂场景（空间变化光照、丰富纹理）的单张图像内在图像分解新方法，通过纹理引导正则与优化框架，分离反照率与阴影，缓解以往方法过度平滑、丢纹理的问题，获得更高质量结果。


<details>
  <summary>Details</summary>
Motivation: 内在图像分解在复杂真实场景中仍困难：光照空间变化强、材质纹理丰富，现有学习法常产生过度平滑、缺纹理的反照率与阴影，影响下游应用。作者希望在保持结构一致性的同时保留纹理并正确建模强光照。

Method: 观察到学习法的结果往往纹理缺失，可由此反推图像中的纹理与光照线索。据此设计“纹理引导”的正则项，将分解表述为一个优化问题：利用从RGB图像推断的纹理/光照信息作为先验，在优化中约束反照率与阴影的分离，使材质纹理与光照效应更可区分。

Result: 在真实复杂场景上，结合纹理感知先验后，分解质量优于现有方法：反照率保留纹理、阴影分量更准确，视觉与（推测）定量指标更佳。

Conclusion: 纹理引导的先验并入优化框架，可有效缓解复杂光照与丰富纹理下的过度平滑问题，提升单图内在分解效果，对真实世界图像更稳健，优于当前方法。

Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer
given a single image. Though this intrinsic image decomposition problem has
been studied for decades, it remains a significant challenge in cases of
complex scenes, i.e. spatially-varying lighting effect and rich textures. In
this paper, we propose a novel method for handling severe lighting and rich
textures in intrinsic image decomposition, which enables to produce
high-quality intrinsic images for real-world images. Specifically, we observe
that previous learning-based methods tend to produce texture-less and
over-smoothing intrinsic images, which can be used to infer the lighting and
texture information given a RGB image. In this way, we design a texture-guided
regularization term and formulate the decomposition problem into an
optimization framework, to separate the material textures and lighting effect.
We demonstrate that combining the novel texture-aware prior can produce
superior results to existing approaches.

</details>


### [53] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: 提出将PnP与DDIM统一起来解决逆问题（单像素成像），把扩散流程分为去噪、数据一致性、采样三段并提出线性组合多种PnP保真项的混合数据一致性模块，直接修正去噪估计而不干扰采样轨迹，实验显示重建更优。


<details>
  <summary>Details</summary>
Motivation: 逆问题（如单像素成像）需要结合物理前向模型与强先验。PnP与扩散模型各有优势但机制不同，缺乏统一视角与能兼顾数据一致性的有效策略。现有方法在保证测量一致性与保持扩散采样稳定性之间存在权衡。

Method: 将扩散过程解构为三阶段：1) 去噪（学习先验）；2) 数据一致性（基于前向模型的校正）；3) 采样（推进时间步）。在此框架下设计混合数据一致性模块，以线性组合多种PnP风格的保真项，对去噪后的估计施加校正，同时不改变扩散采样轨迹。

Result: 在单像素成像实验中，该方法比对比基线取得更好的重建质量（未给具体指标，但表明在视觉和/或定量评分上提升）。

Conclusion: 通过统一PnP与DDIM并引入混合数据一致性校正，可在不扰动扩散采样的前提下增强测量一致性，从而提升逆问题（单像素成像）的重建性能。

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.

</details>


### [54] [A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data](https://arxiv.org/abs/2509.09368)
*Pengxu Wen,Tingting Yu,Ziwei Nie,Cheng Jiang,Zhenyu Yin,Mingyang He,Bo Liao,Xiaoping Yang*

Main category: cs.CV

TL;DR: 提出一个全自动两阶段框架，用眼底超声视频的关键帧识别与ONSD测量并结合临床特征，进行非侵入性ICP分级，显著优于传统阈值法。


<details>
  <summary>Details</summary>
Motivation: ICP升高危及脑功能，但金标准腰穿侵入性高，临床用ONSD替代受手工测量不一致、视图选择主观、阈值不一等限制，需要客观、稳定、可解释的自动化方法。

Method: 两阶段：1）超声视频处理阶段：逐帧解剖分割，依据国际共识的规则进行关键帧识别，精确测量ONSD；2）ICP分级阶段：将ONSD指标与临床特征融合，构建多源数据模型预测ICP分级。

Result: 在五折交叉验证中达到0.845±0.071准确率，独立测试集准确率0.786，显著优于传统基于阈值的方法（验证0.637±0.111，测试0.429）。

Conclusion: 该框架在减少操作员变异、融合多源信息方面有效，实现可靠的非侵入性ICP评估，具备在急性神经疾病管理中的临床应用潜力。

Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.

</details>


### [55] [Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality](https://arxiv.org/abs/2509.09375)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: 提出一种无需外部正常样本的无监督IC缺陷分割方法：从同一测试图像提取可学习的正常信息，约束其与正常区域一致，用只重建正常内容的解码器，利用残差分割缺陷，并用伪异常增强稳定训练；在三道工艺数据上优于现有方法且对产品变化鲁棒。


<details>
  <summary>Details</summary>
Motivation: IC制造中缺陷细粒度多样，传统需与外部正常集比对，受版图差异与对齐困难影响而脆弱；而缺陷多为局部，同一图像内存在可重复的正常模式，启发在单图内自给自足地建模“正常”，避免跨产品依赖。

Method: 1) 设计可学习的正常信息提取器，从测试图像内部聚合代表性的正常特征；2) 引入一致性/连贯性损失(coherence loss)，促使所提特征与正常区域绑定；3) 以这些特征为引导的解码器仅重建正常内容；4) 用重建残差作为缺陷分割图；5) 通过伪异常增强(pseudo-anomaly augmentation)提高训练稳定性与泛化。

Result: 在来自三种IC工艺阶段的数据集上，方法在分割性能上持续优于现有方法，并表现出对产品版图变更的强鲁棒性。

Conclusion: 基于单图内部正常模式建模的无监督缺陷分割可替代对外部正常库与精确配准的依赖；通过提取并约束正常特征、只重建正常内容与残差检测，能稳定、鲁棒地定位多样微小缺陷。

Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.

</details>


### [56] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: 提出DRiFt框架，通过结构化特征解耦与跨模态对齐提升医疗VLM在分布移位下的可靠性与泛化；在分布内显著提升精度（Top-1 +11.4%、Macro-F1 +3.3%），并在未见数据集保持稳健。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在真实世界存在分布移位（成像协议差异、报告自由文本噪声），易学习与任务无关的相关性，导致不可靠与泛化差；需要方法显式剥离临床相关信号与任务无关噪声，提升安全可用性。

Method: 提出DRiFt：1) 结构化特征解耦，利用参数高效微调（LoRA）与可学习提示token，将临床相关特征与任务无关噪声分离；2) 跨模态对齐与不确定性降低：为多样化医疗数据集生成高质量、临床扎实的图像-文本描述以强化对齐；3) 通过消融研究验证解耦与对齐组件的作用。

Result: 相较以往基于prompt的方法，分布内性能提升Top-1准确率+11.4%、Macro-F1 +3.3%；在未见数据集保持强鲁棒性；不确定性降低。

Conclusion: 显式解耦任务相关与无关特征并进行精心跨模态对齐，能显著提升医疗VLM的泛化、稳健性与安全性；为临床可托付的VLM提供方向与实践，代码已开源。

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.

</details>


### [57] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: 提出FS-Diff：一种以扩散/条件生成视角统一图像融合与超分辨，结合语义引导与清晰度感知机制，并在多数据集（含自建AVMS）上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实多模态（如可见光/红外）场景中，远距离与侦察任务常受低分辨率、弱语义、目标/背景结构破坏影响，导致现有融合与超分辨方法效果欠佳。需要一种能同时增强清晰度、恢复细节与语义的联合方法。

Method: 将图像融合+超分辨统一为条件生成问题：以纯高斯噪声初始化目标融合结果；引入双向Feature Mamba提取多模态全局特征；设计清晰度感知的语义引导机制以自适应低分辨感知与跨模态特征抽取；以源图像与语义为条件，使用改进U-Net进行随机迭代去噪训练（多噪声级），生成高分辨、语义丰富的融合结果。

Result: 在六个公开数据集与自建AVMS（600对航拍多场景）上进行联合融合与超分实验，在多倍率下超过现有SOTA，生成的融合图像具有更丰富细节与语义。

Conclusion: FS-Diff验证了以扩散式条件生成框架联合完成融合与超分的有效性；语义引导与清晰度感知机制提升跨模态细节与结构恢复能力，并在多数据集上稳定优于现有方法。

Abstract: As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.

</details>


### [58] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 论文提出一种用于稠密自监督学习（Dense SSL）的显式语义聚合方法，结合“对应关系蒸馏+连续AP排序损失+基于对象的原型过滤”，缓解补丁表征过度离散，显著提升下游密集任务表现。


<details>
  <summary>Details</summary>
Motivation: 图像级自监督方法能避免特征过度离散，原因在于隐式语义聚合（非严格空间对齐带来的实例内一致性，以及类内共享模式带来的图像间一致性）。但这些机制不适用于稠密SSL：需要严格空间对齐且场景复杂，导致补丁特征在同一实例/类别内发散，影响密集任务（检测、分割等）。因此需要面向稠密场景的显式语义聚合。

Method: 1) 对应关系蒸馏：打破严格空间对齐，利用教师模型产生的补丁对应关系作为伪标签进行蒸馏。2) 噪声容忍排序损失：将AP（Average Precision）损失扩展到连续目标，利用其与决策阈值无关且自适应聚焦的性质，提升对噪声与不均衡伪标签的鲁棒性，避免学生模型被误导。3) 面向对象的过滤：通过跨注意力将补丁投射到对象原型空间，用可学习的对象原型表示补丁，从复杂场景中分离类内共享模式。

Result: 在多种下游密集任务和基准上进行实证，方法在稠密表征质量与任务性能上优于主流基线，验证了显式语义聚合策略的有效性；代码开源（GitHub: KID-7391/CoTAP）。

Conclusion: 显式语义聚合是缓解稠密SSL中过度离散的有效途径。通过对应关系蒸馏、连续AP排序损失与对象原型过滤的组合，可在复杂场景中建立稳健的补丁级一致性与类内共享模式表征，从而系统性提升下游密集任务表现。

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [59] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: 提出FlexiD-Fuse，一种基于扩散模型、可处理任意数量输入模态的医学图像融合方法，在哈佛数据集和多类扩展任务上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医学图像融合方法通常只支持固定数量（如双模/三模）输入，限制了临床灵活应用；需要一种在同一模型权重下能处理可变数量模态的通用融合框架。

Method: 将仅支持固定条件的扩散式融合转化为基于扩散过程与分层贝叶斯建模的极大似然估计问题；在扩散采样迭代中引入EM算法，使模型在采样过程中自适应聚合来自任意数量源图像的跨模态信息；同一网络权重端到端同时支持二模与三模融合，并可扩展至任意模态数。

Result: 在哈佛数据集上与最新双模/三模融合方法比较，基于9个主流指标取得最优；在红外-可见、多曝光、多焦点等任意数量输入的扩展任务上亦优于代表性SOTA方法，生成高质量、信息保真且细节丰富的融合图。

Conclusion: FlexiD-Fuse实现对输入模态数量的灵活适配，统一了不同模态数场景下的融合流程，兼具理论可解释性（MLE+分层贝叶斯+EM）与实证优越性，为临床多模态影像融合与更广泛图像融合任务提供通用高效方案。

Abstract: Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.

</details>


### [60] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: 提出一套适用于资源受限环境的3D注意力UNet（含残差块与迁移学习）用于SSA地区胶质瘤MRI分割，在BraTS-Africa上取得较高Dice并具备小模型体积与快速推理，显示良好可部署性与临床潜力。


<details>
  <summary>Details</summary>
Motivation: SSA地区高质量标注MRI稀缺，限制先进分割模型的临床落地；需要在低算力与小样本条件下仍能泛化、可靠、可部署的分割方案以支持诊疗与随访。

Method: 采用3D Attention U-Net作为主干，加入残差块以稳定深层特征学习；利用在BraTS 2021上预训练的权重进行迁移学习以缓解数据稀缺问题；在BraTS-Africa数据集的95例MRI上评测；强调小模型（约90MB）与消费级硬件下每体积分割时间<1分钟。

Result: 在BraTS-Africa上获得Dice：ET 0.76、NETC 0.80、SNFH 0.85；在数据质量与数量受限条件下仍显示良好性能与泛化。

Conclusion: 所提方法在低资源环境中具备准确、快速、轻量与可部署性，有潜力支持SSA地区的临床决策，推动全球健康领域AI的公平可及性。

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [61] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: 提出一个面向政治语境的深伪检测基准：300万真实图像+描述，用这些描述生成约96.3万高质量合成图像（含专有与开源模型），并配套众包对抗平台持续收集更难的伪造样本，以提升检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有检测数据集受限：多为过时生成方法、低真实感或仅单人脸，难以泛化到当下多模态、政治敏感场景中的高逼真深伪传播。人类感知研究还显示新近专有模型生成的图像越来越难被公众分辨，加剧错误信息风险。

Method: 1) 社媒分析以刻画深伪传播的多模态路径；2) 构建政治领域基准数据集：收集300万真实图像及描述性标题，用这些标题驱动多种生成模型（专有+开源）生成96.3万对应高质量合成图像；3) 设计并上线众包对抗平台，激励参与者持续上传更具挑战性的伪造样本，形成动态更新数据源。

Result: 得到覆盖现代生成技术、规模化且高保真度的数据集，并通过人类感知实验验证：最新专有模型生成图像对公众而言更难区分。同时，众包平台持续带来困难样本，推动检测方法评测与改进。

Conclusion: 面向政治场景的综合数据集与持续对抗收集机制可使深伪检测更贴近现实、具备前瞻性与适应性，从而更有效地防范复杂的错误信息威胁。

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [62] [Improving Human Motion Plausibility with Body Momentum](https://arxiv.org/abs/2509.09496)
*Ha Linh Nguyen,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: 论文提出以全身线动量与角动量作为约束，将局部关节运动与全局位移物理一致地耦合，从而减少脚滑与抖动、改善平衡并保持运动重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统做法将人体运动分解为根节点的全局运动与相对根的局部关节运动并分别建模，但二者实际由与环境的相互作用耦合。现有模型难以精确体现这种物理耦合；而从力/力矩数值积分推导全局轨迹代价高且复杂。

Method: 利用“全身线动量与角动量”作为物理中介，将局部关节动态对全局运动的整体效应编码。提出一个新损失项，要求生成的动量时序与真实动量分布一致，把该损失加入到运动生成/重建模型的训练中。

Result: 加入动量一致性损失后，模型显著减少了脚部滑移与抖动，提升了平衡性，同时保持了原有的运动重建精度。

Conclusion: 用全身动量作为连接局部与全局动态的约束是高效且物理合理的，可在不显著增加计算复杂度的情况下改进运动质感与稳定性；代码与数据已开源。

Abstract: Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating
them separately. However, these two components are not independent. Global
movement arises from interactions with the environment, which are, in turn,
driven by changes in the body configuration. Motion models often fail to
precisely capture this physical coupling between local and global dynamics,
while deriving global trajectories from joint torques and external forces is
computationally expensive and complex. To address these challenges, we propose
using whole-body linear and angular momentum as a constraint to link local
motion with global movement. Since momentum reflects the aggregate effect of
joint-level dynamics on the body's movement through space, it provides a
physically grounded way to relate local joint behavior to global displacement.
Building on this insight, we introduce a new loss term that enforces
consistency between the generated momentum profiles and those observed in
ground-truth data. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion. Code and
data are available at the project page https://hlinhn.github.io/momentum_bmvc.

</details>


### [63] [Region-Wise Correspondence Prediction between Manga Line Art Images](https://arxiv.org/abs/2509.09501)
*Yingxuan Li,Jiafeng Mao,Qianru Qiu,Yusuke Matsui*

Main category: cs.CV

TL;DR: 提出一种无需预先分割/标注的漫画线稿区域对应预测任务与方法：将线稿切成patch，用Transformer学习跨图相似性，再经边缘感知聚类与区域匹配得到区域级对应；并构建自动标注与精修数据集，实验显示高patch准确率与一致的区域对应。


<details>
  <summary>Details</summary>
Motivation: 漫画线稿在自动上色、补帧等应用中需要跨图的区域对应，但现实中缺少分割或标注，现有研究对无标注的区域级对应几乎未涉，促使作者定义并解决这一实用任务。

Method: 1) 将线稿图像划分为patch；2) 采用Transformer学习图内与跨图的patch相似性；3) 使用边缘感知聚类将patch聚合为连贯区域；4) 通过区域匹配算法把patch级预测转化为区域级对应；5) 构建自动标注流水线并人工精修一部分数据，形成训练与评测基准。

Result: 在多个数据集上取得高patch级准确率（如96.34%），并能生成一致、连贯的区域级对应结果。

Conclusion: 所提框架在无预先标签/掩码条件下有效学习漫画线稿的区域对应，具备实际应用潜力，并由新构建的数据集与基准支持评估。

Abstract: Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.

</details>


### [64] [Generative Diffusion Contrastive Network for Multi-View Clustering](https://arxiv.org/abs/2509.09527)
*Jian Zhu,Xin Zou,Xi Wang,Ning Zhang,Bian Wu,Yao Yang,Ying Zhou,Lingfang Zeng,Chang Tang,Cheng Luo*

Main category: cs.CV

TL;DR: 提出一种针对多视角聚类中噪声与缺失问题的生成扩散式融合与对比学习框架（SGDF/GDCN），在多数据视角下实现鲁棒融合并取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 多视角聚类通过融合异构视角可提升聚类效果，但现实数据常含噪声与缺失，导致融合质量下降、聚类受损，需要一种能在低质量数据条件下仍稳健发挥的融合方法。

Method: 1) 设计随机生成式扩散融合（SGDF）：为每个样本的多视角特征引入多重生成机制，以扩散/去噪过程建模与重建各视角，从而抑制噪声、补全缺失；2) 在此基础上提出生成扩散对比网络（GDCN）：结合对比学习，利用生成的多视角表示进行一致性对齐与判别性增强。

Result: 在多组深度多视角聚类基准上，GDCN获得了当前最优的聚类性能（SOTA），验证了对噪声与缺失场景的鲁棒性；代码开源。

Conclusion: 扩散式生成融合与对比学习相结合，能够有效缓解多视角数据的噪声与缺失问题，显著提升深度多视角聚类的性能。

Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.

</details>


### [65] [DualTrack: Sensorless 3D Ultrasound needs Local and Global Context](https://arxiv.org/abs/2509.09530)
*Paul F. R. Wilson,Matteo Ronchetti,Rüdiger Göbl,Viktoria Markova,Sebastian Rosenzweig,Raphael Prevost,Parvin Mousavi,Oliver Zettinig*

Main category: cs.CV

TL;DR: 提出DualTrack双编码网络，解耦提取局部（斑点、细粒度运动）与全局（解剖形状、长程依赖）特征，通过轻量级融合预测探头三维轨迹，在公开基准上达SOTA，3D重建平均误差<5 mm。


<details>
  <summary>Details</summary>
Motivation: 传统3D超声昂贵且复杂，限制普及；无传感器3D方案需从2D序列估计探头轨迹。既要利用局部纹理预测相邻帧运动，也要借助全局解剖上下文保证全局一致性。现有方法要么忽略全局特征，要么与局部耦合过紧，难以稳健建模两者互补性。

Method: 提出DualTrack：双编码器解耦设计。局部编码器采用致密时空卷积，专注细粒度局部特征与帧间运动；全局编码器使用图像主干（2D CNN或基础模型）配合时间注意力，编码高层解剖语义与长程时序依赖；随后用轻量级特征融合模块联合两路表示，回归三维探头轨迹。

Result: 在大型公开基准上实现SOTA精度与全局一致的3D重建，优于以往方法；报告的平均重建误差低于5毫米。

Conclusion: 解耦的局部-全局双轨特征建模能够更稳健地估计无传感器3D超声的探头轨迹，提升重建精度与一致性；DualTrack为低成本3D超声提供有效路线。

Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over
conventional 2D imaging, yet its widespread adoption is limited by the cost and
complexity of traditional 3D systems. Sensorless 3D US, which uses deep
learning to estimate a 3D probe trajectory from a sequence of 2D US images, is
a promising alternative. Local features, such as speckle patterns, can help
predict frame-to-frame motion, while global features, such as coarse shapes and
anatomical structures, can situate the scan relative to anatomy and help
predict its general shape. In prior approaches, global features are either
ignored or tightly coupled with local feature extraction, restricting the
ability to robustly model these two complementary aspects. We propose
DualTrack, a novel dual-encoder architecture that leverages decoupled local and
global encoders specialized for their respective scales of feature extraction.
The local encoder uses dense spatiotemporal convolutions to capture
fine-grained features, while the global encoder utilizes an image backbone
(e.g., a 2D CNN or foundation model) and temporal attention layers to embed
high-level anatomical features and long-range dependencies. A lightweight
fusion module then combines these features to estimate the trajectory.
Experimental results on a large public benchmark show that DualTrack achieves
state-of-the-art accuracy and globally consistent 3D reconstructions,
outperforming previous methods and yielding an average reconstruction error
below 5 mm.

</details>


### [66] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出 Align4Gen：在训练视频扩散模型时，将生成器的中间特征与预训练视觉编码器的特征对齐，并通过多特征融合与对齐策略，提升无条件与类条件视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管视频扩散模型在架构与训练目标上快速演进，但对“特征表示能力”的提升关注不足。作者认为若能让生成器中间表示与强大的预训练视觉编码器表征对齐，或可提升生成稳定性、可辨性与时序一致性。

Method: 1) 提出评估视觉编码器适用于视频对齐的两项指标：判别性与时序一致性；2) 对多种预训练视觉编码器进行系统分析与对比；3) 设计 Align4Gen：在训练过程中引入多层多尺度的特征融合与对齐损失，将视频生成器的中间特征与选定编码器特征对齐，形成可与扩散/流匹配等目标联合优化的训练框架。

Result: 在无条件与类条件视频生成任务上，与基线视频扩散模型相比，多项指标（如视觉质量、可辨性与时序连贯性指标）均有提升；项目页提供视频可视化结果。

Conclusion: 通过选择具有高判别性与高时序一致性的视觉编码器，并在训练中进行多特征对齐，能显著增强视频扩散模型的表示能力与生成质量；Align4Gen 为现有训练范式的可插拔增强模块。

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [67] [InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation](https://arxiv.org/abs/2509.09555)
*Sirui Xu,Dongting Li,Yucheng Zhang,Xiyan Xu,Qi Long,Ziyin Wang,Yunzhi Lu,Shuchang Dong,Hezi Jiang,Akshat Gupta,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: 提出InterAct：一个大规模3D人体-物体交互（HOI）基准，整合并标准化21.81小时数据，优化纠正接触与手部问题，利用接触不变原则增广至30.70小时，定义6项基准任务与统一生成视角，方法在多任务上达SOTA并公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有3D HOI数据集规模小、注释不足、质量低（穿插、悬空、手部动作错误），限制了动态人-物交互生成的发展。需要一个高质量、标准化且可扩展的基准与方法体系推动研究。

Method: 1) 数据层面：整合多源HOI数据至21.81小时，统一标准并丰富文本注释；2) 质量提升：提出统一优化框架，减少接触穿插/漂浮并纠正手部动作；3) 增广：基于接触不变性，在保持人-物关系的前提下引入动作变化，将数据扩展到30.70小时；4) 评测与建模：定义6个基准任务，提出统一HOI生成建模视角并给出相应方法。

Result: 在所定义的多项HOI生成相关任务上取得SOTA表现；经大量实验验证，优化与增广显著提升数据质量与实用性。

Conclusion: InterAct作为大规模且高质量的3D HOI基准与方法框架，有效缓解现有数据缺陷，促进HOI生成研究；数据集与代码公开并持续维护。

Abstract: While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.

</details>


### [68] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: 研究揭示：用于阿尔茨海默病（AD）MRI诊断的深度学习模型会学习与种族、性别相关的捷径特征，导致对少数群体的性能偏差。作者以多数据集、多模型验证并用归因分析定位偏差脑区特征，呼吁更公平的MRI智能诊断。


<details>
  <summary>Details</summary>
Motivation: 临床MRI被视为脑影像金标准，但DL模型可能利用与标签非因果的伪特征（捷径），尤其当这些特征与受保护属性（种族、性别）相关时，会引发对代表性不足人群的偏置。需要系统检验AD诊断模型中是否存在这类偏差，并理解其来源。

Method: 三步：1）可识别性测试：用3D脑MRI训练模型预测种族与性别，检验是否存在基于受保护属性的分布差异；2）不平衡训练：人为构造按种族/性别不均衡的数据集，评估AD分类性能变化以检测捷径学习与偏置；3）特征归因：对受保护属性预测与AD分类分别做定量/定性归因，分析不同脑区的贡献。采用多数据集与两类模型（3D ResNet、SwinTransformer）。

Result: 模型能够从3D脑MRI中识别种族与性别；当训练集在种族/性别上不平衡时，AD分类性能下降且群体间差异增大；归因图显示部分脑区特征同时驱动受保护属性与AD预测，提示捷径依赖。上述现象在多数据集与两种模型上均被观察到。

Conclusion: DL基于MRI的AD诊断存在可被触发的种族与性别捷径学习与偏置。应在数据与算法层面引入公平性约束与均衡策略，并结合可解释性方法以减少对受保护属性相关伪特征的依赖，为更公平的临床部署奠定基础。

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [69] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: 提出PeftCD：在多时相/多源遥感变化检测中，基于视觉基础模型通过PEFT（LoRA+Adapter）进行高效适配的轻量框架，采用共享权重Siamese编码器与轻解码器，在多个数据集上取得SOTA并有效抑制伪变化、提升边界精度与跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 变化检测常面临三大痛点：伪变化普遍、标注样本稀缺、跨域泛化差。大规模视觉基础模型具备强表示力，但直接全量微调成本高、易过拟合且难于迁移，亟需一种参数高效、可扩展的适配方法用于遥感多时相/多源数据。

Method: 以VFM为 backbone 构建权重共享的Siamese编码器，结合PEFT策略：在编码器内插入LoRA与Adapter模块，仅训练少量新增参数；探索两种主干（SAM2具强分割先验、DINOv3为自监督表示）；配合极简轻量解码器，突出主干特征；端到端训练以抑制伪变化与提升边界。

Result: 在多个公开数据集上达成SOTA：SYSU-CD IoU 73.81%，WHUCD 92.05%，MSRSCD 64.07%，MLCD 76.89%，CDD 97.01%，S2Looking 52.25%，LEVIR-CD 85.62%；表现出更精确边界与更强伪变化抑制和跨域泛化。

Conclusion: PeftCD在准确性、效率与泛化之间取得良好平衡，证明用PEFT高效适配大规模VFM是遥感变化检测的有效范式；具备可扩展性和落地潜力，代码与预训练模型将开源。

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples,
and the difficulty of cross-domain generalization in multi-temporal and
multi-source remote sensing imagery, we propose PeftCD, a change detection
framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient
Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese
encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly
integrated. This design enables highly efficient task adaptation by training
only a minimal set of additional parameters. To fully unlock the potential of
VFMs, we investigate two leading backbones: the Segment Anything Model v2
(SAM2), renowned for its strong segmentation priors, and DINOv3, a
state-of-the-art self-supervised representation learner. The framework is
complemented by a deliberately lightweight decoder, ensuring the focus remains
on the powerful feature representations from the backbones. Extensive
experiments demonstrate that PeftCD achieves state-of-the-art performance
across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD
(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and
LEVIR-CD (85.62%), with notably precise boundary delineation and strong
suppression of pseudo-changes. In summary, PeftCD presents an optimal balance
of accuracy, efficiency, and generalization. It offers a powerful and scalable
paradigm for adapting large-scale VFMs to real-world remote sensing change
detection applications. The code and pretrained models will be released at
https://github.com/dyzy41/PeftCD.

</details>


### [70] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: 提出Talk2Event：首个面向事件相机数据的语言指引目标定位（referring grounding）大规模基准，强调时空与关系属性以支持可解释、可组合的定位与推理。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级时域精度、抗模糊与强光适应，适合高动态场景；但与自然语言理解的融合研究稀缺，缺少用于语言驱动定位的标准数据与评测，阻碍多模态、时序感知发展。

Method: 构建Talk2Event基准：基于真实驾驶场景收集事件数据，提供5,567个场景、13,458个目标与3万余条经验证的指代表达；为每条表达标注四类结构化属性（外观、状态、与观察者关系、与周围物体关系），以显式编码空间、时间与关系线索，支持可解释与组合式定位评估。

Result: 得到一个规模化、质量受控的数据集与任务设置，能用于评测和分析语言驱动的事件数据目标定位性能，促进对动态环境中的上下文与时序推理能力的研究。

Conclusion: Talk2Event作为首个事件相机+语言指引定位基准，为多模态、时间感知的研究提供基础设施，预期在机器人、人与AI交互等应用中推动发展。

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [71] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 提出Kling-Avatar：一个将多模态指令理解与写实人像生成统一的两阶段级联框架，先由MLLM导演生成语义蓝图视频，再以关键帧指导并行子片段合成，实现长时长、1080p/48fps、语义可控与高保真音视频对齐的数字人生成。


<details>
  <summary>Details</summary>
Motivation: 现有音驱动头像方法仅将指令当作低层跟随信号（声学/视觉跟踪），忽略指令所携带的交互目的与语义意图，导致叙事连贯性与角色表现力不足，难以满足实用场景（直播、vlog）对长时长、表达力与可控性的需求。

Method: 提出两阶段全局到局部的级联框架：1）多模态大语言模型（MLLM）导演阶段，融合多种指令信号生成“蓝图视频”，规定高层语义（角色动作、情绪等）；2）生成阶段，以蓝图关键帧为引导，采用首-末帧策略并行合成多个子片段，再拼接成长视频，既保留细节又忠实表达高层意图；并行架构提升长视频生成的速度与稳定性。另构建含375个多样化指令与复杂场景的基准。

Result: 系统可生成生动流畅的长时视频，分辨率达1080p、帧率48fps；在唇形同步、情绪与动态表现、指令可控性、身份保持与跨域泛化等指标上优于现有方法。

Conclusion: Kling-Avatar将指令语义对齐与写实头像生成有机结合，树立了以语义为基础的高保真音驱动头像合成新标杆，适用于数字人直播、vlog等实际应用，并在长时稳定性与可控性上显著领先。

Abstract: Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.

</details>


### [72] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: 提出一种将肿瘤力学模型与引导式DDIM结合的混合方法，基于先前MRI生成解剖一致的未来扫描，并在小数据情境下提供带有增长概率图的生物学先验约束预测。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的时空进展预测对治疗决策至关重要；纯数据驱动生成在小样本和解剖一致性方面受限，纯机制模型又难以生成逼真的影像，因此需要融合机制先验与生成模型以提升可信度与临床实用性。

Method: 1) 机制层：将肿瘤生长与放疗效应建模为常微分方程，估计未来的肿瘤负担与动态；2) 生成层：使用梯度引导的DDIM，将机制模型输出作为条件，合成与患者解剖和预测生长一致的后续MRI；3) 数据：在BraTS成人与儿科胶质瘤训练，在院内儿科DMG的60个轴位序列上评估；4) 额外产物：根据生成过程构建肿瘤增长概率图。

Result: 生成的随访MRI在空间相似性指标上显示逼真度高；提出的增长概率图能捕获临床相关的肿瘤扩展范围与方向性，95百分位Hausdorff距离指标显示该能力；在数据受限情景下方法表现稳健。

Conclusion: 融合机制先验与扩散生成的框架能够进行生物学约束的时空影像预测，生成解剖可行且与生长动力学一致的未来MRI，为临床随访与治疗规划提供支持，尤其适用于小样本设置。

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


### [73] [Measuring Epistemic Humility in Multimodal Large Language Models](https://arxiv.org/abs/2509.09658)
*Bingkui Tong,Jiaer Xia,Sifeng Shang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 提出HumbleBench，一个评估多模态大模型在“无正确选项时能否拒答”的基准，用于衡量抑制幻觉的能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态评测多聚焦识别准确率（从候选中选对），忽视了当所有选项都错时应拒绝作答的能力，这对可信与安全至关重要。

Method: 基于全景场景图数据集，利用细粒度实体与关系标注为真值，借助GPT-4-Turbo生成多选题，经人工严格筛选；问题涵盖对象、关系、属性三类幻觉，并统一加入“以上皆非”选项以测试错误选项拒绝能力。

Result: 对多种SOTA通用与推理型MLLM进行评测，给出比较与洞见（摘要未给出具体数值），显示不同模型在拒绝错误选项与视觉事实识别方面存在差异。

Conclusion: HumbleBench通过显式的错误选项拒绝测试弥补现有评测空白，更贴近安全关键场景需求，能更真实衡量MLLM的可靠性；代码与数据已开源。

Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a "None of the above"
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.

</details>


### [74] [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)
*Zhiyuan Yan,Kaiqing Lin,Zongjian Li,Junyan Ye,Hui Han,Zhendong Wang,Hao Liu,Bin Lin,Hao Li,Xue Xu,Xinyan Xiao,Jingdong Wang,Haifeng Wang,Li Yuan*

Main category: cs.CV

TL;DR: 提出UAE统一多模态学习框架：把图像理解(I2T)当作编码器、图像生成(T2I)当作解码器，用重建保真度作为统一目标，通过RL的Unified-GRPO三阶段协同优化，实现双向信息流一致并带来互促；并给出衡量统一程度的新基准Unified-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型常将理解与生成割裂训练，缺乏统一目标与双向耦合，难以同时获得强理解与高保真生成。作者希望用“编码-解码/自编码器”视角统一I2T与T2I，以重建目标贯通训练，从而互相增强。

Method: 1) 把I2T视为编码器，把T2I视为解码器，以图像->文本->图像的重建误差（保真度）为统一目标。2) 先用大规模长上下文图文描述预训练解码器，学习细粒度语义与复杂空间关系。3) 提出Unified-GRPO强化学习流程：a) 冷启阶段，用语义重建损失温和初始化编解码器；b) Generation for Understanding：训练编码器生成能最大化解码器重建质量的描述（促理解）；c) Understanding for Generation：用这些描述进一步训练解码器，强迫其利用全部细节，提升长上下文指令跟随与生成保真。4) 构建Unified-Bench评估统一程度。

Result: 在RL推进中观测到“自发对齐”现象：编码器自动生成更具信息量的长描述，解码器对复杂描述的理解同步增强，图像重建的细节与逼真度显著提高。

Conclusion: 以重建保真度为统一目标的UAE框架，实现理解与生成的双向耦合与互促；Unified-GRPO有效驱动这种统一，带来更高保真生成与更强长上下文理解；Unified-Bench为评估统一多模态模型提供了首个专用基准。

Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

</details>


### [75] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: NRMF 提出一种基于神经黎曼运动场的高阶人体运动先验，通过在关节旋转—角速度—角加速度乘积流形上学习神经距离场零水平集，保证生成与重建的3D运动在时间上一致、物理上可行，并在多任务与多模态上显著优于VAE/扩散方法。


<details>
  <summary>Details</summary>
Motivation: 现有VAE/扩散式运动先验多在欧氏空间建模、忽略高阶动力学约束，导致恢复的动作存在时间不一致、物理不合理与跨任务泛化弱的问题。作者希望用尊重关节几何的流形建模与显式高阶动态约束，提升鲁棒性与泛化。

Method: 1) 将人体运动表示为关节旋转、角速度、角加速度的乘积流形；2) 在该流形上学习多个神经距离场（NDF），其零水平集刻画姿态、速度与加速度的可行集合（高阶先验）；3) 提出自适应步长的混合投影算法，将观测或生成的轨迹投影到可行运动集合；4) 设计几何保结构的积分器，在测试时优化与生成中“滚动”出物理一致的轨迹；5) 在AMASS上训练，并对多输入模态/任务评估。

Result: 在AMASS训练后，对去噪、补间（in-betweening）、以及适配部分2D/3D观测等任务上，NRMF取得显著且一致的性能提升，并表现出强泛化与鲁棒性。

Conclusion: 通过在黎曼流形上以神经距离场构建高阶人体运动先验，并结合投影与几何积分策略，NRMF能生成/恢复时间一致、物理合理的3D动作，优于主流VAE/扩散先验，并跨任务与模态泛化良好。

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative
human motion prior that enables robust, temporally consistent, and physically
plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,
our higher-order motion prior explicitly models the human motion in the zero
level set of a collection of neural distance fields (NDFs) corresponding to
pose, transition (velocity), and acceleration dynamics. Our framework is
rigorous in the sense that our NDFs are constructed on the product space of
joint rotations, their angular velocities, and angular accelerations,
respecting the geometry of the underlying articulations. We further introduce:
(i) a novel adaptive-step hybrid algorithm for projecting onto the set of
plausible motions, and (ii) a novel geometric integrator to "roll out"
realistic motion trajectories during test-time-optimization and generation. Our
experiments show significant and consistent gains: trained on the AMASS
dataset, NRMF remarkably generalizes across multiple input modalities and to
diverse tasks ranging from denoising to motion in-betweening and fitting to
partial 2D / 3D observations.

</details>


### [76] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 论文研究扩散模型中“最优去噪器”和深度扩散模型（如UNet）行为差异的来源，发现局部性并非卷积归纳偏置导致，而是由自然图像的像素相关统计性自然涌现；据此构建了更贴近深度模型得分的解析去噪器。


<details>
  <summary>Details</summary>
Motivation: 尽管训练目标存在封闭形式的最优解（最优去噪器），但用其扩散只能复现训练集图像，不能解释或复现深度扩散模型的生成特性。现有训练-free解析模型假设CNN的平移等变与局部性归纳偏置导致性能差距。作者动机是辨析差距真正成因，并改进解析模型以贴近深度模型行为。

Method: 1）提出并分析一个参数化线性去噪器，训练其最优解，并对其“局部性”性质进行度量；2）从理论上将局部性与自然图像像素相关结构联系起来，并给出证明/推导；3）在真实图像数据上进行实验，比较线性最优去噪器、深度UNet去噪器与先前解析模型的局部性与得分匹配度；4）基于上述洞见构造新的解析去噪器。

Result: 发现最优参数化线性去噪器展现出与深度神经去噪器相似的局部性；理论与实验均显示这种局部性直接由数据的像素相关统计引起，而非CNN归纳偏置；基于该认识设计的解析去噪器在与深度扩散模型得分匹配上优于既有专家构造模型。

Conclusion: 深度扩散模型中的局部性是数据分布（自然图像的像素相关）诱发的统计现象，而非卷积架构本身的归纳偏置；利用这一点可构建更准确的训练-free解析去噪器，更好地贴合深度扩散模型的行为。

Abstract: Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

</details>


### [77] [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)
*Jiahao Wang,Yufeng Yuan,Rujie Zheng,Youtian Lin,Jian Gao,Lin-Zhuo Chen,Yajie Bao,Yi Zhang,Chang Zeng,Yanxi Zhou,Xiaoxiao Long,Hao Zhu,Zhaoxiang Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 提出SpatialVID：一个大规模、具多样真实动态场景的视频数据集，含相机位姿、深度、动态掩码、结构化字幕与运动指令等稠密3D标注，旨在缓解空间智能（重建与探索）数据匮乏瓶颈并提升模型泛化与性能。


<details>
  <summary>Details</summary>
Motivation: 现有空间智能模型受限于缺乏大规模高质量训练数据，尤其缺少带真实相机运动的动态真实场景及丰富标注（位姿、深度、动态信息、语义与运动指令）。已有数据集规模、场景多样性和标注丰富度均不足，制约了模型的可扩展性与真实世界拟合。

Method: 收集超21,000小时野外视频，经分层过滤生成270万段片段（合计7,089小时动态内容）；随后通过标注流水线产生稠密时空与语义标注：逐帧相机位姿、深度图、动态掩码、结构化字幕与序列化运动指令；并进行数据统计分析以评估多样性与覆盖度。

Result: 构建了SpatialVID数据集，规模大、场景与相机运动多样，包含丰富稠密3D与语义标注。统计显示数据多样性和丰富度显著，有助于提升模型的泛化能力与性能。

Conclusion: SpatialVID成为视频与3D视觉领域的重要基础设施，缓解数据瓶颈，促进空间重建与世界探索等任务的研究与应用扩展。

Abstract: Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

</details>


### [78] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出FLUX-Reason-6M数据集与PRISM-Bench基准，面向推理导向的文生图；含6M图像、20M中英描述与GCoT，并评测19个模型揭示性能差距，公开资源促进开源T2I能力提升。


<details>
  <summary>Details</summary>
Motivation: 开源T2I因缺乏大规模、强调推理的数据与系统性评测而落后于闭源系统；需要可教复杂推理的训练资源与与人类偏好一致的细粒度评价标准。

Method: 1) 构建FLUX-Reason-6M：使用FLUX生成6M高质图、配20M中英描述；按六类能力（想象、实体、文本渲染、风格、情感、构图）组织；设计生成式CoT（GCoT）给出分步生成解析；共耗约15,000个A100 GPU日的数据策划。2) 设计PRISM-Bench：七大赛道，含使用GCoT的长文本挑战；通过精心提示，借助先进视觉语言模型对提示-图像对齐与美学进行细粒度、人类一致的自动评估。3) 在PRISM-Bench上对19个主流模型进行系统评测。

Result: PRISM-Bench评测显示多模型存在显著性能差距，并定位出需改进的具体维度（如长文本推理、组合与文本渲染等）；提供数据、基准与代码，支持复现与比较。

Conclusion: FLUX-Reason-6M与PRISM-Bench为推理导向文生图提供前所未有的开源数据与评测生态，弥合开源与闭源差距的关键一步，预计将推动下一代具备复杂推理与更佳美学的一致性T2I模型发展。

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>
