<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Face Time Traveller : Travel Through Ages Without Losing Identity](https://arxiv.org/abs/2602.22819)
*Purbayan Kar,Ayush Ghadiya,Vishal Chudasama,Pankaj Wasnik,C. V. Jawahar*

Main category: cs.CV

TL;DR: 提出FaceTT扩散框架，通过属性感知提示、无调参与角反演、自适应注意力，实现高保真且保身份的人脸年龄变换，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用数值年龄控制，忽略生物与环境线索的交互；在大跨度变龄时身份易漂移，注意力机制僵化、扩散反演耗时且难以适配，且难以精细控制并保持背景一致。

Method: 1) Face-Attribute-Aware Prompt Refinement：将内在（生物）与外在（环境）老化线索编码进文本/条件提示，实现上下文感知控制；2) Tuning-free Angular Inversion：无需微调，基于角度度量将真实人脸高效映射到扩散潜空间，快速准确重建；3) Adaptive Attention Control：动态平衡跨注意力（语义老化线索）与自注意力（结构与身份保持），实现细粒度控制与背景一致性。

Result: 在基准数据集与in-the-wild测试中，FaceTT在身份保持、背景保真与老化真实感上均优于SOTA，展现更高的视觉质量与稳定性。

Conclusion: 融合属性感知条件、无调参与角反演及自适应注意的扩散式FaceTT，能实现高保真、身份一致的年龄变换并提升适配性与可控性，为实际场景（娱乐、取证、数字档案）提供更可靠的解决方案。

Abstract: Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.

</details>


### [2] [MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2602.22932)
*Wenhui Tan,Xiaoyi Yu,Jiaze Li,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Ruihua Song,Jian Luan*

Main category: cs.CV

TL;DR: 提出MSJoE框架，通过与轻量关键帧采样器联合进化，提升MLLM对长视频问答的效率与准确性：用多视角文本查询与CLIP相互作用生成相似度矩阵，轻量采样器据此选取少量关键帧，送入MLLM作答，并用强化学习联合优化查询推理、帧采样与理解；在多个长视频基准上优于基线。


<details>
  <summary>Details</summary>
Motivation: 长视频包含大量冗余信息，现有MLLM处理成本高、关键证据稀疏且易被平均池化淹没；需要在保证语义覆盖的同时极大降低输入帧数，并让模型把注意力集中到与问题相关的关键时刻。

Method: 1) 假设“每个问题仅需少量关键帧”。2) 由MLLM先生成多条多样化“查询”以覆盖不同视觉视角。3) 这些查询与冻结的CLIP交互，计算查询-帧相似度矩阵。4) 轻量采样器从相似度矩阵预测每帧采样权重，挑选紧凑帧集。5) 将选中帧输入MLLM生成答案。6) 采用强化学习联合优化MLLM与采样器，使查询生成、帧采样与关键帧理解协同进化。7) 构建包含2.8K视频与7K问答的新长视频QA数据集以支撑训练。

Result: 在VideoMME、LongVideoBench、LVBench、MLVU等基准上，相比基底MLLM准确率提升8.0%；较最强对比方法再提升1.1%。

Conclusion: 联合进化的“查询-采样-理解”闭环有效聚焦与问题相关的少量关键帧，在显著降低计算的同时提升长视频问答表现；RL训练使采样策略与MLLM能力协同增强，优于现有长视频理解方法。

Abstract: Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\% accuracy gain upon the base MLLM, and 1.1\% higher accuracy than strongest baseline method.

</details>


### [3] [pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation](https://arxiv.org/abs/2602.22938)
*Shentong Mo,Xufang Luo,Dongsheng Li*

Main category: cs.CV

TL;DR: 提出pMoE：将多领域专家知识通过Mixture‑of‑Experts式提示调优整合，借助专家专属prompt与可学习的分发器，在多任务（通用与医学、分类与分割）上显著优于现有方法，并在效率与效果间取得更优权衡。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调多用单一预训练模型（通用或医学），忽视多领域知识的互补性，限制了在跨领域、跨任务适配时的表现与泛化。

Method: 设计pMoE：1）为每个领域专家建立专家特定的prompt token集合；2）在多个prompt层引入可学习的动态token分发（dispatcher），按输入与任务自适应加权路由各专家提示；3）在统一框架中联合优化，使不同领域知识在适配过程中按需贡献。

Result: 在47个适配任务（覆盖通用与医学、分类与分割）上，pMoE取得大幅性能提升；同时在计算开销与适配效果之间实现更优折中，超过现有参数高效方法与单域prompt调优。

Conclusion: 融合多专家领域提示并进行动态分发，可显著提升跨域多任务适配的性能与效率；pMoE为视觉任务提供了通用且可扩展的参数高效调优范式。

Abstract: Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.

</details>


### [4] [Velocity and stroke rate reconstruction of canoe sprint team boats based on panned and zoomed video recordings](https://arxiv.org/abs/2602.22941)
*Julian Ziegler,Daniel Matthes,Finn Gerdts,Patrick Frenzel,Torsten Warnke,Matthias Englert,Tina Koevari,Mirco Fuchs*

Main category: cs.CV

TL;DR: 论文提出基于常规转拍/变焦比赛视频的自动化速度与划频重建框架，在无GPS与人工标注条件下为独木/皮艇各组别与距离提供高精度表现分析。核心包括浮标与运动员检测、基于浮标网格的单应估计、U‑Net学习船型特异的船尖/运动员偏移校准、光流鲁棒多艇跟踪，以及从姿态或包围框提取划频。与精英赛GPS对比：速度RRMSE≈0.020±0.011（ρ=0.956），划频RRMSE≈0.022±0.024（ρ=0.932）。


<details>
  <summary>Details</summary>
Motivation: GPS是划艇竞速节奏分析的金标准，但设备稀缺、装载受限且多艇项目部署困难；现有视频方法多局限单艇/定机位，难以应对转拍与变焦。需要一种通用于不同艇型和赛程、可从赛事实况视频自动恢复速度与划频的方案，为教练提供无传感器的赛后/赛中反馈。

Method: 1) 目标检测：用YOLOv8检测赛道浮标与运动员/艇体；2) 几何标定：利用已知浮标网格求取场景单应矩阵，统一到赛道平面坐标；3) 船位估计：提出U‑Net船尖校准，学习“运动员检测中心→船尖”的船型特异偏移，得到更精确的艇位；4) 跟踪：结合光流的鲁棒跟踪策略，适配K2/K4、C2等多运动员艇型；5) 划频提取：基于姿态估计的周期信号或直接从包围框时序振荡提取划频；6) 指标重建：由校准后轨迹求速度曲线与配速；7) 评估：与精英赛事GPS对齐，计算RRMSE与Spearman相关。

Result: 在多学科别与距离上，速度重建达到RRMSE 0.020±0.011、ρ=0.956；划频重建RRMSE 0.022±0.024、ρ=0.932，显示与GPS高度一致。方法在无船载传感器、无需人工标注下稳定运行于转拍/变焦实况视频。

Conclusion: 提出的端到端视频重建框架在多艇型、多距离与复杂摄制条件下均能高精度恢复速度与划频，可为教练与运动员提供自动化、低成本的配速与技术反馈；船型特异偏移学习与光流跟踪是适配多运动员艇型的关键。未来可扩展到实时化、跨场地自适应与更强的遮挡处理。

Abstract: Pacing strategies, defined by velocity and stroke rate profiles, are essential for peak performance in canoe sprint. While GPS is the gold standard for analysis, its limited availability necessitates automated video-based solutions. This paper presents an extended framework for reconstructing performance metrics from panned and zoomed video recordings across all sprint disciplines (K1-K4, C1-C2) and distances (200m-500m). Our method utilizes YOLOv8 for buoy and athlete detection, leveraging the known buoy grid to estimate homographies. We generalized the estimation of the boat position by means of learning a boat-specific athlete offset using a U-net based boat tip calibration. Further, we implement a robust tracking scheme using optical flow to adapt to multi-athlete boat types. Finally, we introduce methods to extract stroke rate information from either pose estimations or the athlete bounding boxes themselves. Evaluation against GPS data from elite competitions yields a velocity RRMSE of 0.020 +- 0.011 (rho = 0.956) and a stroke rate RRMSE of 0.022 +- 0.024 (rho = 0.932). The methods provide coaches with highly accurate, automated feedback without requiring on-boat sensors or manual annotation.

</details>


### [5] [Cross-Task Benchmarking of CNN Architectures](https://arxiv.org/abs/2602.22945)
*Kamal Sherawat,Vikrant Bhati*

Main category: cs.CV

TL;DR: 比较基于ResNet‑18的多种动态CNN（硬/软注意力与ODConv）在图像分类、分割与时间序列任务上的表现；动态机制整体优于传统CNN，ODConv在形态复杂图像上尤佳。


<details>
  <summary>Details</summary>
Motivation: 评估并统一比较不同动态卷积与注意力机制在跨模态、跨任务（图像与时间序列）中的效果与效率，寻找更通用、更高效的CNN设计。

Method: 以ResNet‑18为骨干，构建五种变体：1) 普通CNN；2) 硬注意力CNN；3) 软注意力CNN（局部像素级与全局图像级特征注意）；4) 全向动态卷积（ODConv）。在Tiny ImageNet、Pascal VOC、UCR时间序列库上进行实验，对比精度、效率与计算成本，并分析动态核调制与跨任务泛化。

Result: 注意力与动态卷积整体在准确率、效率与计算性能上优于普通CNN；ODConv对形态复杂图像适应性最强；动态CNN通过自适应核调制提升特征表征与跨任务泛化。

Conclusion: 动态与注意力驱动的CNN是优于传统CNN的通用方案，尤其ODConv对空间形态变化具有优势；为多模态/多任务CNN架构提供设计参考并指向后续神经网络工程的有前景方向。

Abstract: This project provides a comparative study of dynamic convolutional neural networks (CNNs) for various tasks, including image classification, segmentation, and time series analysis. Based on the ResNet-18 architecture, we compare five variants of CNNs: the vanilla CNN, the hard attention-based CNN, the soft attention-based CNN with local (pixel-wise) and global (image-wise) feature attention, and the omni-directional CNN (ODConv). Experiments on Tiny ImageNet, Pascal VOC, and the UCR Time Series Classification Archive illustrate that attention mechanisms and dynamic convolution methods consistently exceed conventional CNNs in accuracy, efficiency, and computational performance. ODConv was especially effective on morphologically complex images by being able to dynamically adjust to varying spatial patterns. Dynamic CNNs enhanced feature representation and cross-task generalization through adaptive kernel modulation. This project provides perspectives on advanced CNN design architecture for multiplexed data modalities and indicates promising directions in neural network engineering.

</details>


### [6] [SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling](https://arxiv.org/abs/2602.23013)
*Camile Lendering,Erkut Akdag,Egor Bondarev*

Main category: cs.CV

TL;DR: SubspaceAD是一种无需训练的少样本工业视觉异常检测方法：用DINOv2提取补丁特征，PCA拟合正常子空间，推理时用重建残差作为异常分数；在MVTec-AD与VisA的一次/少样本设定下达成新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有少样本异常检测虽利用大模型特征但常依赖复杂组件（记忆库、外部辅助数据、多模态微调），实现和部署成本高，作者质疑在强表征下这些复杂性是否必要。

Method: 两阶段、免训练：1）用冻结的DINOv2提取少量正常图像的patch级特征；2）对这些特征做PCA以估计正常变化的低维子空间。推理时将测试特征投影并计算重建残差，得到可解释、具统计依据的异常分数；同时产生像素级热力图。

Result: 在一次样本设定：MVTec-AD图像级/像素级AUROC分别为98.0%/97.6%；VisA为93.3%/98.3%，均超过先前SOTA。少样本场景也保持领先。

Conclusion: 简单的子空间建模结合基础模型特征即可在少样本异常检测中达到SOTA，无需训练、提示调优或记忆库，方法高效、可解释且易于部署。

Abstract: Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.

</details>


### [7] [DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis](https://arxiv.org/abs/2602.23022)
*Xinglong Luo,Ao Luo,Zhengning Wang,Yueqi Yang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 提出DMAligner：用扩散模型进行面向对齐的视图合成来替代光流扭曲，从而更稳健地完成图像对齐，特别应对遮挡与光照变化；并构建DSIA数据集验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统图像对齐多依赖光流与图像扭曲，易受遮挡、动态前景与光照变化影响，导致可视质量差并影响下游任务准确性。需要一种能在复杂动态场景中更鲁棒的对齐范式。

Method: 提出扩散式对齐框架DMAligner：通过条件图像生成进行“对齐导向的视图合成”，避免显式光流扭曲。核心是“动态感知的扩散训练”（Dynamics-aware Diffusion Training），包含动态感知掩码生成模块DMP，以自适应区分动态前景与静态背景，使扩散模型在训练/生成时对动态区域特殊处理，从而提升对齐。另构建基于Blender的DSIA数据集（1033个场景，3万+对图像）。

Result: 在自建DSIA基准及多种常用视频数据集上，DMAligner取得更优的可视对齐质量与定量指标，相比光流类方法在遮挡、光照变化与动态场景下表现显著更稳健。

Conclusion: 基于扩散的对齐导向视图合成可有效替代传统流场扭曲，借助动态感知训练与掩码机制在复杂动态场景中显著提升图像对齐性能；公开代码与数据支持复现与后续研究。

Abstract: Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.

</details>


### [8] [WISER: Wider Search, Deeper Thinking, and Adaptive Fusion for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.23029)
*Tianyue Wang,Leigang Qu,Tianyu Yang,Xiangzhao Hao,Yifan Xu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出WISER：一个零样本复合图像检索的训练-free框架，统一T2I与I2I，通过“检索-验证-精炼”循环与不确定性感知，显著优于现有方法并超越多款需训练的方法。


<details>
  <summary>Details</summary>
Motivation: ZS-CIR需在无标注三元组训练下，用（参考图像+文本修改）检索目标图像。单一路径转换为T2I或I2I各有瓶颈：T2I易丢细粒度视觉信息，I2I难处理复杂语义修改。需要一种能按查询意图自适应融合二者并处理不确定性的方案。

Method: 提出WISER训练-free框架：1）Wider Search并行生成“编辑 caption”（T2I）与“编辑图像”（I2I）以扩大候选；2）Adaptive Fusion利用验证器评估检索置信度，对高置信度结果进行动态融合，对不确定结果触发精炼；3）Deeper Thinking通过结构化自反思生成精炼建议，引导下一轮检索迭代。整体实现“检索-验证-精炼”的闭环，并显式建模“意图感知+不确定性感知”。

Result: 在多基准上显著领先：相对提升CIRCO mAP@5达45%，CIRR Recall@1达57%，并超过多种需训练的方法，体现强泛化与鲁棒性。

Conclusion: WISER在零样本复合检索中有效联合T2I与I2I，借助并行检索、验证融合与自反思精炼，弥补两路径互补性并处理不确定性，达到SOTA乃至超越训练依赖方法的性能。

Abstract: Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a multimodal query (comprising a reference image and a modification text), without training on annotated triplets. Existing methods typically convert the multimodal query into a single modality-either as an edited caption for Text-to-Image retrieval (T2I) or as an edited image for Image-to-Image retrieval (I2I). However, each paradigm has inherent limitations: T2I often loses fine-grained visual details, while I2I struggles with complex semantic modifications. To effectively leverage their complementary strengths under diverse query intents, we propose WISER, a training-free framework that unifies T2I and I2I via a "retrieve-verify-refine" pipeline, explicitly modeling intent awareness and uncertainty awareness. Specifically, WISER first performs Wider Search by generating both edited captions and images for parallel retrieval to broaden the candidate pool. Then, it conducts Adaptive Fusion with a verifier to assess retrieval confidence, triggering refinement for uncertain retrievals, and dynamically fusing the dual-path for reliable ones. For uncertain retrievals, WISER generates refinement suggestions through structured self-reflection to guide the next retrieval round toward Deeper Thinking. Extensive experiments demonstrate that WISER significantly outperforms previous methods across multiple benchmarks, achieving relative improvements of 45% on CIRCO (mAP@5) and 57% on CIRR (Recall@1) over existing training-free methods. Notably, it even surpasses many training-dependent methods, highlighting its superiority and generalization under diverse scenarios. Code will be released at https://github.com/Physicsmile/WISER.

</details>


### [9] [SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation](https://arxiv.org/abs/2602.23103)
*Fuhao Zhang,Lei Liu,Jialin Zhang,Ya-Nan Zhang,Nan Mu*

Main category: cs.CV

TL;DR: SpectralMamba-UNet 通过在频域解耦低/高频信息，把全局结构交互交给频域Mamba、把边界细节保留在高频支路，并用通道频率重加权与频谱引导解码融合，提升医学分割的精度与泛化。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（如Vision Mamba）善于高效建模长程依赖，但一维序列化会削弱局部空间连续性与高频细节表达；医学图像分割同时需要全局解剖结构与细粒度边界，因此亟需一种既保全高频、又能做全局建模的框架。

Method: 提出SpectralMamba-UNet：在编码阶段用离散余弦变换将特征分解为低频与高频；低频支路进入频域Mamba进行全局上下文建模，高频支路用于边界敏感细节保留。设计光谱通道重加权（SCR）实现通道级频率感知注意；在解码端用光谱引导融合（SGF）自适应地进行多尺度融合，平衡不同频段贡献。

Result: 在五个公开医学分割基准（跨模态、跨目标）上均取得一致提升，显示出方法的有效性与可泛化性。

Conclusion: 频域解耦结合Mamba的全局建模与高频细节保留，并通过通道重加权与频谱引导解码融合，有效缓解一维序列化带来的局部连续性与高频损失问题，提升医学图像分割表现与泛化能力。

Abstract: Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.

</details>


### [10] [WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2602.23114)
*Xudong Yan,Songhe Feng,Jiaxin Wang,Xin Su,Yi Jin*

Main category: cs.CV

TL;DR: 论文提出一种在测试时自适应更新多模态原型的方法（WARM-CAT），缓解CZSL因标签空间分布迁移导致的性能退化；通过无监督数据累积文本与视觉知识、动态优先队列与自适应更新权重、以及多模态协同表示对齐，实现闭/开世界SOTA；并发布新数据集C-Fashion与精炼版MIT-States。


<details>
  <summary>Details</summary>
Motivation: CZSL需要识别未见过的属性-对象组合，但测试时标签空间包含由属性与对象重组而成的大量未见组合，产生分布迁移，传统静态原型或固定训练分布假设的方法在此场景下性能明显下降。作者希望在不依赖标注的情况下，于测试阶段利用无监督数据持续累积知识并调整模型表征，从而适应标签分布变化并提升泛化。

Method: 1) 在测试时从无监督数据同时挖掘文本与视觉信息，更新多模态原型（textual/visual prototypes）。2) 设计自适应更新权重，按样本置信与不确定性动态控制原型调整幅度。3) 引入动态优先队列，存储高置信度图像以形成历史视觉原型用于推理；通过训练集图像对已见组合进行“warm-start”，并利用已见与未见文本原型间的映射生成未见视觉原型。4) 采用多模态协同表示学习对齐文本与视觉原型，保证语义一致性与跨模态可交换性。5) 提出新数据集C-Fashion，并清洗MIT-States以减少噪声。

Result: 在四个基准（含闭世界与开世界设置）上取得SOTA表现；动态队列与自适应权重带来显著增益；新数据集与精炼数据集提升了CZSL评测可靠性。代码与数据集开源。

Conclusion: 测试时多模态原型的自适应更新与历史记忆机制可有效缓解CZSL的分布迁移问题；多模态对齐进一步提升组合泛化能力。方法通用、无监督友好，并在多数据集上验证有效，伴随更可靠的评测基准。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .

</details>


### [11] [Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation](https://arxiv.org/abs/2602.23117)
*Xiaosen Wang,Zhijin Ge,Bohan Liu,Zheng Fang,Fengfan Zhou,Ruixuan Zhang,Shaokang Wang,Yuyang Luo*

Main category: cs.CV

TL;DR: 论文综述了对抗样本的迁移性，并提出统一评测框架与标准以公正比较各类迁移攻击方法。


<details>
  <summary>Details</summary>
Motivation: 当前迁移攻击研究繁多但缺乏统一的评测框架与标准，导致结果不可比、结论偏差；需要系统化梳理方法类别、共性策略与评测注意事项。

Method: 1) 系统回顾数百篇相关工作；2) 将迁移式对抗攻击方法归纳为六大类；3) 提出一套用于评测迁移攻击的全面基准与流程；4) 总结可提升迁移性的通用策略；5) 指出常见导致不公平比较的问题；6) 简述在图像分类以外任务上的迁移攻击进展。

Result: 形成了六类方法的清晰分类体系与术语，给出了可复现实验与评测基准框架，提炼出提升迁移性的实用策略，并揭示当前文献中的评测偏差与陷阱；同时补充跨任务（超越图像分类）的简要综述。

Conclusion: 该工作为迁移式攻击研究提供标准化评测与分类参考，有助于公平比较与复现实验，并指导未来在更广任务上的研究与方法改进。

Abstract: Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.

</details>


### [12] [TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement](https://arxiv.org/abs/2602.23120)
*Arian Sabaghi,José Oramas*

Main category: cs.CV

TL;DR: TriLite 是一个单阶段、少参数（<80万）、基于冻结 Dinov2 自监督 ViT 的弱监督目标定位（WSOL）框架，通过新提出的 TriHead 将 patch 表示分解为前景/背景/模糊三类，从而提升目标覆盖并抑制虚假激活，在 CUB-200-2011、ImageNet-1K、OpenImages 上达到新的 SOTA，且训练更简单、成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有 WSOL 方法常需多阶段流程或对大规模骨干进行全量微调，训练成本高，同时易出现只覆盖目标显著部位、对象覆盖不全的问题。研究者希望在不增加训练负担的情况下提升目标覆盖并减少误检。

Method: 冻结自监督预训练（Dinov2）ViT 作为特征提取，新增极少量可训练参数构成 TriLite：核心 TriHead 模块将 patch 特征三分解为前景、背景、模糊区域，通过解耦分类与定位目标，分别进行优化；利用通用表征，无需端到端大规模微调，实现单阶段训练与推理。

Result: 在 CUB-200-2011、ImageNet-1K、OpenImages 上进行大量实验，TriLite 在分类与定位指标上取得新的 SOTA，同时参数效率显著（<80万）且训练更易收敛、流程更简单。

Conclusion: 利用冻结的自监督 ViT 与少量可训练头部并解耦分类/定位，可以在显著降低训练成本的同时提升 WSOL 的目标覆盖与鲁棒性。TriLite 提供了高效且有效的单阶段 WSOL 方案，具有良好实用前景；代码即将开源。

Abstract: Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.

</details>


### [13] [From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification](https://arxiv.org/abs/2602.23133)
*Xin Yuan,Zhiyong Zhang,Xin Xu,Zheng Wang,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出CARE，两阶段（校准->精炼）框架，通过证据校准与传播提升在含噪且每类样本稀疏的人重识别鲁棒性。用PEC打破softmax平移不变并抑制过度自信；用EPR的CAM+COSW精确区分并加权样本，保留难正样本、抑制噪声，三大数据集在多种噪声下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实Re-ID数据常含错误标注与每个身份样本少，现有基于softmax的损失校正/小损样本选择方法会：1) 因softmax平移不变导致对噪声标签过度自信；2) 小损准则误删对判别性学习关键的“难正样本”。需要能抑制过度自信并更准确区分干净/噪声、同时保留难正样本的方法。

Method: 两阶段CARE：
- 校准（PEC）：在相似度函数中引入自适应可学习参数，打破softmax平移不变；配合“证据式校准损失”降低对错标样本的过度自信。
- 精炼（EPR）：基于证据传播更准地区分干净与噪声样本。包含两步：1) 复合角度间隔（CAM）度量，在超球空间区分干净但难学的正样本与误标；2) 以确定性为导向的球面加权（COSW），依据CAM动态分配样本权重，让干净样本主导更新。

Result: 在Market1501、DukeMTMC-ReID、CUHK03三大数据集、随机及模式化噪声设定下取得有竞争力（SOTA级别或接近）的性能，验证方法在噪声场景中的鲁棒性与泛化性。

Conclusion: 通过从校准到精炼的证据传播范式，CARE缓解softmax的结构性缺陷与小损筛选的偏差，既抑制错误标注带来的过度自信，又保留难正样本，显著提升含噪Re-ID的鲁棒性与性能。

Abstract: With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.

</details>
