<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multi-encoder ConvNeXt Network with Smooth Attentional Feature Fusion for Multispectral Semantic Segmentation](https://arxiv.org/abs/2602.10137)
*Leo Thomas Ramos,Angel D. Sappa*

Main category: cs.CV

TL;DR: 提出MeCSAFNet：多分支编码-解码架构，分别处理可见与不可见光谱，融合多尺度特征并用注意力与新激活优化；在FBP与Potsdam显著提升mIoU，含轻量化变体适配算力受限场景。


<details>
  <summary>Details</summary>
Motivation: 多光谱遥感的地物分类需同时保留细粒度空间信息与高层光谱语义，传统单分支或简单早/晚融合方法难以充分利用不同光谱带与指数（如NDVI/NDWI）的互补性，且在资源受限环境下需要更高效的模型。

Method: 构建双ConvNeXt编码器分别处理可见与非可见通道；各自解码器重建空间细节；专门的融合解码器跨尺度整合中间特征，将细粒度空间线索与高层光谱表征结合；引入CBAM增强通道与空间注意力；采用ASAU激活提升训练稳定性与优化效率；支持4通道(RGB+NIR)与6通道(再加NDVI、NDWI)配置，并提供不同规模与紧凑变体。

Result: 在FBP上，MeCSAFNet-base(6c)较U-Net(4c)+19.21%、U-Net(6c)+14.72%、SegFormer(4c)+19.62%、SegFormer(6c)+14.74% mIoU；在Potsdam上，MeCSAFNet-large(4c)较DeepLabV3+(4c)+6.48%、DeepLabV3+(6c)+5.85%、SegFormer(4c)+9.11%、SegFormer(6c)+4.80% mIoU；对多种SOTA也有一致增益；紧凑版在训练时间与推理成本更低的同时保持较好精度。

Conclusion: 多分支专属编码-解码加多尺度融合与注意力对多光谱地物分割有效，6通道配置进一步提升性能；模型在精度与效率间提供可扩展折中，适合资源受限部署。

Abstract: This work proposes MeCSAFNet, a multi-branch encoder-decoder architecture for land cover segmentation in multispectral imagery. The model separately processes visible and non-visible channels through dual ConvNeXt encoders, followed by individual decoders that reconstruct spatial information. A dedicated fusion decoder integrates intermediate features at multiple scales, combining fine spatial cues with high-level spectral representations. The feature fusion is further enhanced with CBAM attention, and the ASAU activation function contributes to stable and efficient optimization. The model is designed to process different spectral configurations, including a 4-channel (4c) input combining RGB and NIR bands, as well as a 6-channel (6c) input incorporating NDVI and NDWI indices. Experiments on the Five-Billion-Pixels (FBP) and Potsdam datasets demonstrate significant performance gains. On FBP, MeCSAFNet-base (6c) surpasses U-Net (4c) by +19.21%, U-Net (6c) by +14.72%, SegFormer (4c) by +19.62%, and SegFormer (6c) by +14.74% in mIoU. On Potsdam, MeCSAFNet-large (4c) improves over DeepLabV3+ (4c) by +6.48%, DeepLabV3+ (6c) by +5.85%, SegFormer (4c) by +9.11%, and SegFormer (6c) by +4.80% in mIoU. The model also achieves consistent gains over several recent state-of-the-art approaches. Moreover, compact variants of MeCSAFNet deliver notable performance with lower training time and reduced inference cost, supporting their deployment in resource-constrained environments.

</details>


### [2] [Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement](https://arxiv.org/abs/2602.10138)
*Zhihang Yi,Jian Zhao,Jiancheng Lv,Tao Wang*

Main category: cs.CV

TL;DR: 这是一篇关于基于多模态大模型（MLLM）的图表理解综述，系统梳理任务、数据集、方法演进与挑战，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 图表理解需融合视觉与文本信息，传统方法与现有MLLM研究分散、缺乏系统化框架与统一基准，阻碍领域发展与比较。

Method: 构建结构化路线图：1）解析图文融合的核心挑战；2）提出面向下游任务与数据集的分类，并区分“规范/非规范”基准；3）回顾方法演进，从经典深度学习到先进MLLM与融合策略；4）批判性分析当前模型在感知与推理方面的不足；5）提出未来方向，如高级对齐与基于强化学习的认知增强。

Result: 形成一套覆盖任务、数据集、方法与评测的系统化框架，梳理从传统到MLLM的技术脉络，揭示现有模型的关键短板（感知与推理不足），并总结代表性基准与应用范围。

Conclusion: 综述为研究者提供清晰地图与统一术语体系，强调改进跨模态对齐、感知与逻辑推理能力的重要性，倡导引入强化学习等手段，推动更稳健、可靠的图表信息融合系统发展。

Abstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.

</details>


### [3] [MPA: Multimodal Prototype Augmentation for Few-Shot Learning](https://arxiv.org/abs/2602.10143)
*Liwen Wu,Wei Wang,Lei Zhao,Zhan Gao,Qika Lin,Shaowen Yao,Zuozhu Liu,Bin Pu*

Main category: cs.CV

TL;DR: 提出MPA多模态原型增强少样本学习框架，结合LLM语义增强、分层多视角增强与自适应不确定类吸收，在多项单域与跨域基准上显著超越SOTA，5-way 1-shot下单域/跨域分别提升12.29%/24.56%.


<details>
  <summary>Details</summary>
Motivation: 现有FSL多依赖视觉单模态，直接由少量支持样本构原型，语义信息贫乏、视角/光照等变化下泛化弱，且对不确定样本缺乏稳健建模，导致跨域与极少样本场景性能受限。

Method: MPA包含三部分：1) LMSE：利用大语言模型生成多样化的类别释义/描述，作为额外语义提示参与原型构建；2) HMA：联合自然增强与多视角增强（距离、角度、光照等）以扩大特征多样性，形成分层表示；3) AUCA：通过插值与高斯采样引入“不确定类”，对边界与噪声样本进行吸收建模，减少原型污染与过拟合。

Result: 在4个单域与6个跨域FSL基准上，MPA在大多数设定中优于SOTA；在5-way 1-shot时，单域提升12.29%，跨域提升24.56%。

Conclusion: 多模态语义增强+多视角特征扩充+不确定性吸收能显著提升FSL鲁棒性与跨域泛化，MPA为少样本识别提供了有效范式。

Abstract: Recently, few-shot learning (FSL) has become a popular task that aims to recognize new classes from only a few labeled examples and has been widely applied in fields such as natural science, remote sensing, and medical images. However, most existing methods focus only on the visual modality and compute prototypes directly from raw support images, which lack comprehensive and rich multimodal information. To address these limitations, we propose a novel Multimodal Prototype Augmentation FSL framework called MPA, including LLM-based Multi-Variant Semantic Enhancement (LMSE), Hierarchical Multi-View Augmentation (HMA), and an Adaptive Uncertain Class Absorber (AUCA). LMSE leverages large language models to generate diverse paraphrased category descriptions, enriching the support set with additional semantic cues. HMA exploits both natural and multi-view augmentations to enhance feature diversity (e.g., changes in viewing distance, camera angles, and lighting conditions). AUCA models uncertainty by introducing uncertain classes via interpolation and Gaussian sampling, effectively absorbing uncertain samples. Extensive experiments on four single-domain and six cross-domain FSL benchmarks demonstrate that MPA achieves superior performance compared to existing state-of-the-art methods across most settings. Notably, MPA surpasses the second-best method by 12.29% and 24.56% in the single-domain and cross-domain setting, respectively, in the 5-way 1-shot setting.

</details>


### [4] [VERA: Identifying and Leveraging Visual Evidence Retrieval Heads in Long-Context Understanding](https://arxiv.org/abs/2602.10146)
*Rongcan Pei,Huan Li,Fang Guo,Qi Zhu*

Main category: cs.CV

TL;DR: 论文发现并利用VLM中负责从长上下文中“找证据”的稀疏注意力头（VER头），通过训练外框架VERA在不改动模型的情况下显著提升长上下文理解与复杂推理表现。


<details>
  <summary>Details</summary>
Motivation: VLM在长上下文与复杂推理上常掉链子，原因机制不清。作者希望开箱分析其内部注意力，找出影响长文档视觉证据检索与推理的关键瓶颈，并在无需再训练的前提下提升性能。

Method: 1) 注意力剖析：区分静态OCR头与动态、与任务相关的Visual Evidence Retrieval（VER）头；通过掩蔽实验验证其因果性。2) 不确定性触发：以熵检测模型不确定时刻。3) 训练外增强VERA：在高熵时显式“言语化”VER头所关注的视觉证据，将其作为中间证据提示输入模型以辅助推理。

Result: 掩蔽VER头显著退化性能，证明其因果作用。将VERA应用于开源VLM（Qwen3-VL-8B-Instruct、GLM-4.1V-Thinking）在五个基准上的长上下文理解平均相对提升分别为21.3%与20.1%。

Conclusion: VLM的长上下文瓶颈与少量关键VER注意力头密切相关。通过基于不确定性的证据显式化（VERA），可在零训练改造下显著增强长上下文推理，提示未来可围绕VER头进行可解释与高效的推理增强设计。

Abstract: While Vision-Language Models (VLMs) have shown promise in textual understanding, they face significant challenges when handling long context and complex reasoning tasks. In this paper, we dissect the internal mechanisms governing long-context processing in VLMs to understand their performance bottlenecks. Through the lens of attention analysis, we identify specific Visual Evidence Retrieval (VER) Heads - a sparse, dynamic set of attention heads critical for locating visual cues during reasoning, distinct from static OCR heads. We demonstrate that these heads are causal to model performance; masking them leads to significant degradation. Leveraging this discovery, we propose VERA (Visual Evidence Retrieval Augmentation), a training-free framework that detects model uncertainty (i.e., entropy) to trigger the explicit verbalization of visual evidence attended by VER heads. Comprehensive experiments demonstrate that VERA significantly improves long-context understanding of open-source VLMs: it yields an average relative improvement of 21.3% on Qwen3-VL-8B-Instruct and 20.1% on GLM-4.1V-Thinking across five benchmarks.

</details>


### [5] [Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization](https://arxiv.org/abs/2602.10159)
*Tao Yu,Yujia Yang,Haopeng Jin,Junhao Gong,Xinlong Chen,Yuxuan Zhou,Shanbin Zhang,Jiabing Yang,Xinming Wang,Hongzhu Yi,Ping Nie,Kai Zou,Zhang Zhang,Yan Huang,Liang Wang,Yeshani,Ruiwen Tao,Jin Ma,Haijin Liang,Jinwen Luo*

Main category: cs.CV

TL;DR: 提出RVMS-Bench与RACLO，用于评测与提升基于模糊多维记忆的开放域视频检索与片段定位；实验证明现有MLLM在此任务上不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索基准依赖封闭视频集与精确描述，无法反映现实中基于模糊、跨模态、多维线索（整体印象、关键片段、时间脉络、声音）的开放网页搜索需求。

Method: 构建RVMS-Bench：从开放网络收集覆盖20类、4时长组、共1440个样本，采用分层描述框架（全局印象、关键时刻、时间上下文、听觉记忆），并以人参与验证流程确保标注可靠。提出RACLO：一种具备溯因（abductive）推理的代理式框架，模拟“回忆-搜索-核验”流程，以处理模糊记忆驱动的视频检索与时刻定位。

Result: 在RVMS-Bench上评测显示，现有多模态大模型在面向真实世界的模糊记忆视频检索与片段定位任务上表现不足。

Conclusion: RVMS-Bench与RACLO为评测与提升现实场景下视频检索稳健性提供了系统化基准与方法，有望推动开放域、非结构化条件下的视频检索发展。

Abstract: Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \textbf{1,440 samples} spanning \textbf{20 diverse categories} and \textbf{four duration groups}, sourced from \textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.

</details>


### [6] [AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems](https://arxiv.org/abs/2602.10160)
*Ishan Sahu,Somnath Hazra,Somak Aditya,Soumyajit Dey*

Main category: cs.CV

TL;DR: 论文在CARLA中对端到端自动驾驶的黑盒对抗鲁棒性进行闭环评估，发现先进代理（Transfuser、Interfuser）在三类感知攻击下表现大幅退化，并提出一种轻量级注意力式时空一致性攻击检测器AD^2，具有更高检测性能与效率。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶虽取得进展，但在实际封闭环驾驶中面对传感器与图像级对抗扰动的鲁棒性研究不足；现有评估多为开环或单一攻击，难以反映安全风险，亟需系统化评测与防护方案。

Method: 在CARLA中搭建闭环评测框架，针对视觉感知链路设计三种黑盒攻击：声学致物理模糊（blur）、电磁干扰导致图像畸变、以及添加“幽灵目标”的有界数字扰动；在Transfuser与Interfuser上测试驾驶得分变化；提出基于注意力的AD^2检测器，利用多相机的时空一致性特征进行异常检测，兼顾轻量与实时性，并与现有方法比较。

Result: 三类攻击可使两种先进代理的驾驶得分在最坏情况下下降至原来的1%（最高降幅约99%），暴露严重脆弱性；AD^2在多摄像头场景下相较现有方法实现更高的检测准确率/召回率与更低的计算开销。

Conclusion: 端到端自动驾驶对黑盒物理与数字攻击高度脆弱，存在现实安全隐患；所提AD^2能高效检测多种攻击并提升系统安全性，但仍需结合防御与鲁棒训练以实现更全面的韧性。

Abstract: End-to-end autonomous driving systems have achieved significant progress, yet their adversarial robustness remains largely underexplored. In this work, we conduct a closed-loop evaluation of state-of-the-art autonomous driving agents under black-box adversarial threat models in CARLA. Specifically, we consider three representative attack vectors on the visual perception pipeline: (i) a physics-based blur attack induced by acoustic waves, (ii) an electromagnetic interference attack that distorts captured images, and (iii) a digital attack that adds ghost objects as carefully crafted bounded perturbations on images. Our experiments on two advanced agents, Transfuser and Interfuser, reveal severe vulnerabilities to such attacks, with driving scores dropping by up to 99% in the worst case, raising valid safety concerns. To help mitigate such threats, we further propose a lightweight Attack Detection model for Autonomous Driving systems (AD$^2$) based on attention mechanisms that capture spatial-temporal consistency. Comprehensive experiments across multi-camera inputs on CARLA show that our detector achieves superior detection capability and computational efficiency compared to existing approaches.

</details>


### [7] [ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop](https://arxiv.org/abs/2602.10173)
*Clement Fuji Tsang,Anita Hu,Or Perel,Carsten Kolve,Maria Shugrina*

Main category: cs.CV

TL;DR: 提出一套面向3D高斯点云(3DGS)的交互式选择与分割工具，将用户在2D中的掩码高效传播到3D选择，并配合手动精修，实现对野外采集场景中任意二分类分割与可控局部编辑；在无需额外优化的情况下支持下游应用并优于现有选择方法。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在物理仿真、动画等方面前景广阔，但从真实场景重建中提取可用对象困难，且可控编辑手段匮乏；多数方法偏自动或高层编辑，缺少让用户精准干预的交互式工具链。

Method: 1) 提出快速AI驱动的2D→3D选择传播：将用户标注的2D掩码映射并聚合为3DGS级别的选择；2) 结合灵活的手动选择与分割工具，允许纠错与精修，实现任意二值分割；3) 以自定义视频扩散模型进行用户引导的局部编辑，并通过选择区域限定AI可修改范围；4) 工具可直接用于野外数据，无需额外优化。

Result: 在3DGS选择任务上与SOTA比较取得更优或更稳健的表现；选区驱动的局部编辑在多种真实场景中展示了有效性与可控性，用户可直接控制AI修改的空间范围。

Conclusion: 交互式、可纠错的选择与分割工具弥补了3DGS编辑可控性不足，能在无需再训练/优化的前提下适用于野外场景，并为下游局部编辑与内容创作提供了高效实用的工作流。

Abstract: Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

</details>


### [8] [When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179)
*Jiacheng Hou,Yining Sun,Ruochong Jin,Haochen Han,Fangming Liu,Wai Kin Victor Chan,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 提出视觉到视觉的越狱攻击VJA与安全基准IESBench，实证显示可高效攻破主流图像编辑模型；并给出基于内省式多模态推理的免训练防御，显著提升安全性且开销极低。


<details>
  <summary>Details</summary>
Motivation: 图像编辑正从文本指令转向“视觉提示”交互（标记、箭头、可视化文本等），大幅提升可用性的同时也把攻击面转为“视觉”。现有安全研究多聚焦文本越狱，几乎未系统研究视觉输入本身作为载体的安全风险。作者希望揭示并量化这一新兴威胁，并提供可复现实验基准与实用防御方案。

Method: 1) 设计Vision-Centric Jailbreak Attack(VJA)：仅通过视觉输入编码恶意意图，诱使模型执行违规编辑；2) 构建安全导向的图像编辑评测集IESBench，覆盖多样攻击与场景；3) 在多款商用/开源大模型上系统评测攻击成功率；4) 提出免训练的防御策略：基于内省式多模态推理，在推理时检测并中和潜在恶意视觉指令，无需额外守卫模型且计算开销小。

Result: 在IESBench上，VJA对多款SOTA商用模型取得高攻破率：Nano Banana Pro达80.9%，GPT-Image-1.5达70.1%。所提防御显著降低不对齐/脆弱模型的攻击成功率，使其安全水平接近商用品质，且无需训练与额外守卫模块，推理开销可忽略。

Conclusion: 视觉提示交互带来了新的越狱面：视觉到视觉的攻击可高效绕过现有防护。作者提供了首个系统化基准与实用免训练防御，为现代图像编辑系统的安全性评估与增强提供工具与路径，并提醒社区正视视觉输入层面的对抗威胁。

Abstract: Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.

</details>


### [9] [DEGMC: Denoising Diffusion Models Based on Riemannian Equivariant Group Morphological Convolutions](https://arxiv.org/abs/2602.10221)
*El Hadji S. Diop,Thierno Fall,Mohamed Daoudi*

Main category: cs.CV

TL;DR: 论文将群形态学卷积与欧氏群等变性引入DDPM，用PDE推导的多尺度膨胀/腐蚀与特征对流增强U-Net，在MNIST/RotoMNIST/CIFAR-10上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有DDPM的U-Net主要具平移等变，难以捕获旋转、反射、置换等更广泛欧氏群对称，同时薄结构与非线性几何特征提取不足，限制生成质量与数据效率。

Method: 在黎曼流形上提出“群形态学卷积”，源自一阶Hamilton–Jacobi型PDE的黏性解，对应多尺度形态学膨胀/腐蚀；在方程中加入对流项并用特征线法求解，以更好表征非线性与细薄结构；将该算子嵌入DDPM预测网络以实现对欧氏群（旋转、反射、置换）的等变性。

Result: 在MNIST、RotoMNIST、CIFAR-10数据上，相比标准DDPM基线取得显著提升（文中称 noticeable improvements），表明几何等变与形态学PDE有助于生成质量与稳健性。

Conclusion: 通过PDE驱动的群形态学卷积与对流项，将更广泛欧氏群等变性和几何先验注入DDPM，可更好捕捉非线性与细薄结构，从而提升生成性能；方法表明几何与形态学思想对扩散模型有效。

Abstract: In this work, we address two major issues in recent Denoising Diffusion Probabilistic Models (DDPM): {\bf 1)} geometric key feature extraction and {\bf 2)} network equivariance. Since the DDPM prediction network relies on the U-net architecture, which is theoretically only translation equivariant, we introduce a geometric approach combined with an equivariance property of the more general Euclidean group, which includes rotations, reflections, and permutations. We introduce the notion of group morphological convolutions in Riemannian manifolds, which are derived from the viscosity solutions of first-order Hamilton-Jacobi-type partial differential equations (PDEs) that act as morphological multiscale dilations and erosions. We add a convection term to the model and solve it using the method of characteristics. This helps us better capture nonlinearities, represent thin geometric structures, and incorporate symmetries into the learning process. Experimental results on the MNIST, RotoMNIST, and CIFAR-10 datasets show noticeable improvements compared to the baseline DDPM model.

</details>


### [10] [XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability](https://arxiv.org/abs/2602.10239)
*Dominik Galus,Julia Farganus,Tymoteusz Zapala,Mikołaj Czachorowski,Piotr Borycki,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: 提出XSPLAIN：首个面向3D高斯splatting(3DGS)分类的原型(Prototype)式可解释框架，在不牺牲分类性能的前提下提供直观“以例释例”的解释，并显著优于基线方法的用户偏好。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽在高保真重建广泛应用，但在分类与决策解释方面缺乏可解释性；现有针对点云的显著性图方法无法反映高斯体元的体素一致性与可解释原型，对3DGS解释力不足。

Method: 构建XSPLAIN框架：1) 以体素聚合的PointNet作为骨干提取特征；2) 提出可逆的正交变换，解耦特征通道以实现可解释性，同时严格保持原分类决策边界；3) 采用原型学习，将解释锚定到具有代表性的训练样例，实现“this looks like that”的原型匹配；保证无性能降级。

Result: 用户研究(N=51)显示，XSPLAIN在解释偏好上显著优于基线：被选为“最佳解释”的比例为48.4%，显著性p<0.001；分类性能不降低。代码已开源。

Conclusion: XSPLAIN为3DGS分类提供首个前摄式、原型驱动的可解释性方案，兼顾决策等价与人类可理解解释，提升透明度与用户信任。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a standard for high-fidelity 3D reconstruction, yet its adoption in multiple critical domains is hindered by the lack of interpretability of the generation models as well as classification of the Splats. While explainability methods exist for other 3D representations, like point clouds, they typically rely on ambiguous saliency maps that fail to capture the volumetric coherence of Gaussian primitives. We introduce XSPLAIN, the first ante-hoc, prototype-based interpretability framework designed specifically for 3DGS classification. Our approach leverages a voxel-aggregated PointNet backbone and a novel, invertible orthogonal transformation that disentangles feature channels for interpretability while strictly preserving the original decision boundaries. Explanations are grounded in representative training examples, enabling intuitive ``this looks like that'' reasoning without any degradation in classification performance. A rigorous user study (N=51) demonstrates a decisive preference for our approach: participants selected XSPLAIN explanations 48.4\% of the time as the best, significantly outperforming baselines $(p<0.001)$, showing that XSPLAIN provides transparency and user trust. The source code for this work is available at: https://github.com/Solvro/ml-splat-xai

</details>


### [11] [PMMA: The Polytechnique Montreal Mobility Aids Dataset](https://arxiv.org/abs/2602.10259)
*Qingwu Liu,Nicolas Saunier,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: 提出PMMA助行器行人目标检测/跟踪数据集，并给出多模型基准；YOLOX、Deformable DETR、Faster R-CNN检测最佳，三种跟踪器差异小；数据与代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现实场景中使用轮椅、拐杖、助步器的行人类别多样且与普通行人在外观与互动模式上差异显著，现有公开数据集覆盖不足，影响检测与跟踪算法在无障碍/安全出行等应用中的可靠性与公平性。

Method: 在户外环境组织志愿者使用轮椅、拐杖、助步器进行拍摄与标注，构建包含9类行人的目标检测数据集PMMA（细分拐杖、两类助步器行走/休息、五类轮椅相关类别含推行场景）。在MMDetection框架下实现7种主流检测器（Faster R-CNN、CenterNet、YOLOX、DETR、Deformable DETR、DINO、RT-DETR）与3种多目标跟踪器（ByteTrack、BOT-SORT、OC-SORT）建立统一基准，并公开数据与训练/处理代码。

Result: 检测方面，YOLOX、Deformable DETR、Faster R-CNN表现最佳；多目标跟踪方面，ByteTrack、BOT-SORT、OC-SORT性能差异较小。提供了可重复的基准实验与指标。

Conclusion: PMMA填补助行器行人细粒度检测/跟踪数据空白，为相关研究与应用提供标准基准与开源资源，有助于推动针对无障碍和安全出行场景的鲁棒感知。

Abstract: This study introduces a new object detection dataset of pedestrians using mobility aids, named PMMA. The dataset was collected in an outdoor environment, where volunteers used wheelchairs, canes, and walkers, resulting in nine categories of pedestrians: pedestrians, cane users, two types of walker users, whether walking or resting, five types of wheelchair users, including wheelchair users, people pushing empty wheelchairs, and three types of users pushing occupied wheelchairs, including the entire pushing group, the pusher and the person seated on the wheelchair. To establish a benchmark, seven object detection models (Faster R-CNN, CenterNet, YOLOX, DETR, Deformable DETR, DINO, and RT-DETR) and three tracking algorithms (ByteTrack, BOT-SORT, and OC-SORT) were implemented under the MMDetection framework. Experimental results show that YOLOX, Deformable DETR, and Faster R-CNN achieve the best detection performance, while the differences among the three trackers are relatively small. The PMMA dataset is publicly available at https://doi.org/10.5683/SP3/XJPQUG, and the video processing and model training code is available at https://github.com/DatasetPMMA/PMMA.

</details>


### [12] [Colorimeter-Supervised Skin Tone Estimation from Dermatoscopic Images for Fairness Auditing](https://arxiv.org/abs/2602.10265)
*Marin Benčević,Krešimir Romić,Ivana Hartmann Tolić,Irena Galić*

Main category: cs.CV

TL;DR: 提出一种经验证的皮肤色调估计模型（Fitzpatrick分型的序回归与ITA颜色回归），用于快速标注与公平性审计；发现公开皮镜数据中深色皮肤（V–VI）极少（<1%）；开源代码与模型。


<details>
  <summary>Details</summary>
Motivation: 皮镜AI在不同肤色上的性能不均，但公开数据缺乏可靠肤色标注，限制了公平性评估与改进。需要能在无人工金标准时自动、准确地估计肤色。

Method: 使用神经网络分别进行：1）Fitzpatrick分型的有序回归；2）基于色度学的ITA连续回归。训练目标来自现场采集的人体Fitzpatrick标签与色度计测量（ITA），并进行大规模预训练（合成与真实的皮镜与临床图像）。将模型与人群众包标注、与简单像素均值方法做对比评估。

Result: Fitzpatrick模型与人群众包的一致性相当；ITA模型与色度计ITA高度一致，显著优于像素均值基线。对ISIC 2020与MILK10k自动标注后，Fitzpatrick V–VI少于1%。

Conclusion: 首次在皮镜图像上以色度计为参考验证的肤色估计网络，提供开源工具以便快速标注与偏差审计；证实并支持关于不同肤色群体（尤其深色皮肤）存在临床相关性能差距的证据，并揭示公共数据在深色皮肤上的严重代表性不足。

Abstract: Neural-network-based diagnosis from dermatoscopic images is increasingly used for clinical decision support, yet studies report performance disparities across skin tones. Fairness auditing of these models is limited by the lack of reliable skin-tone annotations in public dermatoscopy datasets. We address this gap with neural networks that predict Fitzpatrick skin type via ordinal regression and the Individual Typology Angle (ITA) via color regression, using in-person Fitzpatrick labels and colorimeter measurements as targets. We further leverage extensive pretraining on synthetic and real dermatoscopic and clinical images. The Fitzpatrick model achieves agreement comparable to human crowdsourced annotations, and ITA predictions show high concordance with colorimeter-derived ITA, substantially outperforming pixel-averaging approaches. Applying these estimators to ISIC 2020 and MILK10k, we find that fewer than 1% of subjects belong to Fitzpatrick types V and VI. We release code and pretrained models as an open-source tool for rapid skin-tone annotation and bias auditing. This is, to our knowledge, the first dermatoscopic skin-tone estimation neural network validated against colorimeter measurements, and it supports growing evidence of clinically relevant performance gaps across skin-tone groups.

</details>


### [13] [ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting](https://arxiv.org/abs/2602.10278)
*Zehua Ma,Hanhui Li,Zhenyu Xie,Xiaonan Luo,Michael Kampffmeyer,Feng Gao,Xiaodan Liang*

Main category: cs.CV

TL;DR: ERGO 提出一种面向单张图像到3D的自适应优化框架，通过在3D Gaussian Splatting训练中进行“过度风险（excess risk）-贝叶斯误差”分解，按视角动态调权并结合几何/纹理感知目标，从而在利用噪声的合成多视图监督时抑制不一致与错位，提升几何与纹理质量，在GSO与OmniObject3D上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 单视图生成3D因遮挡区缺乏几何与纹理信息而病态；利用生成模型提供的辅助视图虽可补充监督，但其几何不一致与纹理错配会在3D重建中被放大，亟需一种能“识别并抑制不可靠监督”的训练机制。

Method: 基于过度风险分解的自适应优化：将3D Gaussian Splatting训练损失分解为（1）过度风险：表示当前参数到最优参数的次优差距；（2）贝叶斯误差：刻画由合成视图引入的不可约噪声。按视角动态估计过度风险并自适应调整损失权重，降低噪声视角影响。同时设计几何感知与纹理感知目标，形成全局（风险加权）与局部（几何/纹理细化）协同优化。

Result: 在Google Scanned Objects与OmniObject3D上，ERGO在几何保真与纹理质量上均优于SOTA，对噪声监督更鲁棒；实验广泛验证其一致增益。

Conclusion: 通过风险分解驱动的动态加权与几何/纹理协同目标，ERGO能有效利用不完美的合成视图监督，抑制噪声传播并稳定提升单视图到3D重建的质量。

Abstract: Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these views inevitably contain geometric inconsistencies and textural misalignments that propagate and amplify artifacts during 3D reconstruction. To effectively harness these imperfect supervisory signals, we propose an adaptive optimization framework guided by excess risk decomposition, termed ERGO. Specifically, ERGO decomposes the optimization losses in 3D Gaussian splatting into two components, i.e., excess risk that quantifies the suboptimality gap between current and optimal parameters, and Bayes error that models the irreducible noise inherent in synthesized views. This decomposition enables ERGO to dynamically estimate the view-specific excess risk and adaptively adjust loss weights during optimization. Furthermore, we introduce geometry-aware and texture-aware objectives that complement the excess-risk-derived weighting mechanism, establishing a synergistic global-local optimization paradigm. Consequently, ERGO demonstrates robustness against supervision noise while consistently enhancing both geometric fidelity and textural quality of the reconstructed 3D content. Extensive experiments on the Google Scanned Objects dataset and the OmniObject3D dataset demonstrate the superiority of ERGO over existing state-of-the-art methods.

</details>


### [14] [A Low-Rank Defense Method for Adversarial Attack on Diffusion Models](https://arxiv.org/abs/2602.10319)
*Jiaxuan Zhu,Siyu Huang*

Main category: cs.CV

TL;DR: 提出LoRD低秩防御：利用LoRA模块与合并/平衡策略检测并抵御对潜空间扩散模型的对抗样本，在人脸与景观数据上显著优于基线且保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 对扩散模型及其微调过程的对抗攻击快速发展，实际落地受威胁，亟需高效、实用的防御策略，既能识别攻击又不牺牲生成质量。

Method: 在潜空间扩散模型上引入低秩适配（LoRA）模块，结合“合并（merging）”思想与一个平衡参数，形成LoRD模块用于检测与抵御对抗样本；据此构建完整防御流水线，将学习到的LoRD应用于模型微调，使模型在包含对抗与干净样本的训练中仍能稳健生成。

Result: 在人脸与风景图像实验中，相较于多种基线，LoRD在防御效果上显著更好，同时保持生成质量高。

Conclusion: LoRD为LDM提供了一种高效、可实践的对抗防御方案：通过低秩适配与模块合并-平衡机制实现攻击检测与抵御，并在多数据集上验证其优越性与生成质量的保持。

Abstract: Recently, adversarial attacks for diffusion models as well as their fine-tuning process have been developed rapidly. To prevent the abuse of these attack algorithms from affecting the practical application of diffusion models, it is critical to develop corresponding defensive strategies. In this work, we propose an efficient defensive strategy, named Low-Rank Defense (LoRD), to defend the adversarial attack on Latent Diffusion Models (LDMs). LoRD introduces the merging idea and a balance parameter, combined with the low-rank adaptation (LoRA) modules, to detect and defend the adversarial samples. Based on LoRD, we build up a defense pipeline that applies the learned LoRD modules to help diffusion models defend against attack algorithms. Our method ensures that the LDM fine-tuned on both adversarial and clean samples can still generate high-quality images. To demonstrate the effectiveness of our approach, we conduct extensive experiments on facial and landscape images, and our method shows significantly better defense performance compared to the baseline methods.

</details>


### [15] [Flow Matching with Uncertainty Quantification and Guidance](https://arxiv.org/abs/2602.10326)
*Juyeop Han,Lukas Lao Beyer,Sertac Karaman*

Main category: cs.CV

TL;DR: 提出UA-Flow，在流匹配模型中联合预测速度场与异方差不确定性，用以评估与提升生成样本质量。


<details>
  <summary>Details</summary>
Motivation: 采样式生成模型（如Flow Matching）虽成功，但仍会产出质量不稳或退化样本；缺少对“单个样本可靠性”的度量与利用机制，限制了后续筛选与引导生成。

Method: 在标准Flow Matching上增加不确定性头，联合回归速度场与每步（异方差）噪声/方差；将速度不确定性沿流动微分方程传播，得到逐样本不确定性评分；把该评分用于两类引导：基于分类器的uncertainty-aware guidance与无分类器的guidance，以抑制高不确定性方向、放大低不确定性高置信区域，从而在采样过程中自适应调整。

Result: 在图像生成任务上，UA-Flow得到的不确定性与样本保真度的相关性高于基线（更好地预测好坏样本）；使用不确定性引导的采样显著提升生成质量（优于未引导与其他不确定性基线）。

Conclusion: 给Flow Matching注入可学习的异方差不确定性并在生成过程中加以利用，既能提供可靠样本质控信号，也能通过不确定性引导进一步提升生成图像质量。

Abstract: Despite the remarkable success of sampling-based generative models such as flow matching, they can still produce samples of inconsistent or degraded quality. To assess sample reliability and generate higher-quality outputs, we propose uncertainty-aware flow matching (UA-Flow), a lightweight extension of flow matching that predicts the velocity field together with heteroscedastic uncertainty. UA-Flow estimates per-sample uncertainty by propagating velocity uncertainty through the flow dynamics. These uncertainty estimates act as a reliability signal for individual samples, and we further use them to steer generation via uncertainty-aware classifier guidance and classifier-free guidance. Experiments on image generation show that UA-Flow produces uncertainty signals more highly correlated with sample fidelity than baseline methods, and that uncertainty-guided sampling further improves generation quality.

</details>


### [16] [Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks](https://arxiv.org/abs/2602.10343)
*Rafael-Petruţ Gardoş*

Main category: cs.CV

TL;DR: 论文提出面向政治领域的深度伪造图像检测框架，重点把“预测不确定性”纳入判别与决策流程，通过多种随机化与校准方法评估与提升可靠性，并用决策导向指标展示在高风险场景下的不确定性何时真正有用。


<details>
  <summary>Details</summary>
Motivation: 政治类深伪的逼真度与传播速度提升，危及信息完整性和公共信任。现有检测器多给出单点预测且缺乏“何时不可信”的信号，难以支撑高风险舆情治理与取证决策，因此需要一个能量化并校准不确定性的检测方案与评估框架。

Method: 构建政治主题二分类图像数据集（由大型实-伪公共语料经确定性元数据筛选得到）；选用ResNet-18与EfficientNet-B4全量微调；比较确定性推理与多种不确定性策略：单次随机推理、MC Dropout多次前向、温度缩放、模型集成等；在经验性、决策导向的可靠性框架下，以校准质量、适当评分规则、与错误对齐度（全局与置信条件化）评估不确定性；报告ROC-AUC、阈值混淆矩阵、校准指标与生成器失配的OOD性能；并进行系统的置信带分析。

Result: 不确定性感知与概率校准能提升深伪检测在政策/运营场景中的可用性：相比单点预测，经过温度缩放、MC Dropout或集成得到的概率输出在校准与风险区分上更好；在生成器失配的OOD条件下，带不确定性的模型更能暴露自身脆弱性；置信带分析显示在特定置信区间内不确定性信号能显著预测错误，但也存在边际收益递减与与置信度重合的区段。

Conclusion: 将可解释、可校准的不确定性纳入深伪检测，可支持风险敏感的审核与取证策略；不确定性并非处处奏效，其价值依赖数据分布与置信区间，需要结合阈值策略与OOD监测使用。论文界定了优势与边界，并为政治场景部署提供实践指引。

Abstract: Recent advances in generative image models have enabled the creation of highly realistic political deepfakes, posing risks to information integrity, public trust, and democratic processes. While automated deepfake detectors are increasingly deployed in moderation and investigative pipelines, most existing systems provide only point predictions and fail to indicate when outputs are unreliable, being an operationally critical limitation in high-stakes political contexts. This work investigates conditional, uncertainty-aware political deepfake detection using stochastic convolutional neural networks within an empirical, decision-oriented reliability framework. Rather than treating uncertainty as a purely Bayesian construct, it is evaluated through observable criteria, including calibration quality, proper scoring rules, and its alignment with prediction errors under both global and confidence-conditioned analyses. A politically focused binary image dataset is constructed via deterministic metadata filtering from a large public real-synthetic corpus. Two pretrained CNN backbones (ResNet-18 and EfficientNet-B4) are fully fine-tuned for classification. Deterministic inference is compared with single-pass stochastic prediction, Monte Carlo dropout with multiple forward passes, temperature scaling, and ensemble-based uncertainty surrogates. Evaluation reports ROC-AUC, thresholded confusion matrices, calibration metrics, and generator-disjoint out-of-distribution performance. Results demonstrate that calibrated probabilistic outputs and uncertainty estimates enable risk-aware moderation policies. A systematic confidence-band analysis further clarifies when uncertainty provides operational value beyond predicted confidence, delineating both the benefits and limitations of uncertainty-aware deepfake detection in political settings.

</details>


### [17] [Monte Carlo Maximum Likelihood Reconstruction for Digital Holography with Speckle](https://arxiv.org/abs/2602.10344)
*Xi Chen,Arian Maleki,Shirin Jalali*

Main category: cs.CV

TL;DR: 提出PGD-MC，在不显式矩阵求逆的情况下实现可扩展的MLE重建，准确建模有限孔径，显著提升全息重建质量与速度，并优于现有PnP方法。


<details>
  <summary>Details</summary>
Motivation: 基于相干成像的散斑呈乘性噪声，MLE虽原理上合适但在有限孔径数字全息中因需高维矩阵求逆而计算代价高，限制了物理精确孔径建模的应用。

Method: 利用随机线性代数与结构化感知矩阵特性，通过共轭梯度近似似然梯度并进行投影梯度下降，采用蒙特卡洛估计避免显式求逆；引入三种去噪器作为正则以形成灵活的MLE框架。

Result: PGD-MC在多种物理精确孔径模型下具有鲁棒性，重建质量与计算效率大幅提升，能扩展到高分辨率数字全息；在大量实验中，较以往PnP基迭代重建在准确性和速度上均有一致优势。

Conclusion: PGD-MC使MLE在有限孔径数字全息中变得可扩展且实用，既保留物理精确孔径建模，又获得更高质量与更快重建，为相干成像去散斑提供通用有效的重建框架。

Abstract: In coherent imaging, speckle is statistically modeled as multiplicative noise, posing a fundamental challenge for image reconstruction. While maximum likelihood estimation (MLE) provides a principled framework for speckle mitigation, its application to coherent imaging system such as digital holography with finite apertures is hindered by the prohibitive cost of high-dimensional matrix inversion, especially at high resolutions. This computational burden has prevented the use of MLE-based reconstruction with physically accurate aperture modeling. In this work, we propose a randomized linear algebra approach that enables scalable MLE optimization without explicit matrix inversions in gradient computation. By exploiting the structural properties of sensing matrix and using conjugate gradient for likelihood gradient evaluation, the proposed algorithm supports accurate aperture modeling without the simplifying assumptions commonly imposed for tractability. We term the resulting method projected gradient descent with Monte Carlo estimation (PGD-MC). The proposed PGD-MC framework (i) demonstrates robustness to diverse and physically accurate aperture models, (ii) achieves substantial improvements in reconstruction quality and computational efficiency, and (iii) scales effectively to high-resolution digital holography. Extensive experiments incorporating three representative denoisers as regularization show that PGD-MC provides a flexible and effective MLE-based reconstruction framework for digital holography with finite apertures, consistently outperforming prior Plug-and-Play model-based iterative reconstruction methods in both accuracy and speed. Our code is available at: https://github.com/Computational-Imaging-RU/MC_Maximum_Likelihood_Digital_Holography_Speckle.

</details>


### [18] [Comp2Comp: Open-Source Software with FDA-Cleared Artificial Intelligence Algorithms for Computed Tomography Image Analysis](https://arxiv.org/abs/2602.10364)
*Adrit Rao,Malte Jensen,Andrea T. Fisher,Louis Blankemeier,Pauline Berens,Arash Fereydooni,Seth Lirette,Eren Alkan,Felipe C. Kitamura,Juan M. Zambrano Chaves,Eduardo Reis,Arjun Desai,Marc H. Willis,Jason Hom,Andrew Johnston,Leon Lenchik,Robert D. Boutin,Eduardo M. J. M. Farina,Augusto S. Serpa,Marcelo S. Takahashi,Jordan Perchik,Steven A. Rothenberg,Jamie L. Schroeder,Ross Filice,Leonardo K. Bittencourt,Hari Trivedi,Marly van Assen,John Mongan,Kimberly Kallianos,Oliver Aalami,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: 本文介绍并验证两个完全开源且获 FDA 510(k) 批准的深度学习管线（AAQ 与 BMD），用于在CT机会成像中自动评估腹主动脉瘤大小与骨密度，表现达到可临床使用的准确度。


<details>
  <summary>Details</summary>
Motivation: 机会成像可从既有CT中额外提取影像生物标志物而不增加成本与辐射，但现有开源算法缺少严谨验证、商用算法不透明，导致临床落地失败风险。需要既透明又经过监管验证的高性能方案。

Method: 开发并开源Comp2Comp包中的两条深度学习管线：AAQ对腹主动脉进行分割并计算最大直径；BMD对椎体进行分割并估计小梁骨密度，进行骨质疏松风险二分类。多中心外部验证：AAQ在4家机构258例（富集AAA）上与放射科医师测量比对；BMD在4家机构371例上与同期DXA金标准比对（低 vs 正常）。

Result: AAQ最大直径平均绝对误差1.57 mm（95% CI 1.38–1.80）。BMD分类灵敏度81.0%（95% CI 74.0–86.8%）、特异度78.4%（95% CI 72.3–83.7%）。

Conclusion: AAQ与BMD达临床可用精度；作为完全开源且获FDA 510(k) 批准的算法，提升了监管流程透明度，便于医院在正式试点前测试，并为研究者提供高水准参考实现。

Abstract: Artificial intelligence allows automatic extraction of imaging biomarkers from already-acquired radiologic images. This paradigm of opportunistic imaging adds value to medical imaging without additional imaging costs or patient radiation exposure. However, many open-source image analysis solutions lack rigorous validation while commercial solutions lack transparency, leading to unexpected failures when deployed. Here, we report development and validation for two of the first fully open-sourced, FDA-510(k)-cleared deep learning pipelines to mitigate both challenges: Abdominal Aortic Quantification (AAQ) and Bone Mineral Density (BMD) estimation are both offered within the Comp2Comp package for opportunistic analysis of computed tomography scans. AAQ segments the abdominal aorta to assess aneurysm size; BMD segments vertebral bodies to estimate trabecular bone density and osteoporosis risk. AAQ-derived maximal aortic diameters were compared against radiologist ground-truth measurements on 258 patient scans enriched for abdominal aortic aneurysms from four external institutions. BMD binary classifications (low vs. normal bone density) were compared against concurrent DXA scan ground truths obtained on 371 patient scans from four external institutions. AAQ had an overall mean absolute error of 1.57 mm (95% CI 1.38-1.80 mm). BMD had a sensitivity of 81.0% (95% CI 74.0-86.8%) and specificity of 78.4% (95% CI 72.3-83.7%). Comp2Comp AAQ and BMD demonstrated sufficient accuracy for clinical use. Open-sourcing these algorithms improves transparency of typically opaque FDA clearance processes, allows hospitals to test the algorithms before cumbersome clinical pilots, and provides researchers with best-in-class methods.

</details>


### [19] [HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images](https://arxiv.org/abs/2602.10425)
*Yilin Yang,Zhenghui Guo,Yuke Wang,Omprakash Gnawali,Sheng Di,Chengming Zhang*

Main category: cs.CV

TL;DR: 提出通过合成“诱发幻觉图像”(HII)来揭示并评测VLM受语言偏置导致的场景条件幻觉，并用其构建偏好数据进行细粒度对齐，最终显著降低幻觉且保持通用能力，在标准基准上最高提升38%。


<details>
  <summary>Details</summary>
Motivation: 现有缓解VLM幻觉的方法多从对齐或后处理入手，但忽视了由语言先验主导的系统性幻觉模式（例如在特定场景中倾向提到典型物体）。缺少能专门触发并量化此类偏置幻觉的数据与基准，限制了诊断与改进。

Method: 1) 设计流程合成“诱发幻觉图像”(HIIs)：在典型场景中有意移除/遮蔽关键物体以打破视觉证据，同时保留场景线索；2) 用HIIs揭示并验证“场景条件”幻觉模式（模型仍会提及被遮蔽的典型物体）；3) 构建Masked-Object-Hallucination (MOH)基准，系统评测主流对齐框架在该模式下的稳健性；4) 基于HIIs生成细粒度偏好数据，进行对齐训练/偏好优化，抑制场景诱导的语言偏置。

Result: 实验表明：利用HIIs的对齐方法显著降低VLM在场景条件下的物体幻觉，同时保持/不损害通用能力；在标准幻觉评测上较现SOTA最高提升达38%，在MOH基准上也展现更强鲁棒性。

Conclusion: 通过面向语言偏置诱发的场景条件幻觉，提出HII合成与MOH基准以揭示并量化该模式，并用其构建高质量偏好数据实现细粒度对齐，有效抑制幻觉而不牺牲整体性能，提供了诊断—评测—对齐的一体化范式。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable success across diverse multimodal tasks but remain vulnerable to hallucinations rooted in inherent language bias. Despite recent progress, existing hallucination mitigation methods often overlook the underlying hallucination patterns driven by language bias. In this work, we design a novel pipeline to accurately synthesize Hallucination-Inducing Images (HIIs). Using synthesized HIIs, we reveal a consistent scene-conditioned hallucination pattern: models tend to mention objects that are highly typical of the scene even when visual evidence is removed. To quantify the susceptibility of VLMs to this hallucination pattern, we establish the Masked-Object-Hallucination (MOH) benchmark to rigorously evaluate existing state-of-the-art alignment frameworks. Finally, we leverage HIIs to construct high-quality preference datasets for fine-grained alignment. Experimental results demonstrate that our approach effectively mitigates hallucinations while preserving general model capabilities. Specifically, our method achieves up to a 38% improvement over the current state-of-the-art on standard hallucination benchmarks.

</details>


### [20] [Towards Remote Sensing Change Detection with Neural Memory](https://arxiv.org/abs/2602.10491)
*Zhenyu Yang,Gensheng Pei,Yazhou Yao,Tianfei Zhou,Lizhong Ding,Fumin Shen*

Main category: cs.CV

TL;DR: 提出ChangeTitans框架，用Titans记忆+分段局部注意力的VTitans主干、分层Adapter和跨时相CBAM融合，兼顾长程依赖与效率，在四个数据集上达SOTA（如LEVIR-CD IoU 84.36%、F1 91.52%）。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测算法难以在保持计算可控的同时建模长程依赖；Transformer虽能建模全局但复杂度二次方，线性注意力又常捕捉不到复杂时空关系。需要一种既高效又能刻画全局时空关系的新架构。

Method: 1) VTitans：首个基于Titans的视觉骨干，将可学习神经记忆与分段局部注意力结合，增强长程依赖建模并降计算开销；2) VTitans-Adapter：分层、多尺度特征细化与对齐，提升各层信息融合；3) TS-CBAM：两流跨时相注意力模块，抑制伪变化、提升检测精度。整体构建端到端变化检测框架ChangeTitans。

Result: 在LEVIR-CD、WHU-CD、LEVIR-CD+、SYSU-CD四个基准上取得SOTA表现；在LEVIR-CD上IoU达84.36%，F1达91.52%，同时保持竞争性的计算成本。

Conclusion: 将Titans理念引入视觉变化检测可在不显著增加计算开销的情况下有效捕捉长程与跨时相依赖；所提ChangeTitans框架在多基准上验证了优越性，适合作为高效、精确的遥感变化检测新范式。

Abstract: Remote sensing change detection is essential for environmental monitoring, urban planning, and related applications. However, current methods often struggle to capture long-range dependencies while maintaining computational efficiency. Although Transformers can effectively model global context, their quadratic complexity poses scalability challenges, and existing linear attention approaches frequently fail to capture intricate spatiotemporal relationships. Drawing inspiration from the recent success of Titans in language tasks, we present ChangeTitans, the Titans-based framework for remote sensing change detection. Specifically, we propose VTitans, the first Titans-based vision backbone that integrates neural memory with segmented local attention, thereby capturing long-range dependencies while mitigating computational overhead. Next, we present a hierarchical VTitans-Adapter to refine multi-scale features across different network layers. Finally, we introduce TS-CBAM, a two-stream fusion module leveraging cross-temporal attention to suppress pseudo-changes and enhance detection accuracy. Experimental evaluations on four benchmark datasets (LEVIR-CD, WHU-CD, LEVIR-CD+, and SYSU-CD) demonstrate that ChangeTitans achieves state-of-the-art results, attaining \textbf{84.36\%} IoU and \textbf{91.52\%} F1-score on LEVIR-CD, while remaining computationally competitive.

</details>


### [21] [End-to-End LiDAR optimization for 3D point cloud registration](https://arxiv.org/abs/2602.10492)
*Siddhant Katyan,Marc-André Gardner,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 提出一种自适应LiDAR感知框架，将传感器参数与点云配准超参数联合优化，通过把配准反馈纳入采集闭环，在CARLA仿真中优于固定参数基线并保持一定泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有点云配准通常在固定LiDAR配置与离线数据上进行，导致采集不最优，且需要大量采样、去噪与参数调优，带来精度与计算的折中与开销。亟需一种能根据下游任务动态调整采集策略的方法。

Method: 构建感知—配准闭环：在采集中可调节LiDAR参数（如扫描频率、分辨率/通道数、视场、功率等），并与配准模块的超参数（采样率、滤波/去噪、特征提取与匹配阈值、迭代步数等）联合优化；以配准反馈（如配准误差、收敛速度、鲁棒性指标）作为目标或奖励，动态平衡点密度、噪声与稀疏性，实现在线自适应配置。

Result: 在CARLA仿真中，相较固定参数基线取得更高的配准精度与更低的计算开销，并在未见场景上保持一定泛化能力。

Conclusion: 自适应LiDAR通过将任务反馈纳入采集闭环，可联合优化采集与配准，提升精度与效率，具备在自动驾驶与机器人等3D感知场景中的应用潜力。

Abstract: LiDAR sensors are a key modality for 3D perception, yet they are typically designed independently of downstream tasks such as point cloud registration. Conventional registration operates on pre-acquired datasets with fixed LiDAR configurations, leading to suboptimal data collection and significant computational overhead for sampling, noise filtering, and parameter tuning. In this work, we propose an adaptive LiDAR sensing framework that dynamically adjusts sensor parameters, jointly optimizing LiDAR acquisition and registration hyperparameters. By integrating registration feedback into the sensing loop, our approach optimally balances point density, noise, and sparsity, improving registration accuracy and efficiency. Evaluations in the CARLA simulation demonstrate that our method outperforms fixed-parameter baselines while retaining generalization abilities, highlighting the potential of adaptive LiDAR for autonomous perception and robotic applications.

</details>


### [22] [Characterizing and Optimizing the Spatial Kernel of Multi Resolution Hash Encodings](https://arxiv.org/abs/2602.10495)
*Tianxiang Dai,Jonathan Fan*

Main category: cs.CV

TL;DR: 论文从物理系统视角解析多分辨率哈希编码（MHE），以点扩散函数（PSF）刻画其空间行为，给出无碰撞近似的封闭形式，揭示各层网格导致的各向异性与对数型空间衰减；指出有效分辨率受FWHM与N_avg支配而非N_max，并分析有限哈希容量引起的碰撞噪声；据此提出通过分层旋转输入的R-MHE以缓解各向异性、保持效率与参数量。


<details>
  <summary>Details</summary>
Motivation: MHE广泛用于神经场但空间行为缺乏物理化解析，超参数多凭经验选取；需要一个能量化分辨率、保真度与碰撞影响的理论框架，以指导设计与优化。

Method: 以PSF/格林函数视角分析MHE：推导无碰撞情形下PSF的封闭近似，定义与估计空间带宽（FWHM）、平均分辨率N_avg；研究优化过程造成FWHM展宽与其对有效分辨率的主导作用；在有限哈希容量下建模碰撞为斑点噪声并量化对SNR的影响；据此提出在各分辨率层对输入施加不同旋转的R-MHE以减轻各向异性。

Result: 得到碰撞自由PSF的解析近似，显示网格诱发各向异性及对数型空间剖面；证明理想带宽（FWHM）由N_avg决定，且实际FWHM因优化展宽，导致模型有效分辨率受N_avg而非N_max主导；分析碰撞对SNR的劣化机制；R-MHE在不增加参数与计算的情况下显著缓解各向异性。

Conclusion: 以物理化PSF框架系统刻画MHE的空间分辨率与噪声特性，纠正仅依赖N_max的直觉，强调N_avg与FWHM对有效分辨率的决定性；通过R-MHE提供切实可行的结构改进路径，在保持效率的同时提升各向同性与稳健性。

Abstract: Multi-Resolution Hash Encoding (MHE), the foundational technique behind Instant Neural Graphics Primitives, provides a powerful parameterization for neural fields. However, its spatial behavior lacks rigorous understanding from a physical systems perspective, leading to reliance on heuristics for hyperparameter selection. This work introduces a novel analytical approach that characterizes MHE by examining its Point Spread Function (PSF), which is analogous to the Green's function of the system. This methodology enables a quantification of the encoding's spatial resolution and fidelity. We derive a closed-form approximation for the collision-free PSF, uncovering inherent grid-induced anisotropy and a logarithmic spatial profile. We establish that the idealized spatial bandwidth, specifically the Full Width at Half Maximum (FWHM), is determined by the average resolution, $N_{\text{avg}}$. This leads to a counterintuitive finding: the effective resolution of the model is governed by the broadened empirical FWHM (and therefore $N_{\text{avg}}$), rather than the finest resolution $N_{\max}$, a broadening effect we demonstrate arises from optimization dynamics. Furthermore, we analyze the impact of finite hash capacity, demonstrating how collisions introduce speckle noise and degrade the Signal-to-Noise Ratio (SNR). Leveraging these theoretical insights, we propose Rotated MHE (R-MHE), an architecture that applies distinct rotations to the input coordinates at each resolution level. R-MHE mitigates anisotropy while maintaining the efficiency and parameter count of the original MHE. This study establishes a methodology based on physical principles that moves beyond heuristics to characterize and optimize MHE.

</details>


### [23] [The Garbage Dataset (GD): A Multi-Class Image Benchmark for Automated Waste Segregation](https://arxiv.org/abs/2602.10500)
*Suman Kunwar*

Main category: cs.CV

TL;DR: 提出一个面向自动垃圾分类的公开图像数据集GD，含10类、13,348张标注图片；系统分析其类不平衡与背景复杂度，并用多种SOTA模型做基线，EfficientNetV2S最佳（Acc 96.19%、F1 0.96），同时报告碳排放权衡；数据集公开以促进可持续研究与实际部署。


<details>
  <summary>Details</summary>
Motivation: 现实垃圾分类需要自动化，但现有数据集在类别覆盖、规模、标注质量与现实复杂度（背景、光照、噪声）方面不足，也缺少对环境成本（计算碳排放）的衡量，因此需要一个更贴近真实场景、并可用于公平比较与可持续评估的基准。

Method: - 数据采集：来自DWaste移动应用与经筛选的网页资源。
- 标注与质量控制：校验和、离群检测清洗；统计类分布；用PCA/t-SNE评估可分性；用熵与显著性度量背景复杂度与亮度差异。
- 基线实验：在GD上训练/评估EfficientNetV2M/S、MobileNet、ResNet50/101；报告准确率、F1等，并估算训练/推理的碳排放。

Result: EfficientNetV2S在该数据集上表现最佳（Accuracy 96.19%、F1 0.96），碳成本中等；揭示数据集存在类不平衡、某些高离群类别（塑料、纸板、纸张）与亮度变化显著等特性。

Conclusion: GD是一个贴近真实场景的垃圾分类基准，既能推动模型研究，也暴露了实际部署需解决的问题：类不平衡、复杂背景、亮度变化与模型碳排放权衡。数据集已公开，鼓励在可持续目标下开展进一步研究与方法改进（如重采样/重加权、背景鲁棒性增强、轻量化与能效优化）。

Abstract: This study introduces the Garbage Dataset (GD), a publicly available image dataset designed to advance automated waste segregation through machine learning and computer vision. It's a diverse dataset covering 10 common household waste categories: metal, glass, biological, paper, battery, trash, cardboard, shoes, clothes, and plastic. The dataset comprises 13,348 labeled images collected through multiple methods, including DWaste mobile app and curated web sources. Methods included rigorous validation through checksums and outlier detection, analysis of class imbalance and visual separability via PCA/t-SNE, and assessment of background complexity using entropy and saliency measures. The dataset was benchmarked using state-of-the-art deep learning models (EfficientNetV2M, EfficientNetV2S, MobileNet, ResNet50, ResNet101) evaluated on performance metrics and operational carbon emissions. Experiment results indicate EfficientNetV2S achieved the highest performance with 96.19% accuracy and a 0.96 F1-score, though with a moderate carbon cost. Analysis revealed inherent dataset characteristics including class imbalance, a skew toward high-outlier classes (plastic, cardboard, paper), and brightness variations that require consideration. The main conclusion is that GD provides a valuable, real-world benchmark for waste classification research while highlighting important challenges such as class imbalance, background complexity, and environmental trade-offs in model selection that must be addressed for practical deployment. The dataset is publicly released to support further research in environmental sustainability applications.

</details>


### [24] [Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation](https://arxiv.org/abs/2602.10508)
*Salma J. Ahmed,Emad A. Mohammed,Azam Asilian Bidgoli*

Main category: cs.CV

TL;DR: 提出Med-SegLens：用稀疏自编码器将分割模型激活分解为可解释潜在特征，并跨架构/数据集对齐，从而诊断失败与缓解数据集漂移；通过对潜在单元的有针对性干预，无需再训练即可在大多数失败案例中恢复性能（Dice由39.4%提升到74.2%）。


<details>
  <summary>Details</summary>
Motivation: 医学影像分割模型虽性能强，但不可解释性高，难以定位错误、理解数据集迁移（人群/机构差异）与进行可控干预；需要一种机制化、可操作的工具来比较不同模型与数据集的内部表示，从而识别导致失败与漂移的根因并进行快速修复。

Method: 在SegFormer与U-Net上训练稀疏自编码器，对中间激活进行稀疏因子分解，得到可解释潜在特征；进行跨架构与跨数据集（健康、成人、儿科、撒哈拉以南非洲胶质瘤队列）潜在空间对齐，识别共享与特定人群的潜在单元；将这些潜在单元作为干预接口，对导致错误的“因果瓶颈”潜在进行定向调制以纠错与适配，无需回传训练。

Result: 发现稳定的共享表示“骨干”，而数据集漂移主要源于对人群特异潜在的不同依赖；实验证明潜在单元是分割失败的因果瓶颈；对潜在层面的定向干预可在70%的失败样本中纠正错误，使跨数据集Dice从39.4%提升到74.2%。

Conclusion: 潜在层级的模型对比与干预是一种实用、可机制化的方法，可用于诊断分割模型失败并缓解数据集漂移，无需重新训练即可显著提升跨数据集泛化表现。

Abstract: Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Saharan African glioma cohorts, we identify a stable backbone of shared representations, while dataset shift is driven by differential reliance on population-specific latents. We show that these latents act as causal bottlenecks for segmentation failures, and that targeted latent-level interventions can correct errors and improve cross-dataset adaption without retraining, recovering performance in 70% of failure cases and improving Dice score from 39.4% to 74.2%. Our results demonstrate that latent-level model diffing provides a practical and mechanistic tool for diagnosing failures and mitigating dataset shift in segmentation models.

</details>


### [25] [1%>100%: High-Efficiency Visual Adapter with Complex Linear Projection Optimization](https://arxiv.org/abs/2602.10513)
*Dongshuo Yin,Xue Yang,Deng-Ping Fan,Shi-Min Hu*

Main category: cs.CV

TL;DR: 提出CoLin：一种用于视觉基础模型适配的复数低秩适配器，仅引入约1%参数，通过特定损失解决低秩复合矩阵训练收敛难题，在检测、分割、分类与旋转检测等任务上超过全量微调与传统delta-tuning。


<details>
  <summary>Details</summary>
Motivation: 全量微调在视觉基础模型上的计算与存储成本高、效率低；LLM领域的delta-tuning优势难以直接迁移到视觉模型。需要一种既高效又性能强的适配机制，降低参数与训练成本，同时保持甚至提升下游任务性能。

Method: 提出CoLin（Complex Linear Projection Optimization）适配器：1）结构上使用新颖的低秩复数适配器，仅向主干引入约1%额外参数；2）理论上分析低秩复合矩阵在训练中的严重收敛问题；3）为此设计了定制损失以缓解/消除收敛障碍，从而稳定高效训练；4）在多种视觉任务场景中进行评测。

Result: 在目标检测、分割、图像分类与遥感旋转目标检测等任务中，CoLin以约1%参数首次整体优于全量微调与经典delta-tuning方法，展现更高的性能与适配效率。

Conclusion: CoLin为视觉基础模型提供了一种参数高效且性能优越的适配方案：通过复数低秩结构与定制损失克服训练收敛瓶颈，实现以极少参数超越全量微调与传统delta-tuning，适合实际部署；代码已开源。

Abstract: Deploying vision foundation models typically relies on efficient adaptation strategies, whereas conventional full fine-tuning suffers from prohibitive costs and low efficiency. While delta-tuning has proven effective in boosting the performance and efficiency of LLMs during adaptation, its advantages cannot be directly transferred to the fine-tuning pipeline of vision foundation models. To push the boundaries of adaptation efficiency for vision tasks, we propose an adapter with Complex Linear Projection Optimization (CoLin). For architecture, we design a novel low-rank complex adapter that introduces only about 1% parameters to the backbone. For efficiency, we theoretically prove that low-rank composite matrices suffer from severe convergence issues during training, and address this challenge with a tailored loss. Extensive experiments on object detection, segmentation, image classification, and rotated object detection (remote sensing scenario) demonstrate that CoLin outperforms both full fine-tuning and classical delta-tuning approaches with merely 1% parameters for the first time, providing a novel and efficient solution for deployment of vision foundation models. We release the code on https://github.com/DongshuoYin/CoLin.

</details>


### [26] [3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars](https://arxiv.org/abs/2602.10516)
*Zhongju Wang,Zhenhong Sun,Beier Wang,Yifu Wang,Daoyi Dong,Huadong Mo,Hongdong Li*

Main category: cs.CV

TL;DR: 3DXTalker 提出一种面向表达性的三维说话人头像生成系统，通过可扩展身份建模、音频富表示与可控空间动态，在同一框架内同时提升唇形同步、情绪表达与头部姿态生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于身份样本稀缺、音频表示单一、显式可控性弱，难以同时保证身份一致性、精准唇动、细腻情绪与自然头动，从而制约头像的整体“可表达性”。

Method: (1) 数据策展与解耦表征：构建2D→3D的数据策展流水线，实现可扩展的身份建模并提升跨身份泛化；(2) 音频富表示：在常规语音嵌入外，引入逐帧振幅与情绪线索，细化对口型与表情的驱动；(3) 统一动力学建模：采用基于flow-matching的Transformer，将多模态线索对齐为连贯面部动态；(4) 空间动态与可控性：生成自然头姿，并通过提示式条件实现风格化控制。

Result: 在多项实验中，该方法在唇形同步、情绪表达与头姿动态三方面均优于现有方法，展现更强的身份泛化与可控性，形成统一、稳健的3D说话人生成框架。

Conclusion: 3DXTalker 通过数据策展+解耦身份、音频富表示与flow-matching Transformer，将口型、情绪与头动一体化建模，兼顾表达力与可控性，显著推进3D说话人头像生成的综合表现。

Abstract: Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.

</details>


### [27] [MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps](https://arxiv.org/abs/2602.10518)
*Sharat Bhat,Harshita Khandelwal,Tushar Kataria,Vivek Gupta*

Main category: cs.CV

TL;DR: MapVerse 提出一个以真实世界地图为基础的大规模VLM基准，含1,025张地图与11,837个人工问答，覆盖10类地图与多种问题类型；系统评测10个SOTA模型并做细粒度分析，发现模型在分类式任务上尚可，但在复杂空间推理上显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于地图的VLM评测集范围窄、依赖人工生成内容、缺乏真实地理与多维推理深度，难以全面检验模型对空间关系、视觉线索与领域知识的综合推理能力。

Method: 构建MapVerse：从真实地图收集并清洗样本，人工撰写多类别问题与答案（跨10类地图、多个问题维度）；设计评测协议，对10个开源与闭源SOTA模型进行整体与细粒度分项评估，并分析影响推理的视觉因素。

Result: 建立基线表现并量化推理差距：当前VLM在简单/分类型任务上竞争力尚可，但在需要复杂空间推理、跨图层理解与情境整合的高阶任务上普遍失败。

Conclusion: 真实地图上的多模态与空间推理仍是VLM短板。MapVerse为系统评测与后续方法改进提供基准，提示未来需加强空间关系建模、地理先验与结构化视觉-语言对齐。

Abstract: Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.

</details>


### [28] [RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images](https://arxiv.org/abs/2602.10546)
*Hanzhe Yu,Yun Ye,Jintao Rong,Qi Xuan,Chen Ma*

Main category: cs.CV

TL;DR: 提出一个含73万+图像的高质量通用检测数据集（含真/假、多类型生成方式与元数据），并给出基于NLM噪声熵的轻量级检测方法；在跨域泛化与基准性能上优于现有数据集与部分方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测数据集存在泛化差、图像质量低、提示词过于简单、类别与操作类型单一等问题，导致检测模型在现实场景中鲁棒性不足。

Method: 构建大型数据集：覆盖多类别的真实与生成图像。生成途径含文本生成、修复（inpainting）、细化（refinement）、换脸等；使用1万+精心设计提示词；为每张生成图像标注生成方式与类别；修复图像附带二值掩码。基于数据集，提出轻量级检测器：先用非局部均值（NLM）估计噪声并计算熵，得到熵张量，再进行分类。

Result: 用该数据训练的检测模型在跨数据集与多场景测试中表现出更强的泛化；所提基于噪声熵的轻量级方法达到有竞争力的性能，为后续研究提供稳健基线。

Conclusion: 该数据集可作为强基准促进检测研究；结合提供的代码与标注（含掩码与生成元信息）能提升模型鲁棒性；提出的NLM噪声熵检测法简单高效，具备较好通用性与实用价值。

Abstract: The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at distinguishing AI-generated images from real ones. However, existing datasets suffer from limited generalization, low image quality, overly simple prompts, and insufficient image diversity. To address these limitations, we propose a high-quality, large-scale dataset comprising over 730,000 images across multiple categories, including both real and AI-generated images. The generated images are synthesized via state-of-the-art methods, including text-to-image generation (guided by over 10,000 carefully designed prompts), image inpainting, image refinement, and face swapping. Each generated image is annotated with its generation method and category. Inpainting images further include binary masks to indicate inpainted regions, providing rich metadata for analysis. Compared to existing datasets, detection models trained on our dataset demonstrate superior generalization capabilities. Our dataset not only serves as a strong benchmark for evaluating detection methods but also contributes to advancing the robustness of AI-generated image detection techniques. Building upon this, we propose a lightweight detection method based on image noise entropy, which transforms the original image into an entropy tensor of Non-Local Means (NLM) noise before classification. Extensive experiments demonstrate that models trained on our dataset achieve strong generalization, and our method delivers competitive performance, establishing a solid baseline for future research. The dataset and source code are publicly available at https://real-hd.github.io.

</details>


### [29] [Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance](https://arxiv.org/abs/2602.10549)
*Shengyang Sun,Jiashen Hua,Junyi Feng,Xiaojin Gong*

Main category: cs.CV

TL;DR: 提出弱监督多模态视频异常检测中文本引导框架：用情境学习的多阶段文本增强微调文本特征提取器，并以多尺度瓶颈Transformer融合压缩跨模态信息，在UCF-Crime与XD-Violence上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 弱监督多模态异常检测尚未充分利用文本模态。通用语言模型难捕捉异常细粒度语义且真实异常文本稀缺；多模态融合易出现冗余与模态不平衡，导致误报与性能受限。

Method: 1) 文本侧：基于In-Context Learning的多阶段文本增强，自动生成高质量异常描述文本，用于微调文本特征提取器，使其更贴合异常语义。2) 融合侧：设计多尺度瓶颈Transformer，通过压缩的瓶颈token逐级整合多模态信息，减少冗余并缓解模态不平衡。

Result: 在UCF-Crime与XD-Violence两大基准上取得SOTA效果（具体数值未给出），验证了所提文本增强与瓶颈融合模块的有效性。

Conclusion: 针对弱监督多模态异常检测中文本利用不足与融合冗余问题，提出文本增强微调+多尺度瓶颈Transformer融合的文本引导框架，能更好表征异常并降低误报，实验显示优越性。

Abstract: Weakly supervised multimodal video anomaly detection has gained significant attention, yet the potential of the text modality remains under-explored. Text provides explicit semantic information that can enhance anomaly characterization and reduce false alarms. However, extracting effective text features is challenging due to the inability of general-purpose language models to capture anomaly-specific nuances and the scarcity of relevant descriptions. Furthermore, multimodal fusion often suffers from redundancy and imbalance. To address these issues, we propose a novel text-guided framework. First, we introduce an in-context learning-based multi-stage text augmentation mechanism to generate high-quality anomaly text samples for fine-tuning the text feature extractor. Second, we design a multi-scale bottleneck Transformer fusion module that uses compressed bottleneck tokens to progressively integrate information across modalities, mitigating redundancy and imbalance. Experiments on UCF-Crime and XD-Violence demonstrate state-of-the-art performance.

</details>


### [30] [C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2602.10551)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 论文提出C^2RoPE，解决RoPE在多模态视觉序列中的空间连续性破坏与长程注意衰减问题，通过时空连续位置嵌入与Chebyshev因果掩码提升3D场景推理与3D VQA表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D LMM多将视觉特征对齐至LLM，但直接沿用RoPE的1D时间索引会：1) 打断图像列方向的连续性，损失空间局部性；2) 随序列变长对早期视觉token注意力衰减（时间近因假设），导致长程依赖弱化。需要一种更契合视觉的时空位置编码与因果建模。

Method: 提出C^2RoPE：1) 构造时空三元混合索引，将1D时间位置与基于笛卡尔坐标的2D空间坐标融合；2) 设计频率分配策略，在三维索引分量上编码时空位置信息，实现时空连续的旋转位置嵌入；3) 提出Chebyshev因果掩码，以2D空间中token间的Chebyshev距离决定因果依赖，替代仅按时间先后施加的掩码。

Result: 在多个基准（3D场景推理、3D视觉问答等）上验证有效，较基线获得更好表现；显示能更好保留空间局部性并缓解长程注意衰减。代码将开源。

Conclusion: 通过时空连续位置编码与空间感知的因果掩码，C^2RoPE克服RoPE在多模态视觉处理中的空间连续性缺失与长程依赖弱化问题，提升3D LMM在下游任务的推理能力。

Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing. Specifically, applying 1D temporal positional indices disrupts the continuity of visual features along the column dimension, resulting in spatial locality loss. Moreover, RoPE follows the prior that temporally closer image tokens are more causally related, leading to long-term decay in attention allocation and causing the model to progressively neglect earlier visual tokens as the sequence length increases. To address these issues, we propose C^2RoPE, an improved RoPE that explicitly models local spatial Continuity and spatial Causal relationships for visual processing. C^2RoPE introduces a spatio-temporal continuous positional embedding mechanism for visual tokens. It first integrates 1D temporal positions with Cartesian-based spatial coordinates to construct a triplet hybrid positional index, and then employs a frequency allocation strategy to encode spatio-temporal positional information across the three index components. Additionally, we introduce Chebyshev Causal Masking, which determines causal dependencies by computing the Chebyshev distance of image tokens in 2D space. Evaluation results across various benchmarks, including 3D scene reasoning and 3D visual question answering, demonstrate C^2RoPE's effectiveness. The code is be available at https://github.com/ErikZ719/C2RoPE.

</details>


### [31] [MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning](https://arxiv.org/abs/2602.10575)
*Chenhao Zhang,Yazhe Niu,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出MetaphorStar：首个用于图像隐喻理解的端到端视觉强化学习框架，含数据集TFQ-Data、方法TFQ-GRPO与基准TFQ-Bench；在多项隐含意义任务上显著超越主流MLLMs并达SOTA，并提升复杂视觉推理的泛化能力；资源全部开源。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM擅长基础VQA，但难以理解图像中的隐喻、文化情境与情感含义，原因在于缺乏多跳推理、文化背景与心智理论能力。需要专门方法与数据来弥补这一空白。

Method: 构建端到端视觉强化学习框架MetaphorStar，包括：1）细粒度图像隐喻数据集TFQ-Data；2）视觉RL训练算法TFQ-GRPO；3）评测基准TFQ-Bench。基于该框架训练一系列开源模型（如32B），并系统研究参数规模、数据规模、架构与训练策略对性能的影响。

Result: 在图像隐喻理解基准上平均提升82.6%；MetaphorStar-32B在多选与开放式问答上达SOTA，并在判断题上显著优于闭源Gemini-3.0-pro；学习隐喻任务还能提升复杂视觉推理的通用理解能力。

Conclusion: 视觉RL结合专门数据与基准能有效增强MLLM对图像隐喻与潜在含义的把握，并带来更强的复杂推理泛化；方法与资源具有广泛适用性，已全部开源。

Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.
  Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.

</details>


### [32] [Enhancing Underwater Images via Adaptive Semantic-aware Codebook Learning](https://arxiv.org/abs/2602.10586)
*Bosen Lin,Feng Gao,Yanwei Yu,Junyu Dong,Qian Du*

Main category: cs.CV

TL;DR: 提出SUCode通过语义感知的离散码本与注意/频域融合，自适应提升水下图像色彩与细节，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 水下图像退化成因复杂且区域异质，缺乏干净参考，现有方法用单一全局模型忽视不同语义区域的退化差异，易致颜色失真与细节丢失。

Method: 提出SUCode：以语义感知的像素级离散码本表示异质退化；采用三阶段训练避免伪真值污染；设计GCAM（门控通道注意）与FAFF（频率感知特征融合）联合利用通道与频域线索实现颜色校正与纹理恢复。

Result: 在多项基准上实现SOTA，较近期UIE方法在有参考与无参考指标上均显著领先；提供代码仓库以复现。

Conclusion: 语义感知、像素级码本结合注意与频域融合能更好适配区域不一致退化，带来更真实的色彩与更清晰的细节；三阶段训练有助于稳定学习并避免伪标签带来的偏差。

Abstract: Underwater Image Enhancement (UIE) is an ill-posed problem where natural clean references are not available, and the degradation levels vary significantly across semantic regions. Existing UIE methods treat images with a single global model and ignore the inconsistent degradation of different scene components. This oversight leads to significant color distortions and loss of fine details in heterogeneous underwater scenes, especially where degradation varies significantly across different image regions. Therefore, we propose SUCode (Semantic-aware Underwater Codebook Network), which achieves adaptive UIE from semantic-aware discrete codebook representation. Compared with one-shot codebook-based methods, SUCode exploits semantic-aware, pixel-level codebook representation tailored to heterogeneous underwater degradation. A three-stage training paradigm is employed to represent raw underwater image features to avoid pseudo ground-truth contamination. Gated Channel Attention Module (GCAM) and Frequency-Aware Feature Fusion (FAFF) jointly integrate channel and frequency cues for faithful color restoration and texture recovery. Extensive experiments on multiple benchmarks demonstrate that SUCode achieves state-of-the-art performance, outperforming recent UIE methods on both reference and no-reference metrics. The code will be made public available at https://github.com/oucailab/SUCode.

</details>


### [33] [Enhancing YOLOv11n for Reliable Child Detection in Noisy Surveillance Footage](https://arxiv.org/abs/2602.10592)
*Khanh Linh Tran,Minh Nguyen Dang,Thien Nguyen Trong,Hung Nguyen Quoc,Linh Nguyen Kieu*

Main category: cs.CV

TL;DR: 基于YOLOv11n的轻量化部署方案，通过面向监控场景的专用增强与SAHI推理，显著提升低质视频中的儿童检测精度与召回，同时保持边缘设备实时性与低功耗。


<details>
  <summary>Details</summary>
Motivation: 现实安防（失踪儿童预警、托育监控）常见低清CCTV存在遮挡、小目标、低分辨率、运动模糊、弱光等问题，现成检测器在此环境召回不足；需要在不增加模型复杂度的前提下提升鲁棒性并适配低算力部署。

Method: 1) 以YOLOv11n为基础不改结构；2) 设计领域特化数据增强：空间扰动（部分可见、截断、重叠）+光度退化（照度变化、噪声等），合成更贴近真实监控的儿童摆放；3) 推理阶段集成SAHI切片辅助超分辨感知，提升小目标/遮挡实例召回；4) 在Roboflow Daycare的儿童子集上训练与评测。

Result: 相较基线YOLOv11n，mAP@0.5达0.967、mAP@0.5:0.95达0.783，分别提升0.7和2.3个百分点；在低功耗边缘设备上保持实时推理。

Conclusion: 无需改动网络结构即可通过针对场景的数据增强与SAHI推理显著提升低质监控中的儿童检测表现，方案轻量、可部署、适合低成本/资源受限的工业级场景；数据与代码已开源以便复现与扩展。

Abstract: This paper presents a practical and lightweight solution for enhancing child detection in low-quality surveillance footage, a critical component in real-world missing child alert and daycare monitoring systems. Building upon the efficient YOLOv11n architecture, we propose a deployment-ready pipeline that improves detection under challenging conditions including occlusion, small object size, low resolution, motion blur, and poor lighting commonly found in existing CCTV infrastructures. Our approach introduces a domain-specific augmentation strategy that synthesizes realistic child placements using spatial perturbations such as partial visibility, truncation, and overlaps, combined with photometric degradations including lighting variation and noise. To improve recall of small and partially occluded instances, we integrate Slicing Aided Hyper Inference (SAHI) at inference time. All components are trained and evaluated on a filtered, child-only subset of the Roboflow Daycare dataset. Compared to the baseline YOLOv11n, our enhanced system achieves a mean Average Precision at 0.5 IoU (mAP@0.5) of 0.967 and a mean Average Precision averaged over IoU thresholds from 0.5 to 0.95 (mAP@0.5:0.95) of 0.783, yielding absolute improvements of 0.7 percent and 2.3 percent, respectively, without architectural changes. Importantly, the entire pipeline maintains compatibility with low-power edge devices and supports real-time performance, making it particularly well suited for low-cost or resource-constrained industrial surveillance deployments. The example augmented dataset and the source code used to generate it are available at: https://github.com/html-ptit/Data-Augmentation-YOLOv11n-child-detection

</details>


### [34] [Fast Person Detection Using YOLOX With AI Accelerator For Train Station Safety](https://arxiv.org/abs/2602.10593)
*Mas Nurul Achmadiah,Novendra Setyawan,Achmad Arif Bryantono,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: 论文探讨在火车站场景用YOLOX结合边缘AI加速器进行乘客检测，以提升安防；结果显示Hailo‑8在精度与时延上均优于Jetson Orin Nano。


<details>
  <summary>Details</summary>
Motivation: 火车站月台黄线侵入等不安全行为导致事故，需要更高效、低时延、可部署在边缘侧的目标检测方案以提升安防并减少事故。

Method: 采用YOLOX作为目标检测器，部署在不同边缘AI硬件上进行对比评测：Hailo‑8加速器 vs. Jetson Orin Nano。比较指标包含检测精度与推理时延。

Result: 在相同任务与评测条件下，Hailo‑8较Jetson Orin Nano精度提升超过12%，时延降低约20 ms。

Conclusion: 在车站乘客检测场景下，Hailo‑8边缘AI加速器较Jetson Orin Nano更适合，能以更高精度与更低时延提升安防效果。

Abstract: Recently, Image processing has advanced Faster and applied in many fields, including health, industry, and transportation. In the transportation sector, object detection is widely used to improve security, for example, in traffic security and passenger crossings at train stations. Some accidents occur in the train crossing area at the station, like passengers uncarefully when passing through the yellow line. So further security needs to be developed. Additional technology is required to reduce the number of accidents. This paper focuses on passenger detection applications at train stations using YOLOX and Edge AI Accelerator hardware. the performance of the AI accelerator will be compared with Jetson Orin Nano. The experimental results show that the Hailo-8 AI hardware accelerator has higher accuracy than Jetson Orin Nano (improvement of over 12%) and has lower latency than Jetson Orin Nano (reduced 20 ms).

</details>


### [35] [Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation](https://arxiv.org/abs/2602.10619)
*Guangjing Yang,ZhangYuan Yu,Ziyuan Qin,Xinyuan Song,Huahui Yi,Qingbo Kang,Jun Gao,Yiyue Li,Chenlin Du,Qicheng Lao*

Main category: cs.CV

TL;DR: 提出VRFT-Aug：面向医学影像的视觉强化微调框架，通过知识注入、感知驱动策略优化、医学知识奖励塑形与行为模仿，稳定并提升RFT，跨多数据集优于SFT与RFT基线，并总结可迁移的训练启示。


<details>
  <summary>Details</summary>
Motivation: RFT在文本领域进展显著，但在跨模态、尤其需要强感知与结构化推理的医学影像领域仍欠探索。医学场景高风险、数据噪声与标注稀缺，现有SFT或通用RFT难以兼顾感知与推理，亟需定制化方法提升可靠性与可用性。

Method: 提出VRFT-Aug框架，围绕稳定与增强感知-推理能力：1) 先验知识注入：引入医学先验与结构化知识以引导模型；2) 感知驱动的策略细化：利用视觉感知信号反馈调整策略；3) 医学知情的奖励塑形：设计符合医学规则与安全约束的奖励；4) 行为模仿：结合模仿学习稳定训练。

Result: 在多种医学数据集上进行广泛实验，VRFT-Aug在性能上持续优于标准SFT与通用RFT基线，表现出更稳定的训练过程与更强的推理与感知能力。

Conclusion: VRFT-Aug有效将RFT扩展到医学视觉任务，提升稳定性与性能；作者提供的经验与训练启发具有可迁移性，可为构建可靠、具备推理能力的高风险医疗应用模型提供实践指南。

Abstract: While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.
  Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.

</details>


### [36] [A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology](https://arxiv.org/abs/2602.10624)
*Siyuan Yan,Xieji Li,Dan Mo,Philipp Tschandl,Yiwen Jiang,Zhonghua Wang,Ming Hu,Lie Ju,Cristina Vico-Alonso,Yizhen Zheng,Jiahe Liu,Juexiao Zhou,Camilla Chello,Jen G. Cheung,Julien Anriot,Luc Thomas,Clare Primiero,Gin Tan,Aik Beng Ng,Simon See,Xiaoying Tang,Albert Ip,Xiaoyang Liao,Adrian Bowling,Martin Haskett,Shuang Zhao,Monika Janda,H. Peter Soyer,Victoria Mar,Harald Kittler,Zongyuan Ge*

Main category: cs.CV

TL;DR: DermFM-Zero 是一个通过掩码潜变量建模与对比学习在400万+皮肤科多模态数据上训练的视觉-语言基础模型，在20项零样本诊断与多模态检索基准上达到了SOTA，并在三项多国读片研究中显著提升临床决策：基层全科医生在AI协助下差异诊断几乎翻倍，专科癌症评估中超过皮肤科专科医师；其稀疏自编码器可可解释地解耦临床概念并抑制伪影偏差，无需再训练即可增强稳健性，显示出有效、安全、可解释的零样本临床支持能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基础模型在受控基准上表现不错，但实际落地受限于需针对具体任务微调，难以泛化、部署成本高、鲁棒性与可解释性不足。作者希望构建一个无需任务特定适配即可在真实临床场景发挥作用、同时具备可解释与偏差控制能力的皮肤科多模态基础模型。

Method: 提出DermFM-Zero：在超400万多模态（图像-文本/元数据）样本上，通过掩码潜变量建模（masked latent modelling）学习重建式表示，并结合对比学习对齐视觉与语言空间；采用零样本评估（诊断、检索）；通过三项多国读片研究测试AI辅助对不同水平临床医师的影响；利用稀疏自编码器对模型潜表示进行无监督解耦，提取临床概念并进行有针对性的伪影/偏差抑制。

Result: 在20个零样本基准上达成SOTA；三项读片研究覆盖1100+临床人员：基层场景中AI辅助使全科医生对98种皮肤病的差异诊断准确率近乎翻倍；专科肿瘤评估中模型显著优于经认证皮肤科医师；协作工作流下，非专家在AI辅助下超过未受助专家且管理决策更恰当；稀疏自编码器从潜空间中无监督解出临床有意义概念，优于预定义词汇方法，并能定向抑制由伪影引起的偏差，在无需再训练的情况下提升鲁棒性。

Conclusion: DermFM-Zero 展示出无需任务特定微调即可在多任务、多场景中提供高效、安全、可解释的零样本临床决策支持；其可解释潜表示与偏差抑制机制为基础模型在真实医疗环境落地提供了通用路径。

Abstract: Medical foundation models have shown promise in controlled benchmarks, yet widespread deployment remains hindered by reliance on task-specific fine-tuning. Here, we introduce DermFM-Zero, a dermatology vision-language foundation model trained via masked latent modelling and contrastive learning on over 4 million multimodal data points. We evaluated DermFM-Zero across 20 benchmarks spanning zero-shot diagnosis and multimodal retrieval, achieving state-of-the-art performance without task-specific adaptation. We further evaluated its zero-shot capabilities in three multinational reader studies involving over 1,100 clinicians. In primary care settings, AI assistance enabled general practitioners to nearly double their differential diagnostic accuracy across 98 skin conditions. In specialist settings, the model significantly outperformed board-certified dermatologists in multimodal skin cancer assessment. In collaborative workflows, AI assistance enabled non-experts to surpass unassisted experts while improving management appropriateness. Finally, we show that DermFM-Zero's latent representations are interpretable: sparse autoencoders unsupervisedly disentangle clinically meaningful concepts that outperform predefined-vocabulary approaches and enable targeted suppression of artifact-induced biases, enhancing robustness without retraining. These findings demonstrate that a foundation model can provide effective, safe, and transparent zero-shot clinical decision support.

</details>


### [37] [Eliminating VAE for Fast and High-Resolution Generative Detail Restoration](https://arxiv.org/abs/2602.10630)
*Yan Wang,Shijie Zhao,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: 提出GenDR-Pix：将一步扩散超分从潜空间转到像素空间，去除VAE瓶颈，实现更快更省内存的高分辨率重建（含4K一秒、6GB）。


<details>
  <summary>Details</summary>
Motivation: 一步扩散（如GenDR）虽加速推理，但仍受VAE引入的延迟与显存占用限制，导致高分图像必须切片处理；需要彻底移除瓶颈并保持画质。

Method: 用pixel-(un)shuffle替代VAE，把潜空间流程反转为像素空间GenDR-Pix；为避免x8 pixelshuffle带来的重复纹理伪影，提出多阶段对抗蒸馏，逐步移除编解码器，并用上一阶段模型的生成特征指导判别器；引入随机padding增强生成特征、避免判别器塌陷；加入掩膜傅里叶幅度损失抑制频域离群值；推理时结合基于padding的自集成与CFG提升尺度与质量。

Result: 在与GenDR对比中实现2.8倍加速、60%显存节省且画质基本不降；优于其它一步扩散SR；可在约1秒与6GB显存下重建4K图像。

Conclusion: 移除VAE并以像素空间一步扩散配合多阶段对抗蒸馏与频域/增强技巧，可在保持视觉质量的同时显著降低时延与显存，解决高分超分中的切片与资源瓶颈。

Abstract: Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with x8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8x acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6GB.

</details>


### [38] [VideoSTF: Stress-Testing Output Repetition in Video Large Language Models](https://arxiv.org/abs/2602.10639)
*Yuxin Cao,Wei Song,Shangzhi Xu,Jingling Xue,Jin Song Dong*

Main category: cs.CV

TL;DR: 论文揭示并量化了VideoLLM在生成时的“重复退化”问题，并提出VideoSTF框架用于系统性测量与压力测试，显示该问题普遍且对时间扰动极其敏感，甚至可被黑盒攻击利用。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLM评测主要关注准确性与事实性，忽视生成稳定性。作者观察到模型在视频理解生成中会陷入重复循环（短语/句子反复），这一失败模式缺乏系统测量、基准与安全视角评估。

Method: 提出VideoSTF：1）用三种互补的n-gram重复度指标形式化定义“输出重复”；2）构建包含1万条多样视频的标准化测试库；3）提供可控的时间变换（如采样速率、时序打乱等）以进行时间压力测试与对抗诱导；4）对10个先进VideoLLM做普适测试、时间压力测试与黑盒对抗利用。

Result: 发现重复退化在各模型中普遍存在；输出对时间扰动高度敏感；简单的时间变换即可在黑盒设置下高效诱发重复，从而构成可利用的安全漏洞。

Conclusion: 输出重复是现代VideoLLM的基础性稳定性问题，需要在评测中纳入稳定性意识与时间鲁棒性测试。作者开源评测代码与脚本以促进社区复现与改进。

Abstract: Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

</details>


### [39] [Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation](https://arxiv.org/abs/2602.10659)
*Yin Wang,Ziyao Zhang,Zhiying Leng,Haitian Liu,Frederick W. B. Li,Mu Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 提出MP-HOI框架用于文本驱动的3D人-物交互动作生成，通过多模态先验、增强物体表示、模态感知MoE融合与级联扩散+交互监督，分别缓解人/物动作质量差与交互弱的问题，在实验中取得更高保真与细粒度HOI效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从文本到HOI映射，因跨模态鸿沟导致三类问题：人类动作次优、物体运动不自然、人-物交互弱。需要引入更丰富的模态信息、改进物体表示、提升多模态融合能力，并在生成过程中显式强化交互。

Method: 1) 多模态数据先验：利用大规模多模态模型提供的文本、图像、姿态/物体信息作为先验引导生成；2) 增强物体表示：加入几何关键点、接触特征与动力学属性以提升表达力；3) 多模态感知MoE：设计模态感知的专家混合网络进行有效的特征融合；4) 带交互监督的级联扩散：分阶段细化HOI特征并通过专门的交互监督增强人-物耦合。

Result: 在全面实验中，相较现有方法，MP-HOI生成的人与物体动作更高保真、更细粒度，交互更自然，定量与定性指标均显著优于基线。

Conclusion: 多模态先验+增强物体表示+模态感知MoE+级联扩散与交互监督的组合，能有效弥合文本到HOI的跨模态差距，显著提升文本驱动3D人-物交互动作生成的质量与交互性。

Abstract: We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.

</details>


### [40] [AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception](https://arxiv.org/abs/2602.10660)
*Kiarash Ghasemzadeh,Sedigheh Dehghani*

Main category: cs.CV

TL;DR: AurigaNet是一种面向自动驾驶感知的多任务网络，统一完成目标检测、车道线检测与可行驶区域实例分割，并在BDD100K上取得领先精度与嵌入式实时性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要在受限算力下同时解决多种感知任务（检测、车道、可行驶区域），单任务/串行管线在效率与泛化上受限。多任务学习可共享特征、降低延迟、提升鲁棒性，因此需要设计更高效且端到端的多任务架构。

Method: 提出AurigaNet多任务架构：共享主干与特征金字塔，集成三项任务头，其中可行驶区域采用端到端实例分割以改进路径估计的精度与效率；在BDD100K上训练/评估，并在Jetson Orin NX上部署验证实时性。

Result: 在BDD100K上：可行驶区域分割IoU 85.2%（领先0.7%）；车道检测IoU 60.8%（领先30%+）；交通目标检测mAP@0.5:0.95为47.6%（领先2.9%）；在Jetson Orin NX上实现具有竞争力的实时推理。

Conclusion: AurigaNet作为统一的多任务感知系统，在精度与效率上均优于现有方法，并具备嵌入式部署可行性，适合作为自动驾驶感知的稳健高效方案。

Abstract: Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet's potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.

</details>


### [41] [Dynamic Frequency Modulation for Controllable Text-driven Image Generation](https://arxiv.org/abs/2602.10662)
*Tiandong Shi,Ling Zhao,Ji Qi,Jiayi Ma,Chengli Peng*

Main category: cs.CV

TL;DR: 从频率视角分析扩散模型生成过程：低频先搭结构，高频后补纹理；据此提出无需训练的动态频率加权调制，既保持全局结构又支持语义修改，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本引导扩散生成中，直接改动文本提示常导致整体结构被意外改变；现有通过经验性选择特征图干预，稳定性差、依赖选择。作者希望找到更稳健、可控的方法在不破坏结构的前提下实现语义更新。

Method: 从噪声潜变量的频谱出发，分析不同频段在生成各阶段的作用：早期低频主导结构框架，后期高频主导细节纹理。基于此，提出训练自由的频率调制：对噪声潜变量施加随时间动态衰减的频率依赖加权函数，在采样过程中直接调控不同频段权重，从而维持结构一致并定向修改语义，无需内部特征图选择。

Result: 大量实验显示该方法在保持结构框架一致性的同时实现目标语义更新，稳定性与效果显著优于当前SOTA方法，在结构保持与语义变更间取得更佳平衡。

Conclusion: 扩散模型生成具有频率分工：低频奠定结构、高频塑造细节。利用这一规律的训练自由频率调制可避免经验性特征图选择带来的不稳定，实现更可控的编辑，效果领先现有方法。

Abstract: The success of text-guided diffusion models has established a new image generation paradigm driven by the iterative refinement of text prompts. However, modifying the original text prompt to achieve the expected semantic adjustments often results in unintended global structure changes that disrupt user intent. Existing methods rely on empirical feature map selection for intervention, whose performance heavily depends on appropriate selection, leading to suboptimal stability. This paper tries to solve the aforementioned problem from a frequency perspective and analyzes the impact of the frequency spectrum of noisy latent variables on the hierarchical emergence of the structure framework and fine-grained textures during the generation process. We find that lower-frequency components are primarily responsible for establishing the structure framework in the early generation stage. Their influence diminishes over time, giving way to higher-frequency components that synthesize fine-grained textures. In light of this, we propose a training-free frequency modulation method utilizing a frequency-dependent weighting function with dynamic decay. This method maintains the structure framework consistency while permitting targeted semantic modifications. By directly manipulating the noisy latent variable, the proposed method avoids the empirical selection of internal feature maps. Extensive experiments demonstrate that the proposed method significantly outperforms current state-of-the-art methods, achieving an effective balance between preserving structure and enabling semantic updates.

</details>


### [42] [AMAP-APP: Efficient Segmentation and Morphometry Quantification of Fluorescent Microscopy Images of Podocytes](https://arxiv.org/abs/2602.10663)
*Arash Fatehi,David Unnersjö-Jess,Linus Butt,Noémie Moreau,Thomas Benzing,Katarzyna Bozek*

Main category: cs.CV

TL;DR: AMAP-APP 是对 AMAP 的跨平台桌面化与高效化改造：以经典图像处理替代实例分割、保留语义分割并加入改进的 ROI；在常规硬件上速度提升约147倍，测量指标与原法高度一致且统计等效，并且 ROI 更贴近人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有 AMAP 方法虽准确但计算量大、无图形界面且依赖 Linux，门槛高、难以在普通实验室和临床环境推广。

Method: 在保留原语义分割模型的前提下，用经典图像处理流程取代耗时的实例分割；提出改进的 ROI 划定算法；在包含小鼠与人类样本、STED 与共聚焦成像的365张图像上，与原 AMAP 进行对比评估（Pearson 相关与 TOST 等效性检验）。

Result: 在消费级硬件上速度提升约147倍；面积、周长、圆度、裂隙隔密度等形态学输出与原法高度相关（r>0.90）且统计等效（TOST P<0.05）；新 ROI 算法相较原法与人工勾画的偏差更小。

Conclusion: AMAP-APP 通过跨平台 GUI 和显著的计算效率提升，降低了深度学习肾小球足突形态计量的使用门槛，促进在肾脏研究与潜在临床诊断中的广泛采用。

Abstract: Background: Automated podocyte foot process quantification is vital for kidney research, but the established "Automatic Morphological Analysis of Podocytes" (AMAP) method is hindered by high computational demands, a lack of a user interface, and Linux dependency. We developed AMAP-APP, a cross-platform desktop application designed to overcome these barriers.
  Methods: AMAP-APP optimizes efficiency by replacing intensive instance segmentation with classic image processing while retaining the original semantic segmentation model. It introduces a refined Region of Interest (ROI) algorithm to improve precision. Validation involved 365 mouse and human images (STED and confocal), benchmarking performance against the original AMAP via Pearson correlation and Two One-Sided T-tests (TOST).
  Results: AMAP-APP achieved a 147-fold increase in processing speed on consumer hardware. Morphometric outputs (area, perimeter, circularity, and slit diaphragm density) showed high correlation (r>0.90) and statistical equivalence (TOST P<0.05) to the original method. Additionally, the new ROI algorithm demonstrated superior accuracy compared to the original, showing reduced deviation from manual delineations.
  Conclusion: AMAP-APP democratizes deep learning-based podocyte morphometry. By eliminating the need for high-performance computing clusters and providing a user-friendly interface for Windows, macOS, and Linux, it enables widespread adoption in nephrology research and potential clinical diagnostics.

</details>


### [43] [TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning](https://arxiv.org/abs/2602.10675)
*Junhua Liu,Zhangcheng Wang,Zhike Han,Ningli Wang,Guotao Liang,Kun Kuang*

Main category: cs.CV

TL;DR: 提出TwiFF-2.7M与TwiFF模型，将视觉链式思维从静态扩展到视频时序场景，并配套评测集TwiFF-Bench；模型通过迭代生成未来动作帧与文字推理，显著提升动态多模态问答表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉链式思维方法多针对静态图像，难以处理指令执行、事件预测、镜头运动等需要时间维度的推理，缺乏大规模时序标注数据与针对性的评测基准。

Method: 构建首个大规模时序VCoT数据集TwiFF-2.7M（约270万视频片段），并发布评测基准TwiFF-Bench（1078样本），同时提出TwiFF统一模型，融合预训练视频生成与图像理解能力，通过迭代地生成未来动作帧与文本推理，形成时间一致的视觉推理轨迹，并在开放式动态问答中评估“推理可合理性+最终答案正确性”。

Result: 在多项动态推理任务上，TwiFF显著优于现有VCoT和文本CoT基线，展示更强的时序一致性与答案准确率。

Conclusion: 面向动态场景的时序VCoT是有效路径；TwiFF的数据、基准与模型共同验证了通过生成未来视觉线索结合文本推理可显著提升视频问答中的时序推理能力。

Abstract: Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at https://github.com/LiuJunhua02/TwiFF.

</details>


### [44] [OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL](https://arxiv.org/abs/2602.10687)
*Jinjie Shen,Jing Wu,Yaxiong Wang,Lechao Cheng,Shengeng Tang,Tianrui Hui,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出OmniVL-Guard：一个用于多模态（文本/图像/视频）造假检测与定位的一体化框架，通过自进化CoT推理与自适应奖励缩放的策略优化，缓解多任务中的“难度偏置”，显著优于SOTA并具零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实谣言/伪造往往交织文本、图像与视频，现有方法多局限单/双模态，且联合完成检测（真假判别）与定位（伪造区域/成分标注）时，容易出现分类任务主导梯度、细粒度定位受损的“难度偏置”。需要一个统一框架，既覆盖多模态，又在多任务训练中保持平衡。

Method: 提出OmniVL-Guard：1) Self-Evolving CoT Generation：自动合成高质量推理链以缓解冷启动，提升多模态推理监督；2) ARSPO（Adaptive Reward Scaling Policy Optimization）：在强化学习范式下，动态调节不同任务的奖励尺度与损失权重，使检测与定位优化更均衡。整体是一个统一的视觉-语言伪造检测与定位的RL训练框架。

Result: 在多项基准上显著超过SOTA；在跨域/零样本场景中保持稳健泛化；同时提升真假分类准确率与定位精度。

Conclusion: 通过自进化CoT与自适应奖励缩放的RL优化，OmniVL-Guard有效缓解多任务难度偏置，实现对文本/图像/视频混合造假的统一检测与精准定位，并具备良好的零样本泛化能力。

Abstract: Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.

</details>


### [45] [AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models](https://arxiv.org/abs/2602.10698)
*Zhifeng Rao,Wenlong Chen,Lei Xie,Xia Hua,Dongfu Yin,Zhen Tian,F. Richard Yu*

Main category: cs.CV

TL;DR: 将深度估计融入VLA，使模型从RGB中恢复几何线索，并与动作先验约束对齐，从而在3D理解与动作预测上显著提升泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA多依赖2D训练的VLM，缺乏对复杂3D环境的空间理解与动作落地能力，需要一种方式在不依赖昂贵3D数据的前提下增强3D感知并与控制任务对齐。

Method: 用VGGT从RGB推断深度与几何感知特征；设计“action assistant”模块，用动作先验/下游控制一致性来约束与校准深度派生表示；将增强的3D特征与传统2D视觉token融合，形成更丰富的表示用于感知与决策。

Result: 在含几何歧义的场景中提升感知质量，并带来更高的动作预测精度；总体上提高VLA的泛化能力与鲁棒性。

Conclusion: 深度驱动的数据增强与辅助专家式监督能有效弥合2D观测与3D感知决策的鸿沟，为机器人系统带来更稳健的空间理解与控制表现。

Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.

</details>


### [46] [(MGS)$^2$-Net: Unifying Micro-Geometric Scale and Macro-Geometric Structure for Cross-View Geo-Localization](https://arxiv.org/abs/2602.10704)
*Minglei Li,Mengfan He,Chao Chen,Ziyang Meng*

Main category: cs.CV

TL;DR: 提出(MGS)^2框架，通过宏/微几何建模提升跨视角地理定位的稳健性与泛化，University-1652与SUES-200上Recall@1达97%+。


<details>
  <summary>Details</summary>
Motivation: 现有CVGL多在2D特征空间匹配，忽视真实3D几何，导致倾斜航拍与正射卫星间的巨大几何失配；尤其是立面（宏结构）与尺度变化（微尺度）带来噪声与对齐困难。

Method: 提出(MGS)^2：1) MGSF（宏几何结构过滤）用空洞几何梯度抑制高频立面伪影、增强水平面不变信息；2) MGSA（微几何尺度自适应）基于深度先验，通过多分支特征融合动态校正尺度差异；3) GACD（几何-外观对比蒸馏）损失，强化对倾斜遮挡的辨别，提升跨域鲁棒性。

Result: 在University-1652和SUES-200上取得SOTA：Recall@1分别为97.5%与97.02%，并展现更好的跨数据集泛化与对几何歧义的鲁棒性。

Conclusion: 将几何先验融入特征学习，通过宏观结构过滤、微尺度自适应与对比蒸馏，有效缓解跨视角几何失配，显著提升CVGL性能与泛化能力；代码已开源。

Abstract: Cross-view geo-localization (CVGL) is pivotal for GNSS-denied UAV navigation but remains brittle under the drastic geometric misalignment between oblique aerial views and orthographic satellite references. Existing methods predominantly operate within a 2D manifold, neglecting the underlying 3D geometry where view-dependent vertical facades (macro-structure) and scale variations (micro-scale) severely corrupt feature alignment. To bridge this gap, we propose (MGS)$^2$, a geometry-grounded framework. The core of our innovation is the Macro-Geometric Structure Filtering (MGSF) module. Unlike pixel-wise matching sensitive to noise, MGSF leverages dilated geometric gradients to physically filter out high-frequency facade artifacts while enhancing the view-invariant horizontal plane, directly addressing the domain shift. To guarantee robust input for this structural filtering, we explicitly incorporate a Micro-Geometric Scale Adaptation (MGSA) module. MGSA utilizes depth priors to dynamically rectify scale discrepancies via multi-branch feature fusion. Furthermore, a Geometric-Appearance Contrastive Distillation (GACD) loss is designed to strictly discriminate against oblique occlusions. Extensive experiments demonstrate that (MGS)$^2$ achieves state-of-the-art performance, recording a Recall@1 of 97.5\% on University-1652 and 97.02\% on SUES-200. Furthermore, the framework exhibits superior cross-dataset generalization against geometric ambiguity. The code is available at: \href{https://github.com/GabrielLi1473/MGS-Net}{https://github.com/GabrielLi1473/MGS-Net}.

</details>


### [47] [FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection](https://arxiv.org/abs/2602.10710)
*Jialin Ma*

Main category: cs.CV

TL;DR: 提出FGAA-FPN，将前景引导与角度感知注意力结合于特征金字塔，用于遥感斜框目标检测，在DOTA v1.0/1.5上达SOTA（75.5/68.3 mAP）。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖FPN多尺度融合与注意力上下文，但缺少显式前景建模与几何方向先验，导致在复杂背景、尺度变化大、方向多变场景下判别力不足。

Method: 基于层级功能分解设计FGAA-FPN：低层采用前景引导特征调制（FGFM），在弱监督下学习前景显著性以增强目标区域、抑制背景；高层采用角度感知多头注意力（AA-MHA），编码相对方向关系，指导全局语义交互；整体与FPN融合以强化多尺度表示。

Result: 在DOTA v1.0与v1.5上取得分别为75.5%与68.3%的mAP，达到或超越现有最优。

Conclusion: 显式前景建模与方向先验的结合可显著提升斜框检测中的多尺度与旋转鲁棒性；FGAA-FPN验证了该思路的有效性并达SOTA表现。

Abstract: With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.

</details>


### [48] [Ecological mapping with geospatial foundation models](https://arxiv.org/abs/2602.10720)
*Craig Mahlasi,Gciniwe S. Baloyi,Zaheed Gaffoor,Levente Klein,Anne Jones,Etienne Vos,Michal Muszynski,Geoffrey Dawson,Campbell Watson*

Main category: cs.CV

TL;DR: 本文评估地理空间基础模型（GFMs）在生态学任务中的实用性，微调Prithvi-E0-2.0与TerraMind并与ResNet-101对比，发现GFMs整体优于传统基线，TerraMind在多模态条件下优势更明显，但受训练模态差异与标签/分辨率限制。


<details>
  <summary>Details</summary>
Motivation: GFMs迅速发展并在土地覆盖、生态制图等任务展现潜力，但其在高价值生态应用中的效用、挑战与机会尚未系统量化与阐明。作者希望验证GFMs在多类生态任务中的性能边界与适用条件，为实务部署提供依据。

Method: 选择两种预训练GFMs（Prithvi-E0-2.0、TerraMind）并进行微调，对比基线ResNet-101；设置三类生态用例：1）LULC（地表覆盖/利用）生成；2）森林功能性状映射；3）泥炭地识别；评估单模态与多模态输入情形下的模型表现，并分析数据模态匹配与标注质量/分辨率影响。

Result: 所有实验中GFMs均优于ResNet-101；TerraMind整体略优于Prithvi；当引入更多模态时，TerraMind显著超越ResNet与Prithvi；但当下游输入与预训练模态存在偏差时性能受影响。

Conclusion: GFMs对生态应用具备显著优势，尤其在多模态数据融合下；实务上需关注预训练/下游模态一致性，并改进数据分辨率与标注精度以胜任像素级动态映射任务。

Abstract: Geospatial foundation models (GFMs) are a fast-emerging paradigm for various geospatial tasks, such as ecological mapping. However, the utility of GFMs has not been fully explored for high-value use cases. This study aims to explore the utility, challenges and opportunities associated with the application of GFMs for ecological uses. In this regard, we fine-tune several pretrained AI models, namely, Prithvi-E0-2.0 and TerraMind, across three use cases, and compare this with a baseline ResNet-101 model. Firstly, we demonstrate TerraMind's LULC generation capabilities. Lastly, we explore the utility of the GFMs in forest functional trait mapping and peatlands detection. In all experiments, the GFMs outperform the baseline ResNet models. In general TerraMind marginally outperforms Prithvi. However, with additional modalities TerraMind significantly outperforms the baseline ResNet and Prithvi models. Nonetheless, consideration should be given to the divergence of input data from pretrained modalities. We note that these models would benefit from higher resolution and more accurate labels, especially for use cases where pixel-level dynamics need to be mapped.

</details>


### [49] [A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography](https://arxiv.org/abs/2602.10722)
*Davide Evangelista,Pasquale Cascarano,Elena Loli Piccolomini*

Main category: cs.CV

TL;DR: 论文探讨在稀疏/有限角度CT条件下，用扩散式生成模型作为深度生成先验（DGP），并结合可解释的迭代重建优化，以提升重建质量并减少伪影和形变。作者在图像生成、模型设计与迭代算法三个方面提出改进，实验在极稀疏几何下表现出有前景的结果，但仍需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 稀疏或有限角度投影导致欠定与信息缺失，传统重建产生严重伪影/形变。需要在保持模型可解释性的同时，引入强先验以补足丢失信息；扩散模型具备强生成能力，适合作为先验提升CT重建质量。

Method: 采用Deep Generative Prior框架，将扩散式生成模型与迭代优化相结合，解一个含数据一致性项与生成先验约束的最小化问题。作者针对三方面做改进：1) 图像生成流程（基于扩散模型）；2) 生成模型配置/训练；3) 求解最小化的迭代算法（数据一致性与先验步的协同）。

Result: 在高度稀疏/有限角度的采样下，所提方法显著降低伪影与失真，重建质量优于已有方法（摘要暗示性结论，未给出具体数值）。

Conclusion: DGP+扩散模型在稀疏CT重建中具有潜力，改进的生成/模型/迭代策略带来更佳重建，但仍需进一步验证与研究（如更系统的评测与鲁棒性分析）。

Abstract: The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of data typically results in artifacts in the reconstructed image and may even lead to object distortions. For this reason, the use of deep generative models in this context has great interest and potential success. In the Deep Generative Prior (DGP) framework, the use of diffusion-based generative models is combined with an iterative optimization algorithm for the reconstruction of CT images from sinograms acquired under sparse geometries, to maintain the explainability of a model-based approach while introducing the generative power of a neural network. There are therefore several aspects that can be further investigated within these frameworks to improve reconstruction quality, such as image generation, the model, and the iterative algorithm used to solve the minimization problem, for which we propose modifications with respect to existing approaches. The results obtained even under highly sparse geometries are very promising, although further research is clearly needed in this direction.

</details>


### [50] [OccFace: Unified Occlusion-Aware Facial Landmark Detection with Per-Point Visibility](https://arxiv.org/abs/2602.10728)
*Xinhao Xiang,Zhengxin Li,Saurav Dhakad,Theo Bancroft,Jiawei Zhang,Weiyang Li*

Main category: cs.CV

TL;DR: 提出OccFace，一个面向类人脸（人类与风格化/非人角色）的遮挡感知100点关键点检测框架，同时预测关键点坐标与可见性，并提供新评测套件与数据集；在外部遮挡与大姿态下显著更稳健，对可见点精度不降。


<details>
  <summary>Details</summary>
Motivation: 在遮挡（外物或自遮挡）和大外观变化下，现有人脸关键点方法难以鲁棒定位，且多未显式输出逐点可见性，限制了下游利用（如跟踪、重建、表情驱动）。需要一种既统一适配多种类人脸、又能联合定位与可见性预测的方法与评测基准。

Method: - 统一的稠密100点布局与heatmap骨干；
- 新增遮挡模块，融合局部证据与跨关键点上下文，联合回归坐标与逐点可见性；
- 可见性监督：人工标注 + 基于关键点热图与遮挡掩膜重叠的伪标签（landmark-aware masking）；
- 构建遮挡感知评测套件：分别报告可见/被遮挡点的NME，并用Occ AP、F1@0.5、ROC-AUC评估可见性；
- 释放带100点与可见性标注的数据集。

Result: 在存在外部遮挡与大幅旋转（自遮挡）场景下，对被遮挡区域的关键点定位更稳健，同时保持对可见点的准确度不下降；可见性预测在Occ AP、F1@0.5、ROC-AUC上表现优良。

Conclusion: OccFace通过统一布局与遮挡模块实现坐标与可见性的联合学习，配合混合监督与新评测，显著提升遮挡与大姿态条件下的鲁棒性，并不牺牲可见点精度，为广泛类人脸场景提供实用方案。

Abstract: Accurate facial landmark detection under occlusion remains challenging, especially for human-like faces with large appearance variation and rotation-driven self-occlusion. Existing detectors typically localize landmarks while handling occlusion implicitly, without predicting per-point visibility that downstream applications can benefits. We present OccFace, an occlusion-aware framework for universal human-like faces, including humans, stylized characters, and other non-human designs. OccFace adopts a unified dense 100-point layout and a heatmap-based backbone, and adds an occlusion module that jointly predicts landmark coordinates and per-point visibility by combining local evidence with cross-landmark context. Visibility supervision mixes manual labels with landmark-aware masking that derives pseudo visibility from mask-heatmap overlap. We also create an occlusion-aware evaluation suite reporting NME on visible vs. occluded landmarks and benchmarking visibility with Occ AP, F1@0.5, and ROC-AUC, together with a dataset annotated with 100-point landmarks and per-point visibility. Experiments show improved robustness under external occlusion and large head rotations, especially on occluded regions, while preserving accuracy on visible landmarks.

</details>


### [51] [Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning](https://arxiv.org/abs/2602.10744)
*Kian Majlessi,Amir Masoud Soltani,Mohammad Ebrahim Mahdavi,Aurelien Gourrier,Peyman Adibi*

Main category: cs.CV

TL;DR: 提出一种针对真实场景超分辨率图像的无参考质量评价方法S3 RIQA，利用自监督对比学习学习“模型导向”的退化表征，并配合预处理与辅助任务，在多个真实SR-IQA基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真实低分辨率图像的退化复杂且不可预测，现有基于合成LR的SR与IQA方法难以泛化；数据稀缺领域尤其缺少能够适应不同SR算法与缩放因子的评估手段与数据集。

Method: 假设SR图像质量退化主要受SR算法影响而非内容主导。设计自监督对比学习：同一SR模型生成的图像作为正样本、不同模型生成的作为负样本，学习“SR模型导向”的表示；加入针对性预处理以提取互补的质量线索，并引入辅助任务以区分不同放大倍率的退化特征；构建SRMORSS数据集，涵盖多种SR算法在大量真实LR上的结果，用于无监督预训练；最终进行领域自适应无参考IQA。

Result: 在多个人工标注的真实场景SR-IQA基准上，S3 RIQA在相关性与一致性等指标上稳定超过大多数最新方法，表现出更好的跨域与数据稀缺适应能力。

Conclusion: 通过以SR算法为中心的自监督对比学习与配套数据集，S3 RIQA有效建模真实场景SR退化并实现域自适应的无参考质量评估，在实际应用中更可靠、可泛化。

Abstract: Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.

</details>


### [52] [Spectral-Spatial Contrastive Learning Framework for Regression on Hyperspectral Data](https://arxiv.org/abs/2602.10745)
*Mohamad Dhaini,Paul Honeine,Maxime Berar,Antonin Van Exem*

Main category: cs.CV

TL;DR: 提出一个面向高光谱回归任务的谱-空对比学习框架，配套高光谱数据增强变换，可无关模型地提升3D卷积与Transformer等骨干，在合成与真实数据上显著增益。


<details>
  <summary>Details</summary>
Motivation: 对比学习在分类表现优异，但针对回归尤其是高光谱数据的研究不足；需要能泛化到不同骨干的自监督/对比学习方法与适配的高光谱增强来提升回归性能。

Method: 设计谱-空融合的对比学习框架：以高光谱样本在光谱与空间维度上的一致性为约束，构造正负样本；提供一组专为高光谱数据的增强（如谱带扰动、带段遮蔽、空间裁剪/平移、噪声模拟等）；该框架为模型无关，可与3D CNN、Transformer等结合进行预训练/联合训练以优化回归表示。

Result: 在多个人工与真实高光谱回归数据集上，所有考察的骨干在应用该框架与增强后均有显著性能提升。

Conclusion: 谱-空对比学习与高光谱专属增强能有效提升回归型高光谱任务的表现，且具有模型无关性与广泛适用性。

Abstract: Contrastive learning has demonstrated great success in representation learning, especially for image classification tasks. However, there is still a shortage in studies targeting regression tasks, and more specifically applications on hyperspectral data. In this paper, we propose a spectral-spatial contrastive learning framework for regression tasks for hyperspectral data, in a model-agnostic design allowing to enhance backbones such as 3D convolutional and transformer-based networks. Moreover, we provide a collection of transformations relevant for augmenting hyperspectral data. Experiments on synthetic and real datasets show that the proposed framework and transformations significantly improve the performance of all studied backbone models.

</details>


### [53] [Text-to-Vector Conversion for Residential Plan Design](https://arxiv.org/abs/2602.10757)
*Egor Bazhenov,Stepan Kasai,Viacheslav Shalamov,Valeria Efimova*

Main category: cs.CV

TL;DR: 提出一种从文本生成矢量住宅平面图的方法，并给出将光栅平面图矢量化为结构化矢量图的新算法；在CLIPScore上分别较现有方法提升约5%与4%。


<details>
  <summary>Details</summary>
Motivation: 栅格图形易用但缩放失真，矢量图形可无损缩放但生成复杂。建筑与设计领域强需矢量图的可扩展性与精确几何（如直角）。现有方案在质量与结构约束上不足，需改进自动化生成与矢量化流程。

Method: 1) 文本到矢量平面图：以文本描述为输入，生成由数学基元（线段、矩形、墙体等）构成的住宅平面图，内置对直角与可配置参数的约束与优化，采用CLIPScore做视觉语义一致性度量与训练/选择。2) 栅格到矢量：提出新的矢量化算法，将位图平面图解析为结构化矢量要素，输出规则化的几何与拓扑关系，以提升可编辑性与语义对齐。

Result: 在公开或内部基准上，相比现有方法，文本到矢量生成在CLIPScore上提升约5%；栅格到矢量化的结果作为结构化矢量图，其CLIPScore较其他方法提升约4%。

Conclusion: 所提方法在矢量住宅平面图的自动生成与位图矢量化上均取得稳定提升，尤其受益于对直角结构与灵活参数设置的建模；结果更具可缩放性与编辑性，并在语义-视觉一致性上优于现有技术。

Abstract: Computer graphics, comprising both raster and vector components, is a fundamental part of modern science, industry, and digital communication. While raster graphics offer ease of use, its pixel-based structure limits scalability. Vector graphics, defined by mathematical primitives, provides scalability without quality loss, however, it is more complex to produce. For design and architecture, the versatility of vector graphics is paramount, despite its computational demands. This paper introduces a novel method for generating vector residential plans from textual descriptions. Our approach surpasses existing solutions by approximately 5% in CLIPScore-based visual quality, benefiting from its inherent handling of right angles and flexible settings. Additionally, we present a new algorithm for vectorizing raster plans into structured vector images. Such images have a better CLIPscore compared to others by about 4%.

</details>


### [54] [Dual-End Consistency Model](https://arxiv.org/abs/2602.10764)
*Linwei Dong,Ruoyu Guo,Ge Bai,Zehuan Yuan,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: 提出Dual-End Consistency Model (DE-CM)，通过选择关键子轨迹、连续时间CM蒸馏+流匹配边界正则与噪声到噪声(N2N)映射，缓解一致性模型训练不稳与采样不灵活，实现ImageNet 256 one-step生成SOTA（FID 1.70）。


<details>
  <summary>Details</summary>
Motivation: 扩散/流模型推断慢；一致性模型虽高效但在大规模上受训练不稳定与采样不灵活两问题限制。现有工作多从结构或正则入手，忽略了轨迹选择对稳定性与效率的关键作用。

Method: 1) 理论与实证分析：训练不稳源于自监督项导致的loss发散；采样不灵活源于误差在初步步骤累积。2) 轨迹分解：将PF-ODE轨迹分解为若干子轨迹，选取三个关键子轨迹作为优化目标（“Dual-End”强调起点与终点的双端约束）。3) 连续时间一致性目标实现少步蒸馏；4) 以Flow Matching作为边界正则稳定训练；5) 提出噪声到噪声(N2N)映射，可将噪声映射到任意中间点，缓解首步误差累积并提升灵活采样。

Result: 在ImageNet 256x256上一步生成取得FID 1.70，优于现有基于CM的一步方法；多步也具备高质量与稳定性（文中称“广泛实验”支撑）。

Conclusion: 选择性优化PF-ODE的关键子轨迹、结合连续时间CM蒸馏与流匹配正则、以及N2N映射，可同时解决一致性模型的训练不稳定与采样误差累积问题，实现更稳健、灵活且高效的一步/少步生成。

Abstract: The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.

</details>


### [55] [From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?](https://arxiv.org/abs/2602.10771)
*Krishna Kanth Nakka,Vedasri Nakka*

Main category: cs.CV

TL;DR: 提出CyclingVQA基准，从骑行者视角评估VLM在感知、时空理解与交通规则到车道推理上的能力，测试31+模型发现通用VLM有时优于驾驶专用模型，暴露出对骑行特有线索与标志-车道关联的薄弱点，并给出失败模式以指导改进。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在自动驾驶上的评测多以汽车为中心，忽视骑行者在城市交通中的安全与决策需求；缺乏从骑行者视角系统检验感知与规则推理的基准。

Method: 构建CyclingVQA诊断型VQA基准，覆盖骑行者相关的感知、时空理解、以及交通标志与可行驶车道关联的推理任务；收集与标注数据并对31+近期VLM（通用、空间增强、驾驶专用）进行统一评测与系统误差分析。

Result: 当前VLM在部分任务上表现可喜，但在骑行者特定线索识别、将交通标志正确映射到相应车道等方面存在明显不足；多款驾驶专用模型在该基准上落后于强通用VLM，显示车辆中心训练难以直接迁移到骑行辅助场景。

Conclusion: CyclingVQA填补骑行者视角评测空白，揭示了VLM在骑行场景感知与规则-车道推理上的关键短板与常见失败模式，为面向骑行安全的智能辅助系统研发提供方向。

Abstract: Cyclists often encounter safety-critical situations in urban traffic, highlighting the need for assistive systems that support safe and informed decision-making. Recently, vision-language models (VLMs) have demonstrated strong performance on autonomous driving benchmarks, suggesting their potential for general traffic understanding and navigation-related reasoning. However, existing evaluations are predominantly vehicle-centric and fail to assess perception and reasoning from a cyclist-centric viewpoint. To address this gap, we introduce CyclingVQA, a diagnostic benchmark designed to probe perception, spatio-temporal understanding, and traffic-rule-to-lane reasoning from a cyclist's perspective. Evaluating 31+ recent VLMs spanning general-purpose, spatially enhanced, and autonomous-driving-specialized models, we find that current models demonstrate encouraging capabilities, while also revealing clear areas for improvement in cyclist-centric perception and reasoning, particularly in interpreting cyclist-specific traffic cues and associating signs with the correct navigational lanes. Notably, several driving-specialized models underperform strong generalist VLMs, indicating limited transfer from vehicle-centric training to cyclist-assistive scenarios. Finally, through systematic error analysis, we identify recurring failure modes to guide the development of more effective cyclist-assistive intelligent systems.

</details>


### [56] [RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation](https://arxiv.org/abs/2602.10799)
*Zihui Zhou,Yong Feng,Yanying Chen,Guofan Duan,Zhenxi Song,Mingliang Zhou,Weijia Jia*

Main category: cs.CV

TL;DR: 论文提出RSHallu体系，系统研究遥感多模态大模型中的幻觉问题，给出分类框架、评测基准与检测器，以及训练友好与即插即用的缓解策略，在多种RS-MLLM上显著提升无幻觉率（最高+21.63个百分点），且保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: RS场景中MLLM逐步应用于RSVQA、RSVG与多轮对话，但在应急与农业等高风险场景中，模型常产生与影像不一致的“幻觉”，且该问题在RS领域缺乏系统化定义、基准与有效缓解方案。需要RS特定的刻画、可复现实证与高性价比的检测与缓解手段。

Method: 1) 概念与分类：提出面向RS的幻觉分类法，并新增图像级幻觉概念，覆盖模态、分辨率、场景语义等超越目标级别的不一致。2) 评测与校验：构建RSHalluEval（2,023 QA），引入“双模校验”——云端高精审计与本地低成本可复现校验；并用RSHalluCheck（15,396 QA）微调紧凑检查器。3) 缓解：发布RSHalluShield（30k QA）用于训练友好缓解；提出无需训练的即插即用策略，包括解码阶段logit校正与RS感知提示。统一协议下在多种RS-MLLM评测。

Result: 在代表性RS-MLLM上，无幻觉率最高提升21.63个百分点；同时在RSVQA/RSVG等下游任务上维持有竞争力的效果。提供代码与数据集以复现。

Conclusion: RSHallu为RS领域系统化刻画、评测与缓解MLLM幻觉提供了完整方案：从分类与图像级定义，到基准与双模检查器，再到训练友好与即插即用的缓解策略；在不牺牲下游性能的前提下显著降低幻觉，具有实际部署价值。

Abstract: Multimodal large language models (MLLMs) are increasingly adopted in remote sensing (RS) and have shown strong performance on tasks such as RS visual grounding (RSVG), RS visual question answering (RSVQA), and multimodal dialogue. However, hallucinations, which are responses inconsistent with the input RS images, severely hinder their deployment in high-stakes scenarios (e.g., emergency management and agricultural monitoring) and remain under-explored in RS. In this work, we present RSHallu, a systematic study with three deliverables: (1) we formalize RS hallucinations with an RS-oriented taxonomy and introduce image-level hallucination to capture RS-specific inconsistencies beyond object-centric errors (e.g., modality, resolution, and scene-level semantics); (2) we build a hallucination benchmark RSHalluEval (2,023 QA pairs) and enable dual-mode checking, supporting high-precision cloud auditing and low-cost reproducible local checking via a compact checker fine-tuned on RSHalluCheck dataset (15,396 QA pairs); and (3) we introduce a domain-tailored dataset RSHalluShield (30k QA pairs) for training-friendly mitigation and further propose training-free plug-and-play strategies, including decoding-time logit correction and RS-aware prompting. Across representative RS-MLLMs, our mitigation improves the hallucination-free rate by up to 21.63 percentage points under a unified protocol, while maintaining competitive performance on downstream RS tasks (RSVQA/RSVG). Code and datasets will be released.

</details>


### [57] [DMP-3DAD: Cross-Category 3D Anomaly Detection via Realistic Depth Map Projection with Few Normal Samples](https://arxiv.org/abs/2602.10806)
*Zi Wang,Katsuya Hotta,Koichiro Kamide,Yawen Zou,Jianjian Qin,Chao Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 提出DMP-3DAD：一种无需训练的跨类别3D点云异常检测方法，基于多视角逼真深度图投影与冻结CLIP视觉编码器，通过加权特征相似度在小样本下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测多依赖按类别训练，难以在小样本与跨类别场景中泛化与部署；需要一种不依赖类别特定微调、可直接应用的通用方案。

Method: 将点云统一投影为固定集合的多视角逼真深度图；利用冻结的CLIP视觉编码器提取多视角特征；通过加权特征相似度度量实现异常检测，无需微调与类别自适应。

Result: 在ShapeNetPart数据集少样本设定下取得当前最优（SOTA）表现，广泛实验验证其有效性。

Conclusion: DMP-3DAD以简单流程、零训练成本和强泛化能力，为跨类别3D点云异常检测提供了实用有效的解决方案。

Abstract: Cross-category anomaly detection for 3D point clouds aims to determine whether an unseen object belongs to a target category using only a few normal examples. Most existing methods rely on category-specific training, which limits their flexibility in few-shot scenarios. In this paper, we propose DMP-3DAD, a training-free framework for cross-category 3D anomaly detection based on multi-view realistic depth map projection. Specifically, by converting point clouds into a fixed set of realistic depth images, our method leverages a frozen CLIP visual encoder to extract multi-view representations and performs anomaly detection via weighted feature similarity, which does not require any fine-tuning or category-dependent adaptation. Extensive experiments on the ShapeNetPart dataset demonstrate that DMP-3DAD achieves state-of-the-art performance under few-shot setting. The results show that the proposed approach provides a simple yet effective solution for practical cross-category 3D anomaly detection.

</details>


### [58] [DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories](https://arxiv.org/abs/2602.10809)
*Chenlong Deng,Mengjie Deng,Junjie Wu,Dun Zeng,Teng Wang,Qingsong Xie,Jiadeng Huang,Shengjie Ma,Changwang Zhang,Zhaoxiang Wang,Jun Wang,Yutao Zhu,Zhicheng Dou*

Main category: cs.CV

TL;DR: 论文提出DeepImageSearch，将图像检索从单张匹配转变为面向视觉历史的自主探索与多步推理任务，并发布基准DISBench与一个模块化代理基线，实验表明现有SOTA在此设定下显著受挫，凸显“代理式推理”在下一代检索中的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索多以独立样本为单位进行语义匹配，忽视现实视觉流中跨时间、跨场景的上下文依赖与隐含线索，导致对长时程、分布式信息的目标定位能力不足。作者希望构建能利用视觉历史与上下文的检索范式与评测基准，推动模型具备规划与多步推理能力。

Method: 1) 提出DeepImageSearch：将检索视为代理需在原始视觉历史上规划、感知、决策与多步导航的任务；2) 构建DISBench：以互联的视觉数据为载体，要求基于隐式上下文定位目标；3) 数据生成流水线：人-模型协作，先用视觉语言模型挖掘潜在时空关联与上下文，再由人类校验，降低规模化构建成本；4) 基线系统：模块化代理框架，配备细粒度工具与双重记忆（短期/长期）以支持长程导航与推理。

Result: 在DISBench上，当前SOTA多模态检索与大模型在需要利用时空上下文、进行多步推理和长程导航的情境中表现不佳；所提出的代理式基线虽有改进但仍存在较大提升空间，验证了基准的挑战性与设定的有效性。

Conclusion: 多模态检索需要从静态、独立匹配转向具备规划与多步推理的“代理式”范式；DISBench与DeepImageSearch为该方向提供了任务定义、数据与基线，实验结果强调了在下一代检索系统中引入代理推理与记忆机制的必要性。

Abstract: Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.

</details>


### [59] [Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training](https://arxiv.org/abs/2602.10815)
*Aojun Lu,Tao Feng,Hangjie Yuan,Wei Li,Yanan Sun*

Main category: cs.CV

TL;DR: 论文发现VLM在后训练中RL优于SFT的OOD泛化源于数据难度分布差异，提出基于样本难度筛选的DC-SFT，可在更稳定、更高效的前提下超越RL与常规SFT。


<details>
  <summary>Details</summary>
Motivation: 观察到RL微调在OOD任务上常显著优于SFT，但原因不清晰。作者提出一个数据中心视角：RL在优化过程中隐式偏好中等难度样本，从而带来更好的泛化；而SFT常混入过多“困难样本”，导致过拟合与泛化退化。

Method: 1) 定义并量化样本难度，构建难度分层的数据集；2) 系统评估SFT在不同难度子集上的OOD泛化；3) 基于发现提出DC-SFT：在SFT前按难度阈值显式过滤，保留中等难度样本进行训练；4) 与RL和标准SFT在多数据集/任务上对比，并评估稳定性与计算开销。

Result: 实验显示：训练包含大量困难样本会显著降低OOD性能；仅用中等难度样本进行SFT能明显提升OOD泛化。DC-SFT在多项指标上超过标准SFT，并且在多数设置下优于RL微调，同时表现出更低的方差与更小的计算成本。

Conclusion: VLM的OOD泛化差距主要由训练数据难度分布驱动。通过难度策展的SFT即可复现并超越RL的泛化优势，提供了一条稳定、经济的强泛化训练路径；建议今后的训练和数据管线重视样本难度估计与筛选。

Abstract: The adaptation of large-scale Vision-Language Models (VLMs) through post-training reveals a pronounced generalization gap: models fine-tuned with Reinforcement Learning (RL) consistently achieve superior out-of-distribution (OOD) performance compared to those trained with Supervised Fine-Tuning (SFT). This paper posits a data-centric explanation for this phenomenon, contending that RL's generalization advantage arises from an implicit data filtering mechanism that inherently prioritizes medium-difficulty training samples. To test this hypothesis, we systematically evaluate the OOD generalization of SFT models across training datasets of varying difficulty levels. Our results confirm that data difficulty is a critical factor, revealing that training on hard samples significantly degrades OOD performance. Motivated by this finding, we introduce Difficulty-Curated SFT (DC-SFT), a straightforward method that explicitly filters the training set based on sample difficulty. Experiments show that DC-SFT not only substantially enhances OOD generalization over standard SFT, but also surpasses the performance of RL-based training, all while providing greater stability and computational efficiency. This work offers a data-centric account of the OOD generalization gap in VLMs and establishes a more efficient pathway to achieving robust generalization. Code is available at https://github.com/byyx666/DC-SFT.

</details>


### [60] [Resource-Efficient RGB-Only Action Recognition for Edge Deployment](https://arxiv.org/abs/2602.10818)
*Dongsik Yoon,Jongeun Kim,Dayeon Lee*

Main category: cs.CV

TL;DR: 提出一种仅用RGB的轻量级动作识别网络，基于X3D并结合Temporal Shift，加入选择性时域自适应和无参注意力；在NTU RGB+D 60/120上实现高效准确的权衡，并在Jetson Orin Nano上验证更小资源占用与实际可部署性。


<details>
  <summary>Details</summary>
Motivation: 边缘设备在时延、内存、存储与功耗上受限；虽然骨架/深度等多模态可提升准确率，但依赖额外传感器或昂贵姿态估计，削弱边缘场景的实用性。因此需要一个仅用RGB、推理高效、资源占用低且精度具竞争力的模型。

Method: 以X3D风格的轻量3D视觉骨干为基础，集成Temporal Shift以无额外参数地建模时序；进一步提出选择性时域自适应模块以在关键时间维上强化表征，并引入参数无关的注意力机制以凸显重要通道/时刻；整体设计面向设备端高效推理与小占用。

Result: 在NTU RGB+D 60与120数据集上，取得在精度与效率间的良好平衡，超过或匹配现有RGB方法的同时显著降低计算/内存；实机在Jetson Orin Nano上的部署剖析显示更小的设备端占用与更合理的资源利用。

Conclusion: 仅用RGB也能在边缘设备上实现实用的动作识别：通过X3D+Temporal Shift并辅以选择性时域自适应和无参注意力，可在保持或提升准确率的同时，显著降低延迟与资源消耗，具备真实世界部署价值。

Abstract: Action recognition on edge devices poses stringent constraints on latency, memory, storage, and power consumption. While auxiliary modalities such as skeleton and depth information can enhance recognition performance, they often require additional sensors or computationally expensive pose-estimation pipelines, limiting practicality for edge use. In this work, we propose a compact RGB-only network tailored for efficient on-device inference. Our approach builds upon an X3D-style backbone augmented with Temporal Shift, and further introduces selective temporal adaptation and parameter-free attention. Extensive experiments on the NTU RGB+D 60 and 120 benchmarks demonstrate a strong accuracy-efficiency balance. Moreover, deployment-level profiling on the Jetson Orin Nano verifies a smaller on-device footprint and practical resource utilization compared to existing RGB-based action recognition techniques.

</details>


### [61] [Flow caching for autoregressive video generation](https://arxiv.org/abs/2602.10825)
*Yuexiao Ma,Xuzhe Zheng,Jing Xu,Xiwei Xu,Feng Ling,Xiawu Zheng,Huafeng Kuang,Huixia Li,Xing Wang,Xuefeng Xiao,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: FlowCache 是首个面向自回归视频生成的缓存框架：按视频分块独立制定缓存与重算策略，并结合重要性-冗余联合优化的KV缓存压缩，在固定内存下显著提速且基本不降质。


<details>
  <summary>Details</summary>
Motivation: 自回归超长视频生成需按块顺序合成，极慢；传统加速方法假设各帧同质去噪，适用于扩散但不适用于自回归，因不同块在同一时间步的相似性分布不同，导致现有缓存策略低效或失效。

Method: 提出 FlowCache：1) 分块级（chunkwise）缓存策略——每个视频块独立维护缓存与重算策略，动态决定在每个时间步哪些块需更新；2) 联合“重要性-冗余”优化的KV缓存压缩——在固定显存预算下对注意力KV缓存进行选择与压缩，最大化保真同时控制内存；3) 与Transformer式自回归生成无缝集成，适配异步/不同步的去噪动态。

Result: 在 MAGI-1 上加速 2.38×，在 SkyReels-V2 上加速 6.7×；画质几乎无损（VBench：+0.87 与 -0.79 变化，分别对应两个数据集）。

Conclusion: 对自回归视频生成，统一的逐帧缓存并不适用；FlowCache 通过分块自适应缓存和KV压缩，在固定内存下实现显著加速且保持质量，为实时超长视频生成提供可行路径并树立高效合成新基线。

Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

</details>


### [62] [Hyperspectral Smoke Segmentation via Mixture of Prototypes](https://arxiv.org/abs/2602.10858)
*Lujian Yao,Haitao Zhao,Xianghai Kong,Yuhan Xu*

Main category: cs.CV

TL;DR: 引入高光谱/多光谱用于烟雾分割，发布首个高光谱数据集与一个多光谱数据集，并提出原型混合网络（MoP）通过频带拆分、原型表示与双层路由自适应加权，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可见光方法在云层干扰与半透明烟雾下易失效，因光谱信息不足。不同波段在不同空间区域的判别力不一，需要自适应的波段加权策略；现有方法存在光谱交互污染、光谱模式建模不足及复杂加权路由难题。

Method: 1) 构建首个高光谱烟雾分割数据集HSSDataset（基于Many-to-One标注，来自20个真实场景、18k+帧）。2) 另建RGB-红外多光谱数据集MSSDataset。3) 提出MoP网络：a) 频带拆分以实现光谱隔离；b) 基于原型的光谱表征以覆盖多样光谱模式；c) 双层路由器在空间上自适应地为各波段加权。

Result: 在高光谱与多光谱两种模态上进行大量实验，MoP取得显著优于现有方法的分割性能，验证了方法与数据集的有效性。

Conclusion: 高/多光谱信息能够显著提升烟雾分割的鲁棒性；通过MoP实现自适应波段加权可缓解云干扰与半透明区域难题，提出的数据集与方法为光谱驱动的烟雾分割确立新范式。

Abstract: Smoke segmentation is critical for wildfire management and industrial safety applications. Traditional visible-light-based methods face limitations due to insufficient spectral information, particularly struggling with cloud interference and semi-transparent smoke regions. To address these challenges, we introduce hyperspectral imaging for smoke segmentation and present the first hyperspectral smoke segmentation dataset (HSSDataset) with carefully annotated samples collected from over 18,000 frames across 20 real-world scenarios using a Many-to-One annotations protocol. However, different spectral bands exhibit varying discriminative capabilities across spatial regions, necessitating adaptive band weighting strategies. We decompose this into three technical challenges: spectral interaction contamination, limited spectral pattern modeling, and complex weighting router problems. We propose a mixture of prototypes (MoP) network with: (1) Band split for spectral isolation, (2) Prototype-based spectral representation for diverse patterns, and (3) Dual-level router for adaptive spatial-aware band weighting. We further construct a multispectral dataset (MSSDataset) with RGB-infrared images. Extensive experiments validate superior performance across both hyperspectral and multispectral modalities, establishing a new paradigm for spectral-based smoke segmentation.

</details>


### [63] [Stride-Net: Fairness-Aware Disentangled Representation Learning for Chest X-Ray Diagnosis](https://arxiv.org/abs/2602.10875)
*Darakshan Rashid,Raza Imam,Dwarikanath Mahapatra,Brejesh Lall*

Main category: cs.CV

TL;DR: 提出Stride-Net，在胸部X光分类中通过可学习步幅掩码、对抗混淆与语义对齐实现公平且高效；在MIMIC-CXR与CheXpert上跨种族与种族-性别子群提升公平性且不降准确。


<details>
  <summary>Details</summary>
Motivation: 现有胸片深度模型在总体上表现强，但对子群（如不同种族/性别）存在性能劣化，影响安全与公平。许多去偏法改进不稳定，或以牺牲整体诊断性能换取公平，将公平视为事后约束而非表征属性。需要一种能学得既有疾病判别力又对敏感属性不变的表征。

Method: 提出Stride-Net：1) 以patch为粒度，使用可学习的stride-based掩码挑选与标签对齐的影像区域；2) 通过对抗混淆损失抑制敏感属性信息；3) 通过群最优传输（Group Optimal Transport）将图像特征与基于BioBERT的疾病标签嵌入进行语义对齐，减少捷径学习。适用于ResNet与ViT等架构。

Result: 在MIMIC-CXR与CheXpert上，针对种族与交叉的种族-性别子群评测，Stride-Net在保持或提升总体准确率的同时，稳定提升多种公平性指标，并较以往方法达到更优的准确-公平权衡。

Conclusion: 通过局部区域选择、对抗去敏与跨模态语义对齐，Stride-Net学到对人口学属性不敏感且具临床语义的特征，在不同架构和数据集上实现更一致的公平改进而不牺牲性能。

Abstract: Deep neural networks for chest X-ray classification achieve strong average performance, yet often underperform for specific demographic subgroups, raising critical concerns about clinical safety and equity. Existing debiasing methods frequently yield inconsistent improvements across datasets or attain fairness by degrading overall diagnostic utility, treating fairness as a post hoc constraint rather than a property of the learned representation. In this work, we propose Stride-Net (Sensitive Attribute Resilient Learning via Disentanglement and Learnable Masking with Embedding Alignment), a fairness-aware framework that learns disease-discriminative yet demographically invariant representations for chest X-ray analysis. Stride-Net operates at the patch level, using a learnable stride-based mask to select label-aligned image regions while suppressing sensitive attribute information through adversarial confusion loss. To anchor representations in clinical semantics and discourage shortcut learning, we further enforce semantic alignment between image features and BioBERT-based disease label embeddings via Group Optimal Transport. We evaluate Stride-Net on the MIMIC-CXR and CheXpert benchmarks across race and intersectional race-gender subgroups. Across architectures including ResNet and Vision Transformers, Stride-Net consistently improves fairness metrics while matching or exceeding baseline accuracy, achieving a more favorable accuracy-fairness trade-off than prior debiasing approaches. Our code is available at https://github.com/Daraksh/Fairness_StrideNet.

</details>


### [64] [Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation](https://arxiv.org/abs/2602.10880)
*Minggui He,Mingchen Dai,Jian Zhang,Yilun Liu,Shimin Tao,Pufan Zeng,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 提出Chart Specification作为结构化中间表示，并结合Spec-Align奖励进行强化学习，以提升图表到代码生成的结构保真度；在3个基准上以更少数据显著超越现有方法，4K样本达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM通过监督微调容易学到表层token仿写而非真实图表结构，导致代码幻觉与语义不一致，难以保持绘图逻辑的结构保真。需要一种能提供语义对齐、可验证且细粒度的结构化监督方式。

Method: 引入Chart Specification作为中间表示，过滤语法噪声并平衡训练分布；设计Spec-Align Reward，对生成代码与规范在结构层面对齐度进行可验证、细粒度打分；以此奖励进行强化学习，约束并提升绘图逻辑一致性与结构正确性。

Result: 在三个公开基准上持续优于已有方法；仅用3K样本即可在复杂基准上最多提升61.7%；扩展到4K样本在所有评测指标上取得新的SOTA，显示显著的数据效率与可扩展性。

Conclusion: 以结构化规范进行精确监督，加上可验证的对齐奖励与强化学习，可有效提升图表到代码生成的结构保真与整体性能，提供一条高效实现高保真生成的路径。

Abstract: Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper

</details>


### [65] [ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.10884)
*Jinqing Zhang,Zehua Fu,Zelin Xu,Wenying Dai,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 提出TR-World与FGTR，利用时序残差专注动态体建模，并与当前静态BEV融合，生成未来BEV以细化轨迹并防止坍塌，在nuScenes与NAVSIM上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶的世界模型虽提升规划，但静态区域冗余建模、与轨迹交互不足，削弱了对动态目标与未来路况的利用效率。

Method: 1) 提出Temporal Residual World Model：对场景表示计算时间残差，仅以残差信号作为输入，去除静态冗余，直接预测动态目标未来空间分布；与当前BEV中的静态信息融合，得到精确未来BEV特征。2) 提出Future-Guided Trajectory Refinement模块：用当前场景先验轨迹与未来BEV双向交互，利用未来路况细化轨迹，并对未来BEV提供稀疏时空监督，抑制世界模型坍塌。

Result: 在nuScenes与NAVSIM数据集上，方法ResWorld获得最先进的规划性能指标，相较基线显著提升。代码已公开。

Conclusion: 专注动态建模的时序残差世界模型结合未来引导的轨迹细化，可更准确地生成未来BEV并提升规划；减少静态冗余并通过稀疏监督稳定训练，达SOTA表现。

Abstract: The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.

</details>


### [66] [FastUSP: A Multi-Level Collaborative Acceleration Framework for Distributed Diffusion Model Inference](https://arxiv.org/abs/2602.10940)
*Guandong Li*

Main category: cs.CV

TL;DR: 提出FastUSP框架，通过编译级(CUDA Graph与算通序重排)、通信级(FP8量化通信)与算子级(双缓冲流水化Ring注意力)优化，提升USP分布式注意力在扩散模型推理的效率；在FLUX与Qwen-Image上实现最高约1.16×端到端加速，并发现核启动开销是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大参数扩散模型（如FLUX 12B、SD3 8B）推理需多GPU并行；现有统一序列并行(USP, 结合Ulysses与Ring注意力)虽为SOTA，但存在过多kernel启动开销与算-通调度不佳等低效问题，限制端到端吞吐。

Method: 多层次优化框架FastUSP：1) 编译级：CUDA Graph图编译以减少kernel启动开销，并对计算与通信进行重新排序以更好重叠；2) 通信级：跨GPU集体通信采用FP8量化以降低带宽/延迟；3) 算子级：对Ring注意力进行流水化并用双缓冲以提升并发与隐藏等待。

Result: 在NVIDIA RTX 5090上：对FLUX(12B)在2/4/8卡上较基线USP获得1.12×–1.16×端到端提速，主要收益来自编译级优化；对Qwen-Image：2卡提速1.09×；4–8卡受PyTorch Inductor与Ring注意力兼容性限制无法进行编译级优化，基线USP相对2卡可扩展至1.30×–1.46×。

Conclusion: FastUSP在多GPU扩散推理中带来稳定加速，并表明在高带宽互联下，瓶颈更多来自kernel启动开销而非通信延迟；编译级图执行与算通重叠是关键。

Abstract: Large-scale diffusion models such as FLUX (12B parameters) and Stable Diffusion 3 (8B parameters) require multi-GPU parallelism for efficient inference. Unified Sequence Parallelism (USP), which combines Ulysses and Ring attention mechanisms, has emerged as the state-of-the-art approach for distributed attention computation. However, existing USP implementations suffer from significant inefficiencies including excessive kernel launch overhead and suboptimal computation-communication scheduling. In this paper, we propose \textbf{FastUSP}, a multi-level optimization framework that integrates compile-level optimization (graph compilation with CUDA Graphs and computation-communication reordering), communication-level optimization (FP8 quantized collective communication), and operator-level optimization (pipelined Ring attention with double buffering). We evaluate FastUSP on FLUX (12B) and Qwen-Image models across 2, 4, and 8 NVIDIA RTX 5090 GPUs. On FLUX, FastUSP achieves consistent \textbf{1.12$\times$--1.16$\times$} end-to-end speedup over baseline USP, with compile-level optimization contributing the dominant improvement. On Qwen-Image, FastUSP achieves \textbf{1.09$\times$} speedup on 2 GPUs; on 4--8 GPUs, we identify a PyTorch Inductor compatibility limitation with Ring attention that prevents compile optimization, while baseline USP scales to 1.30$\times$--1.46$\times$ of 2-GPU performance. We further provide a detailed analysis of the performance characteristics of distributed diffusion inference, revealing that kernel launch overhead -- rather than communication latency -- is the primary bottleneck on modern high-bandwidth GPU interconnects.

</details>


### [67] [Towards Learning a Generalizable 3D Scene Representation from 2D Observations](https://arxiv.org/abs/2602.10943)
*Martin Gromniak,Jan-Gerrit Habekost,Sebastian Kamp,Sven Magg,Stefan Wermter*

Main category: cs.CV

TL;DR: 提出一种可泛化的NeRF方法，从机器人自我视角观测预测全局工作空间的3D占据；在仅40个真实场景训练下，无需场景特定微调即可在新布置上重建含遮挡区域的几何，达到26mm误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在相机坐标系中估计几何，难以直接用于操作任务，且对新场景泛化差。机器人需要在全局工作空间坐标中获得完整、含遮挡区域的3D占据，以支持稳定的抓取与操作。

Method: 构建在全局（机器人/工作空间）坐标系中的占据表示，使用可灵活组合的多源视角输入；以可泛化的NeRF式隐式场进行体渲染/体积占据预测，学习从自我视角观测到全局占据的映射；不对新场景做特定微调。

Result: 在真人形机器人平台上评测，与3D传感器真值对齐，训练仅40个真实场景后，在包含遮挡的区域仍可重建，整体几何重建误差约26毫米，优于传统双目方法的可见范围限制。

Conclusion: 将NeRF从相机系扩展到全局工作空间并具备泛化能力，可直接服务操控任务；模型能在稀少数据下推理不可见区域的占据，为机器人操作提供更完整的几何先验。

Abstract: We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.

</details>


### [68] [Healthy Harvests: A Comparative Look at Guava Disease Classification Using InceptionV3](https://arxiv.org/abs/2602.10967)
*Samanta Ghosh,Shaila Afroz Anika,Umma Habiba Ahmed,B. M. Shahria Alam,Mohammad Tahmid Noor,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 研究以番石榴果实病害三分类（炭疽病、果蝇侵染、健康）为目标，基于Mendeley数据集（473张），统一到256x256 RGB并进行数据增强至3784张，比较InceptionV3与ResNet50，并结合CutMix/MixUp、混淆矩阵与SHAP解释；InceptionV3准确率98.15%优于ResNet50的94.46%。


<details>
  <summary>Details</summary>
Motivation: 番石榴易受多种病害影响，造成品质与产量下降；早期自动识别有助于减损与管理。现有方法在小样本、多变环境下鲁棒性与可解释性不足，故引入先进深度学习与数据增强、混合策略与可解释分析以提升性能与可信度。

Method: - 数据：Mendeley番石榴图像473张，尺寸与格式不一；统一预处理为256x256 RGB。
- 数据增强：常规与“高级”增强，将数据扩展至3784张；引入CutMix与MixUp进行样本混合以提升泛化。
- 模型：训练InceptionV3与ResNet50两种CNN架构；以混淆矩阵评估分类表现；用SHAP分析关注区域以提升可解释性。

Result: InceptionV3取得98.15%分类准确率，ResNet50为94.46%；通过混淆矩阵观察到整体性能较好（细节未给出）；CutMix/MixUp提升了模型鲁棒性（定量增益未详述）；SHAP显示模型能聚焦于与病害相关的显著图像区域。

Conclusion: 在小规模番石榴病害数据集上，结合统一预处理、数据增强与数据混合策略，InceptionV3优于ResNet50并达高准确率；SHAP提升了解释性。结果表明先进深度学习框架可用于番石榴病害的早期识别，但仍需更大、更多样的真实场景数据与更全面指标验证以确保泛化。

Abstract: Guava fruits often suffer from many diseases. This can harm fruit quality and fruit crop yield. Early identification is important for minimizing damage and ensuring fruit health. This study focuses on 3 different categories for classifying diseases. These are Anthracnose, Fruit flies, and Healthy fruit. The data set used in this study is collected from Mendeley Data. This dataset contains 473 original images of Guava. These images vary in size and format. The original dataset was resized to 256x256 pixels with RGB color mode for better consistency. After this, the Data augmentation process is applied to improve the dataset by generating variations of the original images. The augmented dataset consists of 3784 images using advanced preprocessing techniques. Two deep learning models were implemented to classify the images. The InceptionV3 model is well known for its advanced framework. These apply multiple convolutional filters for obtaining different features effectively. On the other hand, the ResNet50 model helps to train deeper networks by using residual learning. The InceptionV3 model achieved the impressive accuracy of 98.15%, and ResNet50got 94.46% accuracy. Data mixing methods such as CutMix and MixUp were applied to enhance the model's robustness. The confusion matrix was used to evaluate the overall model performance of both InceptionV3 and Resnet50. Additionally, SHAP analysis is used to improve interpretability, which helps to find the significant parts of the image for the model prediction. This study purposes to highlight how advanced models enhan

</details>


### [69] [VFGS-Net: Frequency-Guided State-Space Learning for Topology-Preserving Retinal Vessel Segmentation](https://arxiv.org/abs/2602.10978)
*Ruiqi Song,Lei Liu,Ya-Nan Zhang,Chao Wang,Xiaoning Li,Nan Mu*

Main category: cs.CV

TL;DR: 提出VFGS-Net，一种结合频域注意、双路径卷积与双向不对称空间状态建模（基于Mamba2）的视网膜血管分割网络，在四个数据集上于细小血管、分叉与低对比区域上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保留微细毛细血管与整体拓扑连续性，原因在于血管细长、尺度变化大、对比度低，导致局部纹理与全局依赖难以兼顾。

Method: 端到端VFGS-Net：1) 双路径特征卷积模块，同时学习细粒度局部纹理与多尺度上下文；2) 新的基于频域的血管感知通道注意，在高层特征中自适应重加权谱分量以增强与血管相关的响应；3) 瓶颈处引入基于Mamba2的双向不对称空间建模块，高效捕获长程空间依赖，强化血管全局连续性。

Result: 在四个公开视网膜血管数据集上取得具有竞争力或更优的结果，细小血管、复杂分叉及低对比区域的分割准确率显著提升。

Conclusion: 频域增强+双路径卷积+状态空间长程建模的统一框架能兼顾细节与全局拓扑，提升鲁棒性与临床潜力。

Abstract: Accurate retinal vessel segmentation is a critical prerequisite for quantitative analysis of retinal images and computer-aided diagnosis of vascular diseases such as diabetic retinopathy. However, the elongated morphology, wide scale variation, and low contrast of retinal vessels pose significant challenges for existing methods, making it difficult to simultaneously preserve fine capillaries and maintain global topological continuity. To address these challenges, we propose the Vessel-aware Frequency-domain and Global Spatial modeling Network (VFGS-Net), an end-to-end segmentation framework that seamlessly integrates frequency-aware feature enhancement, dual-path convolutional representation learning, and bidirectional asymmetric spatial state-space modeling within a unified architecture. Specifically, VFGS-Net employs a dual-path feature convolution module to jointly capture fine-grained local textures and multi-scale contextual semantics. A novel vessel-aware frequency-domain channel attention mechanism is introduced to adaptively reweight spectral components, thereby enhancing vessel-relevant responses in high-level features. Furthermore, at the network bottleneck, we propose a bidirectional asymmetric Mamba2-based spatial modeling block to efficiently capture long-range spatial dependencies and strengthen the global continuity of vascular structures. Extensive experiments on four publicly available retinal vessel datasets demonstrate that VFGS-Net achieves competitive or superior performance compared to state-of-the-art methods. Notably, our model consistently improves segmentation accuracy for fine vessels, complex branching patterns, and low-contrast regions, highlighting its robustness and clinical potential.

</details>


### [70] [DFIC: Towards a balanced facial image dataset for automatic ICAO compliance verification](https://arxiv.org/abs/2602.10985)
*Nuno Gonçalves,Diogo Nunes,Carla Guerra,João Marcos*

Main category: cs.CV

TL;DR: 提出并公开DFIC数据集（约5.8万张标注人脸图像与2706段视频、逾1000名受试者），覆盖大量不合规与合规样本；并基于DFIC微调了以空间注意力为核心的新方法，用于自动化ICAO人像合规校验，优于现有SOTA。DFIC具备更均衡的人口分布与近乎均匀的一个分区，有助于鲁棒、公平与泛化能力提升，并可扩展到安全、隐私与公平相关的人脸识别应用。


<details>
  <summary>Details</summary>
Motivation: MRTD（机读旅行证件）的人脸照片需满足ISO/IEC与ICAO标准以确保可靠身份验证。人工质检在高通量场景低效且易错，现有公开数据在人口分布与不合规情况的覆盖不足，限制了自动化合规检测方法的发展与评测。

Method: 构建并发布DFIC：规模大、标注充分、含合规与多类不合规条件、包含图像与视频、人口统计更均衡且提供近乎均匀分布分区。基于DFIC微调一种以空间注意力为主的新合规验证模型，并与现有面向ICAO合规检测的SOTA方法进行对比评测。

Result: 在DFIC上训练/微调的空间注意力模型在ICAO合规自动校验任务上优于SOTA基线；数据集展现出更好的多样性与人口统计均衡性，支持更鲁棒与可适配的模型学习。

Conclusion: DFIC填补了ICAO合规检测领域在规模、标注与人口均衡方面的数据缺口，提升了自动化合规验证的性能与泛化潜力；其多样性使其对提升人脸识别系统的安全性、隐私性与公平性也有价值，并将促进后续模型训练与基准评测。

Abstract: Ensuring compliance with ISO/IEC and ICAO standards for facial images in machine-readable travel documents (MRTDs) is essential for reliable identity verification, but current manual inspection methods are inefficient in high-demand environments. This paper introduces the DFIC dataset, a novel comprehensive facial image dataset comprising around 58,000 annotated images and 2706 videos of more than 1000 subjects, that cover a broad range of non-compliant conditions, in addition to compliant portraits. Our dataset provides a more balanced demographic distribution than the existing public datasets, with one partition that is nearly uniformly distributed, facilitating the development of automated ICAO compliance verification methods.
  Using DFIC, we fine-tuned a novel method that heavily relies on spatial attention mechanisms for the automatic validation of ICAO compliance requirements, and we have compared it with the state-of-the-art aimed at ICAO compliance verification, demonstrating improved results. DFIC dataset is now made public (https://github.com/visteam-isr-uc/DFIC) for the training and validation of new models, offering an unprecedented diversity of faces, that will improve both robustness and adaptability to the intrinsically diverse combinations of faces and props that can be presented to the validation system. These results emphasize the potential of DFIC to enhance automated ICAO compliance methods but it can also be used in many other applications that aim to improve the security, privacy, and fairness of facial recognition systems.

</details>


### [71] [Interpretable Vision Transformers in Image Classification via SVDA](https://arxiv.org/abs/2602.10994)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 将SVD启发的注意力（SVDA）引入ViT，使注意力更可解释、更稀疏并具谱结构，同时保持分类精度；在多数据集上验证其有效性，并提供训练中注意力动力学的可解释性指标。


<details>
  <summary>Details</summary>
Motivation: 传统ViT注意力机制往往不透明、稠密、缺乏结构，不利于解释、诊断与模型压缩；需要一种几何与谱视角下更具结构性与可解释性的注意力机制与评估指标。

Method: 将先前提出的SVDA注意力机制适配到ViT，给出几何上有依据的公式，引入/沿用可解释性指标准测训练过程中的注意力动力学与结构属性（如稀疏性与谱特征）。

Result: 在CIFAR-10、FashionMNIST、CIFAR-100、ImageNet-100上，SVDA在不降低分类准确率的前提下，产生更可解释、更稀疏、具有明确谱结构的注意力图与表示。

Conclusion: SVDA为计算机视觉中的结构化注意力提供了全面的信息化分析工具，奠定了可解释AI、谱诊断与基于注意力的模型压缩的基础；当前更多提供描述性洞见，未来可发展为具处方性的训练与设计指导。

Abstract: Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.

</details>


### [72] [Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception](https://arxiv.org/abs/2602.11004)
*Liangkai Liu,Kang G. Shin,Jinkyu Lee,Chengmo Yang,Weisong Shi*

Main category: cs.CV

TL;DR: PP-DNN通过动态挑选关键帧与ROI并跨多租户DNN调度，在不牺牲准确度的前提下降低图像处理量，实现更可预测、低时延且高性价比的自动驾驶感知。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知需要在资源受限硬件上实时运行多路DNN，传统方法多依赖模型压缩（剪枝/量化）以加速推理，但难以保证在多任务、多模型并发下的时延可预测性与系统级融合周期稳定性。作者希望在不更改或最小更改DNN的前提下，从输入数据侧降低计算并提升时延与融合过程的可预测性。

Method: 提出PP-DNN：1) ROI生成器依据连续帧相似度与交通场景识别动态挑选关键帧与兴趣区域；2) FLOPs/MACs预测器根据所选关键帧与ROI估计推理计算量；3) ROI调度器在多DNN（多租户）间协调关键帧/ROI的处理以满足时延预算；4) 非关键帧由检测预测器进行感知补全；在ROS自动驾驶管线中实现并在BDD100K与nuScenes上评估。

Result: 相较基线，PP-DNN将融合帧数提升至最多7.3倍，融合延迟降低超过2.6倍，延迟抖动降低超过2.3倍；检测完整性提升75.4%，并将性价比提升至多98%，同时保持与原有感知准确度相当。

Conclusion: 从输入数据选择与多模型调度层面实现可预测感知：在不显著牺牲准确度的情况下显著降低计算、时延与抖动，提升多DNN并发环境下的系统级融合效率与鲁棒性，适用于资源受限的自动驾驶平台。

Abstract: Autonomous vehicles (AVs) rely on sensors and deep neural networks (DNNs) to perceive their surrounding environment and make maneuver decisions in real time. However, achieving real-time DNN inference in the AV's perception pipeline is challenging due to the large gap between the computation requirement and the AV's limited resources. Most, if not all, of existing studies focus on optimizing the DNN inference time to achieve faster perception by compressing the DNN model with pruning and quantization. In contrast, we present a Predictable Perception system with DNNs (PP-DNN) that reduce the amount of image data to be processed while maintaining the same level of accuracy for multi-tenant DNNs by dynamically selecting critical frames and regions of interest (ROIs). PP-DNN is based on our key insight that critical frames and ROIs for AVs vary with the AV's surrounding environment. However, it is challenging to identify and use critical frames and ROIs in multi-tenant DNNs for predictable inference. Given image-frame streams, PP-DNN leverages an ROI generator to identify critical frames and ROIs based on the similarities of consecutive frames and traffic scenarios. PP-DNN then leverages a FLOPs predictor to predict multiply-accumulate operations (MACs) from the dynamic critical frames and ROIs. The ROI scheduler coordinates the processing of critical frames and ROIs with multiple DNN models. Finally, we design a detection predictor for the perception of non-critical frames. We have implemented PP-DNN in an ROS-based AV pipeline and evaluated it with the BDD100K and the nuScenes dataset. PP-DNN is observed to significantly enhance perception predictability, increasing the number of fusion frames by up to 7.3x, reducing the fusion delay by >2.6x and fusion-delay variations by >2.3x, improving detection completeness by 75.4% and the cost-effectiveness by up to 98% over the baseline.

</details>


### [73] [Interpretable Vision Transformers in Monocular Depth Estimation via SVDA](https://arxiv.org/abs/2602.11005)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 提出在DPT中引入奇异值分解启发的注意力（SVDA），用可学习对角谱调制解耦方向对齐与谱强度，使注意力内在可解释，并提供6个可量化谱指标；在KITTI与NYU-v2上准确率持平或略升、开销很小。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力强大但不透明，现有解释多为事后可视化、缺乏内在结构与可量化指标；单目深度估计需要对密集预测中的注意力进行更透明、可度量的理解。

Method: 在DPT中将标准q·k^T相似度改写为带有可学习对角矩阵Σ的归一化交互：先做方向对齐（单位范数q,k）再用Σ进行谱调制；由此获得可解释的注意力，并定义6个谱指标（熵、秩、稀疏度、对齐度、选择性、鲁棒性）来量化训练动态与结构特性；实现仅带来轻微计算开销。

Result: 在KITTI与NYU-v2上，SVDA与基线DPT相比精度持平或小幅提升；同时得到稳定、一致的跨数据集与随深度层变化的谱指标模式，揭示注意力在训练中的组织规律，这是标准Transformer难以观测的。

Conclusion: SVDA把注意力从黑箱机制转变为可量化描述子，在不牺牲性能的前提下提供内在可解释性，为单目深度估计与更广泛的密集预测任务提供了通向透明模型的有原则路径。

Abstract: Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.

</details>


### [74] [LaSSM: Efficient Semantic-Spatial Query Decoding via Local Aggregation and State Space Models for 3D Instance Segmentation](https://arxiv.org/abs/2602.11007)
*Lei Yao,Yi Wang,Yawen Cui,Moyun Liu,Lap-Pui Chau*

Main category: cs.CV

TL;DR: LaSSM 提出一种更高效的基于查询的点云场景实例分割方法：用层级语义-空间初始化生成高质量查询，再以坐标引导的状态空间模型解码器进行渐进式细化，在保持SOTA精度的同时显著降低计算量。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的3D实例分割在稀疏点云上初始化困难，且解码器依赖代价高的注意力，易引入噪声与冗余计算，限制了大规模场景的效率与稳定性。

Method: 1) 层级语义-空间查询初始化：从超点出发，结合语义线索与空间分布，覆盖场景且加速收敛。2) 坐标引导SSM解码器：逐步细化查询，引入局部聚合以专注几何连贯区域；设计空间双路径SSM模块，融合坐标以建模查询间依赖，减少注意力开销与噪声。

Result: 在 ScanNet++ V2 榜单夺冠，比此前最优方法 mAP 提升2.5%，FLOPs仅为其1/3；在 ScanNet、ScanNet200、S3DIS、ScanNet++ V1 等基准上也以更低计算量取得有竞争力表现；消融和可视化验证设计有效。

Conclusion: 通过更优的查询初始化与坐标引导的SSM解码，LaSSM在大规模点云实例分割中实现精度与效率的兼得，减少冗余与噪声，具备实用优势并已开源。

Abstract: Query-based 3D scene instance segmentation from point clouds has attained notable performance. However, existing methods suffer from the query initialization dilemma due to the sparse nature of point clouds and rely on computationally intensive attention mechanisms in query decoders. We accordingly introduce LaSSM, prioritizing simplicity and efficiency while maintaining competitive performance. Specifically, we propose a hierarchical semantic-spatial query initializer to derive the query set from superpoints by considering both semantic cues and spatial distribution, achieving comprehensive scene coverage and accelerated convergence. We further present a coordinate-guided state space model (SSM) decoder that progressively refines queries. The novel decoder features a local aggregation scheme that restricts the model to focus on geometrically coherent regions and a spatial dual-path SSM block to capture underlying dependencies within the query set by integrating associated coordinates information. Our design enables efficient instance prediction, avoiding the incorporation of noisy information and reducing redundant computation. LaSSM ranks first place on the latest ScanNet++ V2 leaderboard, outperforming the previous best method by 2.5% mAP with only 1/3 FLOPs, demonstrating its superiority in challenging large-scale scene instance segmentation. LaSSM also achieves competitive performance on ScanNet, ScanNet200, S3DIS and ScanNet++ V1 benchmarks with less computational cost. Extensive ablation studies and qualitative results validate the effectiveness of our design. The code and weights are available at https://github.com/RayYoh/LaSSM.

</details>


### [75] [Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting](https://arxiv.org/abs/2602.11024)
*Rishikesh Bhyri,Brian R Quaranto,Philip J Seger,Kaity Tung,Brendan Fox,Gene Yang,Steven D. Schwaitzberg,Junsong Yuan,Nan Xi,Peter C W Kim*

Main category: cs.CV

TL;DR: 提出“Chain-of-Look”视觉推理框架，通过强制顺序化的视觉链条沿空间轨迹逐一计数，并配合“邻域损失”约束相邻关系，在高密度手术器械计数上优于现有方法；并发布高密度数据集SurgCount-HD（1,464张）。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测或通用多模态大模型在高密度、紧邻、遮挡严重的器械场景中易漏检、重检，且计数过程无序不具可解释性。需要一种更接近人类“顺序扫视—逐一确认”的过程，以提升准确性与物理合理性，保障术中安全。

Method: 1) 设计Chain-of-Look：将计数建模为沿连贯空间轨迹的顺序视觉链，模仿人类从起点到终点逐一扫描并累积计数；2) 邻域损失（neighboring loss）：对视觉链中相邻元素施加空间邻接/物理可达的约束，避免不合理跳跃；3) 构建SurgCount-HD数据集（1,464张高密度器械图像）用于训练与评测。

Result: 在密集器械计数任务上，方法显著优于SOTA计数模型（如CountGD、REC）及多模态大模型（如Qwen、ChatGPT）；在复杂、紧密堆叠场景中表现更稳健，链式推理带来更高的计数精度与一致性。

Conclusion: 顺序化的视觉链和邻域约束可以有效提升高密度场景下的器械计数准确度与可解释性；SurgCount-HD为该领域提供了基准数据。方法为医疗场景中需要精确计数与空间一致性的任务提供了通用思路。

Abstract: Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.

</details>


### [76] [PuriLight: A Lightweight Shuffle and Purification Framework for Monocular Depth Estimation](https://arxiv.org/abs/2602.11066)
*Yujie Chen,Li Zhang,Xiaomeng Chu,Tian Zhang*

Main category: cs.CV

TL;DR: PuriLight是一种轻量高效的自监督单目深度估计框架，结合三大模块（SDC、RAKA、DFSP）在保持结构细节的同时显著降低参数与计算量，达成SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计要么依赖庞大模型导致落地性差，要么轻量化但牺牲结构精度与细节。需要一种既轻量又能保留结构细节的架构，以提升实际部署与性能平衡。

Method: 提出三阶段架构，包含三个关键模块：1) SDC（Shuffle-Dilation Convolution）用于本地特征提取，通过通道打散与空洞卷积结合提升感受野与效率；2) RAKA（Rotation-Adaptive Kernel Attention）进行分层特征增强，引入对旋转敏感/自适应的核注意力提升几何结构感知；3) DFSP（Deep Frequency Signal Purification）做全局频域特征纯化，抑制噪声与伪影保留有用结构信息。三者协同实现轻量且精确的特征提取与处理。

Result: 在广泛实验中，以极少训练参数和较低计算成本达到SOTA性能，兼顾精度与效率（具体数据未在摘要列出）。代码将开源。

Conclusion: PuriLight通过模块化设计在不增加参数负担的前提下兼顾细节保真与计算效率，为自监督单目深度估计提供了可部署的高精度轻量方案。

Abstract: We propose PuriLight, a lightweight and efficient framework for self-supervised monocular depth estimation, to address the dual challenges of computational efficiency and detail preservation. While recent advances in self-supervised depth estimation have reduced reliance on ground truth supervision, existing approaches remain constrained by either bulky architectures compromising practicality or lightweight models sacrificing structural precision. These dual limitations underscore the critical need to develop lightweight yet structurally precise architectures. Our framework addresses these limitations through a three-stage architecture incorporating three novel modules: the Shuffle-Dilation Convolution (SDC) module for local feature extraction, the Rotation-Adaptive Kernel Attention (RAKA) module for hierarchical feature enhancement, and the Deep Frequency Signal Purification (DFSP) module for global feature purification. Through effective collaboration, these modules enable PuriLight to achieve both lightweight and accurate feature extraction and processing. Extensive experiments demonstrate that PuriLight achieves state-of-the-art performance with minimal training parameters while maintaining exceptional computational efficiency. Codes will be available at https://github.com/ishrouder/PuriLight.

</details>


### [77] [Chatting with Images for Introspective Visual Thinking](https://arxiv.org/abs/2602.11073)
*Junfei Wu,Jian Guan,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tienie Tan*

Main category: cs.CV

TL;DR: 提出“与图像聊天”的LVLM框架，通过语言引导的特征调制与动态联合重编码，多区域/多图像上实现更紧耦合的跨模态推理；在八个基准上显著提升，尤其擅长复杂多图与视频空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM多为一次性视觉编码后进行纯文本推理，细粒度视觉信息丢失；“用图像思考”虽能操作图像但视觉状态与语言语义耦合不足，跨区域/多图像的语义与几何关系推理效果差。

Method: 提出“与图像聊天”（chatting with images）：把视觉操作表述为语言引导的特征调制，模型在语言提示引导下对多个图像区域进行动态联合重编码，实现语言推理与视觉状态更新的闭环。具体实现为ViLaVT：配备动态视觉编码器，适配交互式视觉推理；训练采用两阶段课程：先监督微调再强化学习，以诱导有效的推理行为。

Result: 在8个基准上取得稳健一致提升，对复杂多图像与视频空间推理任务提升尤为明显。

Conclusion: 语言引导的动态视觉重编码能有效缓解一次性编码与语义耦合不足的问题，增强跨模态对齐与复杂空间推理能力；ViLaVT验证了该范式的有效性。

Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.

</details>


### [78] [First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges](https://arxiv.org/abs/2602.11086)
*Robyn Larracy,Eve MacDonald,Angkoon Phinyomark,Saeid Rezaei,Mahdi Laghaei,Ali Hajighasem,Aaron Tabor,Erik Scheme*

Main category: cs.CV

TL;DR: StepUP 比赛基于新发布的高分辨率足压数据集 StepUP-P150，评测在鞋类与步速变化等挑战下的足迹生物识别。23 支队伍参赛，最佳团队以 GRM 优化达成 10.77% EER，但对陌生鞋类的泛化仍是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 足压步态识别在安防与安全领域前景广，但受限于缺少大规模、多样化数据，导致对新用户泛化和对鞋类、步速等域移鲁棒性不足。新数据集发布为用深度学习系统性攻克这些问题创造契机。

Method: 组织首届 StepUP 国际竞赛：使用 StepUP-P150 训练模型，并在单独的、针对验证场景和分布转移（如鞋类、速度）的测试集上评估。重点在有限且相对同质的参考数据条件下的验证性能。顶队采用生成式奖励机（GRM）优化策略。

Result: 全球 23 队参赛；最佳团队 Saeid_UCC 取得 10.77% 的等错误率（EER）。多种强力方案出现，但在陌生鞋类条件下性能显著下降。

Conclusion: 大型高分辨率足压数据集推动了基于深度学习的足迹识别进展，竞赛证明了方法有效性，但对鞋类变化的鲁棒性仍是关键未解难题，未来需在跨域泛化、数据增广/合成与鲁棒表示学习上持续突破。

Abstract: Biometric footstep recognition, based on a person's unique pressure patterns under their feet during walking, is an emerging field with growing applications in security and safety. However, progress in this area has been limited by the lack of large, diverse datasets necessary to address critical challenges such as generalization to new users and robustness to shifts in factors like footwear or walking speed. The recent release of the UNB StepUP-P150 dataset, the largest and most comprehensive collection of high-resolution footstep pressure recordings to date, opens new opportunities for addressing these challenges through deep learning. To mark this milestone, the First International StepUP Competition for Biometric Footstep Recognition was launched. Competitors were tasked with developing robust recognition models using the StepUP-P150 dataset that were then evaluated on a separate, dedicated test set designed to assess verification performance under challenging variations, given limited and relatively homogeneous reference data. The competition attracted global participation, with 23 registered teams from academia and industry. The top-performing team, Saeid_UCC, achieved the best equal error rate (EER) of 10.77% using a generative reward machine (GRM) optimization strategy. Overall, the competition showcased strong solutions, but persistent challenges in generalizing to unfamiliar footwear highlight a critical area for future work.

</details>


### [79] [FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference](https://arxiv.org/abs/2602.11105)
*Divya Jyoti Bajpai,Dhruv Bhardwaj,Soumya Roy,Tejas Duseja,Harsh Agarwal,Aashay Sandansing,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: FastFlow是一个可插拔的自适应推理框架，通过在流匹配生成的逐步去噪过程中跳过影响很小的步骤来提速，利用有限差分外推与多臂老虎机策略动态决定跳步数，实现约2.6倍加速且质量基本不降。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在图像/视频生成上质量高但推理慢，现有加速如蒸馏、截断轨迹、一致性方法需要重训、静态且跨任务泛化差。需要一种无需重训、可泛化、在线自适应的加速方法。

Method: 提出FastFlow：1）用前几步的预测构造有限差分速度估计，对未来状态做外推，近似替代某些中间去噪步的全模型前向；2）将“可安全跳过的步数”建模为多臂老虎机问题，在线学习在质量与速度间的最优折中；3）作为推理时插件，无需改动或重训底层流匹配模型，适配图像、视频与编辑。

Result: 在多项图像、视频生成与编辑实验中，无需额外训练即可获得超过2.6倍的速度提升，同时保持高保真输出（质量基本不降）。

Conclusion: 自适应跳步与外推相结合能显著减少冗余去噪计算；通过老虎机策略动态控制跳步，FastFlow在不重训的前提下通用于多任务并稳定提速，提供了实用的流匹配推理加速方案。

Abstract: Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.

</details>


### [80] [HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion](https://arxiv.org/abs/2602.11117)
*Di Chang,Ji Hou,Aljaz Bozic,Assaf Neuberger,Felix Juefei-Xu,Olivier Maury,Gene Wei-Chin Lin,Tuur Stuyck,Doug Roble,Mohammad Soleymani,Stephane Grabli*

Main category: cs.CV

TL;DR: HairWeaver 是一个基于扩散的视频生成管线，可从单张人物图像生成具有真实发丝动态的动画，通过专门的 LoRA 模块实现对头发运动的精细控制并保持写实外观，达到当前最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有单人像动画方法虽能控制身体姿态，但缺乏对头发的专门控制，导致头发僵硬不真实；需要一种既能细粒度控制发丝动态、又能保持人物真实感的方案。

Method: 在视频扩散骨干上引入两个轻量化 LoRA 模块：1) Motion-Context-LoRA 将运动条件（来自专门构造的动态人类运动数据）注入扩散过程，实现对头发运动的精细调控；2) Sim2Real-Domain-LoRA 解决仿真数据与真实图像的域差异，保持主体的照片级外观。同时以CG模拟生成的动态人类数据进行训练以学习发丝随运动的自然响应。

Result: 在综合评估中优于现有方法，生成的发丝细节丰富、动态自然，整体人像动画逼真，展示出新的SOTA表现。

Conclusion: 通过在扩散视频模型中引入运动与域适配的LoRA，HairWeaver 能从单张图像合成具有真实、可控头发动态的人像动画，兼顾可控性与写实性，设立新的性能基线。

Abstract: We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

</details>


### [81] [PhyCritic: Multimodal Critic Models for Physical AI](https://arxiv.org/abs/2602.11124)
*Tianyi Xiong,Shihao Wang,Guilin Liu,Yi Dong,Ming Li,Heng Huang,Jan Kautz,Zhiding Yu*

Main category: cs.CV

TL;DR: PhyCritic 是面向“物理智能（Physical AI）”场景的多模态裁判模型，通过两阶段RLVR流程（物理技能热身+自指裁判微调）提升对包含感知、因果推理与规划任务的判别与打分稳定性与物理正确性，并在物理与通用评测上均显著优于开源基线，作为策略模型也能提升物理任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态裁判主要在通用视觉领域（图文描述、VQA）训练，难以可靠评估涉及物理世界感知、因果与规划等更复杂、开放式的物理智能任务；需要一个能提供成对偏好、数值打分与解释性判据、且在物理一致性上更可靠的评审模型。

Method: 提出 PhyCritic，并用两阶段 RLVR 训练管线：1) 物理技能热身（Physical skill warmup），定向增强与物理相关的感知与推理能力；2) 自指裁判微调（self-referential critic finetuning），裁判在评判前先生成自身“内部参考预测”，再据此评估候选回答，从而提升判决稳定性与物理正确性。

Result: 在物理向与通用多模态评审基准上，相比开源基线取得显著性能提升；当将 PhyCritic 用作策略模型时，还能进一步改善在具身/物理约束任务中的感知与推理表现。

Conclusion: 面向物理AI的专用裁判通过两阶段RLVR与自指评审策略可带来更稳定、更物理一致的判断，并具备外溢效应：既提升评审，也能反哺任务执行模型的物理感知与推理。

Abstract: With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.

</details>


### [82] [Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling](https://arxiv.org/abs/2602.11146)
*Gongye Liu,Bo Yang,Yida Zhi,Zhizhou Zhong,Lei Ke,Didan Deng,Han Gao,Yongxiang Huang,Kaihao Zhang,Hongbo Fu,Wenhan Luo*

Main category: cs.CV

TL;DR: 提出DiNa-LRM：在扩散噪声空间直接进行偏好学习的潜在奖励模型，采用噪声校准Thurstone似然与时间步条件化奖励头，并支持推理时噪声集成；以更低算力成本在对齐基准上优于扩散系奖励方法并接近SOTA VLM，提升偏好优化效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化依赖VLM作为奖励器，虽稳健但计算/内存开销大；且用像素空间奖励去优化潜在扩散模型存在域不匹配，影响对齐效果与效率。需要一种与扩散过程原生兼容、低开销且鲁棒的奖励建模方式。

Method: 构建扩散原生的潜在奖励模型：在带噪潜在状态上定义奖励，使用噪声依赖不确定性的噪声校准Thurstone似然；以预训练潜在扩散骨干为特征抽取器，叠加时间步（timestep）条件化奖励头；推理阶段进行噪声集成（对不同噪声层面的奖励进行集成）以实现测试时扩展与鲁棒打分。

Result: 在多项图像对齐基准上显著优于现有扩散系奖励基线，且以远低于VLM的计算成本达到与SOTA VLM相近的对齐表现。

Conclusion: 在扩散噪声空间进行偏好建模并结合噪声校准似然与时间步条件化奖励，可在保证鲁棒性的同时大幅降低算力开销，改善偏好优化的收敛速度与资源效率，为扩散与流匹配模型的高效对齐提供可扩展的奖励机制。

Abstract: Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.

</details>


### [83] [SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos](https://arxiv.org/abs/2602.11154)
*Yue Gao,Hong-Xing Yu,Sanghyeon Chang,Qianxi Fu,Bo Zhu,Yoonjin Won,Juan Carlos Niebles,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出SurfPhase：从稀疏视角重建两相流三维界面动态，结合动态高斯surfel与SDF并用视频扩散模型进行新视角合成与重建增强，在高速池沸腾数据上仅用双目即可生成高质量视角合成并估计速度。


<details>
  <summary>Details</summary>
Motivation: 两相流界面决定动量、传热与传质，但实验测量困难；传统方法在运动界面附近受限，现有神经渲染多针对单相、边界弥散场景，难以处理尖锐、可变形的液-汽界面。需要一种能在少视角条件下准确重建清晰、可变形界面的新方法。

Method: 提出SurfPhase：1) 用动态高斯surfel表示随时间变化的表面；2) 结合SDF（有符号距离函数）以保持几何一致性与清晰界面；3) 引入视频扩散模型进行新视角视频合成，用于从稀疏观测中自监督/蒸馏式细化重建；4) 在仅两机位输入下同时实现三维几何与界面速度估计。

Result: 在新构建的高速池沸腾视频数据集上，SurfPhase实现高质量新视角合成，并能从两视角准确估计界面速度，效果优于传统和单相神经渲染基线（摘要未给具体数值）。

Conclusion: 将动态高斯surfel、SDF与视频扩散生成相结合，可在极稀疏视角下重建两相流的尖锐、可变形界面并估计其动力学；方法为两相流可视化测量提供了新方向。

Abstract: Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

</details>
