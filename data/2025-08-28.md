<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出实时生成式绘画系统，统一建模“形式意图”（几何与风格）与“语境意图”（语义与主题），通过多阶段管线将结构控制与内容/风格生成结合，支持触控低时延推理与多人协作，实现人机共创。


<details>
  <summary>Details</summary>
Motivation: 现有以文本提示为主的生成系统偏重高层语义，难以捕捉草图中的几何结构与风格细节，且交互延迟与协作支持不足，限制非专业用户的创作与人机共创潜力。

Method: 1) 从草图提取低层几何与构图特征（轮廓、线走向、比例、空间布局）；2) 通过视觉-语言模型抽取高层语义/主题提示；3) 多阶段生成：先做保轮廓的结构控制，再进行风格与内容感知的图像合成；4) 触控界面+分布式推理，低延迟两阶段变换；5) 支持共享画布的多用户实时协作。

Result: 系统在实时交互中稳定实现轮廓保持的同时完成风格化与语义一致的生成；在分布式架构下实现低时延；用户（含非专业者）可在共享画布上同步协作并得到连贯的共创结果。

Conclusion: 统一“形式+语境”意图并行调控可提升实时生成绘画的可控性与表达力；低延迟多人协作界面促进共创实践，展示人机协作从指令执行到共同创作的转变。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [2] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: 提出一种无需再训练的时间融合方法TTF，在VLA推理时选择性地融合历史与当前视觉token，缓解逐帧处理带来的时序信息丢失与噪声敏感问题，在多基准和真实机器人上均显著提升成功率，并对注意力KQV复用的可行性给出新证据。


<details>
  <summary>Details</summary>
Motivation: 现有VLA逐帧独立编码视觉输入，忽视相邻帧的强时序一致性，导致对视觉噪声脆弱、信息利用不足，影响机器人操控的稳健性与成功率。

Method: 训练期零改动的推理阶段模块：1）双维度关键性检测——(a) 灰度像素差分高效检测外观变化；(b) 基于注意力的语义相关性评估，判别历史token是否对当前决策有用。2）选择性时间token融合：对判定有价值的历史token执行hard fusion，并设定关键帧锚点抑制误差累积。3）探索注意力矩阵复用：在多层中选择性复用Query矩阵以加速并保持/提升性能。

Result: 在LIBERO上平均+4.0个百分点（72.4% vs 68.4%），SimplerEnv跨环境相对提升4.8%，真实机器人任务相对提升8.7%。方法对架构无关，适用于OpenVLA与VLA-Cache。

Conclusion: TTF在无需再训练的前提下显著提升VLA的稳健性与成功率，验证了利用时序一致性的有效性；同时发现选择性复用Query矩阵不仅不降性能，反而有益，指向进一步的KQV复用以兼顾算力加速与任务表现的潜在方向。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [3] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 提出一种无监督幻灯片质量评估：将7个视觉设计指标与CLIP-ViT嵌入结合，用孤立森林进行异常评分；在115张真实学术报告幻灯片上与人工评分的皮尔逊相关最高0.83，显著优于多款主流多模态大模型给出的评分。


<details>
  <summary>Details</summary>
Motivation: 现有幻灯片质量评估多依赖主观评分或重型有监督模型，难以实时、可扩展且与人类审美一致；同时单纯的低层视觉指标或纯LLM/VLM评分都难以稳定捕捉“设计质量”。作者希望用可解释的低层设计度量+多模态语义嵌入，构建无需标注、与人类感知高度相关的自动质量评估。

Method: 1) 构建7个专家启发的视觉设计指标：留白、色彩丰富度、边缘密度、亮度对比、文本密度、色彩和谐、版式平衡。2) 计算CLIP-ViT图像嵌入，补充语义/整体风格信息。3) 将上述特征拼接后，用Isolation Forest进行无监督异常检测，异常分数作为“质量差”信号（或其相反）。4) 训练数据为12k专业讲义幻灯片（无人工标签），学习一般分布；评估在6场学术报告的115页幻灯片上，与人工视觉质量评分做相关。还检验与演讲者表现评分的区分效度，以及与总体印象的探索性一致性。

Result: 与人工视觉质量评分皮尔逊相关最高0.83；相较主流VLM（o4-mini-high、o3、Claude Sonnet 4、Gemini 2.5 Pro）的相关度提升1.79x—3.23x。模型与视觉评分呈收敛效度，与演讲者表达评分无关（区分效度成立），与总体印象有一定一致性。

Conclusion: 将可解释的低层设计线索与多模态嵌入结合，并用无监督异常检测，可在无需标注的前提下接近受众对幻灯片质量的感知，能用于实时、可扩展的客观反馈。

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [4] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 提出针对2D Range-View LiDAR语义分割的高效对抗防御：在RV域直接建模攻击，并用数学可解释的轻量化净化网络进行模型级净化，显著提升鲁棒性且计算开销低，优于生成式/对抗训练基线，并在真实车辆上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR分割对抗防御多针对原始3D点云并依赖大型生成模型，成本高；而大量SOTA实际使用更高效的2D Range-View表示，但几乎缺乏专门、轻量且实用的防御方法，存在安全与落地鸿沟。

Method: 1) 在Range-View（RV）域提出直接的对抗攻击定义与威胁模型；2) 构建基于数学优化问题推导的“可解释”净化网络（model-based purification），将受扰RV映射到干净流形；3) 以极低计算开销插入到RV分割流水线中，作为前置净化模块。

Result: 在公开基准上，鲁棒性能与精度达到或超过现有生成式净化与对抗训练方法；计算成本显著更低；在真实演示车辆部署中能稳定输出准确分割，展示实际可行性。

Conclusion: 面向RV域的模型化净化是轻量、有效且可部署的LiDAR分割对抗防御路径，兼顾鲁棒性与效率，可为自动驾驶感知提供更可靠的安全保障。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [5] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 综述LVLM在目标检测中的最新进展：工作原理、架构与训练范式、视觉-文本融合策略、在定位/分割与实时性上的表现比较、局限与未来路线图，判断其即将达到或超越传统方法，并对机器人等应用的影响做出展望。


<details>
  <summary>Details</summary>
Motivation: 对象检测正面临跨场景泛化、上下文理解、类别开放性与交互性等难题；LVLM通过语言与视觉的融合，有望提升适应性、推理与可解释性，弥补传统CV架构的局限。

Method: 采用三步研究评述流程：1) 阐述VLM用于目标检测的工作机制（NLP+CV的结合）；2) 梳理近期LVLM在架构创新、训练范式与输出灵活性方面的进展；3) 系统审视视觉与文本信息融合策略，并提供可视化与基准对比。

Result: 给出全面可视化案例（定位、分割等）与与传统深度学习系统在实时性、适应性、复杂度上的对比；证据显示LVLM在多场景下效果显著、具更强的上下文理解和泛化潜力。

Conclusion: LVLM预计在目标检测上很快达到或超越传统方法；同时指出当前LVLM的若干主要局限并提出解决思路与未来路线图，认为其将对目标检测与机器人应用产生持续的变革性影响。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [6] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 提出一个两级微调的视觉语言模型管线，用于从体育图像生成专业、风格化的解说式字幕，较现有方法在F1和BERTScore上提升显著，并在超级碗LIX实战中以低延迟高吞吐量稳定运行。


<details>
  <summary>Details</summary>
Motivation: 通用LLM/LVLM虽能描述泛化运动动作，但缺乏体育领域术语与风格化表达，难以产出可用于媒体生产的专业字幕。现有方法在准确性、行业化表达和实时性上不足，尤其在体育图像到自然语言的“解说体”生成方面存在明显差距。

Method: 设计一个两级（two-level）微调的LVLM流水线：先进行领域适配与术语注入，再进行风格化与格式对齐微调，从而在保持小内存占用和快速推理的同时，生成符合体育新闻风格的描述。与替代方法对比评测，并在真实赛事中部署验证。

Result: 相较替代方法，F1提升超过8–10%，BERTScore提升超过2–10%；同时具有较小的运行内存占用与快速执行。在超级碗LIX现场以每3–5秒处理6张图、总计超过1000张图的速率，生成高准确度、风格一致的字幕。

Conclusion: 两级微调LVLM能有效弥补通用模型在体育领域的术语与风格缺陷，达到生产级质量与实用的实时性能；方法在大型体育赛事中已验证可用，适合扩展到其他体育项目与媒体生产场景。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [7] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: 论文对三种主流LVLM（LLaVA、BLIP-2、PaliGemma）在带文字生成的生物特征人脸识别任务中的群体公平性进行系统实证评估，发现存在显著人口统计学偏差：PaliGemma与LLaVA在部分族裔（如Hispanic/Latino、Caucasian、South Asian）上差距更大，BLIP-2相对更一致。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLM在多任务上表现强，但在人脸识别与描述任务中可能对不同族群（种族/族裔、性别、年龄）表现不均，影响公平与可靠性。该研究旨在定量刻画并比较这些偏差，填补LVLM在FR文本生成场景下的公平性评测空白。

Method: 构建并微调三种预训练LVLM（LLaVA、BLIP-2、PaliGemma）于作者自建的人口统计学均衡数据集；采用群体分解的BERTScore与Fairness Discrepancy Rate等指标，对不同群体的生成质量与差异进行量化与追踪比较。

Result: 在均衡数据与统一评测下，三模型均显现人口学偏差；其中PaliGemma与LLaVA在Hispanic/Latino、Caucasian、South Asian群体上表现差距更大；BLIP-2在跨群体上的稳定性相对更好。

Conclusion: LVLM在带文本生成的人脸识别任务中仍存在系统性偏差，且不同模型偏差程度不一；需要在数据、训练与评估层面引入公平性导向的方法与指标，以提升跨人群的一致性与可信度。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [8] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec提出基于有符号距离场（SDF）的统一空间表示学习方法，直接在原始几何空间自适应采样并训练网络近似SDF，生成紧凑、几何感知且旋转不变的嵌入，统一适配点/线/面等类型，较现有方法更准确、更高效并更好捕捉拓扑与距离关系。


<details>
  <summary>Details</summary>
Motivation: 现有空间表示要么只支持单一地理实体类型，要么像Poly2Vec那样需将复杂几何分解并做傅里叶变换，计算开销大且在变换空间缺乏几何对齐，依赖均匀采样导致边界/棱角等细节被抹平。为在不分解几何的情况下统一表示点、折线、多边形等，并准确捕捉形状、位置、拓扑与距离关系，同时提升效率与对高频细节的表达，需要一种直接在原空间工作的自适应、几何对齐的方法。

Method: - 提出Geo2Vec：受SDF启发，在几何原空间自适应采样点，计算其到目标几何的有符号距离（外正内负），以此作为监督信号训练神经网络近似该SDF。
- 网络输出作为紧凑嵌入，统一编码点、线、面实体的形状、位置与邻接/距离关系，无需几何分解或傅里叶变换。
- 设计旋转不变的位置信息编码，用以建模高频空间变化，提升嵌入的结构性与鲁棒性。
- 通过对比现有方法的实验，评估形状/位置表征、拓扑与距离关系捕获能力及运行效率。

Result: Geo2Vec在多项指标上稳定优于现有方法：更好地表征几何形状与位置，更准确捕捉拓扑与距离关系，并在真实GeoAI应用中实现更高效率（推理/训练开销更低）。

Conclusion: 基于SDF的Geo2Vec无需几何分解即可在原空间统一学习点/线/面表示，结合旋转不变位置编码，构建紧凑、几何感知且鲁棒的嵌入空间，为下游GeoAI任务带来更高精度与效率；该范式为通用空间表示学习提供了有效路径。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [9] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: 用CNN对5类稻米粒进行自动分类，并结合XAI（SHAP、LIME）解释深度模型；同时对稻叶病（褐斑、稻瘟、白叶枯、条纹叶枯/藤黄病Tungro）进行诊断。基于7.5万张公开图像，模型在多指标上表现优秀且可解释，显示深度学习在农作物质量控制与病害诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: 人工稻米质量检验与病害识别耗时、费力且易出错，影响消费者满意度与国家品牌形象；需要自动化、准确、可解释的方案以提升品质监测与产量管理。

Method: - 任务1：用CNN对五种稻米粒（含长短粒与特定品种如basmati、jasmine等）进行图像分类；数据集为7.5万张公开图片；评估指标含准确率、召回率、精确率、F1、ROC、混淆矩阵。
- 任务2：对稻叶病（褐斑、稻瘟、白叶枯、Tungro）进行诊断；采用CNN、VGG16、ResNet50、MobileNetV2等深度模型；使用XAI方法SHAP、LIME解释特征贡献，提高透明度与可信度。

Result: 在稻米粒品种分类上取得高准确率、误判很少；在叶病诊断上实现高精度且能通过SHAP/LIME定位影响预测的关键纹理/颜色/形状特征，给出可解释性证据。

Conclusion: 深度学习结合XAI可构建稳健、可解释的农作物质量检测与病害诊断系统；有助于自动化质检、提升农作物健康管理，最终惠及农民、消费者与农业经济。

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [10] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [11] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: 用地面照片而非卫星影像来分类18类生境，基于DeepLabV3‑ResNet101并通过数据增强与重采样提升鲁棒性，五折交叉验证平均F1=0.61，易识别类>0.90，提供Web应用，展示生态监测可行性。


<details>
  <summary>Details</summary>
Motivation: 传统生境分类依赖卫星影像与实地验证，存在尺度与验证成本问题。地面图像（可来自公民科学）更易获取、细节更丰富，若能高效自动分类，可提升监测与管理效率，支撑保护与规划。

Method: 收集地面生境照片，按“Living England”框架的18类进行标注；预处理（缩放、归一化、数据增强），并用重采样平衡类别；采用DeepLabV3‑ResNet101进行微调训练，将整张照片分类到单一生境类别；五折交叉验证评估准确率与F1。

Result: 整体表现良好，跨折平均F1=0.61；视觉特征明显的类别（如裸土/淤泥/泥炭、裸砂）F1>0.90；混合或边界模糊的类别较低。

Conclusion: 地面图像驱动的深度学习生境分类在多类任务上可行，适用于生态监测与应用场景；提供了可供从业者使用的简易Web工具，但对混合与模糊类仍需改进。

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [12] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 提出一种可交互的多模态自回归视频生成框架，基于轻改LLM作为控制器，结合扩散头与高压缩自编码器，实现流式、低时延的人类数字人生成与互动；在2万小时对话数据上训练，支持音频/姿态/文本多模态控制，并在多语言合成与双工对话等任务上展现高效率与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有交互式数字人视频生成方法在实时性、计算开销和可控性方面存在瓶颈：高延迟、难以处理多模态输入、生成控制粒度有限。需要一种既能低时延响应、又能灵活接收多模态信号并保持语义-空间一致性的统一框架。

Method: 1) 用“轻量改造”的大语言模型作为自回归控制器，输入多模态条件（音频、姿态、文本）并输出时空一致的表示；2) 以这些表示引导扩散模型（扩散头）进行视频去噪生成；3) 构建约2万小时多来源对话数据，覆盖丰富交互情境；4) 设计深度压缩自编码器（最高64×压缩）以缩短自回归长时序推理路径，支持流式外推；5) 整体采用流式（streaming）推理，实现低延迟交互。

Result: 在双工对话、多语言人像合成与交互式世界模型等任务上取得更低延迟、更高效率及更细粒度多模态可控性；能够进行低时延外推（extrapolation），并保持空间与语义一致。

Conclusion: 将LLM作为多模态自回归控制核心、结合扩散头与高压缩表征，可在不牺牲质量的前提下显著降低交互式视频生成的时延与计算成本，并提升多模态可控性；为实时数字人/交互式生成系统提供可扩展方案。

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [13] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 该综述聚焦在ICAO标准人脸照片的安全问题，评估数字水印与隐写作为对抗变脸/深度伪造与事后篡改的补充性手段，强调在不破坏标准合规性的前提下实现持久可验证性，并给出技术对比、权衡与部署建议。


<details>
  <summary>Details</summary>
Motivation: ICAO标准人脸照片因跨境互操作而广泛用于护照、边检、DTC与金融KYC，但标准化也被恶意利用（人脸合成、变脸、深伪、身份盗用、证件非法共享）。现有PAD只能在采集时生效，缺乏对已采集图像的持续保护与溯源。因此需要探索可嵌入、可验证、且不破坏ICAO合规性的被动防护机制。

Method: 作为综述，系统梳理并对比数字水印与隐写在ICAO合规图像上的方法：嵌入域（像素/频域/学习式特征域）、鲁棒/脆弱/半脆弱水印、可逆与不可逆方案、密钥管理与公私钥签名、检测与认证流程、对抗攻击模型（压缩、裁剪、重采样、去噪、再照相、GAN伪造、Morphing）、与ICAO图像要求（尺寸、分辨率、背景、压缩率、无明显可见伪影）的适配性；并评估嵌入容量、感知质量、鲁棒性、计算与存储开销、跨设备互操作、与现有工作流的集成。

Result: 给出现有技术能力与局限：1) 半脆弱/混合水印在压缩与常规编辑下保持可验证，同时能对结构性篡改（人脸几何/纹理改变、morphing）敏感；2) 可逆水印能更好满足“原图还原”与证据留存需求，但鲁棒性与容量受限；3) 学习式水印/隐写具更高容量与对压缩的鲁棒性，但易受去水印与对抗攻击；4) 公钥签名结合水印可实现离线验证与来源追踪；5) 在JPEG压缩、再采样与打印-再拍等典型通道下存在质量-鲁棒-合规三方权衡。

Conclusion: 数字水印与隐写可作为PAD的事后补充，提供可持续的篡改证据与来源认证，在不破坏ICAO合规的前提下提升身份系统的安全韧性。但需要针对标准约束优化嵌入策略、密钥/证书基础设施与跨设备验证流程，并关注对抗性去水印、隐私与可扩展性问题。作者给出部署指导：优先半脆弱或混合水印；关键路径使用可逆方案保存取证证据；结合公钥基础设施与审计日志；开展跨域基准与开放评测以推动实用化。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [14] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: 提出PRISM，一种将无对比剂心脏cine MRI与结构化EHR融合的自监督生存分析框架，在四个独立队列中对MACE预测优于传统与SOTA方法，并识别影像与临床高危特征。


<details>
  <summary>Details</summary>
Motivation: MACE风险早期且准确预测困难；现有模型要么只用EHR要么只用影像，或缺乏时序同步与可解释性，且跨中心泛化有限，亟需一个能融合多模态、捕捉心脏动力学并具临床可解释性的生存建模方法。

Method: 提出PRISM：1) 自监督学习从多视角cine MRI中提取时序同步、运动感知的影像表征（motion-aware multi-view distillation）；2) 设计医学知识引导的文本prompt对影像与EHR表示进行调制与对齐；3) 将视觉与结构化EHR表征融合用于生存分析；4) 通过prompt引导的归因实现可解释性，挖掘影像与EHR对风险的贡献。

Result: 在四个独立临床队列中，PRISM在内外部验证均超过经典生存模型与SOTA深度基线；联合影像+EHR表征提供更稳健的风险分层；发现三类与高MACE相关的影像表型（侧壁不同步、下壁高敏感、舒张期前壁聚焦升高）；归因分析识别高血压、糖尿病、吸烟为EHR主导风险因子。

Conclusion: 多模态自监督与prompt引导的融合可显著提升MACE生存预测的准确性与可解释性，并在不同队列上具备良好泛化；所识别的影像签名与临床因子为心血管风险评估与个体化管理提供可操作线索。

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [15] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: 提出EffNetViTLoRA：在全量ADNI T1 MRI上，结合CNN与ViT并用LoRA自适应预训练模型，实现AD/MCI/CN三分类，准确率92.52%，F1 92.76%。


<details>
  <summary>Details</summary>
Motivation: AD不可逆且进展伴随认知下降，早诊断关键；MCI处于CN与AD之间，类别边界细微，诊断困难。既有研究常用小数据子集与直接微调大模型，易偏置、泛化差。

Method: 构建端到端模型EffNetViTLoRA：用CNN（提取局部结构特征）+ ViT（捕获全局依赖）；在ViT上应用LoRA以低秩适配跨域差异，减少参数与过拟合风险；使用ADNI全量T1加权MRI进行训练与评估，三分类（AD/MCI/CN）。

Result: 在ADNI全量数据上实现准确率92.52%，F1-score 92.76%，显示较高鲁棒性与临床可靠性。

Conclusion: 融合局部与全局特征并用LoRA进行跨域适配，可在AD/MCI/CN三分类上取得强性能；使用全量数据提升泛化与减小偏差，方法更具临床可用性。

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [16] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: 评估三家商用计算机视觉/AI基于转播画面的球员追踪软件的精度，并考察不同镜头源与分辨率对位置、速度与距离测量的影响；与高清多摄系统TRACAB Gen 5对比，位置RMSE约1.68–16.39 m、速度RMSE约0.34–2.38 m/s，总距离偏差约-21.8%至+24.3%；战术视角优于其他镜头，720p与1080p在合适模型下均可用。


<details>
  <summary>Details</summary>
Motivation: 职业与国家级足球常用昂贵的多摄光学系统，但赛事与媒体通常仅能获取转播源。若商用CV/AI软件可在转播视频上实现足够精度，将极大降低成本并扩大应用场景（战术分析、体能评估、广播增强等）。然而不同镜头源与分辨率可能显著影响检测/跟踪性能，需实证评估。

Method: 选取2022卡塔尔世界杯一场比赛，采用三类转播源（战术、节目、摄像机1）与三家商用CV/AI追踪提供商，输出每名球员的瞬时位置（x,y）与速度（m/s）。以高清多摄TRACAB Gen 5为基准，计算位置与速度的RMSE及总比赛跑动距离的平均偏差，比较不同镜头源与分辨率（720p、1080p）的影响。

Result: 相对TRACAB基准，位置RMSE在1.68–16.39 m之间、速度RMSE在0.34–2.38 m/s之间；总距离平均偏差范围为-1745 m（-21.8%）至+1945 m（+24.3%），不同供应商差异大。战术视角的检测率与精度更高；720p与1080p在合适模型下均能达到可接受精度。

Conclusion: 商用CV/AI基于转播的球员追踪在球员被成功检测时具“尚可”的精度，但误差与偏差仍显著，供应商间差异明显。为提升精度应优先使用战术镜头；分辨率720p或1080p均可行，关键在模型与检测率。用于高精度量化（如微小速度变化、精细跑动里程）的可靠性仍需谨慎评估与进一步验证。

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [17] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: 提出JVLGS：将视觉与语言融合用于红外视频中的气体泄漏分割，并配合后处理抑制无泄漏帧的误报；在监督与小样本设置下均显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 气体泄漏危害健康并污染环境，但现有检测方法受限。红外视觉方法易受气体云模糊、非刚性形态与噪声/伪目标影响，且泄漏事件稀疏导致大量无泄漏帧产生误报。需要更鲁棒、可泛化且在少样本条件也有效的分割方法。

Method: 提出联合视觉-语言气体泄漏分割框架（JVLGS）：以视觉特征捕捉红外视频中气体外观/动态，以文本模态提供类别与先验约束（例如“气体泄漏”语义提示），融合两者以增强表征；针对泄漏稀疏性引入后处理策略，过滤噪声和非目标带来的假阳性；在监督与few-shot场景下训练与评估。

Result: 在多种场景与基准上，JVLGS显著超越现有气体泄漏分割方法；在全监督与小样本设置中均取得稳定且领先的性能，而对比方法往往只能在其中一种或两者都表现不佳。

Conclusion: 多模态（视觉+语言）融合加后处理能有效提升红外视频气体泄漏分割的鲁棒性与准确性；方法在不同数据规模下均具竞争力，具有实际部署前景。

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [18] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 提出UNIFORM框架，将多样异构的预训练教师模型的知识无约束地整合到单一学生中，通过对logit与特征两层面的一致性投票，显著提升无监督目标识别并具备百余教师的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 线上大量风格各异的预训练模型蕴含丰富知识，但因架构、任务、标签空间与数据分布异构，现有整合方法需强假设（同分布、同架构等），导致仅能利用特定模型且引入数据/归纳偏置，难以普适集成。

Method: 提出UNIFORM框架：1) 设计投票机制融合两种共识：a) logit层面：对能覆盖目标类别的教师进行预测投票；b) 特征层面：对任意标签空间教师的视觉表征进行共识聚合。2) 无需对训练数据分布或网络结构做强假设，统一蒸馏到单一学生模型。

Result: 在无监督目标识别任务上，UNIFORM优于强基线；随着教师数量增加至100+仍持续收益，而现有方法在较小规模即饱和。

Conclusion: 跨模型、跨标签空间与跨架构的通用知识整合可行且有效；双层共识投票实现可扩展无监督知识转移，为利用大规模异构教师资源提供实用路径。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [19] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow 用仅依赖卫星影像的、具有置换等变性的扩散模型来生成结构一致的 OD 流矩阵；通过多核编码与对比+等变训练实现对区域重排的鲁棒性，并在真实数据上优于物理与数据驱动基线。


<details>
  <summary>Details</summary>
Motivation: 现有 OD 生成方法依赖昂贵且区域覆盖受限的辅助特征（POI、社会经济统计），且对空间拓扑/索引顺序敏感，区域编号微小变化会破坏结构一致性。需要一种既可全球扩展、又对区域重排鲁棒且不依赖本地辅助数据的方法。

Method: 提出 Sat2Flow：1）多核编码器捕捉多尺度/多类型区域交互；2）置换感知的扩散过程，使不同区域排序下的潜在表示对齐；3）联合对比学习，将卫星影像特征与 OD 模式桥接；4）等变扩散训练，显式约束在任意区域重排下的结构一致性。输入仅为卫星影像，输出为 OD 流矩阵。

Result: 在真实城市数据上，数值精度超过物理模型与数据驱动基线；在索引置换下能保持经验分布与空间结构；显示出对拓扑与编号扰动的鲁棒性。

Conclusion: Sat2Flow 实现了仅用卫星图像的、结构一致且可扩展的 OD 生成，消除了对区域特定辅助数据的依赖，并在任意区域重排下保持结构不变，为数据稀缺环境中的稳健出行建模提供可行方案。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [20] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 提出一种诊断驱动的半监督学习框架，利用未标注数据和伪标签，缓解实际农田中的“阴影偏置”，在分类与检测任务上提升鲁棒性与召回率，适用于低数据场景并经公共数据集验证。


<details>
  <summary>Details</summary>
Motivation: 实际田间环境复杂（光照、阴影等），深度学习模型易受干扰；同时高质量标注昂贵且稀缺，限制了模型泛化与部署。需要一种既能诊断模型失误来源、又能在标注不足下提升性能的方法。

Method: 1) 构建约975张标注与1万张未标注的甘蔗田豚草图像数据集；2) 以ResNet分类、YOLO与RF-DETR检测建立强监督基线；3) 借助可解释性工具进行失误诊断，发现“阴影被误当作植被”的系统性偏差（阴影偏置）；4) 设计半监督伪标签管线，利用未标注数据丰富视觉多样性并针对阴影偏置进行鲁棒化训练；5) 在公共作物-杂草基准的低数据设定下外部验证。

Result: 监督基线：分类F1最高0.90，检测mAP50>0.82；半监督框架带来显著召回提升并减轻阴影偏置，在自动喷洒系统中可减少漏检（weed escapes）。在公共基准的低数据条件下同样有效。

Conclusion: 诊断驱动+半监督的组合为精准农业视觉系统提供了一条可复现、可扩展的路线：先建立强基线并借助解释工具定位偏差，再用未标注数据与伪标签提升鲁棒性，特别是在复杂光照与标注稀缺场景中表现稳健。

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [21] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: 提出TAPO与MotionFLUX：前者用偏好对齐优化细粒度文本-动作语义；后者用确定性rectified flow匹配实现实时动作生成，省去多步扩散去噪。两者联合在一致性、质量与速度上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动动作生成难以：1) 精准对齐文本修饰词与细微动作差异；2) 推理慢，常需多步扩散去噪。作者希望提升语义对齐与实时性。

Method: - TAPO（TMR++ Aligned Preference Optimization）：通过偏好优化/迭代调整，让模型学习将细微动作变化与文本修饰词对应，加强语义落地。
- MotionFLUX：基于确定性的rectified flow matching，构造从噪声分布到动作空间的最优输运路径，线性化概率路径，从而单/少步生成，避免传统扩散的数百步去噪。
- 将两者统一：TAPO保证文本-动作语义一致；MotionFLUX保证高速高质生成。

Result: 实验显示在语义一致性与动作质量上均超越SOTA，同时显著加速推理，实现接近实时的合成。

Conclusion: 通过偏好对齐的语义强化与基于流匹配的确定性高速生成，形成统一系统，兼顾对齐、质量与速度；代码与预训练模型将开源。

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [22] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: 提出CVBench，一个评测多视频跨视频推理能力的基准，覆盖对象关联、事件关联与复杂推理三层级，基于五类视频簇共1000问答，对10+主流MLLM评测显示与人类相比仍显著落后（如因果推理仅约60% vs 91%）。揭示关键瓶颈：跨视频上下文保持与实体歧义消解不足。


<details>
  <summary>Details</summary>
Motivation: 现实应用（多摄像头安防、跨视频流程学习）需要模型在多段视频间整合信息，但现有MLLM主要在单视频任务上评估，缺少系统化的跨视频基准，难以诊断能力缺口与瓶颈。

Method: 构建CVBench：从五个多样领域视频簇采样，设计三层级任务——1) 跨视频对象关联（识别共享实体）；2) 跨视频事件关联（连接时间或因果链条）；3) 跨视频复杂推理（融入常识/领域知识）。共1000个QA。以零样本与CoT提示，对10+领先MLLM（如GPT-4o、Gemini-2.0-flash、Qwen2.5-VL）进行广泛评测与误差分析。

Result: 总体性能显著低于人类；例如在因果推理上，顶尖模型（GPT-4o）仅约60%准确率，而人类达91%。发现两类核心失效：跨视频语境保持不足、对重叠/相似实体的区分能力弱。

Conclusion: CVBench为多视频推理提供了首个系统化评测框架，可用于诊断与推动MLLM架构改进。结果提示需在跨视频记忆/检索、实体对齐与消歧、长程时序与因果建模等方面进行架构与训练策略的强化。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [23] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: 提出Web端轻量级凝视追踪框架WebEyeTrack，结合头部姿态建模与少样本个性化校准，在保障隐私、实时性与准确性的同时，缩小学术方法与商业产品之间的落差。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA注视估计虽在基准上表现优秀，但忽视模型体量、推理延迟和隐私等真实部署因素；基于普通摄像头的方法易受头动影响且精度不足，难以满足实际应用。

Method: 在浏览器内集成轻量级SOTA注视模型；加入基于模型的头部姿态估计以抵抗头动；在端侧进行少样本（≤9点）个性化校准的快速微调/自适应学习；全流程端侧推理以保护隐私与降低延迟。

Result: 在GazeCapture上达到2.32 cm误差；在iPhone 14上实现约2.4 ms/帧的实时推理速度；以极少校准样本即可达到SOTA精度。

Conclusion: WebEyeTrack在浏览器端实现高精度、低时延、重隐私的注视追踪，弥补了学术SOTA与商业应用之间的部署鸿沟，并以开源形式提供，便于复现与扩展。

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [24] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [25] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: 提出FlowDet，一种针对路口交通监控的高效端到端目标检测器，通过对DETR进行解耦式编码器优化，引入GDU与SAA，在自建Intersection-Flow-5k上以更少计算实现更高精度与更快速度。


<details>
  <summary>Details</summary>
Motivation: 端到端（NMS-free）检测器适合实时应用，但在复杂高拥挤场景（如路口监控）计算代价高、尺度变化大、遮挡严重，现有方法在效率与精度间难以兼顾。

Method: 在DETR框架上提出FlowDet：1）解耦式编码器优化策略；2）几何可变形单元（GDU）进行交通场景几何建模，增强对遮挡与运动拓扑的建模能力；3）尺度感知注意力（SAA）提升极端尺度变化下的表示能力；4）构建高拥挤、高遮挡路口数据集Intersection-Flow-5k用于评测。

Result: 在Intersection-Flow-5k上，相比强RT-DETR基线：AP(test)+1.5%，AP50(test)+1.6%，GFLOPs降低63.2%，推理速度提升16.2%，达成SOTA。

Conclusion: FlowDet在保证精度提升的同时大幅降低计算量并加速推理，为复杂交通感知中的端到端检测提供了高效可行的路径；新数据集为该领域基准评测提供支持。

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [26] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: 提出一个结合可训练编码器、原型引导重建与多样性对齐损失的统一框架，用于医学图像异常检测，解决原型塌缩和领域自适应不足，显著提升表示与定位性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测标注稀缺，且与自然图像存在域差距；重建式方法常用冻结的预训练编码器，难以适配特定领域，定位不准；原型学习可解释但易原型塌缩，导致多样性不足与泛化差。

Method: 1) 采用可训练编码器并配合动量分支，稳定地进行域自适应特征学习；2) 设计轻量级原型提取器，从正常样本中挖掘信息量大的原型，用注意力机制引导解码器实现精细重建；3) 提出多样性感知的对齐损失，包含多样性约束与逐原型归一化，鼓励均衡使用原型并防止塌缩。

Result: 在多项医学影像基准上取得显著提升，无论是表示质量还是异常定位都优于现有方法；并通过可视化与原型分配分析验证了防塌缩机制与可解释性提升。

Conclusion: 统一框架有效缓解域自适应与原型塌缩问题，提升异常检测与定位性能，具备更好的可解释性；方法简单高效且适用于多种医学图像场景。

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [27] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: 提出MPAMatch：在半监督病理图像分割中，引入多模态原型引导的像素级对比学习（图像/文本原型↔像素标签），并用病理领域预训练的Uni替换TransUNet的ViT骨干。在GLAS、EBHI-SEG与KPI等数据集上优于SOTA，兼顾结构与语义边界建模。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分割多依赖同模态扰动一致性（如UniMatch），难以获取高层语义先验，尤其在结构复杂、边界模糊的病理图像中；像素级标注昂贵，需要在少标注/无标注条件下提升泛化与语义辨析。

Method: 1) 多模态原型监督：构建图像原型与文本原型（语义描述）并与像素标签进行双重对比学习，提供粗到细的结构与语义监督；2) 半监督框架：对未标注样本进行原型对齐与一致性约束，提升判别性；3) 架构改造：以病理领域基础模型Uni替换TransUNet的ViT骨干，强化领域特征提取。

Result: 在GLAS、EBHI-SEG-GLAND、EBHI-SEG-CANCER、KPI上全面超越当前SOTA（如UniMatch等），表现出更好的边界刻画与类间分离，尤其在低标注设置下优势明显。

Conclusion: 双重（图像/文本）原型引导的像素级对比学习为半监督病理分割提供了同时兼顾结构与语义的监督；引入文本原型用于分割是首次尝试并显著改进语义边界建模；结合病理预训练骨干可进一步提升性能。

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [28] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [29] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出SGDDM和HoloMamba，实现高速高保真全彩视频CGH：通过频谱引导的深度分割复用避免色串扰，并用时空建模的轻量Mamba-UNet加速推理，1080p达260+ FPS，较SOTA提速2.6×且画质更佳。


<details>
  <summary>Details</summary>
Motivation: 现有CGH存在两大瓶颈：1) 学习式方法相位过平滑、角谱窄，导致高帧率全彩显示（如深度分割复用）出现严重色串扰，迫使在帧率与色彩保真间权衡；2) 逐帧优化忽视时空相关性，计算低效。

Method: 1) SGDDM：在深度分割复用框架中引入频谱调制与谱域约束，直接优化相位的角谱分布，扩大/重塑频带以减少通道重叠，从而抑制色串扰并兼顾高帧率输出。2) HoloMamba：提出轻量不对称Mamba-UNet，利用选择性状态空间模型（SSM/Mamba）在编码-解码中显式建模视频时空相关性，降低参数与计算，同时提升重建质量与收敛效率。

Result: 仿真与实拍均验证：SGDDM在高帧率下保持高色彩保真；HoloMamba可生成1080p全彩全息视频>260 FPS，较Divide-Conquer-and-Merge策略快2.6×以上，并获得更高的重建质量指标。

Conclusion: 通过频谱引导的相位优化与时空相关性建模，方法同时解决色串扰与计算瓶颈，实现高帧率、高保真、低延迟的全彩视频CGH；为下一代全息显示提供可扩展的工程化路径。

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [30] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: 提出SBDC，用判别器引导来校正带噪声标注的条件扩散模型，在早期采样阶段施加引导更好；无需重训，推理时间略增，多种噪声场景下优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 大规模数据常含人工标注错误，影响条件扩散模型的生成质量与可控性，但影响程度与有效校正方式研究不足，需要在不重训基础模型的情况下提升鲁棒性与对齐能力。

Method: 训练一个基于对抗损失的判别器，结合现有噪声检测思想，对样本真实性/噪声程度打分；在生成时作为score-based guidance，仅在采样的早期阶段施加，以矫正条件与生成轨迹；无需对扩散模型本体再训练，推理开销小幅增加。

Result: 在多种噪声设置与数据集上，与现有SOTA对齐/噪声鲁棒方法相比，SBDC取得更优的生成质量与控制准确率，同时保持较低的计算开销。

Conclusion: SBDC能有效对齐和纠正带噪标注下的预训练条件扩散模型，早期阶段引导最有效，方法高效实用，可作为无需重训的通用增强模块。

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [31] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [32] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: 系统评估YOLO在多种量化精度与多种输入退化下的鲁棒性；提出“退化感知”INT8校准，但整体并未显著优于常规校准，仅在大模型+特定噪声下有收益；INT8带来1.5–3.3×提速，干净数据mAP掉3–7%。


<details>
  <summary>Details</summary>
Motivation: PTQ有利于在边缘设备上部署检测器，但量化可能削弱模型在真实退化（噪声、模糊、压缩等）下的鲁棒性。现有工作多关注精度/吞吐，对鲁棒性与校准策略的影响研究不足，尤其在YOLO多尺度与多精度平台上的系统性对比缺失。

Method: - 模型：YOLO家族从nano到xlarge多个尺度。
- 精度/后端：FP32、FP16(TensorRT)、动态UINT8(ONNX)、静态INT8(TensorRT)。
- 提出退化感知校准：在TensorRT静态INT8校准时混合干净图与合成退化图（多类型、多强度）。
- 数据与评测：COCO上七类退化（多噪声、模糊、低对比、JPEG压缩等）及混合退化场景，报告mAP50-95及速度；比较常规（干净数据）校准与退化感知校准。

Result: - 性能/速度：静态INT8 TensorRT较FP32获得约1.5–3.3×加速，干净数据mAP50–95下降约3–7%。
- 鲁棒性：在多数模型与退化条件下，退化感知校准未能稳定、显著提升鲁棒性；部分大模型在特定噪声下有改善，显示容量相关性。
- 不同精度对退化敏感性不同，但总体INT8在强退化下的性能仍明显受损。

Conclusion: 单纯通过在校准阶段引入退化数据，并不能普遍增强PTQ后YOLO的鲁棒性；模型容量可能调节其有效性。部署建议：在受控外环境中INT8可带来可接受的精度损失与显著加速；在非受控、强退化环境中需谨慎，考虑更强的鲁棒训练/量化感知训练、或模型放大与专门数据增强。代码与评测表已开源以复现与扩展研究。

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [33] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: 提出在扩散模型与分割网络中引入逆进化层（IEL），抑制生成缺陷与传播伪影，并配合多尺度频率融合（MFF），在DGSS上取得SOTA泛化


<details>
  <summary>Details</summary>
Motivation: 扩散模型合成的数据可提升域泛化，但常含有结构/语义缺陷，直接训练分割器会导致误差累积与性能下降；需要一种机制在数据生成与下游分割中共同抑制缺陷与伪影传播

Method: 1) 在扩散生成流程中插入逆进化层（IEL），以拉普拉斯先验显著化空间不连续与语义不一致，从而过滤不良模式，形成增强的数据增广框架IELDM；2) 将IEL嵌入DGSS分割解码器，提出IELFormer，利用IEL的缺陷抑制能力减少伪影传播；3) 设计多尺度频率融合（MFF）模块，在频域对多分辨率特征进行结构化融合，增强跨尺度一致性

Result: 在多个DGSS基准上，相较现有方法取得更优的跨域泛化表现（未给出具体数值，但宣称SOTA）

Conclusion: 通过在生成与分割两端同时引入IEL并结合MFF，可系统性抑制生成缺陷与特征伪影，提升跨域语义分割的稳健泛化能力

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [34] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: 提出LF-VAR：以量化病灶测量分数与病灶类型为条件，结合多尺度病灶关注的VQVAE与自回归Transformer，实现可控、高保真皮肤病灶图像合成，七类病灶平均FID 0.74，较SOTA提升6.3%。


<details>
  <summary>Details</summary>
Motivation: 真实临床皮肤图像数据稀缺且分布复杂，现有合成方法质量不高、缺乏对病灶位置/类型与临床属性的可控性，难以为下游诊断模型提供有效数据增广。

Method: 1) 训练多尺度、病灶关注的VQVAE，将图像编码为离散token以突出病灶区域结构；2) 在token上训练Visual AutoRegressive Transformer进行图像生成；3) 将病灶区域的量化测量分数（如大小、形状等）与病灶类型标签作为条件嵌入，指导可控生成；4) 语言提示用于指定目标病灶特征，实现位置/类型等可控合成。

Result: 在七类病灶合成任务中获得最佳整体FID，平均0.74，相比上一SOTA提升6.3%；生成图像在临床相关性与保真度方面更优（定量与定性均显示优势）。

Conclusion: LF-VAR能依据临床量化指标和类型标签进行可控皮肤图像合成，提升合成质量与临床相关性，为数据增广和模型训练提供高质量合成数据；代码已开源便于复现与扩展。

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [35] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: 提出DQRoute：难度感知优化+动态专家协作的模块化框架，面向长尾视觉识别；用不确定性/历史表现估计类别难度以自适应加权训练，并以无中心路由的MoE在推理时用各专家的置信/OOD得分加权融合；在标准基准上显著提升，尤其在稀有/困难类。


<details>
  <summary>Details</summary>
Motivation: 长尾识别不仅有类别不平衡，还存在类别间学习难度差异；仅按频次重加权忽视了本质困难类别，导致总体与尾部性能受限。

Method: 1) 难度估计：基于预测不确定性与历史性能得到类别级难度信号；2) 难度感知训练：用难度自适应损失加权指导优化；3) 架构：Mixture-of-Experts，每个专家覆盖不同类别分布区域；4) 推理：用专家特定的OOD探测得到置信分数，对各专家输出进行加权融合，实现去中心化、输入自适应路由；5) 端到端联合训练。

Result: 在标准长尾数据集上总体性能提升明显，尤其提升稀有与困难类别的识别效果，超过仅频次重加权或无难度建模/集中式路由的方案。

Conclusion: 将难度建模与去中心化专家路由融合，可有效缓解长尾与难类别问题；DQRoute在保持端到端训练的同时，提供稳健、可扩展的长尾识别方案。

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [36] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: 提出CoPLOT：用点级(token)而非BEV特征进行多车协同感知，通过语义重排序、频域增强状态空间模型与闭环邻车到自车对齐，兼顾精度与通信/计算开销，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: BEV中间特征虽紧凑易对齐，但会丢失细粒度3D几何，影响识别与定位；点云直接协同又存在无序、规模大、对位敏感等难题，需要新的点级表示与高效建模/对齐机制。

Method: 1) 点级优化Token作为中间表示；2) 语义感知的Token重排序：利用场景级与Token级语义生成自适应1D顺序；3) 频率增强的状态空间模型(SSM)进行序列建模，捕获跨空间与频谱的长程依赖，区分前景与背景；4) 邻车到自车的闭环对齐：先全局代理级纠偏，再局部Token级细化，降低定位噪声。

Result: 在仿真与真实数据集上均超过现有SOTA，同时通信与计算开销更低。

Conclusion: 点级协同感知可在保留3D结构的同时实现高效对齐与建模；通过重排序+频域增强SSM+闭环对齐，CoPLOT在精度-效率上取得兼顾，验证了点级中间表示的优越性。

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [37] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出一种轻量级、无监督的骨架序列动作定位方法，通过ASTGCN自监督预训练与低维嵌入的曲率拐点检测，实现对体育视频中细粒度动作边界的高效定位，mAP达82.66%、平均延迟29.09 ms，并可泛化到野外数据。


<details>
  <summary>Details</summary>
Motivation: 细粒度动作在非剪辑（untrimmed）体育视频中变化快且微妙，现有监督/弱监督方法依赖大量标注与大模型，计算开销大、泛化与部署受限，亟需轻量、少标注甚至无监督且可实时的定位方案。

Method: 1) 使用骨架（人体关键点）表示，构建注意力式时空图卷积网络（ASTGCN）。2) 通过“块状分区的姿态序列去噪”自监督预训练，让模型学习内在运动动力学，无需人工标签。3) 推理阶段从ASTGCN的低维嵌入定义“动作动力学度量”（ADM），对其曲率曲线找拐点以检测运动边界，实现动作片段定位。

Result: 在DSV Diving数据集上，mAP=82.66%，平均定位时延29.09 ms，性能可比当前监督SOTA，同时保持计算效率。

Conclusion: 该无监督、轻量方案能实时、准确地定位体育视频中的细粒度动作边界，并在未见过的野外跳水视频上无需再训练即可稳健泛化，适用于嵌入式或动态环境中的在线动作分析。

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [38] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 提出一种迭代动态卷积核的轻量去噪方法，用少量参数通过像素自适应核在多次迭代中恢复图像，训练只用单一高斯噪声却能泛化到多种噪声分布与强度，兼顾效率与效果。


<details>
  <summary>Details</summary>
Motivation: 深度去噪方法往往假设特定噪声分布，导致对未知噪声泛化差；即便用大量数据和算力也容易过拟合且不经济。作者希望用更高效、鲁棒、可泛化的机制减少对数据分布和算力的依赖。

Method: 构建一个约0.04M参数的模型，包含：1) 特征提取模块，提取对噪声不敏感的表征；2) 全局统计模块与局部相关模块，分别捕获全局噪声特性与局部结构相关性；3) 核预测模块，基于上述线索为每个像素预测自适应卷积核；4) 迭代应用这些像素自适应核进行去噪，逐步恢复图像。

Result: 在仅使用单一水平的高斯噪声训练的情况下，该模型在多种噪声类型与不同噪声强度的测试中表现优秀，显示出强泛化能力；同时参数量极小（约0.04M），计算高效。

Conclusion: 迭代式动态滤波是一条有效的图像去噪途径：通过像素级可变核与全局/局部统计的结合，可在极小模型规模下实现对未知噪声的鲁棒泛化和高质量复原。

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [39] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: 提出Video-LevelGauge基准，系统评估大型视频语言模型（LVLM）的情境位置偏置；包含标准化探针、可控上下文配置与形态学模式识别分析，覆盖438个视频与~1.3k题目，对27个模型评测发现多数开源模型存在显著头部/邻近内容偏好，而商用模型更均衡；并给出缓解偏置的实证建议。


<details>
  <summary>Details</summary>
Motivation: 现有视频评测多以整段表现为主，忽视模型对不同时间位置内容的敏感性与偏置（如更偏好片头或邻近内容）。位置偏置会影响长视频理解与真实应用的可靠性，因此需要一个能精细控制上下文与位置的专用评测基准与分析方法。

Method: 构建Video-LevelGauge：1) 标准化探针与可定制上下文（可调上下文长度、探针位置、上下文类型），模拟多样场景；2) 结合统计指标与形态学模式识别对偏置进行定量与形态特征分析；3) 数据集含438视频，1,177道多选与120道开放题，人工筛选与验证；4) 在27个SOTA LVLM（商用+开源）上系统评测。

Result: 大量开源LVLM呈显著位置偏置，常见为“头部偏好”或“邻近内容偏好”；商用模型（如Gemini2.5-Pro）在整段序列上更稳定、均衡。还观察到上下文长度、上下文变化程度与模型规模对偏置强度与形态有系统影响。

Conclusion: Video-LevelGauge能有效暴露并刻画LVLM的情境位置偏置；当前开源模型仍存在明显弱点，而商用模型较为稳健。作者提供关于控制上下文长度、增强鲁棒训练与扩大模型规模/数据多样性的建议，可用于缓解偏置并指导模型改进。

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [40] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: 提出车内场景的分布式目标检测与定位框架（ODAL），用车端+云端协同调用视觉大模型，并引入综合评测指标ODALbench。微调后的轻量LLaVA实现89% ODAL_score，较基线提升71%，且比GPT-4o高近20%；同时显著降低幻觉，ODAL_SNR为GPT-4o的3倍。


<details>
  <summary>Details</summary>
Motivation: 车内个人助手需要准确识别/定位外来物体，但车载算力受限，难以直接部署大模型。需要一种既满足精度又符合算力与实时性约束的方案和可量化评测标准。

Method: 提出ODAL分布式架构：将感知与部分推理在车端执行，复杂视觉理解在云端由视觉基础模型完成；并提出ODALbench指标体系，统一评估检测、定位与“幻觉”抑制（含ODAL_score与ODAL_SNR）。比较GPT-4o与轻量LLaVA-1.5-7B，并对后者进行有针对性的微调（领域数据/任务对齐）。

Result: 微调后的ODAL-LLaVA在ODALbench上达89% ODAL_score，较其未微调基线提升71%，并超越GPT-4o近20%；在保持检测准确度的同时显著减少幻觉，ODAL_SNR约为GPT-4o的3倍。

Conclusion: 分布式ODAL架构使视觉大模型在算力受限的车载环境可落地；ODALbench提供可复现、全面的评测；经微调的轻量模型可在该任务上超越重型闭源模型并显著降幻觉，提示面向车内场景的专用微调与系统级分工是高性价比道路。

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [41] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: 提出Vision-SR1：一种自我奖励(无需外部标注/教师)的强化学习框架，通过将VLM推理分解为“视觉感知→语言推理”，用模型自身生成的“自包含视觉感知”来计算奖励，从而同时强化视觉与语言，缓解幻觉和语言捷径问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLM后训练多依赖可验证答案匹配，只监督最终输出，缺乏对中间视觉推理的显式约束，导致视觉信号稀疏、模型偏向语言先验，出现视觉幻觉与语言捷径。引入人工或外部大模型的视觉监督虽可改善，但成本高且与策略分布不匹配，易产生reward hacking。

Method: 将推理拆为两阶段：1) 视觉感知阶段：模型被提示生成“自包含”的视觉感知描述（无需再看图即可答题）。2) 语言推理阶段：用同一模型，仅以上述感知为输入完成语言推理，并据此计算自我奖励。再将该自奖励与最终答案监督结合，通过强化学习优化，兼顾感知与推理。

Result: 在多种视觉语言任务上，Vision-SR1提升了视觉推理能力，显著缓解视觉幻觉，降低对语言捷径的依赖（相较基线更稳健）。

Conclusion: 自奖励的两阶段感知-推理范式，可在不依赖外部监督的情况下，为VLM提供密集且自一致的训练信号，平衡并强化视觉感知与语言推理，从而更可靠地解决视觉幻觉与语言捷径问题。

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [42] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [43] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: 提出一种频率感知的自监督学习方法，用于超广域眼底（UWF）图像增强，结合频率解耦去模糊与Retinex引导的照明补偿，并通过非对称通道整合与颜色保持单元，既提升可视化质量又助力疾病诊断。


<details>
  <summary>Details</summary>
Motivation: UWF图像覆盖视网膜广，但常受模糊与不均匀照明影响，细节与病灶被掩盖。现有方法多针对普通眼底影像，无法满足UWF对病理细节保真的特殊需求。

Method: 自监督框架：1) 频率解耦去模糊模块，设计“非对称通道整合”融合高/低频以兼顾局部细节与全局结构；2) Retinex引导的照明补偿模块，加入“颜色保持单元”，在多尺度空间与频率域估计并校正照度，抑制色偏与过度增强。

Result: 实验显示该方法显著提升UWF图像的清晰度与照明均匀性，并在疾病诊断任务上提升性能，尤其在恢复微小病灶与校正强度不均方面有效。

Conclusion: 首个针对UWF图像增强的专门方法，兼顾细节保真与全局一致性，具备临床实用价值，有望改善视网膜疾病的筛查与管理。

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [44] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [45] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: 提出一种物理启发、模型无关的无监督方法，用谱图社区检测识别合成图像：将CNN特征嵌入到多边类型QC-LDPC图，构造在Nishimori温度下的随机键Ising模型，通过Bethe–Hessian谱是否出现特征间隙判断真实/生成。无需合成数据标注与重训，跨GAN与扩散模型在猫狗、性别二分类任务上达94%+准确率，显示对新生成器和后处理更稳健。


<details>
  <summary>Details</summary>
Motivation: GAN与扩散模型生成图像已逼真到难以分辨，监督检测对未见生成器和对抗后处理泛化差；现有无监督方法多依赖低层统计线索，脆弱。需要一种不依赖特定生成器、鲁棒且可解释的无监督检测框架。

Method: 1) 用预训练CNN提取图像特征并降到32维；2) 将每个图像特征映射为多边类型QC-LDPC图的节点；3) 以样本两两相似度为加权边，按Nishimori温度标定得到随机键Ising模型（RBIM）的耦合；4) 计算Bethe–Hessian矩阵的谱，真实图像集合存在社区结构→谱出现明显间隙；生成图像破坏Nishimori对称性→无间隙或谱塌缩；5) 以谱间隙作为Bayes最优的无监督判别准则。

Result: 在Flickr-Faces-HQ与CelebA上的猫vs狗、男vs女二分类数据，将真实照片与来自GAN和扩散模型的合成图混合，无需使用任何标注的合成样本或重训特征提取器，检测准确率超过94%。谱分析显示真实集具有多个清晰分离的谱间隙，而生成集谱塌缩。

Conclusion: 通过将深度特征嵌入LDPC图并在Nishimori温度下构造RBIM，再用Bethe–Hessian谱间隙作为判据，可实现对未知生成器鲁棒的无监督合成图像检测，具备理论可解释性与实证效果。未来将扩展到视频与多类异常检测。

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [46] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: 提出LabelGS：在3D Gaussian Splatting中引入跨视角一致的语义标注与遮挡建模，实现可分割的3D表示，训练速度比Feature-3DGS快22倍且分割更优。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽在重建与渲染上高效高质，但缺乏对场景语义分割/对象解耦的能力，限制其在需要理解与操作场景（如编辑、交互、机器人感知）的应用；需要一种能为高斯添加稳定语义标签并处理遮挡与标签冲突的问题的方案。

Method: 在高斯表示中引入“标签感知”机制：1) 跨视角一致的语义掩码对3D高斯进行标注约束；2) Occlusion Analysis Model避免优化时对遮挡区域的过拟合；3) Main Gaussian Labeling模型将2D语义先验提升至3D高斯并维护一致性；4) Gaussian Projection Filter解决多标签冲突；5) 随机区域采样策略加速并稳定优化，同时实现高斯表示的有效解耦。

Result: 在3D场景分割任务上超过SOTA（含Feature-3DGS），在1440×1080分辨率训练下实现22倍加速，同时获得更高的分割质量；提供开源代码链接。

Conclusion: LabelGS为3DGS赋予可靠的3D语义分割与对象解耦能力，通过遮挡建模与标签冲突处理提升鲁棒性，并显著提高训练效率，为需要场景理解的下游任务提供更实用的3D表示。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [47] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [48] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 提出一种基于人脸基础模型的强泛化视频深伪检测框架，结合多数据集微调、三元组损失与归因式监督，在多基准与真实场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测模型在分布外与野外数据上泛化差，难以应对不断提升的伪造真实感与可得性，需要能跨数据集与操纵类型稳健检测的方法。

Method: 以自监督训练的面部基础模型FSFM为骨干，利用覆盖换脸与表情重演的多深伪数据集进行微调；引入多种三元组损失变体提升真实/伪造嵌入可分性；尝试按操纵类型或数据来源进行归因式监督，分析其对泛化的影响。

Result: 在多样化评测基准上优于现有方法，尤其在具有分布偏移的真实世界场景中展现更强鲁棒性与泛化能力。

Conclusion: 结合基础模型表征、度量学习损失与归因监督可显著提升视频深伪检测的跨域泛化，适用于实际复杂环境。

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [49] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: 提出POEv2，一种统一的线段检测框架，可同时用于通用与线框两类任务；基于边缘强度图与像素方向估计，配合高效边缘检测器，在三大公开数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法分为通用与线框两类，目标不同导致跨任务表现不佳；缺乏一个既鲁棒又统一、可兼容不同边缘检测器的线段检测框架。

Method: 在Pixel Orientation Estimation（POE）基础上改进为POEv2：先由任意边缘检测器生成边缘强度图，再进行像素级方向估计与聚合，从而从边缘图中稳健地提取线段；框架可根据任务偏好（通用/线框）进行适配。

Result: 将POEv2与一个高效边缘检测器结合后，在三个公开数据集上达到最新最优性能（SOTA）。

Conclusion: POEv2作为鲁棒、通用的线段检测框架，统一了通用与线框检测场景，具有模块化（可插拔边缘检测器）与强性能优势。

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [50] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: 提出SPLF-SAM，用自提示机制+UMFEB多尺度嵌入与MAFA频域自适应滤波，提升光场显著性目标检测，尤其对多尺度与小目标抑噪有效，全面超越10个SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LF SOD多忽略提示（prompt）信息利用，且很少分析频域特性，导致小目标被噪声淹没，多尺度目标也难以统一处理。SAM虽强，但缺少任务化自提示与光场特性适配。

Method: 在SAM框架上加入自提示SPLF-SAM：1) UMFEB统一多尺度特征嵌入，增强不同尺寸目标的表征；2) MAFA多尺度自适应过滤器在频域学习，抑制噪声、突出小目标；整体端到端训练，面向光场数据。

Result: 在多个LF SOD基准上广泛实验，较10个SOTA方法取得全面领先（指标未给出，但宣称显著优于对比）。

Conclusion: 融合自提示、多尺度空间表征与频域自适应滤波，可显著提升LF SOD性能，尤其改善小目标与多目标场景表现；代码将开源。

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [51] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar提出一个单一统一的前馈式框架，能从单张图、多视角或单目视频在数秒内重建高质量3DGS头像，通过大型高斯重建Transformer、跨帧聚合与多粒度条件、增量式高斯聚合等设计，实现随输入增加而质量提升，兼顾速度与质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D头像重建耗时高、对数据质量敏感、且无法充分利用多样/增量输入；许多方法对动画引起的错位不鲁棒，且常在更多观测下不能有效提质。

Method: 提出大型Gaussian Reconstruction Transformer：1) VGGT风格的多帧聚合Transformer，注入初始3D提示以预测可聚合的规范化3DGS；2) 多粒度条件编码（相机位姿、FLAME表情、头部姿态）抑制动画导致的错位，适配可变长度输入；3) 基于关键点跟踪与切片融合损失的增量高斯聚合，实现可随观测数递增的重建。

Result: 在多种数据形态下，FastAvatar能在数秒内生成高质量3DGS头像；相较现有方法，质量更高、速度具竞争力，并可随输入增加而进一步提升质量。

Conclusion: FastAvatar提供质量-速度可调的高可用化头像建模范式，统一支持多种日常录制输入，并通过增量重建有效利用更多观测，优于以往浪费输入的数据利用方式。

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [52] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [53] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 提出AIM（自适应网络内调制）方法，识别并利用主导模态网络中的“欠优化参数块”，在不压制强模态的前提下促进弱模态学习，并按网络深度自适应调制，全面提升不平衡多模态学习表现。


<details>
  <summary>Details</summary>
Motivation: 现有不平衡多模态学习多通过削弱强（主导）模态来帮助弱模态，导致整体性能受损。作者观察到这种做法忽略了网络内部不同参数与层次的优化偏置，因而希望在不牺牲强模态的情况下实现平衡训练。

Method: 提出AIM：1）优化偏置建模——识别主导模态中“欠优化”的参数；2）参数重构——将这些欠优化参数解耦为辅助块（Auxiliary Blocks）；3）联合训练策略——在与弱模态联合训练时，鼓励模型更多依赖这些性能受损的辅助块，从而避免对弱模态的抑制并定向优化强模态的短板；4）深度自适应调制——评估各网络深度的模态不平衡程度，按层自适应调整调制强度。方法可无缝适配不同骨干、融合策略与优化器。

Result: 在多个基准数据集上优于现有最先进的不平衡多模态学习方法；在不同骨干网络、融合方式与优化器上展现良好泛化与稳健性。

Conclusion: 通过面向网络内部优化状态的细粒度调制，AIM首次实现“不压制强模态且不牺牲弱模态”的平衡多模态学习，既缓解优化偏置，又提升整体多模态性能，并具备良好可迁移性。

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [54] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [55] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: 提出MAPo：对动态3D Gaussian Splatting按运动强度进行分区，高动态部分时间递归分段并为每段复制/专用形变网络，低动态部分视为静态；配合跨帧一致性损失，在复杂/快速运动区域显著提升渲染质量且计算成本相近。


<details>
  <summary>Details</summary>
Motivation: 现有基于形变场的动态3DGS用单一统一模型表达多样运动，导致高动态区域渲染模糊、细节丢失；需要一种既能针对性建模快速变化，又不显著增加成本的方法。

Method: 1) 动态评分驱动的3D高斯划分：区分高/低动态高斯；2) 对高动态高斯进行时间递归分段，对每个分段复制并专用化其形变网络，捕获细粒度运动；3) 低动态高斯当作静态处理，降低计算；4) 引入跨帧一致性损失，缓解分段边界的跨帧不连续并提升画质。

Result: 在多数据集上，相比基线获得更高的渲染质量指标，优势在复杂或快速运动区域尤为明显，同时总体计算成本与基线相当。

Conclusion: 运动感知的分区与时间分段、专用形变网络结合一致性正则，可在不显著增加成本的情况下，有效解决动态场景中高动态区域的模糊与细节缺失问题，提升动态3DGS重建的保真度与时序连续性。

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [56] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic 提出用于多视图材质估计的一步扩散模型，通过像素域、材质感知损失与细节注入网络（DIN）克服一步扩散过平滑与VAE细节丢失问题，在反照率PSNR与金属度/粗糙度MSE上显著优于现有方法，同时避免多步采样的高耗时与随机性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式材质估计需要多步去噪，推理耗时且具有随机性，与确定性的材质参数回归目标不匹配，导致估计方差大；同时一步扩散往往过平滑，VAE 编码带来细节损失，需要提升效率、稳定性与保真度。

Method: 1) 设计一步扩散（one-step diffusion）框架用于多视图材质估计；2) 在像素空间施加基于材质属性的专用损失（如针对反照率、金属度、粗糙度的不同约束）以缓解一步扩散过平滑；3) 提出细节注入网络（DIN）以弥补VAE编码导致的高频细节流失并提升锐度；4) 以确定性推理减少方差与时间成本。

Result: 在实验中，相比SOTA：反照率PSNR提升9.9%；金属度MSE降低44.4%；粗糙度MSE降低60.0%；预测更稳定、方差更低，推理速度更快。

Conclusion: StableIntrinsic 将扩散估计从多步随机采样转向一步确定性推理，通过像素域材质感知损失与DIN 实现既快又稳且细节清晰的多视图材质参数恢复，并在多项指标上优于现有方法。

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [57] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [58] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: 提出一种改进的编码器-解码器网络，用综合注意力块、Mamba注意机制和数据融合块（含PCA降维）来处理多源（RGB/高光谱/多光谱/融合）废弃物图像，以提升自动分拣准确率与效率，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动分拣面对非生物可降解垃圾时，因废物流成分复杂、外观多变，现有模型鲁棒性与精度不足，尤其在多源成像数据融合与高维谱信息利用方面存在瓶颈。

Method: 在Encoder-Decoder框架上：1) 在解码器加入“综合注意力块”，结合卷积与上采样以细化特征；2) 并行引入Mamba架构的注意机制增强时序/依赖建模与全局感受；3) 设计“数据融合块”，面对>3通道图像先以PCA降到3维保留最大方差信息，再与RGB等输入对齐用于后续处理；在RGB、高光谱、 多光谱及RGB+高光谱场景上统一训练与评测。

Result: 在多个数据类型（RGB/高光谱/多光谱/融合）上，相较现有方法取得显著性能提升（文中未给出具体数值，但称“显著优势”），表明注意力增强与PCA融合策略有效。

Conclusion: 融合综合注意力、Mamba注意与PCA数据融合的改进编码器-解码器能稳健提升垃圾分拣的精度与效率，适用于多源成像场景；未来可探讨端到端的多通道处理（替代PCA）与更通用的自监督/少样本学习以进一步提升。

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [59] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: 提出一套“技巧合集”以在多域病理图像中实现实时有丝分裂像检测：基于RTMDet单阶段检测器，通过多域训练、均衡采样、数据增强与针对性难例挖掘，5折跨数据集F1=0.78–0.84，MIDOG2025初测F1=0.81，兼顾速度与精度，优于更大模型。


<details>
  <summary>Details</summary>
Motivation: 病理切片中MF检测受扫描仪、染色、组织类型和伪影影响，跨域泛化差且临床对实时性有要求；需要一种既鲁棒又高效、可在未知域上稳定表现的方法，推动临床可用性。

Method: 以轻量高效的单阶段RTMDet-S为骨干：1) 汇集多域训练数据（多扫描仪、多组织、多染色）；2) 均衡采样以缓解类别/域不均衡；3) 精心的数据增强以模拟域差；4) 针对坏死/碎屑区域进行硬负样本挖掘，降低误检；5) 组内化的5折跨数据集验证评估泛化；强调实时推理以满足部署。

Result: 在多数据集分组5折交叉验证中F1=0.78–0.84；在MIDOG 2025挑战初始测试集上F1=0.81，超过更大模型；推理速度达实时级（具体帧率未给出）。

Conclusion: 通过一系列训练策略与轻量检测器的结合，可在未知域保持较高准确率并实现临床可用的实时性；方法具备良好的速度-精度权衡，适合实际部署。

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [60] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: 提出CSSL：通过上下文感知阈值动态抑制神经元激活，在事件相机任务上以极高稀疏度达到SOTA同等或更优性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据稀疏且高时域分辨率，但现有深度方法未充分利用稀疏性，难在边缘设备部署；脉冲神经网络能耗低却在复杂任务上性能落后；现有提高网络稀疏度的方法常依赖手工调参的稀疏正则，难稳定泛化。需要一种能自适应输入分布、无须显式稀疏损失即可获得高激活稀疏的学习机制。

Method: 提出上下文感知稀疏时空学习（CSSL）：在网络中引入基于输入分布的上下文感知阈值（context-aware thresholding），动态调节各层或神经元的触发阈值，从而按场景自适应控制激活密度；不依赖额外稀疏损失项。将CSSL嵌入事件视觉任务网络（目标检测、光流估计）以利用事件数据的时空稀疏结构。

Result: 在事件目标检测与光流任务上，CSSL在保持“极高神经元稀疏度”的同时，达到与SOTA相当或更优的精度；相较常规深度模型显著降低激活开销，优于SNN在复杂任务的性能-能效折中（摘要未给具体数字）。

Conclusion: CSSL通过上下文感知阈值实现自然稀疏激活，无需显式稀疏正则，使事件视觉在类神经形态处理上更高效，并在关键任务上不牺牲甚至提升性能，展示在资源受限边缘与神经形态硬件中的应用潜力。

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [61] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [62] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: 提出ERSR半监督框架，用双评分筛选、椭圆约束伪标签细化与对称多重一致性，专攻胎头超声分割，在HC18与PSFH上以少标注达SOTA。


<details>
  <summary>Details</summary>
Motivation: 胎头超声图像噪声大、边界模糊且标注昂贵，现有半监督方法生成伪标签不稳、形状先验利用不足、一致性约束针对性弱，导致分割不鲁棒。

Method: 半监督Teacher–Student框架：1) 双评分自适应过滤——以边界一致性与轮廓规则性两条准则对教师输出打分并筛除低质伪标签；2) 椭圆约束伪标签细化——对通过筛选的预测用最小二乘拟合椭圆，增强椭圆中心附近像素置信度并抑制噪声；3) 基于对称性的多重一致性——在扰动图像、对称区域以及原始预测与伪标签之间实施多层次一致性正则，强化稳定形状表征。

Result: 在HC18：用10%/20%标注，Dice分别为92.05%/95.36%；在PSFH：91.68%/93.70%。均达SOTA。

Conclusion: 将形状先验（椭圆、对称）融入半监督学习，显著提升胎头超声分割鲁棒性与精度，尤其在低标注比例下有效。

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [63] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [64] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出面向机器视觉系统（MVS）的图像质量评估（MIQA）框架与大规模数据集（MIQD-2.5M），并给出区域感知模型（RA-MIQA）。相较于基于人眼视觉（HVS）的指标与多种基线，RA-MIQA在相关性与准确性上显著提升，揭示不同任务对退化的敏感性差异，但在背景退化、面向准确率的估计和细微失真上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估多基于人类主观感知（HVS），难以反映图像退化对机器视觉任务性能（如分类/检测/分割）的真实影响。需要一个以“机器性能”为目标变量的质量评估范式，指导在不利视觉条件下的鲁棒感知与处理。

Method: 1) 定义MIQA范式，建立端到端评估流程，以MVS的一致性与准确性为质量目标。2) 构建MIQD-2.5M数据集：覆盖75个视觉模型、250种退化、3类任务，采集模型在各种退化下的性能响应。3) 提出区域感知的RA-MIQA模型，通过细粒度空间退化分析来预测对MVS的影响。4) 与7个HVS型IQA指标和5个经典骨干（重训）进行系统基准对比。

Result: RA-MIQA在多维度上优于对比方法：例如在图像分类任务上，一致性SRCC提升13.56%，准确性SRCC提升13.37%。实验还表明不同任务对退化类型的敏感性不同；HVS指标难以预测MVS质量，专门的MIQA模型在背景退化、以准确率为导向的估计、以及轻微失真场景仍表现不足。

Conclusion: 以机器为中心的IQA可更好刻画退化对MVS的实际影响，RA-MIQA与MIQD-2.5M为提升MVS可靠性提供了基础设施与方法论，但仍需针对背景退化、准确性预测与细微失真进行优化；该工作为面向机器的图像处理与优化奠定基础。

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [65] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出一个在第一人称视角下联合预测“下一步动作”和“视觉后果”的两阶段框架：先预测手部轨迹，再以因果跨模态注意力引导LDM逐帧合成未来视频；在Ego4D、BridgeData、RLBench上在动作预测与未来视频生成均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLA善于动作预测但不显式建模动作对画面的影响；视频预测模型能合成未来但未条件化在具体动作上，常出现不可信或语境不一致的画面。缺少能同时建模动作和视觉未来、且适用于人类活动理解与机器人操作的统一方案。

Method: 两阶段统一框架、以手部轨迹为条件：1) 连续状态建模融合视觉、语言、动作历史，显式预测未来手轨迹；2) 因果跨注意融合多模态，利用推断的动作信号引导基于图像的Latent Diffusion Model逐帧生成未来视频，并输出动作预测与视觉结果。

Result: 在Ego4D、BridgeData与RLBench上，动作预测和未来视频合成均超越SOTA基线，验证了联合建模和因果跨注意+LDM的有效性。

Conclusion: 该方法首次在统一模型中同时处理第一人称人类活动理解与机器人操作，提供可解释的动作与视觉后果预测，为具身智能中的计划与交互提供更可靠的前瞻能力。

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [66] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: 提出MCMeshGAN：一种多模态、条件化的mesh-to-mesh GAN，用于预测胸主动脉瘤3D形变与生长，结合局部KNN卷积与全局GCN，并引入临床条件分支，在自建纵向数据集TAAMesh上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 主动脉瘤进展预测需要同时刻画细粒度局部形变和全局解剖变化，且要结合临床时间与属性，但现有方法要么过度平滑、缺失细节，要么无法进行时间条件化控制，难以达到临床可用的个体化预测。

Method: 构建首个多模态条件mesh-to-mesh GAN（MCMeshGAN）：双分支生成器将基于KNN的局部卷积网络（KCN）保留细节，和全局GCN捕获长程结构；另设条件分支编码年龄、性别与目标时间间隔，支持回溯/前瞻预测。提出并使用含CT、3D网格、临床数据的纵向数据集TAAMesh进行训练与评估。

Result: 在TAAMesh上，MCMeshGAN在几何重建误差与临床关切的直径估计上均优于最先进基线，生成的形变既解剖合理又时间可控。

Conclusion: MCMeshGAN为个体化3D疾病轨迹建模提供了更准确、可控且临床相关的预测能力，向可部署的临床应用迈出重要一步；代码与基线已开源以促进复现与扩展。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [67] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [68] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: 提出TrajFusionNet：一种融合行人未来轨迹与车辆速度预测作为先验的Transformer模型，通过序列注意力与视觉注意力双分支，在三个主流数据集上达到SOTA，并以少模态实现最低端到端推理时延。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶上路后，需要实时判断行人是否即将过街，以确保安全与顺畅驾驶。现有方法要么依赖重型多模态传感（耗时、复杂），要么忽略未来动态先验（如未来轨迹、车速），限制了准确性与实时性。因此动机是：在保证高精度的同时，以更轻量的输入与更低延迟实现对行人过街意图的鲁棒预测。

Method: 提出TrajFusionNet，核心是两条并行分支并最终融合：1) 序列注意力模块（SAM）：对观测与预测的行人轨迹序列、车辆速度序列进行Transformer式时序建模，学习时序交互与先验。2) 视觉注意力模块（VAM）：将预测的行人边界框叠加到场景图像上，构造轻量视觉表示，再用视觉注意力提取空间语义与运动先验。两分支特征融合后输出过街意图。模型仅使用少量轻量模态（轨迹、车速和叠加框图像），并联合考虑未来预测先验。

Result: 在三大常用PCI数据集上取得SOTA精度；同时端到端总推理时间（含预处理和模型运行）为同类方法最低。

Conclusion: 融合未来行人轨迹与车辆速度先验，并以双注意力分支实现的轻量多模态建模，可同时提升过街意图预测的准确性与实时性，适用于资源受限与实时性要求高的自动驾驶场景。

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [69] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: 提出SMI方法，用互信息与增量训练，从整板全部纤维光谱中估计位置相关的天空背景，解决传统Super Sky平均化导致的环境不匹配与谱线位移问题；在LAMOST数据上验证，尤其在蓝端提升更明显。


<details>
  <summary>Details</summary>
Motivation: 现有多目标光纤光谱的天空背景扣除主要依赖少数天空纤维构建的平均Super Sky，无法刻画各目标周围局部环境与谱线位移，导致扣除残差大，影响目标谱质量，特别在蓝端更严重。

Method: 提出SMI（基于互信息的天空背景构建）：1) 以全板所有纤维光谱为输入；2) 网络一含波长校准模块，从光谱中提取天空特征并按发射线位置校正以缓解特征漂移/位移；3) 网络二采用增量训练与互信息目标：最大化不同光谱表征间的互信息以捕捉共性成分（公共天空背景），同时最小化相邻光谱表征间的互信息以分离个体差异（位置相关的局部成分）；4) 最终为每个目标位置生成个体化天空背景估计。

Result: 在LAMOST数据上实验显示，SMI较传统Super Sky法获得更优的天空背景估计，在观测过程中稳定，且在蓝端改进尤为显著。

Conclusion: SMI能够利用全板数据与互信息学习机制，生成位置自适应的天空背景，缓解谱线漂移与局部环境差异带来的扣除误差；实际数据验证有效，特别提升蓝端表现。

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [70] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: 用多光谱LiDAR与深度学习在城市复杂环境中更精准、快速地从点云中提取树木；最佳方案是将空间几何+伪NDVI输入SPT模型，mIoU约85.3%，错误率较仅几何输入降低10.61百分点。


<details>
  <summary>Details</summary>
Motivation: 城市树木监测支撑绿化政策与电力设施风险管理，但传统机载激光雷达在城市中因目标复杂、树种多样而提取困难。多光谱LiDAR能同时提供三维与光谱信息，或可缓解上述问题。

Method: 基于MS-LiDAR点云，比较三种SOTA深度学习点云模型（SPT、PTv3、PTv1）进行树点提取；设计特征组合实验，评估仅空间特征 vs. 空间+伪NDVI（pNDVI）；以mIoU与错误率、推理时间衡量性能与效率。

Result: SPT在精度与速度上表现最佳，达到mIoU 85.28%；引入pNDVI与空间信息联合输入可将检测错误率相对仅空间信息降低10.61个百分点；整体表明MS-LiDAR的光谱信息显著提升树木识别。

Conclusion: MS-LiDAR结合深度学习可显著改进城市树木点提取与清查效率；SPT配合空间+ pNDVI是当前最优选择之一，建议未来在城市尺度树清查中采用并探索更多光谱特征与泛化能力评估。

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [71] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: 提出PersonaAnimator与PersonaVid，开创“视频到视频的运动个性化”任务：从无约束视频中学习个体化动作风格，并用物理约束正则提升物理一致性，性能超SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法问题：1) 姿态驱动的角色迁移只复制轨迹不学风格，表达力不足；2) 风格迁移依赖昂贵稀缺的动作捕捉数据；3) 生成动作常违背物理规律。亟需一种能从普通视频学习个性化风格、且物理可行的方案。

Method: 提出PersonaAnimator框架：从非结构化视频中直接学习人物的个性化运动模式，实现视频到视频的个性化动作迁移；并提出Physics-aware Motion Style Regularization，在生成过程中施加物理一致性约束。为支持任务，构建PersonaVid数据集，覆盖20类内容与120类风格。

Result: 在广泛实验中，PersonaAnimator在视频到视频动作个性化任务上优于现有运动迁移方法，成为新的基准；生成结果在风格一致性与物理可行性方面表现更佳。

Conclusion: 视频到视频运动个性化是可行且有效的方向。PersonaAnimator结合新数据集与物理正则，可在无需动捕的情况下实现个性化风格迁移，并提升物理合理性，为后续研究提供基准与方法基础。

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [72] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [73] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: 提出对象级集合相似度(OSS)度量，在无需训练检测器的前提下评估与筛选主动学习(AL)方法，并用于挑选具代表性的验证集，从而降低计算开销并提升评测可靠性；在KITTI、BDD100K、CODA上以不确定性采样和两种检测器验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实中的目标检测AL代价高、评测不稳定：每轮需训练多模型（单个检测器可达282 GPU小时），不同验证集导致方法排名波动，安全关键领域难以信赖。需要一种无需反复训练即可判断AL有效性、且能稳定评测的方法。

Method: 提出Object-based Set Similarity (OSS)：基于已标注的对象裁剪的特征（对象级表征），计算训练集与目标域（或验证集）之间的集合相似度。用途：1) 在训练前估计AL采样策略的有效性，剔除无效方法；2) 用相似度挑选代表性验证集，减小排名方差。OSS与检测器无关，可嵌入现有AL流程。

Result: 在KITTI、BDD100K、CODA三数据集上，以不确定性驱动的AL方法为案例，结合EfficientDet与YOLOv3两种检测器实验，证明：OSS能在不训练检测器的情况下预测/区分AL方法效果，并能挑选更具代表性的验证集，使评测更稳健。

Conclusion: OSS首次从对象相似性的角度统一了目标检测AL训练与评估策略：既能大幅降低计算成本，又提升评测可靠性；可作为与模型无关、依赖对象裁剪标注的实用框架，便于在计算资源与安全性要求苛刻的真实场景落地。

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [74] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: 用2D大模型生成的分割掩码投射到3D点云，扩大稀疏标注，并结合置信度/不确定性一致性与伪标签传播，显著提升3D弱监督分割。


<details>
  <summary>Details</summary>
Motivation: 3D点云标注昂贵且困难，现有弱监督方法多局限在3D域，未充分利用2D–3D互补性；已有伪标签/扩展标签方法存在噪声大、利用不足的问题。2D基础模型在分割上表现强，亟需将其能力迁移到3D以缓解3D标注稀缺。

Method: 1) 利用2D基础分割模型为多视角图像生成分割掩码；2) 通过几何对应（相机位姿/投影-反投影）将2D掩码传播到3D点云形成3D掩码；3) 用这些3D掩码扩展极稀疏的3D人工标注，得到更大标注集；4) 在3D点云增强下施加基于置信度与不确定性的一致性正则，筛选可靠伪标签；5) 将筛选的伪标签在3D掩码区域内进一步扩散，持续自训练。

Result: 相比仅用3D弱监督的基线，方法在3D语义分割上取得显著提升（抽象未给具体数值），表明2D基础模型掩码与不确定性一致性结合能有效提升标签质量与覆盖。

Conclusion: 通过2D→3D掩码传播与置信度/不确定性驱动的伪标签选择与扩散，能高效放大量且净化3D弱监督训练信号，弥合稀疏3D标注与高性能分割之间的鸿沟。

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [75] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: 提出WaveHiT-SR：在分层Transformer中嵌入小波变换与自适应分层窗口，扩大感受野、降低复杂度，在SR上以更少参数/更低FLOPs/更快速度达SOTA。


<details>
  <summary>Details</summary>
Motivation: 窗口自注意力的二次复杂度迫使采用小且固定窗口，限制感受野与长程依赖建模；SR需要同时关注全局结构与高频纹理，现有方法在效率与性能间权衡不足。

Method: 1) 在层次化Transformer中使用自适应分层窗口取代固定小窗口，捕获不同尺度特征与长程依赖；2) 引入小波变换，将图像分解为多频子带，分别建模全局（低频）与局部细节（高频），并进行多级分解与逐级重建；3) 通过层次处理降低计算量，同时保持结构细节；4) 将该思想应用于SwinIR-Light、SwinIR-NG、SRFormer-Light等精简变体。

Result: 在广泛实验中，改进版模型在参数更少、FLOPs更低、速度更快的同时取得最先进SR结果；能更好捕获低频细节并增强高频纹理。

Conclusion: WaveHiT-SR有效缓解窗口自注意力的复杂度与感受野矛盾，借助小波多频分解与分层重建实现高效高性能SR，并可通用于现有轻量Transformer SR架构。

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [76] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [77] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [78] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 评估25个最先进视觉-语言模型（VLM）在四个基准数据集上的图像地理定位能力，发现对社交媒体式内容可达高精度（约61%），而对通用街景表现较差，揭示潜在隐私风险与模型推理特征。


<details>
  <summary>Details</summary>
Motivation: VLMs正展现越来越强的图像地理定位能力，可能被用于跟踪与监控；现有研究缺乏对生成式VLM地理定位精度、边界与非预期推断能力的系统性评估。

Method: 对25个SOTA VLM在四个多样环境的数据集上进行系统评测与比较，并分析其内部推理迹象、强弱项与误差模式。

Result: VLM在通用街景上的定位效果差，但在更像社交媒体发布的图像上准确率显著提高（最高约61%）。揭示不同场景下能力差异与潜在隐私泄露面。

Conclusion: 当前VLM已能对社交媒体风格图像进行较高精度定位，带来紧迫的隐私风险；需重视模型滥用防护、隐私保护与更细致的能力边界研究。

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [79] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: 提出GLSim：一种无训练、结合全局与局部跨模态相似度的目标幻觉检测框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言模型在真实场景中会产生目标幻觉（描述不存在的物体），影响安全与可靠性。现有检测方法多仅用全局或局部视角，单一视角限制了检测的稳定性与准确性。

Method: 提出GLSim框架：无需额外训练，计算图像与文本在两种尺度上的嵌入相似度——全局（整图-整句/整体语义）与局部（候选目标/区域-对应词/短语）。将两种信号进行互补融合，得到对象级幻觉分数，用于判断是否幻觉。并在多数据集/场景上进行系统基准评测。

Result: 在全面基准中，GLSim在对象幻觉检测上显著优于强基线，取得更高的准确率/召回率等指标，表现更稳健、泛化更好。

Conclusion: 结合全局与局部的跨模态相似度、且无需训练的方案能更可靠地检测目标幻觉；GLSim为多场景下的幻觉检测提供了有效且实用的工具。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [80] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [81] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [82] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: 提出OpenM3D：在无人工标注、只用多视角RGB-D训练的开集室内3D检测器。核心是单阶段架构，利用2D诱导体素特征（ImGeoNet），配合高质量3D伪框的类无关定位损失与多样CLIP特征的体素-语义对齐损失。通过图嵌入把2D分割合并成一致3D结构生成伪框，并从相关2D段采样多样CLIP特征做对齐。推理仅需多视图图像，速度0.3s/scene，在ScanNet200与ARKitScenes上精度与速度均优于现有方法（含强两阶段和带多视深度基线）。


<details>
  <summary>Details</summary>
Motivation: 开集3D检测在点云上发展较多，但基于图像的方法受限且需要标注成本高。希望在无3D框与类别标注下，利用多视图图像与预训练视觉语言模型，实现高效高精度的开集室内3D检测。

Method: 1) 单阶段检测器OpenM3D：基于ImGeoNet获取2D诱导的体素特征；2) 3D伪框生成：提出基于图嵌入的方法，把多视2D分割合并为一致3D结构，得到高精度高召回的伪3D框；3) 训练目标：a) 类无关3D定位损失，学习自高质量伪框；b) 体素-语义对齐损失，用与每个3D结构关联的多样2D段的CLIP特征，与体素特征对齐，支持开放词汇；4) 训练设置遵循OV-3DET：给定带位姿的RGB-D，无人工标注；5) 推理：仅需多视图图像，单阶段高效。

Result: 生成的伪框较现有方法（含OV-3DET方案）在精度与召回上更优。OpenM3D在ScanNet200与ARKitScenes上，实现更高精度与更快速度（约0.3秒/场景），超过强两阶段（类无关检测+ViT-CLIP分类器）和带多视深度估计的基线。

Conclusion: 高质量3D伪框与多样CLIP语义对齐是训练高精度单阶段开集3D检测器的关键。OpenM3D在无标注、多视图图像输入下，实现了SOTA的精度/速度权衡，优于现有两阶段与多视深度基线。

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [83] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: 论文面向MARIO挑战，通过融合CNN与模型集成完成两时点OCT切片演化分类（任务1），并提出Patch Progression Masked Autoencoder预测三个月后的OCT，再用任务1模型判定演化（任务2），两项均进前十。


<details>
  <summary>Details</summary>
Motivation: AMD患者的渗出性活动需要及时、持续监测以优化抗VEGF治疗方案；自动从OCT追踪病变进展能实现更个性化和高效的随访与治疗决策。

Method: - 任务1：构建融合式CNN（多视角/多分支特征融合）并进行模型集成，输入为两次连续检查的2D切片对，输出演化类别。
- 任务2：提出Patch Progression Masked Autoencoder（以MAE思想进行时序预测），从当前检查生成三个月后的OCT，再调用任务1的分类器比较当前与生成的OCT，输出演化预测。

Result: 在MARIO挑战的两个任务中均取得Top 10的成绩。由于团队成员与主办方同属一机构，按规则不参与奖金评选。

Conclusion: 将生成式时序预测（MAE变体）与判别式分类器级联，有助于OCT中的AMD进展预测与监测；融合CNN+集成策略在配对切片演化分类上有效。

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [84] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: 提出PAUL框架，面向跨视角地理定位中的“配对噪声/错位”问题，通过基于不确定性的分区与协同增强、证据式协同训练，选择性增强高置信区域并抑制错位噪声，在多噪声比例下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实中无人机-卫星图像对常存在GPS漂移等导致的系统性错位，仅有局部对应，现有方法假设完美对齐，导致训练受噪声干扰、泛化差。需建立能处理“部分对应+配对噪声”的训练机制，缩小理想基准与真实应用的差距。

Method: 提出PAUL（Partition and Augmentation by Uncertainty Learning）：1）不确定性感知的协同增强：估计样本与区域级对应不确定性，选择性地只对高对应置信区域进行数据增强和正对比，降低错配区域权重。2）证据式协同训练：结合不确定性与损失差异进行样本/区域分区；利用证据理论或evidence-based分布（如Dirichlet）建模预测置信度，指导特征学习与伪监督；在多视角/多头之间进行协同一致性约束。3）区别于过滤或纠标签，不丢弃数据而是针对性重加权与区域级增强，抑制噪声梯度。

Result: 在多种噪声比例（错位强度）和主流跨视角定位基准上，PAUL的各组件经消融验证有效，整体性能稳定领先于其他针对噪声对应的竞争方法。

Conclusion: 不确定性驱动的分区与协同增强可在存在系统性错位的跨视角定位任务中提供鲁棒监督，缓解噪声对应的负面影响，提升实际场景适用性与性能。

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [85] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 提出一种离散扩散的VLA统一解码器：用单一Transformer在离散动作token上做扩散式逐步细化，兼容VLM训练接口，支持并行与自适应解码，性能显著优于AR与连续扩散。


<details>
  <summary>Details</summary>
Motivation: 现有VLA解码器要么自回归固定顺序，易受顺序瓶颈与误差传播影响；要么采用连续扩散/flow外接头，需专门训练与迭代采样，破坏与VLM的一致性与可扩展性。需要一种既保留扩散渐进细化优势、又原生兼容离散token与跨熵训练的统一架构。

Method: 将动作表示离散化为“动作块”token，使用单一Transformer在这些离散token上执行离散扩散：1) 训练与VLM相同的交叉熵目标，无需特殊噪声回归损失；2) 解码采用渐进细化的扩散过程，支持并行更新；3) 自适应解码顺序：先解决容易的动作元素，再处理困难部分；4) 二次re-masking机制，跨多轮细化重审不确定预测，实现一致性与纠错；5) 统一解码器保留预训练视觉-语言先验并减少函数评估次数。

Result: 在LIBERO达96.3%平均成功率；SimplerEnv Fractal视觉匹配71.2%；SimplerEnv Bridge总体49.3%；均优于自回归与连续扩散基线，并展现更少的函数评估和更强的鲁棒性/一致性。

Conclusion: 离散扩散式动作解码在VLA中实现了统一、可扩展且高效的策略学习：既维持VLM训练范式，又提供并行与自适应解码及纠错能力，为将VLA扩展到更大模型与数据奠定基础。

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [86] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出将双鱼眼相机模型融入3D Gaussian Splatting的标定与渲染框架，联合优化相机畸变与高斯参数，从带缺陷的全景输入合成无缝360°新视图，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 消费级双鱼眼相机生成的全景图存在缝隙与角度畸变，影响VR/机器人等应用；现有360°渲染模型难以真实模拟与纠正这些特定成像缺陷。

Method: 将双鱼眼成像模型嵌入3D Gaussian Splatting管线，构建同时包含镜头间隙与角度畸变的可微标定变量；与3D高斯的形状、颜色与不透明度等参数进行联合优化，使渲染过程既能重现真实双鱼眼伪影，又能在重建中校正误差，输出无缝全景。

Result: 在真实数据集上进行广泛评估，能从存在伪影/接缝的输入中生成无缝360°图像，定量与定性均优于现有360°渲染与重建方法。

Conclusion: 通过在3D Gaussian Splatting中显式建模双鱼眼相机与可微标定，能有效吸收与纠正成像缺陷，实现更高质量的360°新视图合成；方法稳健且在现实数据上表现领先。

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [87] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [88] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: 提出将BioCLIP2知识蒸馏到轻量ConvNeXt-tiny上，用少量专家标注的现场数据，解决自动相机下蛾类细粒度识别的域偏移问题；在101种丹麦蛾数据上取得与大模型相近准确度、但算力需求大幅降低。


<details>
  <summary>Details</summary>
Motivation: 自动相机监测昆虫对研究种群衰退很重要，但现场图像嘈杂、与精修图像存在域偏移，导致细粒度物种识别困难；现有高性能基础模型计算开销大，不便部署到边缘设备。

Method: 采用知识蒸馏框架：以BioCLIP2作为教师模型，在少量专家标注的现场数据上进行蒸馏，将其表征/软标签迁移到学生网络ConvNeXt-tiny；设计轻量分类管线，兼顾准确率与推理成本；在AMI自动相机采集的101种丹麦蛾数据集上实验，并与其他方法对比。

Result: BioCLIP2在该任务上显著优于其他基线；蒸馏后的ConvNeXt-tiny在准确率上接近教师模型，同时计算成本显著降低；对实际部署（低资源环境）更友好。

Conclusion: 通过将强大的生物视觉-语言基础模型知识压缩到轻量网络，可有效弥合精修与野外图像的域差距，实现高效、可部署的细粒度昆虫监测；为构建实用化的自动监测系统提供了方法与经验。

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


### [89] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: N/A


<details>
  <summary>Details</summary>
Motivation: N/A

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>
