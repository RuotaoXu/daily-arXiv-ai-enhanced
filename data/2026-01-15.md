<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 82]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Reading or Reasoning? Format Decoupled Reinforcement Learning for Document OCR](https://arxiv.org/abs/2601.08834)
*Yufeng Zhong,Lei Chen,Zhixiong Zeng,Xuanle Zhao,Deyang Jiang,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: 提出FD-RL，通过基于熵的筛选与格式解耦奖励提升OCR在公式/表格等格式敏感文档上的读取与推理，OmniDocBench均分90.41并做了详尽消融。


<details>
  <summary>Details</summary>
Motivation: 尽管OCR常被视为感知任务，但在公式、表格等格式化文本上，即便先进模型的输出熵显著高于普通文本，显示其不确定性和推理路径不足；需要利用这种高熵信号做有针对性的优化。

Method: 1) 统计输出熵，利用熵驱动的数据筛选聚焦“格式密集”样本；2) 针对不同格式类型（公式、表格等）设计“格式解耦”奖励，进行格式级验证而非逐token记忆；3) 以强化学习范式优化OCR模型，鼓励在多样阅读路径上进行推理。

Result: 在OmniDocBench上取得90.41的平均分，刷新端到端模型最好成绩；并通过对数据、训练、筛选和奖励策略的全面消融验证各组件有效性。

Conclusion: 格式敏感文档导致的高熵可作为优化信号；通过熵筛选与格式解耦奖励的RL可显著降低不确定性并提升OCR性能，同时具备良好的可解释与可验证性。

Abstract: Reading text from images or scanned documents via OCR models has been a longstanding focus of researchers. Intuitively, text reading is perceived as a straightforward perceptual task, and existing work primarily focuses on constructing enriched data engineering to enhance SFT capabilities. In this work, we observe that even advanced OCR models exhibit significantly higher entropy in formatted text (\emph{e.g.}, formula, table, etc.) compared to plain text, often by an order of magnitude. These statistical patterns reveal that advanced OCR models struggle with high output uncertainty when dealing with format sensitive document, suggesting that reasoning over diverse reading pathways may improve OCR performance. To address this, we propose format decoupled reinforcement learning (FD-RL), which leverages high-entropy patterns for targeted optimization. Our approach employs entropy-based data filtration strategy to identify format-intensive instances, and adopt format decoupled rewards tailored to different format types, enabling format-level validation rather than token-level memorization. FD-RL achieves an average score of 90.41 on OmniDocBench, setting a new record for end-to-end models on this highly popular benchmark. More importantly, we conduct comprehensive ablation studies over data, training, filtering, and rewarding strategies, thoroughly validating their effectiveness.

</details>


### [2] [Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models](https://arxiv.org/abs/2601.08860)
*Tarannum Mithila*

Main category: cs.CV

TL;DR: 研究揭示旋转等输入变换会导致多模态模型（VLM与生成图像模型）鲁棒性下降与偏见扩散，并提出结合数据增强、表示对齐和模型正则化的旋转鲁棒缓解策略，显著提升稳健性并减少偏见放大而不牺牲总体性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM与生成模型在多模态任务上表现突出，但其在输入变换（特别是图像旋转）和分布移位下的鲁棒性与公平性问题研究不足。实际应用中图像可能任意方向出现，且人口统计学属性相关偏见可能在变换后被放大，因而需要系统性评估并提出可行缓解方案。

Method: 1) 系统评估：在多数据集上对最先进VLM与生成模型施加旋转与分布移位，分析预测准确度、置信度校准、与群体偏见指标的变化。2) 诊断分析：量化旋转扰动对表示与决策边界的影响，刻画偏见模式如何随角度变化传播。3) 缓解策略：提出结合(a)旋转数据增强，(b)表示对齐（如角度不变特征学习/对比式对齐），(c)模型级正则（如不变性约束、校准损失）的方案。4) 实验验证：在多数据集上比较基线与所提方法的鲁棒性与公平性。

Result: 在多个数据集与任务上，所提方法在不降低总体任务性能的情况下，显著提升对旋转与分布移位的稳健性，改善置信度校准，并减少人口统计学偏见的放大。

Conclusion: 当前多模态系统对输入旋转与分布变化较脆弱，且存在偏见传播问题。联合使用数据增强、表示对齐与模型正则化可有效缓解，在保持性能的同时提高可靠性与公平性，为构建更稳健公正的多模态AI提供实践路径。

Abstract: Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.

</details>


### [3] [R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images](https://arxiv.org/abs/2601.08867)
*Qingyu Liu,Zhongjie Ba,Jianmin Guo,Qiu Wang,Zhibo Wang,Jie Shi,Kui Ren*

Main category: cs.CV

TL;DR: R^2BD提出统一重建+单步残差偏置检测，实现更快更广泛的AIGC检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的AIGC检测依赖扩散模型逆推与多步重建，效率低，且难泛化到GAN等不同生成范式。需要一种既高效又能跨范式泛化的检测方法。

Method: 设计两部分：(1) G-LDM统一重建模型，模拟VAE、GAN、扩散等生成行为，实现跨范式输入的统一重建；(2) 残差偏置计算模块，仅用一次前向推理计算重建残差的偏置特征，从而区分真伪，无需20+步逆推。

Result: 在10个公开数据集基准上，较现有重建法提速超过22倍，同时检测精度更优；跨数据集评测平均超SOTA 13.87%，体现出高效与强泛化。

Conclusion: R^2BD通过统一重建与单步残差偏置实现高效且跨范式的AIGC图像检测，兼具速度与准确性，优于现有重建型方法，并具备良好分布外泛化能力。

Abstract: Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs.
  In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps.
  Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.

</details>


### [4] [Residual Cross-Modal Fusion Networks for Audio-Visual Navigation](https://arxiv.org/abs/2601.08868)
*Yi Wang,Yinfeng Yu,Bin Ren*

Main category: cs.CV

TL;DR: 提出一种用于音视导航的跨模态残差融合网络（CRFN），通过双向残差交互稳健融合音频与视觉，显著提升跨域泛化与性能。


<details>
  <summary>Details</summary>
Motivation: 多模态融合常出现单模态主导或信息退化，尤其在跨域（不同数据集/环境）时更明显；需要一种既能细粒度对齐又不互相干扰的融合机制。

Method: 设计CRFN：在音频与视觉两条流之间引入双向残差连接，显式建模跨模态互补与对齐；与简单拼接/注意力门控不同，保持各自表征独立同时通过残差通路交互；并加入稳定化技术（用于训练收敛与鲁棒性）。在Replica与Matterport3D上训练/评测。

Result: 在Replica与Matterport3D上优于SOTA融合基线，并表现出更强跨域泛化能力。实验还发现不同数据集上智能体对两种模态的依赖程度不同。

Conclusion: CRFN通过残差式双向跨模态交互实现稳健且细粒度的音视融合，缓解单模态支配与信息退化问题，带来性能与跨域泛化的提升；同时揭示了数据集依赖的模态偏好现象，为理解具身智能体的跨模态协作提供新视角。

Abstract: Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.

</details>


### [5] [ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection](https://arxiv.org/abs/2601.08873)
*Hema Hariharan Samson*

Main category: cs.CV

TL;DR: ForensicFormer 是一个分层多尺度的跨注意力 Transformer 框架，联合低层伪影、中层边界与高层语义三类线索，实现跨域图像伪造检测与定位；在7个异构数据集上平均准确率86.8%，显著优于现有通用检测器，并具备更强的JPEG鲁棒性与像素级定位能力。


<details>
  <summary>Details</summary>
Motivation: 传统取证方法在AI生成图像与复杂编辑的跨域检测上失效，现有单一范式模型在分布外测试上准确率不足（<75%），缺乏对未知操纵方式的实用鲁棒性与可解释性。

Method: 提出分层多尺度的ForensicFormer：以跨注意力Transformer统一三层取证信号——低层（噪声/压缩等伪影）、中层（篡改边界与不连续性）、高层（语义一致性与上下文推理）；支持分类与像素级定位；通过层级组件与消融验证其增益。

Result: 在7个多样测试集上平均准确率86.8%，跨传统篡改、GAN与扩散生成均优于SOTA通用检测器；JPEG Q=70下准确率83%（基线66%）；像素级定位F1=0.76；每个层级组件带来4-10%的精度提升并提供可解释取证特征。

Conclusion: ForensicFormer将经典图像取证与深度学习融合，提供对未知操纵的实用跨域检测与定位方案，具备强鲁棒性、可解释性与部署价值。

Abstract: The proliferation of AI-generated imagery and sophisticated editing tools has rendered traditional forensic methods ineffective for cross-domain forgery detection. We present ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Unlike prior single-paradigm approaches, which achieve <75% accuracy on out-of-distribution datasets, our method maintains 86.8% average accuracy across seven diverse test sets, spanning traditional manipulations, GAN-generated images, and diffusion model outputs - a significant improvement over state-of-the-art universal detectors. We demonstrate superior robustness to JPEG compression (83% accuracy at Q=70 vs. 66% for baselines) and provide pixel-level forgery localization with a 0.76 F1-score. Extensive ablation studies validate that each hierarchical component contributes 4-10% accuracy improvement, and qualitative analysis reveals interpretable forensic features aligned with human expert reasoning. Our work bridges classical image forensics and modern deep learning, offering a practical solution for real-world deployment where manipulation techniques are unknown a priori.

</details>


### [6] [Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement](https://arxiv.org/abs/2601.08875)
*Jiahao Qin,Yiwen Wang*

Main category: cs.CV

TL;DR: 问题：跨域图像配准因亮度一致性假设失效而变得病态。方法：提出SAR-Net，将图像分解为域不变“场景”与域特定“外观”潜码，通过重渲染实现配准。理论：给出两条命题保证在共享潜空间中可一致对齐与几何对应。实验：在双向扫描显微成像上显著优于最强基线，实时77 fps。消融：场景一致性损失与域对齐损失缺一不可。


<details>
  <summary>Details</summary>
Motivation: 当源/目标图像存在系统性强度差异（域偏移）时，传统依赖亮度不变的配准方法失效，导致对应估计不适定。需要一种能够跨域对齐、又能适应外观差异的统一框架。

Method: 提出SAR-Net进行“场景-外观”解耦：将观测图像表示为域不变的场景表示与域特异的外观编码之合成，通过在共享潜空间中重渲染而非直接强度匹配来完成配准。提出场景一致性损失与域对齐损失，并给出命题：在一定条件下该分解可实现一致跨域对齐（命题1）；场景一致性损失为几何对应提供充分条件（命题2）。

Result: 在具有域偏移与几何畸变耦合的双向扫描显微数据上，SAR-Net达成SSIM 0.885、NCC 0.979，相比最强基线提升约3.1倍，且实时77 fps。消融显示：去掉场景一致性损失会使SSIM下降约90%，去掉域对齐损失会使潜空间对齐误差增大223倍。

Conclusion: 通过场景-外观解耦与重渲染思路，SAR-Net在理论与实践上均实现了稳健的跨域配准，兼具高精度与实时性；两类损失共同作用是关键。

Abstract: Image registration under domain shift remains a fundamental challenge in computer vision and medical imaging: when source and target images exhibit systematic intensity differences, the brightness constancy assumption underlying conventional registration methods is violated, rendering correspondence estimation ill-posed. We propose SAR-Net, a unified framework that addresses this challenge through principled scene-appearance disentanglement. Our key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. We establish theoretical conditions under which this decomposition enables consistent cross-domain alignment (Proposition 1) and prove that our scene consistency loss provides a sufficient condition for geometric correspondence in the shared latent space (Proposition 2). Empirically, we validate SAR-Net on bidirectional scanning microscopy, where coupled domain shift and geometric distortion create a challenging real-world testbed. Our method achieves 0.885 SSIM and 0.979 NCC, representing 3.1x improvement over the strongest baseline, while maintaining real-time performance (77 fps). Ablation studies confirm that both scene consistency and domain alignment losses are necessary: removing either degrades performance by 90% SSIM or causes 223x increase in latent alignment error, respectively. Code and data are available at https://github.com/D-ST-Sword/SAR-NET.

</details>


### [7] [The Semantic Lifecycle in Embodied AI: Acquisition, Representation and Storage via Foundation Models](https://arxiv.org/abs/2601.08876)
*Shuai Chen,Hao Chen,Yuanchen Bei,Tianyang Zhao,Zhibo Zhou,Feiran Huang*

Main category: cs.CV

TL;DR: 该综述提出“语义生命周期”框架，借助基础模型贯穿获取—表征—存储三阶段，统一审视具身智能中的语义知识流动与维护，并讨论挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 具身智能中的语义信息来源多、阶段多，传统将语义处理割裂为独立模块方法在复杂开放环境下难以稳定支撑感知—行动闭环。基础模型展现跨域泛化和丰富语义先验，有望提供更通用、鲁棒的语义处理能力，需要统一视角系统梳理其在具身语义中的作用。

Method: 提出“语义生命周期”统一框架，从整体视角描述基础模型驱动下语义知识在具身智能中的连续流动与维护；按获取（acquisition）、表征（representation）、存储（storage）三个关键阶段，梳理、分析并比较近期研究进展。

Result: 形成系统化综述：用生命周期框架对现有工作进行分类与对比，揭示基础模型在不同阶段赋能的方式与优势／不足，并抽象出具身语义处理的共性流程与要素。

Conclusion: 基础模型正重塑具身语义研究范式。语义生命周期提供统一视角以指导方法设计与评估，但仍面临鲁棒性、泛化、长期记忆与更新、跨模态对齐、实时性与安全等挑战；文章据此提出若干未来研究方向。

Abstract: Semantic information in embodied AI is inherently multi-source and multi-stage, making it challenging to fully leverage for achieving stable perception-to-action loops in real-world environments. Early studies have combined manual engineering with deep neural networks, achieving notable progress in specific semantic-related embodied tasks. However, as embodied agents encounter increasingly complex environments and open-ended tasks, the demand for more generalizable and robust semantic processing capabilities has become imperative. Recent advances in foundation models (FMs) address this challenge through their cross-domain generalization abilities and rich semantic priors, reshaping the landscape of embodied AI research. In this survey, we propose the Semantic Lifecycle as a unified framework to characterize the evolution of semantic knowledge within embodied AI driven by foundation models. Departing from traditional paradigms that treat semantic processing as isolated modules or disjoint tasks, our framework offers a holistic perspective that captures the continuous flow and maintenance of semantic knowledge. Guided by this embodied semantic lifecycle, we further analyze and compare recent advances across three key stages: acquisition, representation, and storage. Finally, we summarize existing challenges and outline promising directions for future research.

</details>


### [8] [TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts](https://arxiv.org/abs/2601.08881)
*Yu Xu,Hongbin Yan,Juan Cao,Yiji Cheng,Tiankai Hang,Runze He,Zijin Yin,Shiyi Zhang,Yuxin Zhang,Jintao Li,Chunyu Wang,Qinglin Lu,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 提出在统一图像生成与编辑的MoE扩散Transformer中，用“带任务语义的路由”缓解任务干扰，通过层级任务语义标注与对齐正则，使专家形成明确、语义相关的专长，优于稠密基线。


<details>
  <summary>Details</summary>
Motivation: 统一生成与编辑模型在共享参数空间中需同时满足相互冲突的目标（如局部编辑与主体生成），导致任务干扰。Sparse MoE虽可分解容量，但现有门控仅基于局部特征、缺乏对全局任务意图的感知，难以促成专家有意义的专化，因此需要将任务语义注入路由以解决干扰。

Method: 1) 层级任务语义标注：构建结构化任务描述子（如作用范围scope、任务类型type、保留程度preservation）。2) 预测式对齐正则：对门控网络加入正则，将内部路由决策与高层任务语义对齐，把门控从任务无关的执行器演化为“调度中心”。整体集成于扩散Transformer的MoE路由中。

Result: 在统一图像生成与编辑基准上，相较稠密基线在保真度与质量上取得更好表现；路由分析显示专家自发形成清晰、且与任务语义相关的专长分工，任务干扰显著缓解。

Conclusion: 将任务语义显式注入MoE路由并通过对齐正则进行约束，可有效缓解统一图像生成与编辑中的任务干扰，提升质量与保真度，并促成专家的语义化专化。

Abstract: Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.

</details>


### [9] [Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained Optimization](https://arxiv.org/abs/2601.08882)
*Thomas Snyder,H. Lexie Yang,Stefan Schnake,Steffen Schotthöfer*

Main category: cs.CV

TL;DR: 利用流形约束优化（DLRT）在迁移学习中对基于ViT的地理空间基础模型进行结构化低维参数化压缩，在边缘设备上实现高压缩比且几乎不损精度，并优于常见低秩方法（如LoRA）。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型参数量大、在边缘设备部署受限；常规压缩常导致显著精度损失，限制实用化。需要一种在保证下游任务性能的同时显著减参的策略。

Method: 在迁移学习阶段采用DLRT流形约束优化，对参数施加结构化低维表示，使压缩方向与下游任务目标对齐；与LoRA等现成低秩方法对比评估。

Result: 在多个地理空间基准上实现显著参数缩减，同时仅带来极小精度损失；整体性能优于LoRA等低秩微调方案。

Conclusion: DLRT驱动的结构化低维参数化能在不牺牲下游精度的前提下大幅压缩ViT类地理空间基础模型，使其可在资源受限的边缘设备上高效部署。

Abstract: Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. However, their large parameter counts and the accuracy loss often induced by compression limit practical adoption. In this work, we leverage manifold-constrained optimization framework DLRT to compress large vision transformer-based geospatial foundation models during transfer learning. By enforcing structured low-dimensional parameterizations aligned with downstream objectives, this approach achieves strong compression while preserving task-specific accuracy. We show that the method outperforms of-the-shelf low-rank methods as LoRA. Experiments on diverse geospatial benchmarks confirm substantial parameter reduction with minimal accuracy loss, enabling high-performing, on-device geospatial models.

</details>


### [10] [Adaptive few-shot learning for robust part quality classification in two-photon lithography](https://arxiv.org/abs/2601.08885)
*Sixian Jia,Ruo-Syuan Mei,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出一个适用于两光子光刻质检的自适应CV框架，统一骨干模型下集成新颖性检测、增量学习与少样本域自适应，实现对新缺陷、少样本更新与跨几何域迁移的全生命周期维护，在严苛设置下仍取得接近或超过95%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统质检CV模型静态且脆弱：无法发现未知缺陷、难以用少量新数据高效更新、对新零件几何或成像域变化适应差。TPL制造环境动态变化明显，迫切需要能持续学习与跨域适应的质量模型。

Method: 在同一尺度鲁棒的骨干网络上，集成三项方法：1) 基于LDA与统计假设检验的新颖性检测，识别未见类批次；2) 两阶段、带复习（rehearsal）的少样本增量学习以引入新类别；3) 少样本DANN进行域对抗式自适应，处理源（半球）到目标（立方体）结构的域迁移。

Result: 在TPL数据集（源：半球；目标：立方体；三质量等级）上：新颖性检测识别新类批次的准确率99–100%；增量学习用K=20样本集成新类达92%准确率；域自适应在仅K=5样本下目标域准确率96.19%。

Conclusion: 该框架在动态生产场景中实现对质检模型的全生命周期维护，具备数据效率高、对未知类敏感、可快速增量更新并能跨结构域迁移，适合在实际TPL与更广泛AM质检中部署。

Abstract: Two-photon lithography (TPL) is an advanced additive manufacturing (AM) technique for fabricating high-precision micro-structures. While computer vision (CV) is proofed for automated quality control, existing models are often static, rendering them ineffective in dynamic manufacturing environments. These models typically cannot detect new, unseen defect classes, be efficiently updated from scarce data, or adapt to new part geometries. To address this gap, this paper presents an adaptive CV framework for the entire life-cycle of quality model maintenance. The proposed framework is built upon a same, scale-robust backbone model and integrates three key methodologies: (1) a statistical hypothesis testing framework based on Linear Discriminant Analysis (LDA) for novelty detection, (2) a two-stage, rehearsal-based strategy for few-shot incremental learning, and (3) a few-shot Domain-Adversarial Neural Network (DANN) for few-shot domain adaptation. The framework was evaluated on a TPL dataset featuring hemisphere as source domain and cube as target domain structures, with each domain categorized into good, minor damaged, and damaged quality classes. The hypothesis testing method successfully identified new class batches with 99-100% accuracy. The incremental learning method integrated a new class to 92% accuracy using only K=20 samples. The domain adaptation model bridged the severe domain gap, achieving 96.19% accuracy on the target domain using only K=5 shots. These results demonstrate a robust and data-efficient solution for deploying and maintaining CV models in evolving production scenarios.

</details>


### [11] [Variance-Penalized MC-Dropout as a Learned Smoothing Prior for Brain Tumour Segmentation](https://arxiv.org/abs/2601.08956)
*Satyaki Roy Chowdhury,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: 提出UAMSA-UNet：把不确定性建模（MC Dropout）、多尺度特征融合与注意力结合，并用平滑正则损失减少肿瘤浸润区的边界噪声；在BraTS2023/2024较U-Net与最佳基线显著提升Dice/IoU，同时较U-Net++大幅降低FLOPs。


<details>
  <summary>Details</summary>
Motivation: 传统CNN/U-Net在肿瘤浸润区域易产生锯齿/噪声边界，且难同时兼顾细节与全局上下文；需要一种既能量化不确定性、又能多尺度聚合且在计算上高效的方法，得到更平滑、连贯的分割。

Method: 构建不确定性感知的多尺度注意力Bayesian U-Net（UAMSA-UNet）：在推理/训练中使用Monte Carlo Dropout形成随机前向，学习数据驱动的平滑先验；网络融合多尺度特征与注意力图以兼顾细节与全局；损失函数在二元交叉熵上加入跨随机前向的预测方差惩罚，抑制随即波动，使掩膜空间一致。

Result: 在BraTS2023上较U-Net的Dice最高+3.3%，mIoU最高+2.7%；在BraTS2024上较最优基线Dice最高+4.5%，mIoU最高+4.0%；与U-Net++相比FLOPs降低42.5%且精度更高。

Conclusion: 将多尺度注意力与基于MC Dropout的平滑先验结合，可同时提升分割质量与计算效率；该框架为后续与Transformer模块集成提供灵活基础，预期进一步提升分割表现。

Abstract: Brain tumor segmentation is essential for diagnosis and treatment planning, yet many CNN and U-Net based approaches produce noisy boundaries in regions of tumor infiltration. We introduce UAMSA-UNet, an Uncertainty-Aware Multi-Scale Attention-based Bayesian U-Net that in- stead leverages Monte Carlo Dropout to learn a data-driven smoothing prior over its predictions, while fusing multi-scale features and attention maps to capture both fine details and global context. Our smoothing-regularized loss augments binary cross-entropy with a variance penalty across stochas- tic forward passes, discouraging spurious fluctuations and yielding spatially coherent masks. On BraTS2023, UAMSA- UNet improves Dice Similarity Coefficient by up to 3.3% and mean IoU by up to 2.7% over U-Net; on BraTS2024, it delivers up to 4.5% Dice and 4.0% IoU gains over the best baseline. Remarkably, it also reduces FLOPs by 42.5% rel- ative to U-Net++ while maintaining higher accuracy. These results demonstrate that, by combining multi-scale attention with a learned smoothing prior, UAMSA-UNet achieves both better segmentation quality and computational efficiency, and provides a flexible foundation for future integration with transformer-based modules for further enhanced segmenta- tion results.

</details>


### [12] [Thermo-LIO: A Novel Multi-Sensor Integrated System for Structural Health Monitoring](https://arxiv.org/abs/2601.08977)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出Thermo-LIO：将热成像与高分辨率LiDAR及LIO紧密融合，实现对大型土木结构的高覆盖、高精度热异常与缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 二维热成像在复杂几何、不可达区域与次表层缺陷评估方面受限，难以满足大型基础设施SHM对精度、覆盖和实时性的需求。

Method: 1) 提出热像-激光雷达多模态融合：标定与时间同步，建立空间上准确的温度分布重建；2) 将该融合与LiDAR-Inertial Odometry耦合，实现移动扫描、全覆盖建图与跨巡检周期对比；3) 在桥梁与大厅开展实验验证与案例研究。

Result: 系统可在大尺度场景中生成带温度属性的高精度三维模型，较传统方法更准确地定位细微热异常与结构缺陷，支持实时处理并扩大检测范围。

Conclusion: 多模态传感融合（热像+LiDAR+LIO）显著提升SHM诊断精度、效率与覆盖度，适用于大型土木基础设施的周期性检测与健康监测。

Abstract: Traditional two-dimensional thermography, despite being non-invasive and useful for defect detection in the construction field, is limited in effectively assessing complex geometries, inaccessible areas, and subsurface defects. This paper introduces Thermo-LIO, a novel multi-sensor system that can enhance Structural Health Monitoring (SHM) by fusing thermal imaging with high-resolution LiDAR. To achieve this, the study first develops a multimodal fusion method combining thermal imaging and LiDAR, enabling precise calibration and synchronization of multimodal data streams to create accurate representations of temperature distributions in buildings. Second, it integrates this fusion approach with LiDAR-Inertial Odometry (LIO), enabling full coverage of large-scale structures and allowing for detailed monitoring of temperature variations and defect detection across inspection cycles. Experimental validations, including case studies on a bridge and a hall building, demonstrate that Thermo-LIO can detect detailed thermal anomalies and structural defects more accurately than traditional methods. The system enhances diagnostic precision, enables real-time processing, and expands inspection coverage, highlighting the crucial role of multimodal sensor integration in advancing SHM methodologies for large-scale civil infrastructure.

</details>


### [13] [SAM-pose2seg: Pose-Guided Human Instance Segmentation in Crowds](https://arxiv.org/abs/2601.08982)
*Constantin Kolomiiets,Miroslav Purkrabek,Jiri Matas*

Main category: cs.CV

TL;DR: 提出PoseMaskRefine，将姿态关键点融入SAM 2.1的迭代纠错流程，实现在遮挡下更稳健的人体分割；推理时仅用可见度最高的三关键点即可得到高精度掩码，并保持原模型泛化。


<details>
  <summary>Details</summary>
Motivation: SAM在人体分割上表现强，但在遮挡场景下关键部位不可见会导致掩码不稳、误分衣物/肢体等问题；希望在不破坏SAM泛化能力的前提下，引入姿态信息提升鲁棒性与精度，同时简化推理提示。

Method: 对SAM 2.1做最小化编码器改动，提出PoseMaskRefine微调策略：在SAM原有的迭代纠错框架中，注入高可见度的姿态关键点作为引导信号；训练阶段联合掩码与可见度监督；推理时仅选择可见度最高的三处关键点（极端时可单点）作为提示以生成或修正掩码。

Result: 在多个人体分割数据集上，相较原始SAM与常见基线，鲁棒性与精度显著提升，尤其在遮挡、缺失部位、衣物误分类等情况下表现更稳；对提示错误的敏感性降低；同时保留了SAM的广泛泛化能力。

Conclusion: 通过姿态引导的微调与最小改动，SAM可实现对遮挡感知的人体分割；简化的三关键点提示在推理中高效有效，且从单关键点也能产生准确掩码；方法兼顾性能与泛化，代码与预训练模型将开源。

Abstract: Segment Anything (SAM) provides an unprecedented foundation for human segmentation, but may struggle under occlusion, where keypoints may be partially or fully invisible. We adapt SAM 2.1 for pose-guided segmentation with minimal encoder modifications, retaining its strong generalization. Using a fine-tuning strategy called PoseMaskRefine, we incorporate pose keypoints with high visibility into the iterative correction process originally employed by SAM, yielding improved robustness and accuracy across multiple datasets. During inference, we simplify prompting by selecting only the three keypoints with the highest visibility. This strategy reduces sensitivity to common errors, such as missing body parts or misclassified clothing, and allows accurate mask prediction from as few as a single keypoint. Our results demonstrate that pose-guided fine-tuning of SAM enables effective, occlusion-aware human segmentation while preserving the generalization capabilities of the original model. The code and pretrained models will be available at https://mirapurkrabek.github.io/BBox-MaskPose.

</details>


### [14] [Instance camera focus prediction for crystal agglomeration classification](https://arxiv.org/abs/2601.09004)
*Xiaoyu Ji,Chenhao Zhang,Tyler James Downard,Zoltan Nagy,Ali Shakouri,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出一种结合实例对焦预测与实例分割的方法，解决显微图像中晶体因重叠与景深限制造成的“伪团聚”误判，显著提升团聚分类与分割精度。


<details>
  <summary>Details</summary>
Motivation: 二维显微图像景深浅、层间重叠导致不同深度的晶体在图像上看似连接，传统清晰度度量与分割方法难以区分真团聚与非共层重叠，从而影响晶体团聚分析的准确性。

Method: 1) 设计实例级相机对焦预测网络，输出两类对焦水平（清晰/虚焦），以更符合人眼观感的方式量化焦点；2) 将实例分割与对焦预测结果融合，对每个晶体实例赋予焦点层信息，据此进行团聚判定（同层相连为真团聚，异层为非团聚）。

Result: 在过氯酸铵晶体与糖晶体数据集上，相较基线方法，所提方法取得更高的团聚分类准确率与分割精度（文中未给出具体数值）。

Conclusion: 引入实例级对焦信息能有效缓解二维显微成像的层叠歧义，提升晶体团聚识别与分割的可靠性，可推广至其他受景深限制的显微分析任务。

Abstract: Agglomeration refers to the process of crystal clustering due to interparticle forces. Crystal agglomeration analysis from microscopic images is challenging due to the inherent limitations of two-dimensional imaging. Overlapping crystals may appear connected even when located at different depth layers. Because optical microscopes have a shallow depth of field, crystals that are in-focus and out-of-focus in the same image typically reside on different depth layers and do not constitute true agglomeration. To address this, we first quantified camera focus with an instance camera focus prediction network to predict 2 class focus level that aligns better with visual observations than traditional image processing focus measures. Then an instance segmentation model is combined with the predicted focus level for agglomeration classification. Our proposed method has a higher agglomeration classification and segmentation accuracy than the baseline models on ammonium perchlorate crystal and sugar crystal dataset.

</details>


### [15] [Changes in Visual Attention Patterns for Detection Tasks due to Dependencies on Signal and Background Spatial Frequencies](https://arxiv.org/abs/2601.09008)
*Amar Kavuri,Howard C. Gifford,Mini Das*

Main category: cs.CV

TL;DR: 研究利用模拟乳腺断层摄影(DBT)图像与人类观察者实验，分析目标与背景属性如何影响视觉注意与信号检测；结果显示错误多源于后期决策，目标形态与背景复杂度共同决定可检性，注意在不同空间频率特征上差异化投入。


<details>
  <summary>Details</summary>
Motivation: 医学影像诊断仍存在漏诊/误诊；复杂、异质背景中的小目标识别受多因素影响。希望理解图像与信号属性如何通过视觉注意机制影响检测表现，以指导影像设计、读片流程与CAD开发。

Method: 构建不同密度与结构的数字乳腺体模(Bakic、XCAT)，在投影期随机植入两类病灶(3 mm球形、6 mm放射状棘突，空间频率特性不同)，重建DBT切片；6名观察者执行定位+检测任务，同时记录眼动以提取凝视指标并关联检测表现。

Result: - 检测错误主要源于后期知觉/决策阶段，而非仅早期感觉限制。
- 目标可检性受目标形态(空间频率特征)与背景复杂度的交互影响。
- 在棘突状病灶上出现更长的凝视时间，提示注意随背景与信号空间频率依赖性而差异化分配。

Conclusion: 在复杂医学图像中，视觉注意与检测表现受目标—背景交互和后期决策限制主导。优化读片训练、重建与显示参数，以及基于空间频率与背景建模的CAD/AI提示，可能提升检出率并降低决策性错误。

Abstract: We aim to investigate the impact of image and signal properties on visual attention mechanisms during a signal detection task in digital images. The application of insight yielded from this work spans many areas of digital imaging where signal or pattern recognition is involved in complex heterogenous background. We used simulated tomographic breast images as the platform to investigate this question. While radiologists are highly effective at analyzing medical images to detect and diagnose diseases, misdiagnosis still occurs. We selected digital breast tomosynthesis (DBT) images as a sample medical images with different breast densities and structures using digital breast phantoms (Bakic and XCAT). Two types of lesions (with distinct spatial frequency properties) were randomly inserted in the phantoms during projections to generate abnormal cases. Six human observers participated in observer study designed for a locating and detection of an 3-mm sphere lesion and 6-mm spicule lesion in reconstructed in-plane DBT slices. We collected eye-gaze data to estimate gaze metrics and to examine differences in visual attention mechanisms. We found that detection performance in complex visual environments is strongly constrained by later perceptual stages, with decision failures accounting for the largest proportion of errors. Signal detectability is jointly influenced by both target morphology and background complexity, revealing a critical interaction between local signal features and global anatomical noise. Increased fixation duration on spiculated lesions suggests that visual attention is differentially engaged depending on background and signal spatial frequency dependencies.

</details>


### [16] [Depth-Wise Representation Development Under Blockwise Self-Supervised Learning for Video Vision Transformers](https://arxiv.org/abs/2601.09040)
*Jonas Römer,Timo Dickscheid*

Main category: cs.CV

TL;DR: 论文探讨能否在不使用端到端反向传播的情况下训练掩码视频Transformer：将编码器切分为多个块，每块用局部重建损失独立优化。结果显示训练收敛且表征接近端到端基线，但仍有小差距，源于后期块饱和与块间接口成形。


<details>
  <summary>Details</summary>
Motivation: 端到端反传需要长程信用分配，训练代价与生物不可行性受到质疑；而块式自监督学习（BWSSL）近期在图像上进展显著，但在视频时空建模上的适配与其与端到端在学习动态、层深表征发展上的系统比较仍不足。

Method: 将掩码视频自编码ViT的编码器按深度划分为若干块；对每块施加局部掩码重建损失进行独立优化（无需贯穿全网的全局误差信号）；在不同模型规模与划分粒度下训练；通过线性探针与检索评估整体表征，并以深度可解码性、块间相似性、补丁级诊断等指标分析中间表征与学习动态。

Result: 在多种规模与划分设置下，块式训练稳定收敛；其学得的表征在线性探针与检索上接近匹配的端到端基线。分析显示：早期块更早暴露高层结构；后期块表征趋于几何保持、出现饱和；还观察到token层级的分布与混合方式变化，提示早期更强的混合，而这些变化不易被汇总指标捕获。

Conclusion: 块式掩码视频建模无需端到端反传亦可有效学习，但与端到端仍有差距；差距主要由后期块饱和与块间接口成形引起。未来可通过改进块间接口、延缓后期块饱和、设计跨块协调或混合训练策略来缩小差距。

Abstract: End-to-end backpropagation couples all layers through a global error signal, enabling coordinated learning but requiring long-range credit assignment. Motivated by recent progress in blockwise self-supervised learning (BWSSL), we ask whether masked video transformers can be trained without end-to-end backpropagation. Applying BWSSL to masked video modeling remains relatively underexplored and must handle spatiotemporal context and long-range temporal structure. More broadly, analyses that compare BWSSL and end-to-end training in terms of learning dynamics and depth-wise representation development remain sparse. We apply blockwise learning to a masked autoencoding video vision transformer by partitioning the encoder into blocks, each of which is optimized with a local masked reconstruction loss. Across model sizes and partition granularities, training converges and yields representations close to matched end-to-end baselines under linear-probe and retrieval proxies. In order to compare intermediate representations, we analyze depth-wise decodability, inter-block similarity, and patch-level diagnostics. Blockwise training exposes higher-level structure earlier, while later blocks saturate and operate in a more geometry-preserving regime. It can also induce token-level shifts consistent with stronger early mixing that pooled metrics can miss. These findings point to late-block saturation and interface formation as contributors to the remaining gap.

</details>


### [17] [Exploring Reliable Spatiotemporal Dependencies for Efficient Visual Tracking](https://arxiv.org/abs/2601.09078)
*Junze Shi,Yang Yu,Jian Shi,Haibo Luo*

Main category: cs.CV

TL;DR: STDTrack 引入稠密视频采样与时空令牌维护机制，把可靠的时空依赖融入轻量级Transformer跟踪器，在保持实时性的同时显著提升精度，接近甚至追平部分非实时高性能方法。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级Transformer跟踪器训练时普遍采用稀疏采样（每序列仅1模板+1搜索），难以充分挖掘视频中的时空信息，导致与高性能非实时跟踪器存在性能差距。

Method: 1) 稠密视频采样：在训练中引入多帧以最大化时空信息利用；2) 时序传播的时空令牌（spatiotemporal token）引导逐帧特征提取；3) 多帧信息融合模块（MFIFM）利用历史上下文增强当前依赖；4) 构建时空令牌维护器（STM），结合基于质量的更新机制保证存储信息可靠；5) 多尺度预测头，自适应不同目标尺度变化。

Result: 在六个基准上达成SOTA表现；在GOT-10k上，速度达GPU 192 FPS、CPU 41 FPS，精度可与MixFormer等非实时高性能方法比肩。

Conclusion: 通过将可靠的时空依赖与稠密训练策略引入轻量级跟踪框架，STDTrack在保持极高速度的同时显著提升精度，缩小甚至弥合轻量级与高性能非实时跟踪器之间的差距。

Abstract: Recent advances in transformer-based lightweight object tracking have established new standards across benchmarks, leveraging the global receptive field and powerful feature extraction capabilities of attention mechanisms. Despite these achievements, existing methods universally employ sparse sampling during training--utilizing only one template and one search image per sequence--which fails to comprehensively explore spatiotemporal information in videos. This limitation constrains performance and cause the gap between lightweight and high-performance trackers. To bridge this divide while maintaining real-time efficiency, we propose STDTrack, a framework that pioneers the integration of reliable spatiotemporal dependencies into lightweight trackers. Our approach implements dense video sampling to maximize spatiotemporal information utilization. We introduce a temporally propagating spatiotemporal token to guide per-frame feature extraction. To ensure comprehensive target state representation, we disign the Multi-frame Information Fusion Module (MFIFM), which augments current dependencies using historical context. The MFIFM operates on features stored in our constructed Spatiotemporal Token Maintainer (STM), where a quality-based update mechanism ensures information reliability. Considering the scale variation among tracking targets, we develop a multi-scale prediction head to dynamically adapt to objects of different sizes. Extensive experiments demonstrate state-of-the-art results across six benchmarks. Notably, on GOT-10k, STDTrack rivals certain high-performance non-real-time trackers (e.g., MixFormer) while operating at 192 FPS(GPU) and 41 FPS(CPU).

</details>


### [18] [Vision Foundation Models for Domain Generalisable Cross-View Localisation in Planetary Ground-Aerial Robotic Teams](https://arxiv.org/abs/2601.09107)
*Lachlan Holden,Feras Dayoub,Alberto Candela,David Harvey,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 提出一种利用跨视角双编码器网络与语义分割、合成数据和粒子滤波，实现行星探测车用地面单目小视场图像在空中地图中精准定位的方法，并发布真实与合成跨视角数据集。


<details>
  <summary>Details</summary>
Motivation: 未来行星任务需要更高自主性；地空协同机器人将更常见，但缺乏带精确位姿标注的真实训练数据，且地面视角与空中视角存在显著域间差异，给基于机器学习的定位带来挑战。

Method: 1) 设计跨视角定位的双编码器深度网络，将地面单目RGB与空中地图分别编码至共享表征空间；2) 利用视觉基础模型做语义分割，提升跨域不变性；3) 构建大规模合成数据以弥合真实数据稀缺与域差距；4) 发布在类行星模拟场地采集的真实跨视角数据集及配套高容量合成对；5) 结合粒子滤波进行状态估计，利用图像序列将单帧匹配稳健地融合为轨迹级定位。

Result: 在简单和复杂轨迹上，基于该跨视角网络与粒子滤波的管线，能从连续地面视角图像中在空中地图上获得准确位置估计，显示出合成数据与语义分割对跨域泛化的有效性。

Conclusion: 跨视角双编码器+语义分割+合成数据+粒子滤波的组合可在真实场景中实现可靠的地-空跨视角定位，并通过提供新的真实与合成数据集为后续研究奠定基础。

Abstract: Accurate localisation in planetary robotics enables the advanced autonomy required to support the increased scale and scope of future missions. The successes of the Ingenuity helicopter and multiple planetary orbiters lay the groundwork for future missions that use ground-aerial robotic teams. In this paper, we consider rovers using machine learning to localise themselves in a local aerial map using limited field-of-view monocular ground-view RGB images as input. A key consideration for machine learning methods is that real space data with ground-truth position labels suitable for training is scarce. In this work, we propose a novel method of localising rovers in an aerial map using cross-view-localising dual-encoder deep neural networks. We leverage semantic segmentation with vision foundation models and high volume synthetic data to bridge the domain gap to real images. We also contribute a new cross-view dataset of real-world rover trajectories with corresponding ground-truth localisation data captured in a planetary analogue facility, plus a high volume dataset of analogous synthetic image pairs. Using particle filters for state estimation with the cross-view networks allows accurate position estimation over simple and complex trajectories based on sequences of ground-view images.

</details>


### [19] [Small but Mighty: Dynamic Wavelet Expert-Guided Fine-Tuning of Large-Scale Models for Optical Remote Sensing Object Segmentation](https://arxiv.org/abs/2601.09108)
*Yanguang Sun,Chao Wang,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 提出WEFT：一种基于小量可训练参数的动态小波专家引导微调范式，将大规模基础模型高效适配于光学遥感影像分割；在3个ORSI数据集和跨领域场景上超过21个SOTA。


<details>
  <summary>Details</summary>
Motivation: 大模型能显著提升遥感分割，但全参微调带来显存与算力开销过高，限制了其在ORSI中的探索；需一种参数高效、训练友好的适配方法，同时保持或超越SOTA性能。

Method: 1) 任务特定小波专家提取器：从不同视角建模“波段-尺度-方向”小波专家并动态调控其输出，生成富含任务信息的可训练特征；2) 专家引导条件适配器：向冻结的大模型特征注入上述可训练特征以增强细粒度感知，并进行双向迭代更新（可训练特征与冻结特征信息交互），以实现高效微调。总体上以极少参数完成对大模型的条件适配与特征重整。

Result: 在三个ORSI分割数据集上超过21个SOTA方法；并在伪装、自然、医疗等跨领域任务中取得最优或并列最优结果，证明泛化与迁移能力强，同时显著降低显存与计算成本（文中定性表达）。

Conclusion: 通过小波专家引导和条件适配器的动态交互，WEFT在不进行全参微调的前提下高效适配大模型，实现更优的分割精度与更低训练成本，具备跨场景通用性。

Abstract: Accurately localizing and segmenting relevant objects from optical remote sensing images (ORSIs) is critical for advancing remote sensing applications. Existing methods are typically built upon moderate-scale pre-trained models and employ diverse optimization strategies to achieve promising performance under full-parameter fine-tuning. In fact, deeper and larger-scale foundation models can provide stronger support for performance improvement. However, due to their massive number of parameters, directly adopting full-parameter fine-tuning leads to pronounced training difficulties, such as excessive GPU memory consumption and high computational costs, which result in extremely limited exploration of large-scale models in existing works. In this paper, we propose a novel dynamic wavelet expert-guided fine-tuning paradigm with fewer trainable parameters, dubbed WEFT, which efficiently adapts large-scale foundation models to ORSIs segmentation tasks by leveraging the guidance of wavelet experts. Specifically, we introduce a task-specific wavelet expert extractor to model wavelet experts from different perspectives and dynamically regulate their outputs, thereby generating trainable features enriched with task-specific information for subsequent fine-tuning. Furthermore, we construct an expert-guided conditional adapter that first enhances the fine-grained perception of frozen features for specific tasks by injecting trainable features, and then iteratively updates the information of both types of feature, allowing for efficient fine-tuning. Extensive experiments show that our WEFT not only outperforms 21 state-of-the-art (SOTA) methods on three ORSIs datasets, but also achieves optimal results in camouflage, natural, and medical scenarios. The source code is available at: https://github.com/CSYSI/WEFT.

</details>


### [20] [SAM-Aug: Leveraging SAM Priors for Few-Shot Parcel Segmentation in Satellite Time Series](https://arxiv.org/abs/2601.09110)
*Kai Hu,Yaozu Feng,Vladimir Lysenko,Ya Guo Member,Huayi Wu*

Main category: cs.CV

TL;DR: 提出SAM-Aug：利用SAM无监督生成几何感知掩膜先验，并通过RegionSmoothLoss在时间序列内约束预测一致性，在5%标注的PASTIS-R上显著提升few-shot遥感分割性能。


<details>
  <summary>Details</summary>
Motivation: 在遥感时间序列中，标注昂贵稀缺，现有全监督方法在低标注场景显著退化，限制落地应用。需要一种无需增加标注、可泛化的方式来提升少样本土地覆盖制图。

Method: 1) 将时间序列合成为无云复合影像；2) 使用SAM在完全无监督下生成几何感知区域掩膜作为先验；3) 设计RegionSmoothLoss，在SAM区域内、跨时间帧强制预测一致性，从而以先验正则模型；4) 将先验与少量标注联合训练，提升few-shot分割。

Result: 在PASTIS-R 5%标注设置下，三种随机种子平均测试mIoU=36.21%，较SOTA提升+2.33点（相对+6.89%）；最佳种子（42）达到40.28% mIoU，相对提升11.2%，且无需额外标注或微调。

Conclusion: 基础模型（SAM）的几何先验可作为强正则器，在标注匮乏下稳定提升时序遥感few-shot分割，方法可扩展、即插即用、无需人工标注或模型微调，具备良好泛化与实用性。

Abstract: Few-shot semantic segmentation of time-series remote sensing images remains a critical challenge, particularly in regions where labeled data is scarce or costly to obtain. While state-of-the-art models perform well under full supervision, their performance degrades significantly under limited labeling, limiting their real-world applicability. In this work, we propose SAM-Aug, a new annotation-efficient framework that leverages the geometry-aware segmentation capability of the Segment Anything Model (SAM) to improve few-shot land cover mapping. Our approach constructs cloud-free composite images from temporal sequences and applies SAM in a fully unsupervised manner to generate geometry-aware mask priors. These priors are then integrated into training through a proposed loss function called RegionSmoothLoss, which enforces prediction consistency within each SAM-derived region across temporal frames, effectively regularizing the model to respect semantically coherent structures. Extensive experiments on the PASTIS-R benchmark under a 5 percent labeled setting demonstrate the effectiveness and robustness of SAM-Aug. Averaged over three random seeds (42, 2025, 4090), our method achieves a mean test mIoU of 36.21 percent, outperforming the state-of-the-art baseline by +2.33 percentage points, a relative improvement of 6.89 percent. Notably, on the most favorable split (seed=42), SAM-Aug reaches a test mIoU of 40.28 percent, representing an 11.2 percent relative gain with no additional labeled data. The consistent improvement across all seeds confirms the generalization power of leveraging foundation model priors under annotation scarcity. Our results highlight that vision models like SAM can serve as useful regularizers in few-shot remote sensing learning, offering a scalable and plug-and-play solution for land cover monitoring without requiring manual annotations or model fine-tuning.

</details>


### [21] [Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning](https://arxiv.org/abs/2601.09111)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: 提出slow4fast-VLN：在开放场景（GSA-VLN）中，用快-慢双系统交互式推理提升跨环境与指令风格泛化的导航能力。快系统实时决策并积累记忆，慢系统离线反思记忆、提炼可迁移经验，反过来持续优化快系统，从而在未见环境下稳定高效导航。


<details>
  <summary>Details</summary>
Motivation: 传统VLN多为封闭集设定，训练与测试图像/指令风格一致；现实世界存在大量未见环境与异质指令，导致模型泛化弱。人类通过快（直觉）与慢（反思）认知协同形成稳健策略，启发将此机制引入VLN以提升开放场景适应。

Method: 构建动态交互的快-慢推理框架slow4fast-VLN：1) 快推理模块为端到端策略网络，基于实时多模态输入输出动作，并将执行轨迹/状态-动作-回报等记录入历史仓库形成记忆；2) 慢推理模块对快模块产生的记忆进行深度分析与反思，抽取可结构化的“经验”/规则/先验以增强决策泛化；3) 将这些经验回灌至快模块，持续优化其参数或策略（如通过经验蒸馏、记忆检索、正则或元学习更新），实现快慢交互与自适应；4) 目标任务为GSA-VLN，涵盖多样环境与不一致指令风格。

Result: 在未见场景与非一致指令上，框架能持续适配并高效执行导航，相比将快慢系统分离或仅用单一策略网络的方法获得更强泛化与稳定策略（文中摘要未给数值，强调能力与效率提升）。

Conclusion: 通过让快（实时）与慢（反思）模块交互，持续挖掘并注入跨场景可迁移经验，slow4fast-VLN在开放世界GSA-VLN中实现更好的泛化和适应性，相比传统独立或封闭设定方法更稳健。

Abstract: Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios.

</details>


### [22] [LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models](https://arxiv.org/abs/2601.09116)
*Haoyan Gong,Hongbin Liu*

Main category: cs.CV

TL;DR: 提出一种基于Qwen3-VL的端到端结构感知多模态推理框架，用可学习字符槽查询与跨注意力对齐字符位置信息，并通过残差调制注入视觉tokens，结合LoRA实现高效域适配，在严重退化车牌识别上显著优于两阶段与通用VLM。


<details>
  <summary>Details</summary>
Motivation: 两阶段“先复原再识别”在像素级与语义级目标不匹配，易产生伪影与误差累积；通用VLM缺乏对车牌固定长度与顺序等结构先验的显式建模，难以在低质图像上稳健识别。

Method: 基于Qwen3-VL，设计字符感知多模态推理模块（CMRM）：引入可学习的字符槽查询（对应每个字符位置），通过跨注意力从视觉特征中检索细粒度证据；再将字符感知表示以残差调制回注视觉tokens，使语言模型在显式结构先验下自回归生成；配合LoRA进行参数高效微调以完成域适配并保持通用性。

Result: 在合成与真实重度退化数据集上，方法显著优于各类“复原-识别”流水线与通用VLM，说明结构化推理在低质文本识别任务上的有效性。

Conclusion: 通过将字符序列结构先验显式注入大模型并以端到端方式优化，可缓解两阶段失配与误差累积问题，显著提升退化车牌识别性能；结构化多模态推理是面向低质文本识别的有效方向。

Abstract: Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing "restoration-then-recognition" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.

</details>


### [23] [LPCAN: Lightweight Pyramid Cross-Attention Network for Rail Surface Defect Detection Using RGB-D Data](https://arxiv.org/abs/2601.09118)
*Jackie Alex,Guoqiang Huan*

Main category: cs.CV

TL;DR: 提出LPCANet：一种利用RGB-D的轻量级金字塔交叉注意力网络，实现更快更准的轨道缺陷检测；在三套无监督RGB-D数据集上达SOTA，参数少、算力低、速度快，并具良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉轨检方法存在算力开销大、参数过多、精度不理想的问题，难以满足工业实时与部署需求。需一种兼具高效与高精度、能融合RGB与深度信息的模型。

Method: 以MobileNetv2为RGB主干；用轻量级金字塔模块（LPM）提取深度特征；通过跨模态注意力机制（CAM）实现RGB与深度的高效融合；加入空间特征提取器（SFE）强化结构细节；在三套无监督RGB-D轨道数据集上进行全面评测与消融，并在非轨道数据集上测试泛化。

Result: 仅9.90M参数、2.50 GFLOPs、162.60 fps；较18种方法整体领先：Sα提升1.48%、IOU提升0.86%、MAE降低1.77%（相对最佳基线）；消融表明CAM与SFE贡献关键；在DAGM2007、MT、Kolektor-SDD2上保持良好表现。

Conclusion: LPCANet以轻量化与跨模态融合有效提升轨道缺陷检测的准确性与速度，兼具工程可用性与跨域泛化，连接传统与深度方法；后续将进一步压缩以满足实时部署。

Abstract: This paper addresses the limitations of current vision-based rail defect detection methods, including high computational complexity, excessive parameter counts, and suboptimal accuracy. We propose a Lightweight Pyramid Cross-Attention Network (LPCANet) that leverages RGB-D data for efficient and accurate defect identification. The architecture integrates MobileNetv2 as a backbone for RGB feature extraction with a lightweight pyramid module (LPM) for depth processing, coupled with a cross-attention mechanism (CAM) for multimodal fusion and a spatial feature extractor (SFE) for enhanced structural analysis. Comprehensive evaluations on three unsupervised RGB-D rail datasets (NEU-RSDDS-AUG, RSDD-TYPE1, RSDD-TYPE2) demonstrate that LPCANet achieves state-of-the-art performance with only 9.90 million parameters, 2.50 G FLOPs, and 162.60 fps inference speed. Compared to 18 existing methods, LPCANet shows significant improvements, including +1.48\% in $S_α$, +0.86\% in IOU, and +1.77\% in MAE over the best-performing baseline. Ablation studies confirm the critical roles of CAM and SFE, while experiments on non-rail datasets (DAGM2007, MT, Kolektor-SDD2) validate its generalization capability. The proposed framework effectively bridges traditional and deep learning approaches, offering substantial practical value for industrial defect inspection. Future work will focus on further model compression for real-time deployment.

</details>


### [24] [Beyond Seen Bounds: Class-Centric Polarization for Single-Domain Generalized Deep Metric Learning](https://arxiv.org/abs/2601.09121)
*Xin Yuan,Meiqi Wan,Wei Liu,Xin Xu,Zheng Wang*

Main category: cs.CV

TL;DR: 提出CenterPolar用于单域广义深度度量学习，通过类中心的“离心扩张+向心约束”两阶段，提升对未见类别与域的泛化；在多数据集上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: SDG-DML测试时同时面临类别迁移与域迁移，现有方法用代理近邻扩张生成OOD样本但分布集中在类代理附近，难以覆盖现实中更广更远的域偏移，导致泛化不足。

Method: 提出CenterPolar框架，包含两个协同、以类为中心的极化阶段：1) C^3E（Class-Centric Centrifugal Expansion）：将源样本相对其类质心进行离心式偏移，动态扩张域分布，模拟更广的未见域；2) C^4（Class-Centric Centripetal Constraint）：将已见与未见样本共同拉向各自类质心，并强化类间分离，实现域不变的类判别特征。整体为“扩张-约束”策略，以更好覆盖目标域并保持判别性。

Result: 在CUB-200-2011 Ext., Cars196 Ext., DomainNet, PACS, Office-Home等基准上，实验显示CenterPolar优于现有SOTA，验证其有效性与泛化能力。

Conclusion: 通过类中心的离心扩张与向心约束相结合，CenterPolar有效缓解代理扩张的局限，学习到更具域与类泛化的度量空间，并在多数据集上取得领先表现。

Abstract: Single-domain generalized deep metric learning (SDG-DML) faces the dual challenge of both category and domain shifts during testing, limiting real-world applications. Therefore, aiming to learn better generalization ability on both unseen categories and domains is a realistic goal for the SDG-DML task. To deliver the aspiration, existing SDG-DML methods employ the domain expansion-equalization strategy to expand the source data and generate out-of-distribution samples. However, these methods rely on proxy-based expansion, which tends to generate samples clustered near class proxies, failing to simulate the broad and distant domain shifts encountered in practice. To alleviate the problem, we propose CenterPolar, a novel SDG-DML framework that dynamically expands and constrains domain distributions to learn a generalizable DML model for wider target domain distributions. Specifically, \textbf{CenterPolar} contains two collaborative class-centric polarization phases: (1) Class-Centric Centrifugal Expansion ($C^3E$) and (2) Class-Centric Centripetal Constraint ($C^4$). In the first phase, $C^3E$ drives the source domain distribution by shifting the source data away from class centroids using centrifugal expansion to generalize to more unseen domains. In the second phase, to consolidate domain-invariant class information for the generalization ability to unseen categories, $C^4$ pulls all seen and unseen samples toward their class centroids while enforcing inter-class separation via centripetal constraint. Extensive experimental results on widely used CUB-200-2011 Ext., Cars196 Ext., DomainNet, PACS, and Office-Home datasets demonstrate the superiority and effectiveness of our CenterPolar over existing state-of-the-art methods. The code will be released after acceptance.

</details>


### [25] [SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL](https://arxiv.org/abs/2601.09136)
*Lijun Liu,Linwei Chen,Zhishou Zhang,Meng Tian,Hengfu Cui,Ruiyang Li,Zhaocheng Liu,Qiang Ju,Qianxi Li,Hong-Yu Zhou*

Main category: cs.CV

TL;DR: 提出SkinFlow，通过优化视觉信息传输效率而非盲目扩参，显著提升皮肤科诊断表现；以动态视觉编码器与两阶段强化学习对齐显式医学描述与隐式纹理，实现对“弥散注意”的抑制，在Fitzpatrick17k上超越更大通用LVLM。


<details>
  <summary>Details</summary>
Motivation: 通用LVLM在皮肤病学中常因“弥散注意”无法从复杂背景中分离微小病灶，导致诊断不准。作者质疑“只靠扩大参数规模”这一路径，转而从信息几何与传输效率角度提升医学精度与安全。

Method: 1) 将诊断视为视觉信息传输效率最优化问题；2) 设计虚拟宽度的动态视觉编码器（DVE），在不增加物理参数的情况下“展开”病理流形、提升几何容量；3) 两阶段强化学习：Stage I 对齐显式医学描述（文本-临床语义一致性）；Stage II 重建隐式诊断纹理（细粒度视觉信号）并在受限语义空间内优化；4) 提出临床化评测协议，强调诊断安全与层级相关性，而非僵化标签匹配。

Result: 在Fitzpatrick17k上，7B模型相对超大通用模型（如Qwen3VL-235B、GPT-5.2）取得新SOTA：Top-1提升+12.06%，Top-6提升+28.57%。

Conclusion: 通过提升视觉几何容量与信息流（而非单纯扩参）可更好抑制弥散注意，增强皮肤科诊断推理与安全性；SkinFlow为医学多模态提供了轻量高效且可临床对齐的范式。

Abstract: General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to "diffuse attention" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to "unfold" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.

</details>


### [26] [SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection](https://arxiv.org/abs/2601.09147)
*Chenhao Fu,Han Fang,Xiuzheng Zheng,Wenbo Wei,Yonghua Li,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 提出SSVP用于零样本异常检测，通过多视觉编码融合与语义提示协同，在MVTec-AD上达到SOTA（Image-AUROC 93.0%，Pixel-AUROC 92.2%）。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的ZSAD多用单一视觉骨干：CLIP具备全局语义泛化但细粒度结构感知弱；以自监督视觉模型（如DINOv3）为主又缺乏跨模态语义对齐。需要一种能兼顾全局语义与局部结构判别，并能在零样本场景下稳定泛化的方法。

Method: SSVP提出层级语义-视觉协同(HSVS)，将DINOv3的多尺度结构先验深度注入CLIP的语义空间；随后用视觉条件提示生成器(VCPG)通过跨模态注意力从视觉特征动态生成文本提示，使语言查询能锚定具体异常模式；最后以视觉-文本异常映射器(VTAM)进行双门控校准，缓解全局分数与局部证据不一致的问题。整体实现高效的多编码融合与提示协同。

Result: 在7个工业基准上全面评测，方法鲁棒且在MVTec-AD上取得SOTA：Image-AUROC 93.0%，Pixel-AUROC 92.2%，显著优于现有零样本方法。

Conclusion: 多视觉编码与语义提示的协同能在零样本工业检测中兼顾全局语义与细粒度结构判别；通过动态提示与双门控校准提升定位与评分一致性，带来SOTA性能。

Abstract: Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.

</details>


### [27] [From Snow to Rain: Evaluating Robustness, Calibration, and Complexity of Model-Based Robust Training](https://arxiv.org/abs/2601.09153)
*Josué Martínez-Martínez,Olivia Brown,Giselle Zeno,Pooya Khorrami,Rajmonda Caceres*

Main category: cs.CV

TL;DR: 研究通过学习“干扰（nuisance）变化模型”来生成逼真的自然腐化，并提出随机覆盖+对抗细化的混合策略；在CURE-TSR雪/雨场景下，模型式方法在鲁棒性与校准上优于Vanilla、对抗训练和AugMix，模型式对抗训练最强但更耗算力，模型式数据增广以更低复杂度获得相近效果。


<details>
  <summary>Details</summary>
Motivation: 深度模型在自然腐化（如雨雪、噪声、模糊）下性能与校准显著下降，尤其在交通标志识别等安全关键领域。现有对抗训练或通用增广难以充分覆盖真实世界的干扰分布，亟需能捕捉自然变异的生成式/模型式方法。

Method: 学习一个“nuisance”变化模型（可看作生成/仿真器）来产生逼真腐化；设计纯模型式数据增广与“随机覆盖+在nuisance空间中对抗细化”的混合训练。以CURE-TSR数据集的雪/雨腐化与多严重度为基准，比较准确率、校准与训练复杂度，对比Vanilla、对抗训练、AugMix等基线。

Result: 模型式方法在所有腐化与严重度上稳定优于基线；其中模型式对抗训练最鲁棒、最校准，但计算开销最高；仅用模型式数据增广即可以显著更低的训练复杂度获得与最强方法相近、统计上无显著差异的性能。

Conclusion: 学习到的nuisance模型能更好地刻画自然变异，带来更鲁棒且更校准的识别。若资源充足，采用模型式对抗训练最佳；若受限，模型式数据增广提供性价比更高的鲁棒性路径，指向在真实挑战条件下更可靠的深度模型。

Abstract: Robustness to natural corruptions remains a critical challenge for reliable deep learning, particularly in safety-sensitive domains. We study a family of model-based training approaches that leverage a learned nuisance variation model to generate realistic corruptions, as well as new hybrid strategies that combine random coverage with adversarial refinement in nuisance space. Using the Challenging Unreal and Real Environments for Traffic Sign Recognition dataset (CURE-TSR), with Snow and Rain corruptions, we evaluate accuracy, calibration, and training complexity across corruption severities. Our results show that model-based methods consistently outperform baselines Vanilla, Adversarial Training, and AugMix baselines, with model-based adversarial training providing the strongest robustness under across all corruptions but at the expense of higher computation and model-based data augmentation achieving comparable robustness with $T$ less computational complexity without incurring a statistically significant drop in performance. These findings highlight the importance of learned nuisance models for capturing natural variability, and suggest a promising path toward more resilient and calibrated models under challenging conditions.

</details>


### [28] [Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies](https://arxiv.org/abs/2601.09169)
*Jamie Magrill,Leah Gornstein,Sandra Seekins,Barry Magrill*

Main category: cs.CV

TL;DR: 研究评估5个主流文生图平台在建筑图像生成中的历史规则准确性，发现整体准确度有限（均值42%），常见题材显著优于稀有题材；平台在“全错”失败率上差异明显，暴露出过度装饰、风格混淆与术语误读等系统性问题，提示需要标注与溯源标准并谨慎用于教育。


<details>
  <summary>Details</summary>
Motivation: 建筑学是规则密集、术语严格且风格分期清晰的领域，GenAI在此类高约束任务中的真实再现能力尚不明确；为指导研究、教育与行业使用，需要系统量化不同平台的准确性与失误模式。

Method: 选取30条覆盖风格、类型与规范化要素的建筑提示词，分别在5个平台上各生成4图，共600图；两位建筑史学者按预设标准独立评分，分歧协商一致；以每4图集合的“0–4张准确”计分；比较常见与稀有提示、平台间总体与极端结果，并进行质性失误模式分析。

Result: 常见提示的准确度是稀有提示的2.7倍（p<0.05）；平台总体准确度有限（最高52%，最低32%，均值42%）；“全对”比例平台间相近，但“全错”差异大：Imagen 3失败最少，Microsoft Image Generator失败最多；质性上出现过度装饰、中世纪与复兴风格混淆、以及对“egg-and-dart、banded column、pendentive”等术语误读。

Conclusion: 当前GenAI在建筑图像的历史与规则准确性不足，尤其对稀有或细粒度术语表现不佳；应实施可见的合成内容标注、改进训练数据溯源与标准，并在教育场景谨慎使用。

Abstract: Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p < 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.

</details>


### [29] [N-EIoU-YOLOv9: A Signal-Aware Bounding Box Regression Loss for Lightweight Mobile Detection of Rice Leaf Diseases](https://arxiv.org/abs/2601.09170)
*Dung Ta Nguyen Duc,Thanh Bui Dang,Hoang Le Minh,Tung Nguyen Viet,Huong Nguyen Thanh,Dong Trinh Cong*

Main category: cs.CV

TL;DR: 提出NEIoU YOLOv9：在YOLOv9t中引入非单调、几何解耦的IoU回归损失（NEIoU），强化低重叠难样本定位信号，减少梯度干扰；在水稻叶病害数据上较CIoU提高mAP至90.3%（+4.3%），并在TFLite Float16上移动端推理156ms/帧，兼顾精度、稳定与效率。


<details>
  <summary>Details</summary>
Motivation: 农业场景中小目标、低对比度病斑检测难，标准边界框损失在低IoU区域信号弱且梯度耦合易相互干扰，导致定位不稳与收敛慢；需一种既能加强难样本回归信号又具轻量高效、可落地到边缘设备的方案。

Method: 设计NEIoU（Non-monotonic Efficient IoU）损失：1）非单调梯度聚焦，增强低IoU样本的定位梯度；2）宽高几何解耦，降低w/h相互影响的梯度干扰；3）将该损失集成到轻量YOLOv9t；4）在自建5908张、含4类病害+健康叶的稻叶数据集上训练评测；5）模型部署到Android端，TFLite Float16量化测试时延。

Result: 与标准CIoU相比，在更严格评价下定位精度提升，mAP达90.3%，较基线+4.3%；移动端部署平均推理156ms/帧，在保持精度的同时具备实时性。

Conclusion: NEIoU通过非单调聚焦与几何解耦重塑定位梯度，显著提升小/低对比目标的检测与定位，同时保持轻量与推理效率，适合农业边缘监测的实用部署。

Abstract: In this work, we propose N EIoU YOLOv9, a lightweight detection framework based on a signal aware bounding box regression loss derived from non monotonic gradient focusing and geometric decoupling principles, referred to as N EIoU (Non monotonic Efficient Intersection over Union). The proposed loss reshapes localization gradients by combining non monotonic focusing with decoupled width and height optimization, thereby enhancing weak regression signals for hard samples with low overlap while reducing gradient interference. This design is particularly effective for small and low contrast targets commonly observed in agricultural disease imagery. The proposed N EIoU loss is integrated into a lightweight YOLOv9t architecture and evaluated on a self collected field dataset comprising 5908 rice leaf images across four disease categories and healthy leaves. Experimental results demonstrate consistent performance gains over the standard CIoU loss, achieving a mean Average Precision of 90.3 percent, corresponding to a 4.3 percent improvement over the baseline, with improved localization accuracy under stricter evaluation criteria. For practical validation, the optimized model is deployed on an Android device using TensorFlow Lite with Float16 quantization, achieving an average inference time of 156 milliseconds per frame while maintaining accuracy. These results confirm that the proposed approach effectively balances accuracy, optimization stability, and computational efficiency for edge based agricultural monitoring systems.

</details>


### [30] [From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows](https://arxiv.org/abs/2601.09191)
*Qizhen Lan,Aaron Choi,Jun Ma,Bo Wang,Zhaogming Zhao,Xiaoqian Jiang,Yu-Chun Hsu*

Main category: cs.CV

TL;DR: 提出一种面向部署的知识蒸馏框架，将高性能医学图像分割模型转化为一组可伸缩、小型化且与现有临床推理管线兼容的学生模型；在94%参数削减下保留约98.7%的分割精度，并将CPU推理延迟降低至多67%，无需额外部署改造。


<details>
  <summary>Details</summary>
Motivation: 医院本地环境算力固定、云推理受治理与安全限制，高容量分割模型虽准确但算力与维护成本高，难以在常规临床流程中长期落地；需要一种既保持系统兼容性又能系统性降容的可部署方案。

Method: 以高性能教师分割模型为源，通过知识蒸馏训练一族尺寸不同的学生模型；不改动现有推理管线与系统接口，保持架构兼容；在多站点脑MRI（1,104个3D体数据，独立测试101例）上系统评估，并在腹部CT上验证跨模态可迁移性。

Result: 在激进的参数压缩（94%参数减少）下，学生模型保留教师约98.7%的分割准确度；在CPU上实现最高约67%的推理时延下降；无需额外部署开销；在多站点MRI与跨模态CT上均验证有效。

Conclusion: 知识蒸馏为将研究级分割模型转化为可维护、可落地的院内部署组件提供了可靠路径：在保持临床系统兼容性的前提下，系统性降低模型容量与推理延迟，同时基本不损失精度。

Abstract: Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.

</details>


### [31] [Point Tracking as a Temporal Cue for Robust Myocardial Segmentation in Echocardiography Videos](https://arxiv.org/abs/2601.09207)
*Bahar Khodabakhshian,Nima Hashemi,Armin Saadat,Zahra Gholami,In-Chang Hwang,Samira Sojoudi,Christina Luong,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: 提出Point-Seg：用点追踪作为时间线索，提升超声心动图视频心肌分割的一致性与稳定性，在低质视频上优于现有方法，并提供像素级运动信息，利于下游分析。


<details>
  <summary>Details</summary>
Motivation: 超声视频心肌分割受低对比度、噪声、形变与解剖差异影响；现有方法要么逐帧忽视时间信息，要么依赖记忆式特征传播，易随时间累计误差并产生漂移，需要一种稳健利用时序且避免误差堆积的方案。

Method: 构建基于Transformer的分割框架Point-Seg：引入在合成超声数据上训练的点追踪模块，跟踪关键解剖地标并将轨迹作为显式、感知运动的时间提示来引导分割，避免记忆式特征累积；辅以时间平滑损失提升跨帧一致性。

Result: 在公有与私有数据集评测：在高质量超声上Dice与SOTA相当；在低质量超声上分割更准且时间稳定性更好；同时获得像素级心肌运动信息。

Conclusion: 点追踪可作为有效时间线索，带来稳定、可泛化的心肌视频分割，对应变测量与局部室壁运动异常检测等下游任务更有价值；代码开源于GitHub。

Abstract: Purpose: Myocardium segmentation in echocardiography videos is a challenging task due to low contrast, noise, and anatomical variability. Traditional deep learning models either process frames independently, ignoring temporal information, or rely on memory-based feature propagation, which accumulates error over time. Methods: We propose Point-Seg, a transformer-based segmentation framework that integrates point tracking as a temporal cue to ensure stable and consistent segmentation of myocardium across frames. Our method leverages a point-tracking module trained on a synthetic echocardiography dataset to track key anatomical landmarks across video sequences. These tracked trajectories provide an explicit motion-aware signal that guides segmentation, reducing drift and eliminating the need for memory-based feature accumulation. Additionally, we incorporate a temporal smoothing loss to further enhance temporal consistency across frames. Results: We evaluate our approach on both public and private echocardiography datasets. Experimental results demonstrate that Point-Seg has statistically similar accuracy in terms of Dice to state-of-the-art segmentation models in high quality echo data, while it achieves better segmentation accuracy in lower quality echo with improved temporal stability. Furthermore, Point-Seg has the key advantage of pixel-level myocardium motion information as opposed to other segmentation methods. Such information is essential in the computation of other downstream tasks such as myocardial strain measurement and regional wall motion abnormality detection. Conclusion: Point-Seg demonstrates that point tracking can serve as an effective temporal cue for consistent video segmentation, offering a reliable and generalizable approach for myocardium segmentation in echocardiography videos. The code is available at https://github.com/DeepRCL/PointSeg.

</details>


### [32] [Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy](https://arxiv.org/abs/2601.09209)
*Qiang Hu,Qimei Wang,Yingjie Guo,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出PaGKD，一个无需成对NBI-WLI图像的分组级跨模态蒸馏框架，通过群组原型与群组稠密对齐，利用未配对临床数据提升WLI诊断，四个数据集均显著优于SOTA（AUC相对提升3.3%、1.1%、2.8%、3.2%）。


<details>
  <summary>Details</summary>
Motivation: 现实中获取同一病灶的NBI-WLI严格配对代价高、难规模化，导致大量未配对数据无法用于提升仅WLI模型；现有方法依赖图像级配对，易受语义不匹配影响，跨模态迁移受限。

Method: 提出PaGKD（Pairing-free Group-level Knowledge Distillation）。核心：1）GKD-Pro：以病灶感知的共享查询从WLI/NBI群组中提取模态不变的语义原型，形成紧凑群组表示并进行原型级蒸馏；2）GKD-Den：利用由激活导出的关系图引导群组感知注意力，实现跨模态的稠密（像素/区域级）对齐。两模块联合约束全局语义一致性与局部结构连贯性，无需图像级对应。

Result: 在四个临床数据集上，PaGKD相较SOTA均取得显著提升，AUC相对提升分别为3.3%、1.1%、2.8%、3.2%，验证了在未配对数据上进行跨模态蒸馏的有效性与稳健性。

Conclusion: 群组级、无需配对的知识蒸馏可有效从NBI向WLI迁移诊断知识，兼顾全局与局部对齐，实用且可扩展，为未配对跨模态医学影像学习提供新方向。

Abstract: White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.

</details>


### [33] [Affostruction: 3D Affordance Grounding with Generative Reconstruction](https://arxiv.org/abs/2601.09211)
*Chunghyun Park,Seunghyeon Lee,Minsu Cho*

Main category: cs.CV

TL;DR: 提出Affostruction：从单个RGB-D对象图像进行可供性（affordance）落地到完整3D形状（含不可见部分），并显著提升aIoU与重建IoU。


<details>
  <summary>Details</summary>
Motivation: 现有方法只在可见表面预测可供性，忽略被遮挡或不可见区域，导致对动作相关表面理解不完整；需要能从部分观测推断完整几何并在全形体上定位可供性。

Method: 1) 生成式多视角重建：以稀疏体素融合为核心，在保持固定token复杂度下外推不可见几何并重建完整形体；2) 基于流模型的可供性落地：用flow建模多模态/不确定的可供性分布；3) 可供性驱动的主动视角选择：利用已预测的可供性引导智能采样新视角以改进重建与落地。

Result: 在可供性落地上达到19.1 aIoU（相对提升40.4%）；在3D重建上达到32.67 IoU（相对提升67.7%），实现对完整形体（含未观测区域）的更准确可供性预测。

Conclusion: 通过联合生成式重建、流式可供性建模与主动视角选择，可在全形体范围内进行鲁棒的可供性定位，显著优于仅基于可见表面的现有方法。

Abstract: This paper addresses the problem of affordance grounding from RGBD images of an object, which aims to localize surface regions corresponding to a text query that describes an action on the object. While existing methods predict affordance regions only on visible surfaces, we propose Affostruction, a generative framework that reconstructs complete geometry from partial observations and grounds affordances on the full shape including unobserved regions. We make three core contributions: generative multi-view reconstruction via sparse voxel fusion that extrapolates unseen geometry while maintaining constant token complexity, flow-based affordance grounding that captures inherent ambiguity in affordance distributions, and affordance-driven active view selection that leverages predicted affordances for intelligent viewpoint sampling. Affostruction achieves 19.1 aIoU on affordance grounding (40.4\% improvement) and 32.67 IoU for 3D reconstruction (67.7\% improvement), enabling accurate affordance prediction on complete shapes.

</details>


### [34] [Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation](https://arxiv.org/abs/2601.09212)
*Xingyao Li,Fengzhuo Zhang,Cunxiao Du,Hui Ji*

Main category: cs.CV

TL;DR: 提出COOL-SD：一种带退火的“宽松”推测解码方法，为自回归图像生成加速并保持质量/提升质量。其理论分析给出最优重采样分布与退火行为依据，实验显示在速度-质量权衡上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AR图像生成推断慢，因顺序生成与图像token歧义；即便用推测解码（SD）也受限。现有“宽松”SD加速方法缺乏理论支撑，效果不稳定，需有统一理论与更稳健的设计来提升速度同时守住或提升质量。

Method: 1) 建立宽松SD与目标模型之间的TV距离上界，推导出使该上界最小的最优重采样分布；2) 基于扰动分析揭示宽松SD存在退火特性，据此设计COOL-SD的退火策略（逐步放宽/收紧松弛程度与重采样分布）；3) 将两者结合成可实现的解码流程用于AR图像生成。

Result: COOL-SD在相同质量下显著降低延迟，或在相似延迟下提升生成质量；在多组实验中相较以往宽松/标准推测解码均有一致的速度-质量权衡改进。

Conclusion: 宽松推测解码可获得扎实的理论基础；通过最优重采样与退火设计的COOL-SD实现更快或更优的AR图像生成，在实践中稳定优于现有方法。

Abstract: Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.

</details>


### [35] [SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion](https://arxiv.org/abs/2601.09213)
*Jialu Li,Taiyan Zhou*

Main category: cs.CV

TL;DR: 提出SpikeVAEDiff：用VDVAE先粗重建，再用Versatile Diffusion结合CLIP特征细化，从神经脉冲数据重建高分辨率、语义一致的图像；VISI区对重建最关键，消融显示特定脑区数据显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 从高时间/空间分辨率的神经脉冲(spikes)中恢复自然图像，有助于理解视觉表征并推动脑机接口与计算机视觉融合；现有多依赖fMRI，分辨率受限，且缺少能生成高分辨率且语义可信图像的方法。

Method: 两阶段框架SpikeVAEDiff：1) 用VDVAE将脉冲信号映射到潜变量并生成低分辨率初始重建；2) 训练回归器从脉冲信号预测CLIP-Vision与CLIP-Text特征，作为Versatile Diffusion的条件做图像到图像细化。使用Allen Visual Coding-Neuropixels数据，分脑区分析与消融。

Result: 在Allen Neuropixels数据集上实现高分辨率、语义合理的图像重建；VISI区显示最强激活且对质量贡献最大；展示成功/失败案例；VDVAE有效；消融表明某些特定脑区数据显著提升重建性能。

Conclusion: 基于脉冲数据的两阶段生成范式可优于fMRI途径的时空分辨率限制；结合VDVAE与扩散模型并以CLIP条件引导，能更好重建自然场景；关键脑区（如VISI）的信息对性能至关重要。

Abstract: Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.

</details>


### [36] [Disentangle Object and Non-object Infrared Features via Language Guidance](https://arxiv.org/abs/2601.09228)
*Fan Liu,Ting Wu,Chuanyi Zhang,Liang Yao,Xing Ma,Yuhui Zheng*

Main category: cs.CV

TL;DR: 提出一种结合视觉-语言表示学习的红外目标检测方法，通过文本监督提升特征判别性，在M3FD与FLIR上取得SOTA（83.7%/86.1% mAP）。


<details>
  <summary>Details</summary>
Motivation: 红外图像对比度低、边缘弱，传统仅依赖视觉信号难以学到判别性强的目标特征，导致在黑暗、雨雪等复杂环境下检测不稳健。需要引入额外的高层语义监督以更好地区分目标与非目标特征。

Method: 构建视觉-语言联合学习框架：1) 语义特征对齐（SFA）模块，将图像中的目标特征与对应文本特征进行对齐；2) 目标特征解耦（OFD）模块，通过最小化文本对齐的目标特征与非目标特征之间的相关性，实现目标/非目标特征解耦；3) 将解耦后的目标特征输入检测头以进行检测。

Result: 在两个基准上实现优异性能：M3FD 数据集 mAP 83.7%，FLIR 数据集 mAP 86.1%。

Conclusion: 利用文本语义监督进行特征对齐与解耦，可显著提升红外目标检测的判别性并降低噪声，从而带来明显性能增益；方法在多基准上验证有效，代码待开源。

Abstract: Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.

</details>


### [37] [SPOT-Face: Forensic Face Identification using Attention Guided Optimal Transport](https://arxiv.org/abs/2601.09229)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

TL;DR: SPOT-Face提出以超像素图和注意力引导的最优传输实现跨域（骨骼/素描↔人脸）识别，在S2F与CUFS上显著提升Recall与mAP，相比现有图方法更优。


<details>
  <summary>Details</summary>
Motivation: 法医场景中常规DNA线索缺失（如毛发、软组织）时，需依靠骨骼或素描与生前照片进行身份匹配；现有深度人脸识别难以有效建模不同取证模态间的结构对应关系。

Method: 1) 将图像分割为超像素并构建超像素图；2) 使用多种GNN骨干提取图嵌入；3) 通过注意力引导的最优传输在跨域间建立结构级对应与对齐，实现度量/匹配；4) 在统一框架下处理骨骼、素描与人脸三类模态。

Result: 在IIT_Mandi_S2F与CUFS两个公开数据集上，较现有图方法在Recall与mAP等指标显著提升，显示对骨骼/素描到人脸的匹配更有效。

Conclusion: 超像素图+GNN+注意力最优传输的统一框架能有效建模跨域结构对应，提升法医人脸身份识别性能，适用于实务中的骸骨与素描匹配。

Abstract: Person identification in forensic investigations becomes very challenging when common identification means for DNA (i.e., hair strands, soft tissue) are not available. Current methods utilize deep learning methods for face recognition. However, these methods lack effective mechanisms to model cross-domain structural correspondence between two different forensic modalities. In this paper, we introduce a SPOT-Face, a superpixel graph-based framework designed for cross-domain forensic face identification of victims using their skeleton and sketch images. Our unified framework involves constructing a superpixel-based graph from an image and then using different graph neural networks(GNNs) backbones to extract the embeddings of these graphs, while cross-domain correspondence is established through attention-guided optimal transport mechanism. We have evaluated our proposed framework on two publicly available dataset: IIT\_Mandi\_S2F (S2F) and CUFS. Extensive experiments were conducted to evaluate our proposed framework. The experimental results show significant improvement in identification metrics ( i.e., Recall, mAP) over existing graph-based baselines. Furthermore, our framework demonstrates to be highly effective for matching skulls and sketches to faces in forensic investigations.

</details>


### [38] [CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation](https://arxiv.org/abs/2601.09230)
*Haodi Yao,Fenghua He,Ning Hao,Yao Su*

Main category: cs.CV

TL;DR: 提出CLIDD，一种跨层独立可变形描述子，直接从独立特征层级采样并用可学习偏移捕捉细粒度结构；结合核融合加速与蒸馏+度量学习训练，兼顾高精度与超高效率：超小模型仅0.004M参数即可达到SuperPoint精度，高配版本超越DINOv2系SOTA并在边缘设备>200 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有局部特征描述往往在判别性与效率间权衡：统一致密表示计算开销大、实时性差；而轻量方法难以保持高精度。需要一种既能捕捉跨尺度细节、又可在资源受限设备上实时运行的描述子。

Method: 1) CLIDD：跨层独立采样策略，使用可学习偏移(可变形采样)从多层特征层级中直接读取局部信息，避免构建统一致密表示；2) 硬件感知的核融合，提升推理吞吐；3) 可扩展训练框架：轻量网络骨干+度量学习损失并配合知识蒸馏，生成适配不同算力的模型族。

Result: 在多项评测中同时实现高匹配精度与高效推理：超小变体仅0.004M参数、尺寸较SuperPoint降99.7%，但精度相当；高性能配置在边缘设备上>200 FPS，并超越包括基于DINOv2的大模型在内的现有SOTA方法。

Conclusion: CLIDD通过跨层独立可变形采样与系统级加速，实现高判别性与极低计算负担的局部特征匹配，适用于实时机器人/AR等空间智能任务，并可按部署需求灵活扩展。

Abstract: Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality. Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency. To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies. This approach utilizes learnable offsets to capture fine-grained structural details across scales while bypassing the computational burden of unified dense representations. To ensure real-time performance, we implement a hardware-aware kernel fusion strategy that maximizes inference throughput. Furthermore, we develop a scalable framework that integrates lightweight architectures with a training protocol leveraging both metric learning and knowledge distillation. This scheme generates a wide spectrum of model variants optimized for diverse deployment constraints. Extensive evaluations demonstrate that our approach achieves superior matching accuracy and exceptional computational efficiency simultaneously. Specifically, the ultra-compact variant matches the precision of SuperPoint while utilizing only 0.004M parameters, achieving a 99.7% reduction in model size. Furthermore, our high-performance configuration outperforms all current state-of-the-art methods, including high-capacity DINOv2-based frameworks, while exceeding 200 FPS on edge devices. These results demonstrate that CLIDD delivers high-precision local feature matching with minimal computational overhead, providing a robust and scalable solution for real-time spatial intelligence tasks.

</details>


### [39] [Knowledge-Embedded and Hypernetwork-Guided Few-Shot Substation Meter Defect Image Generation Method](https://arxiv.org/abs/2601.09238)
*Jackie Alex,Justin Petter*

Main category: cs.CV

TL;DR: 提出一种结合知识嵌入与超网络引导条件控制的Stable Diffusion框架，在极少标注样本下生成逼真的变电站电表裂纹缺陷图像，显著提升下游检测效果。


<details>
  <summary>Details</summary>
Motivation: 工业场景（如变电站电表）缺陷样本极度稀缺，通用自然图像预训练模型与工业域存在较大域差，导致基于生成的增广质量与可控性不足，需要一种能在小样本下生成真实且可控缺陷图像的方法，以提升检测器训练。

Method: 1) 以DreamBooth式知识嵌入微调Stable Diffusion，学习电表的结构与纹理先验，缩小域差；2) 构建几何裂纹建模模块，对位置、长度、曲率、分支等参数化并生成空间约束的控制图，以像素级引导生成；3) 设计轻量超网络，根据控制图与高层缺陷描述动态调制扩散去噪过程，在保真与可控间自适应权衡。

Result: 在真实变电站电表数据集上，相比主流增广/生成基线：FID降低32.7%，多样性指标提升；用于训练下游缺陷检测器时，mAP提升15.3%。

Conclusion: 该框架在小样本条件下实现高保真、强可控的缺陷合成，显著增强下游检测性能，为工业检验中稀缺缺陷样本提供实用的数据合成方案。

Abstract: Substation meters play a critical role in monitoring and ensuring the stable operation of power grids, yet their detection of cracks and other physical defects is often hampered by a severe scarcity of annotated samples. To address this few-shot generation challenge, we propose a novel framework that integrates Knowledge Embedding and Hypernetwork-Guided Conditional Control into a Stable Diffusion pipeline, enabling realistic and controllable synthesis of defect images from limited data.
  First, we bridge the substantial domain gap between natural-image pre-trained models and industrial equipment by fine-tuning a Stable Diffusion backbone using DreamBooth-style knowledge embedding. This process encodes the unique structural and textural priors of substation meters, ensuring generated images retain authentic meter characteristics.
  Second, we introduce a geometric crack modeling module that parameterizes defect attributes--such as location, length, curvature, and branching pattern--to produce spatially constrained control maps. These maps provide precise, pixel-level guidance during generation.
  Third, we design a lightweight hypernetwork that dynamically modulates the denoising process of the diffusion model in response to the control maps and high-level defect descriptors, achieving a flexible balance between generation fidelity and controllability.
  Extensive experiments on a real-world substation meter dataset demonstrate that our method substantially outperforms existing augmentation and generation baselines. It reduces Frechet Inception Distance (FID) by 32.7%, increases diversity metrics, and--most importantly--boosts the mAP of a downstream defect detector by 15.3% when trained on augmented data. The framework offers a practical, high-quality data synthesis solution for industrial inspection systems where defect samples are rare.

</details>


### [40] [DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos](https://arxiv.org/abs/2601.09240)
*Jiajun Chen,Jing Xiao,Shaohan Cao,Yuming Zhu,Liang Liao,Jun Pan,Mi Wang*

Main category: cs.CV

TL;DR: 提出DeTracker面向未稳像卫星视频的联合检测-跟踪框架，通过全局-局部运动解耦与时序特征金字塔，显著提升微小目标MOT鲁棒性，并新建SDM-Car-SU基准，实验在模拟与真实数据上均达SOTA（MOTA 61.1%/47.3%）。


<details>
  <summary>Details</summary>
Motivation: 未稳像卫星视频存在平台抖动与目标外观微弱、目标尺寸极小等问题，导致现有MOT在轨迹稳定性、运动估计与身份保持方面明显退化，缺乏对不同抖动模式的系统评测数据。

Method: 1) DeTracker框架：联合检测与跟踪。2) Global–Local Motion Decoupling (GLMD)：先进行全局对齐以估计并补偿平台运动，再做局部细化以分离真实目标运动，提升轨迹稳定与运动估计精度。3) Temporal Dependency Feature Pyramid (TDFP)：跨帧时序特征融合，增强微小目标特征的连续性与可分辨性。4) 构建SDM-Car-SU数据集：模拟多方向/多速度平台运动用于鲁棒性评测。

Result: 在SDM-Car-SU上MOTA达61.1%，在真实未稳像卫星视频上MOTA达47.3%，显著优于现有方法。

Conclusion: 对未稳像卫星视频，解耦平台与目标运动并利用时序金字塔融合能有效提升微小目标MOT性能；所建基准促进对抖动鲁棒性的系统评估。

Abstract: Satellite videos provide continuous observations of surface dynamics but pose significant challenges for multi-object tracking (MOT), especially under unstabilized conditions where platform jitter and the weak appearance of tiny objects jointly degrade tracking performance. To address this problem, we propose DeTracker, a joint detection-and-tracking framework tailored for unstabilized satellite videos. DeTracker introduces a Global--Local Motion Decoupling (GLMD) module that explicitly separates satellite platform motion from true object motion through global alignment and local refinement, leading to improved trajectory stability and motion estimation accuracy. In addition, a Temporal Dependency Feature Pyramid (TDFP) module is developed to perform cross-frame temporal feature fusion, enhancing the continuity and discriminability of tiny-object representations. We further construct a new benchmark dataset, SDM-Car-SU, which simulates multi-directional and multi-speed platform motions to enable systematic evaluation of tracking robustness under varying motion perturbations. Extensive experiments on both simulated and real unstabilized satellite videos demonstrate that DeTracker significantly outperforms existing methods, achieving 61.1% MOTA on SDM-Car-SU and 47.3% MOTA on real satellite video data.

</details>


### [41] [A$^2$TG: Adaptive Anisotropic Textured Gaussians for Efficient 3D Scene Representation](https://arxiv.org/abs/2601.09243)
*Sheng-Chi Hsu,Ting-Yu Yen,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: A^2TG 提出“自适应各向异性纹理高斯”，根据梯度自适应地为每个高斯分配纹理分辨率与长宽比，在维持或提升画质的同时显著降低纹理内存，相比固定方形纹理的方案更高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的即时代渲染方法常为每个原语分配固定方形纹理，无法适应场景中细节与方向性的变化，导致内存浪费与画质受限。需要一种能按细节与各向异性自适应分配纹理资源的表示。

Method: 提出 A^2TG：在每个高斯原语上使用各向异性纹理，并通过梯度引导的自适应规则联合决定其纹理分辨率与纵横比，使纹理采样与高斯斑点的各向异性形状和图像细节分布对齐，实现非均匀、细节感知的纹理资源分配。

Result: 在多个基准数据集上，相比采用固定方形纹理的高斯表示，A^2TG 在显著降低内存占用的同时，取得相当或更高的渲染保真度，整体性能稳定领先。

Conclusion: 自适应各向异性纹理与梯度驱动的分配策略可显著提升高斯渲染的纹理效率与图像质量，为实时高质量 3D 场景表示提供更具扩展性的途径。

Abstract: Gaussian Splatting has emerged as a powerful representation for high-quality, real-time 3D scene rendering. While recent works extend Gaussians with learnable textures to enrich visual appearance, existing approaches allocate a fixed square texture per primitive, leading to inefficient memory usage and limited adaptability to scene variability. In this paper, we introduce adaptive anisotropic textured Gaussians (A$^2$TG), a novel representation that generalizes textured Gaussians by equipping each primitive with an anisotropic texture. Our method employs a gradient-guided adaptive rule to jointly determine texture resolution and aspect ratio, enabling non-uniform, detail-aware allocation that aligns with the anisotropic nature of Gaussian splats. This design significantly improves texture efficiency, reducing memory consumption while enhancing image quality. Experiments on multiple benchmark datasets demonstrate that A TG consistently outperforms fixed-texture Gaussian Splatting methods, achieving comparable rendering fidelity with substantially lower memory requirements.

</details>


### [42] [Integrating Diverse Assignment Strategies into DETRs](https://arxiv.org/abs/2601.09247)
*Yiwei Zhang,Jin Gao,Hanshi Wang,Fudong Ge,Guan Luo,Weiming Hu,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出LoRA-DETR：用多样化的一对多标注策略，通过训练期插入多支LoRA分支为主干注入丰富监督，推理时移除，无额外开销，统一且轻量地加速/提升DETR类检测器。


<details>
  <summary>Details</summary>
Motivation: DETR的一对一匹配监督稀疏，导致收敛慢；现有一对多方法往往复杂、与架构绑死、只用单一辅助手段，缺乏统一可扩展的设计。作者想找到更优雅、参数高效且通用的方案。

Method: 在任意DETR主干上，训练期附加多条低秩适配（LoRA）分支，每条分支对应一种不同的一对多标签分配规则；这些分支产生多样化梯度监督共同优化主干，推理时将分支丢弃不增加计算。系统性实验分析显示增益来自“多样性”而非“监督数量”。

Result: 在多种DETR基线上的广泛实验显示，本方法在不改动主干结构与推理开销的前提下，实现更快/更稳的训练与更高精度，达到或逼近SOTA。

Conclusion: 多样化的一对多监督是关键驱动因素；通过LoRA分支可将其以统一、轻量的方式注入任意DETR，实现性能提升且保持模型优雅与推理零开销。

Abstract: Label assignment is a critical component in object detectors, particularly within DETR-style frameworks where the one-to-one matching strategy, despite its end-to-end elegance, suffers from slow convergence due to sparse supervision. While recent works have explored one-to-many assignments to enrich supervisory signals, they often introduce complex, architecture-specific modifications and typically focus on a single auxiliary strategy, lacking a unified and scalable design. In this paper, we first systematically investigate the effects of ``one-to-many'' supervision and reveal a surprising insight that performance gains are driven not by the sheer quantity of supervision, but by the diversity of the assignment strategies employed. This finding suggests that a more elegant, parameter-efficient approach is attainable. Building on this insight, we propose LoRA-DETR, a flexible and lightweight framework that seamlessly integrates diverse assignment strategies into any DETR-style detector. Our method augments the primary network with multiple Low-Rank Adaptation (LoRA) branches during training, each instantiating a different one-to-many assignment rule. These branches act as auxiliary modules that inject rich, varied supervisory gradients into the main model and are discarded during inference, thus incurring no additional computational cost. This design promotes robust joint optimization while maintaining the architectural simplicity of the original detector. Extensive experiments on different baselines validate the effectiveness of our approach. Our work presents a new paradigm for enhancing detectors, demonstrating that diverse ``one-to-many'' supervision can be integrated to achieve state-of-the-art results without compromising model elegance.

</details>


### [43] [Hybrid guided variational autoencoder for visual place recognition](https://arxiv.org/abs/2601.09248)
*Ni Wang,Zihan You,Emre Neftci,Thorben Schoepe*

Main category: cs.CV

TL;DR: 提出一种面向机器人VPR的紧凑模型：事件相机+引导式VAE（编码器为脉冲神经网络），在新建室内数据集上以更低内存与功耗达到与SOTA相当的分类性能，并在光照变化与未知场景上具备鲁棒泛化能力。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在室内等GPS不可用环境需精准定位。现有VPR要么模型庞大不利部署，要么紧凑但鲁棒与泛化不足；同时事件相机与类脑硬件为低功耗、低延迟提供机会。

Method: 构建事件相机驱动的引导式变分自编码器：编码器为适配神经形态硬件的脉冲神经网络；通过引导信号使潜变量学习可分离的地点特征；在包含16个地点、不同光照条件的新室内VPR数据集上训练评估。

Result: 模型在16个室内地点上实现与SOTA可比的分类准确率；在多种光照条件下保持鲁棒；对未见场景的事件输入也能区分不同地点，显示强泛化；内存占用与硬件适配性优于传统VPR。

Conclusion: 引导式事件VAE为移动机器人提供紧凑、低功耗且鲁棒的VPR方案，能够在已知与未知室内环境中支持更可靠的自主导航。

Abstract: Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.

</details>


### [44] [PhyRPR: Training-Free Physics-Constrained Video Generation](https://arxiv.org/abs/2601.09255)
*Yibo Zhao,Hengjia Li,Xiaofei He,Boxi Wu*

Main category: cs.CV

TL;DR: 提出一个三阶段、免训练的视频生成流程PhyRPR，将物理推理与视觉合成解耦，通过LLM物理推理+关键帧、确定性运动脚手架、扩散采样融合精修，使生成视频更符合物理与可控运动。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式视频生成虽画面逼真，但难以遵守物理约束。根因在于单阶段方法把高层物理理解与低层视觉生成纠缠，缺乏显式物理控制，导致需物理推理的内容难以合成。

Method: 提出免训练三阶段流水线：1) PhyReason：使用多模态大模型进行物理状态与因果/约束推理，并用图像生成器合成关键帧；2) PhyPlan：基于推理与关键帧，确定性生成可控的粗运动“脚手架”（轨迹/场景约束）；3) PhyRefine：在扩散采样中通过潜空间融合策略将脚手架注入，细化外观同时保持计划的动力学。

Result: 在多种物理约束场景的实验中，方法稳定提升了物理合理性与运动可控性，相较基线更一致地满足物理规律。

Conclusion: 把物理推理与视觉生成解耦的三阶段、免训练范式能带来更强的物理可遵从与运动控制，为物理感知的视频生成提供了可扩展路线。

Abstract: Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\textit{PhyRPR}:\textit{Phy\uline{R}eason}--\textit{Phy\uline{P}lan}--\textit{Phy\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.

</details>


### [45] [Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery](https://arxiv.org/abs/2601.09262)
*Maria Sdraka,Dimitrios Michail,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: 提出BAM-MRCD模型，融合MODIS与Sentinel-2多分辨率多源影像，实现火灾后快速高精度烧伤区划定，优于现有变化检测与基线方法；代码与数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有卫星影像在空间分辨率与重访频率之间存在权衡，导致实务上需要在火灾发生后尽快获取高精度烧伤区却受限；火烧迹地光谱变化不规则且空间异质，增加了检测难度。

Method: 设计BAM-MRCD深度学习框架，利用MODIS（高时效、低分辨）与Sentinel-2（高分辨、较低时效）多源多分辨率数据进行变化检测与特征融合，生成高时空分辨率的烧伤区地图。

Result: 在小尺度火情检测上也能取得高准确度，整体性能超过同类变化检测模型和稳健基线。

Conclusion: 多源多分辨率融合能兼顾时效与精度，为火灾后快速制图提供可行方案；方法可操作性强且资源开源，便于复现与应用。

Abstract: Delineating wildfire affected areas using satellite imagery remains challenging due to irregular and spatially heterogeneous spectral changes across the electromagnetic spectrum. While recent deep learning approaches achieve high accuracy when high-resolution multispectral data are available, their applicability in operational settings, where a quick delineation of the burn scar shortly after a wildfire incident is required, is limited by the trade-off between spatial resolution and temporal revisit frequency of current satellite systems. To address this limitation, we propose a novel deep learning model, namely BAM-MRCD, which employs multi-resolution, multi-source satellite imagery (MODIS and Sentinel-2) for the timely production of detailed burnt area maps with high spatial and temporal resolution. Our model manages to detect even small scale wildfires with high accuracy, surpassing similar change detection models as well as solid baselines. All data and code are available in the GitHub repository: https://github.com/Orion-AI-Lab/BAM-MRCD.

</details>


### [46] [BrainSegNet: A Novel Framework for Whole-Brain MRI Parcellation Enhanced by Large Models](https://arxiv.org/abs/2601.09263)
*Yucheng Li,Xiaofan Wang,Junyi Wang,Yijie Li,Xi Zhu,Mubai Du,Dian Sheng,Wei Zhang,Fan Zhang*

Main category: cs.CV

TL;DR: 提出BrainSegNet：在SAM基础上加入U-Net跳连、多尺度注意与边界细化，实现对MRI全脑95区高精度分割，在HCP上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 全脑分割需将复杂且形状不规则的小脑区精细划分，传统配准慢且精度受限；通用大模型如SAM可迁移但缺乏脑分割所需的精细度，因此需要兼具速度与高精度的专用框架。

Method: 在SAM的编码-解码架构上做定制化改造：1) 混合编码器：将U-Net式跳跃连接与SAM的Transformer块结合以保留细粒度与全局上下文；2) 多尺度注意解码器：引入金字塔池化以适应不同尺度结构；3) 边界细化模块：专注边缘锐化与边界一致性；面向95个脑区的多标签分割。

Result: 在HCP数据集上，相比多种SOTA方法取得更高精度与鲁棒性，尤其在复杂、多标签的脑区划分任务中表现突出（摘要未给出具体数值）。

Conclusion: 适配与增强SAM可以实现对MRI全脑分割的高精度与高鲁棒性；BrainSegNet为小而不规则结构的多标签脑分割提供了有效范式。

Abstract: Whole-brain parcellation from MRI is a critical yet challenging task due to the complexity of subdividing the brain into numerous small, irregular shaped regions. Traditionally, template-registration methods were used, but recent advances have shifted to deep learning for faster workflows. While large models like the Segment Anything Model (SAM) offer transferable feature representations, they are not tailored for the high precision required in brain parcellation. To address this, we propose BrainSegNet, a novel framework that adapts SAM for accurate whole-brain parcellation into 95 regions. We enhance SAM by integrating U-Net skip connections and specialized modules into its encoder and decoder, enabling fine-grained anatomical precision. Key components include a hybrid encoder combining U-Net skip connections with SAM's transformer blocks, a multi-scale attention decoder with pyramid pooling for varying-sized structures, and a boundary refinement module to sharpen edges. Experimental results on the Human Connectome Project (HCP) dataset demonstrate that BrainSegNet outperforms several state-of-the-art methods, achieving higher accuracy and robustness in complex, multi-label parcellation.

</details>


### [47] [GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials](https://arxiv.org/abs/2601.09265)
*Bei Huang,Yixin Chen,Ruijie Lu,Gang Zeng,Hongbin Zha,Yuru Pei,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出GaussianFluent：把3D高斯点渲染与脆性断裂物理模拟统一，生成物体内部体素级一致纹理并实时高保真渲染、快速断裂。


<details>
  <summary>Details</summary>
Motivation: 现有把物理与3DGS结合多聚焦软体变形，难以处理脆裂；原因是GS缺少具有一致纹理的体积内部表示，且无面向高斯的断裂模拟方法。

Method: 两部分：1) 通过生成模型引导的内部高斯加密，合成结构一致且写实的物体内部纹理；2) 将优化过的连续损伤MPM（CD-MPM）融入高斯框架，实现高效脆性断裂模拟；可处理混合材料与多阶段裂纹传播，并保持与3DGS实时渲染耦合。

Result: 在复杂场景下实现写实、实时的渲染与断裂效果，呈现混合材料和多阶段裂纹传播；速度远高于以往方法，且内部结构与纹理连贯，达到以前方法难以实现的效果。

Conclusion: GaussianFluent为动态对象状态提供统一的模拟与渲染框架，补齐3DGS在脆裂方面短板，具备VR、机器人等下游应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a prominent 3D representation for high-fidelity and real-time rendering. Prior work has coupled physics simulation with Gaussians, but predominantly targets soft, deformable materials, leaving brittle fracture largely unresolved. This stems from two key obstacles: the lack of volumetric interiors with coherent textures in GS representation, and the absence of fracture-aware simulation methods for Gaussians. To address these challenges, we introduce GaussianFluent, a unified framework for realistic simulation and rendering of dynamic object states. First, it synthesizes photorealistic interiors by densifying internal Gaussians guided by generative models. Second, it integrates an optimized Continuum Damage Material Point Method (CD-MPM) to enable brittle fracture simulation at remarkably high speed. Our approach handles complex scenarios including mixed-material objects and multi-stage fracture propagation, achieving results infeasible with previous methods. Experiments clearly demonstrate GaussianFluent's capability for photo-realistic, real-time rendering with structurally consistent interiors, highlighting its potential for downstream application, such as VR and Robotics.

</details>


### [48] [Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain](https://arxiv.org/abs/2601.09298)
*Lianying Chao,Haoran Cai,Xubin Li,Kai Zhang,Sijie Wu,Rui Xu*

Main category: cs.CV

TL;DR: 提出在ICT领域训练域专用图像描述模型（DICModel）的多阶段渐进式训练策略，并配套标准评测体系。通过合成与专家标注数据进行三阶段SFT，使7B参数模型在BLEU与客观题准确率上超越多款32B SOTA多模态模型。


<details>
  <summary>Details</summary>
Motivation: ICT领域的高价值知识分布在文本与图像两种模态。现有做法要么只能解析文本、缺乏图像描述能力，要么是通用MLLM能看图但缺少ICT领域知识，导致域内图像（如网络架构、协议流程、拓扑图）难以被准确转写为高质量文本，从而限制了构建检索增强与域内LLM的知识供给。

Method: 提出多阶段渐进训练：1) 使用Mermaid与LLM合成约7K图文对进行第一阶段SFT；2) 由ICT专家手动标注约2K图文对进行第二阶段SFT，注入高置信域知识；3) 专家与LLM联合合成约1.5K视觉问答数据，进行指令式SFT以提升指令遵循与知识问答能力。同时构建标准评测体系（含BLEU与专家设计的客观题）。模型规模为7B。

Result: DICModel（7B）在BLEU上相较7B与32B SOTA分别提升约56.8%与20.8%；在专家构造的客观题上，准确率比Qwen2.5-VL 32B高1%。

Conclusion: 通过少量高质域数据与多阶段SFT，可让小规模（7B）域专用图像描述模型在ICT任务上超越更大通用MLLM，能高效准确地从图像抽取逻辑文本，潜在加速ICT多模态应用与RAG/域LLM的知识获取。

Abstract: In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.

</details>


### [49] [Frequency Error-Guided Under-sampling Optimization for Multi-Contrast MRI Reconstruction](https://arxiv.org/abs/2601.09316)
*Xinming Fang,Chaoyan Huang,Juncheng Li,Jun Wang,Jun Shi,Guixu Zhang*

Main category: cs.CV

TL;DR: 提出一种频域误差引导的多对比MRI加速重建框架：用条件扩散模型学习频域误差先验，联合优化采样轨迹与重建网络，结合可解释的深度展开、空间对齐与参考特征分解，在多模态与多加速率下优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多对比MRI重建常见问题：参考融合浅层（如简单拼接）、未充分利用参考对比的互补信息、采样模式固定难以适配不同场景，导致重建质量与鲁棒性受限。

Method: 1) 训练条件扩散模型，学习目标对比的Frequency Error Prior（FEP）。2) 将FEP嵌入统一框架，联合优化k-space欠采样模式与重建网络。3) 重建采用模型驱动的深度展开，融合频域与图像域信息。4) 设计空间对齐模块与参考特征分解策略，使参考信息对齐目标并区分共享/特异成分，提升可解释性与效果。

Result: 在多种成像模态、不同加速率（4–30x）与采样方案下，方法在客观指标与视觉质量上均持续超越SOTA；展示了更稳定、更高保真度的细节恢复与伪影抑制。

Conclusion: 频域误差先验与联合优化采样-重建的可解释深度展开框架能更充分挖掘参考互补信息，并自适应采样，实现更优多对比MRI重建；方法通用、可解释且效果稳定，代码已开源。

Abstract: Magnetic resonance imaging (MRI) plays a vital role in clinical diagnostics, yet it remains hindered by long acquisition times and motion artifacts. Multi-contrast MRI reconstruction has emerged as a promising direction by leveraging complementary information from fully-sampled reference scans. However, existing approaches suffer from three major limitations: (1) superficial reference fusion strategies, such as simple concatenation, (2) insufficient utilization of the complementary information provided by the reference contrast, and (3) fixed under-sampling patterns. We propose an efficient and interpretable frequency error-guided reconstruction framework to tackle these issues. We first employ a conditional diffusion model to learn a Frequency Error Prior (FEP), which is then incorporated into a unified framework for jointly optimizing both the under-sampling pattern and the reconstruction network. The proposed reconstruction model employs a model-driven deep unfolding framework that jointly exploits frequency- and image-domain information. In addition, a spatial alignment module and a reference feature decomposition strategy are incorporated to improve reconstruction quality and bridge model-based optimization with data-driven learning for improved physical interpretability. Comprehensive validation across multiple imaging modalities, acceleration rates (4-30x), and sampling schemes demonstrates consistent superiority over state-of-the-art methods in both quantitative metrics and visual quality. All codes are available at https://github.com/fangxinming/JUF-MRI.

</details>


### [50] [Beyond the final layer: Attentive multilayer fusion for vision transformers](https://arxiv.org/abs/2601.09322)
*Laure Ciernik,Marco Morik,Lukas Thede,Luca Eyring,Shinichi Nakajima,Zeynep Akata,Lukas Muttenthaler*

Main category: cs.CV

TL;DR: 提出一种“注意力探测(Attentive Probing)”方法，融合ViT全层表征做线性探测式适配，较仅用末层的线性探测在20个数据集与多种预训练模型上显著提升，尤其对与预训练域差异大的任务收益最大。


<details>
  <summary>Details</summary>
Motivation: 线性探测因高效而常用，但通常只用最后一层特征；然而任务相关信息在网络层级中分布广泛，单一末层难以充分表达，特别在下游任务与预训练域差异较大时。需要一种既保留线性探测效率、又能利用多层信息的适配方法。

Method: 在冻结骨干(如ViT)的前提下，从所有层提取表示，通过可学习的注意力权重对各层特征进行动态加权融合，自动识别目标任务最相关的层，将低层结构线索与高层语义抽象结合；随后用轻量头进行分类/回归，实现“探测式”适配。并可视化注意力热力图分析层重要性。

Result: 在20个多样数据集与多种预训练基础模型上，相比标准仅用末层的线性probe取得稳定且可观的性能提升；可视化显示，当下游任务与预训练域相差越大，中间层贡献越大。

Conclusion: 中间层蕴含重要且互补的信息，基于注意力的跨层融合是一种原则性、任务感知的探测式适配途径，能在保持计算高效的同时显著提升下游性能。

Abstract: With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.

</details>


### [51] [See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval](https://arxiv.org/abs/2601.09350)
*Mingyu Jeon,Sungjin Han,Jinkwon Hwang,Minchol Kwon,Jonghee Kim,Junyeong Kim*

Main category: cs.CV

TL;DR: SMORE提出一种“看得更多、存得更少”的视频片段检索框架，通过语义对齐与自适应压缩，在不超内存的前提下保持高信息分辨率，达到多数据集SOTA。


<details>
  <summary>Details</summary>
Motivation: MLLM在图像理解上进步显著，但视频任务受限于内存——密集抽帧成本高、稀疏抽帧易丢信息，尤其是长视频中的关键片段易遗漏。需要一种既高效又信息保真的视频表示方式来支持VMR。

Method: SMORE包含三点：1) 查询引导字幕生成，将视频表示与用户意图对齐；2) 查询感知的重要性调制，突出与查询相关的时间段；3) 自适应帧压缩，在保证关键语义的同时减少冗余，从而在固定内存预算下处理更多内容。

Result: 在QVHighlights、Charades-STA、ActivityNet-Captions三个基准上取得SOTA表现，证明在内存受限条件下仍能高效准确地进行视频时刻检索。

Conclusion: 通过将查询对齐的语义编码、相关性加权与自适应压缩结合，SMORE有效缓解了视频长时序与内存预算的矛盾，实现高效且精确的VMR，可扩展至其他视频理解任务。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.

</details>


### [52] [Spectral Complex Autoencoder Pruning: A Fidelity-Guided Criterion for Extreme Structured Channel Compression](https://arxiv.org/abs/2601.09352)
*Wei Liu,Xing Deng,Haijian Shao,Yingtao Jiang*

Main category: cs.CV

TL;DR: 提出SCAP：用频域复杂交互场的自编码重建保真度来评估卷积层通道冗余，保真高则可剪，保真低保留；在VGG16/CIFAR-10上以固定阈值0.6实现90.11% FLOPs与96.30%参数压缩，精度仅降1.67%。


<details>
  <summary>Details</summary>
Motivation: 现有通道剪枝多依赖权值范数或数据/梯度敏感性，难以精确反映功能冗余，尤其在激进压缩下易损性能。作者希望构造能刻画单通道与多通道输入之间功能关系的表征，并以可训练但容量受限的模型来度量“可压缩性”，实现结构一致、阈值可控的通道级剪枝。

Method: 对每个卷积层、每个输出通道：将多通道输入激活作为复数实部，将该通道的输出激活（在空间上对齐并在输入通道维广播）作为虚部，形成“复杂交互场”；做频域变换（FFT），对归一化频谱训练低容量自编码器以重建；用每个通道的频谱重建误差（保真度）作为通道重要性（可与滤波器L1范数融合）；按阈值剪枝，得到结构一致的子网络，并进行微调。

Result: 在VGG16上（CIFAR-10），固定阈值0.6时：FLOPs减少90.11%，参数减少96.30%，微调后Top-1从93.44%降至91.77%（绝对下降1.67%）。显示该频域重建保真度能有效作为通道冗余代理，支持高比例压缩。

Conclusion: 频域“复杂交互场”的自编码重建保真度可度量通道的可压缩性，简单阈值即可执行稳定的结构化剪枝；在激进压缩下仍保持较小精度损失，证明方法有效。

Abstract: We propose Spectral Complex Autoencoder Pruning (SCAP), a reconstruction-based criterion that measures functional redundancy at the level of individual output channels. For each convolutional layer, we construct a complex interaction field by pairing the full multi-channel input activation as the real part with a single output-channel activation (spatially aligned and broadcast across input channels) as the imaginary part. We transform this complex field to the frequency domain and train a low-capacity autoencoder to reconstruct normalized spectra. Channels whose spectra are reconstructed with high fidelity are interpreted as lying close to a low-dimensional manifold captured by the autoencoder and are therefore more compressible; conversely, channels with low fidelity are retained as they encode information that cannot be compactly represented by the learned manifold. This yields an importance score (optionally fused with the filter L1 norm) that supports simple threshold-based pruning and produces a structurally consistent pruned network. On VGG16 trained on CIFAR-10, at a fixed threshold of 0.6, we obtain 90.11% FLOP reduction and 96.30% parameter reduction with an absolute Top-1 accuracy drop of 1.67% from a 93.44% baseline after fine-tuning, demonstrating that spectral reconstruction fidelity of complex interaction fields is an effective proxy for channel-level redundancy under aggressive compression.

</details>


### [53] [Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process](https://arxiv.org/abs/2601.09410)
*Sangjun Han,Youngmi Hur*

Main category: cs.CV

TL;DR: 论文提出两种增强高频细节的超分辨方法：基于拉普拉斯金字塔的细节损失，以及重复上采样-下采样以放大细节学习信号；在 CNN 和注意力模型上均带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 超分辨的关键在于恢复高频细节（边缘、纹理）。现有方法对高频关注不足或难以稳定优化，导致超分结果细节不充分。作者希望通过专门的损失与特征操作，让模型更聚焦高频成分，从而提升视觉质量与指标。

Method: 1) 细节损失：利用拉普拉斯金字塔分离高频，构造“总损失=普通SR重建损失+细节损失”，并将SR图与细节图分开生成与控制，使模型强化高频重建。2) 重复上/下采样：在特征层进行多次上采样和下采样，挖掘与放大来自多个低分辨特征的多样信息，从而增强细节损失的有效性。3) 实验设置：a) 设计一个整合上述方法的CNN模型；b) 将细节损失小规模移植到现有注意力模型中进行验证。

Result: 自研CNN模型达到SOTA，超过所有已公开的CNN方法，甚至优于部分注意力模型；在多个注意力模型上添加细节损失后均有增益。

Conclusion: 基于拉普拉斯金字塔的细节损失与重复上/下采样可有效强化高频学习，提升超分辨质量，且对不同架构（CNN与注意力）均适用。

Abstract: With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.

</details>


### [54] [Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification](https://arxiv.org/abs/2601.09416)
*Yaxi Chen,Zi Ye,Shaheer U. Saeed,Oliver Yu,Simin Ni,Jie Huang,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文针对骨肉瘤新辅助化疗后病理切片中“可存活/不可存活肿瘤”判定难题，提出将放射组学特征与图像特征联合输入，并引入分层（二层二分类）与可训练权重的层级损失，以提升跨患者测试集的分类表现与可解释性，在TCIA数据集上达成新的SOTA，代码可复现。


<details>
  <summary>Details</summary>
Motivation: 以往方法在切片小块(tile)层面表现好，但在按患者独立采样的测试集上显著掉点，说明泛化与临床可用性不足；同时，三分类“平lat”建模忽视了类别层级关系，且图像深度特征可解释性有限。

Method: 1) 多模态输入：在常规病理图像深度特征之外，提取并融合放射组学特征，用于训练分类器；2) 任务分解：将问题拆为层级的两个二分类（肿瘤/非肿瘤；可存活/不可存活），设计带可训练权重的层级损失，联合优化两任务；3) 在TCIA OS Tumor Assessment 数据集上对单独与组合策略进行实验评估。

Result: 与仅用图像特征和/或平三分类基线相比，加入放射组学特征与层级损失后，在独立患者级测试集上总体与各类别(尤其是可存活/不可存活)性能显著提升；两种方法组合达最佳，并被认为刷新该公开数据集上的SOTA。

Conclusion: 放射组学-病理图像融合可增强模型判别力与可解释性；利用类别层级结构并通过可训练权重联合优化，可显著提升跨患者泛化性能。该策略在骨肉瘤坏死定量任务上有效，具有临床转化潜力，代码与模型已开放。

Abstract: Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.

</details>


### [55] [Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs](https://arxiv.org/abs/2601.09430)
*Rui Zhu,Xin Shen,Shuchen Wu,Chenxi Miao,Xin Yu,Yang Li,Weikang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CV

TL;DR: 提出Video-MSR基准，系统评测多跳空间推理；揭示现有MLLM在视频多步空间推理上的显著短板；提供MSR-9K指令数据并微调Qwen-VL，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准多为单步感知到判断，缺少对视频场景中复杂多跳空间逻辑链能力的评估，难以反映模型在真实动态情境下的空间推理水平。

Method: 构建Video-MSR视频多跳空间推理基准，涵盖受限定位、链式指代检索、路径规划、反事实物理推断四类任务；通过可扩展、视觉对齐的生成流程结合人工严格校验，得到3052个视频实例与4993个问答；评测20个前沿MLLM，分析错误模式；进一步构造MSR-9K指令调优数据，对Qwen-VL进行微调。

Result: 在Video-MSR上，现有MLLM在表层感知尚可，但在多跳空间推理上显著掉分，常出现空间迷失与多步推理幻觉；使用MSR-9K微调的Qwen-VL在该基准上取得+7.82%的绝对提升。

Conclusion: 多跳空间指令数据能有效增强MLLM的MSR能力；Video-MSR提供了评测与推动该方向研究的关键基准与数据资源。

Abstract: Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.

</details>


### [56] [Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?](https://arxiv.org/abs/2601.09433)
*David Reid,Ognjen Arandjelovic*

Main category: cs.CV

TL;DR: 论文将视觉Transformer（ViT）用于古钱币图像的语义要素识别，并与卷积神经网络（CNN）对比，ViT在准确率上更优；方法利用图像与非结构化文本的多模态自动学习。


<details>
  <summary>Details</summary>
Motivation: 自动化分析古钱币可从大规模馆藏与市场交易中提取更多历史信息并辅助鉴定与理解，但现有方法多依赖CNN，且对多模态（图像+文本）与新架构（如ViT）的适配与评估不足。

Method: 综述相关工作；构建基于ViT与CNN的分类/检测模型，对古钱币图像进行语义要素（如图案、符号）识别；引入多模态自动学习，将图像与非结构化文本联合训练；比较不同模型训练与实现细节并进行性能评测。

Result: 在相同任务与数据设定下，ViT模型在识别准确率上优于重新训练的CNN基线；展示了多模态自动学习的可行性。

Conclusion: ViT在古钱币语义要素识别上优于CNN，表明Transformer架构和多模态策略适合该领域；为后续研究与应用（馆藏数字化、收藏市场辅助识别）提供方向。

Abstract: Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.

</details>


### [57] [PrivLEX: Detecting legal concepts in images through Vision-Language Models](https://arxiv.org/abs/2601.09449)
*Darya Baranouskaya,Andrea Cavallaro*

Main category: cs.CV

TL;DR: PrivLEX提出一种以法律定义的个人数据概念为依据、可解释的图像隐私分类器，利用零样本VLM概念检测与无标签概念瓶颈，实现无需显式概念标注的隐私判定，并分析人类对概念敏感度的感知。


<details>
  <summary>Details</summary>
Motivation: 现有图像隐私分类器要么缺乏可解释性，要么与法律规定的“个人数据”概念脱节；同时获取概念级标注成本高。需要一种既与法律对齐又能在缺少概念标注下工作的模型，并能揭示人类对不同隐私概念的敏感度。

Method: 构建PrivLEX：将视觉语言模型的零样本概念检测作为输入，通过无标签的Concept Bottleneck Model把“法律定义的个人数据概念”作为中间语义层，依据这些概念的存在与敏感度进行图像隐私分类；无需在训练阶段提供显式概念标签。

Result: PrivLEX能够在图像中识别法律相关的个人数据概念，并据此进行可解释的隐私分类；同时给出对各概念在人类标注者眼中的敏感度分析。

Conclusion: 以法律概念为锚的、可解释的隐私分类是可行的；借助VLM的零样本能力与无标签概念瓶颈，可在缺乏概念标注下实现有效分类，并为隐私敏感度提供有意义的人类感知分析。

Abstract: We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.

</details>


### [58] [MAD: Motion Appearance Decoupling for efficient Driving World Models](https://arxiv.org/abs/2601.09452)
*Ahmad Rahimi,Valentin Gerard,Eloi Zablocki,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出一种将通用视频扩散模型高效适配为可控驾驶世界模型的两阶段方法：先学“骨架级”运动，再用同一骨干渲染真实外观；以极低算力达到或超越现有SOTA，并支持文本、车体（ego）、目标级控制。


<details>
  <summary>Details</summary>
Motivation: 通用视频扩散模型虽能生成逼真且时间一致的视频，但在自动驾驶所需的结构化运动、物理一致交互方面不足。现有将其适配到驾驶领域的方法依赖大量特定数据与昂贵微调，缺乏高效可控的世界模型。

Method: 解耦运动与外观的两阶段适配：1) 将模型先适配为预测“骨架化”代理与场景元素的视频，仅学习物理与社交上合理的动态；2) 复用同一骨干，在给定运动序列条件下合成真实RGB视频，相当于给运动“穿衣”（纹理与光照）。该流程对应“推理-渲染”范式，并支持文本、ego与目标级控制。

Result: 在SVD上适配时，以不到6%的计算量达到既有SOTA；扩展到LTX后形成MAD-LTX，性能优于所有开源竞品，并提供全面控制能力。

Conclusion: 解耦运动与外观的适配框架以极低监督和算力将通用视频扩散模型转化为可控驾驶世界模型，兼顾物理合理性与外观逼真度，具备良好可扩展与控制性。

Abstract: Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively "dressing" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/

</details>


### [59] [Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity](https://arxiv.org/abs/2601.09497)
*Ritabrata Chakraborty,Hrishit Mitra,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 论文研究跨数据集目标检测（CD-OD），将数据集按“场景无关（多样日常）”与“场景特定（狭窄环境）”分组，系统评测标准检测器在各训练-测试对上的迁移。结论：同类场景内迁移较稳健，跨类型迁移显著下降且常不对称，尤其从特定→无关最差；即使采用开放标签对齐亦仅带来有限改进，说明域移位是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有检测器在分布内表现优秀，但跨数据集性能锐减。既有研究往往混合了标签集合不匹配与域移位两种因素，缺少一个系统化框架来刻画不同数据集“设置/场景特性”对迁移的影响。作者希望用“设置特异性”视角，拆解并量化CD-OD中的结构性失败模式，并提供更公平的评估协议。

Method: 1) 数据集分组：将检测基准按“场景无关（通用、日常多样）”与“场景特定（某一窄域）”划分；2) 模型与评测：选用一类标准检测器家族，在所有训练-测试组合上评估；3) 封闭标签与开放标签两种协议：封闭标签在共享类别上评估；开放标签利用CLIP相似度将预测类别映射到目标数据集中最相近的标签，从而部分缓解标签集合不一致；4) 分析迁移结构、对称性与失败案例。

Result: - 同类型（无关→无关或特定→特定）迁移相对稳定；- 跨类型迁移显著下滑且常不对称，特定→无关最严重；- 开放标签评估带来一致但有限的提升，许多改正来自语义近邻（near-miss）；- 即使开放标签对齐后，最困难情形仍受域移位主导。

Conclusion: 以设置特异性为视角，CD-OD呈现清晰结构：场景差异导致的域移位是主因，标签不匹配虽可通过开放标签部分缓解，但改进有限。论文据此给出在分布移位下评估检测器的实践建议，并发布代码以复现研究。

Abstract: Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.

</details>


### [60] [V-DPM: 4D Video Reconstruction with Dynamic Point Maps](https://arxiv.org/abs/2601.09499)
*Edgar Sucar,Eldar Insafutdinov,Zihang Lai,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 论文提出V-DPM，将动态点图从图像对扩展到视频，借助VGGT并用少量合成数据适配，实现端到端预测动态场景的3D形状、相机与逐点3D运动，在3D/4D重建上达SOTA，无需多视几何后优化。


<details>
  <summary>Details</summary>
Motivation: 现有DPM仅支持图像对，且多视时需后期优化；而实际应用多为视频。需要一种对视频友好的DPM表征，既能最大化表达力、便于神经网络预测，又能复用现有预训练模型。

Method: 1) 重新表述DPM用于视频：在时间轴上同时编码不变点图（3D形状与相机）与逐点运动场，使之适合神经网络直接回归并可复用既有模型；2) 基于VGGT实现：以其为骨干，将静态训练的VGGT用少量合成动态数据进行适配，使其输出视频DPM（包含深度/点位与全场3D运动）。

Result: 在动态场景的3D与4D重建任务上达SOTA表现；相较于VGGT动态扩展P3等方法，不仅预测动态深度，还恢复场景中每个点的完整3D运动。

Conclusion: V-DPM将DPM从图像对推广到视频，结合VGGT并少量合成数据适配，实现端到端的高精度动态3D/4D重建，避免多视后优化，并提供逐点的完整3D运动估计。

Abstract: Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.

</details>


### [61] [Video Joint-Embedding Predictive Architectures for Facial Expression Recognition](https://arxiv.org/abs/2601.09524)
*Lennart Eing,Cristina Luna-Jiménez,Silvan Mertes,Elisabeth André*

Main category: cs.CV

TL;DR: 论文将视频联合嵌入预测架构（V-JEPA）用于面部表情识别，通过预测被遮挡区域的嵌入而非重建像素，提升表示的任务相关性；在RAVDESS与CREMA-D上取得SOTA/领先表现，并具备良好跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 传统视频预训练多依赖像素级重建，易学习到与任务无关（如背景颜色）的低层细节，影响FER的鲁棒性与泛化。作者希望用纯嵌入级预测的自监督预训练获得更抽象、更任务相关的表征，从而提升FER性能与跨域泛化。

Method: 采用预训练的V-JEPA视频编码器：给定视频，将部分区域遮挡，仅用未遮挡区域的嵌入去预测被遮挡区域的嵌入，无需像素重建。随后在RAVDESS与CREMA-D上冻结编码器或配以浅层分类器进行监督微调/训练，并进行跨数据集评测。

Result: 在RAVDESS上达到SOTA；在CREMA-D上优于所有视觉方法（WAR提升+1.48）。跨数据集实验显示强泛化能力。代码开源于仓库链接。

Conclusion: 纯嵌入级预测的预训练（V-JEPA）能过滤与任务无关信息，学到稳健的视频表征，显著提升FER并具备跨数据集泛化潜力，值得在表情识别等视频理解任务中推广。

Abstract: This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.

</details>


### [62] [GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection](https://arxiv.org/abs/2601.09528)
*Alfio Spoto,Rosario Leonardi,Francesco Ragusa,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: 提出用于工业安全领域的第一人称人-物交互(EHOI)数据增广与识别方案：用扩散模型把逼真的PPE叠加到真实图像上，构建GlovEgo-HOI数据集，并提出利用手部姿态的GlovEgo-Net，实验证明方法有效并公开资源。


<details>
  <summary>Details</summary>
Motivation: 工业场景EHOI对安全关键，但缺乏带标注、贴合工业PPE场景的数据，限制了鲁棒模型训练与泛化。

Method: 1) 数据生成：将合成数据与扩散式图像编辑结合，把逼真的PPE（如手套、头盔等）无缝融入真实第一人称图像以做增广；2) 数据集：发布面向工业EHOI的GlovEgo-HOI基准；3) 模型：提出GlovEgo-Net，包含Glove-Head与Keypoint-Head模块，显式利用手部关键点/姿态信息以提升交互检测。

Result: 在广泛实验中，基于扩散增广的数据与GlovEgo-Net在EHOI检测上优于基线，显示更强的交互识别与泛化能力。

Conclusion: 扩散驱动的PPE增广与手姿态感知的模型可有效缓解工业EHOI数据稀缺，提升检测性能；同时公开数据集、流水线与预训练模型以促进后续研究。

Abstract: Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.

</details>


### [63] [Bipartite Mode Matching for Vision Training Set Search from a Hierarchical Data Server](https://arxiv.org/abs/2601.09531)
*Yue Yao,Ruining Yang,Tom Gedeon*

Main category: cs.CV

TL;DR: 提出分层数据服务器与二部匹配(BMM)对齐源/目标模式，搜索更贴近目标域的训练集，在re-ID与检测上缩小域差并提升精度，且可与现有UDA如伪标签互补叠加收益。


<details>
  <summary>Details</summary>
Motivation: 目标域可访问但无法实时标注，希望从大规模数据服务器中搜出替代训练集。若训练集缺失目标域的语义模式/簇，模型性能受损；现有方法多改进算法流程而忽视数据服务器结构本身。

Method: 构建层级化数据服务器(树状索引)，提出二部模式匹配算法BMM：将目标域的模式(语义簇)与服务器中的源模式进行一对一匹配；在服务器数据树中为每个目标模式寻找最优模式(规模可大可小)，通过二部图最优匹配实现整体最优的模式对齐，生成匹配的训练子集。

Result: 与现有训练集搜索法相比，所匹配的服务器模式与目标域的域间距更小；在行人/车辆re-ID与目标检测等任务上，使用该训练集训练的模型精度更高。

Conclusion: BMM提供数据中心的无监督域自适应路径，与模型中心的UDA方法正交；与伪标签等UDA结合可进一步提升性能，验证其通用性与可叠加性。

Abstract: We explore a situation in which the target domain is accessible, but real-time data annotation is not feasible. Instead, we would like to construct an alternative training set from a large-scale data server so that a competitive model can be obtained. For this problem, because the target domain usually exhibits distinct modes (i.e., semantic clusters representing data distribution), if the training set does not contain these target modes, the model performance would be compromised. While prior existing works improve algorithms iteratively, our research explores the often-overlooked potential of optimizing the structure of the data server. Inspired by the hierarchical nature of web search engines, we introduce a hierarchical data server, together with a bipartite mode matching algorithm (BMM) to align source and target modes. For each target mode, we look in the server data tree for the best mode match, which might be large or small in size. Through bipartite matching, we aim for all target modes to be optimally matched with source modes in a one-on-one fashion. Compared with existing training set search algorithms, we show that the matched server modes constitute training sets that have consistently smaller domain gaps with the target domain across object re-identification (re-ID) and detection tasks. Consequently, models trained on our searched training sets have higher accuracy than those trained otherwise. BMM allows data-centric unsupervised domain adaptation (UDA) orthogonal to existing model-centric UDA methods. By combining the BMM with existing UDA methods like pseudo-labeling, further improvement is observed.

</details>


### [64] [Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling](https://arxiv.org/abs/2601.09566)
*Shuyang Xiang,Hao Guan*

Main category: cs.CV

TL;DR: 用极低分辨率（最低8×8像素）的汉字灰度图像替代离散token ID作为解码器输入，中文字符级建模准确率达39.2%，与索引式基线39.1%相当；且在小样本早期训练中显著“热启动”，0.4%训练量即达12%+，而索引式低于6%。表明极简视觉结构可作为稳健高效信号，补充传统索引表示。


<details>
  <summary>Details</summary>
Motivation: 汉字为表意/形声为主的表语文字，字形蕴含语义与读音线索；而主流LLM将其当作不含结构的信息ID，可能忽视了可用于预测的视觉结构信息。作者想探究：是否可以用极低成本的字形视觉输入替代索引token来进行字符级语言建模，并检验这种表示在数据/训练早期是否更高效。

Method: 将字符不再编码为离散ID，而是将单字渲染为低分辨率灰度图（如8×8），输入到解码式语言模型；与传统索引式字符建模在相同设置下对比，评估整体准确率与训练初期（极低训练比例）性能，即“热启动”效应。

Result: 在8×8等低分辨率条件下，视觉输入模型取得39.2%准确率，几乎与索引基线39.1%持平；在仅0.4%总训练量时，视觉模型准确率超过12%，而索引式低于6%，显示明显的早期优势。

Conclusion: 极简的字形视觉结构已足以提供有效且稳健的信号，能在中文字符级语言建模中匹配传统索引表示，并在低资源/早期训练阶段显著更高效。视觉表示为汉字表征提供了有价值的替代和互补视角。

Abstract: Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \times 8$ pixels. Remarkably, these inputs achieve 39.2\% accuracy, comparable to the index-based baseline of 39.1\%. Such low-resource settings also exhibit a pronounced \emph{hot-start} effect: by 0.4\% of total training, accuracy reaches above 12\%, while index-based models lag at below 6\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.

</details>


### [65] [Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model](https://arxiv.org/abs/2601.09572)
*Tianli Tao,Ziyang Wang,Delong Yang,Han Zhang,Le Zhang*

Main category: cs.CV

TL;DR: DF-DiffCom是一种结合KAN的扩散模型，通过引入形变场作为指导来补全纵向脑MRI缺失时间点，在OASIS-3上优于现有方法（PSNR提升5.6%，SSIM提升0.12），且对模态不敏感，可扩展到不同MRI模态与分割属性图。


<details>
  <summary>Details</summary>
Motivation: 纵向MRI研究常因受试者流失导致时间点缺失，传统仅依赖像素强度的生成模型可信度不足，且指导方式固定、适应性差，难以支持多模态与多场景的灵活应用。

Method: 提出DF-DiffCom：在扩散模型中引入Kolmogorov-Arnold Networks（KAN）增强表示能力，并以形变场（deformation fields）作为核心条件与结构先验，利用跨时间点的空间对应关系进行生成与补全；模型设计为模态无关，可直接迁移到不同MRI模态与属性图。

Result: 在OASIS-3纵向数据集上，DF-DiffCom较SOTA取得更高重建质量，PSNR提升5.6%，SSIM提升0.12，展示更可信的补全结果与更好的下游可用性。

Conclusion: 形变场驱动、KAN增强的扩散框架可显著提升纵向脑影像补全的可信度和灵活性，适用于多模态MRI及分割等属性图，为寿命期研究中缺失数据问题提供实用方案。

Abstract: Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.

</details>


### [66] [OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2601.09575)
*Sheng-Yu Huang,Jaesung Choe,Yu-Chiang Frank Wang,Cheng Sun*

Main category: cs.CV

TL;DR: OpenVoxel 是一种无需训练的稀疏体素分组与描述算法，基于SVR重建的场景体素，借助VLM/MLLM进行文本到文本检索与分组标注，支持开放词汇3D理解（如OVS/RES），在复杂RES上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D场景理解往往依赖训练、特定文本编码器（如CLIP/BERT）或难以处理复杂指代表达；需要一种无需额外训练、可直接对稀疏体素进行语义分组与标注、并能在复杂语言条件下稳健工作的方案。

Method: 输入为由多视图图像经SVR得到的稀疏体素。算法无训练，先在体素层面进行聚类/分组以得到候选对象组；再调用强大的VLM与MLLM对每个组进行多模态描述与文本到文本匹配，实现组级别的语义标注与检索；由此构建场景语义地图，支持开放词汇分割与指代表达分割。与以往不同，不引入CLIP/BERT嵌入，直接用MLLM进行文本到文本搜索。

Result: 在广泛实验中，相比近期工作，尤其在复杂的指代表达分割（RES）任务上取得更优性能；同时维持训练零开销的优势。

Conclusion: OpenVoxel展示了利用VLM/MLLM在稀疏体素上实现训练自由的分组与字幕生成的有效性，能够构建信息丰富的3D场景语义地图，提升开放词汇3D任务（OVS/RES）表现，并将释出代码。

Abstract: We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.

</details>


### [67] [Show, don't tell -- Providing Visual Error Feedback for Handwritten Documents](https://arxiv.org/abs/2601.09586)
*Said Yasin,Torsten Zesch*

Main category: cs.CV

TL;DR: 论文关注为手写文档提供可视化错误反馈这一较少研究但重要的任务，比较了模块化与端到端系统，结论是目前整体质量尚不可接受，并提出关键挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 手写在教育等场景仍然关键，但从手写输入图像到正确定位且信息量足的错误反馈存在巨大鸿沟；现有文献对“精确放置的反馈”关注不足，亟需系统性评估与方法探索。

Method: 定义从手写图像到错误反馈定位与呈现的任务，梳理流程中的关键环节与难点；实现并实证比较两类系统：将识别、布局分析、对齐与错误检测分开的模块化流水线，与直接从图像到反馈的端到端模型；评估整体质量并分析误差来源。

Result: 两类方案在总体质量上均未达到可接受水平；误差在多环节累积，主要瓶颈包括手写识别鲁棒性不足、版面/对齐错误、语义理解与错误分类不准确、反馈位置偏移等。

Conclusion: 当前技术尚不能可靠地产生高质量、精确定位的手写反馈；需面向鲁棒识别与对齐、跨模块不确定性传播、联合学习、数据与标注标准化、用户体验与可解释反馈等方向展开研究，构建更可靠的端到端或弱监督/交互式框架。

Abstract: Handwriting remains an essential skill, particularly in education. Therefore, providing visual feedback on handwritten documents is an important but understudied area. We outline the many challenges when going from an image of handwritten input to correctly placed informative error feedback. We empirically compare modular and end-to-end systems and find that both approaches currently do not achieve acceptable overall quality. We identify the major challenges and outline an agenda for future research.

</details>


### [68] [Iterative Differential Entropy Minimization (IDEM) method for fine rigid pairwise 3D Point Cloud Registration: A Focus on the Metric](https://arxiv.org/abs/2601.09601)
*Emmanuele Barberi,Felice Sfravara,Filippo Cucinotta*

Main category: cs.CV

TL;DR: 提出基于微分熵的新目标函数，用于精配准三维点云，通过最小化熵实现对齐，避免固定参考点云的偏置，并在噪声、密度差、孔洞与局部重叠时优于RMSE/Chamfer/Hausdorff。


<details>
  <summary>Details</summary>
Motivation: 传统以欧氏距离为核心的配准（如ICP、RMSE最小化）在需要选择固定点云、对密度不均、噪声、孔洞、重叠率低等情况表现不稳且易陷入局部最优，缺乏交换性与鲁棒性。作者希望找到一种对称、鲁棒、在复杂现实条件下仍能出现明确最优对齐信号的度量。

Method: 提出迭代微分熵最小化（IDEM）：将两点云在刚体变换参数空间内对齐时的联合分布用核密度估计/体素化等方式近似，计算差异度量为微分熵并进行梯度或搜索式优化，使对齐对应的熵极小；该熵度量本质上对两点云对称，不需指定固定点云。

Result: 在多种合成和真实案例中，与RMSE、Chamfer与Hausdorff对比，IDEM在密度不一致、噪声、缺失（孔洞）和有限重叠的情形下仍能找到更接近真实的对齐，目标函数曲线呈现清晰全局最小值；传统度量时常失效或给出次优配准。

Conclusion: 微分熵作为配准目标具有对称性与鲁棒性，可在复杂问题设置下提供更可靠的优化地形与配准结果，相较传统欧氏距离型度量更适合实际场景的精配准任务。

Abstract: Point cloud registration is a central theme in computer vision, with alignment algorithms continuously improving for greater robustness. Commonly used methods evaluate Euclidean distances between point clouds and minimize an objective function, such as Root Mean Square Error (RMSE). However, these approaches are most effective when the point clouds are well-prealigned and issues such as differences in density, noise, holes, and limited overlap can compromise the results. Traditional methods, such as Iterative Closest Point (ICP), require choosing one point cloud as fixed, since Euclidean distances lack commutativity. When only one point cloud has issues, adjustments can be made, but in real scenarios, both point clouds may be affected, often necessitating preprocessing. The authors introduce a novel differential entropy-based metric, designed to serve as the objective function within an optimization framework for fine rigid pairwise 3D point cloud registration, denoted as Iterative Differential Entropy Minimization (IDEM). This metric does not depend on the choice of a fixed point cloud and, during transformations, reveals a clear minimum corresponding to the best alignment. Multiple case studies are conducted, and the results are compared with those obtained using RMSE, Chamfer distance, and Hausdorff distance. The proposed metric proves effective even with density differences, noise, holes, and partial overlap, where RMSE does not always yield optimal alignment.

</details>


### [69] [Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets](https://arxiv.org/abs/2601.09605)
*Jeremiah Coholich,Justin Wit,Robert Azarcon,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出MANGO：一种面向机器人视觉模仿的无配对图像翻译方法，通过模拟多视角数据到真实域的稳健迁移，缓解视角分布偏移并提升跨视角成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉模仿策略对相机视角变化等分布偏移很脆弱；真实机器人示范数据稀缺且缺少多视角多样性。模拟可大规模覆盖多视角，但存在显著的视觉sim2real鸿沟，需要一种在保持几何/视角一致性的同时实现逼真翻译的方法。

Method: 提出MANGO：在无配对图像翻译框架中引入(1)分割条件的InfoNCE对比损失以约束语义与几何一致性；(2)高正则化的判别器设计以稳定训练并减少过拟合于视角伪影；(3)改进的PatchNCE损失以保持局部特征对应关系。仅需少量固定机位的真实数据作为目标域支撑，利用仿真生成多视角并翻译到真实域，得到多样且与视角一致的真实风格图像。

Result: 在多种图像翻译基线中，MANGO在该任务上全面优于对手；用MANGO增强的数据训练的模仿学习策略，在原本完全失败的新视角上可达最高60%的成功率；能从少量固定相机真实数据外推到未见的多视角真实风格观察。

Conclusion: 通过分割条件InfoNCE、正则化判别器和改进PatchNCE，MANGO实现了视角一致的sim2real翻译，使模拟多视角数据有效用于真实世界训练，显著提升机器人操控策略对视角分布偏移的鲁棒性。

Abstract: Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\% on views that the non-augmented policy fails completely on.

</details>


### [70] [GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.09606)
*Manning Gao,Leheng Zhang,Shiqin Han,Haifeng Hu,Yuncheng Jiang,Sijie Mai*

Main category: cs.CV

TL;DR: 论文提出GRCF：两阶段、组别驱动的排序与校准框架，解决多模态情感分析中点回归的噪声敏感与相关性差问题，以及现有成对排序方法的均匀权重与静态间隔缺陷。阶段1用“优势加权的动态间隔排序损失”构建细粒度序关系并自适应关注难样本；阶段2用MAE目标做绝对分值校准。方法可扩展到分类（幽默、讽刺），在回归基准上SOTA，并在分类上具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有点式回归忽视样本间相对情感强弱，易受标签噪声影响，导致预测不稳与相关性差；成对序学习虽能捕捉相对顺序，但一刀切的比较权重与固定排名间隔无法反映样本难度与组间语义距离。需要一种既保持序结构又能校准绝对分值、并自适应关注难样本的框架。

Method: 提出两阶段Group-wise Ranking and Calibration Framework (GRCF)，灵感来自GRPO。阶段1：基于优势（advantage）加权的动态margin排序损失，按组比较，给难排样本更高权重，margin随组间语义距离自适应变化，构建细粒度序关系。阶段2：使用MAE驱动的目标对预测幅度进行绝对校准，解决仅排序不定标的问题。并将框架扩展到分类任务（幽默与讽刺检测）。

Result: 在多模态情感回归核心基准上达到SOTA，并在分类任务上表现出强泛化能力。

Conclusion: GRCF在保持相对顺序、实现绝对分值校准并自适应关注难样本之间取得平衡，缓解了噪声敏感与静态策略的缺陷，适用于回归与分类的多模态情感相关任务。

Abstract: Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.

</details>


### [71] [CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems](https://arxiv.org/abs/2601.09613)
*Yonglin Tian,Qiyao Zhang,Wei Xu,Yutong Wang,Yihao Wu,Xinyi Li,Xingyuan Dai,Hui Zhang,Zhiyong Cui,Baoqing Guo,Zujun Yu,Yisheng Lv*

Main category: cs.CV

TL;DR: 提出CogRail基准与联合微调框架，面向轨道交通入侵风险的认知感知，显著提升VLM在时空推理与威胁预测上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统系统多在固定视域内做目标分类并基于规则判断入侵，难以感知潜在风险；要提前预判需对目标的空间上下文与时间动态进行认知推理，而现有视觉模型与通用VLM在此方面存在短板。

Method: 1) 构建CogRail基准：整合开源数据集，设计带认知驱动的问答标注，覆盖位置感知、运动预测、威胁分析等时空推理任务。2) 以多模态提示系统评测SOTA VLM，系统揭示能力边界。3) 设计联合微调框架，将位置感知、运动预测、威胁分析三任务联合训练，使通用基础模型适配认知入侵感知。

Result: 实验证明现有大型多模态模型在复杂时空推理与认知入侵感知上表现不足；采用所提联合微调后，任务精度与可解释性显著提升，优于单任务或原始VLM。

Conclusion: 安全关键场景需面向认知推理的专用适配。CogRail提供评测与训练基座，联合多任务微调能有效提升VLM在轨道入侵风险感知中的准确与可解释性；代码将开源以促进研究。

Abstract: Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.

</details>


### [72] [Identifying Models Behind Text-to-Image Leaderboards](https://arxiv.org/abs/2601.09647)
*Ali Naseh,Yuefeng Peng,Anshuman Suri,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.CV

TL;DR: 作者发现匿名投票式T2I模型榜单的匿名性可被轻易打破：不同模型生成的图像在嵌入空间中形成可区分簇，基于质心的去匿名方法在22个模型、280个提示、15万张图像上取得高准确率，并提出逐提示可分辨性度量，揭示某些提示几乎可完美区分模型，指出现有榜单存在安全缺陷并呼吁更强匿名防护。


<details>
  <summary>Details</summary>
Motivation: 社区普遍使用匿名化的投票式榜单评估T2I模型质量，以避免偏见。然而若匿名能被攻破，评测公平性与系统安全都会受损，因此需要检验匿名化的稳健性。

Method: 收集22个T2I模型在280个提示下的生成（约15万张），将图像映射到通用嵌入空间，观察模型生成在该空间中的聚类现象；提出简单的基于类质心的分类/去匿名方法；定义“逐提示可分辨性”指标，分析哪些提示能显著放大模型特有签名。

Result: 在无需控制提示或使用训练数据的情况下，基于质心的去匿名在多模型设置下取得高准确率，表明模型特征签名稳定且系统性存在；同时发现某些提示能使模型之间的区分接近完美。

Conclusion: 当前T2I榜单的匿名策略存在根本性漏洞：模型特征会在嵌入空间中泄露身份。应设计更强的匿名化和防御策略，并在评测与发布流程中考虑去匿名风险。

Abstract: Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.

</details>


### [73] [AquaFeat+: an Underwater Vision Learning-based Enhancement Method for Object Detection, Classification, and Tracking](https://arxiv.org/abs/2601.09652)
*Emanuel da Costa Silva,Tatiana Taís Schein,José David García Ramos,Eduardo Lawson da Silva,Stephanie Loi Brião,Felipe Gomes de Oliveira,Paulo Lilles Jorge Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat+ 是用于水下视觉任务的“即插即用”特征增强流水线，通过端到端训练直接以下游任务损失为监督，显著提升在 FishTrack23 上的检测、分类与跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在低照度、颜色失真和浑浊等问题，导致视觉数据质量差，进而削弱机器人感知模块的表现。现有多数增强方法偏向人类主观视觉质量，未必对自动化视觉任务有效，亟需面向机器感知优化的特征增强方案。

Method: 提出 AquaFeat+，包含颜色校正、分层特征增强、以及自适应残差输出模块；整体端到端训练，并由最终应用（检测/分类/跟踪）的损失函数直接指导，使增强过程与任务目标对齐，实现“任务驱动”的特征增强。

Result: 在 FishTrack23 数据集上训练与评估，AquaFeat+ 在目标检测、分类和多目标跟踪指标上取得显著提升，表明其对水下机器人感知任务的增强有效。

Conclusion: AquaFeat+ 作为可插拔的特征增强组件，能在无需人为感知质量优化的前提下，直接提升水下自动化视觉任务性能，适合部署于水下机器人感知流水线。

Abstract: Underwater video analysis is particularly challenging due to factors such as low lighting, color distortion, and turbidity, which compromise visual data quality and directly impact the performance of perception modules in robotic applications. This work proposes AquaFeat+, a plug-and-play pipeline designed to enhance features specifically for automated vision tasks, rather than for human perceptual quality. The architecture includes modules for color correction, hierarchical feature enhancement, and an adaptive residual output, which are trained end-to-end and guided directly by the loss function of the final application. Trained and evaluated in the FishTrack23 dataset, AquaFeat+ achieves significant improvements in object detection, classification, and tracking metrics, validating its effectiveness for enhancing perception tasks in underwater robotic applications.

</details>


### [74] [Image2Garment: Simulation-ready Garment Generation from a Single Image](https://arxiv.org/abs/2601.09658)
*Selim Emir Can,Jan Ackermann,Kiyohiro Nakayama,Ruofan Liu,Tong Wu,Yang Zheng,Hugo Bertiche,Menglei Chai,Thabo Beeler,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 单图生成可物理仿真服装：先用微调的视觉-语言模型从图片推断面料成分与属性，再用小型映射器将其转为物理参数，实现无需迭代优化的仿真就绪服装，精度与仿真保真度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张图像到仿真服装困难：缺乏图像到物理标注数据，问题病态；现有方法要么需多视角与可微仿真、成本高，要么只给几何而无材料参数，无法真实仿真。

Method: 两阶段前馈框架：1) 微调视觉-语言模型，从真实图片推断面料成分与织物属性；2) 用小规模材料-物理测量数据训练轻量预测器，将属性映射为物理参数（如弯曲、拉伸、剪切等）。引入两个新数据集FTAG与T2P。整体不做迭代优化与可微仿真。

Result: 在材料成分估计与织物属性预测上精度领先；将估计结果经物理参数预测器转化后，仿真效果较现有图像到服装方法更高保真。

Conclusion: 通过语言视觉理解+小样本物理映射，可从单图得到仿真就绪服装。方法降低数据与计算成本，并在材料与仿真质量上优于SOTA；新数据集支撑社区发展。

Abstract: Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.

</details>


### [75] [LiteEmbed: Adapting CLIP to Rare Classes](https://arxiv.org/abs/2601.09661)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: LiteEmbed是一种对CLIP进行轻量级少样本个性化的框架，通过在词汇文本嵌入子空间中进行PCA引导的优化，兼顾语义一致性与细粒度可分性，实现无需重训编码器即可为稀有/新兴类别注入可即插即用的文本特征，并在多任务上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: CLIP零样本很强，但对预训练中罕见、文化特定或新出现类别表现不佳。需要一种无需重训大模型、可快速适配新类的轻量方法，改善在低资源场景下的识别与下游泛化。

Method: 提出LiteEmbed：对CLIP词汇空间中的文本嵌入做子空间引导优化。先以PCA分解将语义方向分为“粗语义方向”和“细粒度变化”，然后用两个互补目标联合优化：1) 粗对齐，保持全局语义一致性；2) 细分离，增强视觉相似类别之间的判别。优化后得到的文本嵌入可直接替换原CLIP文本特征，适用于分类、检索、分割和检测等任务。

Result: 在广泛实验中，相比既有方法取得显著提升，尤其在稀有、代表性不足或未见类别上效果突出。

Conclusion: LiteEmbed能以极小代价为CLIP添加新类别且无需重训编码器，通过PCA子空间与双目标优化兼顾语义一致与判别性，形成可在多任务中即插即用的文本嵌入，实证优于现有方案。

Abstract: Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.

</details>


### [76] [Self-Supervised Animal Identification for Long Videos](https://arxiv.org/abs/2601.09663)
*Xuyang Fang,Sion Hannuna,Edwin Simpson,Neill Campbell*

Main category: cs.CV

TL;DR: 提出一种高效自监督方法，将单视频内动物个体识别从“时序跟踪”重构为“全局聚类”，在已知个体数、仅用检测框与帧对采样下，实现>97%准确率与<1GB/批显存占用，超越或匹配需上千标注的监督基线。


<details>
  <summary>Details</summary>
Motivation: 长时段视频个体识别对生态监测与畜牧管理关键，但人工标注昂贵；现有自监督/对比学习对长序列内存与误差累积敏感、计算负担重，难在消费级硬件上落地。

Method: 假设单视频内个体数量已知且固定；仅需目标检测框与总个体数。对帧对进行采样，采用冻结的预训练骨干提取特征；在批内用匈牙利算法进行伪标签分配（自举），将问题转化为全局聚类学习；损失采用来自视觉-语言模型的BCE式目标替代传统对比损失，降低显存与计算；全流程避免时序依赖，减少误差传播。

Result: 在3D-POP鸽子与8头小牛进食的真实数据集上，达到或超过监督方法（需>1000标注帧）的表现，准确率>97%，同时每批显存<1GB，比标准对比学习低一个数量级。

Conclusion: 方法在无需身份标签、仅需检测与个体数量的前提下，实现高精度、低资源占用的动物个体识别，适合资源受限环境并能在消费级硬件上实用，缓解人工标注瓶颈；代码已开源可复现。

Abstract: Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($>$97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.

</details>


### [77] [SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings](https://arxiv.org/abs/2601.09665)
*Yuchen Wu,Jiahe Li,Xiaohan Yu,Lina Yu,Jin Zheng,Xiao Bai*

Main category: cs.CV

TL;DR: SCE-SLAM 通过学习场景坐标嵌入，在保持实时性的同时显著抑制单目SLAM的尺度漂移，并在KITTI/Waymo/vKITTI上以36 FPS取得更低ATE误差。


<details>
  <summary>Details</summary>
Motivation: 单目视觉SLAM在低成本与互联网视频重建中重要，但缺少跨窗口的全局尺度约束导致尺度漂移累积；现有帧间局部优化虽实时但难以维持长期尺度一致性。

Method: 提出端到端的SCE-SLAM：1) 场景坐标嵌入，学习在规范尺度参考下编码3D几何关系的补丁级表示；2) 几何引导聚合，通过基于3D邻近性的几何调制注意力，将历史观测的尺度信息向当前帧传播；3) 场景坐标Bundle Adjustment，利用从嵌入解码的显式3D坐标约束，将当前估计锚定到参考尺度，实现全局一致。

Result: 在KITTI、Waymo、vKITTI上显著优于现有方法：在KITTI上相对最佳先方法ATE降低8.36米，保持36 FPS，并在大场景中实现尺度一致。

Conclusion: 利用场景坐标嵌入提供可学习的全局尺度约束，可在不牺牲实时性的情况下显著缓解单目SLAM尺度漂移，提升轨迹精度与稳定性。

Abstract: Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.

</details>


### [78] [STEP3-VL-10B Technical Report](https://arxiv.org/abs/2601.09668)
*Ailin Huang,Chengyuan Yao,Chunrui Han,Fanqi Wan,Hangyu Guo,Haoran Lv,Hongyu Zhou,Jia Wang,Jian Zhou,Jianjian Sun,Jingcheng Hu,Kangheng Lin,Liang Zhao,Mitt Huang,Song Yuan,Wenwen Qu,Xiangfeng Wang,Yanlin Lai,Yingxiu Zhao,Yinmin Zhang,Yukang Shi,Yuyang Chen,Zejia Weng,Ziyang Meng,Ang Li,Aobo Kong,Bo Dong,Changyi Wan,David Wang,Di Qi,Dingming Li,En Yu,Guopeng Li,Haiquan Yin,Han Zhou,Hanshan Zhang,Haolong Yan,Hebin Zhou,Hongbo Peng,Jiaran Zhang,Jiashu Lv,Jiayi Fu,Jie Cheng,Jie Zhou,Jisheng Yin,Jingjing Xie,Jingwei Wu,Jun Zhang,Junfeng Liu,Kaijun Tan,Kaiwen Yan,Liangyu Chen,Lina Chen,Mingliang Li,Qian Zhao,Quan Sun,Shaoliang Pang,Shengjie Fan,Shijie Shang,Siyuan Zhang,Tianhao You,Wei Ji,Wuxun Xie,Xiaobo Yang,Xiaojie Hou,Xiaoran Jiao,Xiaoxiao Ren,Xiangwen Kong,Xin Huang,Xin Wu,Xing Chen,Xinran Wang,Xuelin Zhang,Yana Wei,Yang Li,Yanming Xu,Yeqing Shen,Yuang Peng,Yue Peng,Yu Zhou,Yusheng Li,Yuxiang Yang,Yuyang Zhang,Zhe Xie,Zhewei Huang,Zhenyi Lu,Zhimin Fan,Zihui Cheng,Daxin Jiang,Qi Han,Xiangyu Zhang,Yibo Zhu,Zheng Ge*

Main category: cs.CV

TL;DR: STEP3-VL-10B是一款约10B参数、强调小而强的开源多模态基础模型，通过统一全解冻预训练+大规模RL后训练，并在推理阶段用并行协同推理（PaCoRe）扩展测试时算力，在多项基准上逼近或超越百亿到千亿级及商业旗舰模型。


<details>
  <summary>Details</summary>
Motivation: 在多模态模型中，模型越大通常性能越好，但训练与推理成本高，难以复现与部署。作者旨在打破“参数规模=性能”的单一路径，提供一种在较小参数量下仍能达到前沿水平的开源可复现基线，降低算力门槛并提升实际可用性。

Method: 1) 预训练：采用统一、完全解冻的方案，在1.2万亿多模态token上联合训练，将语言对齐的感知编码器与Qwen3-8B解码器深度耦合，形成内生的视觉-语言协同。2) 后训练：构建扩展的强化学习（>1000轮）流水线进行能力提升。3) 推理策略：提出并行协同推理（PaCoRe），在测试时扩展计算，对多样化视觉假设进行并行探索与综合，以增强感知与推理。

Result: 在多个权威基准上取得领先：MMBench 92.2%、MMMU 80.11%，复杂推理方面 AIME2025 94.43%、MathVision 75.95%；在10B规模下可匹敌或超越GLM-4.6V-106B、Qwen3-VL-235B等大模型及Gemini 2.5 Pro、Seed-1.5-VL等商业旗舰。

Conclusion: 通过全解冻统一预训练、规模化RL后训练与PaCoRe测试时算力扩展，STEP3-VL-10B在紧凑参数量下实现前沿多模态智能与强泛化，提供高效、强力且可复现的开源基线，并重新定义效率与性能的权衡。

Abstract: We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\times$-20$\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.

</details>


### [79] [Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering](https://arxiv.org/abs/2601.09697)
*Jieying Chen,Jeffrey Hu,Joan Lasenby,Ayush Tewari*

Main category: cs.CV

TL;DR: 论文提出SRENDER：用扩散模型只生成少量关键帧，再通过3D重建与渲染补全整段视频，显著提升效率与一致性。针对相机轨迹自适应选择关键帧数量，实现在20秒视频上较扩散基线快40+倍，且保持高保真与时序稳定。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式视频生成质量高但推理极慢，难以满足实时或交互式场景（如具身智能、VR/AR）的需求；同时纯2D生成缺乏几何一致性。

Method: 1) 相机条件的静态场景视频生成：先用扩散模型生成稀疏关键帧；2) 将关键帧提升为3D表示（重建场景）并渲染中间视角以合成完整视频，实现计算摊销和几何一致；3) 设计一个模型预测不同相机轨迹所需的最优关键帧数，复杂轨迹用更密关键帧，简单轨迹更稀疏。

Result: 在20秒视频生成任务上，相比纯扩散基线速度提升超过40倍，同时维持高视觉保真度与时间稳定性。

Conclusion: 通过“稀疏关键帧+3D重建渲染”的两阶段策略，并自适应分配关键帧数量，可在保证质量与稳定性的同时大幅降低生成成本，为高效、可控的视频合成提供实用路径。

Abstract: Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.

</details>


### [80] [COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation](https://arxiv.org/abs/2601.09698)
*Tony Danjun Wang,Tolga Birdal,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: 提出COMPOSE：将多视角人体关键点对应从成对匹配改为超图分割，并配合几何剪枝，高效求解；在稀疏多视角3D姿态估计上显著提升（AP提升至多23%，较自监督端到端方法高11%）。


<details>
  <summary>Details</summary>
Motivation: 现有优化式多视角3D姿态估计先做各视角2D关键点检测，再跨视角关联并三角化。主流仅用两两匹配并把全局一致性（循环一致性）当软约束，易被错误关联传播所破坏，尤其在多视角与噪声/遮挡下变得脆弱，需一种能显式建模多视角全局一致性的鲁棒对应方法。

Method: 把跨视角关键点对应建模为超图分割（一个超边可连结多视角同一3D点的候选），并将问题表述为整数线性规划以强制全局、一致、循环一致的组合约束。为克服理论上指数级复杂度，提出高效几何剪枝：基于相机几何与三角化可行性过滤候选超边，极大缩小搜索空间，从而可在实际规模下求解。

Result: 在稀疏多视角数据上，相比以优化为主的现有方法，平均精度最高提升23%；相比自监督端到端学习方法最高提升11%，验证了全局一致的超图建模与剪枝策略的有效性与鲁棒性。

Conclusion: 以超图分割重构多视角对应，配合几何剪枝的可行求解框架，能显式保证全局/循环一致性并抑制错误传播，在稀疏视角3D姿态估计上取得显著性能增益，提供一个具推广潜力的通用对应匹配范式。

Abstract: 3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.

</details>


### [81] [SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3](https://arxiv.org/abs/2601.09699)
*Ruiqi Shen,Chang Liu,Henghui Ding*

Main category: cs.CV

TL;DR: 提出SAM3-DMS：在SAM3基础上，用对象级（而非群组级）记忆选择，提升多目标视频分割的身份保持与稳定性，尤其在高目标密度下优势明显，且为训练免调策略。


<details>
  <summary>Details</summary>
Motivation: 原始SAM3在多目标场景采用基于平均表现的群组同步记忆选择，忽视个体目标的可靠性，导致复杂场景下ID漂移与不稳定。需要一种能细粒度评估并选择每个对象记忆的机制。

Method: 提出训练无关的解耦记忆选择（Decoupled Memory Selection, DMS）：对每个目标独立评估其历史记忆帧的可靠性与效用，进行细粒度的保留/更新/丢弃；避免以全体平均性能触发统一决策。融合到SAM3的记忆模块中，实现对象级动态记忆管理。

Result: 在多目标视频分割实验中，方法显著提升身份一致性与跟踪稳定性；随目标密度增加，相对基线优势更大，表现出更强的鲁棒性。

Conclusion: 对象级细粒度记忆选择能弥补SAM3群组级策略的不足，在无需训练的前提下有效提升多目标场景的性能，为野外同时多目标视频分割提供坚实基础。

Abstract: Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.

</details>


### [82] [Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning](https://arxiv.org/abs/2601.09708)
*Chi-Pin Huang,Yunze Man,Zhiding Yu,Min-Hung Chen,Jan Kautz,Yu-Chiang Frank Wang,Fu-En Yang*

Main category: cs.CV

TL;DR: 提出Fast-ThinkAct：通过可言语化的潜在推理，实现VLA在保持性能的同时大幅降低推理时延（最高降至89.3%）。


<details>
  <summary>Details</summary>
Motivation: 显式CoT能提升VLA泛化，但长推理链导致推理延迟高、执行效率低，难以在动态环境中快速决策。需要一种既保留推理益处又高效的方案。

Method: 构建Fast-ThinkAct框架：1) 使用教师模型蒸馏，学习“潜在CoT”（简洁、可言语化但非冗长的推理表示）；2) 引入基于偏好的目标，对齐操控轨迹，使语言与视觉规划能力迁移到具身控制；3) 将紧凑推理与动作执行联结，实现增强型策略学习。

Result: 在多种具身操控与推理基准上，维持强性能的同时，相比SOTA推理型VLA将推理时延最高降低89.3%；仍保有长时程规划、少样本适应与失败恢复能力。

Conclusion: 潜在、紧凑的推理表示可在不牺牲性能的情况下显著提升VLA推理效率；蒸馏与偏好对齐将语言-视觉规划有效转移到具身控制，促进快速、稳健的策略执行。

Abstract: Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.

</details>
