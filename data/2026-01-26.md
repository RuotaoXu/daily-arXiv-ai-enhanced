<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 53]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GR3EN: Generative Relighting for 3D Environments](https://arxiv.org/abs/2601.16272)
*Xiaoyan Xing,Philipp Henzler,Junhwa Hur,Runze Li,Jonathan T. Barron,Pratul P. Srinivasan,Dor Verbin*

Main category: cs.CV

TL;DR: 提出一种将视频到视频的重光照扩散模型蒸馏到3D重建中的方法，实现可控的房间尺度3D重光照，避免困难的逆渲染，能在新光照下生成高质量新视角。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重光照依赖逆渲染，常欠定/病态，难以在复杂真实场景中得到高质量结果；扩散模型的进展多局限于2D或单个3D物体，缺少针对房间尺度场景的可控3D重光照方案。

Method: 先用视频到视频的重光照扩散模型在多视角视频上生成目标光照下的结果，再将其“蒸馏”/融合进现有的3D重建（例如通过一致性约束与多视角监督）以得到能在新光照下渲染的新场景表示，从而回避显式估计BRDF、光源与间接光的逆问题。

Result: 在合成与真实数据上验证，可在新光照条件下忠实渲染场景新视角，显示较强的细节保持与一致性，相较传统逆渲染方法更稳健、较以往扩散式方法更适用于房间尺度3D。

Conclusion: 通过将视频扩散重光照结果蒸馏进3D，提供了灵活、可控且适用于复杂真实房间尺度场景的重光照系统，避免逆渲染难题并实现高质量新视角渲染。

Abstract: We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.

</details>


### [2] [Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory](https://arxiv.org/abs/2601.16296)
*Dohun Lee,Chun-Hao Paul Huang,Xuelin Chen,Jong Chul Ye,Duygu Ceylan,Hyeonho Jeong*

Main category: cs.CV

TL;DR: Memory-V2V为多轮视频编辑引入外部记忆与压缩机制，利用检索到的历史编辑结果作为条件，提高跨轮一致性，并在保持/提升任务性能的同时将计算开销降至低、速度提升约30%。


<details>
  <summary>Details</summary>
Motivation: 真实视频编辑常是多轮迭代，用户逐步微调外观、运动与机位。然而现有Video-to-Video扩散编辑器在连续多轮操作中难以保持前后结果的跨时序与跨轮一致性，出现外观漂移、风格漂移、结构不稳定等问题。缺乏显式机制来引用并整合过往编辑结果是主要瓶颈。

Method: 在现有V2V扩散框架（含DiT骨干）上增设显式记忆。1) 外部缓存：保存此前轮次的编辑视频；2) 准确检索：从缓存中选取与当前编辑最相关的片段/帧；3) 动态标记化：将检索到的先验结果转为条件token并注入当前生成；4) 可学习token压缩器：在DiT中对冗余条件token进行可微压缩，保留关键视觉线索、减少计算。整体实现对现成模型增量式增强。

Result: 在视频新视角合成与文本条件的长视频编辑上，Memory-V2V显著提升跨轮一致性，且总体计算开销低；通过token压缩获得约30%速度提升，同时在任务指标上与SOTA持平或更好。

Conclusion: 显式记忆+检索与可学习压缩是解决多轮视频编辑跨一致性的有效途径。Memory-V2V在不显著增加成本的前提下提升一致性与质量，并具备对现有V2V模型的通用可插拔性。

Abstract: Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V

</details>


### [3] [FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging](https://arxiv.org/abs/2601.16302)
*Abhijeet Parida,Antonia Alomar,Zhifan Jiang,Pooneh Roshanitabrizi,Austin Tapp,Ziyue Xu,Syed Muhammad Anwar,Maria J. Ledesma-Carbayo,Holger R. Roth,Marius George Linguraru*

Main category: cs.CV

TL;DR: 提出FeTTL，在联邦学习中同时学习全局模板与任务模型以对齐多中心医学影像数据分布，在视盘分割与转移瘤病理分类两任务上显著优于SOTA（p<0.002）。


<details>
  <summary>Details</summary>
Motivation: 联邦学习可保护隐私但在多机构场景中因域偏移与数据异质性导致性能下降，医学影像受采集协议、设备、和人群差异影响尤甚，需要一种在不共享数据的前提下协调分布差异、提升泛化与稳健性的方案。

Method: 提出Federated Template and Task Learning（FeTTL）：在联邦环境中联合学习一个用于分布对齐的全局“模板”和下游任务模型，通过模板使各客户端数据在共享表示空间中对齐，同时与任务训练协同优化，以减轻跨域分布差异对模型的影响。

Result: 在两个多机构任务上评测：眼底视盘分割与病理图像转移瘤分类。FeTTL在两任务中均显著优于现有联邦学习基线（统计学显著，p<0.002）。联合学习模板与任务的重要性在消融中得到验证。

Conclusion: FeTTL为联邦医学影像中的分布偏移问题提供了可扩展、原则化的解决方案，能在真实多机构部署中提升稳健性与性能；联合学习模板与任务是关键。

Abstract: Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.

</details>


### [4] [Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments](https://arxiv.org/abs/2601.16333)
*Aditya K Surikuchi,Raquel Fernández,Sandro Pezzelle*

Main category: cs.CV

TL;DR: 研究构建足球比赛子事件“重要性”数据集，评测多模态大模型甄别关键片段的能力，发现现有模型接近随机、偏重单一模态，呼吁模块化架构与促进跨模态协同的训练。


<details>
  <summary>Details</summary>
Motivation: 现实应用中需从时间序列多模态事件生成叙述或摘要，前提是能识别关键子事件。现有评测和模型可能无法有效从多源信息中抽取“重要性”。作者以足球为代表性场景，系统检验模型是否真的能判别重要与非重要片段。

Method: 无额外标注成本地利用人类偏好隐含于“足球集锦”中作为重要性信号，构建新数据集；在该数据集上对多种SOTA多模态模型进行评测与深入分析（超越常规指标），考察它们对模态依赖与跨模态信息综合能力。

Result: 多模态模型在区分重要/非重要子事件上表现接近随机；分析显示模型常依赖单一主导模态，难以有效综合多源信息。

Conclusion: 当前模型缺乏稳健的多模态重要性理解能力。需要能处理样本级异质性的模块化架构，并配合促进跨模态协同的训练策略，以提升对关键事件的识别与生成质量。

Abstract: Foundation models are used for many real-world applications involving language generation from temporally-ordered multimodal events. In this work, we study the ability of models to identify the most important sub-events in a video, which is a fundamental prerequisite for narrating or summarizing multimodal events. Specifically, we focus on football games and evaluate models on their ability to distinguish between important and non-important sub-events in a game. To this end, we construct a new dataset by leveraging human preferences for importance implicit in football game highlight reels, without any additional annotation costs. Using our dataset, which we will publicly release to the community, we compare several state-of-the-art multimodal models and show that they are not far from chance level performance. Analyses of models beyond standard evaluation metrics reveal their tendency to rely on a single dominant modality and their ineffectiveness in synthesizing necessary information from multiple sources. Our findings underline the importance of modular architectures that can handle sample-level heterogeneity in multimodal data and the need for complementary training procedures that can maximize cross-modal synergy.

</details>


### [5] [Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures](https://arxiv.org/abs/2601.16348)
*Aline Sindel,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 提出一种针对历史板画多模态图像的高效非刚性配准方法，利用裂纹（craquelure）作为跨模态稳健特征，通过CNN检测/描述关键点、GNN进行匹配，并配合局部单应误差筛选与多层级关键点细化，实现从粗到细、跨分辨率对齐，在新建大规模数据集上优于现有关键点与稠密匹配方法。


<details>
  <summary>Details</summary>
Motivation: 艺术科技分析需要对可见光、IRR、UVF、X射线、宏观摄影等多模态图像进行像素级对齐。人工配准耗时且易受尺度差异、非刚性形变与跨模态内容变化影响。历史绘画普遍存在在各模态均可见的细裂纹图案，可作为稳定跨模态特征，激励基于裂纹的自动化非刚性配准研究。

Method: 1) 单阶段非刚性配准：以裂纹为核心特征，使用CNN实现关键点联合检测与描述；采用GNN在patch级进行描述子匹配；基于局部区域单应重投影误差过滤错误匹配；利用薄板样条（TPS）从稀疏对应估计非刚性形变场。2) 粗到细：提出多级关键点细化策略以处理混合分辨率与超大图像，从低分辨率逐层提升至最高分辨率。3) 数据：构建多模态板画数据集，包含大量关键点标注，测试集覆盖五种模态与不同分辨率。4) 消融实验验证各模块有效性。

Result: 在包含五个模态、跨分辨率的大型测试集上，该方法在配准精度上优于主流关键点法与稠密匹配法，以及其它细化方法；消融显示CNN+GNN匹配、局部重投影筛选与多级细化均显著提升性能。

Conclusion: 裂纹驱动的稀疏关键点+TPS的粗到细非刚性配准框架适用于历史板画多模态影像，能在效率与精度上取代手工配准，并在新数据集上取得SOTA表现。

Abstract: Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography. For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually. Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision. Due to varying image resolutions, huge image sizes, non-rigid distortions, and modality-dependent image content, registration is challenging. Therefore, we propose a coarse-to-fine non-rigid multi-modal registration method efficiently relying on sparse keypoints and thin-plate-splines. Historical paintings exhibit a fine crack pattern, called craquelure, on the paint layer, which is captured by all image systems and is well-suited as a feature for registration. In our one-stage non-rigid registration approach, we employ a convolutional neural network for joint keypoint detection and description based on the craquelure and a graph neural network for descriptor matching in a patch-based manner, and filter matches based on homography reprojection errors in local areas. For coarse-to-fine registration, we introduce a novel multi-level keypoint refinement approach to register mixed-resolution images up to the highest resolution. We created a multi-modal dataset of panel paintings with a high number of keypoint annotations, and a large test set comprising five multi-modal domains and varying image resolutions. The ablation study demonstrates the effectiveness of all modules of our refinement method. Our proposed approaches achieve the best registration results compared to competing keypoint and dense matching methods and refinement methods.

</details>


### [6] [Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models](https://arxiv.org/abs/2601.16378)
*Bridget Leonard,Scott O. Murray*

Main category: cs.CV

TL;DR: 论文引入“视角token”，在不改动主干模型架构的前提下，将可体现朝向/旋转的嵌入注入到多模态大模型中，从而显著缓解其对自我视角的偏置，提升在视觉换位（他者视角）空间推理任务上的表现，并显示这种方法轻量、可泛化且符合认知启发。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言模型在语义跨模态任务上已很强，但在需要采用他者视觉视角的空间推理（如相对方位、视线遮挡、物体旋转后的关系判断）上常失败，表现为顽固的“自我中心偏置”。这引发一个问题：模型是否具备分配中心（allocentric）推理的潜能，还是缺乏结构化的视角表示？人类空间认知依赖身体线索与心智旋转能力，启发作者将“视角”显式编码进模型输入空间。

Method: 提出两类“视角token”（专用嵌入）：1）具身型：由人体关键点等身体姿态/朝向线索编码视角；2）抽象型：直接编码旋转与朝向，支持心智旋转式的换位。将这些token与图像-文本输入一起整合到LLaVA-1.5-13B，并通过在合成与自然数据上微调，使模型在训练和推断时可利用视角嵌入进行空间换位。对比实验与表征分析用于评估性能与内部表征变化。

Result: 在Isle Bricks V2、COCO、3DSRBench等基准上，加入视角token后，模型在二级视觉视角采择任务上显著提升准确率；其中基于旋转的抽象token还能泛化到非人参照体。表征分析显示：微调后模型对朝向的潜在敏感性被放大，说明底座模型中已有分配中心推理前驱表征，但缺乏合适的内部结构来调用。

Conclusion: 将符合认知理论的空间结构直接注入到token空间，是一种轻量、模型无关的机制，可有效缓解自我中心偏置，赋能多模态模型进行更类似人类的视角采择与空间推理；同时证明现有MLM具备可被激活的分配中心推理潜能。

Abstract: Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent's visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.

</details>


### [7] [VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection](https://arxiv.org/abs/2601.16381)
*Yuxin Jiang,Yunkang Cao,Yuqi Cheng,Yiheng Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: 提出VTFusion，一个面向工业少样本异常检测的视觉-文本多模态融合框架；通过自适应特征提取与专门的多模态预测融合模块，在MVTec/VisA等基准与真实汽车塑件数据上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有FSAD方法虽引入文本语义，但多依赖自然场景预训练特征，忽视工业域细粒度语义；常见视觉-文本融合多为简单拼接，存在模态语义不对齐与跨模态干扰，影响鲁棒性与精度。

Method: 1) 自适应特征提取：为图像与文本各自设计可针对任务/领域调优的特征提取器，缩小预训练与工业数据的域差；并合成多样化伪缺陷以增强特征判别力。2) 多模态预测融合：构建专门的融合模块，含跨模态信息交互的融合块与在多模态引导下输出像素级异常图的分割网络。

Result: 在2-shot设定下，MVTec AD图像级AUROC=96.8%，VisA图像级AUROC=86.2%；在真实汽车塑件数据集上AUPRO=93.5%，显示良好工业适用性。

Conclusion: VTFusion通过域自适应的视觉/文本特征学习与深度跨模态融合，缓解语义不对齐与干扰问题，显著提升少样本工业异常检测的图像级与像素级性能，并具备实际场景可用性。

Abstract: Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.

</details>


### [8] [ResAgent: Entropy-based Prior Point Discovery and Visual Reasoning for Referring Expression Segmentation](https://arxiv.org/abs/2601.16394)
*Yihao Wang,Jusheng Zhang,Ziyi Tang,Keze Wang,Meng Yang*

Main category: cs.CV

TL;DR: 提出一种用于指代表达分割（RES）的新框架，结合熵驱动的点发现与视觉驱动的推理，在粗框内挑选高信息点并用视觉-语义对齐进行验证，最终以极少提示生成高质量分割掩码，并在四大基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: MLLM方法生成的粗边界框导致点提示冗余/不判别；且依赖文本坐标推理不可靠，容易被外观相似干扰物误导。需要一种既能从粗框中高效挑选有效点、又能以视觉证据而非纯文本坐标来确认目标的方法。

Method: 提出EBD与VBR两大组件并采用粗到细流程：1) 粗框初始化；2) 基于熵的点发现（在粗框内建模空间不确定性，以信息最大化选择高信息候选点）；3) 视觉驱动推理（联合视觉-语义对齐验证点的正确性，替代纯文本坐标推理）；4) 掩码解码生成最终分割。

Result: 在RefCOCO、RefCOCO+、RefCOCOg与ReasonSeg四个数据集上均取得新的SOTA性能，显示以最少点提示能得到准确且语义对齐的分割。

Conclusion: 将信息论驱动的点选择与视觉-语义对齐的验证结合，可缓解粗框与文本坐标推理的不可靠性，实现更鲁棒、少提示且精确的RES；该范式适合扩展到更多多模态定位与分割任务。

Abstract: Referring Expression Segmentation (RES) is a core vision-language segmentation task that enables pixel-level understanding of targets via free-form linguistic expressions, supporting critical applications such as human-robot interaction and augmented reality. Despite the progress of Multimodal Large Language Model (MLLM)-based approaches, existing RES methods still suffer from two key limitations: first, the coarse bounding boxes from MLLMs lead to redundant or non-discriminative point prompts; second, the prevalent reliance on textual coordinate reasoning is unreliable, as it fails to distinguish targets from visually similar distractors. To address these issues, we propose \textbf{\model}, a novel RES framework integrating \textbf{E}ntropy-\textbf{B}ased Point \textbf{D}iscovery (\textbf{EBD}) and \textbf{V}ision-\textbf{B}ased \textbf{R}easoning (\textbf{VBR}). Specifically, EBD identifies high-information candidate points by modeling spatial uncertainty within coarse bounding boxes, treating point selection as an information maximization process. VBR verifies point correctness through joint visual-semantic alignment, abandoning text-only coordinate inference for more robust validation. Built on these components, \model implements a coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding. Extensive evaluations on four benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg) demonstrate that \model achieves new state-of-the-art performance across all four benchmarks, highlighting its effectiveness in generating accurate and semantically grounded segmentation masks with minimal prompts.

</details>


### [9] [A Cosine Network for Image Super-Resolution](https://arxiv.org/abs/2601.16413)
*Chunwei Tian,Chengyuan Zhang,Bob Zhang,Zhiwu Li,C. L. Philip Chen,David Zhang*

Main category: cs.CV

TL;DR: 提出一种用于图像超分辨率的余弦网络（CSRNet），通过奇偶异构块融合线性与非线性结构信息，并用余弦退火与热重启优化训练，在多组实验中达到与SOTA相当的性能。


<details>
  <summary>Details</summary>
Motivation: 深度CNN逐层提取结构信息有利于重建高质量图像，但在SR中如何有效保留并利用这些结构信息仍具挑战；同时训练易陷入局部最小值、学习率调度不佳影响性能。

Method: 1) 设计奇、偶两类异构残块，扩大网络内部结构差异性，提取互补的同源（homologous）结构信息；2) 在网络中显式结合线性与非线性结构通路，缓解仅依赖同质信息的局限并提升鲁棒性；3) 训练使用余弦退火学习率并配合暖重启策略，帮助跳出局部最小、稳定收敛。

Result: 在多个基准数据集上的实验表明，CSRNet在定量与定性指标上与当前SOTA方法具有竞争力。

Conclusion: 通过异构块与线性/非线性信息融合，并辅以余弦退火+热重启的训练策略，CSRNet能够更有效地保留与利用结构信息，从而提升超分辨率重建质量，达到与SOTA相当的表现。

Abstract: Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.

</details>


### [10] [DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target](https://arxiv.org/abs/2601.16428)
*Shuying Li,Qiang Ma,San Zhang,Chuang Yang*

Main category: cs.CV

TL;DR: 提出DCCS-Det用于红外小目标检测，通过方向上下文与跨尺度语义增强，兼顾长程依赖与局部细节，减少冗余与语义稀释，在多数据集上达SOTA并具竞争效率；消融验证DSE与LaSEA有效。


<details>
  <summary>Details</summary>
Motivation: 现有IRSTD方法要么局限于局部或全局特征单一建模，导致目标—背景区分差；要么存在特征冗余与语义稀释，损害小而暗弱目标的表示质量。需要一种同时强化局部-全局建模并抑制冗余噪声的检测器。

Method: 提出DCCS-Det，包括：1) 双流显著性增强块DSE，将局部感知与方向感知上下文聚合结合，捕获长程空间依赖和细节；2) 潜在感知的语义提取与聚合模块LaSEA，通过跨尺度特征提取和随机池化采样，缓解特征退化，增强判别性、抑制噪声。整体框架在检测管线中整合上述模块以提升小目标感知。

Result: 在多组IRSTD数据集上实现最先进检测精度，同时保持较高效率。消融实验显示加入DSE与LaSEA均显著提升在复杂背景下的目标感知与表示质量。

Conclusion: DCCS-Det通过方向上下文与跨尺度语义机制，有效平衡局部与全局信息并降低冗余噪声，提升红外小目标检测的准确性与效率；模块具有可复用性，适用于复杂场景。

Abstract: Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}

</details>


### [11] [AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose](https://arxiv.org/abs/2601.16429)
*Jongmin Yu,Hyeontaek Oh,Zhongtian Sun,Angelica I Aviles-Rivero,Moongu Jeon,Jinhong Yang*

Main category: cs.CV

TL;DR: AlphaFace是一种实时人脸换脸方法，通过引入视觉-语言模型与CLIP的图像/文本嵌入并设计语义对比损失，在极端姿态下保持更强身份一致性与属性保真，超过现有方法并避免显式几何与扩散法的开销。


<details>
  <summary>Details</summary>
Motivation: 现有人脸换脸在极端姿态（大角度转头等）质量显著下降；用显式几何（3D/关键点）虽能增强姿态鲁棒，但依赖多、代价高；扩散模型效果好但无法实时。需要一种既鲁棒又实时、减少外部依赖的方案。

Method: 提出AlphaFace：利用开源视觉-语言模型与CLIP的图像/文本嵌入，构建新颖的视觉与文本语义对比损失，增强身份表示与属性（如表情、光照、姿态相关外观）的约束；在无需显式几何与扩散生成的前提下实现实时换脸。

Result: 在FF++、MPIE、LPFF数据集上，尤其是大姿态场景，AlphaFace超越现有SOTA方法；保持实时推理性能。

Conclusion: 语义对比学习（结合VLM与CLIP）可在不引入几何依赖或扩散计算的情况下，提升姿态鲁棒性与身份/属性保真并实现实时换脸；方法已开源可复现。

Abstract: Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git'.

</details>


### [12] [MDAFNet: Multiscale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection](https://arxiv.org/abs/2601.16434)
*Shuying Li,Qiang Ma,San Zhang,Wuwei Wang,Chuang Yang*

Main category: cs.CV

TL;DR: 提出MDAFNet，用MSDE补偿下采样导致的边缘丢失，并用DAFE在空频双域自适应增强高频目标、抑制高频噪声，在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有IRSTD方法随网络加深导致目标边缘像素逐步退化；传统卷积难以区分频率成分，低频背景干扰高频小目标，高频噪声又易触发误检，影响检测精度与鲁棒性。

Method: 设计MDAFNet，包括：1）MSDE模块——多尺度边缘提取与增强，在下采样过程中补偿目标边缘信息的累积损失；2）DAFE模块——在频域与空域结合的双域自适应特征增强：引入频域处理机制，并在空域模拟频率分解与融合，实现对高频目标的自适应增强与对高频噪声的选择性抑制。网络整体融合两模块以提升特征判别性。

Result: 在多个IRSTD数据集上实验，MDAFNet取得优于现有方法的检测性能（摘要未给具体数值，但声称全面领先）。

Conclusion: 通过多尺度边缘补偿与双域自适应频率增强，MDAFNet有效缓解边缘退化、分离目标与背景/噪声，在多数据集上验证了优越性，适用于复杂场景下的红外小目标检测。

Abstract: Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network's capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.

</details>


### [13] [Masked Face Recognition under Different Backbones](https://arxiv.org/abs/2601.16440)
*Bo Zhang,Ming Zhang,Kun Wu,Lei Bian,Yi Lin*

Main category: cs.CV

TL;DR: 论文更正了表IV和第3页A节数据，并系统比较了不同骨干网络在有无口罩场景下的人脸识别表现：标准场景r100系列最佳，口罩场景r100_mask_v2居首，ViT-S/Tiny在口罩场景有明显增益；给出相应部署建议。


<details>
  <summary>Details</summary>
Motivation: 疫情后旅客戴口罩普遍，传统人脸识别在安检等场景受阻。需要评估不同骨干网络在遮挡（戴口罩）与非遮挡条件下的鲁棒性与准确性，以指导实际部署。

Method: 进行大规模对比实验：以r100、r50、r34_mask_v1、r100_mask_v2、r50_mask_v3以及ViT-Small/Tiny等作为骨干，分别在标准测试与戴口罩测试上评估（包括0.01% FAR下的比对准确率、检索Top1/Top5等），并报告修正后的表格与数据。

Result: 标准测试：r100系列性能领先（比对准确率>98%，检索Top1/Top5高），r50次之，r34_mask_v1落后。口罩测试：r100_mask_v2最佳（90.07%准确率），r50_mask_v3在r50中最佳但不及r100；ViT-S/Tiny在口罩场景表现强、有效性提升。

Conclusion: 不同骨干对口罩遮挡鲁棒性差异显著：高容量r100及其口罩特化版本在遮挡与非遮挡均占优；中等容量（r50）可权衡性能与成本；轻量模型需口罩特化或ViT结构以提升遮挡鲁棒性。论文给出针对有无口罩场景的具体部署建议。

Abstract: Erratum to the paper (Zhang et al., 2025): corrections to Table IV and the data in Page 3, Section A. In the post-pandemic era, a high proportion of civil aviation passengers wear masks during security checks, posing significant challenges to traditional face recognition models. The backbone network serves as the core component of face recognition models. In standard tests, r100 series models excelled (98%+ accuracy at 0.01% FAR in face comparison, high top1/top5 in search). r50 ranked second, r34_mask_v1 lagged. In masked tests, r100_mask_v2 led (90.07% accuracy), r50_mask_v3 performed best among r50 but trailed r100. Vit-Small/Tiny showed strong masked performance with gains in effectiveness. Through extensive comparative experiments, this paper conducts a comprehensive evaluation of several core backbone networks, aiming to reveal the impacts of different models on face recognition with and without masks, and provide specific deployment recommendations.

</details>


### [14] [Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding](https://arxiv.org/abs/2601.16449)
*Xiaojiang Peng,Jingyi Chen,Zebang Cheng,Bao Peng,Fengyi Wu,Yifei Dong,Shuyuan Tu,Qiyu Hu,Huiting Huang,Yuxiang Lin,Jun-Yan He,Kai Wang,Zheng Lian,Zhi-Qi Cheng*

Main category: cs.CV

TL;DR: 提出Emotion-LLaMAv2与MMEVerse，构建端到端多视角情感识别-推理系统与标准化评测基准；通过多视角编码、Conv Attention预融合、感知到认知的课程式指令微调，显著提升情感理解能力；汇合集成12个公开数据集为统一指令格式，重标注得到大规模训练/测试数据。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在情感推理上表现不足，主要受限于：缺少大规模高质量情感标注的多模态数据；缺乏统一的评测基准；先前方法依赖外部人脸检测、隐式/薄弱的跨模态融合，以及数据规模与质量不足。

Method: 1) 端到端多视角编码器：去除外部人脸检测，使用更丰富的时空多视角token捕获细腻情感线索。2) Conv Attention预融合模块：在LLM主干之外实现局部与全局的多模态特征交互，提升融合显式性与效率。3) 感知→认知课程式指令微调：在LLaMA2主干中统一情感识别与自由形式情感推理。4) 数据与基准：将12个公开情感数据集统一为多模态指令格式；通过多智能体（Qwen2 Audio、Qwen2.5 VL、GPT-4o）重标注与扩充，构建130k训练片段与36k测试片段，涵盖18个评测基准（MMEVerse）。

Result: 在统一基准MMEVerse上，Emotion-LLaMAv2实现更强的情感识别与推理表现（摘要暗示全面优于前作和通用MLLM），并提供可复现的训练与评测设置；多视角编码与预融合显著增强细粒度情感线索建模。

Conclusion: Emotion-LLaMAv2通过结构化的多视角编码、显式跨模态预融合与课程式指令微调，打通“感知到认知”的端到端情感理解流程；MMEVerse则为社区提供大规模、统一、可复现的训练与评测生态，为多模态情感推理建立新标准。

Abstract: Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.

</details>


### [15] [VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology](https://arxiv.org/abs/2601.16451)
*Peixian Liang,Songhao Li,Shunsuke Koga,Yutong Li,Zahra Alipour,Yucheng Tang,Daguang Xu,Zhi Huang*

Main category: cs.CV

TL;DR: VISTA-PATH 是一个交互式、类感知的病理分割基础模型，联合图像、文本语义与可选专家空间提示，实现多器官多类别的高精度像素级分割，并支持人机闭环细化；在多项内外部基准上胜过现有模型，并通过提出的肿瘤交互评分（TIS）与生存显著相关，展示临床实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有分割基础模型把分割当作静态视觉任务，难以处理病理图像的异质性、缺乏与临床语义对齐，也无法有效融入专家交互反馈，从而限制了在真实病理流程中的泛化与可解释临床应用。

Method: 提出 VISTA-PATH：将分割条件化在三种信息上——视觉上下文、语义组织学描述（文本）、以及可选的专家空间提示（如框/点）。同时构建大规模配对数据集 VISTA-PATH Data（约160万图-掩膜-文本三联，涵盖9个器官、93个组织类别）。模型支持在稀疏补丁级框注释基础上，传播到整张切片，实现交互式迭代细化。

Result: 在多项留出集与外部数据基准上，VISTA-PATH 一致优于现有分割基础模型；生成的高保真、类感知分割更受计算病理任务青睐。提出的肿瘤交互评分（TIS）基于其分割结果，能显著关联患者生存。

Conclusion: VISTA-PATH 将病理分割从静态预测提升为交互式、临床语义对齐的表示，兼具泛化能力与可用性；其分割可直接驱动有临床意义的下游分析与预后评估，具备实践落地潜力；代码与演示已开放。

Abstract: Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH.

</details>


### [16] [Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos](https://arxiv.org/abs/2601.16471)
*Meng Cao,Haoran Tang,Haoze Zhao,Mingfei Han,Ruyang Liu,Qiang Sun,Xiaojun Chang,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: 论文提出利用游戏视频中的“物理漏洞（glitches）”作为可扩展监督信号，构建PhysGame数据集与GameBench基准，以提升多模态模型的物理世界理解与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有物理推理数据集要么来源于真实视频，标注成本高；要么是合成仿真，真实性与多样性不足。MLLMs虽具一般推理能力，但仍难以达到人类水平的物理常识与因果理解。游戏视频中频繁出现的违背物理定律的“漏洞”天然提供了丰富、低成本且多样的反例信号，可用于训练模型识别与推理物理不合理现象。

Method: 提出以游戏glitches为核心的指令微调范式：1) 构建PhysGame数据集，基于游戏元信息（标题、描述等）设计提示词，自动生成面向五大物理领域、16个细粒度类别的问答，共140,057对，聚焦“违反物理”的异常。2) 为评测构建GameBench：人工标注的880段含glitch的游戏视频，用于衡量物理推理与异常检测能力。3) 在代表性MLLM（如Qwen2.5VL）上进行指令微调与广泛实验，评估跨域迁移（Game→Real、Game→General）。

Result: 在真实世界物理基准PhysBench上提升2.5%；在通用多模态基准MVBench上提升1.9%；在新构建的GameBench上绝对提升3.7%，显示对物理不合理性的鲁棒检测增强与更好迁移性。

Conclusion: 将游戏中的物理异常作为监督信号，是推进多模态物理世界理解的可扩展且有效路径。PhysGame与GameBench为训练与评测提供了新资源，显著提升跨域物理推理与异常识别能力。

Abstract: Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence.

</details>


### [17] [Multi-View Consistent Wound Segmentation With Neural Fields](https://arxiv.org/abs/2601.16487)
*Remi Chierchia,Léo Lebrat,David Ahmedt-Aristizabal,Yulia Arzhaeva,Olivier Salvado,Clinton Fookes,Rodrigo Santa Cruz*

Main category: cs.CV

TL;DR: 论文评估了一种名为 WoundNeRF 的NeRF+SDF方法，用于从自动生成标注中学习多视一致、鲁棒的创面分割，并与ViT和传统栅格化算法对比，显示出在3D一致性与分割精度上的潜力；代码将开源。


<details>
  <summary>Details</summary>
Motivation: 传统创面护理在成本与流程上负担重，自动化评估亟需可靠分割；2D分割难以捕捉3D形态与多视一致性，现有3D扩展仍难从多视2D图像中稳定重建；需要一种能在多视下生成一致、精确3D分割的方案。

Method: 提出并评估WoundNeRF：以NeRF为基础并融合SDF表征，通过从标准RGB多视图及自动生成的标注中学习体渲染与表面隐式函数，获得多视一致的3D结构与语义分割；与SOTA的Vision Transformer分割网络和传统栅格化方法做对比实验。

Result: WoundNeRF在创面分割任务上呈现更鲁棒、准确的结果，尤其在多视一致性和3D恢复方面优于对比方法（ViT与传统算法）。实验表明该范式能够从自动标注中有效学习并提升分割质量。

Conclusion: NeRF+SDF范式可用于获得稳定、精确的创面3D分割，有助于更全面地跟踪愈合进程；该方向具有发展潜力，作者将开源代码以促进后续研究与临床落地。

Abstract: Wound care is often challenged by the economic and logistical burdens that consistently afflict patients and hospitals worldwide. In recent decades, healthcare professionals have sought support from computer vision and machine learning algorithms. In particular, wound segmentation has gained interest due to its ability to provide professionals with fast, automatic tissue assessment from standard RGB images. Some approaches have extended segmentation to 3D, enabling more complete and precise healing progress tracking. However, inferring multi-view consistent 3D structures from 2D images remains a challenge. In this paper, we evaluate WoundNeRF, a NeRF SDF-based method for estimating robust wound segmentations from automatically generated annotations. We demonstrate the potential of this paradigm in recovering accurate segmentations by comparing it against state-of-the-art Vision Transformer networks and conventional rasterisation-based algorithms. The code will be released to facilitate further development in this promising paradigm.

</details>


### [18] [Expert Knowledge-Guided Decision Calibration for Accurate Fine-Grained Tree Species Classification](https://arxiv.org/abs/2601.16498)
*Chen Long,Dian Chen,Ruifei Ding,Zhe Chen,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: 提出EKDC-Net，通过引入外部“领域专家”并结合不确定性校准，实现长尾与高相似类别下的细粒度树种分类SOTA；同时发布102类大规模树种数据集CU-Tree102。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度树种分类方法多依赖复杂网络以适配本地分布，但忽视小样本、长尾分布与高类间相似，导致难以区分尾部或易混类别。类人知识传播中，人会求助专家突破局部认知，因此引入外部专家以弥补模型盲点。

Method: 1) 提出专家知识引导的决策校准网络EKDC-Net，包含两核心模块：
- LPKEM：利用CAM从本地模型的判别区域出发，引导“领域专家”仅关注与分类相关的判别特征，从而提炼可迁移的专家知识；
- UDCM：基于类别总体不确定性与实例级预测不确定性，对本地模型输出进行动态校准，将专家知识与本地预测进行融合与纠偏。
2) 构建CU-Tree102数据集，涵盖102个树种，提升基准多样性。

Result: 在三大基准上取得SOTA；作为轻量即插即用模块，仅增加约0.08M可学习参数，即可提升主干准确率6.42%、精度11.46%。

Conclusion: 外部专家知识在细粒度、长尾与高相似场景下能有效补足本地模型局限；通过不确定性引导的决策校准实现稳定增益。公开数据集与代码支持复现与扩展。

Abstract: Accurate fine-grained tree species classification is critical for forest inventory and biodiversity monitoring. Existing methods predominantly focus on designing complex architectures to fit local data distributions. However, they often overlook the long-tailed distributions and high inter-class similarity inherent in limited data, thereby struggling to distinguish between few-shot or confusing categories. In the process of knowledge dissemination in the human world, individuals will actively seek expert assistance to transcend the limitations of local thinking. Inspired by this, we introduce an external "Domain Expert" and propose an Expert Knowledge-Guided Classification Decision Calibration Network (EKDC-Net) to overcome these challenges. Our framework addresses two core issues: expert knowledge extraction and utilization. Specifically, we first develop a Local Prior Guided Knowledge Extraction Module (LPKEM). By leveraging Class Activation Map (CAM) analysis, LPKEM guides the domain expert to focus exclusively on discriminative features essential for classification. Subsequently, to effectively integrate this knowledge, we design an Uncertainty-Guided Decision Calibration Module (UDCM). This module dynamically corrects the local model's decisions by considering both overall category uncertainty and instance-level prediction uncertainty. Furthermore, we present a large-scale classification dataset covering 102 tree species, named CU-Tree102 to address the issue of scarce diversity in current benchmarks. Experiments on three benchmark datasets demonstrate that our approach achieves state-of-the-art performance. Crucially, as a lightweight plug-and-play module, EKDC-Net improves backbone accuracy by 6.42% and precision by 11.46% using only 0.08M additional learnable parameters. The dataset, code, and pre-trained models are available at https://github.com/WHU-USI3DV/TreeCLS.

</details>


### [19] [SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer](https://arxiv.org/abs/2601.16515)
*Tongcheng Fang,Hanling Zhang,Ruiqi Xie,Zhuo Han,Xin Tao,Tianchen Zhao,Pengfei Wan,Wenbo Ding,Wanli Ouyang,Xuefei Ning,Yu Wang*

Main category: cs.CV

TL;DR: 提出SALAD：在稀疏注意力并联一个轻量线性注意力分支，并用输入依赖门控动态融合，两者兼顾速度与质量；以极少微调数据和步数，实现90%稀疏、1.72×推理加速且生成质量与全注意力相当。


<details>
  <summary>Details</summary>
Motivation: 视频生成的Diffusion Transformer输入序列很长，完整自注意力开销随序列长度二次增长，导致推理延迟高。现有稀疏注意力：训练免调的方法稀疏度受限、加速有限；训练式方法可高稀疏但需要大量数据与算力。需要一种既能高稀疏高加速、又低成本微调且保持生成质量的方案。

Method: 提出SALAD架构：在现有稀疏注意力旁路并联一个轻量的线性注意力分支；通过输入依赖的门控机制（gating）对两分支输出进行细粒度加权，动态平衡全局/局部信息建模与计算成本；以少量视频样本进行高效微调以学习门控与并联结构的融合。

Result: 在视频生成任务中实现约90%注意力稀疏度与1.72×推理加速，同时生成质量与全注意力基线相当。微调仅需约2000段视频、1600步、batch size为8。

Conclusion: 并联线性注意力与稀疏注意力并用，并以输入依赖门控融合，可在极少微调成本下显著降低计算并维持生成质量，为扩展长序列视频生成提供实用路径。

Abstract: Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.

</details>


### [20] [TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning](https://arxiv.org/abs/2601.16520)
*Daixian Liu,Jiayi Kuang,Yinghui Li,Yangning Li,Di Yin,Haoyu Cao,Xing Sun,Ying Shen,Hai-Tao Zheng,Liang Lin,Philip S. Yu*

Main category: cs.CV

TL;DR: 提出TangramPuzzle基准，用于以七巧板为载体评估MLLM的组合式空间推理；引入可机器校验的几何符号框架TCE，并设两类任务（轮廓预测与端到端代码生成）；实验显示模型偏向匹配轮廓而忽视几何约束，导致拼块形变。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准在空间推理上任务简单、依赖语义近似或粗糙相对位置，评测指标缺少严格数学定义，难以精确衡量MLLM的组合几何推理能力。

Method: 构建几何扎根的七巧板评测TangramPuzzle；提出Tangram Construction Expression（TCE），用精确坐标与符号几何描述拼块装配，避免视觉近似歧义；设计两任务：1）Outline Prediction：从局部部件推断整体轮廓；2）End-to-End Code Generation：求解逆向几何装配并生成TCE代码；在多种开源与闭源MLLM上系统评测。

Result: 在多模型实验中发现：模型倾向优先匹配目标轮廓形状，但经常违反几何/拼块约束（如比例、角度、长度守恒），导致拼块扭曲或变形；表明当前MLLM在精确组合空间推理上的能力不足。

Conclusion: TangramPuzzle与TCE提供了可度量、可验证的空间推理评测框架；现有MLLM在满足几何约束的组合推理方面存在显著短板，未来需引入更强的几何推理与符号约束融合方法以提升可靠性。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.

</details>


### [21] [AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding](https://arxiv.org/abs/2601.16532)
*Runmao Yao,Junsheng Zhou,Zhen Dong,Yu-Shen Liu*

Main category: cs.CV

TL;DR: AnchoredDream提出零样本的单视图到全景（360°）室内场景生成流程，通过外观-几何互促机制先稳固几何、再逐步补全与优化，从而显著提升跨视角外观一致性与几何可信度。


<details>
  <summary>Details</summary>
Motivation: 现有单视图生成全景室内场景的方法在大视角变化下难以同时保持外观一致与几何合理，尤其依赖扩散模型与深度网络时易出现结构扭曲、纹理漂移与拼接伪影，限制了全场景重建与生成的实用性。

Method: 提出AnchoredDream零样本管线：1) 外观引导的几何生成，先从输入图像构建高保真3D场景布局；2) 级联生成模块推进全景：包括warp-and-inpaint（基于几何投影与扩散修补）、warp-and-refine（进一步细化外观与结构）、post-optimization（后期一致性与几何优化）、以及新颖的Grouting Block（灌浆块）用于输入视野与生成区域的无缝过渡与缝隙填补；核心是外观-几何相互促进的锚定策略。

Result: 在广泛实验中，相较现有方法，AnchoredDream在外观一致性与几何可信度上均有显著优势，并能在零样本设定下生成完整360°场景。

Conclusion: 将高保真几何作为锚点、通过外观-几何互促实现逐步生成与优化，可显著改善单视图到全景室内场景的质量，显示几何约束对高质量零样本场景生成的关键作用。

Abstract: Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation.

</details>


### [22] [OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding](https://arxiv.org/abs/2601.16538)
*Zixian Liu,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出OnlineSI：面向视频流的在线空间理解框架，使用有限空间记忆与3D点云+语义融合，在不随输入增长增加计算的前提下持续提升MLLM的空间理解；引入Fuzzy F1评估并在两数据集上验证有效，面向真实具身系统部署。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM空间理解方法多为离线/单次推理，难以在动态环境中持续工作，且计算随时间增长；同时缺乏与具身系统部署相关的高效记忆与鲁棒评估。

Method: 1) 在线框架OnlineSI：维护有限容量的空间记忆，逐帧更新，确保每次推理计算不随历史长度增加；2) 融合3D点云（几何）与语义信息以提升目标定位与识别；3) 设计Fuzzy F1-Score以缓解标注与边界模糊带来的评测歧义；4) 在两个代表性数据集上进行实验。

Result: 在两个数据集上，相较基线取得更优的空间理解/定位与识别表现；在视频流设置下保持稳定、可扩展的推理开销。

Conclusion: OnlineSI能在视频流中持续提升空间理解，计算随时间不增长，且3D-语义融合与Fuzzy F1评估有效，为面向真实世界具身系统的部署奠定基础。

Abstract: In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems.

</details>


### [23] [Semi-Supervised Hierarchical Open-Set Classification](https://arxiv.org/abs/2601.16541)
*Erik Wallin,Fredrik Kahl,Lars Hammarstrand*

Main category: cs.CV

TL;DR: 提出一种半监督层级开放集分类框架，利用教师-学生伪标签、子树伪标签与“年龄门控”抑制过置信，在大规模含未知类的数据上显著提升性能，并在iNaturalist19上以极少标注接近全监督。


<details>
  <summary>Details</summary>
Motivation: 现实数据常包含未标注与未知类别；层级开放集分类可把未知样本映射到较高层级，但现有方法多为全监督或自监督后再适配，难以充分利用混合（已知+未知）的海量无标数据。需要一种能在开放集与层级结构下稳健利用无标数据的半监督方法。

Method: 构建教师-学生伪标框架：1) 子树伪标签（subtree pseudo-labels）——对不确定或可能属于未知类的样本，不强行给叶子类别，而赋予其最合适的层级子树作为监督，减少伪标噪声并与开放集目标一致；2) 年龄门控（age-gating）——基于样本/模型“年龄”（如训练步数、置信演化）调节伪标签采纳阈值，抑制早期过置信与错误累积。整体在层级损失上联合优化，使用无标混合数据进行迭代更新。

Result: 在iNaturalist19基准上，使用每类仅20个标注样本时，所提方法优于“自监督预训练+监督适配”的强基线，并可达到接近全监督训练的性能。

Conclusion: 通过子树伪标签和年龄门控，半监督地稳健利用包含未知类的无标数据，可显著提升层级开放集分类；在低标注场景下表现接近全监督，具实用价值与扩展潜力。

Abstract: Hierarchical open-set classification handles previously unseen classes by assigning them to the most appropriate high-level category in a class taxonomy. We extend this paradigm to the semi-supervised setting, enabling the use of large-scale, uncurated datasets containing a mixture of known and unknown classes to improve the hierarchical open-set performance. To this end, we propose a teacher-student framework based on pseudo-labeling. Two key components are introduced: 1) subtree pseudo-labels, which provide reliable supervision in the presence of unknown data, and 2) age-gating, a mechanism that mitigates overconfidence in pseudo-labels. Experiments show that our framework outperforms self-supervised pretraining followed by supervised adaptation, and even matches the fully supervised counterpart when using only 20 labeled samples per class on the iNaturalist19 benchmark. Our code is available at https://github.com/walline/semihoc.

</details>


### [24] [HA2F: Dual-module Collaboration-Guided Hierarchical Adaptive Aggregation Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.16573)
*Shuying Li,Yuchen Wang,San Zhang,Chuang Yang*

Main category: cs.CV

TL;DR: 提出HA2F框架，通过分层特征校准与噪声自适应精炼，缓解跨时相匹配偏差与辐射/几何噪声干扰，在多个RSCD数据集上达SOTA并具高效性。


<details>
  <summary>Details</summary>
Motivation: 现有RSCD方法要么局限于局部补丁，要么全图整体处理，导致跨时相特征匹配偏差，并对辐射与几何噪声敏感，影响变化检测的精度与稳健性。

Method: 提出双模块协同的分层自适应聚合框架HA2F：1) 动态分层特征校准模块（DHFCM），通过感知式特征选择动态融合相邻层级特征，抑制无关差异并缓解跨时相对齐偏差；2) 噪声自适应特征精炼模块（NAFRM），采用双重特征选择机制，突出对变化敏感的区域，生成空间掩膜以抑制无关区域或阴影干扰。整体实现分层聚合与区域注意的协同。

Result: 在LEVIR-CD、WHU-CD、SYSU-CD上取得SOTA，兼顾精度指标与计算效率；消融实验证明DHFCM与NAFRM均有效。

Conclusion: HA2F通过分层动态校准与噪声自适应精炼有效解决跨时相匹配与噪声问题，提升RSCD的准确性与鲁棒性，并具备较高效率与可推广性。

Abstract: Remote sensing change detection (RSCD) aims to identify the spatio-temporal changes of land cover, providing critical support for multi-disciplinary applications (e.g., environmental monitoring, disaster assessment, and climate change studies). Existing methods focus either on extracting features from localized patches, or pursue processing entire images holistically, which leads to the cross temporal feature matching deviation and exhibiting sensitivity to radiometric and geometric noise. Following the above issues, we propose a dual-module collaboration guided hierarchical adaptive aggregation framework, namely HA2F, which consists of dynamic hierarchical feature calibration module (DHFCM) and noise-adaptive feature refinement module (NAFRM). The former dynamically fuses adjacent-level features through perceptual feature selection, suppressing irrelevant discrepancies to address multi-temporal feature alignment deviations. The NAFRM utilizes the dual feature selection mechanism to highlight the change sensitive regions and generate spatial masks, suppressing the interference of irrelevant regions or shadows. Extensive experiments verify the effectiveness of the proposed HA2F, which achieves state-of-the-art performance on LEVIR-CD, WHU-CD, and SYSU-CD datasets, surpassing existing comparative methods in terms of both precision metrics and computational efficiency. In addition, ablation experiments show that DHFCM and NAFRM are effective. \href{https://huggingface.co/InPeerReview/RemoteSensingChangeDetection-RSCD.HA2F}{HA2F Official Code is Available Here!}

</details>


### [25] [X-Aligner: Composed Visual Retrieval without the Bells and Whistles](https://arxiv.org/abs/2601.16582)
*Yuqian Zheng,Mariana-Iuliana Georgescu*

Main category: cs.CV

TL;DR: 提出一种利用VLM并逐步跨模态对齐的CoVR框架，在Webvid-CoVR上达SOTA（R@1=63.93%），并在CIR任务上零样本泛化良好。


<details>
  <summary>Details</summary>
Motivation: 现有组合视频检索多采用单阶段融合视觉与文本，增益有限；需要更有效利用大规模预训练VLM的表征能力，并实现更细粒度的跨模态对齐与泛化。

Method: 在BLIP/BLIP-2上引入跨注意力模块X-Aligner，分层逐步融合视觉与文本并与目标视频表征对齐；额外将视觉查询的描述字幕作为输入以增强多模态查询表征；两阶段训练以保留VLM能力：阶段一仅训练新模块，阶段二再微调文本查询编码器；在Webvid-CoVR上训练，并做CIR零样本评估。

Result: 在Webvid-CoVR-Test上取得SOTA，Recall@1=63.93%；在CIRCO与Fashion-IQ上进行零样本评估，表现强劲。

Conclusion: 逐步跨模态对齐与两阶段训练有效激活VLM表征能力，显著提升CoVR表现并具备良好的跨任务泛化能力。

Abstract: Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.

</details>


### [26] [A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling](https://arxiv.org/abs/2601.16608)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出一种轻量级医学影像分类框架：用SimCLR对MobileNetV2做自监督预训练，并嵌入轻量PQC量子特征增强模块，少量标注上微调；在2–3M参数与低算力下优于不含自监督或量子增强的基线。


<details>
  <summary>Details</summary>
Motivation: 医学影像智能分析受限于标注稀缺、算力受限与模型泛化不足，需要一种在资源受限场景仍能取得高性能的解决方案。

Method: 以MobileNetV2为紧凑骨干，在无标注图像上采用SimCLR式对比自监督预训练；在网络中嵌入轻量参数化量子电路(PQC)作为量子特征增强模块，构建混合经典-量子架构；随后在少量标注数据上进行微调。

Result: 在约2–3M参数与低计算开销下，所提方法在Accuracy、AUC、F1-score上稳定优于不使用自监督或不含量子增强的经典基线；可视化显示特征判别性与表示稳定性提升。

Conclusion: 该框架在资源受限条件下实现高性能医学AI，证明自监督+量子增强的混合范式的有效性与可行性，具备实用与前瞻意义。

Abstract: Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is employed as a compact backbone and pretrained using a SimCLR-style self-supervised paradigm on unlabeled images. A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module, forming a hybrid classical-quantum architecture, which is subsequently fine-tuned on limited labeled data. Experimental results demonstrate that, with only approximately 2-3 million parameters and low computational cost, the proposed method consistently outperforms classical baselines without self-supervised learning or quantum enhancement in terms of Accuracy, AUC, and F1-score. Feature visualization further indicates improved discriminability and representation stability. Overall, this work provides a practical and forward-looking solution for high-performance medical artificial intelligence under resource-constrained settings.

</details>


### [27] [Boundary and Position Information Mining for Aerial Small Object Detection](https://arxiv.org/abs/2601.16617)
*Rongxin Huang,Guangfeng Lin,Wenbo Zhou,Zhirong Li,Wenhuan Wu*

Main category: cs.CV

TL;DR: 提出BPIM框架，通过位置与边界信息挖掘与跨尺度融合，提升无人机小目标检测的精度，在VisDrone2021、DOTA1.0、WiderPerson上优于YOLOv5-P2且计算量相近。


<details>
  <summary>Details</summary>
Motivation: 无人机场景中小目标因尺度不平衡与边缘模糊导致检测困难，现有方法对小目标的边界与位置信息利用不足，需一种有效融合边界、位置与多尺度语义的机制。

Method: 提出BPIM框架，包含五个模块：1) PIG位置信息指导，提取与传播位置信息；2) BIG边界信息指导，显式建模与强化目标边缘；3) CSF跨尺度融合，逐级汇聚浅层细节特征；4) TFF三特征融合，逐步结合位置、边界与语义信息；5) AWF自适应权重融合，以可学习权重灵活融合深层语义与边界上下文。整体以注意力与跨尺度特征融合策略实现对小目标的敏感表征。

Result: 在VisDrone2021、DOTA1.0、WiderPerson数据集上，相比基线YOLOv5-P2取得更好检测性能，并在同类SOTA方法中达到有竞争力的结果，同时保持相近的计算开销。

Conclusion: BPIM有效联合边界、位置与尺度信息，提升小目标检测的可分辨性与感知性；通过自适应加权与跨尺度位置融合，改善上下文判别力与小目标感知，具备较优的精度-计算权衡。

Abstract: Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.

</details>


### [28] [SCHIGAND: A Synthetic Facial Generation Mode Pipeline](https://arxiv.org/abs/2601.16627)
*Ananya Kadali,Sunnie Jehan-Morrison,Orasiki Wellington,Barney Evans,Precious Durojaiye,Richard Guest*

Main category: cs.CV

TL;DR: SCHIGAND提出一条融合StyleCLIP、HyperStyle、InterfaceGAN与扩散模型的合成脸生成管线，在保证身份一致性的同时提升真实感与多样性，用于生物识别测试，并在ArcFace评估下表现出可与真实数据相近的效果。


<details>
  <summary>Details</summary>
Motivation: 现有面部生物识别需要多样且高质量的数据集，但受隐私法规、数据稀缺与伦理约束；现有生成模型在真实感、多样性与身份保持三者难以兼顾，限制了合成数据在生物识别中的实用性。

Method: 构建名为SCHIGAND的管线，整合StyleCLIP的文本/语义可控编辑、HyperStyle的反演与细粒度重建、InterfaceGAN的可分离语义方向操控，以及扩散模型的高保真生成能力；通过管控类内变化（姿态、光照、表情等）与类间可分性，生成可控且身份保持的合成样本；再用ArcFace对生成数据的验证性能进行评估并与真实数据对比。

Result: 实验显示，在ArcFace评估下，SCHIGAND生成的数据在图像质量与多样性间取得更佳平衡，同时提升身份保持与类间区分度，相较既有方法在生物识别测试中的有效性更高。

Conclusion: SCHIGAND能够作为补充，甚至在部分场景替代真实人脸数据，用于隐私合规、可扩展的生物识别数据集构建，缓解现有生成方法的局限。

Abstract: The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation.

</details>


### [29] [Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss](https://arxiv.org/abs/2601.16645)
*Minsu Gong,Nuri Ryu,Jungseul Ok,Sunghyun Cho*

Main category: cs.CV

TL;DR: 提出一种训练-free的结构保持损失（SPL），在扩散生成过程中约束局部结构，从而在潜空间扩散图像编辑中显著提升边缘与几何的一致性，并辅以后处理、掩膜与颜色保持以获得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 潜变量扩散用于文本驱动的图像编辑很灵活，但难以在像素层面保持边缘/结构，导致风格迁移、色调调整等任务中出现形变、走样和解码失真。需要一种不依赖再训练、可直接约束结构的机制。

Method: 提出Structure Preservation Loss（SPL）：基于局部线性模型度量输入与编辑图像的结构差异，将SPL直接并入扩散反演/前向生成步骤以引导采样；此外加入解码失真后处理、区域化掩膜编辑、以及对未编辑区域的颜色保持损失，形成完整流水线。方法为训练-free，可直接作用于现有LDM编辑流程。

Result: 在多种潜扩散编辑任务和数据上，SPL显著提升结构保真度，边缘对齐更好，减少几何扭曲与伪影；相较现有LDM编辑方法达到或刷新SOTA。可视化与定量评估均证实有效。

Conclusion: SPL为潜扩散编辑提供了简洁、训练-free的结构约束，结合后处理、掩膜与颜色保持，可在不牺牲编辑灵活性的前提下显著提升结构与色彩一致性，适用于多种文本驱动编辑场景。

Abstract: Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.

</details>


### [30] [Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training](https://arxiv.org/abs/2601.16652)
*Aurora Pia Ghiardelli,Guangzhi Tang,Tao Sun*

Main category: cs.CV

TL;DR: 提出一种基于脉冲神经网络（SNN）的3D脑肿瘤分割框架，采用三视角集成实现体素级不确定性估计，并用FPTT显著降低训练计算量；在BraTS 2017/2023上取得有竞争力精度与良好校准的不确定性，同时FLOPs降87%，适用于低功耗医疗物联网与即时检测场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D脑肿瘤分割多依赖ANN/Transformer，计算耗能高、难以在低功耗设备部署，且不确定性估计与可靠性不足。作者希望用生物启发的SNN在保持时序学习能力的同时，提供可靠分割与能效优势，并解决SNN在语义分割训练中计算开销大的问题。

Method: 1) 构建三视角（矢状/冠状/轴状）SNN分割模型的集成，对同一体数据进行推理，融合输出以提升鲁棒性并给出体素级不确定性；2) 采用FPTT（Forward Propagation Through Time）替代传统BPTT，保留时间依赖建模能力同时显著降低训练计算成本；3) 在BraTS 2017与2023数据上评估精度与不确定性校准与能耗指标（FLOPs）。

Result: 相较常规训练与/或ANN基线，获得有竞争力的分割精度与更好校准的不确定性；计算复杂度（FLOPs）降低约87%。

Conclusion: SNN结合三视角集成与FPTT可在保证或接近SOTA精度的同时显著降低计算成本并提供可靠不确定性估计，适合低功耗医疗IoT与床旁诊疗部署。

Abstract: We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems.

</details>


### [31] [ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction](https://arxiv.org/abs/2601.16672)
*Ming Li,Hui Shan,Kai Zheng,Chentao Shen,Siyu Liu,Yanwei Fu,Zhen Chen,Xiangru Huang*

Main category: cs.CV

TL;DR: 提出ReWeaver，从少量多视角RGB重建拓扑准确的服装与缝制版型，输出结构化2D-3D表示，优于现有方法；并发布包含10万+样本的GCD-TS数据集。


<details>
  <summary>Details</summary>
Motivation: 现有服装重建多用非结构化表示（如3D Gaussian Splat），难以恢复缝线、版片与其拓扑连接，导致不适用于高保真物理仿真与机器人操作，需要能从稀疏视角精准恢复服装拓扑与缝制结构的方法与数据。

Method: 提出ReWeaver：输入≥4视角RGB，联合预测2D UV空间和3D空间中的缝线（seams）、版片（panels）及其连通关系，使预测与多视图观测严格对齐，输出结构化2D-3D服装表示；并构建大规模合成数据集GCD-TS，含多视图图像、3D服装几何、贴图人体网格与标注版型，用于监督训练。

Result: 在拓扑准确性、几何对齐度和缝线-版片一致性上持续优于现有方法；在稀疏视角（如4视图）下仍能稳定重建结构化服装。

Conclusion: ReWeaver实现从稀疏多视图到拓扑准确、可仿真的2D-3D服装与缝制结构重建，并配套GCD-TS数据集，为3D感知、物理仿真与机器人操控提供高质量输入。

Abstract: High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.

</details>


### [32] [Affinity Contrastive Learning for Skeleton-based Human Activity Understanding](https://arxiv.org/abs/2601.16694)
*Hongda Liu,Yunfan Liu,Min Ren,Lin Sui,Yunlong Wang,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出ACLNet，通过亲和度对比学习在骨架行为理解中更好地区分类别、处理异常正样本，并在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作理解的对比学习方法忽视了类间结构相似性与异常正样本的影响，导致特征空间判别性不足、聚类结构不清晰。

Method: 提出Affinity Contrastive Learning：1) 亲和度度量重估样本/类别相似性，构建活动“超类”，为对比学习提供更有信息量的正负对；2) 动态温度调度，根据超类难度自适应调整对比损失惩罚强度；3) 基于边距的对比策略，增强类内难正/难负的可分离性。模型称为ACLNet。

Result: 在NTU RGB+D 60/120、Kinetics-Skeleton、PKU-MMD、FineGYM、CASIA-B等多个任务（动作识别、步态识别、行人再识别）上取得领先性能，验证方法有效性与泛化性。

Conclusion: 利用超类级别的亲和对比学习与动态温度、边距设计，可更好地刻画类间簇结构、抑制异常正样本，显著提升骨架基础的人体活动理解性能。

Abstract: In skeleton-based human activity understanding, existing methods often adopt the contrastive learning paradigm to construct a discriminative feature space. However, many of these approaches fail to exploit the structural inter-class similarities and overlook the impact of anomalous positive samples. In this study, we introduce ACLNet, an Affinity Contrastive Learning Network that explores the intricate clustering relationships among human activity classes to improve feature discrimination. Specifically, we propose an affinity metric to refine similarity measurements, thereby forming activity superclasses that provide more informative contrastive signals. A dynamic temperature schedule is also introduced to adaptively adjust the penalty strength for various superclasses. In addition, we employ a margin-based contrastive strategy to improve the separation of hard positive and negative samples within classes. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, Kinetics-Skeleton, PKU-MMD, FineGYM, and CASIA-B demonstrate the superiority of our method in skeleton-based action recognition, gait recognition, and person re-identification. The source code is available at https://github.com/firework8/ACLNet.

</details>


### [33] [CER-HV: A CER-Based Human-in-the-Loop Framework for Cleaning Datasets Applied to Arabic-Script HTR](https://arxiv.org/abs/2601.16713)
*Sana Al-azzawi,Elisa Barney,Marcus Liwicki*

Main category: cs.CV

TL;DR: 提出CER-HV框架，用基于CER的噪声检测+人工核验清洗阿拉伯字母系HTR数据；并用精心配置且早停的CRNN在多数数据集上达SOTA，清洗后进一步降CER，方法可泛化到其他HTR。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯字母系HTR落后于拉丁体系，作者认为除模型与数据规模外，数据标注质量是关键瓶颈，现有数据集存在未充分报道的标签与样本问题，需要系统化的错误检测与清洗流程。

Method: 提出CER-HV：1) 训练带早停的CRNN作为基模型，利用预测序列与标签的CER对样本进行噪声打分，并通过配置避免对噪声样本过拟合；2) 对高排名可疑样本引入人类在环审核与修正；3) 用于发现转写、切分、朝向与非文本内容等错误，并统计精确率。

Result: 数据问题在多数据集被揭示：在Muharaf上以约90%精确率、在PHTI上80–86%精确率识别错误样本。CRNN在6个数据集中的5个达SOTA：KHATT 8.45% CER、PHTI 8.26%、Ajami 10.66%、Muharaf 10.11%、PHTD基线11.3%。应用CER-HV后，较干净数据集CER下降0.3–0.6%，较嘈杂数据集下降1.0–1.8%。

Conclusion: 数据质量显著限制阿拉伯字母系HTR性能。CER-HV以低成本高精度发现并清洗标签错误，结合强基线CRNN可显著提升评测CER；框架通用，可迁移到其他文字识别数据集。

Abstract: Handwritten text recognition (HTR) for Arabic-script languages still lags behind Latin-script HTR, despite recent advances in model architectures, datasets, and benchmarks. We show that data quality is a significant limiting factor in many published datasets and propose CER-HV (CER-based Ranking with Human Verification) as a framework to detect and clean label errors. CER-HV combines a CER-based noise detector, built on a carefully configured Convolutional Recurrent Neural Network (CRNN) with early stopping to avoid overfitting noisy samples, and a human-in-the-loop (HITL) step that verifies high-ranking samples. The framework reveals that several existing datasets contain previously underreported problems, including transcription, segmentation, orientation, and non-text content errors. These have been identified with up to 90 percent precision in the Muharaf and 80-86 percent in the PHTI datasets.
  We also show that our CRNN achieves state-of-the-art performance across five of the six evaluated datasets, reaching 8.45 percent Character Error Rate (CER) on KHATT (Arabic), 8.26 percent on PHTI (Pashto), 10.66 percent on Ajami, and 10.11 percent on Muharaf (Arabic), all without any data cleaning. We establish a new baseline of 11.3 percent CER on the PHTD (Persian) dataset. Applying CER-HV improves the evaluation CER by 0.3-0.6 percent on the cleaner datasets and 1.0-1.8 percent on the noisier ones. Although our experiments focus on documents written in an Arabic-script language, including Arabic, Persian, Urdu, Ajami, and Pashto, the framework is general and can be applied to other text recognition datasets.

</details>


### [34] [Using Shadows in Circular Synthetic Aperture Sonar Imaging for Target Analysis](https://arxiv.org/abs/2601.16733)
*Yann Le Gall,Nicolas Burlet,Mathieu Simon,Fabien Novella,Samantha Dugelay,Jean-Philippe Malkasse*

Main category: cs.CV

TL;DR: 该论文针对圆形合成孔径声呐（CSAS）在全方位成像时丢失阴影信息的问题，提出通过子孔径滤波与固定焦点阴影增强（FFSE）恢复并利用阴影，辅以交互可视化与基于阴影分割的空间雕刻，实现目标分析改进与3D重建，验证了阴影在CSAS中的价值。


<details>
  <summary>Details</summary>
Motivation: CSAS能提供360°海底视角并获得高分辨率图像，但由于环形运动导致视差填充阴影区，使传统对目标识别极其有用的阴影特征被削弱/丢失；在水雷作战中，多视角与阴影信息对降低误报与辨析目标形状至关重要，因此需要从CSAS数据中恢复阴影以提升识别与三维重建能力。

Method: 1) 子孔径滤波：将圆轨迹划分为多个视角子孔径，生成多视点图像；2) 固定焦点阴影增强（FFSE）：在各视角图像中对阴影进行聚焦与锐化；3) 交互式界面：沿圆轨迹可视化与浏览阴影；4) 阴影分割与空间雕刻：基于多视角阴影掩膜进行空间一致性约束，进行体素级“空间雕刻”推断目标3D形状。

Result: 通过实验展示：在CSAS数据中可有效恢复并增强多视角阴影；交互工具有助于沿轨迹理解目标投影与遮挡；利用分割阴影进行空间雕刻可得到合理的目标三维形状重建，并提升目标分析能力。

Conclusion: CSAS虽牺牲了传统阴影，但可通过子孔径与FFSE恢复并利用阴影；结合交互可视化与空间雕刻，能改进目标识别并实现3D重建，表明阴影在圆形SAS中的重要性与可用性。

Abstract: Circular Synthetic Aperture Sonar (CSAS) provides a 360° azimuth view of the seabed, surpassing the limited aperture and mono-view image of conventional side-scan SAS. This makes CSAS a valuable tool for target recognition in mine warfare where the diversity of point of view is essential for reducing false alarms. CSAS processing typically produces a very high-resolution two-dimensional image. However, the parallax introduced by the circular displacement of the illuminator fill-in the shadow regions, and the shadow cast by an object on the seafloor is lost in favor of azimuth coverage and resolution. Yet the shadows provide complementary information on target shape useful for target recognition. In this paper, we explore a way to retrieve shadow information from CSAS data to improve target analysis and carry 3D reconstruction. Sub-aperture filtering is used to get a collection of images at various points of view along the circular trajectory and fixed focus shadow enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface is also proposed to allow human operators to visualize these shadows along the circular trajectory. A space-carving reconstruction method is applied to infer the 3D shape of the object from the segmented shadows. The results demonstrate the potential of shadows in circular SAS for improving target analysis and 3D reconstruction.

</details>


### [35] [A Step to Decouple Optimization in 3DGS](https://arxiv.org/abs/2601.16736)
*Renjie Ding,Yaonan Wang,Min Liu,Jialin Zhu,Jiazheng Wang,Jiahao Zhao,Wenting Shen,Feixiang He,Xiang Che*

Main category: cs.CV

TL;DR: 论文重新审视3D Gaussian Splatting（3DGS）的优化过程，指出两类“耦合”问题并提出解耦与重组策略，最终设计出AdamW-GS以同时提升优化效率与表示效果。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽为显式表示，但其优化沿用DNN常规做法（同步更新、Adam自适应等），忽视了3DGS物理属性与特殊设计带来的两类耦合：更新步长耦合导致状态缩放与视野外属性代价，动量中的梯度耦合导致正则失衡。作者希望系统分析并修复这些隐患。

Method: 对3DGS优化进行解耦与重组：（1）Sparse Adam：针对更新步耦合，稀疏化/解耦状态与更新，避免无关属性代价并稳定缩放；（2）Re-State Regularization：重新设计对优化器状态的正则与缩放关系；（3）Decoupled Attribute Regularization：将正则从动量中解耦，单独施加到属性以避免过/欠正则。大量实验在3DGS与3DGS-MCMC框架中分析各组件作用。

Result: 实验证明解耦组件带来更稳定与高效的训练，减少无效更新与状态漂移，提升重建质量与收敛速度。在此基础上，提出AdamW-GS（重新耦合有益部分）表现优于原始Adam设置。

Conclusion: 3DGS优化存在被忽略的耦合问题；通过稀疏化更新、状态正则重设与属性正则解耦可系统改进训练。AdamW-GS综合这些优势，实现更高的优化效率与更好的表示效果。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization. Nevertheless, such a complex coupling is under-explored. After revisiting the optimization of 3DGS, we take a step to decouple it and recompose the process into: Sparse Adam, Re-State Regularization and Decoupled Attribute Regularization. Taking a large number of experiments under the 3DGS and 3DGS-MCMC frameworks, our work provides a deeper understanding of these components. Finally, based on the empirical analysis, we re-design the optimization and propose AdamW-GS by re-coupling the beneficial components, under which better optimization efficiency and representation effectiveness are achieved simultaneously.

</details>


### [36] [Automated Road Crack Localization to Guide Highway Maintenance](https://arxiv.org/abs/2601.16737)
*Steffen Knoblauch,Ram Kumar Muthusamy,Pedram Ghamisi,Alexander Zipf*

Main category: cs.CV

TL;DR: 研究利用开源数据（机载影像+OSM）微调YOLOv11定位高速公路裂缝，并构建瑞士相对裂缝密度指数（RHCD）用于全国维护决策；模型在裂缝/非裂缝分类上取得较高F1，RHCD与温度振幅和交通量相关性弱，但在城市与枢纽附近值偏高，显示指数对维护规划的独立与实用价值。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致温度波动加剧，路面受力与损伤增多，维护成本攀升；现有维护多依赖昂贵专有数据或传统巡检，缺乏可扩展、低成本、全国尺度的及时指标。作者希望验证开源数据与轻量深度学习是否能提供可操作的裂缝检测与区域化维护优先级量化工具。

Method: 整合开源机载遥感影像与OSM道路数据，构建高速路段样本；微调YOLOv11进行裂缝目标定位/分类；在推理结果上计算瑞士范围的相对高速裂缝密度（RHCD）指数；将RHCD与长期地表温度振幅（LT-LST-A）与交通量（TV）做皮尔逊相关分析，并进行空间情境验证（城市、互通附近）。

Result: YOLOv11裂缝分类F1：正类0.84、负类0.97；RHCD与LT-LST-A的相关系数r=-0.05，与TV的r=0.17，均较弱；RHCD在城市中心与交叉口附近显著偏高，空间分布与常识一致。

Conclusion: 开源影像与OSM可支撑有效的裂缝检测与维护指引指数；RHCD提供了区别于温度与交通量的补充信息，可用于全国尺度维护优先级制定；鼓励数据共享以促进公共部门更高效的基础设施维护创新。

Abstract: Highway networks are crucial for economic prosperity. Climate change-induced temperature fluctuations are exacerbating stress on road pavements, resulting in elevated maintenance costs. This underscores the need for targeted and efficient maintenance strategies. This study investigates the potential of open-source data to guide highway infrastructure maintenance. The proposed framework integrates airborne imagery and OpenStreetMap (OSM) to fine-tune YOLOv11 for highway crack localization. To demonstrate the framework's real-world applicability, a Swiss Relative Highway Crack Density (RHCD) index was calculated to inform nationwide highway maintenance. The crack classification model achieved an F1-score of $0.84$ for the positive class (crack) and $0.97$ for the negative class (no crack). The Swiss RHCD index exhibited weak correlations with Long-term Land Surface Temperature Amplitudes (LT-LST-A) (Pearson's $r\ = -0.05$) and Traffic Volume (TV) (Pearson's $r\ = 0.17$), underlining the added value of this novel index for guiding maintenance over other data. Significantly high RHCD values were observed near urban centers and intersections, providing contextual validation for the predictions. These findings highlight the value of open-source data sharing to drive innovation, ultimately enabling more efficient solutions in the public sector.

</details>


### [37] [Curated endoscopic retrograde cholangiopancreatography images dataset](https://arxiv.org/abs/2601.16759)
*Alda João Andrade,Mónica Martins,André Ferreira,Tarcísio Araújo,Luís Lopes,Victor Alves*

Main category: cs.CV

TL;DR: 该论文构建并发布了一个大规模、经专家标注与质控的ERCP图像数据集（约3.8万张图像，其中5,519张带标签），并通过分类实验验证其有效性，旨在为胰胆疾病自动化分析与诊断提供基准数据。


<details>
  <summary>Details</summary>
Motivation: AI在ERCP自动化诊断中潜力巨大，但公开数据集稀缺、规模不足且缺少高质量标注，制约了模型研发、比较与落地。本研究试图通过提供大规模、经严格标注和审核的数据集来弥补这一空缺，并促进基准化评测。

Method: 从1,602名患者收集19,018张原始图像与19,317张处理后图像；其中5,519张提供专家标注。两名具有>5年经验且年ERCP量>400台的消化内镜医师逐图检查与标注，另有>20年经验、年量>400台的资深专家复核。随后以图像分类实验对数据集的可用性与有效性进行验证。

Result: 构建了规模化、质量控制严格的ERCP图像数据集，并在分类任务上取得可证明的数据效用（具体指标未在摘要披露），表明该数据集可支持自动化分析模型的训练与评估。

Conclusion: 该数据集为胰胆疾病的自动化ERCP分析与诊断提供了重要基础与潜在基准，有望推动算法比较、模型开发与临床转化。

Abstract: Endoscopic Retrograde Cholangiopancreatography (ERCP) is a key procedure in the diagnosis and treatment of biliary and pancreatic diseases. Artificial intelligence has been pointed as one solution to automatize diagnosis. However, public ERCP datasets are scarce, which limits the use of such approach. Therefore, this study aims to help fill this gap by providing a large and curated dataset. The collection is composed of 19.018 raw images and 19.317 processed from 1.602 patients. 5.519 images are labeled, which provides a ready to use dataset. All images were manually inspected and annotated by two gastroenterologist with more than 5 years of experience and reviewed by another gastroenterologist with more than 20 years of experience, all with more than 400 ERCP procedures annually. The utility and validity of the dataset is proven by a classification experiment. This collection aims to provide or contribute for a benchmark in automatic ERCP analysis and diagnosis of biliary and pancreatic diseases.

</details>


### [38] [Flow Matching for Probabilistic Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.16763)
*Cuong Le,Pavló Melnyk,Bastian Wandt,Mårten Wadenbäck*

Main category: cs.CV

TL;DR: FMPose提出以流匹配(Flow Matching)为核心的概率式单目3D人体姿态估计：在2D关节点条件下，通过连续归一化流学习从简单先验到可行3D姿态分布的最优传输，较扩散法更快且更准，并在Human3.6M、MPI-INF-3DHP、3DPW上显著超SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态受深度歧义影响严重，传统从2D抬升易产生自信却错误的确定性结果。概率方法可建模不确定性，但现有多基于扩散，采样慢、可能精度受限。需一种既能表达不确定性又高效、准确的生成式方案。

Method: 以Flow Matching/连续归一化流学习条件分布：在2D关节点条件下，优化从简单源分布到目标3D姿态分布的最优传输向量场，实现快速生成。条件编码采用图卷积网络，利用可学习的人体关节图结构进行特征聚合。与扩散不同，直接学习连续流场，无需长链反向扩散。

Result: 在Human3.6M、MPI-INF-3DHP、3DPW三大基准上取得显著优于当前SOTA的误差指标，并在推理速度上快于扩散式方法。

Conclusion: FMPose通过基于最优传输的流匹配实现高效的概率3D人体姿态生成，兼顾不确定性建模、精度与速度，优于扩散范式，适用于单目3D姿态估计场景。

Abstract: Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW.

</details>


### [39] [AutoRegressive Generation with B-rep Holistic Token Sequence Representation](https://arxiv.org/abs/2601.16771)
*Jiahao Li,Yunpeng Bai,Yongkang Dai,Hao Guo,Hongping Gan,Yilei Shi*

Main category: cs.CV

TL;DR: 提出BrepARG：把B-rep的几何与拓扑统一编码为可自回归生成的整体token序列，并用Transformer解码器做下一token预测，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有B-rep方法多用图表示并将几何与拓扑解耦，难以直接使用在序列建模框架（如Transformer）上，限制了生成性能与可扩展性。

Method: 1) 设计三类token：几何token、位置token（几何位置/参数）、面索引token（拓扑）。2) 以层次化方式构造整体序列：先用上述token构造几何块（面、边），再对几何块排序，最终得到完整B-rep序列。3) 采用仅解码、因果掩码的多层Transformer，自回归地学习整体序列的分布并进行下一token预测。

Result: 在实验中，该方法在B-rep生成任务上取得SOTA效果，优于以往基于图的表示与生成管线。

Conclusion: 证明了将B-rep表示为整体token序列的可行性，为基于序列建模的B-rep生成开辟了新方向。

Abstract: Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first attempt to encode B-rep's geometry and topology into a holistic token sequence representation, enabling sequence-based B-rep generation with an autoregressive architecture. Specifically, BrepARG encodes B-rep into 3 types of tokens: geometry and position tokens representing geometric features, and face index tokens representing topology. Then the holistic token sequence is constructed hierarchically, starting with constructing the geometry blocks (i.e., faces and edges) using the above tokens, followed by geometry block sequencing. Finally, we assemble the holistic sequence representation for the entire B-rep. We also construct a transformer-based autoregressive model that learns the distribution over holistic token sequences via next-token prediction, using a multi-layer decoder-only architecture with causal masking. Experiments demonstrate that BrepARG achieves state-of-the-art (SOTA) performance. BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation.

</details>


### [40] [CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts](https://arxiv.org/abs/2601.16773)
*Shuai Huang,Xuhan Lin,Yuwu Lu*

Main category: cs.CV

TL;DR: 提出CASP，在FSCIL中通过对CLS token注意力进行可学习引导，并结合注意力扰动与流形Token混合，提高极少样本增量学习的泛化与稳定性，在多数据集上超越SOTA且参数开销小、增量阶段无需微调。


<details>
  <summary>Details</summary>
Motivation: FSCIL需在极少样本增量下快速适配新类且避免灾忘。现有基于提示的方法虽利用预训练骨干，但在极端few-shot时迁移与泛化不足，缺乏在基阶段学到能跨未来类别共享的表征能力。CLS token具有人类注意般逐步滤除无关信息的特性，若能显式调控其自注意力，可更好地保留可迁移的共享特征。

Method: 提出CLS Token Attention Steering Prompts (CASP)：在CLS token的Q/K/V投影中注入类共享的可训练偏置，显式调制自注意力权重；引入注意力扰动策略增强鲁棒与泛化；在浅层特征空间进行Manifold Token Mixup，合成潜在新类特征，提升对未来任务的表征容量。整个框架在基阶段训练，增量阶段无需微调。

Result: 在CUB200、CIFAR100、ImageNet-R上，CASP在标准与细粒度FSCIL设置下超越现有SOTA，同时显著降低参数开销与增量阶段计算需求。

Conclusion: 通过对CLS注意力的可控引导与特征空间的合成扰动，CASP在不增加增量阶段训练的前提下提升了FSCIL的跨类泛化与抗灾忘能力，提供了高效低开销的增量学习方案。

Abstract: Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model's ability to transfer and generalize becomes critical, and it is thus essential to leverage pretrained knowledge to learn feature representations that can be shared across future categories during the base session. Inspired by the mechanism of the CLS token, which is similar to human attention and progressively filters out task-irrelevant information, we propose the CLS Token Attention Steering Prompts (CASP). This approach introduces class-shared trainable bias parameters into the query, key, and value projections of the CLS token to explicitly modulate the self-attention weights. To further enhance generalization, we also design an attention perturbation strategy and perform Manifold Token Mixup in the shallow feature space, synthesizing potential new class features to improve generalization and reserve the representation capacity for upcoming tasks. Experiments on the CUB200, CIFAR100, and ImageNet-R datasets demonstrate that CASP outperforms state-of-the-art methods in both standard and fine-grained FSCIL settings without requiring fine-tuning during incremental phases and while significantly reducing the parameter overhead.

</details>


### [41] [SLD: Segmentation-Based Landmark Detection for Spinal Ligaments](https://arxiv.org/abs/2601.16782)
*Lara Blomenkamp,Ivanna Kramer,Sabine Bauer,Theresa Schöche*

Main category: cs.CV

TL;DR: 提出一种两步法：先进行椎体三维形状分割，再用领域规则定位韧带附着点，在全脊柱上实现高精度与良好泛化（MAE 0.7 mm、RMSE 1.1 mm）。


<details>
  <summary>Details</summary>
Motivation: 脊柱生物力学建模中，韧带附着点决定了跨椎体力矢量的准确性；现有自动检测方法要么仅适用于局部脊柱区域，要么精度不足，限制了可靠、通用的脊柱建模。

Method: 1) 对三维椎体进行形状驱动的分割与解析；2) 基于解剖先验与领域规则对不同类型韧带附着点进行自动识别与定位；在多数据集、多患者上进行验证。

Result: 在两个独立的多患者脊柱数据集上取得MAE 0.7 mm、RMSE 1.1 mm，优于已有方法，并在所有脊柱节段均表现出强泛化能力。

Conclusion: 形状分割结合领域规则的策略可在全脊柱范围内稳定高精度地自动定位韧带附着点，为构建更可靠的脊柱生物力学模型提供基础。

Abstract: In biomechanical modeling, the representation of ligament attachments is crucial for a realistic simulation of the forces acting between the vertebrae. These forces are typically modeled as vectors connecting ligament landmarks on adjacent vertebrae, making precise identification of these landmarks a key requirement for constructing reliable spine models. Existing automated detection methods are either limited to specific spinal regions or lack sufficient accuracy. This work presents a novel approach for detecting spinal ligament landmarks, which first performs shape-based segmentation of 3D vertebrae and subsequently applies domain-specific rules to identify different types of attachment points. The proposed method outperforms existing approaches by achieving high accuracy and demonstrating strong generalization across all spinal regions. Validation on two independent spinal datasets from multiple patients yielded a mean absolute error (MAE) of 0.7 mm and a root mean square error (RMSE) of 1.1 mm.

</details>


### [42] [REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion](https://arxiv.org/abs/2601.16788)
*Xuewei Li,Xinghan Bao,Zhimin Chen,Xi Li*

Main category: cs.CV

TL;DR: 论文提出REL-SF4PASS：一种结合圆柱坐标深度表征（REL）与球面动态多模态融合（SMMF）的全景语义分割方法，在Stanford2D3D上平均mIoU提升2.35%，对3D扰动的方差下降约70%。


<details>
  <summary>Details</summary>
Motivation: 现有PASS多依赖球面几何+RGB或原始/HHA深度，但未充分利用全景图固有几何（如圆柱展开），且在ERP投影下侧面展开处容易破碎，融合策略缺乏区域适配性，导致鲁棒性与精度受限。

Method: 1) 提出REL深度表征：在圆柱坐标系下由三部分组成——Rectified Depth（校正深度）、Elevation-Gained Vertical Inclination Angle（包含海拔增益的垂直倾角）、Lateral Orientation Angle（横向方位角），同时编码3D位置与表面法向趋势。2) 提出SMMF（Spherical-dynamic Multi-Modal Fusion）：根据全景图不同区域（尤其圆柱侧面/极区）的特性采用差异化融合策略，提升多模态（RGB+REL等）在ERP投影下的稳健性，缓解侧面展开破碎问题。

Result: 在Stanford2D3D全景基准上，3个fold的平均mIoU提升2.35%；在面对3D扰动时，性能方差约降低70%，显示更高鲁棒性与稳定性。

Conclusion: 充分利用圆柱坐标几何的REL表征结合区域自适应的SMMF融合，可显著提升PASS精度与鲁棒性，特别是在ERP展开造成的边界破碎与3D扰动情境下。

Abstract: As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.

</details>


### [43] [Incorporating Eye-Tracking Signals Into Multimodal Deep Visual Models For Predicting User Aesthetic Experience In Residential Interiors](https://arxiv.org/abs/2601.16811)
*Chen-Ying Chien,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出一种融合视觉特征与眼动信号的双分支CNN-LSTM模型，用于预测室内空间的多维审美评价；在224段室内视频与28名参与者数据上，相比视频基线显著提升，尤其在主观维度；眼动作为特权信息训练可提升泛化，部署时仅视觉也能保持性能可用。


<details>
  <summary>Details</summary>
Motivation: 审美体验预测因主观性强与视觉反应复杂而困难，现有视频模型对主观评价（如放松感）刻画不足。引入眼动数据可提供人类注意与生理反应线索，期望提升对室内审美的可解释与可预测性，并形成可用于设计实践的工具。

Method: 构建双分支CNN-LSTM：一支提取视频视觉时空特征，另一支编码眼动与瞳孔反应等时序信号；融合后回归/分类15个审美维度。数据集包含224个室内设计视频及28名被试的同步凝视与瞳孔数据，并进行消融实验（仅视觉、仅眼动、不同眼动成分）。

Result: 在客观维度（如光照）达到72.2%准确率，在主观维度（如放松）达66.8%，均优于最新视频基线，主观任务提升更明显。用眼动参与训练但推理时仅用视觉，性能仍与完全多模态接近。消融显示：瞳孔反应对客观评估贡献最大，凝视与视觉联合最利于主观评估。

Conclusion: 眼动信号作为训练阶段的特权信息可显著增强室内审美预测，尤其在主观维度；部署阶段可仅用视觉实现实用化。该框架为室内设计中的审美评估提供了更有效、可扩展的工具，并强调了瞳孔与凝视线索在不同维度中的差异化作用。

Abstract: Understanding how people perceive and evaluate interior spaces is essential for designing environments that promote well-being. However, predicting aesthetic experiences remains difficult due to the subjective nature of perception and the complexity of visual responses. This study introduces a dual-branch CNN-LSTM framework that fuses visual features with eye-tracking signals to predict aesthetic evaluations of residential interiors. We collected a dataset of 224 interior design videos paired with synchronized gaze data from 28 participants who rated 15 aesthetic dimensions. The proposed model attains 72.2% accuracy on objective dimensions (e.g., light) and 66.8% on subjective dimensions (e.g., relaxation), outperforming state-of-the-art video baselines and showing clear gains on subjective evaluation tasks. Notably, models trained with eye-tracking retain comparable performance when deployed with visual input alone. Ablation experiments further reveal that pupil responses contribute most to objective assessments, while the combination of gaze and visual cues enhances subjective evaluations. These findings highlight the value of incorporating eye-tracking as privileged information during training, enabling more practical tools for aesthetic assessment in interior design.

</details>


### [44] [ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models](https://arxiv.org/abs/2601.16836)
*Chenxi Ruan,Yu Xiao,Yihan Hou,Guosheng Hu,Wei Zeng*

Main category: cs.CV

TL;DR: 提出ColorConceptBench基准，用概率颜色分布评估T2I模型对隐含颜色概念的理解；结果显示主流模型对抽象语义不敏感，常规手段难以改进，需重新思考语义学习方式。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型多能处理显式颜色词，但对“隐含颜色概念”（如季节、情绪、物体类别带来的典型配色）缺少系统评估与理解。缺口在于模型是否能将抽象概念映射到人类一致的颜色分布。

Method: 构建ColorConceptBench：收集1,281个隐含颜色概念，基于6,369条人工标注形成每个概念到概率颜色分布的参考；用该基准对7个主流T2I模型进行系统评测，并测试常见干预（模型规模、引导强度等）对性能的影响。

Result: 各模型在隐含颜色概念上的颜色分布匹配度低，表现出对抽象语义的低敏感性；进一步扩大模型或调整推理引导等常规手段并未显著改善。

Conclusion: 仅靠更大规模或常规推理技巧无法获得类人颜色语义；需要在模型的语义学习与表示机制上进行更根本的变革，以捕获隐含意义与颜色之间的关联。

Abstract: While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.

</details>


### [45] [No Validation, No Problem: Predicting Model Performance from a Single Gradient](https://arxiv.org/abs/2601.16874)
*Fangzheng Wu,Brian Summa*

Main category: cs.CV

TL;DR: 提出一种无需验证集的checkpoint选择信号：单个前向-反向过程中，分类头权重梯度的Frobenius范数||g||_F=||dL/dW||_F。该信号与Top-1准确率强负相关、与训练损失正相关。选择尾段窗口内||g||_F最小的checkpoint，性能接近使用验证集的“oracle”。方法通用于CNN与Transformer，也适用于检测/分割与扩散模型，并可用于早停与监控，开销<0.1% epoch。


<details>
  <summary>Details</summary>
Motivation: 训练深度模型时常依赖验证集进行checkpoint选择与早停，但许多场景缺乏可用标签或希望节省验证成本，需要一种无需标签、开销极低、又能可靠指示泛化/性能的代理信号。

Method: 在一次前向-反向中，取一批从主干分离（detached）的特征，通过分类头计算梯度，对分类头参数梯度矩阵求Frobenius范数作为代理指标；并提出两类归一化：头尺度（head-scale）适用于经典CNN，特征尺度（feature-scale）适用于Transformer与现代CNN。训练过程中在一个短尾窗口内选择||g||_F最小的checkpoint；同一批次探针也可用于检测/分割（COCO）与扩散模型（UNet/DDPM）进度监控。

Result: 在ImageNet-1k上，此代理与Top-1强负相关，与loss正相关；用统一设置，尾窗最小梯度选择与oracle的Top-1差距约4.24%±2.00%，经过轻量族内调参可缩小到约1.12%。在COCO上，该探针能预测检测/分割mAP；在CIFAR-10扩散上，该指标与同分布探针MSE正相关、与FID负相关，并支持接近oracle的尾窗选择。运行开销<0.1% epoch。

Conclusion: 分类头梯度范数可作为通用、低成本、无标签的训练进度与性能代理，可用于验证集缺失或受限场景下的checkpoint选择与早停；通过合适归一化在不同架构族中表现稳定。

Abstract: We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the gap to the oracle (4.24% +/- 2.00% with a universal setup, about 1.12% with light per-family tuning). For practical deployment, a head-scale normalization is more stable within classic CNN families (e.g., ResNets), while a feature-scale normalization works well for Transformers and modern CNNs. The same one-batch probe also predicts COCO detection/segmentation mAP. In diffusion (UNet/DDPM on CIFAR-10), it tracks progress and enables near-oracle tail-window selection; it is positively correlated with same-distribution probe MSE and negatively with FID (lower is better), so it can be used as a lightweight, label-free monitor. Validation labels are never used beyond reporting. The probe adds much less than 0.1% of an epoch and works as a drop-in for validation-free checkpoint selection and early stopping.

</details>


### [46] [GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss](https://arxiv.org/abs/2601.16885)
*Yangfan Xu,Lilian Zhang,Xiaofeng He,Pengdong Wu,Wenqi Wu,Jun Mao*

Main category: cs.CV

TL;DR: 提出一种自监督训练VGGT的方法，在无标注序列上用时序一致的光度与几何联合约束，快速收敛并显著提升大规模定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有VGGT等Transformer视觉几何模型在相机位姿与三维重建上表现出色，但依赖真值标注，难以适配无标注、未知场景与大规模环境。需要一种无需硬标签、可利用视频/序列内部几何的训练方案。

Method: 将传统成对帧监督扩展为序列级自监督：在每个序列中采样多源帧并几何投影到多个目标帧，强化时序特征一致性；以物理光度一致性（重投影后的像素强度一致）与几何一致性（深度-位姿-投影的约束）构成联合优化损失，端到端训练VGGT的局部/全局跨视角注意力与相机/深度头，从而捕获多视几何。

Result: 模型在数百次迭代内即可收敛；在大规模场景定位任务上显著优于基线（定性与定量均提升）。

Conclusion: 自监督的序列级几何与光度联合约束可有效训练VGGT，无需标注即可提升大规模定位与3D理解能力；方法简单高效、可快速收敛，并具备良好的泛化潜力。

Abstract: Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.

</details>


### [47] [Evaluating Large Vision-language Models for Surgical Tool Detection](https://arxiv.org/abs/2601.16895)
*Nakul Poudel,Richard Simon,Cristian A. Linte*

Main category: cs.CV

TL;DR: 研究评估大型视觉-语言模型（VLM）在手术器械检测任务中的效果，发现Qwen2.5在零样本与LoRA微调下均优于LLaVA1.5和InternVL3.5，并在零样本上超越Grounding DINO、微调后相当；Qwen2.5识别更强，DINO定位更佳。


<details>
  <summary>Details</summary>
Motivation: 现有手术AI多为单模态，难以对复杂手术场景与流程进行整体理解；多模态大模型的进展为实现通用手术AI提供可能，但在手术领域的系统性评估不足，尤其在基础视觉任务（如器械检测）上的效果尚不明确。

Method: 选择GraSP机器人手术数据集，评测三种SOTA VLM（Qwen2.5、LLaVA1.5、InternVL3.5）在两种设置下的器械检测能力：零样本与参数高效的LoRA微调；并与开放集检测基线Grounding DINO做对比，分析识别与定位维度表现。

Result: Qwen2.5在两种设置下检测性能均为最佳；相较Grounding DINO，零样本泛化更强、微调后性能相当；细分维度上，Qwen2.5的器械类别识别更准确，DINO在目标定位上更强。

Conclusion: 大型VLM在手术器械检测上具潜力，其中Qwen2.5表现最优并具更强零样本泛化；VLM与专用检测器在识别与定位上各有优势，提示未来可考虑融合策略与面向手术场景的专门优化，以推动通用手术AI的发展。

Abstract: Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.

</details>


### [48] [LoL: Longer than Longer, Scaling Video Generation to Hour](https://arxiv.org/abs/2601.16914)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 论文指出自回归长视频生成中使用attention sink帧会导致“sink-collapse”，即生成不断回退到锚帧，出现场景重置与循环动作。作者分析其根源在RoPE的周期性与多头注意力的相互作用，并提出无需训练的多头RoPE抖动来打破头间同质化，从而缓解长期崩塌，实现实时、流式、近乎无限长度的视频生成（演示至12小时）。


<details>
  <summary>Details</summary>
Motivation: 自回归长视频生成易累积误差并丧失长程一致性。现有通过插入attention sink帧来稳态化，但引入新的失败模式：模型不断吸回到sink帧，导致视觉循环和重置。作者希望找出这一现象的机理并提供低成本、可部署的抑制策略，以实现稳定的超长时长视频生成。

Method: 从理论与经验上分析RoPE的周期结构如何与多头注意力交互，导致各注意力头在长时序上同质化，进而把注意力集中到sink帧。提出“多头RoPE jitter”：在不同注意力头上加入轻量的相位/频率扰动（抖动），打破头间同步与周期对齐，无需额外训练或改模型架构；直接在推理时应用。

Result: 在多组实验中，方法显著降低sink-collapse的发生率，保持生成质量与流畅性；实现实时、流式、无限长度视频生成的稳定演示，连续生成最长达12小时，质量衰减很小，优于采用原始sink策略的基线。

Conclusion: sink-collapse源于RoPE周期性与多头注意力的结构性冲突。通过在多头间引入RoPE抖动可有效抑制崩塌且无需训练，实用易部署。该方法使长时、实时、流式视频生成成为可能，并在极长时长上保持一致性与质量。

Abstract: Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.

</details>


### [49] [Reward-Forcing: Autoregressive Video Generation with Reward Feedback](https://arxiv.org/abs/2601.16933)
*Jingran Zhang,Ning Li,Yuanhao Ban,Andrew Bai,Justin Cui*

Main category: cs.CV

TL;DR: 提出一种以奖励信号指导的自回归视频生成训练方法，替代对教师模型的强依赖，在保持高视觉质量与时序一致性的同时，达到与SOTA自回归方法相当甚至在部分指标上优于同规模双向模型的表现（如VBench得分84.92）。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成多依赖双向结构；为实现接近实时的生成，社区改造为自回归模型，但通常严重依赖教师模型蒸馏，若缺乏强自回归教师则性能受限，质量落后于双向模型。作者希望摆脱教师依赖，提升自回归生成效率与可扩展性，同时维持或提升质量。

Method: 采用基于奖励信号的训练/引导框架：在自回归生成过程中引入奖励来指导模型更新，弱化或避免从强教师模型蒸馏。奖励设计用于同时促进视觉保真度与时间一致性；训练流程简化并更易扩展。未给出具体奖励细节，但核心是以奖励替代异构蒸馏的监督信号。

Result: 在标准基准上进行大量实验：总体性能与现有自回归模型相当；在某些场景下超过同规模的双向模型。具体如VBench总分84.92，接近需要大量异构蒸馏的SOTA自回归方法（84.31），显示在无重蒸馏条件下仍具竞争力。

Conclusion: 以奖励信号引导的自回归视频生成能在减少对教师模型依赖的同时保持高质量与时序一致性，训练更简单、可扩展；在多个基准上达到或接近SOTA，表明该范式是双向与蒸馏密集方法的有力替代。

Abstract: While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.

</details>


### [50] [Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment](https://arxiv.org/abs/2601.16954)
*Ba-Thinh Lam,Thanh-Huy Nguyen,Hoang-Thien Nguyen,Quang-Khai Bui-Tran,Nguyen Lan Vi Vu,Phat K. Huynh,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 提出一种适用于混合域、未知域标签场景的半监督医学图像分割框架，结合跨域Copy-Paste增强与基于聚类的MMD对齐，在教师-学生范式下学到域不变表示，少量标注下也能稳健提升分割性能，并在Fundus与M&Ms上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实际医疗影像来自多中心多设备，存在显著域差且往往无域标签；标注稀缺使纯监督或单域假设的方法难以泛化。现有半监督或域适配多假设单一域偏移或需要显式域索引，不符合真实部署。需要一种同时提升数据多样性并减轻域偏差、且无需域标签的方案。

Method: 构建域不变混合域半监督框架：1) Copy-Paste Mechanism（CPM）在不同域图像之间复制粘贴“信息丰富区域”，扩大分布覆盖并增加难例；2) Cluster Maximum Mean Discrepancy（CMMD）模块对无标注特征进行聚类，并以有标注样本为锚，通过MMD目标对齐各簇与锚分布，促使特征域不变；3) 融合于教师-学生一致性学习，利用伪标签与一致性正则稳定训练。

Result: 在Fundus与M&Ms基准上，方法在少标注与多未知域差条件下，稳定且显著优于主流半监督与域适配基线，获得更高分割精度与鲁棒性。

Conclusion: 通过跨域拷贝粘贴提升样本多样性、CMMD实现无域标签的聚类对齐，并结合教师-学生框架，可在混合域少标注场景下学得域不变表征，提供一种可实用的医学图像分割解决方案。

Abstract: Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.

</details>


### [51] [VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents](https://arxiv.org/abs/2601.16973)
*Zirui Wang,Junyi Zhang,Jiaxin Ge,Long Lian,Letian Fu,Lisa Dunlap,Ken Goldberg,XuDong Wang,Ion Stoica,David M. Chan,Sewon Min,Joseph E. Gonzalez*

Main category: cs.CV

TL;DR: 提出VisGym，一个包含17个环境的多步视觉交互评测与训练套件，揭示现有VLM在长时程感知-记忆-行动整合上表现不佳，并给出通过显式目标、文本反馈和探索示范进行微调可带来稳定收益的改进路径。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在多步交互、尤其是长时程中整合感知、记忆与行动的能力缺乏系统化刻画与基准。需要一个可控、覆盖多任务形态的环境套件来诊断失败模式并指导改进。

Method: 构建VisGym基准：17个环境，涵盖符号谜题、真实图像理解、导航与操作；可调难度、输入表示、规划时域与反馈；提供多步求解器生成结构化示范以支持监督微调。对前沿模型在易/难配置下系统评测，并研究上下文长度、视觉化对任务难度的影响，以及显式目标、文本反馈与探索示范在部分可观测或未知动力学下的作用。

Result: 前沿模型在交互设定下成功率低：易配置46.6%，难配置26.0%。长上下文未被有效利用，完整历史反而劣于截断窗口；文本符号任务一旦以视觉形式呈现显著变难。显式目标观测、文本反馈与探索型示范用于监督微调在相应设定下带来一致性能提升。

Conclusion: 现有VLM在多步视觉决策与长时程整合方面存在系统性短板。VisGym揭示关键失败模式并提供可重复的改进路径：通过更好的记忆/上下文利用、显式目标与反馈机制，以及借助结构化多步示范进行微调，可显著提升交互式能力。

Abstract: Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.

</details>


### [52] [SyncLight: Controllable and Consistent Multi-View Relighting](https://arxiv.org/abs/2601.16981)
*David Serrano-Lozano,Anand Bhattad,Luis Herranz,Jean-François Lalonde,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: SyncLight 提出首个在多视角、无标定相机下实现一致、可参数化重光照的方法，通过单次推理把参考编辑的光强与颜色精确同步到所有视图。方法基于多视角扩散Transformer与“潜空间桥接匹配”训练，依托大规模混合数据集（多样合成场景+真实多视角标定照明数据），尽管仅以成对图像训练，仍能零样本泛化到任意视角且无需相机位姿。


<details>
  <summary>Details</summary>
Motivation: 单视图生成式重光照进步显著，但在多摄像机直播、立体电影、虚拟拍摄等应用中，跨视角光照一致性与可控性至关重要；现有方法难以在多视图间严格同步光强/色彩，并常依赖相机标定或逐视图处理，效率与一致性不足。

Method: 提出多视角扩散Transformer，并以“潜空间桥接匹配”训练目标对齐不同视图在潜空间中的光照编辑；训练使用大规模混合数据：1) 多样化高质量合成场景（来自现有资源与新构建场景）；2) 真实世界多视角采集，具备校准照明以提供精确监督。推理时以单个参考视图的光照编辑作为条件，在一次前向中对整组图像进行一致重光照，无需相机位姿。

Result: 在未见过的数据上实现高保真、跨视角一致的参数化重光照，能精确控制光强与颜色；仅用成对图像训练即可零样本泛化到任意数量视角，并在多视图数据上一次性完成推理，表现优于现有生成式重光照方法。

Conclusion: SyncLight 实现了无需位姿与多步处理的多视角一致重光照，显著提升实用性与可控性，为多摄像机广播、立体电影和虚拟制作提供高效可落地的重光照工作流。

Abstract: We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.

</details>


### [53] [AnyView: Synthesizing Any Novel View in Dynamic Scenes](https://arxiv.org/abs/2601.16982)
*Basile Van Hoorick,Dian Chen,Shun Iwase,Pavel Tokmakov,Muhammad Zubair Irshad,Igor Vasiljevic,Swati Gupta,Fangzhou Cheng,Sergey Zakharov,Vitor Campagnolo Guizilini*

Main category: cs.CV

TL;DR: 提出AnyView：一种扩散式动态视角合成框架，融合2D/3D/4D多源数据训练通用时空隐式表示，可从任意相机轨迹零样本生成多视角一致的视频；在新提出的极端动态基准AnyViewBench上，相比需要大视角重叠的基线更稳健，生成真实且时空一致。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型虽逼真，但在真实动态场景下难以保持多视角与时空一致性；多数方法依赖强几何先验或视角重叠，泛化受限。需要一个偏置更小、可从任意视角生成且在极端动态条件下仍一致的通用方法与评测基准。

Method: 基于扩散模型的动态视角合成框架AnyView：用来自单目(2D)、多视图静态(3D)、多视图动态(4D)的多级监督联合训练通用时空隐式表示；在推理时，从任意相机位置与轨迹进行条件生成，输出多视角时空一致的视频。并构建AnyViewBench——强调极端动态与视角差异的大规模基准。

Result: 在标准基准上达到与SOTA有竞争力的表现；在AnyViewBench上，现有方法因需要较大视角重叠而性能显著下降，而AnyView仍能生成真实、合理且具有时空一致性的视频。

Conclusion: 多源监督+扩散式通用时空隐式表示能实现任意视角的动态视频零样本生成，较少依赖几何假设；在极端动态场景下表现稳健，并提供了新的评测基准推动该方向发展。

Abstract: Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/

</details>
