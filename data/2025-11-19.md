<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 136]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation](https://arxiv.org/abs/2511.13744)
*Zhijie Qiao,Zhong Cao,Henry X. Liu*

Main category: cs.CV

TL;DR: 提出nuCarla：在CARLA内构建、与nuScenes格式完全兼容的大规模BEV感知数据集及高性能BEV主干，填补闭环E2E自动驾驶训练/评测缺口，显著提升检测并加速闭环研究。


<details>
  <summary>Details</summary>
Motivation: 现有E2E自动驾驶多在闭环仿真中训练评测，但真实世界数据多为非交互、开放环路采集，难以支撑闭环；缺乏标准化、规模化、充分验证的数据来学习有意义的中间表示（如BEV），导致闭环端到端模型落后于简单规则基线。

Method: 在CARLA中构建nuScenes风格的BEV感知数据集nuCarla：1) 完全兼容nuScenes数据格式；2) 规模与nuScenes相当但类别更均衡；3) 可直接用于闭环仿真部署；4) 提供高性能BEV主干并在检测上达到SOTA。开放数据与模型作为基准。

Result: 基于nuCarla训练的BEV主干在检测任务上取得SOTA性能；数据集与格式兼容性支持将现实世界感知模型无缝迁移到仿真闭环中。

Conclusion: nuCarla通过提供标准化大规模仿真BEV数据与强基线模型，显著降低闭环E2E训练与评测门槛，加速向可靠且安全感知-规划-控制一体化研究发展。

Abstract: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving.

</details>


### [2] [Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition](https://arxiv.org/abs/2511.13775)
*Dongdong Zhao,Ranxin Fang,Changtian Song,Zhihui Liu,Jianwen Xiang*

Main category: cs.CV

TL;DR: 提出一个缓解“开集识别”中过度自信问题的两阶段框架：用可控参数扰动估计不确定性，再结合学习型未知检测器利用不确定性区分未知/已知，在三数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在开集识别中，未知样本若与已知类语义相近，特征空间会出现类间重叠，导致模型对未知样本给出不合理的高置信度（过度自信），从而被误判为已知类，模糊了已知/未知决策边界。需要一种方法显式地抑制由类间重叠引起的过度自信。

Method: 提出包含两部分的框架：1）扰动式不确定性估计模块：对模型参数施加可控扰动，产生多样化预测并量化预测不确定性；2）未知检测模块：采用学习型分类器，以两阶段流程将估计到的不确定性作为信号，增强已知/未知的可分性。

Result: 在三个公开数据集上，所提框架在开集识别指标上优于现有OSR方法。

Conclusion: 通过显式建模并利用预测不确定性，可有效缓解由类间重叠导致的过度自信，从而提升开集识别中区分已知与未知的能力与整体性能。

Abstract: Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.

</details>


### [3] [Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection](https://arxiv.org/abs/2511.13784)
*Yogesh Kumar,Anand Mishra*

Main category: cs.CV

TL;DR: 提出一种面向新类的时序建模方法用于小样本视频目标检测，通过高置信度特征的选择性传播，在无需显式tube proposals的情况下提升时序一致性与检测精度；在多个基准的5-shot上带来约4–5% AP提升，并在1/3/10-shot也有收益。


<details>
  <summary>Details</summary>
Motivation: 传统视频检测依赖大量标注与复杂的区域/轨迹提议，计算昂贵且对新类泛化差；小样本场景中还要应对遮挡、外观变化导致的跨帧不一致与噪声累积。作者希望在少标注条件下兼顾时序一致性与新类泛化，同时避免复杂的tube提议流程。

Method: 以few-shot训练的检测与分类头为基础，提出“新类感知”的时序特征传播框架：跨帧仅对高置信度的目标特征进行过滤与选择性传播，抑制低置信度噪声，促进稳定的特征进化；不依赖显式的object tube proposal。

Result: 在5-shot设置下：FSVOD-500提升3.7% AP、FSYTV-40提升5.3%、VidOR提升4.3%、VidVRD提升4.5；在1-shot、3-shot、10-shot也有一致改进。

Conclusion: 选择性传播高置信度特征的时序建模在小样本视频目标检测中有效，能在无需显式tube proposals的前提下实现更好的时序一致性与对新类的泛化，带来稳定的性能提升，代码已开源。

Abstract: Few-shot Video Object Detection (FSVOD) addresses the challenge of detecting novel objects in videos with limited labeled examples, overcoming the constraints of traditional detection methods that require extensive training data. This task presents key challenges, including maintaining temporal consistency across frames affected by occlusion and appearance variations, and achieving novel object generalization without relying on complex region proposals, which are often computationally expensive and require task-specific training. Our novel object-aware temporal modeling approach addresses these challenges by incorporating a filtering mechanism that selectively propagates high-confidence object features across frames. This enables efficient feature progression, reduces noise accumulation, and enhances detection accuracy in a few-shot setting. By utilizing few-shot trained detection and classification heads with focused feature propagation, we achieve robust temporal consistency without depending on explicit object tube proposals. Our approach achieves performance gains, with AP improvements of 3.7% (FSVOD-500), 5.3% (FSYTV-40), 4.3% (VidOR), and 4.5 (VidVRD) in the 5-shot setting. Further results demonstrate improvements in 1-shot, 3-shot, and 10-shot configurations. We make the code public at: https://github.com/yogesh-iitj/fs-video-vit

</details>


### [4] [FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching](https://arxiv.org/abs/2511.13794)
*Huayi Zhu,Xiu Shu,Youqiang Xiong,Qiao Liu,Rui Chen,Di Yuan,Xiaojun Chang,Zhenyu He*

Main category: cs.CV

TL;DR: 提出一种基于Flow Matching的多模态图像融合框架，将融合建模为从源模态到融合分布的直接概率传输，结合伪标签选择与精炼、以及多任务持续学习机制，实现高效采样、结构一致与轻量化模型，并在多种融合任务上达成具竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 现有融合方法往往为任务定制，训练成本高、难以扩展；生成式方法虽统一但采样慢（从噪声到图像的长轨迹）。需要一种既统一又高效、能保证结构一致性且无需大规模标注的方案，并能适应多任务与持续学习。

Method: 1) 将图像融合表述为从源模态到融合图像分布的直接概率传输，采用Flow Matching以缩短采样路径并提升结构一致性；2) 收集多种SOTA融合器的输出作为先验，设计任务感知选择函数为每个任务挑选最可靠的伪标签；3) 设计Fusion Refiner模块，分而治之地识别/分解/增强伪标签中的退化成分；4) 多任务场景下引入EWC（弹性权重固化）与经验回放，分别从参数稳定与记忆保持两方面增强跨任务性能与持续学习；5) 轻量化模型设计与高效推理。

Result: 在多类融合任务上取得与SOTA竞争的性能，同时显著提升采样效率并保持模型轻量；跨任务性能在持续学习设定下得到保持与增强。

Conclusion: 基于Flow Matching的直接概率传输为多模态融合提供了统一且高效的建模路径；配合任务感知伪标签选择、精炼与持续学习机制，可在数据监督不足与多任务场景下兼顾性能、效率与模型规模。

Abstract: Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.

</details>


### [5] [A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion](https://arxiv.org/abs/2511.13795)
*Weiying Shen,Hao Yu,Yu Dong,Pan Liu,Yu Han,Xin Wen*

Main category: cs.CV

TL;DR: 提出一个无需轨迹的两阶段车祸检测框架：用扩散模型生成“正常”道路分段动态图（Mapfusion），再将观测图与生成图对比以发现异常（车祸）；在实测数据上能准确检测。


<details>
  <summary>Details</summary>
Motivation: 实时事故检测对主动安全和交通效率至关重要，但依赖车辆轨迹采集与跟踪存在成本高、丢失、遮挡和隐私等问题。作者希望绕开个体轨迹，直接利用路段级动态数据实现鲁棒的实时事故检测。

Method: 两阶段、无轨迹。阶段一：扩散生成模型Mapfusion，做从噪声到“正常”路段动态图的去噪生成；用时间序列嵌入捕捉时序动态，并通过ControlNet注入背景上下文以增强可控生成。在非事故数据上训练，使其学习“正常”运动模式，并对不同采样间隔保持鲁棒。阶段二：用生成的正常演化图与实时监测的路段图比较，偏差异常即判定为可能事故。

Result: Mapfusion在非事故数据上能生成逼真的路段演化图，跨采样间隔表现稳健；在真实事故数据上的实验表明，两阶段方法能准确检测事故。

Conclusion: 基于扩散的轨迹自由框架有效实现了实时事故检测：通过学习正常演化再做差异检测，既规避了轨迹获取的限制，又在真实场景中取得良好精度与鲁棒性。

Abstract: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.

</details>


### [6] [Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model](https://arxiv.org/abs/2511.13800)
*Huiwen Wu,Shuo Zhang,Yi Liu,Hongbin Ye*

Main category: cs.CV

TL;DR: 提出一种针对地震数据的自适应双网格（ADATG）预训练策略与Hilbert编码，通过频谱分解分别处理高低频特征，并按从粗到细的教学式训练，显著提升视觉Transformer在地震领域的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有ViT顺序分块/序列化忽略地震数据的层级与频率结构，难以同时高效捕捉高、低频关键信息；地震图像具有特殊的时空与频谱特性，需要定制化编码与训练策略以构建更强的领域基础模型。

Method: 1) 频谱分解：将地震记录拆分为高频与低频分量；2) 分层Hilbert编码：利用Hilbert曲线的空间保持性为两类频段生成层级式表示（两网格/多尺度）；3) 自适应训练策略：依据ViT的频率学习规律，先强调粗粒度/低频信息，再逐步转向细粒度/高频特征，实现从粗到细的渐进优化；4) 在多项地震预训练与下游任务上进行验证。

Result: 实验显示该方法在效率与效果上均优于基线ViT等既有方法，能够更全面地捕获地震图像的高低频信息，并提升下游地震任务表现。

Conclusion: 数据编码与训练顺序需贴合地震高低频特性。ADATG结合频谱分解与Hilbert层级编码，并以自适应从粗到细的训练方式，有助于构建更强的地震视觉基础模型。

Abstract: Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.

</details>


### [7] [Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video](https://arxiv.org/abs/2511.13802)
*Filippo Cenacchi. Longbing Cao,Mitchell McEwan,Deborah Richards*

Main category: cs.CV

TL;DR: 论文提出一种仅基于面部微小时序动态、无需语音与文本的痴呆被动筛查方法，并发布了一个来自YouTube的野外数据集YT DemTalk；在该数据集上，轻量分类器即可达到高AUROC与AP表现。


<details>
  <summary>Details</summary>
Motivation: 现有痴呆筛查多依赖语音或脚本化访谈，受语言、转写与场景限制，难以在非临床、跨文化/设备环境大规模部署。作者希望以语言无关、无脚本、相机正对的短视频，捕捉自然面部行为，实现早期神经认知变化的被动筛查。

Method: 从相机正对短视频中提取并稳定面部信号，将眨眼、口/下颌细微运动、凝视变化、头部微调等转为可解释的微动态时间序列；进行平滑与短窗汇总，生成紧凑的片段级统计特征。每个时间窗用“活动构成”编码（各通道相对运动份额而非绝对幅度），使模型关注跨通道分布并增强可解释性；使用轻量浅层分类器进行筛查。

Result: 构建YT DemTalk数据集（300段视频：150自述痴呆、150对照）。消融发现凝视不稳定与口/下颌动态最具判别力；在该数据集上，模型达到AUROC 0.953、AP 0.961、F1 0.851、准确率0.857。

Conclusion: 面部微动态（无需语音/文本）即可有效支持痴呆筛查，方法具备跨设备、话题与文化的可迁移性与可解释性；新数据集为该方向提供初步基准。

Abstract: We target passive dementia screening from short camera-facing talking head video, developing a facial temporal micro dynamics analysis for language free detection of early neuro cognitive change. This enables unscripted, in the wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per channel effects transparent. We also introduce YT DemTalk, a new dataset curated from publicly available, in the wild camera facing videos. It contains 300 clips (150 with self reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.

</details>


### [8] [Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark](https://arxiv.org/abs/2511.13853)
*Xinxin Liu,Zhaopan Xu,Kai Wang,Yong Jae Lee,Yuzhang Shang*

Main category: cs.CV

TL;DR: 提出Gen-ViRe基准，用于量化评估视频生成模型的“逐帧链式推理（CoF）”能力，覆盖6大认知维度与24子任务，采用最小提示与VLM辅助的混合评测；实验显示当前SOTA视频质量高但推理深度不足，给出基线与诊断工具。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型虽可通过逐帧生成体现物理连续性与推理轨迹，但主流基准只评忠实度与对齐，不衡量多步规划、算法式逻辑或抽象外推等核心认知能力，导致对模型能力缺乏系统认知与改进指引。

Method: 构建Gen-ViRe：以认知科学与真实应用为基础，将CoF推理分解为6个认知维度与24个子任务；通过多源数据整理与最小化提示协议生成视频；设计细粒度标准并结合VLM辅助自动化与人工核验的混合评测流程，实现量化打分与诊断。

Result: 对多款SOTA视频生成系统实验表明：视觉质量与推理能力显著错位，许多模型在需要多步规划、算法逻辑与抽象模式外推的任务上表现不佳；基准提供了首批量化结果与可比较的基线。

Conclusion: Gen-ViRe首次系统地评估视频模型的CoF推理，揭示当前方法“看起来好但不会想”的差距，为未来提升为真正世界模拟器提供方向与诊断工具。

Abstract: While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.

</details>


### [9] [RSPose: Ranking Based Losses for Human Pose Estimation](https://arxiv.org/abs/2511.13857)
*Muhammed Can Keles,Bedrettin Cetinkaya,Sinan Kalkan,Emre Akbas*

Main category: cs.CV

TL;DR: 提出RSPose：用排序型损失替代热力图MSE/KL，提升关节定位与mAP一致性，并在COCO等数据集刷新SOTA（ViTPose-H达79.9 mAP，SimCC-R50提升1.5 AP）。


<details>
  <summary>Details</summary>
Motivation: 热力图姿态估计存在三大痛点：1) MSE等点对点误差不强调峰值锐化与精确定位；2) 热力图存在空间与类别不平衡；3) 训练损失与评测指标（mAP）不一致，导致置信度与定位质量相关性差，影响NMS与AP。

Method: 设计基于排序（ranking-based）的损失函数，使预测置信度与关键点定位质量单调对齐，直接优化与mAP一致的目标。方法适用于一维、二维热力图，并与现有框架（如ViTPose、SimCC）兼容。理论与实证分析展示其相对MSE、KL的优势与更高的相关性。

Result: 在COCO、CrowdPose、MPII上广泛验证：在COCO-val上以ViTPose-H获得79.9 mAP，超过SOTA；在SimCC ResNet-50上提升1.5 AP至73.6。方法显著提升置信度-定位质量相关性，从而改进NMS与整体mAP。

Conclusion: 基于排序的RSPose损失缓解了热力图损失的三大问题，并首次使损失与mAP评测对齐，通用于1D/2D热力图与多种架构，在多数据集上取得显著SOTA或稳定增益。

Abstract: While heatmap-based human pose estimation methods have shown strong performance, they suffer from three main problems: (P1) "Commonly used Mean Squared Error (MSE)" Loss may not always improve joint localization because it penalizes all pixel deviations equally, without focusing explicitly on sharpening and correctly localizing the peak corresponding to the joint; (P2) heatmaps are spatially and class-wise imbalanced; and, (P3) there is a discrepancy between the evaluation metric (i.e., mAP) and the loss functions.
  We propose ranking-based losses to address these issues.
  Both theoretically and empirically, we show that our proposed losses are superior to commonly used heatmap losses (MSE, KL-Divergence). Our losses considerably increase the correlation between confidence scores and localization qualities, which is desirable because higher correlation leads to more accurate instance selection during Non-Maximum Suppression (NMS) and better Average Precision (mAP) performance. We refer to the models trained with our losses as RSPose.
  We show the effectiveness of RSPose across two different modes: one-dimensional and two-dimensional heatmaps, on three different datasets (COCO, CrowdPose, MPII).
  To the best of our knowledge, we are the first to propose losses that align with the evaluation metric (mAP) for human pose estimation.
  RSPose outperforms the previous state of the art on the COCO-val set and achieves an mAP score of 79.9 with ViTPose-H, a vision transformer model for human pose estimation.
  We also improve SimCC Resnet-50, a coordinate classification-based pose estimation method, by 1.5 AP on the COCO-val set, achieving 73.6 AP.

</details>


### [10] [Segmenting Collision Sound Sources in Egocentric Videos](https://arxiv.org/abs/2511.13863)
*Kranti Kumar Parida,Omar Emara,Hazel Doughty,Dima Damen*

Main category: cs.CV

TL;DR: 提出“碰撞声音来源分割（CS3）”任务：给定碰撞声音与对应视频帧，分割产生该声音的两个交互物体；在两个新基准上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 人类能从交互声音辨别物体属性；碰撞声由两个物体的相互作用产生，受双方属性影响。现有音视任务多处理单一声源或孤立事件，难以在混乱的第一人称视频中定位小而短暂的交互物体，因此需要新的定义与方法。

Method: 提出弱监督、语音条件的分割框架，利用通用基础模型CLIP与SAM2进行视觉-语义与分割迁移；结合第一人称线索（如“在手中”的物体）以锁定可能的行动/发声物体；在无强像素标注下进行训练与推断，实现从音频到视频帧的发声源区域提取。

Result: 在新建的两套CS3基准EPIC-CS3与Ego4D-CS3上，方法在mIoU上分别较强基线提升约3倍与4.7倍，显示显著优势。

Conclusion: CS3定义并验证了从碰撞声到双物体交互源的分割可行性；弱监督结合基础模型与自我中心线索能有效锁定声源物体，为多模态感知与机器人交互等应用提供新方向。

Abstract: Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief.
  To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2). We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3\times$ and $4.7\times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.

</details>


### [11] [GRLoc: Geometric Representation Regression for Visual Localization](https://arxiv.org/abs/2511.13864)
*Changyang Li,Xuejian Ma,Lixiang Liu,Zhan Li,Qingan Yan,Yi Xu*

Main category: cs.CV

TL;DR: 提出几何表示回归（GRR）：先从单张图像回归显式3D几何（射线方向束与点图），再用可微确定性求解器恢复6DoF位姿，显著提升APR的泛化与精度，在7-Scenes与Cambridge上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统绝对位姿回归（APR）把图像直接映射到6DoF位姿，易成为“黑箱”，倾向记忆训练视角而非学习场景几何，导致泛化弱与不稳定。需要引入强几何先验与可解释过程来克服上述问题。

Method: 将APR重构为“逆渲染”：从图像直接回归两种在世界坐标系下可分解的几何表示—(1)用于姿态旋转的射线束方向；(2)用于平移的点图（pointmap）。随后通过一个可微、确定性的几何求解器，将这两种几何中间量组合恢复最终6DoF位姿。核心是显式解耦旋转与平移的学习，并把视觉到几何的映射与最终位姿计算分离。

Result: 在7-Scenes与Cambridge Landmarks数据集上达到当前最优或具竞争力的SOTA表现；消融显示显式解耦旋转与平移能显著提升精度与稳健性。

Conclusion: 将APR视作逆渲染问题、先学几何后解位姿的GRR范式，为绝对位姿估计提供了更具泛化与可解释性的路径。显式几何表示与可微求解器的结合优于端到端黑箱回归。

Abstract: Absolute Pose Regression (APR) has emerged as a compelling paradigm for visual localization. However, APR models typically operate as black boxes, directly regressing a 6-DoF pose from a query image, which can lead to memorizing training views rather than understanding 3D scene geometry. In this work, we propose a geometrically-grounded alternative. Inspired by novel view synthesis, which renders images from intermediate geometric representations, we reformulate APR as its inverse that regresses the underlying 3D representations directly from the image, and we name this paradigm Geometric Representation Regression (GRR). Our model explicitly predicts two disentangled geometric representations in the world coordinate system: (1) a ray bundle's directions to estimate camera rotation, and (2) a corresponding pointmap to estimate camera translation. The final 6-DoF camera pose is then recovered from these geometric components using a differentiable deterministic solver. This disentangled approach, which separates the learned visual-to-geometry mapping from the final pose calculation, introduces a strong geometric prior into the network. We find that the explicit decoupling of rotation and translation predictions measurably boosts performance. We demonstrate state-of-the-art performance on 7-Scenes and Cambridge Landmarks datasets, validating that modeling the inverse rendering process is a more robust path toward generalizable absolute pose estimation.

</details>


### [12] [H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction](https://arxiv.org/abs/2511.13869)
*Xueyang Li,Zongren Wang,Yuliang Zhang,Zixuan Pan,Yu-Jen Chen,Nishchal Sapkota,Gelei Xu,Danny Z. Chen,Yiyu Shi*

Main category: cs.CV

TL;DR: 提出用于膀胱癌复发预测的多序列/多模态MRI数据集与新模型H-CNN-ViT；模型分支独立处理各模态，并用分层门控注意力融合CNN局部与ViT全局特征，在自建数据集上AUC 78.6%，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 膀胱癌复发率高，术后影像受瘢痕与重塑干扰，放射科医师判读困难；现有AI研究缺乏专门的多序列MRI复发评估数据集，限制方法发展与可比性。

Method: 1) 构建并公开一个面向复发预测的多序列、 多模态膀胱MRI数据集，作为基准；2) 提出H-CNN-ViT：多分支架构按模态独立编码；CNN分支学习局部细节，ViT分支捕获全局上下文；通过分层门控注意力在不同层级自适应加权融合全局与局部特征，实现有选择的特征汇聚。

Result: 在自建数据集上，H-CNN-ViT的AUC=78.6%，超过当前多种SOTA基线模型。

Conclusion: 专门数据集与分层门控多分支融合策略可提升膀胱癌复发预测性能；所提方法在多序列MRI上效果领先，并为后续研究提供基准与可复现实装置（代码公开）。

Abstract: Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at https://github.com/XLIAaron/H-CNN-ViT}.

</details>


### [13] [QwenCLIP: Boosting Medical Vision-Language Pretraining via LLM Embeddings and Prompt tuning](https://arxiv.org/abs/2511.13876)
*Xiaoyang Wei,Camille Kurtz,Florence Cloppet*

Main category: cs.CV

TL;DR: 提出QwenCLIP：用LLM嵌入替换CLIP文本编码器并配合可学习提示，以更好处理长篇放射学报告，显著提升图文对齐与下游任务表现。


<details>
  <summary>Details</summary>
Motivation: CLIP文本编码器仅77 token，难以表达信息密集的放射学报告；现有用PubMedBERT/ClinicalBERT等虽更贴近医学，但输入长度受限（~512）且语义表征较浅，难以捕获长文临床语义。

Method: 将CLIP的文本编码器替换为大型语言模型的嵌入模块（如Qwen3-Embedding），并引入可学习prompt以增强跨模态对齐；利用LLM更长上下文窗口与更丰富表示来编码长篇临床文本。

Result: 在放射学基准上，医学图文对齐与多项下游任务性能显著提升（相较传统CLIP及BERT系适配器）。

Conclusion: LLM驱动的文本嵌入结合可学习提示能有效缓解输入长度与语义表征不足的问题，提升医学视觉-语言任务表现；代码已开源（GitHub: Wxy-24/QwenCLIP）。

Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong generalization for vision-language tasks in computer vision and medical domains, yet its text encoder accepts only up to 77 tokens, which limits its ability to represent long and information-rich radiology reports. Recent adaptations using domain-specific encoders, such as PubMedBERT or ClinicalBERT, mitigate this issue by leveraging medical corpora, but remain constrained by their limited input length (typically 512 tokens) and relatively shallow semantic understanding. To address these limitations, we propose QwenCLIP, a vision-language framework that replaces CLIP's text encoder with a large language model (LLM)-based embedding module (e.g., Qwen3-Embedding) and introduces learnable prompts to enhance cross-modal alignment. By leveraging the extended context window and richer representations of LLMs, QwenCLIP captures comprehensive medical semantics from long-form clinical text, substantially improving medical image-text alignment and downstream performance on radiology benchmarks. Our code is publicly available at https://github.com/Wxy-24/QwenCLIP.

</details>


### [14] [Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection](https://arxiv.org/abs/2511.13877)
*Pandiyaraju V,Abishek Karthik,Jaspin K,Kannan A,Jaime Lloret*

Main category: cs.CV

TL;DR: 提出一种结合EfficientNet与VGG19并加入伪牛顿提升层与稀疏特征约简层的混合模型，用于腰椎退变DICOM影像分类，在高维医疗影像中较传统迁移学习显著提升，达Acc 88.1、F1 0.88等。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习在高维医疗影像中易忽略细粒度解剖特征、存在特征冗余与选择不足，导致分类性能受限。需要一种能强化细节特征权重并抑制冗余的架构以提升诊断性能。

Method: 构建混合骨干（EfficientNet+VGG19）提取多尺度特征；引入“伪牛顿提升（Pseudo-Newton Boosting）”层对特征权重进行迭代化、二阶近似式的自适应调整，突出细粒度解剖特征；加入“稀疏诱导特征约简（Sparsity-Induced Feature Reduction）”层以稀疏正则/门控方式去冗余，形成多层级特征选择框架；配合预处理流水线，基于DICOM腰椎数据进行训练评估，并与EfficientNet基线对比。

Result: 相较基线EfficientNet，性能显著提升：precision 0.900、recall 0.861、F1 0.880、loss 0.18、accuracy 88.1%。

Conclusion: 该多层级混合架构在高维医疗影像分类中有效克服传统迁移学习的局限，通过伪牛顿提升与稀疏约简强化关键解剖特征、去除冗余，实现显著性能提升，表明其在自动化医学影像诊断中的应用潜力。

Abstract: This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.

</details>


### [15] [VLMs Guided Interpretable Decision Making for Autonomous Driving](https://arxiv.org/abs/2511.13881)
*Xin Hu,Taotao Jing,Renran Tian,Zhengming Ding*

Main category: cs.CV

TL;DR: 把VLM从“直接给驾驶决策”改成“语义增强器”，用语言丰富场景表征，再与视觉融合并加后处理提升稳健性，在两个AD基准上达SOTA且更可解释。


<details>
  <summary>Details</summary>
Motivation: 直接用VLM在VQA范式里输出驾驶决策依赖手工提示，表现不稳定、泛化差；开源VLM虽有强场景理解但难以稳定给出可靠的高层驾驶动作。

Method: 1) 不再让VLM直接决策，而是生成结构化、语言丰富的场景描述以增强视觉基准；2) 设计多模态交互架构，融合视觉与语言特征，输出高层决策与可解释文本；3) 引入后验精炼模块，利用VLM对预测进行置信与一致性增强。

Result: 在两个自动驾驶基准上取得SOTA，高层决策更准确，解释性文本更好，整体可靠性提升。

Conclusion: 让VLM充当语义增强而非决策器，并配合多模态融合与后验精炼，可提升AD高层决策的准确性、稳健性与可解释性，是集成VLM于实际AD系统的可行方向。

Abstract: Recent advancements in autonomous driving (AD) have explored the use of vision-language models (VLMs) within visual question answering (VQA) frameworks for direct driving decision-making. However, these approaches often depend on handcrafted prompts and suffer from inconsistent performance, limiting their robustness and generalization in real-world scenarios. In this work, we evaluate state-of-the-art open-source VLMs on high-level decision-making tasks using ego-view visual inputs and identify critical limitations in their ability to deliver reliable, context-aware decisions. Motivated by these observations, we propose a new approach that shifts the role of VLMs from direct decision generators to semantic enhancers. Specifically, we leverage their strong general scene understanding to enrich existing vision-based benchmarks with structured, linguistically rich scene descriptions. Building on this enriched representation, we introduce a multi-modal interactive architecture that fuses visual and linguistic features for more accurate decision-making and interpretable textual explanations. Furthermore, we design a post-hoc refinement module that utilizes VLMs to enhance prediction reliability. Extensive experiments on two autonomous driving benchmarks demonstrate that our approach achieves state-of-the-art performance, offering a promising direction for integrating VLMs into reliable and interpretable AD systems.

</details>


### [16] [Revisiting Data Scaling Law for Medical Segmentation](https://arxiv.org/abs/2511.13883)
*Yuetan Chu,Zhongyi Han,Gongning Luo,Xin Gao*

Main category: cs.CV

TL;DR: 论文研究医学解剖分割任务中，性能随训练数据规模呈幂律提升，并提出基于配准的可微分形变生成增强方法，显著提升数据利用效率、加快收敛，甚至超越标准幂律趋势而无需额外数据。


<details>
  <summary>Details</summary>
Motivation: 已有工作表明深度网络在许多任务上随数据规模呈幂律提升，但医学影像分割（多解剖结构、多模态）领域对此研究不足；同时，解剖图像在拓扑结构上具同构性，提示形变类数据增强或可有效放大数据效用、改变扩展规律。

Method: - 在15个语义解剖分割任务与4种影像模态上系统测量“数据量—性能”幂律关系。
- 评估两类形变增强对扩展规律的影响：随机弹性形变与配准引导的形变。
- 提出一种新型可扩展的“生成形变”方法：基于图像配准的测地子空间，生成满足微分同胚（diffeomorphic）的形变场，用于更真实的结构变形增强。
- 比较不同增强策略对性能、收敛速度与数据效率的影响。

Result: - 多任务多模态下，分割性能随数据量一致地呈幂律提升。
- 配准形变与生成形变增强均显著提高数据利用效率。
- 生成形变方法在性能与收敛速度上优于随机弹性与配准形变，且在固定数据量下可超越标准幂律曲线。

Conclusion: 医学解剖分割遵循稳定的幂律扩展，但形变引导的增强，尤其是基于测地子空间的微分同胚生成形变，可在不增加标注数据的情况下显著提升与加速训练，降低标注与计算成本，对医学影像分割的可扩展性与拓扑变化建模提供了新见解。

Abstract: The population loss of trained deep neural networks often exhibits power law scaling with the size of the training dataset, guiding significant performance advancements in deep learning applications. In this study, we focus on the scaling relationship with data size in the context of medical anatomical segmentation, a domain that remains underexplored. We analyze scaling laws for anatomical segmentation across 15 semantic tasks and 4 imaging modalities, demonstrating that larger datasets significantly improve segmentation performance, following similar scaling trends. Motivated by the topological isomorphism in images sharing anatomical structures, we evaluate the impact of deformation-guided augmentation strategies on data scaling laws, specifically random elastic deformation and registration-guided deformation. We also propose a novel, scalable image augmentation approach that generates diffeomorphic mappings from geodesic subspace based on image registration to introduce realistic deformation. Our experimental results demonstrate that both registered and generated deformation-based augmentation considerably enhance data utilization efficiency. The proposed generated deformation method notably achieves superior performance and accelerated convergence, surpassing standard power law scaling trends without requiring additional data. Overall, this work provides insights into the understanding of segmentation scalability and topological variation impact in medical imaging, thereby leading to more efficient model development with reduced annotation and computational costs.

</details>


### [17] [Uni-Hema: Unified Model for Digital Hematopathology](https://arxiv.org/abs/2511.13889)
*Abdul Rehman,Iqra Rasool,Ayesha Imran,Mohsen Ali,Waqas Sultani*

Main category: cs.CV

TL;DR: Uni-Hema提出一个统一的多任务多模态模型，整合检测、分类、分割、形态学预测与推理，覆盖多种血液病理任务，并在46个数据集上达到与单任务专用模型相当或更优的性能，同时提供细胞级可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有数字血液病理方法（单任务、视觉-语言、面向WSI或单细胞模型）各自为战，无法在多疾病、多任务、多模态层面进行统一推理，限制了临床与研究中的通用性与可扩展性。

Method: 构建Uni-Hema框架，核心为Hema-Former多模态模块，分层对齐视觉与文本表征，统一支持检测、分类、分割、形态学预测、掩码语言建模与视觉问答等任务；整合46个公开数据集（>70万图像、2.1万QA），实现跨任务与跨粒度的联合训练与推理。

Result: 在多种血液学任务上，Uni-Hema取得与或优于单任务、单数据集模型的表现；能够在单细胞层面提供形态学相关、可解释的推理与答案。

Conclusion: Uni-Hema树立了数字血液病理多任务多模态学习的新基线，证明统一框架在性能与可解释性上的优势；代码将开源，便于复现与扩展。

Abstract: Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.

</details>


### [18] [Weakly Supervised Ephemeral Gully Detection In Remote Sensing Images Using Vision Language Models](https://arxiv.org/abs/2511.13891)
*Seyed Mohamad Ali Tousi,John A. Lory,G. N. DeSouza*

Main category: cs.CV

TL;DR: 提出一种首个用于短暂沟蚀（Ephemeral Gullies）检测的弱监督管线，结合远程感知与视觉语言模型，利用噪声标签与师生框架在极少精标数据下取得优于基线的性能，并发布包含1.8万张高分辨率影像的半监督数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 短暂沟蚀在农田土壤侵蚀中危害大，但其时空短暂性与标注稀缺使传统视觉与常规机器学习方法难以自动检测；零样本方案实现复杂且效果受限，急需一种能降低标注成本、提升检测可用性的方案。

Method: 基于远程感知影像与视觉语言模型（VLM）构建弱监督流程：1) 利用VLM预训练知识产生噪声候选标签；2) 采用师生模型，教师用VLM噪声标签学习并生成更可靠的伪标签；3) 学生在教师伪标签与噪声感知损失函数的弱监督下训练；同时发布一个包含专家少量标注与大量未标注样本的半监督数据集（13年、1.8万张高分辨率影像）。

Result: 在实验中，弱监督训练的学生模型在检测短暂沟蚀任务上显著优于直接使用VLM与仅用标签模型的基线，验证了方法有效性。

Conclusion: 通过将VLM知识、噪声鲁棒学习与师生框架结合，可在极少精标数据下有效检测短暂沟蚀；所提方法与公开数据/代码为该领域提供了实用基线与资源，推动弱监督遥感地表形态检测研究。

Abstract: Among soil erosion problems, Ephemeral Gullies are one of the most concerning phenomena occurring in agricultural fields. Their short temporal cycles increase the difficulty in automatically detecting them using classical computer vision approaches and remote sensing. Also, due to scarcity of and the difficulty in producing accurate labeled data, automatic detection of ephemeral gullies using Machine Learning is limited to zero-shot approaches which are hard to implement. To overcome these challenges, we present the first weakly supervised pipeline for detection of ephemeral gullies. Our method relies on remote sensing and uses Vision Language Models (VLMs) to drastically reduce the labor-intensive task of manual labeling. In order to achieve that, the method exploits: 1) the knowledge embedded in the VLM's pretraining; 2) a teacher-student model where the teacher learns from noisy labels coming from the VLMs, and the student learns by weak supervision using teacher-generate labels and a noise-aware loss function. We also make available the first-of-its-kind dataset for semi-supervised detection of ephemeral gully from remote-sensed images. The dataset consists of a number of locations labeled by a group of soil and plant scientists, as well as a large number of unlabeled locations. The dataset represent more than 18,000 high-resolution remote-sensing images obtained over the course of 13 years. Our experimental results demonstrate the validity of our approach by showing superior performances compared to VLMs and the label model itself when using weak supervision to train an student model. The code and dataset for this work are made publicly available.

</details>


### [19] [Temporal Realism Evaluation of Generated Videos Using Compressed-Domain Motion Vectors](https://arxiv.org/abs/2511.13897)
*Mert Onur Cakiroglu,Idil Bilge Altun,Zhihe Lu,Mehmet Dalkilic,Hasan Kurban*

Main category: cs.CV

TL;DR: 提出一种基于压缩域运动向量（MV）的通用评估框架，用统计散度度量生成视频的时间真实性，并展示MV与RGB融合可增强判别模型的时序推理。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频评估偏重空间外观，难以敏感捕捉运动与时间一致性；需要轻量、尺度稳健、与模型无关的时序评价信号。

Method: 从H.264/HEVC等编码器直接提取运动向量，构建MV统计分布；以KL、JS、Wasserstein散度比较真实与生成视频的MV分布；可视化MV场与类别条件运动热图以诊断伪影；并探索MV-RGB融合（通道级拼接、交叉注意力、联合嵌入、运动感知融合模块）用于下游分类（ResNet、I3D、TSN）。

Result: 在GenVidBench上：熵相关散度显示Pika与SVD最接近真实，MV-sum统计更偏向VC2与Text2Video-Zero；CogVideo偏差最大，并出现中心偏置、稀疏/分段常值流与网格伪影。融合MV显著提升真伪判别：ResNet-18/34最高达97.4%，I3D达99.0%。

Conclusion: 压缩域MV是有效且可扩展的时序信号，可用于评估生成视频的运动真实性并提升判别模型的时间推理；该方法模型无关、轻量且能捕捉帧级指标忽略的运动缺陷。

Abstract: Temporal realism remains a central weakness of current generative video models, as most evaluation metrics prioritize spatial appearance and offer limited sensitivity to motion. We introduce a scalable, model-agnostic framework that assesses temporal behavior using motion vectors (MVs) extracted directly from compressed video streams. Codec-generated MVs from standards such as H.264 and HEVC provide lightweight, resolution-consistent descriptors of motion dynamics. We quantify realism by computing Kullback-Leibler, Jensen-Shannon, and Wasserstein divergences between MV statistics of real and generated videos. Experiments on the GenVidBench dataset containing videos from eight state-of-the-art generators reveal systematic discrepancies from real motion: entropy-based divergences rank Pika and SVD as closest to real videos, MV-sum statistics favor VC2 and Text2Video-Zero, and CogVideo shows the largest deviations across both measures. Visualizations of MV fields and class-conditional motion heatmaps further reveal center bias, sparse and piecewise constant flows, and grid-like artifacts that frame-level metrics do not capture. Beyond evaluation, we investigate MV-RGB fusion through channel concatenation, cross-attention, joint embedding, and a motion-aware fusion module. Incorporating MVs improves downstream classification across ResNet, I3D, and TSN backbones, with ResNet-18 and ResNet-34 reaching up to 97.4% accuracy and I3D achieving 99.0% accuracy on real-versus-generated discrimination. These findings demonstrate that compressed-domain MVs provide an effective temporal signal for diagnosing motion defects in generative videos and for strengthening temporal reasoning in discriminative models. The implementation is available at: https://github.com/KurbanIntelligenceLab/Motion-Vector-Learning

</details>


### [20] [SAE-MCVT: A Real-Time and Scalable Multi-Camera Vehicle Tracking Framework Powered by Edge Computing](https://arxiv.org/abs/2511.13904)
*Yuqiang Lin,Sam Lockyer,Florian Stanek,Markus Zarbock,Adrian Evans,Wenbin Li,Nic Zhang*

Main category: cs.CV

TL;DR: 提出SAE-MCVT：首个面向城市级、可扩展的实时多摄像头车辆跟踪系统。在边缘端进行检测/跟踪/特征提取与地理映射，仅上传轻量元数据；中心端基于自监督学习到的相邻摄像头时空约束做跨摄关联。在2K@15FPS下实时运行，IDF1=61.2。


<details>
  <summary>Details</summary>
Motivation: 现有MCVT研究偏重精度，忽视实时性与可扩展性；随着城市规模与摄像头数量增长，实时处理与跨摄关联的系统瓶颈凸显，需要一种能在实际部署中高效运行的端边云协同方案。

Method: 提出SAE-MCVT框架：多边缘设备各自处理RTSP直播流，串行模块包括目标检测、单摄跟踪、地理映射与外观特征提取；仅上传车辆位置与深度外观特征到中央工作站；中央端利用自监督学习得到的相邻摄像头时空关系构建摄像头链接模型，在其约束下进行跨摄匹配与轨迹关联，实现分布式-集中式混合处理。

Result: 在RoundaboutHD数据集上，实现2K分辨率15FPS的实时处理，跨摄跟踪IDF1达到61.2。

Conclusion: SAE-MCVT在端边云架构下实现了城市级可扩展的实时多摄车辆跟踪，兼顾通信开销与计算负载；表明通过轻量元数据上传与自监督时空约束可有效提升跨摄关联效率与部署可行性。

Abstract: In modern Intelligent Transportation Systems (ITS), cameras are a key component due to their ability to provide valuable information for multiple stakeholders. A central task is Multi-Camera Vehicle Tracking (MCVT), which generates vehicle trajectories and enables applications such as anomaly detection, traffic density estimation, and suspect vehicle tracking. However, most existing studies on MCVT emphasize accuracy while overlooking real-time performance and scalability. These two aspects are essential for real-world deployment and become increasingly challenging in city-scale applications as the number of cameras grows. To address this issue, we propose SAE-MCVT, the first scalable real-time MCVT framework. The system includes several edge devices that interact with one central workstation separately. On the edge side, live RTSP video streams are serialized and processed through modules including object detection, object tracking, geo-mapping, and feature extraction. Only lightweight metadata -- vehicle locations and deep appearance features -- are transmitted to the central workstation. On the central side, cross-camera association is calculated under the constraint of spatial-temporal relations between adjacent cameras, which are learned through a self-supervised camera link model. Experiments on the RoundaboutHD dataset show that SAE-MCVT maintains real-time operation on 2K 15 FPS video streams and achieves an IDF1 score of 61.2. To the best of our knowledge, this is the first scalable real-time MCVT framework suitable for city-scale deployment.

</details>


### [21] [Mind the Gap: Evaluating LLM Understanding of Human-Taught Road Safety Principles](https://arxiv.org/abs/2511.13909)
*Chalamalasetti Kranti*

Main category: cs.CV

TL;DR: 评估多模态大语言模型对道路安全概念（基于示意与插图）的理解，构建教科书式交通标志/规则小型数据集，零样本测试发现模型在安全推理上表现不足，并分析人与模形之间的差距。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶AI必须遵循道路安全规范，但现有多模态LLM是否真正理解这些规范尚不清楚。人类可通过教科书插图学习规则，模型是否能在类似材料上做到安全推理有待验证。

Method: 构建一个来自学校教材的交通标志与道路安全规范插图/示意图的试点数据集；在零样本设定下评估多模态LLM对这些图文的理解与推理能力；比较模型输出与预期答案，并对错误类型做细粒度分析。

Result: 初步结果显示，多模态LLM在安全相关推理、标志含义判读及规范应用方面表现不佳，呈现明显的人类学习与模型解释之间的差距。

Conclusion: 当前多模态LLM尚难可靠掌握道路安全知识与推理，需针对安全理解、视觉符号语义对齐与推理链设计进行改进，并以此差距分析指导后续研究与数据集扩充。

Abstract: Following road safety norms is non-negotiable not only for humans but also for the AI systems that govern autonomous vehicles. In this work, we evaluate how well multi-modal large language models (LLMs) understand road safety concepts, specifically through schematic and illustrative representations. We curate a pilot dataset of images depicting traffic signs and road-safety norms sourced from school text books and use it to evaluate models capabilities in a zero-shot setting. Our preliminary results show that these models struggle with safety reasoning and reveal gaps between human learning and model interpretation. We further provide an analysis of these performance gaps for future research.

</details>


### [22] [Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding](https://arxiv.org/abs/2511.13924)
*Qingyang Yan,Guangyao Chen,Yixiong Zou*

Main category: cs.CV

TL;DR: 论文提出CuRPO，一种基于课程学习的相对策略优化方法，用CoT长度与gIoU作为难度信号，从易到难训练视觉指代（Visual Grounding），在RefCOCO系列与LISA上显著超过现有方法，最多提升+12.52 mAP，并在小样本下亦稳健高效。


<details>
  <summary>Details</summary>
Motivation: 观察到：1) 用RL微调的链式思维（CoT）在视觉指代中可能适得其反，尤其当推理链过长/复杂；2) 简单扩大数据量并不一定提升效果，因数据难度不均。为此需要一种能感知样本复杂度并循序渐进训练的策略，以避免冗长推理带来的退化，并充分利用异质数据。

Method: 提出Curriculum-based Relative Policy Optimization（CuRPO）：以CoT长度与gIoU奖励作为复杂度指标，将训练样本按由易到难组织；在RL框架下采用相对策略优化（RPO），在课程进度中逐步引入更复杂的样本；以视觉定位精度（gIoU）和推理长度共同指导策略更新，抑制无效冗长CoT。

Result: 在RefCOCO、RefCOCO+、RefCOCOg、LISA上广泛实验，CuRPO稳定优于现有方法（如Visual-RFT），在RefCOCO上最高提升+12.52 mAP；在小样本设置下依然表现强、效率高，特别在文本描述模糊复杂的情形下更显著。

Conclusion: CoT并非越长越好，RL微调可能因复杂推理链导致退化。通过以CoT长度与gIoU为难度信号的课程式RPO训练，可更稳健地提升视觉指代的定位性能与样本效率，优于当前SOTA并具备少样本鲁棒性。

Abstract: Chain-of-Thought (CoT) prompting has recently shown significant promise across various NLP and computer vision tasks by explicitly generating intermediate reasoning steps. However, we find that reinforcement learning (RL)-based fine-tuned CoT reasoning can paradoxically degrade performance in Visual Grounding tasks, particularly as CoT outputs become lengthy or complex. Additionally, our analysis reveals that increased dataset size does not always enhance performance due to varying data complexities. Motivated by these findings, we propose Curriculum-based Relative Policy Optimization (CuRPO), a novel training strategy that leverages CoT length and generalized Intersection over Union (gIoU) rewards as complexity indicators to progressively structure training data from simpler to more challenging examples. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and LISA datasets demonstrate the effectiveness of our approach. CuRPO consistently outperforms existing methods, including Visual-RFT, with notable improvements of up to +12.52 mAP on RefCOCO. Moreover, CuRPO exhibits exceptional efficiency and robustness, delivering strong localization performance even in few-shot learning scenarios, particularly benefiting tasks characterized by ambiguous and intricate textual descriptions.The code is released on https://github.com/qyoung-yan/CuRPO.

</details>


### [23] [Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets](https://arxiv.org/abs/2511.13944)
*Noam Glazner,Noam Tsfaty,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: 提出一种基于聚类的帧选择与数据划分策略，先按视觉相似度聚类视频帧，再在簇层面进行训练/验证/测试拆分，以减少信息泄漏并获得更具代表性和平衡性的分割。


<details>
  <summary>Details</summary>
Motivation: 传统从视频抽帧后再随机划分数据，会把同一或相似片段的帧分散到不同拆分中，导致泄漏（模型在训练中见过几乎相同的内容），从而高估性能、降低泛化可信度。需要一种能在划分前识别相似帧并避免跨集重复的机制。

Method: 对抽取的视频帧进行视觉特征提取（如CNN/自监督表征），基于特征进行聚类，将相似帧归为同一簇；随后在簇级别（而非帧级别）进行分层或比例式划分到训练/验证/测试集，以保持类内相似帧不跨集，同时控制各簇、场景或元数据的均衡与代表性。

Result: 得到的训练/验证/测试划分更平衡、代表性更强，显著降低跨集相似帧带来的信息泄漏，评测结果更可靠，模型泛化更可期。

Conclusion: 基于聚类的帧级数据划分是处理视频衍生帧数据集的有效策略，可在不改变下游模型的前提下提升评测可信度与数据质量，建议作为视频数据集构建的标准流程。

Abstract: We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.

</details>


### [24] [Can You Learn to See Without Images? Procedural Warm-Up for Vision Transformers](https://arxiv.org/abs/2511.13945)
*Zachary Shinnick,Liangze Jiang,Hemanth Saratchandran,Damien Teney,Anton van den Hengel*

Main category: cs.CV

TL;DR: 用程序生成、无视觉语义内容的数据先对ViT进行“热身”预训练，学习抽象计算先验；再进行常规图像训练，可显著提升数据效率、收敛速度与下游性能。


<details>
  <summary>Details</summary>
Motivation: Transformer跨域通用性暗示其存在跨模态有益的归纳偏置，现有视觉预训练大多依赖真实/合成图像。作者动机是：能否用与图像无关的程序化数据，向ViT灌输通用计算先验，从而更数据高效且领域无关。

Method: 用形式文法等简单算法生成无语义、非图像类的序列或结构数据；在预训练“热身”阶段，绕过ViT的视觉patch embedding，对ViT进行这种程序数据训练，使模型内化抽象先验；随后再进行标准的基于图像的数据训练。

Result: 在ImageNet-1k上，仅将1%的训练预算用于程序化数据，最终top-1准确率提升>1.7%；其性能增益相当于增加28%的ImageNet-1k数据。并观察到更快收敛与更高数据效率。

Conclusion: 用与视觉无关的程序生成数据对ViT进行前置预训练，可作为通用、数据高效的预训练策略路径，能提升下游视觉任务表现并加速训练。

Abstract: Transformers show remarkable versatility across domains, suggesting the existence of inductive biases beneficial across modalities. In this work, we explore a new way to instil such generic biases in vision transformers (ViTs) by pretraining on procedurally-generated data devoid of visual or semantic content. We generate this data with simple algorithms such as formal grammars, so the results bear no relationship to either natural or synthetic images. We use this procedurally-generated data to pretrain ViTs in a warm-up phase that bypasses their visual patch embedding mechanisms, thus encouraging the models to internalise abstract computational priors. When followed by standard image-based training, this warm-up significantly improves data efficiency, convergence speed, and downstream performance. On ImageNet-1k for example, allocating just 1% of the training budget to procedural data improves final accuracy by over 1.7%. In terms of its effect on performance, 1% procedurally generated data is thus equivalent to 28% of the ImageNet-1k data. These findings suggest a promising path toward new data-efficient and domain-agnostic pretraining strategies.

</details>


### [25] [Single Tensor Cell Segmentation using Scalar Field Representations](https://arxiv.org/abs/2511.13947)
*Kevin I. Ruiz Vargas,Gabriel G. Galdino,Tsang Ing Ren,Alexandre L. Cunha*

Main category: cs.CV

TL;DR: 提出用标量场视角进行细胞图像分割：训练网络预测满足Poisson或稳态扩散方程的连续标量场，再用分水岭对该场分割，几乎无需正则化，简单高效且边缘锐利，结果在公开数据上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 实例级细胞分割常受噪声、标注异常和复杂形态影响；现有方法常需多通道标签、复杂后处理与较高算力。作者希望用物理一致、鲁棒且实现简洁的标量场来稳定生成细胞实例，并降低训练/推理开销，适合边缘计算。

Method: 将分割建模为学习连续标量场f(x)；网络（如U-Net）输出单通道张量，拟合Poisson方程或热方程稳态的解，通过最小化PDE残差训练（无显式正则）。推理时对预测的标量场应用分水岭生成实例。该框架利用PDE几何性质产生清晰的盆地与脊线，天然适配实例分割。

Result: 在公开细胞数据集上取得与主流方法相当或更优的性能；训练仅需单通道监督/张量，训练与推理更快、内存更小，边界更锐利且对离群标注更鲁棒。

Conclusion: 用PDE残差驱动的标量场学习结合分水岭，可在保持简洁与高效的同时实现高质量细胞实例分割；方法具备几何直观性、鲁棒性与低资源占用，适合边缘场景。

Abstract: We investigate image segmentation of cells under the lens of scalar fields. Our goal is to learn a continuous scalar field on image domains such that its segmentation produces robust instances for cells present in images. This field is a function parameterized by the trained network, and its segmentation is realized by the watershed method. The fields we experiment with are solutions to the Poisson partial differential equation and a diffusion mimicking the steady-state solution of the heat equation. These solutions are obtained by minimizing just the field residuals, no regularization is needed, providing a robust regression capable of diminishing the adverse impacts of outliers in the training data and allowing for sharp cell boundaries. A single tensor is all that is needed to train a \unet\ thus simplifying implementation, lowering training and inference times, hence reducing energy consumption, and requiring a small memory footprint, all attractive features in edge computing. We present competitive results on public datasets from the literature and show that our novel, simple yet geometrically insightful approach can achieve excellent cell segmentation results.

</details>


### [26] [EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation](https://arxiv.org/abs/2511.13948)
*Matin Daghyani,Lyuyang Wang,Nima Hashemi,Bassant Medhat,Baraa Abdelsamad,Eros Rojas Velez,XiaoXiao Li,Michael Y. C. Tsang,Christina Luong,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: EchoAgent 是一个由大语言模型调度多种视觉工具的心超视频分析框架，实现视频级定位、测量与临床解读，并引入“可测性预测”以自动选择合适测量。结果显示其在保证可解释性的同时达到准确且与指南一致的分析。


<details>
  <summary>Details</summary>
Motivation: 现有心脏超声深度学习方法多聚焦单帧或单任务，缺乏视频级时空推理与基于指南的结构化测量与解读，难以满足临床透明可追溯的需求。

Method: 提出 EchoAgent：以LLM为控制器，编排专门化视觉工具执行（1）时间段定位、（2）空间解剖测量、（3）临床解读。核心是“测量可行性预测模型”，评估每帧解剖结构是否可可靠测量，从而自适应选择工具与帧；并构建包含多样视频-查询对的临床验证基准用于评估。

Result: 在具有时空复杂度的心超视频任务上，EchoAgent能生成与视觉证据和临床指南一致的输出，既准确又可解释，支持透明和可追溯工作流。

Conclusion: 证明了利用任务特定工具与LLM代理实现指南对齐的心超视频自动化是可行的，为可信的心脏超声AI开辟方向。

Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.

</details>


### [27] [Learning Skill-Attributes for Transferable Assessment in Video](https://arxiv.org/abs/2511.13993)
*Kumar Ashutosh,Kristen Grauman*

Main category: cs.CV

TL;DR: 提出CrossTrainer，通过学习跨运动通用的“技能属性”（如平衡、控制、手部位置），并结合多模态语言模型，为任意运动视频生成可操作反馈与熟练度评估；在跨运动与同运动评测上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频技能评估模型多为单一运动专用，专家标注稀缺且成本高，难以覆盖长尾运动；需要一种能跨运动迁移、减少专家依赖、同时提供可解释与可操作反馈的通用表示。

Method: 1) 从多运动数据中自动发现抽象的技能属性（如平衡、控制、手部姿势），学习与这些属性对齐的视频表示；2) 将该表示与语言模态对接，训练多模态语言模型，生成两类输出：a) 行动性反馈（如“抬高手以增加力量”）；b) 熟练度分级（如“early expert”）；3) 在跨运动迁移与域内场景进行训练与评测。

Result: 在多个数据集上，CrossTrainer在跨运动与域内任务中相对SOTA提升最高达60%；所学表示在泛化与可解释性上优于多种现有技术，并能为新视频生成高质量、可操作的建议与熟练度判断。

Conclusion: 通过抽象并对齐跨运动共享的技能行为，CrossTrainer实现了更强的迁移与泛化能力，显著提升技能评估与反馈生成的效果，证明了通用技能属性驱动的视频表示可有效增强多模态大模型。

Abstract: Skill assessment from video entails rating the quality of a person's physical performance and explaining what could be done better. Today's models specialize for an individual sport, and suffer from the high cost and scarcity of expert-level supervision across the long tail of sports. Towards closing that gap, we explore transferable video representations for skill assessment. Our CrossTrainer approach discovers skill-attributes, such as balance, control, and hand positioning -- whose meaning transcends the boundaries of any given sport, then trains a multimodal language model to generate actionable feedback for a novel video, e.g., "lift hands more to generate more power" as well as its proficiency level, e.g., early expert. We validate the new model on multiple datasets for both cross-sport (transfer) and intra-sport (in-domain) settings, where it achieves gains up to 60% relative to the state of the art. By abstracting out the shared behaviors indicative of human skill, the proposed video representation generalizes substantially better than an array of existing techniques, enriching today's multimodal large language models.

</details>


### [28] [CD-DPE: Dual-Prompt Expert Network based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2511.14014)
*Xianming Gu,Lihui Wang,Ying Cao,Zeyu Deng,Yingfeng Ou,Guodong Hu,Yi Chen*

Main category: cs.CV

TL;DR: 提出CD-DPE：通过卷积字典特征解耦与双提示专家融合，实现多对比MRI超分辨率，提升细节与泛化。


<details>
  <summary>Details</summary>
Motivation: 多对比MRI可用参考对比度的HR图像辅助重建目标对比度的HR图像，但跨对比的纹理/对齐/统计差异会导致特征混淆与低效融合，限制重建质量与泛化能力。需要一种既能解耦跨/内对比特征、又能自适应选择与融合参考信息的方法。

Method: 1) 提出卷积字典特征解耦模块（CD-FDM）：迭代方式用卷积字典表征，将特征分离为跨对比共享与目标对比专属两部分，减少冗余与干扰。2) 设计双提示特征融合专家模块（DP-FFEM）：— 频域提示用于筛选参考图像中与目标相关的频率/纹理成分；— 自适应路由提示决定参考与目标特征的融合路径和策略；3) 组成双提示专家网络（CD-DPE），端到端训练用于多对比MRI超分。

Result: 在公开多对比MRI数据集上，CD-DPE在细节重建与定量指标上优于SOTA；在未见过的数据集上仍保持较好性能，显示强泛化能力。

Conclusion: 通过卷积字典解耦与双提示自适应融合，可更有效地利用参考对比度信息，缓解跨对比差异带来的干扰，显著提升多对比MRI超分辨率的细节与鲁棒性，并具备良好泛化。

Abstract: Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.

</details>


### [29] [RISE: Single Static Radar-based Indoor Scene Understanding](https://arxiv.org/abs/2511.14019)
*Kaichen Zhou,Laura Dodds,Sayed Saad Afzal,Fadel Adib*

Main category: cs.CV

TL;DR: 提出RISE：首个单静态毫米波雷达的室内场景理解基准与系统，利用双角度多径增强和分层扩散，从低分辨率雷达回波重建布局并检测物体，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 室内使用RGB/LiDAR虽精细但存在遮挡与隐私风险；毫米波雷达具隐私与穿透优势却空间分辨率低、难以几何推理。需一种方法在保隐私前提下实现稳健的布局重建与目标检测。

Method: 1) 提出Bi-Angular Multipath Enhancement，显式建模到达角(AoA)与出发角(AoD)，恢复二次“鬼影”反射，从多径中提取隐藏几何；2) 在增强观测上构建仿真到现实的层次化扩散(Hierarchical Diffusion)框架，将碎片化雷达响应转化为完整的平面布局重建与目标检测；3) 构建RISE数据集：单静态雷达，100条真实室内轨迹，共5万帧。

Result: 在布局重建上，Chamfer Distance相较SOTA降低60%至16 cm；首次实现基于毫米波的目标检测，IoU达58%。

Conclusion: 多径不是噪声而是几何信息源。通过双角度多径增强与层次扩散，单静态毫米波雷达即可实现几何感知且保护隐私的室内场景理解；RISE为该方向提供首个大规模基准与强基线。

Abstract: Robust and privacy-preserving indoor scene understanding remains a fundamental open problem. While optical sensors such as RGB and LiDAR offer high spatial fidelity, they suffer from severe occlusions and introduce privacy risks in indoor environments. In contrast, millimeter-wave (mmWave) radar preserves privacy and penetrates obstacles, but its inherently low spatial resolution makes reliable geometric reasoning difficult.
  We introduce RISE, the first benchmark and system for single-static-radar indoor scene understanding, jointly targeting layout reconstruction and object detection. RISE is built upon the key insight that multipath reflections, traditionally treated as noise, encode rich geometric cues. To exploit this, we propose a Bi-Angular Multipath Enhancement that explicitly models Angle-of-Arrival and Angle-of-Departure to recover secondary (ghost) reflections and reveal invisible structures. On top of these enhanced observations, a simulation-to-reality Hierarchical Diffusion framework transforms fragmented radar responses into complete layout reconstruction and object detection.
  Our benchmark contains 50,000 frames collected across 100 real indoor trajectories, forming the first large-scale dataset dedicated to radar-based indoor scene understanding. Extensive experiments show that RISE reduces the Chamfer Distance by 60% (down to 16 cm) compared to the state of the art in layout reconstruction, and delivers the first mmWave-based object detection, achieving 58% IoU. These results establish RISE as a new foundation for geometry-aware and privacy-preserving indoor scene understanding using a single static radar.

</details>


### [30] [MRI Plane Orientation Detection using a Context-Aware 2.5D Model](https://arxiv.org/abs/2511.14021)
*SangHyuk Kim,Daniel Haehn,Sumientra Rampersad*

Main category: cs.CV

TL;DR: 提出一种2.5D上下文感知模型，自动判别MRI切片的解剖平面（轴位/冠状/矢状），在多源数据中生成或修复平面元数据，精度达99.49%，并在脑肿瘤检测任务中通过不确定性门控策略提升诊断准确率至98.0%。


<details>
  <summary>Details</summary>
Motivation: 临床与科研数据常缺失或错误标注MRI平面方向，导致跨数据集融合产生域偏移，影响后续诊断模型的鲁棒性与准确性。需要一个可靠的自动化方法来生成/纠正平面方向元数据，并验证该元数据对下游任务的实际价值。

Method: 提出2.5D上下文感知分类器：利用相邻多切片信息缓解单张2D切片的歧义，既在3D序列也在静态2D图像上训练。与一个2D基线模型对比；并在下游脑肿瘤检测中，使用不确定性驱动的门控策略，选择性采用带元数据增强的预测。提供交互式Web应用与开源实现。

Result: 2D基线准确率98.74%；2.5D方法准确率99.49%，相对错误率降低约60%。在脑肿瘤检测任务中，纯图像模型97.0%准确率，经门控地使用平面元数据增强后达98.0%，误诊减少33.3%。

Conclusion: 2.5D上下文显著提升MRI平面识别的可靠性，自动生成的平面元数据对下游诊断有实际收益。不确定性门控能稳健地整合元数据以提升性能。系统已集成至交互式Web应用并开源，便于落地与复现。

Abstract: Humans can easily identify anatomical planes (axial, coronal, and sagittal) on a 2D MRI slice, but automated systems struggle with this task. Missing plane orientation metadata can complicate analysis, increase domain shift when merging heterogeneous datasets, and reduce accuracy of diagnostic classifiers. This study develops a classifier that accurately generates plane orientation metadata. We adopt a 2.5D context-aware model that leverages multi-slice information to avoid ambiguity from isolated slices and enable robust feature learning. We train the 2.5D model on both 3D slice sequences and static 2D images. While our 2D reference model achieves 98.74% accuracy, our 2.5D method raises this to 99.49%, reducing errors by 60%, highlighting the importance of 2.5D context. We validate the utility of our generated metadata in a brain tumor detection task. A gated strategy selectively uses metadata-enhanced predictions based on uncertainty scores, boosting accuracy from 97.0% with an image-only model to 98.0%, reducing misdiagnoses by 33.3%. We integrate our plane orientation model into an interactive web application and provide it open-source.

</details>


### [31] [LINGUAL: Language-INtegrated GUidance in Active Learning for Medical Image Segmentation](https://arxiv.org/abs/2511.14028)
*Md Shazid Islam,Shreyangshu Bera,Sudipta Paul,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: 提出LINGUAL框架，用自然语言指导替代繁琐像素级标注，在医学图像分割的主动学习/主动域适应中，以极少专家交互达成与传统方法相当或更优性能，并将标注时间约减80%。


<details>
  <summary>Details</summary>
Motivation: 医学分割常有模糊边界，传统主动学习需在大ROI（成本高但认知负担低）与小ROI（成本低但需要高精度、负担重）之间权衡，整体仍费时费力。希望用语言指导规避精细边界描画，降低专家介入成本。

Method: 引入LINGUAL：专家用自然语言给出指令；系统通过in-context learning将指令翻译为可执行程序；随后自动串行完成一系列子任务，无需人工干预。将其用于主动域适应（ADA）场景进行评估。

Result: 在ADA实验中，LINGUAL与标准主动学习基线达到相当或更优的分割性能，同时将估计标注时间减少约80%。

Conclusion: 语言指导结合程序化执行能显著降低医学分割中的标注负担，同时保持或提升性能；在跨域适应任务中尤为有效，提示以自然语言交互替代精细像素标注是可行且高效的路径。

Abstract: Although active learning (AL) in segmentation tasks enables experts to annotate selected regions of interest (ROIs) instead of entire images, it remains highly challenging, labor-intensive, and cognitively demanding due to the blurry and ambiguous boundaries commonly observed in medical images. Also, in conventional AL, annotation effort is a function of the ROI- larger regions make the task cognitively easier but incur higher annotation costs, whereas smaller regions demand finer precision and more attention from the expert. In this context, language guidance provides an effective alternative, requiring minimal expert effort while bypassing the cognitively demanding task of precise boundary delineation in segmentation. Towards this goal, we introduce LINGUAL: a framework that receives natural language instructions from an expert, translates them into executable programs through in-context learning, and automatically performs the corresponding sequence of sub-tasks without any human intervention. We demonstrate the effectiveness of LINGUAL in active domain adaptation (ADA) achieving comparable or superior performance to AL baselines while reducing estimated annotation time by approximately 80%.

</details>


### [32] [Training-free Detection of AI-generated images via Cropping Robustness](https://arxiv.org/abs/2511.14030)
*Sungik Choi,Hankook Lee,Moontae Lee*

Main category: cs.CV

TL;DR: 提出一种无需训练的AI生成图像检测算法WaRPAD，利用自监督模型的尺度/裁剪不变性，通过小波高频扰动与分块平均评分实现鲁棒检测，对多数据集与23种生成模型表现稳定且抗干扰。


<details>
  <summary>Details</summary>
Motivation: 现有检测器常需针对特定数据集训练且对分辨率/领域变化不稳；自监督模型普遍具备对随机裁剪与缩放的表示一致性，启发用其“应当一致”的性质来识别生成图像在某些操作下的异常敏感性，从而实现零样本/免训练检测。

Method: 基于自监督模型：1) 用Haar小波提取图像高频方向；2) 沿这些高频方向施加微扰，定义“基准分数”度量嵌入对高频扰动的敏感度；3) 将图像缩放到模型输入尺寸的整数倍并切成小块，分别计算基准分数以模拟对裁剪增强的鲁棒性；4) 将各块分数求平均得到最终检测分数；该流程不需额外训练；可替换不同自监督骨干。

Result: 在多分辨率、多领域真实数据集及23种生成模型上取得与SOTA相当或更优的检测性能；对测试时腐蚀/扰动具强鲁棒性；方法可跨多种自监督模型通用。

Conclusion: 利用自监督模型对RandomResizedCrop的内在不变性与高频敏感度差异，WaRPAD实现了通用、免训练、跨分辨率/领域鲁棒的AI生成图像检测，具有良好泛化与可移植性。

Abstract: AI-generated image detection has become crucial with the rapid advancement of vision-generative models. Instead of training detectors tailored to specific datasets, we study a training-free approach leveraging self-supervised models without requiring prior data knowledge. These models, pre-trained with augmentations like RandomResizedCrop, learn to produce consistent representations across varying resolutions. Motivated by this, we propose WaRPAD, a training-free AI-generated image detection algorithm based on self-supervised models. Since neighborhood pixel differences in images are highly sensitive to resizing operations, WaRPAD first defines a base score function that quantifies the sensitivity of image embeddings to perturbations along high-frequency directions extracted via Haar wavelet decomposition. To simulate robustness against cropping augmentation, we rescale each image to a multiple of the models input size, divide it into smaller patches, and compute the base score for each patch. The final detection score is then obtained by averaging the scores across all patches. We validate WaRPAD on real datasets of diverse resolutions and domains, and images generated by 23 different generative models. Our method consistently achieves competitive performance and demonstrates strong robustness to test-time corruptions. Furthermore, as invariance to RandomResizedCrop is a common training scheme across self-supervised models, we show that WaRPAD is applicable across self-supervised models.

</details>


### [33] [FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization](https://arxiv.org/abs/2511.14031)
*Rong Zhang,Jinxiao Li,Jingnan Wang,Zhiwen Zuo,Jianfeng Dong,Wei Li,Chi Wang,Weiwei Xu,Xun Wang*

Main category: cs.CV

TL;DR: 提出FashionMAC：一种无形变、基于扩散的时尚展示图生成框架，通过对已穿着人像中分割出的服装进行直接外扩生成，避免服装变形与纹理扭曲，并引入区域自适应解耦注意力与链式掩码注入以实现细粒度外观可控，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有服装中心的时尚图生成方法需在生成过程中对服装做几何变形，容易造成纹理失真；同时缺乏对人模细粒度属性（如发型、肤色、配饰等）的可控机制，难以满足电商对高保真与高可控性的需求。

Method: 提出FashionMAC：1) 去形变思路：先从穿衣人像中分割服装，直接进行外扩(outpainting)以合成人体与场景，保留服装精细纹理；2) 区域自适应解耦注意力RADA：为每个细粒度文本属性自适应预测其应生成/影响的空间区域；3) 链式掩码注入：将预测到的区域掩码逐步注入扩散过程，强制相应文本属性关注指定区域，实现稳定且可控的属性编辑与合成。

Result: 在多项实验与与SOTA对比中，FashionMAC在保留服装细节、图像质量与文本-图像一致性、以及细粒度可控性方面均取得领先的定量与定性结果。

Conclusion: 去形变的扩散框架结合RADA与链式掩码注入，可在不牺牲服装细节的前提下，实现对人模外观的细粒度可控生成，为电商等应用提供更高质量的时尚展示图。

Abstract: Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the model's appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.

</details>


### [34] [Flood-LDM: Generalizable Latent Diffusion Models for rapid and accurate zero-shot High-Resolution Flood Mapping](https://arxiv.org/abs/2511.14033)
*Sun Han Neo,Sachith Seneviratne,Herath Mudiyanselage Viraj Vidura Herath,Abhishek Saha,Sanka Rasnayaka,Lucy Amanda Marshall*

Main category: cs.CV

TL;DR: 提出用潜空间扩散模型对粗网格洪水图进行超分辨，在保持精度的同时显著降低推理时间，并具备更好的跨区域泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 物理水动力模型精度高但计算昂贵，难以满足大范围实时应用；现有基于CNN的超分辨虽快但对未见区域泛化差、可解释性不足。

Method: 将粗分辨率洪水图映射到潜空间，采用潜扩散模型进行超分辨重建；引入物理相关（physics-informed）的输入特征以约束与解释；并通过迁移学习快速适配新区域。

Result: 在多数据集实验中，潜扩散模型在不牺牲精度的前提下显著缩短生成高保真洪水图的计算时间；在跨地区测试中优于CNN的泛化表现，迁移学习进一步加速对新地理区域的适配。

Conclusion: 潜扩散超分辨可替代部分高耗时的物理模型推演，实现实时洪灾风险管理；方法具备更强泛化与一定可解释性。代码开源于提供的仓库。

Abstract: Flood prediction is critical for emergency planning and response to mitigate human and economic losses. Traditional physics-based hydrodynamic models generate high-resolution flood maps using numerical methods requiring fine-grid discretization; which are computationally intensive and impractical for real-time large-scale applications. While recent studies have applied convolutional neural networks for flood map super-resolution with good accuracy and speed, they suffer from limited generalizability to unseen areas. In this paper, we propose a novel approach that leverages latent diffusion models to perform super-resolution on coarse-grid flood maps, with the objective of achieving the accuracy of fine-grid flood maps while significantly reducing inference time. Experimental results demonstrate that latent diffusion models substantially decrease the computational time required to produce high-fidelity flood maps without compromising on accuracy, enabling their use in real-time flood risk management. Moreover, diffusion models exhibit superior generalizability across different physical locations, with transfer learning further accelerating adaptation to new geographic regions. Our approach also incorporates physics-informed inputs, addressing the common limitation of black-box behavior in machine learning, thereby enhancing interpretability. Code is available at https://github.com/neosunhan/flood-diff.

</details>


### [35] [Saliency-Guided Deep Learning for Bridge Defect Detection in Drone Imagery](https://arxiv.org/abs/2511.14040)
*Loucif Hebbache,Dariush Amirkhani,Mohand Saïd Allili,Jean-François Lapointe*

Main category: cs.CV

TL;DR: 提出一种两阶段方法，用无人机图像对混凝土桥梁缺陷进行自动检测、定位与分类：先用显著性生成缺陷候选，再对显著区域做亮度增强并用YOLOX检测。实验显示精度高且高效，适合自供能巡检系统。


<details>
  <summary>Details</summary>
Motivation: 桥梁缺陷具有不规则、尺度多样、背景复杂等特点，传统方法或单一深度模型易受噪声与背景纹理影响，难以兼顾精度与效率。需要一种能在复杂背景下稳健发现缺陷并具备工程可用性的自动化方案。

Method: 两阶段框架：1) 显著性驱动的缺陷候选生成，利用缺陷相对周围表面纹理的局部不连续性提议区域；2) 在候选框内做亮度增强以提升显著性，得到“显著性增强图”，再用基于YOLOX的目标检测网络进行检测与分类。

Result: 在标准数据集上验证，该方法在准确性与计算效率上表现优异；显著性+亮度增强有助于YOLOX更稳定地定位与识别缺陷。

Conclusion: 显著性引导与YOLOX检测的结合可有效提升桥梁缺陷的自动检测与分类性能，具备部署到自供能无人机巡检系统的潜力。

Abstract: Anomaly object detection and classification are one of the main challenging tasks in computer vision and pattern recognition. In this paper, we propose a new method to automatically detect, localize and classify defects in concrete bridge structures using drone imagery. This framework is constituted of two main stages. The first stage uses saliency for defect region proposals where defects often exhibit local discontinuities in the normal surface patterns with regard to their surrounding. The second stage employs a YOLOX-based deep learning detector that operates on saliency-enhanced images obtained by applying bounding-box level brightness augmentation to salient defect regions. Experimental results on standard datasets confirm the performance of our framework and its suitability in terms of accuracy and computational efficiency, which give a huge potential to be implemented in a self-powered inspection system.

</details>


### [36] [Semantic Context Matters: Improving Conditioning for Autoregressive Models](https://arxiv.org/abs/2511.14063)
*Dongyang Jin,Ryan Xu,Jianhao Zeng,Rui Lan,Yancheng Bai,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出SCAR，一种针对自回归图像模型的语义-上下文驱动编辑方法，通过语义压缩预填充与语义对齐引导，提升指令遵循与画质并兼容多种AR范式。


<details>
  <summary>Details</summary>
Motivation: AR在图像生成上具备可扩展性和多模态整合优势，但在通用图像编辑中因条件弱且低效，易产生指令偏离与伪影，需要更强、更高效的语义条件化机制。

Method: 1) 压缩语义预填充：将高层语义编码成紧凑前缀，基于向量量化的prefilling保留灵活性同时降低成本与语义缺失；2) 语义对齐引导：在AR解码中对最后的视觉隐状态与目标语义进行对齐约束，提升指令忠实度。方法可在next-token与next-set两类AR中以最小改动适配。

Result: 在指令编辑与可控生成基准上实现更高的视觉保真与语义对齐，优于既有AR方法，同时保持可控性与效率。

Conclusion: SCAR弥补AR编辑中的条件化弱点，兼顾通用性与效率，显著提升编辑质量与指令遵循；代码将开源，具备复现与推广价值。

Abstract: Recently, autoregressive (AR) models have shown strong potential in image generation, offering better scalability and easier integration with unified multi-modal systems compared to diffusion-based methods. However, extending AR models to general image editing remains challenging due to weak and inefficient conditioning, often leading to poor instruction adherence and visual artifacts. To address this, we propose SCAR, a Semantic-Context-driven method for Autoregressive models. SCAR introduces two key components: Compressed Semantic Prefilling, which encodes high-level semantics into a compact and efficient prefix, and Semantic Alignment Guidance, which aligns the last visual hidden states with target semantics during autoregressive decoding to enhance instruction fidelity. Unlike decoding-stage injection methods, SCAR builds upon the flexibility and generality of vector-quantized-based prefilling while overcoming its semantic limitations and high cost. It generalizes across both next-token and next-set AR paradigms with minimal architectural changes. SCAR achieves superior visual fidelity and semantic alignment on both instruction editing and controllable generation benchmarks, outperforming prior AR-based methods while maintaining controllability. All code will be released.

</details>


### [37] [CORE: Compact Object-centric REpresentations as a New Paradigm for Token Merging in LVLMs](https://arxiv.org/abs/2511.14072)
*Jingyu Lei,Gaoang Wang,Der-Horng Lee*

Main category: cs.CV

TL;DR: 提出CORE，一种基于对象的视觉token压缩范式：用分割解码器提取对象掩码，引导token合并，并通过质心引导排序恢复空间顺序；在固定和自适应压缩下都显著提升效率并达SOTA，即便仅保留2.2%视觉token仍保留97.4%性能。


<details>
  <summary>Details</summary>
Motivation: LVLM在高分辨率下视觉token随分辨率二次增长，导致算力与内存开销巨大；现有压缩方法缺乏高层语义，容易产生不优合并、冗余或上下文丢失。需要一种既高语义又高效的压缩方式。

Method: 提出CORE：1) 利用高效分割解码器生成对象掩码作为高层语义先验；2) 以对象为单位引导视觉token聚合成紧凑的对象中心表示；3) 设计质心引导的排序机制，重新组织合并后token的空间顺序以保留位置信息；4) 在固定率与自适应率两种压缩设定下进行训练/评估。

Result: 在六个权威基准上固定率压缩达到新的SOTA；自适应率设定下显著提升效率；在极端压缩（仅保留2.2%视觉token）下仍达基线97.4%的性能。

Conclusion: 对象中心的表示对于LVLM高效而有效的处理具有优势；通过对象掩码与质心排序，可以在极高压缩率下保持性能并大幅降低计算/内存成本。

Abstract: Large Vision-Language Models (LVLMs) usually suffer from prohibitive computational and memory costs due to the quadratic growth of visual tokens with image resolution. Existing token compression methods, while varied, often lack a high-level semantic understanding, leading to suboptimal merges, information redundancy, or context loss. To address these limitations, we introduce CORE (Compact Object-centric REpresentations), a new paradigm for visual token compression. CORE leverages an efficient segmentation decoder to generate object masks, which serve as a high-level semantic prior to guide the merging of visual tokens into a compact set of object-centric representations. Furthermore, a novel centroid-guided sorting mechanism restores a coherent spatial order to the merged tokens, preserving vital positional information. Extensive experiments show that CORE not only establishes a new state-of-the-art on six authoritative benchmarks for fixed-rate compression, but also achieves dramatic efficiency gains in adaptive-rate settings. Even under extreme compression, after aggressively retaining with only 2.2% of all visual tokens, CORE still maintains 97.4% of baseline performance. Our work demonstrates the superiority of object-centric representations for efficient and effective LVLM processing.

</details>


### [38] [Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification](https://arxiv.org/abs/2511.14082)
*Yao Qin,Yangyang Yan,YuanChao Yang,Jinhua Pang,Huanyong Bi,Yuan Liu,HaiHua Wang*

Main category: cs.CV

TL;DR: 提出ZS-TMS范式：用大型预训练生成引擎，基于极少样本与文本描述，直接合成任务特定分类器的全部参数，无需训练/微调；在ISIC与自建罕见病数据集的1/5-shot上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 医疗影像领域深度学习依赖大规模高成本标注数据，罕见病样本稀缺使传统训练与微调困难；需要在极低样本下获得可靠分类器的新途径。

Method: 提出Semantic-Guided Parameter Synthesizer（SGPS）：输入最少的多模态任务信息（如1张示例图像+对应临床文本描述），利用大规模预训练生成引擎解释并生成轻量分类器（如EfficientNet-V2）的全部权重，实现“零训练”任务特定模型合成（ZS-TMS），可直接用于推理，无需额外任务特定训练或微调。

Result: 在ISIC 2018皮肤病变与自建罕见病数据集上进行严格few-shot评测，SGPS在1-shot与5-shot等超低样本场景显著优于先进few-shot与zero-shot方法，建立新的SOTA。

Conclusion: ZS-TMS与SGPS证明可在极低数据下直接合成可部署分类器，降低对大数据与标注的依赖，为罕见病等长尾任务的快速AI诊断工具开发铺路。

Abstract: Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on "big data" is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.
  The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.

</details>


### [39] [Automated glenoid bone loss measurement and segmentation in CT scans for pre-operative planning in shoulder instability](https://arxiv.org/abs/2511.14083)
*Zhonghao Liu,Hanxue Gu,Qihang Li,Michael Fox,Jay M. Levin,Maciej A. Mazurowski,Brian C. Lau*

Main category: cs.CV

TL;DR: 提出并验证了一个全自动深度学习流水线，在肩关节三维CT上用线性基于en-face视角的best-circle方法测量盂骨缺损，性能与专家一致性相当且优于术者间一致性，可用于术前规划与筛查。


<details>
  <summary>Details</summary>
Motivation: 手工/半自动盂骨缺损测量耗时且读者间差异大，影响肩关节不稳患者的术前规划与分层；需要一个客观、快速、可复现的自动化工具。

Method: 回顾91例肩关节CT及人工标注（分割、解剖点、缺损百分比），构建三阶段流水线：1) U-Net分割盂骨与肱骨；2) 第二个网络预测盂缘关键点；3) 基于PCA投影至en-face视角并进行圆拟合（best-circle），计算缺损百分比，并按低/中/高分层。

Result: 自动测量与共识读片高度一致，ICC=0.84，优于术者间一致性(ICC=0.78)；低/高缺损亚组ICC分别0.71 vs 0.63与0.83 vs 0.21（均P<0.001）。分层召回：低0.714，高0.857；无低误判为高或高误判为低。

Conclusion: 该全自动DL流水线在时间效率与一致性上具临床可行性，可辅助肩关节不稳术前规划与筛查显著盂骨缺损患者；代码与数据已开源以促进复现与临床转化。

Abstract: Reliable measurement of glenoid bone loss is essential for operative planning in shoulder instability, but current manual and semi-automated methods are time-consuming and often subject to interreader variability. We developed and validated a fully automated deep learning pipeline for measuring glenoid bone loss on three-dimensional computed tomography (CT) scans using a linear-based, en-face view, best-circle method. Shoulder CT images of 91 patients (average age, 40 years; range, 14-89 years; 65 men) were retrospectively collected along with manual labels including glenoid segmentation, landmarks, and bone loss measurements. The multi-stage algorithm has three main stages: (1) segmentation, where we developed a U-Net to automatically segment the glenoid and humerus; (2) anatomical landmark detection, where a second network predicts glenoid rim points; and (3) geometric fitting, where we applied principal component analysis (PCA), projection, and circle fitting to compute the percentage of bone loss. The automated measurements showed strong agreement with consensus readings and exceeded surgeon-to-surgeon consistency (intraclass correlation coefficient (ICC) 0.84 vs 0.78), including in low- and high-bone-loss subgroups (ICC 0.71 vs 0.63 and 0.83 vs 0.21, respectively; P < 0.001). For classifying patients into low, medium, and high bone-loss categories, the pipeline achieved a recall of 0.714 for low and 0.857 for high severity, with no low cases misclassified as high or vice versa. These results suggest that our method is a time-efficient and clinically reliable tool for preoperative planning in shoulder instability and for screening patients with substantial glenoid bone loss. Code and dataset are available at https://github.com/Edenliu1/Auto-Glenoid-Measurement-DL-Pipeline.

</details>


### [40] [Error-Driven Scene Editing for 3D Grounding in Large Language Models](https://arxiv.org/abs/2511.14086)
*Yue Zhang,Zun Wang,Han Lin,Jialu Li,Jianing Yang,Yonatan Bitton,Idan Szpektor,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出DEER-3D：用“分解-诊断评估-编辑-再训练”的误差驱动流程，通过最小3D场景编辑生成针对性反事实数据，迭代微调3D-LLM，从而显著提升语言到3D视觉/空间要素的准确对齐。


<details>
  <summary>Details</summary>
Motivation: 现有3D-LLM在将语言精准落地到3D环境中的视觉与空间元素时存在偏差，主要因训练数据更偏语言推理、缺乏高质量3D资源，导致空间理解与谓词级别（属性/位置关系）对齐不足。需要一种低成本且可控的方式生成精确的反事实监督来纠正这些偏差。

Method: 提出以3D场景编辑为核心的数据生成与训练策略：一旦检测到模型在3D指代/落地上的失败，先细化诊断其谓词级错误（如颜色属性、空间关系）。随后执行与该谓词对齐的最小3D编辑（如重上色、微调位置），产生成对的、针对性强的反事实样本。配合DEER-3D流程（Decompose, Diagnostic Evaluation, Edit, Re-train）进行迭代微调，无需昂贵的场景重建或大规模3D数据采集。

Result: 在多个3D指代与场景理解基准上验证，使用该编辑-再训练的迭代流程均带来稳定、持续的性能提升，显著提高空间落地准确率。

Conclusion: 误差驱动、谓词对齐的精细3D编辑能有效弥合3D-LLM的语言推理与空间落地之间的鸿沟；通过有针对性的反事实监督，相比随机或广泛的数据增广更高效、可控，适合作为提升3D grounding能力的通用范式。

Abstract: Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.

</details>


### [41] [GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention](https://arxiv.org/abs/2511.14087)
*Jun Ding,Shang Gao*

Main category: cs.CV

TL;DR: 提出GCA-ResUNet：在ResNet-50残块中注入分组坐标注意力（GCA），以极小开销引入全局依赖建模，提升医学图像分割精度与效率；在Synapse和ACDC数据集上达SOTA级别Dice（86.11%、92.64%）。


<details>
  <summary>Details</summary>
Motivation: U-Net及其CNN变体虽依赖编码器-解码器与跳连取得良好分割，但难以捕获长程依赖；Transformer能建模全局上下文却计算开销大、数据需求高。需要一种在保持轻量高效的同时引入全局建模的方案，以提升边界刻画与特征表达。

Method: 将“分组坐标注意力（GCA）”模块嵌入ResNet-50残差块：通过坐标方向的聚合与跨通道分组建模，同时编码空间与通道的全局依赖；相比自注意力显著降低参数与FLOPs。整体网络为基于ResUNet的编码-解码结构，利用GCA增强多尺度特征与边界信息。

Result: 在Synapse数据集Dice 86.11%，在ACDC数据集Dice 92.64%；超过多种SOTA基线，同时保持快速推理与良好计算效率，参数与FLOP增量很小。

Conclusion: GCA以低成本为卷积架构注入全局建模能力，显著提升医学图像分割准确性与边界描绘，并兼具资源效率；为在资源受限或中等数据规模下的高性能分割提供实用路径。

Abstract: Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capture long-range dependencies. Transformer-based variants address global context but often require heavy computation and large training datasets. This paper proposes GCA-ResUNet, an efficient segmentation network that integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks. GCA uses grouped coordinate modeling to jointly encode global dependencies across channels and spatial locations, strengthening feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared with self-attention. On the Synapse dataset, GCA-ResUNet achieves a Dice score of 86.11%, and on the ACDC dataset, it reaches 92.64%, surpassing several state-of-the-art baselines while maintaining fast inference and favorable computational efficiency. These results indicate that GCA offers a practical way to enhance convolutional architectures with global modeling capability, enabling high-accuracy and resource-efficient medical image segmentation.

</details>


### [42] [SMGeo: Cross-View Object Geo-Localization with Grid-Level Mixture-of-Experts](https://arxiv.org/abs/2511.14093)
*Fan Zhang,Haoyuan Ren,Fei Ma,Qiang Yin,Yongsheng Zhou*

Main category: cs.CV

TL;DR: 提出SMGeo：一个可提示、端到端的Transformer模型，实现无人机到卫星图像的跨视角目标地理定位，实时交互，精度显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨视角检索-匹配范式在视角/尺度差异与复杂背景下易产生累积误差，且锚框设计带来尺度偏置与匹配复杂度；需要端到端、可交互且鲁棒的定位方法。

Method: 全Transformer架构：Swin-Transformer对无人机与卫星图像进行联合特征编码；跨视编码器中引入网格级稀疏MoE（GMoE），根据网格内容、尺度与来源自适应激活专家以建模跨模态与视内依赖；采用无锚检测头，以热力图监督在参考图上直接回归目标坐标；支持点击提示以实现实时交互。

Result: 在无人机到卫星任务上，SMGeo在IoU=0.25准确率与mIoU等指标显著领先：测试集分别为87.51%、62.50%、61.45%，优于DetGeo的61.97%、57.66%、54.05。消融显示共享编码、查询引导融合与GMoE均有互补增益。

Conclusion: SMGeo通过端到端Transformer与GMoE及无锚坐标回归，实现高精度、可交互的跨视角地理定位，显著减少多阶段累积误差并优于代表性方法。

Abstract: Cross-view object Geo-localization aims to precisely pinpoint the same object across large-scale satellite imagery based on drone images. Due to significant differences in viewpoint and scale, coupled with complex background interference, traditional multi-stage "retrieval-matching" pipelines are prone to cumulative errors. To address this, we present SMGeo, a promptable end-to-end transformer-based model for object Geo-localization. This model supports click prompting and can output object Geo-localization in real time when prompted to allow for interactive use. The model employs a fully transformer-based architecture, utilizing a Swin-Transformer for joint feature encoding of both drone and satellite imagery and an anchor-free transformer detection head for coordinate regression. In order to better capture both inter-modal and intra-view dependencies, we introduce a grid-level sparse Mixture-of-Experts (GMoE) into the cross-view encoder, allowing it to adaptively activate specialized experts according to the content, scale and source of each grid. We also employ an anchor-free detection head for coordinate regression, directly predicting object locations via heat-map supervision in the reference images. This approach avoids scale bias and matching complexity introduced by predefined anchor boxes. On the drone-to-satellite task, SMGeo achieves leading performance in accuracy at IoU=0.25 and mIoU metrics (e.g., 87.51%, 62.50%, and 61.45% in the test set, respectively), significantly outperforming representative methods such as DetGeo (61.97%, 57.66%, and 54.05%, respectively). Ablation studies demonstrate complementary gains from shared encoding, query-guided fusion, and grid-level sparse mixture-of-experts.

</details>


### [43] [BCE3S: Binary Cross-Entropy Based Tripartite Synergistic Learning for Long-tailed Recognition](https://arxiv.org/abs/2511.14097)
*Weijia Fan,Qiufu Li,Jiajun Wen,Xiaoyang Peng*

Main category: cs.CV

TL;DR: 提出BCE3S：用BCE替代CE，并以三元协同学习（联合学习、对比学习、均匀学习）提升长尾识别中特征紧致性、可分性与分类器均衡可分性，在多数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: CE在长尾下会：1) 难以学到头尾类都紧致且可分的特征；2) Softmax分母耦合不均衡的分类器向量，放大类别不平衡。需要一种既提升特征性质又平衡分类器可分性的训练范式。

Method: 基于BCE构建三元协同学习框架：1) BCE联合学习（多Sigmoid）同时优化分类器与特征，解耦特征与不均衡分类器的度量，优于CE联合学习；2) BCE对比学习强化类内紧致；3) BCE均匀学习使分类器向量在角度上均匀分布，并与联合学习交互促进特征质量。

Result: 在CIFAR10-LT、CIFAR100-LT、ImageNet-LT、iNaturalist2018上取得SOTA；训练后特征更紧致、类间更可分，分类器向量分离度更均衡。

Conclusion: 用BCE替代CE并通过三元协同（联合/对比/均匀）可同时缓解特征与分类器层面的长尾不均衡问题，实现更高的识别性能与更稳健的分类器几何结构。

Abstract: For long-tailed recognition (LTR) tasks, high intra-class compactness and inter-class separability in both head and tail classes, as well as balanced separability among all the classifier vectors, are preferred. The existing LTR methods based on cross-entropy (CE) loss not only struggle to learn features with desirable properties but also couple imbalanced classifier vectors in the denominator of its Softmax, amplifying the imbalance effects in LTR. In this paper, for the LTR, we propose a binary cross-entropy (BCE)-based tripartite synergistic learning, termed BCE3S, which consists of three components: (1) BCE-based joint learning optimizes both the classifier and sample features, which achieves better compactness and separability among features than the CE-based joint learning, by decoupling the metrics between feature and the imbalanced classifier vectors in multiple Sigmoid; (2) BCE-based contrastive learning further improves the intra-class compactness of features; (3) BCE-based uniform learning balances the separability among classifier vectors and interactively enhances the feature properties by combining with the joint learning. The extensive experiments show that the LTR model trained by BCE3S not only achieves higher compactness and separability among sample features, but also balances the classifier's separability, achieving SOTA performance on various long-tailed datasets such as CIFAR10-LT, CIFAR100-LT, ImageNet-LT, and iNaturalist2018.

</details>


### [44] [FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration](https://arxiv.org/abs/2511.14099)
*Jingren Liu,Shuning Xu,Qirui Yang,Yun Wang,Xiangyu Chen,Zhong Ji*

Main category: cs.CV

TL;DR: 提出FAPE-IR：用冻结MLLM做“规划”，用频率感知的扩散执行器+LoRA-MoE做“执行”，通过选择高/低频专家、对抗训练和频率正则，实现统一、可解释的多退化图像恢复并达SOTA与零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有AIO-IR方法依赖任务特定设计或隐式路由，难以应对真实世界中多种、混合退化且缺乏可解释性与泛化。需要一种统一且可适配复杂退化的方案。

Method: - 频率感知“规划-执行”框架FAPE-IR。
- 规划器：冻结的多模态大语言模型分析退化图像，生成简洁、带频率提示的修复计划。
- 执行器：扩散模型内嵌LoRA-MoE模块，按计划与输入的频率特征动态选择高频/低频专家；结合对抗训练与频率正则以抑制伪影并提升细节。


Result: 在七个图像恢复任务上实现SOTA；在混合退化的零样本设置中表现强泛化与稳健性，同时提供可解释的路由/计划。

Conclusion: 将语义规划与频率域执行耦合，形成统一、可解释且可泛化的AIO-IR方案；频率感知的MoE选择与正则/对抗训练共同提升质量并减少伪影，适用于复杂真实场景。

Abstract: All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.

</details>


### [45] [Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations](https://arxiv.org/abs/2511.14100)
*Yiqing Shen,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出“推理视频编辑”任务与模型RIVER：先用数字孪生表征视频，再由LLM多跳推理生成结构化编辑指令，最后扩散模型执行像素级修改；通过强化学习联合优化推理与生成；并发布RVEBenchmark，表现优于多基线与现有数据集。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动视频编辑依赖用户提供明确的目标、精确空间与时间边界；当查询是隐含的（基于语义属性、关系或跨时推理）时就不实用。需要一种能理解隐式查询并定位编辑目标的体系。

Method: 1) 将“推理”与“生成”解耦：以数字孪生方式抽取并编码视频的空间关系、时间轨迹、语义属性；2) 用大型语言模型将该结构化表征与隐式文本查询联合输入，进行多跳推理以确定编辑对象与区域/时段，产出结构化编辑指令；3) 以扩散式编辑器按指令执行像素级改动；4) 训练采用强化学习，奖励包括推理准确性与生成质量；5) 提出RVEBenchmark用于评测多层级、多类别的隐式推理编辑能力。

Result: RIVER在自建RVEBenchmark上取得最佳成绩，并在VegGIE与FiVE两套视频编辑基准上超越6个基线方法，达到SOTA表现。

Conclusion: 通过数字孪生表征与LLM多跳推理相结合，再配合扩散编辑与强化学习，RIVER有效解决隐式查询下的推理视频编辑任务，并在多个基准上验证其优越性。

Abstract: Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.

</details>


### [46] [RTS-Mono: A Real-Time Self-Supervised Monocular Depth Estimation Method for Real-World Deployment](https://arxiv.org/abs/2511.14107)
*Zeyu Cheng,Tongfei Liu,Tao Lei,Xiang Hua,Yi Zhang,Chengkai Tang*

Main category: cs.CV

TL;DR: 提出RTS-Mono：一款3M参数、可实时运行的自监督单目深度估计网络，在KITTI上以更低计算量达SOTA，并在Jetson Orin上49FPS。


<details>
  <summary>Details</summary>
Motivation: 现有自监督单目深度估计算法虽灵活易用，但普遍计算开销大；即便压缩模型也常明显降性能，限制了在自动驾驶与机器人中的真实部署。需要一种既轻量高效又保持精度、可实时部署的方法。

Method: 设计轻量级编码器-解码器架构RTS-Mono：编码器采用Lite-Encoder；解码器采用多尺度稀疏融合（multi-scale sparse fusion）以减少冗余、保持多尺度信息与提升推理速度；整体为自监督训练框架（基于视角合成/重投影损失的范式，文摘未细述），目标是在低/高分辨率下均实现高效高精度。

Result: 在KITTI上，以极低参数量（约3M）取得SOTA：相较轻量方法，低分辨率下Abs Rel与Sq Rel分别提升5.6%与9.8%；高分辨率下Sq Rel与RMSE分别提升6.1%与1.9%。实际部署于Nvidia Jetson Orin上达到49 FPS且精度“极高”。

Conclusion: RTS-Mono在保持极小模型规模与高推理速度的同时，实现了领先的自监督单目深度估计精度，验证了其在真实环境中的可部署性；代码已开源，有望推动自动驾驶与机器人端侧实时深度感知应用。

Abstract: Depth information is crucial for autonomous driving and intelligent robot navigation. The simplicity and flexibility of self-supervised monocular depth estimation are conducive to its role in these fields. However, most existing monocular depth estimation models consume many computing resources. Although some methods have reduced the model's size and improved computing efficiency, the performance deteriorates, seriously hindering the real-world deployment of self-supervised monocular depth estimation models in the real world. To address this problem, we proposed a real-time self-supervised monocular depth estimation method and implemented it in the real world. It is called RTS-Mono, which is a lightweight and efficient encoder-decoder architecture. The encoder is based on Lite-Encoder, and the decoder is designed with a multi-scale sparse fusion framework to minimize redundancy, ensure performance, and improve inference speed. RTS-Mono achieved state-of-the-art (SoTA) performance in high and low resolutions with extremely low parameter counts (3 M) in experiments based on the KITTI dataset. Compared with lightweight methods, RTS-Mono improved Abs Rel and Sq Rel by 5.6% and 9.8% at low resolution and improved Sq Rel and RMSE by 6.1% and 1.9% at high resolution. In real-world deployment experiments, RTS-Mono has extremely high accuracy and can perform real-time inference on Nvidia Jetson Orin at a speed of 49 FPS. Source code is available at https://github.com/ZYCheng777/RTS-Mono.

</details>


### [47] [$A^2$GC: $A$symmetric $A$ggregation with Geometric Constraints for Locally Aggregated Descriptors](https://arxiv.org/abs/2511.14109)
*Zhenyu Li,Tianyi Shang*

Main category: cs.CV

TL;DR: 论文提出A^2GC-VPR：一种用于视觉地点识别的非对称最优传输聚合方法，并结合几何约束以提升全局描述子的匹配精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统VPR将深度特征聚合为全局描述子。基于最优传输的聚合常用Sinkhorn算法，但其对源/目标边缘分布的对称处理在图像特征与聚类中心分布差异较大时效果受限，需要能处理分布不匹配并具备空间感知的聚合方法。

Method: 提出A^2GC-VPR：1) 用行列归一化的平均与独立边缘校准实现非对称匹配，适应特征与聚类中心分布差异；2) 通过可学习坐标嵌入引入几何约束，将空间兼容度与特征相似度融合，鼓励空间上相近的局部特征聚到同一簇，从而构建空间感知的局部聚合描述子。

Result: 在MSLS、NordLand、Pittsburgh三个数据集上获得优于现有方法的性能，表明匹配准确性与鲁棒性提升。

Conclusion: 非对称最优传输聚合结合几何坐标嵌入可更好地处理特征-簇分布差异并增强空间意识，从而提升VPR的匹配表现。

Abstract: Visual Place Recognition (VPR) aims to match query images against a database using visual cues. State-of-the-art methods aggregate features from deep backbones to form global descriptors. Optimal transport-based aggregation methods reformulate feature-to-cluster assignment as a transport problem, but the standard Sinkhorn algorithm symmetrically treats source and target marginals, limiting effectiveness when image features and cluster centers exhibit substantially different distributions. We propose an asymmetric aggregation VPR method with geometric constraints for locally aggregated descriptors, called $A^2$GC-VPR. Our method employs row-column normalization averaging with separate marginal calibration, enabling asymmetric matching that adapts to distributional discrepancies in visual place recognition. Geometric constraints are incorporated through learnable coordinate embeddings, computing compatibility scores fused with feature similarities, thereby promoting spatially proximal features to the same cluster and enhancing spatial awareness. Experimental results on MSLS, NordLand, and Pittsburgh datasets demonstrate superior performance, validating the effectiveness of our approach in improving matching accuracy and robustness.

</details>


### [48] [CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer](https://arxiv.org/abs/2511.14111)
*Srivathsan Sivakumar,Faisal Z. Qureshi*

Main category: cs.CV

TL;DR: 提出Cascaded‑ViT（CViT）与新型前馈层CCFFN，通过特征分块实现更高参数与FLOP效率；在ImageNet‑1K上，在保持或提升精度的同时显著降低FLOPs与能耗，并在新指标APF上表现领先，适合电池受限设备部署。


<details>
  <summary>Details</summary>
Motivation: ViT性能强但计算、内存与能耗高，限制在移动、无人机等资源受限平台的部署；需要在不明显牺牲精度的前提下降低FLOPs与能耗，并给出更公平的效率度量。

Method: 设计Cascaded‑ViT架构及其核心CCFFN：将输入特征分块（chunk）并级联处理，以减少冗余计算与参数；评估多个模型规模，在ImageNet‑1K上与EfficientViT等比较；提出Accuracy‑Per‑FLOP（APF）指标衡量精度相对计算效率。

Result: CViT‑XL在ImageNet‑1K上Top‑1=75.5%，相较EfficientViT‑M5减少15% FLOPs与3.3%能耗；整个CViT族在不同规模下能耗最低；在APF指标上名列前茅；CViT‑L比EfficientViT‑M2高2.2%精度且APF相当。

Conclusion: CCFFN驱动的CViT在保持/提升精度的同时显著提高计算与能耗效率，适合部署在电池受限设备；APF提供了评估计算效率与精度折中更合适的度量。

Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.

</details>


### [49] [Coffee: Controllable Diffusion Fine-tuning](https://arxiv.org/abs/2511.14113)
*Ziyao Zeng,Jingcheng Ni,Ruyi Liu,Alex Wong*

Main category: cs.CV

TL;DR: 提出Coffee：一种在微调文本到图像扩散模型时，用语言指定“应避免学习”的概念，并在不额外训练的情况下抑制这些概念被吸收的方法。核心做法是让用户提示的嵌入与不希望的概念保持不对齐，从而在适配中避免概念纠缠与不良迁移。实验显示对比现有方法更能阻止模型学到被排除的概念。


<details>
  <summary>Details</summary>
Motivation: 个性化小样本微调常会把数据中的不良/偏见/敏感概念一并学进模型，导致提示-概念纠缠、恶意适配、偏见放大及差的泛化。需要一种可控微调手段：既能学到用户目标，又能显式“排斥”某些不希望学到的概念，且应当灵活、低成本、可用自然语言指定。

Method: 提出Coffee：在微调时用文本描述指定不希望的概念，构造其文本嵌入；通过正则化使用户提示的嵌入与这些“禁学”概念嵌入不对齐（例如添加角度/相似度惩罚或投影去相关），从而在参数适配期间抑制模型朝这些概念更新。方法无需额外训练阶段，仅通过在微调过程中加入该对齐抑制项；可随时以文本修改不希望的概念列表。

Result: 在与用户提示配对且包含不希望概念的训练图像上进行微调评测，Coffee有效阻止模型在适配中吸收这些概念；生成结果对禁学概念的表达显著减少，同时保持任务相关质量，并优于现有对照方法。

Conclusion: Coffee通过语言可编程的负向概念正则，实现在扩散模型微调中防止不希望概念的学习与纠缠，无需额外训练且概念配置灵活，对偏见缓解、恶意适配防护、属性解纠缠及可泛化微调具有实用价值。

Abstract: Text-to-image diffusion models can generate diverse content with flexible prompts, which makes them well-suited for customization through fine-tuning with a small amount of user-provided data. However, controllable fine-tuning that prevents models from learning undesired concepts present in the fine-tuning data, and from entangling those concepts with user prompts, remains an open challenge. It is crucial for downstream tasks like bias mitigation, preventing malicious adaptation, attribute disentanglement, and generalizable fine-tuning of diffusion policy. We propose Coffee that allows using language to specify undesired concepts to regularize the adaptation process. The crux of our method lies in keeping the embeddings of the user prompt from aligning with undesired concepts. Crucially, Coffee requires no additional training and enables flexible modification of undesired concepts by modifying textual descriptions. We evaluate Coffee by fine-tuning on images associated with user prompts paired with undesired concepts. Experimental results demonstrate that Coffee can prevent text-to-image models from learning specified undesired concepts during fine-tuning and outperforms existing methods. Code will be released upon acceptance.

</details>


### [50] [Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models](https://arxiv.org/abs/2511.14120)
*Hao Zhen,Yunxiang Yang,Jidong J. Yang*

Main category: cs.CV

TL;DR: 提出MP-PVIR框架，将多视角行人-车辆事故视频转化为结构化诊断报告，包含事件触发采集、行为相位分割、相位内多视角推理与层级综合，利用两个专用VLM与一个LLM实现因果链与预防建议生成，在Woven数据集上取得较好分割、描述与问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频系统能检测事故但缺乏对行人行为从认知相位角度的细粒度解析；主流VLM缺少显式时间结构化与多视角整合能力，难以提供可行动的交通安全洞见。

Method: 提出MP-PVIR四阶段统一流程：1) 事件触发的多视角视频获取；2) TG-VLM进行行人行为相位分割；3) PhaVR-VLM在各相位内做多视角协同推理（生成描述与问答）；4) 使用层级综合与因果推理生成诊断报告与预防策略。

Result: TG-VLM在相位分割上mIoU=0.4881；PhaVR-VLM在描述任务得分33.063，问答准确率最高64.70%；整体在Woven Traffic Safety数据集上将多视角视频有效转化为可执行洞见。

Conclusion: MP-PVIR通过相位感知与多视角整合，将视频理解与行为理论结合，生成因果链和预防建议，推进车路协同的AI交通安全分析。

Abstract: Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.

</details>


### [51] [Attention Via Convolutional Nearest Neighbors](https://arxiv.org/abs/2511.14137)
*Mingi Kang,Jeová Farias Sales Rocha Neto*

Main category: cs.CV

TL;DR: 论文提出ConvNN，将卷积与自注意力统一为“k近邻聚合”的同一框架，既可按空间邻近（卷积）也可按特征相似（注意力）选邻并聚合，在VGG与ViT上验证优于基线并显示介于二者之间的插值具有正则化效应。


<details>
  <summary>Details</summary>
Motivation: 视觉领域从CNN转向Transformer，但两者常被视为本质不同。作者观察到二者都在做“邻居选择+聚合”，区别只在邻居选择准则（空间 vs 特征），因此希望提出一个统一、可替换两者的层，系统探索二者之间的连续谱并获得更可解释、可设计的架构。

Method: 提出Convolutional Nearest Neighbors (ConvNN)：以k近邻为核心，统一邻居选择（基于位置或特征相似或其混合）与聚合（加权求和等）。ConvNN可作为卷积层或注意力层的即插即用替代，在架构上提供：1）在VGG中以分支/混合方式结合空间近邻与特征近邻；2）在ViT中用ConvNN替换标准注意力。并进行了k值与架构变体的广泛消融，探索沿“卷积-注意力”谱的插值。

Result: 在CIFAR-10/100上：1）VGG中的混合分支（空间+特征）均提高准确率；2）在ViT中，ConvNN优于标准注意力和其他注意力变体。消融显示调整k与混合比例带来性能与稳定性提升。

Conclusion: 卷积与自注意力可被视为同一k近邻聚合机制的两个端点。沿二者之间连续谱进行插值可带来正则化效益，兼顾局部与全局感受野，有助于更系统、更可解释地设计视觉网络。

Abstract: The shift from Convolutional Neural Networks to Transformers has reshaped computer vision, yet these two architectural families are typically viewed as fundamentally distinct. We argue that convolution and self-attention, despite their apparent differences, can be unified within a single k-nearest neighbor aggregation framework. The critical insight is that both operations are special cases of neighbor selection and aggregation; convolution selects neighbors by spatial proximity, while attention selects by feature similarity, revealing they exist on a continuous spectrum. We introduce Convolutional Nearest Neighbors (ConvNN), a unified framework that formalizes this connection. Crucially, ConvNN serves as a drop-in replacement for convolutional and attention layers, enabling systematic exploration of the intermediate spectrum between these two extremes. We validate the framework's coherence on CIFAR-10 and CIFAR-100 classification tasks across two complementary architectures: (1) Hybrid branching in VGG improves accuracy on both CIFAR datasets by combining spatial-proximity and feature-similarity selection; and (2) ConvNN in ViT outperforms standard attention and other attention variants on both datasets. Extensive ablations on $k$ values and architectural variants reveal that interpolating along this spectrum provides regularization benefits by balancing local and global receptive fields. Our work provides a unifying framework that dissolves the apparent distinction between convolution and attention, with implications for designing more principled and interpretable vision architectures.

</details>


### [52] [SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM](https://arxiv.org/abs/2511.14143)
*An Yu,Weiheng Lu,Jian Li,Zhenfei Zhang,Yunhang Shen,Felix X. -F. Ye,Ming-Ching Chang*

Main category: cs.CV

TL;DR: 提出SMART：基于MLLM的镜头感知多模态（含音频）视频片段检索方法，通过镜头级token压缩与改良提示，提升细粒度时间定位， 在Charades-STA与QVHighlights上超过SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频时刻检索多依赖粗粒度时间理解与单一视觉模态，难以处理复杂视频的细节与跨模态线索；需要同时利用音视频信息并显式建模镜头级时间结构，减少冗余同时保留关键细节。

Method: 构建SMART框架：1）多模态融合，结合视觉与音频特征，输入MLLM；2）镜头感知的Token压缩，在每个镜头内选择高信息token以降冗并保留细粒度时间线索；3）提示词设计优化，引导模型有效利用视听线索进行时刻定位。

Result: 在Charades-STA与QVHighlights上获得显著提升：以Charades-STA为例，R1@0.5提升1.61%，R1@0.7提升2.59%，整体优于现有SOTA方法。

Conclusion: 利用镜头级结构与音频增强的多模态表示，并配合高效token压缩与提示优化，可显著提升MLLM在视频时刻检索中的精细时间定位能力。

Abstract: Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\% increase in R1@0.5 and 2.59\% gain in R1@0.7 on Charades-STA.

</details>


### [53] [iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion](https://arxiv.org/abs/2511.14149)
*Hao Wang,Linqing Zhao,Xiuwei Xu,Jiwen Lu,Haibin Yan*

Main category: cs.CV

TL;DR: 提出iGaussian：一种无需可微渲染、两阶段前馈的单图像相机位姿估计方法，利用3D高斯场景先验、跨模态相关与多视融合，实现0.2°中位旋转误差与2.87 FPS，较优化法提速10倍。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian已成主流场景表示，但现有单视图位姿估计依赖“渲染-比较-迭代优化”的多轮流程，计算开销大、难以满足机器人实时性需求。需要一种摆脱可微渲染、直接从高斯模型快速反演位姿的方案。

Method: 两阶段前馈框架：1) 粗配准：基于“Gaussian Scene Prior”的位姿回归网络，结合空间均匀采样与引导注意力，直接回归6DoF。2) 细化：通过特征匹配与多模型融合进一步优化。核心模块为跨相关（将图像嵌入与3D高斯属性对齐）与加权多视预测器（从策略采样的多视点特征融合）。全流程避免可微渲染。

Result: 在NeRF Synthetic、Mip-NeRF 360、T&T+DB上显著优于现有方法：中位旋转误差达0.2°；在移动机器人上追踪速度2.87 FPS，较基于优化的方法提速约10倍。

Conclusion: 直接3D高斯反演的前馈框架可在保持高精度的同时显著提升实时性；跨相关与加权多视融合是关键。方法为基于高斯表示的视觉导航/SLAM提供了高效单幅位姿估计方案，代码已开源。

Abstract: Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian

</details>


### [54] [Wave-Former: Through-Occlusion 3D Reconstruction via Wireless Shape Completion](https://arxiv.org/abs/2511.14152)
*Laura Dodds,Maisy Lam,Waleed Akbar,Yibo Cheng,Fadel Adib*

Main category: cs.CV

TL;DR: Wave-Former利用可穿透遮挡的毫米波信号，结合物理先验与Transformer形状补全，在全遮挡场景实现高精度3D重建；以三阶段流程从无线原始信号到完整几何，显著提升召回率并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波重建方法覆盖有限、噪声大，难以对被完全遮挡的日常物体实现准确完整的3D形状重建；而在机器人、AR、物流等场景，对隐藏物体的几何恢复有迫切需求。

Method: 提出“物理感知”的三阶段管线：1) 从毫米波原始信号提出候选几何表面（融入毫米波传播与反射物理）；2) 设计面向毫米波特性的Transformer形状补全网络，推断完整3D几何；3) 采用基于熵的表面选择策略，过滤噪声与歧义。训练全部基于合成点云，但可良好泛化到真实数据。

Result: 在与SOTA基线对比中，召回率由54%提升至72%，同时保持85%的高精度，显示在完全遮挡目标的3D重建任务上有显著优势。

Conclusion: 将毫米波物理先验与深度形状补全有效结合，可在全遮挡场景实现高准确度的3D重建；三阶段设计与熵引导选择提升了鲁棒性与泛化，支持从纯合成数据训练迁移到真实世界应用。

Abstract: We present Wave-Former, a novel method capable of high-accuracy 3D shape reconstruction for completely occluded, diverse, everyday objects. This capability can open new applications spanning robotics, augmented reality, and logistics. Our approach leverages millimeter-wave (mmWave) wireless signals, which can penetrate common occlusions and reflect off hidden objects. In contrast to past mmWave reconstruction methods, which suffer from limited coverage and high noise, Wave-Former introduces a physics-aware shape completion model capable of inferring full 3D geometry. At the heart of Wave-Former's design is a novel three-stage pipeline which bridges raw wireless signals with recent advancements in vision-based shape completion by incorporating physical properties of mmWave signals. The pipeline proposes candidate geometric surfaces, employs a transformer-based shape completion model designed specifically for mmWave signals, and finally performs entropy-guided surface selection. This enables Wave-Former to be trained using entirely synthetic point-clouds, while demonstrating impressive generalization to real-world data.In head-to-head comparisons with state-of-the-art baselines, Wave-Former raises recall from 54% to 72% while maintaining a high precision of 85%.

</details>


### [55] [Learning Representation and Synergy Invariances: A Povable Framework for Generalized Multimodal Face Anti-Spoofing](https://arxiv.org/abs/2511.14157)
*Xun Lin,Shuai Wang,Yi Yu,Zitong Yu,Jiale Zhou,Yizhong Liu,Xiaochun Cao,Alex Kot,Yefeng Zheng*

Main category: cs.CV

TL;DR: 跨域多模态人脸活体检测易退化，因“表示不变风险”和“协同不变风险”。本文提出RiSe框架，通过AsyIRM学习径向不变、角向保留域信息的决策边界，及MMSD自监督解耦/混合增强泛化协同，达成SOTA跨域性能。


<details>
  <summary>Details</summary>
Motivation: 多模态FAS在未见域比单模态更脆弱。根因被忽视：1) 类别不对称（伪造多样、真实紧致）在域移下放大泛化误差，且多模态更严重；2) 模态间协同常是域特异的伪相关，迁移失败。

Method: 提出RiSe：- 表示风险：Asymmetric Invariant Risk Minimization (AsyIRM)，在径向空间学习球面不变决策边界以匹配不对称分布，并在角向保留域线索；- 协同风险：Multimodal Synergy Disentanglement (MMSD)，通过跨样本混合和解耦的自监督任务，提升模态内在、可迁移特征，削弱域特异协同。并给出理论上界与可证性质。

Result: 理论推导展示类别不对称与多模态会放大泛化误差上界；RiSe在多个跨域基准上实现SOTA，显著优于现有多/单模态FAS；消融表明AsyIRM与MMSD均贡献显著。

Conclusion: 跨域退化源于表示与协同不变性风险。RiSe通过不对称不变风险最小化与协同解耦，缓解伪相关与类不对称影响，带来稳健的多模态跨域FAS泛化。

Abstract: Multimodal Face Anti-Spoofing (FAS) methods, which integrate multiple visual modalities, often suffer even more severe performance degradation than unimodal FAS when deployed in unseen domains. This is mainly due to two overlooked risks that affect cross-domain multimodal generalization. The first is the modal representation invariant risk, i.e., whether representations remain generalizable under domain shift. We theoretically show that the inherent class asymmetry in FAS (diverse spoofs vs. compact reals) enlarges the upper bound of generalization error, and this effect is further amplified in multimodal settings. The second is the modal synergy invariant risk, where models overfit to domain-specific inter-modal correlations. Such spurious synergy cannot generalize to unseen attacks in target domains, leading to performance drops. To solve these issues, we propose a provable framework, namely Multimodal Representation and Synergy Invariance Learning (RiSe). For representation risk, RiSe introduces Asymmetric Invariant Risk Minimization (AsyIRM), which learns an invariant spherical decision boundary in radial space to fit asymmetric distributions, while preserving domain cues in angular space. For synergy risk, RiSe employs Multimodal Synergy Disentanglement (MMSD), a self-supervised task enhancing intrinsic, generalizable modal features via cross-sample mixing and disentanglement. Theoretical analysis and experiments verify RiSe, which achieves state-of-the-art cross-domain performance.

</details>


### [56] [MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs](https://arxiv.org/abs/2511.14159)
*Huiyi Chen,Jiawei Peng,Dehai Min,Changchang Sun,Kaijie Chen,Yan Yan,Xu Yang,Lu Cheng*

Main category: cs.CV

TL;DR: 提出MVI-Bench，一个专门评估大型视觉语言模型在“误导性视觉输入”下鲁棒性的基准；构建三层次误导类型与6类数据、1248条VQA实例，并引入细粒度鲁棒性指标MVI-Sensitivity；18个SOTA LVLM在该基准上暴露显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒性评测多关注幻觉或误导性文本输入，忽视同样关键的误导性视觉输入对视觉理解的影响；缺乏系统化、可量化的基准来诊断LVLM在视觉层面被误导的弱点。

Method: 1) 提出基于基础视觉要素的三层次误导输入 taxonomy：视觉概念、视觉属性、视觉关系；2) 据此策划6个代表性类别，人工精标1248个VQA样例；3) 设计细粒度鲁棒性指标MVI-Sensitivity，用于度量模型在不同误导维度/实例上的敏感度；4) 在18个主流LVLM上进行系统实验与分析。

Result: 在MVI-Bench上，18个SOTA LVLM均表现出对误导性视觉输入的显著脆弱性；MVI-Sensitivity揭示了鲁棒性在不同层级与类别上的细粒度差异；提供了可操作的分析结果与诊断。

Conclusion: 误导性视觉输入是评估与提升LVLM鲁棒性的关键维度；MVI-Bench与MVI-Sensitivity为社区提供了系统化、可复现的测试与度量工具，可指导更可靠、更稳健的LVLM设计与训练。

Abstract: Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.

</details>


### [57] [AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs](https://arxiv.org/abs/2511.14169)
*Xinliang Zhang,Lei Zhu,Hangzhou He,Shuang Zeng,Ourui Fu,Jiakui Hu,Zhengjian Yao,Yanye Lu*

Main category: cs.CV

TL;DR: 提出一种面向MLLM的“对象级”自适应Token压缩策略，替代传统补丁级（patch）扫描，显著减少图像Token（约用10%）同时保留约96%的性能。


<details>
  <summary>Details</summary>
Motivation: 补丁级切分会导致图像Token数量随分辨率二次增长，造成算力与内存巨大开销；并且与人类视觉的对象中心加工不一致，易产生幻觉与冗余计算。需要一种更符合人类感知、能在压缩与性能间取得更好平衡的机制。

Method: 提出对象级的Token合并（merging）策略：基于检测/分割或语义聚合，将图像表示从patch级提升到object级，按重要性自适应地合并与保留Token，实现Adaptive Token Compression，以减少无关区域和冗余patch。

Result: 在多项综合基准上，仅使用约10%的图像Token即可达到原始模型约96%的性能；与相关工作相比，在相同或更高压缩率下取得更优的效果。

Conclusion: 对象级自适应Token压缩与人类视觉一致，可在显著降低计算与内存成本的同时，基本保持MLLM性能，并有效缓解幻觉与冗余问题；方法通用且优于现有压缩方案，代码将开源。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.

</details>


### [58] [DoGCLR: Dominance-Game Contrastive Learning Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2511.14179)
*Yanshan Li,Ke Ma,Miaomiao Wei,Linhui Dai*

Main category: cs.CV

TL;DR: 提出DoGCLR：基于“支配博弈”的自监督对比学习，用区域加权增强与熵驱动负样本管理，显著提升骨架动作识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自监督骨架对比学习：1）对各骨架区域一视同仁，增强易破坏关键运动语义；2）负样本采用FIFO队列，易保留信息弱样本并丢弃有价值的“困难”负样本，导致对比信号不充分。

Method: 将正负样本构造建模为动态支配博弈：在保持语义与提升判别力间寻优。核心包括：1）时空双权定位，识别关键运动关节/帧并进行区域化增强，提升运动多样性且保持语义；2）熵驱动支配策略管理记忆库，保留高熵（困难）负样本、替换低熵（弱）负样本，持续提供信息量大的对比信号。

Result: 在NTU RGB+D 60 X-Sub/X-View达81.1%/89.4%，在NTU RGB+D 120 X-Sub/X-Set达71.2%/75.5%，分别超SOTA 0.1%、2.7%、1.1%、2.3%；在PKU-MMD Part I/II与SOTA相当，并在Part II提升1.9%。

Conclusion: 支配博弈视角结合时空区域化增强与熵驱动记忆库更新，有效缓解运动信息丢失与非最优负样本选择，提升了自监督骨架动作识别的鲁棒性与泛化。

Abstract: Existing self-supervised contrastive learning methods for skeleton-based action recognition often process all skeleton regions uniformly, and adopt a first-in-first-out (FIFO) queue to store negative samples, which leads to motion information loss and non-optimal negative sample selection. To address these challenges, this paper proposes Dominance-Game Contrastive Learning network for skeleton-based action Recognition (DoGCLR), a self-supervised framework based on game theory. DoGCLR models the construction of positive and negative samples as a dynamic Dominance Game, where both sample types interact to reach an equilibrium that balances semantic preservation and discriminative strength. Specifically, a spatio-temporal dual weight localization mechanism identifies key motion regions and guides region-wise augmentations to enhance motion diversity while maintaining semantics. In parallel, an entropy-driven dominance strategy manages the memory bank by retaining high entropy (hard) negatives and replacing low-entropy (weak) ones, ensuring consistent exposure to informative contrastive signals. Extensive experiments are conducted on NTU RGB+D and PKU-MMD datasets. On NTU RGB+D 60 X-Sub/X-View, DoGCLR achieves 81.1%/89.4% accuracy, and on NTU RGB+D 120 X-Sub/X-Set, DoGCLR achieves 71.2%/75.5% accuracy, surpassing state-of-the-art methods by 0.1%, 2.7%, 1.1%, and 2.3%, respectively. On PKU-MMD Part I/Part II, DoGCLR performs comparably to the state-of-the-art methods and achieves a 1.9% higher accuracy on Part II, highlighting its strong robustness on more challenging scenarios.

</details>


### [59] [UniSER: A Foundation Model for Unified Soft Effects Removal](https://arxiv.org/abs/2511.14183)
*Jingdong Zhang,Lingzhi Zhang,Qing Liu,Mang Tik Chiu,Connelly Barnes,Yizhou Wang,Haoran You,Xiaoyang Liu,Yuqian Zhou,Zhe Lin,Eli Shechtman,Sohrab Amirghodsi,Xin Li,Wenping Wang,Xiaohang Zhan*

Main category: cs.CV

TL;DR: 提出UniSER，一种统一的软效应图像复原模型，利用3.8M配对数据和Diffusion Transformer微调，结合掩膜与强度控制，在去眩光、雾霾、阴影、反射等任务上超越专才与通才模型。


<details>
  <summary>Details</summary>
Motivation: 现实图像常被半透明软效应（镜头炫光、雾、阴影、反射）降低观感；现有方法各自为政，缺乏可扩展性与共享先验，通才大模型虽强但在细粒度去除与身份保持上不稳且需繁琐提示。需要一个能够统一处理多种软效应、稳健且易控的复原框架。

Method: 将软效应统一视为半透明遮挡问题：1) 构建3.8M对的大规模合成与物理可解释数据集，补齐公共基准的缺口；2) 针对Diffusion Transformer设计训练流程，在多样数据上微调，学习通用复原先验；3) 引入细粒度的掩膜与强度控制接口，实现可控编辑与保真复原。

Result: UniSER在野外多种软效应场景中实现稳健、高保真的复原表现，显著优于专用方法与通用大模型（如需复杂提示的GPT-4o等），在去除强度、鲁棒性与身份保持上均有提升。

Conclusion: 通过统一建模软效应为半透明遮挡并结合大规模数据与扩散Transformer微调，UniSER成为通用且可控的基础型复原模型，在多种退化类型上实现领先性能与更强泛化。

Abstract: Digital images are often degraded by soft effects such as lens flare, haze, shadows, and reflections, which reduce aesthetics even though the underlying pixels remain partially visible. The prevailing works address these degradations in isolation, developing highly specialized, specialist models that lack scalability and fail to exploit the shared underlying essences of these restoration problems. While specialist models are limited, recent large-scale pretrained generalist models offer powerful, text-driven image editing capabilities. while recent general-purpose systems (e.g., GPT-4o, Flux Kontext, Nano Banana) require detailed prompts and often fail to achieve robust removal on these fine-grained tasks or preserve identity of the scene. Leveraging the common essence of soft effects, i.e., semi-transparent occlusions, we introduce a foundational versatile model UniSER, capable of addressing diverse degradations caused by soft effects within a single framework. Our methodology centers on curating a massive 3.8M-pair dataset to ensure robustness and generalization, which includes novel, physically-plausible data to fill critical gaps in public benchmarks, and a tailored training pipeline that fine-tunes a Diffusion Transformer to learn robust restoration priors from this diverse data, integrating fine-grained mask and strength controls. This synergistic approach allows UniSER to significantly outperform both specialist and generalist models, achieving robust, high-fidelity restoration in the wild.

</details>


### [60] [GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation](https://arxiv.org/abs/2511.14184)
*Xuan Zhao,Zhongyu Zhang,Yuge Huang,Yuxi Mi,Guodong Mu,Shouhong Ding,Jun Wang,Rizen Guo,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 提出GloTok：用全局关系学习使图像离散表示的语义分布更均匀，并配合残差细节恢复，从而在无需依赖预训练模型的情况下提升AR生成与重建，在ImageNet-1k达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像tokenizer借助预训练视觉模型进行“局部”语义监督，虽能扩展潜在分布但语义分布不够均匀；而VA-VAE显示更均匀的特征分布有助于生成质量。因此需要一种能从全局数据层面传递语义、获得更均匀分布的tokenization方法。

Method: 1) 全局视角的语义迁移：提出“按码本维度的直方图关系学习”，将预训练模型在整个数据集上建模的语义关系转移到离散码本，使token特征在语义上更均匀分布；2) 残差学习模块：在量化后用残差支路恢复细粒度细节，降低重建误差；3) 由此得到的语义潜表示更适配AR训练，且训练阶段无需继续访问预训练模型。

Result: 在ImageNet-1k上实现最先进的重建与生成质量（相较既有方法SOTA），并展现出更均匀的语义潜在分布，利于AR生成模型训练。

Conclusion: 全局关系建模与直方图关系学习可显著改善离散语义分布的均匀性，结合残差细节恢复，GloTok在不依赖预训练模型的AR训练下实现更佳的图像重建与生成表现。

Abstract: Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.

</details>


### [61] [PAVE: An End-to-End Dataset for Production Autonomous Vehicle Evaluation](https://arxiv.org/abs/2511.14185)
*Xiangyu Li,Chen Wang,Yumao Liu,Dengbo He,Jiahao Zhang,Ke Ma*

Main category: cs.CV

TL;DR: 提出首个完全由真实世界“自动驾驶模式”采集的端到端基准数据集，含10 0+小时多车型数据、3.27万关键帧、多模态标注，用于评估黑箱AV的行为安全；基线端到端规划ADE≈1.4 m，数据持续每周+10小时。


<details>
  <summary>Details</summary>
Motivation: 现有主流数据集多源自人工驾驶或模式不明，难以真实评估黑箱控制的自动驾驶车辆在现实环境中的行为安全与端到端性能，缺乏能直接用于规划/行为层面评测的数据基准。

Method: 在市场多款量产自动驾驶车上，以全自动驾驶模式长期采集自然驾驶数据；将原始数据切分为关键帧并同步多摄像头与高精度GNSS/IMU（0.8 cm）；为每个关键帧提供过去6 s与未来5 s的20 Hz自车轨迹，以及周边车辆/行人/信号灯/交通标志的2D标注；赋予丰富场景级属性（意图、道路类型、光照、天气、路面、交通/弱势道路使用者密度、信号灯与标志类别）；提供端到端运动规划基线进行安全评估。

Result: 得到包含100+小时数据、32,727关键帧、四目相机+高精度定位的多模态数据集；端到端运动规划模型在自动驾驶帧上的ADE约1.4 m；数据集每周新增10+小时。

Conclusion: 该数据集填补了真实“自动驾驶模式”端到端评测数据空白，支持对AV行为与安全进行量化研究，并通过持续扩充为社区提供长期可持续的评测与分析基线。

Abstract: Most existing autonomous-driving datasets (e.g., KITTI, nuScenes, and the Waymo Perception Dataset), collected by human-driving mode or unidentified driving mode, can only serve as early training for the perception and prediction of autonomous vehicles (AVs). To evaluate the real behavioral safety of AVs controlled in the black box, we present the first end-to-end benchmark dataset collected entirely by autonomous-driving mode in the real world. This dataset contains over 100 hours of naturalistic data from multiple production autonomous-driving vehicle models in the market. We segment the original data into 32,727 key frames, each consisting of four synchronized camera images and high-precision GNSS/IMU data (0.8 cm localization accuracy). For each key frame, 20 Hz vehicle trajectories spanning the past 6 s and future 5 s are provided, along with detailed 2D annotations of surrounding vehicles, pedestrians, traffic lights, and traffic signs. These key frames have rich scenario-level attributes, including driver intent, area type (covering highways, urban roads, and residential areas), lighting (day, night, or dusk), weather (clear or rain), road surface (paved or unpaved), traffic and vulnerable road users (VRU) density, traffic lights, and traffic signs (warning, prohibition, and indication). To evaluate the safety of AVs, we employ an end-to-end motion planning model that predicts vehicle trajectories with an Average Displacement Error (ADE) of 1.4 m on autonomous-driving frames. The dataset continues to expand by over 10 hours of new data weekly, thereby providing a sustainable foundation for research on AV driving behavior analysis and safety evaluation.

</details>


### [62] [Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation](https://arxiv.org/abs/2511.14186)
*Zhaoyu Liu,Kan Jiang,Murong Ma,Zhe Hou,Yun Lin,Jin Song Dong*

Main category: cs.CV

TL;DR: 提出UMEG-Net，用统一多实体图(人体骨架+运动物体关键点)与高效时空特征提取，并通过多模态蒸馏在小样本精准事件定位任务上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: PES需要在精确时间点识别细粒度体育事件，但受快速变化、运动模糊和细微差异影响，且现有方法依赖大量标注与像素/姿态单模态，少样本下效果差；而大规模标注难以获得。

Method: 构建统一多实体图，将人体骨架与运动特定物体关键点融为一体；采用先进GCN与多尺度时间移位模块进行高效时空特征抽取；引入多模态蒸馏，将基于关键点图的知识迁移到视觉表征，提升少样本表现。

Result: 在少样本PES设定中，方法在有限标注数据下表现稳健，显著优于多种基线；代码已开源。

Conclusion: UMEG-Net在小样本精准事件定位中提供可扩展、有效的方案，通过统一图建模与多模态蒸馏克服数据稀缺与单模态局限。

Abstract: Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at https://github.com/LZYAndy/UMEG-Net.

</details>


### [63] [Hierarchical Semantic Learning for Multi-Class Aorta Segmentation](https://arxiv.org/abs/2511.14187)
*Pengcheng Shi*

Main category: cs.CV

TL;DR: 提出一种结合课程学习与“分形softmax”的分层语义学习框架，用于主动脉及分支血管的3D分割，在精度与推理速度上均显著优于基线，适配临床实时需求。


<details>
  <summary>Details</summary>
Motivation: 血管分割存在两大难点：1) 分层解剖关系常被忽略，导致对复杂分支结构理解不足；2) 类别极度不平衡（主干远多于细小分支），使得小结构难以学习与收敛慢，影响临床可用性。

Method: 以人类认知启发的课程学习策略，从“简单到复杂”逐步引入解剖约束。核心是提出“分形softmax”（fractal softmax）以实现分层语义建模：先对主干/优势类别学习稳健特征，再逐步细化到稀有但关键的分支结构。配合两阶段推理（stage1粗分/筛选，stage2细分）以加速推理。采用nnU‑Net ResEnc M为基线并引入分层语义损失。

Result: 在验证集第50个epoch上，引入分层语义损失使nnU‑Net ResEnc M的Dice提升11.65%；在测试集上，所提模型较基线平均Dice提升5.6%。两阶段推理带来最高5倍速度加速，显著提升临床可用性。

Conclusion: 分层语义+课程学习能有效缓解类别不平衡并更好捕捉解剖层级，提高主动脉及分支分割的精度与效率，具备实时临床应用潜力。相关实现与分形softmax代码将公开。

Abstract: The aorta, the body's largest artery, is prone to pathologies such as dissection, aneurysm, and atherosclerosis, which often require timely intervention. Minimally invasive repairs involving branch vessels necessitate detailed 3D anatomical analysis. Existing methods often overlook hierarchical anatomical relationships while struggling with severe class imbalance inherent in vascular structures. We address these challenges with a curriculum learning strategy that leverages a novel fractal softmax for hierarchical semantic learning. Inspired by human cognition, our approach progressively learns anatomical constraints by decomposing complex structures from simple to complex components. The curriculum learning framework naturally addresses class imbalance by first establishing robust feature representations for dominant classes before tackling rare but anatomically critical structures, significantly accelerating model convergence in multi-class scenarios. Our two-stage inference strategy achieves up to fivefold acceleration, enhancing clinical practicality. On the validation set at epoch 50, our hierarchical semantic loss improves the Dice score of nnU-Net ResEnc M by 11.65%. The proposed model demonstrates a 5.6% higher Dice score than baselines on the test set. Experimental results show significant improvements in segmentation accuracy and efficiency, making the framework suitable for real-time clinical applications. The implementation code for this challenge entry is publicly available at: https://github.com/PengchengShi1220/AortaSeg24. The code for fractal softmax will be available at https://github.com/PengchengShi1220/fractal-softmax.

</details>


### [64] [Online Data Curation for Object Detection via Marginal Contributions to Dataset-level Average Precision](https://arxiv.org/abs/2511.14197)
*Zitang Sun,Masakazu Yoshimura,Junji Otsuka,Atsushi Irie,Takeshi Ohashi*

Main category: cs.CV

TL;DR: DetGain是一种针对目标检测的在线数据精选方法，通过估计每张图像对数据集平均精度（AP）的边际影响来动态挑选训练样本，从而以较小代价获得更高准确率；在COCO上的多种检测器上均有稳定提升，并在低质量数据和与蒸馏结合时表现出强鲁棒性与互补性。


<details>
  <summary>Details</summary>
Motivation: 在规模定律下，高质量数据比海量未筛选数据更有效；然而，现有在线数据精选多用于分类/多模态，难以应用到结构复杂、域间差异显著的目标检测。需要一种能在训练过程中依据模型状态评估样本价值、且适配检测任务的通用方法。

Method: 提出DetGain：基于预测质量建模全局分数分布，估计单张图像对数据集级AP的边际扰动（global AP change）；计算教师-学生贡献差（contribution gap）以度量样本信息量；在每次迭代中选择高收益样本进行训练。方法与架构无关、侵入性小，可无缝集成到多种检测器。

Result: 在COCO数据集和多种代表性检测器上取得一致准确率提升；在低质量数据条件下仍表现稳健；与知识蒸馏联合使用可进一步提升性能。

Conclusion: DetGain为目标检测提供了一种高效、通用且可组合的在线数据精选策略，能在不依赖特定架构的情况下提升AP，尤其在数据噪声场景与蒸馏框架中具有显著效果。

Abstract: High-quality data has become a primary driver of progress under scale laws, with curated datasets often outperforming much larger unfiltered ones at lower cost. Online data curation extends this idea by dynamically selecting training samples based on the model's evolving state. While effective in classification and multimodal learning, existing online sampling strategies rarely extend to object detection because of its structural complexity and domain gaps. We introduce DetGain, an online data curation method specifically for object detection that estimates the marginal perturbation of each image to dataset-level Average Precision (AP) based on its prediction quality. By modeling global score distributions, DetGain efficiently estimates the global AP change and computes teacher-student contribution gaps to select informative samples at each iteration. The method is architecture-agnostic and minimally intrusive, enabling straightforward integration into diverse object detection architectures. Experiments on the COCO dataset with multiple representative detectors show consistent improvements in accuracy. DetGain also demonstrates strong robustness under low-quality data and can be effectively combined with knowledge distillation techniques to further enhance performance, highlighting its potential as a general and complementary strategy for data-efficient object detection.

</details>


### [65] [Multi-Scale Correlation-Aware Transformer for Maritime Vessel Re-Identification](https://arxiv.org/abs/2511.14203)
*Yunhe Liu*

Main category: cs.CV

TL;DR: 提出MCFormer用于船舶Re-ID，通过显式建模全局与局部多尺度相关性，抑制同ID内离群样本与局部缺失带来的影响，在三大基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 船舶图像相比行人Re-ID存在更大同身份内变化与更严重的局部缺失，直接套用行人算法易出现同ID离群样本，影响匹配鲁棒性；需要一种能跨图像、跨尺度建模相关性的机制来缓解这些问题。

Method: 构建Multi-scale Correlation-aware Transformer (MCFormer)，含两大模块：1) 全局相关模块GCM：在整个输入集合上构建全局相似度亲和矩阵，基于跨图像一致性聚合特征，学习全局相关性；2) 局部相关模块LCM：维护动态memory bank，挖掘并对齐正样本间具上下文相似性的局部特征，学习局部相关性，补偿单图中的缺失/遮挡。最终融合多尺度的全局与局部相关特征，提升鲁棒性。

Result: 在三个公开基准上取得SOTA性能（文中未给具体数值，但实验显示全面优于现有方法）。

Conclusion: 显式的多尺度全局与局部相关性建模能有效抑制离群样本与局部缺失带来的负面影响，提升船舶Re-ID鲁棒性与精度；MCFormer为海事监控与态势感知提供更可靠的Re-ID解决方案。

Abstract: Maritime vessel re-identification (Re-ID) plays a crucial role in advancing maritime monitoring and intelligent situational awareness systems. However, some existing vessel Re-ID methods are directly adapted from pedestrian-focused algorithms, making them ill-suited for mitigating the unique problems present in vessel images, particularly the greater intra-identity variations and more severe missing of local parts, which lead to the emergence of outlier samples within the same identity. To address these challenges, we propose the Multi-scale Correlation-aware Transformer Network (MCFormer), which explicitly models multi-scale correlations across the entire input set to suppress the adverse effects of outlier samples with intra-identity variations or local missing, incorporating two novel modules, the Global Correlation Module (GCM), and the Local Correlation Module (LCM). Specifically, GCM constructs a global similarity affinity matrix across all input images to model global correlations through feature aggregation based on inter-image consistency, rather than solely learning features from individual images as in most existing approaches. Simultaneously, LCM mines and aligns local features of positive samples with contextual similarity to extract local correlations by maintaining a dynamic memory bank, effectively compensating for missing or occluded regions in individual images. To further enhance feature robustness, MCFormer integrates global and local features that have been respectively correlated across multiple scales, effectively capturing latent relationships among image features. Experiments on three benchmarks demonstrate that MCFormer achieves state-of-the-art performance.

</details>


### [66] [InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior](https://arxiv.org/abs/2511.14208)
*Weimin Bai,Suzhe Xu,Yiwei Ren,Jinhua Hao,Ming Sun,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: 提出InstantViR：将强大的双向视频扩散教师蒸馏为单次前向的因果自回归学生，实现实时视频逆问题重建（>35 FPS），质量可比/超越扩散基线并带来最高百倍加速。


<details>
  <summary>Details</summary>
Motivation: 视频逆问题（修复、去模糊、超分等）在流媒体、远程协作、AR/VR中需要高感知质量与低延迟并存。现有基于扩散的方案要么在图像扩散上加临时时序正则，易产生时序伪影；要么使用原生视频扩散，但迭代采样过慢，难以实时。需要一种既保留强时序先验又能低延迟推理的方法。

Method: 提出InstantViR的摊销推断框架：以预训练的双向视频扩散模型为教师，通过先验驱动的蒸馏，将其能力转移到因果自回归学生网络，使其从退化视频直接单次前向生成修复结果，无需迭代优化。蒸馏仅依赖教师模型与已知退化算子，不需成对干净/噪声数据。为进一步提速，用高效LeanVAE替换扩散骨干VAE，并设计教师空间正则的蒸馏策略以在低维潜空间中低延迟处理。

Result: 在随机遮挡补全、高斯去模糊、超分等任务上，InstantViR在A100上>35 FPS，较迭代视频扩散求解器最高加速约100倍，同时重建质量匹配或超越扩散基线，并保持良好时序一致性。

Conclusion: 通过先验驱动蒸馏与高效潜空间建模，扩散式视频重建可实现实时交互与可编辑的流式应用，使高质量视频复原成为现代视觉系统的实用组件。

Abstract: Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.

</details>


### [67] [Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution](https://arxiv.org/abs/2511.14210)
*N Dinesh Reddy,Sudeep Pillai*

Main category: cs.CV

TL;DR: Orion 是一个可“任意模态输入—任意模态输出”的视觉智能体框架，通过多工具调用来执行复杂多步视觉任务，结合神经感知与符号执行，达成SOTA/竞品级表现，并从被动描述转向主动、可操作的视觉智能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多停留在描述层面，难以执行需要多步推理与专业视觉算法协作的真实任务；生产级应用需要可编排、多工具、可解释与可扩展的视觉智能体系。

Method: 构建一个具备代理能力的框架：以大模型为中枢进行工具编排与计划，调用对象检测、关键点定位、全景分割、OCR、几何分析等专用视觉工具；支持多模态输入与多模态输出；通过神经感知（模型感知）+符号执行（工具流水线）的混合范式完成复杂视觉工作流。

Result: 在MMMU、MMBench、DocVQA、MMLongBench等基准上取得SOTA或具竞争力的成绩；展现从单体VLM向可生产部署的视觉智能的扩展能力。

Conclusion: Orion 将“工具化”与“代理化”引入视觉AI，证明多工具编排与神经-符号混合范式能提升复杂视觉任务的性能与可用性，推动视觉理解从被动描述走向主动推理与执行。

Abstract: We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.

</details>


### [68] [Measurement-Constrained Sampling for Text-Prompted Blind Face Restoration](https://arxiv.org/abs/2511.14213)
*Wenjie Li,Yulun Zhang,Guangwei Gao,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出一种测量约束采样（MCS）用于盲脸修复，借助文本到图扩散模型与前/反向测量约束，在保证与输入结构一致的同时，实现面向不同文本提示的多样化高质量重建，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 盲脸修复在极低质量输入下存在“一对多”解空间，现有方法多为单一确定性输出，难以反映多样且与语义提示一致的重建需求。作者希望在保持与原始观测一致的前提下，引入文本条件实现多样化且可控的重建。

Method: 将BFR表述为带测量约束的生成逆问题：先对粗修复结果施加可控退化，构造与观测一致的测量空间；在文本到图扩散模型中进行后验引导的采样。核心包括两类测量约束：（1）前向测量（Forward Measurement），保证生成结果通过退化后与输入LQ在结构上匹配；（2）反向测量（Reverse Measurement），构造投影空间，使解能够与不同文本提示对齐，从而实现多样、可控的采样。

Result: 在多组实验中，MCS能根据不同文本提示生成对齐的多样化高质量人脸重建，并在客观指标或主观评价上优于现有BFR方法。

Conclusion: 将测量约束与扩散采样结合能有效刻画BFR的一对多特性，实现既与观测一致又与文本提示对齐的多样化修复；方法在多数据集上表现优越，代码将于接收后开源。

Abstract: Blind face restoration (BFR) may correspond to multiple plausible high-quality (HQ) reconstructions under extremely low-quality (LQ) inputs. However, existing methods typically produce deterministic results, struggling to capture this one-to-many nature. In this paper, we propose a Measurement-Constrained Sampling (MCS) approach that enables diverse LQ face reconstructions conditioned on different textual prompts. Specifically, we formulate BFR as a measurement-constrained generative task by constructing an inverse problem through controlled degradations of coarse restorations, which allows posterior-guided sampling within text-to-image diffusion. Measurement constraints include both Forward Measurement, which ensures results align with input structures, and Reverse Measurement, which produces projection spaces, ensuring that the solution can align with various prompts. Experiments show that our MCS can generate prompt-aligned results and outperforms existing BFR methods. Codes will be released after acceptance.

</details>


### [69] [StreamingTalker: Audio-driven 3D Facial Animation with Autoregressive Diffusion Model](https://arxiv.org/abs/2511.14223)
*Yifan Yang,Zhi Cen,Sida Peng,Xiangwei Chen,Yifu Deng,Xinyu Zhu,Fan Jia,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 提出一种自回归扩散的流式语音驱动3D人脸动画模型，利用有限历史帧+当前音频作为动态条件，逐帧生成，解决长序列性能退化与高延迟问题，实现低延迟高质量实时合成，并提供交互式演示与即将开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的语音驱动3D人脸动画方法一次性处理整段音频：1) 训练时长之外的超长音频容易性能下降；2) 长音频推理延迟显著，难以实时。需一种既能适配可变长度又低延迟的方案。

Method: 设计“自回归+扩散”的流式生成框架：每步仅取固定数量的过去人脸运动帧作为历史上下文，和当前音频窗口拼成动态条件，指导扩散过程迭代产生下一个/一批人脸运动帧；窗口滑动，实现在线逐帧合成。

Result: 在保证表达性与自然度的同时，实现与音频时长无关的低延迟实时生成；对长音频鲁棒，提供实时交互demo，效果与效率兼具。

Conclusion: 流式自回归扩散能够在不牺牲质量的前提下实现语音驱动3D人脸动画的低延迟、可变长度生成；方法实用且可推广，代码与演示将开源。

Abstract: This paper focuses on the task of speech-driven 3D facial animation, which aims to generate realistic and synchronized facial motions driven by speech inputs.Recent methods have employed audio-conditioned diffusion models for 3D facial animation, achieving impressive results in generating expressive and natural animations.However, these methods process the whole audio sequences in a single pass, which poses two major challenges: they tend to perform poorly when handling audio sequences that exceed the training horizon and will suffer from significant latency when processing long audio inputs. To address these limitations, we propose a novel autoregressive diffusion model that processes input audio in a streaming manner. This design ensures flexibility with varying audio lengths and achieves low latency independent of audio duration. Specifically, we select a limited number of past frames as historical motion context and combine them with the audio input to create a dynamic condition. This condition guides the diffusion process to iteratively generate facial motion frames, enabling real-time synthesis with high-quality results. Additionally, we implemented a real-time interactive demo, highlighting the effectiveness and efficiency of our approach. We will release the code at https://zju3dv.github.io/StreamingTalker/.

</details>


### [70] [Breaking the Passive Learning Trap: An Active Perception Strategy for Human Motion Prediction](https://arxiv.org/abs/2511.14237)
*Juncheng Hu,Zijian Zhang,Zeyu Wang,Guoyu Wang,Yingji Li,Kedi Lyu*

Main category: cs.CV

TL;DR: 提出主动感知策略APS，通过商空间表示与辅助恢复学习，显式建模运动属性与时空依赖，在H3.6M/CMU/3DPW上显著超SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体运动预测多依赖隐式深网建模，导致坐标冗余、运动语义耦合、时空关系学习被动且单调，需要一种能够主动引导、显式编码运动属性并强化时空建模的策略。

Method: 1) 数据感知模块：将姿态投影到商空间，解耦几何与坐标冗余；联合编码切向向量与Grassmann投影，实现几何降维、语义解耦与动态约束。2) 网络感知模块：通过“恢复式学习”主动学习时空依赖——有意遮蔽关节或注入噪声，构造辅助监督；专门的辅助学习网络从受扰动信息中自适应恢复。APS为模型无关，可插拔地增强不同预测器的感知能力。

Result: 在标准数据集上显著优于现有方法：H3.6M提升16.3%，CMU Mocap提升13.9%，3DPW提升10.1%。

Conclusion: 显式的商空间表示与主动的恢复式辅助学习，可以有效缓解坐标冗余与被动学习问题，强化时空建模并显著提升3D人体运动预测性能；方法通用、可与多种预测模型结合。

Abstract: Forecasting 3D human motion is an important embodiment of fine-grained understanding and cognition of human behavior by artificial agents. Current approaches excessively rely on implicit network modeling of spatiotemporal relationships and motion characteristics, falling into the passive learning trap that results in redundant and monotonous 3D coordinate information acquisition while lacking actively guided explicit learning mechanisms. To overcome these issues, we propose an Active Perceptual Strategy (APS) for human motion prediction, leveraging quotient space representations to explicitly encode motion properties while introducing auxiliary learning objectives to strengthen spatio-temporal modeling. Specifically, we first design a data perception module that projects poses into the quotient space, decoupling motion geometry from coordinate redundancy. By jointly encoding tangent vectors and Grassmann projections, this module simultaneously achieves geometric dimension reduction, semantic decoupling, and dynamic constraint enforcement for effective motion pose characterization. Furthermore, we introduce a network perception module that actively learns spatio-temporal dependencies through restorative learning. This module deliberately masks specific joints or injects noise to construct auxiliary supervision signals. A dedicated auxiliary learning network is designed to actively adapt and learn from perturbed information. Notably, APS is model agnostic and can be integrated with different prediction models to enhance active perceptual. The experimental results demonstrate that our method achieves the new state-of-the-art, outperforming existing methods by large margins: 16.3% on H3.6M, 13.9% on CMU Mocap, and 10.1% on 3DPW.

</details>


### [71] [Enhancing Generalization of Depth Estimation Foundation Model via Weakly-Supervised Adaptation with Regularization](https://arxiv.org/abs/2511.14238)
*Yan Huang,Yongyi Su,Xin Lin,Le Zhang,Xun Xu*

Main category: cs.CV

TL;DR: WeSTAR提出一种参数高效的弱监督自训练适配框架，结合密集自监督、语义感知分层归一化、成对序关系弱监督与权重正则，稳定适配MDE基础模型，在多种OOD场景达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管MDE基础模型在零样本泛化上表现强，但在可获得少量下游数据时如何进一步提升、尤其在未知与多样域的鲁棒性仍未解决。需要一种既参数高效又稳定的适配方法，避免灾难性遗忘并纠正局部拓扑错误。

Method: 1) 以密集自训练为核心，通过模型自身伪标签进行结构自监督；2) 引入语义感知的分层归一化，利用实例级分割在多尺度上进行更稳定的结构归一化；3) 融入低成本的成对序深度标注作为弱监督，加入序关系约束以缓解局部拓扑错误；4) 采用权重正则（锚定LoRA更新）以稳定训练并保留可泛化知识。

Result: 在真实与受扰动的OOD数据集及多种复杂场景上，WeSTAR稳定提升MDE基础模型泛化能力，跨多项基准取得SOTA。

Conclusion: 通过弱监督自训练与正则化的参数高效适配，WeSTAR能在少量下游监督下显著提升MDE基础模型在未知域的鲁棒性与泛化，并保持训练稳定与知识保留。

Abstract: The emergence of foundation models has substantially advanced zero-shot generalization in monocular depth estimation (MDE), as exemplified by the Depth Anything series. However, given access to some data from downstream tasks, a natural question arises: can the performance of these models be further improved? To this end, we propose WeSTAR, a parameter-efficient framework that performs Weakly supervised Self-Training Adaptation with Regularization, designed to enhance the robustness of MDE foundation models in unseen and diverse domains. We first adopt a dense self-training objective as the primary source of structural self-supervision. To further improve robustness, we introduce semantically-aware hierarchical normalization, which exploits instance-level segmentation maps to perform more stable and multi-scale structural normalization. Beyond dense supervision, we introduce a cost-efficient weak supervision in the form of pairwise ordinal depth annotations to further guide the adaptation process, which enforces informative ordinal constraints to mitigate local topological errors. Finally, a weight regularization loss is employed to anchor the LoRA updates, ensuring training stability and preserving the model's generalizable knowledge. Extensive experiments on both realistic and corrupted out-of-distribution datasets under diverse and challenging scenarios demonstrate that WeSTAR consistently improves generalization and achieves state-of-the-art performance across a wide range of benchmarks.

</details>


### [72] [V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization](https://arxiv.org/abs/2511.14247)
*Wenkai Lin,Qiming Xia,Wen Li,Xun Huang,Chenglu Wen*

Main category: cs.CV

TL;DR: 提出一种基于激光雷达的GNSS无依赖多车协同感知框架：用PGC生成含置信度的位姿，用PASTAT做置信度感知的时空对齐，并发布V2VLoc数据集，实验在GNSS-denied条件下达SOTA并在V2V4Real验证泛化。


<details>
  <summary>Details</summary>
Motivation: 多智能体协同感知依赖准确位姿来对齐特征，但GNSS在遮挡或城市峡谷中易失效，导致跨车特征难以一致对齐，影响检测和感知质量。因此需要一种无需GNSS、鲁棒的位姿估计与对齐机制。

Method: 1) 设计轻量的Pose Generator with Confidence (PGC)，从LiDAR生成紧凑的位姿和对应置信度表示；2) 提出Pose-Aware Spatio-Temporal Alignment Transformer (PASTAT)，利用位姿置信度进行空间对齐，同时建模多帧时间上下文以减弱定位误差影响；3) 构建V2VLoc仿真数据集，含Town1Loc、Town4Loc（多次遍历用于定位训练）与V2VDet（协同检测）。

Result: 在V2VLoc上进行大量实验，方法在无GNSS场景下达到SOTA表现；扩展到真实世界V2V4Real数据集，验证PASTAT和整体框架的有效性与可泛化性。

Conclusion: 基于LiDAR的PGC+PASTAT框架能在GNSS缺失下实现鲁棒协同感知与检测，并通过置信度感知的时空对齐缓解定位误差；新数据集V2VLoc为相关研究提供了统一基准。

Abstract: Multi-agents rely on accurate poses to share and align observations, enabling a collaborative perception of the environment. However, traditional GNSS-based localization often fails in GNSS-denied environments, making consistent feature alignment difficult in collaboration. To tackle this challenge, we propose a robust GNSS-free collaborative perception framework based on LiDAR localization. Specifically, we propose a lightweight Pose Generator with Confidence (PGC) to estimate compact pose and confidence representations. To alleviate the effects of localization errors, we further develop the Pose-Aware Spatio-Temporal Alignment Transformer (PASTAT), which performs confidence-aware spatial alignment while capturing essential temporal context. Additionally, we present a new simulation dataset, V2VLoc, which can be adapted for both LiDAR localization and collaborative detection tasks. V2VLoc comprises three subsets: Town1Loc, Town4Loc, and V2VDet. Town1Loc and Town4Loc offer multi-traversal sequences for training in localization tasks, whereas V2VDet is specifically intended for the collaborative detection task. Extensive experiments conducted on the V2VLoc dataset demonstrate that our approach achieves state-of-the-art performance under GNSS-denied conditions. We further conduct extended experiments on the real-world V2V4Real dataset to validate the effectiveness and generalizability of PASTAT.

</details>


### [73] [ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation](https://arxiv.org/abs/2511.14259)
*Zitong Xu,Huiyu Duan,Xiaoyu Wang,Zhaolin Cai,Kaiwei Zhang,Qiang Hu,Jing Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出ManipBench大规模AI编辑图像篡改检测/定位基准与ManipShield统一检测-定位-解释模型，覆盖广、标注全，性能SOTA并具泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有IMDL基准在内容多样性、生成模型覆盖和可解释性上不足，限制了检测方法的泛化与可解释能力；新一代高保真实用的生成式编辑带来更严峻检测挑战。

Method: 1) 构建ManipBench：包含45万+由25个前沿图像编辑模型生成、覆盖12类操作的篡改样本；其中10万图像带有目标框、判据线索与文本解释标注。2) 提出ManipShield：以MLLM为核心，通过对比式LoRA微调和任务特定解码头，实现统一的检测（真假判别）、定位（框/掩膜）与解释（文本理由）。

Result: 在ManipBench及多项公开数据集上，ManipShield取得SOTA；对未见过的编辑模型也表现出强泛化能力。

Conclusion: ManipBench为AI编辑图像篡改研究提供了大规模、可解释的评测基准；ManipShield展示了统一多任务框架在检测、定位与解释上的有效性与泛化性，资源将随论文公开。

Abstract: With the rapid advancement of generative models, powerful image editing methods now enable diverse and highly realistic image manipulations that far surpass traditional deepfake techniques, posing new challenges for manipulation detection. Existing image manipulation detection and localization (IMDL) benchmarks suffer from limited content diversity, narrow generative-model coverage, and insufficient interpretability, which hinders the generalization and explanation capabilities of current manipulation detection methods. To address these limitations, we introduce \textbf{ManipBench}, a large-scale benchmark for image manipulation detection and localization focusing on AI-edited images. ManipBench contains over 450K manipulated images produced by 25 state-of-the-art image editing models across 12 manipulation categories, among which 100K images are further annotated with bounding boxes, judgment cues, and textual explanations to support interpretable detection. Building upon ManipBench, we propose \textbf{ManipShield}, an all-in-one model based on a Multimodal Large Language Model (MLLM) that leverages contrastive LoRA fine-tuning and task-specific decoders to achieve unified image manipulation detection, localization, and explanation. Extensive experiments on ManipBench and several public datasets demonstrate that ManipShield achieves state-of-the-art performance and exhibits strong generality to unseen manipulation models. Both ManipBench and ManipShield will be released upon publication.

</details>


### [74] [Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery](https://arxiv.org/abs/2511.14270)
*Yiming Zeng,Xi-Le Zhao,Wei-Hao Wu,Teng-Yu Ji,Chao Wang*

Main category: cs.CV

TL;DR: 提出GSLR，用二维与一维高斯splatting分别生成潜在张量与变换矩阵，克服t-SVD对局部高频刻画差与固定基限制，在无监督恢复任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统t-SVD在多维图像表示中：1）潜在张量近似粗糙，难以捕获空间局部高频；2）变换矩阵基于固定原子（DFT/DCT），沿mode-3纤维的局部高频拟合不足。需要一种可压缩、连续且能更好表达局部高频的表示。

Method: 提出Gaussian Splatting-based Low-rank tensor Representation（GSLR）：• 用定制二维高斯splatting生成潜在张量，细致表达空间局部结构；• 用一维高斯splatting生成变换矩阵，替代固定基以自适应拟合mode-3局部高频；两者互补，形成紧凑连续表示。基于该表示构建无监督的多维图像恢复模型。

Result: 在多维图像恢复的广泛实验中，GSLR在总体指标与尤其是局部高频细节重建方面均优于现有SOTA方法。

Conclusion: 以二维/一维高斯splatting联合构建的低秩张量表示能更精确地捕获局部高频信息，突破t-SVD潜在张量粗糙近似与固定基限制，在无监督恢复任务中表现稳定、领先。

Abstract: Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.

</details>


### [75] [Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation](https://arxiv.org/abs/2511.14271)
*Weimin Bai,Yubo Li,Weijian Luo,Zeqiang Lai,Yequan Wang,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: 提出VLM3D，把大规模视觉-语言模型作为可微分的语义与空间“评论家”，用Yes/No对数几率生成双重监督，改善文本到3D的语义对齐与几何一致性；可用于优化式与前馈式两类管线，显著提升基准与纠正空间错误。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-3D方法普遍存在两大痛点：1) 语义对齐粗糙，细粒度提示难以落实；2) 3D空间理解薄弱，导致几何不一致、部件组合与空间关系失败。需要一种可泛化、可微的监督信号，将语言落地到语义和空间两方面，以提升不同范式生成流程的可靠性。

Method: 将VLM重用为“语义+空间评论家”，构造基于VLM二元回答（Yes/No）的对数几率作为可微分的双查询（semantic与spatial）指导信号：- 在优化式管线中，将该信号作为奖励目标驱动参数更新；- 在前馈式管线中，于测试时对迭代采样过程施加引导，动态纠正严重空间错误。该设计不依赖特定模型，形成通用可插拔模块。

Result: 在标准基准上，相较现有优化式方法显著提升；在SOTA原生3D前馈模型中作为测试时引导，能在生成过程中主动修正空间关系与几何错误，表现优于未引导的基线。

Conclusion: VLM3D提供了一条将VLM的语言落地能力与空间理解注入3D生成流程的统一路径。其基于Yes/No对数几率的双重评论家信号通用、可微、易集成，既提升语义细节契合，又改善几何一致性，适用于优化与前馈两大范式。

Abstract: Text-to-3D generation has advanced rapidly, yet state-of-the-art models, encompassing both optimization-based and feed-forward architectures, still face two fundamental limitations. First, they struggle with coarse semantic alignment, often failing to capture fine-grained prompt details. Second, they lack robust 3D spatial understanding, leading to geometric inconsistencies and catastrophic failures in part assembly and spatial relationships. To address these challenges, we propose VLM3D, a general framework that repurposes large vision-language models (VLMs) as powerful, differentiable semantic and spatial critics. Our core contribution is a dual-query critic signal derived from the VLM's Yes or No log-odds, which assesses both semantic fidelity and geometric coherence. We demonstrate the generality of this guidance signal across two distinct paradigms: (1) As a reward objective for optimization-based pipelines, VLM3D significantly outperforms existing methods on standard benchmarks. (2) As a test-time guidance module for feed-forward pipelines, it actively steers the iterative sampling process of SOTA native 3D models to correct severe spatial errors. VLM3D establishes a principled and generalizable path to inject the VLM's rich, language-grounded understanding of both semantics and space into diverse 3D generative pipelines.

</details>


### [76] [Free Lunch to Meet the Gap: Intermediate Domain Reconstruction for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2511.14279)
*Tong Zhang,Yifan Zhao,Liangyu Wang,Jia Li*

Main category: cs.CV

TL;DR: 提出中间域代理（IDP）作为代码本重构目标特征，并用其快速对齐源-目标域，在8个CDFSL基准上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: CDFSL同时面临语义不重叠、域差距大、数据稀缺三重挑战。现有方法多追求通用表征，难兼顾大跨域对齐与少样本稳定学习，缺少对“中间域”属性的系统利用。

Method: 以源域特征嵌入构造代码本，学习中间域代理（IDP），用该代码本重构目标域特征；对IDP从视觉风格与语义内容两方面做实证分析；基于中间域属性提出快速域对齐，将IDP作为学习引导对目标特征做变换；整体采用“中间域重构 + 目标特征变换”的协同训练。

Result: 在8个跨域少样本基准上均超过当前最优方法，取得显著幅度提升（具体数值未在摘要中给出）。

Conclusion: 通过引入可解释的中间域代理并将其用于快速对齐与重构，能有效缓解语义不重叠、域差距与数据稀缺问题，提升CDFSL的泛化与适配性能。

Abstract: Cross-Domain Few-Shot Learning (CDFSL) endeavors to transfer generalized knowledge from the source domain to target domains using only a minimal amount of training data, which faces a triplet of learning challenges in the meantime, i.e., semantic disjoint, large domain discrepancy, and data scarcity. Different from predominant CDFSL works focused on generalized representations, we make novel attempts to construct Intermediate Domain Proxies (IDP) with source feature embeddings as the codebook and reconstruct the target domain feature with this learned codebook. We then conduct an empirical study to explore the intrinsic attributes from perspectives of visual styles and semantic contents in intermediate domain proxies. Reaping benefits from these attributes of intermediate domains, we develop a fast domain alignment method to use these proxies as learning guidance for target domain feature transformation. With the collaborative learning of intermediate domain reconstruction and target feature transformation, our proposed model is able to surpass the state-of-the-art models by a margin on 8 cross-domain few-shot learning benchmarks.

</details>


### [77] [NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction](https://arxiv.org/abs/2511.14283)
*Zi-Chen Xi,Jiahui Huang,Hao-Xiang Chen,Francis Williams,Qun-Ce Xu,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: NeuralSSD提出一种基于神经Galerkin求解的通用方法，从点云重建高质量3D隐式曲面，兼顾精确拟合与泛化，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 隐式表征能稳健处理拓扑变化并精确描述形状，但现有隐式场参数化缺少显式机制确保曲面紧贴输入点云，导致拟合不足或偏差。

Method: 以神经Galerkin为框架，设计新的能量方程以平衡点云置信度与数据一致性，并提出三维卷积网络学习体素/局部3D结构特征，驱动优化，使隐式面与原始点云紧密贴合并引入从点云中学习的归纳偏置。

Result: 在ShapeNet、Matterport等具有挑战性数据集上评估，表明在表面重建的精度与泛化性方面达到SOTA表现。

Conclusion: 通过新能量设计与3D卷积特征结合神经Galerkin求解，NeuralSSD实现稳定、精确的隐式曲面重建，并具备良好泛化，优于现有方法。

Abstract: We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.

</details>


### [78] [NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration](https://arxiv.org/abs/2511.14286)
*Luohong Wu,Matthias Seibold,Nicola A. Cavalcanti,Yunke Ao,Roman Flepp,Aidana Massalimova,Lilian Calvet,Philipp Fürnstahl*

Main category: cs.CV

TL;DR: 提出NeuralBoneReg：基于点云、无需监督、模态无关的骨表面注册框架，在多数据集（CT-超声、CT-RGBD）上达到或超过现有方法的精度（RRE≈1.7–3.8°，RTE≈1.9–2.5 mm），适用于CAOS中的术前-术中跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: CAOS需要把术前个体化计划准确转移到术中，但跨模态（CT、超声、RGB-D等）之间差异大，传统配准易受噪声与模态差距影响，且很多方法依赖标注或特定模态。临床上迫切需要鲁棒、自动、模态无关并且对训练数据需求低的骨表面注册方法。

Method: 以3D点云作为统一表示。框架含两模块：1) 隐式神经无符号距离场（UDF）学习术前骨模型；2) 基于MLP的注册模块，通过生成与评估变换假设实现全局初始化与局部细化，使术中点云与神经UDF对齐。方法为自监督，不需跨受试者训练数据。

Result: 在三个多模态数据集上与多种基线对比，均匹配或优于SOTA：UltraBones100k（CT-超声，腓骨/胫骨）RRE/RTE=1.68°/1.86 mm；新引入UltraBones-Hip（CT-超声，股骨/骨盆）1.88°/1.89 mm；SpineDepth（CT-RGB-D，椎骨）3.79°/2.45 mm。显示对不同解剖与模态的强泛化与精度。

Conclusion: NeuralBoneReg在无需监督与跨模态条件下，实现鲁棒准确的骨表面点云注册，适合在CAOS中将术前计划可靠转移到术中流程；新数据集UltraBones-Hip将公开，促进研究复现与扩展。

Abstract: In computer- and robot-assisted orthopedic surgery (CAOS), patient-specific surgical plans derived from preoperative imaging define target locations and implant trajectories. During surgery, these plans must be accurately transferred, relying on precise cross-registration between preoperative and intraoperative data. However, substantial modality heterogeneity across imaging modalities makes this registration challenging and error-prone. Robust, automatic, and modality-agnostic bone surface registration is therefore clinically important. We propose NeuralBoneReg, a self-supervised, surface-based framework that registers bone surfaces using 3D point clouds as a modality-agnostic representation. NeuralBoneReg includes two modules: an implicit neural unsigned distance field (UDF) that learns the preoperative bone model, and an MLP-based registration module that performs global initialization and local refinement by generating transformation hypotheses to align the intraoperative point cloud with the neural UDF. Unlike SOTA supervised methods, NeuralBoneReg operates in a self-supervised manner, without requiring inter-subject training data. We evaluated NeuralBoneReg against baseline methods on two publicly available multi-modal datasets: a CT-ultrasound dataset of the fibula and tibia (UltraBones100k) and a CT-RGB-D dataset of spinal vertebrae (SpineDepth). The evaluation also includes a newly introduced CT--ultrasound dataset of cadaveric subjects containing femur and pelvis (UltraBones-Hip), which will be made publicly available. NeuralBoneReg matches or surpasses existing methods across all datasets, achieving mean RRE/RTE of 1.68°/1.86 mm on UltraBones100k, 1.88°/1.89 mm on UltraBones-Hip, and 3.79°/2.45 mm on SpineDepth. These results demonstrate strong generalizability across anatomies and modalities, providing robust and accurate cross-modal alignment for CAOS.

</details>


### [79] [GEN3D: Generating Domain-Free 3D Scenes from a Single Image](https://arxiv.org/abs/2511.14291)
*Yuxin Zhang,Ziyu Lu,Hongbo Duan,Keyu Fan,Pengting Luo,Peiyu Zhuang,Mengyu Yang,Houde Liu*

Main category: cs.CV

TL;DR: Gen3d提出从单张RGBD图像出发，逐步扩展并优化世界模型，用高斯喷溅表示生成高质量、广域通用3D场景，能合成一致的高保真新视角且具强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有神经3D重建依赖密集多视角，限制应用；同时，具多样高质量场景的3D生成对具身智能与世界模型训练评测至关重要。

Method: 从单张输入（含深度）的RGBD图像抬升为初始点云；随后维护并扩展世界模型（逐步补全/外推场景）；最终以高斯splatting作为3D表示进行优化（提升几何与外观一致性），用于新视角合成。

Result: 在多样数据集上，方法在生成世界模型与新视角合成方面表现优异，显示出强泛化能力与更高保真度与一致性。

Conclusion: Gen3d能以单张图像生成宽域、高质量、通用3D场景，并在新视角合成中优于现有方法，适用于具身AI与世界模型需求。

Abstract: Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.

</details>


### [80] [SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation](https://arxiv.org/abs/2511.14302)
*Sahar Nasirihaghighi,Negin Ghamsarian,Yiping Li,Marcel Breeuwer,Raphael Sznitman,Klaus Schoeffmann*

Main category: cs.CV

TL;DR: 提出SAM-Fed：用大容量分割基础模型指导轻量客户端，在联邦半监督下提升伪标签质量与稳定性，跨同构/异构设置在皮肤病灶与息肉分割上均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 联邦半监督分割受限于：1) 伪标签可靠性取决于本地模型强度；2) 客户端算力受限需用小模型或异构架构，导致伪标签质量不稳。大模型更准却难在端侧训练/推理。需要一种方法既利用大模型能力又兼容端侧限制。

Method: 提出SAM-Fed框架：在训练期间引入高容量分割基础模型作为“教师”，通过双重知识蒸馏为轻量客户端提供像素级监督，并配合自适应一致性机制（adaptive agreement）来筛选/加权伪标签与来自多源的监督信号，适配同构与异构的联邦场景。

Result: 在皮肤病灶与息肉分割数据集上，在同构和异构客户端设置下，SAM-Fed稳定且显著优于现有FSSL方法。

Conclusion: 利用大模型的指导并通过双重蒸馏与自适应一致性提升伪标签质量，可在算力受限与架构异构的联邦场景中实现更强更稳的半监督医学图像分割。

Abstract: Medical image segmentation is clinically important, yet data privacy and the cost of expert annotation limit the availability of labeled data. Federated semi-supervised learning (FSSL) offers a solution but faces two challenges: pseudo-label reliability depends on the strength of local models, and client devices often require compact or heterogeneous architectures due to limited computational resources. These constraints reduce the quality and stability of pseudo-labels, while large models, though more accurate, cannot be trained or used for routine inference on client devices. We propose SAM-Fed, a federated semi-supervised framework that leverages a high-capacity segmentation foundation model to guide lightweight clients during training. SAM-Fed combines dual knowledge distillation with an adaptive agreement mechanism to refine pixel-level supervision. Experiments on skin lesion and polyp segmentation across homogeneous and heterogeneous settings show that SAM-Fed consistently outperforms state-of-the-art FSSL methods.

</details>


### [81] [Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model](https://arxiv.org/abs/2511.14310)
*Jiancheng Fang,Shaoyu Wang,Junlin Wang,Weiwen Wu,Yikun Zhang,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出Diff-NAF：在多源静态CT的超稀疏视角条件下，通过迭代式“神经衰减场+条件扩散模型”生成与精修投影，逐步完善投影与重建，最终显著提升CT成像质量。


<details>
  <summary>Details</summary>
Motivation: 多源静态CT可实现快速重建，但在实际中常受限于超稀疏视角采样，传统插值与重建方法在此情形下精度不足，导致显著伪影与细节缺失。亟需一种能在少视角条件下补全投影并提升三维重建保真度的方法。

Method: 构建Diff-NAF迭代框架：1) 用超稀疏投影训练初始神经衰减场（NAF）；2) 通过角度先验引导的投影合成（Angle-Prior Guided Projection Synthesis）利用视角间先验生成新视角投影；3) 采用双分支条件扩散模型的“投影复用精修模块”对合成投影进行去噪与细化；4) 将精修投影作为伪标签加入训练集，迭代更新NAF与扩散模型，从而逐步提升投影完整度与重建质量。

Result: 在多组模拟3D CT体数据与真实投影数据上，Diff-NAF在超稀疏视角设置下取得最佳表现，重建更高保真、伪影更少，优于传统方法与基线。

Conclusion: 通过结合神经场表示与条件扩散的迭代伪标签策略，Diff-NAF能在超稀疏视角多源静态CT中有效补全与精修投影，逐步提升重建质量，适用于时敏医疗与工业场景。

Abstract: Multi-source stationary computed tomography (CT) has recently attracted attention for its ability to achieve rapid image reconstruction, making it suitable for time-sensitive clinical and industrial applications. However, practical systems are often constrained by ultra-sparse-view sampling, which significantly degrades reconstruction quality. Traditional methods struggle under ultra-sparse-view settings, where interpolation becomes inaccurate and the resulting reconstructions are unsatisfactory. To address this challenge, this study proposes Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework tailored for multi-source stationary CT under ultra-sparse-view conditions. Diff-NAF combines a Neural Attenuation Field representation with a dual-branch conditional diffusion model. The process begins by training an initial NAF using ultra-sparse-view projections. New projections are then generated through an Angle-Prior Guided Projection Synthesis strategy that exploits inter view priors, and are subsequently refined by a Diffusion-driven Reuse Projection Refinement Module. The refined projections are incorporated as pseudo-labels into the training set for the next iteration. Through iterative refinement, Diff-NAF progressively enhances projection completeness and reconstruction fidelity under ultra-sparse-view conditions, ultimately yielding high-quality CT reconstructions. Experimental results on multiple simulated 3D CT volumes and real projection data demonstrate that Diff-NAF achieves the best performance under ultra-sparse-view conditions.

</details>


### [82] [Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs](https://arxiv.org/abs/2511.14315)
*Yiyi Miao,Taoyu Wu,Tong Chen,Ji Jiang,Zhe Tang,Zhengyong Jiang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 提出Dental3R：在仅有三张无位姿口内照片（正面与双侧颊面）下，实现稳健高保真三维重建与新视角合成，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 远程牙颌正畸通常只能获取稀疏手机照片，传统口内扫描不可得。现有3DGS在无位姿、基线大、光照不一致与高反射环境下易失稳，且稀疏视角的纯光度监督导致频率偏置，生成过度平滑、丢失关键诊断细节。

Method: 提出Dental3R无位姿、图结构引导的流程：1）几何感知配对策略GAPS，从稀疏图像中智能挑选高价值配对构成紧凑子图，强化对应匹配，稳定几何初始化并降内存；2）基于恢复的位姿与点云，训练3DGS并引入小波正则的频带受限目标，用离散小波变换在频域约束，保留牙釉质边界和邻接间隙等细节，同时抑制高频伪影。

Result: 在950例临床样本与195例视频测试集上验证，能在稀疏、无位姿输入下稳定重建并提升新视角的咬合可视化质量，整体优于SOTA。

Conclusion: 通过GAPS稳定几何与小波频域正则，Dental3R在远程正畸场景实现高保真口内3D重建与合成，兼顾细节与稳健性，适合临床三照标准使用。

Abstract: Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.

</details>


### [83] [LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices](https://arxiv.org/abs/2511.14322)
*Nanjun Li,Ziyue Hao,Quanqiang Wang,Xuanyin Wang*

Main category: cs.CV

TL;DR: 提出LSP-YOLO：基于YOLOv11-Pose的单阶段轻量坐姿识别网络，设计Light-C3k2模块与中间监督，将关键点经逐点卷积直接映射到姿态类别；自建6类、5000图数据集；最小模型1.9MB在PC达94.2%准确率、251 FPS，并在SV830C+GC030A上实时运行。


<details>
  <summary>Details</summary>
Motivation: 现有坐姿识别多为两阶段（关键点检测→分类），传感器方案侵入性强，视觉方案计算重、难以在嵌入式端实时运行，亟需低算力设备上的高效、轻量、端侧可部署方法。

Method: 提出单阶段端到端网络LSP-YOLO：1) 结构上融入部分卷积PConv与SimAM，构建轻量特征模块Light-C3k2，在保持表征的同时降算力；2) 识别头中用1×1逐点卷积将关键点直接映射到姿态类别，并采用中间监督促进姿态估计与分类融合；3) 构建包含6类坐姿、5000幅图像的数据集用于训练/测试；4) 在PC与嵌入式平台（SV830C+GC030A）上验证。

Result: 最小模型LSP-YOLO-n仅1.9MB，PC端达到94.2%准确率与251 FPS；在SV830C+GC030A上实现实时高精度推理，体现低延迟与低算力适配。

Conclusion: LSP-YOLO以单阶段、轻量化和中间监督实现高效坐姿识别，兼具准确性、速度与小模型体积，适合在智慧课堂、康复与人机交互场景端侧部署。

Abstract: With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time performance on embedded edge devices. Inspired by YOLOv11-Pose, a lightweight single-stage network for sitting posture recognition on embedded edge devices termed LSP-YOLO was proposed. By integrating partial convolution(PConv) and Similarity-Aware Activation Module(SimAM), a lightweight module, Light-C3k2, was designed to reduce computational cost while maintaining feature extraction capability. In the recognition head, keypoints were directly mapped to posture classes through pointwise convolution, and intermediate supervision was employed to enable efficient fusion of pose estimation and classification. Furthermore, a dataset containing 5,000 images across six posture categories was constructed for model training and testing. The smallest trained model, LSP-YOLO-n, achieved 94.2% accuracy and 251 Fps on personal computer(PC) with a model size of only 1.9 MB. Meanwhile, real-time and high-accuracy inference under constrained computational resources was demonstrated on the SV830C + GC030A platform. The proposed approach is characterized by high efficiency, lightweight design and deployability, making it suitable for smart classrooms, rehabilitation, and human-computer interaction applications.

</details>


### [84] [Step by Step Network](https://arxiv.org/abs/2511.14329)
*Dongchen Han,Tianzhu Ye,Zhuofan Xia,Kaiyi Chen,Yulin Wang,Hanting Chen,Gao Huang*

Main category: cs.CV

TL;DR: 论文提出 StepsNet，一种广义残差架构，通过沿通道逐步扩宽、分步学习来解决“捷径退化”和“宽度受限”，在多任务上优于标准残差网络。


<details>
  <summary>Details</summary>
Motivation: 理论上更深网络具有指数级表达能力，但实际中即使有残差连接，极深网络仍难以兑现理论潜力。作者归因于两点：1）捷径（shortcut）通道在超深层中的退化，抑制了深层学习；2）深度-宽度权衡导致宽度受限，限制表示能力。需要新的宏观架构设计释放深层网络潜力。

Method: 提出 Step by Step Network（StepsNet）：将通道维特征分组/分段，按阶段逐步引入并扩大可学习通道（宽度），通过堆叠宽度递增的模块进行“分步”（progressive）学习。该宏观设计可套用于多种模型，旨在缓解捷径退化并突破固定宽度限制。

Result: 在图像分类、目标检测、语义分割和语言建模等任务上，StepsNet 相比标准残差模型均有一致提升，显示出更好的可扩展性和泛化性。

Conclusion: 逐步扩宽与通道分离的广义残差框架能有效缓解深层训练中的捷径退化与宽度瓶颈，使更深网络更接近其理论能力，是对残差架构的强有力泛化。

Abstract: Scaling up network depth is a fundamental pursuit in neural architecture design, as theory suggests that deeper models offer exponentially greater capability. Benefiting from the residual connections, modern neural networks can scale up to more than one hundred layers and enjoy wide success. However, as networks continue to deepen, current architectures often struggle to realize their theoretical capacity improvements, calling for more advanced designs to further unleash the potential of deeper networks. In this paper, we identify two key barriers that obstruct residual models from scaling deeper: shortcut degradation and limited width. Shortcut degradation hinders deep-layer learning, while the inherent depth-width trade-off imposes limited width. To mitigate these issues, we propose a generalized residual architecture dubbed Step by Step Network (StepsNet) to bridge the gap between theoretical potential and practical performance of deep models. Specifically, we separate features along the channel dimension and let the model learn progressively via stacking blocks with increasing width. The resulting method mitigates the two identified problems and serves as a versatile macro design applicable to various models. Extensive experiments show that our method consistently outperforms residual models across diverse tasks, including image classification, object detection, semantic segmentation, and language modeling. These results position StepsNet as a superior generalization of the widely adopted residual architecture.

</details>


### [85] [ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding](https://arxiv.org/abs/2511.14336)
*Bohan Zhang,Yiyi Miao,Taoyu Wu,Tong Chen,Ji Jiang,Zhuoxiao Li,Zhe Tang,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: ArchMap是一个无需训练、以知识为导向的框架，通过几何归一化与本体驱动的多模态推理，实现对口内3D扫描的稳健结构化理解，并在多项临床任务上优于监督与VLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖特定模态训练、大型标注数据与受控扫描条件，泛化性差；同时口内原始网格存在姿态多变、遮挡导致的不完整几何与缺乏纹理线索，致使统一语义解析困难，影响临床落地。

Method: 提出ArchMap：1) 几何感知的拱形展平模块，将原始3D网格标准化为空间对齐、保持连续性的多视图投影；2) 构建牙科知识库（DKB），编码层级牙齿本体、牙列阶段策略与临床语义，用以约束符号推理空间；整体为训练-free、知识引导的多模态推理流程。

Result: 在1060例正畸前后病例上验证，实现对数牙、解剖分区、牙列阶段分类及拥挤、缺失、修复体、龋齿等临床状态识别的稳健表现；相较监督管线与提示式VLM基线，准确率更高、语义漂移更少，并在稀疏或伪影条件下更稳定。

Conclusion: 几何规范化结合本体引导的多模态推理由于无需训练、可扩展、对设备与工况鲁棒，提供了在现代数字正畸中对3D口内扫描进行结构化分析的实用方案。

Abstract: A structured understanding of intraoral 3D scans is essential for digital orthodontics. However, existing deep-learning approaches rely heavily on modality-specific training, large annotated datasets, and controlled scanning conditions, which limit generalization across devices and hinder deployment in real clinical workflows. Moreover, raw intraoral meshes exhibit substantial variation in arch pose, incomplete geometry caused by occlusion or tooth contact, and a lack of texture cues, making unified semantic interpretation highly challenging. To address these limitations, we propose ArchMap, a training-free and knowledge-guided framework for robust structured dental understanding. ArchMap first introduces a geometry-aware arch-flattening module that standardizes raw 3D meshes into spatially aligned, continuity-preserving multi-view projections. We then construct a Dental Knowledge Base (DKB) encoding hierarchical tooth ontology, dentition-stage policies, and clinical semantics to constrain the symbolic reasoning space. We validate ArchMap on 1060 pre-/post-orthodontic cases, demonstrating robust performance in tooth counting, anatomical partitioning, dentition-stage classification, and the identification of clinical conditions such as crowding, missing teeth, prosthetics, and caries. Compared with supervised pipelines and prompted VLM baselines, ArchMap achieves higher accuracy, reduced semantic drift, and superior stability under sparse or artifact-prone conditions. As a fully training-free system, ArchMap demonstrates that combining geometric normalization with ontology-guided multimodal reasoning offers a practical and scalable solution for the structured analysis of 3D intraoral scans in modern digital orthodontics.

</details>


### [86] [Silhouette-to-Contour Registration: Aligning Intraoral Scan Models with Cephalometric Radiographs](https://arxiv.org/abs/2511.14343)
*Yiyi Miao,Taoyu Wu,Ji Jiang,Tong Chen,Zhe Tang,Zhengyong Jiang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 提出DentalSCR框架，实现IOS三维模型与头影侧位片的稳定高精度3D-2D轮廓配准，通过统一坐标UMDA、轮廓强化投影与双向Chamfer距离的2D相似变换优化，在34例临床数据上显著降低标志点误差与曲线距离，优于传统强度驱动方法。


<details>
  <summary>Details</summary>
Motivation: 临床头影片存在投影放大、几何畸变、牙冠低对比度与设备差异，导致基于强度相似度的3D-2D配准易失稳、陷入局部最优或产生不解剖的结果，需一种对姿态稳定、可解释、对轮廓鲁棒的配准方法。

Method: 1) 构建U-Midline Dental Axis(UMDA)作为跨牙弓统一解剖坐标，标准化初始化与投影几何；2) 在该坐标下，以表面为基础的DRR投影，采用冠状轴透视与Gaussian splatting，保留临床真实源-物-探测器放大并突出外部轮廓；3) 将配准建模为2D相似变换（尺度、旋转、平移），以对称双向Chamfer距离为目标，在分层粗到细的策略下优化，兼顾大捕获范围与亚像素级轮廓一致；4) 用曲线级指标（Chamfer、Hausdorff）与标志点误差评估。

Result: 在34例专家标注病例上：显著降低整体与后牙区域的标志点误差；下颌区域离散度减小；曲线级Chamfer更低、Hausdorff受控。总体性能优于传统强度驱动和基线方法，且结果具有临床可检视性。

Conclusion: DentalSCR通过UMDA稳定姿态、轮廓强化投影与双向Chamfer驱动的2D相似变换，实现对真实临床头影片的鲁棒、可解释的3D-2D配准，提升精度与稳定性，优于传统方法，具有临床应用潜力。

Abstract: Reliable 3D-2D alignment between intraoral scan (IOS) models and lateral cephalometric radiographs is critical for orthodontic diagnosis, yet conventional intensity-driven registration methods struggle under real clinical conditions, where cephalograms exhibit projective magnification, geometric distortion, low-contrast dental crowns, and acquisition-dependent variation. These factors hinder the stability of appearance-based similarity metrics and often lead to convergence failures or anatomically implausible alignments. To address these limitations, we propose DentalSCR, a pose-stable, contour-guided framework for accurate and interpretable silhouette-to-contour registration. Our method first constructs a U-Midline Dental Axis (UMDA) to establish a unified cross-arch anatomical coordinate system, thereby stabilizing initialization and standardizing projection geometry across cases. Using this reference frame, we generate radiograph-like projections via a surface-based DRR formulation with coronal-axis perspective and Gaussian splatting, which preserves clinical source-object-detector magnification and emphasizes external silhouettes. Registration is then formulated as a 2D similarity transform optimized with a symmetric bidirectional Chamfer distance under a hierarchical coarse-to-fine schedule, enabling both large capture range and subpixel-level contour agreement. We evaluate DentalSCR on 34 expert-annotated clinical cases. Experimental results demonstrate substantial reductions in landmark error-particularly at posterior teeth-tighter dispersion on the lower jaw, and low Chamfer and controlled Hausdorff distances at the curve level. These findings indicate that DentalSCR robustly handles real-world cephalograms and delivers high-fidelity, clinically inspectable 3D--2D alignment, outperforming conventional baselines.

</details>


### [87] [ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries](https://arxiv.org/abs/2511.14349)
*Junfu Pu,Teng Wang,Yixiao Ge,Yuying Ge,Chen Li,Ying Shan*

Main category: cs.CV

TL;DR: 提出ARC-Chapter：首个基于百万级长视频章节数据训练的大规模视频分章模型，支持中英双语、时间对齐与层级注释；引入GRACE评测兼顾重叠与语义相似；在F1与SODA上显著超越SOTA，并能迁移提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有长视频分章方法受限于小规模、短且粗粒度标注，难以捕捉细腻过渡并泛化到小时级视频。需要一个更大规模、细粒度、多模态且双语的训练与评测体系来提升分章质量与实用性。

Method: 构建中英双语长视频章节数据集：通过结构化流水线融合ASR字幕、场景文字与视觉描述为多层级注释（从短标题到长摘要），并保证时间对齐与层级化标注；基于该数据训练ARC-Chapter模型，并提出新的评测指标GRACE，考虑多对一时间重叠和语义相似以更贴近真实分章需求；进行数据规模与标注强度扩展实验。

Result: 数据规模（数量与标注强度）提升带来显著性能增益；ARC-Chapter在基准上以F1提升14.0%、SODA提升11.3%超越此前最佳；在YouCook2等下游任务（如稠密视频描述）上取得新的SOTA或显著改进，显示良好可迁移性。

Conclusion: 大规模、双语、层级化、时间对齐的章节数据与模型能显著提升长视频分章质量；GRACE指标更符合实际应用评估；ARC-Chapter树立新SOTA并对下游多模态视频理解任务有广泛促进作用。

Abstract: The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.

</details>


### [88] [IBGS: Image-Based Gaussian Splatting](https://arxiv.org/abs/2511.14357)
*Hoang Chuong Nguyen,Wei Mao,Jose M. Alvarez,Miaomiao Liu*

Main category: cs.CV

TL;DR: 提出“基于图像的高斯溅射”（IBGS）：在标准3DGS基础色上，加入由邻近训练图像推断的学习残差，从而在不增加存储的情况下显著提升细节与视角相关效果。


<details>
  <summary>Details</summary>
Motivation: 3DGS用于新视角合成虽快且质量高，但低阶球谐难以表达空间变化颜色与高频视角相关效应（如高光）。现有用全局纹理图难适配复杂场景，用每高斯纹理又存储开销大，亟需兼顾质量与效率的新方案。

Method: 将每个渲染像素的颜色表示为：标准3DGS渲染得到的基色 + 来自相邻训练图像的学习残差。残差由对邻近视角图像的检索/对齐（促进表面一致性）并通过网络预测得到，从而利用高分辨率输入图像补充高频细节与视角相关颜色。整体无需为每个高斯存额外纹理或增加全局纹理复杂度。

Result: 在标准NVS基准上，渲染质量显著优于以往高斯溅射方法，尤其在高频细节和镜面等视角相关效应上更准确，同时总体存储占用不增加。

Conclusion: 通过将图像引导的残差学习融合进3DGS渲染，可在不增加存储的前提下提升复杂场景的细节与视角相关性，成为比全局或每高斯纹理更高效的NVS方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a fast, high-quality method for novel view synthesis (NVS). However, its use of low-degree spherical harmonics limits its ability to capture spatially varying color and view-dependent effects such as specular highlights. Existing works augment Gaussians with either a global texture map, which struggles with complex scenes, or per-Gaussian texture maps, which introduces high storage overhead. We propose Image-Based Gaussian Splatting, an efficient alternative that leverages high-resolution source images for fine details and view-specific color modeling. Specifically, we model each pixel color as a combination of a base color from standard 3DGS rendering and a learned residual inferred from neighboring training images. This promotes accurate surface alignment and enables rendering images of high-frequency details and accurate view-dependent effects. Experiments on standard NVS benchmarks show that our method significantly outperforms prior Gaussian Splatting approaches in rendering quality, without increasing the storage footprint.

</details>


### [89] [Clinically-Validated Innovative Mobile Application for Assessing Blinking and Eyelid Movements](https://arxiv.org/abs/2511.14361)
*Gustavo Adolpho Bonesso,Carlos Marcelo Gurjão de Godoy,Tammy Hentona Osaki,Midori Hentona Osaki,Bárbara Moreira Ribeiro Trindade dos Santos,Regina Célia Coelho*

Main category: cs.CV

TL;DR: 研究验证了一款名为 Bapp 的手机应用，利用 Google ML Kit 在设备端实时分析眨眼，基于45段真实患者视频与专家标注比对，取得精确率98.4%、召回率96.9%、总体准确率98.3%，显示其作为便携客观的眼睑运动监测工具的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有眨眼/眼睑运动评估工具复杂、昂贵、临床适用性有限，缺乏可在临床与随访中便捷、客观、可及的量化手段，因此需要开发并验证一款可在移动端实时运行、成本低、易部署的评估工具。

Method: 开发Bapp（Flutter框架 + Google ML Kit）实现端侧实时眼睑运动检测；收集来自真实患者的45段视频；由眼科专家人工标注眨眼作为真实值；采用Precision、Recall、F1-Score/Accuracy等标准指标评估。

Result: 在45段专家标注视频上，Bapp达到Precision 98.4%、Recall 96.9%、总体准确率98.3%，表明与人工标注高度一致。

Conclusion: Bapp作为便携、可获得、客观的眨眼与眼睑运动监测工具表现可靠，可替代或补充人工计数，支持临床中的连续眼表健康监测与术后评估。

Abstract: Blinking is a vital physiological process that protects and maintains the health of the ocular surface. Objective assessment of eyelid movements remains challenging due to the complexity, cost, and limited clinical applicability of existing tools. This study presents the clinical validation of Bapp (Blink Application), a mobile application developed using the Flutter framework and integrated with Google ML Kit for on-device, real-time analysis of eyelid movements. The validation occurred using 45 videos from real patients, whose blinks were manually annotated by ophthalmology specialists from the Paulista School of Medicine of the Federal University of Sao Paulo (EPM-UNIFESP) to serve as the ground truth. Bapp's performance was evaluated using standard metrics, including Precision, Recall, and F1-Score, with results demonstrating 98.4% precision, 96.9% recall, and an overall accuracy of 98.3%. These outcomes confirm the reliability of Bapp as a portable, accessible, and objective tool for monitoring both normal and abnormal eyelid movements. The application offers a promising alternative to traditional manual blink counting, supporting continuous ocular health monitoring and postoperative evaluation in clinical environments.

</details>


### [90] [O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model](https://arxiv.org/abs/2511.14368)
*Rishi Gupta,Mukilan Karuppasamy,Shyam Marjit,Aditay Tripathi,Anirban Chakraborty*

Main category: cs.CV

TL;DR: 提出大规模“素描-照片-指令”三元数据与基于其训练的LVLM（O3SLM），显著提升素描理解与推理，在定位、计数、检索与VQA上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM难以理解抽象/手绘素描，主要因缺乏同时覆盖素描、写实图像与自然语言指令的规模化数据，限制了跨模态对齐与指令跟随能力。

Method: 1) 构建大规模图像-素描-指令三元组数据，覆盖预训练与指令微调需求，并整合QuickDraw!、Sketchy、TU Berlin及新生成的SketchVCL；2) 基于该数据训练一个LVLM（O3SLM），用于素描理解与推理；3) 在多类素描任务上评测（定位、计数、检索SBIR/细粒度SBIR、VQA）。

Result: O3SLM在多个素描基准与任务上取得SOTA，显著优于现有LVLM在素描理解与推理方面的表现。

Conclusion: 构建与利用三元组数据能有效弥补LVLM在抽象素描理解的短板；O3SLM验证了该策略的有效性，为素描到照片与语言的多模态对齐提供可行路径。

Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.

</details>


### [91] [Blur-Robust Detection via Feature Restoration: An End-to-End Framework for Prior-Guided Infrared UAV Target Detection](https://arxiv.org/abs/2511.14371)
*Xiaolin Wang,Houzhang Fang,Qingshan Li,Lu Wang,Yi Chang,Luxin Yan*

Main category: cs.CV

TL;DR: 提出JFD3端到端“去模糊+检测”框架，通过清晰/模糊双分支共享骨干、特征恢复与频域结构引导，以及自监督特征一致性约束，专注提升红外无人机在运动模糊下的可分判特征与检测性能，并发布IRBlurUAV基准；实验显示在实时性下取得更优检测效果。


<details>
  <summary>Details</summary>
Motivation: 红外无人机影像在快速运动下易产生运动模糊，目标与背景对比度降低，导致检测性能显著下降。现有方法多将去模糊作为独立的预处理以提升视觉质量，忽视与检测任务相关的判别特征增强，难以在模糊条件下有效提升检测鲁棒性与准确性。

Method: 构建JFD3端到端联合学习框架：1) 双分支共享权重架构（清晰分支与模糊分支），以清晰分支为“教师”指导模糊分支；2) 轻量特征恢复网络，利用清晰分支特征作为特征级监督，引导模糊分支恢复判别特征；3) 频率结构引导模块，从恢复网络提炼结构先验并注入浅层检测特征以丰富目标结构信息；4) 在两分支检测骨干间施加特征一致性自监督损失，逼近模糊分支与清晰分支的特征表征；5) 构建IRBlurUAV数据集（3万合成+4118真实，涵盖多样运动模糊）并进行训练/评测。

Result: 在IRBlurUAV上进行大量实验，JFD3在保持实时推理效率的同时，实现了显著优于现有方法的检测精度；在多种模糊强度与场景下均表现稳定。

Conclusion: 将去模糊与检测在特征域内深度耦合，利用清晰样本引导、频域结构先验与特征一致性约束，可有效提升模糊红外UAV目标的可分判性与检测性能；JFD3在IRBlurUAV基准上验证了其准确与高效的优势，数据集亦为相关研究提供标准评测平台。

Abstract: Infrared unmanned aerial vehicle (UAV) target images often suffer from motion blur degradation caused by rapid sensor movement, significantly reducing contrast between target and background. Generally, detection performance heavily depends on the discriminative feature representation between target and background. Existing methods typically treat deblurring as a preprocessing step focused on visual quality, while neglecting the enhancement of task-relevant features crucial for detection. Improving feature representation for detection under blur conditions remains challenging. In this paper, we propose a novel Joint Feature-Domain Deblurring and Detection end-to-end framework, dubbed JFD3. We design a dual-branch architecture with shared weights, where the clear branch guides the blurred branch to enhance discriminative feature representation. Specifically, we first introduce a lightweight feature restoration network, where features from the clear branch serve as feature-level supervision to guide the blurred branch, thereby enhancing its distinctive capability for detection. We then propose a frequency structure guidance module that refines the structure prior from the restoration network and integrates it into shallow detection layers to enrich target structural information. Finally, a feature consistency self-supervised loss is imposed between the dual-branch detection backbones, driving the blurred branch to approximate the feature representations of the clear one. Wealso construct a benchmark, named IRBlurUAV, containing 30,000 simulated and 4,118 real infrared UAV target images with diverse motion blur. Extensive experiments on IRBlurUAV demonstrate that JFD3 achieves superior detection performance while maintaining real-time efficiency.

</details>


### [92] [A Quantitative Method for Shoulder Presentation Evaluation in Biometric Identity Documents](https://arxiv.org/abs/2511.14376)
*Alfonso Pedro Ridao*

Main category: cs.CV

TL;DR: 提出一种基于两肩三维关键点的“肩部呈现评估（SPE）”算法，用于身份证件照的肩部方正度自动检测，相关性高、实现轻量、能有效筛除不合规样本。


<details>
  <summary>Details</summary>
Motivation: 证件照标准要求肩部正对相机，但现有自动质检方法缺乏针对“肩部方正（平整）”的可量化评估指标，影响自动化审核与采集合规性。

Method: 仅使用常见姿态估计框架输出的两侧肩部三维坐标，计算肩部的偏航（yaw）与横滚（roll），构建SPE得分衡量肩部方正度；并采用改造的Error-versus-Discard分析评估该指标的过滤能力。

Result: 在121张肖像数据集上，SPE得分与人工标签的皮尔逊相关系数约0.80，显示与人工评估高度一致；E-vs-D分析表明该指标在剔除不合规样本方面有效。

Conclusion: SPE是一个轻量、可集成到采集与登记系统中的自动合规检测工具，可用于识别肩部姿态不合规的证件照样本。

Abstract: International standards for biometric identity documents mandate strict compliance with pose requirements, including the square presentation of a subject's shoulders. However, the literature on automated quality assessment offers few quantitative methods for evaluating this specific attribute. This paper proposes a Shoulder Presentation Evaluation (SPE) algorithm to address this gap. The method quantifies shoulder yaw and roll using only the 3D coordinates of two shoulder landmarks provided by common pose estimation frameworks. The algorithm was evaluated on a dataset of 121 portrait images. The resulting SPE scores demonstrated a strong Pearson correlation (r approx. 0.80) with human-assigned labels. An analysis of the metric's filtering performance, using an adapted Error-versus-Discard methodology, confirmed its utility in identifying non-compliant samples. The proposed algorithm is a viable lightweight tool for automated compliance checking in enrolment systems.

</details>


### [93] [Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving](https://arxiv.org/abs/2511.14386)
*Kangqiao Zhao,Shuo Huai,Xurui Song,Jun Luo*

Main category: cs.CV

TL;DR: 提出首个面向自动驾驶立体匹配的3D纹理化物理对抗样本（PAE），通过全局伪装纹理与立体渲染模块，在多视角下稳定欺骗双目深度估计，并引入“融合攻击”将目标无缝融入环境以提高隐蔽性与杀伤力。


<details>
  <summary>Details</summary>
Motivation: 现有物理对抗多数为2D局部贴片且面向单目感知，难以在双目立体中保持跨视角一致与有效性；双目存在视差几何约束，传统方法难以对齐现实位置与朝向；现有隐藏类攻击难以与背景无缝融合，隐蔽性不足。

Method: 1) 设计3D物理对抗样本，采用全局伪装纹理替代局部2D贴片，保证多视角一致性；2) 提出3D立体匹配渲染模块，将PAE与场景几何对齐，考虑双目视差与相机姿态，实现与真实世界位置/朝向一致；3) 提出“融合攻击”，通过细粒度纹理优化使目标外观与环境无缝融合，提升隐蔽性与攻击成功率。

Result: 在多种立体匹配模型和自动驾驶场景中，所生成的3D PAE能稳定误导模型产生错误深度输出；相较现有隐藏攻击，融合攻击在隐蔽性与破坏性上显著提升。

Conclusion: 立体视觉并非天然对物理对抗鲁棒。通过全局纹理化3D PAE与立体渲染、融合优化，可在双目系统中实现稳定、隐蔽且有效的深度欺骗，提示需要面向双目的专用防御与鲁棒性评估。

Abstract: Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception. Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored. To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving. Our method employs a 3D PAE with global camouflage texture rather than a local 2D patch-based one, ensuring both visual consistency and attack effectiveness across different viewpoints of stereo cameras. To cope with the disparity effect of these cameras, we also propose a new 3D stereo matching rendering module that allows the PAE to be aligned with real-world positions and headings in binocular vision. We further propose a novel merging attack that seamlessly blends the target into the environment through fine-grained PAE optimization. It has significantly enhanced stealth and lethality upon existing hiding attacks that fail to get seamlessly merged into the background. Extensive evaluations show that our PAEs can successfully fool the stereo models into producing erroneous depth information.

</details>


### [94] [Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition](https://arxiv.org/abs/2511.14391)
*Fabian Schmidt,Noushiq Mohammed Kayilan Abdul Nazar,Markus Enzweiler,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出TLS-Assist：在LLM自动驾驶代理前加入交通灯与交通标志检测的模块化冗余层，将检测结果转成结构化自然语言注入LLM输入，显著降低交通违规并提升闭环驾驶表现（相对LMDrive提升至多14%，相对BEVDriver提升7%）。


<details>
  <summary>Details</summary>
Motivation: LLM在自动驾驶规划与决策中具备推理和泛化潜力，但缺乏显式守法机制，且对小而关键目标（信号灯、路牌）感知不稳，导致安全风险与违规。

Method: 设计TLS-Assist为即插即用、与模型无关的冗余感知层：使用专门的交通灯/标志检测器（支持单目与多目），将检测输出转化为结构化自然语言提示（包含状态、类别、位置等），与场景描述一起输入LLM，强制其关注安全关键线索；在CARLA的LangAuto基准上做闭环评测，对比LMDrive与BEVDriver。

Result: 在闭环评测中，TLS-Assist带来显著驾驶性能提升：相对LMDrive最高+14%，相对BEVDriver+7%，并稳定减少交通灯与标志相关违规。

Conclusion: 通过外置、可插拔的交通信号/标志识别并以结构化语言注入LLM，可在不改动LLM核心的情况下提升安全合规与整体驾驶表现；方法通用、可扩展且已开源。

Abstract: Large Language Models (LLMs) are increasingly used for decision-making and planning in autonomous driving, showing promising reasoning capabilities and potential to generalize across diverse traffic situations. However, current LLM-based driving agents lack explicit mechanisms to enforce traffic rules and often struggle to reliably detect small, safety-critical objects such as traffic lights and signs. To address this limitation, we introduce TLS-Assist, a modular redundancy layer that augments LLM-based autonomous driving agents with explicit traffic light and sign recognition. TLS-Assist converts detections into structured natural language messages that are injected into the LLM input, enforcing explicit attention to safety-critical cues. The framework is plug-and-play, model-agnostic, and supports both single-view and multi-view camera setups. We evaluate TLS-Assist in a closed-loop setup on the LangAuto benchmark in CARLA. The results demonstrate relative driving performance improvements of up to 14% over LMDrive and 7% over BEVDriver, while consistently reducing traffic light and sign infractions. We publicly release the code and models on https://github.com/iis-esslingen/TLS-Assist.

</details>


### [95] [BEDLAM2.0: Synthetic Humans and Cameras in Motion](https://arxiv.org/abs/2511.14394)
*Joachim Tesch,Giorgio Becherini,Prerana Achar,Anastasios Yiannakidis,Muhammed Kocabas,Priyanka Patel,Michael J. Black*

Main category: cs.CV

TL;DR: BEDLAM2.0 提供更丰富真实的合成视频数据与标注（人体与相机），显著提升视频中三维人体在世界坐标系下的估计效果，相比原 BEDLAM 有系统性改进。


<details>
  <summary>Details</summary>
Motivation: 现有从视频推断三维人体的方法多在图像坐标系评估，但许多应用（AR/VR、机器人、动作分析）需要世界坐标系的人体与相机联合估计；受限于缺少含真实相机运动的丰富视频及人体/相机真值数据。

Method: 构建并发布新数据集 BEDLAM2.0：引入更丰富真实的相机模型与运动；提升人体形体、动作、衣物、头发、三维环境多样性与逼真度；新增鞋子资产；提供渲染视频、人体参数与相机运动真值，以及可用的3D资产与第三方链接。用该数据训练并评测最先进方法，尤其针对世界坐标估计。

Result: 在与使用 BEDLAM 训练的同类 SOTA 方法对比中，使用 BEDLAM2.0 训练可显著降低误差、提升精度，优势在含相机与人体同时运动、需要世界坐标输出的场景尤为明显。

Conclusion: BEDLAM2.0 成为更适合世界坐标系三维人体与相机联合估计的训练资源，较 BEDLAM 全面提升数据多样性与真实性，带来显著性能增益，并提供开放的渲染视频、标注与资产以促进研究。

Abstract: Inferring 3D human motion from video remains a challenging problem with many applications. While traditional methods estimate the human in image coordinates, many applications require human motion to be estimated in world coordinates. This is particularly challenging when there is both human and camera motion. Progress on this topic has been limited by the lack of rich video data with ground truth human and camera movement. We address this with BEDLAM2.0, a new dataset that goes beyond the popular BEDLAM dataset in important ways. In addition to introducing more diverse and realistic cameras and camera motions, BEDLAM2.0 increases diversity and realism of body shape, motions, clothing, hair, and 3D environments. Additionally, it adds shoes, which were missing in BEDLAM. BEDLAM has become a key resource for training 3D human pose and motion regressors today and we show that BEDLAM2.0 is significantly better, particularly for training methods that estimate humans in world coordinates. We compare state-of-the art methods trained on BEDLAM and BEDLAM2.0, and find that BEDLAM2.0 significantly improves accuracy over BEDLAM. For research purposes, we provide the rendered videos, ground truth body parameters, and camera motions. We also provide the 3D assets to which we have rights and links to those from third parties.

</details>


### [96] [Stage Aware Diagnosis of Diabetic Retinopathy via Ordinal Regression](https://arxiv.org/abs/2511.14398)
*Saksham Kumar,D Sridhar Aditya,T Likhil Kumar,Thulasi Bikku,Srinivasarao Thota,Chandan Kumar*

Main category: cs.CV

TL;DR: 提出一种基于序数回归的糖网病分级模型，在APTOS-2019上用GC提取、噪声掩蔽与CLAHE预处理，QWK达0.8992，创数据集新高。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是可预防致盲主因之一；及时筛查需准确、与临床分级一致的自动化算法，因此需要在标准数据集上提升分级性能与一致性。

Method: 在APTOS-2019眼底图像上，采用预处理组合（绿色通道提取、噪声掩蔽、CLAHE）增强病灶可见性；使用序数回归框架进行DR严重度分级；以二次加权Kappa（QWK）评估与临床分级的一致性。

Result: 所提序数回归方法在APTOS上获得QWK=0.8992，优于既有方法，报告为新的基线/基准。

Conclusion: 在标准预处理配合序数回归建模下，DR分级与临床一致性显著提升；该框架在APTOS上达SOTA，说明序数信息建模对多级医学分级任务有效。

Abstract: Diabetic Retinopathy (DR) has emerged as a major cause of preventable blindness in recent times. With timely screening and intervention, the condition can be prevented from causing irreversible damage. The work introduces a state-of-the-art Ordinal Regression-based DR Detection framework that uses the APTOS-2019 fundus image dataset. A widely accepted combination of preprocessing methods: Green Channel (GC) Extraction, Noise Masking, and CLAHE, was used to isolate the most relevant features for DR classification. Model performance was evaluated using the Quadratic Weighted Kappa, with a focus on agreement between results and clinical grading. Our Ordinal Regression approach attained a QWK score of 0.8992, setting a new benchmark on the APTOS dataset.

</details>


### [97] [Language as an Anchor: Preserving Relative Visual Geometry for Domain Incremental Learning](https://arxiv.org/abs/2511.14401)
*Shuyi Geng,Tao Zhou,Yi Zhou*

Main category: cs.CV

TL;DR: 提出LAVA框架，用文本锚定的相对几何对齐来解决DIL中跨域对齐与遗忘的矛盾，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DIL需要在分布持续漂移下学习而不遗忘。但把所有域映射到单一视觉空间会产生跨域干扰与语义扭曲；而完全域专属参数又会形成“知识孤岛”，阻碍重用并加剧遗忘。需要一种既能共享语义又避免硬对齐失真的机制。

Method: 提出Language-Anchored Visual Alignment（LAVA）。不用直接对齐各域特征，而是以文本（类名）为参考锚，用类名之间的语义相似度构造目标的“相对几何结构”（即保持类间成对相似度关系）。训练时引导每个新域的视觉表示去镜像该相对几何，从而跨域共享基于语言先验的、类感知的结构；实现跨域检索与聚合而不过度耦合域特征。

Result: 在标准DIL基准上广泛实验，LAVA显著超过当前SOTA，表现出更强的跨域泛化、知识保留与特征聚合能力。

Conclusion: 以语言为锚的相对对齐可在不牺牲语义与不形成知识孤岛的前提下统一各域表示，有效缓解DIL中的干扰与遗忘，提供了稳健的跨域知识桥梁。

Abstract: A key challenge in Domain Incremental Learning (DIL) is to continually learn under shifting distributions while preserving knowledge from previous domains. Existing methods face a fundamental dilemma. On one hand, projecting all domains into a single unified visual space leads to inter-domain interference and semantic distortion, as large shifts may vary with not only visual appearance but also underlying semantics. On the other hand, isolating domain-specific parameters causes knowledge fragmentation, creating "knowledge islands" that hamper knowledge reuse and exacerbate forgetting. To address this issue, we propose LAVA (Language-Anchored Visual Alignment), a novel DIL framework that replaces direct feature alignment with relative alignment driven by a text-based reference anchor. LAVA guides the visual representations of each incoming domain to preserve a consistent relative geometry, which is defined by mirroring the pairwise semantic similarities between the class names. This anchored geometric structure acts as a bridge across domains, enabling the retrieval of class-aware prior knowledge and facilitating robust feature aggregation. Extensive experiments on standard DIL benchmarks demonstrate that LAVA achieves significant performance improvements over state-of-the-arts. Code is available at https://github.com/ShuyiGeng/LAVA.

</details>


### [98] [Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays](https://arxiv.org/abs/2511.14411)
*Ravi Shankar Prasad,Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: 提出Cranio-ID：先用YOLO‑pose在2D颅骨X光与对应人脸光学图像上自动标注颅面点，再将两模态的标注构成图，用跨注意力+最优传输做跨模态匹配；在S2F与相似的CUHK数据上实验，可靠性与准确性显著提升，并可泛化到跨域的颅骨-人脸与素描-人脸法医匹配。


<details>
  <summary>Details</summary>
Motivation: 颅面鉴别与生物医学中需要精准的颅测标志点。人工标注费时且需专家；现有基于叠合法和深度学习自动标注的方法缺乏大规模验证，稳定性与可信度不足。因此需要一个既能自动标注又能在不同模态间稳健匹配的框架，并通过系统实验验证其可靠性。

Method: 两阶段框架Cranio-ID：1) 使用训练好的YOLO-pose模型在2D颅骨X光与对应的人脸光学图像上自动检测/标注关键点；2) 将关键点构造成图表示，利用跨注意力机制获取语义对应关系，并结合最优传输求解两模态图之间的匹配，实现跨模态关联。

Result: 在S2F和与其相似的CUHK数据集上进行了大量实验，显示该方法在可靠性和精度方面显著优于现有方法，同时在跨域任务（颅骨对人脸、素描对人脸）中也表现有效。

Conclusion: Cranio-ID能有效实现跨模态的颅骨-人脸关键点自动标注与匹配，经过实验验证在可靠性与准确性上有明显提升，并具有跨域应用潜力，适用于法医鉴定等场景。

Abstract: In forensic craniofacial identification and in many biomedical applications, craniometric landmarks are important. Traditional methods for locating landmarks are time-consuming and require specialized knowledge and expertise. Current methods utilize superimposition and deep learning-based methods that employ automatic annotation of landmarks. However, these methods are not reliable due to insufficient large-scale validation studies. In this paper, we proposed a novel framework Cranio-ID: First, an automatic annotation of landmarks on 2D skulls (which are X-ray scans of faces) with their respective optical images using our trained YOLO-pose models. Second, cross-modal matching by formulating these landmarks into graph representations and then finding semantic correspondence between graphs of these two modalities using cross-attention and optimal transport framework. Our proposed framework is validated on the S2F and CUHK datasets (CUHK dataset resembles with S2F dataset). Extensive experiments have been conducted to evaluate the performance of our proposed framework, which demonstrates significant improvements in both reliability and accuracy, as well as its effectiveness in cross-domain skull-to-face and sketch-to-face matching in forensic science.

</details>


### [99] [Learning to See Through a Baby's Eyes: Early Visual Diets Enable Robust Visual Intelligence in Humans and Machines](https://arxiv.org/abs/2511.14440)
*Yusen Cai,Bhargava Satya Nunna,Qing Lin,Mengmi Zhang*

Main category: cs.CV

TL;DR: 论文提出模拟婴儿早期“低清、去色、连续”的视觉发育过程（CATDiet：从灰到彩、从模糊到清晰、保持时序连续），用自监督在物体中心视频上训练模型；在10个数据集上的识别鲁棒性、形状纹理冲突、剪影、深度和“视觉悬崖”等测试中显著提升，并呈现与生物发育一致的表征与行为。进一步提出CombDiet：先用CATDiet初始化再进行常规SSL训练，继续保留时序连续，跨域表现更佳。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿早期视觉并非一次性成熟，而是经历模糊→清晰、灰度→彩色、时间连续的渐进过程。作者猜测这种“分阶段视觉饮食”可能是形成鲁棒视觉表征的关键启发，想检验将其引入机器自监督学习是否能提升鲁棒性、泛化和与生物对齐的特性。

Method: 设计CATDiet训练范式：在物体中心视频上进行自监督学习，同时施加三类限制——C（灰度到彩色渐进）、A（模糊到清晰渐进）、T（保留时间连续性），并构建覆盖10个数据集的综合基准。进一步提出CombDiet：先用CATDiet预训练初始化，再在保持时间连续性的前提下进行标准SSL训练；还使用头戴式婴儿视频与物体中心视频作为训练源。

Result: 所有CATDiet变体在仅用物体中心视频训练的情况下，都在干净与腐化的图像识别、形状-纹理冲突、剪影识别、深度顺序分类和视觉悬崖范式上表现更鲁棒。模型展现与生物发育对齐的趋势：如与猕猴V1突触密度变化相似的“可塑性”轨迹，以及类似婴儿视觉悬崖的行为。使用CombDiet在物体中心或婴儿头戴视频上训练，可在域内与跨域的物体识别与深度感知上超越标准SSL。

Conclusion: 婴儿早期视觉经验的渐进式发展可作为“逆向工程”鲁棒视觉智能的有效框架。通过CATDiet与CombDiet，模拟早期视觉限制和时序连续性能提升自监督表征的鲁棒性、泛化和生物对齐性；代码与数据将开源。

Abstract: Newborns perceive the world with low-acuity, color-degraded, and temporally continuous vision, which gradually sharpens as infants develop. To explore the ecological advantages of such staged "visual diets", we train self-supervised learning (SSL) models on object-centric videos under constraints that simulate infant vision: grayscale-to-color (C), blur-to-sharp (A), and preserved temporal continuity (T)-collectively termed CATDiet. For evaluation, we establish a comprehensive benchmark across ten datasets, covering clean and corrupted image recognition, texture-shape cue conflict tests, silhouette recognition, depth-order classification, and the visual cliff paradigm. All CATDiet variants demonstrate enhanced robustness in object recognition, despite being trained solely on object-centric videos. Remarkably, models also exhibit biologically aligned developmental patterns, including neural plasticity changes mirroring synaptic density in macaque V1 and behaviors resembling infants' visual cliff responses. Building on these insights, CombDiet initializes SSL with CATDiet before standard training while preserving temporal continuity. Trained on object-centric or head-mounted infant videos, CombDiet outperforms standard SSL on both in-domain and out-of-domain object recognition and depth perception. Together, these results suggest that the developmental progression of early infant visual experience offers a powerful reverse-engineering framework for understanding the emergence of robust visual intelligence in machines. All code, data, and models will be publicly released.

</details>


### [100] [Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding](https://arxiv.org/abs/2511.14446)
*Hong Gao,Yiming Bao,Xuezhen Tu,Yutong Xu,Yue Jin,Yiyang Mu,Bin Zhong,Linan Yue,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 提出AVI，一个无需训练的代理式视频智能框架，通过三阶段推理、结构化知识库与开源模型集成，在多项长视频基准上取得有竞争力的表现并具高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM对视频多为单次前向、难以反复查证与迭代推理；新兴代理方法要么依赖昂贵的专有大模型，要么需大量强化学习训练，限制了可用性与成本效益。

Method: 设计训练无关的系统级代理框架AVI：1) 人类启发的三阶段推理流程Retrieve-Perceive-Review，兼顾全局探索与局部精析；2) 以实体图为核心的结构化视频知识库，配合多粒度工具，构建交互环境；3) 开源模型集成，将具推理能力的LLM与轻量级CV基础模型和VLM组合，避免专有API与RL训练。

Result: 在LVBench、VideoMME-Long、LongVideoBench、Charades-STA等基准上实现与SOTA相近或有竞争力的成绩，同时展现更强的可解释性。

Conclusion: AVI以系统设计替代重训练与私有依赖，实现可复现、成本友好、可解释的长视频理解与推理，为视频智能的代理化路径提供实用范式。

Abstract: Video understanding requires not only visual recognition but also complex reasoning. While Vision-Language Models (VLMs) demonstrate impressive capabilities, they typically process videos largely in a single-pass manner with limited support for evidence revisit and iterative refinement. While recently emerging agent-based methods enable long-horizon reasoning, they either depend heavily on expensive proprietary models or require extensive agentic RL training. To overcome these limitations, we propose Agentic Video Intelligence (AVI), a flexible and training-free framework that can mirror human video comprehension through system-level design and optimization. AVI introduces three key innovations: (1) a human-inspired three-phase reasoning process (Retrieve-Perceive-Review) that ensures both sufficient global exploration and focused local analysis, (2) a structured video knowledge base organized through entity graphs, along with multi-granularity integrated tools, constituting the agent's interaction environment, and (3) an open-source model ensemble combining reasoning LLMs with lightweight base CV models and VLM, eliminating dependence on proprietary APIs or RL training. Experiments on LVBench, VideoMME-Long, LongVideoBench, and Charades-STA demonstrate that AVI achieves competitive performance while offering superior interpretability.

</details>


### [101] [DIR-TIR: Dialog-Iterative Refinement for Text-to-Image Retrieval](https://arxiv.org/abs/2511.14449)
*Zongwei Zhen,Biqing Zeng*

Main category: cs.CV

TL;DR: 提出DIR-TIR用于交互式文本到图像检索，通过对话与图像双重精炼逐步逼近目标图像，显著提升命中率与交互体验。


<details>
  <summary>Details</summary>
Motivation: 单轮、一次性描述的检索对用户表达不完整与歧义不鲁棒，缺乏可控性与纠错能力；需要一种能在多轮交互中主动澄清需求、弥补语义-视觉差距的框架。

Method: 设计两大模块协同迭代：1) Dialog Refiner主动向用户提问，抽取关键信息，生成更精确的目标图像描述；2) Image Refiner比较生成/候选图像与用户意图，识别感知差距并据此缩小视觉-语义偏差；在多轮对话中逐步收敛检索结果。

Result: 在多种图像数据集上，相较仅用初始描述的基线方法，DIR-TIR显著提升目标命中率与检索精度；两模块协同带来更佳可控性、容错性与交互体验。

Conclusion: 多轮对话驱动的双精炼框架能有效弥合用户意图与图像检索之间的鸿沟，优于单查询范式，并具备更强的可控与容错能力。

Abstract: This paper addresses the task of interactive, conversational text-to-image retrieval.
  Our DIR-TIR framework progressively refines the target image search through two specialized modules: the Dialog Refiner Module and the Image Refiner Module.
  The Dialog Refiner actively queries users to extract essential information and generate increasingly precise descriptions of the target image.
  Complementarily, the Image Refiner identifies perceptual gaps between generated images and user intentions, strategically reducing the visual-semantic discrepancy. By leveraging multi-turn dialogues, DIR-TIR provides superior controllability and fault tolerance compared to conventional single-query methods, significantly improving target image hit accuracy.
  Comprehensive experiments across diverse image datasets demonstrate our dialogue-based approach substantially outperforms initial-description-only baselines, while the synergistic module integration achieves both higher retrieval precision and enhanced interactive experience.

</details>


### [102] [CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring](https://arxiv.org/abs/2511.14469)
*Mingchen Zhong,Xin Lu,Dong Li,Senyan Xu,Ruixuan Jiang,Xueyang Fu,Baocai Yin*

Main category: cs.CV

TL;DR: 提出CompEvent：一种复数值神经网络，用事件相机与RGB视频进行全流程融合，解决低照度与运动模糊的联合退化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 夜间监控、自动驾驶等低照场景中，长曝光与弱光导致视频严重模糊。事件相机具有高时间分辨率与弱光优势，但现有事件-RGB融合多为分阶段处理，难以同时应对低照与运动模糊的耦合退化，融合不连续、互补性未被充分挖掘。

Method: 构建CompEvent复数值深度框架，进行事件数据与RGB帧的“全流程”时空融合：1) 复数时序对齐GRU（Complex Temporal Alignment GRU）：用复数卷积，并以GRU迭代处理视频与事件流，实现跨模态时序对齐与连续融合；2) 复数空-频学习模块（Complex Space-Frequency Learning）：在空间与频域进行统一的复数信号处理，通过空间结构与系统级特性进行深度融合。整体利用复数神经网络的整体表征能力，实现时空一致的联合恢复。

Result: 在低照视频去模糊任务上，CompEvent在多项实验中超过SOTA方法，表现出更强的弱光与运动模糊联合恢复能力；代码已开源。

Conclusion: 复数值网络的整体表征与空-频联合建模，使事件与RGB的互补信息在时空全流程中被充分利用，从而显著提升低照视频去模糊效果，优于分阶段融合策略。

Abstract: Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task. The code is available at https://github.com/YuXie1/CompEvent.

</details>


### [103] [Learning Subglacial Bed Topography from Sparse Radar with Physics-Guided Residuals](https://arxiv.org/abs/2511.14473)
*Bayu Adhi Tama,Jianwu Wang,Vandana Janeja,Mostafa Cham*

Main category: cs.CV

TL;DR: 提出一种物理引导的残差学习框架，从稀疏雷达与表面观测重建冰下床面：在BedMachine先验之上预测厚度残差，结合多项轻量物理与数据项，配合泄漏安全的分块留出评估，在格陵兰两子区优于多种基线并具高结构一致性与可泛化性。


<details>
  <summary>Details</summary>
Motivation: 冰盖数值模拟亟需精确的冰下地形，但雷达测量稀疏且分布不均，直接学习易过拟合且缺乏物理一致性。需要一种既能利用有限观测、又能注入冰流物理并对域移位稳健的方法。

Method: 采用“先验+残差”的策略：以BedMachine床面为先验，网络预测厚度残差。架构为标准编码器（如ResNet-50）+DeepLabV3+解码器。损失由轻量物理与数据项构成：多尺度质量守恒、沿流向的总变分正则、拉普拉斯抑制（平滑）、厚度非负约束、随训练逐步增强的先验一致性项、以及带掩膜与置信图调制的Huber雷达拟合项。评估采用泄漏安全的纵/横向分块留出并设置缓冲区，只在被遮蔽核心区报告指标。

Result: 在格陵兰两处子区域上，相比U-Net、Attention U-Net、FPN和普通CNN，本方法在测试核心区取得更高精度与结构保真度，重建的床面更连贯、物理可行。

Conclusion: 物理引导的残差-先验设计能在稀疏、非均匀雷达数据和域移位情况下产生空间连贯、物理合理的冰下床面重建，具备面向业务化制图的潜力。

Abstract: Accurate subglacial bed topography is essential for ice sheet modeling, yet radar observations are sparse and uneven. We propose a physics-guided residual learning framework that predicts bed thickness residuals over a BedMachine prior and reconstructs bed from the observed surface. A DeepLabV3+ decoder over a standard encoder (e.g.,ResNet-50) is trained with lightweight physics and data terms: multi-scale mass conservation, flow-aligned total variation, Laplacian damping, non-negativity of thickness, a ramped prior-consistency term, and a masked Huber fit to radar picks modulated by a confidence map. To measure real-world generalization, we adopt leakage-safe blockwise hold-outs (vertical/horizontal) with safety buffers and report metrics only on held-out cores. Across two Greenland sub-regions, our approach achieves strong test-core accuracy and high structural fidelity, outperforming U-Net, Attention U-Net, FPN, and a plain CNN. The residual-over-prior design, combined with physics, yields spatially coherent, physically plausible beds suitable for operational mapping under domain shift.

</details>


### [104] [2D Gaussians Spatial Transport for Point-supervised Density Regression](https://arxiv.org/abs/2511.14477)
*Miao Shang,Xiaopeng Hong*

Main category: cs.CV

TL;DR: 提出Gaussian Spatial Transport (GST)，用高斯splatting将图像坐标空间的概率测度高效搬运到标注图上，并以此指导训练，避免传统最优传输在训练中反复求计划。


<details>
  <summary>Details</summary>
Motivation: 许多视觉任务（如人群计数、关键点/地标检测）需要将像素分布与稀疏或密集标注对齐。传统最优传输方法计算代价高、训练时需反复迭代求解运输计划，效率低且难以无缝融入标准网络优化。作者希望用一种既能刻画像素-标注对应关系又高效可微的机制替代。

Method: 以高斯splatting估计像素到标注的对应：在图像坐标空间对像素概率（或特征）进行高斯化/投影至标注域，构造基于贝叶斯概率的运输计划（无需OT迭代求解）。进一步推导一个可与常规模型联合训练的传输后差异损失，将运输计划融入端到端优化。

Result: 在人群计数与地标检测上进行大量实验，GST在精度与训练效率上优于或匹配传统最优传输基线；显著减少训练阶段的运输计划迭代计算时间。

Conclusion: GST提供了一种基于高斯splatting的高效可微“概率搬运”框架，能替代传统OT在像素-标注对齐中的角色，既提升效率又保持/提升性能，适用于多种视觉任务。

Abstract: This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.

</details>


### [105] [Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation](https://arxiv.org/abs/2511.14481)
*Aditi Agarwal,Anjali Jain,Nikita Saxena,Ishan Deshpande,Michal Kazmierski,Abigail Annkah,Nadav Sherman,Karthikeyan Shanmugam,Alok Talekar,Vaibhav Rajan*

Main category: cs.CV

TL;DR: 提出SEED-SR：利用条件潜在扩散与多源多光谱基础模型，在“分割感知的潜变量空间”而非像素空间做参考超分，直接输出农田分割，支持20×尺度，在两大真实数据上分割指标相对提升至多25.5%（实例）与12.9%（语义）。


<details>
  <summary>Details</summary>
Motivation: 小农田边界划分需要高分影像但重访率低，低分影像虽频密但分辨率不足。现有参考超分方法追求感知质量，模糊关键边界特征，且难以满足高放大倍率；两阶段（SR后分割）难以充分融合多源卫星数据。需要一种既能用高频低分数据，又能保留分割所需几何细节、并支持大倍率的方案。

Method: 提出SEED-SR：以条件潜在扩散模型为核心，引入大规模多源、多光谱地理空间基础模型作条件/先验，将高频LR与稀疏HR参考共同编码到“分割感知”的潜空间，在该空间执行参考超分并直接生成高分辨率分割图，而非恢复HR像素。实现20×尺度的Ref-SR到分割映射。

Result: 在两套大型真实数据集上，相比基于SOTA参考超分的两阶段基线，实例分割与语义分割分别获得最高25.5%与12.9%的相对提升；能稳定在20×尺度输出高质量分割。

Conclusion: 绕开像素级SR、转向分割感知潜空间的Ref-SR可显著提升小农田边界分割，尤其在高倍率与多源遥感条件下有效；为季内监测提供可行途径并优于传统两阶段流程。

Abstract: Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images -- having higher revisit frequency (e.g., weekly) -- using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\textbf{25.5}$ and $\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.

</details>


### [106] [Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM](https://arxiv.org/abs/2511.14499)
*Jack Qin,Zhitao Wang,Yinan Zheng,Keyu Chen,Yang Zhou,Yuanxin Zhong,Siyuan Cheng*

Main category: cs.CV

TL;DR: 提出Risk Semantic Distillation（RSD），用VLM蒸馏“风险注意”到E2E自动驾驶BEV骨干中，以提升泛化、感知与规划，且可解释、计算更可控。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶在复杂/新奇场景与传感器配置下泛化不足。用VLM做少样本/零样本虽有效，但形成“混合AD”导致规划不一致；VLA端到端直接出控制又算力开销大。需要一种既利用VLM的语义与推理能力、又能保持单一E2E规划一致性和计算可控的方法。

Method: 提出RSD框架：设计可插拔的RiskHead模块，将VLM产生的因果“风险估计/注意”蒸馏到BEV特征中，生成可解释的风险注意图。训练时通过风险引导，使BEV特征学习对关键目标、空间边界与高风险实体的细粒度表征，从而提升E2E骨干的感知与规划。

Result: 在Bench2Drive基准上，采用RSD的模型在复杂与不可预测驾驶条件下表现更优，感知与规划指标均显著提升。

Conclusion: 用VLM的风险语义蒸馏强化E2E AD的BEV表征，可在不引入混合系统不一致性、且低于VLA算力成本的前提下，提高泛化能力与可解释性，贴近人类驾驶关注风险的行为。

Abstract: The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.

</details>


### [107] [Parameter Aware Mamba Model for Multi-task Dense Prediction](https://arxiv.org/abs/2511.14503)
*Xinzhuo Yu,Yunzhi Zhuge,Sitong Gong,Lu Zhang,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出PAMM：用状态空间模型（S4）与参数专家推进多任务致密预测中的任务交互；配合多方向Hilbert扫描构造序列；在NYUD-v2与PASCAL-Context上验证有效。


<details>
  <summary>Details</summary>
Motivation: 多任务致密预测需要有效建模任务间关系。现有多用卷积/注意力或Transformer整体关系，存在交互粒度粗、可扩展性与效率受限的问题。作者希望利用状态空间模型的参数可扩展性与全局建模能力，精细注入任务先验并提升任务交互。

Method: 提出解码器式框架PAMM：1）引入双状态空间参数专家（dual state space parameter experts），为不同任务设定并融合任务特定参数先验；2）通过结构化状态空间序列模型S4实现全局整合任务先验与跨任务交互；3）采用多方向Hilbert扫描，将2D特征映射为多角度序列以增强S4对图像的感知；4）用于多任务致密预测的解码。

Result: 在NYUD-v2与PASCAL-Context两个基准上进行大量实验，显示PAMM优于或至少不劣于现有方法（摘要未给具体数值），验证其有效性。代码已开源。

Conclusion: 状态空间驱动的解码器与参数专家可有效刻画和融合多任务先验，提升致密预测中的任务交互；多方向Hilbert扫描进一步增强2D到序列建模的适配性。PAMM在主流基准上表现良好并具实用价值。

Abstract: Understanding the inter-relations and interactions between tasks is crucial for multi-task dense prediction. Existing methods predominantly utilize convolutional layers and attention mechanisms to explore task-level interactions. In this work, we introduce a novel decoder-based framework, Parameter Aware Mamba Model (PAMM), specifically designed for dense prediction in multi-task learning setting. Distinct from approaches that employ Transformers to model holistic task relationships, PAMM leverages the rich, scalable parameters of state space models to enhance task interconnectivity. It features dual state space parameter experts that integrate and set task-specific parameter priors, capturing the intrinsic properties of each task. This approach not only facilitates precise multi-task interactions but also allows for the global integration of task priors through the structured state space sequence model (S4). Furthermore, we employ the Multi-Directional Hilbert Scanning method to construct multi-angle feature sequences, thereby enhancing the sequence model's perceptual capabilities for 2D data. Extensive experiments on the NYUD-v2 and PASCAL-Context benchmarks demonstrate the effectiveness of our proposed method. Our code is available at https://github.com/CQC-gogopro/PAMM.

</details>


### [108] [D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images](https://arxiv.org/abs/2511.14518)
*Taifour Yousra Nabila,Azeddine Beghdadi,Marie Luong,Zuheng Ming,Habib Zaidi,Faouzi Alaya Cheikh*

Main category: cs.CV

TL;DR: 提出D-PerceptCT：结合语义先验与全局-局部状态空间建模，并配合人眼对比敏感性启发的感知损失，提升低剂量CT的结构与纹理保真，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LDCT为降低辐射剂量而牺牲图像质量，现有增强方法常过度平滑、噪声估计过高，损伤关键解剖细节与病灶可见性；需要一种更符合人类视觉感知、兼顾语义与细节的增强方案，以提升对诊断相关特征的可见性。

Method: 1) 架构D-PerceptCT由两大模块组成：a) ViDex视觉双路径特征提取器，将预训练DINOv2的高层语义先验与局部空间特征融合，实现语义感知的增强；b) 全局-局部State-Space模块，建模长程依赖与多尺度信息，保留重要结构与细微纹理。2) 训练时引入深度感知相关性损失DPRLF，受人眼对比敏感函数启发，强化感知上重要的特征与频段。

Result: 在Mayo2016数据集上进行大量实验，D-PerceptCT在结构与纹理保真、关键解剖与病灶细节的可见性方面优于现有SOTA方法（定性与定量评估均获改进）。

Conclusion: 人类视觉启发的语义-结构联合建模与感知损失可有效提升LDCT增强质量，减少过度平滑并更好保留诊断相关细节，具备临床应用潜力。

Abstract: Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks. However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development. While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details. In this paper, we introduce D-PerceptCT, a novel architecture inspired by key principles of the Human Visual System (HVS) to enhance LDCT images. The objective is to guide the model to enhance or preserve perceptually relevant features, thereby providing radiologists with CT images where critical anatomical structures and fine pathological details are perceptu- ally visible. D-PerceptCT consists of two main blocks: 1) a Visual Dual-path Extractor (ViDex), which integrates semantic priors from a pretrained DINOv2 model with local spatial features, allowing the network to incorporate semantic-awareness during enhancement; (2) a Global-Local State-Space block that captures long-range information and multiscale features to preserve the important structures and fine details for diagnosis. In addition, we propose a novel deep perceptual loss, designated as the Deep Perceptual Relevancy Loss Function (DPRLF), which is inspired by human contrast sensitivity, to further emphasize perceptually important features. Extensive experiments on the Mayo2016 dataset demonstrate the effectiveness of D-PerceptCT method for LDCT enhancement, showing better preservation of structural and textural information within LDCT images compared to SOTA methods.

</details>


### [109] [A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement](https://arxiv.org/abs/2511.14521)
*Yufeng Tian,Yifan Chen,Zhe Sun,Libang Chen,Mingyu Dou,Jijun Lu,Ye Zheng,Xuelong Li*

Main category: cs.CV

TL;DR: 提出以陆地自然图像为清晰参考，通过无配对图像到图像翻译合成6类水下退化数据，构建大规模、具真实监督的训练集，显著提升水下图像复原与增强的色彩还原与泛化性能，并在多架构与多测试集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像复原/增强深度学习方法受限于高质量成对数据稀缺；水下场景难以获取真实无退化的参考标签，现有基准多以人工挑选的增强结果充当“真值”，存在全局色彩不一致和监督不真实的问题，导致模型在色彩复原、细节增强与泛化上受限。

Method: 以“空气中自然图像”为明确无歧义的干净参考目标，利用无配对图像到图像翻译框架，将其转化为具有6种代表性水下退化类型的合成图像，形成大规模合成数据；该生成式数据框架为每个退化样本提供精确的地面真值（对应的清晰空气图），从而学习从水下退化到清晰场景外观的映射；在6种典型网络架构与3个独立测试集上进行训练与评测。

Result: 基于该合成数据训练的模型在色彩还原与泛化性能上与使用现有基准训练的模型相当或更优；定量与定性实验均验证了优势，跨多架构、多测试集保持稳健表现。

Conclusion: 通过以空气图像为真值、无配对翻译合成水下退化数据的方式，可获得真实而可扩展的监督信号，缓解成对数据缺乏的问题，可靠提升水下图像复原/增强性能；数据集已公开供社区使用。

Abstract: Underwater image restoration and enhancement are crucial for correcting color distortion and restoring image details, thereby establishing a fundamental basis for subsequent underwater visual tasks. However, current deep learning methodologies in this area are frequently constrained by the scarcity of high-quality paired datasets. Since it is difficult to obtain pristine reference labels in underwater scenes, existing benchmarks often rely on manually selected results from enhancement algorithms, providing debatable reference images that lack globally consistent color and authentic supervision. This limits the model's capabilities in color restoration, image enhancement, and generalization. To overcome this limitation, we propose using in-air natural images as unambiguous reference targets and translating them into underwater-degraded versions, thereby constructing synthetic datasets that provide authentic supervision signals for model learning. Specifically, we establish a generative data framework based on unpaired image-to-image translation, producing a large-scale dataset that covers 6 representative underwater degradation types. The framework constructs synthetic datasets with precise ground-truth labels, which facilitate the learning of an accurate mapping from degraded underwater images to their pristine scene appearances. Extensive quantitative and qualitative experiments across 6 representative network architectures and 3 independent test sets show that models trained on our synthetic data achieve comparable or superior color restoration and generalization performance to those trained on existing benchmarks. This research provides a reliable and scalable data-driven solution for underwater image restoration and enhancement. The generated dataset is publicly available at: https://github.com/yftian2025/SynUIEDatasets.git.

</details>


### [110] [DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation](https://arxiv.org/abs/2511.14530)
*Xiangchen Yin,Jiahui Yuan,Zhangchi Hu,Wenzhang Sun,Jie Chen,Xiaozhen Qiao,Hao Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出DeCo-VAE，通过将视频内容显式解耦为关键帧、运动与残差，学习更紧凑潜变量并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频VAE忽视帧间内容相似性，直接对像素编码导致冗余潜变量，难以高效、稳定地同时建模静态与动态信息。

Method: 将视频内容显式分解为三部分：关键帧（静态场景/外观）、运动（时序变化）、残差（难以解释的细节）。为每个组件设计专用编码器，避免跨组件干扰；使用共享的3D解码器统一重建以保持时空一致性。训练上采用“解耦式自适应”：分阶段冻结部分编码器、交替训练其他编码器，以稳定优化并分别精准学习静态与动态特征。

Result: 在多项定量与定性实验中，DeCo-VAE取得更优的视频重建表现，相比基线在保真度与时空一致性上都有显著提升。

Conclusion: 通过对视频内容进行组件级解耦并配合专用编码器与共享3D解码器，以及分阶段冻结/训练策略，能实现更紧凑的潜变量表示并提升重建质量。

Abstract: Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.

</details>


### [111] [Learning Compact Latent Space for Representing Neural Signed Distance Functions with High-fidelity Geometry Details](https://arxiv.org/abs/2511.14539)
*Qiang Bai,Bojian Wu,Xi Yang,Zhizhong Han*

Main category: cs.CV

TL;DR: 提出一种在共同空间表示多个神经SDF的方法，结合泛化式与过拟合式学习优势，并配以新采样策略，从而以更紧凑潜码恢复更高保真几何并提升训练效率、减少伪影，在基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单一SDF隐式表示效果好，但在多形状/多场景时，潜空间信息容量有限、细节丢失，难以兼顾高保真几何与紧凑潜码；需要一种能在共享空间高质量表示多SDF的方案。

Method: 构建一个可同时容纳多个SDF的公共表示空间：融合“可泛化”的元/条件式网络与“可过拟合”的实例细化机制，既保持紧凑潜码，又保留高频几何细节；并提出针对训练查询的采样策略，抑制不同SDF间相互干扰、提升效率与稳定性。

Result: 在常用基准上进行数值与可视化评估，方法在表达能力（高保真重建）与潜码紧凑性上均优于最新方法，同时训练效率更高、伪影更少。

Conclusion: 融合泛化与过拟合式学习并辅以新采样，能在共享空间中高效表示多个SDF，实现更紧凑潜表示与更高几何保真，验证了该框架在多SDF建模上的优势。

Abstract: Neural signed distance functions (SDFs) have been a vital representation to represent 3D shapes or scenes with neural networks. An SDF is an implicit function that can query signed distances at specific coordinates for recovering a 3D surface. Although implicit functions work well on a single shape or scene, they pose obstacles when analyzing multiple SDFs with high-fidelity geometry details, due to the limited information encoded in the latent space for SDFs and the loss of geometry details. To overcome these obstacles, we introduce a method to represent multiple SDFs in a common space, aiming to recover more high-fidelity geometry details with more compact latent representations. Our key idea is to take full advantage of the benefits of generalization-based and overfitting-based learning strategies, which manage to preserve high-fidelity geometry details with compact latent codes. Based on this framework, we also introduce a novel sampling strategy to sample training queries. The sampling can improve the training efficiency and eliminate artifacts caused by the influence of other SDFs. We report numerical and visual evaluations on widely used benchmarks to validate our designs and show advantages over the latest methods in terms of the representative ability and compactness.

</details>


### [112] [Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction](https://arxiv.org/abs/2511.14540)
*Hao Tian,Chenyangguang Zhang,Rui Liu,Wen Shen,Xiaolin Qin*

Main category: cs.CV

TL;DR: 提出一种无先验的手-物交互动态3D高斯点渲重建方法，通过交互感知的高斯与动态场实现几何与外观共同建模，并以渐进优化与多重正则提升稳定性与真实感，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D Gaussian Splatting 方法在手-物强耦合场景中易受互相遮挡、边缘模糊与复杂非刚性运动影响，且常依赖物体先验或难以同时稳定优化几何与外观。因此需要一种无需对象先验、能处理互遮挡与细节边缘、并能稳健拟合手与物体紧密交互形变的统一模型与优化框架。

Method: 1) 交互感知手-物高斯：在动态3D-GS框架中为高斯引入可优化的新参数，采用分段线性假设以获得更清晰的结构表示，缓解互遮挡与边缘模糊；2) 交互感知动态场：将手部信息注入到物体的形变场中，利用手与物形状的互补与贴合关系建模柔性运动；3) 渐进式优化策略：先分离并逐步优化动态区域与静态背景；4) 显式正则化：对运动平滑、物理交互一致性与光照一致性进行约束，稳定手-物表示。

Result: 在手-物交互重建任务上，方法在多个基于动态3D-GS的基线之上取得显著提升，达到当前最优的几何与外观重建质量，并在运动过渡平滑性与交互真实性上表现更好。

Conclusion: 交互感知的高斯表示与动态场结合渐进优化与正则化，能在无物体先验条件下有效重建手-物交互的动态几何与外观，并以更高稳定性和真实感达成SOTA性能。

Abstract: This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.

</details>


### [113] [ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection](https://arxiv.org/abs/2511.14554)
*Mohammad Romani*

Main category: cs.CV

TL;DR: 提出ForensicFlow：融合RGB、纹理与频域三模态证据的Deepfake视频取证框架，结合时序注意力与自适应分支融合，在Celeb-DF(v2)上显著优于单流基线。


<details>
  <summary>Details</summary>
Motivation: 单一CNN难以同时捕获空间、纹理与频率多尺度伪造迹象，导致鲁棒性与泛化性不足；Deepfake对信息安全构成威胁，急需更强健的检测方法。

Method: 三分支架构：RGB分支用ConvNeXt-tiny提取全局视觉不一致；纹理分支用Swin-T检测精细融合伪迹；频域分支用CNN+SE建模周期性谱噪声。引入注意力式时间池化以动态突出高证据信帧，自适应注意力融合平衡各分支贡献；采用Focal Loss训练。

Result: 在Celeb-DF(v2)上取得AUC 0.9752、F1 0.9408、准确率0.9208，超过单流基线；消融实验验证分支协同增益；Grad-CAM显示模型关注取证相关区域。

Conclusion: 多模态（空间-纹理-频域）特征融合与时序/分支注意力能有效提升Deepfake检测的鲁棒性与对细微伪造的敏感性。

Abstract: Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.

</details>


### [114] [Explaining Digital Pathology Models via Clustering Activations](https://arxiv.org/abs/2511.14558)
*Adam Bajger,Jan Obdržálek,Vojtěch Kůr,Rudolf Nenutil,Petr Holub,Vít Musil,Tomáš Brázdil*

Main category: cs.CV

TL;DR: 提出一种基于聚类的可解释性方法，用于CNN病理图像模型，展示模型全局行为并提供细粒度信息，优于仅针对单张切片的显著性图；在前列腺癌检测模型上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有可解释方法（遮挡、GradCAM、相关传播等）多聚焦单张切片的局部重要区域，缺乏对模型整体决策行为的全局理解，难以提升临床信任与采纳。需要一种能揭示模型全局模式、同时具备细粒度解释的方案。

Method: 对模型在病理切片上的表征（如特征图/嵌入）进行聚类，得到代表性簇；可视化每个簇的典型图块/区域，展示模型在不同组织形态上的响应与决策模式。与显著性图不同，该方法汇总大量样本的特征，形成全局解释，同时保留簇内细粒度实例。

Result: 得到可视化的聚类簇，能直观呈现模型偏好的组织学模式与判别线索；在前列腺癌检测的现有CNN上实验，显示该方法有助于理解模型并提升对其行为的信心。

Conclusion: 聚类驱动的解释框架可为数字病理模型提供全局且细粒度的可解释性，补充并改进传统显著性方法，并在临床应用（如前列腺癌检测）中具实用价值。

Abstract: We present a clustering-based explainability technique for digital pathology models based on convolutional neural networks. Unlike commonly used methods based on saliency maps, such as occlusion, GradCAM, or relevance propagation, which highlight regions that contribute the most to the prediction for a single slide, our method shows the global behaviour of the model under consideration, while also providing more fine-grained information. The result clusters can be visualised not only to understand the model, but also to increase confidence in its operation, leading to faster adoption in clinical practice. We also evaluate the performance of our technique on an existing model for detecting prostate cancer, demonstrating its usefulness.

</details>


### [115] [OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models](https://arxiv.org/abs/2511.14582)
*Keda Tao,Kele Shao,Bohan Yu,Weiqiang Wang,Jian liu,Huan Wang*

Main category: cs.CV

TL;DR: 提出OmniZip：一种训练免调的音频引导型多模态（音视频）Token压缩框架，在不牺牲性能的前提下显著加速推理并降低内存。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM在统一音视频理解中需处理大量音视频token，计算与内存成为瓶颈。现有压缩方法大多针对单模态或未考虑音视频联合压缩与跨模态信息对齐的需求。

Method: 训练免调框架OmniZip：1) 先从音频中识别显著音频token；2) 以时间组为单位计算音频保留分数，度量信息密度；3) 利用该分数动态指导相应时间窗内的视频token剪枝，并结合跨模态相似度保留与音频“锚点”相关的视频线索；4) 在每个时间窗对视频token采用交织的时空压缩策略进行合并与裁剪。

Result: 在广泛实验中，OmniZip相较其它强基线实现约3.42倍推理加速与1.4倍内存占用降低，同时在无训练条件下基本保持任务性能。

Conclusion: 音频引导的训练免调联合压缩能有效缓解OmniLLM的计算瓶颈，在不重训模型的前提下实现显著加速与节省内存，体现跨模态协同压缩的价值。

Abstract: Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.

</details>


### [116] [Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease](https://arxiv.org/abs/2511.14588)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出一个深度学习框架，既能稳健分割WMH又能按解剖区域量化其负荷；区域级WMH与萎缩指标联合用于AD等疾病分类，AUC最高0.97，显示前部白质通路的局部易感性。


<details>
  <summary>Details</summary>
Motivation: 现有WMH自动分割多只给出全局病灶负荷，忽视空间分布；而WMH的区域差异可能与AD等疾病表型、早期诊断和分层密切相关，需要既可靠分割又能区域定位与量化的工具。

Method: 构建并评估一个深度学习分割与定位框架：在多公共数据集与独立ADNI队列上进行训练/验证；在全脑准确分割WMH后，将病灶体积映射到解剖定义的白质区域；把区域WMH体积与脑结构体积（萎缩指标）结合，进行诊断分类并比较全局负荷与区域指标的效能。

Result: 分割出的病灶负荷与参考标准高度一致，表现对病灶负荷大小、扫描参数与人群差异具有鲁棒性；区域WMH体积在疾病分类上稳定优于全局负荷；与脑萎缩体积联合时分类性能进一步提高，AUC最高达0.97；发现多个空间上可复现的关键区域，尤其是前部白质束与诊断状态显著相关。

Conclusion: 仅用全局WMH不足以捕捉与AD相关的病理差异；区域化WMH定量具有增益，与萎缩指标联合可显著提升早期诊断与分层潜力，应在神经退行性疾病研究与临床评估中常规纳入。

Abstract: White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.

</details>


### [117] [CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/abs/2511.14599)
*Dongqing Xie,Yonghuang Wu,Zisheng Ai,Jun Min,Zhencun Jiang,Shaojin Geng,Lei Wang*

Main category: cs.CV

TL;DR: 提出CCSD框架，能在多模态MRI缺失任意模态时仍实现高精度脑肿瘤分割，通过共享-特定编码解码与两种自蒸馏策略提升泛化与稳健性，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 临床多模态MRI常有模态缺失，导致深度分割模型性能与泛化下降；需要一种能利用可得模态、在任意缺失组合下仍稳健的方案。

Method: 采用共享-特定(shared-specific)编码器-解码器架构；设计两类自蒸馏：1) 分层模态自蒸馏，将不同层级模态间的知识迁移以缓解语义差异；2) 渐进式模态组合蒸馏，在训练中逐步模拟模态掉落与组合变化，使模型在任意模态缺失场景下保持一致预测。

Result: 在公共脑肿瘤分割基准上，对多种模态缺失情形均取得SOTA表现，表现出强泛化与稳定性。

Conclusion: CCSD通过跨模态组合自蒸馏与共享-特定结构，有效应对临床模态缺失问题，提升脑肿瘤分割的鲁棒性与准确性，具有实际落地潜力。

Abstract: The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.

</details>


### [118] [MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts](https://arxiv.org/abs/2511.14601)
*Nathaniel Putera,Daniel Vilet Rodríguez,Noah Videcrantz,Julia Machnio,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 研究比较表格特征与基于Transformer的MRI表征在阿尔茨海默病认知下降预测中的作用：表格特征更擅长识别高风险进展，ViT嵌入更擅长识别稳定人群；两者互补，建议多模态融合。


<details>
  <summary>Details</summary>
Motivation: 现有利用表格/临床变量的模型能给出整体风险，但难以捕捉脑影像中的细微变化；而影像模型虽能表征结构信息，却缺少针对个体化进展轨迹的标注与评估框架。作者希望厘清两类表征在不同进展模式中的优势，并设计更贴合纵向认知变化的标签与表征学习方式。

Method: 1) 利用动态时间规整（DTW）聚类纵向认知评分，构建“轨迹感知”的进展标签（稳定、轻度、中度、重度）。2) 对MRI进行统一化与增强，训练3D ViT以无监督重建获得保持解剖结构的嵌入，无需进展标签。3) 以传统机器学习分类器与深度学习头评估ViT嵌入的预测能力，并与临床/体积表格特征及卷积网络基线比较。

Result: - 临床与体积表格特征：在预测轻度与重度进展时AUC≈0.70，为高风险极端人群提供最强信号。- ViT MRI嵌入：在区分认知稳定个体时最佳，AUC=0.71。- 所有方法在异质性的“中度”组表现不佳。- 总体呈现模态互补。

Conclusion: 表格临床/体积特征擅长识别快速进展风险，Transformer MRI嵌入更敏感于稳定的细微结构标记；建议采用多模态融合以覆盖不同进展谱系，改进对AD进展的个体化建模。

Abstract: Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.

</details>


### [119] [XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation](https://arxiv.org/abs/2511.14604)
*Yilin Zhang,Leo D. Westbury,Elaine M. Dennison,Nicholas C. Harvey,Nicholas R. Fuggle,Rahman Attar*

Main category: cs.CV

TL;DR: 提出XAttn-BMD：利用髋部X光与临床结构化元数据，通过双向交叉注意力融合，预测股骨颈骨密度；并用加权Smooth L1损失处理不平衡。相比简单拼接与多种基线，在回归与筛查任务上显著提升。


<details>
  <summary>Details</summary>
Motivation: 骨质疏松相关骨折负担沉重，低BMD是关键风险因素。传统仅用影像或简单多模态融合的做法难以充分利用互补信息，且BMD分布不均、临床上更关注低BMD病例，需更鲁棒、可泛化的多模态方法。

Method: 构建多模态深度学习框架XAttn-BMD：输入髋部X光图像与结构化临床元数据；设计双向交叉注意力模块，实现图像与元数据特征的动态互相查询与增强；提出加权Smooth L1损失以缓解BMD失衡并强调临床重要的低BMD区间；在Hertfordshire Cohort Study数据上进行训练、对比与消融。

Result: 与基线相比具备更好的回归泛化与鲁棒性；交叉注意力融合优于直接特征拼接：MSE下降16.7%，MAE下降6.03%，R²提升16.4%；加权Smooth L1与交叉注意力均在消融中被验证有效。

Conclusion: 双向交叉注意力的多模态融合与定制损失共同提升了股骨颈BMD估计效果，并在按临床阈值的二分类筛查中表现良好，显示其在实际临床应用中的潜力。

Abstract: Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the model's potential in real-world scenarios.

</details>


### [120] [3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology](https://arxiv.org/abs/2511.14613)
*Mohammad Vali Sanian,Arshia Hemmat,Amirhossein Vahidi,Jonas Maaskola,Jimmy Tsz Hang Lee,Stanislaw Makarchuk,Yeliz Demirci,Nana-Jane Chipampe,Omer Bayraktar,Lassi Paavolainen,Mohammad Lotfollahi*

Main category: cs.CV

TL;DR: HoloTea提出一种可扩展的3D感知流匹配框架，从H&E切片生成/补全空间转录组表达，利用相邻切片信息以提升三维一致性与精度，并在多数据集上优于2D与现有3D方法。


<details>
  <summary>Details</summary>
Motivation: 当前多数方法将每张切片独立建模，忽略三维结构；现有3D感知方法多非生成式且难以扩展。需要一种兼顾三维解剖连续性、计数分布特性、并能在大规模3D数据上高效训练与推断的模型，以更准确重建3D虚拟组织、辅助生物标志物发现。

Method: 提出HoloTea：1) 通过共享特征空间在相邻切片中检索形态对应的spot，并以轻量级ControlNet融合跨切片上下文，实现沿解剖连续性的条件控制；2) 为流匹配引入三维一致先验：将学习到的ZINB先验与由邻近切片构建的空间经验先验相结合，更贴合计数型基因表达；3) 设计全局注意力模块，将3D H&E建模的计算复杂度随slide中spot数量线性扩展，从而支持大规模3D ST训练与推理。

Result: 在三种不同组织类型与分辨率的空间转录组数据集上，相比2D与3D基线，HoloTea在三维表达预测准确性与泛化性上均有一致提升。

Conclusion: HoloTea能稳定、可扩展地将H&E映射到三维基因表达，利用相邻切片与计数先验提高3D一致性与精度，有望推动高精度3D虚拟组织构建，加速生物标志物发现并深化对疾病的理解。

Abstract: A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.

</details>


### [121] [Fusing Biomechanical and Spatio-Temporal Features for Fall Prediction: Characterizing and Mitigating the Simulation-to-Reality Gap](https://arxiv.org/abs/2511.14620)
*Md Fokhrul Islam,Sajeda Al-Hammouri,Christopher J. Arellano,Kavan Hazeli,Heman Shakeri*

Main category: cs.CV

TL;DR: 该论文提出BioST-GCN，一种结合人体姿态与生物力学信息并通过跨注意力融合的双流模型，在两套模拟数据集上较ST-GCN基线提升F1（+5.32%/+2.91%），但从模拟到真实的零样本泛化显著下滑（89.0%→35.9%），主要因模拟数据偏差（如“有意跌倒”线索）。论文主张个性化与隐私保护数据管道以推动真实场景验证。


<details>
  <summary>Details</summary>
Motivation: 老年人跌倒常致伤残与失独立，视觉预测可在碰撞前数秒预警，但缺乏真实跌倒数据限制了模型开发。现有基于姿态的ST-GCN虽有效，但对关键关节/阶段解释有限，且模拟-现实域差导致泛化差。研究动机是融合更丰富的生物力学信息、提升可解释性，并探索缩小模拟与现实差距。

Method: 提出BioST-GCN：双流架构（姿态流+生物力学流），通过跨注意力进行信息融合；在ST-GCN分支中引入时空注意以突出关键关节与时间段，实现可解释性。在模拟数据集（MCF-UA特技演员、MUVIM）上进行训练与评估，并考察从模拟到未见主体的零样本泛化表现。

Result: 在模拟数据上，较vanilla ST-GCN的F1提升：MCF-UA +5.32%，MUVIM +2.91%；全监督模拟场景F1达89.0%。但对未见主体的零样本泛化F1降至35.9%，揭示强烈的域间性能崩塌。注意力可定位关键关节与时间阶段，提供一定可解释性。

Conclusion: BioST-GCN通过融合姿态与生物力学并用跨注意力提升了模拟数据上的预测与可解释性，但存在严重的模拟-现实鸿沟，可能源于模拟数据的偏差（如“意图跌倒”信号）及老年特殊运动学差异。作者建议采用个性化策略与隐私保护数据管道，以获取并验证真实世界数据，从而为高风险老年群体构建有效的跌倒预测系统。

Abstract: Falls are a leading cause of injury and loss of independence among older adults. Vision-based fall prediction systems offer a non-invasive solution to anticipate falls seconds before impact, but their development is hindered by the scarcity of available fall data. Contributing to these efforts, this study proposes the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model that combines both pose and biomechanical information using a cross-attention fusion mechanism. Our model outperforms the vanilla ST-GCN baseline by 5.32% and 2.91% F1-score on the simulated MCF-UA stunt-actor and MUVIM datasets, respectively. The spatio-temporal attention mechanisms in the ST-GCN stream also provide interpretability by identifying critical joints and temporal phases. However, a critical simulation-reality gap persists. While our model achieves an 89.0% F1-score with full supervision on simulated data, zero-shot generalization to unseen subjects drops to 35.9%. This performance decline is likely due to biases in simulated data, such as `intent-to-fall' cues. For older adults, particularly those with diabetes or frailty, this gap is exacerbated by their unique kinematic profiles. To address this, we propose personalization strategies and advocate for privacy-preserving data pipelines to enable real-world validation. Our findings underscore the urgent need to bridge the gap between simulated and real-world data to develop effective fall prediction systems for vulnerable elderly populations.

</details>


### [122] [SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2511.14633)
*Meiying Gu,Jiawei Zhang,Jiahe Li,Xiaohan Yu,Haonan Luo,Jin Zheng,Xiao Bai*

Main category: cs.CV

TL;DR: 提出一种在稀疏视角下同时提升表面重建与新视图合成质量的方法，通过对齐立体几何与纹理并引入伪特征增强的几何一致性，缓解高各向异性高斯导致的过拟合，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 基于高斯Splatting的几何优化在视角稠密时能重建细致表面，但稀疏视角下易过拟合。现有做法用“扁平化”各向异性高斯并配合深度正则以拟合表面与缓解歧义，但各向异性增大会放大过拟合，导致几何不准与合成质量下降。

Method: 提出\net{}：1) Stereo Geometry-Texture Alignment（立体几何-纹理对齐），将渲染质量与几何估计耦合，使表面拟合与新视图合成相互促进；2) Pseudo-Feature Enhanced Geometry Consistency（伪特征增强的几何一致性），在训练与未见视角上共同施加多视图几何一致性约束，以弱化稀疏监督引起的过拟合。

Result: 在DTU、BlendedMVS、Mip-NeRF360等数据集上达到SOTA，表明在表面重建精度与新视图渲染质量上均优于现有方法。

Conclusion: 通过几何-纹理对齐与伪特征增强的一致性约束，可在稀疏视角下有效抑制各向异性高斯的过拟合，同时提升几何与渲染表现，实现SOTA。

Abstract: Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.

</details>


### [123] [SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology](https://arxiv.org/abs/2511.14639)
*Marco Acerbis,Swarnadip Chatterjee,Christophe Avenel,Joakim Lindblad*

Main category: cs.CV

TL;DR: 提出SLAM-AGS：在超低目击率条件下，通过“按切片标签感知”的多任务预训练（弱监督相似+自监督对比）并配合自适应梯度手术稳定训练，显著提升骨髓细胞学的袋级分类与异常细胞检索。


<details>
  <summary>Details</summary>
Motivation: 计算细胞学中实例级标注昂贵且不可靠，且阳性实例在整张切片中的目击率极低，导致弱监督学习易崩塌、迁移差。需要一种能利用切片级标签并在低目击率下稳定学习的预训练策略。

Method: 提出Slide-Label-Aware Multitask (SLAM) 预训练：对切片阴性patch进行弱监督的相似性目标，对切片阳性patch进行自监督对比学习；同时引入Adaptive Gradient Surgery缓解多任务梯度冲突、防止塌陷。预训练编码器接入注意力MIL聚合器用于袋级预测，并用注意力热度引导检索袋内最异常实例。

Result: 在公开骨髓细胞学数据集上，模拟目击率从10%降至0.5%，SLAM-AGS在袋级F1和Top-400阳性细胞检索上均优于其他预训练方法，且目击率越低提升越显著。

Conclusion: 通过解决多任务梯度干扰并利用切片级标签信息，SLAM-AGS实现稳定预训练，显著提升下游袋级分类与异常实例检索性能；代码与评测框架已开源以促进复现。

Abstract: Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.

</details>


### [124] [RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT](https://arxiv.org/abs/2511.14649)
*John M. Oyer,Ali Namvar,Benjamin A. Hoff,Wassim W. Labaki,Ella A. Kazerooni,Charles R. Hatt,Fernando J. Martinez,MeiLan K. Han,Craig J. Galbán,Sundaresh Ram*

Main category: cs.CV

TL;DR: RepAir 是一个三阶段的3D气道分割框架：先用nnU-Net得到初始分割，再用骨架化检测断裂并生成重连候选，最后用1D卷积分类器筛选真实解剖连接。在健康与重病变数据集上均优于现有U-Net系方法，兼顾体素精度与拓扑完整性。


<details>
  <summary>Details</summary>
Motivation: 气道分割对肺部定量分析至关重要，但手工标注难以规模化，现有U-Net方法常出现断裂与不连通，影响生物标志物的稳定提取与下游分析的可靠性。因此需要既准确又拓扑一致的自动分割。

Method: 提出RepAir三阶段流程：1）nnU-Net产生初始气道掩膜；2）基于骨架的算法定位潜在不连续处并提出重连候选；3）利用1D卷积分类器对候选连接进行真伪判别，过滤错误或病灶导致的伪通路，从而进行基于解剖先验的拓扑修复。

Result: 在ATM’22（以健康为主）与AeroPath（重度病变）两数据集上，RepAir在体素级与拓扑指标均超越Bronchinet、NaviAirway等3D U-Net方法，产生更完整、解剖一致的气道树，同时保持高分割精度。

Conclusion: 结合分割网络与解剖驱动的拓扑校正能显著提升气道树的连通性与完整性，增强跨数据集与病理条件的鲁棒性，为可靠的肺部生物标志物提取提供更稳健基础。

Abstract: Accurate airway segmentation from chest computed tomography (CT) scans is essential for quantitative lung analysis, yet manual annotation is impractical and many automated U-Net-based methods yield disconnected components that hinder reliable biomarker extraction. We present RepAir, a three-stage framework for robust 3D airway segmentation that combines an nnU-Net-based network with anatomically informed topology correction. The segmentation network produces an initial airway mask, after which a skeleton-based algorithm identifies potential discontinuities and proposes reconnections. A 1D convolutional classifier then determines which candidate links correspond to true anatomical branches versus false or obstructed paths. We evaluate RepAir on two distinct datasets: ATM'22, comprising annotated CT scans from predominantly healthy subjects and AeroPath, encompassing annotated scans with severe airway pathology. Across both datasets, RepAir outperforms existing 3D U-Net-based approaches such as Bronchinet and NaviAirway on both voxel-level and topological metrics, and produces more complete and anatomically consistent airway trees while maintaining high segmentation accuracy.

</details>


### [125] [Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms](https://arxiv.org/abs/2511.14654)
*Marius Dubosc,Yann Fischer,Zacharie Auray,Nicolas Boutry,Edwin Carlinet,Michael Atlan,Thierry Geraud*

Main category: cs.CV

TL;DR: 提出一种将脉搏时序特征注入标准U-Net的简洁方法，用于时间多普勒全息视网膜动静脉分割，性能可比复杂注意力/迭代模型，并强调时间预处理的重要性。


<details>
  <summary>Details</summary>
Motivation: 多普勒全息在视网膜血流动力学定量中有高时间分辨率，但现有分割方法只用空间信息，忽视时序信息，限制了动静脉分割精度与下游定量分析。

Method: 在时间多普勒全息序列上设计专门的脉搏分析预处理，提取能表征动静脉动态差异的时序特征，并将这些特征作为附加通道或特征嵌入输入到标准分割网络（如U-Net），无需复杂注意力或迭代模块。

Result: 将时序脉搏特征与U-Net结合，在动静脉分割上达到与更复杂的注意力/迭代模型相当的表现。作者还提供公开数据集以支持复现与研究。

Conclusion: 时间分辨的预处理能有效释放深度学习在多普勒全息中的潜力，使简洁架构即可实现强分割性能，为定量视网膜血流动力学研究提供新方向。

Abstract: Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/

</details>


### [126] [Impact of Image Resolution on Age Estimation with DeepFace and InsightFace](https://arxiv.org/abs/2511.14689)
*Shiyar Jamo*

Main category: cs.CV

TL;DR: 研究评估图像分辨率对年龄估计的影响，发现DeepFace与InsightFace在224×224分辨率最准确；低分辨率与过高分辨率都会降低精度，且InsightFace更快、总体更准。


<details>
  <summary>Details</summary>
Motivation: 实际年龄核验场景中输入图像分辨率差异大，但现有系统对分辨率敏感性与最佳分辨率缺乏系统量化评估，影响模型部署与推理效率配置。

Method: 选取IMDB-Clean数据集1000张人脸图像，缩放为7种分辨率共7000样本；分别用DeepFace与InsightFace进行年龄估计；以MAE、SD、MedAE评估误差，并比较不同分辨率下的性能与推理速度。

Result: 两框架在224×224像素取得最佳准确度：DeepFace MAE=10.83岁，InsightFace MAE=7.46岁；分辨率过低时误差显著上升，分辨率过高时也会劣化；InsightFace在所有分辨率下推理更快。

Conclusion: 输入分辨率对年龄估计有显著且一致的影响；存在性能“甜点”224×224；不宜盲目升高或降低分辨率；部署时可优先选择InsightFace并将输入标准化到224×224以在精度与速度间取得最佳折中。

Abstract: Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.

</details>


### [127] [HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring](https://arxiv.org/abs/2511.14698)
*Sriram Srinivasan,Srinivasan Aruchamy,Siva Ram Krisha Vadali*

Main category: cs.CV

TL;DR: 提出HyMAD：结合SincNet谱特征、RNN时序建模与自注意力及跨模态融合的深度网络，用于多标签同时活动（人/动物/车辆）地震信号检测，在实地边境数据上表现竞争力。


<details>
  <summary>Details</summary>
Motivation: 地下微型地震传感器隐蔽性强，适合边境监控，但地震信号噪声大且多源活动常重叠，传统方法难以区分人、动物、车辆等同时发生的事件，导致误报漏报，降低可靠性。

Method: 构建Hybrid Multi-Activity Detection(HyMAD)框架：1) 采用SincNet提取频谱特征；2) 用RNN建模时序依赖；3) 引入自注意力增强各模态内表示；4) 通过跨模态融合模块进行鲁棒的多标签分类；在真实边境场景录制的数据集上训练与评估。

Result: 在包含人、动物、车辆且活动可同时出现的实地数据集上取得有竞争力的性能，表现出良好的泛化到复杂重叠场景的能力。

Conclusion: HyMAD能有效分离并识别同时发生的多类地震事件，提升边境安防监测的可靠性；框架模块化，便于扩展到实际安防应用中的地震活动识别。

Abstract: Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.

</details>


### [128] [Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images](https://arxiv.org/abs/2511.14702)
*Farheen Ramzan,Yusuf Kiberu,Nikesh Jathanna,Meryem Jabrane,Vicente Grau,Shahnaz Jamil-Copley,Richard H. Clayton,Chen,Chen*

Main category: cs.CV

TL;DR: 提出利用ECG电生理信息与AHA-17解剖先验，并通过时间感知特征融合（TAFF）整合非同期采集的多模态数据，实现对LGE心肌瘢痕更精准的分割，显著优于图像单模态基线。


<details>
  <summary>Details</summary>
Motivation: LGE心肌MRI的瘢痕分割受对比度变化与伪影影响，单靠图像难以稳定、准确定位瘢痕。ECG反映传导异常，具备与瘢痕相关的互补生理信息；AHA-17提供标准化解剖分区，可引导分割的生理一致性。如何在非同步采集条件下有效融合这些信息，是提高鲁棒性的关键。

Method: 构建多模态框架：1) 从ECG中提取电生理表征；2) 引入AHA-17心肌分区作为解剖先验；3) 设计时间感知特征融合（TAFF）模块，根据ECG与LGE采集时间差动态加权并融合特征；4) 以LGE作为主要影像输入，配合ECG与解剖先验进行联合学习，用于瘢痕分割。

Result: 在临床数据集上，相比图像单模态基线nnU-Net，瘢痕Dice由0.6149显著提升至0.8463，同时获得高精度（0.9115）与高敏感性（0.9043），整体性能大幅提升。

Conclusion: 融合电生理（ECG）与解剖先验（AHA-17），并考虑跨时采集差异的TAFF，可实现“超越图像”的生理一致性瘢痕分割，显著优于现有方法，并为稳健、具生理依据的心脏瘢痕分割提出新方向。

Abstract: Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.

</details>


### [129] [FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation](https://arxiv.org/abs/2511.14712)
*Yunfeng Wu,Jiayi Song,Zhenxiong Tan,Zihao He,Songhua Liu*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法“FreeSwim”，用预训练视频扩散Transformer在更高分辨率上生成视频。核心是“向内滑动窗口注意力”，并用双路径与跨注意力覆盖+缓存，既保细节又保全局一致，效率高，并在VBench上优于多种需训练方法。


<details>
  <summary>Details</summary>
Motivation: Transformer视频生成在超高分辨率下因注意力的二次时间与内存复杂度而难以端到端训练；需要一种能在不额外训练的情况下放大生成分辨率、同时保持细节与全局一致性的方案。

Method: 1) 向内滑动窗口注意力：保持每个query在其原训练尺度的感受野，避免高分辨率时感受野不匹配导致的细节损失。2) 双路径管线：主分支用局部窗口注意力生成细节；辅分支具有全局感受野，通过“跨注意力覆盖”引导主分支，保证全局语义与连贯性。3) 跨注意力缓存：对全局分支的跨注意力进行缓存，减少频繁的3D全注意力计算开销。方法为完全训练自由，直接用预训练Diffusion Transformer。

Result: 在无需微调的前提下，生成超高分辨率视频，细节丰富、全局一致且高效。在VBench基准上优于或可比于需训练的替代方法，同时效率具竞争力或更优。

Conclusion: 通过向内滑动窗口注意力与双路径跨注意力覆盖（配合缓存），可在不训练的条件下将预训练视频扩散Transformer扩展到超高分辨率生成，兼顾细节、全局一致性与效率，并在基准上取得领先表现。

Abstract: The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim

</details>


### [130] [Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model](https://arxiv.org/abs/2511.14716)
*Xiyuan Wang,Muhan Zhang*

Main category: cs.CV

TL;DR: 作者提出将编码器、解码器与扩散网络三者合一，端到端训练，解决传统潜空间扩散的“潜表示崩塌”，在ImageNet 256×256 上以较小参数量与50个epoch取得优异FID且无需CFG。


<details>
  <summary>Details</summary>
Motivation: 现有潜空间扩散模型采用分离的编码器/解码器/扩散网络并分阶段训练，带来算力低效、性能次优且难以与单网络视觉基础模型统一。尝试简单端到端合训会因扩散目标干扰表征学习而导致潜表示崩塌，因此需要新的训练框架以稳定潜空间并实现真正的一体化。

Method: 提出Diffusion as Self-Distillation (DSD)。作者将扩散过程与自蒸馏无监督学习建立类比，从目标层面对潜空间稳定性问题进行诊断，并修改训练目标：在同一网络中同时学习编码、解码与扩散，通过自蒸馏化的损失设计与正则化，避免扩散目标压制表征学习，稳定联合训练。

Result: 在ImageNet 256×256 条件生成上，单网络端到端训练50个epoch，在不同参数规模（42M/118M/205M）下获得FID 13.44/6.38/4.25，且无需classifier-free guidance。

Conclusion: DSD首次稳定地将编码、解码、扩散统一到单一网络端到端训练，提升效率与性能，并为与单网络视觉基础模型的统一提供可行路径。

Abstract: Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.

</details>


### [131] [Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising](https://arxiv.org/abs/2511.14719)
*Yifan Wang,Liya Ji,Zhanghan Ke,Harry Yang,Ser-Nam Lim,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出一种零样本的合成视频写实化方法：以扩散式视频基座模型为骨架，不需微调，通过辅助模型从合成视频估计结构先验（深度、语义、边缘），在去噪生成过程中进行条件引导，从而在保持空间与时间多层结构一致的同时获得高写实度；实验显示结构一致性优于现有方法且写实度达SOTA。


<details>
  <summary>Details</summary>
Motivation: 仿真/合成视频常缺乏真实感，限制了其在感知、仿真到现实迁移、内容制作等场景的实用性；现有写实化方法要么需针对任务/数据微调，要么难以同时保证时空结构与语义一致性。作者希望在不依赖模拟器内部信息、无需再训练的前提下，将合成视频重渲染为具有高写实度且结构保持的结果。

Method: 基于无需微调的扩散视频基础模型，修改其去噪/生成过程，使其条件化于从输入合成视频估计的结构化引导信号（深度图、语义分割、边缘图）。这些信号由辅助模型预测而来，而非直接从模拟器导出。通过在时空维度注入多层结构约束，保证生成视频与原视频在几何与语义上的一致性，同时维持扩散模型的写实生成能力。

Result: 与现有基线相比，在与原合成视频的结构一致性指标上显著更优，同时写实质量达到或接近当前最优水平。方法通用、简单、可零样本应用于不同合成视频。

Conclusion: 零样本、无需微调的结构感知条件扩散框架可有效将合成视频重渲染为高写实视频，并在保持时空结构与语义一致性的同时达到SOTA级写实质量，适合广泛合成到真实的应用场景。

Abstract: We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.

</details>


### [132] [A Neural Field-Based Approach for View Computation & Data Exploration in 3D Urban Environments](https://arxiv.org/abs/2511.14742)
*Stefan Cobeli,Kazi Shahrukh Omar,Rodrigo Valença,Nivan Ferreira,Fabio Miranda*

Main category: cs.CV

TL;DR: 提出基于视图的3D城市数据探索框架：用神经场构建隐式环境表示，以快速进行可见性/日照等指标的正向查询与避免遮挡的反向查询，提升大规模探索效率。


<details>
  <summary>Details</summary>
Motivation: 3D城市数据愈发丰富，但几何复杂与遮挡严重、视角调节繁琐，导致计算与交互成本高，难以高效开展大规模可视化与分析（如可见性、日照、视觉影响评估）。

Method: 以“视图向量场”为核心，将环境中的视点需求编码。提出基于神经场（neural field）的隐式表示，支持：1）高效正向查询（快速计算视图评估指标）；2）反向查询（在目标约束下搜索满足模式的视点、规避遮挡）。系统面向典型城市分析任务（可见性评估、太阳辐射/日照评估、新开发项目的视觉影响分析）。

Result: 通过定量实验、真实城市问题案例与专家反馈验证：方法能高效找到理想视点，改进建筑立面可见性分析，并有效评估户外空间视图质量；在查询速度与探索效率上优于传统显式几何与手动视角试错。代码与数据已公开。

Conclusion: 基于神经场的视图向量场为3D城市环境提供高效的隐式表示，统一支持正向/反向视图查询，缓解遮挡与交互负担，显著提升城市可视分析与设计评估的可扩展性与实用性。

Abstract: Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.

</details>


### [133] [Vision Large Language Models Are Good Noise Handlers in Engagement Analysis](https://arxiv.org/abs/2511.14749)
*Alexander Vedernikov,Puneet Kumar,Haoyu Chen,Tapio Seppänen,Xiaobai Li*

Main category: cs.CV

TL;DR: 用VLM辅助清洗与引导训练：先用问卷抽取行为线索，将数据按可靠性分层；再用课程学习与软标签细化，逐步纳入不确定样本。在EngageNet等基准上小幅超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 视频中的“参与度”标签主观性强、带噪，直接训练会限制模型上限；需要一种方法提升标签可靠性并让训练能感知不确定性。

Method: (1) 设计问卷从VLM获取行为线索，对样本可靠性打分，划分高/低可靠子集；(2) 以高可靠子集先训经典CV模型；(3) 课程学习逐步引入低可靠样本；(4) 软标签细化：根据不确定性调整监督强度/标签分布；(5) 训练过程中由VLM提供注释与指导。

Result: 在EngageNet的6种特征配置中有3种取得最高，最大提升+1.21%；在DREAMS与PAFE基准上F1分别提升+0.22与+0.06。

Conclusion: 用VLM提升注释质量并配合课程+软标签机制，能缓解主观与噪声标签带来的性能瓶颈，对经典CV模型也有效，整体带来稳健但幅度不大的SOTA提升。

Abstract: Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.

</details>


### [134] [Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers](https://arxiv.org/abs/2511.14751)
*Yutian Chen,Yuheng Qiu,Ruogu Li,Ali Agha,Shayegan Omidshafiei,Jay Patrikar,Sebastian Scherer*

Main category: cs.CV

TL;DR: 提出Co-Me：一种无需再训练的置信度引导Token合并方法，加速视觉几何Transformer，通过预测不确定性合并低置信token，保持空间覆盖与性能的同时实现大幅提速（VGGT最高11.3×，MapAnything 7.2×）。


<details>
  <summary>Details</summary>
Motivation: 视觉几何Transformer在多视角/流式场景中计算量随序列长度迅速增长，现有相似度合并或剪枝可能破坏关键区域，难以在实时3D感知与重建中部署。需要一种不改动基座、可靠指示关注区域且推理时可扩展的加速机制。

Method: 蒸馏训练一个轻量级置信度预测器，对每个token进行不确定性评估与排序；在推理时优先合并低置信token，确保合并后仍保持空间覆盖；无须对原始Transformer再训练或微调；方法可直接插入多视角与流式视觉几何Transformer流水线，合并比例随序列长度扩展。

Result: 相较基于相似度的合并/剪枝，置信度信号更稳定地对齐Transformer关注区域，实现显著加速而不降性能；在VGGT与MapAnything上分别实现最高11.3×与7.2×速度提升。

Conclusion: 置信度引导的Token合并能在保持性能与空间覆盖的同时大幅降低计算，使视觉几何Transformer更适用于实时3D感知与重建，并具备良好的通用性与可扩展性。

Abstract: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.

</details>


### [135] [UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning](https://arxiv.org/abs/2511.14760)
*Rui Tian,Mingfei Gao,Haiming Gang,Jiasen Lu,Zhe Gan,Yinfei Yang,Zuxuan Wu,Afshin Dehghan*

Main category: cs.CV

TL;DR: UniGen-1.5 是一款统一式多模态大模型，面向图像理解、生成与编辑，借助统一的RL与轻量指令对齐显著提升编辑，同时保持强理解与生成性能，达到SOTA/接近商用水准。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多聚焦于理解或生成，编辑能力常受限；训练流程割裂、奖励不统一，导致任务间迁移不足；需要一种统一架构和训练范式，同时提升理解、生成与编辑，尤其强化编辑指令理解以发挥RL效果。

Method: 在UniGen基础上改进模型架构与训练管线；提出统一的强化学习策略，用共享奖励模型同时优化图像生成与编辑；引入轻量级的编辑指令对齐阶段（Edit Instruction Alignment），提升对编辑指令的理解，为后续RL提供良好起点。

Result: 在GenEval与ImgEdit基准上分别获得0.89与4.31总体分数，超过开源SOTA（如BAGEL），并与专有模型（如GPT-Image-1）性能相当；在理解、生成与编辑三方面均表现竞争力。

Conclusion: 统一式RL与轻量编辑指令对齐能在不牺牲理解/生成的前提下显著增强编辑能力，形成一个在三大任务上均衡强劲的MLLM框架。

Abstract: We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.

</details>


### [136] [ARC Is a Vision Problem!](https://arxiv.org/abs/2511.14761)
*Keya Hu,Ali Cy,Linlu Qiu,Xiaoman Delores Ding,Runqian Wang,Yeyin Eva Zhu,Jacob Andreas,Kaiming He*

Main category: cs.CV

TL;DR: 将ARC视为视觉任务：把谜题映射为图像到图像翻译，用画布表示输入，直接用ViT等标准视觉模型训练，并在测试时自适应；在ARC-1上达60.4%准确率，接近人类平均且优于从零训练的既有方法。


<details>
  <summary>Details</summary>
Motivation: ARC旨在研究抽象推理，但主流方法过于语言化（LLM/递归推理），忽视了任务本质上的视觉属性；作者希望引入视觉先验，探索纯视觉范式是否能更好或更简洁地解决ARC并缩小与人类的差距。

Method: 将ARC统一建模为图像到图像翻译：把网格输入/输出嵌入到可被视觉模型处理的“画布”表示；采用标准视觉架构（如原生ViT）从零开始在ARC数据上训练；通过测试时训练（test-time training）在新任务上自适应，以提高泛化。

Result: 提出的Vision ARC（VARC）在ARC-1基准上达到60.4%准确率，显著超过其他同样从零训练的方法；性能与领先LLM具有竞争力，并接近人类平均水平。

Conclusion: 以视觉为中心的范式在ARC上有效：无需依赖语言模型，仅凭标准视觉网络加画布表示与测试时自适应即可取得强性能，缩小与人类的差距，提示视觉先验对抽象推理有重要价值。

Abstract: The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.

</details>
