<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Facial Emotion Recognition does not detect feeling unsafe in automated driving](https://arxiv.org/abs/2509.04490)
*Abel van Elburg,Konstantinos Gkentsidis,Mathieu Sarrazin,Sarah Barendswaard,Varun Kotian,Riender Happee*

Main category: cs.CV

TL;DR: 研究通过模拟器实验探讨自动驾驶风格与行人情境对乘客感知风险与舒适度的影响，并以车辆运动与皮电训练神经网络预测感知风险，发现动态驾驶显著提升不适与风险感，行人仅在动态风格下加剧不适，面部表情难以可靠反映风险，而生理与运动数据可较好预测主观风险。


<details>
  <summary>Details</summary>
Motivation: 公众对自动驾驶的信任与安全感是采纳的关键；现有主观评价存在偏差且缺乏客观实时的风险感知度量，因此需要系统检验不同驾驶风格与突发行人事件对乘客感知风险/舒适的影响，并探索可客观预测风险感的指标与模型。

Method: 在驾驶模拟器中设置两种自动驾驶风格（平稳/动态），并在部分情景引入横穿行人；32名参与者提供连续主观舒适/风险评分，同时采集车辆运动数据、皮电、心率、眼动与人脸视频；对表情进行识别；基于车辆运动与皮电训练神经网络回归模型预测感知风险，并与主观评分对比。

Result: 连续主观评分显示转弯与制动阶段风险上升/舒适下降，随后缓解；动态风格较平稳风格显著增加不适与风险；行人在平稳风格下影响不显著，但在动态风格下使舒适度下降加倍；24人表情可分析但多数无明显反应；有反应者以“快乐”表情为主，少量“惊讶”，未观察到以“恐惧”为主；基于运动+皮电的神经网络与主观风险具有较好相关。

Conclusion: 乘客感知风险高度依赖驾驶动态与关键交互后果，动态风格与行人结合显著放大不适；面部表情识别不适合作为自动驾驶风险感评估；融合车辆运动与皮电的模型可客观估计感知风险，减少主观偏差，值得在更大样本与真实道路中进一步验证与拓展。

Abstract: Trust and perceived safety play a crucial role in the public acceptance of
automated vehicles. To understand perceived risk, an experiment was conducted
using a driving simulator under two automated driving styles and optionally
introducing a crossing pedestrian. Data was collected from 32 participants,
consisting of continuous subjective comfort ratings, motion, webcam footage for
facial expression, skin conductance, heart rate, and eye tracking. The
continuous subjective perceived risk ratings showed significant discomfort
associated with perceived risk during cornering and braking followed by relief
or even positive comfort on continuing the ride. The dynamic driving style
induced a stronger discomfort as compared to the calm driving style. The
crossing pedestrian did not affect discomfort with the calm driving style but
doubled the comfort decrement with the dynamic driving style. This illustrates
the importance of consequences of critical interactions in risk perception.
Facial expression was successfully analyzed for 24 participants but most
(15/24) did not show any detectable facial reaction to the critical event.
Among the 9 participants who did, 8 showed a Happy expression, and only 4
showed a Surprise expression. Fear was never dominant. This indicates that
facial expression recognition is not a reliable method for assessing perceived
risk in automated vehicles. To predict perceived risk a neural network model
was implemented using vehicle motion and skin conductance. The model correlated
well with reported perceived risk, demonstrating its potential for objective
perceived risk assessment in automated vehicles, reducing subjective bias and
highlighting areas for future research.

</details>


### [2] [PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting](https://arxiv.org/abs/2509.04545)
*Linqing Wang,Ximing Xing,Yiji Cheng,Zhiyuan Zhao,Jiale Tao,Qixun Wang,Ruihuang Li,Xin Li,Mingrui Wu,Xinchi Deng,Chunyu Wang,Qinglin Lu*

Main category: cs.CV

TL;DR: 提出PromptEnhancer：通过强化学习训练的链式思维(CoT)提示重写器，配合细粒度奖励模型AlignEvaluator，在不改动T2I扩散模型权重的前提下，显著提升复杂指令（属性绑定、否定、组合关系等）的图文对齐；并发布人类偏好基准。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型虽能生成高保真图像，但对复杂文本指令经常理解不准，尤其在属性绑定、否定和组合关系上失真，导致用户意图与结果不匹配。需要一种通用、无需微调生成器且能细粒度对齐的解决方案。

Method: 提出一个与生成器解耦的提示重写框架：1) 训练一个链式思维(CoT)重写器；2) 设计并训练AlignEvaluator作为奖励模型，基于对T2I常见失败模式的系统性分类（24个关键点）提供显式、细粒度反馈；3) 通过强化学习优化CoT重写器，使其最大化AlignEvaluator的奖励，从而生成更易被任意预训练T2I模型准确执行的重写提示；4) 无需改动或微调底层T2I模型权重。

Result: 在HunyuanImage 2.1上进行大量实验，PromptEnhancer在多种语义和组合挑战上显著提升图文对齐度；同时构建并发布一个高质量的人类偏好基准，用于评测与后续研究。

Conclusion: 通过将提示重写从生成模型中解耦，并用细粒度奖励指导的强化学习训练CoT重写器，能普遍提升现有T2I模型对复杂指令的遵循能力；方法通用、无需改动生成器，且伴随公开的人类偏好基准推动领域发展。

Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated
remarkable capabilities in generating high-fidelity images. However, these
models often struggle to faithfully render complex user prompts, particularly
in aspects like attribute binding, negation, and compositional relationships.
This leads to a significant mismatch between user intent and the generated
output. To address this challenge, we introduce PromptEnhancer, a novel and
universal prompt rewriting framework that enhances any pretrained T2I model
without requiring modifications to its weights. Unlike prior methods that rely
on model-specific fine-tuning or implicit reward signals like image-reward
scores, our framework decouples the rewriter from the generator. We achieve
this by training a Chain-of-Thought (CoT) rewriter through reinforcement
learning, guided by a dedicated reward model we term the AlignEvaluator. The
AlignEvaluator is trained to provide explicit and fine-grained feedback based
on a systematic taxonomy of 24 key points, which are derived from a
comprehensive analysis of common T2I failure modes. By optimizing the CoT
rewriter to maximize the reward from our AlignEvaluator, our framework learns
to generate prompts that are more precisely interpreted by T2I models.
Extensive experiments on the HunyuanImage 2.1 model demonstrate that
PromptEnhancer significantly improves image-text alignment across a wide range
of semantic and compositional challenges. Furthermore, we introduce a new,
high-quality human preference benchmark to facilitate future research in this
direction.

</details>


### [3] [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](https://arxiv.org/abs/2509.04548)
*Hongyang Wei,Baixin Xu,Hongbo Liu,Cyrus Wu,Jie Liu,Yi Peng,Peiyu Wang,Zexiang Liu,Jingwen He,Yidan Xietian,Chuanxin Tang,Zidong Wang,Yichen Wei,Liang Hu,Boyi Jiang,William Li,Ying He,Yang Liu,Xuchen Song,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: 提出UniPic2-SD3.5M-Kontext与UniPic2-Metaquery：在仅约20亿参数的DiT基础上，通过架构改造+高质数据预训练+渐进双任务强化(PDTR)，在图像生成与编辑上超越更大模型（如BAGEL 7B、Flux-Kontext 12B），并通过与Qwen2.5-VL-7B连接实现理解/生成/编辑统一的多模态模型，验证了Skywork UniPic 2.0训练范式的有效性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前开源多模态生成/编辑模型多靠参数规模堆叠，忽视训练策略优化，导致效率与性能受限；亟需在较小模型上通过更优训练范式实现统一的生成与编辑能力并可无缝扩展到多模态。

Method: 1) 对SD3.5-Medium进行架构改造，形成约2B参数的DiT骨干；2) 使用高质量大规模数据进行预训练，联合学习文本到图像生成与图像编辑；3) 提出渐进式双任务强化(PDTR)：分阶段对生成与编辑进行强化，强调指令跟随与编辑一致性，且两阶段互惠无负干扰；4) 参考MetaQuery，以连接器将UniPic2-SD3.5M-Kontext与Qwen2.5-VL-7B联接并联合训练，得到统一多模态模型UniPic2-Metaquery。

Result: UniPic2-SD3.5M-Kontext在图像生成与编辑上优于更大参数模型（如BAGEL 7B、Flux-Kontext 12B）；PDTR显示两任务强化相互促进、无负迁移；联接Qwen2.5-VL-7B后的UniPic2-Metaquery在理解、生成、编辑等多任务上取得一流表现。

Conclusion: 通过Skywork UniPic 2.0范式（架构改造+高质预训+PDTR+多模态连接器联合训练），可在较小模型上实现并统一强化生成与编辑能力，并可扩展到多模态统一模型，兼具性能、效率与可扩展性。

Abstract: Recent advances in multimodal models have demonstrated impressive
capabilities in unified image generation and editing. However, many prominent
open-source models prioritize scaling model parameters over optimizing training
strategies, limiting their efficiency and performance. In this work, we present
UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which
achieves state-of-the-art image generation and editing while extending
seamlessly into a unified multimodal framework. Our approach begins with
architectural modifications to SD3.5-Medium and large-scale pre-training on
high-quality data, enabling joint text-to-image generation and editing
capabilities. To enhance instruction following and editing consistency, we
propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which
effectively strengthens both tasks in a staged manner. We empirically validate
that the reinforcement phases for different tasks are mutually beneficial and
do not induce negative interference. After pre-training and reinforcement
strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and
editing capabilities than models with significantly larger generation
parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following
the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a
connector and perform joint training to launch a unified multimodal model
UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and
editing, achieving top-tier performance across diverse tasks with a simple and
scalable training paradigm. This consistently validates the effectiveness and
generalizability of our proposed training paradigm, which we formalize as
Skywork UniPic 2.0.

</details>


### [4] [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](https://arxiv.org/abs/2509.04582)
*Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: Inpaint4Drag将拖拽编辑从“改潜空间”转为“像素域双向形变+局部补全”的两步法，实时预览形变并把拖拽转成通用修复掩码与条件，从而适配任意图像修复模型，速度与精度显著优于以往潜空间方法。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽编辑依赖生成模型潜空间，存在精度不足、交互延迟大（分钟级）、且强依赖特定模型架构的缺点。需要一种既精确可控、实时交互、又与模型无关且可继承修复技术进步的方案。

Method: 提出Inpaint4Drag：1）像素域双向warping，模拟物理弹性形变，使被拖拽区域保持自然形状；2）将拖拽输入转化为标准图像修复(inpainting)的掩码与条件，作为通用适配器；3）实时形变预览（约0.01s）+高效修复（约0.3s，512×512）；无需修改下游修复模型架构。

Result: 与现有潜空间拖拽法相比，实现更高视觉质量、更精确几何控制与显著更快的交互速度；广泛实验显示方法在512×512下可实时交互（形变0.01s、修复0.3s），并能无缝利用不同修复模型。

Conclusion: 将拖拽编辑解耦为像素形变与修复，使之成为对任意修复模型的通用前端/适配器，既提升可控性与实时性，又能自动继承未来修复模型的提升。

Abstract: Drag-based image editing has emerged as a powerful paradigm for intuitive
image manipulation. However, existing approaches predominantly rely on
manipulating the latent space of generative models, leading to limited
precision, delayed feedback, and model-specific constraints. Accordingly, we
present Inpaint4Drag, a novel framework that decomposes drag-based editing into
pixel-space bidirectional warping and image inpainting. Inspired by elastic
object deformation in the physical world, we treat image regions as deformable
materials that maintain natural shape under user manipulation. Our method
achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at
512x512 resolution, significantly improving the interaction experience compared
to existing methods that require minutes per edit. By transforming drag inputs
directly into standard inpainting formats, our approach serves as a universal
adapter for any inpainting model without architecture modification,
automatically inheriting all future improvements in inpainting technology.
Extensive experiments demonstrate that our method achieves superior visual
quality and precise control while maintaining real-time performance. Project
page: https://visual-ai.github.io/inpaint4drag/

</details>


### [5] [DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models](https://arxiv.org/abs/2509.04597)
*Jin Ma,Mohammed Aldeen,Christopher Salas,Feng Luo,Mashrur Chowdhury,Mert Pesé,Long Cheng*

Main category: cs.CV

TL;DR: 提出DISPATCH：基于扩散模型的“再生成-校正”式对抗补丁防御，用于目标检测；无需先验、攻击无关，在多检测器/多攻击上显著提升mAP并降低创建攻击成功率，对自适应攻击也稳健。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测器易受对抗补丁攻击，能隐匿或伪造目标，现实风险高；已有防御多为“检测并移除”补丁，依赖先验、泛化差、易被自适应攻击绕过。需要一种有效、通用、对自适应攻击稳健的防御。

Method: 提出DISPATCH框架：利用扩散模型对整幅图像进行再生成，使其回到数据分布；随后执行校正流程，定位并以再生成的良性区域替换可疑（对抗）区域。方法对攻击无关，无需补丁位置/形态先验，跨检测器适用。

Result: 在多种检测器与多类攻击（隐藏与创建）上，DISPATCH优于现有防御：隐藏攻击场景mAP@0.5达到89.3%；非定向创建攻击将攻击成功率降至24.8%；同时对自适应攻击保持强鲁棒性。

Conclusion: 扩散模型驱动的“再生成-校正”策略能在不破坏图像语义的前提下有效中和对抗补丁，对未知与自适应攻击均具备通用性与稳健性，适合作为实用的目标检测防御方案。

Abstract: Object detection is fundamental to various real-world applications, such as
security monitoring and surveillance video analysis. Despite their
advancements, state-of-theart object detectors are still vulnerable to
adversarial patch attacks, which can be easily applied to real-world objects to
either conceal actual items or create non-existent ones, leading to severe
consequences. Given the current diversity of adversarial patch attacks and
potential unknown threats, an ideal defense method should be effective,
generalizable, and robust against adaptive attacks. In this work, we introduce
DISPATCH, the first diffusion-based defense framework for object detection.
Unlike previous works that aim to "detect and remove" adversarial patches,
DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative
models to disarm attack effects while preserving the integrity of the input
image. Specifically, we utilize the in-distribution generative power of
diffusion models to regenerate the entire image, aligning it with benign data.
A rectification process is then employed to identify and replace adversarial
regions with their regenerated benign counterparts. DISPATCH is attack-agnostic
and requires no prior knowledge of the existing patches. Extensive experiments
across multiple detectors and attacks demonstrate that DISPATCH consistently
outperforms state-of-the-art defenses on both hiding attacks and creating
attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and
lowering the attack success rate to 24.8% on untargeted creating attacks.
Moreover, it maintains strong robustness against adaptive attacks, making it a
practical and reliable defense for object detection systems.

</details>


### [6] [WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human](https://arxiv.org/abs/2509.04600)
*Qijun Ying,Zhongyuan Hu,Rui Zhang,Ronghui Li,Yu Lu,Zijiao Zeng*

Main category: cs.CV

TL;DR: 提出WATCH框架，从单目野外视频端到端重建“世界坐标系下”的人和相机的全局轨迹与姿态，解决相机方向与平移信息利用不足的问题，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目野外视频的全局人体运动重建需要把人体从相机坐标映射到世界坐标，但受深度/运动歧义以及相机与人运动纠缠影响。现有以人体为中心的方法虽能保留细节和物理合理性，但没有充分利用相机朝向信息，且难以有效整合相机平移线索。

Method: 提出WATCH统一框架：1) 分析式航向角（heading）分解技术，相比几何法更高效、更易扩展，用于更好利用相机朝向；2) 借鉴world models设计相机轨迹整合机制，不再仅靠硬解码，能有效吸收相机平移信息；整体实现相机-人体联合建模与端到端轨迹重建。

Result: 在野外基准上，WATCH在端到端轨迹重建方面达到最新最优（SOTA），验证相机-人体联合建模与相机平移整合的有效性。

Conclusion: 联合建模相机与人体运动、使用分析式航向角分解与基于world model的相机轨迹整合，可显著提升全局人体运动重建；为长期存在的相机平移整合难题提供了新思路。

Abstract: Global human motion reconstruction from in-the-wild monocular videos is
increasingly demanded across VR, graphics, and robotics applications, yet
requires accurate mapping of human poses from camera to world coordinates-a
task challenged by depth ambiguity, motion ambiguity, and the entanglement
between camera and human movements. While human-motion-centric approaches excel
in preserving motion details and physical plausibility, they suffer from two
critical limitations: insufficient exploitation of camera orientation
information and ineffective integration of camera translation cues. We present
WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and
Human), a unified framework addressing both challenges. Our approach introduces
an analytical heading angle decomposition technique that offers superior
efficiency and extensibility compared to existing geometric methods.
Additionally, we design a camera trajectory integration mechanism inspired by
world models, providing an effective pathway for leveraging camera translation
information beyond naive hard-decoding approaches. Through experiments on
in-the-wild benchmarks, WATCH achieves state-of-the-art performance in
end-to-end trajectory reconstruction. Our work demonstrates the effectiveness
of jointly modeling camera-human motion relationships and offers new insights
for addressing the long-standing challenge of camera translation integration in
global human motion reconstruction. The code will be available publicly.

</details>


### [7] [Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning](https://arxiv.org/abs/2509.04602)
*MinJu Jeon,Si-Woo Kim,Ye-Chan Kim,HyunGee Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 提出Sali4Vid，用显著性加权与语义自适应检索两招，同时解决时间戳仅监督文本与固定块检索忽视场景切换的问题，在YouCook2与ViTT上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有端到端密集视频描述方法：仅在文本侧用时间戳监督，视频帧被等权；并且用固定长度视频块做检索，忽略场景/镜头转场，导致定位不准与描述不佳。需要一个能把时间戳监督转为帧级重要性并能感知场景变化的框架。

Method: Sali4Vid包含两关键组件：1) Saliency-aware Video Reweighting：将时间戳标注转为sigmoid形帧重要性权重，对视频编码时进行加权，使与事件相关的帧贡献更大。2) Semantic-based Adaptive Caption Retrieval：基于帧间相似度对视频进行自适应分段，捕捉场景转场；从这些语义一致的片段中检索/生成更贴合的事件字幕。整体为一个简单有效、可端到端训练的框架。

Result: 在YouCook2与ViTT数据集上取得state-of-the-art性能（具体指标未给出），显示显著性加权与自适应检索的结合优于以往仅文本监督与固定块检索的方法。

Conclusion: 将时间戳监督扩展到帧级显著性并结合语义自适应分段检索，可显著提升密集视频描述的事件定位与字幕质量；联合改进视频加权与检索是有效方向。

Abstract: Dense video captioning aims to temporally localize events in video and
generate captions for each event. While recent works propose end-to-end models,
they suffer from two limitations: (1) applying timestamp supervision only to
text while treating all video frames equally, and (2) retrieving captions from
fixed-size video chunks, overlooking scene transitions. To address these, we
propose Sali4Vid, a simple yet effective saliency-aware framework. We introduce
Saliency-aware Video Reweighting, which converts timestamp annotations into
sigmoid-based frame importance weights, and Semantic-based Adaptive Caption
Retrieval, which segments videos by frame similarity to capture scene
transitions and improve caption retrieval. Sali4Vid achieves state-of-the-art
results on YouCook2 and ViTT, demonstrating the benefit of jointly improving
video weighting and retrieval for dense video captioning

</details>


### [8] [UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis](https://arxiv.org/abs/2509.04624)
*Ali Khanpour,Tianyi Wang,Afra Vahidi-Shams,Wim Ectors,Farzam Nakhaie,Amirhossein Taheri,Christian Claudel*

Main category: cs.CV

TL;DR: 基于无人机的交通监测系统：在200米高空采集视频，结合多尺度/多角度模板匹配、卡尔曼滤波与单应性标定，实现车辆检测、分类、跟踪与行为分析；在城市实测中达到高精度，并自动识别多类交通违规，支持多维度出行与拥堵分析，具备可扩展、无需路侧基础设施的优势。


<details>
  <summary>Details</summary>
Motivation: 传统固定摄像头与地面传感器覆盖有限、适应性与扩展性差，难以在复杂城市环境中进行广域、精准、可扩展的交通监测与执法辅助。需要一种基础设施依赖低、部署灵活、能进行检测、跟踪、分类与违规识别的一体化解决方案。

Method: 使用无人机在约200米高空采集航拍视频；算法层面采用多尺度/多角度模板匹配用于车辆检测与分类，卡尔曼滤波实现多目标跟踪，基于单应性（homography）进行相机与地面坐标标定；融合地理围栏、运动过滤与轨迹偏差分析进行违规检测；集成OD追踪、车辆计数、类别相关性分析、热力图拥堵建模、进出场轨迹画像、路段密度估计与方向记录等分析模块。

Result: 在城市实测案例中，检测精度91.8%，F1=90.5%；跟踪指标MOTA=92.1%，MOTP=93.7%。系统可稳定分类五类车辆，并自动识别不安全变道、违章双排停车与人行横道占用等违规行为；生成多种可视化与统计分析结果。

Conclusion: 该UAV系统在真实城市环境中展现出高精度、可扩展与实用性，能在无路侧基础设施条件下提供执法感知的交通监测与多尺度出行分析，适用于下一代智慧城市交通管理与执法辅助。

Abstract: Traffic congestion and violations pose significant challenges for urban
mobility and road safety. Traditional traffic monitoring systems, such as fixed
cameras and sensor-based methods, are often constrained by limited coverage,
low adaptability, and poor scalability. To address these challenges, this paper
introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance
system capable of accurate vehicle detection, classification, tracking, and
behavioral analysis in real-world, unconstrained urban environments. The system
leverages multi-scale and multi-angle template matching, Kalman filtering, and
homography-based calibration to process aerial video data collected from
altitudes of approximately 200 meters. A case study in urban area demonstrates
robust performance, achieving a detection precision of 91.8%, an F1-score of
90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.
Beyond precise detection, the system classifies five vehicle types and
automatically detects critical traffic violations, including unsafe lane
changes, illegal double parking, and crosswalk obstructions, through the fusion
of geofencing, motion filtering, and trajectory deviation analysis. The
integrated analytics module supports origin-destination tracking, vehicle count
visualization, inter-class correlation analysis, and heatmap-based congestion
modeling. Additionally, the system enables entry-exit trajectory profiling,
vehicle density estimation across road segments, and movement direction
logging, supporting comprehensive multi-scale urban mobility analytics.
Experimental results confirms the system's scalability, accuracy, and practical
relevance, highlighting its potential as an enforcement-aware,
infrastructure-independent traffic monitoring solution for next-generation
smart cities.

</details>


### [9] [VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation](https://arxiv.org/abs/2509.04669)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: VCMamba是一种结合CNN与多方向Mamba状态空间模型的混合视觉骨干：前期用卷积提取局部细节，后期用多方向Mamba建模长程依赖与全局上下文，在保持对分辨率线性复杂度的同时，在ImageNet-1K与ADE20K上取得优于同类的方法的精度与参数效率。


<details>
  <summary>Details</summary>
Motivation: ViT与SSM（如Mamba）擅长全局/长序列建模但对细粒度局部特征不如CNN；CNN具备强局部归纳偏置却欠缺全局推理。需要一种同时兼具强局部表征与高效全局依赖建模、且复杂度可扩展的视觉骨干。

Method: 提出VCMamba：采用卷积stem与层级结构；早期stage使用卷积块聚合丰富局部特征；后期stage引入多方向Mamba SSM块以高效捕获长程依赖与全局上下文；整体在图像分辨率上保持近线性复杂度。

Result: 在ImageNet-1K分类上，VCMamba-B达82.6% top-1：比PlainMamba-L3高0.3%，参数少37%；比Vision GNN-B高0.3%，参数少64%。在ADE20K语义分割上，VCMamba-B得47.1 mIoU，比EfficientFormer-L7高2.0 mIoU，参数少62%。

Conclusion: 混合式设计有效结合CNN的局部归纳偏置与Mamba的全局/长程建模能力，带来更强特征表征与更高参数效率，验证了在分类与分割任务中的优越性。

Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)
have challenged the dominance of Convolutional Neural Networks (CNNs) in
computer vision. ViTs excel at capturing global context, and SSMs like Mamba
offer linear complexity for long sequences, yet they do not capture
fine-grained local features as effectively as CNNs. Conversely, CNNs possess
strong inductive biases for local features but lack the global reasoning
capabilities of transformers and Mamba. To bridge this gap, we introduce
\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs
and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a
hierarchical structure with convolutional blocks in its early stages to extract
rich local features. These convolutional blocks are then processed by later
stages incorporating multi-directional Mamba blocks designed to efficiently
model long-range dependencies and global context. This hybrid design allows for
superior feature representation while maintaining linear complexity with
respect to image resolution. We demonstrate VCMamba's effectiveness through
extensive experiments on ImageNet-1K classification and ADE20K semantic
segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,
surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming
Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains
47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing
62% fewer parameters. Code is available at
https://github.com/Wertyuui345/VCMamba.

</details>


### [10] [Guideline-Consistent Segmentation via Multi-Agent Refinement](https://arxiv.org/abs/2509.04687)
*Vanshika Vats,Ashwani Rathee,James Davis*

Main category: cs.CV

TL;DR: 提出一个无需训练的多智能体迭代框架，以遵循复杂文本标注指南进行语义分割：由“工作者”分割、“监督者”对照检索到的指南批评并改进，加上轻量RL停止策略，显著提升对Waymo与ReasonSeg的表现与指令遵循性。


<details>
  <summary>Details</summary>
Motivation: 真实场景语义分割不仅要准确，还要严格遵循冗长复杂的文本标注规范；现有人类与自动标注都常违背规范；传统方法需为每套/更新的指南昂贵再训练；开放词汇分割对短提示有效，但对段落级复杂规则易失败。

Method: 构建一个无需额外训练的多代理迭代“工作者-监督者”框架：通用视觉-语言模型作为工作者执行分割；监督者依据检索到的指南对结果提出批评与修正建议；在循环中加入轻量级强化学习的停止策略，在遵循指南与资源消耗之间权衡。

Result: 在Waymo与ReasonSeg数据集上显著优于SOTA基线，展现出更强的泛化能力与对复杂指令/指南的遵循性。

Conclusion: 多代理、训练-free、带RL停止策略的迭代框架能有效将通用VLM对齐到复杂标注指南，获得更一致且资源可控的分割结果，并具备跨数据集的良好泛化。

Abstract: Semantic segmentation in real-world applications often requires not only
accurate masks but also strict adherence to textual labeling guidelines. These
guidelines are typically complex and long, and both human and automated
labeling often fail to follow them faithfully. Traditional approaches depend on
expensive task-specific retraining that must be repeated as the guidelines
evolve. Although recent open-vocabulary segmentation methods excel with simple
prompts, they often fail when confronted with sets of paragraph-length
guidelines that specify intricate segmentation rules. To address this, we
introduce a multi-agent, training-free framework that coordinates
general-purpose vision-language models within an iterative Worker-Supervisor
refinement architecture. The Worker performs the segmentation, the Supervisor
critiques it against the retrieved guidelines, and a lightweight reinforcement
learning stop policy decides when to terminate the loop, ensuring
guideline-consistent masks while balancing resource use. Evaluated on the Waymo
and ReasonSeg datasets, our method notably outperforms state-of-the-art
baselines, demonstrating strong generalization and instruction adherence.

</details>


### [11] [Domain Adaptation for Different Sensor Configurations in 3D Object Detection](https://arxiv.org/abs/2509.04711)
*Satoshi Tanaka,Kok Seang Tan,Isamu Yamashita*

Main category: cs.CV

TL;DR: 该论文聚焦跨LiDAR传感器配置的3D目标检测域适配，提出“下游微调+部分层微调”策略，在多数据联合训练后对特定数据集微调，并仅更新部分层以提升跨配置泛化，实验表明优于朴素联合训练。


<details>
  <summary>Details</summary>
Motivation: 不同车辆平台搭载的LiDAR在视场、线数、安装位置等配置差异导致点云分布显著变化，致使在一种配置上训练的检测器迁移到另一配置时性能下降。既有工作多关注环境域差异或同一传感器的密度变化，对跨传感器配置的域差距缺乏系统研究，因此需要面向多平台部署的实用适配方法。

Method: 1) 多数据集联合训练获取通用表征；2) 下游微调：在各目标配置/数据集上分别进行少量数据的特定微调；3) 部分层微调：在微调阶段只更新网络的子集层（例如高层/任务头等），冻结其余层以促进跨配置泛化并降低过拟合与计算成本；4) 使用同一地理区域、不同配置的成对数据集进行评估。

Result: 与朴素的联合训练相比，加入下游微调与部分层微调在各传感器配置上均获得稳定且一致的性能提升，验证了方法对跨配置域适配的有效性与可扩展性。

Conclusion: 通过“联合训练+下游微调+部分层微调”的范式，可在无需复杂对齐或生成策略的情况下，有效适配3D检测模型到不同LiDAR配置，提供了面向多车辆平台的实用、可扩展解决方案。

Abstract: Recent advances in autonomous driving have underscored the importance of
accurate 3D object detection, with LiDAR playing a central role due to its
robustness under diverse visibility conditions. However, different vehicle
platforms often deploy distinct sensor configurations, causing performance
degradation when models trained on one configuration are applied to another
because of shifts in the point cloud distribution. Prior work on multi-dataset
training and domain adaptation for 3D object detection has largely addressed
environmental domain gaps and density variation within a single LiDAR; in
contrast, the domain gap for different sensor configurations remains largely
unexplored. In this work, we address domain adaptation across different sensor
configurations in 3D object detection. We propose two techniques: Downstream
Fine-tuning (dataset-specific fine-tuning after multi-dataset training) and
Partial Layer Fine-tuning (updating only a subset of layers to improve
cross-configuration generalization). Using paired datasets collected in the
same geographic region with multiple sensor configurations, we show that joint
training with Downstream Fine-tuning and Partial Layer Fine-tuning consistently
outperforms naive joint training for each configuration. Our findings provide a
practical and scalable solution for adapting 3D object detection models to the
diverse vehicle platforms.

</details>


### [12] [CD-Mamba: Cloud detection with long-range spatial dependency modeling](https://arxiv.org/abs/2509.04729)
*Tianxiang Xue,Jiayi Zhao,Jingsheng Li,Changlu Chen,Kun Zhan*

Main category: cs.CV

TL;DR: 提出CD-Mamba：将卷积与Mamba状态空间模型融合的云检测网络，兼顾像素级纹理与长距离依赖，在多尺度上提升遥感云检测准确性，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 遥感图像常被云覆盖，影响数据完整性与可靠性。云检测需同时处理短程空间冗余（局部纹理/边界）与长程大气相似性（跨补丁的云结构与形态），单一模型难以兼顾。

Method: 构建混合架构CD-Mamba：以卷积模块捕捉局部空间依赖与像素级纹理；引入Mamba（选择性状态空间模型）以高效建模长距离依赖和跨补丁关系；在统一网络中联合建模像素级交互和补丁级全局依赖，面向多尺度云检测。

Result: 在广泛实验中，CD-Mamba在检测准确性上优于现有方法，显示其在多尺度和不同场景下的有效性（具体数据未在摘要给出）。

Conclusion: 融合卷积与Mamba的混合网络能同时处理局部与长程依赖，提升云检测的鲁棒性与精度，是遥感云检测的有效方案。

Abstract: Remote sensing images are frequently obscured by cloud cover, posing
significant challenges to data integrity and reliability. Effective cloud
detection requires addressing both short-range spatial redundancies and
long-range atmospheric similarities among cloud patches. Convolutional neural
networks are effective at capturing local spatial dependencies, while Mamba has
strong capabilities in modeling long-range dependencies. To fully leverage both
local spatial relations and long-range dependencies, we propose CD-Mamba, a
hybrid model that integrates convolution and Mamba's state-space modeling into
a unified cloud detection network. CD-Mamba is designed to comprehensively
capture pixelwise textural details and long term patchwise dependencies for
cloud detection. This design enables CD-Mamba to manage both pixel-wise
interactions and extensive patch-wise dependencies simultaneously, improving
detection accuracy across diverse spatial scales. Extensive experiments
validate the effectiveness of CD-Mamba and demonstrate its superior performance
over existing methods.

</details>


### [13] [Exploiting Unlabeled Structures through Task Consistency Training for Versatile Medical Image Segmentation](https://arxiv.org/abs/2509.04732)
*Shengqian Zhu,Jiafei Wu,Xiaogang Xu,Chengrong Yu,Ying Song,Zhang Yi,Guangjun Li,Junjie Hu*

Main category: cs.CV

TL;DR: 提出一种无需额外模型的任务一致性训练（TCT）框架，用于在部分标注数据下解决多类别医学分割的类别不平衡与伪标签噪声问题；通过主分割头与多辅助任务头的一致性约束、低一致性数据筛除与不确定性加权损失，显著提升跨站点腹部数据集上的分割性能。


<details>
  <summary>Details</summary>
Motivation: 在多类别医学图像分割中，获取所有类别的完整标注代价高，部分标注数据普遍存在类别分布不均，导致类别不平衡与训练偏置。现有方法依赖生成伪全标注，常需额外模型且易引入噪声，影响性能。需要一种在不引入额外模型的情况下，能有效利用未标注结构并缓解类别不平衡的新策略。

Method: 提出Task Consistency Training（TCT）框架：1）共享骨干网络+主分割头（MSH）输出多通道多类预测；2）多个辅助任务头（ATH）进行任务特定（子集/器官级）预测；3）对MSH与各ATH之间施加一致性约束，使未标注结构也参与学习；4）设计筛选策略剔除低一致性、潜在噪声的样本/像素，避免错误传播；5）提出统一的辅助不确定性加权损失（UAUWL），根据不确定性与任务难度自适应调权，抑制某些任务主导导致的退化。

Result: 在来自多个临床站点的八个腹部数据集上进行广泛实验，TCT在部分标注场景下取得优于现有方法的分割性能，并在类别不平衡与跨域泛化方面表现稳健。

Conclusion: TCT无需额外模型，通过任务一致性、噪声筛除与不确定性加权有效利用未标注结构，缓解类别不平衡和伪标签噪声问题，提升部分标注医学分割的准确性与泛化性。

Abstract: Versatile medical image segmentation (VMIS) targets the segmentation of
multiple classes, while obtaining full annotations for all classes is often
impractical due to the time and labor required. Leveraging partially labeled
datasets (PLDs) presents a promising alternative; however, current VMIS
approaches face significant class imbalance due to the unequal category
distribution in PLDs. Existing methods attempt to address this by generating
pseudo-full labels. Nevertheless, these typically require additional models and
often result in potential performance degradation from label noise. In this
work, we introduce a Task Consistency Training (TCT) framework to address class
imbalance without requiring extra models. TCT includes a backbone network with
a main segmentation head (MSH) for multi-channel predictions and multiple
auxiliary task heads (ATHs) for task-specific predictions. By enforcing a
consistency constraint between the MSH and ATH predictions, TCT effectively
utilizes unlabeled anatomical structures. To avoid error propagation from
low-consistency, potentially noisy data, we propose a filtering strategy to
exclude such data. Additionally, we introduce a unified auxiliary
uncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines
caused by the dominance of specific tasks. Extensive experiments on eight
abdominal datasets from diverse clinical sites demonstrate our approach's
effectiveness.

</details>


### [14] [Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization](https://arxiv.org/abs/2509.04735)
*Dharsan Ravindran,Kevin Wang,Zhuoyuan Cao,Saleh Abdelrahman,Jeffery Wu*

Main category: cs.CV

TL;DR: 提出两种将不确定性建模引入SAM/SAM2以增强恶劣天气下自动驾驶场景分割鲁棒性的方案，并在多数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: SAM/SAM2在常规基准上很强，但在恶劣天气等高歧义场景表现欠佳，关键原因是缺乏不确定性量化与利用。医学影像表明不确定性感知训练能提升模糊边界与罕见情况的可靠性，作者希望将其迁移到自动驾驶。

Method: (1) 对SAM2进行多阶段微调：在训练目标中显式加入不确定性度量（如像素级/区域级不确定性）相关的损失项，引导模型在高不确定区域更稳健；(2) 将医学影像中的Uncertainty-Aware Adapter（UAT）适配到驾驶场景，形成UAT-SAM，与标准SAM对比。

Result: 在CamVid、BDD100K与GTA等数据集上评测：UAT-SAM在极端天气条件下优于标准SAM；采用不确定性感知损失的SAM2在多样驾驶场景整体性能提升。

Conclusion: 在安全关键的自动驾驶中，显式的不确定性建模能提升在恶劣与多变环境下的分割可靠性；UAT与不确定性加权训练为提升基础视觉模型在实际场景中的稳健性提供有效路径。

Abstract: Recent advances in vision foundation models, such as the Segment Anything
Model (SAM) and its successor SAM2, have achieved state-of-the-art performance
on general image segmentation benchmarks. However, these models struggle in
adverse weather conditions where visual ambiguity is high, largely due to their
lack of uncertainty quantification. Inspired by progress in medical imaging,
where uncertainty-aware training has improved reliability in ambiguous cases,
we investigate two approaches to enhance segmentation robustness for autonomous
driving. First, we introduce a multi-step finetuning procedure for SAM2 that
incorporates uncertainty metrics directly into the loss function, improving
overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter
(UAT), originally designed for medical image segmentation, to driving contexts.
We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.
Experiments show that UAT-SAM outperforms standard SAM in extreme weather,
while SAM2 with uncertainty-aware loss achieves improved performance across
diverse driving scenes. These findings underscore the value of explicit
uncertainty modeling for safety-critical autonomous driving in challenging
environments.

</details>


### [15] [WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches](https://arxiv.org/abs/2509.04736)
*Taeyoung Yeon,Vasco Xu,Henry Hoffmann,Karan Ahuja*

Main category: cs.CV

TL;DR: WatchHAR 是一款完全在智能手表端运行的多模态（音频+惯性）精细粒度人类活动识别系统，通过端到端统一的预处理与推理架构和组件级优化，在>25类活动上保持90%+准确率，同时将事件检测与分类延迟降至毫秒级（9.3 ms/11.8 ms），实现隐私友好、低时延的在设备侧连续追踪。


<details>
  <summary>Details</summary>
Motivation: 现有精细粒度HAR虽有进展，但多数依赖云端/手机侧计算，带来隐私与时延问题，且在非受控环境与手表有限算力下难以落地。作者动机是在智能手表上实现端侧、可持续、准确且高效的多模态HAR。

Method: 提出WatchHAR：1) 以音频与惯性传感为主的多模态管线；2) 对管线各环节（预处理、模型、运行时）进行系统级优化以叠加性能收益；3) 设计将传感数据预处理与推理统一为端到端可训练模块的新架构，减少手工特征与数据搬运开销；4) 在手表上部署，实现事件检测与活动分类的高效推理。

Result: 在>25类活动上维持90%+准确率；相较SOTA，在事件检测与活动分类任务上均有更优性能；通过端到端统一架构达成约5倍处理提速；在手表上达到9.3 ms事件检测与11.8 ms多模态分类延迟。

Conclusion: WatchHAR证明了在手表端可实现高精度、低时延、隐私友好的多模态活动识别；端到端统一架构与系统级优化对资源受限设备尤为有效，推动智能手表成为独立、最小侵扰的连续行为追踪平台。

Abstract: Despite advances in practical and multimodal fine-grained Human Activity
Recognition (HAR), a system that runs entirely on smartwatches in unconstrained
environments remains elusive. We present WatchHAR, an audio and inertial-based
HAR system that operates fully on smartwatches, addressing privacy and latency
issues associated with external data processing. By optimizing each component
of the pipeline, WatchHAR achieves compounding performance gains. We introduce
a novel architecture that unifies sensor data preprocessing and inference into
an end-to-end trainable module, achieving 5x faster processing while
maintaining over 90% accuracy across more than 25 activity classes. WatchHAR
outperforms state-of-the-art models for event detection and activity
classification while running directly on the smartwatch, achieving 9.3 ms
processing time for activity event detection and 11.8 ms for multimodal
activity classification. This research advances on-device activity recognition,
realizing smartwatches' potential as standalone, privacy-aware, and
minimally-invasive continuous activity tracking devices.

</details>


### [16] [MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery](https://arxiv.org/abs/2509.04757)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 提出MCANet用于飓风后多标签损伤识别，结合Res2Net多尺度表征与多头类别特异残差注意力，在RescueNet上mAP达91.75%（8头达92.35%），显著提升如“道路阻断”等难类，且具可解释性与实务价值。


<details>
  <summary>Details</summary>
Motivation: 现有CNN难以同时捕获多尺度空间特征，且对外观相似或共现的灾损类别区分不足，影响快速准确的灾后评估与应急决策。

Method: 构建多标签分类框架MCANet：1）以Res2Net为层级主干，丰富跨尺度上下文；2）引入多头、类别特异的残差注意力模块，不同分支关注不同空间粒度，在局部细节与全局语境间权衡；3）采用类激活映射验证定位能力。

Result: 在RescueNet（4,494张飓风迈克尔后UAV影像）上，MCANet mAP=91.75%，优于ResNet、Res2Net、VGG、MobileNet、EfficientNet、ViT；注意力头数为8时mAP=92.35%，困难类别（如道路阻断）AP提升>6%。

Conclusion: MCANet有效提升多标签灾损识别的精度与可解释性，可用于风险制图、应急路径规划、数字孪生响应。未来可与灾害知识图谱和多模态大模型结合，以增强对未见灾情的泛化与语义理解。

Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster
response and recovery. Yet existing CNN-based methods struggle to capture
multi-scale spatial features and to distinguish visually similar or
co-occurring damage types. To address these issues, we propose MCANet, a
multi-label classification framework that learns multi-scale representations
and adaptively attends to spatially relevant regions for each damage category.
MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context
across scales and a multi-head class-specific residual attention module to
enhance discrimination. Each attention branch focuses on different spatial
granularities, balancing local detail with global context. We evaluate MCANet
on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.
MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,
Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,
performance further improves to 92.35%, boosting average precision for
challenging classes such as Road Blocked by over 6%. Class activation mapping
confirms MCANet's ability to localize damage-relevant regions, supporting
interpretability. Outputs from MCANet can inform post-disaster risk mapping,
emergency routing, and digital twin-based disaster response. Future work could
integrate disaster-specific knowledge graphs and multimodal large language
models to improve adaptability to unseen disasters and enrich semantic
understanding for real-world decision-making.

</details>


### [17] [Dynamic Group Detection using VLM-augmented Temporal Groupness Graph](https://arxiv.org/abs/2509.04758)
*Kaname Yokoyama,Chihiro Nakatani,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出一种在视频中进行动态人群组检测的方法，结合帧内局部外观与全局场景上下文，并通过跨时序全局优化实现组结构随时间动态变化的稳健检测，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 复杂人群组不仅依赖个体的局部外观，还需利用场景级全局上下文；现有方法多假设组结构随时间不变，难以处理现实中组的形成、解散与重组的动态过程，因此需要一种能随时间自适应且鲁棒的检测框架。

Method: 在每帧中使用为群组检测增强的视觉-语言模型（VLM，如基于CLIP的groupness特征）同时抽取局部成员外观与全局上下文特征；将所有帧的“组倾向（groupness）”概率构建为图上的代价/相似度，并进行跨全视频的全局优化，允许组结构在时间上动态变化，不依赖“组不变”的假设。

Result: 在公共数据集上，相比现有群组检测SOTA方法取得更优性能；实验表明所提VLM增强的groupness特征与全局图优化能有效提升动态组检测的准确与稳定性。

Conclusion: 利用VLM提取的局部与全局特征结合跨时序图优化，可实现对视频中动态变化的人群组稳定检测，超越当前方法；提供代码以复现与扩展。

Abstract: This paper proposes dynamic human group detection in videos. For detecting
complex groups, not only the local appearance features of in-group members but
also the global context of the scene are important. Such local and global
appearance features in each frame are extracted using a Vision-Language Model
(VLM) augmented for group detection in our method. For further improvement, the
group structure should be consistent over time. While previous methods are
stabilized on the assumption that groups are not changed in a video, our method
detects dynamically changing groups by global optimization using a graph with
all frames' groupness probabilities estimated by our groupness-augmented CLIP
features. Our experimental results demonstrate that our method outperforms
state-of-the-art group detection methods on public datasets. Code:
https://github.com/irajisamurai/VLM-GroupDetection.git

</details>


### [18] [FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph](https://arxiv.org/abs/2509.04772)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 提出FloodVision：结合GPT-4o语义推理与领域知识图谱的零样本洪水水深估计框架，在众包图像上以8.17cm MAE优于GPT-4o基线与以往CNN方法，具备泛化与近实时性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的洪水检测虽能识别淹没，但依赖固定目标检测器与特定任务训练，导致精度不足与跨场景泛化差；需要一种能跨多样场景、准确且可部署的水深估计方法，服务道路通行与应急响应。

Method: 构建FloodVision零样本框架：利用GPT-4o进行语义理解与参考物体识别；以结构化领域知识图谱提供常见城市物体（车辆、行人、基础设施）的标准尺寸以物理约束推理并缓解幻觉；估计物体被淹比率（submergence ratio），结合知识图谱检索高度，进行统计异常值过滤，输出最终水深；在RGB图像上动态选择可见参考物体；无需特定任务再训练。

Result: 在MyCoast New York的110张众包图像上，FloodVision的MAE为8.17cm，相比GPT-4o基线10.28cm降低20.5%，并超过以往CNN方法；表现出良好的跨场景泛化与近实时运行能力。

Conclusion: 将大模型的语义推理与知识图谱的物理先验结合，可实现零样本、可泛化且精确的洪水水深估计；适合集成至数字孪生与市民上报应用，提升智慧城市洪灾韧性。

Abstract: Timely and accurate floodwater depth estimation is critical for road
accessibility and emergency response. While recent computer vision methods have
enabled flood detection, they suffer from both accuracy limitations and poor
generalization due to dependence on fixed object detectors and task-specific
training. To enable accurate depth estimation that can generalize across
diverse flood scenarios, this paper presents FloodVision, a zero-shot framework
that combines the semantic reasoning abilities of the foundation
vision-language model GPT-4o with a structured domain knowledge graph. The
knowledge graph encodes canonical real-world dimensions for common urban
objects including vehicles, people, and infrastructure elements to ground the
model's reasoning in physical reality. FloodVision dynamically identifies
visible reference objects in RGB images, retrieves verified heights from the
knowledge graph to mitigate hallucination, estimates submergence ratios, and
applies statistical outlier filtering to compute final depth values. Evaluated
on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean
absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and
surpassing prior CNN-based methods. The system generalizes well across varying
scenes and operates in near real-time, making it suitable for future
integration into digital twin platforms and citizen-reporting apps for smart
city flood resilience.

</details>


### [19] [Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval](https://arxiv.org/abs/2509.04773)
*Bangxiang Lan,Ruobing Xie,Ruixiang Zhao,Xingwu Sun,Zhanhui Kang,Gang Yang,Xirong Li*

Main category: cs.CV

TL;DR: 提出混合塔(Hybrid-Tower)框架与PIG方法，为T2VR在不增加推理存储/计算开销的前提下，将两塔高效与单塔高效能相结合，利用视频对应的伪查询实现细粒度跨模态交互，在五个基准上将R@1提升1.6%~3.9%，效率与两塔相当、性能接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 两塔(Two-Tower)检索推理高效但跨模态交互弱、效果较差；单塔(Single-Tower)交互强、效果好但推理低效。需要一种同时兼顾有效性与效率的框架以提升T2VR在大规模视频库中的实用性。

Method: 提出Hybrid-Tower框架与PIG(Fine-grained Pseudo-query Interaction and Generation)。核心是为每个视频离线生成一个伪文本查询( pseudo-query )，并让视频特征与该伪查询文本特征进行细粒度交互，提前“单塔式”融合以学习更判别的检索表征；在推理时对真实查询仍采用两塔式高效相似度计算，且无需额外存储或计算开销。

Result: 在五个常用文本-视频检索基准上，较基线R@1提升1.6%~3.9%；总体效率与两塔模型匹配，同时达到接近SOTA的准确率。

Conclusion: Hybrid-Tower + PIG有效弥合两塔与单塔的效率-效果折衷：保持两塔级别的推理效率，同时通过伪查询实现细粒度跨模态交互带来显著效果提升，验证了混合塔范式的优势与可行性。

Abstract: The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by
textual queries with the same semantic meanings. Recent CLIP-based approaches
have explored two frameworks: Two-Tower versus Single-Tower framework, yet the
former suffers from low effectiveness, while the latter suffers from low
efficiency. In this study, we explore a new Hybrid-Tower framework that can
hybridize the advantages of the Two-Tower and Single-Tower framework, achieving
high effectiveness and efficiency simultaneously. We propose a novel hybrid
method, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,
which includes a new pseudo-query generator designed to generate a pseudo-query
for each video. This enables the video feature and the textual features of
pseudo-query to interact in a fine-grained manner, similar to the Single-Tower
approaches to hold high effectiveness, even before the real textual query is
received. Simultaneously, our method introduces no additional storage or
computational overhead compared to the Two-Tower framework during the inference
stage, thus maintaining high efficiency. Extensive experiments on five commonly
used text-video retrieval benchmarks demonstrate that our method achieves a
significant improvement over the baseline, with an increase of $1.6\% \sim
3.9\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower
models while achieving near state-of-the-art performance, highlighting the
advantages of the Hybrid-Tower framework.

</details>


### [20] [Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data](https://arxiv.org/abs/2509.04775)
*R. Makharia,J. G. Singla,Amitabh,N. Dube,H. Sharma*

Main category: cs.CV

TL;DR: 论文评估多种特征匹配算法（SIFT、ASIFT、AKAZE、RIFT2、SuperGlue）在月球跨传感器（光学/高光谱/雷达）影像配准中的表现，提出含地理参考、分辨率对齐、强度归一、直方图均衡、PCA与阴影校正的预处理流程；结果显示SuperGlue在误差与速度上均最优，传统方法在赤道区尚可、极区退化，凸显预处理与学习方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 月球探测需要将来自不同传感器与成像条件的影像准确对齐，以支持制图、资源定位与任务规划；但跨模态、分辨率差异、照度变化与传感器畸变使配准困难，需系统评估算法与预处理策略的有效性。

Method: 构建包含赤道与极区的跨模态影像对（光学、高光谱、雷达），设计预处理管线（地理参考、分辨率对齐、强度归一、自适应直方图均衡、PCA通道选择/融合、阴影校正），对SIFT、ASIFT、AKAZE、RIFT2与深度学习匹配器SuperGlue进行比较，使用配准RMSE与运行时间作为指标。

Result: SuperGlue在所有测试中取得最低RMSE且运行时间最快；SIFT与AKAZE在赤道区表现尚可，但在极区弱光/长阴影条件下显著退化；预处理显著提升各法鲁棒性，尤其在跨模态与极区场景。

Conclusion: 结合完善预处理与学习型匹配器（如SuperGlue）能在多传感器、复杂光照与几何差异条件下实现更稳健的月面影像配准；传统特征法在极端光照场景局限明显，建议在月球任务中优先采用学习方法并重视预处理。

Abstract: Accurate image registration is critical for lunar exploration, enabling
surface mapping, resource localization, and mission planning. Aligning data
from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera,
Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer),
and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya
mission) -- is challenging due to differences in resolution, illumination, and
sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT,
AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using
cross-modality image pairs from equatorial and polar regions. A preprocessing
pipeline is proposed, including georeferencing, resolution alignment, intensity
normalization, and enhancements like adaptive histogram equalization, principal
component analysis, and shadow correction. SuperGlue consistently yields the
lowest root mean square error and fastest runtimes. Classical methods such as
SIFT and AKAZE perform well near the equator but degrade under polar lighting.
The results highlight the importance of preprocessing and learning-based
approaches for robust lunar image registration across diverse conditions.

</details>


### [21] [Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images](https://arxiv.org/abs/2509.04800)
*Asif Newaz,Masum Mushfiq Ishti,A Z M Ashraful Azam,Asif Ur Rahman Adib*

Main category: cs.CV

TL;DR: 构建移动设备拍摄的50+皮肤病类大数据集，比较CNN与Transformer，Swin Transformer表现最佳；结合Grad-CAM提升可解释性，显示对临床相关区域的关注，指向资源受限环境中可及的AI筛查。


<details>
  <summary>Details</summary>
Motivation: 传统皮肤病诊断昂贵、复杂且在低资源地区难以获得；现有自动分类多依赖皮镜图像、类别狭窄，难以覆盖真实世界移动端场景，亟需更广泛数据与更强模型支持可及性诊断。

Method: 1) 构建涵盖50余类皮肤病、以移动设备采集的真实世界数据集；2) 评测多种CNN与Transformer架构；3) 重点分析Swin Transformer对全局上下文的建模能力；4) 引入Grad-CAM用于可解释性，定位与疾病相关的区域。

Result: Transformer整体优于CNN，Swin Transformer取得最优分类性能；Grad-CAM可突出临床相关区域，提升对模型预测的透明度。

Conclusion: Transformer（尤其Swin）适用于移动端皮肤病变分类，结合可解释性方法有望用于低资源环境中的可及性AI辅助筛查与早诊。

Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet
conventional diagnostic methods are often costly, complex, and unavailable in
low-resource settings. Automated classification using deep learning has emerged
as a promising alternative, but existing studies are mostly limited to
dermoscopic datasets and a narrow range of disease classes. In this work, we
curate a large dataset of over 50 skin disease categories captured with mobile
devices, making it more representative of real-world conditions. We evaluate
multiple convolutional neural networks and Transformer-based architectures,
demonstrating that Transformer models, particularly the Swin Transformer,
achieve superior performance by effectively capturing global contextual
features. To enhance interpretability, we incorporate Gradient-weighted Class
Activation Mapping (Grad-CAM), which highlights clinically relevant regions and
provides transparency in model predictions. Our results underscore the
potential of Transformer-based approaches for mobile-acquired skin lesion
classification, paving the way toward accessible AI-assisted dermatological
screening and early diagnosis in resource-limited environments.

</details>


### [22] [Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation](https://arxiv.org/abs/2509.04816)
*Svetlana Pavlitska,Beyza Keskin,Alwin Faßbender,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出利用专家混合（MoE）在无需改动结构的情况下，从语义分割模型中直接提取校准良好的预测不确定性；与集成方法相比，在OOD条件正确性上更可靠，并探讨路由不确定性与专家数量对校准的影响。


<details>
  <summary>Details</summary>
Motivation: 在交通场景等安全关键视觉任务中，需要既准确又校准良好的不确定性估计。常用的集成法代价高且推理时不够高效；MoE可通过门控网络按输入自适应聚合专家，可能以更低成本提供不确定性，但其不确定性的提取与校准特性尚需系统验证。

Method: 基于先前用于语义分割的MoE框架，不改动架构直接从MoE输出提取三类预测不确定性：预测熵、互信息、专家方差；并评估门控熵作为路由不确定性。采用两个专家在A2D2语义拆分训练，比较MoE与模型集成在OOD条件正确性指标上的不确定性质量；进一步在Cityscapes上考察增加专家数量对校准的影响，并比较简单门控与更复杂的按类别门控。

Result: 在A2D2 OOD测试中，MoE的不确定性在条件正确性等指标上优于集成；简单门控机制的路由不确定性（门控熵）比复杂类级门控校准更好；在Cityscapes上，增加专家数量可进一步提升不确定性校准。

Conclusion: MoE无需架构更改即可提供校准良好的预测与路由不确定性，较集成在OOD场景下更可靠；选择简单的门控并适当增加专家数能进一步改进校准效果。

Abstract: Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.

</details>


### [23] [Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution](https://arxiv.org/abs/2509.04824)
*Haosong Liu,Xiancheng Zhu,Huanqiang Zeng,Jianqing Zhu,Jiuwen Cao,Junhui Hou*

Main category: cs.CV

TL;DR: 提出LFMT：结合Mamba与Transformer的混合框架，通过子空间简单扫描(Sub-SS)与双阶段建模（SA-RSMB浅层空间-角度、EPMB+EPTB深层EPI）高效捕获长程与非局部空间-角度关联，实现LFSR在精度与复杂度上的兼优，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于Mamba的LFSR虽具长程建模与线性复杂度优势，但多方向扫描在复杂光场数据上产生低效冗余特征提取；同时状态空间难以同时保留空间-角度与视差信息，限制对非局部空间-角度关联的充分挖掘。

Method: 1) 提出子空间简单扫描(Sub-SS)策略，设计Subspace Simple Mamba Block(SSMB)以减少冗余、提升提取效率与精度。2) 双阶段建模：阶段I使用Spatial-Angular Residual Subspace Mamba Block(SA-RSMB)进行浅层空间-角度特征提取；阶段II采用双分支并行的Epipolar Plane Mamba Block(EPMB)与Epipolar Plane Transformer Block(EPTB)进行深层EPI特征精炼。3) 构建混合Mamba-Transformer框架LFMT，统筹空间、角度与EPI域的信息探索。

Result: 在真实与合成光场数据集上，相较现有SOTA显著提升LFSR性能，同时保持较低计算复杂度。

Conclusion: Sub-SS与双阶段/双分支设计有效缓解多向扫描冗余与状态空间保真不足的问题；LFMT融合Mamba与Transformer优势，全面挖掘空间-角度与EPI非局部关联，实现性能与效率兼得，并在多数据集上取得领先表现。

Abstract: Recently, Mamba-based methods, with its advantage in long-range information
modeling and linear complexity, have shown great potential in optimizing both
computational cost and performance of light field image super-resolution
(LFSR). However, current multi-directional scanning strategies lead to
inefficient and redundant feature extraction when applied to complex LF data.
To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)
strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to
achieve more efficient and precise feature extraction. Furthermore, we propose
a dual-stage modeling strategy to address the limitation of state space in
preserving spatial-angular and disparity information, thereby enabling a more
comprehensive exploration of non-local spatial-angular correlations.
Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace
Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage
II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba
Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar
feature refinement. Building upon meticulously designed modules and strategies,
we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates
the strengths of Mamba and Transformer models for LFSR, enabling comprehensive
information exploration across spatial, angular, and epipolar-plane domains.
Experimental results demonstrate that LFMT significantly outperforms current
state-of-the-art methods in LFSR, achieving substantial improvements in
performance while maintaining low computational complexity on both real-word
and synthetic LF datasets.

</details>


### [24] [PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination](https://arxiv.org/abs/2509.04833)
*Ming Dai,Wenxuan Cheng,Jiedong Zhuang,Jiang-jiang Liu,Hongshen Zhao,Zhenhua Feng,Wankou Yang*

Main category: cs.CV

TL;DR: PropVG提出首个端到端的“提案+指代”一体化视觉指代框架，并通过对比式评分与多粒度判别模块，在多数据集上显著提升REC/RES与缺失目标识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前端到端直指范式效率高但只依赖被指目标监督，忽视潜在显著候选目标；同时缺乏多粒度判别，导致复杂场景下识别鲁棒性不足。

Method: 1) PropVG：端到端提案式框架，联合前景目标提案生成与指代理解，无需额外检测器；2) CRS模块：在句子级与词级进行对比学习，增强对被指目标与干扰目标的区分；3) MTD模块：融合目标级与语义级信息，提升对缺失/不在场目标的辨识。

Result: 在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO、RefCOCO (REC/RES)等基准上取得显著性能提升，验证所提框架与模块的有效性。

Conclusion: 将提案生成与指代理解无缝整合，并通过对比学习与多粒度融合，改善定位与分割精度及对缺失目标的识别；方法通用、端到端、无需外部检测器。

Abstract: Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.

</details>


### [25] [TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution](https://arxiv.org/abs/2509.04834)
*Yifei Jia,Shiyu Cheng,Yu Dong,Guan Li,Dong Tian,Ruixiao Peng,Xuyi Lu,Yu Wang,Wei Yao,Guihua Shan*

Main category: cs.CV

TL;DR: 提出TemporalFlowViz：面向参数的可视分析流程，用预训练ViT提取时间序列燃烧流场帧的高维特征，降维+密度聚类发现潜在燃烧模式，并在嵌入空间构建时间轨迹；专家为簇中心打标签，结合VLM生成帧级与案例级自然语言总结；系统支持参数筛选、相似检索与多视图联动，助力解释性模式发现与跨案例比较。


<details>
  <summary>Details</summary>
Motivation: scramjet燃烧仿真数据规模大、维度高且为时间序列，给视觉解读、特征区分与不同工况比较带来困难，需要一种能将高维时空模式压缩、分簇并可与领域知识对齐的工具，以加速假设形成与知识发现。

Method: - 数据：数百个不同初始条件的燃烧仿真实验，每个产出时间序列流场图像。
- 表征：用预训练Vision Transformer对每帧提取高维嵌入。
- 模式发现：降维（如UMAP/t-SNE）+密度聚类（如HDBSCAN）得到潜在燃烧模式簇；在嵌入空间构建各模拟的时间轨迹。
- 语义对齐：专家给簇中心打描述性标签，作为上下文提示给视觉-语言模型，生成帧级与案例级自然语言总结。
- 交互：参数过滤、相似案例检索、协调多视图联动探索。

Result: 两个专家参与的案例研究与反馈显示：系统能揭示潜在燃烧模式与其时间演化，提供可解释的文本摘要，提升跨案例比较与相似检索效果，促进假设生成与模式理解。

Conclusion: TemporalFlowViz将自监督视觉表征、聚类与专家语义标注、VLM生成和交互可视分析结合，显著增强大规模scramjet燃烧时序数据的可解释分析与知识发现能力。

Abstract: Understanding the complex combustion dynamics within scramjet engines is
critical for advancing high-speed propulsion technologies. However, the large
scale and high dimensionality of simulation-generated temporal flow field data
present significant challenges for visual interpretation, feature
differentiation, and cross-case comparison. In this paper, we present
TemporalFlowViz, a parameter-aware visual analytics workflow and system
designed to support expert-driven clustering, visualization, and interpretation
of temporal flow fields from scramjet combustion simulations. Our approach
leverages hundreds of simulated combustion cases with varying initial
conditions, each producing time-sequenced flow field images. We use pretrained
Vision Transformers to extract high-dimensional embeddings from these frames,
apply dimensionality reduction and density-based clustering to uncover latent
combustion modes, and construct temporal trajectories in the embedding space to
track the evolution of each simulation over time. To bridge the gap between
latent representations and expert reasoning, domain specialists annotate
representative cluster centroids with descriptive labels. These annotations are
used as contextual prompts for a vision-language model, which generates
natural-language summaries for individual frames and full simulation cases. The
system also supports parameter-based filtering, similarity-based case
retrieval, and coordinated multi-view exploration to facilitate in-depth
analysis. We demonstrate the effectiveness of TemporalFlowViz through two
expert-informed case studies and expert feedback, showing TemporalFlowViz
enhances hypothesis generation, supports interpretable pattern discovery, and
enhances knowledge discovery in large-scale scramjet combustion analysis.

</details>


### [26] [Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations](https://arxiv.org/abs/2509.04848)
*Enze Ye,Wei Lin,Shaochi Ren,Yakun Liu,Xiaoping Li,Hao Wang,He Sun,Feng Pan*

Main category: cs.CV

TL;DR: 提出OmniFHT：一种无需已知姿态的高速流式3D折射率成像方法，结合傅里叶衍射定理与隐式神经表示，联合同步估计细胞多轴旋转轨迹与体素结构，实现稀疏视角与有限角覆盖下的高保真重建，适用于不规则细胞，实现对整个人群的在流全断层成像与无标记形态计量分析。


<details>
  <summary>Details</summary>
Motivation: 现有流式3D相位成像依赖假设细胞围绕单轴均匀旋转并且姿态已知，这仅适用于近球形细胞，导致对形状复杂、旋转多轴的不规则细胞成像不准确，从而限制了人群层面统计分析与应用范围。需要一种可在未知姿态、任意几何和多轴旋转条件下仍能高通量、高保真重建3D折射率的方法。

Method: 基于弱散射近似下的傅里叶衍射定理，将多角度透射（在流条件下获得）与隐式神经表示（INR）相结合，把每个细胞的体积RI场和其随时间变化的旋转轨迹作为可学习的连续参数，共同优化；INR的连续特性允许从稀疏视角和受限角度范围进行重建。

Result: 方法对任意几何和多轴旋转细胞有效，能在视角数目极少（如10个）或仅120度角覆盖时仍获得高保真3D RI重建；在高通量流式条件下实现整个人群的在位断层成像。

Conclusion: OmniFHT实现了无需已知姿态的流式3D RI断层成像，扩展至不规则与多轴旋转细胞，降低对视角覆盖与采样密度的要求，从而提供可扩展、无偏的群体级无标记形态计量分析能力。

Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables
label-free, volumetric characterization of individual cells by reconstructing
their refractive index (RI) distributions from multiple viewing angles during
flow through microfluidic channels. However, current imaging methods assume
that cells undergo uniform, single-axis rotation, which require their poses to
be known at each frame. This assumption restricts applicability to
near-spherical cells and prevents accurate imaging of irregularly shaped cells
with complex rotations. As a result, only a subset of the cellular population
can be analyzed, limiting the ability of flow-based assays to perform robust
statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction
framework that leverages the Fourier diffraction theorem and implicit neural
representations (INRs) for high-throughput flow cytometry tomographic imaging.
By jointly optimizing each cell's unknown rotational trajectory and volumetric
structure under weak scattering assumptions, OmniFHT supports arbitrary cell
geometries and multi-axis rotations. Its continuous representation also allows
accurate reconstruction from sparsely sampled projections and restricted
angular coverage, producing high-fidelity results with as few as 10 views or
only 120 degrees of angular range. OmniFHT enables, for the first time, in
situ, high-throughput tomographic imaging of entire flowing cell populations,
providing a scalable and unbiased solution for label-free morphometric analysis
in flow cytometry platforms.

</details>


### [27] [CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus](https://arxiv.org/abs/2509.04859)
*Hannah Schieber,Dominik Frischmann,Simon Boche,Victor Schaack,Angela Schoellig,Stefan Leutenegger,Daniel Roth*

Main category: cs.CV

TL;DR: 提出CoRe-GS：在移动机器人场景中，先用语义高斯成像快速得到可分割粗场景，再对关注对象（PoI）进行颜色引导的有效滤波精修，从而以约3/4训练时间实现更快更优的新视角合成与重建；在SCRREAM与NeRDS 360上验证了更短运行时与更高NVS质量。


<details>
  <summary>Details</summary>
Motivation: 移动自主空中机器人在遥操作与灾害响应中需要既快又准的3D重建。全场景精细优化耗时且不必要，因为任务往往只关注特定目标（PoIs），且这些目标可在早期探测得到。近期Gaussian Splatting（GS）能增量学习并支持语义编辑，但完整语义GS训练周期较长，若能在训练未完成前实现语义对象的隔离与精修，可显著降低时间成本。

Method: 提出CoRe-GS两阶段：1) 语义GS生成“可分割的粗场景”，在训练未完成前即可进行语义编辑；2) 针对已知PoI，采用新颖的基于颜色的有效滤波（effective filtering）以隔离并精修语义对象，集中计算资源到PoI上，从而加速训练与提升质量。

Result: 在SCRREAM（真实户外）与NeRDS 360（合成室内）上，CoRe-GS相对完整语义GS训练实现约25%训练时间缩减（即训练时长减少约四分之一），同时得到更高的新视角合成（NVS）质量与更短的整体运行时。

Conclusion: 围绕任务相关PoI的语义引导与颜色滤波精修，CoRe-GS在保持/提升NVS质量的同时显著缩短训练时间，适合移动重建与无人机等实时性场景；方法在真实与合成数据上均验证有效。

Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential
for critical applications such as tele-guidance and disaster response. These
tasks demand both accurate 3D reconstruction and fast scene processing. Instead
of reconstructing the entire scene in detail, it is often more efficient to
focus on specific objects, i.e., points of interest (PoIs). Mobile robots
equipped with advanced sensing can usually detect these early during data
acquisition or preliminary analysis, reducing the need for full-scene
optimization. Gaussian Splatting (GS) has recently shown promise in delivering
high-quality novel view synthesis and 3D representation by an incremental
learning process. Extending GS with scene editing, semantics adds useful
per-splat features to isolate objects effectively.
  Semantic 3D Gaussian editing can already be achieved before the full training
cycle is completed, reducing the overall training time. Moreover, the
semantically relevant area, the PoI, is usually already known during capturing.
To balance high-quality reconstruction with reduced training time, we propose
CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS
and then refine it for the semantic object using our novel color-based
effective filtering for effective object isolation. This is speeding up the
training process to be about a quarter less than a full training cycle for
semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,
outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher
novel-view-synthesis quality.

</details>


### [28] [Cryo-RL: automating prostate cancer cryoablation planning with reinforcement learning](https://arxiv.org/abs/2509.04886)
*Trixia Simangan,Ahmed Nadeem Abbasi,Yipeng Hu,Shaheer U. Saeed*

Main category: cs.CV

TL;DR: 提出Cryo-RL，将前列腺癌冷冻消融术的探针放置规划建模为MDP，用强化学习在仿真环境中顺序选择探针位置与冰球尺寸，以肿瘤覆盖为核心奖励，无需人工方案即可学得最优策略；在583例回顾病例上Dice较几何优化基线提升8个点，匹配专家且更省时。


<details>
  <summary>Details</summary>
Motivation: 现有冷冻消融探针规划依赖人工，耗时、经验依赖强、可变性大且难以规模化；自动化几何优化方法效果有限，难以同时满足临床解剖约束与术中不确定性。需要一种能在复杂约束和随机性下学习接近专家、可复现且高效的规划方法。

Method: 将探针放置建模为马尔可夫决策过程：状态包含当前已放置探针与覆盖情况；动作是选择下一枚探针的位置与冰球直径；在包含临床约束与术中随机性的仿真环境中与环境交互；以肿瘤覆盖（Dice等）为主的奖励函数驱动策略学习；通过强化学习训练得到顺序决策策略，无需人工标注的规划。

Result: 在583例前列腺癌回顾数据上，Cryo-RL较最佳几何优化自动基线的Dice提升>8个百分点；与人类专家规划性能相当，同时显著减少规划时间。

Conclusion: 强化学习可在满足临床约束与应对术中变异的同时，实现高覆盖、可复现且高效的冷冻消融探针规划；Cryo-RL展示了将RL用于介入治疗术前规划的临床可行性与潜在推广价值。

Abstract: Cryoablation is a minimally invasive localised treatment for prostate cancer
that destroys malignant tissue during de-freezing, while sparing surrounding
healthy structures. Its success depends on accurate preoperative planning of
cryoprobe placements to fully cover the tumour and avoid critical anatomy. This
planning is currently manual, expertise-dependent, and time-consuming, leading
to variability in treatment quality and limited scalability. In this work, we
introduce Cryo-RL, a reinforcement learning framework that models cryoablation
planning as a Markov decision process and learns an optimal policy for
cryoprobe placement. Within a simulated environment that models clinical
constraints and stochastic intraoperative variability, an agent sequentially
selects cryoprobe positions and ice sphere diameters. Guided by a reward
function based on tumour coverage, this agent learns a cryoablation strategy
that leads to optimal cryoprobe placements without the need for any
manually-designed plans. Evaluated on 583 retrospective prostate cancer cases,
Cryo-RL achieved over 8 percentage-point Dice improvements compared with the
best automated baselines, based on geometric optimisation, and matched human
expert performance while requiring substantially less planning time. These
results highlight the potential of reinforcement learning to deliver clinically
viable, reproducible, and efficient cryoablation plans.

</details>


### [29] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: 用预训练视觉模型微调来预测“蜘蛛图片引发的恐惧评分”，在313张标准化图集上通过迁移学习与交叉验证取得MAE≈10–11，解释性分析指向与蜘蛛相关的视觉特征，误差在远景和人工/绘制蜘蛛上更高；数据量过少显著伤绩效，继续增大数据未带来明显收益。


<details>
  <summary>Details</summary>
Motivation: 为实现可根据患者反应动态调节刺激的计算机化暴露疗法，需要能从视觉刺激中自动估计恐惧强度，验证现成计算机视觉模型是否能可靠预测对蜘蛛图像的恐惧评分，并评估解释性与数据规模对性能的影响。

Method: 选取三种异质的预训练视觉模型，采用迁移学习对其进行微调，用313张带0–100恐惧评分的蜘蛛相关图像进行训练与交叉验证；绘制学习曲线评估数据规模效应；利用可解释性方法检查模型关注的图像区域；按类别（例如视距、真实/人工）做误差分析。

Result: 三模型交叉验证MAE约10.1–11.0；减小训练集显著恶化性能，继续增大样本后收益递减；解释性热图聚焦蜘蛛相关区域；在远景和人工/绘制蜘蛛类别上误差更高。

Conclusion: 可解释的计算机视觉模型能在一定精度上预测对蜘蛛图像的恐惧评分；开发情绪感知治疗技术需兼顾模型可解释性与足够的数据规模，同时关注易出错的视觉情境以改进鲁棒性。

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


### [30] [SynGen-Vision: Synthetic Data Generation for training industrial vision models](https://arxiv.org/abs/2509.04894)
*Alpana Dubey,Suma Mani Kuriakose,Nitish Bhardwaj*

Main category: cs.CV

TL;DR: 提出用视觉语言模型结合3D仿真与渲染引擎合成训练数据，以解决工业磨损/锈蚀检测数据稀缺，训练出的锈蚀检测模型在真实图像上mAP50=0.87，优于对比方法，方法可扩展到其他磨损场景。


<details>
  <summary>Details</summary>
Motivation: 工业预测性维护需要准确的磨损与老化（如锈蚀）检测，但真实数据采集昂贵且覆盖不同场景困难，限制了CV模型的训练与泛化，因此需要一种高效可控的数据生成方案以缓解数据匮乏与标注成本问题。

Method: 利用视觉语言模型指导3D仿真与渲染引擎，生成覆盖多种锈蚀条件的合成图像数据集；用该合成数据训练一个锈蚀检测模型，并在真实工业对象图像上进行测试，与其他训练方案对比。

Result: 使用所生成的合成数据训练的模型在真实图像上的mAP50达到0.87，优于其他方法。

Conclusion: 合成数据管线能有效提升工业锈蚀检测性能，具有可定制性与可扩展性，可推广至其他工业磨损与老化检测任务。

Abstract: We propose an approach to generate synthetic data to train computer vision
(CV) models for industrial wear and tear detection. Wear and tear detection is
an important CV problem for predictive maintenance tasks in any industry.
However, data curation for training such models is expensive and time-consuming
due to the unavailability of datasets for different wear and tear scenarios.
Our approach employs a vision language model along with a 3D simulation and
rendering engine to generate synthetic data for varying rust conditions. We
evaluate our approach by training a CV model for rust detection using the
generated dataset and tested the trained model on real images of rusted
industrial objects. The model trained with the synthetic data generated by our
approach, outperforms the other approaches with a mAP50 score of 0.87. The
approach is customizable and can be easily extended to other industrial wear
and tear detection scenarios

</details>


### [31] [Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting](https://arxiv.org/abs/2509.04895)
*Maryam Adelipour,Gustavo Carneiro,Jeongkwon Kim*

Main category: cs.CV

TL;DR: 提出用于皮脂细胞图像分析的注意力型MIL框架，并与简单的MLP聚合基线比较；结果显示MLP更稳定、误差更低，而MIL偶有更优但不稳定，提示需改进池化与正则化。


<details>
  <summary>Details</summary>
Motivation: 手工统计皮脂细胞内脂滴数量既耗时又主观，需要自动化且可量化的方法来提升效率与一致性。

Method: 将Nile Red染色的皮脂细胞图像按脂滴数量标注为14类，并经数据增强扩充至约5万细胞；对比两种模型：1) 以聚合的patch级计数训练的MLP基线；2) 以ResNet-50特征为实例输入、带注意力加权的MIL模型；采用五折交叉验证评估，指标为MAE。

Result: MLP在各折表现更稳定，平均MAE为5.6；注意力MIL平均MAE为10.7且一致性较差，但在某些折中可超过MLP。

Conclusion: 对该任务而言，简单的包级（bag-level）聚合方法是稳健基线；注意力MIL需更贴合任务的池化策略与正则化设计，方能发挥潜力用于皮脂细胞脂滴计数。

Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the
accumulation of intracellular lipid droplets, making their quantification a key
readout in sebocyte biology. Manual counting is labor-intensive and subjective,
motivating automated solutions. Here, we introduce a simple attention-based
multiple instance learning (MIL) framework for sebocyte image analysis. Nile
Red-stained sebocyte images were annotated into 14 classes according to droplet
counts, expanded via data augmentation to about 50,000 cells. Two models were
benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated
patch-level counts, and an attention-based MIL model leveraging ResNet-50
features with instance weighting. Experiments using five-fold cross-validation
showed that the baseline MLP achieved more stable performance (mean MAE = 5.6)
compared with the attention-based MIL, which was less consistent (mean MAE =
10.7) but occasionally superior in specific folds. These findings indicate that
simple bag-level aggregation provides a robust baseline for slide-level droplet
counting, while attention-based MIL requires task-aligned pooling and
regularization to fully realize its potential in sebocyte image analysis.

</details>


### [32] [UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features](https://arxiv.org/abs/2509.04932)
*Haowang Cui,Rui Chen,Tao Luo,Rui Li,Jiaze Wang*

Main category: cs.CV

TL;DR: 提出UniView：利用相似对象的参考图像与MLLM辅助检索、适配器生成参考特征、以及三重解耦注意力来对齐融合多分支特征，从而缓解单视图新视角合成中的歧义与失真，并在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 单幅图像新视角合成对未观测区域存在多重解释，现有方法多依赖模糊先验与邻域插值，易产生严重形变与细节缺失。需要一种能够引入外部强先验（来自相似对象）的机制，以稳健补全不可见区域并减少失真。

Method: 1) 构建“检索+增强”系统：从大规模图库检索相似对象图像，并用多模态大语言模型评估/筛选满足视角与语义要求的参考图像；2) 设计可插拔的适配器模块，含多级隔离层，按目标视角动态生成参考特征，避免干扰主干表示；3) 提出解耦三重注意力机制，面向输入图、参考分支与目标视角分支进行对齐与融合，既利用参考先验又保持源图细节。

Result: 在多项挑战性数据集上，相比现有SOTA显著提升新视角合成质量（失真降低、细节更好、总体指标优于对比方法）。

Conclusion: 引入相似对象参考先验、配合MLLM检索筛选与适配器+三重注意力的融合策略，可有效缓解单图新视角合成的歧义与形变问题，取得了领先性能。

Abstract: The task of synthesizing novel views from a single image is highly ill-posed
due to multiple explanations for unobserved areas. Most current methods tend to
generate unseen regions from ambiguity priors and interpolation near input
views, which often lead to severe distortions. To address this limitation, we
propose a novel model dubbed as UniView, which can leverage reference images
from a similar object to provide strong prior information during view
synthesis. More specifically, we construct a retrieval and augmentation system
and employ a multimodal large language model (MLLM) to assist in selecting
reference images that meet our requirements. Additionally, a plug-and-play
adapter module with multi-level isolation layers is introduced to dynamically
generate reference features for the target views. Moreover, in order to
preserve the details of an original input image, we design a decoupled triple
attention mechanism, which can effectively align and integrate multi-branch
features into the synthesis process. Extensive experiments have demonstrated
that our UniView significantly improves novel view synthesis performance and
outperforms state-of-the-art methods on the challenging datasets.

</details>


### [33] [Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper](https://arxiv.org/abs/2509.04957)
*Gehui Chen,Guan'an Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.CV

TL;DR: 提出MFM-Mapper：利用双视觉编码器特征融合，并以GPT-2取代线性映射器，将视频特征更好地对齐到文本到音频生成模型，实现更高语义/时序一致性与显著更低训练开销（仅16%）。


<details>
  <summary>Details</summary>
Motivation: 现有V2A需从视频中抽取语义与时序特征以条件生成，但从零训练代价高。借助跨模态泛化强的基础模型成为趋势。已有工作用轻量映射器把视觉编码器接到文本到音频生成器，但语义/时序信息与对齐能力仍受限。

Method: 提出Multiple Foundation Model Mapper（MFM-Mapper）：1）并行使用两个预训练视觉编码器，进行特征融合以获得更丰富的语义与时序线索；2）将传统线性映射器替换为GPT-2，将跨模态特征映射视作自回归翻译任务，以提升对齐与表达能力；3）整体作为轻量可微组件，微调连接视觉编码与TTA（text-to-audio）生成模型。

Result: 在语义与时序一致性评测上优于先前的mapper方法；训练效率显著提升，仅用其16%的训练规模达成更好表现；与大规模训练的模型相比也具备有竞争力的效果。

Conclusion: 多FM特征融合与GPT-2自回归映射可有效提升V2A的语义/时序一致性，同时显著降低训练资源需求，为高效V2A提供实用方案。

Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and
temporal features from video to condition generative models. Training these
models from scratch is resource intensive. Consequently, leveraging foundation
models (FMs) has gained traction due to their cross-modal knowledge transfer
and generalization capabilities. One prior work has explored fine-tuning a
lightweight mapper network to connect a pre-trained visual encoder with a
text-to-audio generation model for V2A. Inspired by this, we introduce the
Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper
approach, MFM-Mapper benefits from richer semantic and temporal information by
fusing features from dual visual encoders. Furthermore, by replacing a linear
mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels
between cross-modal features mapping and autoregressive translation tasks. Our
MFM-Mapper exhibits remarkable training efficiency. It achieves better
performance in semantic and temporal consistency with fewer training consuming,
requiring only 16\% of the training scale compared to previous mapper-based
work, yet achieves competitive performance with models trained on a much larger
scale.

</details>


### [34] [Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework](https://arxiv.org/abs/2509.05000)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: 提出GD^2Fusion：结合VLM感知退化与频/空双域联合优化的一体化IVIF框架，避免预增强-融合解耦带来的误差累积，在双源退化场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有红外-可见光融合通常假设输入高质量。面对双源退化（如噪声、模糊、低照等），需手工挑选并串行应用多步预增强，导致误差累积、效率低、稳健性差。需要一个能同时感知退化并联合处理与融合的端到端框架。

Method: 提出Guided Dual-Domain Fusion（GD^2Fusion）。利用VLM进行退化感知与引导。1）频域：GFMSE模块在频域进行退化感知与抑制，区分性地提取与融合相关的子带特征；2）空域：GSMAF模块在空域执行跨模态退化滤除与自适应多源特征聚合，增强模态互补与结构一致性。整体实现频/空双域联合优化，端到端完成去退化与融合。

Result: 在多种双源退化场景下进行了大量定性与定量实验，GD^2Fusion在融合质量上优于现有算法与策略，表现出更强的鲁棒性与一致性。

Conclusion: 将VLM的退化感知与频/空双域联合优化相结合的GD^2Fusion可有效避免预增强-融合管线的误差累积，在复杂退化条件下取得领先融合性能；代码将在论文接收后开源。

Abstract: Most existing infrared-visible image fusion (IVIF) methods assume
high-quality inputs, and therefore struggle to handle dual-source degraded
scenarios, typically requiring manual selection and sequential application of
multiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion
pipeline inevitably leads to error accumulation and performance degradation. To
overcome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),
a novel framework that synergistically integrates vision-language models (VLMs)
for degradation perception with dual-domain (frequency/spatial) joint
optimization. Concretely, the designed Guided Frequency Modality-Specific
Extraction (GFMSE) module performs frequency-domain degradation perception and
suppression and discriminatively extracts fusion-relevant sub-band features.
Meanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries
out cross-modal degradation filtering and adaptive multi-source feature
aggregation in the spatial domain to enhance modality complementarity and
structural consistency. Extensive qualitative and quantitative experiments
demonstrate that GD^2Fusion achieves superior fusion performance compared with
existing algorithms and strategies in dual-source degraded scenarios. The code
will be publicly released after acceptance of this paper.

</details>


### [35] [Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study](https://arxiv.org/abs/2509.05004)
*Mohammad Abbadi,Yassine Himeur,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 研究比较经典机器学习与深度学习在乳腺超声图像癌症分类中的效果，ResNet‑18 表现最佳（约99.7%准确率、恶性灵敏度100%），Grad‑CAM提透明性，支持临床集成。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌致死率高、超声安全且适用于致密乳腺，但人工判读主观且耗时；需要评估并验证机器学习/深度学习方法在多数据集上的可行性与临床可用性。

Method: 基于多公开超声数据集（BUSI、BUS-BRA、BrEaST-Lesions USG），比较SVM、KNN等经典模型与ResNet‑18、EfficientNet‑B0、GoogLeNet等CNN；同时使用深度特征提升经典模型；采用Grad‑CAM进行可解释性可视化；以准确率与恶性灵敏度等指标评估。

Result: ResNet‑18在三数据集上的总体准确率最高（达99.7%），对恶性病灶灵敏度达到100%；经典模型在直接训练时弱于CNN，但结合深度特征后达到有竞争力的性能；Grad‑CAM能突出与诊断相关的区域，提高模型透明度。

Conclusion: 深度CNN（尤其ResNet‑18）在超声乳腺癌检测中表现卓越；结合Grad‑CAM可构建高性能、可解释的AI工具，具备临床工作流集成与部署的可行性。

Abstract: Breast cancer remains a leading cause of cancer-related mortality among women
worldwide. Ultrasound imaging, widely used due to its safety and
cost-effectiveness, plays a key role in early detection, especially in patients
with dense breast tissue. This paper presents a comprehensive study on the
application of machine learning and deep learning techniques for breast cancer
classification using ultrasound images. Using datasets such as BUSI, BUS-BRA,
and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,
KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,
GoogLeNet). Experimental results show that ResNet-18 achieves the highest
accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML
models, though outperformed by CNNs, achieve competitive performance when
enhanced with deep feature extraction. Grad-CAM visualizations further improve
model transparency by highlighting diagnostically relevant image regions. These
findings support the integration of AI-based diagnostic tools into clinical
workflows and demonstrate the feasibility of deploying high-performing,
interpretable systems for ultrasound-based breast cancer detection.

</details>


### [36] [A biologically inspired separable learning vision model for real-time traffic object perception in Dark](https://arxiv.org/abs/2509.05012)
*Hulin Li,Qiliang Ren,Jun Li,Hanbing Wei,Zheng Liu,Linfang Fan*

Main category: cs.CV

TL;DR: 提出Dark-traffic低光交通数据集与SLVM模型，在检测、分割与光流任务上于低光环境显著优于现有方法，并以更低计算开销达SOTA。


<details>
  <summary>Details</summary>
Motivation: 低光交通场景中，光照退化导致视觉线索贫乏，主流感知模型难以快速适配并精准预测；同时缺乏大规模、专注低光交通的基准数据集，限制了研究与评测。

Method: 1) 构建物理可解释的低光退化方法，合成并发布大规模、稠密标注的Dark-traffic数据集，覆盖检测、实例分割、光流任务。2) 提出生物启发的SLVM框架，包括：a. 类瞳孔的光自适应机制以进行光照敏感特征提取；b. 特征级可分学习策略以高效表示；c. 面向多任务的任务解耦分支，实现可分学习；d. 空间错位感知融合模块，精确对齐并融合多源特征。

Result: 在Dark-traffic上：检测较RT-DETR提升11.2个百分点，实例分割较YOLOv12提升6.1个百分点，光流端点误差EPE较基线降低12.37%。在LIS基准上：端到端SLVM较Swin Transformer+EnlightenGAN与ConvNeXt-T+EnlightenGAN在关键指标上平均领先约11个点，并较带光照增强的Mask RCNN高3.1个点。计算开销更低。

Conclusion: 通过物理建模的低光退化与大规模低光交通数据集，以及具备光适应与可分学习/对齐机制的SLVM，显著提升低光条件下的多任务感知精度与效率，为低光交通场景提供强基准与有效方法。

Abstract: Fast and accurate object perception in low-light traffic scenes has attracted
increasing attention. However, due to severe illumination degradation and the
lack of reliable visual cues, existing perception models and methods struggle
to quickly adapt to and accurately predict in low-light environments. Moreover,
there is the absence of available large-scale benchmark specifically focused on
low-light traffic scenes. To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation. We further propose the Separable
Learning Vision Model (SLVM), a biologically inspired framework designed to
enhance perception under adverse lighting. SLVM integrates four key components:
a light-adaptive pupillary mechanism for illumination-sensitive feature
extraction, a feature-level separable learning strategy for efficient
representation, task-specific decoupled branches for multi-task separable
learning, and a spatial misalignment-aware fusion module for precise
multi-feature alignment. Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead. Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end
trained SLVM surpasses Swin Transformer+EnlightenGAN and
ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key
metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage
points. The Dark-traffic dataset and complete code is released at
https://github.com/alanli1997/slvm.

</details>


### [37] [Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition](https://arxiv.org/abs/2509.05019)
*Mohsine El Khayati,Ayyad Maafiri,Yassine Himeur,Hamzah Ali Alkhazaleh,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 研究将迁移学习与移动端轻量CNN（MbNets）结合用于阿拉伯手写字符识别（AHCR），比较全量微调、部分微调与从零训练在四个轻量网络上的效果，发现全量微调总体最佳；MobileNet综合表现最强，ShuffleNet泛化好；不同数据集上最佳模型与精度各异。


<details>
  <summary>Details</summary>
Motivation: AHCR面临计算资源受限与标注数据稀缺两大难题。移动端部署需要高效轻量模型，而数据不足又限制从零训练的效果。研究动机是评估将迁移学习引入轻量MbNets，能否在有限资源和多数据集条件下兼顾精度、鲁棒性与高效性。

Method: 选取四个轻量网络（MobileNet、SqueezeNet、MnasNet、ShuffleNet），在三种训练策略（全量微调、部分微调、从零训练）下，于三套基准数据集（AHCD、HIJJA、IFHCDB）进行系统实验与对比，评估指标包括准确率、收敛速度、鲁棒性与泛化能力。

Result: 整体上，全量微调表现最佳，兼具较高准确率与较快收敛；部分微调在多项指标上欠佳。MobileNet在准确率、鲁棒性与效率上最稳健；ShuffleNet在泛化上突出，尤其在全量微调时。按数据集：IFHCDB上MnasNet+全量微调达约99%准确率；AHCD上ShuffleNet约97%；HIJJA因样本变异性高，最佳为ShuffleNet约92%。

Conclusion: 迁移学习与轻量MbNets的结合可在资源受限场景下有效提升AHCR性能。全量微调通常优于部分微调和从零训练；不同数据集需匹配不同架构。未来应在结构改造、数据增强、数据集特征剖析及敏感性分析上进一步优化，以提升鲁棒性与泛化。

Abstract: The study explores the integration of transfer learning (TL) with
mobile-enabled convolutional neural networks (MbNets) to enhance Arabic
Handwritten Character Recognition (AHCR). Addressing challenges like extensive
computational requirements and dataset scarcity, this research evaluates three
TL strategies--full fine-tuning, partial fine-tuning, and training from
scratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and
ShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,
HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently
achieving superior accuracy, robustness, and efficiency, with ShuffleNet
excelling in generalization, particularly under full fine-tuning. The IFHCDB
dataset yielded the highest results, with 99% accuracy using MnasNet under full
fine-tuning, highlighting its suitability for robust character recognition. The
AHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA
posed significant challenges due to its variability, achieving a peak accuracy
of 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall
performance, balancing accuracy and convergence speed, while partial
fine-tuning underperformed across metrics. These findings underscore the
potential of combining TL and MbNets for resource-efficient AHCR, paving the
way for further optimizations and broader applications. Future work will
explore architectural modifications, in-depth dataset feature analysis, data
augmentation, and advanced sensitivity analysis to enhance model robustness and
generalizability.

</details>


### [38] [LUIVITON: Learned Universal Interoperable VIrtual Try-ON](https://arxiv.org/abs/2509.05030)
*Cong Cao,Xianhang Cheng,Jingyuan Liu,Yujian Zheng,Zhenhui Lin,Meriem Chkir,Hao Li*

Main category: cs.CV

TL;DR: LUIVITON 提出一个端到端的全自动虚拟试衣系统，通过将服装与人体对齐问题拆分为“服装↔SMPL”和“身体↔SMPL”两类对应，结合几何学习与扩散模型，实现复杂多层服装在多样化、任意姿态的人形角色上的高质量披挂与快速定制。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣在面对任意体型/姿态、多层复杂服装、非流形网格及缺乏缝制版型等实际场景时，往往需大量人工并难以稳健泛化。作者希望构建一个无需人工、能在广泛人形对象（人类、机器人、卡通、外星体）上稳定工作的自动化系统，并支持后续快捷尺寸与材料调参。

Method: 以SMPL为中间代理，拆解为两种对应任务：1) 服装→SMPL：使用基于几何学习的部分到完整形状对应预测，解决复杂服装拓扑、非流形等问题；2) 身体→SMPL：引入基于扩散模型的方法，结合多视一致的外观特征与预训练2D基础模型，预测身体到SMPL的对应关系。完成双向对应后实现服装在目标身体上的自动披挂与调整。同时提供快速服装尺寸与材料属性后编辑能力。

Result: 系统能够高质量地将复杂、多层次服装自动披挂到多样且任意姿态的人形角色上，在缺乏2D缝制版型情况下依然工作良好；支持非流形网格与复杂几何；在计算效率上满足实用需求，并具备良好的跨域泛化能力。

Conclusion: 通过SMPL代理与两阶段对应（几何学习+扩散模型）的结合，LUIVITON实现了无需人工的端到端虚拟试衣，兼顾质量、泛化与效率，并支持披挂后的快速定制。

Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on,
capable of draping complex, multi-layer clothing onto diverse and arbitrarily
posed humanoid characters. To address the challenge of aligning complex
garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy
representation and separate the clothing-to-body draping problem into two
correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,
where each has its unique challenges. While we address the clothing-to-SMPL
fitting problem using a geometric learning-based approach for
partial-to-complete shape correspondence prediction, we introduce a diffusion
model-based approach for body-to-SMPL correspondence using multi-view
consistent appearance features and a pre-trained 2D foundation model. Our
method can handle complex geometries, non-manifold meshes, and generalizes
effectively to a wide range of humanoid characters -- including humans, robots,
cartoon subjects, creatures, and aliens, while maintaining computational
efficiency for practical adoption. In addition to offering a fully automatic
fitting solution, LUIVITON supports fast customization of clothing size,
allowing users to adjust clothing sizes and material properties after they have
been draped. We show that our system can produce high-quality 3D clothing
fittings without any human labor, even when 2D clothing sewing patterns are not
available.

</details>


### [39] [Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization](https://arxiv.org/abs/2509.05034)
*Jingqi Wu,Hanxi Li,Lin Yuanbo Wu,Hao Chen,Deyin Liu,Peng Wang*

Main category: cs.CV

TL;DR: 提出ADClick与ADClick-Seg，通过少量点击+文本提示实现高效像素级缺陷标注，并以跨模态原型对齐提升工业异常检测与定位的精度，在MVTec AD上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 工业AD常仅用无缺陷数据训练，虽可采集缺陷样本，但像素级标注成本高、难扩展。需要一种低成本、精准的标注与利用缺陷信息的方法，并能在多类别场景下稳健检测与定位异常。

Method: 1) ADClick：交互式图像分割(IIS)方案，用户提供少量点击和简短文本描述，模型生成像素级异常掩膜，作为精确标注以训练/提升AD模型。2) ADClick-Seg：跨模态框架，采用原型(prototype)-驱动的视觉-文本对齐；将语言提示与视觉特征对齐，结合像素级先验与语言引导进行异常检测与定位。

Result: ADClick用于标注可显著提升AD性能，MVTec AD上AP达96.1%。ADClick-Seg在“多类别”AD任务上取得SOTA：AP 80.0%、PRO 97.5%、Pixel-AUROC 99.1%（均在MVTec AD）。

Conclusion: 少量交互+文本即可获得高质量像素级异常标注；跨模态原型对齐有效融合语言与视觉线索，在多类工业AD中实现SOTA，证明方法在效率与精度上的优势。

Abstract: Industrial product inspection is often performed using Anomaly Detection (AD)
frameworks trained solely on non-defective samples. Although defective samples
can be collected during production, leveraging them usually requires
pixel-level annotations, limiting scalability. To address this, we propose
ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial
anomaly detection. ADClick generates pixel-wise anomaly annotations from only a
few user clicks and a brief textual description, enabling precise and efficient
labeling that significantly improves AD model performance (e.g., AP = 96.1\% on
MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that
aligns visual features and textual prompts via a prototype-based approach for
anomaly detection and localization. By combining pixel-level priors with
language-guided cues, ADClick-Seg achieves state-of-the-art results on the
challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC =
99.1\% on MVTec AD).

</details>


### [40] [Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction](https://arxiv.org/abs/2509.05071)
*Mojtaba Safari,Zach Eidex,Richard L. J. Qiu,Matthew Goette,Tonghe Wang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 系统综述与荟萃分析：深度学习（尤其生成模型）用于MRI运动伪影检测与校正总体有效，但泛化、配对训练数据依赖与潜在视觉失真限制其临床可用性。


<details>
  <summary>Details</summary>
Motivation: MRI常受患者运动影响产生伪影，降低图像质量与诊断准确性。AI方法近年快速发展，需系统评估其在检测与校正运动伪影方面的进展、效果与不足，以指导未来研究与临床转化。

Method: 开展系统综述与荟萃分析，聚焦深度学习方法，尤其生成模型；提取数据集类型与规模、网络架构、评价指标等定量信息，综合比较检测与校正性能。

Result: 深度学习（生成模型为主）在降低运动伪影、提升图像质量方面显示出潜力与较好效果。然而普适性有限、训练常依赖成对数据（含无伪影-有伪影配对），且存在引入视觉失真或结构性伪影的风险，亟需标准化数据与报告。

Conclusion: AI（特别是生成式DL）可显著改善MRI图像质量并缓解运动伪影，有望提升诊断准确性并降低成本。但需解决三方面关键挑战：1）构建全面公开数据集；2）建立伪影等级与报告标准；3）发展更先进且可适配、对配对数据依赖更低的模型。解决后将促进临床落地并改善患者结局。

Abstract: Background: To systematically review and perform a meta-analysis of
artificial intelligence (AI)-driven methods for detecting and correcting
magnetic resonance imaging (MRI) motion artifacts, assessing current
developments, effectiveness, challenges, and future research directions.
Methods: A comprehensive systematic review and meta-analysis were conducted,
focusing on deep learning (DL) approaches, particularly generative models, for
the detection and correction of MRI motion artifacts. Quantitative data were
extracted regarding utilized datasets, DL architectures, and performance
metrics. Results: DL, particularly generative models, show promise for reducing
motion artifacts and improving image quality; however, limited
generalizability, reliance on paired training data, and risk of visual
distortions remain key challenges that motivate standardized datasets and
reporting. Conclusions: AI-driven methods, particularly DL generative models,
show significant potential for improving MRI image quality by effectively
addressing motion artifacts. However, critical challenges must be addressed,
including the need for comprehensive public datasets, standardized reporting
protocols for artifact levels, and more advanced, adaptable DL techniques to
reduce reliance on extensive paired datasets. Addressing these aspects could
substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and
improve patient care outcomes.

</details>


### [41] [GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting](https://arxiv.org/abs/2509.05075)
*Yangming Li,Chaoyu Liu,Lihao Liu,Simon Masnou,Carola-Bibian Schönlieb*

Main category: cs.CV

TL;DR: 提出GeoSplat，将一阶(法向)与二阶(曲率)几何先验贯穿高斯splatting训练（初始化、梯度更新、密度化），并用更稳健的几何估计法动态提供先验，显著提升新视角合成表现。


<details>
  <summary>Details</summary>
Motivation: 现有把几何先验用于高斯splatting的研究多只用低阶先验（如法向），且多依赖对噪声敏感的局部PCA估计，导致先验不稳、收益有限。需要一个既利用更丰富几何信息又具鲁棒估计的统一框架来系统提升训练流程。

Method: 提出GeoSplat：1) 几何约束优化框架，联合使用一阶与二阶几何量。2) 在训练各阶段注入先验：—初始化：依据主曲率设定3D高斯的尺度，实现更贴合表面的初始覆盖；—梯度更新：在优化中用几何结构（如局部流形）引导更新；—密度化：利用几何先验指导新增/合并高斯。3) 设计高效、抗噪的几何估计方法（基于局部流形等），动态产出先验供上述步骤使用。

Result: 在多数据集的新视角合成实验中，GeoSplat显著提升高斯splatting的表现，全面优于既有基线。

Conclusion: 结合鲁棒估计得到的一、二阶几何先验，并在初始化、优化与密度化全流程中施加约束，可系统提升高斯splatting的重建与渲染质量，优于仅用低阶且噪声敏感先验的方法。

Abstract: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.

</details>


### [42] [Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction](https://arxiv.org/abs/2509.05078)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出Scale-Interaction Transformer（SIT），结合多尺度CNN特征与Transformer自注意力建模尺度间交互，在SCUT-FBP5500上刷新FBP性能（Pearson 0.9187）。


<details>
  <summary>Details</summary>
Motivation: CNN在固定尺度上提取特征，难以显式捕捉不同粒度（局部-全局）面部特征之间的依赖关系，影响对人类审美感知的建模；需要一种方法同时获取多尺度信息并建模其相互作用。

Method: 构建混合架构：1) 多尺度并行卷积模块获取不同感受野的面部表征；2) 将多尺度特征序列化；3) 采用Transformer编码器，通过自注意力显式建模尺度间与上下文关系；4) 端到端训练进行图像回归（美貌评分）。

Result: 在SCUT-FBP5500基准上取得SOTA：Pearson相关系数0.9187，优于先前方法；通过消融和对比实验证明尺度交互建模带来增益。

Conclusion: 显式建模多尺度视觉线索之间的相互作用对高性能FBP至关重要；混合CNN-Transformer架构在需要整体、具上下文意识的图像回归任务上具有潜力。

Abstract: Automated Facial Beauty Prediction (FBP) is a challenging computer vision
task due to the complex interplay of local and global facial features that
influence human perception. While Convolutional Neural Networks (CNNs) excel at
feature extraction, they often process information at a fixed scale,
potentially overlooking the critical inter-dependencies between features at
different levels of granularity. To address this limitation, we introduce the
Scale-Interaction Transformer (SIT), a novel hybrid deep learning architecture
that synergizes the feature extraction power of CNNs with the relational
modeling capabilities of Transformers. The SIT first employs a multi-scale
module with parallel convolutions to capture facial characteristics at varying
receptive fields. These multi-scale representations are then framed as a
sequence and processed by a Transformer encoder, which explicitly models their
interactions and contextual relationships via a self-attention mechanism. We
conduct extensive experiments on the widely-used SCUT-FBP5500 benchmark
dataset, where the proposed SIT model establishes a new state-of-the-art. It
achieves a Pearson Correlation of 0.9187, outperforming previous methods. Our
findings demonstrate that explicitly modeling the interplay between multi-scale
visual cues is crucial for high-performance FBP. The success of the SIT
architecture highlights the potential of hybrid CNN-Transformer models for
complex image regression tasks that demand a holistic, context-aware
understanding.

</details>


### [43] [Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers](https://arxiv.org/abs/2509.05086)
*Svetlana Pavlitska,Haixi Fan,Konstantin Ditschuneit,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 在ResNet上用稀疏MoE层替换部分深层模块，并结合对抗训练，可在不增加推理开销的前提下提升PGD/AutoPGD鲁棒性；使用switch loss会导致路由塌缩，少数专家因被集中训练而更稳健，部分单专家甚至比门控MoE更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统提升CNN对抗鲁棒性的方法代价高（训练/推理成本大），且鲁棒性与模型容量常存在权衡。作者希望在不增加推理开销的情况下提升模型容量与鲁棒性，并理解MoE路由/平衡机制对鲁棒性的影响。

Method: 在ResNet（CIFAR-100）中选择性用稀疏MoE替换残块或卷积层（深层阶段更有效），每次仅激活少数专家以保持推理成本不变；与对抗训练结合（PGD/AutoPGD）。引入并分析switch loss的负载均衡效应及其对路由与鲁棒性的影响；比较门控MoE与单专家子路径的鲁棒性。

Result: 插入单个深层MoE层即可在PGD与AutoPGD攻击下稳定提升鲁棒性（配合对抗训练）；使用switch loss导致路由集中到少数专家，这些专家因被集中对抗训练而更鲁棒；部分单专家子路径在鲁棒性上优于整个门控MoE。

Conclusion: 稀疏MoE能在不增加推理成本的同时增强对抗鲁棒性；路由平衡损失会引发专家过度使用与“鲁棒子路径”专化现象，提示未来可直接选择或蒸馏这些鲁棒专家/子路径以获得更强且高效的鲁棒模型。

Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.

</details>


### [44] [Semi-supervised Deep Transfer for Regression without Domain Alignment](https://arxiv.org/abs/2509.05092)
*Mainak Biswas,Ambedkar Dukkipati,Devarajan Sridharan*

Main category: cs.CV

TL;DR: 提出CRAFT：一种用于回归任务的源数据不可用（source-free）半监督迁移学习方法，基于Contradistinguisher（CUDA）思想，在目标域仅少量标注与大量未标注样本下进行高效适配，在EEG注视点预测与MRI脑龄预测上，相比微调最高降低RMSE约9%，并超过四个SFDA基线3%以上，并在其他回归基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实应用（医学/神经科学）存在域移，常规DA依赖完整源数据但受隐私、体量、算力等限制不可得；目标域标注也稀缺，且任务是连续值回归（非分类）。现有CUDA面向无监督分类且需源数据，不满足源不可用、半监督、回归的需求，故需新方法。

Method: 在CUDA对比/区分思想基础上，提出CRAFT：针对已预训练源模型，在无源数据条件下进行灵活正则化训练，利用少量目标标注样本与大量未标注目标样本；避免显式中间表示对齐，通过对比式或“相异-相似”判别正则、一致性/伪标签与回归损失的联合优化，实现半监督的目标域适配。

Result: 在两项神经科学任务（EEG凝视预测、结构MRI脑龄预测）上，标注样本稀缺时CRAFT相较于常规微调模型RMSE最高提升约9%；同时利用未标注目标数据，整体超过四个最新源自由域自适应回归基线超过3%的RMSE优势；并在另外两个真实世界回归基准上复现有效性。

Conclusion: CRAFT为面向生物医学常见回归任务的源自由、半监督深度迁移提供了一种高效方案，在源数据不可共享、目标标注稀缺的场景下优于微调与现有SFDA方法，具有通用性与实际部署价值。

Abstract: Deep learning models deployed in real-world applications (e.g., medicine)
face challenges because source models do not generalize well to domain-shifted
target data. Many successful domain adaptation (DA) approaches require full
access to source data. Yet, such requirements are unrealistic in scenarios
where source data cannot be shared either because of privacy concerns or
because it is too large and incurs prohibitive storage or computational costs.
Moreover, resource constraints may limit the availability of labeled targets.
We illustrate this challenge in a neuroscience setting where source data are
unavailable, labeled target data are meager, and predictions involve
continuous-valued outputs. We build upon Contradistinguisher (CUDA), an
efficient framework that learns a shared model across the labeled source and
unlabeled target samples, without intermediate representation alignment. Yet,
CUDA was designed for unsupervised DA, with full access to source data, and for
classification tasks. We develop CRAFT -- a Contradistinguisher-based
Regularization Approach for Flexible Training -- for source-free (SF),
semi-supervised transfer of pretrained models in regression tasks. We showcase
the efficacy of CRAFT in two neuroscience settings: gaze prediction with
electroencephalography (EEG) data and ``brain age'' prediction with structural
MRI data. For both datasets, CRAFT yielded up to 9% improvement in
root-mean-squared error (RMSE) over fine-tuned models when labeled training
examples were scarce. Moreover, CRAFT leveraged unlabeled target data and
outperformed four competing state-of-the-art source-free domain adaptation
models by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two
other real-world regression benchmarks. We propose CRAFT as an efficient
approach for source-free, semi-supervised deep transfer for regression that is
ubiquitous in biology and medicine.

</details>


### [45] [A Scalable Attention-Based Approach for Image-to-3D Texture Mapping](https://arxiv.org/abs/2509.05131)
*Arianna Rampini,Kanika Madan,Bruno Roy,AmirHossein Zamani,Derek Cheung*

Main category: cs.CV

TL;DR: 提出一种Transformer框架，从单张参考图像与给定网格直接预测3D纹理场，结合Triplane表示与基于深度的反投影损失，无需UV与可微渲染，单次前向0.2秒生成高保真纹理，较现有方法在保真度与感知质量上更优。


<details>
  <summary>Details</summary>
Motivation: 现有生成式纹理方法速度慢、依赖UV展平且常偏离参考图像，影响写实与可扩展的3D内容生产。因此需要一种既高保真又高效、对参考图像忠实且不依赖UV/可微渲染的纹理生成方案。

Method: 以Transformer为核心，从单张图像与输入网格直接回归3D纹理场。引入Triplane表示实现高效空间编码；设计深度引导的反投影损失，将图像信息投射至3D以监督纹理一致性；训练完成后推理为单次前向传播，无需UV映射与可微渲染。

Result: 在单图像纹理重建任务上，通过定性、定量与用户偏好评测，方法在输入图像忠实度与感知质量上均优于SOTA基线；推理速度快，每个形状约0.2秒生成高保真纹理。

Conclusion: 该方法实现可扩展、可控且高质量的3D纹理生成，摆脱UV与可微渲染依赖，训练与推理高效，并在多项评测中优于现有方法，具有实际应用价值。

Abstract: High-quality textures are critical for realistic 3D content creation, yet
existing generative methods are slow, rely on UV maps, and often fail to remain
faithful to a reference image. To address these challenges, we propose a
transformer-based framework that predicts a 3D texture field directly from a
single image and a mesh, eliminating the need for UV mapping and differentiable
rendering, and enabling faster texture generation. Our method integrates a
triplane representation with depth-based backprojection losses, enabling
efficient training and faster inference. Once trained, it generates
high-fidelity textures in a single forward pass, requiring only 0.2s per shape.
Extensive qualitative, quantitative, and user preference evaluations
demonstrate that our method outperforms state-of-the-art baselines on
single-image texture reconstruction in terms of both fidelity to the input
image and perceptual quality, highlighting its practicality for scalable,
high-quality, and controllable 3D content creation.

</details>


### [46] [SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing](https://arxiv.org/abs/2509.05144)
*Chaolei Wang,Yang Luo,Jing Du,Siyu Chen,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: 提出SGS-3D：一种“先分后长”的训练无关3D实例分割精炼框架，通过几何原语过滤与分裂2D提升掩码，并基于空间连续性与高层特征生长成完整实例，在ScanNet200/ScanNet++/KITTI-360上显著提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 2D到3D提升的实例分割方法在实例级精度上受限：语义引导含糊、深度约束不足，导致提升过程累积误差与错误掩码传递，削弱3D实例级一致性与可用性。

Method: 提出SGS-3D训练无关精炼：1) 语义侧，利用3D几何原语的共现关系进行掩码过滤，识别并剔除含糊/不可靠的提升掩码，并将其按几何边界进行“分裂”；2) 几何侧，利用空间连续性与高层特征将分裂后的可靠片段“生长”为完整实例，处理跨物体语义歧义；3) 语义与几何信息联合融合，实现两层表征的有效协同。

Result: 在ScanNet200、ScanNet++、KITTI-360上，相较依赖原始提升掩码的基线，显著提升3D实例分割精度与对不准掩码的鲁棒性，并在室内外多场景具备良好泛化。

Conclusion: 通过“先分后长”的掩码净化与几何引导生长，无需再训练即可从噪声2D提升掩码中构建高保真3D实例；方法通用、鲁棒、可跨数据集与场景泛化。

Abstract: Accurate 3D instance segmentation is crucial for high-quality scene
understanding in the 3D vision domain. However, 3D instance segmentation based
on 2D-to-3D lifting approaches struggle to produce precise instance-level
segmentation, due to accumulated errors introduced during the lifting process
from ambiguous semantic guidance and insufficient depth constraints. To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.
Unlike existing approaches that directly rely on raw lifted masks and sacrifice
segmentation accuracy, SGS-3D serves as a training-free refinement method that
jointly fuses semantic and geometric information, enabling effective
cooperation between the two levels of representation. Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances. For the geometric refinement, we construct fine-grained object
instances by exploiting both spatial continuity and high-level features,
particularly in the case of semantic ambiguity between distinct objects.
Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that
SGS-3D substantially improves segmentation accuracy and robustness against
inaccurate masks from pre-trained models, yielding high-fidelity object
instances while maintaining strong generalization across diverse indoor and
outdoor environments. Code is available in the supplementary materials.

</details>


### [47] [SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition](https://arxiv.org/abs/2509.05188)
*Ariel Basso Madjoukeng,Jérôme Fink,Pierre Poitier,Edith Belise Kenmogne,Benoit Frenay*

Main category: cs.CV

TL;DR: 提出一种针对手语识别的自监督框架，结合“无负样本”对比式学习与专门的数据增强，缓解视频中关键信息稀疏与跨手语共享动作导致的混淆问题，在线性评估、半监督与跨语言迁移上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手语识别标注数据稀缺，传统对比学习在视频上不区分关键信息片段，且不同手语间存在相似动作，使负样本与正样本过于相近，导致学到的特征不具判别性，影响下游表现。

Method: 提出一个自监督学习框架，核心包括：1）无负样本（free-negative）的自监督目标，避免将含有共享动作的样本硬性推远；2）面向手语的专用数据增强策略，突出视频中与识别相关的关键时空片段，弱化无关部分。两者协同以学习更聚焦且判别的表征。

Result: 在多项评测中取得显著提升：线性探测、半监督设置以及跨手语迁移任务上均优于多种主流对比与自监督基线。

Conclusion: 通过去除对负样本的依赖并引入任务定制的增强，框架更好地捕捉手语视频中的关键语义与差异，缓解共享动作带来的混淆，最终提升了表征质量与下游性能。

Abstract: Sign language recognition (SLR) is a machine learning task aiming to identify
signs in videos. Due to the scarcity of annotated data, unsupervised methods
like contrastive learning have become promising in this field. They learn
meaningful representations by pulling positive pairs (two augmented versions of
the same instance) closer and pushing negative pairs (different from the
positive pairs) apart. In SLR, in a sign video, only certain parts provide
information that is truly useful for its recognition. Applying contrastive
methods to SLR raises two issues: (i) contrastive learning methods treat all
parts of a video in the same way, without taking into account the relevance of
certain parts over others; (ii) shared movements between different signs make
negative pairs highly similar, complicating sign discrimination. These issues
lead to learning non-discriminative features for sign recognition and poor
results in downstream tasks. In response, this paper proposes a self-supervised
learning framework designed to learn meaningful representations for SLR. This
framework consists of two key components designed to work together: (i) a new
self-supervised approach with free-negative pairs; (ii) a new data augmentation
technique. This approach shows a considerable gain in accuracy compared to
several contrastive and self-supervised methods, across linear evaluation,
semi-supervised learning, and transferability between sign languages.

</details>


### [48] [Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet](https://arxiv.org/abs/2509.05198)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari*

Main category: cs.CV

TL;DR: 提出改进数据集ModelNet-R与轻量级网络Point-SkipNet，在更干净一致的数据上显著提升3D点云分类效果，并以更少参数达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有主流数据集ModelNet40存在标注不一致、来源为2D生成、尺度不匹配、类别区分不足等问题，导致训练与评测存在偏差，限制模型上限与可比性。需要一个更可靠的基准来客观评估方法，并推动效率与精度的共同提升。

Method: 1) 数据集：对ModelNet40进行系统清洗与重构，形成ModelNet-R，修正标签、统一尺度、剔除问题样本、增强类间可分性。2) 模型：提出Point-SkipNet——基于图的轻量网络，包含高效采样与邻域分组、局部特征聚合、跨层跳连以缓解梯度消失与特征丢失，在保证精度的同时降低计算与参数量。3) 实验：在ModelNet-R上系统对比多种方法，评估准确率、参数规模与计算开销。

Result: 在ModelNet-R上，多数现有模型的性能均显著提升，表明数据集质量瓶颈被缓解。Point-SkipNet以显著更低的参数量达到或超过当代模型的准确率，达成SOTA水平。

Conclusion: 高质量与一致性的基准对3D点云分类至关重要。ModelNet-R能够更公正地评估方法并提升训练效果；Point-SkipNet展示了在更优数据上的高效能，与低参数量并存的高精度。代码公开，便于复现与后续研究。

Abstract: The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.

</details>


### [49] [Symbolic Graphics Programming with Large Language Models](https://arxiv.org/abs/2509.05208)
*Yamei Chen,Haoquan Zhang,Yangyi Huang,Zeju Qiu,Kaipeng Zhang,Yandong Wen,Weiyang Liu*

Main category: cs.CV

TL;DR: 研究LLM从自然语言生成可渲染的符号图形程序（SGP，聚焦SVG），提出基准SGP-GenBench并发现闭源前沿模型显著领先；为提升开源模型，提出带可验证奖励的强化学习：格式校验确保可渲染、跨模态奖励用强视觉编码器对齐文本与渲染图。应用于Qwen-2.5-7B显著提升，接近前沿系统，并揭示RL促使更细粒度分解与更强上下文细节，从而增强跨模态落地与可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成上表现出色，但其生成能精准落地到视觉内容的符号图形程序能力尚未系统评估与提升；需要一个精细的任务与评测来检验LLM对视觉世界的结构化理解，并探索如何通过学习信号提升这种跨模态对齐能力。

Method: 1) 构建SGP-GenBench基准，覆盖对象/场景保真与可组合性（属性绑定、空间关系、数性）。2) 提出“可验证奖励”的RL训练框架：设置格式有效性门控以确保SVG可渲染；使用强视觉编码器（SigLIP进行文图对齐、DINO进行图图相似）提供跨模态奖励信号，驱动模型优化SVG生成的语义与几何。3) 在Qwen-2.5-7B上实施并分析训练动态。

Result: 在SGP-GenBench上，闭源前沿模型显著优于开源模型，性能与通用编程能力相关；应用所提RL后，Qwen-2.5-7B的SVG生成质量与语义明显提升，达到接近前沿系统的水平；训练过程中观察到更细的对象分解与更丰富的上下文细节，提升场景一致性。

Conclusion: 符号图形编程为检验与提升LLM跨模态对齐提供了精确、可解释的视角；通过可验证奖励的RL可有效提升开源LLM在SVG生成上的保真与语义对齐，缩小与闭源前沿模型差距，并促进可控、可组合的视觉程序生成。

Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to
produce symbolic graphics programs (SGPs) that render into precise visual
content remains underexplored. We study symbolic graphics programming, where
the goal is to generate an SGP from a natural-language description. This task
also serves as a lens into how LLMs understand the visual world by prompting
them to generate images rendered from SGPs. Among various SGPs, our paper
sticks to scalable vector graphics (SVGs). We begin by examining the extent to
which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a
comprehensive benchmark covering object fidelity, scene fidelity, and
compositionality (attribute binding, spatial relations, numeracy). On
SGP-GenBench, we discover that frontier proprietary models substantially
outperform open-source models, and performance correlates well with general
coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to
generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards
approach, where a format-validity gate ensures renderable SVG, and a
cross-modal reward aligns text and the rendered image via strong vision
encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to
Qwen-2.5-7B, our method substantially improves SVG generation quality and
semantics, achieving performance on par with frontier systems. We further
analyze training dynamics, showing that RL induces (i) finer decomposition of
objects into controllable primitives and (ii) contextual details that improve
scene coherence. Our results demonstrate that symbolic graphics programming
offers a precise and interpretable lens on cross-modal grounding.

</details>


### [50] [COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization](https://arxiv.org/abs/2509.05249)
*Yassine Taoudi-Benchekroun,Klim Troyan,Pascal Sager,Stefan Gerber,Lukas Tuggener,Benjamin Grewe*

Main category: cs.CV

TL;DR: 提出COGITAO：一个面向视觉组合泛化的模块化数据生成框架与基准，可在网格环境中以可调深度组合28种可互操作的规则变换，生成海量任务规则与样本；基线显示SOTA视觉模型在新组合上显著失泛化。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在“组合与迁移”能力上落后于人类智能，尤其是将已学概念在新情境下组合应用。现有数据集/基准规模与可控性不足，难以系统研究视觉领域的组合性与泛化。

Method: 构建COGITAO：受ARC-AGI启发，在网格环境上对对象施加规则化变换；提供28种可互操作的变换（可按深度进行组合），并可细粒度控制网格参数与对象属性。通过框架生成数百万条不同任务规则，每条规则可无限采样。开放源码与数据集。

Result: 对一系列SOTA视觉模型进行基线实验：在训练分布内表现良好，但在由熟悉元素组成但新颖的组合上显著失败，暴露出组合泛化不足。

Conclusion: COGITAO为研究视觉组合性与泛化提供了高可扩展、可控、开放的基准；其结果表明当前模型缺乏对新组合的系统泛化能力，呼吁社区使用该平台推动方法改进。

Abstract: The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.

</details>


### [51] [WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool](https://arxiv.org/abs/2509.05296)
*Zizun Li,Jianjun Zhou,Yifan Wang,Haoyu Guo,Wenzheng Chang,Yang Zhou,Haoyi Zhu,Junyi Chen,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: WinT3R 是一个面向在线重建的前馈模型，通过滑动窗口信息交互与全局相机token池，实现实时高质量点云与精确相机位姿预测，兼顾速度与精度并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往在线重建方法在重建质量与实时性间存在权衡：要么高质量但慢、要么实时但精度差。作者希望在保持实时推理的同时提高几何重建与位姿估计的准确性与稳定性。

Method: 1) 设计滑动窗口机制，窗口内多帧间进行充分的信息交换，提升几何预测质量且计算开销可控；2) 使用紧凑相机表征并维护全局相机token池，增强相机位姿估计的可靠性；3) 前馈式架构实现端到端在线预测高质量点图与相机位姿。

Result: 在多样数据集上验证，WinT3R 在在线重建质量、相机位姿估计精度与重建速度方面均达到或超过现有方法（SOTA）；代码与模型已开源。

Conclusion: 通过滑动窗口信息融合与全局相机token机制，WinT3R 实现了高效且高精度的在线重建与位姿估计，缓解了质量-实时性的传统权衡，并具备实际部署价值。

Abstract: We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.

</details>


### [52] [FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases](https://arxiv.org/abs/2509.05297)
*Matteo Poggi,Fabio Tosi*

Main category: cs.CV

TL;DR: FlowSeek是一种资源友好的光流框架，结合轻量网络设计、单幅图像深度基础模型与低维运动参数化，在单张消费级GPU上训练即可，在Sintel、KITTI等跨数据集上优于SOTA（较SEA-RAFT在Sintel Final与KITTI上分别提升约10%、15%），同时在Spring与LayeredFlow上也表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有高精度光流方法训练成本高、对硬件依赖重且跨数据集泛化有限。需要一种在低硬件预算下仍能训练、同时保持或提升跨数据集泛化能力的光流方法。

Method: 提出FlowSeek：1) 采用当代光流网络设计空间中的紧凑架构；2) 融合单幅图像深度基础模型提供的几何先验；3) 使用经典的低维运动参数化以约束和简化估计；从而在保证准确性的同时显著降低训练资源需求。

Result: 在单块消费级GPU上完成训练（较大多数近期方法约低8倍硬件预算）。在跨数据集评测上取得领先：Sintel Final与KITTI相对优于SEA-RAFT约10%与15%；在Spring与LayeredFlow数据集也获得优异结果。

Conclusion: 通过将深度先验与低维运动参数化注入紧凑光流网络，FlowSeek在极低训练硬件成本下实现强泛化与高精度，刷新多数据集基准并证明资源高效的光流训练是可行的。

Abstract: We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training. FlowSeek marries the latest advances on the
design space of optical flow networks with cutting-edge single-image depth
foundation models and classical low-dimensional motion parametrization,
implementing a compact, yet accurate architecture. FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.

</details>
