<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 276]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 论文发布EDU-CIRCUIT-HW数据集（1300+真实大学STEM手写解答），用以评测多模态大模型对含公式/图示/文本的手写作答的识别与自动评分。结果显示当前MLLM在上游识别存在大量潜在错误，难以可靠用于高风险教育场景；基于错误模式的少量人工干预（约4%样本）可显著提升自动评分鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在教育中的潜力大，但缺少面向真实、学科特定的手写多模态基准；当前评估多依赖下游任务结果，覆盖面有限，无法全面反映模型对复杂手写逻辑的理解能力。

Method: 构建并公开EDU-CIRCUIT-HW：包含1300+真实大学课程手写解答，附专家核验逐字转写与评分报告；分别评估多种MLLM的上游识别保真度与下游自动评分表现；进一步基于识别错误模式，设计少量人工校正介入的流程，并在未见过的学生作答上进行案例验证。

Result: 发现MLLM对手写内容的识别存在大规模潜在失败，导致自动评分与理解导向应用不可靠；通过利用已识别的错误模式进行预检与修正，仅需约4%样本的人为介入即可显著提升部署系统在新样本上的鲁棒性。

Conclusion: 当前MLLM尚不适合直接用于高风险教育自动评分；应在真实数据基准上联合评估上游识别与下游任务，并通过基于错误模式的轻量人工干预来提升系统稳定性。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [2] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: 提出“Simulate Anything”，用3D高斯溅射重建现实场景、结合生成模型恢复物理属性并精校准尺度，把多视角视频转成可编辑、物理一致的仿真世界，用于高保真数据生成，训练的VLA在零样本任务上可与甚至超过真实数据。


<details>
  <summary>Details</summary>
Motivation: 现实交互数据稀缺限制了具身智能的可扩展性；现有仿真与现实存在视觉/物理鸿沟，并依赖昂贵传感器、精确标定或深度信息，难以规模化。

Method: 1) 从多视角环境视频与通用资源出发；2) 用3D Gaussian Splatting重建高保真外观与细节几何；3) 借助生成模型恢复物理上可模拟的表示（材质、动力学等）；4) 通过精密标定靶实现与真实世界的尺度对齐；5) 将上述统一为可编辑、物理一致的世界模型并用于仿真数据生成与VLA训练。

Result: 基于该框架生成的数据训练的视觉-语言-行动模型在下游任务上实现强零样本性能，达到或超过用真实数据训练的结果。

Conclusion: 重建驱动的世界建模能以低成本从视频构建高保真、物理一致的仿真环境，为可扩展、实用的具身智能训练提供可行路径。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [3] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出R3G：先生成简要推理计划，再经粗检索+细粒度重排选取证据图像，提升VQA中的视觉线索补充与整合，达成SOTA并验证推理步骤与“充分性感知”重排互补。


<details>
  <summary>Details</summary>
Motivation: VQA常需外部图像以弥补缺失视觉线索，但难在：如何选到“对”的图像、以及如何将其有效融入推理。现有方法在检索精度或推理整合上均存在不足。

Method: 模块化Reasoning-Retrieval-Reranking框架：1) 先生成简要推理计划，明确所需视觉线索；2) 两阶段检索：粗检索得到候选证据图像；3) 细粒度、面向“充分性”的重排，优先满足推理计划需求的图像；4) 将选中图像与原问题共同输入MLLM进行推理。

Result: 在MRAG-Bench上，跨6个MLLM骨干和9个子场景整体精度提升，达成SOTA；消融显示：引入推理步骤与充分性感知重排均有增益，且二者互补。

Conclusion: R3G能更好地选取并利用证据图像，显著增强检索增强VQA的表现；方法对不同MLLM有普适性，代码与数据已开源。

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [4] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: 提出HYPE-EDIT-1基准：100个参考式图像编辑任务，二元通过判定；每任务采样10次，评估通过率、pass@10、在重试上限下的期望尝试数与包含模型费与人工审核时间的有效单次成功成本。结果显示不同模型单次通过率34%-83%、有效成功成本$0.66-$1.42；低单价模型在考虑重试与人工审核后未必便宜。


<details>
  <summary>Details</summary>
Motivation: 公开演示常为最佳案例，无法反映真实工作流中的重试与审核成本。需要一个标准化、可复现的基准，衡量参考式营销/设计编辑在现实约束下的有效性能与成本。

Method: 构建100个参考式编辑任务，二元通过/失败判定；每任务生成10个独立输出，计算：单次通过率、pass@10、在重试上限下的期望尝试次数与综合有效成本（模型调用价格+人工审核时间）。发布50个公开任务与50个私有保留集用于服务器端评测；提供标准化JSON架构与适用于VLM/人工的评审工具链。

Result: 在被测模型中，单次通过率覆盖34%-83%；综合有效成功成本范围为$0.66-$1.42。观察到某些看似便宜的低每图价格模型，由于需要更多重试与审核，实际每次成功编辑成本更高。

Conclusion: HYPE-EDIT-1能更真实地反映图像编辑模型在生产工作流中的性价比；评估应考虑重试与人工审核，而非仅看每次调用单价或最佳案例表现。

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [5] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

TL;DR: 提出一种融合激光雷达与毫米波雷达信息的多模态无人机轨迹预测方法，通过深度融合网络显著提升预测精度（较基线提升约40%），并在MMAUD数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 低空经济背景下需对未经授权的无人机进行管理与预警，单一传感器（如仅LiDAR或仅毫米波雷达）对小型/快速机动目标的检测与轨迹预测存在鲁棒性与精度不足的问题，需利用多模态互补信息提升轨迹预测性能与可靠性。

Method: 构建多模态深度融合框架：为LiDAR与毫米波雷达分别设计结构相同但独立的特征编码器；经编码后进入双向交叉注意力（Bidirectional Cross-Attention）模块实现跨模态信息互补与语义对齐；结合不同损失函数与后处理策略优化预测结果。在CVPR 2024 UG2+挑战赛使用的MMAUD数据集上进行训练与测试。

Result: 多模态融合模型在轨迹预测准确度上较基线提升约40%；消融实验表明所选损失函数与后处理策略进一步提升性能并验证各组件有效性。

Conclusion: 融合LiDAR与毫米波雷达的深度双向交叉注意力框架能有效挖掘几何与动态反射互补信息，显著提升未经授权UAV轨迹预测效果，为低空经济场景中的入侵管理提供高效可行的解决方案。

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

</details>


### [6] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE 是一个用于带空间约束的计数任务的视觉-语言模型数据集，旨在在简单2D与现实复杂数据集之间架桥，并显著提升模型对分布外图像的计数泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有计数数据集两端失衡：一类如 VLMCountBench 过于简单（二维、受控但缺少真实复杂性），另一类如 TallyQA 贴近真实但存在遮挡与空间构成不可控、歧义大等问题，难以系统评估“带空间约束的计数”。需要一个可控又具复杂度的数据集来有效训练与评估 VLM 在此类计数任务上的泛化。

Method: 构建名为 SITUATE 的数据集，引入显式空间约束与可控的遮挡/构图因素；以此对 VLM（如 Qwen VL 2.5 7B）进行微调，并在多套计数基准上评测。与同规模的 Pixmo count 微调集进行对照实验，分析在分布外（OOD）图像上的泛化效果。

Result: 用 SITUATE 微调的 Qwen VL 2.5 7B 在 Pixmo count 测试集上的准确率提升；而用 Pixmo count 微调并不反向提升在 SITUATE 或其他基准上的表现。跨多个计数基准的比较与与同规模 Pixmo 微调集的对照验证了该结论。

Conclusion: SITUATE 在控制空间构成与遮挡的同时保留任务复杂性，能更有效训练出具备更强 OOD 泛化能力的计数型 VLM；相较同规模的 Pixmo 微调，SITUATE 带来更稳健的跨基准性能提升。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [7] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: 论文评估了在远程身份验证场景中，商业人脸活体检测（PAD）在低光与自动拍摄流程下的鲁棒性，发现多数系统在这些条件下错误率显著上升，唯有一套系统在所有场景下将真实样本误判率维持在3%以下。


<details>
  <summary>Details</summary>
Motivation: RIV（远程身份验证）依赖PAD来抵御展示攻击，但实际应用环境多变（光照不足、自动采集流程等），现有系统在这些非理想条件下是否仍可靠尚不清楚，影响安全与用户体验。

Method: 基于RIV场景测试，系统性设置两类扰动条件：低光环境与自动图像采集（auto-capture）流程，对多套商业PAD进行对比评估；采用模型预测与误差率统计（如bona fide presentation classification error）衡量性能变化。

Result: 低光条件下错误率约增加4倍；自动采集流程下错误率概率约翻倍。仅有一套系统对这些扰动保持鲁棒，在所有场景下其真实样本误判率≤3%。

Conclusion: 商业PAD在低光与自动采集条件下易性能退化，部署需覆盖多样环境测试与优化；选择或设计对环境鲁棒的系统至关重要，以确保真实世界中的可靠性。

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [8] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: 提出一种结合遥感影像与地理空间辅助信息的新模型：用地理空间嵌入与引导注意力，实现跨模态对齐与可解释性提升，并在疾病流行预测上优于现有地理基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言/多模态模型偏重语义对齐，缺乏对结构化地理空间图层的表达与推理能力，难以充分利用多源地理辅助数据（如矢量图层、栅格、元数据）来提升遥感任务表现与可解释性。

Method: 1) 地理空间嵌入：将多样的地理空间数据转换为与影像patch空间对齐的嵌入patch；2) 引导注意力模块：根据与辅助数据的相关性动态计算注意力权重，聚焦最相关区域；3) 头部角色分配：不同注意力头承担互补的引导功能，提升对指导信息不同方面的建模与解释性。

Result: 在多模态地理理解任务上验证，特别是在疾病流行率预测中，所提框架优于现有预训练地理空间基础模型，显示更强的性能与更好的引导效果。

Conclusion: 通过地理空间嵌入与引导注意力，将结构化地理信息有效融入遥感视觉模型，增强跨模态交互与可解释性，推动从语义对齐向地理空间理解的能力提升，并在实际应用中取得领先表现。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [9] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: 研究考察14个荷兰商业化牧场中群养乳牛犊的场地面积与玩耍行为的关系（2.66–17.98 m²/犊），并开发计算机视觉自动监测流水线。结果：玩耍占观测期约1.0%（约17小时内10分钟），空间与玩耍呈非线性：8–10 m²/犊最高（1.6%），6–8与12–14 m²/犊最低（<0.6%）。空间效应在控制年龄、健康、群规模后仍显著。视觉分类器在独立测试集上对“主动玩耍”检测准确率97.6%、召回率99.4%。建议8–10 m²/犊为兼顾福利与经济性的实践目标，并显示自动化监测可规模化持续福利评估。


<details>
  <summary>Details</summary>
Motivation: 玩耍是牛犊积极福利的指标，但商业条件下中高空间配给（6–20 m²/犊）的作用尚不清楚；同时，传统人工标注难以规模化，需要自动化工具实现持续监测。

Method: 在14个商业牧场收集60群牛犊视频，使用详尽的行为谱手工标注并计算玩耍占比（%OP）；采用线性混合模型（牧场为随机效应）评估空间与玩耍的关系，并控制年龄、健康、群规模等协变量。并训练计算机视觉分类器（基于6个牧场108小时标注数据），在独立留出测试集上验证。

Result: 牛犊平均玩耍1.0% OP；空间-玩耍关系非线性，在8–10 m²/犊达到峰值（1.6% OP），6–8与12–14 m²/犊显著较低（<0.6% OP）；空间在控制协变量后仍显著。计算机视觉模型对主动玩耍的分类在测试集上达97.6%准确率、99.4%召回率。

Conclusion: 在商业化条件下，8–10 m²/犊可能是平衡福利与成本的合理空间目标；同时，自动化计算机视觉可将小规模标注转化为可持续的福利评估系统。

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [10] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

TL;DR: 提出一个基于多视角摄影测量与深度学习分割的AI烧伤评估与管理平台，可从普通相机多角度照片重建患者3D表面，量化客观指标（面积、TBSA、几何深度代理、体积变化）、对齐随访以评估愈合趋势，并支持工作流与报告自动化；仿真评估显示重建稳定、指标一致、趋势合理。


<details>
  <summary>Details</summary>
Motivation: 传统肉眼与二维照片评估主观性强、缺乏可重复性，难以做跨时间客观比较；临床与法证需要标准化、量化的烧伤面积/深度/愈合跟踪方法，且应低成本、可扩展、无创、适用于门急诊。

Method: - 使用消费级相机获取多角度图像；
- 多视角摄影测量与3D表面重建生成患者特异的烧伤表面模型；
- 深度学习分割烧伤区域并映射到解剖表面；
- 计算真实尺度下的客观指标（表面积、TBSA、深度相关几何代理、体积变化）；
- 多次随访重建进行空间配准以量化收缩与“深度”降低；
- 平台化实现：结构化接诊、引导拍摄、3D分析可视化、治疗建议、自动报告。

Result: 基于仿真数据的评估表明：3D重建稳定、指标计算一致，纵向趋势与临床预期相符，证明该方法在客观、几何感知的烧伤评估与决策支持方面具有可行性与可扩展性。

Conclusion: 该平台以低成本相机和AI为核心，实现面向临床工作流的3D几何感知与纵向量化评估，为急诊与门诊场景提供可重复、客观、可扩展的烧伤评估与管理工具，但仍需真实临床数据验证与前瞻性研究以确立效用与泛化。

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

</details>


### [11] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 提出1S-DAug：在测试时仅凭单张图像即可生成多样且保真的增广样本，用于提升小样本学习(FSL)在新类上的泛化，显著提高1-shot性能。


<details>
  <summary>Details</summary>
Motivation: 传统测试时增广在FSL场景（尤其是1-shot）效果有限，因样本极少难以覆盖类内变化；需要一种在测试时、仅凭单例仍能产生多样且忠实变体的方法，以增强表示稳健性而无需微调模型。

Method: 设计1S-DAug为训练无关、模型无关的插件：将几何扰动与受控噪声注入相结合，并以原图为条件驱动去噪扩散过程生成多样但保真的图像；随后对生成图与原图编码并聚合为联合表示，用于更稳健的FSL预测。

Result: 在4个数据集的标准FSL基准上持续带来提升，无需任何参数更新；在miniImageNet 5-way 1-shot上实现超过10%的相对准确率提升。代码将开源。

Conclusion: 测试时的单例生成式增广可显著强化小样本学习的泛化，1S-DAug作为即插即用的训练无关组件在多基准上有效且稳定，显示了扩散生成与传统增广互补的价值。

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [12] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: 提出一种用于事件相机数据的小规模事件簇实时检测的异步事件驱动算法，基于时空距离进行层次凝聚式聚类，但通过利用事件相机的异步特性与简洁的决策机制，将复杂度降为线性 O(n)，且与像素阵列尺寸无关。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生的事件流具有高时间分辨率与稀疏、异步特性，传统聚类方法在实时性、复杂度和与传感器分辨率的耦合上存在瓶颈。需要一种既能适配异步数据结构、又能在硬实时条件下检测小事件簇的高效方法。

Method: 以层次凝聚式聚类思想为框架，用事件的时空距离作为相似度，但在实现上采用异步事件驱动的更新与决策：每到达一个事件即局部评估并合并/创建簇，避免全局重计算；通过简洁高效的判定规则，使处理每个事件的代价为常数级，从而整体复杂度为 O(n)。

Result: 算法在理论上实现线性时间复杂度，且运行时间与像素阵列维度无关，可实时检测小规模事件簇。

Conclusion: 充分利用事件相机异步数据结构和局部决策机制，可在保持聚类效果的同时大幅提升实时性与可扩展性，实现对小事件簇的高效检测。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [13] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

TL;DR: 提出一个对话式、能生成Python代码的EO分析代理，将自然语言转为可执行、可审计的工作流，在分类/分割/检测/指数与地理算子统一API上运行，在土地组成与火灾后评估两任务上优于通用LLM/VLM，并提升透明可复现性。


<details>
  <summary>Details</summary>
Motivation: EO分析对非专家门槛高，且现有系统多为不可解释的黑箱预测，难以审计与复现；希望利用工具型LLM将自然语言意图转为透明、可控、可验证的分析流程。

Method: 构建一个对话式代码生成代理：1) 统一、可扩展的工具API覆盖分类、分割、定向目标检测、光谱指数与地理空间算子；2) 代理将用户查询编译为可执行Python工作流；3) 在三个层面评估与控制：工具层（公开基准）、代理层（代码有效性与避免幻觉）、任务层（具体用例）。

Result: 在两项用例评估中，代理在土地组成映射达64.2%准确率（优于GPT-4o/LLaVA的51.7%），在火灾后损害评估达50%（基线为0%），并输出可验证代码、结果透明易解释。

Conclusion: 代码生成代理能将EO分析从黑箱推断转为透明、可复现的流程，统一API与多层评估带来稳健与可控性，并在关键任务上超越通用LLM/VLM基线。

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [14] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: 提出VDE Bench，一个用于多语言、密集文本视觉文档编辑的基准，含中英高密度文档数据与解耦评估框架，并验证自动指标与人工一致性，系统评测现有多模态编辑模型。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型多聚焦通用或稀疏文本、以英文为主，难以处理密集、结构复杂的文档及非拉丁文字（如中文）；缺少系统性的基准与客观评测来衡量此类任务性能。

Method: 构建人类标注与审核的VDE Bench：覆盖英文与中文、含论文、海报、幻灯片、试卷、报纸等高密度文本文档；提出在OCR解析层面的解耦评估框架，对文本修改的准确性进行细粒度量化；基于该基准对代表性SOTA图像编辑模型进行系统评测，并进行人工一致性验证。

Result: 基准显示现有方法在多语言、密集文本文档编辑上存在明显不足；自动评估指标与人工判断具有较高一致性，证明评测框架有效。

Conclusion: VDE Bench是首个面向多语言、密集文本视觉文档编辑的系统性基准，提供高质量数据与可复现的细粒度评测，为后续模型改进与研究提供标准化参考。

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [15] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

TL;DR: 提出“情境感知自编码器（CAE）”，在海事AIS异常检测中通过引入上下文特定阈值提升检测集体与情境异常的准确性与效率，优于传统自编码器。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器在海事场景下难以识别依赖船舶上下文（如船型、作业状态、航区、时间等）的集体与情境异常；统一阈值会掩盖不同情境下的差异，导致误报/漏报与高计算成本。

Method: 设计将“上下文特定阈值”融合进自编码器重构误差判定的框架，提出四种CAE变体，并与常规模型对比；以“渔船作业状态”异常为案例，在基于AIS的时间序列上评估；分析上下文对重构损失分布的影响。

Result: 上下文显著改变重构损失与判别边界；CAE在时间序列异常检测上优于常规自编码器，检测准确率更高且计算成本更低；四个变体均受益于情境阈值。

Conclusion: 在海事监控中融入上下文的重要性得到验证：采用情境特定阈值的CAE能更好识别集体与情境异常并提升效率，适用于改进船舶交通监视的异常检测。

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [16] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

TL;DR: 论文提出D3R-Net，一种面向制造视觉检测的无监督异常检测重建框架，通过双域（空间与频域）约束与自监督“修复”任务，提升细节缺陷的定位一致性与分割精度，在MVTec AD多类上相较MSE基线显著提升PRO AUC与像素ROC AUC，同时保持高速度与轻量化。


<details>
  <summary>Details</summary>
Motivation: 重建式UAD方法简单高效但对高频细节过度平滑，导致细微缺陷被部分重建而非被突出，限制了分割性能；需要一种既保留细节又避免恒等映射的训练与正则策略。

Method: 提出D3R-Net：1) 自监督“修复”训练——向正常图像注入合成损坏，让网络从受损输入重建干净目标，避免学到恒等映射并逼近无缺陷纹理流形；2) 双域损失——在空间域使用MSE，在频域加入FFT幅度损失以保持高频一致性，且可选加入SSIM项；3) 采用轻量卷积自编码器骨干，从零训练，推理时以重建误差进行异常分割。

Result: 在MVTec AD Hazelnut上，加入FFT损失的D3R-Net相较仅空间损失基线，PRO AUC从0.603升至0.687，图像级ROC AUC保持稳健；在15个MVTec类别上，像素ROC AUC从0.733升至0.751，PRO AUC从0.417升至0.468；单GPU约20 FPS。

Conclusion: 频域感知的双域重建与自监督修复可缓解高频过平滑问题，显著提升缺陷定位一致性与像素级检测性能；轻量、从零训练、速度快，为重特征预训练方法提供实用替代。

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [17] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

TL;DR: 提出POVNet+多模态深度架构，实现ADL多活动识别并区分已知、未知及异常执行情形，用于社交助手机器人主动辅助；在对比与真实HRI实验中优于现有方法并成功触发合适交互。


<details>
  <summary>Details</summary>
Motivation: 现有社交助手机器人难以长期部署，关键障碍是无法同时感知并协助多种日常生活活动（ADLs），尤其是对未见过或异常执行的活动缺乏可靠识别与触发机制。

Method: 提出POVNet+：构建ADL与运动（motion）双嵌入空间，区分“已知ADL”“未见ADL”“已知ADL的非典型执行”；并在运动嵌入空间上引入新颖的用户状态估计方法以识别新ADL并监测执行质量；将ADL感知结果用于主动发起机器人辅助交互。

Result: 在与最新人类活动识别方法的对比实验中，POVNet+获得更高的ADL分类准确率；在多用户、杂乱家居环境的人机交互实验中，部署于Leia机器人上，能成功识别已见/未见/非典型ADL并触发相应辅助。

Conclusion: POVNet+实现对多ADL的鲁棒多模态感知与状态估计，提升分类准确率并支持社交助手机器人在真实场景中主动且恰当地提供辅助。

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [18] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: 提出SCANNER：首个面向仇恨视频检测的测试时自适应（TTA）框架，通过“稳定核心”对齐、样本级自适应质心对齐与簇内多样性正则，解决目标域强语义漂移问题，平均Macro-F1较最优基线提升4.69%。


<details>
  <summary>Details</summary>
Motivation: 现有HVD方法假设训练与测试分布一致，但仇恨内容为规避审查会不断演化成不规则、含混表述，导致源-目标域严重语义漂移，既有TTA多针对轻度分布偏移，难以应对HVD的剧烈变化。因此需要一种能在推理时自适应、并能抓住仇恨表达不变“核心”（如针对性基于性别、种族等）的新方法。

Method: 提出SCANNER框架：1) 以“稳定核心”为桥梁，采用基于质心引导的对齐机制，从含混布局中显化稳定核心；2) 为减轻与质心弱相关的离群样本影响，引入样本级自适应质心对齐策略，动态调整先验，提升对齐稳健性；3) 为防簇内输出过度一致导致语义塌缩，加入簇内多样性正则，提升簇级语义丰富度；整体在测试时更新模型以缩小源-目标域差距。

Result: 在仇恨视频检测任务上，SCANNER在所有基线之上取得最优表现，Macro-F1平均提升4.69%。

Conclusion: 利用仇恨表达的稳定核心进行质心引导与样本级自适应对齐，并配合簇内多样性正则，可在强语义漂移下实现有效测试时自适应，显著提升HVD性能。

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [19] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

TL;DR: 提出LLaVA-FA：在频域进行联合低秩+量化压缩，并配合复数矩阵的极坐标量化与可选的对角校准，实现更紧凑、低误差的LMM压缩，在多基准上优于现有效率模型。


<details>
  <summary>Details</summary>
Motivation: 现有LMM压缩多将低秩分解与量化分离，误差叠加；多模态结构存在跨模态冗余，导致传统方法重构误差大，部署受限于算力/内存。

Method: 将权重映射到傅里叶频域，利用去相关与共轭对称性做联合低秩+量化近似；为复数频域权重设计PolarQuant极坐标量化（幅度/相位分别量化）；提出可选的ODC（对角校准）以免大规模校准数据；整体形成高效的LLaVA-FA框架。

Result: 在多项视觉-语言基准上，激活参数最少、计算开销低的前提下，性能超过现有高效多模态模型；频域联合近似比时域或分步方法重构更精确。

Conclusion: 频域的联合低秩+量化与极坐标量化、ODC能有效缓解误差叠加与跨模态冗余，实现高效且性能优的LMM压缩，具备实际部署价值。

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [20] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

TL;DR: 论文提出一种可扩展的增量学习分类器LR-RGDA与无训练的表征漂移补偿模块HopDC，在保持Bayes最优判别性的同时将推理复杂度从O(C d^2)降为O(d^2 + C r d^2)，并在多种CIL基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: ViT在类增量学习中，分类器重建通常依赖迭代SGD，计算昂贵；解析的RGDA虽精度可比，但推理复杂度二次方随类别数增长，不适用于大规模CIL。需要一种既保留RGDA表达力又具线性分类器效率的方案，并解决因骨干更新导致的历史类统计漂移。

Method: 1) 低秩分解的正则化高斯判别分析（LR-RGDA）：利用协方差的低秩结构和Woodbury恒等式，将判别函数分解为全局仿射项+低秩二次扰动，将复杂度由O(C d^2)降至O(d^2 + C r d^2)，其中r≪d。2) Hopfield-based Distribution Compensator（HopDC）：基于连续Hopfield网络的联想记忆，在无标签锚点上对历史类统计进行重校准，并给出估计误差上界；训练自由，缓解表征漂移。

Result: 在多种CIL基准上取得SOTA表现（具体数据未给出），验证了LR-RGDA的可扩展性与HopDC的有效漂移补偿能力。

Conclusion: LR-RGDA提供了兼具Bayes最优表达与线性级可扩展性的增量学习分类器，HopDC则在无训练条件下稳定历史统计；两者结合，为基于ViT的大规模类增量学习提供高效且精度优异的解决方案。

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [21] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

TL;DR: 研究提出利用红外热成像与多视角深度学习（DensiThAI）来无电离辐射地评估乳腺密度，并在3500名受试者多中心数据上取得平均AUROC=0.73，显示热成像可作为乳腺密度评估的可行补充。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度既是乳腺癌风险生物标志物，也影响钼靶敏感性；现有评估几乎全依赖电离辐射的X线钼靶。若能用非电离手段（热成像）可靠估计密度，将改善患者安全与体验并优化流程。

Method: 提出DensiThAI：多视角（五个标准热视角）深度学习分类框架，从红外热图预测乳腺密度等级。以钼靶推导的密度标签为参考，进行10次随机划分验证，评估AUROC并做统计显著性检验；同时分年龄层分析一致性。

Result: 在3500名女性的多中心数据上，DensiThAI在五视角输入下10次随机划分平均AUROC为0.73；所有划分的密度类别区分均具统计显著性（p≪0.05）；不同年龄层表现一致。

Conclusion: 热成像结合AI可在无电离辐射条件下实现具有实用潜力的乳腺密度分类；虽性能中等，但跨年龄一致且统计显著，提示其可作为临床流程与患者体验的优化补充手段，需进一步提升精度与前瞻性验证。

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [22] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: 提出NGFF：一个将3D高斯感知与物理动力学建模端到端融合的框架，用于从多视角RGB生成可交互、物理一致的4D视频，并以两数量级加速此前高斯物理模拟；同时发布大规模4D高斯碰撞数据集GSCollision以支撑训练与评测。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成虽具备高视觉质量，但缺乏对物理定律的显式建模，导致物理不一致；把3D高斯与传统物理引擎结合的方法计算代价高、重建与仿真缓慢且对复杂真实场景鲁棒性不足。

Method: 提出Neural Gaussian Force Field（NGFF）：端到端神经框架，将3D Gaussian Splatting用于场景/物体表示与感知，并学习神经化力场与动力学约束，实现基于物理的动态建模与交互式4D视频生成；训练上配合自研GSCollision数据集（多材质、多物体交互、复杂场景，约64万段、约4TB），并在合成与真实3D场景上评测。

Result: 在物理一致性与交互式视频预测上表现出强泛化与鲁棒性；相较以往高斯模拟器在重建与仿真速度上提升约两数量级；在多种场景中生成更符合物理规律的4D视频。

Conclusion: NGFF将感知与物理动力学统一于一个可学习框架，显著提升物理一致的视频预测与交互能力；配套的GSCollision数据集促进基于物理的世界模型研究，推动视频预测朝物理扎根的方向发展。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [23] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

TL;DR: 提出SDCM框架，用雷达点云稠密化、雷达补偿映射与Mamba交互融合，提升4D雷达-视觉3D目标检测在恶劣场景下的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 4D雷达点云稀疏，难以形成高质量3D表示；视觉在低光、远距离、遮挡等场景退化，导致融合阶段纹理信息不可靠，制约车联网场景下的稳健3D检测。

Method: 1) SimDen：基于3D核密度估计提取关键点，采用高斯模拟生成点，并结合曲率模拟生成轮廓，实现雷达点云“模拟稠密化”。2) RCM：利用4D雷达全天候、实时性的优势，对视觉退化进行补偿映射，降低对不可靠视觉特征的依赖。3) MMIF：利用模态特征张量差值作为有效互补信号，通过Mamba建模实现异质性降低与跨模态交互融合。

Result: 在VoD、TJ4DRadSet、Astyx HiRes 2019数据集上取得最优性能，同时参数更少、推理更快。

Conclusion: 通过点云稠密化、雷达主导的补偿与基于Mamba的交互融合，SDCM在多种数据集上实现高精度、轻量且高效的4D雷达-视觉3D检测，适用于复杂车联网场景。

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [24] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: 评估多实例学习框架下，基于大规模病理预训练的基础模型在回归型生物标志物（HRD分数）预测上的效果，发现其较对比学习特征显著提升准确性与泛化，并提出分布式上采样缓解目标不平衡。


<details>
  <summary>Details</summary>
Motivation: 虽然病理基础模型在分类等任务上成功显著，但对回归型生物标志物预测（如连续HRD分数）的价值尚不清楚，且真实数据存在目标分布不平衡与跨癌种泛化挑战。

Method: 以WSI为输入，在多实例学习（MIL）框架中用五个SOTA病理基础模型提取patch级特征，与对比学习特征作对比；在乳腺、子宫内膜、肺癌跨两大公开数据集上训练回归模型；提出基于目标分布的上采样策略以缓解HRD分数不平衡；并进行采样策略与实例bag size消融。

Result: 使用基础模型特征的模型在HRD连续预测上优于对比学习基线，且在跨队列/癌种泛化上更稳健；不同基础模型之间性能存在系统性差异；分布式上采样显著提升稀有但临床关键人群的召回率与平衡准确率；采样与bag size对性能有可测影响。

Conclusion: 大规模病理预训练为回归型生物标志物预测带来更高精度与可迁移性；配合分布式上采样可改善临床重要少数群体的识别，有望推动AI驱动的精准肿瘤学。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [25] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

TL;DR: 提出HPPI-Net用于微控制器上的实时人体行为识别，在ARM Cortex‑M4上仅用22.3 KiB RAM与439.5 KiB ROM达96.70%准确率，较MobileNetV3更准且更省内存。


<details>
  <summary>Details</summary>
Motivation: 边缘设备对高精度、低延迟、低功耗的人体行为识别需求增长，但现有方法难以兼顾准确率与算力/内存约束，且缺乏可解释性。

Method: 设计资源感知的两层分层网络HPPI-Net：第一层用FFT谱图做初步特征；第二层依据静/动态状态自适应激活模块：静态动作由专用模块识别，动态动作用并行LSTM‑MobileNet(PLMN)。PLMN并行编码FFT、Wavelet、Gabor三种谱图，经ECA注意力与深度可分离卷积进行通道级重加权与高效特征提炼，减少MAC并提供可解释性。模型经优化部署于ARM Cortex‑M4微控器。

Result: 在Cortex‑M4上实现96.70%准确率，内存占用22.3 KiB RAM、439.5 KiB ROM；相较MobileNetV3，准确率+1.22%，RAM−71.2%，ROM−42.1%。

Conclusion: HPPI-Net在边缘受限平台上实现精度‑效率的优良折中并具可解释性，适用于可穿戴、工业与智能家居的实时HAR。

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [26] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: 提出一种直接在压缩域上运行的轻量级视频跟踪模型，利用码流中的运动向量与变换系数，无需完整RGB解码，实现最高约3.7倍加速，在MOTS15/17/20上mAP@0.5仅下降约4%。


<details>
  <summary>Details</summary>
Motivation: 传统视频跟踪需解码到RGB，计算与带宽开销大，不利于大规模实时监控；而压缩域已包含运动与频域信息，若能直接利用可显著降低算力与延迟。

Method: 从视频编码器输出中提取运动向量与变换系数，构建深度模型在压缩域进行特征建模与目标框跨帧传播，避免像素级重建；与RGB基线在相同数据集上对比。

Result: 在MOTS15/17/20数据集上，相比RGB基线达到最高约3.7倍速度提升，mAP@0.5仅下降约4%。

Conclusion: 压缩域的运动建模对实时多目标跟踪有效，可在大型监控系统中以较小精度损失换取显著吞吐与延迟收益。

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [27] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

TL;DR: 提出一个基于人体姿态的机器学习框架，从常规门诊视频中提取关键点时间序列，并通过多类运动学特征来客观区分多种高动力性运动障碍表型。


<details>
  <summary>Details</summary>
Motivation: HMD（肌张力障碍、震颤、舞蹈样动作、肌阵挛、抽动等）表现波动且常共存，导致临床识别与随访依赖主观评估、存在评估者间差异；缺乏可扩展、客观的方法能从日常临床视频中区分重叠表型。

Method: 将标准门诊视频转化为解剖学关键点的时间序列（pose estimation），随后计算统计、时间域、频域以及更高阶不规则性/复杂度等运动学描述符，构建基于这些特征的机器学习分类框架。

Result: 框架能够从常规视频中提取具有解剖意义的关键点序列，并得到多维度运动学特征以区分不同HMD表型（摘要未给出具体性能指标）。

Conclusion: 基于姿态与多模态运动学特征的ML框架为HMD客观化评估与表型区分提供了可扩展路径，可减轻主观性与评估者差异，适用于常规临床视频。

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [28] [YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2602.00168)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: YOLOE-26将YOLOv26的端到端、无NMS架构与YOLOE的开放词表学习融合，面向实时开放词表实例分割，统一支持文本、视觉示例与无提示三种模式，兼顾精度与部署效率。


<details>
  <summary>Details</summary>
Motivation: 传统YOLO在封闭类别集上表现优异，但难以在实时条件下进行开放词表识别与实例分割；同时，现有开放词表方法常带来额外推理开销或复杂管线，影响实时部署与确定性。作者希望在保持YOLO家族效率与确定性的前提下，把能力扩展到开放词表实例分割，并兼容主流训练/部署生态。

Method: 基于YOLOv26的卷积主干与PAN/FPN特征金字塔，实现端到端回归与实例分割头；将固定类别logit替换为对象嵌入头，以与提示嵌入（文本、视觉示例或内置词表）进行相似度匹配完成分类。提出三项关键组件：1) 可重参数化区域-文本对齐（RepRTA），实现零额外开销的文本提示；2) 语义激活的视觉提示编码器（SAVPE），用于示例引导的分割；3) Lazy Region Prompt Contrast，实现无提示推理下的判别学习。三种提示方式在统一的对象嵌入空间运行，可无缝切换。采用大规模检测与grounding数据进行多任务训练，并与Ultralytics全流程兼容。

Result: 在多种模型规模与使用场景（文本提示、视觉提示、无提示）下表现出一致的可扩展性与良好的精度-效率权衡，实现实时开放词表实例分割；在不牺牲部署确定性与速度的情况下，开放词表能力得到显著提升。

Conclusion: YOLOE-26提供了一个实用、可扩展、端到端、无NMS的统一框架，在保持YOLO部署友好和确定性的同时，将实例分割扩展到开放词表设定，并支持文本、视觉和无提示的多模态推理，适用于动态真实场景的实时应用。

Abstract: This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.

</details>


### [29] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

TL;DR: 提出SPCL用于心脏分割，通过区分类内“内部像素”和“边界像素”，并引入边界对比损失，提升边界表征与分割精度，在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 像素级对比学习在器官分割中易受边界混淆（类间接触处表征污染）影响，导致类内表征不纯与边界漏检/伪影，需要一种能细化类内差异并专门强化边界判别的学习框架。

Method: 1) 提出类内细分的像素对比学习（SPCL），在同一类别内将像素表征划分为“内部区域”和“边界区域”；2) 引入“Unconcerned sample”概念，用于在类内对比时忽略对边界无关的样本对，缓解表征污染；3) 设计专门的边界对比损失，强化跨边界的判别性；4) 给出理论分析证明上述设计能提升判别与稳健性；5) 在公开心脏数据集上验证。

Result: 在多个公开心脏影像数据集上，SPCL显著提升总体分割指标与边界精度，全面优于现有基线与先进方法（代码已开源）。

Conclusion: 通过类内细分与边界对比，SPCL有效缓解边界表征污染，增强类内一致性与跨边界可分性，从而显著提升心脏图像分割质量与边界精度。

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [30] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出噪声—频率续接(Continuation)框架，用带通(随噪声而变)的测量一致性指导来稳定扩散后验采样；先粗后细，多分辨率地注入可靠低频再逐步引入高频，从而在超分辨、修补、去模糊等任务上显著提升细节与稳健性，运动去模糊PSNR较强基线提升最高约5 dB。


<details>
  <summary>Details</summary>
Motivation: 传统扩散后验采样把测量一致性项与噪声层弱耦合：高噪声早期阶段的梯度来自不可靠估计，方向与真实后验几何不一致，导致早期漂移、虚假高频伪影，并对调度和病态算子极敏感，致细节恢复差。需要一种在扩散噪声层次上与频率细节相匹配的、一致性逐步增强的机制。

Method: 提出噪声—频率Continuation：构造一族连续中间后验，使似然只在与当前噪声水平相匹配的频带内 enforcing 一致性。具体采样器由三部分组成：1) 扩散预测器(标准去噪/预测步)；2) 频带限制的似然引导，对低噪声时放宽至更高频、对高噪声仅约束低频；3) 多分辨率一致性策略，先强力校正可置信的低频/粗尺度，再在可辨识时谨慎引入高频细节。

Result: 在超分辨、图像修补、去模糊三类逆问题上达到SOTA；尤其在运动去模糊上，相比强基线PSNR最高提升约5 dB，同时减少早期漂移与高频伪影，对调度与算子条件更稳健。

Conclusion: 将测量一致性与扩散噪声层按频率带耦合、并采取自粗到细的续接采样，可显著稳定扩散后验采样、提升细节与鲁棒性，适用于多种成像逆问题。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [31] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: CamReasoner将相机运动理解从黑盒分类改为“观察-思考-回答”结构化推理，结合SFT推理链与RL逻辑对齐，利用几何与时空线索抑制幻觉并在多基准上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型易将物理上不同的相机运动混淆，依赖表面视觉模式而非几何线索，导致缺乏对电影摄影逻辑与物理动态的可靠理解。

Method: 提出CamReasoner框架：以Observation-Thinking-Answer（O-T-A）范式，将相机运动理解显式化为对时空轨迹、视锥等几何线索的解码与推理；构建大规模推理轨迹数据集（约18k SFT推理链、38k RL反馈样本）；首次在该任务中采用强化学习进行逻辑对齐，使推理更贴合物理几何而非语境猜测。

Result: 通过在O-T-A范式上应用RL以减少幻觉与误判，CamReasoner在多个基准上取得SOTA性能，展示了更稳健的几何一致推理与运动判别能力。

Conclusion: 结构化推理结合RL的逻辑对齐能显著提升相机运动理解的几何可靠性与泛化；CamReasoner有效弥合感知与电影摄影逻辑之间的鸿沟，为视频空间智能提供更可信的运动推理。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [32] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: 论文发现现有修复检测器主要依赖全局重建伪迹（尤其是VAE带来的频谱偏移），而非局部合成内容；提出INP-X操作与90K测试集，证明在去除全局伪迹后SOTA检测器准确率大幅下滑，并给出理论解释与改进训练方案。


<details>
  <summary>Details</summary>
Motivation: 深度修复使局部篡改极其逼真，可靠检测变难。现有检测器可能借助修复模型的副作用（全局伪迹）而非真正定位被编辑区域，导致泛化与定位能力不足，需要验证并纠正这种偏差。

Method: 1) 观察：VAE重建对整幅图产生细微但普遍的高频衰减与频谱偏移。2) 提出INP-X：在修复图中将未编辑区域替换回原图像像素，仅保留合成区域，从而隔离全局伪迹。3) 构建含真实、修复、以及INP-X交换图的9万样本测试集。4) 评估预训练SOTA（含商用）检测器在该干预下的性能变化，并进行理论分析，将性能下降与VAE信息瓶颈导致的高频衰减关联。5) 以该数据进行再训练，评估泛化与定位能力。

Result: 在INP-X干预后，多个SOTA与商用检测器准确率从约91%骤降至约55%，接近随机水平，显示其主要依赖全局频谱伪迹而非局部合成。理论上证实VAE瓶颈导致高频成分被抑制。用新数据训练可显著改善跨方法泛化与篡改区域定位。

Conclusion: 当前修复检测器存在“全局伪迹依赖”脆弱性；应转向面向内容、关注局部合成特征的检测与定位。INP-X与所建数据集为评估与训练此类检测器提供基准，并能提升实际泛化与定位表现。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [33] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

TL;DR: 提出SemiEarth：在遥感半监督语义分割中，用VLM-PP模块利用视觉-语言模型净化teacher伪标签，显著提升边界与低置信区域质量，体系结构无关并具开放世界纠错能力，实验在多数据集达SOTA且更可解释。


<details>
  <summary>Details</summary>
Motivation: 传统半监督语义分割（教师-学生框架）严重依赖伪标签，但其质量常低、尤其在多类别边界处错误多，导致学生学习被误导。遥感场景类间混淆和长尾问题更突出，迫切需要低成本地提升伪标签可靠性并增强可解释性。

Method: 构建SemiEarth框架：在教师-学生S4中插入VLM伪标签净化（VLM-PP）模块。该模块利用通用视觉-语言模型对教师产生的低置信伪标签进行校正与筛选，重点处理多类边界区域；当VLM预测与伪标签不一致时触发纠错，以开放世界能力识别并更正误分类；模块与具体S4架构解耦，可即插即用。

Result: 在多套遥感数据集上进行广泛实验，SemiEarth取得SOTA性能；相较以往方法在边界区域与低置信样本上提升显著，并提供更好的可解释性。

Conclusion: 将VLM引入S4以净化伪标签是有效路径。VLM-PP显著改善教师伪标签质量并提升学生训练效果，具架构无关、开放世界与可解释优势；SemiEarth为遥感半监督分割提供了通用、强性能且可解释的方案。

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [34] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: 提出VCoR：把无监督可变形配准重构为多跳视觉推理过程，通过迭代的局部空间细化与交叉参考注意力实现高可解释、高可靠的配准，并在肺CT与脑MRI上达竞品精度且提供不确定性估计与中间可视化。


<details>
  <summary>Details</summary>
Motivation: 无监督医学图像配准需在无标注下对齐复杂解剖结构，现有深度方法虽准但黑箱，易误差漂移，缺乏临床信任与可解释性；需要既稳健又可解释、能处理大形变并量化不确定性的框架。

Method: 将配准表述为多跳视觉推理：每一跳包含局部空间细化（LSR）以增强特征表征、与交叉参考注意力（CRA）以引导迭代细化并保持解剖一致；产生一系列中间形变场与预测，并给出理论收敛/界；基于多跳间形变场的稳定性与收敛性估计不确定性。

Result: 在DIR-Lab 4D CT（肺）与IXI T1 MRI（脑）上，VCoR在精度上具竞争力；对大形变更鲁棒，并能输出可视化的中间结果与可信度/不确定性度量。

Conclusion: 通过嵌入隐式视觉推理范式，VCoR实现了可解释、可靠且具临床可行性的无监督医学图像配准，兼顾精度、透明度与不确定性量化。

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [35] [Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images](https://arxiv.org/abs/2602.00212)
*Sathish Krishna Anumula,Vetrivelan Tamilmani,Aniruddha Arjun Singh,Dinesh Rajendran,Venkata Deepak Namburi*

Main category: cs.CV

TL;DR: 提出一种轻量化自定义CNN，用于在胸部X光片上高精度、低计算成本地自动检测肺炎，并通过CLAHE与几何增强缓解类别不平衡与提升泛化；在5863张AP位X光数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 手工解读胸片存在专家短缺、观察者间差异与疲劳等问题，导致诊断效率与一致性不足；通用迁移学习模型参数冗余，不适配灰度医学图像纹理。

Method: 构建面向灰度纹理的定制CNN，采用深度可分离卷积以减参与降算；预处理包含CLAHE增强对比度、几何数据增强以缓解类别不平衡并提升泛化；在5863张AP位胸片上进行训练与测试。

Result: 模型能够以较高精度、较低计算开销识别胸片中的肺炎（摘要未给出具体数值）。

Conclusion: 定制化、轻量的深度可分离卷积CNN结合CLAHE与数据增强，可在资源受限场景实现对肺炎的快速、准确、可泛化的自动诊断。

Abstract: Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.

</details>


### [36] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: 提出MFM-Geom：一种在bp-MRI与临床报告上预训练/微调的几何多模态基础模型，通过SPD与黎曼学习融合影像与文本，实现更鲁棒PCa判别；在仅用10%训练数据下，AUC-PR达90.67，较基线提升8.3%，外部集AUC-PR 90.6。


<details>
  <summary>Details</summary>
Motivation: 现有PCa CAD多依赖影像单模态与专家判读，忽视临床变量语境，且数据稀缺导致表征不稳、泛化差；需要能整合影像与临床文本并在小样本下仍稳健的模型。

Method: 构建几何多模态FM：以bp-MRI与临床报告联合学习表征；在分类头中使用SPD矩阵与黎曼深度学习，将影像-文本多模态表示在Riemann流形上对齐与融合；以少量标注数据进行微调，并与class token嵌入式基线比较；在外部数据集上评估泛化。

Result: 用10%训练数据即可超越基线：AUC-PR 90.67，较class token基线提升+8.3%；在外部数据集上微调模型仍达AUC-PR 90.6，显示良好稳健与泛化。

Conclusion: 几何多模态融合（SPD+黎曼学习）能有效整合bp-MRI与临床变量文本，在数据受限场景下显著提升PCa识别性能与泛化，优于传统基于class token的分类方式。

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [37] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

TL;DR: 开发离线移动应用，集成深度学习模型，帮助菲律宾小农户识别可可病害与黑荚病感染程度；验证准确率分别为96.93%与79.49%，田间测试与专家一致率84.2%。


<details>
  <summary>Details</summary>
Motivation: 小农可可种植者缺乏数据与良好农业规范，受病虫害困扰且资源有限，难以像大型种植园那样获取诊断与管理支持；偏远地区网络不足，亟需可离线的实用工具提升病害识别与管理能力。

Method: 构建并训练深度学习图像识别模型用于可可病害分类与黑荚病感染等级判别；将模型嵌入可离线运行的移动应用，实现田间拍照诊断与管理建议；通过验证集评估模型性能，并在田间与专家评估进行对比测试。

Result: 病害识别模型验证准确率96.93%；黑荚病感染水平检测模型验证准确率79.49%；应用在田间测试与专家技术员评估的一致率为84.2%。

Conclusion: 离线AI手机应用可在资源受限与网络不佳环境中为小农提供可靠的病害识别支持，提升可可作物健康与生产力；尽管整体表现良好，但黑荚病分级仍有改进空间。

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [38] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: 提出CAPA框架，通过基于注意力贡献的视觉token裁剪与FFN线性近似，提升LVLM推理效率且保持性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LVLM推理成本高，主要因需处理大量视觉token。现有用注意力概率评估token重要性不准确，且对视觉注意力“sink”与FFN冗余缺乏机制化理解与利用。

Method: 1) 定义注意力贡献（Attention Contribution）：将注意力概率按对应value向量范数加权，用作更准确的视觉token重要性度量；2) 经验分析区分两类视觉注意力sink：低贡献的概率汇（Probability Dumps，可安全裁剪）与高贡献的结构锚（Structural Anchors，需保留）；3) 发现与量化视觉token在FFN中的冗余，尤其在中间层呈线性行为；4) CAPA双策略：在关键功能转折层基于注意力贡献进行视觉token裁剪；并对与视觉token相关的FFN以高效线性近似减少计算。

Result: 在多项基准与多种基线模型上，CAPA在效率—性能权衡上优于对照方法，并展现更强鲁棒性。

Conclusion: 注意力贡献优于注意力概率作为视觉token选择标准；视觉sink具有异质性，需区分裁剪与保留；结合token裁剪与FFN线性近似的CAPA可在不显著损失性能下显著加速LVLM推理，并提升鲁棒性。

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [39] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 提出SANEval，一个面向开放词表的文本到图像(T2I)组合性评测基准，结合LLM理解与LLM增强开放词表检测器，能在属性绑定、空间关系与数目理解上更贴近人工评价，并将开源数据与评测管线。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型对多物体、属性与空间关系等复杂提示的忠实生成仍差，部分原因是评测基准受限：封闭词表、诊断粒度不足、缺乏可解释反馈，难以定位并修复具体组合性错误。

Method: 构建SANEval评测管线：用LLM进行深层提示解析，串联LLM增强的开放词表目标检测器，对图像中对象、属性与关系进行开放词表匹配与一致性判定，从而在无需固定词表的前提下评估组合性遵从度。

Result: 在六个SOTA T2I模型上实验，SANEval的自动评分与人工评估更一致；其指标在属性绑定、空间关系与数目任务上取得更高的Spearman秩相关，并与现有基准相比差异具有统计显著性。

Conclusion: SANEval为开放词表的组合性评估提供可扩展、可解释的方案，将发布数据集与开源评测管线，助力后续T2I组合性生成与评测研究。

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [40] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

TL;DR: 提出CSC：一种针对缺失数据的对比自监督子空间聚类框架。通过对部分观测输入做掩码增强、用SimCLR式对比损失学到缺失不变的表示，再用稀疏子空间聚类完成分簇；在六个数据集上优于传统与深度基线，具备鲁棒性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现实数据常有缺失值，而多数子空间聚类方法假设数据完整，导致性能与适用性受限。需要一种能在不完整观测下仍能学习有效子空间表示并保持可扩展性的方案。

Method: 构造对缺失条目敏感的多视图掩码增强；用对比自监督学习（SimCLR风格）训练深度网络，优化同一样本不同掩码视图的一致性以获得缺失不变的嵌入；随后对学到的嵌入应用稀疏子空间聚类（SSC）得到聚类划分。

Result: 在六个基准数据集上，CSC在聚类指标上持续优于经典与深度学习基线，尤其在高缺失率下表现更稳健，同时在大规模数据上表现出良好扩展性。

Conclusion: 对比自监督可在缺失观测下学习鲁棒的子空间表示；将其与SSC结合能显著提升不完整数据的聚类性能，并兼顾鲁棒性与可扩展性。

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [41] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: World-Shaper 在等距矩形投影(ERP)域内统一处理全景图生成与编辑，通过“先生成再编辑”的范式合成成对数据，并采用几何感知学习以缓解球面失真，在新基准PEBench上实现更好的几何一致性、编辑保真与文本可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于透视图的编辑无法建模全景球面结构；基于立方体贴图的分解会因与球面几何不匹配而破坏全局一致性。需要一种直接在ERP域进行、几何一致且可控的全景编辑方法。

Method: 1) 统一框架World-Shaper，在ERP域内将生成与编辑集成为以编辑为中心的流程；2) 采用“生成→编辑”两阶段：先做可控全景生成以合成多样化成对数据，再进行监督式编辑学习；3) 几何感知学习：显式位置感知形状监督+渐进式训练以内化全景先验，缓解ERP几何畸变；4) 在PEBench基准上评估。

Result: 在PEBench上，相比SOTA，World-Shaper在几何一致性、编辑保真度与文本可控性上全面提升，实现连贯、灵活的360°世界创建与统一编辑控制。

Conclusion: 直接在ERP域进行、结合生成辅助与几何感知训练的World-Shaper，有效解决全景编辑的几何一致性与数据稀缺问题，为高保真、可控的360°编辑提供统一方案；代码、模型与数据将开源。

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [42] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: 提出PLACID框架，利用带文本控制的预训练图生视频扩散模型与合成序列数据策划，实现多物体高保真合成，在身份、背景、颜色保持与布局完整性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽能合成逼真图像，但在“工作室级”多物体拼接上易出现物体细节被改动、遗漏/重复、相对尺寸与呈现不一致等问题；需要一种既保持每个对象身份与背景色彩，又能精确控制布局与完整展示的方案。

Method: 1) 使用带文本控制的预训练图生视频(I2V)扩散模型，引入视频的时间先验以增强对象一致性与背景细节保真；2) 设计合成数据策划：生成序列中对象从随机位置平滑移动到目标位置，使训练分布与视频时间先验对齐。推理时从随机初始化出发，在文本引导下逐帧收敛，最终帧作为合成结果。

Result: 大规模定量评测与用户研究显示，PLACID在多物体合成上显著优于SOTA：更好地保持对象身份、背景和颜色，减少对象遗漏，整体视觉更具吸引力。

Conclusion: 通过将视频时间先验与合成序列数据对齐，PLACID实现了稳定且可控的多物体合成，解决了现有方法在身份保持与布局一致性方面的关键缺陷，并在客观与主观指标上取得领先。

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [43] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 论文提出一种在自回归视频生成推理阶段的轻量方法，通过检测并移除“不稳定”的潜在token，抑制误差在长时序上的传播，从而显著降低时间漂移，无需改动模型架构或训练。


<details>
  <summary>Details</summary>
Motivation: 自回归视频在长时序上会因误差累积导致严重时间漂移。作者认为问题主要源自推理时错误传播（被污染的潜在token被反复用作条件），而非模型容量不足，因此动机是从推理机制入手减少误差传递。

Method: 在每个自回归步，比较当前批次与前一批次生成的潜在token表示，度量其偏离程度；将偏离显著（不稳定/可能被污染）的token判定为不可靠，并在后续条件构造时将其剔除（而非修改整块空间区域或模型参数），从而避免其影响后续生成。

Result: 在无需更改架构、训练或脱离潜在空间的前提下，该方法显著提升长时序一致性，减轻时间漂移并稳定生成质量。

Conclusion: 通过仅在推理时识别并移除不稳定潜在token，可有效阻断误差在自回归视频生成中的传播，提供了简单通用、代价低的长视频一致性改进方案。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [44] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind基准用最小成对视频设计，隔离静态视觉线索，专测细粒度时空理解（事件识别、属性、依赖）。20+前沿MLLM在实例准确率仅48.2%，远低于人类98.2%，显示模型依赖静态捷径而非时间逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM善于静态语义，却在时间动态和组合时空推理上脆弱；现有基准常将识别与时间推理混淆、存在语言先验偏置，缺乏能纯粹诊断时间理解能力的工具。

Method: 提出TimeBlind：以认知科学为依据将时间理解分为三层（原子事件识别、事件属性刻画、事件相互依赖推理）；构造最小成对视频，只在时间结构上不同而静态内容一致；配对互补问题消除语言先验；在600个实例（2400视频-问题对）上评测20+最先进MLLM。

Result: 最佳模型在“实例准确率”（需正确区分成对两视频）仅48.2%，远低于人类98.2%；表明模型高度依赖静态视觉捷径而非真实时间逻辑。

Conclusion: TimeBlind揭示当前MLLM在细粒度时空理解上的显著缺陷，提供一个严谨的诊断基准，推动下一代视频理解与具身智能研究。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [45] [Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory](https://arxiv.org/abs/2602.00289)
*Alan Yuille,Daniel Kersten*

Main category: cs.CV

TL;DR: 用贝叶斯决策理论作为统一视角，综述计算机视觉与认知科学的关联，比较贝叶斯方法与深度神经网络，并讨论其互补性与超越BDT的融合方向。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域庞杂，现有两大主线（贝叶斯推断与深度学习）各有优势但缺乏统一框架；作者希望用贝叶斯决策理论（BDT）提供一个能同时解释二者并与认知科学相呼应的理论视角，并指出其局限与未来融合路径。

Method: 以BDT为理论框架：阐释视觉任务中的决策、损失与不确定性；在该框架下对比两种路线：(i) 概念上贴近认知科学的贝叶斯观；(ii) 受腹侧通路层级结构启发、在工业界成功的深度神经网络。分析各自优劣与相互关系，并讨论BDT不能覆盖之处及扩展思路。

Result: BDT能作为统一语言关联两种方法，刻画其优势与弱点；同时揭示仅靠BDT不足以完全解释或整合两者。

Conclusion: BDT提供了抓要点的理论镜头，既能对照贝叶斯与深度网络，也暴露其限制；面向未来需在BDT之上构建更丰富的框架以融合两者优势。

Abstract: This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.

</details>


### [46] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: LogicGaze提出一个基准，用带有因果链与视觉矛盾扰动的数据，系统评测VLM在逐步推理时是否真正“看见”并避免幻觉；结果显示多种SOTA模型在三项任务上均暴露明显缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前VLM强调多步推理能力，但缺乏对其是否将推理与真实视觉证据对齐的严格检验；尤其是模型易被语言上合理但视觉上不成立的描述诱导产生幻觉，需要一个能够逐步验证每一推理环节的评测框架。

Method: 构建LogicGaze基准：从40k ShareGPT4Video视频片段和Flickr30k子集采样，设计包含因果序列的样本，并注入在语言上合理、在视觉上矛盾的扰动；提出三部分评测协议——因果验证、扎根叙事合成、扰动拒绝——要求模型逐步核对并拒绝不符视觉证据的推理步骤。

Result: 在LogicGaze上，多个先进VLM（例如Qwen2.5-VL-72B）在三项子任务中表现出明显脆弱性，难以稳定地识别并拒绝与视觉证据冲突的因果链或叙事，显示出显著的幻觉与不扎根问题。

Conclusion: VLM的顺序推理仍缺乏稳健的视觉扎根能力。LogicGaze为评测与推动可信多模态推理提供了公开资源与标准，呼吁社区关注因果链级别的扎根验证与抗幻觉能力。

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [47] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: 提出SAM2CT，将放射科日常读片中的稀疏注释（箭头、线测量）转为3D CT分割，用于大规模数据挖掘；在公开基准和临床GSPS上效果好，具备零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 高质量3D分割标注昂贵且稀缺，但PACS中普遍存在放射科医师在常规流程产生的稀疏注释（箭头、线），若能转为3D分割，可低成本构建大规模CT分割数据集。

Method: 提出“Opportunistic Promptable Segmentation”范式与模型SAM2CT：在SAM2基础上扩展提示编码器以支持箭头和线作为提示；引入针对3D体数据的Memory-Conditioned Memories（MCM）记忆编码策略；将GSPS对象与CT体积联合输入，生成3D分割。

Result: 在公开病灶分割基准上，使用箭头提示Dice=0.649，线提示Dice=0.757，优于现有可提示分割模型与相似训练基线；在N=60的临床PACS GSPS上，87%案例生成的3D分割被放射科医师评为临床可用或仅需小幅修改；在部分急诊科发现上表现出较强零样本能力。

Conclusion: 利用历史GSPS稀疏注释可规模化构建3D CT分割数据集；SAM2CT有效将临床日常注释转化为高质量3D分割，具备临床可用性与跨任务泛化潜力。

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [48] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

TL;DR: 本文评估自动驾驶感知系统在不利与对抗性场景下的稳健性，提出基于多模型集成的不确定性/敏感度量化框架，并用停车标志制动距离准则来关联感知性能与行驶安全；实验显示弱光（雾、低太阳高度）与遮挡显著恶化模型表现，距离增大也显著降低鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶依赖实时、准确、可靠的感知，但自然与对抗性干扰会致误检、漏检与时延。需要量化感知鲁棒性并寻找提升策略，以支撑安全决策与操控。

Method: 构建基于模型集成（YOLOv8-v9、DETR50/101、RT-DETR）的预测敏感度量化：以模型分歧与推理变异性为不确定性指标；在仿真与真实不利驾驶条件下测试。提出一个感知评估体系结构，并以在不同路面（干/湿沥青）与速度下接近停车标志时的制动距离作为感知-安全关联的评估准则。

Result: 弱光（雾、低太阳高度）对所有模型打击最大；道路目标遮挡等对抗条件提升感知敏感度并降低性能；对抗条件与恶劣天气叠加时性能进一步下降；目标距离越大，感知性能越差，鲁棒性越弱。

Conclusion: 多模型敏感度/不确定性评估能有效揭示感知系统在不利条件下的脆弱性；弱光、遮挡与远距离是主要痛点。基于制动距离的准则可将感知性能与安全需求对齐，为提升自动驾驶感知鲁棒性与系统设计提供指导。

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [49] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

TL;DR: 提出SynerNet多智能体框架，缓解VLM在OOD概念下的跨模态对齐退化，通过四个专门单元与消息传递协调，实现少样本/零样本性能显著提升（精度+1.2%~5.4%）。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在遇到分布外概念时，跨模态对齐易退化，导致识别与理解能力下降，需一种可在少样本条件下稳健适配的新机制。

Method: 构建由四个协同单元组成的SynerNet：视觉感知、语言语境、名词嵌入与全局协调。通过结构化消息传播在潜在空间进行“命名法”获取与对齐；引入语义上下文互换算法以加强few-shot适配；采用自适应动态平衡机制在不同模态/单元间调节权重。

Result: 在VISTA-Beyond基准上，few-shot与zero-shot均显著提升，多个领域精度提升1.2%~5.4%。

Conclusion: SynerNet通过多智能体协同与语义交换/动态平衡策略，有效缓解OOD下的跨模态对齐退化，提升VLM的泛化与少样本适应能力。

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [50] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: 论文提出MAD-RAG，一种无需训练的RAG干预方法，通过双问题建模与注意力混合，缓解检索文本对视觉注意力的全局干扰（AD），在OK-VQA、E-VQA、InfoSeek上显著优于基线，并以极低开销修复多数失败案例。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM的RAG在知识型VQA中常失败，既有工作将失败归因于对检索文本关注不足并建议削弱图像token注意力。然而作者发现另一关键失效模式：当检索文本足够相关时，会全局压制视觉注意力并使其偏离与问题相关的图像区域，导致原本无需检索也能答对的问题反而出错。

Method: 提出MAD-RAG（无需训练）：1）双问题表述，解耦“视觉落地/证据定位”与“上下文整合”，避免检索文本在定位阶段分散视觉注意；2）注意力混合，将仅基于图像的问题注意力（图像条件证据）与包含检索上下文的问题注意力进行融合，从而保留关键视觉证据并稳健利用文本信息。

Result: 在OK-VQA、E-VQA、InfoSeek三套基准上，MAD-RAG在不同模型族上稳定优于基线，相对vanilla RAG绝对提升分别最高达4.76%、9.20%、6.18%；并能以可忽略的计算开销纠正多达74.68%的失败案例。

Conclusion: RAG失败不止源于对检索信息注意不足，更关键的是注意力被检索文本全局分散（AD）。MAD-RAG通过解耦视觉证据定位与文本整合并进行注意力混合，有效缓解AD、提升VQA性能且无需额外训练与开销。

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [51] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出AdaFuse：用强化学习实现按患者自适应的多模态选择与融合，用更少计算在NLST肺癌风险预测上取得最高AUC。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法要么等权/加权融合全部模态，要么预先固定子集，忽略了个体层面“是否需要使用某些模态”的决策，导致不必要的计算与潜在噪声引入。

Method: 将多模态融合建模为序贯决策过程：策略网络按已获得的信息逐步决定是否引入下一模态或停止并输出预测；以强化学习训练策略，实现患者级模态选择与早停，并与固定/自适应基线比较；任务为NLST肺癌风险预测。

Result: 在NLST上，AdaFuse AUC=0.762，优于最佳单模态0.732、最佳固定融合0.759以及自适应基线DynMM 0.754与MoE 0.742；同时相较三模态方法计算FLOPs更低。

Conclusion: 强化学习驱动的按需模态选择可实现个体化、多模态高效融合：在保证或提升准确性的同时减少不必要模态与计算，推动从“一刀切”融合转向自适应诊断流程。

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [52] [MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI](https://arxiv.org/abs/2602.00348)
*Zhengyi Lu,Ming Lu,Chongyu Qu,Junchao Zhu,Junlin Guo,Marilyn Lionts,Yanfan Zhu,Yuechen Yang,Tianyuan Yao,Jayasai Rajagopal,Bennett Allan Landman,Xiao Wang,Xinqiang Yan,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出MASC：在MRI中联合优化“金属感知”采样与伪影校正的强化学习框架，端到端训练在加速采集中显著减少金属伪影并优于传统采样策略，具备跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 金属植入物在MRI中产生严重伪影，破坏诊断价值；现有方法通常将金属伪影校正（MAR）与加速采样分开处理，导致采样策略与重建/校正网络不匹配、性能受限。需要一个能同时学习最优采样与伪影校正、并面向金属场景的统一框架。

Method: 1) 基于物理仿真构建配对数据集：对含/不含金属的三维MRI生成匹配的k-space与重建，提供“金属污染—干净参考”对，用于监督。2) 将主动采集建模为序列决策：用PPO智能体在采集预算内选择相位编码线（k-space行），以金属伪影感知的奖励最大化重建质量。3) MAR网络采用U-Net，对欠采样重建进行伪影校正。4) 端到端联合训练：采样策略学习为伪影去除挑选最有利的k-space线，同时MAR网络适应由策略引起的欠采样分布。

Result: 学到的采样策略优于常规（如均匀、变量密度等）采样；端到端联合优化优于固定预训练MAR网络；在FastMRI上注入物理仿真伪影的跨数据集实验显示良好泛化。

Conclusion: 通过将金属感知采样与伪影校正统一到一个RL+深度重建框架，并进行端到端训练，可在加速MRI中显著降低金属伪影并提升重建质量，且具备跨数据集的适用性；代码公开，便于复现与扩展。

Abstract: Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc

</details>


### [53] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

TL;DR: 提出ReLAPSe：将概念恢复视为强化学习问题，用模型内在的噪声预测损失作为可验证奖励，学习可迁移的提示操控策略，以高效破解“机器遗忘”后仍残留的潜在视觉信息，实现近实时恢复被删除的身份与风格。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘用于从文本到图像扩散模型中移除未授权概念，但实证表明潜在视觉信息仍会泄露。现有对抗恢复要么每例迭代优化、计算昂贵，要么依赖启发式/推理、缺乏来自模型潜在表征的直接反馈，难以高效、可迁移地恢复被忘概念。

Method: 将“概念恢复”重构为强化学习任务，提出ReLAPSe框架：以RLVR（可验证奖励的强化学习）训练策略代理，把扩散模型的噪声预测损失作为内生、可验证的奖励信号，形成闭环，用文本提示操控与潜在残差信息直接对齐，从而学习可泛化的恢复策略而非逐例优化提示。

Result: 在多种最先进的遗忘方法上，能高效、近实时地恢复细粒度身份与风格；策略具有可迁移性，相比迭代优化大幅降低开销，可作为可扩展的红队化工具。实验含涉及敏感视觉概念（如裸露）的评估。代码开源。

Conclusion: 通过从逐例优化转向全局策略学习，ReLAPSe用模型内在损失作为可验证反馈，实现对“遗忘”扩散模型的高效对抗恢复，揭示现有机器遗忘的脆弱性并为更严格的安全评估提供工具。

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [54] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 该论文提出用比较学习取代直接评分来评估图像字幕与图像的匹配度，并在VICR数据集上验证其效果：回归模型更强，但比较学习随数据增长逼近基线；人工实验显示比较标注更快且一致性更高。


<details>
  <summary>Details</summary>
Motivation: 人工对图像字幕的绝对准确度打分既耗时又主观，但人类更擅长在同一图像下比较两条字幕孰优。希望构建能利用较易获取、成本更低且一致性更好的比较性标注的模型，以降低标注成本并更贴合人类偏好。

Method: 在VICR数据集上：视觉特征用ResNet-50提取，文本特征用MiniLM提取；训练两类模型——(1) 基于绝对评分的回归模型；(2) 基于成对比较偏好的比较学习模型（将比较判断建模为偏好/排序学习问题），并用其对未见图文对进行排序。另进行小规模人为对比实验：绝对评分、成对比较、同图比较的效率与一致性。

Result: 回归模型性能更高（Pearson ρ=0.7609，Spearman r_s=0.7089）；比较学习模型随数据增多稳步提升并逼近回归基线。人类实验表明：比较式标注速度更快、注释者间一致性更高。

Conclusion: 比较学习能够有效捕捉人类对图像字幕匹配度的偏好，同时显著降低标注成本；尽管当前回归模型更强，但随着比较数据规模扩大，比较学习有望成为实用替代。

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [55] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

TL;DR: 比较YOLOv5与Faster R-CNN在自动驾驶目标检测中的表现：YOLOv5整体mAP、召回率与训练效率更优；Faster R-CNN在小远目标与复杂光照下更有优势。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶对目标检测依赖极强，但通用架构（YOLO、SSD、Faster R-CNN）在具体自动驾驶场景中的适配与取舍缺乏明确指导，需要系统对比其在精度、速度、鲁棒性等维度的差异。

Method: 基于包含真实与合成图像的多样化数据集，对一阶段YOLOv5与两阶段Faster R-CNN进行对比实验；评估指标包括mAP、召回率与推理速度；分析不同置信阈值、不同分辨率、不同数据规模与多种真实场景条件下的模型行为。

Result: YOLOv5在mAP、召回率和训练效率上占优，且优势随数据规模与图像分辨率提升而增强；Faster R-CNN在小、远目标检测与复杂光照场景中表现更好。

Conclusion: 自动驾驶中若需求偏实时与总体精度，优先选YOLOv5；若任务强调小目标与极端光照鲁棒性，可选Faster R-CNN。应依据场景与传感器配置选型，并通过阈值与分辨率调整权衡性能。

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [56] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

TL;DR: 提出利用4D-CTA多时相自动去骨/去软组织以强化血管显影，并用同一金标准在多相训练，显著扩增数据与提升对比相位鲁棒性；基于该数据集训练的nnUNet在TopBrain上动脉mDC 0.846、静脉0.957，形状与拓扑指标亦优，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有基于CTA的脑血管分割面临标注昂贵、静态单相对比依赖强、跨相位泛化差等问题。作者希望利用动态4D-CTA时序信息减少人工标注成本、增强血管显影并提升模型对不同对比相位的鲁棒性。

Method: 1) 从4D-CTA多时相中通过时相差分进行去骨与去软组织，突出动静脉；2) 以多时相共享同一金标准标注，将同一分割用于4-5个相位以数据增广；3) 构建包含25名受试者110张训练图像、14名受试者165张测试图像的数据集；4) 训练nnUNet并在TopBrain等数据上评估，采用mDC、aDHD、tSens等指标。

Result: 在TopBrain数据集上，动脉mDC=0.846、静脉mDC=0.957；aDHD动脉0.304 mm、静脉0.078 mm；tSens动脉0.877、静脉0.974；相较两套规模相近的CTA分割数据集，上述指标全面更优。

Conclusion: 利用4D-CTA多时相进行预处理与多相共享标注可显著降低标注难度并提升分割精度与形态/拓扑保真度；所建数据集对对比相位具有鲁棒性，训练的nnUNet在动静脉分割上达到先进水平；代码与模型公开，具备复现与扩展潜力。

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [57] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: 论文评估葡萄牙语（巴葡）图像描述模型：对比“母语人工标注版”和“英文自动翻译版”Flickr30K，在交叉训练/测试中衡量翻译影响；加入注意力热图与CLIP-Score。结果：Swin‑DistilBERTimbau总体最稳、泛化佳；ViTucano在传统文本指标上优于更大多语模型；GPT‑4系在CLIP‑Score最佳但文本指标不一定领先；注意力分析发现性别、计数与空间偏差。


<details>
  <summary>Details</summary>
Motivation: 现有IC研究以英语为主，巴葡缺乏高质量本地数据与专门模型。常见做法是把英文数据自动翻译，可能引入噪声且影响评测公平性与泛化。因此作者欲系统比较母语人工标注与机器翻译数据对巴葡IC的训练与评测影响，并考察不同Transformer视觉语言模型的表现与偏差。

Method: - 数据：两版Flickr30K——(1) 巴葡母语者人工写作描述；(2) 英文到葡语的自动翻译版。
- 训练/评测：同域与交叉域（在一版训练、另一版测试）以衡量翻译数据的影响与泛化。
- 模型：多种Transformer视觉编码器×葡语/多语文本编码器组合（含Swin‑DistilBERTimbau、ViTucano、多语大模型如GPT‑4o、LLaMA 3.2 Vision）。
- 解释性：注意力图分析。
- 评价：传统文本相似度指标与CLIP‑Score（图文对齐度）。

Result: - Swin‑DistilBERTimbau在两数据域内与交叉情境中表现稳定并领先，泛化较强。
- ViTucano（巴葡预训练VLM）在BLEU/METEOR/CIDEr等文本指标上优于更大的多语模型（如GPT‑4o、LLaMA 3.2 Vision）。
- GPT‑4系列在CLIP‑Score上最高，表明图文对齐更强，但传统文本指标不一定最优。
- 注意力揭示系统性偏差：性别误判、对象计数错误、空间关系不一致。

Conclusion: 使用母语人工标注数据能更好考察与提升巴葡IC模型；翻译数据存在域差异但仍具价值。Swin‑DistilBERTimbau是强基线，ViTucano在文本指标上具竞争力，而GPT‑4系在对齐度上占优。注意力分析暴露的偏差应通过数据与训练策略改进。作者开源了数据与模型以促进巴葡IC研究。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [58] [Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences](https://arxiv.org/abs/2602.00394)
*Manoj Reddy Bethi,Sai Rupa Jhade,Pravallika Yaganti,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 论文提出用成对比较替代直接评分来建模艺术审美偏好，结合ResNet-50特征，分别训练回归模型与双分支比较模型；比较学习在无评分情况下接近回归表现，标注效率更高，但个体偏好预测仍困难。


<details>
  <summary>Details</summary>
Motivation: 审美判断主观性强、个体差异大，且获取标注成本高。基于比较判断较评分更一致、负担更低，可降低标注成本并提升可扩展性。

Method: 从绘画图像提取ResNet-50深度特征；构建两类模型：(1) 深度神经网络回归预测平均审美评分；(2) 双分支成对比较模型，基于成对偏好学习相对偏好。围绕四个研究问题评估：与手工特征线性回归对比；在无直接评分时比较学习vs回归；个体评分者的内/跨评分者预测；直接评分与成对比较的标注成本权衡。

Result: 深度回归显著优于手工特征基线，R^2最高提升至328%；比较模型在未使用评分值情况下接近回归性能；个体偏好预测（内/跨评分者）明显低于平均评分预测；用户实验显示成对比较每项标注时间降低约60%。

Conclusion: 用成对比较可在缺乏评分的情形下实现接近回归的效果，并显著降低标注成本；但个体化偏好预测仍具挑战，需要进一步方法改进（如更多个体数据或个性化建模）。

Abstract: Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.

</details>


### [59] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: 提出3DGS^2-TR：一种针对3D Gaussian Splatting场景训练的二阶优化器，用Hutchinson近似得到对角Hessian，配合以Hellinger距离为基础的参数级信赖域，达到与ADAM同阶O(n)复杂度，却在相同初始化且无致密化下，用一半迭代数取得更好重建质量，并以<1GB额外显存开销实现良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 3DGS训练存在强非线性与大规模参数问题；现有二阶法（如3DGS-LM、3DGS2）需显式或稠密曲率，内存和算力昂贵；一阶法ADAM收敛慢。需要一种既保留二阶收敛优势、又具O(n)成本与稳定性的优化方法。

Method: - 用Hutchinson随机估计Hessian对角，获得曲率近似；
- 全矩阵无显式存储，计算与内存O(n)，与ADAM同级；
- 基于平方Hellinger距离构建参数级信赖域，限制高非线性栅格化下的更新步长；
- 训练流程与ADAM相似，但用对角二阶信息自适应缩放更新。

Result: 在相同初始化且不做densification条件下：
- 用比ADAM少约50%的迭代达到更高重建质量；
- 峰值GPU额外内存<1GB（比ADAM多17%，比3DGS-LM少85%）；
- 对超大场景可扩展，具潜在分布式训练适用性。

Conclusion: 3DGS^2-TR以轻量对角二阶信息与Hellinger信赖域，在保持O(n)代价的同时显著加速并稳定3DGS训练，相比ADAM效率与质量更优，相比传统二阶法显存与复杂度更低，适合大规模与分布式场景。

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [60] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: 该论文构建合成视觉安全数据、评测VLM在实验室安全识别上的局限，并提出以场景图对齐的后训练上下文工程方法提升“仅视觉”下的隐患检测。


<details>
  <summary>Details</summary>
Motivation: 现实实验室安全事件多以非结构化文本记录，缺乏成规模的、可用于视觉评测的图像与标注；现有VLM虽可多模态，但在复杂场景中难从像素提取结构化关系，影响安全监测可用性。

Method: 1) 数据生成管线：将文本化实验室情景经LLM转为场景图，再用图像生成模型渲染图像，形成(图像-场景图-真值)三元组；2) 基准评测：在1,207个样本、362个情景上评测多种开源/闭源VLM，分别在提供场景图与仅视觉输入两种条件下对比；3) 提出“场景图引导对齐”后训练上下文工程：把视觉输入先转译为结构化场景图，再与VLM推理对齐以提升隐患检测。

Result: VLM在获得文本场景图时表现良好；仅依赖视觉像素时性能显著下降，显示其难以直接抽取对象与关系。采用场景图引导对齐后，“仅视觉”条件下的隐患检测性能得到改善。

Conclusion: 缺乏结构化视觉数据限制了实验室安全监测研究；VLM对结构化关系高度依赖。通过将视觉感知转为场景图并与VLM推理对齐，可缓解感知-推理落差并提升安全隐患检测效果。

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [61] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出Text-DJ，一种利用OCR与分散干扰的越狱攻击：把有害请求拆成若干较温和子问题并混入无关干扰，将所有文本转成图像网格呈现，成功规避LVLM安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM的安全防护多聚焦于显式文本或相关视觉内容过滤，忽视了OCR通道与多图分散输入的组合可能被滥用以绕过审核。

Method: 三阶段：(1) 将单个有害查询语义分解为多个相对温和的子查询；(2) 选择与原有害意图最大不相关的干扰查询；(3) 将子查询与干扰查询一起以图像网格输入（子查询置于网格中部），通过把文本转换为图像调用OCR，并利用高比例无关项分散注意。

Result: 在多种最先进LVLM上实现稳定越狱，能够规避其安全对齐与文本过滤，对分散、多图、OCR驱动的输入表现出明显脆弱性。

Conclusion: LVLM存在对碎片化多模态输入与OCR通道的关键安全缺口；需设计能跨图像与文本聚合语义、识别分散攻击的防御机制。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [62] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: DISK 是一种无需再训练的自适应推理方法，通过跨模态跳步控制在视频与自车轨迹的两路扩散Transformer间协调，以保持运动-外观一致性，同时显著加速长时域推理且不损失精度与质量。


<details>
  <summary>Details</summary>
Motivation: 当前自回归世界模型在长时域视频与轨迹联合生成中计算代价高、累积误差易放大；现有跳步/加速策略往往需要再训练或破坏跨模态一致性，难以在闭环驾驶场景中稳定落地。

Method: 提出 DISK：训练免调整的自适应推理框架。核心是双分支控制器对视频扩散Transformer与轨迹扩散Transformer进行联动跳步（skip）决策，并引入跨模态的skip同步以维护运动-外观一致性；把高阶潜变量差分的跳测从单步扩展到自回归的“前向链”范式；在长滚动（rollout）循环中传播控制器统计量以维持长时稳定。

Result: 在 NuPlan 与 NuScenes 共1500个样本的闭环驾驶 rollout 上（NVIDIA L40S），轨迹扩散加速约2x、视频扩散加速约1.6x；同时保持规划 L2 误差、视觉质量（FID/FVD）与 NAVSIM PDMS 指标不劣化。

Conclusion: DISK 能在无需再训练的前提下降低视频与轨迹联合自回归生成的计算成本，并在真实闭环驾驶场景中实现长时稳定、质量不损失的推理加速，具备实用价值与可扩展性。

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [63] [Model Optimization for Multi-Camera 3D Detection and Tracking](https://arxiv.org/abs/2602.00450)
*Ethan Anderson,Justin Silva,Kyle Zheng,Sameer Pusegaonkar,Yizhou Wang,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 论文评测 Sparse4D 在室内多相机多目标跟踪场景的稳健性与效率：在降帧率、量化、跨数据集迁移与混合精度下的表现与取舍。提出 AvgTrackDur 指标衡量身份持续性。结论：中等降帧可接受，但 <2 FPS 会导致身份关联崩溃；选择性量化骨干/颈部最优，注意力层对低精度敏感；低 FPS 预训练对 WILDTRACK 零样本迁移收益大，小规模微调增益有限；Transformer Engine 混合精度降延迟、提扩展但可能破坏身份传播，需要稳定性验证。


<details>
  <summary>Details</summary>
Motivation: 室内多相机“外向内”感知需在遮挡与视角异质下实现稳定的多人3D检测与跟踪，实际部署常受算力与带宽限制（低帧率、低精度、跨场景迁移、多相机扩展）。作者希望系统性评估在这些工程约束下，基于查询与时空融合的 Sparse4D 的鲁棒性与精度-效率权衡，并弥补仅用常规检测/跟踪指标难以刻画身份稳定性的不足。

Method: 采用 Sparse4D：将多视角特征对齐到统一世界坐标，利用实例记忆传播稀疏目标查询以做3D检测+跟踪。实验维度包括：1) 输入帧率逐步降低；2) 训练后量化（INT8/FP8），并比较对不同模块（骨干、颈部、注意力相关层）的敏感性；3) 迁移到 WILDTRACK，比较低FPS预训练与小规模微调；4) 使用 Transformer Engine 混合精度微调评估延迟与可扩展性；5) 引入 AvgTrackDur 指标衡量身份保持时长。

Result: - 帧率：在中等降帧下性能基本稳定，但 <2 FPS 时即使检测仍稳，身份关联大幅崩溃（AvgTrackDur 显著下降）。- 量化：选择性量化骨干/颈部获得最佳速度-精度折中；注意力模块对低精度持续敏感。- 迁移：在 WILDTRACK 上，低FPS预训练带来显著零样本提升；小规模微调额外收益有限。- 混合精度：Transformer Engine 降低延迟、改善相机可扩展性，但会削弱身份传播稳定性。

Conclusion: Sparse4D 在实际部署相关约束下总体稳健，但身份保持对帧率与数值精度高度敏感。工程上应：保持 ≥2 FPS；对骨干/颈部做选择性量化，避免/谨慎量化注意力；优先通过低FPS预训练做跨域迁移，小样本微调收益可能不大；采用混合精度时需引入以身份稳定性为核心的验证与监控（如 AvgTrackDur）。

Abstract: Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.

</details>


### [64] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: 提出LatentLens，通过与大规模语料的上下文化文本表征进行近邻匹配，把VLM中视觉token的潜在表示转化为可读自然语言描述；实证表明视觉token在多数模型与层中是可解释的，现有如LogitLens方法低估了解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管用浅层MLP即可把视觉特征映射到LLM嵌入并形成VLM，但我们并不清楚LLM各层对视觉token编码了什么，需要新的可解释性工具揭示视觉-语言表征对齐与层内语义。

Method: 构建LatentLens：先对大规模文本语料进行编码，保存每个token在上下文中的表示；将VLM中的视觉token表示与这些文本表示做相似度检索，取top-k近邻文本表示作为视觉token的自然语言描述；在10个VLM上与LogitLens等方法比较。

Result: 在所有被测VLM与各层中，大多数视觉token都能通过LatentLens获得语义一致、细粒度的自然语言描述；相比之下，LogitLens显著低估了解释性；定性结果显示LatentLens描述更细致、对人更有意义。

Conclusion: 视觉与语言潜表示具有更强的一致性与可解释性；LatentLens为分析VLM潜表征提供新工具与证据，提示未来可基于这种对齐改进模型分析与训练。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [65] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

TL;DR: 提出PSGS，两阶段从文本生成高保真全景与3D场景：先语义布局+MLLM自优化生成一致全景，再以滑动全景初始化全局一致的3D Gaussian Splatting点云，并加深度与语义损失提升细节与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动3D生成受限于稀缺的3D-文本数据与多视角拼接不一致，导致场景过于简单、细节不足，难以满足VR/AR/游戏等沉浸式应用对高保真与全局一致性的要求。

Method: 两阶段框架PSGS：1) 全景生成阶段采用两层优化架构——布局推理层将文本解析为结构化空间关系，指导全景语义一致；自优化层通过MLLM迭代反馈细化视觉细节。2) 3D重建阶段提出“全景滑动”机制，从全景中策略性采样重叠视角以初始化全局一致的3D Gaussian Splatting点云；训练时引入深度损失与语义一致性损失，提升几何与语义对齐。

Result: 实验显示PSGS在全景生成质量与3D场景吸引力上优于现有方法，渲染细节、更高的语义连贯性与全局一致性得到明显提升。

Conclusion: PSGS提供了可扩展的高保真实景生成方案，以文本到全景到3D的链路实现更稳定的全局一致性与细节逼真度，适合沉浸式内容生产。

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [66] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

TL;DR: 提出ZS-TreeSeg零样本树冠实例分割框架，结合Canopy语义分割与细胞实例分割思想（Cellpose+SAM），用拓扑流场与星凸假设强制分离粘连树冠；在NEON与BAMFOREST上具备跨传感器与密度的鲁棒泛化，无需训练，可用于自动标注。


<details>
  <summary>Details</summary>
Motivation: 密集、重叠树冠难以精确分割；监督深度学习标注成本高、泛化差；通用基础模型（如SAM）缺乏林业领域知识，易在密集区欠分割。需要一种无需训练、可泛化且能处理粘连树冠的方案。

Method: 提出ZS-TreeSeg：从两个成熟任务迁移—(1)树冠/冠层语义分割提供前景区域；(2)细胞实例分割（Cellpose）提供基于拓扑流场的星凸对象建模与向量收敛分离；并与SAM结合形成“Cellpose-SAM”，在流场中对接触树冠进行数学上的实例分离，实现零样本推理与标签生成。

Result: 在NEON与BAMFOREST数据集以及可视化评估中，框架在不同传感器类型与冠层密度下表现出稳健的跨域泛化，显著缓解密集区域的欠分割；无需额外训练即可产出实例分割与用于监督的标注。

Conclusion: ZS-TreeSeg通过将树冠建模为星凸对象并利用拓扑流场实现接触实例的强制分离，弥补了基础模型的领域知识缺失，提供了训练免费的树冠实例分割与标签生成方案，具有良好的跨数据集与传感器的泛化能力。

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [67] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出GTATrack用于鱼眼相机下足球MOT，结合Deep-EIoU在线关联与GTA全局轨迹关联，并用伪标签提升小目标检出，在SoccerTrack 2025夺冠，HOTA=0.60、FP=982。


<details>
  <summary>Details</summary>
Motivation: 体育场景下MOT受不规则运动、服装相似、频繁遮挡影响；鱼眼静态相机又带来几何畸变与极端尺度变化，现有方法难以兼顾短期鲁棒匹配与长期ID一致性。

Method: 分层两阶段框架：1) Deep-EIoU用于与运动无关的在线局部关联，提升短期匹配稳健性；2) GTA进行全局轨迹级关联，进行长期身份一致性与轨迹修复；3) 伪标签策略提升检测器对小且畸变目标的召回。

Result: 在SoccerTrack Challenge 2025中获得第一，HOTA=0.60，显著降低误检至982，表现出SOTA的鱼眼足球跟踪精度与稳定性。

Conclusion: 局部在线关联与全局轨迹推理的协同，结合提升召回的伪标签策略，有效缓解ID切换、遮挡与碎片化问题，在鱼眼足球跟踪中取得领先表现。

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [68] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

TL;DR: 提出SketchMod，通过学习并应用缩放、旋转与平移三类偏移属性，对源笔画进行变换以与目标素描的模式对齐，从而实现更精确、灵活的笔画级编辑。


<details>
  <summary>Details</summary>
Motivation: 仅靠重新定位源笔画难以保证与目标素描的语义一致与视觉保真，尤其当源笔画的尺寸与方向差异较大时会产生不合理的编辑结果（如过大的笔画未缩放导致语义失真）。

Method: 以目标素描的模式为约束，从源笔画到目标笔画学习三种关键偏移属性：尺度（scale）、方向（orientation）和位置（position）。随后对源笔画执行：1）按比例缩放匹配空间比例；2）旋转对齐局部几何；3）平移以符合语义布局；并暴露笔画属性以精细控制笔画轮廓。

Result: 实验表明，SketchMod在笔画级素描编辑任务上实现了精确且灵活的效果，相较仅重定位的方法更能保持语义一致与视觉保真。

Conclusion: 通过同时学习并应用尺度、方向、位置三种偏移属性，SketchMod能有效对齐源笔画与目标素描的模式，实现高质量的笔画级编辑与可控的笔画轮廓调整。

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [69] [HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion](https://arxiv.org/abs/2602.00490)
*Chia-Ming Lee,Yu-Hao Ho,Yu-Fan Lin,Jen-Wei Lee,Li-Wei Kang,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出HSSDCT，通过分层稠密残差Transformer与空间-光谱相关层，实现高效高质的HSI融合，线性复杂度、低冗余、SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有HSI融合方法受限于：卷积/局部注意力感受野不足、光谱冗余未充分抑制、自注意力二次复杂度高，导致效率与鲁棒性欠佳。

Method: 构建HSSDCT框架，包含两大模块：1) HDRTB：逐级扩大全局/局部窗口，采用稠密-残差连接以汇聚多尺度特征；2) SSCL：显式因子化空间与光谱依赖，将自注意力从二次降至线性复杂度，并缓解光谱冗余。整体以LR-HSI与HR-MSI为输入，联合重建HR-HSI。

Result: 在多基准数据集上取得更优重建质量且计算开销显著降低，达到新的SOTA表现。

Conclusion: 分层多尺度聚合与空间-光谱因子化注意力能同时提升精度与效率，适合高效稳健的HSI融合；代码已开源。

Abstract: Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.

</details>


### [70] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出RGBX-R1框架，通过“理解-关联-验证”(UAV)提示构建视觉模态链式思维（VM-CoT），并采用两阶段训练（CS-SFT与ST-RFT）提升MLLM在非RGB模态（红外、深度、事件）的感知与推理；还构建首个RGBX-Grounding基准，显著超越基线22.71%。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要在RGB上预训练，导致面对红外、深度、事件等关键模态时能力不足，限制复杂场景下的多模态理解与定位能力，缺乏统一的跨模态推理机制与评测基准。

Method: 1) UAV提示策略：引导模型先理解RGB语义，再关联到目标X模态，并通过验证步骤形成VM-CoT以迁移与对齐跨模态推理。2) 两阶段训练：CS-SFT利用VM-CoT进行冷启动监督微调，灌输基本模态认知；在此基础上，基于GRPO的ST-RFT提出MuST奖励（兼顾模态理解与时空一致性）进行强化微调，逐步提升复杂时空推理与定位能力。3) 构建RGBX-Grounding基准用于评测。

Result: 在三个RGBX grounding任务上，方法在多模态理解与空间感知方面优于现有基线，综合提升22.71%。

Conclusion: RGBX-R1通过VM-CoT与两阶段微调（含MuST奖励）有效将RGB能力迁移到X模态，显著增强MLLM的跨模态感知与时空推理，并提供首个RGBX-Grounding基准作为评测与研究推动。

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [71] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 提出SparseCut：在MLLM中通过稀疏快捷连接与多粒度视觉特征融合，分层注入视觉信息到LLM，不增加LLM计算量，显著提升多模态基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多依赖放大LLM或更好数据，跨模态对齐常仅用高层视觉特征，忽视中低层语义，导致跨模态理解不足与信息损失。需要一种既保留多层视觉语义、又不增加LLM推理成本的融合方式。

Method: 设计SparseCut：在跨模态编码器与LLM之间建立稀疏快捷连接，将多层级视觉表示以选择性、分层方式注入LLM；并在注入前引入高效的多粒度特征融合模块，对视觉特征进行融合压缩，保留语言上下文且不增加序列长度，从而不提高LLM计算复杂度。

Result: 在多种多模态基准上显著优于现有方法；该框架对不同底座LLM具有通用性与可扩展性，同时保持计算开销稳定。

Conclusion: 稀疏快捷连接+多粒度融合可更充分利用不同层级视觉语义，在不增加LLM负担的前提下提升跨模态理解与生成能力；SparseCut为MLLM跨模态融合提供通用高效范式。

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [72] [DuoGen: Towards General Purpose Interleaved Multimodal Generation](https://arxiv.org/abs/2602.00508)
*Min Shi,Xiaohui Zeng,Jiannan Huang,Yin Cui,Francesco Ferroni,Jialuo Li,Shubham Pachori,Zhaoshuo Li,Yogesh Balaji,Haoxiang Wang,Tsung-Yi Lin,Xiao Fu,Yue Zhao,Chieh-Yun Chen,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: DuoGen 提出一个通用的“文本-图像交错生成”框架，通过大规模高质指令数据、解耦架构（MLLM + DiT）与两阶段对齐训练，大幅提升在多种基准上的文本质量、图像保真度与文图对齐度，并在统一生成模型中达到SOTA的图生图/文生图与编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有交错多模态生成在通用指令下表现受限，主要因：1）高质量训练数据不足；2）基础模型能力有限或预训练成本高。需要一个能够系统解决数据、架构与评测的方案，以解锁如分步指南、视觉规划、推理草图等能力。

Method: 数据：构建大规模高质量指令微调集，来源于精选网站对话重写与覆盖日常场景的多样合成样本。架构：采用预训练多模态LLM（强视觉理解）+ 预训练于视频生成的扩散Transformer（DiT，强视觉生成），避免昂贵的单模态预训练并支持灵活底座选择。训练：两阶段解耦——先对MLLM做指令微调，再用精心整理的交错图文序列对DiT与MLLM进行对齐，以实现流畅的交错生成。

Result: 在公共与新提出的基准上，DuoGen在文本质量、图像保真度与图文对齐上均优于既有开源模型；同时在统一生成设置下于文生图与图像编辑任务上达成SOTA。

Conclusion: 通过系统性的数据策划、解耦式架构与两阶段对齐训练，DuoGen显著提升交错多模态生成的通用性与质量，并且具备可扩展与替换底座的灵活性；数据与代码将开源。

Abstract: Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.

</details>


### [73] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出一种将无训练分割重构为扩散亲和图上的随机流平衡问题的新方法，用马尔可夫随机游走与自适应剪枝进行标签传播，克服谱聚类式方法需预设簇数、边界过平滑与对噪声敏感等缺陷，并在7个基准上零样本取得SOTA，边界更锐利、区域更一致、掩膜更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散亲和的训练免分割多依赖谱图划分/特征向量分解，存在：1) 需预先设定簇数；2) 谱松弛导致边界过平滑；3) 对噪声、多模态亲和分布敏感；4) 忽视局部邻域结构对稳定传播和细节轮廓的重要性。为解决这些结构性局限，作者寻求一种兼顾全局扩散注意与局部邻域的稳健机制。

Method: 把分割表述为扩散诱导亲和图上的随机流平衡：构建稀疏而表达力强的亲和结构，将全局扩散注意与来自Stable Diffusion的局部邻域融合；在此基础上提出马尔可夫传播方案，进行随机游走式标签扩散，并配自适应剪枝，抑制不可靠转移、强化高置信路径。

Result: 在7个常用语义分割基准上做零样本评估，相比谱聚类范式取得SOTA：边界更锐利、区域更连贯、掩膜稳定性显著提升。

Conclusion: 将训练免分割从谱划分转向随机流平衡与马尔可夫传播，可避免预设簇数与谱松弛问题，并用局部邻域稳健化扩散亲和，带来更精细、稳定的零样本分割表现。

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [74] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: MRAD提出用“记忆-检索”替代复杂拟合：冻结CLIP构建图像/像素两级记忆库，推理时直接相似度检索得分；再以轻量微调和文本偏置增强泛化，跨16个工业与医疗数据集在分类与分割上均优。


<details>
  <summary>Details</summary>
Motivation: 现有ZSAD多依赖提示学习或复杂分布建模，训练/推理成本高、跨域稳定性差。作者希望避免参数化拟合，直接利用原始数据的经验分布以提升零样本异常检测的效率与泛化。

Method: 提出MRAD统一框架：1) MRAD-TF（train-free）：冻结CLIP图像编码器，用辅助数据构建两级记忆库，显式存储特征-标签键值对；推理以记忆检索相似度给出异常分数。2) MRAD-FT：在检索度量上加两层线性微调以放大正常/异常可分性。3) MRAD-CLIP：将MRAD-FT得到的正常/异常区域先验作为动态偏置注入CLIP可学习文本提示，增强对未见类别的泛化。

Result: 在16个工业与医疗数据集上，于零训练与训练增强两种设置下，MRAD在异常分类与分割指标上均优于现有方法。

Conclusion: 直接利用数据经验分布的记忆检索可替代复杂模型拟合，以更低成本、更强跨域泛化实现更优的零样本异常检测表现；代码将开源。

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [75] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

TL;DR: SAGE 动态调整推测解码的树结构，用输出熵作为置信度信号，高置信走深、低置信走宽，从而提高接受长度并加速 VLM 解码，且不损质量，在多基准上对 72B 级模型实现最高约 3.2–3.4× 提速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码使用固定树形结构，无法适应不同时间步的预测难度，导致接受长度不足、加速有限。需要一种能随时步不确定性自适应地分配“深度 vs. 广度”的机制。

Method: 提出 SAGE：以输出熵作为实时置信度指标，利用其跨步的时间相关性，动态构建推测树。对高置信（低熵）步采用“更深更窄”的树以最大化深度；对低置信（高熵）步采用“更浅更宽”的树以增加分支多样性。在线调整树形以提高一次性验收的 token 数。

Result: 在多项基准上，在不降低输出质量的前提下，相比静态树基线显著提高接受长度并获得更高加速：LLaVA-OneVision-72B 最高 3.36×、Qwen2.5-VL-72B 最高 3.18×。

Conclusion: 动态、熵驱动的推测树优于静态结构，可在保持质量的同时显著加速 VLM 解码；输出熵是有效且可泛化的置信度信号。

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [76] [Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment](https://arxiv.org/abs/2602.00531)
*Tianyi Zhang,Antoine Simoulin,Kai Li,Sana Lakdawala,Shiqing Yu,Arpit Mittal,Hongyu Fu,Yu Lin*

Main category: cs.CV

TL;DR: 提出VLDet，一个重塑特征金字塔、加强视觉-语言对齐的开放词汇目标检测框架；通过VL-PUB适配CLIP到检测、多尺度对齐，并以SigRPN的sigmoid式锚框-文本对比损失提升新类检测；在COCO新类AP 58.7、LVIS AP 24.8，显著超越SOTA，并在封闭集零样本检测上也更优。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测要么难以把CLIP等单尺度图像骨干迁移到多尺度检测框架，要么视觉-语言对齐不稳健，导致对新类别的检测效果受限。需要一种能在多尺度上充分利用视觉-语言知识并稳定对齐的检测方法。

Method: 提出VLDet：1) 设计VL-PUB模块，将CLIP视觉特征与检测的特征金字塔融合，在多尺度上进行细粒度视觉-语言对齐并适配检测骨干；2) 引入SigRPN模块，用基于sigmoid的锚框-文本对比对齐损失，在区域生成阶段强化类别无关但与文本可对齐的表示，从而更好检测新类别。

Result: 在COCO2017新类别上取得58.7 AP，在LVIS上取得24.8 AP，分别较现有方法提升约27.6%与6.9%；同时在封闭集设定下的零样本检测也优于既有方法。

Conclusion: 多尺度的视觉-语言对齐与对比式RPN联合带来显著的开放词汇检测收益。VLDet有效适配CLIP至检测框架，强化新类别识别，并在多数据集与设定下均取得领先性能。

Abstract: Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.

</details>


### [77] [SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal](https://arxiv.org/abs/2602.00536)
*Yifan Zhang,Qian Chen,Yi Liu,Wengen Li,Jihong Guan*

Main category: cs.CV

TL;DR: SADER是一种结构感知的多时相遥感云去除扩散框架，结合多时相条件扩散网络、云感知注意力损失与确定性重采样策略，在多数据集上全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式云去除方法采样效率低，且在多时相场景下对结构先验与时序先验利用不足，导致云覆盖严重时恢复质量不稳定。

Method: (1) 提出可扩展的多时相条件扩散网络MTCDN，通过时序融合与混合注意力同时挖掘多时相与多模态关联；(2) 设计云感知注意力损失，依据云厚度与亮度差异突出云主导区域；(3) 为连续扩散模型提出确定性重采样，在固定步数下迭代细化样本，用引导纠正替换异常值。

Result: 在多个多时相数据集上，SADER在所有评价指标上均优于现有最先进云去除方法，展示了更好的重建质量与稳定性。

Conclusion: 结构与时序先验的显式建模、结合云区域加权与确定性重采样，可显著提升多时相遥感云去除的性能与采样效率；代码已开源，具有实际应用潜力。

Abstract: Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.

</details>


### [78] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: NPNet 是一种完全非参数的 3D 点云分类与部件分割方法，不含可学习权重，依靠确定性算子与自适应高斯-傅里叶位置编码，在多数据集上对非参数基线取得领先，尤其在小样本下表现突出，并具备更优的内存与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有点云方法多依赖大量可学习参数，易受尺度、采样密度变化影响，并在小样本场景泛化受限；非参数方法虽具效率与稳健潜力，但表达力和跨尺度稳定性不足。作者希望设计无需训练权重、仍能在多尺度与不同采样密度下稳定且高效的点云表示方法。

Method: 构建完全非参数的 NPNet：使用最远点采样、k 近邻与池化等确定性算子构造点特征；核心为自适应高斯-傅里叶位置编码，其带宽与高斯-余弦混合比例由输入几何自适应确定，从而跨尺度、跨密度稳定。分割任务中额外加入固定频率的傅里叶特征以提供全局上下文。

Result: 在 ModelNet40/ModelNet-R、ScanObjectNN 与 ShapeNetPart 上，相较非参数基线取得强竞争力；在 ModelNet40 的小样本设置下尤为出色；在内存占用与推理速度上优于以往非参数方法。

Conclusion: 无需学习权重也能通过自适应位置编码与确定性几何算子实现强大的点云分类与分割性能；该框架在小样本与资源受限场景具备实际优势，并展示了非参数点云建模的可行性与效率潜力。

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [79] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: 提出OmniVCHall基准系统性评测视频多模态大模型在孤立与组合型幻觉上的能力，并提出TriCD三路径对比解码框架，通过自适应扰动与显著性增强（RL优化）缓解组合型幻觉，在多骨干上平均提升10%+。


<details>
  <summary>Details</summary>
Motivation: 现有视频幻觉研究多聚焦单一错误类型，忽视由多空间-时间因素交互导致的组合型幻觉；缺乏覆盖多领域、细粒度分类与能防止捷径推理的标准化评测基准。

Method: 1) 构建OmniVCHall：覆盖多视频域，提出相机相关幻觉类型与细粒度分类；设计带“全都正确/以上皆非”等对抗选项的选择题以抑制投机。2) TriCD：对比式解码框架，包含三路径校准：a) 自适应扰动控制器选择干扰操作生成负向视频变体；b) 显著性引导的增强模块自适应强化与答案相关的逐token视觉证据；c) 结合多路径对比校准；整体以强化学习优化。

Result: 在39个代表性VLLM上评测显示即便先进模型（如Qwen3-VL、GPT-5）在OmniVCHall上显著退化。TriCD在两类代表性骨干上稳定提升，平均准确率提高超过10%。

Conclusion: 组合型视频幻觉是当前VLLM的关键短板。OmniVCHall提供系统化评测基准，TriCD提供有效缓解方案，显著提升对复杂时空因素下的稳健推理能力。

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [80] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: GLAD提出用扩散模型进行文本与模板图像的生成式融合，提升低语义/模糊场景下的跨模态跟踪效果，并在多基准上达SOTA且推理高效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言跟踪在低语义图像（模糊、低分辨等）下，跨模态理解受损；直接拼接/融合文本与视觉特征因模态鸿沟较大效果有限，需更强的语言辅助与更兼容的融合机制。

Method: 提出GLAD：以扩散模型为核心的生成式多模态融合框架，将文本描述与模板图像在生成过程里深度耦合，恢复/增强模板语义信息，输出更兼容的多模态特征以供Transformer式跟踪模块使用；强调对模糊与语义含糊模板的恢复与特征对齐。

Result: 在多个基准数据集上取得新的SOTA性能，同时保持令人印象深刻的推理速度，与传统直接融合范式相比有显著提升。

Conclusion: 生成式（扩散）语言辅助融合能有效弥合文图鸿沟、增强模板语义，从而显著提升视觉-语言跟踪的鲁棒性与精度；GLAD验证了该范式的有效性并具实用效率。

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [81] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出BDG框架，通过把退化判别与图像生成/复原桥接起来，实现通用图像复原；核心是MAS-GLCM用于细粒度退化识别，并在扩散模型中引入三阶段训练（生成-桥接-复原），在不改网络架构下显著提升多任务与真实超分的保真度且不牺牲感知质量。


<details>
  <summary>Details</summary>
Motivation: 通用图像复原需同时应对多种退化与等级差异，难点在于：1）高质量图像分布的有效采样；2）输出需依据具体退化自适应调整。现有扩散/生成方法注重纹理还原但对退化类型敏感性不足，判别式方法又难以兼顾细节与泛化。因此需要一种将退化判别信息与生成复原能力协同利用的机制。

Method: 1）提出MAS-GLCM：在多角度与多尺度上统计灰度共生矩阵，作为精细退化类型与强度的判别特征。2）扩散训练三阶段：a) Generation阶段保留基础纹理生成/复原能力；b) Bridging阶段将MAS-GLCM的判别信息嵌入扩散过程，学习从退化特征到生成先验的对齐；c) Restoration阶段在多任务/多退化场景下利用上述对齐进行条件复原。3）不改原有网络架构，仅通过训练流程与条件注入实现。

Result: 在all-in-one复原与真实场景超分任务上，相比基线显著提升保真度（PSNR等）且维持/不损伤感知质量（LPIPS/主观细节），显示出更强的多任务、多退化适应能力。代码与预训练模型已开源。

Conclusion: 将退化判别（MAS-GLCM）与扩散生成通过三阶段训练有效耦合，既保留细节纹理复原能力，又实现对多类退化的准确感知与条件化复原；方法通用、无需改架构，适合推广到通用图像复原与真实超分等场景。

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [82] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: 提出MAUGen，一个基于扩散的多模态框架，可从单条文本提示同时生成高保真面孔表情图像与解剖一致的AU出现与强度标签，并构建了含多身份的大规模合成数据集MIFA。实验显示其在图像逼真度、人口多样性与语义对齐标签方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AU识别泛化受制于缺乏大规模、人口统计学多样且带精确AU出现与强度标注的人脸数据。现有数据昂贵、标注困难且群体代表性不足，限制了模型的性能与公平性。需要一种既能生成高质量表情图像又能提供可靠AU标签的方式。

Method: 提出MAUGen，含两大模块：1) 多模态表示学习(MRL)，在统一潜在空间建模文本描述、身份、表情图像与AU激活的关系；2) 扩散式图像-标签生成器(DIG)，将联合表示解码为对齐的人脸图像与AU标签（出现与强度），覆盖多样身份。基于该框架构建MIFA多模态合成数据集。

Result: 在合成质量、人口多样性与文本—图像—AU标签的语义对齐方面优于现有方法；可生成大规模、含多身份且带精细AU标签的样本。

Conclusion: MAUGen能从文本条件联合生成逼真面孔与一致的AU标签，缓解真实数据稀缺与偏差问题；MIFA为AU识别提供可扩展的训练资源，促进更具泛化性与公平性的AU识别系统。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [83] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

TL;DR: Pix2Fact 是一个面向精细视觉定位与知识密集型多跳推理的全新VQA基准，含4K+高清图像与专家标注问题；现有顶尖VLM在其上表现低迷（最高24%），与人类56%存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准往往将视觉感知与知识推理分开评测，无法检验两者协同能力；而真实任务需要细粒度感知与外部知识的多跳推理联合。作者希望提供一个能系统检验这种“感知+知识”融合能力的评测集。

Method: 构建Pix2Fact数据集：1,000张4K+高分辨率图像，覆盖8类日常场景；由具有博士背景的标注专家与专业标注团队共同设计问答，确保每题同时要求精细视觉定位、多跳推理与外部知识整合。随后对9个SOTA VLM（含专有如Gemini-3-Pro、GPT-5）进行系统评测。

Result: 在Pix2Fact上，最强模型的平均准确率仅24.0%，显著低于人类水平56%，显示出现有模型在该任务上的明显短板。

Conclusion: Pix2Fact揭示了VLM在“细粒度视觉+知识密集推理”上的显著能力缺口，可作为推动下一代多模态智能体（兼具精细感知与稳健知识推理）发展的关键基准。

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [84] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: 提出Tune-Your-Style：一种可调强度的3D风格迁移方法，基于3D Gaussian Splatting，引入可学习风格调谐器与“高斯神经元”显式建模风格强度，并结合跨视角一致的扩散式风格引导与两阶段优化，实现从零到强风格的连续可控、视角一致的3D风格化。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS风格化速度快但输出强度固定，难以满足用户对内容保持与风格注入之间不同偏好的可控需求；缺乏跨视角一致且可调节强度的稳定风格引导机制。

Method: 1) 引入“高斯神经元”作为风格强度的显式参数，并设计可学习的style tuner实现强度可调的风格注入；2) 提出可调风格化引导：通过跨视角风格对齐从扩散模型获取多视角一致的风格化视图；3) 两阶段优化，将全风格引导（来自风格化视图）与零风格引导（来自初始渲染）按强度进行动态加权，稳定且高效地训练3DGS。

Result: 在多组实验中实现了高保真、视角一致且风格强度可连续调节的3D风格化；较现有方法在视觉质量与交互可定制性上更优，训练与渲染保持3DGS的高效性。

Conclusion: Tune-Your-Style实现了用户可控的3D风格强度调节，兼顾内容保持与风格表现，并通过跨视角一致的扩散引导与两阶段优化提高了稳定性与效率，为3DGS风格迁移提供了更灵活的范式。

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [85] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 论文通过在表示层面引入稀疏自编码器（SAE）分解视觉嵌入，发现“总是激活”与“图像特异”两类神经元；幻觉多源自后者的扰动/伪激活。基于此提出对比神经引导（CNS），在前填充阶段对图像特异神经元做选择性放大/抑制，使表征更稳健、语义更落地，兼容解码阶段方法，并在多基准上降低幻觉且不损整体能力。


<details>
  <summary>Details</summary>
Motivation: 现有缓解多模态幻觉的方法多在输出层做后处理，缺乏对内部机制的可解释理解与可控干预，难以系统提升鲁棒性与可视基线对齐。

Method: 1) 用SAE将密集视觉嵌入分解为稀疏、可解释神经元；神经元级别分析并分类为“总是激活”和“图像特异”。2) 观察并验证幻觉与图像特异神经元的扰动/伪激活相关。3) 提出CNS：通过干净-噪声输入的对比分析定位图像特异神经元，选择性放大信息性激活、抑制扰动诱发激活；在前填充阶段实施。

Result: - 发现：总是激活神经元稳定，幻觉主要由图像特异神经元异常驱动。
- 介入：对图像特异神经元的增强/抑制可显著改善视觉对齐与减少幻觉。
- 实证：在幻觉专项与通用多模态基准上，CNS稳定降低幻觉，同时保持/不损多模态理解性能；与解码阶段方法可叠加。

Conclusion: 表示层面的可解释分解与有针对性的神经元操控能有效缓解LVLM幻觉。CNS在前填充阶段通过对比定位并引导图像特异神经元，改善视觉语义落地和鲁棒性，且与现有解码策略兼容，具有通用、可组合的实用价值。

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [86] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: FaceSnap是一种基于Stable Diffusion的单图像个性化人像生成方法：无需微调、单步推理即可在多姿态下保持极高身份一致性与细节保真，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有个性化人像生成方法要么需要耗时微调、泛化差，要么无法兼顾人脸细节与身份一致性，尤其在不同姿态与视角下易失真。

Method: 在SD框架上提出即插即用的单参考图方案：1）Facial Attribute Mixer融合低层具体特征与高层抽象语义以提供更全面的生成引导；2）Landmark Predictor在多姿态下预测关键点以维持身份一致并提供细粒度空间控制；3）ID-preserving模块将上述条件注入UNet，单阶段推理实现个性化生成。

Result: 实验显示在个性化与定制人像生成上，FaceSnap在身份一致性与面部细节保真度方面优于现有SOTA，并实现单张参考、单次推理的高稳定输出。

Conclusion: FaceSnap通过特征融合与关键点引导，在无需微调的前提下实现高保真、高一致的人像生成，并具备良好可扩展性与即插即用特性，可广泛适配不同SD模型。

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [87] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

TL;DR: 提出S^3POT：结合人脸生成与自监督空间提示，实现无标注遮挡分割，显著提升在真实遮挡下的人脸解析。


<details>
  <summary>Details</summary>
Motivation: 遮挡是高层概念，难以穷尽其类别且精确标注代价高，现有人脸解析易将遮挡误判为面部组件。需要无需遮挡真值标注、能泛化多样遮挡的分割方法。

Method: 提出对比驱动的S^3POT框架，包含三模块：1) 参考生成（RF）：利用人脸生成器在已解析结构引导下重建消除遮挡的参考图；2) 特征增强（FE）：对原图与参考图token做对比获取初始提示，并通过跨注意力用提示调制图像特征；3) 提示选择（PS）：基于增强特征构建正/负提示集，经自注意力筛选供掩码解码器使用。整体以三种互补目标函数自监督训练，无需遮挡真值。

Result: 在专门采集的数据集上进行大量实验，S^3POT在遮挡分割与人脸解析上优于现有方法，各模块有效。

Conclusion: 通过把生成式参考与自监督提示结合，S^3POT能在无遮挡标注的情况下准确区分遮挡与面部区域，具备强泛化与实用价值。

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [88] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

TL;DR: 提出VIZOR：一个无需训练、端到端、从原始3D场景直接生成稠密且视角不变的3D场景图的方法，基于对象自身正前方向定义空间关系，并支持开放词汇零样本关系推断，在场景图生成与下游零样本指代定位上优于SOTA（Replica +22%，Nr3D +4.81%）。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解依赖多模态输入与人工标注关系，且“左/右”等关系随视角变化而不一致，泛化性差，导致跨视角推理不稳健。

Method: 设计VIZOR框架：在无训练条件下，直接从原始3D场景构建稠密场景图；以每个物体的前向朝向为参照定义空间与接近关系，实现视角不变；采用开放词汇关系推断，无需标注数据，端到端输出场景图。

Result: 在场景图生成和下游基于查询的目标定位任务上进行广泛定量与定性评估，较SOTA取得显著提升：零样本定位在Replica提升22%，在Nr3D提升4.81%。

Conclusion: VIZOR能稳定、清晰地表达跨视角一致的3D空间关系，支持零样本开放词汇推断，显著提升场景图生成质量与下游指代定位性能。

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [89] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: Diff-PC 是一个零样本的人像定制扩散框架，利用3D面部先验与新设计的ID特征编码、控制与注入模块，实现高身份保真、可控表情/姿态与多样背景，并在自建以身份为核心的数据集上训练，超过现有方法的ID保持、面部控制与文生图一致性表现。


<details>
  <summary>Details</summary>
Motivation: 现有的人像定制方法在身份保真与面部控制（表情、姿态）方面不足，且难以同时兼顾文本指令一致性与风格/背景多样性。作者旨在建立一种无需额外微调（零样本）即可高精度保留身份并灵活控制面部属性的通用框架。

Method: 1) 使用3D人脸预测器重建包含参考身份、目标表情与姿态的3D感知面部先验；2) 设计ID-Encoder融合局部与全局人脸特征以捕捉细粒度细节；3) 提出ID-Ctrl利用3D先验对ID特征对齐和引导；4) 引入ID-Injector在扩散过程注入并强化ID特征以提升身份保真与可控性；5) 在自建的以身份为中心的数据集上训练以增强人脸相似度与文生图对齐；6) 与多种多风格基础模型兼容。

Result: 在ID保真、面部控制能力（表情/姿态）与文本一致性上优于SOTA；生成真实感强、背景多样；可与多风格基础模型协同工作。

Conclusion: Diff-PC通过3D面部先验与ID特征的编码-控制-注入范式，实现零样本人像定制中身份保持与面部可控性的兼顾，并在多项指标与主观评测上超越现有方法，具有良好的通用性与兼容性。

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [90] [A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2602.00650)
*Mohammadreza Gholipour Shahraki,Mehdi Rezaeian,Mohammad Ghasemzadeh*

Main category: cs.CV

TL;DR: 提出Mamba-SAM：用冻结SAM编码器结合Mamba型SSM实现高效3D医学图像分割，在ACDC心脏MRI上达SOTA级精度与速度折中。


<details>
  <summary>Details</summary>
Motivation: 通用大模型SAM在医学分割上受域偏移、2D设计与微调代价高的限制；3D MRI/CT需要长程依赖建模与高效推理，亟需参数高效且能处理体数据的方案。

Method: 提出两种参数高效适配策略：1) 双分支：冻结SAM ViT提取通用特征，训练VMamba编码器学习医学域特征，通过跨注意力融合；2) 适配器：在冻结SAM ViT中注入轻量3D感知的Tri-Plane Mamba (TPMamba)模块以隐式建模体上下文。并引入Multi-Frequency Gated Convolution (MFGC)，结合3D离散余弦变换与自适应门控，联合利用空间与频域信息。

Result: 在ACDC数据集上，Mamba-SAM-Base平均Dice 0.906，接近UNet++(0.907)，且在心肌(0.910)与左心室(0.971)上优于所有基线；适配器TP MFGC变体推理速度4.77 FPS且Dice 0.880，精度与速度兼优。

Conclusion: 将冻结的基础模型与高效SSM混合，并通过3D感知适配/频域增强，可在3D医学分割中实现强精度与高效率，是实用有效的范式。

Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.

</details>


### [91] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: 提出NOVA：一种非对比的视觉-语言对齐方法，用联合嵌入预测和分布正则化取代对比学习；在胸片零样本分类上优于多种基线且训练更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有如CLIP的对比学习依赖大批量、复杂负样本与繁琐调参，训练不稳定且成本高，尤其在医学等特定领域受限。作者希望在保持/提升性能的同时，降低对比式框架的复杂性与不稳定性。

Method: 固定领域文本编码器（如ClinicalBERT），训练视觉编码器从增强后的图像视角直接预测对应文本嵌入（联合嵌入预测），并通过“草图化各向同性高斯正则化”（SIGReg）将视觉嵌入约束为各向同性高斯结构。无需负样本、动量编码器或stop-gradient，整体目标函数仅余一个超参数。

Result: 在MIMIC-CXR上从零训练ViT并与固定的ClinicalBERT对齐，进行跨三套基准的胸片零样本分类评测。NOVA在零样本分类准确性上超过多种标准基线，且不同训练运行间方差显著更小（更一致）。

Conclusion: 非对比式视觉-语言预训练可以在更少超参与更简单目标下，提供更稳定且更优的性能，是对比方法的有效替代。

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [92] [Schrödinger-Inspired Time-Evolution for 4D Deformation Forecasting](https://arxiv.org/abs/2602.00661)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 提出一种受薛定谔方程启发的物理引导神经网络，用于稳定、可解释的4D（3D+时间）预测，在医学成像等任务上能准确预测体素强度与形变场。


<details>
  <summary>Details</summary>
Motivation: 现有时空预测模型多为“黑箱”，易在长时域漂移、误差累积，缺乏物理一致性与可解释性，且在医学场景中需要保持解剖结构与形变的物理合理性。

Method: 在深度卷积框架中显式嵌入可微的、解卷积式的薛定谔时间演化算子。模型从体数据序列中学习体素级复波函数ψ=A·e^{iφ}的幅度A、相位φ与势场V，并通过展开的薛定谔步进器做前向时间演化；幅度/相位/势分别承载结构强度、传输动力学与时空交互。与形变合成模块自然兼容以保持解剖保真。

Result: 在模拟但逼真的形变与拓扑变化基准上，实现对未来4D状态（体强度与形变场）的准确、稳定预测，显著减轻长时域漂移与误差累积，并提供可解释潜表示。

Conclusion: 这是首个将薛定谔型演化算子端到端并入4D神经预测的框架，兼具深度网络表达力与物理建模稳健性，提供走向可解释、稳定且解剖一致的时空预测的范式。

Abstract: Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schrödinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $ψ= A e^{iφ}$, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schrödinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.

</details>


### [93] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出一种在解剖摄影重建的各向异性3D体数据上进行高效超分辨插片的方法，生成各向同性体积，从而提升分割、皮层表面重建和配准精度。通过域随机化的合成数据训练，方法对不同切片协议和大厚度切片具有鲁棒性，并已开源。


<details>
  <summary>Details</summary>
Motivation: 基于2D解剖照片重建的3D脑体积在高各向异性（厚切片）条件下会出现结构过于平滑、边界粗糙，影响形态测量、分割与与MRI等影像配准的准确性，限制了病理与影像学之间的联通。

Method: 在已有的从2D解剖照片重建得到的各向异性体数据上，加入一个计算高效的超分辨插片步骤：对缺失层面进行“插片”以补齐体素，生成解剖一致的各向同性体。训练采用域随机化的合成数据以提升跨协议泛化与对大厚度切片的鲁棒性。

Result: 与原始各向异性重建相比，插片后的体数据在自动分割中取得更高的Dice，尤其在皮层和白质区域；在表面重建与图谱/ MRI配准任务中得到更准确的皮层表面与更可靠的MRI配准。

Conclusion: 该超分辨插片策略显著提升了基于照片的病理重建的分辨率与解剖保真度，强化了神经病理与神经影像之间的桥梁；方法通用、对大厚度切片鲁棒，并已公开发布以促进应用与复现。

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [94] [HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression](https://arxiv.org/abs/2602.00671)
*Yangzhi Ma,Bojun Liu,Wenting Liao,Dong Liu,Zhu Li,Li Li*

Main category: cs.CV

TL;DR: 提出HPC：一种用于动态Gaussian Splatting（DGS）流式传输的端到端压缩框架，采用分层点式潜表示与跨帧网络参数压缩，显著降低存储与传输开销（比基线降67%）同时保持高保真重建。


<details>
  <summary>Details</summary>
Motivation: 现有DGS流式压缩方法多通过潜变量驱动网络预测帧间Gaussian残差：栅格式潜表示会浪费在空占空间的参数，点式潜表示又未利用局部相关性而不够紧凑；同时，网络本身的跨帧冗余未被压缩。需要一种既避免空域冗余又能挖掘局部与跨帧相关性的方案。

Method: 1) 分层点式（per-Gaussian）潜表示：按Gaussian实例分配潜点，配合定制的聚合机制，在多尺度上汇聚邻域信息，提升紧凑度并减少空间冗余；2) 首次针对DGS流式场景进行神经网络压缩：挖掘并利用跨帧参数相关性，对驱动网络进行跨帧共享/预测/量化等压缩；3) 将潜表示压缩与网络压缩联合，形成端到端训练与压缩流程。

Result: 在综合实验中，相比SOTA显著提升率失真性能；相对基线实现67%的存储下降，同时保持高重建保真度，适用于高效流式传输。

Conclusion: 分层点式潜表示可避免空占空间冗余并捕获局部相关性，结合跨帧网络参数压缩可构成高效的端到端DGS流式压缩框架（HPC），在存储和重建质量上均优于现有方法。

Abstract: While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.

</details>


### [95] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

TL;DR: 论文提出一套面向视频理解的“时间关系建模”系统性方案，涵盖自动标注、参数高效微调、长视频建模、新型对比学习以及LVLM时间推理瓶颈分析，并给出相应基准与实践配方，整体显著提升对视频中时序动态的表示与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法对时间关系刻画不足：标注成本高且噪声多；小样本/低数据下难以建模时序；长时长视频效率与记忆受限；动作-时刻的细粒度关系缺失；LVLM在时间推理上受多模态接口瓶颈制约。为此需要端到端的时间建模与训练/评测体系。

Method: (1) 用大规模视觉-语言模型自动生成标注，并配以具“减性角度边距”的抗噪对比学习目标；(2) 设计“循环适配器”进行参数高效微调，以捕获时序动态，适用于低数据场景；(3) 将状态空间层(SSL)融入长视频建模，并发布两个长时长基准（第一视角与长片级）；(4) 提出显式建模“动作—视频时刻”细粒度关系的对比学习框架；(5) 系统评测LVLM并定位视觉-语言接口为时间推理瓶颈，进而给出面向时间的放大配方。

Result: 在多个基准（含新建的长时长数据集）上，所提方法在表示质量、时序推理与长视频效率方面取得一致提升；循环适配器在低数据下效果显著；SSL实现高效长程依赖；细粒度对比学习更好刻画动作—时刻关系；LVLM实验验证接口瓶颈假设并通过“时间导向配方”带来增益。

Conclusion: 显式的时间建模是提升视频理解的关键。通过自动标注+抗噪训练、参数高效时序适配、SSL支撑的长视频建模、细粒度对比学习与LVLM接口改良，可系统性增强模型对视频流动性的表示与推理能力，并为长时长与低数据场景提供可扩展路径。

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [96] [V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication](https://arxiv.org/abs/2602.00687)
*Yuankun Zeng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.CV

TL;DR: 提出V2X-DSC：把多车协同感知中的中间特征共享建模为分布式源编码问题，利用接收端本地特征作“旁信息”进行条件重建，从而在KB级带宽下实现高精度3D感知融合。


<details>
  <summary>Details</summary>
Motivation: 协同感知需在车-车/车-路（V2X）链路上传输稠密BEV特征，但带宽受限；不同代理观测同一场景，特征高度相关，直接共享存在大量冗余。需要一种能只传“创新信息”的通信与编码机制，以兼顾精度与带宽。

Method: 提出V2X-DSC框架：受分布式源编码启发，设计条件编解码器（DCC）。发送端将BEV特征压缩为紧凑码；接收端以自身本地特征为条件进行重建，把比特分配给互补信息而非冗余部分。该条件结构作为正则，促使模型学习增量表示并降低特征噪声。该方法可作为即插即用通信层，适配多种融合骨干。

Result: 在DAIR-V2X、OPV2V、V2X-Real上，于KB级带宽下实现SOTA精度-带宽权衡；相比现有方法在低带宽条件下具有更高3D感知性能与更稳健的融合效果。

Conclusion: 将协同感知特征共享重述为分布式源编码问题，通过条件编解码聚焦“创新”信息，显著降低带宽同时提升或保持精度，具备通用可插拔性与强泛化能力。

Abstract: Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

</details>


### [97] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: JoyAvatar提出可生成长时长、高指令对齐的视频虚拟人，支持全身大幅动作与动态镜头，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频头像/虚拟人模型在口播、演讲、唱歌等场景表现不错，但对复杂文本指令的对齐较弱，尤其涉及全身大动作、动态机位、背景切换、人-物交互等，难以长时间保持自然与连贯。

Method: 1) 双教师增强训练：将基础扩散/视频生成模型的文本可控性“蒸馏”到头像模型，同时学习音视频对齐（口型同步）。2) 条件强度动态调制：在扩散去噪的不同时间步，动态调节多模态条件（音频、文本等）的权重，缓解异构条件间冲突，兼顾身份一致、口型正确与动作/镜头可控。

Result: 在GSB基准上优于SOTA（如Omnihuman-1.5、KlingAvatar 2.0），能生成自然、时间一致的全身动作与动态镜头，同时保持口型同步与身份一致；支持更复杂应用，如多人对话与非人角色扮演。

Conclusion: 双教师蒸馏+条件强度随时间步调制显著提升了文本指令对齐与多模态协同，扩大了头像视频生成的能力边界，实现更长时长、更复杂场景的视频化身生成。

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [98] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

TL;DR: 提出一个半监督实例分割框架，通过高分辨率图像切片+伪标签，显著提升高通量表型中对高粱微小气孔结构（孔、保卫细胞、复合区域）的分割精度。


<details>
  <summary>Details</summary>
Motivation: 高粱抗旱强，水分利用效率提升依赖精确量化气孔性状。但气孔极小且形态多样，人工标注成本高、自动分割对嵌套小结构效果差，限制了大规模表型研究与育种应用。

Method: 构建包含11,060个人工标注补丁的高粱叶片显微图像数据集，覆盖多基因型与叶面；将高分辨率图像裁切为重叠小补丁以增强对微小目标的检测；对未标注图像进行伪标注，新增56,428个伪标注补丁；在语义与实例分割模型上进行基准测试，比较有无半监督与切片策略的差异。

Result: 采用补丁化+伪标注后，语义分割的最佳mIoU由65.93%提升到70.35%，实例分割的最佳AP由28.30%提升到46.10%，对微小、嵌套气孔结构的检测显著改善。

Conclusion: 补丁化预处理结合半监督学习能有效缓解小目标与标注瓶颈问题，显著提升气孔细粒度分割性能，支持可扩展的气孔性状提取，推动AI表型在作物科学中的应用。

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [99] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 基于扩散模型的化妆迁移方法：构建高质数据集、设计身份-妆容解耦扩散框架，并引入文本引导的细粒度区域控制，实现更高保真与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有化妆迁移常受限于数据稀缺、身份与妆容表征混缠以及可控性弱，导致风格多样性与人脸身份保持难以兼顾。作者希望借助扩散模型的稳定生成能力与可控性改进这些问题。

Method: (1) 提出“训练-生成-筛选-再训练”的数据构建流程，融合合成/真实/过滤样本，提升多样性与逼真度；(2) 设计扩散式解耦框架，将身份特征（结构、肤色）与妆容特征分离，实现准确迁移且保持人脸一致性；(3) 文本引导机制支持细粒度、区域化控制（如眼妆、唇妆、底妆）通过自然语言指令进行编辑。

Result: 在基准与真实场景实验中，相比现有方法在保真度、身份保持与灵活性方面均取得提升，并展示了多样且精确的妆容迁移效果。

Conclusion: 通过高质量数据构建、身份-妆容解耦的扩散框架与文本引导的区域控制，方法在化妆迁移任务上实现更稳定与可控的生成，兼顾逼真度与身份一致性；提供的数据集示例进一步支持其有效性。

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [100] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

TL;DR: 提出一种基于扩散的算法，从存在TSDF融合“双层伪影”的双层点云中分离内层与外层表面，针对有开边界与闭合模型均适用，约10秒可从各2万点中提取内层。


<details>
  <summary>Details</summary>
Motivation: TSDF在室内/医疗重建中因截断阈值不对称会产生双层壳与法向混乱等问题，尤其在存在开边界的点云中，传统方法难以稳健地区分真实表面与伪影。需要一种轻量的事后处理方法分离内外层以获得准确表面。

Method: 提出扩散（扩散/粒子）驱动的分离策略：在双层点云中建模粒子在开边界可“逃逸”的特性，通过扩散过程区分位于内层与外层的点（重点提取真实内层），适用于闭合与开边界几何；作为TSDF融合后的独立模块运行。

Result: 在包含开边界和闭合模型的双层点云上，能稳健分离内外层，约10秒即可从4万点（内外各2万）中提取内层，缓解重叠表面与法向无序问题。

Conclusion: 该轻量级后处理方法可有效消除TSDF双层伪影，获得更准确的表面表示，适合室内场景建模与医疗成像等应用，不旨在取代完整的变分或学习型重建流程。

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [101] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: 提出HSI-VAR，将高光谱图像复原视为自回归生成问题，通过对谱-空依赖逐步建模，结合潜在条件对齐、退化感知引导与空谱自适应模块，实现更快、更准的全能型复原，较扩散法显著提速并提升PSNR与结构保真。


<details>
  <summary>Details</summary>
Motivation: 真实HSI常出现噪声、模糊、缺失波段等复合退化；扩散模型计算昂贵、回归法易过平滑且细节丢失，亟需兼顾效率与细节保真的统一复原方案。

Method: 将HSI复原重构为自回归生成：逐步建模谱与空域依赖。三大设计：1) 潜在-条件对齐，保证潜在先验与条件嵌入的语义一致性；2) 退化感知引导，将混合退化线性组合编码到嵌入空间，实现自动可控恢复并降低推理成本；3) 空-谱自适应解码模块，在解码阶段联合细化空间与光谱细节。

Result: 在9个一体化HSI复原基准上达SOTA：ICVL上PSNR提升3.77 dB；相较扩散法推理最高加速达95.5×，且结构保真更好；推理阶段计算成本约降50%。

Conclusion: HSI-VAR以自回归范式高效建模空谱依赖，兼具速度与质量，适用于真实场景下的全能型HSI复原，优于扩散与回归方法。

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [102] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: 研究评估使用U-Net对臂丛神经超声进行分割，探讨数据来源与标注策略对性能的影响：多机型联合训练对弱域有正则化收益但不优于匹配单域；从二分类扩展到多类别监督显著降低神经Dice；神经尺寸与分割准确度中度正相关，小神经最难。


<details>
  <summary>Details</summary>
Motivation: 临床超声引导神经阻滞需要准确定位神经，但受低对比度、散斑噪声和解剖差异影响，手动识别困难。现有深度学习方法在不同设备数据与标注设置下的鲁棒性与迁移性尚不清楚，需系统量化其影响以指导实际系统开发。

Method: 以U-Net为骨干，在臂丛神经超声数据上进行分割实验。比较：1) 单一超声设备数据与跨设备（SIEMENS NX3 Elite与Philips EPIQ5）组合训练对目标域的影响；2) 任务设置从仅神经二分类到包含动脉、静脉、肌肉的多类别监督。评估以Dice等指标，并分析神经尺寸与性能的相关性（Pearson相关）。

Result: - 跨设备联合训练对低性能采集源有正则化增益，但当训练域与测试域匹配时，单源训练表现更佳，联合不具优势。
- 多类别监督相较二分类使神经Dice下降9%–61%，归因于类别不平衡与边界模糊。
- 神经尺寸与分割准确度呈中度正相关（r=0.587, p<0.001），小体积神经更难分割。

Conclusion: 在现实临床数据约束下，开发鲁棒神经分割系统应：优先使用与目标域匹配的数据；跨设备数据可作为对弱域的正则化补充；谨慎引入多类别监督，需处理类不平衡与边界不确定；针对小神经设计特定策略。

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [103] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: 提出DVLA-RL，通过双层语义构建与强化学习门控跨模态融合，实现从低到高语义的渐进、可适配对齐，在九大FSL基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有把LLM引入FSL的方法多仅用类名生成语义，缺少从局部到全局、随网络层次自适应的逐步对齐，导致语义增益有限、泛化不足。

Method: DVLA-RL包含：1) 双层语义构建（DSC）：以类名+支持样本作为LLM条件，生成判别性属性；通过逐步筛选得到相关属性，并合成为一致的类描述，形成低层细粒度属性+高层整体描述的双层语义。2) 强化学习门控注意（RLA）：将跨模态融合视为序贯决策，使用轻量策略与episodic REINFORCE，动态调节各网络层中自注意与交叉注意的权重，按层融合文本与视觉token。

Result: 浅层更聚焦局部属性、深层强调全局语义，实现更精确的跨模态对齐与类判别，少样本下获得更强泛化；在三类FSL场景、九个基准上取得新的SOTA。

Conclusion: 通过双层语义构建与RL门控的层级自适应融合，可在少样本条件下显著提升跨模态对齐与泛化能力，优于现有基线并具备通用性。

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [104] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 提出Any3D-VLA：将2D视觉输入显式升维为点云并与2D融合，缓解3D数据稀缺与跨域差异，提升VLA在仿真与真实场景的性能与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VLA多依赖2D图像，难以在复杂场景中进行可靠的空间理解与操作；同时真实3D数据稀缺、跨环境与深度尺度偏差导致严重域间落差。

Method: 开展不同观察空间与视觉表示的对比试验，发现点云能有效补充2D表示；据此提出Any3D-VLA：统一使用仿真器点云、真实传感器点云与模型估计深度生成的点云，在同一训练管线中构造多样化输入，学习与2D可融合的、域不敏感的3D表示。

Result: 在仿真与真实实验中，Any3D-VLA较基线显著提升任务表现，并有效缓解跨环境与深度尺度引起的域间差距。

Conclusion: 显式引入3D点云并与2D联合学习能增强VLA的空间理解与泛化；统一多来源点云的训练策略是应对3D数据稀缺与域偏差的有效途径。

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [105] [VVLoc: Prior-free 3-DoF Vehicle Visual Localization](https://arxiv.org/abs/2602.00810)
*Ze Huang,Zhongyang Xiao,Mingliang Song,Longan Yang,Hongyuan Yuan,Li Sun*

Main category: cs.CV

TL;DR: VVLoc提出一个统一的多相机单网络方案，同时完成拓扑与度量定位，并输出置信度；仅需图像对与真值位姿监督，训练高效；在公开与自采高难度数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有车载定位方法常将拓扑与度量任务割裂处理、仅支持单相机，且依赖3D语义/位姿先验，缺乏置信度评估，工程落地性不足。需要一种统一、轻依赖且可量化不确定性的方案。

Method: 设计VVLoc统一流水线：单一神经网络接收多相机观测，先估计观测间地理接近度实现拓扑检索，再通过匹配策略回归相对度量位姿，并同时输出结果置信度。训练只需视觉对与对应GT位姿，无需额外3D语义或先验。

Result: 在公开数据集与更具挑战性的自采数据集上评测，VVLoc在多类定位任务上达到或超越SOTA精度，并提供可靠的置信度评估。

Conclusion: VVLoc在统一框架下高效实现多相机拓扑+度量定位与置信度估计，降低数据依赖并提升工业可用性，展现出优异通用性与准确性。

Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.

</details>


### [106] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

TL;DR: 论文提出Paracosm：直接生成“心像”用于组合图像检索（CIR），并为数据库每张真实图生成对应的合成图，以在合成域中进行匹配；方法零训练、零样本，在四个基准上显著优于现有零样本方法。


<details>
  <summary>Details</summary>
Motivation: 现有零样本CIR多将多模态查询（参考图+修改文本）转写成长文本，再用VLM做图文匹配，但“心像”只被隐式描述，文本化会丢失视觉细节与几何/风格信息，导致匹配不准。作者希望从第一性原理出发，显式构造可比对的“心像”。

Method: - 用大型多模态模型（LMM）对给定参考图与修改文本进行提示，直接生成对应的“心像”（合成目标图）。
- 为消除合成-真实域差距，对数据库中每张真实图也由LMM生成一个对应的合成版本。
- 在这个由LMM构建的“平行合成世界”（paracosm）中，将查询“心像”与数据库的合成图进行相似度匹配，进而检索出最接近的真实目标图。
- 整体为训练无关、零样本流程。

Result: 在四个具挑战性的CIR基准上，Paracosm作为训练自由、零样本方法取得新的SOTA，显著超越现有零样本方案（具体指标未给出但表述为大幅领先）。

Conclusion: 将隐式的“心像”显式化并在合成域内对齐，是提升零样本CIR的有效途径。Paracosm无需训练即可显著提升性能，显示利用LMM构建“paracosm”进行匹配的潜力。

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [107] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: 提出一种适用于皮肤科联邦学习的“身份无关、病理保真”的本地隐私工具，基于免反演的Rectified Flow Transformer（FlowEdit）和“生成即分割”机制，在保持病灶特征的同时快速去身份化并生成差异红斑掩码，从而在边缘侧生成合规合成数据，降低梯度泄露风险。


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要保护患者隐私，但常规去标识会损伤病理特征；标准生成式编辑依赖高耗时的反演流程，不适合资源受限的临床边缘设备，亟需同时兼顾隐私与诊断保真的低时延方法。

Method: 在客户端采用免反演的Rectified Flow Transformers（FlowEdit）实现高保真身份变换（<20秒）；提出“Segment-by-Synthesis”，在本地生成健康/病理对偶图像对，计算二者的差异以得到与生物特征与语义伪影（如首饰）解耦的红斑掩码；将生成的隐私合规代理数据用于联邦环境中。

Result: 在高分辨率临床样本上，小样本试验显示跨不同合成身份的IoU稳定性>0.67，且编辑过程近实时（<20秒），可本地部署。

Conclusion: 该框架在边缘侧生成隐私合规的合成替身与差异掩码，兼顾隐私与病理保真，降低源端梯度泄露风险，为皮肤图像的高精度联邦分析提供安全可行路径。

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [108] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 提出TransNormal：利用扩散先验与DINOv3语义特征，实现单步透明物体法线估计，显著超越SOTA，并发布合成数据集。


<details>
  <summary>Details</summary>
Motivation: 实验室自动化需要对透明器具进行可靠的法线估计；但折射/反射导致深度与法线传感器失效，使具身AI难以部署。现有方法对无纹理透明表面与复杂光学现象适配不足。

Method: 将预训练扩散模型适配为单步法线回归器；通过跨注意力融合DINOv3的稠密语义特征以弥补透明表面缺乏纹理；引入多任务学习目标与基于小波的正则化保持细粒度结构；构建物理渲染的TransNormal-Synthetic数据集提供高保真法线监督与场景多样性。

Result: 在ClearGrasp上，平均误差下降24.4%，11.25°准确率提升22.8%；在ClearPose上平均误差降低15.2%；展示对透明器具的显著鲁棒性与细节保真。

Conclusion: TransNormal通过扩散先验+语义跨注意力+小波正则与多任务训练，实现对透明物体的高精度单目法线估计，并以新数据集支撑训练与评测，优于现有方法且具实用潜力。

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [109] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

TL;DR: 提出一种无需训练的二阶几何统计框架，用SPD协方差描述场景，利用黎曼几何映射将其线性化嵌入，从而提升VPR在环境与视角剧变下的鲁棒性，零样本下亦达SOTA水准。


<details>
  <summary>Details</summary>
Motivation: 现有VPR聚合方法要么依赖大量监督训练，要么只用一阶统计，忽视结构相关性，导致对环境/视角剧变鲁棒性不足与泛化差。

Method: 将图像/场景表示为对称正定（SPD）流形上的协方差描述子，扰动被建模为可解析的同余变换；通过几何感知的黎曼映射（如Log-Euclidean或指数/对数映射）把SPD点投影到线性欧氏嵌入，解耦结构信号与噪声；使用固定的预训练 backbone，不进行参数更新，实现训练-free的表征与匹配。

Result: 在广泛实验中，与SOTA基线相比取得高度竞争力，尤其在零样本设置与挑战性场景变化下表现更优。

Conclusion: 二阶几何统计能在无需额外训练的前提下有效捕获稳定结构，提高VPR的泛化与鲁棒性，是对现有一阶/监督范式的有力替代。

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [110] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: Distill3R提出将大型3D多视角重建基础模型的几何推理蒸馏到可在单机训练的小模型，通过离线缓存监督与置信度感知损失，实现参数与推理加速的大幅压缩，同时保持结构一致性与可用的3D理解能力，作为低算力实验室的可复现实践基线。


<details>
  <summary>Details</summary>
Motivation: 当前多视角3D重建依赖超大算力训练的基础模型，学术界多数实验室难以复现与研究，形成“算力鸿沟”。作者希望提供一种可在单机上训练的小模型方案，使研究者能在有限资源下获得具备全局一致几何理解的模型，并便于在自有数据上微调与部署。

Method: 1) 离线缓存：将教师模型在训练前的大量推理结果（压缩后的监督信号）缓存，训练时不再反复调用教师，解耦重推成本。2) 置信度感知蒸馏损失：引入教师不确定性指导学生学习，降低噪声监督影响，提升在普通硬件上的稳定训练。提出一个约72M参数的学生网络架构。

Result: 学生模型参数从650M降至72M（约9倍压缩），推理速度提升约5倍；学生在结构一致性与几何理解上保持与教师接近的质量。训练成本从需要大规模GPU集群一周降至单机三天内可完成。

Conclusion: Distill3R提供一套可复现、单机可训练的3D几何蒸馏流程，作为民主化3D视觉研究与高效边缘部署的切入点；目标不是追赶最强SOTA，而是为低算力环境提供可用基线以在特定领域低成本训练与专化。

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [111] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: 提出DIAMOND：一种训练免、在推理阶段进行“轨迹校正”的方法，显著减少T2I与扩散模型中的视觉/解剖伪影，避免改权重与昂贵的区域细化，零样本即可提升保真与无伪影合成。


<details>
  <summary>Details</summary>
Motivation: 现有去伪影方法多为事后处理或需修改权重、区域细化代价高，且无法在生成核心阶段有效干预，难以满足专业实践中对高保真、无伪影图像的需求。

Method: 在扩散/生成轨迹的每一步重构“干净样本”估计，并据此对当前潜变量进行纠偏，引导采样远离会产生伪影的潜在状态；方法对T2I与标准扩散模型均适用，无需额外训练或权重改动。

Result: 在多种生成架构上零样本应用，明显降低视觉与解剖伪影，提升图像保真度与一致性；推理时开销低于区域细化，无需权重注入或微调。

Conclusion: DIAMOND通过推理期轨迹校正，实现对现代生成模型的训练免伪影抑制与质量提升，提供实用、鲁棒的高保真图像合成路径；代码公开可复现。

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [112] [OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection](https://arxiv.org/abs/2602.00904)
*Kunal Mahatha,Ali Bahri,Pierre Marza,Sahar Dastani,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出OCTOPUS：在图像中沿8个方向进行离散复现的多方向SSM，实现线性复杂度下同时捕获全局上下文与局部空间结构，在分类与分割任务上优于现有V-SSM，边界与区域一致性更好。


<details>
  <summary>Details</summary>
Motivation: 传统SSM在文本中因因果序列建模高效，但在视觉中因“因果”传递破坏空间邻接关系，导致忽略局部相邻像素/patch的相关性、甚至连接非相邻区域，局限了其在视觉任务上的表现。需要一种既保留SSM线性复杂度又兼顾全局与局部空间结构的机制。

Method: 提出OCTOPUS：在图像平面沿水平、垂直、两条对角线共8个主方向进行前向/后向的离散复现（multi-directional recurrence），使空间上相关的区域可高效信息交互，而互不相关的patch保持独立；以SSM级别的线性复杂度实现多方向、全局-局部兼容的建模。

Result: 在分类与分割基准上取得更优表现，相比现有V-SSM模型具有更好的分割边界保留与区域一致性，同时分类精度相对更高。

Conclusion: OCTOPUS提供了一种可扩展、有效的多方向复现范式，兼顾全局上下文与局部空间结构，为构建空间感知且计算高效的视觉架构奠定基础。

Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.

</details>


### [113] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: 提出ConsensusDrop：融合视觉编码器显著性与LLM跨注意力的共识排序，以训练-free方式在预LLM阶段裁剪/合并视觉token，显著改善VLM在相同token预算下的精度-效率权衡。


<details>
  <summary>Details</summary>
Motivation: VLM推理成本高，因LLM需处理大量冗余视觉token。现有压缩方法要么依赖视觉编码器显著性（广泛但与查询无关），要么依赖LLM跨注意力（与查询相关但稀疏且开销大）。单一信号不足以兼顾准确性与效率，且两者在可用阶段和语义上不对称，直接融合不易。

Method: 提出ConsensusDrop：在不训练的前提下，将视觉编码器显著性与查询相关的跨注意力进行对齐并“达成共识”，对视觉token进行排序与选择；对保留外的token采用由编码器引导的token合并以压缩信息；在多个开源VLM（如LLaVA-1.5/NeXT、Video-LLaVA）上作为预-LLM裁剪模块接入。

Result: 在相同token预算下，一致优于先前裁剪方法；在激进的token削减下仍能接近基线精度，同时降低首token延迟（TTFT）与KV缓存占用，形成更优的精度-效率Pareto前沿。

Conclusion: 通过训练-free的跨模态显著性共识排序与合并，既能进行高效的预-LLM视觉token压缩，又能保持甚至提升VLM的精度-效率表现。

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [114] [Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images](https://arxiv.org/abs/2602.00949)
*Xiang Zhang,Boxuan Zhang,Alireza Naghizadeh,Mohab Mohamed,Dongfang Liu,Ruixiang Tang,Dimitris Metaxas,Dongfang Liu*

Main category: cs.CV

TL;DR: 提出两套互补的数据增强框架（IAAA 与 SAAA）来缓解CAR-T/NK免疫突触（IS）显微数据标注稀缺，显著提升IS检测与分割性能，从而改进影像生物标志物的稳健性与预测力。


<details>
  <summary>Details</summary>
Motivation: IS质量可能预测CAR-T/NK疗效，但显微数据标注稀缺限制ANN泛化与定量准确性。需生成与真实分布匹配的高保真合成数据，扩充训练集并保留解剖/语义一致性。

Method: 双路径增强：1) IAAA（Instance Aware Automatic Augmentation）对原始IS图像与掩膜进行实例保真的自动策略搜索与增强，兼容荧光/明场及患者样本；2) SAAA（Semantic-Aware AI Augmentation）以扩散式掩膜生成器产出多样、解剖逼真的分割掩膜，再用Pix2Pix条件生成与掩膜对齐的高保真IS图像，扩展超出IAAA可达的分布。

Result: 两种增强生成的合成图像在视觉与结构上贴近真实IS数据，显著提升CAR-T/NK IS的检测与分割精度与鲁棒性。

Conclusion: 通过实例保真与语义感知的联合增强，缓解小样本瓶颈，改进IS量化与影像生物标志物的可靠性，有望更好预测患者对CAR-T/NK治疗的应答。

Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.

</details>


### [115] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出将拓扑数据分析（TDA）与DenseNet121融合，用OASIS结构MRI做阿尔茨海默病四分类，显著优于现有方法，精度99.93%、AUC 100%。


<details>
  <summary>Details</summary>
Motivation: 现有基于MRI的AD自动诊断在早期识别、特征充分性与泛化上仍有限；传统CNN可能忽视脑结构的全局/拓扑特性，影响四阶段细粒度分类。

Method: 构建混合深度学习框架：用TDA提取脑结构的拓扑特征（如持久同调/持久图），并以DenseNet121从MRI切片学习层级空间特征；将两类特征融合以增强类别可分性；在OASIS-1 Kaggle数据上进行四分类训练与评估。

Result: 在OASIS-1上，TDA+DenseNet121取得99.93%的准确率与100%的AUC，超过近期CNN、迁移学习、集成与多尺度方法的SOTA表现。

Conclusion: 将拓扑信息纳入深度学习流程可大幅提升AD分期自动诊断性能；该框架在OASIS-1上表现鲁棒且高精度，显示为临床辅助决策的有前景工具。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [116] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 论文提出面向情感理解的ToM（心智理论）框架与评测：构建分层基准HitEmotion，设计ToM引导的推理链，并用TMPO强化学习以过程级监督优化模型，在多项实验中显著提升复杂情感任务的准确性与解释一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在深层情感理解方面不足，缺乏对他人信念、意图、欲望等心智状态的显式建模；作者认为真实的“情感智能”应建立在ToM这一认知基础上，需要可测可控的基准与可执行的推理与训练方法。

Method: 1) HitEmotion：按认知深度分层的ToM-情感评测基准，用于诊断能力断点；2) ToM-guided reasoning chain：在推理过程中显式追踪心智状态并校准跨模态证据，实现可解释、忠实的情感推理；3) TMPO：以中间心智状态为过程级监督的强化学习策略，强化推理链质量与最终任务表现。

Result: 实验证明SOTA MLLMs在高认知负荷情感任务上存在明显短板；采用ToM引导推理链与TMPO后，端到端准确率提升，生成的理由更连贯、更忠实于证据。

Conclusion: 提供一套实践工具包：分层评测（HitEmotion）+ ToM引导推理+ 过程级RL（TMPO），可系统评估并增强MLLM基于认知的情感理解能力；数据与代码公开以促进社区研究。

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [117] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 团队在NeurIPS 2025“Mouse vs. AI: Robust Visual Foraging”两赛道均夺冠：用极简两层CNN+GLU与观测归一化在鲁棒性赛道得分95.4%；用16层ResNet风格、GLU门控、1780万参数的深模形在神经对齐赛道夺冠。训练步数与性能呈非单调关系，约200K步最优；消融与失败分析解释了“鲁棒性偏简单、对齐偏深”的差异。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统兼具强鲁棒性与与神经反应的一致性，现有人工视觉代理难以同时满足。比赛提供统一任务框架，检验哪些架构与训练策略能在视觉鲁棒性与神经对齐两方面接近或匹配生物视觉。

Method: - 赛道1：以简为主，搭建两层轻量CNN，引入Gated Linear Units（GLU）进行门控，配合观测归一化以提升跨扰动泛化。
- 赛道2：采用ResNet式深网络（16卷积层）并在残差块中引入GLU门控，参数量约17.8M，针对神经预测优化。
- 系统性实验：比较10个从6万到114万步的训练检查点，做消融与失败案例分析，量化训练时长与性能关系。

Result: - 赛道1最终分数95.4%，以极简结构在多扰动条件下实现最强泛化。
- 赛道2取得top-1神经预测成绩，模型规模17.8M参数。
- 训练步数与表现非单调，约200K步最优，进一步训练可能过拟合或偏移目标。

Conclusion: 在视觉觅食任务中，“越简单越鲁棒，越深越对齐”：简化架构配合关键组件可带来更好的鲁棒泛化，而更深且带门控的高容量网络更擅长逼近神经数据。结果挑战了“更复杂即更好”的直觉，并为构建鲁棒、类生物视觉代理提供了实证指南。

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [118] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: 提出VAMOS-OCTA：用2.5D U-Net与血管感知的多轴正交监督损失，自动修补手持OCTA因运动导致的B扫空带，兼顾B扫清晰度与体数据投影质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手持OCTA适用于婴幼儿或不配合人群，但体数据采集对运动极其敏感；突发位移会造成整条B扫未采样，导致体渲染和en face投影出现空白带，严重限制临床可用性与数据分析。需要一种既能恢复B扫结构又能保证体/投影血管连续性的自动化重建方法。

Method: 构建2.5D U-Net，以邻近多帧B扫堆栈输入重建被破坏的中心B扫；提出VAMOS损失：1) 血管加权的强度重建（突出血管区域误差）；2) 轴向与横向投影一致性约束（与正交投影保持血管连续）；联合合成与真实受扰动数据训练，强调跨平面血管拓扑连贯。

Result: 在感知质量与像素级指标上均优于现有方法；在严重运动伪影下仍能恢复清晰毛细血管、提高血管连通性，并生成干净的en face投影；同时改善B扫锐度与体投影准确性。

Conclusion: 多轴血管感知监督为3D OCTA运动伪影修复提供强约束；VAMOS-OCTA有效修补B扫空带并提升体数据质量，具有实际临床与研究应用潜力；代码已开源。

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [119] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

TL;DR: 提出CortiNet：受皮层启发的轻量级双流网络，通过结构/纹理频率分离与晚期融合，在胆囊疾病超声诊断上以极少参数达98.74%准确率。


<details>
  <summary>Details</summary>
Motivation: 超声成像低分辨率与斑点噪声导致诊断可靠性受限，现有大规模CNN虽有效但难以在临床落地，需一种既物理可解释又高效轻量的模型。

Method: 仿人类视皮层的并行通路，先做多尺度、频率选择的信号分解，将低频结构与高频细节显式分离，分别进入专用编码分支；以物理先验（结构化频域表征）提升特征学习效率；通过晚期“皮层式”融合整合结构与纹理线索；解释性方面仅在结构分支上应用Grad-CAM，使注意力集中于抗噪的结构特征。

Result: 在10,692张、9类胆囊疾病专家标注数据上，CortiNet以显著更少参数实现98.74%诊断准确率，优于或可匹敌传统深度卷积模型。

Conclusion: 将物理可解释的频域分解与皮层启发的双流学习结合，可在超声这类噪声重的医学影像中以极小模型获得高性能与更稳健的可解释性，具备落地潜力。

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [120] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: 提出SRVAU-R1：在视频异常理解中将“自我反思-自我修正”的链式推理引入多模态大模型，配合反思导向的CoT数据与双阶段微调，显著提升异常定位与推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的方法多停留在对异常的表层描述，缺少对异常行为的深层因果与错误纠正能力（如显式自我反思与自我修正），导致推理质量与定位精度受限。

Method: 1) 构建首个面向视频异常理解的反思导向CoT数据集，包含初始推理→自我反思→修订推理的结构化监督；2) 提出反思感知学习范式：先以监督微调对齐多模态推理与反思过程，再通过强化微调（强化学习）优化推理质量与反思有效性，从而强化异常识别与定位；3) 将上述流程集成到SRVAU-R1框架中用于VAU任务。

Result: 在多个视频异常基准上，SRVAU-R1稳定优于现有方法，在时间定位精度与推理质量上均取得显著提升。

Conclusion: 将自我反思机制系统性融入MLLM的多模态推理对VAU有效；反思导向数据与监督+强化的双阶段训练协同提升异常定位与可解释推理，具有通用扩展潜力。

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [121] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出LocalScore，通过利用图库特征的局部密度（k近邻）来提升开放集生物识别的检索与验证性能；方法与网络/损失无关、几乎零开销，跨模态显著降低FNIR并提升TAR，并给出何时最有效的理论与实证分析。


<details>
  <summary>Details</summary>
Motivation: 现实应用中常有未注册的探测样本（non-mated probes），传统系统难以识别，且多样本图库越来越常见。现有方法常将同一人的多样本压缩为单一全局表示，忽视类内变化与局部结构，导致决策边界次优、开放集鲁棒性差。

Method: 提出LocalScore：在匹配评分时显式利用图库特征分布的局部密度信息，用k近邻来刻画局部结构和拥挤程度，作为评分修正或加权；该算法与架构无关、训练损失无关、计算开销可忽略，可直接即插即用地集成到现有系统。

Result: 在多种生物识别模态与数据集上，开放集检索和验证均取得显著提升：例如FNIR@FPIR从53%降至40%，TAR@FAR由51%升至74%。

Conclusion: 考虑图库的局部密度能有效改进开放集决策边界与鲁棒性；LocalScore提供简单通用且高效的插件式方案，并通过理论与实证阐明其在何种数据特性下收益最大。

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [122] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 自动整理的甲状结节超声数据用于训练深度学习分类器，比人工标注数据能显著提升AUC（0.694 vs 0.643），且使用全部自动整理数据优于只用高准确子集。


<details>
  <summary>Details</summary>
Motivation: 深度学习在甲状结节良恶性分类可达放射科医师水平，但受限于高成本的数据标注与数据稀缺。先前提出的自动整理方法虽有一定产出率与准确性，但其生成数据对模型训练是否真正有益尚不明确。

Method: 比较三种训练方案：1) 仅用人工标注数据；2) 用自动整理的全量数据（63%产出率、83%准确性）；3) 用自动整理数据中更高准确性的较小子集。分别训练深度学习模型并评估AUC与置信区间，进行统计显著性检验。

Result: 人工数据训练AUC=0.643（95%CI:0.62–0.66）；全量自动整理数据训练AUC=0.694（95%CI:0.67–0.73，P<.001），显著更好；高准确子集AUC=0.689（95%CI:0.66–0.72，P>0.43），与全量自动整理数据差异不显著且略低。

Conclusion: 自动整理的数据可显著提升甲状结节分类模型性能；建议优先使用全部自动整理数据而非仅限高准确子集，以最大化性能提升。

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [123] [GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration](https://arxiv.org/abs/2602.01033)
*Chentian Sun*

Main category: cs.CV

TL;DR: GMAC 提出一种无需显式重建或人工标定的多相机外参自动标定框架，利用多视图隐式几何表示并联合重投影与循环一致性，实现稳健、准确、可在线的外参估计。


<details>
  <summary>Details</summary>
Motivation: 现有多相机外参标定方法依赖标定板、显式几何建模或特定任务网络，鲁棒性与通用性在动态/在线场景中不足，部署成本高，难以适应实际应用需求。

Method: 将多相机外参作为全局变量，借助多视图重建网络的隐式几何表征；对现有网络进行裁剪与结构重构，使潜特征可通过轻量回归头直接预测外参；联合优化跨视角重投影一致性与多视图循环一致性，提升几何一致性、精度与稳定性；无需设计全新网络与显式3D重建流程。

Result: 在合成与真实多相机数据集上，GMAC在无需标定物与显式重建的条件下实现了高精度、稳定的外参估计，优于或可比于现有方法（摘要未给出具体数值）。

Conclusion: GMAC提供了一种适用于实际部署与在线标定的通用多相机外参估计方案，兼具准确性与稳定性，减少工程开销，并对复杂动态环境具有较强适应性。

Abstract: Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.

</details>


### [124] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: 提出FUSE-Flow：一种帧级、无状态、线性可扩展的多相机点云实时重建框架，依靠置信度与3D一致性加权融合，并用自适应空间哈希做加权聚合，实现高吞吐、低延迟与高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有体素融合、时序累积或全局优化方法在计算与内存开销大、难以线性扩展到多相机、难兼顾实时性与质量，尤其在大规模、重叠视角、深度不连续与动态场景下表现受限。

Method: 逐帧独立生成点云片段；通过测量置信度与三维距离一致性两种权重进行无状态融合以抑噪保形；引入自适应空间哈希的加权聚合，根据局部密度自适应划分3D空间、为每格选代表点并做加权融合，兼顾稀疏与稠密区域；在GPU上并行实现以获得线性复杂度与低延迟。

Result: 在多相机与大规模场景中实现高吞吐、低延迟的点云生成与融合；在重叠、深度突变与动态场景中提升重建稳定性与几何保真；在现代GPU上保持实时帧率。

Conclusion: FUSE-Flow在不依赖时序状态与全局优化的前提下，通过双权重融合与自适应空间哈希，实现了对多相机的线性扩展与高质量实时点云重建，具有效率、鲁棒性与可扩展性。

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [125] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出VEQ量化框架，面向MoE式VLM的后训练量化，双重感知专家与模态差异，显著提升在W3A16下的多模态任务精度。


<details>
  <summary>Details</summary>
Motivation: MoE VLM性能强但显存与计算代价高；PTQ可训练免量化，但现有方法忽视两类异质性：视觉/语言模态差异，以及不同专家贡献不均，导致量化误差在关键专家与关键模态上放大。

Method: 提出Visual Expert Quantization (VEQ)：1) 模态-专家感知量化：统计专家激活频率，优先减少高贡献专家的量化误差；2) 模态亲和感知量化：基于token-专家亲和度结合模态信息，构建增强Hessian，引导校准与权重量化分配。整体为训练后量化流程，面向MoE结构的路由/专家层。

Result: 在多基准上优于SOTA。W3A16配置下，相比此前最佳，Kimi-VL平均+2.04%，Qwen3-VL平均+3.09%，在多模态任务上更稳健。

Conclusion: 面向MoE VLM的双重感知PTQ能有效应对模态与专家异质性，显著提升量化鲁棒性与精度；方法通用且具实践价值（代码开源）。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [126] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: 提出HowToDIV：从单人教学视频自动生成双人多模态任务指导对话的数据集与流程，规模为507个会话、6636问答、24小时视频，并给出Gemma 3与Qwen 2.5基线。


<details>
  <summary>Details</summary>
Motivation: AR与实际任务辅助需要多轮、视频落地的多模态对话数据，但真实采集昂贵且难以规模化，限制了智能体研究进展。

Method: 构建全自动流水线：以大语言模型为核心，将单人教学视频解析为步骤与语义，再合成“专家-新手”的多轮问答对话，生成与视频对齐的多模态任务指导数据；随后整理为HowToDIV数据集并用于评测。

Result: 得到覆盖多领域的HowToDIV：507个会话、6636问答、24小时视频；在该数据集上对Gemma 3与Qwen 2.5进行基线评测并报告初步性能。

Conclusion: 自动化视频到对话的生成流程可显著降低多模态任务指导数据构建成本，为AR与程序性任务助手提供可扩展数据来源；HowToDIV为该方向建立了初始基准。

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [127] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: 提出ReLayout框架，实现无需三元组数据的自动化版式编辑：通过关系图保持未编辑元素结构，用关系感知重建（RADR）自监督学习多动作编辑，基于多模态大模型，显著提升编辑质量、准确性与结构保持。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述的用户意图模糊，版式编辑既要满足指定编辑操作又要保持未编辑部分的布局结构；同时缺乏（原设计、编辑操作、编辑后设计）的三元组数据，限制监督学习。

Method: 1) 定义四类基础编辑动作并标准化操作格式；2) 提出关系图，编码未编辑元素之间的位置与尺寸关系作为结构保持约束；3) 关系感知设计重建（RADR）：给定元素、关系图与合成的编辑操作，训练模型从中重建设计，以自监督方式模拟编辑流程；4) 以多模态大语言模型为骨干，统一多种编辑动作，经微调实现通用编辑。

Result: 在定性、定量与用户研究中，ReLayout在编辑质量、操作准确性和布局结构保持方面显著优于基线模型。

Conclusion: ReLayout无需三元组数据即可实现多样化且结构保持的设计版式编辑，验证了关系约束与自监督重建的有效性，为自动化设计工作流提供了通用可扩展的解决方案。

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [128] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: 提出一种无需训练的解码策略Residual Decoding（ResDec），利用历史信息与logits演化来抑制语言先验导致的多模态幻觉，提升视觉对齐与多项基准表现。


<details>
  <summary>Details</summary>
Motivation: LVLM虽具备强多模态推理能力，但受语言先验影响，常生成与图像不符的“幻觉”。需要一种能在不再训练模型的前提下，在解码阶段抑制幻觉、增强视觉证据利用的方法。

Method: 基于LVLM内部的隐式推理与token logits随时间的演化特性，引入Residual Decoding：在生成过程中利用历史信息（如先前步的概率分布/残差信号）对当前logits进行校正，纠偏由语言先验带来的偏移，从而更好地对齐视觉证据。该方法为训练无关、可即插即用的解码策略。

Result: 在多组实验中，ResDec显著降低由语言先验引起的幻觉，提升视觉指称与目标一致性，减少对象层面的幻觉；在综合LVLM基准上也获得优秀成绩，显示广泛适用性。

Conclusion: ResDec作为一种训练无关的历史感知解码方案，能够有效抑制多模态幻觉、增强视觉对齐，并在多项基准中带来整体性能提升，具备通用和易用的优势。

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [129] [Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis](https://arxiv.org/abs/2602.01055)
*Bo Deng,Yitong Tang,Jiake Li,Yuxin Huang,Li Wang,Yu Zhang,Yufei Zhan,Hua Lu,Xiaoshen Zhang,Jieyun Bai*

Main category: cs.CV

TL;DR: 提出FM_UIA 2026超声图像多任务基准，并给出统一的多头多任务学习基线，验证其在分割/分类/检测/回归27子任务上的可行与稳健。


<details>
  <summary>Details</summary>
Motivation: 超声图像在解剖部位与采集协议上高度异质，导致难以训练具备泛化能力、可临床部署的通用模型；现有方法多为任务特定，缺乏统一基础模型与标准基准。

Method: 构建包含27个子任务的大规模基准；以单一共享网络的多头多任务学习（MH-MTL）为官方基线：采用ImageNet预训练EfficientNet-B4作为骨干，结合FPN获取多尺度上下文；通过任务路由策略使全局任务使用高层语义特征，密集预测任务使用FPN的高分辨率表征；训练使用复合损失、任务自适应学习率缩放与余弦退火调度。

Result: 在验证集上，统一设计在多类任务上表现稳健且可行，提供强基线；具备扩展性并适配不同任务类型。

Conclusion: 统一的MH-MTL框架能在超声多任务场景中有效工作，为构建可泛化的超声基础模型奠定基线与标准；代码与数据已开源，推动领域研究。

Abstract: Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.

</details>


### [130] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: 提出一种基于3D高斯射线追踪的断层重建方法，相较于基于EWA/alpha blending的3DGS与R2-Gaussian，避免局部仿射近似带来的偏差，并便于精确施加非线性几何校正，从而提升投影物理一致性与重建准确性，扩展到更广泛的实际成像系统。


<details>
  <summary>Details</summary>
Motivation: 3DGS在新视角合成上高效准确，R2-Gaussian将其引入CT重建并达SOTA，但为可微分计算而采用局部仿射近似（把3D高斯局部映射为探测器上的2D高斯并alpha混合），导致物理模型与积分存在偏差，且难以纳入非线性几何校正（如PET的弧形校正），限制了在真实系统中的适用性与准确性。

Method: 以射线追踪形式对3D高斯与射线的相交进行解析化处理，直接计算沿射线路径对3D高斯密度的线积分；不进行局部仿射塌缩，保持三维几何；利用射线起点与方向的显式控制，将非线性几何校正（如弧形校正）无缝并入前向投影；整体形成可微的重建框架。

Result: 相较基于splatting/仿射近似的模型，前向投影更物理一致、积分更精确，重建的定量精度与投影准确性提升；方法适用于更广泛的真实断层系统（如含非线性几何误差或需弧形校正的PET）。

Conclusion: 射线追踪式3D高斯断层重建避免了仿射近似引入的误差，并提供对射线几何的精确控制，从而提高投影与重建精度，并增强在现实成像系统中的适用性。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [131] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: 提出DRFormer，将视觉基础模型（如DINO）的细粒度局部纹理与视觉-语言模型（如CLIP）的全局语义相融合，通过双重正则和双向Transformer协调二者，从而在ReID中同时应对遮挡与姿态变化，并在五个基准上达到有竞争力的SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法多依赖单一路线：要么偏重局部细节（易捕捉外观纹理却缺乏语义鲁棒性），要么偏重全局语义（语义强但可能忽略细粒度差异）。在遮挡、姿态变化等场景下，单一范式难以兼顾。作者希望分析并整合DINO与CLIP的互补性，统一局部与全局表征以提升鲁棒性与识别精度。

Method: 提出Dual-Regularized Bidirectional Transformer（DRFormer）。核心：1）双重正则化机制，约束并鼓励来自DINO（局部）与CLIP（全局）的特征在分布与贡献上保持互补与均衡，避免一方主导；2）双向Transformer交互，将两路特征进行跨模态/跨视角对齐与融合，促进细粒度与语义信息的协同；3）在标准ReID训练流程中集成本方法，联合优化。

Result: 在五个ReID基准上进行大量实验，方法有效协调局部与全局表示，在Rank-1与mAP等指标上取得与现有SOTA相当或更优的结果。

Conclusion: 整合视觉基础模型与视觉-语言模型的互补优势，通过双重正则与双向Transformer实现更均衡的表征学习，显著提升遮挡与姿态变化下的ReID性能，并在多基准上展现具有竞争力的效果。

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [132] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: 将显微图像分割视为带物理先验的PDE约束优化，通过在深度网络训练中加入反应-扩散与相场能量的可微残差正则项，显著提高跨细胞类型与小样本情形下的精度、边界质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 显微图像分割存在噪声大、边界弱、标注少，单纯经验风险最小化的深度网络易过拟合且不稳定，需要把物理/变分先验嵌入学习过程以改善泛化与稳健性。

Method: 将分割建模为PDE约束的变分优化：目标由数据一致性项与源自反应-扩散方程及相场界面能的正则项组成，这些PDE/能量以可微残差损失嵌入网络训练；以UNet为基线，在LIVECell上训练两种细胞类型、在未见过的第三种类型上评测，比较无约束深度模型与PDE正则模型。

Result: 相较UNet等无约束基线，PDE正则模型在分割精度与边界保真度上持续提升；在小样本训练下更稳定、泛化更好。

Conclusion: PDE约束优化为数据驱动分割引入结构化先验，连接变分方法、统计学习与科学机器学习，能系统性增强深度模型的稳定性与跨分布泛化能力。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [133] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: 提出PISA：一种训练无关的分段稀疏注意力，在保持全注意力覆盖的同时实现次二次复杂度，加速扩散Transformer的图像/视频生成且质量领先其他稀疏方法。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在图像/视频生成中性能强但受限于注意力的O(N^2)计算。现有块稀疏注意力通过仅保留“关键”块来加速，但在高稀疏度下丢失上下文导致质量显著下降。作者观察到非关键块的注意力分数具有稳定的分布特性，可能被高效近似而非直接丢弃。

Method: 提出PISA（Piecewise Sparse Attention）：不改变训练、以推理时替换注意力。首先挑选关键块并进行精确计算；对其余非关键块，不再丢弃，而是进行块级泰勒展开近似，从而以“精确-或-近似”的分段策略覆盖全局注意力，同时实现次二次复杂度。

Result: 在Wan2.1-14B和Hunyuan-Video上分别获得约1.91×与2.57×的速度提升，并在稀疏注意力方法中保持最高生成质量；在FLUX的图像生成上实现约1.2×加速且无可感知的质量损失。

Conclusion: 利用非关键块注意力分数的分布稳定性，通过分段“精确-或-近似”设计，PISA在不训练的前提下逼近全注意力，兼顾速度与质量，优于传统“保留或丢弃”的稀疏注意力。代码已开源。

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [134] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: 提出MedAD-38K大规模多模态多中心基准与两阶段训练，结合一致性奖励的Con-GRPO，使MedAD-R1在医学异常检测上达SOTA并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LMM的医学异常检测多依赖在简单、碎片化数据上的SFT，导致推理贫乏、跨模态泛化弱、答案与推理脱节，难以满足临床可信与可解释需求。

Method: 1) 构建MedAD-38K：包含多模态、多中心数据，提供诊断型CoT注释与结构化VQA对；2) 两阶段训练：阶段一“认知注入”以SFT对齐“先思考再回答”的范式与基础医学知识；阶段二提出Con-GRPO，在策略优化中加入“推理—答案一致性奖励”，约束生成的推理与最终诊断逻辑一致。

Result: 提出的MedAD-R1在MedAD-38K基准上取得SOTA，较强基线提升超过10%，并展示出更透明、连贯的诊断推理路径。

Conclusion: 通过数据基准与带一致性约束的训练框架，可显著提升医学异常检测中LMM的推理一致性、泛化与可解释性，为临床决策支持的可信AI提供有效路径。

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [135] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: 提出DVE：一种面向flow matching文生图模型的训练免调概念擦除方法，通过在推理时沿“目标概念 vs. 锚概念”的差分方向投影并移除概念相关分量，实现NSFW、风格与对象等概念的精确抑制，同时保持画质与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除多依赖DDPM并需代价高的微调，不适用于新兴的flow matching生成范式；实际部署需要对NSFW、版权风格与特定对象进行可控、安全的抑制。

Method: 观察到语义概念隐含在生成流的速度场方向结构中：构造“目标概念—锚概念”的差分向量场，推理时将原速度场在该差分方向上进行投影并去除概念相关分量，以选择性抑制目标概念。无需训练，仅在推理阶段操作。

Result: 在FLUX上广泛实验，相比现有基线，在NSFW抑制、风格移除、对象擦除等任务中更稳定有效，并能保持图像质量与多样性。

Conclusion: DVE为flow matching模型提供高效、训练免调的概念擦除机制，精准抑制目标概念且最小化对无关语义的影响，优于既有方法并具备实用性。

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [136] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: PandaPose通过将2D姿态先映射到统一的3D锚点空间，再进行关节预测，显著缓解2D误差传播与遮挡难题，在Human3.6M等数据集上相对SOTA降错14.7%。


<details>
  <summary>Details</summary>
Motivation: 直接从2D关键点到3D关节的逐关节映射易受2D估计误差影响，且在自遮挡下深度歧义难解，需要一个能融合稳健先验与深度信息的中间表示来提升鲁棒性与准确性。

Method: 构建“3D锚点空间”作为统一中间表示：1）在规范坐标系中为每个关节定义3D锚点，提供稳定先验；2）进行“深度感知”的逐关节特征提升，分层融合深度信息以化解遮挡歧义；3）设计锚点-特征交互解码器，将3D锚点与提升后的特征融合为统一的锚点查询，编码关节锚集合、视觉线索与几何深度；最终用锚点查询执行锚到关节的集成预测。

Result: 在人类三大基准（Human3.6M、MPI-INF-3DHP、3DPW）上取得优于SOTA的性能，尤其在Human3.6M的困难条件下相对误差降低14.7%，并有定性可视化验证鲁棒性。

Conclusion: 以3D锚点空间为桥梁的姿态提升框架能有效缓解2D误差传播与自遮挡问题，提升跨数据集与复杂场景的稳健性与精度。

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [137] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: 提出一种面向“模态不完整”场景的有害表情包检测方法：通过为多模态（图像、文本）学习独立投射到共享空间的表示，在文本缺失（如OCR失败）时仍能稳健识别；在两数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中表情包的文本常因OCR等问题缺失，现有多模态检测模型依赖完整文本+图像输入，遇到缺失会显著掉点；需要一种对缺失模态鲁棒的检测框架，降低对文本的过度依赖。

Method: 提出新基线：对每种模态分别投射（独立编码）到一个共享表示空间，学习跨模态的对齐/融合，使任一模态可单独产生可比对的语义表示；在推理时若某模态缺失，利用现存模态的共享表示完成判别。

Result: 在两个基准数据集上，当文本缺失时，该方法性能超过现有方法；同时显示更好地利用视觉特征，降低了对文本的依赖。

Conclusion: 首次系统研究模态不完整下的有害表情包检测，所提共享空间基线在文本缺失场景更稳健，推动其向真实世界落地（模态缺失情况下仍有效）。

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [138] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: 提出LightCity，一个含多种复杂光照（多光源、间接光与阴影）的高质量合成城市数据集，用于推进城市场景逆渲染及相关任务评测。


<details>
  <summary>Details</summary>
Motivation: 城市场景逆渲染在自动驾驶与数字孪生中关键，但复杂光照（多光照、间接光与阴影）让本征分解与3D重建困难，且缺少能刻画这些因素的数据集与基准，阻碍了系统性研究。

Method: 构建LightCity数据集：超过300种可控天空光照；街景与航拍多尺度视角，累计5万+图像；提供深度、法线、材质分解、直接光与间接光等丰富标注。基于该数据集对城市环境中的三项基础任务进行基准测试与系统分析。

Result: 获得一个高质量、光照可控且包含间接光/阴影效应的城市合成数据集，并完成对三项基础任务的全面基准评测与分析。

Conclusion: LightCity为在复杂光照下的城市逆渲染、本征分解与3D重建研究提供了缺失的数据基础与评测平台，有望显著推动相关领域发展。

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [139] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: 提出 Koo-Fu CLIP：在白化后的嵌入空间上用 Fukunaga–Koontz LDA 做线性投影，使类间更可分、类内更紧，同时降维与压缩；在 ImageNet 上显著提升原生 CLIP 的监督分类表现，并实现10–12×压缩几乎不掉点。


<details>
  <summary>Details</summary>
Motivation: 原始 CLIP 表征虽通用，但用于有监督分类时：类间分离不足、维度过高、推理存储与检索成本大。因此需要一种轻量、可解释、可闭式求解的适配方法，既增强判别性又降低维度与计算。

Method: 对 CLIP 图像嵌入先进行白化处理；在该空间中应用 Fukunaga–Koontz 线性判别分析（FK-LDA），构造抑制类内方差、强化类间差异的投影矩阵；得到闭式线性映射，将原始嵌入变换到判别性更强且维度更低的“Koo-Fu CLIP”空间；使用最近原型分类进行评估。

Result: 在 ImageNet-1K 上，最近原型分类的 top-1 从75.1%提升到79.1%；在14K与21K类的扩展标签空间中仍有一致增益；在10–12倍降维压缩下几乎无精度损失，支持大规模分类与检索。

Conclusion: 简单闭式的线性投影即可有效重塑 CLIP 几何结构，显著提升监督分类可分性并实现高比率压缩，作为一种轻量高效的 CLIP 适配方案适用于大规模分类与检索场景。

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [140] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: 提出Corruption Restoration Transformer（CRT），作为可插拔的视觉变换器，在不微调VLA本体的情况下恢复被传感器级腐蚀破坏的视觉输入，从而显著提升VLA在真实世界图像扰动下的稳健性。


<details>
  <summary>Details</summary>
Motivation: VLA在受控环境表现优异，但在真实部署中对视觉扰动（尤其是传感器层面的图像腐蚀，如电子噪声、坏点、镜头污渍）极其脆弱；现有研究多关注几何遮挡，忽视传感器腐蚀这一关键失效模式。需要一种无需昂贵微调即可提高鲁棒性的通用方案。

Method: 设计CRT：一个可插拔、与模型无关的Vision Transformer，位于感知输入前端，将受损图像映射回“干净”观测。通过对抗式训练目标学习从腐蚀到清洁的恢复，无需对下游VLA（如π_{0.5}、SmolVLA）进行再训练或微调。

Result: 在LIBERO与Meta-World基准上，作者量化表明主流VLA在常见信号腐蚀下成功率由约90%骤降至最低约2%。加入CRT后，可显著恢复性能，在严重腐蚀场景下仍接近无腐蚀的基线成功率。

Conclusion: 传感器级图像腐蚀是VLA在现实部署中的关键短板。CRT作为前端恢复模块能在不修改VLA的前提下大幅缓解该问题，使VLA在多种严重腐蚀下保持接近基线的表现，提升了通用机器人操控的实际可用性。

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [141] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

TL;DR: 提出一个结合遥感影像与多模态大语言模型的无人机紧急迫降点评估框架，通过语义分割预筛选与视觉-语言推理结合POI检测细微风险，构建ELSS基准并验证优于几何基线，且能给出可解释理由。


<details>
  <summary>Details</summary>
Motivation: 传统基于几何的迫降选址仅关注地形平坦性，无法识别语义层面的动态或临时风险（如人群、活动设施），难以满足安全与可解释性的实战需求。

Method: 采用粗到细两阶段：1) 轻量级语义分割对遥感图像进行快速预筛，得到候选安全区；2) 视觉-语言推理代理融合图像视觉特征与POI数据，识别细粒度语义风险并给出文本化解释。并构建公开ELSS数据集与评测基准。

Result: 在ELSS基准上，该框架在风险识别准确率上显著优于仅用几何信息的基线；定性分析显示能生成接近人类、可解释的决策理由。

Conclusion: 融合RS影像与MLLM的上下文感知策略可更可靠地评估无人机迫降点，兼顾准确性与可解释性，优于传统几何方法；所发布的ELSS数据集支持后续研究复现与比较。

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [142] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出EEmoDB与EEmo-Logic，构建百万级图像诱发情绪问答与细粒度评估数据，并通过指令微调+GRPO优化的MLLM，在域内外基准上取得领先的情绪理解与评估表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型在图像诱发情绪理解上多为粗粒度或推理能力不足，难以覆盖多维属性与强度细节，限制机器共情与人机交互应用。

Method: 1) 构建EEmoDB数据集：涵盖5个分析维度与5类任务；从12.5万图像自动生成120万QA对（EEmoDB-QA）；从2.5万图像人工精修得到3.6万细粒度评估数据（EEmoDB-Assess）。2) 训练EEmo-Logic：在多模态大模型基础上进行指令微调，并提出任务定制的组相对偏好优化（GRPO）与新型奖励设计，实现统一式情绪理解与推理。3) 进行广泛实验与跨域评测。

Result: EEmo-Logic在域内与跨域数据集上均取得稳健表现，在情绪QA与细粒度评估任务上表现突出，优于现有方法。

Conclusion: 大规模、多维度的EEmoDB与结合GRPO的新型训练范式显著提升了MLLM对图像诱发情绪的全面理解与推理能力，为机器共情与人机交互应用提供可靠基础。

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [143] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出CurriSeg：结合课程学习与反课程学习的双阶段训练框架，用于解决上下文-内容纠缠的分割任务（如伪装目标检测），在不增加参数或训练时长的前提下显著提升鲁棒性与泛化。


<details>
  <summary>Details</summary>
Motivation: 生物学习从易到难强化感知与鲁棒性。CECS场景中，目标与背景共享内在视觉模式，传统分割多靠结构改进，忽视在纠缠分布下影响鲁棒性的学习动力学，因此需要一种能在数据难度推进与挑战中权衡的训练范式。

Method: 双阶段框架CurriSeg：1）Curriculum Selection阶段，基于样本损失的时间统计动态选择数据，区分“难但有信息”的样本与噪声/模糊样本，稳定提升能力；2）Anti-Curriculum Promotion阶段，提出Spectral-Blindness Fine-Tuning，通过抑制高频成分，迫使模型依赖低频结构与上下文线索，从而增强泛化。

Result: 在多种CECS基准上持续优于现有方法，且无需增加参数或总训练时间。

Conclusion: 将课程与反课程原则统一，可在上下文纠缠的分割任务中提升表示可靠性与泛化；提出的CurriSeg提供了关于“循序渐进+刻意挑战”如何共同塑造鲁棒、具上下文感知分割的系统性视角。

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [144] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: 提出EMFormer与配套微调与损失设计，提升长时段天气预报的准确性与稳定性，并显著降低计算开销与推理时间，同时在视觉任务上具备良好泛化与速度优势。


<details>
  <summary>Details</summary>
Motivation: 现有长时段天气预报方法依赖微调延长预测，但面临灾难性遗忘、误差累积与训练开销大，难以兼顾短期精度、长期一致性和效率。

Method: 1) 设计Efficient Multi-scale Transformer（EMFormer），以单次卷积实现多尺度特征提取，统一用于训练与推理；2) 提出累积式上下文微调（accumulative context finetuning），在不牺牲短期精度的前提下增强时间一致性与长程建模；3) 复合损失采用正弦加权，动态平衡不同损失项，在预训练与微调阶段自适应引导优化路径。

Result: 在天气与极端事件预测上取得显著提升，尤其是长时段预测精度大幅提高；与传统多尺度模块相比，EMFormer在ImageNet-1K与ADE20K上表现出良好泛化，并实现约5.69倍加速。

Conclusion: 该管线在预训练—微调—预测全流程提升长程建模能力并降低计算成本，为长时段天气预报提供更稳定高效的方案，且具备跨视觉任务的通用性与效率优势。

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [145] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 提出Med3D-R1：面向3D医学影像的视觉-语言模型，通过两阶段（SFT+RL）训练与可解释性奖励，提升临床推理与诊断一致性，在CT-RATE与RAD-ChestCT上达SOTA（41.92%、44.99%）。


<details>
  <summary>Details</summary>
Motivation: 3D体数据高维复杂、模型易过拟合报告表面模式、缺乏鼓励可解释推理的奖励，导致现有3D医影多模态模型临床推理薄弱、可解释性与可靠性不足。

Method: 两阶段框架Med3D-R1：1) SFT阶段引入残差对齐，将高维3D特征与文本嵌入对齐；并通过“异常重加权”策略突出临床关键信号、降低报告结构偏置。2) RL阶段重设计“一致性奖励”，显式鼓励连贯、逐步的诊断推理，提升答案与推理过程的匹配与可解释性。

Result: 在3D诊断VQA基准CT-RATE与RAD-ChestCT上达到SOTA：41.92%与44.99%准确率，表现出更好的异常识别与临床推理一致性，优于既有方法。

Conclusion: 通过对齐机制、异常重加权与可解释性导向的RL奖励，Med3D-R1提升了3D医影VLM的诊断可靠性与透明度，显示其在真实诊断工作流中的应用潜力。

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [146] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 提出TRA框架，在点监督TAL中引入文本描述，含PTR与PMA两模块，经多模态对齐与对比学习提升定位效果，且计算开销可在单张3090上运行。


<details>
  <summary>Details</summary>
Motivation: 现有点监督时序动作定位方法只用视觉特征，忽略文本语义信息，限制了定位精度与鲁棒性；需要一种低成本地引入语义线索的方法以增强特征表达与跨模态一致性。

Method: 1) 用预训练多模态模型为视频帧生成描述文本；2) PTR：结合点标注与多种预训练模型对初始描述进行去噪/纠错与精炼；3) PMA：将视觉与文本特征投射到统一语义空间，进行点级别多模态特征对比学习，缩小跨模态差距；4) 将增强后的多模态特征送入动作检测器做精确定位；并在五个基准上评测与计算开销分析。

Result: 在五个常用基准上优于多种SOTA点监督方法；同时框架能在单张24GB RTX 3090上运行，显示出良好的实用性与可扩展性。

Conclusion: 引入文本描述并通过PTR与PMA实现精炼与对齐，能在点监督TAL中显著提升定位性能且保持可负担的计算成本，验证多模态语义对增强时序动作定位的有效性。

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [147] [OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth](https://arxiv.org/abs/2602.01268)
*Jaehyeon Cho,Jhonghyun An*

Main category: cs.CV

TL;DR: 将零样本单目深度（相对尺度）与少量稀疏测距数据进行标定，作为伪度量先验，再用跟随先验且可必要偏离的精炼网络，实现少样本条件下稳定、边界清晰的度量深度估计与补全。


<details>
  <summary>Details</summary>
Motivation: 单目基础模型虽强，但仅输出相对深度，无法直接用于需要绝对尺度的机器人/自动驾驶；现实中度量标注稀缺且难以获取，需要一种用极少标注即可得到可部署的度量深度的方法。

Method: 1) 观察相对深度能保留全局布局与边界；2) 以少量稀疏测距对相对深度做尺度与偏置标定，得到伪度量深度先验；3) 设计精炼网络：在先验可靠处跟随，在不可靠处偏离，融合先验与图像特征，输出度量深度；4) 以极少带标定样本进行训练/适配。

Result: 在缺乏精心验证数据与少样本情境下，系统仍能维持稳定尺度与清晰边缘，相比直接使用相对深度或传统少样本方法得到更准确的度量深度与补全表现。

Conclusion: 将强大的单目基础先验与稀疏度量“锚点”结合，是在真实世界标注稀缺下获得鲁棒、可部署深度补全的有效路径。

Abstract: Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.

</details>


### [148] [Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.01273)
*Xun Zhang,Kaicheng Yang,Hongliang Lu,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出Q-DiT4SR：面向DiT的实超分PTQ框架，借助分层SVD与方差感知的时空混合精度分配，在W4A6/W4A4下达成SOTA，并在W4A4实现5.8×模型压缩与>60×算力节省，同时保局部纹理质量。


<details>
  <summary>Details</summary>
Motivation: DiT在真实场景超分中生成纹理强但推理开销大；现有PTQ方法多针对U-Net或文生图DiT，直接套用会破坏局部纹理，缺乏针对超分DiT的专门量化方案。

Method: 提出Q-DiT4SR：1) H-SVD（分层SVD），在相同参数预算下将全局低秩分解与局部块级rank-1分支结合，兼顾全局与局部纹理表达；2) 方差感知的时空混合精度：VaSMP在无数据条件下依据率失真理论进行跨层权重量化比特分配；VaTMP在扩散时间步内以动态规划在层内分配激活精度，使用极少校准数据。

Result: 在多真实世界数据集上，于W4A6和W4A4量化设置均达SOTA。W4A4实现模型大小缩减5.8×、计算量>60×下降，同时保持高质量局部纹理。

Conclusion: 面向DiT的Real-ISR量化框架Q-DiT4SR有效缓解推理负担并维护纹理细节；其H-SVD与方差感知时空混合精度策略为扩散式超分量化提供通用可扩展思路。

Abstract: Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.

</details>


### [149] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: 提出一种利用交通流信息增强车道线感知的模块TFM，可实时、低成本提升在遮挡/缺失场景下的检测效果，在NuScenes上带来最高+4.1% mAP增益。


<details>
  <summary>Details</summary>
Motivation: 视觉车道检测在遮挡、车道线缺失等弱视觉线索场景性能下降；高清地图虽可辅佐，但成本高、实时性差，需寻找低成本且实时的信息源。

Method: 引入交通流作为新信息源，设计TrafficFlow-aware Lane perception Module（TFM）：从实时交通流中提取特征，并与现有车道感知算法无缝融合；在公开算法与数据集上实现与验证。

Result: 在四个主流模型、两个公开数据集（NuScenes、OpenLaneV2）上，通过标准评测指标验证，TFM对性能均有稳定提升，在NuScenes上mAP最高提升+4.1%。

Conclusion: 交通流是有效且零额外成本的先验与上下文信息源；将其与现有车道感知结合能显著增强鲁棒性与精度，尤其在视觉线索不充分的场景。

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [150] [DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction](https://arxiv.org/abs/2602.01278)
*Zhengbo Zhang,Yihe Tian,Wanke Xia,Lin Chen,Yue Sun,Kun Ding,Ying Wang,Bing Xu,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出DSFC-Net：结合CNN与空间-频率混合Transformer（SFT），通过跨频交互注意（CFIA）与通道特征融合模块（CFFM）提升乡村道路提取的连通性与精度，在WHU-RuR+、DeepGlobe、Massachusetts上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 乡村道路在高分辨率遥感中存在材料多样导致类内差异大、与背景可分性低、植被遮挡打断连续性、道路狭窄等问题；现有多为面向结构化城市场景的方法，难以处理这些特性。

Method: 构建双编码框架DSFC-Net：1）CNN分支捕获细粒度边界和短程连续性；2）空间-频率混合Transformer（SFT）建模全局拓扑关系，抗遮挡；其中CFIA基于拉普拉斯金字塔显式解耦高/低频并实现跨频交互，缓解注意力的频率偏置；3）通道特征融合模块CFFM自适应重标定通道响应，融合局部纹理与全局语义；实现端到端分割。

Result: 在WHU-RuR+、DeepGlobe、Massachusetts数据集上进行全面实验，DSFC-Net在各项指标上超过现有最优方法，表现出更好的道路连通性和细窄道路检测能力。

Conclusion: 融合空间与频域信息、跨频交互与通道自适应融合，有效缓解乡村道路提取中的遮挡与窄路断裂问题，提升连通性与精度；方法具有鲁棒性并可推广至复杂场景的细目标提取。

Abstract: Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.

</details>


### [151] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出并验证“跨语言共享安全神经元（SS-Neurons）”，少量但关键，能共同调控多语言安全拒答，并据此提出只微调该子集的训练策略，在显著提升低资源语言安全性的同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 多语言安全性不均衡：高资源语言安全较强，非高资源语言薄弱；同时，尽管存在跨语表示迁移，但安全对齐的神经机制不清晰，缺乏可解释和可操控的方法来提升NHR语言的安全。

Method: 1) 在单语环境中识别“单语安全神经元（MS-Neurons）”，通过定向激活/抑制验证其对安全拒答的因果作用；2) 在跨语比较中从MS-Neurons里找出跨语共享子集SS-Neurons，证明其在高/非高资源语言间迁移安全能力的桥梁作用；3) 观察性与干预性实验：抑制SS-Neurons导致多NHR语言安全同时下降，增强则提高一致性；4) 设计基于语言资源分布和模型结构的“神经元定向微调”策略，仅针对SS-Neurons进行微调。

Result: 实验证明：微调极小的SS-Neurons子集即可超过现有SOTA多语言安全方法，在显著提升NHR语言安全性的同时，保持模型总体能力与跨语防御一致性。

Conclusion: LLMs内部存在可识别、可干预的跨语安全神经元子集。围绕SS-Neurons进行轻量、定向训练能够有效把高资源语言的安全能力迁移到低资源语言，实现多语言安全的高效提升且不损伤通用性能。

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [152] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LiP-Map提出将3D线与平面片联合建模与优化，把3D线视为有限平面片的边界，实现高效（3–5分钟/场景）、精确且完整的3D线映射，并提升基于线的视觉定位表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D线重建多依赖稀疏特征或成对共面约束，难以同时兼顾精度、完备性与结构一致性；而人造环境中线往往是平面边缘，利用平面拓扑应能提升线重建与定位。

Method: 提出LiP-Map：显式学习与优化可微的线与平面原语，构建线-平面耦合的联合优化框架，不以成对共面约束为主，而是通过显式的平面与线交互来注入拓扑结构；基于多视RGB输入进行全局优化与重建。

Result: 在ScanNetV2、ScanNet++、Hypersim、7Scenes、Tanks&Temple等100+场景上，相比SOTA提升线重建的精度与完备性；在7Scenes上显著提升线辅助视觉定位性能；总体重建时间约3–5分钟/场景。

Conclusion: 将平面拓扑显式引入3D线映射能带来结构化、准确且高效的重建，并为基于线的定位提供显著增益；LiP-Map为人造环境的结构化重建提供了可复制的路线。

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [153] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: 提出ICOR任务：移除目标物体及其交互痕迹；给出REORM框架，利用多模态大模型推理哪些元素需一并删除，配合掩码引导与自纠错；并发布ICOREval基准，实验优于现有编辑系统。


<details>
  <summary>Details</summary>
Motivation: 传统图像移除通常只删“被点名的物体”，却保留光影、接触/连接、目标产生物、语境关联等交互线索，导致语义不一致和视觉破绽。需要一个能理解并处理这些交互依赖的系统。

Method: 定义ICOR任务；提出REORM：以MLLM进行多阶段推理分析交互元素→生成/细化联合移除掩码→图像编辑执行→自我纠错回路（检测残留交互痕迹并迭代修复）；提供模块化设计和本地部署变体，适配资源受限环境；构建ICOREval数据集，基于指令驱动、含丰富交互依赖的案例进行评测。

Result: 在ICOREval上，REORM相较现有图像编辑SOTA取得更优的交互一致性与总体编辑质量指标，能更全面地清除目标与其交互痕迹。

Conclusion: 将对象移除扩展为“交互一致”的联合清除更贴近实际需求；借助MLLM的推理与自纠错，REORM在多场景下实现更可靠的一致性编辑，并通过ICOREval促进后续研究。

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [154] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: ReDiStory 是一种推理时、无需训练的多图视觉故事生成方法，通过重组提示词嵌入以减少跨帧语义干扰，从而在不改动扩散模型参数的前提下提升身份一致性并保持语义忠实。


<details>
  <summary>Details</summary>
Motivation: 现有多帧故事生成需同时维持角色身份一致与每帧的场景语义，但把“身份+帧语义”简单拼接会导致帧间语义互相干扰，尤其在复杂剧情中破坏身份保持。需要一种无需额外训练或监督、在推理阶段即可缓解干扰的方案。

Method: 在推理阶段对文本嵌入进行显式分解与重组：将文本嵌入分为“身份相关成分”和“帧特定成分”，并对多帧的帧特定嵌入做去相关处理（抑制各帧共享的方向），以降低跨帧共享语义对身份嵌入的影响；整个过程不改动扩散模型参数、无需额外监督。

Result: 在与相同的扩散骨干与推理设置下，相比 1Prompt1Story，ReDiStory 在 ConsiStory+ 基准上的多项身份一致性指标取得一致提升，同时保持对每帧提示的忠实度。

Conclusion: 通过推理期的提示嵌入重组与去相关，ReDiStory 有效削弱跨帧语义干扰，显著提升多帧故事的身份一致性而不牺牲语义保真，且无需训练或修改模型。

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [155] [StoryState: Agent-Based State Control for Consistent and Editable Storybooks](https://arxiv.org/abs/2602.01305)
*Ayushman Sarkar,Zhenyu Yu,Wei Tang,Chu Chen,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: StoryState 提出一个在训练无关的文生图流程之上加入“显式可编辑故事状态”的编排层，用少量LLM代理维护角色表、全局设定与逐页场景约束，以纯提示词驱动生成与编辑，实现细粒度多页编辑和一致性提升，接近一键式系统的一致性，同时减少回合与时间。


<details>
  <summary>Details</summary>
Motivation: 一键生成绘本流程（用户给简述→得到多页插画）在编辑与一致性上存在痛点：故事的“隐式状态”（角色、世界观、逐页物体）不可直接操控，导致编辑粗粒度、容易破坏跨页视觉一致性。需要一种无需模型再训练、可显式建模与编辑故事状态的机制。

Method: 提出 StoryState：以结构化对象表示故事（角色表、全局设定、逐页场景约束），并用少量LLM代理维护和更新该状态，再将其转译为1Prompt1Story风格的提示词，用于训练无关的文本到图像后端生成与编辑；全程通过提示词操作，保持模型无关性，兼容多种生成后端。

Result: 在系统级多页编辑任务上，相比1Prompt1Story，StoryState实现本地化页面编辑，提升跨页一致性，减少非预期变化、交互回合和编辑时间；一致性效果接近Gemini Storybook的一次性生成水平。

Conclusion: 显式故事状态+代理式编排能在不改动底层生成模型的前提下，实现更可控、细粒度的多页编辑与一致性管理，为多模态故事生成提供通用、可扩展的无训练框架。

Abstract: Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState

</details>


### [156] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: 提出DeCorStory：在无需训练的前提下，通过在推理阶段对跨帧提示嵌入去相关与重加权，并结合身份保持的跨注意力，显著提升文本到图像故事生成的跨帧一致性与语义对齐。


<details>
  <summary>Details</summary>
Motivation: 训练自由的一次性拼接多帧提示（如One-Prompt-One-Story）会造成嵌入强相关，导致颜色泄露、背景混叠、角色身份漂移等跨帧干扰问题，亟需在不改模型与不微调的条件下抑制跨帧语义干扰。

Method: 在扩散推理阶段：1) 对帧级提示嵌入做Gram-Schmidt正交化以去相关；2) 对奇异值进行重加权以强化帧特异语义；3) 在扩散过程的跨注意力中引入身份保持机制以稳固角色身份。整体为无训练、无结构改动、可即插即用的管线。

Result: 在多数据/场景上，较现有训练自由基线取得更好的提示-图像对齐、身份一致性与视觉多样性，达到同类SOTA；可与现有扩散管线无缝集成。

Conclusion: 通过推理期的嵌入正交化与权重调整并配合身份稳固的注意力，可有效降低跨帧语义干扰，实现更稳健的故事生成一致性与质量，无需额外训练或模型修改。

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [157] [FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching](https://arxiv.org/abs/2602.01329)
*Divya Jyoti Bajpai,Shubham Agarwal,Apoorv Saxena,Kuldeep Kulkarni,Subrata Mitra,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: 提出FlowCast：一种无需训练的推理加速框架，通过常速外推在流匹配生成中跳过冗余步骤，实现>2.5×加速且无质量损失。


<details>
  <summary>Details</summary>
Motivation: 流匹配（FM）在视觉生成质量高但推理慢，需要大量去噪步，难以实时/交互应用。现有加速（蒸馏、截断、一致性训练）要么降质、要么昂贵重训、或泛化差，迫切需要无需训练且通用的高效加速方法。

Method: 基于FM“恒定速度”训练目标，提出FlowCast：在每步以当前速度对未来速度进行外推得到“投机”更新；若与真实更新的MSE低于阈值则接受，从而跳过冗余稳定区间步数，在复杂区域保留精细步。无需辅助网络、可插拔、适用于任意FM。并给出理论分析，界定投机轨迹与完整FM轨迹的最坏偏差上界。

Result: 在图像生成、视频生成与编辑任务上，FlowCast实现超过2.5倍加速，相比完整生成无质量下降，并优于现有加速基线。

Conclusion: 利用FM的恒速性质进行无训练投机外推，可在不牺牲质量的前提下显著减少推理步数、加速多种视觉生成任务，具有通用性与理论保证。

Abstract: Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.

</details>


### [158] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: 论文提出MED框架，用于区分视觉工具使用RL带来的收益与模型内在能力变化，发现性能提升主要来自内在学习；工具使用RL更多在减少工具带来的伤害而非真正掌握工具纠错。


<details>
  <summary>Details</summary>
Motivation: 现有视觉工具使用RL能显著提升VLM表现，但不清楚提升源于更好用工具还是模型本身能力变化。需要一个方法定量分离工具效应与内在能力变化，并分析工具收益与伤害的构成与演化机制。

Method: 提出MED（Measure-Explain-Diagnose）粗到细框架：1）Measure：在不同训练检查点与多基准上测量带工具与不带工具的性能，分离内在能力变化与工具诱发效应；2）Explain：将工具诱发的性能差分解为gain（工具带来的纠错收益）与harm（工具调用引入的错误、模式干扰等）；3）Diagnose：诊断不同损益项随训练演化的机制，如调用错误减少、工具图式干扰减弱、但对内在失败的工具性纠错有限。使用两种具有不同工具先验的VLM与六个基准进行对比与检查点级分析。

Result: 跨两类VLM与六个基准，性能提升主要由内在能力学习驱动；工具使用RL显著降低工具诱发的harm（更少的调用错误、更弱的工具模式干扰），但仅带来有限的gain（对内在失败的工具性纠错有限）。

Conclusion: 当前视觉工具使用RL更多学会与工具安全共存、减少副作用，而非真正掌握并利用工具来纠正模型内在失败。需要新的方法侧重提升工具的主动纠错与掌握能力。

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [159] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 提出视觉隐喻迁移（VMT）任务与多智能体框架，通过“模式语法”抽象与跨域重映射，实现从参考图抽离创造性逻辑并注入到目标主体，显著优于现有生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型多停留在像素对齐与表观保持，难以把握隐喻所需的抽象逻辑与跨域语义融合。需要一种能从参考中抽离“创造性本质”，并在新主体上重物化该逻辑的机制。

Method: 提出基于概念混融理论（CBT）的认知启发式多智能体系统：1）感知代理将参考图蒸馏为“模式语法”（G），把关系不变量与具体实体解耦；2）迁移代理保持泛化空间不变，发现合适载体完成跨域对齐；3）生成代理进行高保真合成；4）分层诊断代理像专业评论家闭环回溯，逐级纠错（抽象逻辑、部件选择、提示编码）。

Result: 在大规模实验与人工评价中，本方法在隐喻一致性、类比恰当性与视觉创造性上显著超越SOTA基线。

Conclusion: 通过“模式语法”与多代理协作，有效实现视觉隐喻的抽象逻辑解耦与跨域重映射，为广告与媒体等高影响创意应用铺路，并将开源代码。

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [160] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: 提出将固定压缩率的连续VAE转化为支持多级时间压缩的模型，通过最小微调缓解高压缩率下性能下降，并验证其与扩散式生成模型（如DiT）的兼容与联合训练效果。


<details>
  <summary>Details</summary>
Motivation: LVDM需要VAE在时间与空间上高效压缩视频，但在不增加通道维度的前提下叠加更多采样层会导致高压缩率下效率和质量明显下降，因此需要一种既能提升/调整时间压缩率又能保持性能的简单方法。

Method: 将原固定压缩率的连续VAE转换为多级时间压缩VAE：在时间维度上引入可切换的多级压缩机制（通过额外的采样/下采样层或门控路径等）并采用最小量的微调以适配不同压缩等级；系统性分析不同压缩级别对不同视频片段（运动强度、纹理复杂度等）的影响；将该多级时间压缩VAE与DiT类扩散生成模型对接，实现联合训练与无缝兼容。

Result: 在更高的时间压缩率下，相较直接堆叠采样层的方法，所提最小微调策略显著减轻了重建/生成质量的下降；在不同特性的视频片段上，展示了不同压缩级别的性能差异与优势；证明了与DiT等扩散模型的并行训练与集成可行且有效。

Conclusion: 多级时间压缩为视频VAE提供了灵活的压缩-质量权衡，能在高压缩率下保持较好性能并与扩散生成模型兼容，具有在实际视频生成与压缩场景中的应用潜力。

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [161] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: NOVA 是一种无需再训练的视觉自回归(VAR)加速框架，通过熵分析自适应地减少推理中的视觉token，显著降低计算开销且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有VAR加速方法在token裁剪上依赖启发式阶段划分、固定/非自适应调度，且加速范围有限，无法捕捉建模动态随时间/层级演化的特性，导致潜在加速空间被浪费。熵随时间的变化可刻画预测不确定性的转变，为动态加速提供可解释信号。

Method: 提出NOVA：在推理中在线监测“尺度熵”(scale entropy)的增长，识别拐点以自适应决定启动加速的尺度；通过尺度联动与层联动(linkage)策略为不同尺度与层计算不同的token裁剪比例；基于熵阈值裁剪低熵token，并复用前一尺度残差缓存以保持上下文与质量，从而在不改变模型参数的前提下加速VAR推理。

Result: 在多项实验与消融分析中，NOVA在无需训练的条件下实现显著加速，同时基本保持或仅轻微影响生成质量；相较既有启发式或固定调度方法在效率与效果上取得更优的综合表现。

Conclusion: 利用熵变化来刻画VAR推理动态，可在不改动模型的情况下自适应地进行token裁剪与缓存复用，NOVA提供了一种简单有效、可广泛应用的训练免加速方案。

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [162] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: 提出T2M Mamba，通过耦合“周期性-关键帧显著性”建模与跨模态鲁棒对齐，显著提升文本生成三维人体动作的稳定性与一致性，在HumanML3D与KIT-ML上取得SOTA（FID 0.068）。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作方法虽能生成高保真动作，但存在两点痛点：1）将动作的周期性与关键帧显著性割裂建模，导致长序列生成漂移；2）对语义等价的文本改写敏感，词汇同义替换会扭曲文本嵌入并传播至解码器，产生不稳定或错误动作。

Method: 提出T2M Mamba，包含两大设计：1）“周期性-显著性感知”Mamba框架（PSA-Mamba）：利用改进的密度峰聚类估计关键帧权重，并用FFT加速的自相关估计动作周期性，以低开销捕获两者耦合动态；2）构建“周期微分跨模态对齐模块”（PDCAM）：在文本与动作嵌入间进行鲁棒对齐，引入基于周期性的差分信号以抑制同义改写引起的嵌入扰动。

Result: 在HumanML3D与KIT-ML上进行大量实验，整体指标均有一致提升，FID达0.068，并在其他评估指标上获得稳定增益。

Conclusion: 耦合周期性与关键帧显著性的时序建模并结合鲁棒跨模态对齐，可有效缓解长序列漂移与语义改写脆弱性，提升文本到动作生成的稳定性与质量。

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [163] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: 提出面向视频MoE模型的时间Lipschitz引导攻击与对抗训练框架，分别针对路由器与专家模块的独立与协同脆弱性进行攻击与防御，并在提升鲁棒性的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击多把MoE视为整体，忽视路由器与专家模块在时间维度上的独立与协同弱点，导致对脆弱性的认识与防御不足。

Method: 1) 设计针对路由器的时间Lipschitz引导攻击（TLGA），度量并利用时间维的Lipschitz敏感性暴露路由器独立弱点；2) 提出联合攻击J-TLGA，同时扰动路由器与专家以放大对抗效应，揭示协同弱点（Achilles' Heel）；3) 基于此提出联合时间Lipschitz对抗训练（J-TLAT），在训练中联合强化路由器与专家的稳健性；4) 框架可即插即用，且保持MoE稀疏激活特性以降低推理开销。

Result: J-TLGA相较单独攻击显著增强攻击成功率并更全面地破坏MoE；J-TLAT在多数据集与多架构上稳定提升对抗鲁棒性，同时相较稠密模型降低超过60%的推理成本。

Conclusion: 视频MoE存在组件级别的独立与协同脆弱性。以时间Lipschitz为导向的联合攻击与联合对抗训练能有效揭示并缓解这些弱点，实现稳健性提升与高效推理的兼得。

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [164] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen提出用多种架构的生成模型共同构建合成视觉-语言数据，并配合程序化“困难负样本”课程，以结构多样性替代单一模型扩规模，显著提升多任务与组合性基准表现。


<details>
  <summary>Details</summary>
Motivation: 单一生成器扩规模带来谱偏差与特征多样性不足，限制视觉-语言预训练效果；需要一种能覆盖更广流形、减少模型特有伪影、强化细粒度语法理解的合成数据构建范式。

Method: 提出PolyGen：1) Polylithic多源生成——在多种架构的生成器交集上取样，边训练边“边缘化”掉各自的模型伪影，提升流形覆盖与特征多样性；2) Programmatic Hard Negative课程——通过程序化构造细粒度语法与组合性冲突的难负例，强制模型学习精确的句法/语义对齐；3) 在固定数据预算下，从“更多独特标题”重分配为“多源变体”，形成结构化多样数据。

Result: 在综合多任务基准上相比单源最强基线SynthCLIP提升+19.0%，在SugarCrepe++组合性基准上提升+9.1%，表明结构多样性带来更稳健、更具组合性的一般化能力。

Conclusion: 相较于单一来源的样本数量扩张，结构多样性（多架构生成+难负例课程）是一条更高效的数据扩展规律，可缓解生成器特有偏差、提升视觉-语言模型的鲁棒性与组合性理解。

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [165] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

TL;DR: 提出PromptRL：在流匹配模型(FM)的强化学习环中引入可训练语言模型作为提示重写代理，以提升样本效率并缓解提示过拟合；在T2I与大规模编辑任务上达SOTA，用更少rollouts取得更高上限。


<details>
  <summary>Details</summary>
Motivation: 现有针对FM的RL存在两大问题：1) 生成多样性不足导致样本效率低；2) 提示过拟合，模型记忆训练表述，对语义等价但风格变化的提示性能崩塌。需要一种既能提升探索多样性又能改善泛化的策略。

Method: 在RL优化闭环中引入可训练的语言模型作为提示重写器：LM根据奖励信号学习重写/精炼原始提示，从而引导FM产生更丰富且对奖励更敏感的样本。该协同训练改变优化动力学：LM快速学会高质量改写，FM在多样且稳健的提示分布上被强化。评估于T2I与大规模图像编辑（如FLUX.1-Kontext）场景。

Result: 在生成评测上取得SOTA：GenEval 0.97、OCR准确率0.98、PickScore 24.05。在编辑任务上，用0.06M rollouts将EditReward从1.19提升到1.43，超过Gemini 2.5 Flash Image(1.37)，接近ReasonNet(1.44)，且无需复杂多阶段训练与细粒度标注。整体上较单纯flow-only RL，所需rollouts减少超过2倍并达到更高性能上限。

Conclusion: 将LM作为提示重写代理嵌入FM的RL环可同时提高样本效率与泛化能力，缓解提示过拟合并提升多项基准表现；该范式在T2I与图像编辑中均验证有效，显示出相比传统RL更优的性价比与可扩展性。

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [166] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: 提出ALI方法，在图像到图像重光照任务中融合像素对齐编码器与潜在内在表征，并配合自监督精炼，仅用未标注真实成对数据训练，在金属、玻璃等复杂高光材质上显著提升重光照质量。


<details>
  <summary>Details</summary>
Motivation: 现有依赖潜在内在表征的重光照方法约束不足，尤其在金属、玻璃等高反射材质上易失败。直觉上可用更强的语义先验弥补，但作者发现高层语义特征会损害光度保真度，存在语义抽象与光度保真的根本权衡，需要新的表示来平衡二者。

Method: 提出Augmented Latent Intrinsics (ALI)：在潜在内在框架中引入像素对齐的视觉编码器特征，与稠密光度结构融合；并设计自监督的精炼策略以缓解真实成对数据稀缺，仅用未标注真实世界成对图像进行训练。

Result: ALI在图像重光照任务上取得显著提升，特别是在复杂、镜面反射材质上获得最大收益。

Conclusion: 语义抽象与光度保真存在权衡。通过将像素对齐的稠密光度先验与潜在内在表征融合，并配合自监督精炼，ALI在真实数据上实现稳健、优质的重光照效果。

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [167] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出Parabolic Position Encoding（PaPE），一种以抛物线为核心的视觉位置编码，在多种视觉模态与注意力架构中表现优异，具备平移/旋转不变、距离衰减、方向性与上下文感知，跨8个数据集多数最优，并在ImageNet-1K外推中显著领先。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码多从语言的一维序列扩展到视觉的n维结构，但往往仅部分考虑视觉特性（如平移/旋转不变性、距离衰减、方向性与局部上下文），导致在多模态与外推场景下表现受限。

Method: 基于视觉先验设计抛物线位置编码PaPE：以抛物线函数构造相对位置映射，显式满足平移不变、具备旋转不变变体（PaPE-RI），在几何距离上实现可控衰减，同时编码方向与上下文；可用于图像、点云、视频、事件流等视觉token的注意力模块中。

Result: 在4种模态、8个数据集上评测，PaPE或其旋转不变版本PaPE-RI在7/8数据集上获得最佳；在ImageNet-1K外推实验中，相较次优位置编码，绝对提升最高达10.5%。

Conclusion: 抛物线式位置编码更符合视觉几何特征，统一适配多种视觉模态，提供更强的外推与泛化能力；PaPE/PaPE-RI在广泛基准上达SOTA，代码已开源。

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [168] [BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images](https://arxiv.org/abs/2602.01435)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: BioTamperNet提出用于生物医学图像复制篡改检测的新框架，基于SSM近似的亲和引导注意力，实现细粒度、高效的篡改与来源区域联合定位，并在基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像取证方法多在自然图像上训练，面对生物医学图像中微小而关键的复制篡改时表现不佳，影响实验可信度与可重复性，亟需能捕捉细微相似性并高效定位的专用模型。

Method: 提出BioTamperNet：1) 亲和引导自注意力模块用于建模单图像内相似性；2) 亲和引导交叉注意力模块用于建模跨图像（或候选源区域）对应关系；3) 采用受状态空间模型（SSM）启发的线性注意力以降低复杂度并提升长程依赖建模；4) 端到端训练，同时预测篡改区域与其来源区域位置。

Result: 在生物取证基准数据集上，BioTamperNet在复制篡改区域检测与定位精度方面显著超过竞争基线，展现更优的细粒度定位与效率。

Conclusion: 亲和引导的自/交叉注意力结合SSM风格的线性注意力可有效提升生物医学图像复制篡改检测与溯源性能，提供高效、准确的取证工具，代码已开源。

Abstract: We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet

</details>


### [169] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: 论文将驾驶员注视点与道路场景语义进行对齐，比较三类视觉方法（目标检测、分割+分类、VLM查询），发现YOLOv13与大尺寸VLM（Qwen2.5-VL-32b）在宏平均F1>0.84，尤其大VLM在夜间和小目标（如信号灯）上更稳健；分割辅助方法因“局部-整体”语义鸿沟召回差；总结传统检测的实时性与大模型的语境鲁棒性之间的权衡，并为人-感知的驾驶监控系统提供设计指导。


<details>
  <summary>Details</summary>
Motivation: 驾驶安全与人机协同需要理解驾驶员在何时注视何物。将注视点与道路对象语义精确对应，可为下一代ADAS/驾驶员监测提供数据与算法依据，尤其在复杂/夜间场景中识别关键小目标（如红绿灯、行人）至关重要。

Method: 将问题表述为前视摄像头图像上的“注视语义识别”。比较三大路线：1) 直接目标检测（YOLOv13）；2) 分割辅助分类（SAM2生成掩膜，配合EfficientNetV2或与YOLOv13对比）；3) 基于查询的视觉-语言模型（Qwen2.5-VL-7b与Qwen2.5-VL-32b）。以宏平均F1等指标评估，关注小目标与夜间鲁棒性。

Result: YOLOv13与Qwen2.5-VL-32b显著优于其他方法，宏平均F1均超过0.84。大VLM在小且安全关键对象（如交通信号灯）和夜间条件下表现尤佳。分割辅助范式因“局部对整体”的语义错配导致召回率大幅下降。

Conclusion: 存在实时性（传统检测）与上下文理解/鲁棒性（大型VLM）之间的基本权衡。对人感知驾驶监控系统的设计建议：在需要强鲁棒与小目标识别时优先大VLM或与检测器融合；对时延敏感场景优先轻量检测器；谨慎使用分割辅助方案以避免语义颗粒度不匹配的问题。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [170] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: 本文研究小型视觉Transformer在量化（特别是4-bit）下的表现，发现预训练于更大数据集（ImageNet-22k）的模型在ID与尤其OOD任务上更易受量化影响，AUPR-out显著下降；相比之下，仅用ImageNet-1k预训练和采用数据增强可能更有利于保持低比特量化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer需在资源受限与实时应用中运行，量化是关键手段，但已有工作多聚焦于ID性能，忽视了注意力机制在OOD场景下暴露的量化脆弱性。作者动机是系统性评估小型ViT家族在低比特量化下的ID与OOD行为，以揭示影响量化鲁棒性的训练与数据因素。

Method: 选取热门小型ViT变体（DeiT、DeiT3、ViT），比较不同预训练规模（ImageNet-1k vs ImageNet-22k），进行低比特（尤其4-bit）量化与校准；在ID上评估分类表现与稳定性，在多种常用OOD数据集上评估检测指标（如AUPR-out），分析量化导致的性能跌幅（quantization delta）。

Result: ID方面：4-bit模型初始不稳定，特别是从较强的FP32模型（如DeiT3-22k）量化后跌幅大（示例：精度骤降约17%）。OOD方面：ViT与DeiT3在22k预训练下从FP32到4-bit的AUPR-out平均跌幅分别约15.0%与19.2%，而仅1k预训练的对应模型跌幅较小（约9.5%与12.0%）。说明更大规模预训练反而降低低比特量化下的OOD鲁棒性。

Conclusion: 大规模数据集预训练并不必然带来低比特量化下的鲁棒性提升，反而可能在OOD检测上更脆弱；在追求可部署性时，优先考虑更保守的预训练规模与强化数据增强，可能比直接扩大预训练数据更有利于量化后的稳定表现。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [171] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 提出一种Logit Lens Loss（LLL），在不改模型结构和大规模训练的前提下，保持VLM视觉token的局部语义，使Logit Lens热力图可解释且同时提升分割等视觉任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有Logit Lens可在VLM中显示图像token与概念（如“cat”）的对应，但因视觉信息在自回归生成中扩散到文本token，导致图像token的局部语义丢失，从而热力图不再可解释。需要一种方法在训练中约束图像与文本token的混合，保持图像token的局部语义表达。

Method: 在常规下一词预测（NTP）训练目标之外，加入Logit Lens Loss（LLL）：鼓励视觉token嵌入与其对应图像区域的文本描述（如包含“cat”的patch）在语义空间对齐，从而抑制注意力层中过度混合图像与文本token。无需改动架构或大规模再训练。

Result: 加入LLL后，Logit Lens可产生有意义的对象置信热力图；同时在分割等以视觉为中心的任务上，无需额外专用头也获得性能提升。

Conclusion: LLL在不改变模型结构的条件下，保留并强化视觉token的局部语义，使Logit Lens具备实际可解释性，并带来下游视觉任务（如分割）的性能增益。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [172] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

TL;DR: 提出SW-PS特征提取与轻量级LRU分类器的端到端框架，在大角度随机旋转情况下实现在线手写字符高鲁棒识别，并在三类CASIA-OLHWDB1.1子集上取得SOTA级精度与更快收敛。


<details>
  <summary>Details</summary>
Motivation: 在线手写识别虽利用笔画时序和动态信息优于离线识别，但实际应用中的任意旋转会破坏笔画空间布局，显著降低准确率。如何提取对旋转不敏感的判别性特征仍是难点。

Method: 用滑动窗口路径签名（SW-PS）从轨迹中提取局部结构与动态信息，结合线性递归单元（LRU）作为分类器。LRU兼具RNN的增量处理优势与SSM的并行训练效率，可稳定建模笔画时序特性。并采用集成学习提升最终性能。在CASIA-OLHWDB1.1的数字、英文大写、汉字部件子集上对±180°随机旋转样本进行训练与测试。

Result: 在三子集上的集成后识别率分别为99.62%、96.67%、94.33%。与对比模型相比，所提框架在收敛速度更快、测试精度更高。

Conclusion: SW-PS+LRU在强旋转扰动下实现鲁棒在线手写识别，证明了局部路径签名与轻量级状态空间式递归单元的有效结合；适合需要高效并行训练与实时增量推理的场景。

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [173] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: 提出InteractAvatar：用于文本对齐的具身人-物交互说话头像生成的双流框架，解耦环境感知/交互规划与视频合成，结合检测增强感知，联合生成动作与视频并建立GHOI评测基准GroundedInter，显著缓解控制-质量矛盾并取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有说话头像多为简单肢体动作，缺乏与环境中物体的“落地”交互能力；GHOI生成需要同时理解环境与精确控制动作和视频质量，存在感知不足与控制-质量两难。

Method: 提出双流框架InteractAvatar：1) 感知与交互模块PIM，利用目标检测增强环境感知，依据文本生成对齐的人-物交互动作；2) 音频-交互感知生成模块AIM，合成包含物体交互的生动说话头像视频；3) 设计motion-to-video对齐器，使PIM与AIM结构相似并可并行协同生成动作与视频，从而缓解控制-质量矛盾；4) 构建GHOI评测基准GroundedInter。

Result: 在所提出的GroundedInter基准与多种比较中，方法能生成文本对齐、动作合理、视频质量高的具身人-物交互说话头像，优于现有方法。

Conclusion: 解耦感知/规划与合成的双流设计结合检测与并行共生成，可有效提升GHOI说话头像的可控性与真实性；GroundedInter为该任务提供评测基准。

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [174] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

TL;DR: 提出FSCA-Net，通过显式分离共享与专有特征并用跨注意力融合与互信息优化，缓解多数据集联合训练中的负迁移，显著提升跨域人群计数泛化性能。


<details>
  <summary>Details</summary>
Motivation: 多数据集/多场景下人群计数存在显著域差异，直接联合训练会导致共享与域特定表征纠缠，出现负迁移，跨数据集泛化差。

Method: 1) 设计FSCA-Net，将特征显式拆分为域不变与域特定两部分；2) 引入跨注意力融合模块，自适应建模两类特征的交互，实现有效知识迁移且保留数据集判别性；3) 提出互信息优化目标，最大化域不变特征的一致性、最小化域特定特征的冗余，促进共享-私有表征互补。

Result: 在多个人群计数基准上实验，FSCA-Net有效缓解负迁移，跨数据集泛化达到或超过现有方法（SOTA）。

Conclusion: 显式特征分离+跨注意力融合+互信息正则的统一框架，可在多域场景下实现稳健、可扩展的人群计数，适合真实世界部署。

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [175] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: 提出“认知超感知”训练范式：在多模态大模型中加入潜在视觉意象预测（LVIP）头，学习视觉认知潜变量序列并与答案对齐，形成类人视觉内在推理链；再用强化学习基于该视觉潜变量优化文本推理路径。构建CogSense-Bench评估五类认知能力。实验显示在该基准与跨域数理/科学VQA上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽然在开放词汇感知任务上强，但在需要抽象视觉细节与视觉记忆的复杂认知任务上乏力。多数方法把CoT扩展在文本空间，忽略类似人类视觉空间工作记忆和视觉意象的机制，导致语言单通道难以进行清晰、结构化推理。

Method: 引入Cognitive Supersensing训练范式：1) 在MLLM上新增LVIP头，联合学习“视觉认知潜变量”序列并与最终答案对齐，作为基于视觉的内部推理链；2) 基于该对齐的视觉潜变量进行RL阶段，优化文本CoT路径，使文本推理受“内在视觉”约束与引导；3) 构建CogSense-Bench，覆盖五个认知维度的VQA评测。

Result: 采用该范式训练的MLLM在CogSense-Bench上显著超过现有SOTA，并在域外数学与科学VQA上具备更强泛化能力。

Conclusion: 引入可学习的内在视觉意象与视觉对齐的推理链，有望弥合感知识别与认知理解之间的差距；所提基准与权重将开源，促进后续研究。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [176] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出CLEAR框架，首次系统解决屏摄图像中摩尔纹与条纹闪烁的联合去除，并配套大规模数据集与仿真管线，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 屏幕被手机拍摄常见，但因摩尔纹与条纹闪烁同时存在且强耦合，现有针对单一退化的方法在复合场景下失效，亟需统一建模与恢复方案。

Method: 1) 构建含摩尔纹与条纹闪烁的大规模数据集；2) 设计基于ISP的闪烁仿真管线，稳定训练并扩展退化分布；3) 提出统一恢复框架CLEAR：包含频域分解-重组模块以分离/重构复合伪影，并引入轨迹对齐损失以更好建模耦合退化。

Result: 在多项评价指标与复杂真实场景上，CLEAR一致性优于现有图像复原方法，取得最好性能。

Conclusion: 联合建模摩尔纹与条纹闪烁是必要且可行；通过数据、仿真与频域模块及对齐损失的结合，可显著提升屏摄图像质量。

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [177] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: 提出MUN基准与R-ICL框架，评测并提升多模态模型在非常规/反直觉情境下的常识推理，借助MER检索异质匹配，实现小模型从大模型迁移，较基线ICL平均提升8.3%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在偏离常规的图文情境（罕见、反常、文化多样）中常识推理薄弱，缺乏系统评测与方法来提高对低频、非典型场景的鲁棒性与适应性。

Method: 1) 构建MUN基准：将视觉场景与“出人意料”的自然语言结果配对，要求模型在不协调的图文下进行合理化或发现非常规解释。2) 提出检索式上下文学习R-ICL：利用多模态集成检索器MER，在图文故意不一致的情况下也能找出语义相关的示例；用这些示例做ICL，把大模型的推理范式迁移给小模型，无需额外训练。

Result: 在多种设置下，相比标准ICL基线平均提升8.3%，证明R-ICL在低频、非典型情境下有效。

Conclusion: MUN为评测与推动视觉-语言模型在非原型、文化多样、现实世界反常场景中的鲁棒性与适应性提供新方向；R-ICL与MER展示了通过检索驱动的示例迁移来增强小模型常识推理的可行性。

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [178] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: 提出一种单步扩散的图像压缩方法，在保持与最新扩散压缩相当性能的同时，将解码推理速度提升约46倍。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式图像压缩在低码率下有很好的感知质量，但需要多步去噪解码，导致推理延迟大、计算开销重，难以实际部署。

Method: 将扩散解码过程设计为单步扩散生成，大幅减少去噪步骤；同时在判别器层面不在像素域而在紧凑特征域进行对抗训练，以更好捕捉高层纹理与结构；整体在压缩框架中结合单步扩散解码与特征域判别器进行端到端训练。

Result: 在与近期扩散压缩方法相当的率失真/感知表现下，实现约46×更快的推理速度；代码与模型开源（OSDiff）。

Conclusion: 单步扩散+特征域判别器能在不显著牺牲压缩性能的情况下大幅降低推理成本，为扩散式图像压缩的实际应用提供可行路径。

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [179] [SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01574)
*Haobo Wang,Weiqi Luo,Xiaojun Jia,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出SGHA-Attack：通过多参考语义引导与分层对齐，显著提升对黑盒VLM的定向迁移攻击效果，并对预处理/净化防御保持鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有定向迁移攻击易在代理模型的最终层语义空间过拟合，只用单一参考、忽视中间层与跨模态中间语义，导致对异构VLM迁移性差。

Method: 1) 参考生成与筛选：用冻结的文本到图像模型根据目标提示生成参考图像池；在代理模型下选取Top-K语义最相关的锚点并加权混合，提供稳定优化目标。2) 分层语义对齐：在多深度层次对中间视觉表示做全局与空间粒度对齐，将目标语义注入特征层级。3) 早期跨模态监督：在共享潜在子空间同步中间视觉与文本特征，于最终投影前提供跨模态一致性约束。

Result: 在多种开源与商用黑盒VLM上，较现有方法实现更强的定向迁移成功率；对图像预处理与净化防御保持较强鲁棒性。

Conclusion: 多参考语义引导与中间层分层对齐能有效缓解代理过拟合，提升对异构VLM的定向迁移与抗防御能力，优于依赖单参考和仅终层对齐的方法。

Abstract: Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

</details>


### [180] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出HandMCM：基于Mamba的对应建模与多模态特征融合方法，提升在严重遮挡下的3D手部关键点估计精度，三大基准显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D手势关键点估计对AR等HCI应用至关重要，但手部自遮挡与手物交互遮挡导致关键点拓扑关系学习困难、鲁棒性不足。需要一种能在多种遮挡场景下建模动态运动学拓扑并增强输入表征的方法。

Method: 构建HandMCM：以状态空间模型Mamba为骨干，引入局部信息注入/过滤与“对应建模”模块，学习关键点间的动态时空关联与运动学拓扑；同时融合多模态图像特征，提升对遮挡的鲁棒性与表征能力。

Result: 在三个基准数据集上，HandMCM在总体指标与严重遮挡子集上均显著优于当前SOTA，显示更强的准确性与稳定性。

Conclusion: 通过将Mamba的序列建模优势与对应关系学习相结合，并配合多模态特征融合，HandMCM有效缓解遮挡带来的估计难题，为实际场景中的3D手势估计提供更高准确性与可靠性。

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [181] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: 提出TAFS-GRPO：在流匹配文本到图像模型中，通过温度退火的少步采样与组相对策略优化，提升少步生成质量与人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的流匹配少步生成需较多去噪步且奖励稀疏、噪声大，导致对齐不足与训练低效，需要一种能在极少步采样下获得稳定、密集奖励并保持语义的优化框架。

Method: 1) 温度退火少步采样：从单步样本出发，迭代注入自适应时序噪声并退火，增加随机性同时保持语义一致性；2) 步感知优势整合：将组相对策略优化（GRPO）用于不可微奖励，按采样步骤提供密集、特定步的优势信号，稳定策略更新；整体训练将上述采样与优势估计闭环迭代，面向流匹配生成器。

Result: 在少步文本到图像生成上取得强性能，相比现有方法显著提升与人类偏好的对齐度；实验广泛验证了效率与稳定性提升（代码与模型将开源）。

Conclusion: TAFS-GRPO通过温度退火采样与步感知GRPO，为流匹配模型提供高效、稳定的少步RL优化路径，在保持语义的同时显著增强人类偏好对齐，适合推进少步T2I生成研究与应用。

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [182] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出纯Mamba架构SOD模型Samba与其多任务版Samba+，在多模态/视频SOD任务上以更低计算开销取得SOTA。关键是为SOD重构Mamba扫描策略（SNS）、上下采样对齐（CAU），以及多模态融合与持续学习方案（HGA、MACL）。


<details>
  <summary>Details</summary>
Motivation: CNN感受野受限、Transformer计算复杂度高，不利于同时获得全局建模与效率；现有SOD方法往往“任务专用”，难以统一处理多模态/多场景（RGB、RGB-D、RGB-T、视频等），且在持续适配中易遗忘与跨模态冲突。

Method: 1) 纯Mamba骨干：以状态空间模型获取长程依赖与线性复杂度。2) Saliency-Guided Mamba Block（SGMB）：提出空间邻域扫描（SNS）以保持显著区域的空间连续性。3) Context-Aware Upsampling（CAU）：在解码阶段建模上下文依赖，促进层级特征对齐与聚合。4) Samba+：多任务联合训练，构建统一多模态/多场景SOD。包含：a) Hub-and-Spoke Graph Attention（HGA）进行自适应跨模态交互融合；b) Modality-Anchored Continual Learning（MACL）缓解跨模态冲突与灾难性遗忘，实现任意模态输入与持续适配。

Result: 在6类SOD任务、22个数据集上，Samba单模型即超过现有方法且计算成本更低；Samba+以单一训练的多任务模型在同样任务与数据集上进一步提升表现；附加实验显示框架的可扩展潜力。

Conclusion: Mamba为SOD提供了兼具全局感受野与效率的可行途径。通过SNS与CAU改进特征建模与重建，结合HGA与MACL实现统一的多模态/持续学习方案，Samba/Samba+在广泛SOD场景中达到更优精度-效率权衡与更强泛化。

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [183] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: 提出UV-M3TL框架，结合DB-SCME与AFD-Loss，在AIDE与多数据集上实现多任务SOTA并缓解任务间负迁移。


<details>
  <summary>Details</summary>
Motivation: ADAS需要同时理解驾驶员行为/情绪、车辆行为与交通环境，但多模态多任务联合学习常出现任务冲突与负迁移，导致整体性能受损。

Method: 设计统一多模态多任务框架UV-M3TL，含两核心：1) 双分支空间-通道多模态嵌入（DB-SCME），显式建模任务共享与任务特定特征，增强跨任务知识迁移并缓解冲突；2) 自适应特征解耦多任务损失（AFD-Loss），基于学习动态的自适应权重与解耦约束，稳定联合优化并鼓励多样表示学习。

Result: 在AIDE数据集上四个任务（驾驶员行为、驾驶员情绪、车辆行为、交通语境）均达SOTA；在BDD100K、CityScapes、NYUD-v2、PASCAL-Context等多任务感知基准上也取得持续强劲表现，多数任务达SOTA。

Conclusion: UV-M3TL能在多模态多任务场景中有效缓解负迁移并稳定优化，通过DB-SCME与AFD-Loss实现通用、强性能的联合感知与理解，具备良好跨数据集与任务组合的通用性。

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [184] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

TL;DR: 提出ToPi：一种无需训练的DiT上下文内生成Token剪枝方法，通过离线灵敏度标定选关键注意力层、计算上下文token影响力并随扩散步长自适应更新，实现>30%推理加速且保持结构与视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 上下文内生成需要把参考图像与目标潜变量拼接，导致序列极长，计算成本高。现有token压缩方法多为文本到图像场景，且对不同角色token一刀切，未考虑参考上下文与目标潜变量在空间、时间、功能上的不对称性，效果欠佳。

Method: 提出ToPi（Training-free token Pruning for in-context generation in DiTs）：1）离线校准驱动的灵敏度分析，识别对性能敏感的关键注意力层，作为冗余估计代理；2）在这些层上定义新的影响力指标，度量每个上下文token对生成的贡献，据此选择性剪枝；3）结合扩散过程的时间维度，引入随步数变化的动态更新策略，适应扩散轨迹演化；全流程无需额外训练。

Result: 在复杂图像生成任务中，相比无剪枝的基线，ToPi在保持结构保真与视觉一致性的前提下，实现超过30%的推理加速。

Conclusion: 针对DiT的上下文内生成，ToPi以训练无关、分层与时序自适应的token剪枝，有效缓解长序列计算瓶颈，在不牺牲质量的情况下显著提升推理效率。

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [185] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: 提出Omni-Judge：用具备音频-视频-文本理解能力的omni-LLM来评估文本驱动的音视频生成；在语义对齐相关指标上与传统指标相当甚至更优，但在高帧率感知（画质、音画同步）上受限于时间分辨率；提供可解释反馈以支持迭代优化。


<details>
  <summary>Details</summary>
Motivation: Sora 2、Veo 3等模型能从文本直接生成同步音视频，但评估三模态输出仍困难：人工评测昂贵难扩展；自动指标多为两两模态（FVD、CLAP、ViCLIP），对复杂提示不鲁棒、可解释性弱。omni-LLM天然处理多模态并具备推理与可解释链路，可能成为统一评测器。

Method: 构建Omni-Judge框架，利用omni-LLM对文本条件下的音视频生成进行打分与解释。在九个感知与对齐指标上评估其与人类和传统指标的相关性，重点考察音-文对齐、视-文对齐、音-视-文一致性以及高FPS相关的画质与音画同步。提供链式推理式解释以暴露语义或物理不一致。

Result: Omni-Judge与传统自动指标总体相关性相当；在语义要求高的任务（音-文、视-文对齐及三模态一致性）上表现更佳；在视频质量与音画同步等高时间分辨率感知指标上逊色，揭示omni-LLM的时间建模不足。

Conclusion: omni-LLM可作为多模态生成的统一、可解释评测器的有力候选，尤其适合语义与对齐评估；但需提升时间分辨率与感知能力以覆盖高FPS画质与同步等维度。Omni-Judge可用于提供可操作的反馈，辅助生成模型迭代优化。

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [186] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: PISCES 提出一种无需人工标注的文本生成视频后训练方法，通过“双重最优传输（OT）对齐奖励”提升视频质量、时序一致性与文本语义对齐，并在VBench和人偏好上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2V后训练要么依赖大量人类偏好数据，成本高且难扩展；要么使用与人类判断不一致的预训练多模态嵌入，导致监督信号偏差与性能受限。需要一种既无需标注又能更贴近人类偏好的奖励设计。

Method: 设计Dual OT-aligned Rewards：1) 分布式OT对齐的质量奖励，跨文本与视频嵌入的分布层面衡量整体视觉质量与时序一致性；2) 离散token级OT对齐的语义奖励，约束文本token与视频token的语义与时空对应。该奖励可用于多种优化范式（反向传播、RL微调）。

Result: 在短视频与长视频生成上，PISCES在VBench的Quality与Semantic指标均超过基于标注与免标注的对比方法；人类偏好实验也验证其优越性。

Conclusion: 通过OT在分布与token两层面对齐文本-视频表示，可在无需标注的情形下提供更可靠的奖励信号，统一适配多种后训练优化方式并显著提升T2V质量与语义对齐。

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [187] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: 论文指出当前世界模型研究碎片化，主攻单一任务（视觉预测、3D、符号落地等），缺乏统一框架。作者批判这种割裂做法，并提出一套统一设计规范：世界模型应规范性地整合交互、感知、符号推理与空间表示，以引导构建更通用、稳健、原则化的模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法将世界知识注入孤立任务虽能提效，但难以支撑整体性的世界理解与跨任务泛化；缺失统一定义与框架阻碍了系统化进展。因此需提出规范化设计以弥合碎片化研究。

Method: 不是具体算法，而是立场与框架性工作：系统分析当前任务导向方法的局限，提出世界模型的统一设计规范/组成要素，并主张将交互、感知、符号推理、空间表示以规范化方式耦合。

Result: 理论与规范层面的产出：给出一个统一视角和设计规范，用于评价与指导世界模型构建；强调从“能力清单”转向“规范性框架”。

Conclusion: 世界模型应作为整合交互、感知、符号推理和空间表征的统一规范，而非松散能力集。该规范有望引导未来研究走向更通用、稳健且可原则化的世界建模。

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [188] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 提出一种结合动态自适应焦点损失（DAFL）与客户端感知聚合的联邦学习框架，用于在数据异质与类别不平衡条件下训练ViT等模型，在三大医学影像数据集上显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 深度模型（特别是ViT）需要大量数据，但医疗等领域因隐私限制难以集中汇集；同时，各客户端数据异质且类别极度不平衡，导致全局模型泛化差。需要一种在不共享数据前提下兼顾类别不平衡与客户端差异的联邦训练方案。

Method: 1) 动态自适应焦点损失（DAFL）：根据每个客户端的样本量与类别分布，动态调整类别不平衡系数与焦点参数，使少数类获得更大权重并抑制易分类样本的主导。2) 客户端感知加权聚合：在联邦聚合阶段，引入随客户端数据规模与数据特征（如分布差异/难度）变化的权重，提升对异质客户端的适配能力。3) 在ViT及主流CNN/混合架构上进行对比与消融。

Result: 在ISIC、Ocular Disease、RSNA-ICH三个公共数据集上，相较DenseNet121、ResNet50、ViT-S/16、ViT-L/32、FedCLIP、Swin Transformer、CoAtNet、MixNet，多数情况下取得最佳结果，准确率提升约0.98%–41.69%；在不平衡的ISIC上，消融显示DAFL与客户端感知聚合均带来稳定增益，优于传统损失与常见FL方法。

Conclusion: 面向隐私受限与分布异质的医学影像场景，结合DAFL与客户端感知聚合的FL框架能有效缓解类别不平衡与客户端差异，显著提升ViT等模型的联邦分类性能；方法通用、可扩展，代码已开源。

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [189] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 论文提出 ReCALL 框架，解决把生成式多模态大模型改造为单向量检索器时导致的能力退化问题，通过“诊断-生成-精炼”流程挖掘盲点、生成纠错指令与训练三元组并进行质量控制与对比学习训练，最终在 CIRR 与 FashionIQ 上取得 SOTA。


<details>
  <summary>Details</summary>
Motivation: 将生成式 MLLM 适配为组合图像检索（CIR）的判别式单嵌入检索器时，出现范式冲突：单向量匹配弱化了模型原生的细粒度多模态推理，导致检索表现与推理能力退化。需要一个通用方法既保留/恢复生成式模型的细粒度理解，又让其在检索场景中表现强。

Method: 提出 ReCALL 框架，包含三步：1) 诊断：自导向的信息性样本挖掘以定位检索器的“认知盲点”；2) 生成：用基础 MLLM 的链式思维（CoT）提示生成纠错指令与图文三元组，并用基于 VQA 的一致性过滤进行质量控制；3) 精炼：采用分组式对比学习，对这些三元组进行持续训练，内化细粒度视觉-语义区分，并让判别式嵌入空间与 MLLM 的组合推理对齐。方法对底座模型无关。

Result: 在 CIRR 与 FashionIQ 上进行广泛实验，表明 ReCALL 能稳定修复因检索适配造成的细粒度推理退化，并达到最新最优（SOTA）表现。

Conclusion: 通过诊断盲点、生成高质量纠错样本并持续对比精炼，ReCALL 缓解了生成式 MLLM 向判别式检索器适配时的范式冲突，实现能力再校准与性能提升，具备模型无关性与可推广性。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [190] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 提出CaCoVID：一种基于贡献感知的视频Token压缩方法，通过强化学习直接优化选择对正确预测贡献最大的Token组合，降低推理计算开销并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM推理成本高，因视频token冗余；主流压缩以注意力分数为依据，但注意力强度与对正确答案的真实贡献并不一致，导致压缩次优。

Method: 1) 贡献驱动：构建强化学习框架，训练策略网络选择能最大提升正确预测的token组合，将压缩从“保留高注意力”转为“主动搜索最优组合”。2) 组合优化：提出在线组合空间采样的组合策略优化算法，显著缩小token组合探索空间并加速收敛。

Result: 在多种视频理解基准上进行大量实验，显示在显著降低计算量的同时保持或提升性能；代码将开源。

Conclusion: 基于贡献的token选择优于基于注意力的启发式，CaCoVID通过RL与高效组合采样，实现高效且有效的视频LLM推理。

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [191] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: 提出一个针对视频中人体密集预测的时序一致性方案：用可扩展的拟真合成数据（含深度/法线/掩码与运动对齐序列）+ 统一ViT预测器（人体几何先验与通道重加权）+ 两阶段训练（静态预训+动态序列监督），在THuman2.1与Hi4D达SOTA并泛化到野外视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法单帧精度高但在运动、遮挡与光照变化下易闪烁，且缺乏跨多密集任务的人体视频配对监督数据；需要既能提供空间标签又能提供时序监督的数据与模型。

Method: 1) 构建可扩展的高拟真人体合成数据流水线，输出逐帧精确标签（深度、法线、掩码）与运动对齐的视频序列；2) 训练统一的ViT密集预测器：引入CSE嵌入显式人体几何先验；在特征融合后加入轻量通道重加权模块以增强几何相关特征稳定性；3) 两阶段训练：先用静态数据学稳健空间表示，再以序列监督优化时序一致性。

Result: 在THuman2.1与Hi4D数据集上取得SOTA，同时对真实“野外”视频具有良好泛化与稳定的时序一致性。

Conclusion: 利用合成序列级监督与几何先验的统一架构，可显著减少人体密集预测的时序闪烁，并在多数据集上验证有效性与泛化能力。

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [192] [Moonworks Lunara Aesthetic II: An Image Variation Dataset](https://arxiv.org/abs/2602.01666)
*Yan Wang,Partho Hassan,Samiha Sadeka,Nada Soliman,M M Sayeef Abdullah,Sabit Hassan*

Main category: cs.CV

TL;DR: Lunara Aesthetic II 是一个公开、合伦理来源的高美学图像数据集，含 2,854 组“同一身份+上下文变化”的配对图像，用于评估与训练生成/编辑模型的上下文一致性与身份保持；在维持高审美质量的同时提供可解释的关系式监督，适合基准测试、微调与稳健性分析。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成与编辑系统在改变光照、天气、视角、构图、色调、情绪等上下文因素时，常难以稳定保持主体身份，同时缺乏具备高美学质量且带有“身份保持+上下文变化”明确监督的公开数据集，导致难以系统评估与改进模型的上下文泛化与编辑稳健性。

Method: 从 Moonworks 原创艺术与摄影中构建“锚点-变体”配对：每对图像在保持同一基础身份的前提下，施加不同的上下文变换（光照、天气、视角、构图、色调、情绪等）。将“身份保持的上下文变化”形式化为监督信号，并保留 Lunara 系列的高美学评分。数据以 Apache 2.0 开源，面向基准、微调与分析使用。

Result: 数据集展示出高身份稳定性、强目标属性实现能力以及优于大规模网络数据集的稳健美学特征，在上下文一致性评估与训练中表现出有效性与实用价值。

Conclusion: Lunara Aesthetic II 为图像生成与图像到图像系统提供了可解释、关系式的监督信号，可用于评测上下文泛化、身份保持与编辑稳健性，并以高美学质量与开源许可支持社区基准与微调实践。

Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.

</details>


### [193] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: 论文评估将NetVLAD用于SLAM中的回环检测，提出细粒度Top-K精确率-召回曲线，并在KITTI上对比DBoW；结合Faiss近邻检索，NetVLAD在保持实时性的同时显著提升准确性与鲁棒性，成为可直接替代的LCD方案。


<details>
  <summary>Details</summary>
Motivation: 传统DBoW在外观变化与感知混淆场景下性能下降，而深度VPR虽更鲁棒但被认为难以实时；需要系统性验证在SLAM回环检测中深度描述子的可行性与收益，并改进评估指标以匹配多/零真值匹配的LCD特性。

Method: 在KITTI上把NetVLAD作为LCD模块，与DBoW进行对比；提出“细粒度Top-K精确率-召回曲线”用于更贴合LCD的评价；使用Faiss加速的近邻搜索保证查询速度，实测其实时性与准确性。

Result: NetVLAD在KITTI上的准确率与鲁棒性均优于DBoW；借助Faiss实现了实时级的查询速度；新评估曲线更准确反映多真值或无真值情形下的LCD表现。

Conclusion: 深度VPR（NetVLAD）在Faiss加速下可作为SLAM回环检测的“即插即用”替代方案，相比DBoW兼具更高准确性与对外观变化/混淆的鲁棒性，新提出的评估指标更符合实际LCD需求。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [194] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: 提出VRGaussianAvatar：仅用头显追踪信号，实时在VR中渲染单张图重建的全身3D高斯点云化身，并通过双目批处理加速立体渲染，用户研究显示外观相似度、具身感与可信度更高。


<details>
  <summary>Details</summary>
Motivation: 当前VR化身多依赖多摄像头/全身追踪或网格视频驱动，难以在消费级头显上实时、逼真、立体高分辨率呈现由单张图重建的全身形象；同时立体双目渲染存在重复计算成为性能瓶颈。

Method: 系统采用前后端并行管线：VR前端用逆运动学由HMD信号估计全身姿态并传送与双目相机参数；GA后端以3D Gaussian Splatting渲染由单张图重建的化身。提出Binocular Batching，将左右眼视图在一次批处理内联合计算，减少冗余并适配高分辨率VR显示。

Result: 通过定量性能测试与被试内用户研究，相比基于图像/视频的网格化身基线，系统保持交互级VR帧率，并在主观评估中获得更高的外观相似度、具身化与可信度评分。

Conclusion: VRGaussianAvatar能在仅用头显追踪的条件下实时提供高质量全身3DGS化身立体渲染；双目批处理显著提升效率，整体在性能与体验上优于传统网格基线，具备实用价值并已开源。

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [195] [SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking](https://arxiv.org/abs/2602.01677)
*Yinchao Ma,Dengqing Yang,Zhangyu He,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 提出SMTrack：一种基于状态空间模型的视觉跟踪时序建模范式，利用选择性状态感知结构实现长程时序依赖，训练线性复杂度、跟踪时通过隐状态传播降低计算，达到高效且鲁棒的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN/Transformer在视觉跟踪中建模长程时序依赖存在固有限制：要么需定制复杂模块，要么计算成本高，难以在动态场景下保持鲁棒。需要一种既能高效建模长时依赖又易于训练与推理的时序机制。

Method: 引入状态空间模型（SSM）的新范式SMTrack：1）提出“选择性状态感知”空间模型，采用按状态（state-wise）参数化以捕获更丰富的时序线索；2）在训练阶段通过SSM实现线性复杂度的长程时序交互；3）在推理/跟踪阶段采用隐状态的传播与更新，使当前帧能与历史已跟踪帧交互而无需显式堆叠高成本时序模块。

Result: 在多项基准上做了广泛实验，显示SMTrack在保持较低计算成本的同时取得了有竞争力/领先的跟踪精度与鲁棒性。

Conclusion: SMTrack为视觉跟踪提供了一个无需复杂定制模块、具线性训练复杂度且推理高效的时序建模方案，能有效构建长程依赖并在动态场景中表现出色。

Abstract: Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.

</details>


### [196] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

TL;DR: FreshMem是一种面向在线流式视频理解的训练免调优记忆架构，通过频域与空间两路混合记忆，在不牺牲细节的同时保持长期语境连贯，显著提升了Qwen2-VL在多项流式基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在从离线转向在线流式视频时，缺乏灵活的自适应记忆与压缩策略，导致长时序过程中的细节不可逆丢失与上下文割裂，需要一种既能保持短期高保真又能维护长期连贯性的记忆机制。

Method: 提出FreshMem：受启发于大脑的对数感知与记忆巩固，设计两大协同模块。1) 多尺度频率记忆MFM：将溢出的帧映射为代表性的频率系数，并结合残差信息重构全局“要旨”（gist），用于长期语义保留。2) 空间缩略图记忆STM：将连续视频自适应划分为情节簇，通过自适应压缩将其蒸馏为高密度空间缩略图，以保存关键空间细节。二者协同，实现短期保真与长期连贯的权衡。

Result: 在Qwen2-VL基线之上，FreshMem在StreamingBench、OV-Bench、OVO-Bench分别提升5.20%、4.52%、2.34%。作为训练免的方案，其效果超过多种完全微调的方法，且具备高效率与长时程适应能力。

Conclusion: 频域-空间混合记忆为流式视频理解提供了高效且训练免的范式，兼顾细节与长程语境，能够在现有MLLM上即插即用并显著提升性能。

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [197] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

TL;DR: 提出CMAFNet跨模态对齐与融合网络，用RGB+深度以“先净化、再融合”策略，提高输电线路细小缺陷检测，在小目标占94.5%的TLRGBD上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: RGB检测在复杂背景、光照变化下难以从几何相近的构件中分辨细微缺陷；小目标占比高、色彩对比弱导致传统方法性能受限。引入深度几何信息并解决跨模态对齐与融合噪声问题以提升检测鲁棒性与精度。

Method: 提出CMAFNet：1) 语义重组模块（字典/码本驱动的特征净化），抑制模态特有噪声，保留与缺陷相关的判别信息；引入位置归一化与重构约束实现显式跨模态对齐，确保RGB与深度特征统计一致性；2) 上下文语义集成框架，通过部分通道注意力建模全局空间依赖，增强结构语义推理；整体遵循“净化-对齐-融合”的范式，并提供轻量化变体。

Result: 在TLRGBD基准（94.5%为小目标）上，CMAFNet达成32.2% mAP@50、12.5% APs，较最强基线分别提升9.8和4.0个百分点；轻量版仅4.9M参数，228 FPS，mAP@50为24.8，超越所有YOLO系并以更低算力匹配部分Transformer方法。

Conclusion: 跨模态净化-对齐-融合策略有效缓解小目标与复杂背景下的RGB局限，显著提升输电线路缺陷检测的精度与效率；所提模块具备鲁棒性与可部署性，轻量版本在实时性和性能间取得良好平衡。

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [198] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: 提出一个无人工标注的显微图像语义分割框架：用相场模拟生成形貌与掩膜，用CycleGAN把模拟图转为逼真的SEM，再用U-Net仅在合成数据上训练；在真实数据上获得接近SOTA的边界F1=0.90、IoU=0.88。


<details>
  <summary>Details</summary>
Motivation: 显微组织分割对材料高通量表征很关键，但人工标注昂贵且主观、数据稀缺；直接用物理模拟训练存在“域间鸿沟”，导致在真实实验图像上泛化差。需要一种既无需人工标注又能跨域泛化的方案。

Method: 1) 用相场模拟批量生成多样微观组织，同时获得“完美”真值掩膜；2) 采用CycleGAN进行非配对图像到图像翻译，把干净的模拟图转成具有真实纹理、噪声与成像伪影的“逼真SEM”；3) 仅用这些合成(风格迁移后)图像及其掩膜训练U-Net分割器；4) 通过t-SNE特征投影与香农熵统计验证合成数据与真实数据在特征与统计上不可区分。

Result: 在未见过的实验图像上，模型实现Boundary F1=0.90、IoU=0.88；特征分布(t-SNE)与统计(熵)显示合成数据与真实数据同分布；证明了从模拟到现实的域间迁移成功。

Conclusion: 该生成式无标注框架将数据稀缺问题转化为数据充足：完全脱离人工标注，仍能在真实SEM上实现高质量分割，为加速材料发现与分析提供稳健、自动化的解决方案。

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [199] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: FastPhysGS提出一个面向3D高斯点（3DGS）的高速稳健物理模拟框架，结合MPM，通过实例感知粒子填充与双向图解耦优化，实现1分钟内、7GB内存的高逼真动态模拟，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有将3DGS扩展到4D物理的方案要么需要繁琐的手动参数调优、要么依赖视频扩散模型蒸馏，泛化与优化效率差；LLM/VLM驱动方法存在感知鸿沟导致物理不稳，还忽略3DGS表面结构引发不合理运动。因此需要一个同时兼顾几何保真、参数自适应与高效性的物理模拟框架。

Method: 在MPM框架下提出两项关键技术：(1) 实例感知粒子填充（IPF）结合蒙特卡洛重要性采样（MCIS），高效向3DGS内部填充粒子并保持几何保真；(2) 双向图解耦优化（BGDO），将来自VLM预测的材料参数作为初值，利用自适应的双向图结构进行快速稳定的参数优化与解耦更新。

Result: 在仅用约7GB运行内存的条件下，实现约1分钟完成高保真物理模拟；在定量与定性评估上优于现有方法，展现更稳健的物理行为与更好的几何一致性。

Conclusion: FastPhysGS能以低资源高速度实现3DGS的物理一致动态模拟，缓解了VLM感知鸿沟和参数调优难题，并在广泛应用场景中具备优势。

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [200] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: 提出DenVisCoM Mamba块与混合架构，联合估计光流与视差，兼顾实时性、内存与精度，实验显示在多数据集上实现高精度实时表现。


<details>
  <summary>Details</summary>
Motivation: 光流与视差（多视几何与运动）任务本质相关，但现有方法难以在统一模型中同时兼顾实时推理、内存占用与精度。需要面向联合估计的高效架构以满足实际应用。

Method: 设计DenVisCoM（Mamba块）与Transformer注意力相结合的混合架构，面向联合的光流与视差估计；在模型设计上优化实时推理、内存占用与精度，并在多数据集上进行权衡评估。

Result: 在大量数据集上进行基准测试，显示该模型可在实时条件下准确估计光流与视差；提供代码与模型以复现。

Conclusion: 统一的DenVisCoM+Transformer混合架构能在单一模型中实现光流与视差的高精度实时联合估计，兼顾效率与资源使用，具备实践价值。

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [201] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: 用大规模视觉基础模型的冻结特征配合简单线性分类器，在常规与野外场景的AIGI检测上达到或超越专用检测器，尤其在真实分布中显著提升；但在再拍、传输退化、VAE重建与局部编辑下仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 专用AIGI检测器在精心构造的基准上高分，但在真实复杂环境“崩塌”。作者希望找到更稳健、可泛化的方法，并检验基础模型是否因预训练规模和数据多样性而内生具备取证能力。

Method: 冻结现代视觉基础模型（Perception Encoder、MetaCLIP 2、DINOv3）提取特征，训练一个简单的线性分类器作为AIGI检测器；在标准基准、未见过的生成器以及真实“野外”分布上进行系统评估，并分析不同预训练范式（VLM vs SSL）带来的能力来源。

Result: 在标准基准上可匹配最强专用检测器，在野外数据上显著胜出，准确率提升超过30%；VLM表现出对“伪造”语义的显式内部化，SSL模型则隐式学习到可区分的取证线索。

Conclusion: 简单基于基础模型特征的线性分类器即能达到SOTA并在真实环境更可靠；预训练数据中合成内容的广泛暴露使这种能力涌现。但方法在再拍与传输退化、VAE重建以及局部编辑方面仍脆弱。作者呼吁从静态基准的过拟合转向利用基础模型不断演进的“世界知识”，以获得更可靠的AI取证。

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [202] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 提出TAPTQ：面向3D几何学习的尾部感知后训练量化框架，实现更少校准开销与更高精度。核心做法：紧凑校准集构建、对量化区间的三分搜索优化、基于TRE的模块级误差补偿；在VGGT与Pi3上优于SOTA PTQ并显著降低校准时间。


<details>
  <summary>Details</summary>
Motivation: 传统PTQ多针对2D ViT设计，直接迁移到3D模型时碰到两难：3D特征分布更复杂（长尾激活、模块敏感性强），且校准数据获取昂贵（3D数据规模与标注难度高），导致精度下降与部署耗时高。需要一种既节省校准成本又能抑制量化误差积累的专门方法。

Method: 1) 渐进式由粗到细的校准集构建：在保证统计纯度与几何代表性的前提下，从全数据中选出极其紧凑的子集用于校准，缓解数据规模瓶颈。2) 将量化区间搜索表述为优化问题，采用三分搜索求解，将复杂度从O(N)降至O(log N)，显著加速部署。3) TRE引导的模块级补偿：用Tail Relative Error度量长尾激活带来的失真，识别对尾部分布敏感的模块并进行有针对性的补偿，降低误差累积。

Result: 在VGGT与Pi3基准上，TAPTQ在精度上稳定优于现有PTQ方法，同时显著缩短校准时间；给出了与SOTA的对比，显示更高准确率与更低校准成本。

Conclusion: TAPTQ为3D几何模型提供了高效、鲁棒的后训练量化方案：通过紧凑校准、快速区间搜索和TRE驱动补偿，解决了3D场景下特征长尾与校准开销大的痛点，实现了更快、更准的部署。代码即将开源。

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [203] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ObjEmbed 是一种面向对象的多模态嵌入模型，把一张图在一次前向中同时编码为全局和多区域（对象）向量，并结合语义相似度与预测的IoU作为最终匹配分数，在视觉指代、局部/全局检索等任务上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入模型擅长图文全局对齐，但在短语-区域级细粒度对齐上效果欠佳，难以同时兼顾语义与定位质量；实际应用（视觉指代、局部检索）需要稳定、可扩展且高效的对象级对齐能力。

Method: 提出ObjEmbed：将图像分解为多个区域嵌入与全局嵌入；每个区域生成两类互补向量——对象嵌入（语义匹配）与IoU嵌入（定位质量预测）。最终对象匹配分数=语义相似度+基于IoU预测的加权或融合。模型一次前向同时编码所有对象与整图，统一支持区域级与图像级任务。

Result: 在18个多样基准上取得优越性能，展示出更强的语义判别力与检索/定位效果，特别在视觉指代与局部检索任务上领先。

Conclusion: ObjEmbed通过“对象嵌入+IoU嵌入”的联合表示，实现高效、通用且精细的图文对齐，统一支持局部与全局视觉理解任务，并在多基准上验证其有效性。

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [204] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: 该论文提出在资源受限的边缘设备上实现车位级智能停车监测：基于距离感知匹配与自适应框划分，实现98.80%平衡准确率与8秒推理延迟，并引入数字阴影与TV盒改造的应用支撑服务以提升可扩展性与可持续性。


<details>
  <summary>Details</summary>
Motivation: 先前方法以区域内车辆计数估算空位数，虽整体准确但无法提供车位级洞察，限制了高阶应用（如精准导航、动态定价、违停检测）；需要在边缘设备上实现可扩展、低成本、可持续的车位级监测与城市级系统集成能力。

Method: 在YOLOv11m（40.5MB）检测的基础上，提出：1）具空间容差的距离感知匹配，将检测到的车辆与预定义车位进行稳健配对；2）对困难车位采用自适应边界框分割（Adaptive Bounding Box Partitioning）以提升定位与匹配鲁棒性；3）系统层面引入数字阴影（可视化停车要素，为数字孪生演进奠基）与基于改造TV盒的应用支撑服务器，实现与云、停车终端、统计机器人间的可扩展通信。

Result: 在边缘设备上达到98.80%的平衡准确率与8秒的推理时间，相较基础YOLOv11m能力得到增强；系统实现车位级占用统计与稳定通信。

Conclusion: 方法在资源受限环境中实现高精度车位级监控，弥补仅计数方法的不足，并通过数字阴影与低成本服务器促进可扩展、可持续的智慧停车与城市应用落地。

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [205] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

TL;DR: Mind-Brush提出一个具代理性的统一生成框架，把“文本到图像”从静态解码器升级为会思考、检索与推理的动态工作流；并发布覆盖实时与知识推理场景的Mind-Bench。实验显示在Mind-Bench上相对Qwen-Image基线实现从0到1的跃迁，并在WISE/RISE等基准上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有T2I多为静态文本到像素的解码器，难以理解隐含意图；统一理解-生成模型虽提升了理解，但在复杂知识推理与动态现实适配上仍受限于静态先验。需要一个能外部检索、工具化推理、适应实时变化的框架。

Method: 提出Mind-Brush：将生成流程设计为“思—研—创”（think-research-create）。- 主动检索多模态证据以锚定分布外概念与实时信息；- 通过推理工具解析与满足隐式视觉约束；- 在统一模型内以代理式工作流协调理解、检索、推理与生成。并构建Mind-Bench，含500样本，覆盖实时新闻、新概念、数学与地理推理等。

Result: Mind-Brush显著提升统一模型能力：在Mind-Bench上相较Qwen-Image基线实现从0到1的能力跃迁；在既有WISE、RISE基准上取得更优成绩。

Conclusion: 将生成从静态解码转为动态、知识驱动的代理式流程，可更好理解隐含意图、处理复杂推理并适应动态世界；Mind-Bench为此类能力提供系统评测。

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [206] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: 提出MagicFuse：在仅有单张降质可见光图像的极端条件下，生成跨谱（含红外语义）的场景表征，视觉与语义表现可媲美多模态融合方法。


<details>
  <summary>Details</summary>
Motivation: 现实中常遇到红外等多模态传感器缺失或失效的情况，传统多模态图像融合依赖多源输入，难以在只具备可见光的恶劣环境继续发挥优势；因此需要从单张可见光中“提炼/生成”跨谱知识与表征，以保留融合带来的鲁棒性和下游可用性。

Method: 提出“单图像融合”概念与MagicFuse框架：1) 基于扩散模型的可见域内知识增强分支，挖掘被遮蔽/退化的可见光场景信息；2) 基于扩散模型的跨谱知识生成分支，学习从可见光到红外的辐射分布模式迁移；3) 多域知识融合分支，将两条扩散流的概率噪声进行整合，并通过连续采样得到跨谱场景表征；4) 施加视觉与语义双重约束，使表征兼顾人类可读性与下游语义决策需求。

Result: 在大量实验中，MagicFuse仅依赖单张降质可见光图像，即可获得与SOTA多模态融合（含红外）方法相当或更优的视觉质量与语义表征性能。

Conclusion: 单图像融合可在无红外等多模态传感器时延续融合优势。MagicFuse通过扩散驱动的跨谱知识生成与多域融合，产生兼顾视觉与语义的跨谱表征，展现出在恶劣条件下的实用价值与潜在推广前景。

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [207] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: 用MEMS-LiDAR获取匿名三维点云，并结合CARLA仿真合成数据，训练人员入侵检测模型；混合数据将AP提升44个百分点、人工标注减少50%，在满足GDPR的同时具备可扩展与高性价比。


<details>
  <summary>Details</summary>
Motivation: 工业室内安全场景亟需可靠的未授权人员检测。现有视觉深度学习方法依赖图像、易受光照遮挡影响且存在GDPR隐私合规问题，同时收集与标注训练数据成本高且易错。

Method: 采用仅采集匿名3D点云的MEMS-LiDAR以规避个人身份信息；用CARLA仿真生成合成LiDAR场景，与少量真实LiDAR数据混合训练；以混合数据减少实采与标注需求并提升检测性能。

Result: 与仅用真实数据训练相比，混合数据训练的模型平均精度提高44个百分点，人工标注工作量减少50%。

Conclusion: 基于MEMS-LiDAR与仿真合成数据的混合训练方案在工业环境下实现高性能、隐私合规的人体检测，具备可扩展、成本效益好，可作为纯真实数据方案的有效替代。

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [208] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: DDP-WM 提出一种将世界模型中的状态演化拆分为“稀疏主动力学”和“背景上下文更新”的高效框架，通过动态定位与跨注意力机制，大幅降低计算开销，同时保持或提升预测精度，在多类机器人任务上实现显著加速与更高控制成功率。


<details>
  <summary>Details</summary>
Motivation: 现有密集Transformer世界模型计算代价高、难以实时部署；而机器人规划需要既高保真又高效率的环境动力学预测，因此需要一种兼顾速度与精度的模型设计。

Method: 提出“可分解动力学预测（DDP）”假设：场景潜在状态的演化是异质的，可分为由物理交互驱动的稀疏主动力学与由上下文驱动的次要背景更新。模型架构包含：高效历史信息处理、动态定位以隔离主动力学区域、以及用于背景更新的跨注意力机制；通过这种解耦与资源重分配，优化推理效率并为规划器提供更平滑的优化景观。

Result: 在导航、精细台面操作、复杂可变形体或多体交互等任务上验证，相较最先进密集模型显著提速并提升性能；在Push-T任务上推理速度约提升9倍，MPC成功率由90%提升至98%。

Conclusion: 基于DDP思想的DDP-WM在保持高保真度的同时显著提升推理效率，验证了动力学解耦与动态资源分配对机器人世界模型的有效性，为高效世界模型的后续研究与实际部署提供了有前景的方向。

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [209] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 提出一种自动从矿洞岩面点云中识别不连续面组的算法：单次滤波分离平面、极向量到笛卡尔的循环变换表示取向、再用层次聚类自适应分组；在真实矿仓数据上，倾角和倾向平均绝对误差约2°，离散度<3°，优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 地下矿洞岩面不连续结构集（节理、层理等）决定稳定性与安全。UAV/移动激光扫描能高效获取点云，但在封闭空间等复杂场景中，自动、稳健、且无需人工设定簇数的结构集识别仍欠缺。现有方法易受噪声/高曲率干扰、在极坐标取向数据上用笛卡尔聚类失真、以及对密度变化敏感。

Method: 1) 单次(single-shot)信号处理式滤波：在一遍处理里抑制噪声和高曲率伪影，同时保留并提取近似平面片区。2) 取向循环变换：将极坐标的倾角与倾向映射到笛卡尔空间，解决角度周期性导致的欧氏聚类失真。3) 层次聚类：在变换后的取向空间自动识别不连续面组，适应不同密度分布且无需预设簇数。4) 以真实矿房(stope)数据验证，并以人工“虚拟罗盘”选取的基准平面与主流自动化方法对比。

Result: 在真实矿房数据上，方法实现最小的平均绝对误差：名义倾角1.95°、倾向2.20°，簇内离散度误差<3°，优于常用自动结构测绘技术。

Conclusion: 所提流程在封闭矿洞岩面点云上能稳健、自动地识别不连续面组，解决了噪声、曲率伪影及取向周期性带来的聚类问题，并在精度上超过现有方法。

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [210] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: 提出STT-LTF：一种统一的时空Transformer，用多尺度空间补丁与长时间序列联合建模，从40年Landsat无标注数据自监督学习，直接预测未来任意时点，在地中海异质景观的长期SITS预测上显著优于传统与现有深度方法。


<details>
  <summary>Details</summary>
Motivation: 地中海等高度异质地区的长期SITS受复杂空间格局、季节性与多年变化交织影响；纯时间或自回归方法易误差累积、难利用空间上下文且面对不规则采样与可变预测跨度性能不足。

Method: 构建Spatio-Temporal Transformer for Long Term Forecasting（STT-LTF）：以统一Transformer同时编码多尺度空间patch与长达20年的时间序列；使用空间掩码、时间掩码与预测视距采样的自监督预训练，从40年Landsat无标注数据学习；融合空间patch嵌入、周期性时间编码与地理坐标；非自回归直接预测任意未来时点，支持不规则时间步。

Result: 基于1984–2024年Landsat实验，下一年预测MAE=0.0328、R^2=0.8412，优于统计模型、CNN、LSTM与标准Transformer；可稳健处理不规则采样与可变预测视距。

Conclusion: 整合空间语境与时间序列的Transformer在异质景观长期预测中具备优势；自监督预训练与非自回归预测减少误差累积并提升泛化，适合快速生态转变场景的长期监测与预报。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [211] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 提出一套针对自回归视频扩散模型推理阶段注意力瓶颈的训练免改进：通过缓存压缩与近邻稀疏化，实现在长时序生成中维持稳定吞吐与近乎恒定显存，并带来5–10倍加速且几乎不损画质。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散在长视频、世界模型与交互式引擎中需要随时间滚动生成，但注意力KV缓存随步数线性累积导致延迟和显存爆炸，限制有效时间上下文与长程一致性。作者观察到注意力存在冗余：跨帧缓存键近似重复，查询/键语义缓慢演化导致大量无效计算，长提示的跨注意力中每帧仅少量token相关。

Method: 提出统一、免训练的注意力框架，包含三模块：1) TempCache：利用跨帧时域对应关系压缩KV缓存，限制缓存增长；2) AnnCA：在跨注意力中用快速近似最近邻选择与当前帧相关的提示token，减少计算；3) AnnSA：在自注意力中为每个查询仅匹配语义相近的键（以轻量ANN实现），实现稀疏化。模块即插即用，兼容现有自回归扩散与世界模型骨干。

Result: 实验证明端到端可达5–10倍加速；在长序列推理中维持稳定吞吐和近乎恒定的峰值显存；视觉质量与基线几乎一致。

Conclusion: 通过识别并利用自回归视频扩散注意力中的时间冗余与语义稀疏性，所提三模块在无需再训练的前提下显著降低注意力计算与显存，解决长时生成中的扩展性瓶颈，同时保持画质与一致性。

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [212] [FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing](https://arxiv.org/abs/2602.01805)
*Menglin Han,Zhangkai Ni*

Main category: cs.CV

TL;DR: 提出FlowBypass：在Rectified Flow框架下，为无训练图像编辑构建从反演到重建的“旁路”轨迹，减少误差累积，兼顾提示对齐与保真。


<details>
  <summary>Details</summary>
Motivation: 训练免编辑效率高但普遍依赖反演-重建轨迹：长轨迹误差累积导致失真，短轨迹又难以满足编辑提示；现有缓解方案多做骨干特征操控，通用性差。需要一种既降低误差又不依赖特征工程、具备模型无关性的解决方案。

Method: 基于Rectified Flow给出反演与重建两条解析轨迹的形式化推导，并据此构造一条直接连接二者的“bypass”轨迹；从推导得到近似旁路公式与其数值解法，实现轨迹间的无缝切换，无需对特征做专用操作或额外训练。

Result: 在多组实验中，FlowBypass相较SOTA无训练编辑方法，能更好地对齐文本提示，同时在与编辑无关区域保持更高的细节保真；总体性能稳定且泛化良好。

Conclusion: 通过在Rectified Flow中构建反演-重建的旁路轨迹，可有效缓解误差累积并提升编辑对齐与保真，且方法不依赖特征操控，具有更强的通用性与实用价值。

Abstract: Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

</details>


### [213] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: 提出LDRNet，一种用于胸部CT大形变配准的无监督深度学习方法，采用粗到细的形变场预测，并引入“refine block”和“rigid block”，在私有数据与SegTHOR上优于传统与主流DL方法且更快。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习配准多聚焦于脑部，胸部CT存在更大形变、复杂背景与重叠区域，现有方法在速度/精度上难以兼顾，亟需面向大形变、快速且稳健的方案。

Method: 采用两阶段粗到细的配准框架：先预测低分辨率粗形变场，再逐级细化。核心组件包括：(1) refine block：在多分辨率上迭代细化形变场；(2) rigid block：从高层特征学习刚性变换矩阵，与形变场协同以稳定大位移对齐。训练为无监督，基于胸部CT数据（私有+SegTHOR），并与VoxelMorph、RCN、LapIRN及传统方法对比。

Result: 在私有数据与SegTHOR数据集上取得SOTA的配准精度，同时推理速度显著快于传统与对比的深度学习方法。

Conclusion: LDRNet针对胸部CT大形变提出粗到细+刚性/非刚性协同的无监督框架，既提升精度又加速推理，适合复杂背景与大位移场景的临床部署。

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [214] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: 提出Guided Progressive Distillation（GPD），用教师渐进引导与频域约束，将视频扩散模型采样步数从48降至6，保持高质量与更简单流程。


<details>
  <summary>Details</summary>
Motivation: 视频扩散生成计算瓶颈在于多步去噪，现有降步法在视频上易显著降质，需要一种既能大幅加速又能保留时空细节与稳定性的方案。

Method: 提出GPD框架：教师模型逐步引导学生模型以更大步长去噪；采用在线生成训练目标以降低优化难度和算耗；在潜空间施加频域约束，强调高频细节与时间一致性，确保细粒度与动态保真。应用于Wan2.1视频扩散模型。

Result: 在VBench上，采样步数由48减至6，仍保持竞争级视觉质量；相较现有蒸馏方法，在流程简洁性与质量保持上具有明显优势。

Conclusion: GPD能显著加速视频扩散生成且保质保真，频域约束与在线目标是关键；为高效视频生成提供更简单有效的蒸馏范式。

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [215] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: VIA-Bench提出用于测试多模态大模型在视觉错觉与异常场景下的鲁棒性，发现当前MLLM在这些非典型输入上普遍脆弱，CoT也难以改善。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评测多基于分布内、常识一致的数据，难以揭示模型在违背常识先验（如视觉错觉、异常）情境下的真实稳健性与人机感知差异。

Method: 构建VIA-Bench基准，覆盖6类视觉错觉与异常（颜色、运动、格式塔、几何与空间、通用错觉、视觉异常）；经人工审校，形成1K+高质量问答样例，要求细致视觉推理；对20+最先进专有、开源与推理增强型MLLM进行系统评测，并分析CoT对鲁棒性的影响。

Result: 多数MLLM在VIA-Bench上显著失分，表现出对错觉与异常的系统性脆弱；CoT并未提升鲁棒性，反而常产生“海市蜃楼”式脆弱推理，在错觉刺激下逻辑崩溃。

Conclusion: 机器与人类感知存在根本分歧；解决感知瓶颈对逼近通用人工智能至关重要。作者将开放数据与代码。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [216] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出一种利用街景图像指导跨国家ADAS/ADS数据采集的方法，通过POI打分挑选代表性地点，在仅使用一半目标域数据下实现与随机采样相当的交通标志检测性能，并证明全国规模处理的成本可行。


<details>
  <summary>Details</summary>
Motivation: 跨国家部署ADAS/ADS时，因法规、道路与视觉规范差异导致感知模型出现域迁移，传统靠上路采集既昂贵又低效，难以快速找到具有代表性的地点，需要一种低成本、可扩展、可复现的数据获取策略。

Method: 以公开街景图像为先导进行“选址—再采集”。提出两种POI评分：1) 基于视觉基础模型特征+KNN的特征距离法；2) 基于视觉-语言模型的视觉归因法。构建“collect-detect”评测协议，利用Zenseact Open Dataset与Mapillary街景配对形成同址数据集，用交通标志检测作为敏感任务检验。

Result: 在交通标志检测上，街景引导的采集以仅使用目标域一半数据，达到与随机采样相当的性能；并给出全国范围成本估算，显示大规模街景处理在经济上可行。

Conclusion: 街景引导的数据采集能够高效、低成本地支持跨国家模型适配，减少目标域标注/采集需求，同时具备可复现的评测流程，适合扩展到更大范围与其他感知任务。

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [217] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: 提出SPIRIT框架，使红外小目标检测可统一处理单帧与视频，并利用轻量物理先验插件适配视觉基础模型，显著提升SOTA。


<details>
  <summary>Details</summary>
Motivation: 红外小目标信号弱、语义少，直接套用以可见光语义为主的VFM会导致：层次特征聚合淹没局部小峰；基于外观的跨帧关联易与杂波误匹配。亟需一种既能弥补红外数据稀缺、又能稳健时空建模的方案。

Method: 提出SPIRIT，一个与VFM兼容的统一框架，包含两个轻量物理启发插件：1) 空间插件PIFR，通过近似秩-稀疏分解抑制结构化背景并增强稀疏目标；2) 时间插件PGMA，在记忆跨注意力中注入来自历史的软空间先验，限制跨帧关联；在无时序时自然退化为单帧推理。

Result: 在多个IRSTD基准上，相比VFM基线一致提升，并取得SOTA表现（文中未给数值，但强调全面优于现有方法）。

Conclusion: 通过物理先验驱动的特征精炼与带先验的记忆注意力，SPIRIT有效弥合VFM与红外小目标的模态鸿沟，实现单/多帧统一、稳健且SOTA的检测性能。

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [218] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: 提出CDG任务与CloDS框架，实现从多视角视频无监督学习布料动力学：先由视频重建几何再训练动力学模型，核心是基于网格的高斯splatting与双位置不透明度调制以应对大形变与自遮挡；实验显示对未见配置也能泛化。


<details>
  <summary>Details</summary>
Motivation: 现有深度方法常需已知物理属性作为监督或输入，限制在未知条件下的应用；布料具有强非线性形变与自遮挡，难以从视觉观测直接学习其动力学。

Method: 定义无监督的“Cloth Dynamics Grounding (CDG)”场景；提出三阶段CloDS：1) 视频到几何的grounding（利用基于网格的高斯splatting）；2) 通过引入“绝对与相对位置”联合建模的双位置不透明度调制，实现2D观测与3D几何的双向映射，缓解大形变与自遮挡；3) 在grounded网格上训练动力学模型。

Result: 在多项实验中，从纯视觉数据即可有效学得布料动力学，并在未见初始/边界配置上保持强泛化。

Conclusion: CloDS在无监督设置下把视频观测与3D布料几何和动力学成功对齐，通过双位置不透明度调制的高斯splatting应对形变与遮挡，开辟了无需物理先验的布料动力学学习路径；代码与可视化已开源。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [219] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 提出WS-IMUBench，对仅序列级标签的IMU弱监督时序动作定位进行系统基准评测，比较多种跨模态弱监督方法在7个IMU数据集上的可迁移性与效果，总结成功与失败因素并给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有IMU行为识别侧重短片段分类，难以捕获真实行为的时序结构；全监督TAL需要昂贵的帧级边界标注，限制规模化研究。因此需要探索仅有序列级标签的弱监督IMU-TAL，并建立可复现的评测基准以系统比较方法、揭示瓶颈与机会。

Method: 不提出新算法，而是从音频、图像、视频领域选取7种代表性弱监督定位范式，迁移到IMU-TAL；在7个公共IMU数据集上进行大规模训练与推理（>3,540次训练、>7,080次推理）；围绕可迁移性、有效性与洞见三问进行量化与定性分析，检视方法稳定性、数据集特性影响与失败模式。

Result: (i) 迁移具模态依赖：时间域方法更稳定，图像启发的proposal式方法在IMU上较不稳；(ii) 在有利数据（动作较长、传感维度高）上，弱监督可具竞争力；(iii) 主要失败来自短时动作、时间模糊与proposal质量不佳。

Conclusion: WS-IMUBench提供可复现模板、数据集与协议，量化了跨模态弱监督方法在IMU-TAL上的上限与不足，并指向未来方向：IMU特化的proposal生成、具边界意识的目标函数、强化时间推理，以推动可扩展的弱监督IMU-TAL研究。

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [220] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: VIBE提出一个以视觉指令为核心的图像编辑基准和评估框架，分三层难度，配合LMM评审指标，系统性测评17个模型，发现商用模型领先但在更高难度任务上显著退化。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑多依赖文本指令，而人类沟通常含视觉要素（如草图、指点），当前缺乏能刻画“视觉指令跟随”的系统基准与客观评估方法。

Method: 构建VIBE基准，设计三层交互层级：指示性定位（deictic grounding）、形态/结构操作（morphological manipulation）、因果推理（causal reasoning），逐层增加复杂度；为每层策划高质量多样化测试样例；提出以大型多模态模型为裁判（LMM-as-a-judge）的评估框架，配套任务特定指标，实现可扩展、细粒度评测；用该框架对17个开源与商用图像编辑模型进行系统评测。

Result: 商用模型已展现早期的视觉指令跟随能力并整体优于开源模型；但随着任务难度提升，所有模型（包括最强者）性能明显下降。

Conclusion: 视觉指令驱动的图像编辑仍处早期阶段，VIBE与LMM评审可作为统一衡量标准；当前系统在高难度层级存在显著短板，提示未来需在跨模态对齐、结构理解和因果推理等方面持续攻关。

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [221] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: 论文系统评估像素级深伪检测器在多模态谣言场景中的作用，发现其独立价值有限，且融入事实核查流程会因“真实性先验”而降效；相反，基于证据的语义与外部检索推理主导性能。


<details>
  <summary>Details</summary>
Motivation: 实际多模态虚假信息往往由图文语义与语境共同构成，而非仅源于图像像素伪造。尽管深伪检测器被集成进自动化事实核查流程，但它们忽略主张层面含义，可能带来误导性的真实性先验。需要系统性评估：像素级信号是否有助于图文主张核验，还是反而削弱基于证据的推理。

Method: 构建两类评测：MMFakeBench 与 DGM4。比较三组系统：(1) 仅图像的最先进深伪检测器；(2) 基于证据的事实核查系统，采用MCTS进行工具引导的检索，并用多智能体辩论（MAD）做审慎推理；(3) 将检测器输出作为辅助证据注入的混合系统。在两个数据集上统一以F1等指标评估。

Result: 深伪检测器单独表现有限：在MMFakeBench上F1为0.26–0.53，在DGM4上为0.33–0.49。把其预测注入核查管线会因非因果的“真实性”假设让性能下降0.04–0.08 F1。以证据为中心的系统最佳：MMFakeBench约0.81 F1，DGM4约0.55 F1。

Conclusion: 多模态主张核验主要依赖语义理解与外部证据检索/推理，像素级伪造特征不足以稳定提升对真实世界图文错误信息的推理；将深伪检测器机械纳入核查流程可能引入误导先验，应倾向证据驱动的多步检索与推理设计。

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [222] [Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling](https://arxiv.org/abs/2602.01864)
*Yuan Wang,Yuhao Wan,Siming Zheng,Bo Li,Qibin Hou,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出Ada-RefSR：在单步扩散框架中以“信任但验证”原则自适应利用参考图像，靠AICG模块在注意力中调节参考引导，既保真又自然且高效，并对参考错配鲁棒。


<details>
  <summary>Details</summary>
Motivation: 扩散式图像恢复虽能细化细节，但易产生幻觉。RefSR引入参考图像可缓解，但真实退化下LQ与参考的对应关系不可靠：硬匹配脆弱、忽视相关性会错用或少用参考。因此需要一种能依据相关性强弱自适应控制参考使用的机制。

Method: 提出Ada-RefSR单步扩散框架，核心为自适应隐式相关门控（AICG）。AICG通过可学习的summary tokens提炼参考主导模式，并与LQ特征建立隐式相关；将其嵌入注意力骨干中，实现对参考引导的轻量化、自适应调节，在相关可靠时放大参考信息，不可靠时抑制，作为内置防错融合的门控。

Result: 在多数据集上验证，Ada-RefSR在保真度、自然度与效率间取得良好平衡；在不同参考对齐程度下仍表现稳健。

Conclusion: 自适应隐式相关门控使参考利用“信任但验证”，有效避免错误融合，提升扩散式RefSR的实用性与鲁棒性。

Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a "Trust but Verify" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.

</details>


### [223] [ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding](https://arxiv.org/abs/2602.01881)
*Ye Chen,Yupeng Zhu,Xiongzhen Zhang,Zhewen Wan,Yingzhe Li,Wenjun Zhang,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出一种层次化代理参数图像表示，将语义、几何、纹理解耦，并用自适应Bezier拟合与区域细分构建代理几何，结合多尺度隐式纹理与局部自适应特征索引，实现高保真重建、直观可控编辑与无需生成模型的背景补全，且可与位置式动力学结合做物理驱动实时动画。


<details>
  <summary>Details</summary>
Motivation: 现有显式（栅格、高斯）与隐式（潜空间）图像表示要么冗余、编辑繁琐，要么缺乏从潜变量到语义实例/部件的可解释映射，难以进行细粒度、可控的图像/视频编辑。

Method: 基于输入图像的语义感知分解，构建层次化代理几何：通过自适应Bezier曲线拟合轮廓、迭代内部区域细分与网格化；在几何感知的代理节点中嵌入多尺度隐式纹理参数，实现连续、高保真的像素域重建与与实例/部件无关的语义编辑；引入局部性自适应特征索引，保证空间纹理一致性并支持高质量背景补全；将代理节点与Position-Based Dynamics结合，用轻量隐式渲染实现实时物理驱动动画。

Result: 在ImageNet、OIR-Bench、HumanEdit上实现SOTA渲染保真度，参数量显著更少；支持直观、交互式、物理合理的编辑；在视频/动画中较生成式方法具有更好的时域一致性与视觉真实感。

Conclusion: 层次化代理参数表示有效解耦语义-几何-纹理并提升可控性与效率，在重建、编辑与物理动画任务上兼具高保真、低参数量与强交互性，为可控图像/视频编辑与实时动画提供新范式。

Abstract: Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.

</details>


### [224] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: 提出Lazy Attention：跨层共享相似注意力模式与查询缓存（Q Cache），在不破坏KV缓存完整性的前提下显著降低MLLM推理成本，KV占用降35%+、吞吐提升1.5×、性能仅约降1%。


<details>
  <summary>Details</summary>
Motivation: MLLM推理成本高，主要因视觉编码器产生大量冗余视觉token，导致注意力计算与KV缓存占用激增。现有按token裁剪的方法虽能降开销，但会破坏KV缓存完整性，影响长文本生成稳定性与性能。因此需要一种不依赖激进token剪枝、还能保持KV一致性的高效注意力机制。

Method: 从跨层角度分析解码层注意力，发现超过一半层的注意力模式语义相似。据此提出Lazy Attention：允许后续层继承前一层的注意力模式，减少冗余计算；并设计轻量的层共享查询缓存Q Cache，在相邻层间复用Queries，与现有Flash Attention与KV cache完全兼容；方法与token级技术正交，可单独使用或与剪枝结合。

Result: 在多基准与多种MLLM上验证：KV缓存使用降低35%以上，吞吐量提升约1.5倍；任务性能仅约下降1%；与SOTA token剪枝法相比，精度保留更好。

Conclusion: 跨层共享注意力与Q Cache可在不损害KV完整性的情况下显著提升MLLM推理效率，兼容主流推理框架、灵活可组合，并相对token剪枝更稳健地保持性能。

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [225] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

TL;DR: STELLAR 通过将视觉特征因子分解为“语义概念×空间分布”，在极少稀疏语义token下同时做到DINO式语义对齐与MAE式重建：用少量（≈16）语义token做增广对齐，空间定位矩阵保留像素级几何用于重建，实现2.60 FID与79.10% ImageNet准确率，弥合判别与生成SSL的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有SSL两难：判别式方法（如DINO）为增强对齐而追求位置不变的全局token，牺牲重建所需的空间坐标；生成式方法（如MAE）保留致密网格利于重建，却难以学到高层语义抽象。需要一种既保语义又保空间的统一表示。

Method: 提出STELLAR：将视觉表示因子分解为低秩的语义token（表达概念）与定位矩阵（表达空间分布）。在训练时，对语义token执行DINO式增强对齐以获取位置不变的高层语义；同时保持定位矩阵的精确空间映射以支持像素级重建。以极少的稀疏语义token（约16个）实现表示压缩与解耦。

Result: 在仅使用约16个稀疏token的情况下，同时实现高质量重建（FID=2.60）并达到与致密主干相当的语义性能（ImageNet准确率79.10%）。展示了在判别与生成任务上的强竞争力。

Conclusion: 通过语义-几何解耦的低秩因子分解，STELLAR提供了一种既稀疏又通用的表示，既能做语义对齐又能像素级重建，有效桥接了判别式与生成式视觉学习范式。

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [226] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: 提出DSXFormer：结合双池化谱通道挤压-扩展与动态上下文注意力的高效Transformer用于高光谱影像分类，兼顾光谱判别力与计算效率，并在四个基准上取得SOTA精度。


<details>
  <summary>Details</summary>
Motivation: 现有HSIC方法在高光谱维度高、谱-空相关复杂和标注样本有限情况下，难以在保持计算效率的同时获得足够的光谱判别力与跨波段依赖建模能力。Transformer虽强，但常受限于算力与对局部谱空关系的建模不足。

Method: 设计DSXFormer，包括：1) DSX块：结合全局平均池化与最大池化对谱通道进行自适应挤压-扩展重标定，强化光谱判别与跨波段依赖；2) 在窗口化Transformer中引入动态上下文注意力（DCA），动态捕获局部谱-空关系并降低复杂度；3) 采用patch提取、嵌入与合并以实现多尺度高效特征学习。

Result: 在四个数据集（SA、IP、PU、KSC）上，分类精度分别为99.95%、98.91%、99.85%、98.52%，稳定优于现有SOTA方法。

Conclusion: 双池化谱挤压-扩展与DCA的联合，使模型在光谱强调与空间上下文建模间取得良好平衡，实现高效且高精度的HSIC；方法具有推广潜力。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [227] [Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network](https://arxiv.org/abs/2602.01951)
*Shuyang Wu,Yifu Qiu,Ines P. Nearchou,Sandrine Prost,Jonathan A Fallowfield,Hakan Bilen,Timothy J Kendall*

Main category: cs.CV

TL;DR: 提出一种可插拔的多尺度金字塔网络（MSPN），在注意力MIL上进行渐进式多尺度分析，通过网格重映射与粗粒度引导网络增强WSI表征，跨多任务/框架/基模均带来稳定提升且轻量易用。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度MIL常依赖多放倍率独立输入与后期特征融合，忽视跨尺度特征联系，且受厂商放大倍率限制，不灵活、算力开销大。需要一种在单一流程内保留跨尺度关联、减少计算并提升泛化的方案。

Method: 在注意力MIL框架上引入MSPN两部分：1）基于网格的重映射，用高倍特征推导低/粗尺度特征，形成金字塔并保持空间/尺度对应；2）粗粒度引导网络（CGN），学习粗尺度上下文，对细粒度特征进行引导与融合，实现渐进式多尺度分析。作为可插拔模块，适配多种注意力MIL与多种基础模型。

Result: 将MSPN作为附加模块，针对4个临床相关任务、4类基础模型以及预训练MIL框架进行基准评测，结果显示在所有对比配置与任务中均带来一致性能提升，同时保持轻量、易用。

Conclusion: MSPN有效保留并利用跨尺度关联，减少对多倍率独立输入与后期融合的依赖，在CPath中的MIL任务上实现稳定增益，具有通用性与实用性。

Abstract: Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.

</details>


### [228] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 提出RS-MPOD：在遥感开放词汇目标检测中，引入实例视觉提示与文本提示及其多模态融合，缓解仅文本提示在语义偏移与歧义下失效的问题。


<details>
  <summary>Details</summary>
Motivation: 仅依赖文本提示假设了预训练带来的稳健文图对齐，但在遥感任务中类别语义常具任务/场景特异性，导致开放词汇下类别指定不稳定。

Method: 构建多模态开放词汇检测框架RS-MPOD：1) 视觉提示编码器，从样例实例中提取外观类线索，实现无文本的类别指定；2) 文本提示照常提供语义信息；3) 多模态融合模块在两者同时可用时进行融合，以提升对齐与鲁棒性。

Result: 在标准、跨数据集与细粒度遥感基准上，视觉提示在语义歧义与分布移位下更可靠；当文本语义对齐良好时，多模态提示保持有竞争力。

Conclusion: 通过引入实例驱动的视觉提示与多模态融合，开放词汇遥感检测在类别指定的稳定性与鲁棒性上显著提升，文本与视觉提示可互补以适应不同语义对齐条件。

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [229] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 提出一种基于贝叶斯决策理论的后验校准方法，通过在目标分布的小验证集上学习单一标量对模型logit进行矫正，提升AI生成图像检测器在分布移位下的稳健性与阈值对齐，无需重训与标签。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在测试时常将伪造图像判为真实，表现出系统性偏差。原因被归因为：生成方法变化导致伪样本分布移位；模型在训练时对表面伪迹过拟合并形成隐式先验，致使阈值在新分布下失配。

Method: 基于贝叶斯决策理论，保持骨干冻结，引入一个可学习的标量对模型输出logit进行统一平移/缩放式校正（文中侧重为标量矫正），在目标分布的小规模验证集上优化该标量以最小化决策风险，实现后验阈值的再对齐。该过程不需要真实标签或仅需极少标注，属于轻量级后处理。

Result: 在多项具有挑战性的基准上，所提校准在无需重训的条件下显著提升检测鲁棒性与跨分布泛化能力，降低将伪造判为真实的错误率。

Conclusion: 简单的参数化logit校正即可有效补偿分布移位引起的决策边界错位，为开放世界场景下的AI生成图像检测提供了轻量、可适配且具理论依据的解决方案；代码已开源。

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [230] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: 提出一种简单无额外开销的方法，通过放大图像分隔符token的隐藏状态，抑制跨图像信息泄漏，从而显著提升LVLM在多图、多文档/多表任务上的区分与推理能力。


<details>
  <summary>Details</summary>
Motivation: LVLM在单图任务表现强，但多图输入时性能明显下降，主要因模型难以区分不同图像的信息，出现跨图像信息泄漏。现有用分隔符token标注每张图像边界，但实际并不能有效阻断泄漏，亟需更有效且成本低的解决方案。

Method: 在推理阶段对分隔符token的隐藏状态进行缩放（加强），以强化图像内部的注意力与交互，同时限制跨图像的不必要注意力与干扰；无需改变模型结构或进行再训练，也不增加推理计算量。

Result: 在多图基准（Mantis、MuirBench、MIRB、QBench2）上取得显著提升；在需要清晰区分的信息检索与理解的文本任务（TQABench、MultiNews、WCEP-10）上也有改进。

Conclusion: 简单、通用、无训练与额外开销的方法，通过增强分隔符token的表示有效缓解跨实例信息泄漏，提升多图与多文档场景的区分与推理能力。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [231] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 提出一种在扩散模型中实现精确局部可控生成的方法，通过掩膜与新损失设计，在保证整体高质量生成的同时，对用户指定区域施加局部条件控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的扩散生成难以通过文字细粒度操控；加入图像级条件（边缘、分割、深度）虽有效，但条件通常全局施加，缺乏对局部区域的精细控制。

Method: 设计新的训练框架：1) 引入掩膜机制，对用户指定的图像局部区域施加条件，其余区域由原始文本提示自洽生成；2) 新增损失项，利用在任意扩散步对初始潜向量的预测，增强当前步与潜空间最终样本的一致性，从而提升局部条件与最终结果的对应关系。

Result: 在多组实验中，实现了在保持高画质的同时，对指定局部区域进行精确、稳定的可控生成，优于仅全局条件的方法。

Conclusion: 通过掩膜与初始潜向量预测约束，方法实现了对局部区域的精细控制，同时维持未受控区域的高质量与多样性，为文本到图像扩散的精确编辑与个性化生成提供了有效途径。

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [232] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: SurfSplat 提出用2D高斯点（2DGS）替代3DGS，在稀疏视角下实现连续曲面与高保真纹理重建，并引入高分辨率一致性指标HRRC，实验在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的通用重建方法在稀疏图像下易产生离散、色偏点云：常分辨率看似可行，但近景出现严重伪影与不连续表面，缺乏连续曲面与高分辨率一致性的度量。

Method: 采用基于2D Gaussian Splatting 的前馈框架SurfSplat，利用更强各向异性与更高几何精度；引入表面连续性先验与强制alpha混合策略以获得连贯几何与真实纹理；并提出高分辨率渲染一致性（HRRC）作为新评价指标。

Result: 在 RealEstate10K、DL3DV、ScanNet 上，SurfSplat 在标准指标与HRRC上均优于既有方法，重建更连贯、近景伪影更少、纹理更忠实。

Conclusion: 2DGS结合连续性先验与改进混合策略，可在稀疏视角下实现高保真、连续表面重建；HRRC提供了评估高分辨率重建质量的新工具。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [233] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: 提出UniDriveDreamer：单阶段统一多模态世界模型，直接联合生成多摄像头视频与LiDAR未来观测，通过两类VAE与统一潜变量锚定(ULA)对齐潜空间，并用扩散Transformer融合时空与几何，辅以场景布局条件；在视频与LiDAR生成上超越SOTA，并提升下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型多聚焦于单一模态（视频或LiDAR），缺乏端到端联合生成多模态未来观测的能力，往往依赖中间表示或级联模块，造成跨模态不一致与训练不稳、难以捕捉几何与时序的统一建模。

Method: - 设计单阶段统一多模态框架UniDriveDreamer；
- 为LiDAR与多摄像头图像分别构建专用VAE，获得各自潜表示；
- 提出Unified Latent Anchoring(ULA)对齐两模态潜分布，保证兼容与稳定；
- 将对齐后的特征输入扩散Transformer，联合建模跨模态几何对应与时间演化；
- 引入结构化场景布局作为每模态条件信号，引导合成。

Result: 在视频与LiDAR序列生成上均优于此前SOTA；同时对下游任务（未具体列举）带来可量化性能提升。

Conclusion: 统一的多模态世界模型可在无需中间表示或级联的情况下，稳定而一致地生成视频与LiDAR未来观测；通过潜空间对齐与扩散Transformer的联合建模，实现更高保真度与时序一致性，并对下游感知/规划等任务产生积极影响。

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [234] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: 提出ClueRecall指标与ClueTracer插件，通过追踪从问题到输出再到视觉token的线索传播，抑制多模态推理模型的幻觉，在无需训练的前提下显著提升推理与非推理基准性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理模型依赖长链推理从图像收集线索，但会产生“推理漂移”：模型在取线索时过度关注与问题无关的实体，削弱对关键线索的聚焦，导致视觉落地性减弱，使现有基于定位或干预的方法在推理场景下失效。需要一种能评估并纠正线索检索与聚焦偏差的机制。

Method: 1) 定义ClueRecall指标，用于衡量模型是否成功检索到与任务相关的视觉线索；2) 提出ClueTracer：训练免、参数免、架构无关的插件，沿“问题→输出→视觉token”的推理路径追踪关键线索传播；通过抑制对无关区域的注意力、强化对相关patch的关注来减少幻觉。

Result: 在多种“推理型”架构（R1-OneVision、Ocean-R1、MM-Eureka等）上无需额外训练即带来约1.21×的性能提升；迁移到“非推理”设置也有约1.14×增益。

Conclusion: 推理漂移是多模态长链推理产生幻觉的关键原因。ClueTracer通过问题导向的线索追踪实现更好的视觉落地与线索聚焦，无需训练即可抑制幻觉并普适提升多种架构与任务性能。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [235] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA将基因组建模从“一维序列的LLM”转为“文档OCR式视觉-语言理解”，用视觉编码压缩DNA为可还原的视觉token，并在阅读、定位、检索、补全等任务上以更少token和更少可训练参数取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型沿用LLM按位顺序读序列，难以匹配基因组稀疏、非连续的语义结构，导致大量计算浪费在背景区且限制了超长上下文的有效压缩与理解。需要一种结构对齐、可压缩且能保留细粒度信息的表示与建模范式。

Method: 提出OpticalDNA：将DNA渲染为有结构的视觉版面，以OCR式视觉-语言模型进行建模。包含视觉DNA编码器（生成紧凑、可重构的视觉token，实现高保真压缩）与文档解码器；在此表示上定义面向基因组原语的提示条件目标：阅读、区域定位、子序列检索、掩码片段补全，学习版面感知的DNA表示。

Result: 在多种基因组基准上优于近期基线；对长至45万碱基的序列，以近20倍更少的有效token取得最好整体表现；在仅微调25.6万可训练参数的情况下，性能超过激活参数多达985倍的模型。

Conclusion: 以视觉OCR范式对齐了基因组的稀疏与版面结构，实现高效压缩与细粒度信息保留，显著提升长序列任务的性价比与可扩展性，为未来基于布局感知的基因组理解与检索生成提供新路径。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [236] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

TL;DR: 提出OSMF框架，通过“产品感知的自适应分群+群体偏好条件生成+Group-DPO对齐”来为不同用户群定制广告图，提升群体级CTR；并发布首个大规模群体偏好数据集GAIP与代码。


<details>
  <summary>Details</summary>
Motivation: 现有广告图生成多以总体CTR为目标，“一刀切”忽视不同用户群的偏好差异，导致特定群体效果不佳、精准营销受限，需要能对齐群体偏好的生成方法与数据。

Method: 1) 产品感知自适应分群：结合用户属性与商品特征动态聚类，得到含丰富集体偏好特征的群表示。2) 偏好条件生成：构建G-MLLM，使其同时理解群特征与多模态信息，按群体条件生成广告图。3) 预训练+微调：先多模态预训练G-MLLM；再用提出的Group-DPO实施群体偏好对齐，以提升各群的CTR。4) 数据：发布GAIP，含约40M用户构建的约60万群体的偏好数据。5) 评测：离线与在线实验验证有效性。

Result: OSMF在离线指标与在线A/B测试中的CTR等关键指标上均达SOTA；针对各群体的CTR也显著提升。

Conclusion: 面向广告图生成的OSMF能统一建模并对齐多群体偏好，实现“一个框架，多种匹配”，有效提升精细化投放效果；所发布的GAIP与代码将推动该方向研究与应用。

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [237] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 论文提出Auto-Comp自动化合成基准，用最小描述与上下文描述成对生成图像与文本，系统分析VLM在组合推理中的失败（颜色绑定与空间关系）。发现CLIP/SigLIP家族普遍易受“混淆基准”低熵干扰（重复物体/颜色）影响；上下文有助空间推理却损害局部属性绑定。


<details>
  <summary>Details</summary>
Motivation: VLM常把“红立方体与蓝球体”和“蓝立方体与红球体”混淆，现有评测难以剥离视觉与语言因素。需要可控、可扩展、细粒度的基准来诊断组合推理、颜色绑定与空间关系的真实弱点。

Method: 提出Auto-Comp：全自动合成数据管线。生成最小描述与由LLM扩展的上下文描述两类文本，并为每个文本合成成对图像，构造成A/B测试以隔离“核心绑定能力”与“视听语言复杂性”影响；构建颜色绑定、空间关系与“混淆基准”（含低熵干扰，如重复物体/颜色）。评测20个VLM（含CLIP与SigLIP系列）。

Result: 在颜色绑定与空间关系任务上，多个VLM出现系统性组合失败；在“混淆基准”中，对低熵干扰高度敏感，错误不仅是简单属性交换；上下文描述提高空间关系表现，却因视觉杂讯增加而削弱局部属性绑定。

Conclusion: VLM存在普遍的组合性缺陷，尤其在低熵干扰下暴露无遗。应区分最小与上下文设置以诊断不同能力；上下文引入的全局线索与局部绑定存在权衡。作者开源Auto-Comp管线与基准，促进后续可控评测。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [238] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: 提出SegmentMIL：一种基于Transformer的多视角多实例学习框架，使用仅患者级标注即可进行冠状动脉狭窄分类并定位左右冠脉及分段；在内部与外部测试上优于单视角模型与经典MIL基线。


<details>
  <summary>Details</summary>
Motivation: 现有狭窄检测多依赖昂贵且稀缺的视角级标注，且忽略多视角之间的时序与依赖关系，不符合临床实际（多视角、多时序、患者级结论）。

Method: 构建SegmentMIL：以多视角血管造影序列为包（patient-level bags），实例为视角/时间片段；利用Transformer建模跨视角与时间的依赖；在仅有患者级狭窄标签下进行多实例学习，并同时进行解剖部位定位（左右冠脉及其分段），无需视角级标注。

Result: 在真实临床数据上进行训练与评估，内部与外部测试均达到高性能；相较于需要视角级标注的单视角深度模型和经典MIL方法均有显著提升。

Conclusion: SegmentMIL在缺乏细粒度标注的现实条件下，实现了患者级狭窄检测与可定位性，具备临床可行性与可扩展性。

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [239] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: UrbanGS提出面向城市级场景的可扩展3D高斯重建：用深度一致法向正则与自适应置信加权提升几何一致性；用空间自适应高斯裁剪与统一分块/视图分配提升内存与计算可扩展性；在多城市场景上实现更佳渲染质量、几何精度与内存效率。


<details>
  <summary>Details</summary>
Motivation: 3DGS在小范围场景表现优异，但扩展到城市级环境时面临多视几何不一致、内存冗余和计算不可扩展等问题，亟需一种在保持高保真渲染的同时能稳定优化几何并降低资源开销的框架。

Method: 1) 深度一致D-Normal正则：结合外部深度监督与法向约束，同时更新位置与旋转；并以梯度一致性与反深度偏差构建自适应置信加权，强化多视深度对齐。2) 空间自适应高斯裁剪(SAGP)：依据局部几何复杂度与可见性动态调节高斯密度以去冗余。3) 统一分块与视图分配：避免块边界伪影并平衡计算负载。

Result: 在多种城市数据集上，渲染质量、几何精度与内存效率均优于现有方法，表明该框架在大规模场景重建中具有显著优势。

Conclusion: UrbanGS系统性解决了城市级3DGS在几何一致性与可扩展性上的关键瓶颈，实现高保真、内存友好、计算可扩展的超大场景重建。

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [240] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: FSVideo 是一个快速的 Transformer 式图生视频扩散框架，核心包括高压缩视频自编码器、新的带层记忆的DiT、以及多分辨率少步上采样器。14B基模+14B上采样器在速度上快一个数量级，同时保持与主流开源模型相当的质量。


<details>
  <summary>Details</summary>
Motivation: 现有开源I2V模型推理慢、视频压缩效率与重建质量权衡不佳，且DiT在长程时空建模与层间信息复用不足，限制了生成保真度与吞吐。作者旨在在保证质量的同时显著提升生成速度与可扩展性。

Method: 1) 设计视频自编码器，将视频压缩到64×64×4的时空潜空间，兼顾高压缩与重建质量；2) 采用Diffusion Transformer，并引入层记忆（layer memory）机制，促进跨层信息流动与上下文复用；3) 采用多分辨率生成：先用14B DiT基模型生成低分辨率潜表示，再用少步数的14B DiT上采样器进行多尺度细化，提高保真度；4) 结合高效训练与推理策略以提升吞吐和稳定性。

Result: 最终模型由14B基模型+14B上采样器构成，相比其他流行开源I2V模型在保持竞争性画质的同时，推理速度快约一个数量级；自编码器在高压缩比下仍具竞争性重建质量。

Conclusion: 通过高压缩视频VAE、带层记忆的DiT及少步上采样的多分辨率方案，FSVideo在I2V任务中实现“快且好”的折中，为大规模、可部署的视频生成提供参考。

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [241] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出DSKD：用教师分类器引导学生特征的扩散去噪采样，并在原始/去噪学生特征间用LSH蒸馏，避免直接师生对齐带来的分布不匹配，显著优于现有KD。


<details>
  <summary>Details</summary>
Motivation: 传统KD多通过对齐师生特征或设计损失来传递知识，但师生网络结构/容量差异导致特征分布不一致，直接对齐可能让学生学习到不兼容信息，限制性能。

Method: 1) 训练一个轻量扩散模型在学生特征空间进行去噪采样；2) 用教师分类器作为指导信号，指导学生特征的扩散去噪过程，使去噪后的学生特征蕴含教师知识；3) 以去噪学生特征为“教师”，对原始与去噪学生特征进行LSH引导的蒸馏对齐，规避师生映射方式与分布差异。

Result: 在多种视觉识别任务、不同模型与数据集上，DSKD显著优于现有KD方法；实验显示所提流程稳定、泛化好。

Conclusion: 绕开直接师生对齐，通过教师引导的学生特征扩散去噪与LSH自蒸馏，能有效缓解分布/映射不匹配问题，提升学生模型性能。

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [242] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: 提出改进的连续条件扩散模型iCCDM，引入矩阵化EDM与自适应邻域训练，在多数据集和分辨率上实现更高图像质量与更高采样效率，超越现有方法与大规模文生图模型。


<details>
  <summary>Details</summary>
Motivation: 原CCDM依赖过时的扩散框架，采样轨迹长、效率低，且已被GAN方法CcGAN-AVAR超越，亟需在质量与效率上同时提升。

Method: 将Elucidated Diffusion Model(EDM)融入并大幅改造：1) 提出矩阵形式的EDM表述，便于连续标签条件建模与数值稳定/高效计算；2) 设计自适应vicinal（邻域）训练策略，根据连续标签分布自适应采样邻域样本，增强条件泛化与鲁棒性；3) 总体框架命名为iCCDM，缩短采样轨迹以提升速度。

Result: 在四个基准数据集（分辨率64×64至256×256）上，iCCDM在生成质量与采样效率上均优于现有方法，包括超过CcGAN-AVAR及多款SOTA大规模文生图扩散模型（如Stable Diffusion 3、FLUX.1、Qwen-Image），同时显著降低采样成本。

Conclusion: iCCDM通过矩阵化EDM与自适应邻域训练，解决了CCDM的过时框架与低效采样问题，实现质量与效率的双提升，并在广泛设定中达到SOTA表现。

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [243] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit 是一个免训练、基于光流的长时长视频编辑框架，通过分段编辑并在段边界与全局一致性上做专门处理，实现分钟级视频的稳定、高保真编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在短视频上有效，但难以扩展到分钟级视频：计算成本高，且难以在成千上万帧上保持全局时序一致，易出现闪烁、边界伪影与结构漂移。

Method: 采用分而治之的分段编辑策略，并引入两大核心模块：1）Velocity Blend：对相邻片段的光流场进行对齐与融合，修正段边界的运动不一致，消除闪烁和边界伪影；2）Attention Sink：将局部片段的特征锚定到全局参考帧，抑制随时间累积的结构漂移。框架为免训练、基于光流。

Result: 在大量定量与定性实验中，MLV-Edit 在时间稳定性与语义保真度上均优于现有最先进方法。

Conclusion: 通过分段编辑+光流对齐与全局特征锚定，MLV-Edit 能以低开销实现分钟级视频的稳定编辑，解决长视频的时序一致性与边界伪影问题，并在多项指标上取得 SOTA 性能。

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [244] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: 提出一个用于毒理学小鼠肝脏WSI的AI异常检测框架：用带像素级标注数据微调DINOv2（LoRA）做组织/病变分割，并用马氏距离做OOD检测，结合类别特异阈值，显著降低误判，在真实毒理样本中能发现罕见异常。


<details>
  <summary>Details</summary>
Motivation: 毒性导致的研发失败率高，早期发现不良反应至关重要；传统病理依赖专家、难以规模化，需要自动化方法提升效率并能识别少见病变。

Method: 构建健康与已知病变的像素级标注数据集；以DINOv2作为视觉骨干，用LoRA进行微调以实现组织/病变分割；提取特征后用马氏距离进行OOD病变检测；为不同类别设定特异阈值，并以FN与FP均值优化阈值。

Result: 在小鼠肝脏WSI上，已知病变与健康组织分割准确；OOD检测能发现罕见形态学异常；通过类别特异阈值，病变被误判为健康仅0.16%，健康被误判为病变0.35%。

Conclusion: 该AI框架可在毒理病理中早期、准确地检测异常（含少见病变），缓解病理学瓶颈，潜在降低研发后期失败并提升药物开发效率。

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [245] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: 作者指出以注册配对训练的CBCT→sCT监督学习受不可避免的配准误差污染，提出用物理仿真CBCT生成天然几何对齐的训练对，并用与输入CBCT的几何对齐指标评估。实验显示在两套骨盆数据上，合成训练优于常规在几何一致性（NMI 0.31 vs 0.22），且临床偏好与NMI一致而与传统强度指标相反。


<details>
  <summary>Details</summary>
Motivation: 传统监督sCT需要CBCT与CT的精确配准，但临床上难以达到完美配准，导致训练和评估都受到“配准偏差”影响，可能优化出更像配准伪影而非真实解剖的模型。需要一种既能提供几何对齐训练样本、又能避免偏置评估的方法。

Method: 用基于物理的CBCT仿真，从GT CT出发生成与CT天然几何对齐的CBCT，从而构成无配准误差的训练对；评估阶段不再对齐到偏置GT，而是使用与输入CBCT的几何对齐度量（如NMI）。在两套骨盆数据上比较以真实配准数据训练与以仿真数据训练的模型，并进行临床观察者研究。

Result: 以仿真数据训练的模型在几何对齐方面更好（NMI 0.31 vs 0.22），尽管传统强度指标更低。对于形变配准数据，强度指标与临床评估呈反相关/倒置关联；NMI与观察者偏好稳定正相关（ρ=0.31, p<0.001）。临床观察者在87%的案例中更偏好仿真训练的输出。

Conclusion: 监督sCT训练与评估若依赖配准会引入系统性偏差。基于物理的CBCT仿真可提供几何对齐训练对，并应以与输入CBCT的一致性（如NMI）作为评价标准。临床需求更重视几何保真而非与偏置GT的强度一致性，采用该方案能更符合临床偏好。

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [246] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: 提出一个全自动、深度学习驱动的框架，从大规模历史地图中进行细粒度城市变化分析；在巴黎(1868–1937)验证，展现异质性的时空城市演变，并具备可移植的模块化设计。


<details>
  <summary>Details</summary>
Motivation: 历史地图记录了长期城市演化，但受制于地图错位、制图风格差异、文档退化等问题，难以获得一致、细粒度的变化信息，现有研究多停留在小范围或定性层面。需要一种能在大规模历史地图上进行系统、定量分析的自动化方法。

Method: 构建模块化、全自动的深度学习框架，包含：1) 稠密地图配准(对齐不同年代地图，纠正空间错位)；2) 多时相目标检测(在各期地图上自动识别建筑/道路等对象)；3) 变化剖面分析(对检测结果进行跨时相对比，量化与描述变化)。框架可适配不同制图语境与下游任务。

Result: 实验表明所提出的配准与目标检测方法鲁棒有效；在巴黎1868–1937的应用揭示了城市改造的空间与时间异质性，能稳定地产出定量化的变化信息。

Conclusion: 该框架将历史地图研究从临时性的视觉比对转向系统的定量表征，支持社会科学与人文学研究；其模块化设计便于迁移至多样的地图数据与应用场景。

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [247] [LoopViT: Scaling Visual ARC with Looped Transformers](https://arxiv.org/abs/2602.02156)
*Wen-Jie Shu,Xuerui Qiu,Rui-Jie Zhu,Harold Haodong Chen,Yexin Liu,Harry Yang*

Main category: cs.CV

TL;DR: 提出Loop-ViT：通过权重共享的递归迭代与无参数的动态退出机制，在ARC-AGI-1上以18M参数达65.8%准确率，优于更大模型，表明自适应迭代计算比单纯扩宽更高效。


<details>
  <summary>Details</summary>
Motivation: 传统ViT为前馈结构，计算深度受参数规模限制，难以拟合人类归纳所需的迭代、算法式推理。需要一种能在不增加参数的情况下加深推理深度、并能根据不确定性自适应停机的视觉推理框架。

Method: 提出Loop-ViT：采用权重共享（weight-tied）的混合模块（局部卷积+全局注意力）反复迭代，形成潜在的“思维链”。并引入基于预测熵的无参数Dynamic Exit，当内部状态进入低不确定性的“吸引子”时自动停止推理。

Result: 在ARC-AGI-1基准上，18M参数的Loop-ViT达到65.8%准确率，超越参数达73M的集成模型，显示在效率与性能上的优势。

Conclusion: 将推理深度与模型容量解耦，并通过不确定性驱动的自适应迭代，能显著提升视觉推理效率与效果；与其简单增加网络宽度，不如沿“可变深度、可停机”的计算轴进行扩展。

Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.

</details>


### [248] [Reg4Pru: Regularisation Through Random Token Routing for Token Pruning](https://arxiv.org/abs/2602.02163)
*Julian Wyatt,Ronald Clark,Irina Voiculescu*

Main category: cs.CV

TL;DR: 提出Reg4Pru训练正则化以缓解视觉Transformer中token剪枝导致的分割性能退化，在FIVES血管分割上以29%推理加速换来平均精度绝对提升46%。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer计算量随token数二次增长，直接剪枝虽能提速但会因保留表示不稳定、深层密集预测性能下降，亟需在保持效率的同时稳定表示与路由。

Method: 在采用“剪枝+从保留表示中再激活token”的框架下，引入名为Reg4Pru的训练期正则化，约束与引导被剪枝/再激活的token表示，使路由稳定并减少信息丢失；与无该正则的同架构进行对比。

Result: 在FIVES血管分割数据集上，相比无路由正则的同模型，平均精度提升绝对46%，同时相对墙钟时间加速29%，优于非剪枝基线的效率-性能权衡。

Conclusion: Reg4Pru能显著缓解token剪枝带来的分割性能损失，在保持显著加速的同时提升精度，是token减少策略的有效正则工具。

Abstract: Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.

</details>


### [249] [Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks](https://arxiv.org/abs/2602.02171)
*Lu Cao,Xiquan He,Junying Zeng,Chaoyun Mai,Min Luo*

Main category: cs.CV

TL;DR: 提出TSGAN两阶段GAN：先用StyleGAN生成语义分割掩膜控制解剖形态，再用改进的DL-Pix2Pix通过多重注意力生成CT细节；在LUNA16上提升检测准确率4.6%、mAP 4%。


<details>
  <summary>Details</summary>
Motivation: 现有肺结节CT数据样本少、类别与形态多样性不足，合成图像纹理单一且解剖结构失真，限制检测模型的泛化。

Method: 两阶段解耦结构与纹理：1) 使用StyleGAN生成掩膜（结节与背景语义），提供可控的解剖形态；2) 以DL-Pix2Pix进行掩膜到CT的图像翻译，引入局部重要性注意力捕获局部特征，并用动态权重多头窗口注意力增强结节纹理与背景建模。

Result: 在LUNA16数据集，相比原始数据训练的检测模型，准确率提升4.6%，mAP提升4%。合成图像质量与检测性能均提升。

Conclusion: 通过结构-纹理解耦的两阶段生成框架，实现了对解剖形态与纹理的可控合成，提升数据多样性与真实感，从而增强肺结节检测模型的表现与泛化。

Abstract: The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.

</details>


### [250] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: CIEC提出仅用图像/句子级弱标注，在图文对上进行多模态篡改定位，效果接近全监督。


<details>
  <summary>Details</summary>
Motivation: 现有多模态篡改定位依赖昂贵的细粒度标注（patch/token级），成本高、效率低，限制了实际应用与数据规模扩展。

Method: 提出CIEC框架，含图像分支与文本分支的弱监督定位。图像分支引入TRPS：结合文本引导与空间先验筛选可疑patch，并用背景静默与空间对比约束抑制无关区域；文本分支引入VCTG：关注语义关键词，利用相对视觉偏差辅助token定位，并以非对称稀疏约束与语义一致性约束减轻噪声、提升可靠性。仅用图像/句子级标注训练。

Result: 在多项评测指标上达到与全监督方法相当的性能，验证了弱监督设定下的有效性。

Conclusion: 将隐式（弱标注、对比/稀疏/一致性约束）与显式（跨模态引导与空间先验）线索耦合，可在无需细粒度标注的情况下实现有效的图文篡改定位，显著降低标注成本且保持高性能。

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [251] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: 提出VDR-Bench基准与多轮裁剪检索流程，专测MLLM在现实场景下的视觉与文本检索式深度研究能力，并缓解现有基准的泄露与过理想化问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态深度研究系统虽可用搜索引擎完成复杂图文查证，但评估困难：1) 现有数据集非“视觉搜索中心”，答案常被文本线索或模型常识偷渡；2) 评测场景过于理想化：图像侧可用整图近似匹配拿到答案，文本侧检索过直接、挑战不足。

Method: 构建包含2000个VQA实例的VDR-Bench，通过多阶段严谨筛选与专家审核，确保问题需真实依赖跨图文检索且避免泄露；并提出简单的“多轮裁剪检索”工作流：对图像区域逐步裁剪/放大与检索，迭代地定位关键信息，以提升现实视觉检索表现。

Result: 在VDR-Bench上验证：现有MLLM对真实视觉检索能力不足；采用多轮裁剪检索策略可显著提升在复杂、现实检索场景下的表现。

Conclusion: VDR-Bench为评估Vision-DeepResearch系统提供更贴近现实的基准，多轮裁剪检索可作为实用策略改进视觉检索；为未来多模态深度研究系统设计提供实践指引与开放资源（代码将开源）。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [252] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: TopoField 是一种拓扑感知的隐式建模框架，用于在CT提取的肺部树结构中自动修复缺失/断裂分支，并在一次前向推理中同时完成拓扑修复、解剖标注和肺段重建，速度约1秒/例。


<details>
  <summary>Details</summary>
Motivation: 现有从CT提取的肺部树（气道/血管）常存在拓扑不完整（缺失、断连），导致后续解剖分析与建模准确性和鲁棒性下降；现有方法多依赖稠密体素处理或显式图推理，效率低、对结构破坏不鲁棒。

Method: 提出TopoField：以稀疏的表面点云与骨架点云表示肺部解剖，学习连续的隐式场来显式建模与修复拓扑；通过在“已不完整”的树上合成进一步的结构破坏进行训练，无需完整或断连的显式标注；在修复后的隐式表示之上，利用任务特定的隐式函数，实现解剖标签预测与肺段重建的统一多任务推理，单次前向完成。

Result: 在Lung3D+数据集上，TopoField显著提升拓扑完整性，并在不完整场景下实现准确的解剖标注与肺段重建；由于隐式表示的高效性，整套流程每例仅需约1秒。

Conclusion: 将拓扑修复视为一等建模目标的隐式框架能在存在拓扑损伤的现实场景中兼顾精度与效率，适合大规模与时敏临床应用；代码与数据将开源以促进复现与应用。

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [253] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: 提出SSI-DM：在扩散模型反演前先注入微小噪声以跳过早期去噪奇异区，得到接近高斯的可编辑噪声并保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像反演到噪声空间的方法在早期加噪步存在不准确，导致反演噪声偏离高斯分布、可编辑性差。作者识别到根因是数学奇异性使得反演在该区间病态。

Method: 在标准反演流程开始前，对真实图像先加入极小随机噪声，从而绕开早期时间步的奇异区域；随后按常规扩散反演进行。该方法即插即用，适用于通用扩散模型。

Result: 得到分布更接近自然高斯的反演噪声，同时保持高保真重建；在公共图像数据集上的重建与插值任务中优于现有方法。

Conclusion: 通过“奇异区跳过”策略，反演问题得到稳定与可解化，兼顾可编辑性与重建质量，为扩散模型反演提供了高效、通用、原理清晰的解决方案。

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [254] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: MAIN-VLA 通过“意图抽象 + 环境语义抽象”的双重表征，并在两者对齐时产生无参数的注意力收缩与token裁剪，从而在复杂动态环境中实现更高效、更稳健的决策与推理，刷新Minecraft与大型PvP任务的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLA在3D开放世界与大型PvP等高维、强动态、实时交互场景中，难以从冗余的多模态传感流中提取关键决策信号，导致推理低效、泛化弱。需要一种能从语言与视觉中捕捉“与行动相关的高层语义”的机制，避免仅靠表面匹配。

Method: 提出MAIN-VLA，核心包括：1) 意图抽象（IA）：把冗长语言指令与其推理过程压缩为显式语义原语（action-relevant primitives）；2) 环境语义抽象（ESA）：将海量视觉流映射为结构化、拓扑化的可供性表示（affordance topology）；3) 跨抽象对齐：对齐IA与ESA以诱发注意力集中效应，并据此进行无参数token裁剪，过滤感知冗余而不降性能。

Result: 在Minecraft与大型PvP（和平精英、Valorant）中取得新的SOTA：更高决策质量、更强泛化、更快推理（高推理效率），证明该抽象-对齐-裁剪范式有效。

Conclusion: 通过把语言意图与环境可供性提升到抽象层并进行深度对齐，MAIN-VLA能在复杂动态场景中稳定聚焦行动关键因素，实现高效、可泛化的视觉-语言-动作决策。

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [255] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 论文指出：把双向视频扩散模型蒸馏成少步自回归模型会因注意力从全局变因果而出现架构鸿沟；现有用ODE蒸馏做初始化在理论上不成立，导致性能退化。作者提出用自回归教师做ODE初始化的“Causal Forcing”，在各项指标显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 实时交互式视频生成需要少步AR推理，但主流教师是双向扩散模型。将全注意力替换为因果注意力引入架构差异；现有通过PF-ODE蒸馏假设帧级可逆（噪声帧唯一映射到干净帧），但当教师是双向而学生是AR时该假设被破坏，导致只能学到条件期望而非真实流，性能下降。

Method: 提出Causal Forcing：用“自回归教师”而非双向教师进行PF-ODE初始化，从源头对齐注意力因果性，理论上可恢复正确的流映射，避免条件期望解。随后再进行少步AR蒸馏以实现实时生成。

Result: 在动态性（Dynamic Degree）、VisionReward、指令跟随等指标上全面SOTA，相比Self Forcing分别提升约19.3%、8.7%、16.7%。

Conclusion: 通过以AR教师进行ODE初始化，成功弥合双向到因果的架构鸿沟，避免条件期望陷阱，显著提升实时AR视频生成质量与对齐度；方法通用且可作为未来少步视频生成蒸馏的标准范式。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [256] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: 提出HieraNav层级化、开放词表的语言导航任务与LangMap基准，覆盖场景/房间/区域/实例四级目标，并在真实3D室内扫描上提供高质量、精炼而判别性的区域与实例描述，含18K+任务，用于评测不同指令风格。评估显示更多上下文与记忆有助成功，但长尾、小型、依赖上下文、远距离和多目标任务仍困难。


<details>
  <summary>Details</summary>
Motivation: 实现“物—语”对齐对人机交流与具身智能至关重要。现有导航基准在层级粒度、开放词表覆盖、真实场景规模与高质量判别性语言上不足，难以系统研究从宏观场景到微观实例的自然语言导航。

Method: 提出HieraNav任务，按场景/房间/区域/实例四级目标定义导航。构建LangMap基准：基于真实3D室内扫描，提供全面人工校验的区域标签、判别性区域与实例描述（覆盖414类）、每个目标有简洁与详细两种描述，共18K+任务。与现有数据集比较其判别性与简洁性；对零样本与监督模型进行系统评测，分析上下文、记忆等因素。

Result: LangMap在判别性上较GOAT-Bench提升23.8%，且使用更少（约1/4）词数。实验显示更丰富上下文与记忆提升导航成功率；但对长尾类别、小目标、强上下文依赖、远距离目标及多目标完成，模型仍表现欠佳。

Conclusion: HieraNav与LangMap为语言驱动具身导航提供了严格、层级化、开放词表的测试平台，促进对不同语义粒度与指令风格的研究；当前模型仍受限于长尾与复杂情境，未来需在记忆、上下文理解与长距离规划上改进。

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [257] [MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2602.02222)
*Ruiqi Liu,Manni Cui,Ziheng Qin,Zhiyuan Yan,Ruoxin Chen,Yi Han,Zhiheng Li,Junkai Chen,ZhiJin Chen,Kaiqing Lin,Jialiang Shen,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: 提出MIRROR：将AIGI检测从“伪迹分类”转为“参考-比较”，用可学习离散记忆库编码真实图像流形，通过稀疏线性组合重构理想参考并以残差做检测信号；在14个基准上显著优于现有方法，并在新的人类基准Human-AIGI上达到89.6%准确，超越普通用户与视觉专家。


<details>
  <summary>Details</summary>
Motivation: 现有检测器依赖伪迹，随生成技术演进易失效；人类判断依托稳定的现实世界规律。作者希望通过对“真实图像流形”的一致性检验获得更具可泛化性的检测信号，并评估模型是否达到可替代人类专家的“超人交叉点”。

Method: 1) 将检测表述为与真实流形的一致性验证；2) 设计MIRROR：用可学习的离散记忆库作为现实先验；3) 将输入投影为记忆原型的稀疏线性组合以得到“理想参考”；4) 利用重构残差作为鲁棒检测特征；5) 构建Human-AIGI基准，含心理物理筛选的人眼难辨子集，覆盖27个生成器。

Result: 在14个基准上，MIRROR在6个标准基准平均提升约2.1%，在7个野外基准平均提升约8.1%；在Human-AIGI上达89.6%准确，超越普通用户与视觉专家，并随预训练骨干规模提升进一步逼近人类感知上限。

Conclusion: 基于现实流形参考-比较的检测范式较伪迹学习更具泛化性；通过记忆库稀疏重构与残差信号可稳定识别AIGI。MIRROR在多基准、多人类对照中验证有效，表明向“超人级”AIGI检测迈进。

Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the "superhuman crossover" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR

</details>


### [258] [Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type](https://arxiv.org/abs/2602.02223)
*Junchi Feng,Nikhil Ballem,Mahya Beheshti,Giles Hamilton-Fletcher,Todd Hudson,Maurizio Porfiri,William H. Seiple,John-Ross Rizzo*

Main category: cs.CV

TL;DR: 本文系统评估了移动与静态场景下OCR性能，量化距离、角度、速度与设备/安装方式对准确率的影响。结果：走动越快、视角越斜准确率越低；Google Vision总体最佳，PaddleOCR开源中最强；手机主摄优于超广角；肩挂略优于头戴与手持但差异不显著。


<details>
  <summary>Details</summary>
Motivation: 现有OCR评测多基于静态数据集，难以反映盲人/低视力用户在移动场景中的真实挑战，需要在真实使用条件（距离、角度、运动、安装位置与设备差异）下进行系统性量化评估，以指导技术选型与辅助产品设计。

Method: 设计静态与动态两类实验：静态测试在1–7 m距离、0–75°水平角度下测量检测与识别性能；动态测试改变行走速度（0.8–1.8 m/s）与相机安装位置（头/肩/手持），对比智能手机（主摄与超广角）与智能眼镜。基准四种OCR引擎（Google Vision、PaddleOCR 3.0、EasyOCR、Tesseract），并用PaddleOCR进一步测速度影响。以字符级Levenshtein比率对比人工标注真值计算准确率，并作统计分析。

Result: - 准确率随行走速度增加与视角变宽而下降。
- Google Vision总体识别最好；PaddleOCR 3.0为最强开源备选；EasyOCR与Tesseract落后。
- 手机主摄优于超广角；跨设备以手机主摄最高。
- 肩挂位置平均最好，但与头戴、手持差异不显著。

Conclusion: 移动与斜视角是OCR性能的关键瓶颈；选用更佳传感配置（手机主摄）与尽量减小运动/倾角可显著提升真实使用中的识别率。Google Vision目前最优，PaddleOCR为可行开源方案。安装位置影响有限，肩挂略优但非决定性。

Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.

</details>


### [259] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出LatentMorph，在T2I生成中将“隐式潜在推理”内嵌到生成过程，避免显式逐步思维造成的低效与信息损失；通过四个轻量模块与自适应调用策略，显著提升多项基准性能并降低开销。


<details>
  <summary>Details</summary>
Motivation: 现有T2I推理增强多依赖显式思维：在固定步长把中间推理解码为文本并频繁图像解码/再编码，导致效率低、信息丢失、与模型“认知”流程不匹配。需要一种能在连续潜空间内动态推理与自我改进的方法，以更贴近人类创作过程。

Method: 提出LatentMorph框架，在连续潜空间进行隐式推理与自我修正。包含四个轻量组件：(i) Condenser：将中间生成状态压缩为紧凑视觉记忆；(ii) Translator：把潜在“思维”转为可执行的指导信号；(iii) Shaper：动态引导下一步图像token预测；(iv) Invoker：通过强化学习训练，自适应决定何时触发推理。整个过程避免把推理显式化为离散文本，减少解码/再编码开销。

Result: 在Janus-Pro上：GenEval提升16%，T2I-CompBench提升25%；在抽象推理任务（WISE、IPV-Txt）相对显式范式（如TwiG）分别提升15%与11%；同时推理时间减少44%，token消耗减少51%；推理调用的“认知对齐度”与人类直觉达71%。

Conclusion: 在T2I中采用隐式潜在推理可更高效、更自适应地进行自我完善，较显式思维范式实现更强的抽象/组合能力与更低的推理成本；RL驱动的自适应调用进一步提升与人类直觉的一致性。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [260] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: 提出LiFlow：用于自动驾驶3D激光雷达场景补全的流匹配框架，避免扩散模型训练/推理分布不一致问题，并通过邻域流匹配与Chamfer损失兼顾局部结构与全局覆盖，取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 激光雷达点云受遮挡与远距离稀疏影响，导致场景感知不完整。现有基于扩散的点级去噪方法需要预测高斯噪声，训练与推理初始分布不一致，影响补全质量与稳定性。需要一种在训练与推理阶段初始分布一致、兼顾局部与全局几何的生成式补全方法。

Method: 引入基于流匹配(flow matching)的生成框架替代扩散：1) 保持训练与推理使用一致的初始分布；2) 设计最近邻流匹配损失，使生成的点流局部对齐目标点云的几何；3) 引入Chamfer距离作为全局覆盖约束，提升整体场景对齐；4) 在3D LiDAR场景补全任务上实现点云从不完整到完整的生成对齐流程。

Result: 在多项指标上达到SOTA，较扩散基线在局部结构保持与全局覆盖方面均有提升；代码开源（LiFlow）。

Conclusion: 基于流匹配的LiDAR场景补全优于噪声预测型扩散方法：消除训练/推理初始分布不匹配，结合局部与全局几何损失实现更稳定更准确的点云补全，并在标准基准上取得领先表现。

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [261] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: DiScene提出一种稀疏查询框架，通过多层次蒸馏与教师引导初始化，实现高效且鲁棒的三维占据预测；在Occ-ScanNet上以23.2 FPS在无深度先验下显著超越OPUS，并在引入深度后达SOTA，泛化至Occ3D-nuScenes与野外场景。


<details>
  <summary>Details</summary>
Motivation: 占据预测对机器人几何与语义理解至关重要，但密集方法在空体素上浪费算力，稀疏查询方法在复杂室内场景鲁棒性不足；亟需兼顾效率与鲁棒性的方案。

Method: 提出DiScene稀疏查询框架，并引入两项核心设计：1) 多层一致性知识蒸馏（从大教师向轻量学生进行四层对齐：编码器特征对齐、查询特征匹配、先验空间引导、锚点级高置信知识迁移）；2) 教师引导初始化（参数warm-up优化）以加速收敛。

Result: 在Occ-ScanNet上，无深度先验达23.2 FPS，性能较基线OPUS提升36.1%，并优于带深度的OPUS†；加入深度后，DiScene†超越EmbodiedOcc 3.7%，推理提速1.62×。在Occ3D-nuScenes与野外场景亦展现良好泛化与鲁棒性。

Conclusion: 多层蒸馏与教师引导初始化有效提升稀疏查询占据预测的效率与鲁棒性，刷新或追平SOTA并具跨数据集与真实场景的适用性；代码模型已开源。

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [262] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: 提出用RVQ-VAE结合对比学习与信息泄漏损失，实现人体动作风格与内容的有效解耦，并通过量化码本交换在推理时进行零样本风格迁移与相关应用。


<details>
  <summary>Details</summary>
Motivation: 人体动作同时包含粗粒度的语义内容与细粒度的表现风格，传统方法难以稳定解耦，造成风格迁移、风格移除与动作混合等任务受限，需要一种能在不微调的情况下对未见风格泛化的方法。

Method: 以“内容=粗粒度属性、风格=细粒度细节”的层级假设为指导：1) 使用残差向量量化VAE（RVQ-VAE）学习由粗到细的多级码本表示；2) 融合对比学习以拉近相同内容/风格的特征并分离不同者；3) 设计信息泄漏损失，配合码本学习约束，将内容与风格分配到不同层级码本以减少互相渗漏；4) 推理时提出量化码交换（Quantized Code Swapping），在不微调的前提下用目标风格码替换源序列的风格码，实现风格迁移。

Result: 在多种推理应用中表现出强泛化与稳健性：可在未见风格上无需微调完成风格迁移，并支持风格移除与动作混合等任务，定性与定量结果均显示优于或可比于现有方法。

Conclusion: 通过层级量化表示与对比/泄漏约束实现风格-内容的清晰解耦，结合推理期码本交换，实现通用、零样本友好的人体动作风格编辑与融合。

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [263] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: LongVPO提出两阶段DPO流程，在无长视频标注下，让短上下文视觉语言模型实现超长视频理解：阶段一用锚定片段+干扰片段合成偏好三元组并做过滤与近似评分；阶段二用递归字幕生成场景元数据，再由LLM构造多段推理问题与劣质回答进行偏好对齐。仅用1.6万合成样本、零人工标注，即在多项长视频基准上超越SOTA并保持短视频性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源视觉语言模型通常受上下文长度限制，难以进行超长视频理解；长视频标注昂贵且难以规模化。需要一种无需长视频人工标注、仍能训练模型掌握跨段信息整合与多段推理能力的方法。

Method: 两阶段Direct Preference Optimization：
- 阶段1（短片段对齐）：从长视频切出短片段，围绕锚定片段构造问题，将真实相关片段与多个干扰片段交错，生成偏好三元组；通过视觉相似度与问题特异性过滤缓解位置偏置与歧义；参考模型长上下文评分以“仅评锚定片段”的近似替代以降算力。
- 阶段2（长视频推理对齐）：对长视频进行递归式字幕/场景级元数据生成；借助LLM基于这些元数据合成多段推理问题与带偏差的劣质回答，形成偏好数据，进行多段推理任务上的DPO训练。

Result: 在仅用约16K合成训练样本、无人工标签的条件下，在多项长视频理解基准上超过开源SOTA；同时在短视频基准（如MVBench）上保持强竞争力。

Conclusion: 通过合成偏好数据与两阶段DPO对齐，可在无长视频人工标注的前提下，将短上下文VLM扩展到稳健的超长视频理解，具有效率高、可扩展、性能优的优势。

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [264] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

TL;DR: 提出并系统评估一种以UV坐标为输入、连续域上的纹理隐式神经表示（INR），兼顾画质、显存与推理时延，并展示在实时渲染与下游任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统纹理多为离散贴图，受分辨率、存储与采样伪影限制；而INR在多领域已被证实具备高保真与压缩潜力，因而有动机将其引入纹理表示，探索在连续UV空间中是否能以更小内存和更好尺度泛化实现高质量渲染，并服务实时图形与相关任务。

Method: 以UV坐标为输入，设计并比较多种网络结构作为纹理INR（如不同MLP/调制/频谱编码等），系统实验评估图像质量、内存占用与渲染推理时间，并研究三者权衡；同时将所学INR用于实时渲染相关流程与下游应用，包括mipmap拟合与在INR空间中的生成。

Result: 实验表明多种INR在纹理重建质量上表现良好，并具备可观的（相对较低的）内存占用，但在渲染时延上存在开销；作者量化了质量-内存-速度的折中，并在实时渲染与mipmap/生成等应用中验证可行性。

Conclusion: 连续UV域的纹理INR是可行且有效的：在保证高画质的同时可降低存储，代价是一定的推理时间；给出不同网络设计下的折中分析，并展示其在实时渲染与若干下游任务中的应用潜力。

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [265] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 提出NAB神经自适应分箱方法，将矩形先验通过可微分的分箱编码融入稀视角CT重建，实现端到端优化并在工业数据上显著优于现有方法，对医疗数据亦具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 稀视角CT可降成本但重建质量受限；现有隐式神经网络难以利用对象形状先验。许多工业件呈矩形/长方体结构，若能将该先验融入，将提升重建精度与效率。

Method: 设计可微的“自适应分箱”编码：把坐标映射到分箱向量空间，分箱由移位双曲正切差分构成，扩展支持绕入射平面法向的旋转；编码参数（位置、尺寸、陡峭度、旋转）与网络共同端到端训练，从投影数据反传梯度；通过调节分箱平滑度推广到更复杂几何。

Result: 在两套工业数据集上取得优于现有稀视角CT重建的性能；当将分箱函数推广为更一般表达时，在医疗数据上仍保持稳健表现。代码将开源。

Conclusion: NAB提供了一种将矩形先验融入神经重建的新范式，可微分、可端到端学习，提升稀视角CT重建质量，并具备对更复杂形状的可推广性。

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [266] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: 提出在数字病理分类中使用SNGP（谱归一化+GP输出层）以提升不确定性估计与OOD检测，同时保持分布内准确率。


<details>
  <summary>Details</summary>
Motivation: 数字病理中的深度模型在分布外样本上常过度自信、校准差，影响安全与临床信任，需要可内生地量化不确定性并能拒绝OOD输入的方法。

Method: 在标准分类网络中加入谱归一化，并以高斯过程层替代最终全连接层（SNGP）；与确定性模型和MC Dropout比较，在三类任务六个数据集（白细胞、淀粉样斑块、结直肠病理）上评测分布内性能、校准与OOD检测。

Result: SNGP在分布内准确率与基线相当，但不确定性估计显著更好，OOD检测性能显著提升；相较MC Dropout可用单模型实现更稳健的不确定性。

Conclusion: SNGP为数字病理提供轻量、可部署的不确定性感知分类框架，可更安全地拒绝OOD输入，增强临床可采纳性与病理医师信任。

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [267] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: UnifiedReward-Flex 是一个用于视觉生成的个性化奖励模型，将奖励建模与可灵活、可上下文自适应的推理结合，通过语义理解与证据落地，动态构建层级化评价标准，并在GRPO框架下显著提升图像与视频合成表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态奖励模型多采用统一偏好假设或固定评分准则，忽视内容特定线索与主观、情境相关的人类偏好，导致评估与优化偏差，难以契合真实用户需求。

Method: 提出 UnifiedReward-Flex：给定提示与生成内容，先解析语义意图并基于视觉证据对齐，再在预设与自生成的高层维度下实例化细粒度标准，构建层级化评估。训练分两阶段：1) 蒸馏闭源VLM的结构化高质量推理轨迹进行SFT，习得灵活、上下文自适应推理；2) 在精挑的偏好对上进行DPO，强化推理可信度与判别对齐。最后将其集成到GRPO以优化图像/视频生成。

Result: 在图像与视频合成任务中，集成 UnifiedReward-Flex 的GRPO训练取得广泛、显著优于现有方法的性能，显示更好的个性化与情境对齐能力。

Conclusion: 通过将个性化、可适配推理引入奖励模型，并以SFT蒸馏+ DPO训练范式实现，UnifiedReward-Flex 能更贴合多样人类偏好，提升视觉生成质量与对齐性，优于一刀切或固定准则的现有RM。

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [268] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出MultiBO：基于多选偏好的人类反馈优化，迭代引导扩散模型生成更接近用户心中目标图像的结果。


<details>
  <summary>Details</summary>
Motivation: 语言提示在个性化图像生成中存在瓶颈：当文本已无法细化差异时，人仍能判断哪个候选更接近心中目标。利用这种相对偏好可继续缩小生成图像与心中目标的差距。

Method: 设计MultiBO（多选偏好贝叶斯优化）：从当前最好图像x^{p*}出发，生成K个新候选；收集用户对这些候选的相对偏好（哪一个更接近目标x*）；将偏好反馈更新贝叶斯优化后验并据此调整扩散模型的生成；重复该过程B轮。核心在于偏好建模与采集策略相结合，逐轮提出“信息量大”的K个候选。

Result: 在包含30位用户的主观评测与对5个基线的客观指标比较上，MultiBO在相同的反馈轮数内更接近用户心中目标图像，显示出明显优于基线的方法性能。

Conclusion: 多选人类偏好反馈可被有效地整合进扩散模型生成流程，通过有限轮交互显著缩小目标心像与生成图像间的差距，适合个性化图像生成场景。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [269] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: Infinite-World提出一种能在真实复杂场景中跨1000+帧保持一致视觉记忆的交互式世界模型，通过无姿态的层级记忆压缩、动作不确定性标注与重访密集微调，显著提升画面质量、动作可控性与空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型多在具有完美真值的合成数据上训练良好，但在真实视频中因位姿估计噪声大、视角重访稀缺，难以维护长期一致的记忆与可控生成，缺乏有效训练范式。

Method: 1) 提出层级无姿态记忆压缩器（HPMC），递归蒸馏历史潜变量为固定预算表征，并与生成主干联合优化，使模型在无显式几何先验下以有界计算成本锚定远距过去记忆；2) 提出不确定性感知动作标注，将连续运动离散为三态逻辑，最大化利用原始视频同时防止噪声轨迹污染确定性动作空间；3) 基于先导玩具实验洞见，采用“重访密集微调”策略，用30分钟紧凑数据激活长程闭环能力。

Result: 在客观指标与用户研究上，模型在视觉质量、动作可控性与空间一致性方面优于现有方法，并能在1000+帧维持连贯记忆和稳健交互。

Conclusion: 通过无姿态层级记忆压缩、动作不确定性建模与针对性微调，Infinite-World在真实世界视频中实现长期一致与可控的生成/交互，减少对几何先验和精准位姿的依赖，提供了稳健的训练与推理范式。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [270] [Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation](https://arxiv.org/abs/2602.02401)
*Xinshun Wang,Peiming Li,Ziyi Wang,Zhongbin Fang,Zhichao Deng,Songtao Wu,Jason Li,Mengyuan Liu*

Main category: cs.CV

TL;DR: 提出“Superman”统一框架：用视觉引导的运动分词器构建跨模态骨架运动词表，并以单一MLLM统一完成视频到3D骨架感知与骨架运动生成（预测、补间），在Human3.6M等上达到SOTA或具竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前人体运动分析割裂：1）“感知”模型能看视频但只输出文本；“生成”模型不能直接感知视觉；2）多模态大模型多停留在单帧静态姿态，依赖SMPL，难处理时序；3）现有运动词汇仅由骨架数据构建，与视觉域脱节。需要一个能跨视觉与骨架、覆盖时序任务的统一方法。

Method: 两部分：1）视觉引导的运动分词器（Vision-Guided Motion Tokenizer），利用3D骨架与图像的几何对齐，联合学习视觉与骨架，得到统一的跨模态运动词表。2）在该运动语言上训练单一统一的MLLM，能处理多种时序输入，统一完成视频到3D骨架姿态估计（感知）与基于骨架的运动预测、in-betweening（生成）。

Result: 在Human3.6M等标准基准上，对所有相关运动任务达到SOTA或具竞争力表现，验证了统一框架的有效性。

Conclusion: 跨模态运动词表+统一MLLM可在单一系统内同时完成感知与生成任务，提升效率与可扩展性，为基于骨架的生成式运动分析提供更通用路径。

Abstract: Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.

</details>


### [271] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

TL;DR: ReasonEdit 是首个在推理密集型任务上编辑多模态大模型（VLM）的编辑器，通过让用户在编辑时提供“人类推理”并将其存入代码本（codebook），在推理阶段用一种受网络科学启发的拓扑均衡多模态嵌入方法检索相关事实，从而实现更好的编辑泛化与最先进表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法多针对语言或简单视觉任务，难以在需要复杂视觉-语言推理的场景中既修正错误又不伤及无关能力；缺乏利用人类可解释推理来支撑编辑与泛化的机制。

Method: 提出 ReasonEdit：在编辑时收集并结构化存储人类的推理链为代码本；在推理时使用新颖的“拓扑均衡”多模态嵌入检索与当前问题最相关的推理事实，将其用于引导与约束VLM的回答，从而实现定点修正与行为保持。

Result: 在四个不同的VLM与多个需要理由/解释的视觉问答数据集上，ReasonEdit达到当前最优的编辑性能，展现出显著的编辑泛化能力。

Conclusion: 把人类推理显式引入编辑过程并用拓扑均衡检索机制选择相关事实，可在不破坏无关行为的前提下显著提升VLM在推理密集任务上的编辑效果与泛化。

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [272] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: 提出 Catalyst：利用预池化特征图的通道统计计算输入自适应缩放因子γ，与现有OOD分数相乘以拉大ID/OOD差异，在多数据集与方法上显著降FPR。


<details>
  <summary>Details</summary>
Motivation: 现有后处理OOD检测多仅依赖logit或GAP后的特征向量，忽略了在GAP过程中丢失的通道级原始统计信息（均值、方差、最大激活等）。作者认为这些统计包含互补信号，可用于提升ID/OOD可分性。

Method: 提出Catalyst框架：从预池化特征图计算样本级通道统计，动态生成缩放因子γ；将γ与任意基线OOD分数（logit类如Energy、ReAct、SCALE，或距离类如KNN）做乘性融合，实现“弹性缩放”，从而在分布上拉开ID与OOD评分。

Result: 在CIFAR-10/100(ResNet-18)与ImageNet(ResNet-50)上，与多种后处理基线结合取得一致提升：平均FPR分别降低约32.87、27.94%、22.25%。

Conclusion: 预池化通道统计是被忽视但有效的OOD信号；Catalyst作为通用可插拔的乘性缩放模块，可与现有方法互补并显著提升OOD检测性能。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [273] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: 提出SelvaMask热带树冠数据集（8800+标注，3处新热带站点）与一个模块化检测-分割管线，利用VFM并配合领域化检测提示，实现在密集热带林中树冠分割的SOTA，并具外部数据集可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有树冠分割在热带森林中表现差，尽管有Transformer等进展，但高密度冠层、遮挡与标注一致性问题仍突出；缺乏专门面向热带、带有系统一致性评估的高质量基准数据集，限制了模型发展与泛化监测。

Method: 1) 构建SelvaMask：在巴拿马、巴西、厄瓜多尔3处新热带森林，用高分辨率航片人工精细勾画>8,800个树冠，并提供多标注者一致性评估；2) 设计模块化检测-分割流程：以视觉基础模型(VFMs)为核心，通过“领域化检测-prompter”先做检测再分割，将通用模型适配到热带森林场景；3) 与零样本通用模型和端到端全监督方法比较，并在外部热带与温带数据集上验证。

Result: 在密集热带森林树冠分割上取得SOTA，显著优于零样本通用模型和全监督端到端方法；在外部数据集上同样保持优势，显示良好泛化。

Conclusion: SelvaMask既是困难而标准化的热带森林基准，又通过配套的VFM适配流程推动树冠分割在复杂林分中的性能与泛化，促进大尺度森林监测；代码与数据将公开。

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [274] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: UniReason 提出一个将文生图与图像编辑通过“规划→精修”双重推理统一的框架，并配套构建推理中心数据集，显著提升在多项推理密集型基准与通用合成任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在复杂合成与深度推理上乏力，且把生成与编辑割裂，缺少像人类那样“先计划后校正”的一体化过程，导致约束不显式、细粒度错误难以纠正。

Method: 提出UniReason：1) 将文本生成图像视为带外部世界知识增强的规划，以注入隐含约束；2) 将图像编辑用于精细化视觉改进与自反思纠错；3) 在共享表示中统一生成与编辑；4) 构建约30万条、覆盖五大知识域的规划数据集与代理生成的自纠错语料；在此框架下进行联合训练与评估。

Result: 在WISE、KrisBench、UniREditBench等推理密集型基准上达到先进水平，同时保持出色的通用合成能力。

Conclusion: 通过“规划+精修”的统一推理范式与大规模推理数据支持，可同时强化生成与编辑两类能力，实现更强的复杂推理与高质量视觉合成。

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [275] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: 提出一种带检测门控的多头Transformer（基于Swin U-Net）用于放疗自动分割，通过切片级检测结果门控像素级分割，显著抑制在无解剖结构切片中的虚假分割，在TCIA前列腺边界案例数据集显著优于无门控基线。


<details>
  <summary>Details</summary>
Motivation: 常规深度分割在不存在目标结构的切片产生解剖学不合理的假阳性（幻觉），影响临床放疗自动勾画的可靠性，需要提升鲁棒性与解剖合理性。

Method: 在Swin U-Net上引入：1）多头Transformer与跨切片上下文融合；2）并行检测头（MLP）进行切片级结构在/不在判别；3）检测输出作为门控抑制分割流在无效切片的预测；4）采用切片级Tversky损失缓解类别不平衡。

Result: 在TCIA的Prostate-Anatomical-Edge-Cases数据集上，门控模型相较无门控基线显著提升：平均Dice loss 0.013±0.036 vs 0.732±0.314；检测概率与解剖存在高度相关，几乎消除伪分割；基线在各切片仍有高变异与持续假阳性。

Conclusion: 检测驱动的门控机制在不牺牲有效切片分割质量的前提下，大幅降低幻觉与解剖不合理性，提升自动分割的临床可用性，可用于提高放疗自动勾画流程的可靠性。

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [276] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen 是一种直接在像素空间扩散、引入感知监督（LPIPS 与 DINO）以避免 VAE/潜变量两阶段瓶颈的方法，在 ImageNet-256 上以 80 轮、无 CFG 达到 FID 5.11，并在大规模文生图上获 GenEval 0.79，提供更简单但更强的生成范式。


<details>
  <summary>Details</summary>
Motivation: 像素扩散避免了 VAE/潜空间流程的伪影与瓶颈，但高维像素流形包含大量与感知无关的信号，优化困难，导致以往像素扩散落后于潜空间扩散。需要一种方法让像素扩散聚焦于“感知相关”的流形。

Method: 提出 PixelGen：在标准像素扩散训练中加入两种互补的感知损失以塑造感知流形——(1) LPIPS 损失促进局部纹理/细节学习；(2) 基于 DINO 的感知损失强化全局语义一致性。无需 VAE、潜表示或辅助阶段，端到端在像素域训练。

Result: 在 ImageNet-256 上，80 个 epoch、无 classifier-free guidance 即达 FID 5.11；在大规模文生图任务上具有良好可扩展性，GenEval 得分 0.79，并优于强潜扩散基线。

Conclusion: 感知监督能显著缓解像素空间优化难题，使像素扩散在保持简单端到端范式的同时超越潜扩散表现；PixelGen 以更少训练复杂度与阶段实现更强生成质量，代码已开源。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>
