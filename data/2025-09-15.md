<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: 提出ASOS数据集：包含来自澳洲超市的50种常见商品的高质量贴图3D网格，用于机器人与计算机视觉基准；强调可获得性、真实场景适用性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多依赖合成模型或难以获得的专业物体，限制了真实场景中的可复现性与评测公平性。需要一个成本低、易获取、涵盖日常物品、且高质量的3D数据集以推动检测、位姿估计与机器人研究。

Method: 从澳大利亚大型连锁超市采购10类50件家用品；采用高分辨率成像与基于SfM（结构自运动）的重建流程，生成封闭（水密）的高质量三维贴图网格；确保多样的形状、尺寸与重量分布。

Result: 获得一个覆盖10类别、50件可复现采购物品的高质量水密3D网格数据集，贴图完整，适合用于多任务基准（检测、位姿估计、机器人抓取/操作等）。

Conclusion: ASOS通过可获得、真实、成本友好的物体集填补了现有数据集在可复现性与实际应用之间的缺口，为计算机视觉与机器人基准测试提供了更贴近现实的标准与资源。

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [2] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 提出用于灾后住房理赔与资源规划的多模态RAG（MM-RAG），结合图像与文本检索-生成，实现跨模态对齐与端到端多任务训练，显著提升检索准确率与损伤分类性能（Top-1检索提升9.6%）。


<details>
  <summary>Details</summary>
Motivation: 灾后需要快速、准确地评估房屋损伤以支持保险理赔与资源调度。现有方法要么仅用图像/文本，难以综合证据；要么RAG仅限文本，无法充分利用视觉信息，且跨模态对齐与生成阶段对不同模态信息的重要性无法动态调节。

Method: 在经典RAG上扩展出双分支多模态编码器：图像分支使用ResNet+Transformer提取灾损视觉特征；文本分支用BERT对社媒帖、保险条款等向量化并构建可检索的修复索引；引入跨模态交互模块通过多头注意力对齐图文语义；生成模块加入模态注意门控，动态调节视觉证据与文本先验的贡献；端到端训练，联合对比损失、检索损失、生成损失进行多任务优化，实现图像理解与政策匹配的协同学习。

Result: 在检索准确率与损伤严重度分类指标上优于对比方法，Top-1检索准确率提升9.6%。

Conclusion: MM-RAG通过跨模态对齐与门控生成，在灾后场景实现更准确的检索与分类，支持理赔与资源规划；端到端多任务优化促进图像理解与政策匹配协同提升。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [3] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 提出一种通过多重增强+对齐融合来稳定LLM在嘈杂历史文档上的文字抽取，基于Gemini 2.0 Flash转写并用类Needleman–Wunsch比对生成共识和置信度；在622份宾州死亡记录数据集上较单次转写提升约4个百分点，并分析不同增强的贡献。


<details>
  <summary>Details</summary>
Motivation: 历史文档往往噪声大、变形多，单次LLM转写不稳定且缺乏可靠置信度评估；需要一种稳健、可扩展的方法提升准确率并能区分高低置信的结果。

Method: 对每张图像生成多种增强版本（如填充、模糊、网格扭曲等），分别用Gemini 2.0 Flash转写；然后用自定义的类Needleman–Wunsch序列对齐器对多路输出进行比对与融合，得到共识转写和整体置信度评分；并在自建的622份宾夕法尼亚州死亡记录数据集上评测。

Result: 相较单次（single-shot）基线，整体转写准确率提升约4个百分点；填充与模糊增强对提升准确率最有效；网格扭曲对区分高/低置信样本最有帮助。

Conclusion: 多增强+对齐的简单可扩展集成框架能稳定LLM的嘈杂文档转写，提供可靠置信度，并可直接迁移到其他文档集合与转写模型。

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [4] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 提出MITS：首个面向智能交通监控的大规模多模态数据集，含17万+实景监控图像、细粒度标注与500万指令式VQA，显著提升主流LMM在ITS任务上的效果（如LLaVA/Qwen家族大幅增益），并开源数据、代码与模型。


<details>
  <summary>Details</summary>
Motivation: 通用LMM在交通监控场景表现欠佳，关键原因是缺乏面向ITS的专用大规模多模态数据与任务设定；现有数据集无法覆盖监控视角、交通事件/目标细粒度类别及多环境条件，限制了模型迁移与泛化。

Method: 构建MITS数据集：1）采集170,400张真实交通监控图像，标注8大类、24子类的ITS对象/事件，覆盖多环境与时段；2）通过系统化管线生成高质量图像描述与约500万条指令跟随VQA样本；3）定义并覆盖五类关键ITS任务（识别、计数、定位、背景分析、事件推理）；4）在主流LMM（LLaVA-1.5/1.6、Qwen2-VL/2.5-VL）上进行微调与评测。

Result: 在ITS应用评测上，微调后模型显著提升：LLaVA-1.5从0.494→0.905（+83.2%）、LLaVA-1.6从0.678→0.921（+35.8%）、Qwen2-VL从0.584→0.926（+58.6%）、Qwen2.5-VL从0.732→0.930（+27.0%）。

Conclusion: MITS作为首个大规模ITS多模态基准，系统补齐数据缺口，显著增强通用LMM在交通监控场景的能力；相关数据、代码与模型完全开源，为ITS与LMM研究/应用提供高价值资源与标准评测平台。

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [5] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: 研究用决策树式的结构化推理来增强VLM在细粒度与分层标签上的分类，但尽管模型能理解树知识（98.2%），树推理仍普遍弱于标准零样本提示；加入LLM生成类别与图像描述能提升两者表现。


<details>
  <summary>Details</summary>
Motivation: VLM在零样本分类强，但对细粒度和大层级标签空间下的表现与可解释性不足；希望通过可解释的树结构推理提升性能与可解释性，并检验其有效性。

Method: 构建将分类分解为可解释决策的树型框架；在细粒度GTSRB与粗粒度CIFAR-10上评测。比较标准零样本提示与树推理；再用LLM生成的类别扩展与图像描述增强树提示以改进对齐。评估包括模型对树知识理解（准确率）与最终分类性能。

Result: 模型对树知识理解达98.2%准确率，但在两数据集上，树推理分类表现始终劣于标准零样本提示；加入图像描述后，树推理与零样本两种方法的性能都得到提升。

Conclusion: 当前结构化（树）推理未能在视觉分类上超越简单零样本提示，暴露其在细粒度和层级任务中的局限；然而它带来可解释性，并在描述增强下有改进空间，为设计更可解释的VLM系统提供了实践洞见。

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [6] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: 提出PSI：一种从视频等数据中学习可控、可提示的世界模型的三步循环框架（概率预测→结构抽取→结构集成），在1.4T视频标记上训练，既提升预测理解，又零样本抽取中间结构（光流、深度、分割）并反哺训练，实现性能与可控性双提升。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型/视频模型要么难以精确条件控制、要么缺乏统一的提示语言；同时很难在无监督下显式获得低维“中间结构”（如光流、深度、分割）并用于进一步提升模型。需要一种统一框架，既能学习全面条件分布，又能零样本因果方式抽取结构，并把结构回灌使模型更强、更可控。

Method: 三步循环：1）概率预测：训练随机访问的自回归序列图模型Psi，学习任意变量对任意变量集合的条件分布（完整条件族）。2）结构抽取：在Psi上做因果推断，零样本发掘低维“中间结构”（光流、深度、对象）。3）结构集成：将这些结构作为新token类型加入训练语料，既作为条件信号也作为预测目标，持续迭代，每轮扩展能力与控制柄；形成类似LLM的通用提示语言。

Result: 在1.4万亿网络视频token上训练的Psi可执行多种视频预测与理解任务；零样本提取并达到SOTA的光流、自监督深度与对象分割；将提取的结构回灌后实现预测性能提升与更强的可控生成/推理。

Conclusion: PSI实现了“学—挖—融”的闭环：从数据学到完整条件概率→零样本因果抽取关键结构→把结构变为可提示的离散控制柄并回融训练，带来理解与生成的双提升，展示了可扩展的、LLM式可提示的世界模型方向。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [7] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 首篇系统性研究联邦学习中视频数据在梯度反演攻击下的泄露风险：使用特征提取器较直接处理原始帧更耐攻击，但仍非万无一失；攻击者可用超分重建更清晰视频，威胁现实且需深入研究触发条件。


<details>
  <summary>Details</summary>
Motivation: 联邦学习以只共享梯度而非原始数据维护隐私，但梯度反演已在图像、文本、表格上显示可重构私密数据；视频作为重要且更敏感的数据模态，其在FL中的泄露风险尚未被系统评估，存在关键研究空白。

Method: 针对视频分类的两种常见范式进行对比评估：（1）使用预训练特征提取器的流水线；（2）对原始视频帧做简单变换后直接训练分类器。对二者施加梯度反演攻击，并在攻击后采用图像超分辨技术提升重建帧质量；分别考察攻击者拥有0/1/多参考帧的场景，衡量泄露程度与抗性。

Result: 使用预训练特征提取器的方案对梯度反演更具鲁棒性；直接用原始帧的方案更易泄露。引入超分技术能显著提升从梯度反演得到的帧质量，进而重建更高质量视频。在部分设置下，即便有特征提取器，若后端分类器复杂度不足，仍可发生可感知泄露。

Conclusion: 联邦学习中的视频数据确实存在可行的泄露风险；特征提取器能缓解但不能消除攻击，且超分等后处理会放大泄露效果。应进一步研究在何种模型复杂度、特征抽取策略与攻击者先验条件下会触发或放大泄露，并据此设计更强的防护。

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [8] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 提出一种面向零售密集货架场景的半监督协同训练(object detection)框架：用Faster R-CNN(ResNet)与YOLO(Darknet)互换伪标签，辅以XGBoost/RandomForest/SVM集成做分类增强，并用元启发式算法调参；在SKU-110k上表现强，降低标注成本、适应商品/布局频繁变化。


<details>
  <summary>Details</summary>
Motivation: 零售货架场景目标密集、遮挡/重叠严重且标注昂贵，传统有监督检测在少标注数据下效果差；需要能在复杂环境中提升检测精度、降低人工成本、并快速适应频繁变动的解决方案。

Method: - 协同训练：Faster R-CNN(精定位、ResNet骨干)与YOLO(全局上下文、Darknet骨干)在未标注数据上生成高置信伪标签并互相蒸馏/交换，迭代提升。
- 分类增强：在检测到的候选上，用XGBoost、随机森林、SVM集成，融合多样特征表征以提高类别判别鲁棒性。
- 超参优化：使用元启发式(如进化/群体智能)驱动的搜索同时优化检测器与分类集成的关键超参，以兼顾精度与效率。
- 适配零标注/少标注：最大化利用未标注货架图像，减少对昂贵标注的依赖。

Result: 在SKU-110k数据集进行了实验，显示该框架取得“强性能”（文摘未给出具体mAP/召回/吞吐数值），验证了方法在高密度、遮挡重叠场景中的有效性与可扩展性。

Conclusion: 该半监督协同训练+集成分类+元启发式调参的框架在零售真实场景中实用、可扩展，能降低标注成本，并适用于库存追踪、商品监控与自助结算等应用。

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [9] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出一种无需反向传播的测试时自适应方法Token Purging（PG），在注意力层前剔除受域移位影响严重的token；提供PG-SP与PG-SF两种变体，在多个3D点云鲁棒性基准上显著优于现有方法且更快更省显存。


<details>
  <summary>Details</summary>
Motivation: 3D点云分类在测试分布与训练分布不一致时性能大幅下降；现有TTA常需迭代更新和反向传播，带来延迟与资源开销，难以部署。需要一种轻量、稳定、无需参数更新的TTA方案。

Method: 在Transformer式点云分类器中，于注意力层前对输入token进行“清洗”：评估每个token受域移位影响的程度并剔除高风险token，从而减少噪声对注意力计算的干扰。提供两种实现：PG-SP使用源域统计指导剔除阈值；PG-SF不依赖源数据，借助CLS token驱动的适应策略实现纯源自由。方法不涉及反向传播或迭代更新。

Result: 在ModelNet40-C、ShapeNet-C、ScanObjectNN-C等腐化/分布移位基准上，PG-SP相比现有无反向传播TTA平均提升约+10.3%准确率；PG-SF在源自由设定下创下新SOTA。相较基线，推理速度快12.4倍、显存节省5.5倍。

Conclusion: PG以token级清洗实现快速、稳健的测试时自适应，兼顾精度、速度与资源效率，适合实际部署；既可利用源统计进一步增强（PG-SP），也可在完全源自由场景运行（PG-SF）。

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [10] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出一种高精度且可解释的细粒度跨视角定位方法：不再整体BEV变换，而是直接在地面—航拍间建立局部特征对应，并用单目深度将匹配点“抬升”到BEV；通过尺度感知Procrustes对齐估计3DoF位姿，并在相对深度情形下可恢复尺度，弱监督下在跨区域与未知朝向等挑战条件中优于以往且对多种相对深度模型通用。


<details>
  <summary>Details</summary>
Motivation: 传统做法将地面图像变换到鸟瞰BEV再与航拍对齐，但此过程因透视畸变与高度压缩导致信息丢失，削弱配准质量。需要一种既保留几何信息又能在训练—测试深度分布不一致时稳健工作的跨视角定位策略。

Method: 不进行全图BEV变换，改为直接检测并匹配地面与航拍局部特征；利用单目深度先验仅将匹配关键点抬升到BEV空间。深度可为度量或相对深度；采用尺度感知的Procrustes对齐从对应点估计相机3DoF位姿，并在相对深度时联合恢复尺度。训练仅用相机位姿弱监督以促使网络学习高质量对应关系。

Result: 在多项实验中取得更高定位精度与稳健性，尤其在跨区域泛化与未知朝向等困难场景下优于现有方法；在无需针对不同相对深度模型微调的情况下亦能保持强性能。

Conclusion: 直接跨视角建立局部对应并用尺度感知对齐进行位姿估计，可在弱监督下实现准确、可解释且对深度先验模型友好的3DoF跨视角定位，具备实际部署的灵活性与可靠性。

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [11] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 提出基于智能手机红光反射（Bruckner测试数字化）的儿童视力筛查App KidsVisionCheck，使用深度神经网络在标注瞳孔图像上训练，未见测试集上准确率约90%，并总结了最佳采集条件以实时反馈，旨在提高全球儿童早期视力异常筛查可及性。


<details>
  <summary>Details</summary>
Motivation: 传统Bruckner测试需眼科医生与专业设备，限制了早期筛查可及性；智能手机与AI发展使移动端复现成为可能，期望以低成本、可扩展方式开展儿童视力异常早筛。

Method: 收集并由眼科医生标注儿童瞳孔红光反射图像，训练深度神经网络模型进行分类；在未见测试集评估准确率；并分析不同拍摄条件对质量与判别性能的影响，以提炼最优采集条件并在App内提供即时反馈。

Result: 模型在未见测试集达到约90%准确率；识别并总结了有利于数据采集的最佳条件，可用于用户引导与质量控制。

Conclusion: 基于手机的红光反射AI筛查在无需专用设备的情况下可实现高可靠性，是迈向可及的儿科视力筛查与早期干预的重要一步，但仍处于首研阶段，未来需更大规模验证与临床落地。

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [12] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出DGFusion：用深度引导的多模态感知网络，利用局部深度token与全局条件token动态调节跨模态融合，并配以鲁棒深度损失，在MUSES与DELIVER上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有融合方法在空间上对各传感器一视同仁，难以应对不同深度/场景条件下各模态可靠性差异；需要一种能随空间和条件自适应的融合策略。

Method: 将多模态分割设为多任务学习：以激光雷达既作输入又作深度学习的监督，增加辅助深度头学习深度感知特征；把这些特征编码为空间变化的局部深度token，并结合全局条件token，以注意力机制进行跨模态融合；为稀疏、噪声LiDAR设计鲁棒深度损失。

Result: 在MUSES与DELIVER两个具有挑战性的数据集上，在全景与语义分割任务上取得SOTA表现。

Conclusion: 深度感知与条件自适应的注意力式融合能有效提升多传感器语义/全景分割的鲁棒性，尤其在深度变化大与恶劣条件下表现突出；方法可泛化并有可复现代码。

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [13] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 提出基于ResNet-18的面部“局部图像块”（patch）检测策略，用局部皮损区域替代整脸输入，在准确率/敏感度上与整图相当或更优，同时提升鲁棒性、可解释性并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮需要精准、早期检测以提升疗效；整脸输入可能包含大量无关信息、隐私风险和泛化差，模型易分散注意力。通过仅使用局部病变相关区域的patch，有望提升性能、聚焦临床相关特征并减少可识别信息。

Method: 以ResNet-18为骨干，系统性构建和比较多种patch策略：从人脸不同尺寸、形状、位置提取局部图像块；开展消融/对比实验，评估局部信息对深度模型表现的影响；与全图方法在准确率、敏感度等指标上比较；分析模型关注区域与隐私暴露程度。

Result: 多种patch策略在准确率与敏感度上达到或优于整图基线；模型更聚焦临床相关区域，表现出更强鲁棒性与可解释性；仅用局部块能天然去除可识别面部特征，降低隐私风险。

Conclusion: 基于局部patch的ResNet-18检测框架可作为自动皮肤科诊断的有效方案：在不牺牲甚至提升性能的同时，提高模型可解释性与稳健性并保护隐私，具有实际应用价值。

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [14] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 提出一种隐私保护的玫瑰痤疮自动检测方法：用“红色度”先验构建固定面部掩膜，仅保留中央面部红斑相关区域，用该掩膜生成的合成数据训练ResNet-18；在真实测试集上，相比全脸基线在准确率、召回率、F1上都有提升。


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮症状弥散、标注数据稀缺且人脸隐私敏感，使自动检测困难。需要一种既能提升检测性能又能保护隐私，并能绕过真实隐私数据匮乏的问题。

Method: 1) 基于临床先验：玫瑰痤疮主要表现为中央面部红斑。2) 通过在多张人脸图像上选择红色通道强度稳定偏高的区域，构建固定“红度引导”掩膜（覆盖双颊、鼻、额头，排除身份特征）。3) 生成并使用掩膜后的合成训练数据。4) 用ResNet-18在这些掩膜-合成图上训练。5) 在真实数据上评估，与全脸输入的基线比较。

Result: 在真实世界测试集上，相较全脸基线，所提方法的准确率、召回率和F1显著提升，显示掩膜+合成数据训练的有效性与泛化能力。

Conclusion: 临床先验与合成数据结合可在保护隐私的同时实现对玫瑰痤疮的准确自动检测，适用于远程医疗与大规模筛查等隐私敏感场景。

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [15] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: 对ULW腹腔镜去烟框架做系统消融：分别去掉可学习维纳滤波器、以及只用复合损失中的单项（MSE/SSIM/感知损失），在公开成对数据集上用SSIM/PSNR/MSE/CIEDE2000与可视化对比评估其贡献。


<details>
  <summary>Details</summary>
Motivation: 验证ULW框架中各子模块与损失项的必要性与有效性，弄清哪些组件对去烟质量与稳定性贡献最大，指导后续模型设计与简化。

Method: 基于U-Net骨干、复合损失（MSE+SSIM+感知损失）与可微可学习维纳滤波模块。通过消融实验：1) 移除维纳滤波器；2) 将复合损失拆分成单项分别训练；在同一公开成对腹腔镜数据集上统一训练与评测。

Result: 各变体基于SSIM、PSNR、MSE、CIEDE2000的量化指标与主观可视化进行对比，展示每个组件对性能的影响。不过摘要未给出具体数值。

Conclusion: 消融框架能够明确各组件的相对贡献；组合损失与可学习维纳滤波预计能提升结构保真与感知质量。实际结论细节需参考正文与实验表格。

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [16] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: 提出WAVE-DETR：将可见光RGB与声学信号融合，用Deformable DETR与Wav2Vec2端到端联合，实现在复杂环境下更稳健的无人机检测；多种融合策略中门控融合最佳，在ARDrone数据上显著提升mAP，尤其小目标提升11.1%–15.3%。


<details>
  <summary>Details</summary>
Motivation: 单模态视觉无人机检测在噪声、遮挡、远距离、小目标等真实场景下性能受限；鸟类等易与无人机混淆。声学特征可补充视觉弱点，因此需要一个有效的多模态融合检测器与公开数据验证其有效性。

Method: 将Wav2Vec2提取的声学嵌入与Deformable DETR的多尺度特征图进行融合，设计四种融合策略（门控、线性、MLP、交叉注意力），在Drone-vs-Bird与新建含7500+同步图像与音频的ARDrone数据上训练与评测。

Result: 所有融合方式均提高检测性能，门控融合效果最佳：在ARDrone内分布与OOD测试上，小尺寸无人机在IoU 0.5–0.9区间mAP提升11.1%–15.3%；中大尺寸也有提升，整体各尺寸平均增益3.27%–5.84%。

Conclusion: 视觉+声学多模态融合能显著增强真实场景无人机检测的鲁棒性；门控融合在该设置下最优，说明声学信号为小目标检测提供关键补充。新ARDrone数据集为未来研究提供基准。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [17] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 提出“代理监督”（surrogate supervision）训练范式：在训练时将估计的形变应用到可控的代理图像/域上计算监督，从而解耦输入域与监督域，显著提升深度配准网络在伪影、视野不一致与模态差异等输入变化下的鲁棒性与泛化能力，并在三类任务上验证效果优异且不增复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有深度可变形配准在标准数据上准确，但对输入变化（伪影、视野不匹配、跨模态差异）敏感，导致临床与多中心应用中性能退化。需要一种在不增加模型复杂度的前提下提升鲁棒性与泛化的通用训练范式。

Method: 提出代理监督：用网络从真实输入对估计形变，但将该形变应用到“代理图像/域”（如无伪影、统一视野或同模态空间）上计算监督信号（相似性/正则等），从而将输入域与监督域解耦。框架在三类任务上实现：抗伪影脑MR、无需掩膜的肺CT、以及多模态MR配准。

Result: 在上述三种代表性任务中，对不均匀场、视野不一致、模态差异等变化保持显著鲁棒性，同时在高质量、精心整理的数据上维持与或接近于现有最佳性能。

Conclusion: 代理监督为训练鲁棒且可泛化的配准网络提供了一个有原则且通用的框架，无需增加模型结构复杂度。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [18] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 提出结合卷积自编码器(AE)与视觉Transformer(ViT)的框架，用于法医牙龄评估中第二、第三磨牙分期；相较基线ViT显著提升分类准确率，并通过潜空间与重建分析诊断出第三磨牙数据的类内差异大是主要瓶颈，强调单一可解释方式（如注意力图）不足。


<details>
  <summary>Details</summary>
Motivation: 法医高风险场景（如牙龄评估）对模型的可靠性与可解释性要求高，但深度学习常被视为“黑箱”。同时，在同一任务中第二与第三磨牙自动分期表现差异显著，亟需方法既提升性能又解释差距来源，以更好辅助专家决策。

Method: 构建AE+ViT的混合框架：先用卷积自编码器学习影像潜空间与重建，结合ViT进行分类；通过潜空间度量与重建质量分析数据特性与不确定性；对比仅用ViT的基线，并检视注意力图与多模态诊断信号的差异。

Result: 分类准确率提升：牙37由0.712→0.815，牙38由0.462→0.543。潜空间与重建分析显示第三磨牙（牙38）存在较高类内形态变异，解释了仍存性能差距。注意力图虽解剖上看似合理，但未能暴露数据层面问题。

Conclusion: 该框架在提升准确率的同时提供多维度、数据中心的诊断可解释性，可揭示模型不确定性的来源（如类内变异），比单一注意力可视化更可靠；为法医年龄评估提供更稳健的模型与决策支持，并提示改进方向在于数据质量与均衡。

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [19] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: 提出SCoDA：在源数据不可用的场景下，用自监督教师+几何流形对齐的空间相似损失，配合EMA教师更新，实现源自由领域自适应，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA依赖有监督源模型并用余弦相似度对齐L2归一化特征，丢失了源模型潜在流形的重要几何信息，且容易灾难性遗忘。希望在无监督预训练起点下保留/利用几何结构并稳健适配目标域。

Method: 1) 用完全自监督(SSL)预训练的教师模型初始化，不依赖有标注源数据；2) 学生在目标域上训练，目标包含：实例级特征匹配(蒸馏) + 空间相似性损失(对齐潜在流形几何)；3) 用EMA更新教师参数以缓解灾难性遗忘，形成教师-学生持续自适应框架。

Result: 在多项基准数据集上进行广泛实验，SCoDA在SFDA任务上明显优于现有最先进方法。

Conclusion: 通过自监督教师初始化与几何流形对齐的组合，并以EMA维持知识，SCoDA在源自由领域自适应中更有效稳健，取得了显著性能提升。

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [20] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 提出零样本细胞跟踪框架，将通用分割大模型SAM2纳入时序跟踪流程，在无需标注与微调的条件下，于2D与大规模3D显微序列中取得接近或可比的准确率，同时具备跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有细胞跟踪深度学习方法依赖大量人工标注，成本高、耗时长，且对未见数据泛化差；显微序列存在分裂事件、低信噪、边界模糊、密集聚集与外观相似等挑战，亟需无需标注且可泛化的方案。

Method: 构建零样本的细胞跟踪管线，将通用图像/视频分割基础模型SAM2集成，用其在2D与3D时序显微图像中产生实例分割/掩膜并随时间关联，结合分裂检测逻辑实现跟踪与有丝分裂事件识别，全流程不依赖特定数据集训练或微调。

Result: 在多种2D与大规模3D时间序列显微数据上取得与现有方法相当的精度（competitive accuracy），在无需数据集特定适配的情况下实现稳定跟踪与分裂检测性能。

Conclusion: 零样本、完全无监督的SAM2驱动跟踪框架能跨多样显微数据泛化，减少标注与适配成本，同时保持较高准确度，为细胞跟踪与分裂检测提供可扩展的通用方案。

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [21] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: 提出一种将任意在线2D多摄跟踪系统扩展到3D的通用方法：用深度信息把目标重建到点云，聚类并细化航向得到3D框，并以局部ID一致性增强在线跨帧全局ID关联；在AI City 2025 3D MTMC上获第3名。


<details>
  <summary>Details</summary>
Motivation: 现有MTMC在有深度与标定时可投影到3D获得更高感知，但将2D流水线整体替换为3D代价高、不可行。需要一种能复用现有2D系统、平滑迁移到3D的方案，同时提升跨相机的ID一致性。

Method: 在任意在线2D多摄跟踪输出基础上，利用深度将目标重建到点云；对点云进行聚类得到实例，再通过航向(yaw)细化恢复稳定的3D包围框；提出增强型在线数据关联策略，利用目标的局部ID一致性来进行跨帧全局ID分配。

Result: 在2025 AI City Challenge的3D MTMC数据集上评测，排行榜第3名，显示方法有效。

Conclusion: 该框架无需重写2D组件即可扩展到3D，通过点云重建与航向细化获取高质量3D框，并借助改进的数据关联提高全局ID一致性，具有通用性与实用价值。

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [22] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 将指代表达理解（REC）改写为“候选框逐一的视觉-语言真/假验证”流程，在零样本条件下（无需REC特训）即可达到并超越多项强基线与已训练模型。


<details>
  <summary>Details</summary>
Motivation: 传统REC依赖专门训练的定位模型，易受跨框竞争影响且缺乏对不确定性/多目标的处理。作者想探究：是否通过合理的工作流设计，利用通用检测器+通用VLM，在零样本下也能获得强性能，并检验验证式而非选择式提示是否更优。

Method: 1) 使用通用目标检测器（YOLO-World，COCO-clean）生成候选框；2) 将每个候选区域与文本指令组成True/False判别问题，由通用VLM独立判断是否匹配；3) 独立逐框验证以减少跨框干扰，允许拒答（abstention）与多重匹配；4) 进行对照实验：与选择式提示、与（零样本/已训练）GroundingDINO和GroundingDINO+CRG比较，并在相同候选框下对比。

Result: 在RefCOCO/RefCOCO+/RefCOCOg数据集上：该零样本验证流程不仅超过零样本GroundingDINO，还超过在REC上训练的GroundingDINO与GroundingDINO+CRG；在相同proposal下，“验证式”显著优于“选择式”提示；结论在开放VLM上也成立。

Conclusion: 无需REC特定预训练，通过恰当的工作流（检测+逐框真/假验证）即可强力推动零样本REC性能，强调流程设计比任务专训更关键；方法具备可拒答与多匹配的实用优势。

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [23] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: 提出一种针对小样本类别不平衡的分割增强方法：随机投影复制-粘贴（RPCP），通过提取稀有虫害斑块、随机几何变换、语义合理粘贴与随机投影滤波融合，显著提升虫害类分割而不损伤其他类。


<details>
  <summary>Details</summary>
Motivation: 虫害像素在小麦叶片病虫害分割中极其稀少，导致模型偏向常见类别、稀有类别学习不足并总体性能下降，需要一种兼顾稀有类学习且不破坏整体分布的简单有效数据增强策略。

Method: 从标注训练图中提取稀有“虫害”斑块；对斑块施加随机几何变换（旋转、缩放、翻转等）以增加形态多样性；在不与病斑或已有虫害区域重叠的“合适背景”上粘贴；对粘贴区域施加随机投影滤波以细化局部特征并与新背景自然融合。

Result: 在目标数据集上，虫害类别分割显著提升，同时其他类别保持或略有提升，表明该有针对性的增强有效缓解极端像素不平衡。

Conclusion: 面向农业图像分割的RPCP是一种简洁而有效的定向增强方法，可缓解稀有类像素失衡并提升整体性能，具有推广潜力。

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [24] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 提出一个结合不确定身份信息与跟踪的HMM框架，利用零散的真实身份提示（如喂食器识别）纠正长期MOT中的ID切换，显著提升ByteTrack与FairMOT在猪群视频和MOT17/20数据集上的F1，且对身份噪声鲁棒、随提示频率增加而更优。


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪在长时段视频中因ID切换累计导致性能下降，难以进行分钟级行为分析；而现实场景（如畜牧业）可获得零星身份线索，如何将含不确定性的身份提示有效注入跟踪以缓解长期漂移是待解问题。

Method: 提出基于隐马尔可夫模型（HMM）的融合框架：将轨迹状态与身份作为隐变量，检测与零散身份读数（含不确定性）作为观测，联合建模转移与观测概率；在现有MOT（如ByteTrack、FairMOT）产生的轨迹之上，用HMM对轨迹-身份进行后验推断与纠错，实现对ID切换的平滑与重连，并可处理不确定身份输入。

Result: 在10分钟猪只跟踪数据（喂食点提供21次身份）上，相比带重识别的ByteTrack，F1显著提升；对身份不确定度具有鲁棒性，且身份提供越频繁性能越好；在MOT17、MOT20基准上，结合ByteTrack与FairMOT同样获得改进。

Conclusion: 利用HMM将稀疏且不确定的身份提示与MOT结果融合，可有效缓解长期ID切换并提升长期跟踪性能；方法通用、可叠加到主流MOT器上，且代码与10分钟猪只数据已开源。

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [25] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 综述事件相机与传统帧相机融合在视频修复与3D重建中的进展，聚焦时域与空域增强方法、数据集与挑战，梳理深度学习驱动的最新成果与趋势。


<details>
  <summary>Details</summary>
Motivation: 事件相机具备超低延迟、低功耗与超高时间分辨率，但单独使用或传统帧相机单独使用各有局限；通过两者融合可在复杂动态与极端成像条件下显著提升视觉质量与鲁棒性，需要系统化梳理方法、应用与数据资源以指引研究。

Method: 系统性文献调研与分类：按任务将深度学习方法划分为时域增强（插帧、去运动模糊）与空域增强（超分、弱光/HDR增强、伪影抑制）；同时综述事件驱动融合在3D重建中的演进；对代表性工作进行对比与讨论，并整理公开数据集以支持复现与基准测试。

Result: 给出事件-帧融合在多类视觉复原与重建任务中的最新方法脉络与性能概况，揭示在快速运动、低光、高动态范围等挑战场景中的显著优势；提供全面数据集清单与实践要点，指出当前方法的有效性与局限。

Conclusion: 事件相机与帧相机的深度学习融合已在视频复原、增强与3D重建中展现出明显收益；未来应在更稳健的跨模态对齐、端到端可泛化模型、统一评测协议与更丰富的数据集上深化研究，以推动实际应用落地。

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [26] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: 提出ISTASTrack：首个采用Transformer的ANN-SNN混合RGB-Event目标跟踪器，通过ISTA适配器实现双分支特征双向融合，并用时间下采样注意力对齐SNN多步与ANN单步特征；在FE240hz、VisEvent、COESOT、FELT上达SOTA且节能高效。


<details>
  <summary>Details</summary>
Motivation: RGB图像与事件数据互补：RGB擅长空间纹理，事件流提供高时间分辨与动态鲁棒性。但现有ANN难以充分利用稀疏、异步事件；混合ANN-SNN虽有效，但跨范式特征融合困难、时序对齐问题突出。

Method: 双分支Transformer：RGB分支用ViT提取空间上下文，事件分支用SNN Transformer捕获时空动态；提出基于稀疏表示理论的ISTA适配器，通过展开迭代收缩阈值算法实现可学习的双向特征交互；在适配器中加入时间下采样注意力，将SNN多时间步特征与ANN单步特征在潜在空间中对齐以增强时序融合。

Result: 在FE240hz、VisEvent、COESOT、FELT等RGB-Event跟踪基准获得SOTA性能，同时保持高能效，验证了方法的有效性与实用性。

Conclusion: 利用ANN-SNN混合与基于ISTA的可解释适配器，可有效弥合模态与范式差异，实现强鲁棒、能效兼顾的RGB-Event跟踪；为未来混合范式视觉感知提供了可推广的融合框架。

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [27] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出一种基于多重深度状态空间模型的太阳耀斑72小时内最大等级预测方法，并配合FLARE损失处理极端类别不均衡，在11年多波段数据上相较基线在GMGS与TSS两项标准指标上同时提升性能与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有太阳耀斑预报对关键基础设施防护至关重要，但预测能力不足，尤其因耀斑等级分布极度不均衡，导致模型偏向多数类、置信度不可靠，难以支撑实用化决策。

Method: 1) 设计由多个深度状态空间模型（DSSM）组成的框架，挖掘多波段太阳影像的时空动态特征以预测未来72小时内最大耀斑等级。2) 提出FLARE损失（frequency & local-boundary-aware reliability loss），结合类别频率重加权与局部决策边界/校准项以同时优化区分度与概率可靠性，缓解类别不均衡。3) 在覆盖一个11年太阳活动周期的多波段数据集上训练与评估，并与多种基线比较。

Result: 在基准数据集上，相较现有方法，所提模型在Gandin-Murphy-Gerrity score（GMGS）与True Skill Statistic（TSS）两项标准指标上均显著领先，表现出更好的分类性能与更高的预测可靠性。

Conclusion: 基于多重DSSM并配合FLARE损失的方案能有效处理太阳耀斑预测中的类别不均衡问题，提升72小时最大耀斑等级预测的准确性与可靠性；为空间天气预报提供更稳健的工具，具有工程应用潜力。

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [28] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: 提出TUNI，一种将多模态特征提取与跨模态融合统一在同一RGB-T编码器中的方法，通过RGB+伪热成像的大规模预训练与精简热分支，实现更高效、更实时的RGB-热红外语义分割；在FMB、PST900、CART上达到SOTA级表现，并在Jetson Orin NX上27 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T分割多用RGB预训练编码器分别处理RGB与热数据，并在后端做额外的跨模态融合，导致：热模态特征提取受限、融合不足、双编码器结构冗余且实时性差。需要一种能在编码阶段即统一完成特征提取与融合、同时更轻量高效的架构。

Method: 提出TUNI：1) 设计统一的RGB-T编码器，由多层堆叠模块同时进行多模态特征提取与融合；2) 使用RGB与伪热数据进行大规模预训练，使编码器学会统一的提取-融合策略；3) 精简热分支以获得更紧凑的结构；4) 提出RGB-T Local Module，利用自适应余弦相似度选择性增强RGB与热模态在局部的一致与差异显著特征，从而强化局部跨模态融合。

Result: 在FMB、PST900、CART数据集上取得与SOTA相当的精度，同时参数量和计算量更低；在Jetson Orin NX上推理速度达27 FPS，验证部署级实时性。

Conclusion: 统一式RGB-T编码器与局部融合模块有效提升了热模态特征表达与跨模态融合效率，在保持或提升准确率的同时显著降低模型复杂度与推理成本，具备实用的实时部署价值。

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [29] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出一种基于“少量部件示例”的字体生成模型：仅用若干局部形状，就能推断并生成整套字体。


<details>
  <summary>Details</summary>
Motivation: 传统少样本字体生成仍需少量“完整字符”作示例，获取成本高、设计迭代慢，且难以研究局部设计元素对整体字形结构的影响。作者希望降低示例获取难度，提升创作效率，并建立从“局部设计→整体字形”的一致性建模，以解释局部细节如何支配全字结构风格。

Method: 构建“少部件示例→整字形”的生成框架：以局部曲线/笔画/部件的形状与风格特征为条件输入，通过学习到的全字形结构先验与部件到全字的映射，将局部风格外推到完整字符；相较传统方法，不再依赖完整字符示例，而是对局部形状进行编码与风格迁移并结合结构约束进行全字合成。

Result: 在仅提供部分形状的条件下，模型可生成风格一致、结构合理的整套字符；在效率上减少示例采集与标注成本；定性/定量评估显示与传统少样本（需完整字符）方法相当或更优的保真度与风格一致性，并展示了局部设计变化对全字整体的可控影响。

Conclusion: 以少量局部形状即可驱动整套字体生成，显著提升设计效率，并为“局部到全局”的字形风格传递提供可解释视角；未来可扩展到更多文字脚本、更复杂部件关系与交互式设计流程。

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [30] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: 提出一套面向微/纳无人机的高效低功耗VIO管线，在RISC‑V超低功耗并行SoC上量化优化并实机验证；利用刚体运动模型提升平面运动精度，ORB版RMSE较基线最高降至3.65倍；PX4FLOW在<24像素/帧速度下以更低运行时达与ORB相当精度。


<details>
  <summary>Details</summary>
Motivation: 现有高精度VIO多依赖算力强的平台，不适合资源受限的微控制器/微型无人机；需要在超低功耗芯片上实现实时、准确且可部署的VIO，并明确不同特征跟踪器在算力与精度间的权衡。

Method: 构建一条VIO管线，集成并比较三类特征检测/跟踪器（SuperPoint、PX4FLOW、ORB），在RISC‑V超低功耗并行SoC（GAP9）上进行量化与算子级优化；引入刚体运动模型以降低平面运动估计误差；从计算负载与量化后跟踪精度评估实时性；在真实硬件上实现与验证。

Result: 在GAP9上，优化后的管线相较基线（使用ORB跟踪）平均RMSE最高降低至3.65×；复杂度分析显示PX4FLOW在运动速度低于24像素/帧时，以更低运行时间达到与ORB相当的跟踪精度；整体实现满足超低功耗SoC的实时性并保持高精度。

Conclusion: 该设计在超低功耗RISC‑V SoC上实现了面向微/纳UAV的高精度实时VIO，弥合了高算力平台与微控制器实现之间的鸿沟；刚体运动模型对平面运动有效；在低速场景下PX4FLOW是更高性价比选择，而在更复杂/高速情形下ORB可能更优。

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [31] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出MLANet：从单张野外2D人脸图像重建3D人脸（几何、纹理、姿态、光照）的CNN方法，采用分层骨干与多级注意力、半监督（3DMM参数+可微渲染器）端到端训练，并在AFLW2000-3D与MICC Florence上做定量定性、对比与消融实验，验证有效。


<details>
  <summary>Details</summary>
Motivation: 在野外条件下从2D图像恢复3D人脸应用广泛，但缺乏带真值的训练数据且环境复杂，现有方法难以兼顾精细细节与鲁棒性，需一种可在真实场景下有效学习的模型与训练范式。

Method: 提出分层多级注意力网络（MLANet）：采用预训练的层级式骨干网络，在特征提取的不同阶段引入多级注意力机制，预测3DMM参数（几何与纹理）、姿态与光照；使用半监督策略，结合公共数据集提供的3DMM参数监督与可微渲染器进行重投影一致性，实现端到端训练；在3D重建与对齐任务上进行对比与消融。

Result: 在AFLW2000-3D与MICC Florence数据集上，MLANet在3D人脸重建与3D对齐指标上取得优于或可比于现有方法的定量结果，并提供高质量的可视化；消融显示多级注意力与半监督渲染损失均有显著贡献。

Conclusion: 多级注意力结合分层骨干与可微渲染的半监督训练，可在缺乏真值与复杂场景下实现高精度3D人脸重建与对齐；方法有效且具实用性。

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [32] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: 提出LaV-CoT：面向多语种多模态VQA的语言感知视觉链式推理框架，通过多阶段可解释推理与多维奖励优化，显著提升mVQA性能，超过同规模与更大规模开源模型及部分商用模型。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在mVQA中虽进步明显，但大多依赖纯文本CoT，缺乏对多语种与多模态（视觉+语言）协同推理的系统支持，影响真实场景部署与可解释性。需要一种能在多语环境中同时处理图像细节、空间关系与语言一致性的可扩展方法。

Method: 1) 设计语言感知视觉CoT流水线：文本摘要含BBox、语言识别、空间对象级描述、逐步逻辑推理；2) 自动数据生成：迭代生成-纠错-精炼，产生高质量多语CoT标注；3) 训练范式：SFT + 语言感知GRPO（组相对策略优化），以可验证的多维奖励（语言一致性、结构正确性、语义对齐）指导；4) 多方面评测与在线A/B测试验证。

Result: 在MMMB、Multilingual MMBench、MTVQA等数据集上，相比同规模开源基线最高提升约9.5%准确率，并以约2.6%超越2倍规模模型；同时优于GPT-4o-0513与Gemini-2.5-flash等商用模型。在线A/B测试显示在真实业务数据上亦有效。

Conclusion: LaV-CoT通过语言感知的视觉CoT和多维奖励强化，显著提升多语种多模态推理能力与可解释性，具备可扩展的数据构建与工业可用性。代码已开源。

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [33] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出一种无需训练的T2I颜色对齐框架：用LLM消解含糊颜色词，并在文本嵌入空间进行基于CIELAB关系的颜色混合，引导扩散模型生成更准确颜色且不损画质。


<details>
  <summary>Details</summary>
Motivation: 扩散模型对细腻/复合颜色词（如“Tiffany蓝、酸橙绿、艳粉”）常出现偏色，与人类意图不符；现有方案依赖注意力操作、参考图像或微调，却未系统解决颜色语义歧义。

Method: 两阶段训练免方法：1) 利用LLM解析并消解提示词中的含糊颜色术语，得到明确的颜色描述；2) 将解析出的颜色在CIELAB空间建模其空间关系，在文本嵌入空间执行对应的颜色混合/引导操作，从而细化文本嵌入，以引导扩散模型生成目标颜色。无需额外数据或模型训练。

Result: 实验显示在不牺牲图像质量的前提下显著提升颜色对齐/准确度，相比现有方法更稳定、无需参考图像与微调。

Conclusion: 通过LLM消歧与CIELAB引导的文本嵌入操作，可在训练免设定下提升T2I颜色一致性，缩小文本语义与视觉生成的鸿沟。

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [34] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: 提出AVI-Math基准，系统评测VLM在无人机航拍视角下的多模态数学推理；覆盖6学科20主题、3773问，显示14个主流VLM在几何/逻辑/代数等任务上表现欠佳；链式思维提示与微调有改进潜力，指向面向真实UAV应用的可信VLM研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多在自然图像与简单计数上评测，缺乏对UAV场景下严谨数学推理（距离/面积/轨迹/空间分析）的检验，影响其在遥感与无人机任务中的可靠落地。

Method: 构建AVI-Math基准：从不同高度与视角的UAV图像中收集车辆相关问题，覆盖6类数学科目与20个主题，设计高质量多样化题目；对14个代表性VLM进行系统评测；开展误差与能力缺陷分析；探索链式思维提示和微调以提升推理表现。

Result: 在AVI-Math上，14个现有VLM普遍显著失衡与失准，尤其在几何、逻辑和代数等需要精确空间与数量推理的任务上表现不佳；链式思维与微调带来一定提升但仍未达理想水平。

Conclusion: 当前VLM在UAV航拍场景的数学推理能力不足，限制其在真实遥感应用中的可信性；AVI-Math为社区提供了标准化评测平台，并指明通过更强的推理建模、领域知识纳入与训练策略改进等方向推进可信UAV-VLM的发展。

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [35] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: 提出BEVTraj：在鸟瞰(BEV)空间、仅用实时传感器数据、无需预构建HD地图的端到端轨迹预测框架，借助可变形注意力提取上下文并用稀疏目标候选(SGCP)实现无后处理推断，性能可与依赖HD地图的SOTA相当且更灵活。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测多依赖HD地图或在线局部建图，但HD地图区域受限且难应对瞬时变化；在线建图只识别预定义元素，可能遗漏关键细节或引入误差，进而影响预测准确与稳健性。需要一种摆脱预建地图、直接从传感器中获取环境语义并进行精确预测的方法。

Method: 1) 将多模态实时传感器数据投影/编码到BEV特征空间；2) 采用可变形注意力，在密集BEV特征中高效聚合与目标相关的上下文；3) 设计稀疏目标候选提议(SGCP)模块，直接产生多模态未来轨迹候选并选择/回归，无需后处理；4) 端到端训练整套网络。

Result: 在标准数据集的广泛实验中，BEVTraj在核心指标上达到与依赖HD地图的SOTA方法相当的准确度，同时保持鲁棒与高效；并展示在无需任何预建地图的前提下具备强泛化与实时性（文中称“可比SOTA，且更灵活”）。

Conclusion: BEVTraj证明了仅依赖实时传感器、在BEV空间端到端预测的可行性与竞争力：通过可变形注意力与SGCP消除对HD地图和后处理的依赖，在保持精度的同时提升部署灵活性与适应动态场景的能力。

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [36] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: 提出一种利用多视角信息训练以提升多人解析在遮挡场景下表现的方法，包含弱监督的实例层监督与多视角一致性损失，并通过半自动标注管线生成实例掩码；在遮挡场景相比基线相对提升最高4.20%。


<details>
  <summary>Details</summary>
Motivation: 现有最先进方法在公开数据集上表现良好，但在多人身体相互遮挡、重叠时分割和实例归属明显退化。直觉是：从另一视角看，重叠的人可能被自然分离，因而可用多视角互补信息缓解遮挡问题；然而缺乏适配的数据与标注。

Method: 提出一个训练框架：在训练期引入多视角信息，采用对人体实例的弱监督（由多视RGB+D与3D骨架半自动生成的实例分割掩码）以及多视角一致性损失，促进不同视图下对同一人的部位分割与实例关联保持一致。并设计半自动标注策略从多视RGB+D与3D姿态生成实例掩码。

Result: 在遮挡场景中，相比基线的多人解析性能获得最高4.20%的相对提升，证明该训练框架在处理重叠人体方面有效。

Conclusion: 多视角引导的弱监督与一致性约束能缓解遮挡带来的实例与部位混淆，提升多人解析精度；半自动数据标注为该任务提供了可行的数据支撑。

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [37] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: 提出VARCO-VISION-2.0：开源权重、支持韩英双语的视觉-语言模型，强化多图理解、版面感知OCR与空间定位；经四阶段课程与高效训练优化，对齐更好、语言与安全性提升；14B在同规模OpenCompass VLM榜单获第8，并发布适用于端侧的1.7B版本。


<details>
  <summary>Details</summary>
Motivation: 提升韩英双语场景下的视觉-语言理解能力，尤其是多图文档、图表、表格等复杂布局的处理，并在保持语言能力与安全性的同时改进多模态对齐和空间定位；同时兼顾大模型性能与小模型端侧部署需求。

Method: 采用四阶段课程式训练与内存高效技术，进行多模态对齐与指令微调；引入多图输入能力与版面感知OCR（同时预测文本与空间位置）；通过偏好优化提升安全与输出质量；提供14B与1.7B两种规模实现。

Result: 在多项基准上展现出强空间定位与竞争性双语表现；14B模型在OpenCompass VLM同规模榜单中排名第8；保持核心语言能力并改善安全表现；发布可实际部署的1.7B轻量版。

Conclusion: VARCO-VISION-2.0推进了韩英双语VLM的发展：在多图理解、布局OCR与空间对齐上显著增强，同时兼顾性能与部署实用性；提供14B与1.7B两种开源权重版本，方便研究与应用落地。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [38] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: 提出一种轻量高效的人脸图像质量评估（FIQA）方法：用MobileNetV3-Small与ShuffleNetV2组成小型双分支集成，预测层平均融合，并引入结合MSE与皮尔逊相关正则的相关感知损失MSECorrLoss；在VQualA基准上达到SRCC 0.9829、PLCC 0.9894，在保持效率约束下兼顾精度与计算成本。


<details>
  <summary>Details</summary>
Motivation: 通用无参考图像质量评估难以捕捉人脸特有退化（姿态、遮挡、表情、对齐等），而现有SOTA FIQA模型计算开销大、难落地；需要一种既贴近人类主观感知、又可在非受控真实场景高效运行的FIQA方法。

Method: 采用两种轻量CNN（MobileNetV3-Small、ShuffleNetV2）并行作为基学习器，通过简单平均进行预测级融合；为提高与人类感知一致性，设计MSECorrLoss，将MSE主损失与皮尔逊相关正则项联合优化，使模型同时最小化误差并最大化与主观分数的线性相关。

Result: 在VQualA基准上，模型在满足比赛效率限制的前提下，取得SRCC 0.9829、PLCC 0.9894，显示与人工评分高度一致且具有良好排序与线性拟合能力。

Conclusion: 所提FIQA方法在准确性与计算效率间实现有效权衡，适合真实世界部署；相关感知损失与轻量模型集成是提升感知一致性与实用性的关键。

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [39] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出一种用于真实图像超分辨的“可控真实感的一步扩散”（RCOD），在保持一步扩散高效性的同时，通过潜在域分组、退化感知采样与视觉提示注入，实现对保真度-真实感权衡的显式可控，并在定量与视觉上优于现有一步方法。


<details>
  <summary>Details</summary>
Motivation: 一步扩散（OSD）效率高但通常以单时间步训练/蒸馏，缺乏如多步扩散那样可通过调采样步数在不同场景下灵活平衡保真与真实感的机制；现实世界退化复杂多变，现有方法难以兼顾多样退化下的重建精度与感知质量。

Method: 1) 提出RCOD框架：在噪声预测阶段引入潜在域分组（latent domain grouping），以极少训练改动与原始数据即可对保真-真实感进行显式控制；2) 退化感知的采样策略，将蒸馏正则与分组策略对齐，增强权衡控制；3) 视觉提示注入模块，以退化感知的视觉token取代文本提示，提高还原精度与语义一致性。

Result: 在不显著增加计算开销的前提下，RCOD在多组真实世界超分实验中，在客观指标（如保真度相关指标）与主观感知质量上均优于最新一步扩散方法；并能在推理阶段灵活调节真实感。

Conclusion: RCOD弥补了一步扩散在可控性上的短板，以简单训练改动实现对保真-真实感的显式可控与更优的综合性能，为真实世界超分提供了高效、可控且效果更佳的方案；代码将开源。

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [40] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: 提出Grad-CL，一种源数据不可用的领域自适应分割框架，通过梯度引导的伪标签精炼与基于余弦相似度的对比学习，提升视盘/视杯跨域分割精度与边界质量。


<details>
  <summary>Details</summary>
Motivation: 跨域（不同设备/成像协议）导致视盘/视杯分割模型在目标域性能显著下降；实际场景常无法访问源数据，需在仅有源模型与无标注目标数据下进行稳健自适应。

Method: 两阶段：1）梯度引导伪标签精炼——利用梯度提取显著类特征，做不确定性评估与原型估计，从而修正噪声伪标签；2）基于余弦相似度的对比学习——以梯度知情的特征为锚，显式增大视杯与视盘的类间可分性，强化边界判别。

Result: 在多组具有域移的眼底图像数据上，较当前SOTA的无监督/源自由DA方法获得更高的分割精度与更好的边界描绘性能。

Conclusion: Grad-CL在无需源数据的前提下，通过梯度辅助的伪标签优化与对比学习，实现对目标域的稳健适配，适合推广到跨域医学图像分割场景；代码已开源。

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [41] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: 提出VQBridge与学习退火结合，解决VQ训练不稳与码本利用率低问题，实现100%码本使用（FVQ），在重建与下游图像生成上达SOTA，并具备可扩展与通用性。


<details>
  <summary>Details</summary>
Motivation: 现有图像离散分词器中的向量量化训练受直通估计偏差、延迟更新与稀疏梯度影响，导致重建性能欠佳、码本利用率低，限制了更大码本与更强生成模型（如自回归）的潜力。

Method: 提出VQBridge：基于map function的稳健可扩展投影器，采用“压缩-处理-恢复”的管线以稳定更新码向量；与学习退火（learning annealing）联用，在训练与码本扩张过程中维持高（达100%）码本使用率；适配多种VQ变体与不同码本规模/通道数。

Result: 在多种配置下实现100%码本使用（包括26.2万规模码本）；达到SOTA重建表现，随着更大码本/更多向量通道/更长训练持续提升；泛化到不同VQ变体；与LlamaGen整合后，图像生成显著提升，rFID优于视觉自回归模型0.5、优于扩散模型0.2。

Conclusion: 通过VQBridge+学习退火形成的FVQ可稳定高效地训练大规模码本并充分利用，显著提升重建与下游生成质量，表明高质量离散分词器对强自回归图像生成至关重要。

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [42] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock 利用“由浅到深层收敛”的训练现象，在视频MAE中按层逐步冻结参数，把像素级预测平滑过渡到潜表示预测，实现更快训练、避免表示坍塌，并在多达40亿参数模型与4DS套件上优于非潜掩码预测。


<details>
  <summary>Details</summary>
Motivation: 视频MAE训练中不同深度层的收敛速度不一致：浅层先收敛、深层后收敛。这一规律可能导致冗余计算、训练低效；同时，直接做潜表示预测常出现表示坍塌问题。作者希望既加速训练、又安全地使用潜表示预测。

Method: 基于ViT层“按深度依次收敛”的观察，提出显式时间表逐步冻结网络层（progressive layer freezing）。随着训练推进，先冻结已收敛的浅层，让计算集中于未收敛的深层；并用同一冻结计划把学习目标从像素重建逐步过渡到潜表示预测，实现从像素到潜的平滑切换，避免坍塌。方法可扩展至大型模型（至4B参数）。

Result: 在4DS感知套件上，采用LayerLock的模型优于传统非潜掩码预测基线；训练更高效（因大量层被提前冻结），且在大规模模型（至40亿参数）上表现出色；潜表示训练未出现表示坍塌。

Conclusion: 利用层收敛次序进行逐步冻结，可同时带来训练加速与性能提升，并提供一种稳定的潜表示预测路径。LayerLock在大模型和下游评测上验证了优越性，相较标准MAE更高效、也更强。

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [43] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 对比隐式与显式新视角合成在空间场景3D重建中的表现：外观嵌入提升了成像一致性但并未显著改善几何精度；凸面splatting比高斯更紧凑、少杂散，适合安全关键任务。


<details>
  <summary>Details</summary>
Motivation: 空间机器人任务（交互、避碰）要求高几何精度与可解释的表示。现有NVS方法常借助外观嵌入拟合光照/外观变化，但其对几何恢复是否有实质帮助不明；同时，显式与隐式方法在效率与精度间的权衡需要系统评估。

Method: 基于SPEED+数据集，系统比较三类方法：K-Planes（隐式/混合）、Gaussian Splatting与Convex Splatting（显式）。在是否使用外观嵌入的设置下，评估 photometric 误差、几何精度（重建误差）、以及所需原语数与表示紧凑性/杂散度。

Result: 外观嵌入显著降低光度误差，但对几何精度提升有限或无显著影响；在显式方法中，嵌入主要减少所需原语数量。凸面splatting比高斯splatting更紧凑、少杂点和杂散结构。

Conclusion: 外观嵌入对以几何为中心的任务帮助有限，不能替代几何约束；选择方法需在重建质量与表示效率间权衡。凸面splatting因紧凑与低杂散更适合空间机器人等安全关键应用。

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [44] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 提出GAMMA训练框架，通过多样化操控与多任务监督提升AI生成图像检测的跨模型泛化与鲁棒性，在GenImage基准上SOTA并对新模型如GPT-4o保持强健。


<details>
  <summary>Details</summary>
Motivation: 现有检测器对未见生成模型泛化差，因依赖生成器特定伪迹（风格先验、压缩模式）；需要一种减少域偏置、强调语义一致性的训练机制。

Method: 设计GAMMA训练框架：1) 多样化操作生成训练样本，包括基于修复(inpainting)的操控和保持语义的扰动，确保真假内容在语义上一致；2) 多任务监督，含两个分割头+一个分类头，实现像素级来源归因与图像级判别；3) 反向交叉注意力，让分割头引导分类分支，纠正其偏置表征。

Result: 在GenImage基准上取得SOTA，准确率提升5.8%；对新发布生成模型（如GPT-4o）也表现出强鲁棒性。

Conclusion: 通过减少域偏置、增强语义对齐并引入分割引导的注意力，GAMMA显著提升AI图像检测的跨域泛化与实用鲁棒性，优于现有方法。

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [45] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 评测三种胎儿脑MRI三维超分重建（NiftyMIC、SVRTK、NeSVoR）在健康与病理（脑室扩大）数据上的表现：NeSVoR成功率最高且稳定；不同方法产生显著体积差异，但对VM诊断分类无显著影响。


<details>
  <summary>Details</summary>
Motivation: 胎儿运动导致多视角2D序列存在低分辨率与运动伪影，限制3D解剖评估与后续体积分析/诊断。现有SRR方法众多，但在病理案例中的对比表现及对下游体积测量与诊断任务的影响缺乏系统评估。

Method: 对140例胎儿脑MRI（含健康与脑室扩大）分别用三种先进SRR（NiftyMIC、SVRTK、NeSVoR）进行高分辨率重建；用BoUNTi分割九个主要脑结构并提取体积；比较可视质量、重建成功率、体积一致性及基于体积的VM分类性能。

Result: NeSVoR在HC与PC中均取得>90%的最高且最一致的重建成功率；不同SRR之间体积估计存在显著差异；尽管体积有差异，基于这些体积的VM分类性能在三种方法间无显著差别。

Conclusion: NeSVoR在SRR任务上更为稳健；尽管SRR会引入体积变异，但VM诊断分类表现对SRR方法不敏感，提示下游诊断具有一定鲁棒性。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [46] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: 提出一种用于图像对象移除任务的训练策略——Mask Consistency Regularization（MCR），通过对训练时的掩码进行膨胀与重塑扰动，并约束不同扰动分支与原掩码分支的输出一致性，从而减少掩码幻觉与掩码形状偏置，提升修复的一致性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像修复中仍存在两大痛点：1）掩码幻觉——在被移除区域生成与上下文不相关的臆造内容；2）掩码形状偏置——模型倾向用与掩码形状相似的物体填充而非依据周围语境。需要一种针对对象移除的训练机制来约束模型利用上下文、打破形状先验。

Method: 提出MCR训练策略：在训练阶段对掩码进行两类扰动：掩码膨胀（dilation）与掩码重塑（reshape）。通过构建含原始掩码与两种扰动掩码的多分支训练，并施加跨分支一致性正则，使各分支输出在被遮挡区域与边界处保持一致。膨胀促使模型更好对齐周围内容并抑制边界伪影；重塑迫使模型摆脱对固定掩码形状的依赖，减轻形状偏置。

Result: 实验显示，应用MCR的模型显著减少掩码区域的臆造内容与形状偏置，生成结果在语义与纹理上更契合上下文，客观指标与主观感知均有提升。

Conclusion: MCR作为面向对象移除的通用训练正则，可与扩散式修复框架结合，增强对上下文的一致性建模与鲁棒性，有效缓解掩码幻觉与形状偏置，提升图像修复质量。

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [47] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出MagicMirror框架：构建细粒度生成图像伪影分类体系，标注340K大规模人审数据MagicData340K，训练伪影评测VLM MagicAssessor（含新采样与多级奖励的GRPO），并据此自动化基准MagicBench，发现现有顶尖T2I模型仍存在显著伪影问题。


<details>
  <summary>Details</summary>
Motivation: 当前T2I虽在指令遵循与美学上进步显著，但普遍存在解剖/结构等物理伪影，严重影响可感知质量与落地；现有基准缺乏系统、细粒度的伪影评估体系与标注数据，无法有效诊断与比较模型缺陷。

Method: 1) 构建生成伪影细粒度分类体系；2) 人工精标340K生成图像得到MagicData340K（多维伪影标签）；3) 基于该数据训练VLM评测器MagicAssessor；4) 设计应对类别不均衡与reward hacking的数据采样策略与多级奖励GRPO训练；5) 用MagicAssessor构建自动化评测基准MagicBench以评测各T2I模型伪影。

Result: MagicAssessor可输出细粒度伪影评估与标签；MagicBench对主流T2I模型进行自动化对比，显示即便是顶级模型（如GPT-image-1）也稳定存在显著伪影，定量揭示当前系统性缺陷。

Conclusion: 细粒度伪影评估框架与大规模人审数据结合VLM评测器与GRPO训练，提供了标准化、可扩展的自动化基准；实验表明伪影仍是T2I发展的关键瓶颈，未来需聚焦伪影减少与结构一致性提升。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [48] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: 提出SignClip框架，将手势（手动信号）与口型（非手动信号）融合，并通过分层对比学习在手口与视文两种模态对齐，以提升手语翻译；在PHOENIX14T与How2Sign上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SLT方法多依赖手势等手动信号，忽视了口型等非手动线索；然而口型在区分外观相似手势与承载语言信息中至关重要，缺失导致翻译歧义与性能上限。

Method: 提出SignClip：1) 特征层面融合手势空间特征与唇部运动特征；2) 设计分层（层级）对比学习，包含手-口（sign-lip）对齐与视觉-文本（video-text）对齐的多级目标，确保跨模态语义一致；3) 在无Gloss（不依赖词素中间表示）设置下训练评估。

Result: 在PHOENIX14T与How2Sign两基准上取得领先：以PHOENIX14T为例，Gloss-free设置下BLEU-4从24.32提升至24.71，ROUGE从46.57提升到48.38，相比此前SOTA SpaMo更优。

Conclusion: 融合非手动口型线索并采用分层对比对齐可有效提升SLT性能，验证了口型在消歧与语义捕获中的关键作用；方法通用，可为后续多模态SLT提供参考。

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [49] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: 评估开闭源大型视觉语言模型在文本篡改检测上的表现，发现开源模型仍落后于闭源（如GPT-4o），专为图像篡改检测训练的模型在文本篡改任务上泛化欠佳；并在实景场景文字与仿真身份证两类数据上基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于图像篡改检测中视觉层面的操控，对文本内容被篡改（例如图像中的文字被改动）的检测关注不足；需要系统比较不同类型VLM在此类任务上的能力及其泛化性，尤其是贴近真实滥用场景（如伪造证件）。

Method: 对多种闭源与开源VLM进行基准评测，涵盖普通VLM与专为图像篡改检测训练的VLM；构建/采用两类数据：自然场景文本篡改数据与仿真身份证文本篡改数据；比较模型在不同数据分布中的检测与泛化能力。

Result: 开源VLM“逐步接近但仍落后”闭源模型（如GPT-4o）在文本篡改检测上的表现；针对图像篡改检测优化的VLM在文本篡改任务上出现显著泛化不足；在更具挑战的仿真身份证数据上性能更受影响。

Conclusion: 文本篡改检测对VLM提出更高跨模态与语义鲁棒性要求；当前开源模型尚需提升，专用图像篡改模型缺乏跨任务泛化；应面向真实世界滥用场景设计更具泛化与稳健性的VLM与数据基准。

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [50] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: 提出MCL-AD用于零样本3D异常检测，联合点云、RGB与文本，通过多模态提示学习与协同调制机制，在无标注条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-3D方法多仅用点云，忽视RGB与文本的语义先验，限制了在数据稀缺、隐私或高标注成本场景下的检测能力。需要一个能融合多模态、提升表征与跨模态协同的框架。

Method: 1) 多模态协作学习框架MCL-AD，联合点云、RGB与文本。2) 多模态提示学习机制（MPLM）：设计“与对象无关”的解耦文本提示，并引入多模态对比损失，增强模态内表征与模态间对齐/协同。3) 协同调制机制（CMM）：通过RGB引导分支与点云引导分支的联合调制，充分利用两者的互补性。

Result: 在多组实验中，MCL-AD在零样本3D异常检测上达到当前最优（SOTA）性能，相比仅点云或单模态方法显著提升。

Conclusion: 融合点云、RGB与文本的协同学习与调制机制能在无标注场景下有效提升3D异常检测；所提MCL-AD在ZS-3D任务中取得SOTA，验证方法有效性。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [51] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: 提出一种基于Lipschitz引导的随机深度（DropPath）策略：随层深增加提高丢弃概率，以控制网络有效Lipschitz常数；在ViT-Tiny上既保留干净准确率、提升对抗鲁棒性，又降低FLOPs。


<details>
  <summary>Details</summary>
Motivation: 现有对抗防御要么计算开销大（如对抗训练、多步攻击内循环），要么缺乏形式化保障；同时ViT/深层网络易受对抗扰动。希望以低成本、具可解释性（Lipschitz约束）的方式提升鲁棒性，并兼顾推理效率与准确率。

Method: 设计深度相关的DropPath日程：层越深，drop概率越高，从而抑制深层放大的Lipschitz常数，达到隐式正则化。该策略在训练与推理中控制有效路径长度与梯度放大，兼容ViT结构；与基线和线性DropPath比较。

Result: 在CIFAR-10上以ViT-Tiny为例：在接近基线干净准确率的同时，对FGSM、PGD-20与AutoAttack的鲁棒准确率提升；计算量（FLOPs）显著低于基线与线性DropPath。

Conclusion: Lipschitz引导的深度依赖DropPath能以更低计算成本提升对抗鲁棒性且不牺牲干净精度，为ViT等深层模型提供了一种简单有效的鲁棒性正则化手段。

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [52] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 提出一种基于能量图的概率框架与随机生灭优化，用于复杂城市环境中街道家具（如路灯）高精度定位；在都柏林中心区仿真与真实地理数据上验证，具备可扩展且准确的资产映射能力，代码开源。


<details>
  <summary>Details</summary>
Motivation: 城市资产（路灯、标志牌等）需要精确地理定位以支持城市管理、维护和私营部门运营；现有方法在复杂城市环境中受遮挡、噪声及上下文约束难以综合利用，精度与可扩展性受限。

Method: 构建“能量图”以编码对象位置的空间似然，将能量以地图地理坐标表示，便于融合外部地理信息（GIS层、道路网络、布设约束等）；通过随机生-灭（birth-and-death）过程的优化算法，从能量图中推断最可能的资产配置。

Result: 在以都柏林市中心路灯数据为依据的现实感仿真中评估，展示该方法在复杂环境下的定位精度提升与可扩展性；给出可复现实验与开源实现。

Conclusion: 能量图+生灭随机优化的框架能够有效融合外部地理上下文，实现城市资产的高精度与可扩展定位；为城市基础设施管理提供实用方案，代码已开源以促进复现与扩展。

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [53] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出ClusCa：在扩散Transformer中进行空间聚类的特征缓存，仅计算每个聚类的代表token并将信息传播至同簇其他token，在不训练的前提下显著加速且保持质量（如在FLUX上4.96倍加速，ImageReward达99.49%）。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer虽能生成高质量图像/视频，但迭代去噪计算代价高。已有“时间维度”的特征缓存仅复用相邻时间步的特征，忽略了同一时间步内“空间维度”token之间的相似性，未充分挖掘冗余计算。

Method: 提出Cluster-Driven Feature Caching（ClusCa）：对每个时间步的tokens做空间聚类（cluster），只计算每个簇的一个代表token，将其特征/更新传播到同簇其他tokens，从而将有效token数量减少>90%。该方法与现有时间缓存正交，可直接插拔到任意扩散Transformer（如DiT、FLUX、HunyuanVideo），无需额外训练。

Result: 在文本到图像与文本到视频任务上广泛实验：显著加速且保持或略超原始质量。示例：在FLUX上实现4.96×加速，ImageReward保持99.49%（较原模型提升0.51%）。

Conclusion: 空间聚类驱动的特征缓存能在不训练的情况下大幅减少token计算，和时间缓存互补，通用于多种扩散Transformer，并在不显著损失甚至提升质量的同时带来显著加速。

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [54] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: 提出I-Segmenter：首个全整数ViT语义分割框架，以整数替代浮点算子，并引入λ-ShiftGELU、去除L2归一化、用最近邻替代双线性上采样，实现端到端整数推理与训练；在几乎不牺牲太多精度（平均落后5.1%）的情况下，显著减小模型体积（至多3.8x）并带来最高1.2x推理加速，甚至一张图像的PTQ也具竞争力。


<details>
  <summary>Details</summary>
Motivation: ViT在语义分割上表现强，但在资源受限设备上因内存与算力开销大而难以部署；常规量化在ViT分割的深层编码器-解码器结构中误差容易累积、稳定性差，需要一个稳定、可部署的低比特（整数）方案。

Method: 基于Segmenter框架，系统性将浮点运算替换为整数算子；提出λ-ShiftGELU以缓解均匀量化在长尾激活分布下的失真；移除L2归一化；将解码器中的双线性插值换为最近邻上采样；保证训练与推理全流程整数可执行；同时评估一拍后训练（PTQ）场景。

Result: 在保持合理精度损失（平均较FP32下降约5.1%）的同时，模型大小最高缩小3.8倍；在优化运行时下推理最高加速1.2倍；即便仅用单张校准图像进行一次性PTQ也取得有竞争力的精度。

Conclusion: 全整数ViT语义分割可行且实用；通过新激活与算子替换，可在轻量部署上实现较优精度-效率权衡，为资源受限设备上的分割提供可落地方案。

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [55] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: 提出GARD：基于Gamma分布的扩散模型用于OCT去斑，在保持解剖结构的同时显著降噪，并通过DDIM式加速实现高效推理。


<details>
  <summary>Details</summary>
Motivation: OCT图像受散斑噪声影响严重，现有去噪方法常在降噪与细节/解剖结构保真之间权衡不佳；多数扩散模型假设高斯噪声，不契合散斑统计特性。

Method: 1) 设计Denoising Diffusion Gamma Model，用Gamma分布建模散斑噪声替代高斯假设；2) 引入Noise-Reduced Fidelity Term，以预处理得到的低噪图像作为引导，抑制高频噪声回流；3) 将DDIM框架适配到Gamma扩散以加速推理；4) 在成对的噪声/低噪OCT B-scan数据上训练与评估。

Result: 在PSNR、SSIM、MSE等指标上显著优于传统与SOTA深度去噪方法；主观上边缘更锐利、细微解剖结构保留更好。

Conclusion: 面向OCT去斑的GARD通过Gamma扩散与引导保真项有效平衡降噪与结构保真，并可高效推理，优于现有方法。

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [56] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 提出GLAM，一种利用几何指导的多视角乳腺X线（Mammo）VLM预训练方法，通过全局与局部、视觉-视觉与视觉-语言对比学习，对同侧多视图进行对齐，显著提升多数据集下表现。


<details>
  <summary>Details</summary>
Motivation: 现有从自然图像迁移的VLM忽视乳腺X线的多视角成像特性，未建模同侧视图间的对应关系，导致丢失几何上下文与细粒度线索，限制筛查解读的速度与准确性。

Method: 基于乳腺摄影成像先验，设计GLAM：1) 通过几何指导学习同侧多视图的局部跨视图对齐；2) 结合全局与局部的对比学习；3) 同时进行视觉-视觉与视觉-语言的对比优化，促进细粒度局部特征学习；4) 在大型公开乳腺数据EMBED上进行预训练。

Result: 在多个数据集与设置上优于各类基线方法，展示更强的多视图对应建模与判别能力。

Conclusion: 面向乳腺摄影的几何感知多视图VLM预训练能有效捕获跨视图对应与细粒度特征，缓解跨域与数据稀缺问题，推动筛查AI性能提升。

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [57] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 这是一篇关于“视觉指代落地（visual grounding）”在通用视觉-语言模型（VLMs）中的综述：阐明其重要性、现代方法论的核心组件、应用与评测、与多模态链式思维和推理的关联，并总结挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 视觉落地使模型能从图像/视频中定位与文本描述相匹配的区域，这是实现精细层次问答、指代表达理解、可指称的图文生成、以及在仿真/真实环境中进行低/高层控制的基础能力。综述旨在系统梳理该能力在通用VLM中的地位、方法与应用版图。

Method: 作为综述，结构包括：1) 阐述在VLM中进行落地的重要性；2) 归纳当代“有落地”的VLM开发范式与关键组件；3) 盘点应用、基准与评测指标（含面向落地的多模态生成评测）；4) 讨论视觉落地与多模态CoT及推理之间的多层关系；5) 分析挑战并提出未来研究方向。

Result: 汇总代表性工作与实践经验，形成对视觉落地生态的系统性视图：方法学框架、模型组件、应用案例与评测体系，以及与推理/链式思维的互动机理。

Conclusion: 视觉落地是通用VLM走向可靠理解、精细对齐与可控生成的关键支柱。当前仍面临标注成本高、跨模态对齐与推理协同、评测不完备、在开放世界和时空场景中的泛化与稳健性等挑战；未来可沿更强的可解释推理、弱/自监督与合成数据、交互式与具身落地、统一评测协议与安全鲁棒性等方向推进。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [58] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 提出“Attention Attack”，通过扰乱文本与图像之间的跨注意力来破坏文本引导的图像编辑，使编辑显著失效且对人眼几乎不可见；并引入两种新评估指标（Caption Similarity 与语义IoU）验证攻击有效性。


<details>
  <summary>Details</summary>
Motivation: 文本引导图像编辑虽精细强大，但易受攻击；现有防御/评估指标难可靠衡量免疫效果。需要一种无需知道编辑器细节与提示词、可泛化且隐蔽的攻击，以及更可信的评估方法。

Method: 1) 生成源图像的自动描述作为编辑提示的代理；2) 在扩散式编辑流水线的跨注意力层对齐处注入扰动，故意错配文本语义与图像表示，从而破坏文本-图像对齐；3) 无需访问具体编辑方法或真实提示（黑盒）；4) 评估上提出Caption Similarity度量原始与对抗编辑的语义一致性，和语义IoU衡量经分割掩码得到的空间布局破坏程度。

Result: 在TEDBench++上，攻击显著降低编辑质量（语义与空间一致性显著下降），同时对人类视觉几乎不可察觉，优于现有攻击基线。

Conclusion: 跨注意力是文本引导图像编辑的脆弱环节；利用自动描述作为代理即可在黑盒条件下有效破坏编辑。新提出的Caption Similarity与语义IoU为评估免疫/攻击提供更可靠的标准。

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [59] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: 利用知识蒸馏降低深度学习图像压缩模型的计算/能耗成本，同时保持或接近大模型压缩性能，并在多种网络规模与码率-失真权衡下有效。


<details>
  <summary>Details</summary>
Motivation: 深度学习图像压缩性能优于传统编解码器，但推理开销大，难以在资源受限、实时场景部署。需要方法在保证压缩质量的同时显著降低算力与能耗需求。

Method: 采用知识蒸馏：以性能更强的教师压缩模型指导更小的学生模型训练。学生网络在编码-量化-熵编解码-解码流程下，部分损失由教师输出（如重建图像/中间表示/概率分布）监督。考察不同网络规模、不同码率-失真权衡设置与蒸馏超参；讨论替换教师、损失项设计与向Transformer模型扩展的可行性。代码开源。

Result: 实验显示：在多种模型规模与目标码率范围内，蒸馏后的学生模型较独立训练基线取得更好的率失真（RD）性能，同时显著降低推理计算与能耗；在实际部署中具备更好的实时性。

Conclusion: 知识蒸馏是降低神经图像压缩资源消耗的有效策略，可在保持高压缩质量的同时节省计算与能量。未来可探索不同教师、损失函数与Transformer架构上的蒸馏，以及更系统的超参设置。

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [60] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 该论文提出一种无需训练的数据驱动方法，只用一对可见光与热成像图像，即可将图像分解为阴影(照明)与反射率(材质)成分；通过可见光与热强度的序关系，构建密集自监督信号，优化神经网络以恢复本征图像；在定量与多样户外场景定性实验中优于近期学习法，并为规模化获取真实世界序监督提供路径。


<details>
  <summary>Details</summary>
Motivation: 本征图像分解长期受制于真实场景缺乏大规模标注(尤其户外)，现有方法依赖合成数据或稀疏标注，泛化与可扩展性不足。作者观察到不被反射的可见光能量会被不透明表面吸收并转化为热量，热相机可测到，从而将可见光/热成像与阴影/反射率的序关系联系起来，以替代难以获取的标注。

Method: 提出一种训练免(无外部数据训练)的优化框架：输入仅为同一场景的一对可见光和热成像图像。利用物理能量守恒直觉建立“可见光/热强度与阴影/反射率的序关系(ordinalities)”约束，构造成对或密集的排序损失，作为自监督信号，驱动一个可优化的神经网络分解出阴影与反射率层；在自然与人造光下进行定量评估，并在多样户外场景做定性验证。

Result: 在已知反射率与阴影的设置下(自然/人造光)定量结果优于近期学习方法；在复杂户外场景中也展现出更佳的分解质量与稳健性。方法可密集地产生自监督信号，超越以往依赖稀疏标注或合成数据的限制。

Conclusion: 通过可见光与热成像的序关系实现训练免的本征图像分解，显著优于现有学习方法，并提供了可扩展的真实世界序监督获取方案，为大规模、真实场景下的本征分解研究铺路。

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [61] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 论文综述压缩视频质量增强（CVQE）领域，提出方法学分类、统一评测框架，并系统分析性能-复杂度权衡，为标准化评测与模型选型提供依据。


<details>
  <summary>Details</summary>
Motivation: 现有CVQE综述未能系统关联方法与具体编解码标准与伪影类型，缺乏跨编码类型的架构对比，也缺少一致的基准与多指标评测，导致研究结果难以横向比较与实际部署指导不足。

Method: 1) 构建新分类法：按架构范式、编码标准（H.264/AVC、H.265/HEVC、H.266/VVC等）及压缩域特征利用程度对CVQE方法进行归类；2) 设计统一基准：整合现代压缩协议与标准测试序列，进行公平的多准则评价；3) 系统化分析SOTA方法在重建质量与计算复杂度之间的权衡，并梳理未来研究方向。

Result: 给出一个覆盖主流方法与标准的分类图谱；提出并运行统一评测框架，实现跨方法、跨标准、跨指标的可比性；总结出不同架构在不同编码类型下的相对优势与代价，并量化性能-复杂度的关键趋势。

Conclusion: 该综述为CVQE建立一致的评测与对比基础，帮助研究者与工程师依据标准、伪影、资源约束进行模型选择，同时指出未来在压缩域建模、轻量化与公平基准上的研究机会。

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [62] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: 提出MM SAM-adapter，将多模态特征通过轻量适配器注入SAM的RGB特征，实现在困难场景下更鲁棒的语义分割，并在DeLiVER、FMB、MUSES上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单一RGB语义分割在弱光、遮挡、恶劣天气等条件下易退化；多模态（LiDAR、红外等）可提供互补信息，但如何有效融合而不牺牲SAM在开放场景的泛化能力仍是难点。

Method: 基于Segment Anything Model，设计一个适配器网络对多模态特征进行融合后，以选择性方式注入SAM的丰富RGB表示中：仅在辅助模态能带来额外线索时启用，从而保持RGB主干的泛化并实现高效多模态利用。

Result: 在DeLiVER、FMB、MUSES三个具有挑战性的基准上取得SOTA；将DeLiVER与FMB划分为RGB-easy与RGB-hard子集分析表明，在有利与不利条件下均优于现有方法。

Conclusion: 多模态适配到SAM的策略能在不破坏RGB泛化的前提下，稳健地利用辅助传感器信息，提升复杂场景下的语义分割性能；方法高效、鲁棒且具有广泛适用性。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [63] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: 提出InfGen：用一次性生成器替代VAE解码器，从扩散模型固定大小潜变量一次解码到任意分辨率图像，实现不重训、低复杂度的任意分辨率生成，将4K生成从>100秒降至<10秒。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的计算量随分辨率近似二次增长，导致高分辨率（如4K）生成极慢，难以在多设备上提供一致、快速的视觉体验；需要一种在不重训主扩散模型的前提下，以更低计算代价生成任意分辨率图像的方案。

Method: 将扩散模型产生的固定大小潜变量视为内容表示，设计一个“一步生成器”（one-step generator）替换原有VAE解码器，直接从该紧凑潜变量一次性解码到任意目标分辨率图像；保持潜空间兼容，从而可插拔应用到任意使用相同潜空间的模型，无需重训扩散模型本体。

Result: 在多模型上验证：InfGen可将原模型升级为支持任意高分辨率生成，同时显著降低时延；4K图像生成时间由超过100秒降至10秒以内。

Conclusion: InfGen将任意分辨率生成问题转化为从固定潜变量的一步解码，简化流程、降低计算复杂度、具备广泛兼容性，为高分辨率扩散生成提供高效实用的工程化路径。

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [64] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 论文将时间自监督学习用于3D脑MRI阿尔茨海默病预测，跨四数据集预训练，并在七个下游任务中多数优于监督学习，且能处理变长时间序列输入。


<details>
  <summary>Details</summary>
Motivation: 深度学习在AD预测受限于标注数据匮乏、跨数据集泛化差、以及对输入次数与时间间隔不灵活。需要能从未标注、多时间点MRI中学习稳健时空表征的方法。

Method: 改编三种最先进的时间自监督学习（SSL）方法用于3D MRI：包括时间顺序预测与对比学习，并加入能处理变长输入的扩展与增强空间特征鲁棒性的设计。将4个公开数据集（3161名患者）的纵向MRI用于预训练；随后在诊断分类、转化检测与未来转化预测等下游任务进行评估。

Result: 使用包含时间顺序预测与对比学习的SSL模型在七个下游任务中的六个上优于纯监督学习；在不同数量的输入图像与不均匀时间间隔下表现稳定，显示良好的适配性与泛化性。

Conclusion: 时间SSL能在有限标注情形下从纵向MRI中学习稳健时空特征，较监督学习在多AD相关任务中更具优势，并能适应临床上变长、非等间隔随访场景；代码与模型已公开以促进复现与应用。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>
