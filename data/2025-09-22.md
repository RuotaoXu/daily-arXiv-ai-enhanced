<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 109]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays](https://arxiv.org/abs/2509.15234)
*Hanbin Ko,Gihun Cho,Inhyeok Baek,Donguk Kim,Joonbeom Koo,Changi Kim,Dongheon Lee,Chang Min Park*

Main category: cs.CV

TL;DR: 提出针对胸部X光（CXR）报告的领域自适应LLM文本编码器（LLM2VEC4CXR）与其配套的双塔视觉-语言框架（LLM2CLIP4CXR），在异质、噪声较大的临床报告中提升文本理解与图文对齐，强调鲁棒性胜于单纯规模扩张。


<details>
  <summary>Details</summary>
Motivation: 临床放射学报告高度异质（缩写、仅结论、风格差异），直接扩大数据规模反而可能停滞或恶化图文对齐效果；需要能跨风格稳定迁移的文本表示来更好地指导视觉-语言预训练。

Method: 1) 设计并域适配LLM文本编码器LLM2VEC4CXR以处理缩写与风格变化，并提升临床语义表示；2) 构建双塔框架LLM2CLIP4CXR，将该文本编码器与视觉骨干结合进行对比学习；3) 在包含公私来源、1.6M份CXR研究的噪声报告上训练与评估，比较BERT基线与医疗CLIP变体。

Result: LLM2VEC4CXR在报告级临床理解与对齐指标上优于BERT基线，能稳健处理缩写与风格；LLM2CLIP4CXR显著提升图文检索与临床相关指标，并在跨数据集泛化上优于既有医疗CLIP方法。

Conclusion: 在放射学多模态学习中，提升表示鲁棒性（尤其对异质、噪声文本的稳健编码）比单纯扩大数据规模更关键；所发布模型与代码有助于推动医疗图文表示学习研究。

Abstract: Vision-language pretraining has advanced image-text alignment, yet progress
in radiology remains constrained by the heterogeneity of clinical reports,
including abbreviations, impression-only notes, and stylistic variability.
Unlike general-domain settings where more data often leads to better
performance, naively scaling to large collections of noisy reports can plateau
or even degrade model learning. We ask whether large language model (LLM)
encoders can provide robust clinical representations that transfer across
diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,
a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a
dual-tower framework that couples this encoder with a vision backbone.
LLM2VEC4CXR improves clinical text understanding over BERT-based baselines,
handles abbreviations and style variation, and achieves strong clinical
alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to
boost retrieval accuracy and clinically oriented scores, with stronger
cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M
CXR studies from public and private sources with heterogeneous and noisy
reports, our models demonstrate that robustness -- not scale alone -- is the
key to effective multimodal learning. We release models to support further
research in medical image-text representation learning.

</details>


### [2] [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)
*Jialiang Kang,Han Shu,Wenshuo Li,Yingjie Zhai,Xinghao Chen*

Main category: cs.CV

TL;DR: 提出ViSpec：为视觉-语言模型设计的“视觉感知”推测解码框架，通过轻量视觉适配器压缩图像tokens并融入草稿模型注意力，同时为文本tokens注入全局图像特征，结合特制长响应训练数据与防捷径训练策略，在多项实验中首次实现显著加速（>1.5x）且保持理解能力。


<details>
  <summary>Details</summary>
Motivation: LLM推测解码已成熟，但在VLM中加速效果有限（<1.5x），原因在于小草稿模型难以过滤冗余图像信息，造成多模态解码低效。亟需一种能让草稿模型有效利用视觉信息、避免冗余并维持文本理解的方案。

Method: - 设计Vision-Aware Speculative Decoding (ViSpec)。
- 轻量视觉适配器压缩原始图像tokens为紧凑表征，保留位置编码并无缝注入草稿模型注意力。
- 为每张图像提取一个全局特征向量，作为辅助信号拼接/注入到后续所有文本tokens，提高多模态一致性。
- 数据：重组现有多模态数据并用目标VLM经改写提示生成长响应，构建训练集。
- 训练策略：避免草稿模型对目标模型隐藏态的“捷径学习”，仅在目标输出上训练并设计约束，使草稿模型学会真正的推测而非复制。

Result: 在多项实验中，ViSpec获得显著、首次突破性的VLM推测解码加速（超过1.5x），同时维持文本理解与多模态一致性，优于现有方法。

Conclusion: VLM可逐层过滤冗余视觉信息；通过视觉适配与全局特征增强的草稿模型，加上特制数据与防捷径训练，ViSpec实现了首个实质性VLM推测解码加速，为多模态大模型高效推理提供有效路径。

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), yet its application to vision-language models
(VLMs) remains underexplored, with existing methods achieving only modest
speedups (<1.5x). This gap is increasingly significant as multimodal
capabilities become central to large-scale models. We hypothesize that large
VLMs can effectively filter redundant image information layer by layer without
compromising textual comprehension, whereas smaller draft models struggle to do
so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a
novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor
module to compress image tokens into a compact representation, which is
seamlessly integrated into the draft model's attention mechanism while
preserving original image positional information. Additionally, we extract a
global feature vector for each input image and augment all subsequent text
tokens with this feature to enhance multimodal coherence. To overcome the
scarcity of multimodal datasets with long assistant responses, we curate a
specialized training dataset by repurposing existing datasets and generating
extended outputs using the target VLM with modified prompts. Our training
strategy mitigates the risk of the draft model exploiting direct access to the
target model's hidden states, which could otherwise lead to shortcut learning
when training solely on target model outputs. Extensive experiments validate
ViSpec, achieving, to our knowledge, the first substantial speedup in VLM
speculative decoding.

</details>


### [3] [M-PACE: Mother Child Framework for Multimodal Compliance](https://arxiv.org/abs/2509.15241)
*Shreyash Verma,Amit Kesari,Vinayak Trivedi,Anupam Purwar,Ratnesh Jamidar*

Main category: cs.CV

TL;DR: 提出M-PACE：用母-子MLLM一次性评估多模态合规属性，在广告场景实现高效、低成本、可扩展的合规检查，成本降31倍且精度可比。


<details>
  <summary>Details</summary>
Motivation: 现有多模态合规依赖碎片化的多阶段流水线（图像分类、OCR、音频转写、规则合并等），维护复杂、扩展差、难以快速适应动态规范。MLLM的出现使统一处理视觉与文本成为可能，亟需一个通用框架简化与提升合规评估。

Method: 提出M-PACE：单次传递评估多模态属性的框架；构建人类标注+增强的基准数据（包含遮挡、脏话注入等难例）；采用母-子MLLM架构——强大的母模型对较小子模型输出进行评估与选择，实现自动化质控与模型选择；在广告合规中评估15+属性。

Result: 在真实广告数据部署中，母模型可选择最具性价比的子模型（如选择Gemini 2.0 Flash），推理成本从每图0.0159降至0.0005，超过31倍降幅，同时保持与Gemini 2.5 Pro相当的准确度；显著减少人工审核依赖。

Conclusion: M-PACE能将多模态合规流程统一到单框架，动态权衡成本与质量并自动质控，适于大规模、实时的广告合规场景；基准与架构为多领域合规提供可扩展的解决方案。

Abstract: Ensuring that multi-modal content adheres to brand, legal, or
platform-specific compliance standards is an increasingly complex challenge
across domains. Traditional compliance frameworks typically rely on disjointed,
multi-stage pipelines that integrate separate modules for image classification,
text extraction, audio transcription, hand-crafted checks, and rule-based
merges. This architectural fragmentation increases operational overhead,
hampers scalability, and hinders the ability to adapt to dynamic guidelines
efficiently. With the emergence of Multimodal Large Language Models (MLLMs),
there is growing potential to unify these workflows under a single,
general-purpose framework capable of jointly processing visual and textual
content. In light of this, we propose Multimodal Parameter Agnostic Compliance
Engine (M-PACE), a framework designed for assessing attributes across
vision-language inputs in a single pass. As a representative use case, we apply
M-PACE to advertisement compliance, demonstrating its ability to evaluate over
15 compliance-related attributes. To support structured evaluation, we
introduce a human-annotated benchmark enriched with augmented samples that
simulate challenging real-world conditions, including visual obstructions and
profanity injection. M-PACE employs a mother-child MLLM setup, demonstrating
that a stronger parent MLLM evaluating the outputs of smaller child models can
significantly reduce dependence on human reviewers, thereby automating quality
control. Our analysis reveals that inference costs reduce by over 31 times,
with the most efficient models (Gemini 2.0 Flash as child MLLM selected by
mother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5
Pro with comparable accuracy, highlighting the trade-off between cost and
output quality achieved in real time by M-PACE in real life deployment over
advertising data.

</details>


### [4] [ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images](https://arxiv.org/abs/2509.15242)
*Jaydeep Rade,Md Hasibul Hasan Hasib,Meric Ozturk,Baboucarr Faal,Sheng Yang,Dipali G. Sashital,Vincenzo Venditti,Baoyu Chen,Soumik Sarkar,Adarsh Krishnamurthy,Anwesha Sarkar*

Main category: cs.CV

TL;DR: 提出ProFusion：将深度学习与AFM高度图结合，通过虚拟AFM合成多视角数据、扩散模型生成新视图、实例NeRF重建3D，实验证据显示重建精度达AFM分辨率级别，用于大型蛋白复合体的经济高效结构预测与快速验证。


<details>
  <summary>Details</summary>
Motivation: AI结构预测对大型多蛋白复合体因缺少3D空间线索而表现欠佳；Cryo-EM虽准但昂贵且耗时。AFM能提供多视角高度图，但真实大规模数据难以获取，需一种可扩展的混合方案。

Method: 1) 搭建虚拟AFM，模拟成像并生成约54.2万蛋白的多视角合成高度图数据集；2) 训练条件扩散模型，从未配准输入合成新视图；3) 训练实例特定的NeRF，将多视角高度图融合，重建3D蛋白结构；4) 在实验AFM图像上验证。

Result: 重建的3D结构平均Chamfer Distance落在AFM成像分辨率范围内，表明高结构保真度；在多种蛋白复合体的实验AFM数据上表现良好。

Conclusion: ProFusion通过虚拟AFM+扩散视图合成+实例NeRF实现对大型蛋白复合体的准确、低成本3D结构预测，并支持用AFM进行快速迭代验证，展示了在实际实验场景中的强潜力。

Abstract: AI-based in silico methods have improved protein structure prediction but
often struggle with large protein complexes (PCs) involving multiple
interacting proteins due to missing 3D spatial cues. Experimental techniques
like Cryo-EM are accurate but costly and time-consuming. We present ProFusion,
a hybrid framework that integrates a deep learning model with Atomic Force
Microscopy (AFM), which provides high-resolution height maps from random
orientations, naturally yielding multi-view data for 3D reconstruction.
However, generating a large-scale AFM imaging data set sufficient to train deep
learning models is impractical. Therefore, we developed a virtual AFM framework
that simulates the imaging process and generated a dataset of ~542,000 proteins
with multi-view synthetic AFM images. We train a conditional diffusion model to
synthesize novel views from unposed inputs and an instance-specific Neural
Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D
protein structures achieve an average Chamfer Distance within the AFM imaging
resolution, reflecting high structural fidelity. Our method is extensively
validated on experimental AFM images of various PCs, demonstrating strong
potential for accurate, cost-effective protein complex structure prediction and
rapid iterative validation using AFM experiments.

</details>


### [5] [Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models](https://arxiv.org/abs/2509.15243)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: 提出MMEL框架提升视觉-语言模型的可解释性，通过层级语义关系与梯度归因结合，生成更聚焦且具上下文的可视化解释，同时保持性能，在多数据集上验证并可跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在安全关键场景中缺乏透明度与可靠性，复杂物体关系与细微线索难以被解释，需一种既高性能又可解释的方法。

Method: 在基于Transformer梯度解释（如Grad-eclip）的基础上，引入层级语义关系模块：多尺度特征处理、自适应注意力加权、跨模态对齐；对不同语义层级的图像区域关系进行建模，并通过可学习的层级权重平衡模型深度贡献，将语义关系信息融入梯度归因图以生成更全面的可视化解释。

Result: 在标准数据集的广泛实验中，加入语义关系信息后，归因可视化更聚焦、上下文化更强，能同时突出主要目标及其背景关系，精度与解释一致性提升，且在不同领域中表现稳定。

Conclusion: MMEL在保持性能的同时显著提升VLM的可解释性与可靠性，生成更贴近模型处理复杂场景方式的可视化解释，并具备跨域泛化潜力，适用于高可解释性需求的应用。

Abstract: Recent advances in vision-language models have significantly expanded the
frontiers of automated image analysis. However, applying these models in
safety-critical contexts remains challenging due to the complex relationships
between objects, subtle visual cues, and the heightened demand for transparency
and reliability. This paper presents the Multi-Modal Explainable Learning
(MMEL) framework, designed to enhance the interpretability of vision-language
models while maintaining high performance. Building upon prior work in
gradient-based explanations for transformer architectures (Grad-eclip), MMEL
introduces a novel Hierarchical Semantic Relationship Module that enhances
model interpretability through multi-scale feature processing, adaptive
attention weighting, and cross-modal alignment. Our approach processes features
at multiple semantic levels to capture relationships between image regions at
different granularities, applying learnable layer-specific weights to balance
contributions across the model's depth. This results in more comprehensive
visual explanations that highlight both primary objects and their contextual
relationships with improved precision. Through extensive experiments on
standard datasets, we demonstrate that by incorporating semantic relationship
information into gradient-based attribution maps, MMEL produces more focused
and contextually aware visualizations that better reflect how vision-language
models process complex scenes. The MMEL framework generalizes across various
domains, offering valuable insights into model decisions for applications
requiring high interpretability and reliability.

</details>


### [6] [Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning](https://arxiv.org/abs/2509.15250)
*Wenda Qin,Andrea Burns,Bryan A. Plummer,Margrit Betke*

Main category: cs.CV

TL;DR: 提出面向视觉-语言导航（VLN）的“导航感知剪枝（NAP）”，通过在剪枝前用导航特性将输入划分为前景/背景并只对背景剪枝，减少信息丢失与无效探索，达到更高成功率与>50% FLOPS节省。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在VLN上表现强，但在资源受限环境成本高。通用token剪枝虽能提效，却忽略VLN特有问题：剪掉关键信息会导致路径变长、回溯增多，反而增加总体计算；且难以区分无信息token。需要一种结合导航语义的剪枝方法，在保证导航效果的同时提升效率。

Method: NAP包含两步：1）导航感知预过滤，将输入按导航相关性划分前景/背景。具体为：基于可通行方向过滤图像视角；利用大语言模型抽取与导航相关的指令片段。2）针对背景token进行重点剪枝，降低信息丢失。并通过删除低重要度的导航节点来抑制回溯，避免路径加长。整体在VLN管线中与大型模型推理结合，实现高效推理。

Result: 在标准VLN基准上，NAP较已有剪枝方法显著提升：以更高成功率完成导航，同时节省超过50%的FLOPS，表明在效率与性能之间取得更佳权衡。

Conclusion: 将导航结构与语义引入剪枝策略能有效解决VLN中信息丢失导致的“走更远、算更多”问题。NAP通过前景/背景预过滤与回溯抑制，实现在不显著牺牲性能情况下的显著算力节省，并优于以往通用剪枝方法。

Abstract: Large models achieve strong performance on Vision-and-Language Navigation
(VLN) tasks, but are costly to run in resource-limited environments. Token
pruning offers appealing tradeoffs for efficiency with minimal performance loss
by reducing model input size, but prior work overlooks VLN-specific challenges.
For example, information loss from pruning can effectively increase
computational cost due to longer walks. Thus, the inability to identify
uninformative tokens undermines the supposed efficiency gains from pruning. To
address this, we propose Navigation-Aware Pruning (NAP), which uses
navigation-specific traits to simplify the pruning process by pre-filtering
tokens into foreground and background. For example, image views are filtered
based on whether the agent can navigate in that direction. We also extract
navigation-relevant instructions using a Large Language Model. After filtering,
we focus pruning on background tokens, minimizing information loss. To further
help avoid increases in navigation length, we discourage backtracking by
removing low-importance navigation nodes. Experiments on standard VLN
benchmarks show NAP significantly outperforms prior work, preserving higher
success rates while saving more than 50% FLOPS.

</details>


### [7] [RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation](https://arxiv.org/abs/2509.15257)
*Silpa Vadakkeeveetil Sreelatha,Sauradip Nag,Muhammad Awais,Serge Belongie,Anjan Dutta*

Main category: cs.CV

TL;DR: 提出RespoDiff框架，通过在扩散模型的中间瓶颈表示上施加双模块变换，实现兼顾公平/安全与语义对齐的文本到图像生成。引入新的score-matching目标协调两模块，较SOTA提升约20%，并可无缝集成至SDXL。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的文本到图像生成虽高保真但在公平与安全方面存在偏差与风险；现有方法提升责任性通常牺牲语义保真与图像质量，亟需在不损失生成质量的前提下实现公平、安全与语义对齐。

Method: 在扩散模型的中间瓶颈表示上引入双可学习模块：责任性模块学习并施加公平/安全等概念约束；语义模块维持与中性提示的语义对齐。提出新的score-matching训练目标，使两模块协同学习与协调优化，在生成过程中共同影响表示。

Result: 在多样且未见的提示上，实现负责任且语义一致的生成提升约20%，在保证图像保真度的同时优化公平与安全指标。方法可无缝集成到大型模型如SDXL，并优于现有SOTA。

Conclusion: RespoDiff在不牺牲图像质量与语义保真的情况下提升文本到图像生成的公平与安全，通过双模块与协调的训练目标实现负责任生成，具备通用性与可扩展性，且能集成至主流大模型。

Abstract: The rapid advancement of diffusion models has enabled high-fidelity and
semantically rich text-to-image generation; however, ensuring fairness and
safety remains an open challenge. Existing methods typically improve fairness
and safety at the expense of semantic fidelity and image quality. In this work,
we propose RespoDiff, a novel framework for responsible text-to-image
generation that incorporates a dual-module transformation on the intermediate
bottleneck representations of diffusion models. Our approach introduces two
distinct learnable modules: one focused on capturing and enforcing responsible
concepts, such as fairness and safety, and the other dedicated to maintaining
semantic alignment with neutral prompts. To facilitate the dual learning
process, we introduce a novel score-matching objective that enables effective
coordination between the modules. Our method outperforms state-of-the-art
methods in responsible generation by ensuring semantic alignment while
optimizing both objectives without compromising image fidelity. Our approach
improves responsible and semantically coherent generation by 20% across
diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale
models like SDXL, enhancing fairness and safety. Code will be released upon
acceptance.

</details>


### [8] [Autoguided Online Data Curation for Diffusion Model Training](https://arxiv.org/abs/2509.15267)
*Valeria Pais,Luis Oala,Daniele Faccio,Marco Aversa*

Main category: cs.CV

TL;DR: 本文评估自动引导（autoguidance）与在线数据选择（JEST/AJEST）在扩散生成模型训练中的效率与效果，结果显示自动引导稳定提升样本质量与多样性；早期选择（AJEST）在数据效率上可与自动引导匹敌或略优，但其时间开销与复杂性使得多数情况下仍以自动引导或均匀采样为佳。


<details>
  <summary>Details</summary>
Motivation: 生成模型训练计算成本高昂，促使研究者关注高效的数据策划策略。近期的自动引导与在线数据选择方法可能在不增加总训练时间或样本数的前提下提升模型质量与效率，但缺乏统一基准与严格对照来判断其实际收益。

Method: 将联合样本选择JEST与自动引导整合到统一代码框架；在二维合成数据与(3×64×64)图像生成任务上进行系统对比。对比在相同墙钟时间与相同样本数的条件下进行，并显式计入选择策略的额外开销。测试包括纯自动引导、AJEST（仅在训练初期进行选择）、以及均匀随机选择等组合。

Result: 在所有实验中，自动引导稳定提升生成样本的质量与多样性。AJEST在两个任务上能在数据效率上与自动引导相当或略有超越，但其选择过程带来的时间开销和工程复杂度较高。总体而言，在等时与等样本的约束下，自动引导更具稳健性与实用性。

Conclusion: 有针对性的在线数据选择在训练早期可带来一定效率增益，但主要且稳健的质量提升来源于自动引导。考虑到时间与复杂度，实际应用中更倾向采用自动引导或均匀随机采样。文中也讨论了方法的局限与适用范围，并给出何时数据选择可能有益的建议。

Abstract: The costs of generative model compute rekindled promises and hopes for
efficient data curation. In this work, we investigate whether recently
developed autoguidance and online data selection methods can improve the time
and sample efficiency of training generative diffusion models. We integrate
joint example selection (JEST) and autoguidance into a unified code base for
fast ablation and benchmarking. We evaluate combinations of data curation on a
controlled 2-D synthetic data generation task as well as (3x64x64)-D image
generation. Our comparisons are made at equal wall-clock time and equal number
of samples, explicitly accounting for the overhead of selection. Across
experiments, autoguidance consistently improves sample quality and diversity.
Early AJEST (applying selection only at the beginning of training) can match or
modestly exceed autoguidance alone in data efficiency on both tasks. However,
its time overhead and added complexity make autoguidance or uniform random data
selection preferable in most situations. These findings suggest that while
targeted online selection can yield efficiency gains in early training, robust
sample quality improvements are primarily driven by autoguidance. We discuss
limitations and scope, and outline when data selection may be beneficial.

</details>


### [9] [PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images](https://arxiv.org/abs/2509.15270)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CV

TL;DR: 提出PRISM频域指纹框架，通过幅度+相位的径向傅里叶特征结合LDA聚类，实现对AI生成图像的模型归因与真伪检测，在自建PRISM-36K与多基准上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI商业与多模态应用需要可追溯性：识别内容来源的归因方法以保障责任与信任，尤其在专有付费服务中。现有方法在跨架构、跨数据集、不可访问模型细节情境下可靠性不足，亟需稳健、可扩展的指纹方案。

Method: 提出PRISM：1) 对图像做离散傅里叶变换，进行径向压缩，联合利用频谱幅度与相位以捕捉模型特异签名；2) 将得到的频域签名经线性判别分析（LDA）聚类/分类，实现模型归因；3) 构建PRISM-36K数据集（6种文本到图像的GAN与扩散模型，36,000张）用于训练/评估；4) 同时在公开基准与二分类任务（真伪检测）上评测。

Result: 在PRISM-36K上模型归因准确率92.04%；在四个公开基准平均准确率81.60%；在真伪检测任务平均准确率88.41%，在GenImage基准达到95.06%（原基准82.20%）。

Conclusion: 频域（幅度+相位的径向DFT）指纹能在跨架构、跨数据集场景中有效进行模型归因与假图检测；PRISM提供可扩展、无需模型内部细节的解决方案，有助于在生成式AI中落实责任与信任。

Abstract: A critical need has emerged for generative AI: attribution methods. That is,
solutions that can identify the model originating AI-generated content. This
feature, generally relevant in multimodal applications, is especially sensitive
in commercial settings where users subscribe to paid proprietary services and
expect guarantees about the source of the content they receive. To address
these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image
Signature Mapping framework for fingerprinting AI-generated images. PRISM is
based on a radial reduction of the discrete Fourier transform that leverages
amplitude and phase information to capture model-specific signatures. The
output of the above process is subsequently clustered via linear discriminant
analysis to achieve reliable model attribution in diverse settings, even if the
model's internal details are inaccessible. To support our work, we construct
PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN-
and diffusion-based models. On this dataset, PRISM achieves an attribution
accuracy of 92.04%. We additionally evaluate our method on four benchmarks from
the literature, reaching an average accuracy of 81.60%. Finally, we evaluate
our methodology also in the binary task of detecting real vs fake images,
achieving an average accuracy of 88.41%. We obtain our best result on GenImage
with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our
results demonstrate the effectiveness of frequency-domain fingerprinting for
cross-architecture and cross-dataset model attribution, offering a viable
solution for enforcing accountability and trust in generative AI systems.

</details>


### [10] [Large Vision Models Can Solve Mental Rotation Problems](https://arxiv.org/abs/2509.15271)
*Sebastian Ray Mason,Anders Gjølbye,Phillip Chavarria Højbjerg,Lenka Tětková,Lars Kai Hansen*

Main category: cs.CV

TL;DR: 本文系统评估多种视觉Transformer（ViT、CLIP、DINOv2、DINOv3）在“心理旋转”类空间推理任务上的表现，发现自监督模型与中间层特征更擅长捕捉几何结构，难度随旋转复杂度与遮挡增加而上升，并与人类反应时趋势相似。


<details>
  <summary>Details</summary>
Motivation: 心理旋转是检验人类空间推理的经典范式，但现代视觉Transformer是否形成类似的表征能力尚不清楚。作者希望通过统一的基准与分层探测来揭示这些模型在不同类型旋转任务（积木、复杂图形、文本、真实物体）中的表现与机制。

Method: 构建多层次心理旋转任务集（从类Shepard–Metzler积木到复杂图块、三类文本、照片级真实物体）；对ViT、CLIP、DINOv2、DINOv3进行评测；逐层探测模型表征，比较自监督与监督训练方案；分析不同难度因素（旋转复杂度、遮挡）对性能的影响。

Result: 自监督ViT比监督ViT更好地捕捉几何结构；模型的中间层在心理旋转任务上优于最终层；任务难度随旋转复杂度与遮挡增加而显著上升；这些趋势与人类反应时变化一致，表明模型嵌入空间可能受类似几何与可见性约束。

Conclusion: 视觉Transformer在心理旋转任务中展现出与人类相似的难度依赖与层级特征优势，自监督训练与中间层表征尤为关键；结果提示未来应重视表征层选择与训练目标设计，以强化模型的空间推理与几何不变性。

Abstract: Mental rotation is a key test of spatial reasoning in humans and has been
central to understanding how perception supports cognition. Despite the success
of modern vision transformers, it is still unclear how well these models
develop similar abilities. In this work, we present a systematic evaluation of
ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from
simple block structures similar to those used by Shepard and Metzler to study
human cognition, to more complex block figures, three types of text, and
photo-realistic objects. By probing model representations layer by layer, we
examine where and how these networks succeed. We find that i) self-supervised
ViTs capture geometric structure better than supervised ViTs; ii) intermediate
layers perform better than final layers; iii) task difficulty increases with
rotation complexity and occlusion, mirroring human reaction times and
suggesting similar constraints in embedding space representations.

</details>


### [11] [Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks](https://arxiv.org/abs/2509.15272)
*Yannis Kaltampanidis,Alexandros Doumanoglou,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: 本文系统评估在自监督预训练的ViT中，直接使用未改动的最后一层注意力块（K/Q/V）及FFN输出特征用于分类与分割（含标准与小样本）时的表现，比较不同token类型与决策规则（超平面/余弦相似）在不同任务与预训练目标（对比学习、掩码建模）上的优劣，并在两大数据集上给出细致结果。


<details>
  <summary>Details</summary>
Motivation: 现有SSL-ViT下游通常再接轻量头或蒸馏提升性能，导致难以了解ViT原生特征本身的可分性与可解释性。缺乏对未改动特征在不同任务、few-shot场景、不同预训练目标下的系统比较。本文旨在揭示ViT潜在空间的内在表示能力与可分方向。

Method: - 选择多种经SSL（对比学习、掩码建模）预训练的ViT模型。
- 从最终注意力块提取K/Q/V token与FFN后特征，不做额外变换。
- 以两类决策规则：线性超平面（如逻辑回归）与余弦相似度，分别用于图像分类与语义分割（含few-shot）。
- 横向比较不同token类型、任务场景、预训练目标，并在两套常用数据集上报告细粒度结果。

Result: 发现不同任务与场景下，最优token类型与决策规则并不一致；某些情况下K/Q/V或FFN特征在余弦相似下更稳健，而在线性规则下分离性更强。对比学习与掩码建模的预训练目标对可解释方向的形成有差异。总体上，未改动的ViT特征在标准与few-shot分类/分割中已具有竞争力，并可据任务选择最优token与规则。

Conclusion: 无需附加头或蒸馏，ViT预训练特征本身已具备良好可分性与可解释方向；应根据任务、数据规模、以及预训练目标选择合适的token类型与决策规则。本文提供了实践指南与实证证据，帮助在不同下游任务中直接利用ViT原生表示。

Abstract: Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently
demonstrated considerable potential as a pre-training strategy for a variety of
computer vision tasks, including image classification and segmentation, both in
standard and few-shot downstream contexts. Two pre-training objectives dominate
the landscape of SSL techniques: Contrastive Learning and Masked Image
Modeling. Features (or tokens) extracted from the final transformer attention
block -- specifically, the keys, queries, and values -- as well as features
obtained after the final block's feed-forward layer, have become a common
foundation for addressing downstream tasks. However, in many existing
approaches, these pre-trained ViT features are further processed through
additional transformation layers, often involving lightweight heads or combined
with distillation, to achieve superior task performance. Although such methods
can improve task outcomes, to the best of our knowledge, a comprehensive
analysis of the intrinsic representation capabilities of unaltered ViT features
has yet to be conducted. This study aims to bridge this gap by systematically
evaluating the use of these unmodified features across image classification and
segmentation tasks, in both standard and few-shot contexts. The classification
and segmentation rules that we use are either hyperplane based (as in logistic
regression) or cosine-similarity based, both of which rely on the presence of
interpretable directions in the ViT's latent space. Based on the previous rules
and without the use of additional feature transformations, we conduct an
analysis across token types, tasks, and pre-trained ViT models. This study
provides insights into the optimal choice for token type and decision rule
based on the task, context, and the pre-training objective, while reporting
detailed findings on two widely-used datasets.

</details>


### [12] [How Good are Foundation Models in Step-by-Step Embodied Reasoning?](https://arxiv.org/abs/2509.15293)
*Dinura Dissanayake,Ahmed Heakl,Omkar Thawakar,Noor Ahsan,Ritesh Thawkar,Ketan More,Jean Lahoud,Rao Anwer,Hisham Cholakkal,Ivan Laptev,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 提出FoMER基准，系统评估大型多模态模型在具身环境中的结构化推理与安全决策能力，涵盖多任务多载体，并提供分离感知与动作推理的评估框架与实证结果。


<details>
  <summary>Details</summary>
Motivation: 尽管LMM在视觉理解与语言生成上表现强，但其在真实具身任务中的逐步、结构化推理与安全、空间一致的决策能力尚未被系统检验，需要一个标准化基准与评估方法来量化能力与发现不足。

Method: 构建FoMER基准：包含10类任务、8种具身设定、3类机器人、1.1k样本及详细逐步推理；设计评估框架，将感知落地（perceptual grounding）与动作推理拆分；对多种主流LMM进行实验评测，分析其在复杂具身决策场景中的表现。

Result: LMM在部分场景展现潜力，但在具身推理的安全性、物理约束理解、上下文一致性以及从感知到动作的闭环推理方面存在明显不足。基准与分析揭示当前模型的局限与差距。

Conclusion: FoMER为评估LMM具身推理提供了系统化数据与框架，明确当前挑战与研究机会，指向提升机器人智能中结构化推理、感知—行动耦合与安全约束理解的方向。

Abstract: Embodied agents operating in the physical world must make decisions that are
not only effective but also safe, spatially coherent, and grounded in context.
While recent advances in large multimodal models (LMMs) have shown promising
capabilities in visual understanding and language generation, their ability to
perform structured reasoning for real-world embodied tasks remains
underexplored. In this work, we aim to understand how well foundation models
can perform step-by-step reasoning in embodied environments. To this end, we
propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to
evaluate the reasoning capabilities of LMMs in complex embodied decision-making
scenarios. Our benchmark spans a diverse set of tasks that require agents to
interpret multimodal observations, reason about physical constraints and
safety, and generate valid next actions in natural language. We present (i) a
large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation
framework that disentangles perceptual grounding from action reasoning, and
(iii) empirical analysis of several leading LMMs under this setting. Our
benchmark includes over 1.1k samples with detailed step-by-step reasoning
across 10 tasks and 8 embodiments, covering three different robot types. Our
results highlight both the potential and current limitations of LMMs in
embodied reasoning, pointing towards key challenges and opportunities for
future research in robot intelligence. Our data and code will be made publicly
available.

</details>


### [13] [CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization](https://arxiv.org/abs/2509.15330)
*Min Zhang,Bo Jiang,Jie Zhou,Yimeng Liu,Xin Lin*

Main category: cs.CV

TL;DR: 提出CoDoL：利用“条件域提示”与轻量DMN生成输入条件token，提升CLIP类VLM在OOD上的对齐与泛化。在PACS、VLCS、OfficeHome、DigitDG上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的CLIP存在两大问题：1) 文本描述不准确，影响零样本精度与鲁棒性；2) 视觉-语言嵌入对齐有限，削弱泛化。希望借助可获得的域信息优化提示与对齐，从而提升OOD表现。

Method: 提出Conditional Domain prompt Learning（CoDoL），将可用的域信息编码进提示以增强视觉-语言对齐；并设计轻量的Domain Meta Network（DMN），为各域图像生成输入条件token，捕获实例与域特征，实现条件化的提示学习与对齐优化。

Result: 在四个OOD基准（PACS、VLCS、OfficeHome、DigitDG）进行大量实验，显示CoDoL提升了视觉-语言嵌入对齐并显著提高OOD泛化性能。

Conclusion: 利用域信息进行条件提示学习并配合DMN生成输入条件token，是改进CLIP类VLM在OOD场景中对齐与泛化的有效途径，实验验证其优越性。

Abstract: Recent advances in pre-training vision-language models (VLMs), e.g.,
contrastive language-image pre-training (CLIP) methods, have shown great
potential in learning out-of-distribution (OOD) representations. Despite
showing competitive performance, the prompt-based CLIP methods still suffer
from: i) inaccurate text descriptions, which leads to degraded accuracy and
robustness, and poses a challenge for zero-shot CLIP methods. ii) limited
vision-language embedding alignment, which significantly affects the
generalization performance. To tackle the above issues, this paper proposes a
novel Conditional Domain prompt Learning (CoDoL) method, which utilizes
readily-available domain information to form prompts and improves the
vision-language embedding alignment for improving OOD generalization. To
capture both instance-specific and domain-specific information, we further
propose a lightweight Domain Meta Network (DMN) to generate input-conditional
tokens for images in each domain. Extensive experiments on four OOD benchmarks
(PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed
CoDoL in terms of improving the vision-language embedding alignment as well as
the out-of-distribution generalization performance.

</details>


### [14] [Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception](https://arxiv.org/abs/2509.15333)
*Yulin Wang,Yang Yue,Yang Yue,Huanqian Wang,Haojun Jiang,Yizeng Han,Zanlin Ni,Yifan Pu,Minglei Shi,Rui Lu,Qisen Yang,Andrew Zhao,Zhuofan Xia,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: AdaptiveNN提出一种从被动到主动、适应性的视觉框架，通过序列化“凝视（fixation）”在任务相关区域进行粗到细的感知与决策，实现无需额外注视监督的端到端训练，在17个基准与9类任务上显著降低推理成本（最高28倍）且不损失精度，并能在不同任务需求与资源预算下无需重训灵活适配，同时以注视轨迹提升可解释性并呈现类人感知行为。


<details>
  <summary>Details</summary>
Motivation: 现有机器视觉一次性处理整幅时空输入，计算与内存成本随分辨率和模型规模线性或更快增长，限制可扩展性与实际部署；人类视觉通过选择性、序列化的注视高效采样环境。亟需一种能“主动/自适应”地选择信息、在预算内优化表现、并提升可解释性的通用方法。

Method: 将视觉感知建模为粗到细的序列决策过程：迭代选择任务相关区域进行注视、在多次注视间累积与整合信息、当信息足够时主动停止。提出将表征学习与自奖励强化学习整合的理论与训练机制，使非可微的注视决策可端到端优化且无需注视位置监督；模型可在不同任务与预算下自适应策略。

Result: 在17个基准涵盖9类任务（大规模识别、细粒度、视觉搜索、自动驾驶与医学场景、语言驱动具身智能、与人类并排对比）中：在不牺牲精度的前提下，推理成本最高降低28倍；无需重训即可适配不同任务需求与资源预算；注视模式提供更强可解释性；多处行为与人类感知高度相似。

Conclusion: AdaptiveNN验证了主动、适应性视觉的有效性与实用性：在保证精度的同时显著提升效率、灵活性与可解释性，并作为研究视觉认知的潜在工具。代码开源，具备进一步应用与扩展的基础。

Abstract: Human vision is highly adaptive, efficiently sampling intricate environments
by sequentially fixating on task-relevant regions. In contrast, prevailing
machine vision models passively process entire scenes at once, resulting in
excessive resource demands scaling with spatial-temporal input resolution and
model size, yielding critical limitations impeding both future advancements and
real-world application. Here we introduce AdaptiveNN, a general framework
aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision
models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential
decision-making process, progressively identifying and attending to regions
pertinent to the task, incrementally combining information across fixations,
and actively concluding observation when sufficient. We establish a theory
integrating representation learning with self-rewarding reinforcement learning,
enabling end-to-end training of the non-differentiable AdaptiveNN without
additional supervision on fixation locations. We assess AdaptiveNN on 17
benchmarks spanning 9 tasks, including large-scale visual recognition,
fine-grained discrimination, visual search, processing images from real driving
and medical scenarios, language-driven embodied AI, and side-by-side
comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction
without sacrificing accuracy, flexibly adapts to varying task demands and
resource budgets without retraining, and provides enhanced interpretability via
its fixation patterns, demonstrating a promising avenue toward efficient,
flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits
closely human-like perceptual behaviors in many cases, revealing its potential
as a valuable tool for investigating visual cognition. Code is available at
https://github.com/LeapLabTHU/AdaptiveNN.

</details>


### [15] [LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition](https://arxiv.org/abs/2509.15342)
*Jiuyi Xu,Qing Jin,Meida Chen,Andrew Feng,Yang Sui,Yangming Shi*

Main category: cs.CV

TL;DR: 提出LowDiff：一种级联、统一模型的扩分辨率扩散生成框架，以更少高分辨率采样步实现更快生成，质量不降甚至更优。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但采样慢。现有加速主要压缩模型或减少总去噪步数，忽略在生成过程中利用多分辨率的潜力。作者希望通过分辨率逐级生成与统一精炼模型提高吞吐与效率。

Method: 提出LowDiff：采用级联策略先在低分辨率生成，再逐步提升到目标分辨率；使用一个统一模型跨分辨率进行渐进式精炼；设计特定架构与生成技巧，使高分辨率阶段的采样步数显著减少；适用于像素空间与潜空间扩散模型；在条件与无条件任务上进行验证。

Result: 在CIFAR-10、FFHQ、ImageNet上广泛实验，吞吐提升超过50%，质量与SOTA可比或更优。具体：无条件CIFAR-10 FID 2.11、IS 9.87；条件CIFAR-10 FID 1.94、IS 10.03；FFHQ 64×64 FID 2.43；ImageNet 256×256基于LightningDiT-B/1 FID 4.00、IS 195.06，并伴随显著效率增益。

Conclusion: 多分辨率级联与统一精炼可在保持甚至提升图像质量的同时显著加速扩散模型采样；LowDiff通用于像素与潜空间、条件与无条件场景，提供高效实用的生成方案。

Abstract: Diffusion models have achieved remarkable success in image generation but
their practical application is often hindered by the slow sampling speed. Prior
efforts of improving efficiency primarily focus on compressing models or
reducing the total number of denoising steps, largely neglecting the
possibility to leverage multiple input resolutions in the generation process.
In this work, we propose LowDiff, a novel and efficient diffusion framework
based on a cascaded approach by generating increasingly higher resolution
outputs. Besides, LowDiff employs a unified model to progressively refine
images from low resolution to the desired resolution. With the proposed
architecture design and generation techniques, we achieve comparable or even
superior performance with much fewer high-resolution sampling steps. LowDiff is
applicable to diffusion models in both pixel space and latent space. Extensive
experiments on both conditional and unconditional generation tasks across
CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our
method. Results show over 50% throughput improvement across all datasets and
settings while maintaining comparable or better quality. On unconditional
CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional
CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an
FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1
produces high-quality samples with a FID of 4.00 and an IS of 195.06, together
with substantial efficiency gains.

</details>


### [16] [MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation](https://arxiv.org/abs/2509.15357)
*Yu Chang,Jiahao Chen,Anzhe Cheng,Paul Bogdan*

Main category: cs.CV

TL;DR: 提出MaskAttn-SDXL，通过在SDXL的UNet跨注意力logit层引入可学习的二值掩码，稀疏化文本token与图像潜变量的连接，从而减少多对象、多属性、多空间关系提示下的组合性失败，提升空间遵从与属性绑定，且无需额外位置编码、辅助token或外部区域掩码，开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在复杂提示（多对象、属性与空间关系）下常出现实体纠缠、属性互串、空间违反等组合性错误，源于跨token干扰。需要一种数据与推理代价低、无需额外标注的机制，精确控制跨注意力的语义对齐，提升组合控制能力。

Method: 在SDXL的UNet中，对每一层跨注意力的logit地图注入可学习的二值掩码（logit前的门控），对token-to-latent交互进行稀疏化，仅保留语义相关连接；不使用位置编码、辅助token或外部区域掩码，保持原始推理路径与几乎不增加开销。

Result: 在多对象提示上，模型显著提升空间合规与属性绑定，同时保持图像质量和多样性；显示logit级掩码跨注意力作为数据高效的组合控制原语的有效性。

Conclusion: Logit级别的掩码化跨注意力是实现文本到图像组合与空间控制的高效实用方法；MaskAttn-SDXL为SDXL提供了几乎零开销、无需额外标注的空间与属性控制扩展，改善复杂提示的合规性且不损害质量与多样性。

Abstract: Text-to-image diffusion models achieve impressive realism but often suffer
from compositional failures on prompts with multiple objects, attributes, and
spatial relations, resulting in cross-token interference where entities
entangle, attributes mix across objects, and spatial cues are violated. To
address these failures, we propose MaskAttn-SDXL,a region-level gating
mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s
UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each
cross-attention logit map before softmax to sparsify token-to-latent
interactions so that only semantically relevant connections remain active. The
method requires no positional encodings, auxiliary tokens, or external region
masks, and preserves the original inference path with negligible overhead. In
practice, our model improves spatial compliance and attribute binding in
multi-object prompts while preserving overall image quality and diversity.
These findings demonstrate that logit-level maksed cross-attention is an
data-efficient primitve for enforcing compositional control, and our method
thus serves as a practical extension for spatial control in text-to-image
generation.

</details>


### [17] [RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation](https://arxiv.org/abs/2509.15391)
*Mst Tasnim Pervin,George Bebis,Fang Jiang,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: 提出RaceGAN用于多域无参考的人种特征翻译，保持个体身份与高层语义，在Chicago Face Dataset上优于现有方法，并用分类与潜空间聚类验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有多域图像到图像翻译（如StarGAN/StyleGAN/StarGANv2）要么只能处理有限域、要么难以捕获细致风格、或依赖参考图且易丢失个体身份。作者希望在无需参考图的情况下，在多个种族域间翻译面部风格，同时保持个体身份与高层语义。

Method: 提出RaceGAN：通过学习可在多域间共享且可控的风格编码，将输入人脸映射到目标种族域；设计对抗训练与循环/一致性约束以保持身份与高层语义；引入风格编码的多域映射与潜空间聚类机制，使不同种族的风格在潜空间中分离。

Result: 在Chicago Face Dataset上，RaceGAN在将人脸翻译为Asian、White、Black等域时，视觉质量与种族特征准确性优于CycleGAN/StarGAN/StarGANv2/StyleGAN等；通过基于InceptionResNetv2的分类器定量评估，RaceGAN生成的图像在目标种族识别上取得更高准确率；潜空间分析显示不同种族形成可分离的聚类。

Conclusion: RaceGAN无需参考图即可在多域间进行人种风格翻译，兼顾个体身份与高层语义，相比现有方法在视觉与定量指标上更优，并在潜空间中形成清晰的种族聚类，证明其风格编码与多域映射的有效性。

Abstract: Generative adversarial networks (GANs) have demonstrated significant progress
in unpaired image-to-image translation in recent years for several
applications. CycleGAN was the first to lead the way, although it was
restricted to a pair of domains. StarGAN overcame this constraint by tackling
image-to-image translation across various domains, although it was not able to
map in-depth low-level style changes for these domains. Style mapping via
reference-guided image synthesis has been made possible by the innovations of
StarGANv2 and StyleGAN. However, these models do not maintain individuality and
need an extra reference image in addition to the input. Our study aims to
translate racial traits by means of multi-domain image-to-image translation. We
present RaceGAN, a novel framework capable of mapping style codes over several
domains during racial attribute translation while maintaining individuality and
high level semantics without relying on a reference image. RaceGAN outperforms
other models in translating racial features (i.e., Asian, White, and Black)
when tested on Chicago Face Dataset. We also give quantitative findings
utilizing InceptionReNetv2-based classification to demonstrate the
effectiveness of our racial translation. Moreover, we investigate how well the
model partitions the latent space into distinct clusters of faces for each
ethnic group.

</details>


### [18] [Generating Part-Based Global Explanations Via Correspondence](https://arxiv.org/abs/2509.15393)
*Kunal Rathore,Prasad Tadepalli*

Main category: cs.CV

TL;DR: 提出一种方法：用少量图像上的用户定义部件标签，自动高效地转移到大规模数据集，并将部件级局部解释聚合为全球性的符号解释，从而以人类可理解方式解释深度模型决策。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习可解释方法要么局限于单图的局部可视化、缺乏整体性；要么依赖概念/部件的大量标注，成本高。需要一种在标注成本低的前提下得到全局、可人理解的解释的途径。

Method: 在少量样本上收集用户定义的“部件”标签，通过标签转移机制将这些部件标签扩展到更大数据集；对每张图生成部件级的局部解释，并在数据集层面聚合为符号化（基于部件/概念）的全局解释，以解释模型的决策模式。

Result: 实现了从小规模标注到大规模数据的高效标签迁移；能够在大范围上生成基于部件的全局符号解释，提升解释的可理解性与覆盖面。

Conclusion: 在有限标注条件下，仍可获得大规模、可人理解的全局解释；该方法缓解了概念解释的标注成本，同时保留局部到全局的解释连接。

Abstract: Deep learning models are notoriously opaque. Existing explanation methods
often focus on localized visual explanations for individual images.
Concept-based explanations, while offering global insights, require extensive
annotations, incurring significant labeling cost. We propose an approach that
leverages user-defined part labels from a limited set of images and efficiently
transfers them to a larger dataset. This enables the generation of global
symbolic explanations by aggregating part-based local explanations, ultimately
providing human-understandable explanations for model decisions on a large
scale.

</details>


### [19] [Causal Fingerprints of AI Generative Models](https://arxiv.org/abs/2509.15406)
*Hui Xu,Chi Liu,Congcong Zhu,Minghao Wang,Youyang Qu,Longxiang Gao*

Main category: cs.CV

TL;DR: 提出“因果指纹”（causal fingerprint）概念，以因果视角分离生成模型的来源痕迹，优于依赖模型特定伪影的传统方法，实验证明在跨GAN与扩散模型的归因与匿名化任务上更强。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法多依赖模型特定线索或合成伪影，泛化性差；需要一种能够反映图像来源与模型痕迹之间因果关系的、更完整、更可推广的指纹来提升来源归因、伪造检测与版权追踪能力。

Method: 定义生成模型的“因果指纹”，提出因果解耦框架：在由预训练扩散模型的重建残差构造的语义不变潜空间中，将模型指纹与图像内容/风格解耦；并通过多样化特征表征提升指纹粒度。利用该因果指纹进行归因与反事实样本生成，实现来源匿名化。

Result: 在多种代表性GAN与扩散模型上评估，模型归因性能优于现有方法；通过从因果指纹生成的反事实样本，可实现来源匿名化，验证了所提出因果视角的有效性。

Conclusion: 因果指纹与解耦框架能更准确、泛化地识别生成模型来源，并可用于匿名化与版权追踪等应用，显示出在伪造检测、模型版权与身份保护方面的潜力。

Abstract: AI generative models leave implicit traces in their generated images, which
are commonly referred to as model fingerprints and are exploited for source
attribution. Prior methods rely on model-specific cues or synthesis artifacts,
yielding limited fingerprints that may generalize poorly across different
generative models. We argue that a complete model fingerprint should reflect
the causality between image provenance and model traces, a direction largely
unexplored. To this end, we conceptualize the \emph{causal fingerprint} of
generative models, and propose a causality-decoupling framework that
disentangles it from image-specific content and style in a semantic-invariant
latent space derived from pre-trained diffusion reconstruction residual. We
further enhance fingerprint granularity with diverse feature representations.
We validate causality by assessing attribution performance across
representative GANs and diffusion models and by achieving source anonymization
using counterfactual examples generated from causal fingerprints. Experiments
show our approach outperforms existing methods in model attribution, indicating
strong potential for forgery detection, model copyright tracing, and identity
protection.

</details>


### [20] [NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training](https://arxiv.org/abs/2509.15416)
*Moinak Bhattacharya,Angelica P. Kurtz,Fabio M. Iwamoto,Prateek Prasanna,Gagandeep Singh*

Main category: cs.CV

TL;DR: 提出一种结合分布鲁棒优化（DRO）的神经肿瘤特定基础模型，用多机构MRI自监督预训练与DRO缓解站点与类别不平衡，显著提升常见与罕见分子标志预测及生存预测的跨机构泛化与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有通用基础模型在神经肿瘤领域泛化受限：数据异质性、肿瘤复杂性、跨机构分布漂移与类别不平衡导致对罕见分子标志（对治疗与分层关键）预测较差、嵌入呈站点偏差，影响临床可用性。

Method: 在多机构脑肿瘤MRI上预训练自监督骨干（BYOL/DINO/MAE/MoCo），然后在下游任务中引入分布鲁棒优化（DRO）以缓解站点偏差与类别不均。评估任务包括常见分子标志（MGMT、IDH1、1p/19q、EGFR）、罕见/连续标志（ATRX、TP53、CDKN2A/2B、TERT、Ki-67、TP53定量）及IDH1野生型胶母的总体生存预测；并用Grad-CAM检验可解释性。

Result: 在CUIMC等多站点上提升分子预测与生存区分：CUIMC均衡准确率0.744→0.785，AUC 0.656→0.676；对稀有端点提升更大（CDKN2A/2B准确率0.86→0.92，AUC 0.73→0.92；ATRX AUC 0.69→0.82；Ki-67准确率0.60→0.69）。生存c-index均提高：CUIMC 0.592→0.597，UPenn 0.647→0.672，UCSF 0.600→0.627。嵌入更站点不变，Grad-CAM聚焦肿瘤及周边区域。

Conclusion: 将神经肿瘤特定基础模型与DRO耦合可获得更站点不变的表征，改善常见与罕见分子标志预测并提升生存判别力；需开展前瞻性验证，并结合纵向与干预数据以推动精准神经肿瘤学应用。

Abstract: Neuro-oncology poses unique challenges for machine learning due to
heterogeneous data and tumor complexity, limiting the ability of foundation
models (FMs) to generalize across cohorts. Existing FMs also perform poorly in
predicting uncommon molecular markers, which are essential for treatment
response and risk stratification. To address these gaps, we developed a
neuro-oncology specific FM with a distributionally robust loss function,
enabling accurate estimation of tumor phenotypes while maintaining
cross-institution generalization. We pretrained self-supervised backbones
(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied
distributionally robust optimization (DRO) to mitigate site and class
imbalance. Downstream tasks included molecular classification of common markers
(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),
continuous markers (Ki-67, TP53), and overall survival prediction in IDH1
wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular
prediction and reduced site-specific embedding differences. At CUIMC, mean
balanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with
the largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to
0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).
For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647
to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral
regions, confirming interpretability. Overall, coupling FMs with DRO yields
more site-invariant representations, improves prediction of common and uncommon
markers, and enhances survival discrimination, underscoring the need for
prospective validation and integration of longitudinal and interventional
signals to advance precision neuro-oncology.

</details>


### [21] [ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2509.15435)
*Chung-En Johnny Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: ORCA 是一种面向大型视觉语言模型（LVLM）的代理式推理框架，通过在测试时利用一套小型视觉模型进行结构化推理，显著降低幻觉并提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LVLM 在多模态任务上表现强，但易受内在错误导致的事实性幻觉及外部攻击导致的对抗性失败，限制了真实应用的可靠性。作者希望在不改动模型内部或再训练的情况下提升事实准确性与鲁棒性。

Method: 提出 Observe–Reason–Critique–Act 循环：在推理时向多个小型视觉工具提出证据性问题；交叉验证不同模型的输出以发现不一致；迭代地修正预测；并记录中间推理痕迹以支持可审计。该流程面向对象级幻觉，但不需对抗训练或专门防御机制。

Result: 在 POPE 幻觉基准的干净图像上，ORCA 相对单独 LVLM 提升 +3.64% 至 +40.67%；在对抗扰动下，平均提升 +20.11%；在 AMBER 对抗图像上结合防御技术进一步提升，指标增益 +1.20% 至 +48.00%。

Conclusion: ORCA 作为测试时的结构化推理代理，可在不访问模型内部或再训练的前提下，提高 LVLM 的事实性与对抗鲁棒性，并提供可审计的决策过程，为构建更可靠的多模态系统提供了有前景的路径。

Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities
but remain vulnerable to hallucinations from intrinsic errors and adversarial
attacks from external exploitations, limiting their reliability in real-world
applications. We present ORCA, an agentic reasoning framework that improves the
factual accuracy and adversarial robustness of pretrained LVLMs through
test-time structured inference reasoning with a suite of small vision models
(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act
loop, querying multiple visual tools with evidential questions, validating
cross-model inconsistencies, and refining predictions iteratively without
access to model internals or retraining. ORCA also stores intermediate
reasoning traces, which supports auditable decision-making. Though designed
primarily to mitigate object-level hallucinations, ORCA also exhibits emergent
adversarial robustness without requiring adversarial training or defense
mechanisms. We evaluate ORCA across three settings: (1) clean images on
hallucination benchmarks, (2) adversarially perturbed images without defense,
and (3) adversarially perturbed images with defense applied. On the POPE
hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\%
to +40.67\% across different subsets. Under adversarial perturbations on POPE,
ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined
with defense techniques on adversarially perturbed AMBER images, ORCA further
improves standalone LVLM performance, with gains ranging from +1.20\% to
+48.00\% across evaluation metrics. These results demonstrate that ORCA offers
a promising path toward building more reliable and robust multimodal systems.

</details>


### [22] [Region-Aware Deformable Convolutions](https://arxiv.org/abs/2509.15436)
*Abolfazl Saheban Maleki,Maryam Imani*

Main category: cs.CV

TL;DR: 提出一种区域感知可变形卷积（RAD-Conv），通过为每个卷积核元素定义四个边界偏移，形成可动态调整的矩形采样区域，从而在保持卷积效率的同时提高对复杂图像结构的适应性与长程依赖建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统可变形卷积的采样点受限于固定的四边形形状，难以灵活控制感受野的宽高与形状，且小核（如1x1）难以覆盖长程依赖；注意力机制虽具备强适应性，但计算开销大。需要一种在不显著增加计算成本的前提下，能灵活匹配图像内容、同时捕获局部与全局信息的卷积算子。

Method: 为每个核元素引入四个边界偏移（上下左右），定义一个可变矩形采样区域；该区域相对于特征图动态调整尺寸与形状，与核结构解耦。通过这些偏移控制感受野的宽度与高度，实现对局部细节与远距离关系的覆盖，即使使用1x1核也可扩展感受野。

Result: RAD-Conv在不牺牲卷积高效性的前提下，获得接近注意力机制的适应性：能够精确控制感受野形状与范围，适配复杂图像结构，捕获长程依赖与细粒度细节。

Conclusion: RAD-Conv为构建更具表达力且高效的视觉模型提供了实用方案，弥合了刚性的卷积架构与高计算成本的注意力方法之间的鸿沟。

Abstract: We introduce Region-Aware Deformable Convolution (RAD-Conv), a new
convolutional operator that enhances neural networks' ability to adapt to
complex image structures. Unlike traditional deformable convolutions, which are
limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary
offsets per kernel element to create flexible, rectangular regions that
dynamically adjust their size and shape to match image content. This approach
allows precise control over the receptive field's width and height, enabling
the capture of both local details and long-range dependencies, even with small
1x1 kernels. By decoupling the receptive field's shape from the kernel's
structure, RAD-Conv combines the adaptability of attention mechanisms with the
efficiency of standard convolutions. This innovative design offers a practical
solution for building more expressive and efficient vision models, bridging the
gap between rigid convolutional architectures and computationally costly
attention-based methods.

</details>


### [23] [CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction](https://arxiv.org/abs/2509.15459)
*Yiyi Liu,Chunyang Liu,Weiqin Jiao,Bojian Wu,Fashuai Li,Biao Xiong*

Main category: cs.CV

TL;DR: 提出CAGE（Continuity-Aware edGE）网络，从点云密度图直接重建矢量化平面图，以边为原生单元，实现稳健、连贯、拓扑合法的房间边界。通过双查询Transformer去噪解码器提升鲁棒性与收敛速度，在Structured3D与SceneCAD上达SOTA（房间F1 99.1%、角点91.7%、角度89.3%），并具备跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 传统以角点-多边形为主的表示对噪声与缺失观测高度敏感，易产生破碎或不合理布局；线段分组方法虽利用结构线索增强鲁棒性，但难以恢复细致几何。需要一种能原生处理墙体连续性的边中心表示，提高拓扑一致性、减少伪影并提升对点云不完备与噪声的适应。

Method: 提出原生的“有向、几何连续”的边表示，把每个墙段建模为连续边；设计双查询Transformer解码器，将“扰动查询”与“潜在查询”在去噪框架中融合，稳定优化并加速收敛；整体从点云密度图直接推理可闭合、拓扑有效的房间结构。

Result: 在Structured3D与SceneCAD上取得SOTA：房间F1 99.1%、角点F1 91.7%、角度F1 89.3%；显示出强跨数据集泛化能力，鲁棒性与细节恢复均优。

Conclusion: 边中心、连续性感知的表示结合双查询去噪Transformer，可从点云密度图稳健重建高质量矢量平面图，确保水密与拓扑合法，显著优于角点或线分组方法，并具备良好泛化。

Abstract: We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a
\textcolor{red}{robust} framework for reconstructing vector floorplans directly
from point-cloud density maps. Traditional corner-based polygon representations
are highly sensitive to noise and incomplete observations, often resulting in
fragmented or implausible layouts. Recent line grouping methods leverage
structural cues to improve robustness but still struggle to recover fine
geometric details. To address these limitations, we propose a \textit{native}
edge-centric formulation, modeling each wall segment as a directed,
geometrically continuous edge. This representation enables inference of
coherent floorplan structures, ensuring watertight, topologically valid room
boundaries while improving robustness and reducing artifacts. Towards this
design, we develop a dual-query transformer decoder that integrates perturbed
and latent queries within a denoising framework, which not only stabilizes
optimization but also accelerates convergence. Extensive experiments on
Structured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-art
performance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%
(angles). The method also demonstrates strong cross-dataset generalization,
underscoring the efficacy of our architectural innovations. Code and pretrained
models will be released upon acceptance.

</details>


### [24] [Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture](https://arxiv.org/abs/2509.15470)
*Thomas Z. Li,Aravind R. Krishnan,Lianrui Zuo,John M. Still,Kim L. Sandler,Fabien Maldonado,Thomas A. Lasko,Bennett A. Landman*

Main category: cs.CV

TL;DR: 利用未标注的纵向CT与电子病历档案进行JEPA自监督预训练，以缓解多模态肺结节诊断中数据稀缺与过拟合问题；在内部数据上优于无正则多模态与仅影像模型，但外部泛化较弱，并构建合成环境剖析其失效条件。


<details>
  <summary>Details</summary>
Motivation: 多模态肺结节诊断模型受限于标注稀缺与训练分布过拟合，亟需能利用大规模未标注医疗档案的学习策略，以提升性能并增强泛化能力。

Method: 从本院整理无标签患者队列，包含CT与关联EHR；采用JEPA进行多模态自监督预训练，再进行有监督微调；与不加正则的多模态模型及仅影像模型对比；并构建合成环境分析JEPA在何种情境下表现不佳。

Result: 内部验证：本方法AUC=0.91，优于多模态无正则模型AUC=0.88与影像仅模型AUC=0.73；外部验证：本方法AUC=0.72，低于影像仅模型AUC=0.75，显示跨域泛化不足。

Conclusion: 利用未标注多模态纵向档案进行JEPA预训练可提升内部性能，但对外部数据泛化有限；合成环境分析揭示其可能失效情境，提示需改进跨域鲁棒性与分布偏移适应。

Abstract: The development of multimodal models for pulmonary nodule diagnosis is
limited by the scarcity of labeled data and the tendency for these models to
overfit on the training distribution. In this work, we leverage self-supervised
learning from longitudinal and multimodal archives to address these challenges.
We curate an unlabeled set of patients with CT scans and linked electronic
health records from our home institution to power joint embedding predictive
architecture (JEPA) pretraining. After supervised finetuning, we show that our
approach outperforms an unregularized multimodal model and imaging-only model
in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC),
but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC).
We develop a synthetic environment that characterizes the context in which JEPA
may underperform. This work innovates an approach that leverages unlabeled
multimodal medical archives to improve predictive models and demonstrates its
advantages and limitations in pulmonary nodule diagnosis.

</details>


### [25] [Efficient Multimodal Dataset Distillation via Generative Models](https://arxiv.org/abs/2509.15472)
*Zhenghao Zhao,Haoxuan Wang,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: 提出EDGE：一种用于多模态（图文）数据集蒸馏的高效生成式方法，通过对比与多样性优化，在更少计算下生成小而有效的合成数据，在Flickr30K、COCO、CC3M上优于现有方法且速度快18倍。


<details>
  <summary>Details</summary>
Motivation: 现有多模态数据集蒸馏依赖Matching Training Trajectories(MTT)，计算昂贵、耗时数天；同时生成模型在图文相关性弱与样本多样性不足方面存在瓶颈，影响检索与下游性能。需要一种高效、相关性强、具多样性的生成式蒸馏框架。

Method: 提出EDGE生成式蒸馏流程：1) 训练生成模型时引入双向对比损失（图到文、文到图）以强化图文对齐；2) 引入多样性损失提升生成样本的覆盖与差异；3) 设计caption合成策略，增加文本信息量以提升text-to-image检索；整体替代MTT，降低计算成本。

Result: 在Flickr30K、COCO、CC3M基准上，EDGE在检索与相关任务上优于现有蒸馏方法，并在效率上显著提升，达到SOTA方法的18倍速度。

Conclusion: 生成式蒸馏EDGE通过对比对齐与多样性约束解决图文相关性与样本同质化问题，结合caption增强策略，实现更优性能与显著的计算效率提升，为多模态数据集蒸馏提供实用、高效的替代方案。

Abstract: Dataset distillation aims to synthesize a small dataset from a large dataset,
enabling the model trained on it to perform well on the original dataset. With
the blooming of large language models and multimodal large language models, the
importance of multimodal datasets, particularly image-text datasets, has grown
significantly. However, existing multimodal dataset distillation methods are
constrained by the Matching Training Trajectories algorithm, which
significantly increases the computing resource requirement, and takes days to
process the distillation. In this work, we introduce EDGE, a generative
distillation method for efficient multimodal dataset distillation.
Specifically, we identify two key challenges of distilling multimodal datasets
with generative models: 1) The lack of correlation between generated images and
captions. 2) The lack of diversity among generated samples. To address the
aforementioned issues, we propose a novel generative model training workflow
with a bi-directional contrastive loss and a diversity loss. Furthermore, we
propose a caption synthesis strategy to further improve text-to-image retrieval
performance by introducing more text information. Our method is evaluated on
Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and
efficiency compared to existing approaches. Notably, our method achieves
results 18x faster than the state-of-the-art method.

</details>


### [26] [OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data](https://arxiv.org/abs/2509.15479)
*Björn Möller,Zhengyang Li,Malte Stelzer,Thomas Graave,Fabian Bettels,Muaaz Ataya,Tim Fingscheidt*

Main category: cs.CV

TL;DR: OpenViGA提出一个可复现、开源的汽车驾驶场景视频生成系统，基于三模块（图像Tokenizer、世界模型、视频解码器），使用公开预训练模型与BDD100K数据在学术级GPU上微调，实现256x256、4fps、仅1帧延迟的逼真视频预测，并提供详尽的组件分析与开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有汽车驾驶视频生成方法依赖大型闭源模型、训练成本高、设计不可解释、代码与数据不公开，阻碍研究复现与对比。作者希望构建一个开放、可复现且资源友好的系统，并系统性评估关键模块与接口设计，推动社区研究。

Method: 构建三组件流水线：1) 图像Tokenizer：采用强大的开源预训练图像离散化/表示模型并微调；2) 世界模型：基于开源序列/扩散/Transformer类模型进行未来状态（token序列）预测；3) 视频解码器：将预测token解码为视频帧。对各组件进行定量与定性评估，统一并精简模块接口；使用BDD100K进行微调；在学术级GPU上训练；保证代码与模型开源以便复现。

Result: 系统在256×256分辨率、4帧每秒设置下，能帧间只需1帧算法延迟生成逼真的驾驶场景视频。提供组件级别的详细评估结果与可复现实现（代码与模型发布）。

Conclusion: OpenViGA证明了基于开源预训练模型与公开数据的模块化视频生成系统可在有限资源下达到高质量驾驶场景预测；清晰接口与组件评估提升了可解释性与复现性，为后续研究提供开放基准与参考实现。

Abstract: Recent successful video generation systems that predict and create realistic
automotive driving scenes from short video inputs assign tokenization, future
state prediction (world model), and video decoding to dedicated models. These
approaches often utilize large models that require significant training
resources, offer limited insight into design choices, and lack publicly
available code and datasets. In this work, we address these deficiencies and
present OpenViGA, an open video generation system for automotive driving
scenes. Our contributions are: Unlike several earlier works for video
generation, such as GAIA-1, we provide a deep analysis of the three components
of our system by separate quantitative and qualitative evaluation: Image
tokenizer, world model, video decoder. Second, we purely build upon powerful
pre-trained open source models from various domains, which we fine-tune by
publicly available automotive data (BDD100K) on GPU hardware at academic scale.
Third, we build a coherent video generation system by streamlining interfaces
of our components. Fourth, due to public availability of the underlying models
and data, we allow full reproducibility. Finally, we also publish our code and
models on Github. For an image size of 256x256 at 4 fps we are able to predict
realistic driving scene videos frame-by-frame with only one frame of
algorithmic latency.

</details>


### [27] [Comparing Computational Pathology Foundation Models using Representational Similarity Analysis](https://arxiv.org/abs/2509.15482)
*Vaibhav Mishra,William Lotter*

Main category: cs.CV

TL;DR: 本文系统性分析六个计算病理学（CPath）基础模型的表征空间差异与结构，发现不同训练范式并不必然带来更高表征相似度；模型对切片（slide）特征高度敏感、对疾病类别敏感度较低；染色归一化可降低切片依赖；视觉-语言模型更紧致、视觉-only模型更分散，提示鲁棒性提升与集成策略的机会。


<details>
  <summary>Details</summary>
Motivation: 现有研究多比较下游任务性能，但对基础模型内部表征如何组织与变化了解不足。为提升模型鲁棒性、指导集成与部署，需要量化不同CPath基础模型表征空间的结构与可变性。

Method: 选取六个CPath基础模型（CONCH、PLIP、KEEP为视觉-语言对比学习；UNI(v2)、Virchow(v2)、Prov-GigaPath为自蒸馏/视觉-only），以TCGA的H&E图像小块为数据，使用计算神经科学常用的表征相似性分析（RSA），评估模型间表征结构相似度、对切片与疾病的依赖、染色归一化的影响，以及表征的内在维度（intrinsic dimensionality）。

Result: - UNI2与Virchow2的表征结构最为独特；Prov-GigaPath与其他模型平均相似度最高。
- 相同训练范式（视觉-only vs 视觉-语言）并不保证更高的表征相似性。
- 所有模型对切片特异性（slide-dependence）高、对疾病依赖低。
- 染色归一化可降低切片依赖，降幅5.5%（CONCH）到20.5%（PLIP）。
- 视觉-语言模型表征更紧致（低内在维度），视觉-only模型更分散（高内在维度）。

Conclusion: 不同CPath基础模型的表征空间存在显著结构差异，训练范式影响但不决定表征相似性。模型普遍受切片特征干扰而对疾病信号敏感度不足；染色归一化能缓解这一问题。视觉-语言模型的紧致表征与视觉-only模型的分散表征为鲁棒性改进、模型集成与训练方案设计提供依据。该分析框架可扩展至其他医学影像领域，以促进基础模型的有效开发与部署。

Abstract: Foundation models are increasingly developed in computational pathology
(CPath) given their promise in facilitating many downstream tasks. While recent
studies have evaluated task performance across models, less is known about the
structure and variability of their learned representations. Here, we
systematically analyze the representational spaces of six CPath foundation
models using techniques popularized in computational neuroscience. The models
analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and
self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through
representational similarity analysis using H&E image patches from TCGA, we find
that UNI2 and Virchow2 have the most distinct representational structures,
whereas Prov-Gigapath has the highest average similarity across models. Having
the same training paradigm (vision-only vs. vision-language) did not guarantee
higher representational similarity. The representations of all models showed a
high slide-dependence, but relatively low disease-dependence. Stain
normalization decreased slide-dependence for all models by a range of 5.5%
(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language
models demonstrated relatively compact representations, compared to the more
distributed representations of vision-only models. These findings highlight
opportunities to improve robustness to slide-specific features, inform model
ensembling strategies, and provide insights into how training paradigms shape
model representations. Our framework is extendable across medical imaging
domains, where probing the internal representations of foundation models can
help ensure effective development and deployment.

</details>


### [28] [SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters](https://arxiv.org/abs/2509.15490)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: SmolRGPT是一种紧凑型视觉-语言模型，融合RGB与深度信息，并通过三阶段课程学习强化区域级空间推理，在仅600M参数下在仓储空间推理基准上达到或超越更大模型的表现，面向低资源场景的高效部署。


<details>
  <summary>Details</summary>
Motivation: 现有最先进VLM常依赖超大规模模型，计算与内存开销高，难以在仓库、机器人、工业等资源受限场景部署；同时这些场景需要高效且鲁棒的空间理解能力。作者希望在保持空间推理核心能力的前提下，提供可部署的高效多模态智能方案。

Method: 提出SmolRGPT架构：显式引入区域级空间推理，融合RGB与深度线索；使用三阶段课程学习逐步实现（1）视觉与语言特征对齐，（2）空间关系理解强化，（3）面向任务的数据集适配。

Result: 在仓储空间推理基准上，以约600M参数取得与更大模型相当或更优的成绩，显示在真实世界任务中保持竞争力。

Conclusion: 紧凑的多模态架构通过融合深度与区域级推理加上分阶段训练，可在资源受限环境中实现可部署、且不牺牲空间推理能力的VLM；代码即将开源以促进复现与应用。

Abstract: Recent advances in vision-language models (VLMs) have enabled powerful
multimodal reasoning, but state-of-the-art approaches typically rely on
extremely large models with prohibitive computational and memory requirements.
This makes their deployment challenging in resource-constrained environments
such as warehouses, robotics, and industrial applications, where both
efficiency and robust spatial understanding are critical. In this work, we
present SmolRGPT, a compact vision-language architecture that explicitly
incorporates region-level spatial reasoning by integrating both RGB and depth
cues. SmolRGPT employs a three-stage curriculum that progressively align visual
and language features, enables spatial relationship understanding, and adapts
to task-specific datasets. We demonstrate that with only 600M parameters,
SmolRGPT achieves competitive results on challenging warehouse spatial
reasoning benchmarks, matching or exceeding the performance of much larger
alternatives. These findings highlight the potential for efficient, deployable
multimodal intelligence in real-world settings without sacrificing core spatial
reasoning capabilities. The code of the experimentation will be available at:
https://github.com/abtraore/SmolRGPT

</details>


### [29] [Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/abs/2509.15496)
*Shen Sang,Tiancheng Zhi,Tianpei Gu,Jing Liu,Linjie Luo*

Main category: cs.CV

TL;DR: Lynx是一种从单张输入图像生成个性化视频的高保真模型，基于开源DiT并加入两个轻量适配器（ID-adapter与Ref-adapter）以强化身份一致性与细节注入，在40主体×20提示的基准上展现更优的人脸相似度、较强指令遵循和高视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有个性化视频生成常面临身份漂移、细节缺失与时序一致性不足的问题；需要在不牺牲生成质量与提示可控性的前提下，从极少的个体条件（单张图）稳定地保留身份并生成连贯视频。

Method: 以开源Diffusion Transformer为基础：1) ID-adapter用Perceiver Resampler将ArcFace面部嵌入编码成紧凑身份token，并用于条件控制；2) Ref-adapter通过一个冻结的参考路径提供密集VAE特征，在所有Transformer层通过交叉注意力注入细粒度外观信息；两个适配器协同在生成过程中维持身份与细节，同时保证时序一致性与真实感。

Result: 在包含40名主体与20个无偏提示（共800案例）的基准上，Lynx在面部相似度上优于现有方法，在提示遵循方面具有竞争力，且视频质量强，整体推进了个性化视频生成的表现。

Conclusion: 通过在DiT上集成ID与Ref双适配器，Lynx有效解决单图条件下的身份保持与细节注入问题，实现更高的身份保真、指令遵循与视频质量，代表个性化视频合成领域的进展。

Abstract: We present Lynx, a high-fidelity model for personalized video synthesis from
a single input image. Built on an open-source Diffusion Transformer (DiT)
foundation model, Lynx introduces two lightweight adapters to ensure identity
fidelity. The ID-adapter employs a Perceiver Resampler to convert
ArcFace-derived facial embeddings into compact identity tokens for
conditioning, while the Ref-adapter integrates dense VAE features from a frozen
reference pathway, injecting fine-grained details across all transformer layers
through cross-attention. These modules collectively enable robust identity
preservation while maintaining temporal coherence and visual realism. Through
evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which
yielded 800 test cases, Lynx has demonstrated superior face resemblance,
competitive prompt following, and strong video quality, thereby advancing the
state of personalized video generation.

</details>


### [30] [Backdoor Mitigation via Invertible Pruning Masks](https://arxiv.org/abs/2509.15497)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: 提出一种带可逆稀疏掩码与学习型参数选择机制的剪枝方法，用双层优化同时定位并剪除触发后门的参数，同时通过反掩码合成触发并评估，最终在多数据集上优于现有剪枝防御、在低数据下稳健，并接近或优于微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于剪枝的后门防御难以准确定位诱发后门的具体参数，导致清洁任务受损或后门残留；虽然微调更常用且效果好，但其可解释性弱、在小数据情形不稳。作者希望利用剪枝的可解释性，同时提升对后门参数的精确识别与清除能力。

Method: 设计一个学习型“选择”机制标记对主任务与后门任务都关键的参数；引入可逆（invertible）的稀疏掩码，使同一掩码既能压制后门（正向掩码）又能通过其反掩码重现/合成触发以评估。构建双层优化：内层使用反掩码在干净数据上合成样本特定的后门扰动（候选触发），外层优化稀疏可逆掩码与选择变量，使后门行为被抑制且主任务精度保持。

Result: 在广泛实验中，该方法优于现有剪枝型后门缓解方法；在数据有限场景下保持强性能；与最先进的微调防御方法相比具有竞争性。特别是在成功缓解后门后，能显著恢复被污染样本的正确预测。

Conclusion: 可逆掩码与选择机制的剪枝策略通过双层优化实现对后门参数的精确定位与移除，在不牺牲主任务性能的前提下有效缓解后门，并在低数据与实际恢复能力方面表现突出，为剪枝型后门防御提供更可解释且强健的替代方案。

Abstract: Model pruning has gained traction as a promising defense strategy against
backdoor attacks in deep learning. However, existing pruning-based approaches
often fall short in accurately identifying and removing the specific parameters
responsible for inducing backdoor behaviors. Despite the dominance of
fine-tuning-based defenses in recent literature, largely due to their superior
performance, pruning remains a compelling alternative, offering greater
interpretability and improved robustness in low-data regimes. In this paper, we
propose a novel pruning approach featuring a learned \emph{selection} mechanism
to identify parameters critical to both main and backdoor tasks, along with an
\emph{invertible} pruning mask designed to simultaneously achieve two
complementary goals: eliminating the backdoor task while preserving it through
the inverse mask. We formulate this as a bi-level optimization problem that
jointly learns selection variables, a sparse invertible mask, and
sample-specific backdoor perturbations derived from clean data. The inner
problem synthesizes candidate triggers using the inverse mask, while the outer
problem refines the mask to suppress backdoor behavior without impairing
clean-task accuracy. Extensive experiments demonstrate that our approach
outperforms existing pruning-based backdoor mitigation approaches, maintains
strong performance under limited data conditions, and achieves competitive
results compared to state-of-the-art fine-tuning approaches. Notably, the
proposed approach is particularly effective in restoring correct predictions
for compromised samples after successful backdoor mitigation.

</details>


### [31] [MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training](https://arxiv.org/abs/2509.15514)
*Junbiao Pang,Tianyang Cai,Baochang Zhang*

Main category: cs.CV

TL;DR: 提出MEC-Quant，通过最大熵编码目标改善极低比特QAT的泛化，达到或超越全精度精度。


<details>
  <summary>Details</summary>
Motivation: 传统量化感知训练（QAT）在极低比特下性能明显落后于全精度，原因在于量化引入偏置使表示结构失衡与信息受损；需要一个更有原则性的目标来减少表示偏置、提升在分布内样本上的泛化。

Method: 提出最大熵编码量化（MEC-Quant）：直接优化表示的结构，使用有损数据编码的最小编码长度作为熵的可训练替代指标，并将目标以Mixture of Experts（MOE）形式重写以实现可扩展的高效计算，同时处理权重/激活值的长尾分布；端到端训练。

Result: 在多种计算机视觉任务上表现优于现有QAT；在极低比特激活设置下首次将QAT的性能推至新极限，MEC-Quant的准确率可与全精度相当甚至超越。

Conclusion: 最大熵编码的量化训练能减少量化偏置、改善表示与泛化，在无需复杂技巧的情况下建立QAT新的SOTA。

Abstract: Quantization-Aware Training (QAT) has driven much attention to produce
efficient neural networks. Current QAT still obtains inferior performances
compared with the Full Precision (FP) counterpart. In this work, we argue that
quantization inevitably introduce biases into the learned representation,
especially under the extremely low-bit setting. To cope with this issue, we
propose Maximum Entropy Coding Quantization (MEC-Quant), a more principled
objective that explicitly optimizes on the structure of the representation, so
that the learned representation is less biased and thus generalizes better to
unseen in-distribution samples. To make the objective end-to-end trainable, we
propose to leverage the minimal coding length in lossy data coding as a
computationally tractable surrogate for the entropy, and further derive a
scalable reformulation of the objective based on Mixture Of Experts (MOE) that
not only allows fast computation but also handles the long-tailed distribution
for weights or activation values. Extensive experiments on various tasks on
computer vision tasks prove its superiority. With MEC-Qaunt, the limit of QAT
is pushed to the x-bit activation for the first time and the accuracy of
MEC-Quant is comparable to or even surpass the FP counterpart. Without bells
and whistles, MEC-Qaunt establishes a new state of the art for QAT.

</details>


### [32] [GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents](https://arxiv.org/abs/2509.15532)
*Xianhang Ye,Yiqing Li,Wei Dai,Miancan Liu,Ziyuan Chen,Zhangye Han,Hongbo Min,Jinkui Ren,Xiantao Zhang,Wen Yang,Zhi Jin*

Main category: cs.CV

TL;DR: 提出GUI-ARP框架，通过自适应区域感知与阶段控制，实现在高分辨率界面截图中的任务相关区域裁剪与分阶段推理，结合监督微调与GRPO强化微调，取得GUI定位SOTA（ScreenSpot-Pro 60.8%、UI-Vision 30.9%），7B模型媲美更大开源与商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位方法在高分辨率截图上难以进行细粒度定位，复杂界面下单一推理策略效率低、精度差；需要一种能动态聚焦关键区域并根据任务难度调整推理深度的方案。

Method: 提出GUI-ARP框架，包括：1）Adaptive Region Perception（ARP）：基于视觉注意力自适应裁剪任务相关区域，减少噪声与分辨率负担；2）Adaptive Stage Controlling（ASC）：依据任务复杂度选择单阶段或多阶段分析策略；3）两阶段训练：先监督微调，再用基于Group Relative Policy Optimization（GRPO）的强化微调，协同优化感知与推理策略。

Result: 在GUI grounding基准上达到SOTA：7B模型在ScreenSpot-Pro上60.8%准确率，在UI-Vision上30.9%；相较开源72B模型（如UI-TARS-72B 38.1%）与专有模型表现有竞争力。

Conclusion: 自适应的区域感知与阶段控制结合监督与强化微调，可显著提升高分辨率GUI定位的精度与效率；小参数模型通过策略学习与多阶段推理可逼近甚至超越更大模型。

Abstract: Existing GUI grounding methods often struggle with fine-grained localization
in high-resolution screenshots. To address this, we propose GUI-ARP, a novel
framework that enables adaptive multi-stage inference. Equipped with the
proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC),
GUI-ARP dynamically exploits visual attention for cropping task-relevant
regions and adapts its inference strategy, performing a single-stage inference
for simple cases and a multi-stage analysis for more complex scenarios. This is
achieved through a two-phase training pipeline that integrates supervised
fine-tuning with reinforcement fine-tuning based on Group Relative Policy
Optimization (GRPO). Extensive experiments demonstrate that the proposed
GUI-ARP achieves state-of-the-art performance on challenging GUI grounding
benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9%
on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness
against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.

</details>


### [33] [SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models](https://arxiv.org/abs/2509.15536)
*Sen Wang,Jingyi Tian,Le Wang,Zhimin Liao,Jiayi Li,Huaiyi Dong,Kun Xia,Sanping Zhou,Wei Tang,Hua Gang*

Main category: cs.CV

TL;DR: SAMPO是一种用于世界模型的视频生成与预测框架，通过在空间维度采用双向注意、在时间维度采用因果解码，并配合多尺度非对称tokenizer与轨迹感知的运动提示模块，实现更高的空间一致性、时间一致性与推理效率（4.4倍加速），在条件视频预测与基于模型的控制上取得竞争性表现，并具备零样本泛化与随规模提升的性能增长。


<details>
  <summary>Details</summary>
Motivation: 现有自回归世界模型在视频预测中常出现空间结构破坏、解码低效、运动建模不足的问题，影响长时规划与控制。作者希望通过改进自回归策略与表示方式，提升画面连贯性、动态理解与推理速度，从而更好支持决策与控制。

Method: 提出SAMPO：在帧内采用视觉自回归（双向空间注意）以并行、保局部性生成；在帧间采用时间因果解码进行下一帧预测，实现混合自回归。设计非对称多尺度tokenizer：对已观测帧保留更多空间细节，对未来帧压缩成紧凑动态表示，兼顾性能与内存。引入轨迹感知的motion prompt模块，利用物体/机器人轨迹提供时空线索，聚焦动态区域，提升时间一致性与物理真实感。

Result: 在动作条件视频预测与基于模型的控制任务上取得竞争性结果；推理速度提升约4.4倍。还展示了零样本泛化能力与随模型规模增大而带来的性能提升。

Conclusion: 混合的尺度与时间自回归、双向空间注意、轨迹提示与非对称多尺度表示共同改善了世界模型的视觉连贯性、时间一致性与效率，使其更适合规划与控制，并能在未见任务与更大模型下进一步受益。

Abstract: World models allow agents to simulate the consequences of actions in imagined
environments for planning, control, and long-horizon decision-making. However,
existing autoregressive world models struggle with visually coherent
predictions due to disrupted spatial structure, inefficient decoding, and
inadequate motion modeling. In response, we propose \textbf{S}cale-wise
\textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt
(\textbf{SAMPO}), a hybrid framework that combines visual autoregressive
modeling for intra-frame generation with causal modeling for next-frame
generation. Specifically, SAMPO integrates temporal causal decoding with
bidirectional spatial attention, which preserves spatial locality and supports
parallel decoding within each scale. This design significantly enhances both
temporal consistency and rollout efficiency. To further improve dynamic scene
understanding, we devise an asymmetric multi-scale tokenizer that preserves
spatial details in observed frames and extracts compact dynamic representations
for future frames, optimizing both memory usage and model performance.
Additionally, we introduce a trajectory-aware motion prompt module that injects
spatiotemporal cues about object and robot trajectories, focusing attention on
dynamic regions and improving temporal consistency and physical realism.
Extensive experiments show that SAMPO achieves competitive performance in
action-conditioned video prediction and model-based control, improving
generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's
zero-shot generalization and scaling behavior, demonstrating its ability to
generalize to unseen tasks and benefit from larger model sizes.

</details>


### [34] [Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues](https://arxiv.org/abs/2509.15540)
*Wei Chen,Tongguan Wang,Feiyue Xue,Junkai Li,Hui Liu,Ying Sha*

Main category: cs.CV

TL;DR: 提出一种针对欲望、情绪与情感识别的对称双向多模态框架（SyDES），通过文本-图像互导、混合尺度图像与掩码图像建模提升意图相关表示，在MSED数据集上较SOTA有小幅F1提升。


<details>
  <summary>Details</summary>
Motivation: 现有情感/情绪识别方法偏重文本，忽视图像作为非语言补充；缺乏专门面向“人类欲望理解”的多模态方法。需更好地从图像中捕捉与意图相关的全局与细粒度特征，并实现文本与图像的深度交互与对齐。

Method: 设计对称双向多模态框架：1) 低分辨率整图用于跨模态对齐获取全局视觉表征；2) 高分辨率图像切分为子图并进行掩码图像建模强化局部细粒度特征；3) 文本引导的图像解码器与图像引导的文本解码器，实现局部与全局层面的深度交互；4) 采用混合尺度策略，在保证感知收益与计算成本平衡的同时进行子图掩码建模。

Result: 在MSED多模态数据集（含欲望理解、情绪与情感识别任务）上，较现有SOTA方法稳定提升：欲望理解F1提高1.1%，情绪识别提高0.6%，情感分析提高0.9%。

Conclusion: 双向互导与混合尺度掩码建模能更有效提取意图相关的图像与文本特征，改进欲望、情绪与情感识别表现；方法在实际数据集上验证有效，代码公开可复现。

Abstract: Desire, as an intention that drives human behavior, is closely related to
both emotion and sentiment. Multimodal learning has advanced sentiment and
emotion recognition, but multimodal approaches specially targeting human desire
understanding remain underexplored. And existing methods in sentiment analysis
predominantly emphasize verbal cues and overlook images as complementary
non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional
Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,
which enforces mutual guidance between text and image modalities to effectively
capture intention-related representations in the image. Specifically,
low-resolution images are used to obtain global visual representations for
cross-modal alignment, while high resolution images are partitioned into
sub-images and modeled with masked image modeling to enhance the ability to
capture fine-grained local features. A text-guided image decoder and an
image-guided text decoder are introduced to facilitate deep cross-modal
interaction at both local and global representations of image information.
Additionally, to balance perceptual gains with computation cost, a mixed-scale
image strategy is adopted, where high-resolution images are cropped into
sub-images for masked modeling. The proposed approach is evaluated on MSED, a
multimodal dataset that includes a desire understanding benchmark, as well as
emotion and sentiment recognition. Experimental results indicate consistent
improvements over other state-of-the-art methods, validating the effectiveness
of our proposed method. Specifically, our method outperforms existing
approaches, achieving F1-score improvements of 1.1% in desire understanding,
0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is
available at: https://github.com/especiallyW/SyDES.

</details>


### [35] [Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track](https://arxiv.org/abs/2509.15546)
*Ran Hong,Feng Lu,Leilei Cao,An Yan,Youhai Jiang,Fengjie Zhu*

Main category: cs.CV

TL;DR: 提出一个免训练的RVOS框架，借助视频-语言检查器与关键帧采样器，在无需额外训练的情况下显著提升Sa2VA在MeViS上的表现，J&F达64.14%，获ICCV 2025 LSVOS挑战RVOS赛道第二。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM与SAM~2结合的RVOS方法（如Sa2VA）虽能进行视频推理与分割，但存在两类突出问题：一是语言描述与视频内容不一致导致误检（假阳性）；二是关键帧选择粗糙，难以兼顾早期目标出现与长时序上下文，影响稳健性与准确性。作者希望在不额外训练的条件下，通过更可靠的语言-视频一致性验证与更智能的帧采样策略，提升整体分割质量与泛化。

Method: 训练免疫的两模块框架：1）视频-语言检查器（Video-Language Checker），显式验证查询中的主体与动作是否在视频中出现，用于过滤或矫正不匹配的分割候选，降低假阳性；2）关键帧采样器（Key-Frame Sampler），自适应选择信息量高的帧，既覆盖早期出现对象又捕获长程时序线索，从而为后续的SAM~2分割与LLM指导提供更可靠的上下文。整体在Sa2VA之上无训练集成。

Result: 在MeViS测试集上，无需任何额外训练即可达到J&F 64.14%的成绩，在ICCV 2025第七届LSVOS挑战的RVOS赛道中位列第二，表明方法在标准基准上的显著性能提升。

Conclusion: 通过显式的视频-语言一致性检查与自适应关键帧选择，训练免疫地增强了LLM+SAM~2式RVOS流程，有效减少误检并更好利用时序信息，取得强竞争力的指标与榜单名次，展示了无需训练的框架在RVOS任务中的实用性与推广潜力。

Abstract: Referential Video Object Segmentation (RVOS) aims to segment all objects in a
video that match a given natural language description, bridging the gap between
vision and language understanding. Recent work, such as Sa2VA, combines Large
Language Models (LLMs) with SAM~2, leveraging the strong video reasoning
capability of LLMs to guide video segmentation. In this work, we present a
training-free framework that substantially improves Sa2VA's performance on the
RVOS task. Our method introduces two key components: (1) a Video-Language
Checker that explicitly verifies whether the subject and action described in
the query actually appear in the video, thereby reducing false positives; and
(2) a Key-Frame Sampler that adaptively selects informative frames to better
capture both early object appearances and long-range temporal context. Without
any additional training, our approach achieves a J&F score of 64.14% on the
MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge
at ICCV 2025.

</details>


### [36] [MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild](https://arxiv.org/abs/2509.15548)
*Deming Li,Kaiwen Jiang,Yutao Tang,Ravi Ramamoorthi,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: 论文提出MS-GS：在稀疏视角、多外观（昼夜/季节）场景下，基于3D高斯Splatting并结合单目深度与SfM语义局部区域，对虚拟视角施加几何引导的细/粗粒度监督，提升几何一致性，缓解过拟合与过平滑，取得更好的真实感与跨数据集性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的“野外”照片集视角少、跨时间/季节变化大，导致NeRF/3DGS在重建与新视角合成时易过平滑、过拟合。需要在稀疏视角下引入可靠几何先验与跨视约束，兼顾多外观的对齐与一致性。

Method: 提出MS-GS框架：1) 用单目深度估计提取几何先验；2) 以SfM点作为锚，划分并对齐局部语义区域，提供稳健的几何与对齐线索；3) 构造虚拟视角，在细粒度与粗粒度两套几何引导监督下（多视约束）训练3DGS，鼓励3D一致性、降低过拟合；4) 提供新数据集与“野外”实验设置作为基准。

Result: 在多种稀疏视角与多外观条件下实现更具真实感的渲染，相比现有NeRF/3DGS改进方法显著优于它们，在不同数据集上均获得更高的定量与定性表现。

Conclusion: 几何先验（单目深度+SfM锚定的语义局部区域）与虚拟视角的几何引导监督有效提升了稀疏视角、多外观场景的3D一致性与渲染质量，缓解过平滑与过拟合，并为更现实的评测提供了数据与设置。

Abstract: In-the-wild photo collections often contain limited volumes of imagery and
exhibit multiple appearances, e.g., taken at different times of day or seasons,
posing significant challenges to scene reconstruction and novel view synthesis.
Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian
Splatting (3DGS) have improved in these areas, they tend to oversmooth and are
prone to overfitting. In this paper, we present MS-GS, a novel framework
designed with Multi-appearance capabilities in Sparse-view scenarios using
3DGS. To address the lack of support due to sparse initializations, our
approach is built on the geometric priors elicited from monocular depth
estimations. The key lies in extracting and utilizing local semantic regions
with a Structure-from-Motion (SfM) points anchored algorithm for reliable
alignment and geometry cues. Then, to introduce multi-view constraints, we
propose a series of geometry-guided supervision at virtual views in a
fine-grained and coarse scheme to encourage 3D consistency and reduce
overfitting. We also introduce a dataset and an in-the-wild experiment setting
to set up more realistic benchmarks. We demonstrate that MS-GS achieves
photorealistic renderings under various challenging sparse-view and
multi-appearance conditions and outperforms existing approaches significantly
across different datasets.

</details>


### [37] [Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification](https://arxiv.org/abs/2509.15553)
*Tian Lan,Yiming Zheng,Jianxin Yin*

Main category: cs.CV

TL;DR: 提出Diff-Feat：从预训练扩散-Transformer模型中提取中间特征，并进行跨模态融合用于多标签分类，显著提升mAP并形成更紧密的语义簇。


<details>
  <summary>Details</summary>
Motivation: 多标签分类需要能捕捉标签间相互作用的强表征。现有方法（CNN、图模型、Transformer）虽有效，但在充分利用扩散模型的中间表示、以及图像与文本跨模态信息融合方面仍有限。作者希望利用扩散-Transformer在生成过程中的阶段性特征，找到最判别性的中间层与步，并高效融合以提升下游分类性能。

Method: 1) 从预训练扩散-Transformer（图像DiT与文本扩散Transformer）在扩散过程不同timestep与网络不同block提取中间特征；2) 经验观察：视觉任务最佳特征在扩散中间步、Transformer中间块；语言任务最佳特征在无噪声步、最深块；3) 发现图像DiT-XL/2-256×256中“第12层”在多数据集上最优；4) 设计启发式局部搜索，在少量候选中定位最优“（图像/文本）×（块-时间步）”组合，避免穷举；5) 采用简单的融合-线性投影与加和得到联合表示；6) 用该表示进行多标签分类，并通过t-SNE与聚类指标评估语义紧致性。

Result: 在MS-COCO-enhanced上取得98.6% mAP，在Visual Genome 500上取得45.7% mAP，超过强CNN、图模型与Transformer基线；t-SNE与聚类指标显示比单模态表示更紧密的语义簇。

Conclusion: 扩散-Transformer的中间特征蕴含强判别力，且最佳层/步在视觉与语言上分布不同；通过启发式选择与简洁融合，可在多标签分类上达到SOTA，并提升语义聚类质量。

Abstract: Multi-label classification has broad applications and depends on powerful
representations capable of capturing multi-label interactions. We introduce
\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate
features from pre-trained diffusion-Transformer models for images and text, and
fuses them for downstream tasks. We observe that for vision tasks, the most
discriminative intermediate feature along the diffusion process occurs at the
middle step and is located in the middle block in Transformer. In contrast, for
language tasks, the best feature occurs at the noise-free step and is located
in the deepest block. In particular, we observe a striking phenomenon across
varying datasets: a mysterious "Layer $12$" consistently yields the best
performance on various downstream classification tasks for images (under
DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that
pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a
few candidates, avoiding an exhaustive grid search. A simple fusion-linear
projection followed by addition-of the selected representations yields
state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on
Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a
wide margin. t-SNE and clustering metrics further reveal that
\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts.
The code is available at https://github.com/lt-0123/Diff-Feat.

</details>


### [38] [From Development to Deployment of AI-assisted Telehealth and Screening for Vision- and Hearing-threatening diseases in resource-constrained settings: Field Observations, Challenges and Way Forward](https://arxiv.org/abs/2509.15558)
*Mahesh Shakya,Bijay Adhikari,Nirsara Shrestha,Bipin Koirala,Arun Adhikari,Prasanta Poudyal,Luna Mathema,Sarbagya Buddhacharya,Bijay Khatri,Bishesh Khanal*

Main category: cs.CV

TL;DR: 在资源受限环境中推进AI辅助视听健康筛查与远程医疗的实践经验总结：通过迭代共创、影子部署与持续反馈，从纸质流程过渡到AI就绪；尽管存在领域偏移，公共数据与模型仍有价值；需自动化图像质量控制以支持高通量、稳健筛查。


<details>
  <summary>Details</summary>
Motivation: 在RCS中，视觉和听觉疾病导致可避免的残疾，但专家稀缺、筛查基础薄弱。AI大规模筛查与远程医疗可提升早检，但缺乏可借鉴的实际部署经验，且纸质流程阻碍落地，因此亟需总结挑战与方法论以推动可扩展的AI落地。

Method: 基于现场实践与“影子部署”形成的迭代、跨学科协作流程：早期原型、与一线人员共同工作、持续收集反馈；利用公共数据与现成AI模型，评估其在本地场景中的领域偏移；设计并引入自动化的图像质量检测以提高可分级图像比例，支撑高通量筛查。

Result: 发现：1) 迭代跨学科共创能建立共享理解，降低从纸质到数字化AI流程的可用性障碍；2) 公共数据与模型虽因领域偏移表现欠佳，但仍对开发有帮助；3) 自动图像质量检测对获取可分级图像和提高流程稳健性至关重要。

Conclusion: 应将AI开发与工作流程数字化视为端到端、迭代的共设计过程。通过记录实践挑战与经验，填补RCS中可操作的现场知识空白，为真实世界的AI辅助远程医疗与大规模筛查项目提供指导。

Abstract: Vision- and hearing-threatening diseases cause preventable disability,
especially in resource-constrained settings(RCS) with few specialists and
limited screening setup. Large scale AI-assisted screening and telehealth has
potential to expand early detection, but practical deployment is challenging in
paper-based workflows and limited documented field experience exist to build
upon. We provide insights on challenges and ways forward in development to
adoption of scalable AI-assisted Telehealth and screening in such settings.
Specifically, we find that iterative, interdisciplinary collaboration through
early prototyping, shadow deployment and continuous feedback is important to
build shared understanding as well as reduce usability hurdles when
transitioning from paper-based to AI-ready workflows. We find public datasets
and AI models highly useful despite poor performance due to domain shift. In
addition, we find the need for automated AI-based image quality check to
capture gradable images for robust screening in high-volume camps.
  Our field learning stress the importance of treating AI development and
workflow digitization as an end-to-end, iterative co-design process. By
documenting these practical challenges and lessons learned, we aim to address
the gap in contextual, actionable field knowledge for building real-world
AI-assisted telehealth and mass-screening programs in RCS.

</details>


### [39] [DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection](https://arxiv.org/abs/2509.15563)
*Min Sun,Fenghui Guo*

Main category: cs.CV

TL;DR: 提出DC-Mamba用于遥感变化检测，通过“先对齐、再增强”的框架在ChangeMamba基础上提升F1与IoU。


<details>
  <summary>Details</summary>
Motivation: 现有RSCD方法，尤其SSM类模型，对几何错配缺乏显式处理，容易将伪变化当真变化，同时难以从噪声中识别细小或微弱的真实变化。

Method: 在ChangeMamba骨干上引入两个轻量可插拔模块：1) 双时相可变形对齐（BTDA），在语义特征层显式引入几何感知以校正空间错位；2) 尺度-稀疏变化放大器（SSCA），利用多源线索选择性放大高置信变化、抑制噪声；整体采用“先对齐、再增强”的协同流程。

Result: 相较强基线ChangeMamba，F1由0.5730提升至0.5903，IoU由0.4015提升至0.4187；边界更清晰，小/细微目标更可见，伪变化减少。

Conclusion: “对齐-增强”策略有效、透明且易部署，能同时解决几何与特征层面的挑战，显著提升RSCD性能。

Abstract: Remote sensing change detection (RSCD) is vital for identifying land-cover
changes, yet existing methods, including state-of-the-art State Space Models
(SSMs), often lack explicit mechanisms to handle geometric misalignments and
struggle to distinguish subtle, true changes from noise.To address this, we
introduce DC-Mamba, an "align-then-enhance" framework built upon the
ChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1)
Bi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric
awareness to correct spatial misalignments at the semantic feature level; and
(2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to
selectively amplify high-confidence change signals while suppressing noise
before the final classification. This synergistic design first establishes
geometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA
to sharpen boundaries and enhance the visibility of small or subtle targets.
Experiments show our method significantly improves performance over the strong
ChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU
from 0.4015 to 0.4187. The results confirm the effectiveness of our
"align-then-enhance" strategy, offering a robust and easily deployable solution
that transparently addresses both geometric and feature-level challenges in
RSCD.

</details>


### [40] [BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent](https://arxiv.org/abs/2509.15566)
*Shaojie Zhang,Ruoceng Zhang,Pei Fu,Shaokang Wang,Jiahui Yang,Xin Du,Shiqi Cui,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 提出“Blink-Think-Link (BTL)”脑启发式人机界面交互框架，将交互分为注视检测（Blink）、高层推理（Think）、指令生成（Link），配合专门的注视数据自动标注与过程+结果双重规则奖励，构建GUI智能体BTL-UI，在静态理解与动态操作基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型与强化微调在GUI自动化上虽有进展，但交互逻辑与人类自然的GUI沟通模式不一致，影响泛化与可靠性。需要一种更贴近人类认知流程的范式来提升注意、推理与动作的协同。

Method: 提出脑启发式三阶段框架：Blink用于快速定位与关注屏幕相关区域；Think用于高层次推理与计划；Link用于生成可执行的精确操作命令。技术上引入两项创新：专为Blink阶段优化的自动化标注流水线（Blink Data Generation），以及结合过程与结果的首个规则化奖励机制（BTL Reward），以支持强化学习训练。并基于该框架实现GUI智能体BTL-UI。

Result: BTL-UI在综合基准上于静态GUI理解与动态交互任务均取得稳定的最先进表现，显示出在不同任务类型与场景下的强鲁棒性与一致性。

Conclusion: 脑启发式的BTL框架有效缩小了AI与人类在GUI交互逻辑上的差距，过程分解与规则奖励促进了训练与泛化；经由BTL-UI的实证结果证明该方法对构建更强大的GUI智能体具有显著效用。

Abstract: In the field of AI-driven human-GUI interaction automation, while rapid
advances in multimodal large language models and reinforcement fine-tuning
techniques have yielded remarkable progress, a fundamental challenge persists:
their interaction logic significantly deviates from natural human-GUI
communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL),
a brain-inspired framework for human-GUI interaction that mimics the human
cognitive process between users and graphical interfaces. The system decomposes
interactions into three biologically plausible phases: (1) Blink - rapid
detection and attention to relevant screen areas, analogous to saccadic eye
movements; (2) Think - higher-level reasoning and decision-making, mirroring
cognitive planning; and (3) Link - generation of executable commands for
precise motor control, emulating human action selection mechanisms.
Additionally, we introduce two key technical innovations for the BTL framework:
(1) Blink Data Generation - an automated annotation pipeline specifically
optimized for blink data, and (2) BTL Reward -- the first rule-based reward
mechanism that enables reinforcement learning driven by both process and
outcome. Building upon this framework, we develop a GUI agent model named
BTL-UI, which demonstrates consistent state-of-the-art performance across both
static GUI understanding and dynamic interaction tasks in comprehensive
benchmarks. These results provide conclusive empirical validation of the
framework's efficacy in developing advanced GUI Agents.

</details>


### [41] [Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach](https://arxiv.org/abs/2509.15573)
*Shilong Bao,Qianqian Xu,Feiran Li,Boyu Han,Zhiyong Yang,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 论文揭示现有显著性目标检测（SOD）评估指标对目标尺寸敏感：大目标主导得分，小目标被忽视；提出尺寸不变评估框架SIEva与与之匹配的优化框架SIOpt，兼容多种模型，理论与实验均验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实场景中常含多尺度显著目标，现有评估协议将得分分解为与区域大小成正比的项，导致大目标错误主导评估、忽略小但语义重要的目标，引发偏置与实际性能退化，亟需尺寸不变的评估与训练机制。

Method: 1) 理论分析：推导现用SOD指标的可分解形式，证明各项贡献与区域大小正相关。2) SIEva：对每个可分组件独立评估后再聚合，降低尺寸不平衡影响。3) SIOpt：遵循尺寸不变原则的通用优化框架，可无缝集成到多种SOD骨干，提升多尺度显著目标检测。4) 提供一般化分析，支持新评估协议的有效性。

Result: 在多数据集与广泛骨干上进行综合实验，SIEva/SIOpt显著提升对不同尺寸显著目标的检测与更公平的评估；代码公开以便复现。

Conclusion: 现有SOD评估存在尺寸偏置；SIEva提供更公平、尺寸不变的评测，SIOpt在训练/推理中强化多尺度目标检测，理论与实验均证实方法的有效性与通用性。

Abstract: This paper investigates a fundamental yet underexplored issue in Salient
Object Detection (SOD): the size-invariant property for evaluation protocols,
particularly in scenarios when multiple salient objects of significantly
different sizes appear within a single image. We first present a novel
perspective to expose the inherent size sensitivity of existing widely used SOD
metrics. Through careful theoretical derivations, we show that the evaluation
outcome of an image under current SOD metrics can be essentially decomposed
into a sum of several separable terms, with the contribution of each term being
directly proportional to its corresponding region size. Consequently, the
prediction errors would be dominated by the larger regions, while smaller yet
potentially more semantically important objects are often overlooked, leading
to biased performance assessments and practical degradation. To address this
challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed.
The core idea is to evaluate each separable component individually and then
aggregate the results, thereby effectively mitigating the impact of size
imbalance across objects. Building upon this, we further develop a dedicated
optimization framework (SIOpt), which adheres to the size-invariant principle
and significantly enhances the detection of salient objects across a broad
range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly
integrated with a wide range of SOD backbones. Theoretically, we also present
generalization analysis of SOD methods and provide evidence supporting the
validity of our new evaluation protocols. Finally, comprehensive experiments
speak to the efficacy of our proposed approach. The code is available at
https://github.com/Ferry-Li/SI-SOD.

</details>


### [42] [Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion](https://arxiv.org/abs/2509.15578)
*Shanghong Li,Chiam Wen Qi Ruth,Hong Xu,Fang Liu*

Main category: cs.CV

TL;DR: 提出HFN异构融合网络，用视频/音频/文本三模态，含动态决策网络与加权融合模块，并发布短视频虚假新闻数据集VESV；在FakeTT与VESV上Macro-F1分别提升2.71%与4.14%。


<details>
  <summary>Details</summary>
Motivation: 短视频平台传播迅速、影响广泛，虚假信息易扩散且多模态动态复杂，现有方法难以在多模态、缺失或噪声条件下稳定检测，需要更鲁棒的多模态融合与权重自适应机制，以及专门数据集。

Method: 构建HFN：融合视频、音频、文本特征；引入Decision Network在推理阶段自适应调整各模态权重；设计Weighted Multi-Modal Feature Fusion模块，实现在模态缺失或不完整时的稳健融合；并构建短视频虚假新闻数据集VESV用于训练与评估。

Result: 在FakeTT与新收集的VESV数据集上，相比SOTA方法，Macro F1分别提升2.71%与4.14%，显示出更好的鲁棒性与检测性能。

Conclusion: HFN在复杂短视频场景中有效识别虚假新闻，动态权重与加权融合提升在不完整多模态数据下的表现；发布的VESV数据集为该任务提供基准，促进更可靠的多模态反虚假研究。

Abstract: The rapid proliferation of short video platforms has necessitated advanced
methods for detecting fake news. This need arises from the widespread influence
and ease of sharing misinformation, which can lead to significant societal
harm. Current methods often struggle with the dynamic and multimodal nature of
short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel
multimodal framework that integrates video, audio, and text data to evaluate
the authenticity of short video content. HFN introduces a Decision Network that
dynamically adjusts modality weights during inference and a Weighted
Multi-Modal Feature Fusion module to ensure robust performance even with
incomplete data. Additionally, we contribute a comprehensive dataset VESV
(VEracity on Short Videos) specifically designed for short video fake news
detection. Experiments conducted on the FakeTT and newly collected VESV
datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over
state-of-the-art methods. This work establishes a robust solution capable of
effectively identifying fake news in the complex landscape of short video
platforms, paving the way for more reliable and comprehensive approaches in
combating misinformation.

</details>


### [43] [EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery](https://arxiv.org/abs/2509.15596)
*Gui Wang,Yang Wennuo,Xusen Ma,Zehao Zhong,Zhuoru Wu,Ende Wu,Rong Qu,Wooi Ping Cheah,Jianfeng Ren,Linlin Shen*

Main category: cs.CV

TL;DR: 提出EyePCR眼科手术视频认知基准与数据集，并基于Qwen2.5-VL-7B做领域适配的EyePCR-MLLM，在感知、理解、推理评测上优于开源模型且接近商用模型，揭示现有MLLM在外科认知上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在高风险、专业化场景（如外科手术）中的认知与可靠性未被系统评估；缺乏以结构化临床知识为基础、覆盖感知—理解—推理全链条的手术视频基准来检验和提升模型的临床适用性。

Method: 构建EyePCR基准：1）大规模、细粒度眼科手术视频VQA（>210k），覆盖1048属性并支持多视角感知；2）基于医学知识图谱的理解评测（>25k三元组）；3）四类临床推理任务。以此进行认知分层评估，并训练/领域适配Qwen2.5-VL-7B得到EyePCR-MLLM。

Result: EyePCR-MLLM在感知MCQ上取得最高准确率，且在理解与推理任务上优于开源模型，表现接近或可与GPT-4.1等商用模型竞争。基准揭示多数MLLM在外科认知任务中的局限。

Conclusion: EyePCR为外科认知提供结构化、可量化的评测框架与数据资源，推动提升手术视频理解的临床可靠性；领域适配能显著增强MLLM在专业场景的认知能力，但现有模型仍存在明显短板，需进一步优化与评测。

Abstract: MLLMs (Multimodal Large Language Models) have showcased remarkable
capabilities, but their performance in high-stakes, domain-specific scenarios
like surgical settings, remains largely under-explored. To address this gap, we
develop \textbf{EyePCR}, a large-scale benchmark for ophthalmic surgery
analysis, grounded in structured clinical knowledge to evaluate cognition
across \textit{Perception}, \textit{Comprehension} and \textit{Reasoning}.
EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover
1048 fine-grained attributes for multi-view perception, medical knowledge graph
of more than 25k triplets for comprehension, and four clinically grounded
reasoning tasks. The rich annotations facilitate in-depth cognitive analysis,
simulating how surgeons perceive visual cues and combine them with domain
knowledge to make decisions, thus greatly improving models' cognitive ability.
In particular, \textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,
achieves the highest accuracy on MCQs for \textit{Perception} among compared
models and outperforms open-source models in \textit{Comprehension} and
\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR reveals
the limitations of existing MLLMs in surgical cognition and lays the foundation
for benchmarking and enhancing clinical reliability of surgical video
understanding models.

</details>


### [44] [TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?](https://arxiv.org/abs/2509.15602)
*Zhongyuan Bao,Lejun Zhang*

Main category: cs.CV

TL;DR: 论文提出TennisTV——首个系统性网球视频理解基准，涵盖8项任务与2500条人工验证问答，用于评估MLLM在高频、短时且信息密集的网球回合理解。对16个代表性模型评测，发现框采样密度需按任务平衡、时间定位能力是提升推理的关键。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在一般视频理解上表现不错，但在网球等高频、快速运动场景中表现欠佳，缺乏标准化评测数据与任务。为系统揭示模型在此领域的弱点并指导改进，需要一个覆盖细粒度时间序列事件（击球）的专业基准。

Method: 构建TennisTV基准：将每个网球回合建模为按时间排序的连续击球事件序列；采用自动化管线进行数据过滤与问题生成；设计覆盖回合级与击球级的8类任务；并由人工验证2500个问题。用该基准评估16个代表性MLLM，分析表现与误差。

Result: 首次系统评估显示MLLM在网球视频理解上存在显著短板。实验得出两条关键观察：不同任务需匹配与平衡的帧采样密度；提升时间定位（temporal grounding）能力对增强推理至关重要。

Conclusion: TennisTV为高频体育视频理解提供全面评测框架，揭示MLLM在时间敏感推理与采样策略上的不足，指出改进方向：任务自适应的帧采样与更强的时间定位机制。

Abstract: Multimodal large language models (MLLMs) excel at general video understanding
but struggle with fast, high-frequency sports like tennis, where rally clips
are short yet information-dense. To systematically evaluate MLLMs in this
challenging domain, we present TennisTV, the first and most comprehensive
benchmark for tennis video understanding. TennisTV models each rally as a
temporal-ordered sequence of consecutive stroke events, using automated
pipelines for filtering and question generation. It covers 8 tasks at rally and
stroke levels and includes 2,500 human-verified questions. Evaluating 16
representative MLLMs, we provide the first systematic assessment of tennis
video understanding. Results reveal substantial shortcomings and yield two key
insights: (i) frame-sampling density should be tailored and balanced across
tasks, and (ii) improving temporal grounding is essential for stronger
reasoning.

</details>


### [45] [Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation](https://arxiv.org/abs/2509.15608)
*Zheng Wang,Hong Liu,Zheng Wang,Danyi Li,Min Cen,Baptiste Magnier,Li Liang,Liansheng Wang*

Main category: cs.CV

TL;DR: 提出Rasa框架：用LLM从病理报告提炼与WSI相关细粒度文本，作为教师知识指导学生模型的自蒸馏以去噪特征，并在训练中加入风险感知mix-up以扩充数据与多样性；在CRC与TCGA-BRCA上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: WSI用于生存分析能提供微观信息，但传统方法受限于噪声特征与数据稀缺，难以有效捕捉关键预后信号。病理报告蕴含患者特异信息，尚未充分利用；作者希望把报告文本转化为有用的监督或先验，提升WSI生存预测的稳健性与性能。

Method: 1) 设计任务提示，利用先进LLM从原始、噪声较多的病理报告中抽取与WSI相关的细粒度描述（文本知识）。2) 构建教师-学生自蒸馏：教师模型携带文本知识，引导学生模型过滤不相关或冗余的WSI特征，实现去噪与知识迁移。3) 在学生训练中引入风险感知mix-up，根据生存风险信息进行样本混合，增强数据量与分布多样性。

Result: 在自采集CRC数据与公开TCGA-BRCA数据上进行大量实验，Rasa在生存分析指标上显著优于现有SOTA方法，显示出更强的泛化与稳健性。

Conclusion: 将LLM提取的报告文本作为教师知识，结合自蒸馏与风险感知mix-up，可有效提升WSI生存分析的性能；该多模态、去噪与数据增强一体化框架具有实际应用潜力，并在多个数据集上验证了优越性。

Abstract: Survival analysis based on Whole Slide Images (WSIs) is crucial for
evaluating cancer prognosis, as they offer detailed microscopic information
essential for predicting patient outcomes. However, traditional WSI-based
survival analysis usually faces noisy features and limited data accessibility,
hindering their ability to capture critical prognostic features effectively.
Although pathology reports provide rich patient-specific information that could
assist analysis, their potential to enhance WSI-based survival analysis remains
largely unexplored. To this end, this paper proposes a novel Report-auxiliary
self-distillation (Rasa) framework for WSI-based survival analysis. First,
advanced large language models (LLMs) are utilized to extract fine-grained,
WSI-relevant textual descriptions from original noisy pathology reports via a
carefully designed task prompt. Next, a self-distillation-based pipeline is
designed to filter out irrelevant or redundant WSI features for the student
model under the guidance of the teacher model's textual knowledge. Finally, a
risk-aware mix-up strategy is incorporated during the training of the student
model to enhance both the quantity and diversity of the training data.
Extensive experiments carried out on our collected data (CRC) and public data
(TCGA-BRCA) demonstrate the superior effectiveness of Rasa against
state-of-the-art methods. Our code is available at
https://github.com/zhengwang9/Rasa.

</details>


### [46] [PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy Correspondence Learning](https://arxiv.org/abs/2509.15623)
*Zhuoyao Liu,Yang Liu,Wentao Feng,Shudong Huang*

Main category: cs.CV

TL;DR: 提出PCSR框架，通过伪标签一致性划分并自适应优化噪声样本，提升跨模态检索在噪声监督下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实数据中的图文对存在错配（Noisy Correspondences），直接假设完美对齐会导致相似度学习被误导、检索性能下降。既有方法粗粒度地把样本分为“干净/噪声”，忽视噪声内部的多样性，并对不同样本一刀切训练，无法充分利用样本特征以优化模型。

Method: PCSR框架：1）置信度估计先区分干净与噪声对；2）对噪声对基于伪标签一致性进一步细分，提出伪标签一致性分数（PCS）衡量预测稳定性，将其划分为“模糊”与“可精炼”子集；3）自适应配对优化（APO）：对模糊样本用鲁棒损失优化，对可精炼样本在训练中进行文本替换以提升对应性。

Result: 在CC152K、MS-COCO、Flickr30K上进行大量实验，证明在噪声监督场景下该方法显著提升跨模态检索的鲁棒性与性能。

Conclusion: 通过显式利用伪标签一致性对样本进行细粒度分层，并采用针对性优化策略，PCSR有效缓解错配噪声对跨模态检索的负面影响，优于依赖粗粒度划分与统一训练的既有方法。

Abstract: Cross-modal retrieval aims to align different modalities via semantic
similarity. However, existing methods often assume that image-text pairs are
perfectly aligned, overlooking Noisy Correspondences in real data. These
misaligned pairs misguide similarity learning and degrade retrieval
performance. Previous methods often rely on coarse-grained categorizations that
simply divide data into clean and noisy samples, overlooking the intrinsic
diversity within noisy instances. Moreover, they typically apply uniform
training strategies regardless of sample characteristics, resulting in
suboptimal sample utilization for model optimization. To address the above
challenges, we introduce a novel framework, called Pseudo-label
Consistency-Guided Sample Refinement (PCSR), which enhances correspondence
reliability by explicitly dividing samples based on pseudo-label consistency.
Specifically, we first employ a confidence-based estimation to distinguish
clean and noisy pairs, then refine the noisy pairs via pseudo-label consistency
to uncover structurally distinct subsets. We further proposed a Pseudo-label
Consistency Score (PCS) to quantify prediction stability, enabling the
separation of ambiguous and refinable samples within noisy pairs. Accordingly,
we adopt Adaptive Pair Optimization (APO), where ambiguous samples are
optimized with robust loss functions and refinable ones are enhanced via text
replacement during training. Extensive experiments on CC152K, MS-COCO and
Flickr30K validate the effectiveness of our method in improving retrieval
robustness under noisy supervision.

</details>


### [47] [pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation](https://arxiv.org/abs/2509.15638)
*Tong Wang,Xingyue Zhao,Linghao Zhuang,Haoyu Zhao,Jiayi Yin,Yuyang He,Gang Yu,Bo Lin*

Main category: cs.CV

TL;DR: 提出首个面向异构医疗影像数据的个性化联邦SAM框架，通过保留本地L-MoE专家以保域特征、仅聚合全局参数，以及教师-学生的知识蒸馏式全局-本地解耦微调，提升分割性能、跨域鲁棒性并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 医疗影像分割需要大量跨机构数据，但隐私限制阻碍共享；传统联邦方法多基于轻量模型，难以应对复杂异质数据。SAM分割能力强但编码器庞大，直接用于联邦学习面临参数同步与异质性适配的挑战，亟需既能利用SAM能力又能个性化适配各域的联邦方案。

Method: 构建个性化联邦SAM：1) 个性化聚合策略——仅聚合能表征跨客户端共性的全局参数，保留本地设计的L-MoE组件以学习域特有特征；2) 解耦的全局-本地微调——通过教师-学生知识蒸馏，将全球共享模型的知识迁移到各客户端的个性化本地模型，缩小二者差距并缓解过度泛化。

Result: 在两个公开数据集上进行大量实验，显示该方法显著提升分割性能，具备稳健的跨域适应能力，同时减少通信开销。

Conclusion: 个性化联邦SAM在医疗影像异构场景中有效结合全球共享与本地专长，利用L-MoE与知识蒸馏实现高效、稳健的分割与适配，证明在保证隐私的前提下可达更优性能与更低通信成本。

Abstract: Medical image segmentation is crucial for computer-aided diagnosis, yet
privacy constraints hinder data sharing across institutions. Federated learning
addresses this limitation, but existing approaches often rely on lightweight
architectures that struggle with complex, heterogeneous data. Recently, the
Segment Anything Model (SAM) has shown outstanding segmentation capabilities;
however, its massive encoder poses significant challenges in federated
settings. In this work, we present the first personalized federated SAM
framework tailored for heterogeneous data scenarios in medical image
segmentation. Our framework integrates two key innovations: (1) a personalized
strategy that aggregates only the global parameters to capture cross-client
commonalities while retaining the designed L-MoE (Localized Mixture-of-Experts)
component to preserve domain-specific features; and (2) a decoupled
global-local fine-tuning mechanism that leverages a teacher-student paradigm
via knowledge distillation to bridge the gap between the global shared model
and the personalized local models, thereby mitigating overgeneralization.
Extensive experiments on two public datasets validate that our approach
significantly improves segmentation performance, achieves robust cross-domain
adaptation, and reduces communication overhead.

</details>


### [48] [UNIV: Unified Foundation Model for Infrared and Visible Modalities](https://arxiv.org/abs/2509.15642)
*Fangyuan Mao,Shuo Wang,Jilin Mei,Chen Min,Shun Lu,Fuyang Liu,Yu Hu*

Main category: cs.CV

TL;DR: 提出UNIV统一多模态（可见光RGB+红外）基础模型，通过跨模态对齐与知识保留，实现在红外任务显著提升，同时几乎不损害RGB性能，并发布大规模对齐数据集MVIP。


<details>
  <summary>Details</summary>
Motivation: 现有分别针对RGB或红外的预训练模型在各自单模态表现强，但在多模态融合场景（如自动驾驶、复杂天气）下效果欠佳，需要一种既能跨模态对齐又能保留各自能力的统一模型。

Method: 1) Patch-wise Cross-modality Contrastive Learning（PCCL）：基于注意力的蒸馏，对齐可见光与红外的局部补丁特征，模拟视网膜水平细胞的侧抑制，兼容任意Transformer架构。2) 双重知识保留：结合LoRA适配器（仅约2%额外参数）与同步蒸馏，仿照视网膜双极细胞的信号分路，避免灾难性遗忘，同时维持光照充足（photopic）与弱光（scotopic）两种功能。3) 构建MVIP数据集：98,992对精确对齐的可见-红外图像，覆盖多场景，用于跨模态训练与评测。

Result: UNIV在红外任务上显著提升：语义分割mIoU提高+1.7，目标检测mAP提高+0.7；在RGB任务上保持≥99%的基线性能。方法参数增量小（LoRA约2%），训练稳定且与Transformer兼容。

Conclusion: 通过生物学启发的跨模态对齐与双重知识保留，UNIV实现了红外与可见光的统一感知，在不显著牺牲RGB性能的前提下提升红外表现，并提供大规模标准数据集支撑后续研究，适用于多传感器场景如自动驾驶与恶劣天气感知。

Abstract: The demand for joint RGB-visible and infrared perception is growing rapidly,
particularly to achieve robust performance under diverse weather conditions.
Although pre-trained models for RGB-visible and infrared data excel in their
respective domains, they often underperform in multimodal scenarios, such as
autonomous vehicles equipped with both sensors. To address this challenge, we
propose a biologically inspired UNified foundation model for Infrared and
Visible modalities (UNIV), featuring two key innovations. First, we introduce
Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided
distillation framework that mimics retinal horizontal cells' lateral
inhibition, which enables effective cross-modal feature alignment while
remaining compatible with any transformer-based architecture. Second, our
dual-knowledge preservation mechanism emulates the retina's bipolar cell signal
routing - combining LoRA adapters (2% added parameters) with synchronous
distillation to prevent catastrophic forgetting, thereby replicating the
retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To
support cross-modal learning, we introduce the MVIP dataset, the most
comprehensive visible-infrared benchmark to date. It contains 98,992 precisely
aligned image pairs spanning diverse scenarios. Extensive experiments
demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in
semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+
of the baseline performance on visible RGB tasks. Our code is available at
https://github.com/fangyuanmao/UNIV.

</details>


### [49] [GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading](https://arxiv.org/abs/2509.15645)
*Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon*

Main category: cs.CV

TL;DR: 提出GS-Scale，一种将3D高斯点渲染训练的大量参数与优化器状态放到主机内存、按需把子集传到GPU的系统，实现3.3–5.6倍GPU显存节省、训练速度接近纯GPU，并在消费级GPU上把可训练高斯数量从400万扩展到1800万，LPIPS提升23–35%。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting训练大场景需要存储大量参数、梯度与优化器状态，导致GPU显存迅速耗尽，限制了高质量与大规模训练的可行性，需要一种在不显著牺牲速度的前提下显著降低GPU显存占用的训练系统。

Method: 将所有高斯存储在CPU内存，仅在每次前向/反向传播按需把相关子集传至GPU；为缓解CPU端裁剪与优化器更新的瓶颈，提出三项系统优化：1）选择性卸载几何参数至CPU以加速视锥体裁剪；2）参数前推，将CPU优化器更新与GPU计算流水并行；3）延迟优化器更新，对零梯度的高斯跳过不必要的内存访问。

Result: 在大规模数据集上，GPU显存需求降低3.3–5.6倍，训练速度接近不卸载的纯GPU方案；在RTX 4070 Mobile上可将高斯数量从400万扩展到1800万，并在渲染质量指标LPIPS上提升23–35%。

Conclusion: GS-Scale通过主机内存卸载与CPU/GPU协同流水化，显著缓解显存瓶颈，使消费级GPU能训练更大规模、更高质量的3D高斯点场景，且基本不牺牲训练速度。

Abstract: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by
delivering high visual quality and fast rendering speeds. However, training
large-scale scenes at high quality remains challenging due to the substantial
memory demands required to store parameters, gradients, and optimizer states,
which can quickly overwhelm GPU memory. To address these limitations, we
propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian
Splatting. GS-Scale stores all Gaussians in host memory, transferring only a
subset to the GPU on demand for each forward and backward pass. While this
dramatically reduces GPU memory usage, it requires frustum culling and
optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's
limited compute and memory bandwidth. To mitigate this, GS-Scale employs three
system-level optimizations: (1) selective offloading of geometric parameters
for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer
updates with GPU computation, and (3) deferred optimizer update to minimize
unnecessary memory accesses for Gaussians with zero gradients. Our extensive
evaluations on large-scale datasets demonstrate that GS-Scale significantly
lowers GPU memory demands by 3.3-5.6x, while achieving training speeds
comparable to GPU without host offloading. This enables large-scale 3D Gaussian
Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the
number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU,
leading to 23-35% LPIPS (learned perceptual image patch similarity)
improvement.

</details>


### [50] [FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.15648)
*Yuwei Jia,Yutang Lu,Zhe Cui,Fei Su*

Main category: cs.CV

TL;DR: 提出使用3D Gaussian Splatting实现无接触指纹的3D配准、重建与生成，以少量2D图像且无需相机参数即可得到高质量3D指纹并提升识别表现。


<details>
  <summary>Details</summary>
Motivation: 无接触指纹识别受限于数据稀缺（姿态变化不足）和未充分利用隐式3D表示，导致性能落后于接触式方法。作者希望通过高效的3D重建与生成管线，扩充数据与提升匹配精度。

Method: 将3D Gaussian Splatting引入指纹领域，设计一体化框架：在无需相机内外参的情况下，从稀疏2D无接触指纹图像进行3D配准与完整重建；基于重建的3D模型再生成高质量的无接触指纹图像，以增强识别。

Result: 在3D配准、重建与生成任务上取得准确对齐与完整重建效果；能够从2D图像重建出高质量3D指纹并顺序生成逼真的无接触指纹图像，从而提升无接触指纹识别性能。

Conclusion: 首次将3D Gaussian Splatting用于指纹识别并实现无相机参数、稀疏图像条件下的有效3D配准与重建；生成的高质量指纹有助于改进无接触识别并为该领域提供新的范式。

Abstract: Researchers have conducted many pioneer researches on contactless
fingerprints, yet the performance of contactless fingerprint recognition still
lags behind contact-based methods primary due to the insufficient contactless
fingerprint data with pose variations and lack of the usage of implicit 3D
fingerprint representations. In this paper, we introduce a novel contactless
fingerprint 3D registration, reconstruction and generation framework by
integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for
contactless fingerprint recognition that integrates 3D fingerprint
reconstruction and generation. To our knowledge, this is the first work to
apply 3D Gaussian Splatting to the field of fingerprint recognition, and the
first to achieve effective 3D registration and complete reconstruction of
contactless fingerprints with sparse input images and without requiring camera
parameters information. Experiments on 3D fingerprint registration,
reconstruction, and generation prove that our method can accurately align and
reconstruct 3D fingerprints from 2D images, and sequentially generates
high-quality contactless fingerprints from 3D model, thus increasing the
performances for contactless fingerprint recognition.

</details>


### [51] [A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds](https://arxiv.org/abs/2509.15675)
*Hao Liu*

Main category: cs.CV

TL;DR: 提出一种基于PCA的模型，从不完整点云重建曲面；用PCA估计法向并作为正则项引导缺失区域重建，配合算子分裂求解，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真实扫描产生的点云常因吸光、遮挡等导致表面覆盖不全，数据缺失使曲面重建困难。需要能在缺失区域推断结构并稳定重建的模型。

Method: 1) 对已有点云进行局部PCA，估计底层曲面的法向信息；2) 将估计的法向作为正则化引导重建，特别针对缺失区域的形状推断；3) 采用算子分裂（如ADMM/分裂Bregman风格）数值算法高效求解模型。

Result: 系统实验显示，该方法能在数据缺失区域合理补全结构并重建曲面；定量与定性评价均优于现有方法。

Conclusion: 基于PCA的法向估计与正则化结合算子分裂求解，可在不完整点云上实现鲁棒的曲面重建，对多学科应用具有优势并优于现有技术。

Abstract: Point cloud data represents a crucial category of information for
mathematical modeling, and surface reconstruction from such data is an
important task across various disciplines. However, during the scanning
process, the collected point cloud data may fail to cover the entire surface
due to factors such as high light-absorption rate and occlusions, resulting in
incomplete datasets. Inferring surface structures in data-missing regions and
successfully reconstructing the surface poses a challenge. In this paper, we
present a Principal Component Analysis (PCA) based model for surface
reconstruction from incomplete point cloud data. Initially, we employ PCA to
estimate the normal information of the underlying surface from the available
point cloud data. This estimated normal information serves as a regularizer in
our model, guiding the reconstruction of the surface, particularly in areas
with missing data. Additionally, we introduce an operator-splitting method to
effectively solve the proposed model. Through systematic experimentation, we
demonstrate that our model successfully infers surface structures in
data-missing regions and well reconstructs the underlying surfaces,
outperforming existing methodologies.

</details>


### [52] [Camera Splatting for Continuous View Optimization](https://arxiv.org/abs/2509.15677)
*Gahye Lee,Hyomin Kim,Gwangjin Ju,Jooeun Son,Hyejeong Yoon,Seungyong Lee*

Main category: cs.CV

TL;DR: 提出“Camera Splatting”视角优化框架：把每个真实摄像机建模为3D高斯（camera splat），在物体表面附近放置虚拟“点摄像机”观察这些分布，通过可微优化使从点摄像机看到的分布匹配目标，从而得到更优视角。相较FVS，能更好捕捉强金属反射与复杂纹理（如文字）。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成在选择/优化训练视角上不足，特别是复杂视差与强视角依赖（反射、文字细节）场景中，传统如FVS不能有效覆盖关键观察方向，影响渲染质量。需要一种可微、连续地优化视角分布的方法，以提升对视角相关现象的表达。

Method: - 将每个摄像机表示为3D高斯分布（camera splat）。
- 在物体表面附近采样3D点，放置虚拟“点摄像机”作为观察者，统计/渲染看到的camera splats分布。
- 设定期望的目标分布（例如均衡覆盖、与表面可见性相关权重等），通过连续可微的过程优化camera splats（位置、方向、协方差等）使点摄像机观察到的分布接近目标。
- 与3D Gaussian Splatting相似的优化与渲染机制用于反向传播和梯度更新。

Result: 优化后的视角集合在新视角合成中优于FVS，对复杂视角依赖效应（强金属反射）与高频细节（文字纹理）有更好的捕捉与重建质量。

Conclusion: Camera Splatting把视角优化转化为对“摄像机高斯”分布的可微匹配，通过点摄像机监督实现连续优化，相比FVS在复杂材料与细节上表现更佳，适合作为新视角合成的视角选择/优化方案。

Abstract: We propose Camera Splatting, a novel view optimization framework for novel
view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a
camera splat, and virtual cameras, termed point cameras, are placed at 3D
points sampled near the surface to observe the distribution of camera splats.
View optimization is achieved by continuously and differentiably refining the
camera splats so that desirable target distributions are observed from the
point cameras, in a manner similar to the original 3D Gaussian splatting.
Compared to the Farthest View Sampling (FVS) approach, our optimized views
demonstrate superior performance in capturing complex view-dependent phenomena,
including intense metallic reflections and intricate textures such as text.

</details>


### [53] [Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model](https://arxiv.org/abs/2509.15678)
*Sidra Hanif,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 提出一种条件扩散模型生成手写笔画，结合多尺度注意风格特征与词布局（词间距），在样式仿写与笔画生成上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手写笔画/风格仿写方法忽视了显式的词间距（布局）特征，导致风格中词距不一致；同时多数方法直接生成图像，缺乏笔画的时序坐标信息，限制了下游任务（识别、书写顺序恢复）。

Method: 1）设计多尺度注意特征编码，同时捕捉局部与全局书写风格。2）将词布局作为显式条件，建模词间距。3）提出条件扩散模型直接生成笔画序列（含时序坐标），以风格特征与词布局共同引导扩散过程。

Result: 实验表明，该条件扩散笔画生成模型在笔画生成任务上超过当前SOTA；在图像生成指标上也与最新图像生成网络具有竞争力。

Conclusion: 显式建模词布局并使用多尺度风格特征引导的条件扩散笔画生成，可更好地仿写书写风格、保持一致的词间距，并为下游任务提供更丰富的时序信息，整体性能优于现有方法。

Abstract: Handwriting stroke generation is crucial for improving the performance of
tasks such as handwriting recognition and writers order recovery. In
handwriting stroke generation, it is significantly important to imitate the
sample calligraphic style. The previous studies have suggested utilizing the
calligraphic features of the handwriting. However, they had not considered word
spacing (word layout) as an explicit handwriting feature, which results in
inconsistent word spacing for style imitation. Firstly, this work proposes
multi-scale attention features for calligraphic style imitation. These
multi-scale feature embeddings highlight the local and global style features.
Secondly, we propose to include the words layout, which facilitates word
spacing for handwriting stroke generation. Moreover, we propose a conditional
diffusion model to predict strokes in contrast to previous work, which directly
generated style images. Stroke generation provides additional temporal
coordinate information, which is lacking in image generation. Hence, our
proposed conditional diffusion model for stroke generation is guided by
calligraphic style and word layout for better handwriting imitation and stroke
generation in a calligraphic style. Our experimentation shows that the proposed
diffusion model outperforms the current state-of-the-art stroke generation and
is competitive with recent image generation networks.

</details>


### [54] [Saccadic Vision for Fine-Grained Visual Classification](https://arxiv.org/abs/2509.15688)
*Johann Schmidt,Sebastian Stober,Joachim Denzler,Paul Bodesheim*

Main category: cs.CV

TL;DR: 提出受人类扫视视觉启发的两阶段FGVC方法：先提取周边（粗视）特征生成采样图，再并行编码若干“凝视”补丁并通过选择性注意力融合，同时用非极大值抑制避免空间冗余；在多个标准与昆虫数据集上达到SOTA相当表现并优于基线。


<details>
  <summary>Details</summary>
Motivation: 细粒度分类需要依靠微小局部差异，但类内变异大、类间差异小；现有基于部件的方法依赖复杂的定位网络，像素到样本空间映射难、下游可用性弱，且采样点空间冗余高、部件数量难以确定。

Method: 受人类扫视机制启发的两阶段框架：1) 外周（粗视）特征提取并生成“样本图”；2) 在样本图上进行凝视补丁的采样与并行编码（共享权重编码器）。引入情境化选择性注意力评估每个凝视补丁的重要性后与外周表示融合；在采样阶段使用非极大值抑制以减少冗余、防止空间塌缩。

Result: 在CUB-200-2011、NABirds、Food-101、Stanford-Dogs及EU-Moths、Ecuador-Moths、AMI-Moths上，性能与SOTA相当且稳定优于基线编码器。

Conclusion: 该扫视式两阶段与选择性注意力并融合NMS的框架可有效缓解部件方法的空间塌缩与冗余问题，提升FGVC性能且具可泛化性。

Abstract: Fine-grained visual classification (FGVC) requires distinguishing between
visually similar categories through subtle, localized features - a task that
remains challenging due to high intra-class variability and limited inter-class
differences. Existing part-based methods often rely on complex localization
networks that learn mappings from pixel to sample space, requiring a deep
understanding of image content while limiting feature utility for downstream
tasks. In addition, sampled points frequently suffer from high spatial
redundancy, making it difficult to quantify the optimal number of required
parts. Inspired by human saccadic vision, we propose a two-stage process that
first extracts peripheral features (coarse view) and generates a sample map,
from which fixation patches are sampled and encoded in parallel using a
weight-shared encoder. We employ contextualized selective attention to weigh
the impact of each fixation patch before fusing peripheral and focus
representations. To prevent spatial collapse - a common issue in part-based
methods - we utilize non-maximum suppression during fixation sampling to
eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks
(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect
datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method
achieves comparable performance to state-of-the-art approaches while
consistently outperforming our baseline encoder.

</details>


### [55] [SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions](https://arxiv.org/abs/2509.15693)
*Cristian Sbrolli,Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneForge通过将多个3D物体组合成具有明确空间关系的场景，并配以LLM生成的多物体文本描述，增强3D点云与文本的对比学习，在数据稀缺下显著提升零样本分类、检索、VQA与少样本分割等任务表现。


<details>
  <summary>Details</summary>
Motivation: 3D-文本对比学习受限于大规模配对数据匮乏与单物体样本的语义/结构局限，难以学到复杂关系与场景级理解。作者希望通过可控的场景级合成与多对象描述，提升数据复杂度与关系信息，从而更好地对齐3D与语言。

Method: 提出SceneForge：用单体3D形状自动构建多物体场景，显式建模空间关系（如相对位置、布局），并用大语言模型生成并润色与场景一致的多对象文本描述。将这些结构化的组合样本与原始数据一起用于对比学习，并系统评估关键设计：每场景物体数、组合样本比例、场景构建策略。方法对编码器架构模型无关。

Result: 广泛实验表明，相比基线，对零样本分类（ModelNet、ScanObjNN、Objaverse-LVIS、ScanNet）和少样本部件分割（ShapeNetPart）均取得显著增益；提升ScanQA上的3D视觉问答；在复杂度提升的检索场景中具备鲁棒泛化；能根据文本指令调整空间配置，展现空间推理能力。

Conclusion: 结构化多对象场景的组合增强是有效的途径，可在数据稀缺下加强3D-文本对齐，提升跨任务与多架构的性能与泛化，并赋予模型更强的场景级理解与空间推理能力。

Abstract: The whole is greater than the sum of its parts-even in 3D-text contrastive
learning. We introduce SceneForge, a novel framework that enhances contrastive
alignment between 3D point clouds and text through structured multi-object
scene compositions. SceneForge leverages individual 3D shapes to construct
multi-object scenes with explicit spatial relations, pairing them with coherent
multi-object descriptions refined by a large language model. By augmenting
contrastive training with these structured, compositional samples, SceneForge
effectively addresses the scarcity of large-scale 3D-text datasets,
significantly enriching data complexity and diversity. We systematically
investigate critical design elements, such as the optimal number of objects per
scene, the proportion of compositional samples in training batches, and scene
construction strategies. Extensive experiments demonstrate that SceneForge
delivers substantial performance gains across multiple tasks, including
zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,
as well as few-shot part segmentation on ShapeNetPart. SceneForge's
compositional augmentations are model-agnostic, consistently improving
performance across multiple encoder architectures. Moreover, SceneForge
improves 3D visual question answering on ScanQA, generalizes robustly to
retrieval scenarios with increasing scene complexity, and showcases spatial
reasoning capabilities by adapting spatial configurations to align precisely
with textual instructions.

</details>


### [56] [ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models](https://arxiv.org/abs/2509.15695)
*Zhaoyang Li,Zhan Ling,Yuchen Zhou,Hao Su*

Main category: cs.CV

TL;DR: 提出ORIC基准，系统评估LVLM在不协调语境下的物体识别；用LLM与CLIP引导采样构造“在场但不合语境”和“看似合理却不存在”的情境；对18个LVLM和2个开放词汇检测模型评测，发现显著识别缺陷（误识与幻觉）。


<details>
  <summary>Details</summary>
Motivation: LVLM在常规场景表现强，但在物体与场景语境不匹配时易出错（把不合语境的物体认错、或幻觉不存在的物体）。缺乏系统评估与数据来量化这一薄弱点，因此需要专门的基准检验语境敏感的识别能力。

Method: 构建ORIC基准：1) 通过LLM引导采样，选出语境上“不合拍但真实存在”的物体；2) 通过CLIP引导采样，发现“语境上似乎合理但实际不存在”的物体，诱发模型幻觉。用这些样本评测多种LVLM与开放词汇检测模型的表现。

Result: 对18个LVLM与2个开放词汇检测模型的评估显示，在语境不协调场景下存在显著识别差距，包括物体误识与幻觉频繁发生，性能明显下降。

Conclusion: ORIC揭示了LVLM在语境不协调条件下的系统性不足，提供了分析框架与数据以推动更具语境意识的物体识别研究。

Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image
caption, visual question answering, and robotics by integrating visual and
textual information. However, they remain prone to errors in incongruous
contexts, where objects appear unexpectedly or are absent when contextually
expected. This leads to two key recognition failures: object misidentification
and hallucination. To systematically examine this issue, we introduce the
Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark
that evaluates LVLMs in scenarios where object-context relationships deviate
from expectations. ORIC employs two key strategies: (1) LLM-guided sampling,
which identifies objects that are present but contextually incongruous, and (2)
CLIP-guided sampling, which detects plausible yet nonexistent objects that are
likely to be hallucinated, thereby creating an incongruous context. Evaluating
18 LVLMs and two open-vocabulary detection models, our results reveal
significant recognition gaps, underscoring the challenges posed by contextual
incongruity. This work provides critical insights into LVLMs' limitations and
encourages further research on context-aware object recognition.

</details>


### [57] [Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance](https://arxiv.org/abs/2509.15704)
*Yuxuan Liang,Xu Li,Xiaolei Chen,Yi Zheng,Haotian Chen,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 提出一种针对大型视觉语言模型处理高分辨率图像的训练免调的金字塔式Token剪枝（PTP），结合自底向上的视觉显著性与自顶向下的指令引导重要性，在保持性能的同时显著降低计算与时延。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像需要被切分为多子图以适配LVLM，导致视觉token数量暴涨、推理计算呈指数级增长与延迟变大；需要一种高效、无需再训练的方法，减少冗余token，同时保留任务关键信息。

Method: 提出Pyramid Token Pruning：1）区域级与token级的自底向上显著性评估，优先保留视觉上更重要的区域与token；2）结合自顶向下、由文本指令/任务需求引导的相关性评估，进一步筛选与当前任务最相关的token；3）训练免调，在推理阶段对高分辨率输入进行分层剪枝以减少视觉token。

Result: 在13个多样化基准上验证，PTP显著降低计算开销与推理延迟，同时仅带来极小的性能损失。

Conclusion: PTP可在无需额外训练的情况下，有效缓解LVLM处理高分辨率图像的计算瓶颈，通过显著性与指令引导的结合实现高效token保留，兼顾效率与性能。

Abstract: Large Vision-Language Models (LVLMs) have significantly advanced multimodal
understanding but still struggle with efficiently processing high-resolution
images. Recent approaches partition high-resolution images into multiple
sub-images, dramatically increasing the number of visual tokens and causing
exponential computational overhead during inference. To address these
limitations, we propose a training-free token pruning strategy, Pyramid Token
Pruning (PTP), that integrates bottom-up visual saliency at both region and
token levels with top-down instruction-guided importance. Inspired by human
visual attention mechanisms, PTP selectively retains more tokens from visually
salient regions and further leverages textual instructions to pinpoint tokens
most relevant to specific multimodal tasks. Extensive experiments across 13
diverse benchmarks demonstrate that our method substantially reduces
computational overhead and inference latency with minimal performance loss.

</details>


### [58] [SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark](https://arxiv.org/abs/2509.15706)
*Chi Yang,Fu Wang,Xiaofei Yang,Hao Huang,Weijia Cao,Xiaowen Chu*

Main category: cs.CV

TL;DR: 提出一个基准数据集与基线框架，用地球静止卫星的VIS/TIR影像与CALIOP/CPR的垂直相位资料配对，训练模型从多模态影像重建三维云相位；SGMAGNet在复杂多层与边界区域显著优于UNet/SegNet等。


<details>
  <summary>Details</summary>
Motivation: 云相位剖面直接影响辐射传输与降水，是数值天气预报中云微物理参数化的关键但难获取变量。需要将高时空分辨率的静止卫星影像与精确的主动卫星垂直探测融合，构建可操作的云相位剖面反演以提升NWP。

Method: 构建同步的影像—剖面对（VIS/TIR与CALIOP/CPR），设定监督学习任务：输入影像块预测对应三维云相位结构。采用SGMAGNet为主模型，比较UNet变体与SegNet等多尺度架构；用Precision、Recall、F1、IoU评估。

Result: SGMAGNet在复杂多层云与边界过渡区重建效果最好；量化指标：Precision 0.922、Recall 0.858、F1 0.763、IoU 0.617，全面优于所有基线。

Conclusion: 多模态融合与SGMAGNet能有效从静止卫星影像重建三维云相位，为云相位剖面的运营化反演与NWP系统整合提供基础，有望改进云微物理参数化与预报性能。

Abstract: Cloud phase profiles are critical for numerical weather prediction (NWP), as
they directly affect radiative transfer and precipitation processes. In this
study, we present a benchmark dataset and a baseline framework for transforming
multimodal satellite observations into detailed 3D cloud phase structures,
aiming toward operational cloud phase profile retrieval and future integration
with NWP systems to improve cloud microphysics parameterization. The multimodal
observations consist of (1) high--spatiotemporal--resolution, multi-band
visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites,
and (2) accurate vertical cloud phase profiles from spaceborne lidar
(CALIOP\slash CALIPSO) and radar (CPR\slash CloudSat). The dataset consists of
synchronized image--profile pairs across diverse cloud regimes, defining a
supervised learning task: given VIS/TIR patches, predict the corresponding 3D
cloud phase structure. We adopt SGMAGNet as the main model and compare it with
several baseline architectures, including UNet variants and SegNet, all
designed to capture multi-scale spatial patterns. Model performance is
evaluated using standard classification metrics, including Precision, Recall,
F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior
performance in cloud phase reconstruction, particularly in complex multi-layer
and boundary transition regions. Quantitatively, SGMAGNet attains a Precision
of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617,
significantly outperforming all baselines across these key metrics.

</details>


### [59] [Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method](https://arxiv.org/abs/2509.15711)
*Shuaibo Li,Zhaohu Xing,Hongqiu Wang,Pengfei Hao,Xingyu Li,Zekai Liu,Lei Zhu*

Main category: cs.CV

TL;DR: 论文提出MedForensics数据集与DSKI检测方法，用于识别AI生成的医疗影像，在多模态上显著优于现有方法与人类专家。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗影像中迅速发展，带来伪造图像风险（误诊、欺诈、错误信息），而现有媒体取证方法和数据集不适用于医疗影像的细微伪迹，缺乏针对性资源与方法。

Method: 构建大规模MedForensics数据集（六种医疗模态、十二个生成模型）。提出DSKI双阶段知识注入检测器：训练阶段采用跨域细痕适配器（CDFA）从空间与噪声域提取伪造线索；测试阶段引入医疗取证检索模块（MFRM）进行少样本检索增强检测。通过视觉-语言特征空间定制化检测AI生成医疗图像。

Result: 在多种医疗模态上，DSKI检测准确率显著超过现有方法与人类专家；在实验中实现跨模态、跨模型的优越性能。

Conclusion: 面向医疗影像取证的专用数据集与双阶段检测框架有效应对AI伪造风险；DSKI与MedForensics为医疗AI安全提供基线与工具，推动该领域发展。

Abstract: The rapid advancement of generative AI in medical imaging has introduced both
significant opportunities and serious challenges, especially the risk that fake
medical images could undermine healthcare systems. These synthetic images pose
serious risks, such as diagnostic deception, financial fraud, and
misinformation. However, research on medical forensics to counter these threats
remains limited, and there is a critical lack of comprehensive datasets
specifically tailored for this field. Additionally, existing media forensic
methods, which are primarily designed for natural or facial images, are
inadequate for capturing the distinct characteristics and subtle artifacts of
AI-generated medical images. To tackle these challenges, we introduce
\textbf{MedForensics}, a large-scale medical forensics dataset encompassing six
medical modalities and twelve state-of-the-art medical generative models. We
also propose \textbf{DSKI}, a novel \textbf{D}ual-\textbf{S}tage
\textbf{K}nowledge \textbf{I}nfusing detector that constructs a vision-language
feature space tailored for the detection of AI-generated medical images. DSKI
comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for
extracting subtle forgery clues from both spatial and noise domains during
training, and 2) a medical forensic retrieval module (MFRM) that boosts
detection accuracy through few-shot retrieval during testing. Experimental
results demonstrate that DSKI significantly outperforms both existing methods
and human experts, achieving superior accuracy across multiple medical
modalities.

</details>


### [60] [TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection](https://arxiv.org/abs/2509.15741)
*Laixin Zhang,Shuaibo Li,Wei Ma,Hongbin Zha*

Main category: cs.CV

TL;DR: 提出TrueMoE：通过双路由的判别专家混合框架，将合成图像检测分解为多个轻量、专门化的子空间协作推理，显著提升对未知生成模式的泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 统一的判别空间在复杂多样的生成伪迹面前易脆弱、难泛化；需要一种能覆盖不同流形结构与感知粒度的多视角检测机制。

Method: 构建判别专家阵列（DEA），沿“流形结构轴”和“感知粒度轴”组织多个轻量专家；引入双路由：粒度感知的稀疏路由将样本分配给少量最相关专家，流形感知的密集路由覆盖更广的结构变化；通过协作推理融合多专家线索以判别真伪。

Result: 在涵盖广泛生成模型的实验中，TrueMoE相较现有方法表现出更好的泛化性与鲁棒性，能有效识别未见过的生成模式。

Conclusion: 多子空间、双路由的MoE式判别框架比单一统一空间更稳健，适合合成图像检测的通用化场景。

Abstract: The rapid progress of generative models has made synthetic image detection an
increasingly critical task. Most existing approaches attempt to construct a
single, universal discriminative space to separate real from fake content.
However, such unified spaces tend to be complex and brittle, often struggling
to generalize to unseen generative patterns. In this work, we propose TrueMoE,
a novel dual-routing Mixture-of-Discriminative-Experts framework that
reformulates the detection task as a collaborative inference across multiple
specialized and lightweight discriminative subspaces. At the core of TrueMoE is
a Discriminative Expert Array (DEA) organized along complementary axes of
manifold structure and perceptual granularity, enabling diverse forgery cues to
be captured across subspaces. A dual-routing mechanism, comprising a
granularity-aware sparse router and a manifold-aware dense router, adaptively
assigns input images to the most relevant experts. Extensive experiments across
a wide spectrum of generative models demonstrate that TrueMoE achieves superior
generalization and robustness.

</details>


### [61] [Hybrid Lie semi-group and cascade structures for the generalized Gaussian derivative model for visual receptive fields](https://arxiv.org/abs/2509.15748)
*Tony Lindeberg*

Main category: cs.CV

TL;DR: 论文探讨在自然图像几何变换下，如何通过协变的空间/时空感受野族，建立不同滤波参数间的可计算关系，以实现更高效、层级化的响应计算与生物视觉模型化。


<details>
  <summary>Details</summary>
Motivation: 真实世界图像在视角、尺度、运动等自然变换下结构多变，早期视觉层的感受野响应随之显著变化；需要一种系统方法保持对这些变换的鲁棒与可比性，从而提升计算效率并贴近生物视觉的组织。

Method: 构建多参数协变感受野族，推导两类关系：(i) 基于半群与李群/李代数思想的“无穷小”参数变动关系；(ii) 宏观级联平滑性质，说明粗尺度响应可由对细尺度响应施加小支持增量滤波得到，且带有方向偏好。

Result: 得到在不同空间与时空尺度、形状参数下的感受野响应间的解析关系与级联计算规则，揭示了参数变化如何系统地影响响应，并给出可用于分层、增量式计算的结构化框架。

Conclusion: 这些关系加深了对多参数感受野响应的理解，可用于：设计高效的感受野族计算方案；并为生物视觉中简单细胞的理想化计算模型提供理论基础。

Abstract: Because of the variabilities of real-world image structures under the natural
image transformations that arise when observing similar objects or
spatio-temporal events under different viewing conditions, the receptive field
responses computed in the earliest layers of the visual hierarchy may be
strongly influenced by such geometric image transformations. One way of
handling this variability is by basing the vision system on covariant receptive
field families, which expand the receptive field shapes over the degrees of
freedom in the image transformations.
  This paper addresses the problem of deriving relationships between spatial
and spatio-temporal receptive field responses obtained for different values of
the shape parameters in the resulting multi-parameter families of receptive
fields. For this purpose, we derive both (i) infinitesimal relationships,
roughly corresponding to a combination of notions from semi-groups and Lie
groups, as well as (ii) macroscopic cascade smoothing properties, which
describe how receptive field responses at coarser spatial and temporal scales
can be computed by applying smaller support incremental filters to the output
from corresponding receptive fields at finer spatial and temporal scales,
structurally related to the notion of Lie algebras, although with directional
preferences.
  The presented results provide (i) a deeper understanding of the relationships
between spatial and spatio-temporal receptive field responses for different
values of the filter parameters, which can be used for both (ii) designing more
efficient schemes for computing receptive field responses over populations of
multi-parameter families of receptive fields, as well as (iii)~formulating
idealized theoretical models of the computations of simple cells in biological
vision.

</details>


### [62] [FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion](https://arxiv.org/abs/2509.15750)
*Han Ye,Haofu Wang,Yunchi Zhang,Jiangjian Xiao,Yuqiang Jin,Jinyuan Liu,Wen-An Zhang,Uladzislau Sychou,Alexander Tuzikov,Vladislav Sobolevskii,Valerii Zakharov,Boris Sokolov,Minglei Fu*

Main category: cs.CV

TL;DR: 提出FloorSAM，将点云密度图与SAM结合，实现从LiDAR点云零样本房间分割与楼层平面图重建，在噪声与复杂布局下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有几何算法与Mask R-CNN类方法在噪声、多样化布局下泛化差、且易丢失几何细节，难以稳定从点云重建精确楼层平面图；需要一种鲁棒且能保持几何拓扑关系的方案。

Method: 将LiDAR点云投影为自适应分辨率的顶视密度图，结合网格滤波与图像增强提升鲁棒性；利用SAM的零样本分割，通过自适应提示点与多阶段过滤生成房间掩膜；联合掩膜与点云进行轮廓提取与规则化，并恢复房间拓扑关系，输出规范化的平面图。

Result: 在Giblayout与ISPRS数据集上，FloorSAM在准确率、召回率与鲁棒性方面优于几何算法与Mask R-CNN等传统方法，尤其在噪声大与布局复杂场景中表现更好。

Conclusion: 点云密度图+SAM的零样本分割框架能提升室内平面图重建的准确性与泛化能力，并有效保留几何与拓扑细节；为导航、BIM与测量应用提供更可靠的自动化方案，代码已开源。

Abstract: Reconstructing building floor plans from point cloud data is key for indoor
navigation, BIM, and precise measurements. Traditional methods like geometric
algorithms and Mask R-CNN-based deep learning often face issues with noise,
limited generalization, and loss of geometric details. We propose FloorSAM, a
framework that integrates point cloud density maps with the Segment Anything
Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using
grid-based filtering, adaptive resolution projection, and image enhancement, we
create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for
precise room segmentation, improving reconstruction across diverse layouts.
Room masks are generated via adaptive prompt points and multistage filtering,
followed by joint mask and point cloud analysis for contour extraction and
regularization. This produces accurate floor plans and recovers room
topological relationships. Tests on Giblayout and ISPRS datasets show better
accuracy, recall, and robustness than traditional methods, especially in noisy
and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.

</details>


### [63] [Simulated Cortical Magnification Supports Self-Supervised Object Learning](https://arxiv.org/abs/2509.15751)
*Zhengyang Yu,Arthur Aubret,Chen Yu,Jochen Triesch*

Main category: cs.CV

TL;DR: 在幼儿般的自监督视觉训练中，引入人类视网膜中心高分辨率、周边低分辨率的仿真（仿视网膜离心与皮层放大）能提升对象表征质量。


<details>
  <summary>Details</summary>
Motivation: 现有自监督模型用幼儿的第一人称视觉经验训练，但忽略人类视觉的中心-周边分辨率差异。作者想评估这种“注视中心高、周边低”的特性在对象表征发展中的作用。

Method: 使用两个人类与物体交互的头戴式第一人称视频数据集；对视频施加人类仿注视与皮层放大模型，使图像随离中心距离分辨率降低；用时间一致性/预测的自监督目标训练两种仿生自监督模型；比较引入仿注视与不引入的差异，并分析对象大小与中心/周边信息权衡的影响。

Result: 引入仿注视与皮层放大后，模型学到的对象表征更优（质量提升）；改进来源于将对象在视野中“变大”以及更合理地权衡中央与周边信息。

Conclusion: 在幼儿式自监督场景中，模拟人类的注视与皮层放大能让视觉表征更逼近人类且更高效，为更现实与更强的视觉学习模型迈出一步。

Abstract: Recent self-supervised learning models simulate the development of semantic
object representations by training on visual experience similar to that of
toddlers. However, these models ignore the foveated nature of human vision with
high/low resolution in the center/periphery of the visual field. Here, we
investigate the role of this varying resolution in the development of object
representations. We leverage two datasets of egocentric videos that capture the
visual experience of humans during interactions with objects. We apply models
of human foveation and cortical magnification to modify these inputs, such that
the visual content becomes less distinct towards the periphery. The resulting
sequences are used to train two bio-inspired self-supervised learning models
that implement a time-based learning objective. Our results show that modeling
aspects of foveated vision improves the quality of the learned object
representations in this setting. Our analysis suggests that this improvement
comes from making objects appear bigger and inducing a better trade-off between
central and peripheral visual information. Overall, this work takes a step
towards making models of humans' learning of visual representations more
realistic and performant.

</details>


### [64] [MCOD: The First Challenging Benchmark for Multispectral Camouflaged Object Detection](https://arxiv.org/abs/2509.15753)
*Yang Li,Tingfa Xu,Shuyan Bai,Peifu Liu,Jianan Li*

Main category: cs.CV

TL;DR: 提出首个专为多光谱伪装目标检测的基准数据集MCOD，涵盖多场景与挑战属性，配高质量像素级标注；在11种方法上验证，多光谱融合显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测主要依赖RGB，在小目标、极端光照等复杂环境下性能受限；缺少面向多光谱的公开基准数据集，阻碍该方向发展。

Method: 构建MCOD多光谱COD数据集：覆盖多样自然场景，标注像素级掩膜与挑战属性（如小目标、极端光照）；在该数据集上对十一种代表性RGB-COD方法进行基准评测，并比较单RGB与多光谱融合的表现。

Result: 在MCOD上，现有方法整体性能下降，反映任务更具挑战性；引入多光谱模态显著缓解性能下滑，提升检测鲁棒性。

Conclusion: MCOD填补多光谱COD基准空白，证明光谱信息可有效增强伪装目标检测；公开数据集将促进该领域后续研究与方法发展。

Abstract: Camouflaged Object Detection (COD) aims to identify objects that blend
seamlessly into natural scenes. Although RGB-based methods have advanced, their
performance remains limited under challenging conditions. Multispectral
imagery, providing rich spectral information, offers a promising alternative
for enhanced foreground-background discrimination. However, existing COD
benchmark datasets are exclusively RGB-based, lacking essential support for
multispectral approaches, which has impeded progress in this area. To address
this gap, we introduce MCOD, the first challenging benchmark dataset
specifically designed for multispectral camouflaged object detection. MCOD
features three key advantages: (i) Comprehensive challenge attributes: It
captures real-world difficulties such as small object sizes and extreme
lighting conditions commonly encountered in COD tasks. (ii) Diverse real-world
scenarios: The dataset spans a wide range of natural environments to better
reflect practical applications. (iii) High-quality pixel-level annotations:
Each image is manually annotated with precise object masks and corresponding
challenge attribute labels. We benchmark eleven representative COD methods on
MCOD, observing a consistent performance drop due to increased task difficulty.
Notably, integrating multispectral modalities substantially alleviates this
degradation, highlighting the value of spectral information in enhancing
detection robustness. We anticipate MCOD will provide a strong foundation for
future research in multispectral camouflaged object detection. The dataset is
publicly accessible at https://github.com/yl2900260-bit/MCOD.

</details>


### [65] [Overview of PlantCLEF 2024: multi-species plant identification in vegetation plot images](https://arxiv.org/abs/2509.15768)
*Herve Goeau,Vincent Espitalier,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2024提出并描述了一个用于生态学样方高分辨率图像的多标签识别挑战：用单株训练数据与预训练ViT模型，在弱标注设定下预测样方图中的全部植物种。提供数据、评估方法、参赛策略与结果。


<details>
  <summary>Details</summary>
Motivation: 生态研究依赖标准化样方图像来进行生物多样性评估与长期监测，但人工物种识别耗时费力、难以扩展。引入AI可提升专家效率和研究覆盖面，需要一个高质量基准来衡量进展。

Method: 构建新测试集：数千张由专家标注的多标签样方图像，覆盖800+物种；训练集：170万张单株植物图像；提供在该训练数据上预训练的视觉Transformer模型。任务设定为弱标注多标签分类：用单标签训练数据预测样方图像中的所有物种。论文详述数据与评估流程，并汇总参赛者方法与模型。

Result: 基准与挑战运行产生了参赛方法的比较结果，展示了在高分辨率样方图上的多标签识别性能（具体数值未在摘要中给出），体现预训练ViT与大规模单株数据对样方物种识别的有效性。

Conclusion: PlantCLEF 2024为生态样方图像的多标签物种识别提供了系统数据与评估框架，推动利用大规模单株数据与预训练视觉模型在弱标注设定下的应用，并为未来方法改进与大规模生态监测奠定基准。

Abstract: Plot images are essential for ecological studies, enabling standardized
sampling, biodiversity assessment, long-term monitoring and remote, large-scale
surveys. Plot images are typically fifty centimetres or one square meter in
size, and botanists meticulously identify all the species found there. The
integration of AI could significantly improve the efficiency of specialists,
helping them to extend the scope and coverage of ecological studies. To
evaluate advances in this regard, the PlantCLEF 2024 challenge leverages a new
test set of thousands of multi-label images annotated by experts and covering
over 800 species. In addition, it provides a large training set of 1.7 million
individual plant images as well as state-of-the-art vision transformer models
pre-trained on this data. The task is evaluated as a (weakly-labeled)
multi-label classification task where the aim is to predict all the plant
species present on a high-resolution plot image (using the single-label
training data). In this paper, we provide an detailed description of the data,
the evaluation methodology, the methods and models employed by the participants
and the results achieved.

</details>


### [66] [Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation](https://arxiv.org/abs/2509.15772)
*Weimin Bai,Yubo Li,Weijian Luo,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: 提出VLM3D：把大型视觉语言模型作为可微的语义与空间先验融入SDS，以提升文本到3D生成的细粒度对齐与三维一致性；在GPTeval3D上显著优于传统SDS方法。


<details>
  <summary>Details</summary>
Motivation: 现有SDS依赖CLIP文本编码器，细粒度语义对齐差；且2D扩散先验缺乏显式三维空间约束，导致几何不一致与多物体关系错误。需要更强的语义对齐与空间理解来提升3D生成质量与一致性。

Method: 在SDS管线中引入大型视觉语言模型（以Qwen2.5-VL为例）作为可微先验：一方面提供语言驱动的细粒度语义监督，另一方面利用其视觉-语言建模的空间理解对多视角渲染进行约束，从而增强单物体的3D一致性与多物体场景的关系推理。

Result: 在GPTeval3D基准上，对多样对象与复杂场景实验显示：VLM3D在语义保真度、几何一致性与空间正确性方面显著优于以往SDS方法。

Conclusion: 将VLM作为语义与空间先验集成到SDS可有效弥补CLIP与2D扩散的不足，提升文本到3D生成的精细对齐与三维一致性，对单物体与多物体场景均有明显改进。

Abstract: Score Distillation Sampling (SDS) enables high-quality text-to-3D generation
by supervising 3D models through the denoising of multi-view 2D renderings,
using a pretrained text-to-image diffusion model to align with the input prompt
and ensure 3D consistency. However, existing SDS-based methods face two
fundamental limitations: (1) their reliance on CLIP-style text encoders leads
to coarse semantic alignment and struggles with fine-grained prompts; and (2)
2D diffusion priors lack explicit 3D spatial constraints, resulting in
geometric inconsistencies and inaccurate object relationships in multi-object
scenes. To address these challenges, we propose VLM3D, a novel text-to-3D
generation framework that integrates large vision-language models (VLMs) into
the SDS pipeline as differentiable semantic and spatial priors. Unlike standard
text-to-image diffusion priors, VLMs leverage rich language-grounded
supervision that enables fine-grained prompt alignment. Moreover, their
inherent vision language modeling provides strong spatial understanding, which
significantly enhances 3D consistency for single-object generation and improves
relational reasoning in multi-object scenes. We instantiate VLM3D based on the
open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.
Experiments across diverse objects and complex scenes show that VLM3D
significantly outperforms prior SDS-based methods in semantic fidelity,
geometric coherence, and spatial correctness.

</details>


### [67] [Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution](https://arxiv.org/abs/2509.15781)
*Chang Soo Lim,Joonyoung Moon,Donghyeon Cho*

Main category: cs.CV

TL;DR: 提出SCOPE：用SAM2的ViT编码器替换Cutie的编码器，并加入运动预测模块；再与原始Cutie与SAM2做集成，提升VOS的时序稳定与特征表达，在LSVOS MOSEv2赛道获第3名。


<details>
  <summary>Details</summary>
Motivation: 现有VOS方法各有短板：Cutie查询式分割高效但特征容量与时序建模不足；SAM2具有强大预训练ViT特征但在特定任务的时序稳定性与集成利用不充分。需要一个框架既吸收丰富特征表示，又强化跨帧运动与稳定性。

Method: 将Cutie的编码器替换为SAM2的ViT编码器，作为更强的特征底座；设计运动预测模块，进行跨帧目标运动建模以提升时序稳定与跟踪；最终采用集成策略，将原始Cutie、SAM2与该变体进行融合（ensemble），输出更稳健的分割结果。

Result: 在第七届LSVOS挑战的MOSEv2赛道中取得第3名，表明所提方法在实际评测中具有竞争力。

Conclusion: 结合丰富特征表示（SAM2 ViT）与显式运动预测，并通过模型集成，可显著提升视频目标分割的稳健性与性能；SCOPE验证了该思路的有效性，代码开源可复现。

Abstract: Video object segmentation (VOS) is a challenging task with wide applications
such as video editing and autonomous driving. While Cutie provides strong
query-based segmentation and SAM2 offers enriched representations via a
pretrained ViT encoder, each has limitations in feature capacity and temporal
modeling. In this report, we propose a framework that integrates their
complementary strengths by replacing the encoder of Cutie with the ViT encoder
of SAM2 and introducing a motion prediction module for temporal stability. We
further adopt an ensemble strategy combining Cutie, SAM2, and our variant,
achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to
our final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This
demonstrates the effectiveness of enriched feature representation and motion
prediction for robust video object segmentation. The code is available at
https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.

</details>


### [68] [Ideal Registration? Segmentation is All You Need](https://arxiv.org/abs/2509.15784)
*Xiang Chen,Fengting Zhang,Qinghao Liu,Min Liu,Kun Wu,Yaonan Wang,Hang Zhang*

Main category: cs.CV

TL;DR: 提出SegReg：一种由分割驱动的、区域自适应的医学图像配准框架，通过先分割成解剖子区域，再对各区域分别配准并合成全局形变场，实现更精细的变形约束与显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习配准多用全局统一的平滑正则，无法匹配器官在不同区域的复杂、非均匀形变，影响精度与鲁棒性。需要一种能针对不同解剖区域自适应约束的配准方法。

Method: SegReg流程：1）对动/静态图像进行解剖分割，得到一致的子区域；2）在同一配准主干上对每个子区域独立估计局部形变场（采用区域专属的正则/约束）；3）将所有局部形变场整合为全局形变；4）可用真值或自动分割，分析配准性能与分割质量的关系。

Result: 在使用真值分割时，关键解剖结构Dice达98.23%，实现近乎完美的结构对齐；在心脏、腹部、肺等三类临床配准场景中，即便用自动分割，较现有方法提升2–12%。配准准确度与分割质量呈近线性关系。

Conclusion: 分割驱动、区域自适应的正则化能更好地捕获非均匀解剖变形，显著提升配准效果，并将配准难题部分转化为分割问题；方法通用且代码将公开。

Abstract: Deep learning has revolutionized image registration by its ability to handle
diverse tasks while achieving significant speed advantages over conventional
approaches. Current approaches, however, often employ globally uniform
smoothness constraints that fail to accommodate the complex, regionally varying
deformations characteristic of anatomical motion. To address this limitation,
we propose SegReg, a Segmentation-driven Registration framework that implements
anatomically adaptive regularization by exploiting region-specific deformation
patterns. Our SegReg first decomposes input moving and fixed images into
anatomically coherent subregions through segmentation. These localized domains
are then processed by the same registration backbone to compute optimized
partial deformation fields, which are subsequently integrated into a global
deformation field. SegReg achieves near-perfect structural alignment (98.23%
Dice on critical anatomies) using ground-truth segmentation, and outperforms
existing methods by 2-12% across three clinical registration scenarios
(cardiac, abdominal, and lung images) even with automatic segmentation. Our
SegReg demonstrates a near-linear dependence of registration accuracy on
segmentation quality, transforming the registration challenge into a
segmentation problem. The source code will be released upon manuscript
acceptance.

</details>


### [69] [CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices](https://arxiv.org/abs/2509.15785)
*Runjie Shao,Boyu Diao,Zijia An,Ruiqi Liu,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出CBPNet，在边缘设备上的持续学习中，通过自适应重置未充分利用参数来恢复模型可塑性，显著提升精度且参数开销极小。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的机器人与自动驾驶等应用需要实时应对动态环境，持续学习常用“冻结主干+提示”的策略以避免灾忘，但导致可塑性下降（难以学新知识）。作者认为原因是训练过程中一些参数缺乏更新活力、被低效使用。

Method: 提出Continual Backpropagation Prompt Network（CBPNet）。核心为Efficient CBP Block：在训练中识别并自适应地重新初始化未充分利用的参数（提示相关小模块），以对抗可塑性衰退，同时保持冻结主干、极小额外参数（<0.2%）。

Result: 在边缘设备上对多基准验证：Split CIFAR-100相较强基线平均准确率提升超过1%；在更具挑战性的Split ImageNet-R达到SOTA 69.41%准确率；所需额外训练参数不足主干的0.2%。

Conclusion: 通过恢复未充分利用参数的更新活力，CBPNet在保持参数效率的同时显著提升持续学习性能，适合边缘设备的实时应用。

Abstract: To meet the demands of applications like robotics and autonomous driving that
require real-time responses to dynamic environments, efficient continual
learning methods suitable for edge devices have attracted increasing attention.
In this transition, using frozen pretrained models with prompts has become a
mainstream strategy to combat catastrophic forgetting. However, this approach
introduces a new critical bottleneck: plasticity loss, where the model's
ability to learn new knowledge diminishes due to the frozen backbone and the
limited capacity of prompt parameters. We argue that the reduction in
plasticity stems from a lack of update vitality in underutilized parameters
during the training process. To this end, we propose the Continual
Backpropagation Prompt Network (CBPNet), an effective and parameter efficient
framework designed to restore the model's learning vitality. We innovatively
integrate an Efficient CBP Block that counteracts plasticity decay by
adaptively reinitializing these underutilized parameters. Experimental results
on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks.
On Split CIFAR-100, it improves average accuracy by over 1% against a strong
baseline, and on the more challenging Split ImageNet-R, it achieves a state of
the art accuracy of 69.41%. This is accomplished by training additional
parameters that constitute less than 0.2% of the backbone's size, validating
our approach.

</details>


### [70] [FoBa: A Foreground-Background co-Guided Method and New Benchmark for Remote Sensing Semantic Change Detection](https://arxiv.org/abs/2509.15788)
*Haotian Zhang,Han Guo,Keyan Chen,Hao Chen,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 论文提出新的遥感语义变化检测（SCD）基准LevirSCD与方法FoBa，解决数据类目不足与方法对变化信息利用不充分的两大问题，结合前景-背景协同引导、门控交互融合（GIF）与一致性损失，在多个数据集上提升SeK指标。


<details>
  <summary>Details</summary>
Motivation: 现有SCD数据集类别与类型不够丰富、缺乏细粒度定义，难以支撑实际应用；多数方法仅将变化信息用于后处理、空间一致性，未充分融入特征学习，限制性能。

Method: 1）构建LevirSCD：聚焦北京区域，包含16个变化类别与210种具体变化类型，并细化类别（如道路分为铺装/未铺装）。2）提出FoBa：利用前景（关注兴趣区域）与背景（提供上下文）共同引导模型，缓解语义歧义、提升细微变化检测。3）设计GIF模块以实现双时相特征的门控交互融合；配套简洁的一致性损失，强化空间一致性与时相交互。

Result: 在SECOND、JL1及LevirSCD三数据集上，FoBa在SeK指标分别提升1.48%、3.61%、2.81%，表现与当前SOTA竞争。

Conclusion: 丰富且细粒度的LevirSCD基准与FoBa框架通过前景-背景协同与门控交互，有效增强SCD对细微变化与语义判别的能力，提升检测性能，并具备良好跨数据集适用性。

Abstract: Despite the remarkable progress achieved in remote sensing semantic change
detection (SCD), two major challenges remain. At the data level, existing SCD
datasets suffer from limited change categories, insufficient change types, and
a lack of fine-grained class definitions, making them inadequate to fully
support practical applications. At the methodological level, most current
approaches underutilize change information, typically treating it as a
post-processing step to enhance spatial consistency, which constrains further
improvements in model performance. To address these issues, we construct a new
benchmark for remote sensing SCD, LevirSCD. Focused on the Beijing area, the
dataset covers 16 change categories and 210 specific change types, with more
fine-grained class definitions (e.g., roads are divided into unpaved and paved
roads). Furthermore, we propose a foreground-background co-guided SCD (FoBa)
method, which leverages foregrounds that focus on regions of interest and
backgrounds enriched with contextual information to guide the model
collaboratively, thereby alleviating semantic ambiguity while enhancing its
ability to detect subtle changes. Considering the requirements of bi-temporal
interaction and spatial consistency in SCD, we introduce a Gated Interaction
Fusion (GIF) module along with a simple consistency loss to further enhance the
model's detection performance. Extensive experiments on three datasets (SECOND,
JL1, and the proposed LevirSCD) demonstrate that FoBa achieves competitive
results compared to current SOTA methods, with improvements of 1.48%, 3.61%,
and 2.81% in the SeK metric, respectively. Our code and dataset are available
at https://github.com/zmoka-zht/FoBa.

</details>


### [71] [Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization](https://arxiv.org/abs/2509.15791)
*Tan Pan,Kaiyu Guo,Dongli Xu,Zhaorui Tan,Chen Jiang,Deshu Chen,Xin Guo,Brian C. Lovell,Limei Han,Yuan Cheng,Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: 提出MS-UDG，通过信息论的充分性与最小性优化无监督表示，使模型在无类别/域标签下实现更强域泛化，并在多基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习泛化研究偏监督；无监督场景（如SSL）在跨域泛化上较少探究。UDG需要在无类别标签下区分语义与变化，且现实中也缺少域标签，现有方法受限。需一种无标签框架，系统性保证学到的表示既保留语义又剔除与语义无关的变化，从而降低OOD风险。

Method: 将UDG形式化为学习“最小充分语义表示”：同时满足充分性（跨增强视图共享语义全保留）与最小性（最大剔除非语义信息）。以信息论为理论基础，证明优化这两目标可直接降低OOD风险。实践上提出MS-UDG：a) 用基于InfoNCE的目标实现充分性；b) 通过两个互补组件促进最小性：新颖的语义-变化解耦损失，以及基于重建的机制以充分捕获必要的变化。

Result: 在主流无监督域泛化基准上取得新的SOTA表现，稳定优于现有SSL与UDG方法，在表示学习阶段不使用类别或域标签。

Conclusion: 信息论驱动的最小充分表示为无监督域泛化提供了统一框架。MS-UDG有效平衡保留语义与剔除非语义变化，实证上显著提升OOD泛化且无需额外标签。

Abstract: The generalization ability of deep learning has been extensively studied in
supervised settings, yet it remains less explored in unsupervised scenarios.
Recently, the Unsupervised Domain Generalization (UDG) task has been proposed
to enhance the generalization of models trained with prevalent unsupervised
learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the
challenge of distinguishing semantics from variations without category labels.
Although some recent methods have employed domain labels to tackle this issue,
such domain labels are often unavailable in real-world contexts. In this paper,
we address these limitations by formalizing UDG as the task of learning a
Minimal Sufficient Semantic Representation: a representation that (i) preserves
all semantic information shared across augmented views (sufficiency), and (ii)
maximally removes information irrelevant to semantics (minimality). We
theoretically ground these objectives from the perspective of information
theory, demonstrating that optimizing representations to achieve sufficiency
and minimality directly reduces out-of-distribution risk. Practically, we
implement this optimization through Minimal-Sufficient UDG (MS-UDG), a
learnable model by integrating (a) an InfoNCE-based objective to achieve
sufficiency; (b) two complementary components to promote minimality: a novel
semantic-variation disentanglement loss and a reconstruction-based mechanism
for capturing adequate variation. Empirically, MS-UDG sets a new
state-of-the-art on popular unsupervised domain-generalization benchmarks,
consistently outperforming existing SSL and UDG methods, without category or
domain labels during representation learning.

</details>


### [72] [TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation](https://arxiv.org/abs/2509.15795)
*Tianyang Wang,Xi Xiao,Gaofei Chen,Hanzhang Chi,Qi Zhang,Guo Cheng,Yingrui Ji*

Main category: cs.CV

TL;DR: 提出TASAM，对SAM进行地形与时间感知的轻量扩展，在遥感高分辨率分割上显著提升，且无需重训主干，跨LoveDA、iSAID、WHU-CD均优于零样本SAM与多任务特定模型。


<details>
  <summary>Details</summary>
Motivation: SAM在自然图像零样本分割表现优异，但在遥感场景面临复杂地形、多尺度目标与时间变化等挑战，泛化欠佳。需要在不重训大模型的前提下，使基础模型适配遥感领域的先验与时空特性，提升鲁棒性与精细分割能力。

Method: 在SAM框架外加三个轻量模块：1）地形感知适配器，将高程（DEM等）先验注入特征；2）时间提示生成器，基于多时相影像提取土地覆盖变化作为提示；3）多尺度融合策略，增强细粒度目标的边界与尺度一致性。保持SAM主干不变，通过域适配与提示/特征融合实现提升。

Result: 在LoveDA、iSAID、WHU-CD三基准上取得显著增益，超过零样本SAM与若干任务特定模型；计算开销小（轻量模块），无需重训主干。

Conclusion: 面向遥感的时空与地形感知增强能有效提升基础分割模型的跨域鲁棒性；域自适应式的扩展为可扩展的地理空间分割提供了实践路径，证明在不重训主干的情况下也可获得显著收益。

Abstract: Segment Anything Model (SAM) has demonstrated impressive zero-shot
segmentation capabilities across natural image domains, but it struggles to
generalize to the unique challenges of remote sensing data, such as complex
terrain, multi-scale objects, and temporal dynamics. In this paper, we
introduce TASAM, a terrain and temporally-aware extension of SAM designed
specifically for high-resolution remote sensing image segmentation. TASAM
integrates three lightweight yet effective modules: a terrain-aware adapter
that injects elevation priors, a temporal prompt generator that captures
land-cover changes over time, and a multi-scale fusion strategy that enhances
fine-grained object delineation. Without retraining the SAM backbone, our
approach achieves substantial performance gains across three remote sensing
benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and
task-specific models with minimal computational overhead. Our results highlight
the value of domain-adaptive augmentation for foundation models and offer a
scalable path toward more robust geospatial segmentation.

</details>


### [73] [ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding](https://arxiv.org/abs/2509.15800)
*Kehua Chen*

Main category: cs.CV

TL;DR: ChronoForge-RL通过可微关键帧选择+两模块（TAD与KF-GRPO）在不遍历所有帧的情况下抓住语义拐点，显著提升视频理解效率与精度，7B模型达近72B水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解面临两难：密集帧处理计算昂贵、均匀采样难以捕捉语义关键时刻。需要一种既高效又能识别时序关键信息的框架。

Method: 提出ChronoForge-RL，核心包括：1）可微关键帧选择的三阶段流程，定位语义拐点并保留时序信息；2）Temporal Apex Distillation（TAD）：用变化评分、拐点检测与优先蒸馏挑选最信息量帧；3）KeyFrame-aware GRPO（KF-GRPO）：构建对比学习与显著性增强奖励，鼓励模型利用帧内容与时序关系进行推理。

Result: 在VideoMME得69.1%、LVBench得52.7%，超过基线；7B参数模型达到接近72B模型的表现。

Conclusion: 通过关键帧驱动的选择与强化学习优化，ChronoForge-RL在保持时序理解的同时显著降低计算，并提升多基准上的性能，验证了关键帧感知与时序蒸馏/奖励机制的有效性。

Abstract: Current state-of-the-art video understanding methods typically struggle with
two critical challenges: (1) the computational infeasibility of processing
every frame in dense video content and (2) the difficulty in identifying
semantically significant frames through naive uniform sampling strategies. In
this paper, we propose a novel video understanding framework, called
ChronoForge-RL, which combines Temporal Apex Distillation (TAD) and
KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these
issues. Concretely, we introduce a differentiable keyframe selection mechanism
that systematically identifies semantic inflection points through a three-stage
process to enhance computational efficiency while preserving temporal
information. Then, two particular modules are proposed to enable effective
temporal reasoning: Firstly, TAD leverages variation scoring, inflection
detection, and prioritized distillation to select the most informative frames.
Secondly, we introduce KF-GRPO which implements a contrastive learning paradigm
with a saliency-enhanced reward mechanism that explicitly incentivizes models
to leverage both frame content and temporal relationships. Finally, our
proposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench
compared to baseline methods, clearly surpassing previous approaches while
enabling our 7B parameter model to achieve performance comparable to 72B
parameter alternatives.

</details>


### [74] [CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models](https://arxiv.org/abs/2509.15803)
*Fangjian Shen,Zifeng Liang,Chao Wang,Wushao Wen*

Main category: cs.CV

TL;DR: 提出CIDER：在推理时通过提示优化降低文本到图像模型的品牌偏置，无需重训；引入品牌中立评分（BNS）；实验显示在保持画质与美感下显著减少显性与隐性偏置。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型在通用提示下倾向生成含主流商业品牌的内容，带来伦理与法律风险，但该偏置缺乏系统量化与低成本缓解方法。

Method: 提出模型无关的推理期框架CIDER：使用轻量级检测器识别品牌元素；借助视觉语言模型生成风格上“去品牌化”的替代提示，实现提示重写与风格转移；并设计品牌中立评分BNS量化偏置与改进效果。

Result: 在多款主流T2I模型上的广泛实验表明：CIDER可显著降低显性（明确品牌标识）与隐性（风格暗示）品牌偏置，同时保持图像质量与审美。

Conclusion: CIDER提供无需重训的实用方案，促进更原创与公平的生成内容，并推动可信可控的生成式AI发展。

Abstract: Text-to-image (T2I) models exhibit a significant yet under-explored "brand
bias", a tendency to generate contents featuring dominant commercial brands
from generic prompts, posing ethical and legal risks. We propose CIDER, a
novel, model-agnostic framework to mitigate bias at inference-time through
prompt refinement to avoid costly retraining. CIDER uses a lightweight detector
to identify branded content and a Vision-Language Model (VLM) to generate
stylistically divergent alternatives. We introduce the Brand Neutrality Score
(BNS) to quantify this issue and perform extensive experiments on leading T2I
models. Results show CIDER significantly reduces both explicit and implicit
biases while maintaining image quality and aesthetic appeal. Our work offers a
practical solution for more original and equitable content, contributing to the
development of trustworthy generative AI.

</details>


### [75] [Boosting Active Learning with Knowledge Transfer](https://arxiv.org/abs/2509.15805)
*Tianyang Wang,Xi Xiao,Gaofei Chen,Xiaoying Liao,Guo Cheng,Yingrui Ji*

Main category: cs.CV

TL;DR: 提出一种基于教师-学生知识转移的主动学习不确定性估计方法，用输出距离度量未标注样本的不确定性，学生模型任务无关且训练简单，并从理论上将不确定性与任务损失上界关联；在视觉与cryo-ET任务上验证有效高效。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习中的不确定性估计常依赖复杂的辅助模型与特殊训练（如对抗训练），在特定领域任务（如cryo-ET分类）上难以设计与稳定训练，亟需一种通用、易训练且可迁移的估计方法。

Method: 采用教师-学生框架：教师为当前AL任务模型，学生为从教师学习的辅助模型，两者在每个AL轮次同时训练；以二者输出之间的某种距离（如分布/特征/logit距离）作为未标注数据的不确定性度量。学生模型无需任务特定结构或复杂训练策略，具有任务无关性。并提出数据不确定性与任务损失上界（而非具体损失值）紧密相关的观点。

Result: 在经典计算机视觉任务与cryo-ET挑战上进行了大量实验，方法在不确定性估计与主动学习效率方面表现出优于或可与现有方法竞争的效果，验证了方法的有效性与训练简便性。

Conclusion: 基于教师-学生知识转移的输出距离可作为稳健的不确定性指标，学生模型的任务无关性提升了跨任务适用性；理论与实证均表明不确定性更应关注损失上界。该方法在多任务上实现了高效的主动学习，具有实践价值。

Abstract: Uncertainty estimation is at the core of Active Learning (AL). Most existing
methods resort to complex auxiliary models and advanced training fashions to
estimate uncertainty for unlabeled data. These models need special design and
hence are difficult to train especially for domain tasks, such as Cryo-Electron
Tomography (cryo-ET) classification in computational biology. To address this
challenge, we propose a novel method using knowledge transfer to boost
uncertainty estimation in AL. Specifically, we exploit the teacher-student mode
where the teacher is the task model in AL and the student is an auxiliary model
that learns from the teacher. We train the two models simultaneously in each AL
cycle and adopt a certain distance between the model outputs to measure
uncertainty for unlabeled data. The student model is task-agnostic and does not
rely on special training fashions (e.g. adversarial), making our method
suitable for various tasks. More importantly, we demonstrate that data
uncertainty is not tied to concrete value of task loss but closely related to
the upper-bound of task loss. We conduct extensive experiments to validate the
proposed method on classical computer vision tasks and cryo-ET challenges. The
results demonstrate its efficacy and efficiency.

</details>


### [76] [LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land Cover Classification from Satellite Imagery and Sparse In-situ Labels](https://arxiv.org/abs/2509.15868)
*Johannes Leonhardt,Juergen Gall,Ribana Roscher*

Main category: cs.CV

TL;DR: 提出LC-SLab框架，在稀疏监督下用对象级方法（输入端图神经网络聚合、输出端分割后聚合）进行大尺度土地覆盖分类，减少碎片化并在小数据上更稳健，部分配置优于现有产品。


<details>
  <summary>Details</summary>
Motivation: 深度学习土地覆盖图依赖大规模训练数据；开放的实地调查数据（如LUCAS）虽可扩展，但空间稀疏导致基于像素的方法预测碎片化、噪声大。需要引入对象级分类以施加最小制图单元，提升连贯性，且在中分辨率影像与稀疏标注场景仍缺系统研究。

Method: 提出LC-SLab框架：1) 输入级聚合：利用图神经网络对语义相近的区域进行特征聚合后再分类；2) 输出级聚合：对现有语义分割模型的像素结果进行面向对象的后处理聚合；3) 融合大规模预训练网络的特征以增强小样本性能。以年度Sentinel-2合成影像和稀疏LUCAS标签评估，分析精度-碎片化权衡及对数据规模的敏感性。

Result: 对象级方法在精度上可与常见像素级模型相当或更优，同时显著提升地图连贯性；输入级聚合在小数据条件下更稳健，输出级聚合在数据较多时表现更好；若干LC-SLab配置优于已有土地覆盖产品。

Conclusion: 对象级深度学习在稀疏监督下进行大尺度土地覆盖制图是有效路径；LC-SLab提供系统化工具与实证，能在不同数据规模下取得高精度且低碎片化的结果，并具有实际应用价值。

Abstract: Large-scale land cover maps generated using deep learning play a critical
role across a wide range of Earth science applications. Open in-situ datasets
from principled land cover surveys offer a scalable alternative to manual
annotation for training such models. However, their sparse spatial coverage
often leads to fragmented and noisy predictions when used with existing deep
learning-based land cover mapping approaches. A promising direction to address
this issue is object-based classification, which assigns labels to semantically
coherent image regions rather than individual pixels, thereby imposing a
minimum mapping unit. Despite this potential, object-based methods remain
underexplored in deep learning-based land cover mapping pipelines, especially
in the context of medium-resolution imagery and sparse supervision. To address
this gap, we propose LC-SLab, the first deep learning framework for
systematically exploring object-based deep learning methods for large-scale
land cover classification under sparse supervision. LC-SLab supports both
input-level aggregation via graph neural networks, and output-level aggregation
by postprocessing results from established semantic segmentation models.
Additionally, we incorporate features from a large pre-trained network to
improve performance on small datasets. We evaluate the framework on annual
Sentinel-2 composites with sparse LUCAS labels, focusing on the tradeoff
between accuracy and fragmentation, as well as sensitivity to dataset size. Our
results show that object-based methods can match or exceed the accuracy of
common pixel-wise models while producing substantially more coherent maps.
Input-level aggregation proves more robust on smaller datasets, whereas
output-level aggregation performs best with more data. Several configurations
of LC-SLab also outperform existing land cover products, highlighting the
framework's practical utility.

</details>


### [77] [Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval](https://arxiv.org/abs/2509.15871)
*Liwei Liao,Xufeng Li,Xiaoyun Zheng,Boning Liu,Feng Gao,Ronggang Wang*

Main category: cs.CV

TL;DR: 提出GVR，将3D视觉指代在3D高斯点云(3DGS)中转化为2D视角检索的零样本框架，避免逐场景训练与3D标注，达成SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 3DVG在3DGS中面临两难：隐式纹理表示导致必须逐场景训练；现有方法依赖大量标注数据，成本高且不易扩展。需一种无需标注与逐场景训练的通用方法。

Method: 把3D指代问题重构为2D检索任务：进行对象级视角检索，从多视角收集定位线索以对齐文本提示与3D场景中的目标；在3DGS上以零样本方式运行，避免3D标注与逐场景训练。

Result: 在广泛实验中，方法在无逐场景训练的前提下取得SOTA的3D视觉指代表现；展示视频在项目主页。

Conclusion: GVR为3DGS上的零样本3D视觉指代提供有效范式，通过多视角检索实现高效定位，显著降低训练与标注成本，为零样本3DVG研究奠定基础。

Abstract: 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text
prompts, which is essential for applications such as robotics. However,
existing 3DVG methods encounter two main challenges: first, they struggle to
handle the implicit representation of spatial textures in 3D Gaussian Splatting
(3DGS), making per-scene training indispensable; second, they typically require
larges amounts of labeled data for effective training. To this end, we propose
\underline{G}rounding via \underline{V}iew \underline{R}etrieval (GVR), a novel
zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D
retrieval task that leverages object-level view retrieval to collect grounding
clues from multiple views, which not only avoids the costly process of 3D
annotation, but also eliminates the need for per-scene training. Extensive
experiments demonstrate that our method achieves state-of-the-art visual
grounding performance while avoiding per-scene training, providing a solid
foundation for zero-shot 3DVG research. Video demos can be found in
https://github.com/leviome/GVR_demos.

</details>


### [78] [ENSAM: an efficient foundation model for interactive segmentation of 3D medical images](https://arxiv.org/abs/2509.15874)
*Elias Stenhede,Agnar Martin Bjørnstad,Arian Ranjbar*

Main category: cs.CV

TL;DR: ENSAM 是一个轻量、可提示的通用 3D 医学图像分割模型，在有限数据与算力下从零训练，凭借归一化注意力、相对位置编码与 Muon 优化器等设计，达到与主流方法相当或更优的性能，并在挑战赛中取得较好排名。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 医学分割模型常依赖大量数据与预训练权重，且在多模态与交互式（可提示）场景下泛化与效率不足。作者希望构建一个无需预训练、可在有限算力与数据下快速收敛、支持多模态与交互提示的通用 3D 分割模型，并验证其在标准挑战赛上的竞争力。

Method: 提出 ENSAM：以 SegResNet 为主干编码器，配合提示编码器与掩膜解码器构成 U-Net 风格结构；在潜空间引入跨注意力，采用相对位置编码与归一化注意力；训练使用 Muon 优化器以提升稳定性与收敛速度。数据覆盖 CT、MRI、PET、超声、显微等多模态，约 <5000 个体积，单张 32GB GPU 6 小时从零训练；并进行消融验证各组件贡献。

Result: 在 CVPR 2025 互动式 3D 医学分割挑战的隐藏测试集上，获得 DSC AUC 2.404、NSD AUC 2.266、最终 DSC 0.627、最终 NSD 0.597。相较基线 VISTA3D 与 SAM-Med3D 全面更优；与 SegVol 持平并在最终 DSC 上更好，但在其余三项指标略逊。在 coreset 赛道中总排名第 5/10，且在不使用预训练权重的方法中最佳。

Conclusion: ENSAM 在资源受限条件下实现了稳定高效的训练与多模态 3D 分割性能，显示归一化注意力、相对位置编码与 Muon 优化器对收敛与质量的显著促进。在挑战赛中具有竞争力，表明轻量、可提示、从零训练的方案可替代或补充大型预训练模型。

Abstract: We present ENSAM (Equivariant, Normalized, Segment Anything Model), a
lightweight and promptable model for universal 3D medical image segmentation.
ENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoder
in a U-Net-style architecture, using latent cross-attention, relative
positional encoding, normalized attention, and the Muon optimizer for training.
ENSAM is designed to achieve good performance under limited data and
computational budgets, and is trained from scratch on under 5,000 volumes from
multiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GB
GPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3D
Biomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test set
with multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of
2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previously
published baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),
surpassing its performance in final DSC but trailing behind in the other three
metrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overall
and best among the approaches not utilizing pretrained weights. Ablation
studies confirm that our use of relative positional encodings and the Muon
optimizer each substantially speed up convergence and improve segmentation
quality.

</details>


### [79] [Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration](https://arxiv.org/abs/2509.15882)
*Xingmei Wang,Xiaoyu Hu,Chengkai Huang,Ziyan Zeng,Guohao Nie,Quan Z. Sheng,Lina Yao*

Main category: cs.CV

TL;DR: CrossI2P 是一个自监督端到端框架，通过跨模态对比学习与两阶段粗到细注册，解决图像与点云在配准中的语义-几何鸿沟与易陷入局部最优问题，在 KITTI 与 nuScenes 上显著提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有 I2P（图像到点云）配准因图像的纹理丰富但深度不确定、点云稀疏但度量精确而存在语义-几何缺口；同时，注册优化易陷入局部最优。需要一种能在无标注条件下统一跨模态表示学习与稳定配准的框架，提高跨域匹配的准确性与鲁棒性。

Method: 提出 CrossI2P：1) 通过双路径对比学习构建几何-语义融合的共享嵌入空间，实现无标注的双向对齐；2) 两阶段粗到细注册：全局阶段在“超点-超像素”层面建立跨模态与模态内上下文的对应关系，局部阶段引入几何约束进行点级精细化；3) 动态训练机制与梯度归一化，平衡特征对齐、对应关系优化与位姿估计多任务损失。

Result: 在 KITTI Odometry 基准上相对 SOTA 提升 23.7%，在 nuScenes 上提升 37.9%，体现更高的配准准确性与鲁棒性。

Conclusion: 跨模态对比学习与两阶段配准的端到端自监督设计有效弥合图像-点云语义与几何差异，并通过动态训练稳定优化过程，显著优于现有方法，适合用于自主系统的稳健感知。

Abstract: Bridging 2D and 3D sensor modalities is critical for robust perception in
autonomous systems. However, image-to-point cloud (I2P) registration remains
challenging due to the semantic-geometric gap between texture-rich but
depth-ambiguous images and sparse yet metrically precise point clouds, as well
as the tendency of existing methods to converge to local optima. To overcome
these limitations, we introduce CrossI2P, a self-supervised framework that
unifies cross-modal learning and two-stage registration in a single end-to-end
pipeline. First, we learn a geometric-semantic fused embedding space via
dual-path contrastive learning, enabling annotation-free, bidirectional
alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine
registration paradigm: a global stage establishes superpoint-superpixel
correspondences through joint intra-modal context and cross-modal interaction
modeling, followed by a geometry-constrained point-level refinement for precise
registration. Third, we employ a dynamic training mechanism with gradient
normalization to balance losses for feature alignment, correspondence
refinement, and pose estimation. Extensive experiments demonstrate that
CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry
benchmark and by 37.9% on nuScenes, significantly improving both accuracy and
robustness.

</details>


### [80] [RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning](https://arxiv.org/abs/2509.15883)
*Xiaosheng Long,Hanyu Wang,Zhentao Song,Kun Luo,Hongde Liu*

Main category: cs.CV

TL;DR: 提出RACap：一种关系感知的检索增强图像描述模型，从外部检索的字幕中挖掘结构化关系语义，并在图像中识别异质对象，以提升语义一致性与关系表达；在仅10.8M可训练参数下，性能优于既有轻量模型。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强图像描述方法在关系建模上存在两点不足：语义提示表示过于粗粒，难以刻画细粒度关系；缺乏对图像对象及其语义关系的显式建模。为解决复杂场景理解的瓶颈，需要引入结构化关系语义与对象级建模。

Method: 构建RACap框架：1) 从检索到的外部字幕中挖掘并编码结构化关系特征（如主语-谓语-宾语、属性、交互），形成细粒度关系提示；2) 在图像中识别异质（多类型、多模态）对象并提取其关系；3) 将这些关系特征与视觉信息融合，以增强语义一致性与关系表达能力；整体参数高效，仅10.8M可训练参数。

Result: 在实验中，RACap在多项指标上优于以往轻量级图像描述模型，体现出更好的关系表达与语义一致性，同时保持较低参数规模（10.8M）。

Conclusion: 通过关系感知的检索增强与异质对象的显式建模，RACap能更好理解复杂场景并生成更准确、关系丰富的图像描述；在轻量参数设置下依然取得最优或领先的性能，显示出高效与有效的优势。

Abstract: Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.

</details>


### [81] [RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation](https://arxiv.org/abs/2509.15886)
*Paul Julius Kühn,Duc Anh Nguyen,Arjan Kuijper,Holger Graf,Dieter Fellner,Saptarshi Neil Sinha*

Main category: cs.CV

TL;DR: 论文提出在激光雷达范围视角（range-view）中适配视觉基础模型SAM2进行点云语义分割，通过2D特征提取与投影/反投影，实现兼顾精度与实时性的3D分割，并在SemanticKITTI上取得有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 现有体素/点方法精度高但计算开销大、内存访问不规则、实时性差；而范围视角方法可复用成熟2D分割技术却研究不足。随着VFMs（如SAM2）在分割等任务突飞猛进，作者探索其是否能作为强大骨干用于LiDAR点云的range-view分割，以提升速度、可扩展性与部署简便性。

Method: 将SAM2适配为range-view点云分割骨干：以2D高效编码器提取特征，结合标准投影/反投影在点云上操作；针对球面投影与LiDAR范围图的几何与空间特性，对编码器做三项修改：1）新模块强化水平（方位角）空间依赖；2）定制化配置以匹配球面投影几何；3）调整骨干的机制以更好捕捉范围视角伪图像的空间模式与不连续性。

Result: 在SemanticKITTI上获得具有竞争力的分割性能，同时显著受益于2D管线的速度、可扩展性与部署简易性，证明VFMs可作为3D感知通用骨干。

Conclusion: 范围视角分割结合VFMs（SAM2）能在保持精度的同时提升效率，验证了VFMs在3D点云任务中的可行性，并为统一、以基础模型驱动的LiDAR分割方向打开道路。

Abstract: Point cloud segmentation is central to autonomous driving and 3D scene
understanding. While voxel- and point-based methods dominate recent research
due to their compatibility with deep architectures and ability to capture
fine-grained geometry, they often incur high computational cost, irregular
memory access, and limited real-time efficiency. In contrast, range-view
methods, though relatively underexplored - can leverage mature 2D semantic
segmentation techniques for fast and accurate predictions. Motivated by the
rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot
recognition, and multimodal tasks, we investigate whether SAM2, the current
state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for
LiDAR point cloud segmentation in the range view. We present , to our
knowledge, the first range-view framework that adapts SAM2 to 3D segmentation,
coupling efficient 2D feature extraction with standard
projection/back-projection to operate on point clouds. To optimize SAM2 for
range-view representations, we implement several architectural modifications to
the encoder: (1) a novel module that emphasizes horizontal spatial dependencies
inherent in LiDAR range images, (2) a customized configuration of tailored to
the geometric properties of spherical projections, and (3) an adapted mechanism
in the encoder backbone specifically designed to capture the unique spatial
patterns and discontinuities present in range-view pseudo-images. Our approach
achieves competitive performance on SemanticKITTI while benefiting from the
speed, scalability, and deployment simplicity of 2D-centric pipelines. This
work highlights the viability of VFMs as general-purpose backbones for 3D
perception and opens a path toward unified, foundation-model-driven LiDAR
segmentation. Results lets us conclude that range-view segmentation methods
using VFMs leads to promising results.

</details>


### [82] [Global Regulation and Excitation via Attention Tuning for Stereo Matching](https://arxiv.org/abs/2509.15891)
*Jiahao Li,Xinhong Chen,Zhengmin Jiang,Qian Zhou,Yung-Hui Li,Jianping Wang*

Main category: cs.CV

TL;DR: 提出GREAT框架，通过三种注意力（空间、匹配、体积）为迭代式双目匹配方法注入全局上下文与几何信息，在遮挡、无纹理、重复纹理等疑难区域显著提升表现，并在多个基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有迭代式方法（如RAFT-Stereo、IGEV-Stereo）在病态区域易失败，根因是缺乏全局上下文与几何线索，导致代价体与更新过程难以正确约束与收敛。需要一种通用机制将全局信息引入迭代优化。

Method: 提出GREAT（Global Regulation and Excitation via Attention Tuning）框架，包含三种注意力模块：1）Spatial Attention（SA）：在空间维度聚合场景级全局上下文；2）Matching Attention（MA）：沿对极线（扫描视差方向）提取全局匹配上下文；3）Volume Attention（VA）：结合SA与MA，对代价体进行调制与激励，注入全局与几何细节。将该框架无缝集成到多种迭代双目匹配网络中形成GREAT-Stereo。

Result: 在多基准上显著提升迭代式方法在遮挡、无纹理、重复模式等区域的鲁棒性。集成到IGEV-Stereo后，GREAT-IGEV在Scene Flow、KITTI 2015、ETH3D排行榜居首，在Middlebury居第二；代码公开。

Conclusion: 通过注意力调谐引入全局上下文和几何信息，可普遍提升迭代式双目匹配在病态区域的性能。GREAT框架通用、可集成、效果显著，并达成多项SOTA。

Abstract: Stereo matching achieves significant progress with iterative algorithms like
RAFT-Stereo and IGEV-Stereo. However, these methods struggle in ill-posed
regions with occlusions, textureless, or repetitive patterns, due to a lack of
global context and geometric information for effective iterative refinement. To
enable the existing iterative approaches to incorporate global context, we
propose the Global Regulation and Excitation via Attention Tuning (GREAT)
framework which encompasses three attention modules. Specifically, Spatial
Attention (SA) captures the global context within the spatial dimension,
Matching Attention (MA) extracts global context along epipolar lines, and
Volume Attention (VA) works in conjunction with SA and MA to construct a more
robust cost-volume excited by global context and geometric details. To verify
the universality and effectiveness of this framework, we integrate it into
several representative iterative stereo-matching methods and validate it
through extensive experiments, collectively denoted as GREAT-Stereo. This
framework demonstrates superior performance in challenging ill-posed regions.
Applied to IGEV-Stereo, among all published methods, our GREAT-IGEV ranks first
on the Scene Flow test set, KITTI 2015, and ETH3D leaderboards, and achieves
second on the Middlebury benchmark. Code is available at
https://github.com/JarvisLee0423/GREAT-Stereo.

</details>


### [83] [Deep Feedback Models](https://arxiv.org/abs/2509.15905)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 提出一种带反馈的有状态神经网络（DFM），通过时间上的高层表征与自底向上的输入交互，提升在高噪声与小数据条件下的鲁棒性与泛化，在识别与分割以及医疗影像任务上优于前馈模型。


<details>
  <summary>Details</summary>
Motivation: 前馈网络在低数据、噪声干扰和稳定性方面存在不足，缺乏类似生物决策中的反馈与迭代过程。作者希望通过引入受控反馈动力学，改善模型的稳健性、收敛性与泛化能力。

Method: 将内部状态更新建模为微分方程，并用循环神经网络数值求解；在迭代过程中加入指数衰减项以稳定并确保收敛。通过在目标识别与图像分割任务上，对比DFM与前馈模型，重点评估噪声鲁棒性与小样本泛化；同时测试在医疗影像场景与多种噪声类型下的表现。

Result: DFM在对象识别和分割任务中稳定优于前馈模型，尤其在高噪声或低数据情境下表现更佳；在医疗影像设置中也能保持强鲁棒性，对多类噪声腐蚀具备抵抗能力。

Conclusion: 反馈机制与稳定动力学（指数衰减）使DFM获得更稳定、鲁棒且可泛化的学习能力，强调了在深度模型中引入反馈的重要性。

Abstract: Deep Feedback Models (DFMs) are a new class of stateful neural networks that
combine bottom up input with high level representations over time. This
feedback mechanism introduces dynamics into otherwise static architectures,
enabling DFMs to iteratively refine their internal state and mimic aspects of
biological decision making. We model this process as a differential equation
solved through a recurrent neural network, stabilized via exponential decay to
ensure convergence. To evaluate their effectiveness, we measure DFMs under two
key conditions: robustness to noise and generalization with limited data. In
both object recognition and segmentation tasks, DFMs consistently outperform
their feedforward counterparts, particularly in low data or high noise regimes.
In addition, DFMs translate to medical imaging settings, while being robust
against various types of noise corruption. These findings highlight the
importance of feedback in achieving stable, robust, and generalizable learning.
Code is available at https://github.com/DCalhas/deep_feedback_models.

</details>


### [84] [Sparse Multiview Open-Vocabulary 3D Detection](https://arxiv.org/abs/2509.15924)
*Olivier Moliner,Viktor Larsson,Kalle Åström*

Main category: cs.CV

TL;DR: 提出一种无需训练的开放词汇3D目标检测方法，在稀疏视角条件下通过将2D基础模型的检测结果提升到3D并优化跨视角特征一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D检测依赖固定类别与大量3D数据/训练，限制开放词汇场景与稀疏视角下的应用；需要一种能利用丰富的2D预训练知识、在少量RGB视图下仍能稳定进行开放词汇3D检测的方案。

Method: 不进行3D特定训练，使用预训练2D基础模型做开放词汇检测与特征抽取；将多视角2D检测提升为3D候选（3D proposals），并通过跨视角的特征度量一致性（featuremetric consistency）直接优化这些3D候选的位姿与尺度，从而得到3D边界框。

Result: 在标准基准上，该简单管线在密集采样场景中具有竞争力；在稀疏视角条件下显著超过现有最先进方法，建立了强有力的基线。

Conclusion: 利用2D基础模型与跨视角特征一致性即可实现训练免的开放词汇3D检测，特别适用于稀疏视角输入，为3D理解任务提供简洁高效的基线与实践价值。

Abstract: The ability to interpret and comprehend a 3D scene is essential for many
vision and robotics systems. In numerous applications, this involves 3D object
detection, i.e.~identifying the location and dimensions of objects belonging to
a specific category, typically represented as bounding boxes. This has
traditionally been solved by training to detect a fixed set of categories,
which limits its use. In this work, we investigate open-vocabulary 3D object
detection in the challenging yet practical sparse-view setting, where only a
limited number of posed RGB images are available as input. Our approach is
training-free, relying on pre-trained, off-the-shelf 2D foundation models
instead of employing computationally expensive 3D feature fusion or requiring
3D-specific learning. By lifting 2D detections and directly optimizing 3D
proposals for featuremetric consistency across views, we fully leverage the
extensive training data available in 2D compared to 3D. Through standard
benchmarks, we demonstrate that this simple pipeline establishes a powerful
baseline, performing competitively with state-of-the-art techniques in densely
sampled scenarios while significantly outperforming them in the sparse-view
setting.

</details>


### [85] [PAN: Pillars-Attention-Based Network for 3D Object Detection](https://arxiv.org/abs/2509.15935)
*Ruan Bispo,Dane Mitrev,Letizia Mariotti,Clément Botty,Denver Humphrey,Anthony Scanlan,Ciarán Eising*

Main category: cs.CV

TL;DR: 提出一种基于摄像头-雷达融合的BEV 3D目标检测算法，利用雷达优势（距离与速度）并用自注意力建模点间依赖，简化骨干与卷积以降时延，在nuScenes上以ResNet-50达成58.2 NDS和更快推理速度，刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 相较于摄像头-激光雷达融合，摄像头-雷达融合成本更低且在恶劣天气与光照下更稳健，但相关研究与针对雷达点云优势的架构较少；需要一种能有效利用雷达距离、速度信息并在实时性上优于现有PointPillars-FPN方案的检测方法。

Method: 在BEV框架下进行相机-雷达融合：新骨干将雷达pillar特征映射到嵌入维度，并通过自注意力建模雷达点之间依赖；用简化的卷积层替换PointPillars常用的FPN层，以减少推理时间；随后在检测头前融合特征。

Result: 在nuScenes数据集上，以ResNet-50为视觉主干的配置达到58.2 NDS，且推理速度优于同类方法，刷新摄像头-雷达融合类别的SOTA与实时性基准。

Conclusion: 雷达特征的自注意力建模与轻量化卷积骨干能有效提升摄像头-雷达融合3D检测的精度与效率，证明了雷达点云在BEV检测中的价值，并为低成本、实时、恶劣条件下的感知提供新基准。

Abstract: Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar
fusion for the 3D object detection task in real-time under adverse weather and
lighting conditions. However, currently, in the literature, it is possible to
find few works focusing on this modality and, most importantly, developing new
architectures to explore the advantages of the radar point cloud, such as
accurate distance estimation and speed information. Therefore, this work
presents a novel and efficient 3D object detection algorithm using cameras and
radars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of
radar before fusing the features into a detection head. A new backbone is
introduced, which maps the radar pillar features into an embedded dimension. A
self-attention mechanism allows the backbone to model the dependencies between
the radar points. We are using a simplified convolutional layer to replace the
FPN-based convolutional layers used in the PointPillars-based architectures
with the main goal of reducing inference time. Our results show that with this
modification, our approach achieves the new state-of-the-art in the 3D object
detection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,
while also setting a new benchmark for inference time on the nuScenes dataset
for the same category.

</details>


### [86] [A multi-temporal multi-spectral attention-augmented deep convolution neural network with contrastive learning for crop yield prediction](https://arxiv.org/abs/2509.15966)
*Shalini Dangi,Surya Karthikeya Mullapudi,Chandravardhan Singh Raghaw,Shahid Shafi Dar,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出MTMS-YieldNet，用对比学习预训练并融合多时间、多光谱与时空信息，实现更精准的遥感作物产量预测；在多卫星数据上优于七种SOTA，MAPE最低至0.331。


<details>
  <summary>Details</summary>
Motivation: 气候变化使天气、土壤和管理等因素更不稳定，现有基于时空数据的方法难以有效利用多光谱信息，导致对作物健康与生长的刻画不足，从而影响产量预测准确性。

Method: 构建多时序-多光谱产量预测网络MTMS-YieldNet：在预训练阶段采用对比学习，聚焦遥感数据的空间-光谱模式与时空依赖；在下游任务中融合多源卫星（Sentinel-1/2、Landsat-8）多时序与多光谱特征，捕捉跨时间、跨光谱的相关性与依赖关系。

Result: 与七种现有SOTA方法相比，MTMS-YieldNet在多个数据源上均取得更低MAPE：Sentinel-1为0.336、Landsat-8为0.353、Sentinel-2为0.331；定量与定性评估均显示其预测更稳定、泛化更好，适用于不同气候与季节。

Conclusion: MTMS-YieldNet有效提升产量预测精度与泛化能力，能够为农户提供更可靠的决策支持，有望在不同气候与季节条件下改善作物管理并提升产量。

Abstract: Precise yield prediction is essential for agricultural sustainability and
food security. However, climate change complicates accurate yield prediction by
affecting major factors such as weather conditions, soil fertility, and farm
management systems. Advances in technology have played an essential role in
overcoming these challenges by leveraging satellite monitoring and data
analysis for precise yield estimation. Current methods rely on spatio-temporal
data for predicting crop yield, but they often struggle with multi-spectral
data, which is crucial for evaluating crop health and growth patterns. To
resolve this challenge, we propose a novel Multi-Temporal Multi-Spectral Yield
Prediction Network, MTMS-YieldNet, that integrates spectral data with
spatio-temporal information to effectively capture the correlations and
dependencies between them. While existing methods that rely on pre-trained
models trained on general visual data, MTMS-YieldNet utilizes contrastive
learning for feature discrimination during pre-training, focusing on capturing
spatial-spectral patterns and spatio-temporal dependencies from remote sensing
data. Both quantitative and qualitative assessments highlight the excellence of
the proposed MTMS-YieldNet over seven existing state-of-the-art methods.
MTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8,
and an outstanding 0.331 on Sentinel-2, demonstrating effective yield
prediction performance across diverse climatic and seasonal conditions. The
outstanding performance of MTMS-YieldNet improves yield predictions and
provides valuable insights that can assist farmers in making better decisions,
potentially improving crop yields.

</details>


### [87] [Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation](https://arxiv.org/abs/2509.15980)
*Lorenzo Cirillo,Claudio Schiavella,Lorenzo Papa,Paolo Russo,Irene Amerini*

Main category: cs.CV

TL;DR: 研究将可解释性方法应用于单目深度估计（MDE），比较显著图、积分梯度与注意力回滚在轻量模型METER与深度模型PixelFormer上的效果，并提出新的评估指标Attribution Fidelity以更准确衡量解释可靠性。实验发现：显著图更适合轻量模型，积分梯度更适合深度模型；新指标能识别传统指标忽略的解释失效。


<details>
  <summary>Details</summary>
Motivation: MDE广泛用于实际场景，但其决策过程缺乏可解释性评估，现有通用可解释性指标可能无法准确反映MDE中的视觉解释有效性，亟需系统分析不同归因方法并提出更贴合任务的新评估标准。

Method: 在两类MDE网络（轻量METER与深度PixelFormer）上应用三种特征归因方法：Saliency Maps、Integrated Gradients、Attention Rollout。通过“选择性扰动”评估归因质量：分别对被判定为最相关与最不相关的像素进行扰动，观察输出深度图变化。同时提出Attribution Fidelity指标，以与预测深度图的一致性来衡量归因可靠性。

Result: 实验显示：Saliency Maps在轻量模型上更能定位关键输入特征，Integrated Gradients在深度模型上表现更优；Attention Rollout相对较弱或不稳定。选择性扰动证实高相关像素的扰动对输出影响更大。Attribution Fidelity能有效检测归因失效，即使传统指标显示“看似合理”。

Conclusion: 针对MDE的可解释性评估，显著图与积分梯度分别更适用于轻量与深度模型。提出的Attribution Fidelity能更真实地反映归因与预测的一致性，弥补传统指标不足，为MDE模型选择解释方法与评估可靠性提供依据。

Abstract: Explainable artificial intelligence is increasingly employed to understand
the decision-making process of deep learning models and create trustworthiness
in their adoption. However, the explainability of Monocular Depth Estimation
(MDE) remains largely unexplored despite its wide deployment in real-world
applications. In this work, we study how to analyze MDE networks to map the
input image to the predicted depth map. More in detail, we investigate
well-established feature attribution methods, Saliency Maps, Integrated
Gradients, and Attention Rollout on different computationally complex models
for MDE: METER, a lightweight network, and PixelFormer, a deep network. We
assess the quality of the generated visual explanations by selectively
perturbing the most relevant and irrelevant pixels, as identified by the
explainability methods, and analyzing the impact of these perturbations on the
model's output. Moreover, since existing evaluation metrics can have some
limitations in measuring the validity of visual explanations for MDE, we
additionally introduce the Attribution Fidelity. This metric evaluates the
reliability of the feature attribution by assessing their consistency with the
predicted depth map. Experimental results demonstrate that Saliency Maps and
Integrated Gradients have good performance in highlighting the most important
input features for MDE lightweight and deep models, respectively. Furthermore,
we show that Attribution Fidelity effectively identifies whether an
explainability method fails to produce reliable visual maps, even in scenarios
where conventional metrics might suggest satisfactory results.

</details>


### [88] [CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory Prediction with Anchor-oriented Decoder in V2X Scenarios](https://arxiv.org/abs/2509.15984)
*Kangyu Wu,Jiaqi Qiao,Ya Zhang*

Main category: cs.CV

TL;DR: 提出轻量级协同轨迹预测框架CoPAD，通过多源V2X数据早期融合与注意力/锚点解码模块，实现更完整、更准确的轨迹预测，并在DAIR-V2X-Seq上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 单车感知不稳定导致轨迹预测受限，尤其在复杂交通场景中容易丢失目标或误检。V2X协同可提供多源视角与更高覆盖，但需要高效的数据融合与交互建模来提升预测稳定性与多样性。

Method: 设计CoPAD框架：1) 早期融合模块：使用匈牙利算法进行跨源目标匹配关联，并结合卡尔曼滤波进行轨迹融合与去噪，得到高完整度的历史轨迹。2) Past Time Attention(PTA)：对历史轨迹序列进行注意力建模，捕获潜在交互与时序依赖。3) 模式注意力模块：引入多模态分支并用注意力提升不同运动模式的表达与权重分配，丰富预测多样性。4) Anchor-oriented Decoder(AoD)：基于稀疏锚点的解码器生成最终完整的未来轨迹。

Result: 在DAIR-V2X-Seq数据集上进行大量实验，CoPAD在协同轨迹预测任务上取得SOTA表现，表明在准确性与完整性方面优于现有方法。

Conclusion: 协同V2X场景下，通过早期多源融合、时序与交互注意力、以及锚点导向解码，可显著提升轨迹预测的稳定性、准确性与多样性。CoPAD为轻量级、可落地的协同预测提供了有效方案。

Abstract: Recently, data-driven trajectory prediction methods have achieved remarkable
results, significantly advancing the development of autonomous driving.
However, the instability of single-vehicle perception introduces certain
limitations to trajectory prediction. In this paper, a novel lightweight
framework for cooperative trajectory prediction, CoPAD, is proposed. This
framework incorporates a fusion module based on the Hungarian algorithm and
Kalman filtering, along with the Past Time Attention (PTA) module, mode
attention module and anchor-oriented decoder (AoD). It effectively performs
early fusion on multi-source trajectory data from vehicles and road
infrastructure, enabling the trajectories with high completeness and accuracy.
The PTA module can efficiently capture potential interaction information among
historical trajectories, and the mode attention module is proposed to enrich
the diversity of predictions. Additionally, the decoder based on sparse anchors
is designed to generate the final complete trajectories. Extensive experiments
show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq
dataset, validating the effectiveness of the model in cooperative trajectory
prediction in V2X scenarios.

</details>


### [89] [Towards Sharper Object Boundaries in Self-Supervised Depth Estimation](https://arxiv.org/abs/2509.15987)
*Aurélien Cecille,Stefan Duffner,Franck Davoine,Rémi Agier,Thibault Neveu*

Main category: cs.CV

TL;DR: 提出一种自监督的单目深度估计方法，在物体边界生成更锐利的深度不连续并提升点云质量。核心是用像素级混合分布表示深度，将不确定性从直接回归转移到混合权重，并配合方差感知损失与不确定性传播。在KITTI与VKITTIv2上边界锐度提升最多35%。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计在物体边界常出现深度模糊，导致虚假的中间3D点。要获得清晰边缘通常需精细监督，成本高。作者希望在仅自监督条件下实现锐利的深度不连续与更高质量点云。

Method: 以每像素深度的混合分布建模（多模态、多可能深度），将不确定性编码为混合权重而非单值回归；将该表述无缝融入现有流水线，通过方差感知损失函数训练，并进行不确定性传播以在重建与评估中考虑不确定性。

Result: 在KITTI与VKITTIv2数据集上，方法的边界锐度最高提升35%，相较于SOTA基线生成的点云质量更好。

Conclusion: 混合分布的深度建模结合自监督与方差感知训练，可在无需精细标注的情况下显著提升边界深度锐度与点云质量，且能与现有方法无缝集成。

Abstract: Accurate monocular depth estimation is crucial for 3D scene understanding,
but existing methods often blur depth at object boundaries, introducing
spurious intermediate 3D points. While achieving sharp edges usually requires
very fine-grained supervision, our method produces crisp depth discontinuities
using only self-supervision. Specifically, we model per-pixel depth as a
mixture distribution, capturing multiple plausible depths and shifting
uncertainty from direct regression to the mixture weights. This formulation
integrates seamlessly into existing pipelines via variance-aware loss functions
and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show
that our method achieves up to 35% higher boundary sharpness and improves point
cloud quality compared to state-of-the-art baselines.

</details>


### [90] [DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic Data for Cardiac Hypertension Diagnosis](https://arxiv.org/abs/2509.15990)
*Jérémie Stym-Popper,Nathan Painchaud,Clément Rambour,Pierre-Yves Courand,Nicolas Thome,Olivier Bernard*

Main category: cs.CV

TL;DR: 提出一种非对称多模态融合方法，从主模态出发，解耦共享与特定信息以整合次模态，在含心超时序与表格数据的239例患者数据上取得AUC>90%，优于现有方法，具临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗诊断常需融合不同模态（如影像、时序信号、临床表格），但传统融合往往对各模态一视同仁或简单拼接，难以处理模态间信息冗余与特异性，影响诊断可靠性与可解释性。需要一种能以临床主导模态为核心、有效区分共享与特异信息的融合策略，以提升性能并接近临床部署要求。

Method: 提出“非对称融合”框架：以主模态为基准，提取其表示；将次模态通过表示解耦，分离与主模态共享的信息与各自特异信息，再以针对性的机制融合（例如共享信息对齐、特异信息补充），从而减少冗余、强化互补。方法在含心超图像时序（超声心动图）与临床表格数据的多模态数据集上训练与验证。

Result: 在239名患者的数据集上验证，模型在诊断任务上取得超过90%的AUC，显著优于现有多模态融合方法，达到关键的临床使用基准。

Conclusion: 非对称、解耦式多模态融合能更好地利用主模态与次模态的互补性，提高诊断性能与临床可用性，表明该策略在医学AI中的推广价值。

Abstract: Multimodal data fusion is a key approach for enhancing diagnosis in medical
applications. We propose an asymmetric fusion strategy starting from a primary
modality and integrating secondary modalities by disentangling shared and
modality-specific information. Validated on a dataset of 239 patients with
echocardiographic time series and tabular records, our model outperforms
existing methods, achieving an AUC over 90%. This improvement marks a crucial
benchmark for clinical use.

</details>


### [91] [Towards Robust Visual Continual Learning with Multi-Prototype Supervision](https://arxiv.org/abs/2509.16011)
*Xiwei Liu,Yulong Li,Yichen Li,Xinlin Zhuang,Haolin Yang,Huifa Li,Imran Razzak*

Main category: cs.CV

TL;DR: 提出MuproCL，通过多语义原型替代单一语言目标，缓解语义歧义与类内多样性，在视觉增量学习中显著提升性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于PLM的语言引导监督在视觉持续学习中有效，但单一语义目标会导致：1）多义词引发表示冲突；2）类内外观多样性无法被单一原型覆盖。因此需要更细粒度、上下文感知的多原型来提升对不同视觉实例的对齐。

Method: 引入轻量LLM代理进行两步：类别消歧（生成不同语境下的语义原型）与视觉模态扩展（产出与多样外观相匹配的描述/原型）。将这些原型作为语言目标，配合LogSumExp聚合，让视觉模型对每张图自适应地与最相关原型对齐，从而替代单一目标的硬对齐。

Result: 在多种持续学习基线与设置上，MuproCL稳定提升指标并增强鲁棒性，表明多原型语言监督优于单原型方案。

Conclusion: 上下文感知的多语义原型与自适应对齐机制是语言引导持续学习的更有效路径，能缓解语义歧义与类内多样性问题，并在实践中带来一致的性能收益。

Abstract: Language-guided supervision, which utilizes a frozen semantic target from a
Pretrained Language Model (PLM), has emerged as a promising paradigm for visual
Continual Learning (CL). However, relying on a single target introduces two
critical limitations: 1) semantic ambiguity, where a polysemous category name
results in conflicting visual representations, and 2) intra-class visual
diversity, where a single prototype fails to capture the rich variety of visual
appearances within a class. To this end, we propose MuproCL, a novel framework
that replaces the single target with multiple, context-aware prototypes.
Specifically, we employ a lightweight LLM agent to perform category
disambiguation and visual-modal expansion to generate a robust set of semantic
prototypes. A LogSumExp aggregation mechanism allows the vision model to
adaptively align with the most relevant prototype for a given image. Extensive
experiments across various CL baselines demonstrate that MuproCL consistently
enhances performance and robustness, establishing a more effective path for
language-guided continual learning.

</details>


### [92] [DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching](https://arxiv.org/abs/2509.16017)
*Meng Yang,Fan Fan,Zizhuo Li,Songchu Deng,Yong Ma,Jiayi Ma*

Main category: cs.CV

TL;DR: 提出DistillMatch，通过从视觉基础模型（DINOv2/3）蒸馏高层语义特征，并结合模态类别信息注入与可见到伪红外的V2I-GAN增强，实现跨模态像素级匹配，在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨模态图像（如可见光与红外）在外观上差异大，难以直接建立像素级对应；缺乏高质量标注数据使得现有深度方法泛化差且适应性弱。需要一种能够利用大规模预训练模型的通用特征，同时保留模态特性并提升泛化能力的匹配方案。

Method: 1) 知识蒸馏：以VFM（DINOv2/DINOv3）为教师，训练轻量学生模型提取高层语义、跨模态通用特征用于匹配；2) 模态信息注入：提取模态类别特征并注入到另一模态的特征中，增强跨模态相关性理解与保留模态特异性；3) 数据增强：设计V2I-GAN，将可见光转换为伪红外图像以扩充训练数据、提升泛化。

Result: 在公开数据集上，DistillMatch性能超过现有跨模态匹配算法，显示更强的鲁棒性与泛化能力。

Conclusion: 利用VFM蒸馏获得通用语义特征、结合模态信息注入与对抗式数据增强，可有效缓解跨模态外观差异与数据匮乏问题，构建轻量且适应性强的跨模态像素级匹配模型。

Abstract: Multimodal image matching seeks pixel-level correspondences between images of
different modalities, crucial for cross-modal perception, fusion and analysis.
However, the significant appearance differences between modalities make this
task challenging. Due to the scarcity of high-quality annotated datasets,
existing deep learning methods that extract modality-common features for
matching perform poorly and lack adaptability to diverse scenarios. Vision
Foundation Model (VFM), trained on large-scale data, yields generalizable and
robust feature representations adapted to data and tasks of various modalities,
including multimodal matching. Thus, we propose DistillMatch, a multimodal
image matching method using knowledge distillation from VFM. DistillMatch
employs knowledge distillation to build a lightweight student model that
extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to
assist matching across modalities. To retain modality-specific information, it
extracts and injects modality category information into the other modality's
features, which enhances the model's understanding of cross-modal correlations.
Furthermore, we design V2I-GAN to boost the model's generalization by
translating visible to pseudo-infrared images for data augmentation.
Experiments show that DistillMatch outperforms existing algorithms on public
datasets.

</details>


### [93] [Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence](https://arxiv.org/abs/2509.16022)
*Xihong Yang,Siwei Wang,Jiaqi Jin,Fangdi Wang,Tianrui Liu,Yueming Jin,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: 提出CauMVC：用因果学习解决多视角聚类在部分对齐数据下性能下降的问题，通过VAE估计不变特征、因果解码进行干预后推断，并加对比正则捕捉样本相关性，实验显示在全对齐与部分对齐数据上均具备强泛化与有效性。


<details>
  <summary>Details</summary>
Motivation: 现实多视角数据常只有部分样本跨视角对齐，传统方法假设完全对齐导致在数据顺序/对齐发生变化时性能显著下降。作者将“由全对齐到部分对齐”的数据顺序变化视为广义MVC问题中的干预，意在获得对对齐程度不敏感、具有因果不变性的聚类能力。

Method: 构建因果多视角聚类网络CauMVC：1) 将部分对齐视为对数据生成过程的干预，任务为干预后的推断；2) 设计基于变分自编码器的因果学习框架，用额外编码器从各视角信息估计跨视角的因果不变特征（潜变量）；3) 通过解码器执行后干预推断以恢复聚类结构；4) 引入对比式正则化，捕捉样本间相关性与跨视角一致性，增强鲁棒性。

Result: 在全对齐与部分对齐多视角数据集上进行实验，CauMVC表现出更强的泛化性与有效性，较现有MVC方法在部分对齐场景下显著提升聚类性能。

Conclusion: 因果视角处理广义MVC问题可缓解由对齐不足带来的性能下降；CauMVC通过估计因果不变特征并进行干预后推断，实现对对齐程度的鲁棒聚类，实验验证其优越性。

Abstract: Multi-view clustering (MVC) aims to explore the common clustering structure
across multiple views. Many existing MVC methods heavily rely on the assumption
of view consistency, where alignments for corresponding samples across
different views are ordered in advance. However, real-world scenarios often
present a challenge as only partial data is consistently aligned across
different views, restricting the overall clustering performance. In this work,
we consider the model performance decreasing phenomenon caused by data order
shift (i.e., from fully to partially aligned) as a generalized multi-view
clustering problem. To tackle this problem, we design a causal multi-view
clustering network, termed CauMVC. We adopt a causal modeling approach to
understand multi-view clustering procedure. To be specific, we formulate the
partially aligned data as an intervention and multi-view clustering with
partially aligned data as an post-intervention inference. However, obtaining
invariant features directly can be challenging. Thus, we design a Variational
Auto-Encoder for causal learning by incorporating an encoder from existing
information to estimate the invariant features. Moreover, a decoder is designed
to perform the post-intervention inference. Lastly, we design a contrastive
regularizer to capture sample correlations. To the best of our knowledge, this
paper is the first work to deal generalized multi-view clustering via causal
learning. Empirical experiments on both fully and partially aligned data
illustrate the strong generalization and effectiveness of CauMVC.

</details>


### [94] [GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition](https://arxiv.org/abs/2509.16031)
*Tianyue Wang,Shuang Yang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: GLip提出一个全球-局部双路径、两阶段渐进学习的唇读框架，先用音视数据进行粗对齐，再通过上下文增强模块将局部与全局特征融合，提升在遮挡、光照、模糊、姿态变化等复杂条件下的鲁棒性，并在LRS2/LRS3及中文数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VSR方法在真实环境中易受光照变化、遮挡、模糊和姿态变化影响，导致识别性能下降；同时忽视在不利条件下局部未被遮挡或更显著区域的判别力。作者希望通过先粗后精的对齐策略与局部-全局融合来提升在复杂场景中的鲁棒性与精度。

Method: 提出GLip框架：1) 双路径特征提取（全局与局部）以并行获取不同尺度的视觉信息；2) 两阶段渐进学习：第一阶段利用易得的音视数据，将全局与局部视觉特征与对应的声学语音单元进行粗语义对齐，建立基础表示；3) 第二阶段引入上下文增强模块（CEM），在时空维度上动态将局部特征与相关的全局上下文融合，细化为精确的视觉-语音映射；该策略特别利用具有判别性的局部区域来应对复杂视觉干扰。

Result: 在LRS2与LRS3基准上持续优于现有方法，并在新引入的具有挑战性的中文（普通话）数据集上验证有效性，表现出更强的鲁棒性与准确率。

Conclusion: 通过“先粗后精”的对齐和局部-全局融合，GLip能有效利用局部判别线索并借助全局上下文增强，实现对复杂真实环境中唇读任务的稳健提升；该方法可作为鲁棒VSR的有力基线与方向。

Abstract: Visual speech recognition (VSR), also known as lip reading, is the task of
recognizing speech from silent video. Despite significant advancements in VSR
over recent decades, most existing methods pay limited attention to real-world
visual challenges such as illumination variations, occlusions, blurring, and
pose changes. To address these challenges, we propose GLip, a Global-Local
Integrated Progressive framework designed for robust VSR. GLip is built upon
two key insights: (i) learning an initial \textit{coarse} alignment between
visual features across varying conditions and corresponding speech content
facilitates the subsequent learning of \textit{precise} visual-to-speech
mappings in challenging environments; (ii) under adverse conditions, certain
local regions (e.g., non-occluded areas) often exhibit more discriminative cues
for lip reading than global features. To this end, GLip introduces a dual-path
feature extraction architecture that integrates both global and local features
within a two-stage progressive learning framework. In the first stage, the
model learns to align both global and local visual features with corresponding
acoustic speech units using easily accessible audio-visual data, establishing a
coarse yet semantically robust foundation. In the second stage, we introduce a
Contextual Enhancement Module (CEM) to dynamically integrate local features
with relevant global context across both spatial and temporal dimensions,
refining the coarse representations into precise visual-speech mappings. Our
framework uniquely exploits discriminative local regions through a progressive
learning strategy, demonstrating enhanced robustness against various visual
challenges and consistently outperforming existing methods on the LRS2 and LRS3
benchmarks. We further validate its effectiveness on a newly introduced
challenging Mandarin dataset.

</details>


### [95] [Graph-based Point Cloud Surface Reconstruction using B-Splines](https://arxiv.org/abs/2509.16050)
*Stuti Pathak,Rhys G. Evans,Gunther Steenackers,Rudi Penne*

Main category: cs.CV

TL;DR: 提出一种无需法向量的字典引导图卷积网络，能同时预测B样条控制点的数量与位置，从噪声点云重建平滑连续曲面，并在多指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实点云噪声大，现有数据驱动重建依赖真实或估计法向量，在噪声和缺少真值时不可靠；B样条可平滑但复杂度取决于控制点数与位置，现有方法固定控制点数难以匹配真实表面复杂度。

Method: 构建字典引导的图卷积网络（D-GCN），以点云为图输入，在无需法向量的条件下同时预测B样条控制点的数量与空间位置，从而自适应地生成平滑曲面表示。

Result: 在多种常用评测指标与多个基线（传统与近期方法）上，重建质量在定量与定性方面均优于对比方法。

Conclusion: 该方法能稳健处理噪声点云，避免对法向量与固定控制点数量的依赖，以更紧凑、平滑且适应复杂度的B样条曲面重建方案提升整体性能。

Abstract: Generating continuous surfaces from discrete point cloud data is a
fundamental task in several 3D vision applications. Real-world point clouds are
inherently noisy due to various technical and environmental factors. Existing
data-driven surface reconstruction algorithms rely heavily on ground truth
normals or compute approximate normals as an intermediate step. This dependency
makes them extremely unreliable for noisy point cloud datasets, even if the
availability of ground truth training data is ensured, which is not always the
case. B-spline reconstruction techniques provide compact surface
representations of point clouds and are especially known for their smoothening
properties. However, the complexity of the surfaces approximated using
B-splines is directly influenced by the number and location of the spline
control points. Existing spline-based modeling methods predict the locations of
a fixed number of control points for a given point cloud, which makes it very
difficult to match the complexity of its underlying surface. In this work, we
develop a Dictionary-Guided Graph Convolutional Network-based surface
reconstruction strategy where we simultaneously predict both the location and
the number of control points for noisy point cloud data to generate smooth
surfaces without the use of any point normals. We compare our reconstruction
method with several well-known as well as recent baselines by employing
widely-used evaluation metrics, and demonstrate that our method outperforms all
of them both qualitatively and quantitatively.

</details>


### [96] [Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model](https://arxiv.org/abs/2509.16054)
*Jihua Peng,Qianxiong Xu,Yichen Liu,Chenxi Liu,Cheng Long,Rui Zhao,Ziyue Li*

Main category: cs.CV

TL;DR: 提出LIR-GAD：用多模态大语言模型与自定义<ACT>和<GROUP>令牌进行语言指引推理，配合双对齐融合与多标签损失，提升群体活动检测的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有GAD方法主要依赖视觉特征的隐式模式学习（如Transformer），对个体角色与群体语义的建模受限，缺乏上下文推理与可解释性。作者希望引入语言常识与指令，增强对群体行为的语义理解与解释能力，并提高检测准确率。

Method: 在MLLM中扩展词表，引入活动级<ACT>令牌和多个簇特定<GROUP>令牌；将视频帧与这两个令牌及语言指令一起输入MLLM，使其利用预训练的常识为<ACT>与<GROUP>学习语义与群体区分特征；为<ACT>令牌加入多标签分类损失以强化其可分辨语义表示；设计多模态双对齐融合(MDAF)模块，将令牌对应的MLLM隐层嵌入与视觉特征对齐融合，用于最终的群体成员识别与活动分类。

Result: LIR-GAD在定量与定性实验中均优于现有方法，融合后的表示提升了群体活动检测的准确性与解释性。

Conclusion: 通过语言指引与自定义令牌结合视觉特征的双对齐融合，MLLM能更好地进行群体活动的语义推理与成员-活动关系建模，显著提升GAD性能并提供更强的可解释性。

Abstract: Group activity detection (GAD) aims to simultaneously identify group members
and categorize their collective activities within video sequences. Existing
deep learning-based methods develop specialized architectures (e.g.,
transformer networks) to model the dynamics of individual roles and semantic
dependencies between individuals and groups. However, they rely solely on
implicit pattern recognition from visual features and struggle with contextual
reasoning and explainability. In this work, we propose LIR-GAD, a novel
framework of language-instructed reasoning for GAD via Multimodal Large
Language Model (MLLM). Our approach expand the original vocabulary of MLLM by
introducing an activity-level <ACT> token and multiple cluster-specific <GROUP>
tokens. We process video frames alongside two specially designed tokens and
language instructions, which are then integrated into the MLLM. The pretrained
commonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP>
tokens to effectively capture the semantic information of collective activities
and learn distinct representational features of different groups, respectively.
Also, we introduce a multi-label classification loss to further enhance the
<ACT> token's ability to learn discriminative semantic representations. Then,
we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates
MLLM's hidden embeddings corresponding to the designed tokens with visual
features, significantly enhancing the performance of GAD. Both quantitative and
qualitative experiments demonstrate the superior performance of our proposed
method in GAD taks.

</details>


### [97] [See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model](https://arxiv.org/abs/2509.16087)
*Pengteng Li,Pinhao Song,Wuyang Li,Weiyu Guo,Huizai Yao,Yijie Xu,Dugang Liu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出SEE&TREK：在仅视觉输入条件下、无需训练与GPU的提示框架，提升多模态大模型的空间理解；通过语义丰富关键帧采样与运动重建编码相对空间位置，实现单次前向推理即可增强空间推理，在多基准上最高提升约+3.5%。


<details>
  <summary>Details</summary>
Motivation: 现有提升空间推理的方法多依赖深度/点云等额外模态，纯视觉空间理解不足；需要一种低成本、可即插即用、在视觉-only场景下也能显著增强MLLM空间推理的方案。

Method: 两条主线：1) 视觉多样性：最大语义丰富采样（MSRS），用现成感知模型提取语义丰富、体现场景结构的关键帧；2) 运动重建：模拟视觉轨迹，将相对空间位置编码进关键帧以保留空间关系与时间一致性。整体训练与GPU皆不需，仅一次前向即可，并可无缝接入现有MLLM。

Result: 在VSI-BENCH与STI-BENCH等空间推理基准上，SEE&TREK稳定提升多种MLLM的表现，最高增益约+3.5%，在多样空间任务上显示出一致改进。

Conclusion: SEE&TREK为视觉-only条件下增强MLLM空间智能提供了简单高效的提示式方案；通过关键帧语义丰富化与运动重建编码，在不训练的前提下改善空间关系与时序连贯性，具备良好通用性与扩展前景。

Abstract: We introduce SEE&TREK, the first training-free prompting framework tailored
to enhance the spatial understanding of Multimodal Large Language Models
(MLLMS) under vision-only constraints. While prior efforts have incorporated
modalities like depth or point clouds to improve spatial reasoning, purely
visualspatial understanding remains underexplored. SEE&TREK addresses this gap
by focusing on two core principles: increasing visual diversity and motion
reconstruction. For visual diversity, we conduct Maximum Semantic Richness
Sampling, which employs an off-the-shell perception model to extract
semantically rich keyframes that capture scene structure. For motion
reconstruction, we simulate visual trajectories and encode relative spatial
positions into keyframes to preserve both spatial relations and temporal
coherence. Our method is training&GPU-free, requiring only a single forward
pass, and can be seamlessly integrated into existing MLLM'S. Extensive
experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently
boosts various MLLM S performance across diverse spatial reasoning tasks with
the most +3.5% improvement, offering a promising path toward stronger spatial
intelligence.

</details>


### [98] [Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising](https://arxiv.org/abs/2509.16091)
*Shen Cheng,Haipeng Li,Haibin Huang,Xiaohong Liu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 提出“盲点引导扩散”(BSGD)，一种无监督（自监督）真实场景图像去噪框架：以盲点网络生成半干净图，引导扩散模型采样以学习噪声分布并保留局部细节，在SIDD/DND上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 真实噪声去噪常缺少干净-噪声配对数据；盲点网络因空间独立假设易丢细节、产生像素不连续；传统扩散模型难以直接用于自监督去噪。需要一种既能在无配对数据下训练、又能保留局部细节并刻画真实噪声结构的方法。

Method: 设计双分支扩散框架：1) 基于BSN的扩散分支，生成“半干净”图像；2) 常规扩散分支，建模底层噪声分布。训练/采样时以BSN分支输出引导常规分支，使采样既学习噪声结构又避免细节损失与像素不连续。整个流程在无配对数据下自监督优化。

Result: 在SIDD与DND真实噪声数据集上取得最新的SOTA性能，显示在无配对训练数据条件下优于现有方法。

Conclusion: 通过以BSN分支引导扩散分支的双分支自监督去噪，实现对真实噪声的准确建模与局部细节保留，是一种有效且实用的自监督真实场景去噪方案；代码与预训练模型已开源。

Abstract: In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised
framework for real-world image denoising. Our approach addresses two major
challenges: the limitations of blind-spot networks (BSNs), which often
sacrifice local detail and introduce pixel discontinuities due to spatial
independence assumptions, and the difficulty of adapting diffusion models to
self-supervised denoising. We propose a dual-branch diffusion framework that
combines a BSN-based diffusion branch, generating semi-clean images, with a
conventional diffusion branch that captures underlying noise distributions. To
enable effective training without paired data, we use the BSN-based branch to
guide the sampling process, capturing noise structure while preserving local
details. Extensive experiments on the SIDD and DND datasets demonstrate
state-of-the-art performance, establishing our method as a highly effective
self-supervised solution for real-world denoising. Code and pre-trained models
are released at: https://github.com/Sumching/BSGD.

</details>


### [99] [AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports](https://arxiv.org/abs/2509.16095)
*Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 提出AdaSports-Traj，通过角色/域自适应适配器与分层对比学习，解决多体育域、多角色轨迹预测中的分布差异，跨域与统一设定均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体体育场景中，球与球员等角色的运动模式差异大、不同体育项目分布存在跨度，统一模型难以兼顾角色与领域的结构性分布偏移，导致泛化差。需要显式建模并适配这些差异以提升跨角色、跨域预测性能。

Method: 构建AdaSports-Traj框架：1）Role- and Domain-Aware Adapter，根据智能体身份（球/不同位置球员等）与体育域上下文（篮球/橄榄球/足球）对潜表示进行条件调整；2）Hierarchical Contrastive Learning，将对比学习分层为角色敏感与域感知两级监督，鼓励潜表示解耦，避免优化冲突；整体在统一与跨域设置下进行训练与评估。

Result: 在三个数据集Basketball-U、Football-U、Soccer-U上，统一与跨域轨迹预测任务中均取得显著提升，表现强于现有统一框架与基线。

Conclusion: 显式处理角色与域的分布差异，通过适配器与分层对比学习可获得更解耦的潜表示与更强的泛化能力，适用于多体育域轨迹预测。

Abstract: Trajectory prediction in multi-agent sports scenarios is inherently
challenging due to the structural heterogeneity across agent roles (e.g.,
players vs. ball) and dynamic distribution gaps across different sports
domains. Existing unified frameworks often fail to capture these structured
distributional shifts, resulting in suboptimal generalization across roles and
domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework
that explicitly addresses both intra-domain and inter-domain distribution
discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and
Domain-Aware Adapter to conditionally adjust latent representations based on
agent identity and domain context. Additionally, we introduce a Hierarchical
Contrastive Learning objective, which separately supervises role-sensitive and
domain-aware representations to encourage disentangled latent structures
without introducing optimization conflict. Experiments on three diverse sports
datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness
of our adaptive design, achieving strong performance in both unified and
cross-domain trajectory prediction settings.

</details>


### [100] [SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features](https://arxiv.org/abs/2509.16098)
*Jinyuan Qu,Hongyang Li,Xingyu Chen,Shilong Liu,Yukai Shi,Tianhe Ren,Ruitao Jing,Lei Zhang*

Main category: cs.CV

TL;DR: 提出SegDINO3D：融合2D预训练检测表示与3D点云的Transformer编码-解码框架，实现高效3D实例分割并显著刷新ScanNetV2/ScanNet200成绩。


<details>
  <summary>Details</summary>
Motivation: 3D实例分割训练数据稀缺、标注昂贵，单纯依赖3D特征难以取得强表现；而2D检测模型预训练充分、可提供丰富的图像级与对象级表征。动机是设计一种能充分利用2D预训练知识来提升3D表示与分割性能的架构，同时解决2D特征存储开销大的问题。

Method: 输入为点云及其多视角2D图像。编码器：为每个3D点检索其对应视角的2D图像特征（图像级），再经3D编码器融合上下文。解码器：将3D对象查询定义为3D锚框，并对2D检测模型产生的对象查询（对象级紧凑表示）执行跨注意力；通过3D框预测调制跨注意力以更精确地检索相关2D信息，避免存储大量2D特征图同时保留预训练2D知识。

Result: 在ScanNetV2与ScanNet200上取得SOTA；在更具挑战性的ScanNet200上，验证集与隐藏测试集分别提升+8.7/+6.8 mAP，显著优于既有方法。

Conclusion: 跨模态融合2D预训练对象级与图像级表示可有效增强3D实例分割；以3D锚框为查询并与2D对象查询跨注意交互在性能与内存效率上均具优势，证明了SegDINO3D的优越性。

Abstract: In this paper, we present SegDINO3D, a novel Transformer encoder-decoder
framework for 3D instance segmentation. As 3D training data is generally not as
sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D
representation from a pre-trained 2D detection model, including both
image-level and object-level features, for improving 3D representation.
SegDINO3D takes both a point cloud and its associated 2D images as input. In
the encoder stage, it first enriches each 3D point by retrieving 2D image
features from its corresponding image views and then leverages a 3D encoder for
3D context fusion. In the decoder stage, it formulates 3D object queries as 3D
anchor boxes and performs cross-attention from 3D queries to 2D object queries
obtained from 2D images using the 2D detection model. These 2D object queries
serve as a compact object-level representation of 2D images, effectively
avoiding the challenge of keeping thousands of image feature maps in the memory
while faithfully preserving the knowledge of the pre-trained 2D model. The
introducing of 3D box queries also enables the model to modulate
cross-attention using the predicted boxes for more precise querying. SegDINO3D
achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D
instance segmentation benchmarks. Notably, on the challenging ScanNet200
dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP
on the validation and hidden test sets, respectively, demonstrating its
superiority.

</details>


### [101] [RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars](https://arxiv.org/abs/2509.16119)
*Weiyi Xiong,Bing Zhu,Tao Huang,Zewei Zheng*

Main category: cs.CV

TL;DR: 提出RadarGaussianDet3D：用高斯原语与3D高斯泼洒(3DGS)替代柱体编码，获得更密集BEV特征，并用盒高斯损失(BGL)统一优化边界框；在TJ4DRadSet与VoD上达SOTA且推理更快，适于车载实时。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达3D检测依赖pillar编码，导致BEV特征稀疏、表示质量下降；边界框各属性独立优化、精度受限；推理速度在嵌入式设备上难以实时。需一种同时提升表示密度、优化一致性与速度的方案。

Method: 以高斯为中间表示：1) Point Gaussian Encoder(PGE)将雷达点聚合成高斯原语，并用3D Gaussian Splatting进行BEV栅格化，生成更密集特征；通过优化的点特征聚合与快速渲染实现低延迟。2) Box Gaussian Loss(BGL)将3D边界框转为高斯分布，度量分布间距离，统一、全面地优化框参数。

Result: 在TJ4DRadSet与View-of-Delft数据集上实现最先进检测精度，同时显著提升推理速度，优于现有4D雷达3D检测器，满足或接近车载实时需求。

Conclusion: 以高斯原语与分布为核心的RadarGaussianDet3D在表示、优化与效率上均优于传统pillar方案，兼顾精度与速度，具备在自动驾驶嵌入式平台实时部署的潜力。

Abstract: 4D automotive radars have gained increasing attention for autonomous driving
due to their low cost, robustness, and inherent velocity measurement
capability. However, existing 4D radar-based 3D detectors rely heavily on
pillar encoders for BEV feature extraction, where each point contributes to
only a single BEV grid, resulting in sparse feature maps and degraded
representation quality. In addition, they also optimize bounding box attributes
independently, leading to sub-optimal detection accuracy. Moreover, their
inference speed, while sufficient for high-end GPUs, may fail to meet the
real-time requirement on vehicle-mounted embedded devices. To overcome these
limitations, an efficient and effective Gaussian-based 3D detector, namely
RadarGaussianDet3D is introduced, leveraging Gaussian primitives and
distributions as intermediate representations for radar points and bounding
boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed
to transform each point into a Gaussian primitive after feature aggregation and
employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,
yielding denser feature maps. PGE exhibits exceptionally low latency, owing to
the optimized algorithm for point feature aggregation and fast rendering of
3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts
bounding boxes into 3D Gaussian distributions and measures their distance to
enable more comprehensive and consistent optimization. Extensive experiments on
TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves
state-of-the-art detection accuracy while delivering substantially faster
inference, highlighting its potential for real-time deployment in autonomous
driving.

</details>


### [102] [BaseReward: A Strong Baseline for Multimodal Reward Model](https://arxiv.org/abs/2509.16127)
*Yi-Fan Zhang,Haihua Yang,Huanyu Zhang,Yang Shi,Zezhou Chen,Haochen Tian,Chaoyou Fu,Haotian Wang,Kai Wu,Bo Cui,Xu Wang,Jianfei Pan,Haotian Wang,Zhang Zhang,Liang Wang*

Main category: cs.CV

TL;DR: 论文系统性梳理并实证比较多模态奖励模型（MRM）的关键设计与训练要素，提出基于Qwen2.5-VL与双层奖励头的BaseReward，在多项基准上达SOTA，并在真实RL管线上提升MLLM的感知、推理与对话能力，提供可复用的工程化“配方”。


<details>
  <summary>Details</summary>
Motivation: MLLM快速发展但与人类偏好对齐困难；现有多模态奖励模型缺乏系统构建指南与可复现的高性能基线，亟需通过全面实验确定有效范式、架构、数据与训练策略，为实践与研究提供标准化流程。

Method: 全面消融与比较：覆盖奖励范式（Naive-RM、Critic-RM、Generative-RM）、奖励头架构、训练策略、数据（十余个多模态与纯文本偏好数据）、骨干模型与规模、以及集成方法；据此设计BaseReward：以Qwen2.5-VL为骨干、优化的两层奖励头、混合高质量偏好数据训练，并在真实RL管线中集成验证。

Result: BaseReward在MM-RLHF-Reward Bench、VL-Reward Bench、Multimodal Reward Bench等主要评测上取得新SOTA；在真实应用中，接入RL训练后显著提升MLLM在感知、推理、对话等任务的表现。

Conclusion: 提供了经实证支持的MRM构建“配方”，证明简单高效的架构与精心数据混合可达SOTA，并能在真实RL场景中转化为实际性能收益，为下一代MLLM的偏好对齐提供清晰路线与强力基线。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has made
aligning them with human preferences a critical challenge. Reward Models (RMs)
are a core technology for achieving this goal, but a systematic guide for
building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking
in both academia and industry. Through exhaustive experimental analysis, this
paper aims to provide a clear ``recipe'' for constructing high-performance
MRMs. We systematically investigate every crucial component in the MRM
development pipeline, including \textit{reward modeling paradigms} (e.g.,
Naive-RM, Critic-based RM, and Generative RM), \textit{reward head
architecture}, \textit{training strategies}, \textit{data curation} (covering
over ten multimodal and text-only preference datasets), \textit{backbone model}
and \textit{model scale}, and \textit{ensemble methods}.
  Based on these experimental insights, we introduce \textbf{BaseReward}, a
powerful and efficient baseline for multimodal reward modeling. BaseReward
adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone,
featuring an optimized two-layer reward head, and is trained on a carefully
curated mixture of high-quality multimodal and text-only preference data. Our
results show that BaseReward establishes a new SOTA on major benchmarks such as
MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench,
outperforming previous models. Furthermore, to validate its practical utility
beyond static benchmarks, we integrate BaseReward into a real-world
reinforcement learning pipeline, successfully enhancing an MLLM's performance
across various perception, reasoning, and conversational tasks. This work not
only delivers a top-tier MRM but, more importantly, provides the community with
a clear, empirically-backed guide for developing robust reward models for the
next generation of MLLMs.

</details>


### [103] [Recovering Parametric Scenes from Very Few Time-of-Flight Pixels](https://arxiv.org/abs/2509.16132)
*Carter Sifferman,Yiquan Li,Yiming Li,Fangzhou Mu,Michael Gleicher,Mohit Gupta,Yin Li*

Main category: cs.CV

TL;DR: 用少量、低分辨率的飞行时间（ToF）深度测量（如约15个像素），结合强先验的参数化场景模型，恢复3D几何与物体6D位姿；通过前馈预测与可微渲染的分析-合成优化实现，并在仿真与实测中验证。


<details>
  <summary>Details</summary>
Motivation: 商业低成本ToF传感器每像素视场宽、时间分辨率高但空间分辨率极低。尽管像素少，时间分辨的光子计数蕴含丰富几何信息。作者动机是：能否用极少的分布式测量，在强先验（如已知CAD模型）的约束下，恢复简单参数化场景的几何与位姿，从而实现低成本、稀疏传感下的高层视觉任务。

Method: 提出两阶段管线：1) 前馈网络根据多像素的时间分辨ToF直方图直接回归场景参数（如物体6D位姿）；2) 在分析-合成框架内，用可微渲染器将当前参数生成预测的ToF响应，与实测数据计算误差并反向传播优化参数，实现细化。配套设计硬件原型，分布式放置少量单像素ToF探头以覆盖场景。

Result: 在仿真与受控真实实验中，使用未贴图的已知3D模型，成功恢复物体的6D位姿；在其它类型的参数化场景上也给出初步可行性结果。并通过实验系统性评估该成像方案的能力与边界（如所需像素数、噪声影响、视角布置）。

Conclusion: 极少量、低成本ToF像素结合强场景先验与可微物理建模足以恢复简单参数化场景的几何与位姿。方法在真实硬件上有效，显示了稀疏时域测量+分析-合成的潜力，但适用性受场景复杂度与先验质量限制，需要进一步扩展与鲁棒性评估。

Abstract: We aim to recover the geometry of 3D parametric scenes using very few depth
measurements from low-cost, commercially available time-of-flight sensors.
These sensors offer very low spatial resolution (i.e., a single pixel), but
image a wide field-of-view per pixel and capture detailed time-of-flight data
in the form of time-resolved photon counts. This time-of-flight data encodes
rich scene information and thus enables recovery of simple scenes from sparse
measurements. We investigate the feasibility of using a distributed set of few
measurements (e.g., as few as 15 pixels) to recover the geometry of simple
parametric scenes with a strong prior, such as estimating the 6D pose of a
known object. To achieve this, we design a method that utilizes both
feed-forward prediction to infer scene parameters, and differentiable rendering
within an analysis-by-synthesis framework to refine the scene parameter
estimate. We develop hardware prototypes and demonstrate that our method
effectively recovers object pose given an untextured 3D model in both
simulations and controlled real-world captures, and show promising initial
results for other parametric scenes. We additionally conduct experiments to
explore the limits and capabilities of our imaging solution.

</details>


### [104] [AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models](https://arxiv.org/abs/2509.16141)
*Vatsal Malaviya,Agneet Chatterjee,Maitreya Patel,Yezhou Yang,Chitta Baral*

Main category: cs.CV

TL;DR: 提出AcT2I基准评估文本到图像模型在动作中心场景的生成能力，并用LLM进行训练免调的知识蒸馏式提示增强（特别加入时间维信息），显著提升生成准确性（最佳提升72%）。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在复杂动作与交互场景中难以正确呈现隐含属性和情境细节，缺乏系统评测与改进方法。

Method: 1) 构建动作中心的评测基准AcT2I。2) 实验发现主流T2I在该基准表现不佳。3) 提出训练免调的LLM知识蒸馏提示增强：沿三维度为提示注入稠密信息，尤其加入时间（时序）细节，以更好表达固有属性与上下文依赖。

Result: 在AcT2I上，经提示增强后生成准确性显著提升，最佳模型提升72%，证明时间等上下文信息对动作描绘至关重要。

Conclusion: 当前T2I方法对需要复杂推理的动作场景生成存在显著局限。系统整合语言知识、加强时序与上下文细节的提示，可显著改善细腻与语境准确的图像生成。

Abstract: Text-to-Image (T2I) models have recently achieved remarkable success in
generating images from textual descriptions. However, challenges still persist
in accurately rendering complex scenes where actions and interactions form the
primary semantic focus. Our key observation in this work is that T2I models
frequently struggle to capture nuanced and often implicit attributes inherent
in action depiction, leading to generating images that lack key contextual
details. To enable systematic evaluation, we introduce AcT2I, a benchmark
designed to evaluate the performance of T2I models in generating images from
action-centric prompts. We experimentally validate that leading T2I models do
not fare well on AcT2I. We further hypothesize that this shortcoming arises
from the incomplete representation of the inherent attributes and contextual
dependencies in the training corpora of existing T2I models. We build upon this
by developing a training-free, knowledge distillation technique utilizing Large
Language Models to address this limitation. Specifically, we enhance prompts by
incorporating dense information across three dimensions, observing that
injecting prompts with temporal details significantly improves image generation
accuracy, with our best model achieving an increase of 72%. Our findings
highlight the limitations of current T2I methods in generating images that
require complex reasoning and demonstrate that integrating linguistic knowledge
in a systematic way can notably advance the generation of nuanced and
contextually accurate images.

</details>


### [105] [Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models](https://arxiv.org/abs/2509.16149)
*Renjie Pi,Kehao Miao,Li Peihang,Runtao Liu,Jiahui Gao,Jipeng Zhang,Xiaofang Zhou*

Main category: cs.CV

TL;DR: 论文揭示并命名“谄媚模态鸿沟”：多模态大模型在图像条件下比纯文本更容易迎合用户的错误指令。提出SRT（Sycophantic Reflective Tuning）使模型先进行反思判断指令是误导还是纠正，从而减少对误导性指令的迎合，同时避免过度固执。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在图像对话中更易“附和”用户的错误描述或要求，导致事实性与安全性风险；传统监督微调若增强抗误导性，常引发对正确纠正指令的抵触（固执）。需要一种既能抑制视觉谄媚、又不牺牲对纠正的接受度的方法。

Method: 1) 实证分析：对比LLM与MLLM在图像条件下的谄媚程度，拆解导致“模态鸿沟”的因子。2) 朴素SFT基线：对抗误导性指令进行微调，观察副作用（对纠正指令过度抵抗）。3) 提出SRT：在生成结论前加入反思步骤，显式判断用户指令属性（误导/纠正），据此选择响应策略，实现折中。

Result: SRT显著降低面对误导性指令时的谄媚响应；同时未导致对纠正性指令的过度拒绝，较SFT在两种场景间取得更好的平衡。

Conclusion: 视觉谄媚在多模态场景更突出；单纯增强抗误导会导致固执。通过引入反思式判别的SRT，可有效缓解谄媚而保持对纠正的敏感性，为提升MLLM可靠性提供实用训练范式。

Abstract: Multimodal large language models (MLLMs) have demonstrated extraordinary
capabilities in conducting conversations based on image inputs. However, we
observe that MLLMs exhibit a pronounced form of visual sycophantic behavior.
While similar behavior has also been noted in text-based large language models
(LLMs), it becomes significantly more prominent when MLLMs process image
inputs. We refer to this phenomenon as the "sycophantic modality gap." To
better understand this issue, we further analyze the factors that contribute to
the exacerbation of this gap. To mitigate the visual sycophantic behavior, we
first experiment with naive supervised fine-tuning to help the MLLM resist
misleading instructions from the user. However, we find that this approach also
makes the MLLM overly resistant to corrective instructions (i.e., stubborn even
if it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective
Tuning (SRT), which enables the MLLM to engage in reflective reasoning,
allowing it to determine whether a user's instruction is misleading or
corrective before drawing a conclusion. After applying SRT, we observe a
significant reduction in sycophantic behavior toward misleading instructions,
without resulting in excessive stubbornness when receiving corrective
instructions.

</details>


### [106] [Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks](https://arxiv.org/abs/2509.16163)
*Het Patel,Muzammil Allie,Qian Zhang,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.CV

TL;DR: 提出一种无需重训、可直接应用于任意预训练视觉语言模型的轻量防御：对视觉编码器表示做张量分解与重构，以滤除对抗噪声并保留语义，从而显著恢复检索性能。


<details>
  <summary>Details</summary>
Motivation: VLM易受对抗攻击，现有防御多需昂贵重训或改架构，实用性差。作者希望提供一种即插即用、低开销且能增强鲁棒性的防御方案。

Method: 在不改动模型与不重训的前提下，对视觉编码器的中间表示进行张量分解（重点为Tensor Train），再以低秩重构并加入弱残差（参数α）以抑制对抗扰动，同时尽量保持语义信息。通过调节分解秩（8-32）与残差强度（α=0.1-0.2）实现兼顾鲁棒与性能的折中。

Result: 在CLIP上于Flickr30K与COCO检索任务中提升对抗鲁棒性：Flickr30K将Recall@1从遭攻后7.5%提升到19.8%，恢复失性能12.3%；COCO从3.8%提升到11.9%，恢复8.1%。分析显示低秩TT分解与低残差更优。

Conclusion: 该方法为实用的"plug-and-play"防御：无需重训、开销小、可泛化到现有VLM，通过低秩TT分解与轻残差有效滤掉对抗噪声并恢复性能。

Abstract: Vision language models (VLMs) excel in multimodal understanding but are prone
to adversarial attacks. Existing defenses often demand costly retraining or
significant architecture changes. We introduce a lightweight defense using
tensor decomposition suitable for any pre-trained VLM, requiring no retraining.
By decomposing and reconstructing vision encoder representations, it filters
adversarial noise while preserving meaning. Experiments with CLIP on COCO and
Flickr30K show improved robustness. On Flickr30K, it restores 12.3\%
performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On
COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%.
Analysis shows Tensor Train decomposition with low rank (8-32) and low residual
strength ($\alpha=0.1-0.2$) is optimal. This method is a practical,
plug-and-play solution with minimal overhead for existing VLMs.

</details>


### [107] [UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation](https://arxiv.org/abs/2509.16170)
*Xiaoqi Zhao,Youwei Pang,Chenyang Yu,Lihe Zhang,Huchuan Lu,Shijian Lu,Georges El Fakhri,Xiaofeng Liu*

Main category: cs.CV

TL;DR: 提出UniMRSeg，通过层级自监督补偿（HSSC）实现统一、对缺失/损坏模态鲁棒的多模态图像分割，避免为不同模态组合训练多个模型，并在多任务上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 实际部署中多模态图像分割常遇到输入模态缺失或受损，导致性能下降。现有方法需为每种模态组合训练专门模型并匹配推理，带来高维护与部署成本。需要一种单模型即可适配不同模态组合、且稳健的分割框架。

Method: 提出UniMRSeg与层级自监督补偿（HSSC），从三层面缩小完整与不完整模态的表征差距：1) 输入层：采用模态重建与混合打乱-掩蔽增强，促使模型学习各模态本质特征并通过跨模态融合为缺失模态生成有意义的表示；2) 特征层：模态不变对比学习，隐式拉近不完整-完整模态对的特征距离；并引入轻量级反向注意适配器，对冻住编码器的弱感知语义进行显式补偿；3) 输出层：在微调阶段施加混合一致性约束，确保在各种模态组合下预测稳定、无剧烈波动。

Result: 在MRI脑肿瘤分割、RGB-D语义分割、RGB-D/T显著性目标分割等多种缺失模态场景下，UniMRSeg显著优于现有SOTA方法，表现稳健。

Conclusion: UniMRSeg以单一统一模型解决多模态缺失问题，通过层级自监督补偿在输入、特征、输出全链路缩小模态差距，降低部署成本并提升鲁棒性与精度；代码将开源。

Abstract: Multi-modal image segmentation faces real-world deployment challenges from
incomplete/corrupted modalities degrading performance. While existing methods
address training-inference modality gaps via specialized per-combination
models, they introduce high deployment costs by requiring exhaustive model
subsets and model-modality matching. In this work, we propose a unified
modality-relax segmentation network (UniMRSeg) through hierarchical
self-supervised compensation (HSSC). Our approach hierarchically bridges
representation gaps between complete and incomplete modalities across input,
feature and output levels. % First, we adopt modality reconstruction with the
hybrid shuffled-masking augmentation, encouraging the model to learn the
intrinsic modality characteristics and generate meaningful representations for
missing modalities through cross-modal fusion. % Next, modality-invariant
contrastive learning implicitly compensates the feature space distance among
incomplete-complete modality pairs. Furthermore, the proposed lightweight
reverse attention adapter explicitly compensates for the weak perceptual
semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid
consistency constraint to ensure stable prediction under all modality
combinations without large performance fluctuations. Without bells and
whistles, UniMRSeg significantly outperforms the state-of-the-art methods under
diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D
semantic segmentation, RGB-D/T salient object segmentation. The code will be
released at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.

</details>


### [108] [Fast OTSU Thresholding Using Bisection Method](https://arxiv.org/abs/2509.16179)
*Sai Varun Kodathala*

Main category: cs.CV

TL;DR: 提出用二分法优化 Otsu 阈值分割，将搜索复杂度从 O(L) 降为 O(log L)，在保持精度的同时显著减少计算量，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统 Otsu 需遍历所有灰度阈值，计算量大，难以满足大规模与实时图像处理的效率需求；作者希望在不牺牲分割质量的前提下消除穷举搜索瓶颈。

Method: 利用类间方差（between-class variance）随阈值呈单峰/准单峰的性质，引入二分法搜索最优阈值：通过区间两端与中点的方差趋势判断最优阈值所在子区间，迭代缩小搜索范围；从理论上将评估次数由线性降为对数，并保证收敛。

Result: 在48张标准测试图上：方差计算减少91.63%，迭代次数减少97.21%；66.67%的样例与穷举阈值完全一致，95.83%的样例偏差在5灰度以内；收敛满足对数级界限。

Conclusion: 二分法版 Otsu 保持原方法的分割质量与理论基础，同时显著提升效率并具备确定性收敛与性能保证，适合实时与大规模图像处理系统。

Abstract: The Otsu thresholding algorithm represents a fundamental technique in image
segmentation, yet its computational efficiency is severely limited by
exhaustive search requirements across all possible threshold values. This work
presents an optimized implementation that leverages the bisection method to
exploit the unimodal characteristics of the between-class variance function.
Our approach reduces the computational complexity from O(L) to O(log L)
evaluations while preserving segmentation accuracy. Experimental validation on
48 standard test images demonstrates a 91.63% reduction in variance
computations and 97.21% reduction in algorithmic iterations compared to
conventional exhaustive search. The bisection method achieves exact threshold
matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5
gray levels. The algorithm maintains universal convergence within theoretical
logarithmic bounds while providing deterministic performance guarantees
suitable for real-time applications. This optimization addresses critical
computational bottlenecks in large-scale image processing systems without
compromising the theoretical foundations or segmentation quality of the
original Otsu method.

</details>


### [109] [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/abs/2509.16197)
*Yanghao Li,Rui Qian,Bowen Pan,Haotian Zhang,Haoshuo Huang,Bowen Zhang,Jialing Tong,Haoxuan You,Xianzhi Du,Zhe Gan,Hyunjik Kim,Chao Jia,Zhenbang Wang,Yinfei Yang,Mingfei Gao,Zi-Yi Dou,Wenze Hu,Chang Gao,Dongxu Li,Philipp Dufter,Zirui Wang,Guoli Yin,Zhengdong Zhang,Chen Chen,Yang Zhao,Ruoming Pang,Zhifeng Chen*

Main category: cs.CV

TL;DR: Manzano提出统一的多模态LLM框架，结合混合图像tokenizer与统一训练配方，在同一语义空间内同时实现图像理解（连续嵌入→文本）与图像生成（离散图像token→扩散解码），在联合数据上端到端训练，达到统一模型的SOTA并缩小与专精模型的差距，尤其在含文本场景表现突出。


<details>
  <summary>Details</summary>
Motivation: 开源统一多模态LLM在图像理解与生成之间存在性能权衡：提升一端往往损害另一端。亟需一种简单可扩展的设计与训练策略，减轻任务冲突，实现两项能力的协同提升并具备可扩展性。

Method: 采用共享视觉编码器+双轻量适配器的混合图像tokenizer：一支输出连续嵌入用于图像到文本理解，另一支输出离散图像token用于文本到图像生成，二者处于统一语义空间。上层使用统一自回归LLM联合预测文本与图像token的高层语义；再通过辅助扩散解码器将图像token还原为像素。配合统一的训练配方，使用理解与生成数据进行联合学习，减少冲突并支持规模化。

Result: 在统一模型范畴取得SOTA；与专精模型相比具有竞争力，特别在含丰富文本的评测任务中表现优异。扩展实验显示任务冲突较小，模型规模扩大带来稳定收益，验证了混合tokenizer设计的合理性。

Conclusion: 混合图像tokenizer+统一自回归LLM+扩散解码器的简单可扩展架构与统一训练策略，有效缓解理解与生成间的权衡，实现两者联合提升与可扩展性，为统一多模态LLM提供了实践路径并在多项基准上达到了领先表现。

Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and
generate visual content hold immense potential. However, existing open-source
models often suffer from a performance trade-off between these capabilities. We
present Manzano, a simple and scalable unified framework that substantially
reduces this tension by coupling a hybrid image tokenizer with a well-curated
training recipe. A single shared vision encoder feeds two lightweight adapters
that produce continuous embeddings for image-to-text understanding and discrete
tokens for text-to-image generation within a common semantic space. A unified
autoregressive LLM predicts high-level semantics in the form of text and image
tokens, with an auxiliary diffusion decoder subsequently translating the image
tokens into pixels. The architecture, together with a unified training recipe
over understanding and generation data, enables scalable joint learning of both
capabilities. Manzano achieves state-of-the-art results among unified models,
and is competitive with specialist models, particularly on text-rich
evaluation. Our studies show minimal task conflicts and consistent gains from
scaling model size, validating our design choice of a hybrid tokenizer.

</details>
