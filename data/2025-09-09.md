<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 162]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Label Smoothing++: Enhanced Label Regularization for Training Neural Networks](https://arxiv.org/abs/2509.05307)
*Sachin Chhabra,Hemanth Venkateswara,Baoxin Li*

Main category: cs.CV

TL;DR: 提出Label Smoothing++：在保持目标类概率固定的同时，为非目标类分配可学习的非零概率，以减轻过度自信并保留类间关系。


<details>
  <summary>Details</summary>
Motivation: 传统一热标签训练易导致过度自信与过拟合。经典Label Smoothing将均匀噪声分配给所有非目标类，虽能正则化但忽略不同非目标类间的相关性，破坏类间结构信息。

Method: 固定目标类的标签概率（非零且不随训练改变），对所有非目标类设置可学习的概率分布，让网络基于数据自适应地学习非目标类间的相关性，从而形成更合理的软标签。实质是“半固定半学习”的标签正则化策略：目标类=固定，非目标类=学习；等价于在损失中引入对非目标类分布的可训练参数或由网络输出引导的软标签更新。

Result: 在多个数据集上进行广泛实验，显示相较于标准交叉熵与传统Label Smoothing，能显著降低过度自信，提升对非目标类关系的建模，并提高泛化性能。

Conclusion: Label Smoothing++在不破坏目标信息的前提下，利用可学习的非目标类分布保留并利用类间关系，缓解过度自信与过拟合，提升泛化能力。

Abstract: Training neural networks with one-hot target labels often results in
overconfidence and overfitting. Label smoothing addresses this issue by
perturbing the one-hot target labels by adding a uniform probability vector to
create a regularized label. Although label smoothing improves the network's
generalization ability, it assigns equal importance to all the non-target
classes, which destroys the inter-class relationships. In this paper, we
propose a novel label regularization training strategy called Label
Smoothing++, which assigns non-zero probabilities to non-target classes and
accounts for their inter-class relationships. Our approach uses a fixed label
for the target class while enabling the network to learn the labels associated
with non-target classes. Through extensive experiments on multiple datasets, we
demonstrate how Label Smoothing++ mitigates overconfident predictions while
promoting inter-class relationships and generalization capabilities.

</details>


### [2] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

TL;DR: 提出VILOD：一个用于目标检测标注的人机协同可视分析工具，结合t‑SNE特征投影、模型不确定性热图与模型状态视图，支持多样化交互式选样策略；实验显示其人机可视策略与自动不确定性采样基线表现相当，同时提升流程透明性与可管理性。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测受制于高成本、高耗时的精确标注数据获取；传统主动学习虽能减少标注但透明性不足、限制专家策略发挥，且可能遗漏与其查询策略不一致的关键信样本。需要一种将人类直觉与智能贯穿全流程、并以可视分析提升可解释性与可操作性的HITL方案。

Method: 构建VILOD可视交互标注系统：以t‑SNE对图像特征降维投影，配合不确定性热图与模型状态视图，展示数据分布、模型不确定性和训练进度；在迭代式HITL工作流中，支持用户理解模型与数据、参考AL建议、并灵活实施多种样本选择与标注策略；通过对比用例进行经验性评估。

Result: 对比用例表明：1) 可视化与交互使模型状态与数据特性更可解释，促进不同标注策略的实施（RQ1）；2) 在VILOD中采用的多种可视引导策略，其目标检测性能轨迹与自动化不确定性采样AL基线相竞争（RQ2）。

Conclusion: VILOD作为新型工具展示了在人机协作与主动学习结合下，提高OD标注流程的透明性、可管理性与潜在有效性；提供了实证证据与设计见解，支持以可视分析增强HITL‑AL工作流。

Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


### [3] [Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification](https://arxiv.org/abs/2509.05319)
*Zhengda Li*

Main category: cs.CV

TL;DR: 提出一种自适应知识蒸馏（AKD），将固定α改为可学习与动态计算，并用上下文感知模块重加权教师输出，在CIFAR-10上优于固定权重KD且收敛更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统KD用固定超参α平衡硬标签CE与软标签蒸馏损失，但训练过程中最优权衡随阶段/难度变化，固定α导致次优性能与不稳定。

Method: (1) 将α设为可学习参数，随训练优化；(2) 设计基于师生差异的动态α计算公式，反映学生与教师预测/表示的gap并据此自适应调节硬/软监督比重；(3) 提出Context-Aware Module（MLP+Attention）对教师类别概率做上下文敏感的类间重加权，使蒸馏信号更贴合样本与类别间关系；在ResNet-50→ResNet-18、CIFAR-10上评测。

Result: 与固定α的KD基线相比，AKD在CIFAR-10上取得更高精度，并表现出更平滑、更稳定的收敛曲线。

Conclusion: 自适应地学习与动态调整硬/软监督权重，并结合上下文感知的教师输出重加权，能提高学生模型的性能与训练稳定性；AKD优于传统固定权重KD。

Abstract: Knowledge distillation (KD) is a widely used technique to transfer knowledge
from a large teacher network to a smaller student model. Traditional KD uses a
fixed balancing factor alpha as a hyperparameter to combine the hard-label
cross-entropy loss with the soft-label distillation loss. However, a static
alpha is suboptimal because the optimal trade-off between hard and soft
supervision can vary during training.
  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.
First we try to make alpha as learnable parameter that can be automatically
learned and optimized during training. Then we introduce a formula to reflect
the gap between the student and the teacher to compute alpha dynamically,
guided by student-teacher discrepancies, and further introduce a Context-Aware
Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher
outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as
student demonstrate that our approach achieves superior accuracy compared to
fixed-weight KD baselines, and yields more stable convergence.

</details>


### [4] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

TL;DR: 提出Video2EEG-SPGN-Diffusion开源框架：用SEED-VD视频刺激生成与之对齐的个体化EEG，并发布>1000对视频-生成EEG与情感标签的数据，助力多模态训练与情感/BCI应用。


<details>
  <summary>Details</summary>
Motivation: 多模态模型缺少高质量、对齐的“视频-EEG-情感”数据，真实EEG采集成本高、隐私和规模受限；需要一种可扩展的工程流程与生成式方法，既实现视频-EEG对齐，又支持个体化情感脑信号的合成，促进情感计算与BCI研究。

Method: 构建Video2EEG-SPGN-Diffusion框架：1) 基于SEED-VD的视频刺激与EEG进行对齐的工程流水线；2) 采用自博弈图网络（SPGN）建模个体化特征；3) 将SPGN与扩散模型结合，在视频条件下生成62通道、200 Hz的EEG序列；4) 产出带有情感标签的配对数据用于多模态大模型训练与对齐。

Result: 发布一个包含1000+样本的新数据集：每个样本为SEED-VD视频、对应生成的62通道200 Hz EEG与情感标签；证明可实现视频-EEG对齐与个体化EEG生成的可行性，为多模态训练与数据增广提供资源。

Conclusion: 该框架与数据集为情感分析、数据增广及BCI提供新工具，工程与研究价值显著；可用于训练具EEG对齐能力的多模态大模型，推动视频-脑信号跨模态联学习与应用。

Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,
that leverages the SEED-VD dataset to generate a multimodal dataset of EEG
signals conditioned on video stimuli. Additionally, we disclose an engineering
pipeline for aligning video and EEG data pairs, facilitating the training of
multimodal large models with EEG alignment capabilities. Personalized EEG
signals are generated using a self-play graph network (SPGN) integrated with a
diffusion model. As a major contribution, we release a new dataset comprising
over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG
signals at 200 Hz and emotion labels, enabling video-EEG alignment and
advancing multimodal research. This framework offers novel tools for emotion
analysis, data augmentation, and brain-computer interface applications, with
substantial research and engineering significance.

</details>


### [5] [Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19](https://arxiv.org/abs/2509.05322)
*Pavithra Elumalai,Sudharsan Vijayaraghavan,Madhumita Mondal,Areejit Samal*

Main category: cs.CV

TL;DR: 论文探讨用三种以“边”为中心的网络测度（Forman-Ricci曲率FRC、Ollivier-Ricci曲率ORC、边介数EBC）对随机连线神经网络(RWNN)进行剪枝，以在保持胸片新冠分类性能的同时提升压缩率与推理效率；结果显示FRC在计算更高效的同时剪枝效果接近ORC。


<details>
  <summary>Details</summary>
Motivation: 现有工作表明网络拓扑影响深度学习效率与性能，但如何利用“边级”拓扑指标高效进行剪枝仍不清晰。ORC已被用于RWNN剪枝但计算昂贵，因此亟需评估计算更快的FRC及经典EBC在不同随机图生成器（ER/WS/BA）上的可行性与有效性。

Method: 以RWNN为基准，在COVID-19胸片分类任务上训练模型；分别计算网络中边的FRC、ORC、EBC重要性分数，基于阈值或排序选择性保留高分边，剪去其余连接；在三类随机图生成器（ER、WS、BA）下比较三种测度的剪枝表现，并评估压缩率与理论加速；同时分析剪枝后网络的模块度与全局效率，刻画模块分离与信息传递效率的权衡。

Result: 三种测度均可在不同生成器下实现有效压缩与理论加速；FRC剪枝在保持准确率、特异性、灵敏度方面与ORC相当，同时计算开销更低；结构分析显示剪枝改变了网络的模块度与全局效率，呈现出模块化增强与效率下降之间的权衡。

Conclusion: FRC是一种计算高效且实用的边级指标，能在RWNN中达到与ORC相近的剪枝效果并带来显著计算优势；结合结构指标分析，可在压缩与性能之间取得更优平衡，为基于拓扑的网络压缩提供了有前景的路径。

Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for
investigating the impact of network topology in deep learning by capturing how
different connectivity patterns impact both learning efficiency and model
performance. At the same time, they provide a natural framework for exploring
edge-centric network measures as tools for pruning and optimization. In this
study, we investigate three edge-centric network measures: Forman-Ricci
curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness
centrality (EBC), to compress RWNNs by selectively retaining important synapses
(or edges) while pruning the rest. As a baseline, RWNNs are trained for
COVID-19 chest x-ray image classification, aiming to reduce network complexity
while preserving performance in terms of accuracy, specificity, and
sensitivity. We extend prior work on pruning RWNN using ORC by incorporating
two additional edge-centric measures, FRC and EBC, across three network
generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and
Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the
pruning performance of the three measures in terms of compression ratio and
theoretical speedup. A central focus of our study is to evaluate whether FRC,
which is computationally more efficient than ORC, can achieve comparable
pruning effectiveness. Along with performance evaluation, we further
investigate the structural properties of the pruned networks through modularity
and global efficiency, offering insights into the trade-off between modular
segregation and network efficiency in compressed RWNNs. Our results provide
initial evidence that FRC-based pruning can effectively simplify RWNNs,
offering significant computational advantages while maintaining performance
comparable to ORC.

</details>


### [6] [Optical Music Recognition of Jazz Lead Sheets](https://arxiv.org/abs/2509.05329)
*Juan Carlos Martinez-Sevilla,Francesco Foscarin,Patricia Garcia-Iasci,David Rizo,Jorge Calvo-Zaragoza,Gerhard Widmer*

Main category: cs.CV

TL;DR: 提出首个针对手写爵士lead sheet的OMR数据集与模型：293张乐谱（163首）、2021谱表，配对Humdrum **kern与MusicXML；并给出从真值生成的合成图像与开源代码模型。模型在特定分词设计、利用合成数据与预训练带来优势。


<details>
  <summary>Details</summary>
Motivation: 现有OMR多聚焦印刷谱或不含和弦的元素，难以处理手写、噪声大且包含和弦标记的爵士lead sheet。缺少公开数据与针对性的模型，阻碍研究与应用。

Method: 1) 构建并发布手写爵士lead sheet数据集，含与Humdrum **kern及MusicXML对齐的真值，并生成合成乐谱图像；2) 设计适配lead sheet的标注与分词方案；3) 训练OMR模型，强调使用合成数据预训练/增强与预训练权重的迁移。

Result: 得到可用的OMR系统，能处理旋律与和弦；提供完整的开源数据、代码与模型；实验证明分词选择、合成数据与预训练带来性能提升（摘要未给出具体数值）。

Conclusion: 针对手写爵士lead sheet的OMR问题，数据与方法双管齐下：提供标准化基准与强力模型方案，证实合成数据与预训练的有效性，并为后续研究提供开源资源。

Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR)
for handwritten jazz lead sheets, a widely used musical score type that encodes
melody and chords. The task is challenging due to the presence of chords, a
score component not handled by existing OMR systems, and the high variability
and quality issues associated with handwritten images. Our contribution is
two-fold. We present a novel dataset consisting of 293 handwritten jazz lead
sheets of 163 unique pieces, amounting to 2021 total staves aligned with
Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score
images generated from the ground truth. The second contribution is the
development of an OMR model for jazz lead sheets. We discuss specific
tokenisation choices related to our kind of data, and the advantages of using
synthetic scores and pretrained models. We publicly release all code, data, and
models.

</details>


### [7] [RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness](https://arxiv.org/abs/2509.05333)
*Junghyun Park,Tuan Anh Nguyen,Dugki Min*

Main category: cs.CV

TL;DR: 提出RT-VLM：用“4-Clues”标注的合成数据+参数高效微调+两阶段自我反思推理，显著提升在多种领域移位下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实部署中模型遇到域偏移（低级统计、姿态/视角变化、遮挡、相邻类别混淆）会显著掉点；现有方法难以在多种移位上保持稳定鲁棒与可迁移的视觉理解。

Method: 1) 构建合成数据流水线，为每张图提供“4-Clues”：精确框、类别名、对象级细粒度描述、整图上下文描述；2) 基于该数据对Llama 3.2 11B Vision Instruct进行参数高效的有监督微调；3) 推理时采用两阶段“Re-Thinking”：先生成自身的四条线索，再将其作为证据进行自我审视与迭代纠正。

Result: 在针对单一域移位因素的多项鲁棒性基准上，RT-VLM稳定超越强基线，表现出更好的抗统计变化、姿态/视角变化、遮挡与类别混淆的能力。

Conclusion: 将结构化多模态证据（4-Clues）与显式自我批判循环结合，是迈向可靠、可迁移视觉理解的一条有效途径。

Abstract: Real world deployments often expose modern object recognition models to
domain shifts that precipitate a severe drop in accuracy. Such shifts encompass
(i) variations in low level image statistics, (ii) changes in object pose and
viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent
classes. To mitigate this degradation, we introduce the Re-Thinking Vision
Language Model (RT-VLM) framework. The foundation of this framework is a unique
synthetic dataset generation pipeline that produces images annotated with
"4-Clues": precise bounding boxes, class names, detailed object-level captions,
and a comprehensive context-level caption for the entire scene. We then perform
parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this
resource. At inference time, a two stage Re-Thinking scheme is executed: the
model first emits its own four clues, then re examines these responses as
evidence and iteratively corrects them. Across robustness benchmarks that
isolate individual domain shifts, RT-VLM consistently surpasses strong
baselines. These findings indicate that the integration of structured
multimodal evidence with an explicit self critique loop constitutes a promising
route toward reliable and transferable visual understanding.

</details>


### [8] [A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices](https://arxiv.org/abs/2509.05334)
*Diwen Huang*

Main category: cs.CV

TL;DR: 提出基于智能手机的视频分析系统，利用YOLOv5检测和卡尔曼滤波跟踪羽毛球轨迹，通过时空标定进行速度估计，在移动端自动计算杀球速度，成本低、易用，面向大众运动员。


<details>
  <summary>Details</summary>
Motivation: 传统速度与角度等运动表现指标采集设备昂贵、复杂，业余/大众难以获得。羽毛球普及度高但缺少大众可用的杀球速度测量工具，亟需低成本、易部署的方案。

Method: 在手机拍摄的视频上：1) 自训练YOLOv5检测羽毛球；2) 用卡尔曼滤波进行鲁棒轨迹跟踪；3) 结合时空尺度标定的基于视频的运动学速度估计，自动计算速度；4) 集成为直观的移动端应用。

Result: 系统能从普通视频中自动检测、跟踪并估计羽毛球杀球速度，实现低成本、自动化测量，满足业余和大众用户使用场景。

Conclusion: 所提方法将高水平表现分析民主化：以智能手机即可获得杀球速度反馈，便于不同水平球员自我分析与训练提升；为普及型运动科学测量提供可行路径。

Abstract: Performance metrics in sports, such as shot speed and angle, provide crucial
feedback for athlete development. However, the technology to capture these
metrics has historically been expensive, complex, and largely inaccessible to
amateur and recreational players. This paper addresses this gap in the context
of badminton, one of the world's most popular sports, by introducing a novel,
cost-effective, and user-friendly system for measuring smash speed using
ubiquitous smartphone technology. Our approach leverages a custom-trained
YOLOv5 model for shuttlecock detection, combined with a Kalman filter for
robust trajectory tracking. By implementing a video-based kinematic speed
estimation method with spatiotemporal scaling, the system automatically
calculates the shuttlecock's velocity from a standard video recording. The
entire process is packaged into an intuitive mobile application, democratizing
access to high-level performance analytics and empowering players at all levels
to analyze and improve their game.

</details>


### [9] [A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research](https://arxiv.org/abs/2509.05335)
*Zebo Xu,Shaoyun Yu,Mark Torrance,Guido Nottbusch,Nan Zhao,Zhenguang Cai*

Main category: cs.CV

TL;DR: 构建大型中文手写数据库与升级工具箱，揭示正字法与语音因素在字、部件、笔画层面对手写准备与执行的分层影响（字符最强→部件→笔画最弱）。


<details>
  <summary>Details</summary>
Motivation: 手写涉及多层级语言成分（语音、语义、正字法），但其在汉字的字/部件/笔画层面的具体作用缺乏系统证据；同时缺少可捕捉笔画级轨迹并可批处理的开放工具与大规模数据。

Method: 招募42名中文母语者，在听写条件下手写1200个汉字；使用升级的OpenHandWrite_Toolbox记录笔画级轨迹并批处理潜伏期、书写时长、笔压等指标；对字符、部件、笔画层面的多种正字法与语音预测因子进行多元回归分析。

Result: 多元回归显示：正字法预测因子影响准备与执行（在字、部件、笔画各层面均显著）；语音因素也在三层面影响执行。效应呈层级性递减：字符层面最强，其次部件层面，笔画层面最弱。

Conclusion: 汉字手写的准备与执行在部件与笔画层面同样受语言成分调制，体现自上而下的层级衰减模式；所构建数据库与升级工具箱为跨语言的（神经）心理语言学手写研究提供了可复用、可扩展的高分辨率资源。

Abstract: Understanding what linguistic components (e.g., phonological, semantic, and
orthographic systems) modulate Chinese handwriting at the character, radical,
and stroke levels remains an important yet understudied topic. Additionally,
there is a lack of comprehensive tools for capturing and batch-processing
fine-grained handwriting data. To address these issues, we constructed a
large-scale handwriting database in which 42 Chinese speakers for each
handwriting 1200 characters in a handwriting-to-dictation task. Additionally,
we enhanced the existing handwriting package and provided comprehensive
documentation for the upgraded OpenHandWrite_Toolbox, which can easily modify
the experimental design, capture the stroke-level handwriting trajectory, and
batch-process handwriting measurements (e.g., latency, duration, and
pen-pressure). In analysing our large-scale database, multiple regression
results show that orthographic predictors impact handwriting preparation and
execution across character, radical, and stroke levels. Phonological factors
also influence execution at all three levels. Importantly, these lexical
effects demonstrate hierarchical attenuation - they were most pronounced at the
character level, followed by the radical, and were weakest at the stroke
levels. These findings demonstrate that handwriting preparation and execution
at the radical and stroke levels are closely intertwined with linguistic
components. This database and toolbox offer valuable resources for future
psycholinguistic and neurolinguistic research on the handwriting of characters
and sub-characters across different languages.

</details>


### [10] [Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory](https://arxiv.org/abs/2509.05337)
*Younggeol Cho,Gokhan Solak,Olivia Nocentini,Marta Lorenzini,Andrea Fortuna,Arash Ajoudani*

Main category: cs.CV

TL;DR: 提出一种将动态图神经网络（DGNN）与LSTM解耦组合的前瞻性跌倒检测方法：DGNN负责三态步态分类（稳定/过渡/跌倒），LSTM负责人体运动预测，以实时骨架序列为输入，实现更早、更准的跌倒预警，优于仅DGNN及文献方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注事后跌倒检测，缺乏对“即将跌倒”的提前预警与对稳定到失稳之间短暂过渡态的分析。对辅助机器人与护理系统而言，提前预测和监测过渡态可显著提升干预时机与安全性。

Method: 使用视频骨架关键点的实时序列作为输入；将任务解耦为（1）DGNN进行步态状态分类：稳定、过渡、跌倒；（2）LSTM进行未来时刻的人体运动预测。通过DGNN的结构建模时序图关系，LSTM对未来骨架演化进行预测，实现早期预警。基于OUMVLP-Pose与URFD数据集进行训练与验证。

Result: 与仅用DGNN以及现有文献模型相比，该混合解耦模型在预测误差与识别精度上更优；能够更早检测到跌倒，并有效监测“过渡态”。

Conclusion: 解耦“运动预测”和“状态分类”优于统一由DGNN完成的做法，可提升前瞻性跌倒检测性能；同时对过渡态的可监测性为高级辅助系统提供有价值的时序风险信息。

Abstract: Detecting and preventing falls in humans is a critical component of assistive
robotic systems. While significant progress has been made in detecting falls,
the prediction of falls before they happen, and analysis of the transient state
between stability and an impending fall remain unexplored. In this paper, we
propose a anticipatory fall detection method that utilizes a hybrid model
combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory
(LSTM) networks that decoupled the motion prediction and gait classification
tasks to anticipate falls with high accuracy. Our approach employs real-time
skeletal features extracted from video sequences as input for the proposed
model. The DGNN acts as a classifier, distinguishing between three gait states:
stable, transient, and fall. The LSTM-based network then predicts human
movement in subsequent time steps, enabling early detection of falls. The
proposed model was trained and validated using the OUMVLP-Pose and URFD
datasets, demonstrating superior performance in terms of prediction error and
recognition accuracy compared to models relying solely on DGNN and models from
literature. The results indicate that decoupling prediction and classification
improves performance compared to addressing the unified problem using only the
DGNN. Furthermore, our method allows for the monitoring of the transient state,
offering valuable insights that could enhance the functionality of advanced
assistance systems.

</details>


### [11] [Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging](https://arxiv.org/abs/2509.05340)
*Dibya Jyoti Bora,Mrinal Kanti Mishra*

Main category: cs.CV

TL;DR: 作者比较了MRI脑肿瘤分割中的两类聚类方法：硬聚类（K-Means）与软聚类（FCM）。在BraTS2020数据上，K-Means更快（0.3s/图像），FCM更准（DSC=0.67 vs 0.43），代价是更高计算开销（1.3s/图像）。结论：精度与效率存在权衡。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤形态与强度分布高度异质，传统阈值或简单方法难以稳定分割，临床对边界精确性要求高，因而需要系统评估不同聚类范式在分割上的适用性与权衡。

Method: 在BraTS2020数据集上，先进行高斯滤波与CLAHE预处理；分别以K-Means（硬分配）与FCM（模糊隶属度）进行分割；以Dice系数衡量精度，并记录运行时间，比较两者性能。

Result: K-Means平均运行0.3秒/图像、DSC=0.43；FCM平均运行1.3秒/图像、DSC=0.67。FCM精度更高、边界更精细，但计算代价更大。

Conclusion: MRI脑肿瘤分割中，软聚类（FCM）优于硬聚类（K-Means）在准确度与边界刻画上，但牺牲速度；方法选择需根据临床场景在精度与效率之间权衡。

Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a
pivotal challenge in medical image analysis due to the heterogeneous nature of
tumor morphology and intensity distributions. Accurate delineation of tumor
boundaries is critical for clinical decision-making, radiotherapy planning, and
longitudinal disease monitoring. In this study, we perform a comprehensive
comparative analysis of two major clustering paradigms applied in MRI tumor
segmentation: hard clustering, exemplified by the K-Means algorithm, and soft
clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each
pixel strictly to a single cluster, FCM introduces partial memberships, meaning
each pixel can belong to multiple clusters with varying degrees of association.
Experimental validation was performed using the BraTS2020 dataset,
incorporating pre-processing through Gaussian filtering and Contrast Limited
Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice
Similarity Coefficient (DSC) and processing time, which collectively
demonstrated that K-Means achieved superior speed with an average runtime of
0.3s per image, whereas FCM attained higher segmentation accuracy with an
average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher
computational cost (1.3s per image). These results highlight the inherent
trade-off between computational efficiency and boundary precision.

</details>


### [12] [Handling imbalance and few-sample size in ML based Onion disease classification](https://arxiv.org/abs/2509.05341)
*Abhijeet Manoj Pal,Rajbabu Velmurugan*

Main category: cs.CV

TL;DR: 提出一种改进CNN的注意力增强模型与数据增广策略，用于洋葱作物病虫害多分类，在真实田间数据上达96.90%准确率与0.96 F1，优于同数据集的其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为二分类，无法满足实际需要中对具体病虫害类别的精确识别，限制了精细农业中的精准干预与传播控制。作者因此希望构建能在真实场景中稳定进行多类别识别的模型。

Method: 以预训练CNN为骨干，集成注意力模块以强化判别特征；设计全面的数据增广流水线缓解类别不平衡。使用真实田间图像数据集进行训练与评估，并与使用相同数据集的其他方法对比。

Result: 在真实场景数据上取得96.90%整体准确率与0.96 F1，并在对比中优于其他方案。

Conclusion: 注意力增强的预训练CNN结合系统性数据增广可有效实现洋葱病虫害多分类，具有较强的实用潜力与推广价值。

Abstract: Accurate classification of pests and diseases plays a vital role in precision
agriculture, enabling efficient identification, targeted interventions, and
preventing their further spread. However, current methods primarily focus on
binary classification, which limits their practical applications, especially in
scenarios where accurately identifying the specific type of disease or pest is
essential. We propose a robust deep learning based model for multi-class
classification of onion crop diseases and pests. We enhance a pre-trained
Convolutional Neural Network (CNN) model by integrating attention based modules
and employing comprehensive data augmentation pipeline to mitigate class
imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1
score on real-world field image dataset. This model gives better results than
other approaches using the same datasets.

</details>


### [13] [Delta Velocity Rectified Flow for Text-to-Image Editing](https://arxiv.org/abs/2509.05342)
*Gaspard Beaudouin,Minghan Li,Jaeyeon Kim,Sunghoon Yoon,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出Delta Velocity Rectified Flow（DVRF），一种在整流流模型中用于文本-图像编辑的、无需反演且路径感知的编辑框架，通过蒸馏建模源/目标速度场差分并引入时间依赖位移项，缓解过度平滑，提升对目标分布的对齐与可控性，并统一/推广既有方法（Delta Denoising Score与FlowEdit）。


<details>
  <summary>Details</summary>
Motivation: 现有基于蒸馏采样的编辑方法常出现过度平滑、编辑质量与保真度下降；同时，如何在不做结构改动且不依赖复杂反演的情况下实现高质量、可控的文本到图像编辑仍是挑战。

Method: 在整流流（rectified flow）框架中：1）显式学习源与目标速度场的差分（Delta Velocity）以进行蒸馏，减少过度平滑；2）引入时间依赖的shift项，将噪声潜变量推向目标轨迹以增强与目标分布的对齐；3）理论上证明当关闭shift时方法退化为Delta Denoising Score；当shift为线性调度并处于rectified-flow动力学下，DVRF推广并解释FlowEdit。

Result: 在多项文本-图像编辑任务上，DVRF无需网络结构改动即可实现更好的编辑质量、保真度与可控性，优于现有方法；同时具有高效与广泛适用性。

Conclusion: DVRF通过路径感知的速度差分蒸馏与时间依赖位移，统一与推广既有优化视角，显著提升文本-图像编辑的质量与控制，且无需模型结构改动，实用性强。

Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,
path-aware editing framework within rectified flow models for text-to-image
editing. DVRF is a distillation-based method that explicitly models the
discrepancy between the source and target velocity fields in order to mitigate
over-smoothing artifacts rampant in prior distillation sampling approaches. We
further introduce a time-dependent shift term to push noisy latents closer to
the target trajectory, enhancing the alignment with the target distribution. We
theoretically demonstrate that when this shift is disabled, DVRF reduces to
Delta Denoising Score, thereby bridging score-based diffusion optimization and
velocity-based rectified-flow optimization. Moreover, when the shift term
follows a linear schedule under rectified-flow dynamics, DVRF generalizes the
Inversion-free method FlowEdit and provides a principled theoretical
interpretation for it. Experimental results indicate that DVRF achieves
superior editing quality, fidelity, and controllability while requiring no
architectural modifications, making it efficient and broadly applicable to
text-to-image editing tasks. Code is available at
https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.

</details>


### [14] [Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis](https://arxiv.org/abs/2509.05343)
*Zahid Ullah,Minki Hong,Tahir Mahmood,Jihie Kim*

Main category: cs.CV

TL;DR: 将注意力机制系统性嵌入VGG16、ResNet18、InceptionV3、DenseNet121、EfficientNetB5等CNN，以提升医学影像分类。在脑肿瘤MRI与胚胎残留物（POC）病理图像两数据集上，注意力增强模型均优于基线，其中带混合注意力的EfficientNetB5表现最佳，并改进特征定位与跨模态泛化。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在医学影像中难以充分捕捉细粒度、复杂判别特征，影响诊断准确与可解释性。作者希望通过引入注意力提高模型对关键区域与重要通道的聚焦能力，从而提升性能、可解释性与跨数据集的泛化。

Method: 在五种主流CNN（VGG16、ResNet18、InceptionV3、DenseNet121、EfficientNetB5）中嵌入两类注意力模块：SE（Squeeze-and-Excitation）与混合CBAM（通道+空间）。对每个基线模型分别构建带不同注意力的变体，并在两个数据集（多亚型脑肿瘤MRI、包含四类组织的POC病理图像）上进行系统比较评估，分析分类性能与特征定位。

Result: 注意力增强的CNN在所有评估指标上均优于对应基线；其中使用混合注意力的EfficientNetB5获得整体最佳表现，并在两个数据集上带来显著增益。注意力还改善了特征定位与跨异质成像模态的泛化能力。

Conclusion: 系统性地证明了在多种CNN中嵌入注意力模块的有效性与通用性。实践上，推荐在医学影像任务中采用混合注意力，尤其在EfficientNetB5上效果突出；注意力同时提升可解释性与临床可用性，为开发稳健、可解释、可迁移的临床决策支持系统提供了参考框架。

Abstract: Deep learning has become a powerful tool for medical image analysis; however,
conventional Convolutional Neural Networks (CNNs) often fail to capture the
fine-grained and complex features critical for accurate diagnosis. To address
this limitation, we systematically integrate attention mechanisms into five
widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,
DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient
regions and improve discriminative performance. Specifically, each baseline
model is augmented with either a Squeeze and Excitation block or a hybrid
Convolutional Block Attention Module, allowing adaptive recalibration of
channel and spatial feature representations. The proposed models are evaluated
on two distinct medical imaging datasets, a brain tumor MRI dataset comprising
multiple tumor subtypes, and a Products of Conception histopathological dataset
containing four tissue categories. Experimental results demonstrate that
attention augmented CNNs consistently outperform baseline architectures across
all metrics. In particular, EfficientNetB5 with hybrid attention achieves the
highest overall performance, delivering substantial gains on both datasets.
Beyond improved classification accuracy, attention mechanisms enhance feature
localization, leading to better generalization across heterogeneous imaging
modalities. This work contributes a systematic comparative framework for
embedding attention modules in diverse CNN architectures and rigorously
assesses their impact across multiple medical imaging tasks. The findings
provide practical insights for the development of robust, interpretable, and
clinically applicable deep learning based decision support systems.

</details>


### [15] [Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset](https://arxiv.org/abs/2509.05348)
*Ashen Rodrigo,Isuru Munasinghe,Asanka Perera*

Main category: cs.CV

TL;DR: 评估YOLOv3、Faster R-CNN、RetinaNet、EfficientDet、Swin Transformer在太阳能板缺陷与污染物检测上的性能与速度权衡，提供模型选型依据，并公开COCO格式数据集与配套界面。


<details>
  <summary>Details</summary>
Motivation: 光伏系统需要及时准确地发现物理/电气缺陷与表面污染物（灰尘、污垢、鸟粪）以维持发电效率与可靠性，但缺少针对该场景的对比评测与专用数据集。

Method: 构建面向太阳能板缺陷与污染检测的COCO标注自定义数据集与训练/评测界面；选取五种主流目标检测器（YOLOv3、Faster R-CNN、RetinaNet、EfficientDet、Swin Transformer）统一训练与评估；比较mAP、Precision、Recall与推理速度，分析准确率与计算效率的权衡。

Result: 各模型在检测精度与推理速度上呈现不同侧重：有的精度高但速度慢，有的速度快但mAP较低；实验量化了这些差异并揭示了各模型的相对优势与限制。

Conclusion: 根据场景需求可在精度与实时性之间权衡选择合适模型；研究提供了实践指导与可复用的数据集资源（GitHub开源）。

Abstract: Timely and accurate detection of defects and contaminants in solar panels is
critical for maintaining the efficiency and reliability of photovoltaic
systems. This study presents a comprehensive evaluation of five
state-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,
EfficientDet, and Swin Transformer, for identifying physical and electrical
defects as well as surface contaminants such as dust, dirt, and bird droppings
on solar panels. A custom dataset, annotated in the COCO format and
specifically designed for solar panel defect and contamination detection, was
developed alongside a user interface to train and evaluate the models. The
performance of each model is assessed and compared based on mean Average
Precision (mAP), precision, recall, and inference speed. The results
demonstrate the trade-offs between detection accuracy and computational
efficiency, highlighting the relative strengths and limitations of each model.
These findings provide valuable guidance for selecting appropriate detection
approaches in practical solar panel monitoring and maintenance scenarios.
  The dataset will be publicly available at
https://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.

</details>


### [16] [Unsupervised Instance Segmentation with Superpixels](https://arxiv.org/abs/2509.05352)
*Cuong Manh Hoang*

Main category: cs.CV

TL;DR: 提出一个无需人工标注的实例分割框架：用自监督特征+MultiCut得粗掩码，过滤提纯；以超像素引导的新损失训练分割网络；再用自适应损失的自训练提升。实验在分割与检测任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 实例分割常依赖大量昂贵的人为标注。为降低标注成本、提升可扩展性，希望在无标注条件下仍能得到高质量实例掩码，推动机器人、交互、自动驾驶等应用。

Method: 1) 用自监督特征提取图像表征；2) 以MultiCut对特征进行分割得到粗掩码；3) 通过掩码过滤器筛选高质量粗掩码；4) 基于低层特征生成超像素，并设计“超像素引导的掩码损失”（包含hard与soft两部分）来训练分割网络；5) 采用带新自适应损失的自训练流程，利用模型预测进一步提升掩码质量。

Result: 在公开实例分割与目标检测数据集上进行验证，该框架在无标注设置下超过先前SOTA，显示更高的分割/检测性能。

Conclusion: 无需人工标注即可取得强性能：多阶段粗到精的掩码生成与过滤、超像素约束的损失设计，以及自适应自训练共同带来显著收益，证明该框架有效且具实用价值。

Abstract: Instance segmentation is essential for numerous computer vision applications,
including robotics, human-computer interaction, and autonomous driving.
Currently, popular models bring impressive performance in instance segmentation
by training with a large number of human annotations, which are costly to
collect. For this reason, we present a new framework that efficiently and
effectively segments objects without the need for human annotations. Firstly, a
MultiCut algorithm is applied to self-supervised features for coarse mask
segmentation. Then, a mask filter is employed to obtain high-quality coarse
masks. To train the segmentation network, we compute a novel superpixel-guided
mask loss, comprising hard loss and soft loss, with high-quality coarse masks
and superpixels segmented from low-level image features. Lastly, a
self-training process with a new adaptive loss is proposed to improve the
quality of predicted masks. We conduct experiments on public datasets in
instance segmentation and object detection to demonstrate the effectiveness of
the proposed framework. The results show that the proposed framework
outperforms previous state-of-the-art methods.

</details>


### [17] [Augmented Structure Preserving Neural Networks for cell biomechanics](https://arxiv.org/abs/2509.05388)
*Juan Olalla-Pombo,Alberto Badías,Miguel Ángel Sanz-Gómez,José María Benítez,Francisco Javier Montáns*

Main category: cs.CV

TL;DR: 提出一种结合结构保持神经网络与通用神经网络/计算机视觉特征的混合模型，用于高精度预测细胞迁移轨迹，并附带有基于同一特征的有丝分裂事件预测。


<details>
  <summary>Details</summary>
Motivation: 细胞生物力学过程（胚胎发生、修复、肿瘤生长等）复杂且相互作用机制未明，尤其是群体/簇状细胞决策受力学与环境共同影响的整合建模缺口明显。现有方法难以同时保留力学结构约束并融合可观测环境因素。

Method: 将结构保持神经网络（SPNN）用于把细胞运动视为纯力学系统，确保物理/几何守恒与稳定性；并以计算机视觉从实验影像中提取的环境特征作为输入，借助人工神经网络（ANN）进行融合。整体采用roll-out策略预测多步完整轨迹；另设计基于相同特征的神经网络架构预测有丝分裂事件。在仿真与真实细胞迁移数据上测试。

Result: 混合模型可高精度预测细胞完整迁移轨迹，roll-out 多步预测表现良好；在模拟与真实数据上均达到高准确度；有丝分裂预测模型亦能有效识别/预测分裂事件。

Conclusion: 将结构保留的力学建模与数据驱动的环境特征学习相结合，可更准确地捕捉细胞迁移与分裂等关键动力学，证明了物理先验与视觉感知融合在细胞生物力学建模中的有效性与通用性。

Abstract: Cell biomechanics involve a great number of complex phenomena that are
fundamental to the evolution of life itself and other associated processes,
ranging from the very early stages of embryo-genesis to the maintenance of
damaged structures or the growth of tumors. Given the importance of such
phenomena, increasing research has been dedicated to their understanding, but
the many interactions between them and their influence on the decisions of
cells as a collective network or cluster remain unclear. We present a new
approach that combines Structure Preserving Neural Networks, which study cell
movements as a purely mechanical system, with other Machine Learning tools
(Artificial Neural Networks), which allow taking into consideration
environmental factors that can be directly deduced from an experiment with
Computer Vision techniques. This new model, tested on simulated and real cell
migration cases, predicts complete cell trajectories following a roll-out
policy with a high level of accuracy. This work also includes a mitosis event
prediction model based on Neural Networks architectures which makes use of the
same observed features.

</details>


### [18] [Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding](https://arxiv.org/abs/2509.05431)
*GodsGift Uzor,Tania-Amanda Nkoyo Fredrick Eneye,Chukwuebuka Ijezue*

Main category: cs.CV

TL;DR: 提出EMCAD高效多尺度卷积注意力解码器，用于BraTS2020脑肿瘤分割，在较低算力场景兼顾效率与效果；初步结果Dice最佳0.31，均值0.285±0.015，表现中等且无过拟合迹象。


<details>
  <summary>Details</summary>
Motivation: 现有分割解码器在资源受限场景下计算成本高，影响临床可用性与部署效率，需要一种能在保证精度的同时显著降低计算开销的解码机制。

Method: 设计EMCAD：一种高效多尺度卷积注意力解码器，融合多尺度特征与注意力以强化肿瘤区域表征，集成于MRI脑肿瘤分割网络并在BraTS2020数据集上训练与验证。强调轻量化与推理效率。

Result: 在BraTS2020上取得最佳Dice 0.31，训练过程中均值Dice稳定在0.285±0.015；验证集表现稳定，无明显过拟合。

Conclusion: EMCAD在有限计算资源下实现了较好效率-性能权衡，但当前分割精度仅属中等，显示改进空间；方法具有稳定性与可部署潜力，需进一步优化以提升Dice。

Abstract: Brain tumor segmentation is a critical pre-processing step in the medical
image analysis pipeline that involves precise delineation of tumor regions from
healthy brain tissue in medical imaging data, particularly MRI scans. An
efficient and effective decoding mechanism is crucial in brain tumor
segmentation especially in scenarios with limited computational resources.
However these decoding mechanisms usually come with high computational costs.
To address this concern EMCAD a new efficient multi-scale convolutional
attention decoder designed was utilized to optimize both performance and
computational efficiency for brain tumor segmentation on the BraTs2020 dataset
consisting of MRI scans from 369 brain tumor patients. The preliminary result
obtained by the model achieved a best Dice score of 0.31 and maintained a
stable mean Dice score of 0.285 plus/minus 0.015 throughout the training
process which is moderate. The initial model maintained consistent performance
across the validation set without showing signs of over-fitting.

</details>


### [19] [FAVAE-Effective Frequency Aware Latent Tokenizer](https://arxiv.org/abs/2509.05441)
*Tejaswini Medi,Hsien-Yi Wang,Arianna Rampini,Margret Keuper*

Main category: cs.CV

TL;DR: 提出一种频率感知的VAE（FA-VAE），通过小波分解显式解耦低/高频优化，提升潜变量分词器对高频细节的重建，缓解过平滑与伪影，改进高逼真图像合成。


<details>
  <summary>Details</summary>
Motivation: 现有潜变量生成模型依赖 learned tokenizer 将图像压缩到潜空间，但常用目标函数偏向低频，导致纹理和锐利过渡等高频细节丢失、重建过平滑，影响感知质量与下游生成效果。作者希望诊断并纠正这种频率偏置。

Method: 对SOTA潜变量分词器进行频域分解分析，发现联合优化时对低频偏置。基于此提出FA-VAE：利用小波变换将图像分解为低/高频分量，分别设计与权衡损失，显式解耦优化通道，使低频保全结构、高频聚焦细节纹理；整体作为频率感知的变分自编码器训练。

Result: 频率分解实验证明现有方法重建时牺牲高频；FA-VAE在相同框架下显著提升高频细节与整体感知质量，减少过平滑与视觉伪影，缩小现有latent tokenizer的保真度差距。

Conclusion: 频率感知的优化对于真实感图像表示至关重要。通过小波式低/高频解耦，FA-VAE同时保留全局结构与细节纹理，对内容创作、神经渲染与医学影像等应用具有广泛意义。

Abstract: Latent generative models have shown remarkable progress in high-fidelity
image synthesis, typically using a two-stage training process that involves
compressing images into latent embeddings via learned tokenizers in the first
stage. The quality of generation strongly depends on how expressive and
well-optimized these latent embeddings are. While various methods have been
proposed to learn effective latent representations, the reconstructed images
often lack realism, particularly in textured regions with sharp transitions,
due to loss of fine details governed by high frequencies. We conduct a detailed
frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers
and show that conventional objectives inherently prioritize low-frequency
reconstruction, often at the expense of high-frequency fidelity. Our analysis
reveals these latent tokenizers exhibit a bias toward low-frequency
information, when jointly optimized, leading to over-smoothed outputs and
visual artifacts that diminish perceptual quality. To address this, we propose
a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework
that explicitly decouples the optimization of low- and high-frequency
components. This decoupling enables improved reconstruction of fine textures
while preserving global structure. Our approach bridges the fidelity gap in
current latent tokenizers and emphasizes the importance of frequency-aware
optimization for realistic image representation, with broader implications for
applications in content creation, neural rendering, and medical imaging.

</details>


### [20] [Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's](https://arxiv.org/abs/2509.05446)
*Iftekhar Haider Chowdhury,Zaed Ikbal Syed,Ahmed Faizul Haque Dhrubo,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: 提出“差分敏感度融合剪枝”（DSFP），单次打分+剪枝，通过融合多种重要性指标的差异性来评估卷积滤波器的稳定性/冗余，在50–70%剪枝下显著降算量（>80% FLOPs 减少）且几乎不掉点（最高保留98.23%基线精度）。


<details>
  <summary>Details</summary>
Motivation: CNN在视觉任务表现突出，但在边缘/移动端部署受限于计算与内存开销；现有剪枝方法常迭代、代价高或依赖启发式、稳定性一般，亟需高效、可扩展、鲁棒的单次剪枝方案。

Method: 提出差分敏感度融合剪枝框架：对每个卷积滤波器计算三类重要性/敏感度分数——(1)基于梯度的敏感度，(2)一阶泰勒展开估计的影响，(3)激活分布KL散度。将三者间的“差异/不一致”融合为“差分敏感度”分数；再用指数缩放放大跨指标不一致性，突出结构不稳定或贡献较低的滤波器；仅需一次前向-反向传播即可完成打分与剪枝，过程确定且高效。

Result: 在50–70%剪枝率范围内，显著降低模型复杂度，FLOPs减少超过80%；在70%剪枝时依旧可保留最高98.23%的原始精度，整体优于传统启发式方法的压缩率与泛化能力。

Conclusion: DSFP提供了一种高效、可扩展、确定性的单次滤波器剪枝方法，通过融合多指标不一致性来定位冗余/不稳定结构，实现高压缩比与保持精度，适合在边缘与移动平台部署CNN。

Abstract: Deep Convolutional Neural Networks have achieved state of the art performance
across various computer vision tasks, however their practical deployment is
limited by computational and memory overhead. This paper introduces
Differential Sensitivity Fusion Pruning, a novel single shot filter pruning
framework that focuses on evaluating the stability and redundancy of filter
importance scores across multiple criteria. Differential Sensitivity Fusion
Pruning computes a differential sensitivity score for each filter by fusing the
discrepancies among gradient based sensitivity, first order Taylor expansion,
and KL divergence of activation distributions. An exponential scaling mechanism
is applied to emphasize filters with inconsistent importance across metrics,
identifying candidates that are structurally unstable or less critical to the
model performance. Unlike iterative or reinforcement learning based pruning
strategies, Differential Sensitivity Fusion Pruning is efficient and
deterministic, requiring only a single forward-backward pass for scoring and
pruning. Extensive experiments across varying pruning rates between 50 to 70
percent demonstrate that Differential Sensitivity Fusion Pruning significantly
reduces model complexity, achieving over 80 percent Floating point Operations
Per Seconds reduction while maintaining high accuracy. For instance, at 70
percent pruning, our approach retains up to 98.23 percent of baseline accuracy,
surpassing traditional heuristics in both compression and generalization. The
proposed method presents an effective solution for scalable and adaptive Deep
Convolutional Neural Networks compression, paving the way for efficient
deployment on edge and mobile platforms.

</details>


### [21] [Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging](https://arxiv.org/abs/2509.05483)
*Jinhao Wang,Florian Vogl,Pascal Schütz,Saša Ćuković,William R. Taylor*

Main category: cs.CV

TL;DR: Veriserum 是一个开放的双平面透视（X-ray）膝关节植入物影像与姿态标注数据集，含约11万张图像、1600次实验与自动/少量人工配准真值，旨在为2D/3D配准等任务提供可复现基准。


<details>
  <summary>Details</summary>
Motivation: 现有用于双平面透视运动学与2D/3D配准的公开数据稀缺、规模小且缺少统一标注与基准，限制了深度学习与计算机视觉方法在骨科植入物分析中的发展与可比性。

Method: 构建包含10种股骨-胫骨植入组合在日常动作情景（平地行走、下坡等）下拍摄的双平面X光影像库；为每张图像提供自动配准得到的姿态真值，并为200张提供人工精配准以作基准；同时发布相机标定与失真校正工具，提供可重复的评测流程与数据访问链接。

Result: 数据集规模约11万张图像，涵盖1600次试验与多种动作工况；包含双平面影像、相机标定信息与姿态注释；提供200张人工金标准以量化评测自动配准的精度与鲁棒性。

Conclusion: Veriserum 为2D/3D配准、分割、X射线畸变校正与3D重建等任务提供公开、可复现实验基准，期望推动计算机视觉与医学影像领域在膝关节植入分析方面的研究与算法比较；数据已在ETH苏黎世仓储公开可获取。

Abstract: Veriserum is an open-source dataset designed to support the training of deep
learning registration for dual-plane fluoroscopic analysis. It comprises
approximately 110,000 X-ray images of 10 knee implant pair combinations (2
femur and 5 tibia implants) captured during 1,600 trials, incorporating poses
associated with daily activities such as level gait and ramp descent. Each
image is annotated with an automatically registered ground-truth pose, while
200 images include manually registered poses for benchmarking.
  Key features of Veriserum include dual-plane images and calibration tools.
The dataset aims to support the development of applications such as 2D/3D image
registration, image segmentation, X-ray distortion correction, and 3D
reconstruction. Freely accessible, Veriserum aims to advance computer vision
and medical imaging research by providing a reproducible benchmark for
algorithm development and evaluation. The Veriserum dataset used in this study
is publicly available via
https://movement.ethz.ch/data-repository/veriserum.html, with the data stored
at ETH Z\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.

</details>


### [22] [An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures](https://arxiv.org/abs/2509.05490)
*Andrzej D. Dobrzycki,Ana M. Bernardos,José R. Casar*

Main category: cs.CV

TL;DR: 研究系统评估在YOLOv8/YOLOv10上不同层冻结策略对小型资源受限场景（如无人机）迁移学习的影响，发现最优冻结深度取决于数据特性：冻结骨干可保留通用特征、浅层冻结更适合极端类不平衡；在降低GPU显存最高28%的同时，部分配置mAP@50优于全量微调。


<details>
  <summary>Details</summary>
Motivation: 实时目标检测在资源受限平台（如UAV）部署需要高效迁移学习。现有“冻结层”做法常用，但缺乏对当代YOLOv8/YOLOv10在不同冻结深度、数据特点与训练动态之间相互作用的系统性研究与量化证据。

Method: 在YOLOv8/YOLOv10多种变体与四个关键基础设施监测数据集上，系统比较多种冻结配置；结合梯度行为分析（L2范数）与Grad-CAM可视化，观察不同冻结策略下的训练动态与特征关注；评估mAP@50、显存占用等指标。

Result: 不存在通用最优冻结策略，需按数据特性选择：冻结backbone在保留通用特征时有效；面对极端类别不平衡时，较浅的冻结（解冻更多层）更优。相较全量微调，部分冻结可减少GPU显存最高28%，且在部分任务上mAP@50超过全量微调。梯度分析显示中等冻结深度具有独特且更稳定的收敛模式。

Conclusion: 给出基于证据的冻结层选择指南：根据数据分布与任务需求调节冻结深度，实现在资源受限场景下精度-资源的更佳平衡；强调需结合梯度与可视化诊断来动态调整策略。

Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object
detection. However, deploying it in resource-constrained environments such as
unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although
layer freezing is a common technique, the specific impact of various freezing
configurations on contemporary YOLOv8 and YOLOv10 architectures remains
unexplored, particularly with regard to the interplay between freezing depth,
dataset characteristics, and training dynamics. This research addresses this
gap by presenting a detailed analysis of layer-freezing strategies. We
systematically investigate multiple freezing configurations across YOLOv8 and
YOLOv10 variants using four challenging datasets that represent critical
infrastructure monitoring. Our methodology integrates a gradient behavior
analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper
insights into training dynamics under different freezing strategies. Our
results reveal that there is no universal optimal freezing strategy but,
rather, one that depends on the properties of the data. For example, freezing
the backbone is effective for preserving general-purpose features, while a
shallower freeze is better suited to handling extreme class imbalance. These
configurations reduce graphics processing unit (GPU) memory consumption by up
to 28% compared to full fine-tuning and, in some cases, achieve mean average
precision (mAP@50) scores that surpass those of full fine-tuning. Gradient
analysis corroborates these findings, showing distinct convergence patterns for
moderately frozen models. Ultimately, this work provides empirical findings and
practical guidelines for selecting freezing strategies. It offers a practical,
evidence-based approach to balanced transfer learning for object detection in
scenarios with limited resources.

</details>


### [23] [Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection](https://arxiv.org/abs/2509.05512)
*Bryce Grant,Peng Wang*

Main category: cs.CV

TL;DR: 提出QUAN：用实数近似四元数卷积，实现旋转等变的分类与检测，精度更高、参数更省、收敛更快，并在检测中表现出更强的旋转处理与参数效率，适合资源受限机器人感知。


<details>
  <summary>Details</summary>
Motivation: 传统CNN对旋转不鲁棒；全四元数网络训练与部署成本高。需要一种既保留四元数几何性质、又能高效实现且易训练的旋转感知网络，用于分类、检测与机器人等实际场景。

Method: - 用哈密顿积分解，将四元数卷积近似为实值运算，配合自定义CUDA核实现高效计算。
- 提出独立四元数批归一化(IQBN)提升训练稳定性。
- 将四元数操作扩展到空间注意力机制。
- 在多个任务与数据集上评测：CIFAR-10/100、ImageNet（分类），COCO、DOTA（检测），以及机器人感知任务。

Result: 在分类上，较传统实值卷积与既有四元数模型：更高准确率、更少参数、更快收敛。在检测上，相比标准CNN：更好的参数效率与旋转处理能力；并在四元数CNN范畴内达到SOTA。

Conclusion: QUAN通过实值近似实现四元数卷积，兼顾旋转等变与工程可行性；在分类与检测均优于对照并具参数效率，适合资源受限、需旋转感知的机器人系统，也具备在其他领域推广的潜力。

Abstract: This paper introduces Quaternion Approximate Networks (QUAN), a novel deep
learning framework that leverages quaternion algebra for rotation equivariant
image classification and object detection. Unlike conventional quaternion
neural networks attempting to operate entirely in the quaternion domain, QUAN
approximates quaternion convolution through Hamilton product decomposition
using real-valued operations. This approach preserves geometric properties
while enabling efficient implementation with custom CUDA kernels. We introduce
Independent Quaternion Batch Normalization (IQBN) for training stability and
extend quaternion operations to spatial attention mechanisms. QUAN is evaluated
on image classification (CIFAR-10/100, ImageNet), object detection (COCO,
DOTA), and robotic perception tasks. In classification tasks, QUAN achieves
higher accuracy with fewer parameters and faster convergence compared to
existing convolution and quaternion-based models. For objection detection, QUAN
demonstrates improved parameter efficiency and rotation handling over standard
Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion
CNNs in this downstream task. These results highlight its potential for
deployment in resource-constrained robotic systems requiring rotation-aware
perception and application in other domains.

</details>


### [24] [OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation](https://arxiv.org/abs/2509.05513)
*Ahad Jawaid,Yu Xiang*

Main category: cs.CV

TL;DR: OpenEgo 提供统一手部姿态标注与意图对齐的动作原语，整合6个公开数据集共1107小时自中心操作视频，覆盖290类任务与600+环境，用于训练语言条件的模仿学习策略预测灵巧手轨迹，旨在降低从自中心视频学习灵巧操控的门槛并促进可复现研究。


<details>
  <summary>Details</summary>
Motivation: 现有自中心人类视频资源缺乏细粒度、时间对齐的动作描述或缺少灵巧手标注，限制了模仿学习在复杂操控中的效果与可复现性。

Method: 构建并统一六个数据集：标准化手部姿态布局，提供带时间戳的动作原语（与意图对齐的描述）；在此基础上训练语言条件的模仿学习策略，预测灵巧手的运动轨迹，以验证数据集的效用。

Result: 得到一个规模为1107小时、覆盖290种操控任务和600+环境的多模态自中心操控数据集；基于该数据集训练的语言条件模仿策略能够预测灵巧手轨迹，证明数据集的有效性。

Conclusion: OpenEgo 作为统一的多模态自中心操控基准，降低了从自中心视频学习灵巧操控的门槛，提升了研究的可复现性，并为视觉-语言-行动学习提供了标准化资源（代码与说明将开源）。

Abstract: Egocentric human videos provide scalable demonstrations for imitation
learning, but existing corpora often lack either fine-grained, temporally
localized action descriptions or dexterous hand annotations. We introduce
OpenEgo, a multimodal egocentric manipulation dataset with standardized
hand-pose annotations and intention-aligned action primitives. OpenEgo totals
1107 hours across six public datasets, covering 290 manipulation tasks in 600+
environments. We unify hand-pose layouts and provide descriptive, timestamped
action primitives. To validate its utility, we train language-conditioned
imitation-learning policies to predict dexterous hand trajectories. OpenEgo is
designed to lower the barrier to learning dexterous manipulation from
egocentric video and to support reproducible research in vision-language-action
learning. All resources and instructions will be released at
www.openegocentric.com.

</details>


### [25] [Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2509.05515)
*Sen Wang,Kunyi Li,Siyun Liang,Elena Alegret,Jing Ma,Nassir Navab,Stefano Gasperini*

Main category: cs.CV

TL;DR: VALA提出可见性感知的语言特征聚合框架，通过射线级边际贡献与门控筛除不可见高斯，并以余弦空间的流式加权几何中值融合多视图噪声特征，从而得到稳健、视角一致的开放词汇3D语义嵌入，提升定位与分割性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有将开放词汇语言特征蒸馏到3D高斯的方法存在两大问题：1）对某像素贡献极小的背景高斯与主导前景高斯获得相同语言特征，导致语义污染；2）多视图语言嵌入含视角特定噪声造成不一致。需要一种既考虑可见性又能稳健融合多视图的聚合策略。

Method: 提出Visibility-Aware Language Aggregation（VALA）：（a）对每条射线计算各高斯的边际贡献，使用可见性门控仅保留可见（贡献显著）的高斯以分配语言特征；（b）设计在余弦空间的流式加权几何中值，用于跨视图融合噪声语言特征，实现快速、内存友好的稳健聚合，得到视角一致的3D语言嵌入。

Result: 在开放词汇的定位与分割任务上，VALA在多个基准上稳定优于现有方法；同时具备更快和更省内存的特性，并产生更稳健、视角一致的语言特征。

Conclusion: 基于可见性门控与流式加权几何中值的VALA有效缓解了背景干扰与多视图不一致，提供快速、内存高效且稳健的3D语言特征蒸馏方案，显著提升开放词汇3D场景理解表现。

Abstract: Recently, distilling open-vocabulary language features from 2D images into 3D
Gaussians has attracted significant attention. Although existing methods
achieve impressive language-based interactions of 3D scenes, we observe two
fundamental issues: background Gaussians contributing negligibly to a rendered
pixel get the same feature as the dominant foreground ones, and multi-view
inconsistencies due to view-specific noise in language embeddings. We introduce
Visibility-Aware Language Aggregation (VALA), a lightweight yet effective
method that computes marginal contributions for each ray and applies a
visibility-aware gate to retain only visible Gaussians. Moreover, we propose a
streaming weighted geometric median in cosine space to merge noisy multi-view
features. Our method yields a robust, view-consistent language feature
embedding in a fast and memory-efficient manner. VALA improves open-vocabulary
localization and segmentation across reference datasets, consistently
surpassing existing works.

</details>


### [26] [DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation](https://arxiv.org/abs/2509.05543)
*Haitao Tian,Pierre Payeur*

Main category: cs.CV

TL;DR: 提出DuoCLR对比学习框架，用“打乱与变形”增强，结合CPC与ROR两种代理任务，学习多尺度、跨序列骨架表示，预训练于裁剪序列后显著提升未裁剪人体动作分割表现，并有消融验证。


<details>
  <summary>Details</summary>
Motivation: 现有自监督/对比学习多面向动作识别，依赖单序列整体特征，难以捕获动作分割所需的多尺度时序与跨序列上下文。需要利用裁剪（单动作）数据预训练，使模型能泛化到未裁剪（多动作）分割场景。

Method: 提出“Shuffle and Warp”数据增强，生成多种多动作排列；设计双代理任务：1) CPC跨排列对比，跨不同排列对同类动作片段进行对比，学习类内相似；2) ROR相对顺序推理，预测两种排列间的映射/相对顺序，建模类间上下文。将二者整合为DuoCLR，学习多尺度特征用于动作分割；在裁剪骨架数据上预训练，再迁移到未裁剪分割任务。

Result: 在未裁剪数据集上的多类与多标签动作分割任务上，相比现有方法取得显著提升；并通过消融实验验证每个组件（Shuffle and Warp、CPC、ROR、多尺度表示）的有效性。

Conclusion: DuoCLR通过双代理对比任务与“打乱与变形”增强，能从裁剪骨架序列中学习到适用于分割的多尺度、跨序列表示，迁移到未裁剪场景显著提升性能，方法组件互补且有效。

Abstract: In this paper, a contrastive representation learning framework is proposed to
enhance human action segmentation via pre-training using trimmed (single
action) skeleton sequences. Unlike previous representation learning works that
are tailored for action recognition and that build upon isolated sequence-wise
representations, the proposed framework focuses on exploiting multi-scale
representations in conjunction with cross-sequence variations. More
specifically, it proposes a novel data augmentation strategy, 'Shuffle and
Warp', which exploits diverse multi-action permutations. The latter effectively
assists two surrogate tasks that are introduced in contrastive learning: Cross
Permutation Contrasting (CPC) and Relative Order Reasoning (ROR). In
optimization, CPC learns intra-class similarities by contrasting
representations of the same action class across different permutations, while
ROR reasons about inter-class contexts by predicting relative mapping between
two permutations. Together, these tasks enable a Dual-Surrogate Contrastive
Learning (DuoCLR) network to learn multi-scale feature representations
optimized for action segmentation. In experiments, DuoCLR is pre-trained on a
trimmed skeleton dataset and evaluated on an untrimmed dataset where it
demonstrates a significant boost over state-the-art comparatives in both
multi-class and multi-label action segmentation tasks. Lastly, ablation studies
are conducted to evaluate the effectiveness of each component of the proposed
approach.

</details>


### [27] [RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation](https://arxiv.org/abs/2509.05554)
*Yihong Leng,Siming Zheng,Jinwei Chen,Bo Li,Jiaojiao Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出RED：一种针对事件摄像头引导去模糊的鲁棒网络，通过随机事件遮挡增强鲁棒性，并用解耦的全域注意力建模跨模态与运动相关性，在合成与真实数据上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 事件相机阈值机制导致事件流不完整（敏感度与噪声的权衡），削弱运动先验的完整性，使现有依赖跨模态交互的去模糊方法在真实复杂场景下效果受限，亟需能抵御事件缺失与噪声的鲁棒方案。

Method: 1) 鲁棒性导向扰动策略RPS：对事件流施加随机mask，模拟不完整事件以提升模型对未知场景与缺失模式的适应性。2) 解耦OmniAttention：分别显式建模帧内运动、帧间/多运动以及跨模态（模糊图像↔事件）相关性，从两种互补源（模糊图、被破坏的事件）提取可靠特征。3) 交互模块×2：a) 增强模糊图中的运动敏感区域；b) 为不完整事件注入语义上下文。整体形成RED框架，学习模态特异且可交互的表示。

Result: 在合成与真实数据集的广泛实验中，RED在准确性与鲁棒性均取得一致的SOTA表现，相比现有方法更能应对事件缺失与噪声。

Conclusion: 通过在训练中显式暴露不完整事件并以解耦注意力融合跨模态与多种运动相关性，RED能稳定恢复运动模糊，兼具精度与鲁棒性，适用于真实世界复杂条件。

Abstract: Event cameras provide sparse yet temporally high-temporal-resolution motion
information, demonstrating great potential for motion deblurring. Existing
methods focus on cross-modal interaction, overlooking the inherent
incompleteness of event streams, which arises from the trade-off between
sensitivity and noise introduced by the thresholding mechanism of Dynamic
Vision Sensors (DVS). Such degradation compromises the integrity of motion
priors and limits the effectiveness of event-guided deblurring. To tackle these
challenges, we propose a Robust Event-guided Deblurring (RED) network with
modality-specific disentangled representation. First, we introduce a
Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to
events, which exposes RED to incomplete patterns and then foster robustness
against various unknown scenario conditions.Next, a disentangled OmniAttention
is presented to explicitly model intra-motion, inter-motion, and cross-modality
correlations from two inherently distinct but complementary sources: blurry
images and partially disrupted events. Building on these reliable features, two
interactive modules are designed to enhance motion-sensitive areas in blurry
images and inject semantic context into incomplete event representations.
Extensive experiments on synthetic and real-world datasets demonstrate RED
consistently achieves state-of-the-art performance in both accuracy and
robustness.

</details>


### [28] [Sensitivity-Aware Post-Training Quantization for Deep Neural Networks](https://arxiv.org/abs/2509.05576)
*Zekang Zheng,Haokun Li,Yaofo Chen,Mingkui Tan,Qing Du*

Main category: cs.CV

TL;DR: 提出一种高效的PTQ方法：按参数敏感度优先量化高敏感参数，利用低敏感参数抵消误差，并基于列聚类的敏感度结构设计行并行量化与全局共享逆Hessian更新，较OBQ提速20-200倍，精度损失<0.3%。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ在高压缩比下为保精度需迭代更新，计算与资源开销大，不适合边缘与实时推理。需要一种在不牺牲精度的前提下降本增效的PTQ方案。

Method: 1) 进行参数敏感度分析，按敏感度优先量化高敏感参数；2) 将低敏感参数暂不量化，用作误差补偿以减轻精度下降；3) 观察到敏感度在列方向聚类，提出行并行的量化框架；4) 引入全局共享的逆Hessian矩阵更新机制，显著降低计算复杂度（数量级）。

Result: 在ResNet-50与YOLOv5s上，相比Optimal Brain Quantization实现20-200倍量化速度提升，平均精度损失<0.3%。

Conclusion: 该方法在保持高精度的同时显著提升PTQ效率，适用于资源受限与实时场景；结构化的敏感度利用与全局逆Hessian共享是关键。

Abstract: Model quantization reduces neural network parameter precision to achieve
compression, but often compromises accuracy. Existing post-training
quantization (PTQ) methods employ iterative parameter updates to preserve
accuracy under high compression ratios, incurring significant computational
complexity and resource overhead, which limits applicability in
resource-constrained edge computing and real-time inference scenarios. This
paper proposes an efficient PTQ method guided by parameter sensitivity
analysis. The approach prioritizes quantization of high-sensitivity parameters,
leveraging unquantized low-sensitivity parameters to compensate for
quantization errors, thereby mitigating accuracy degradation. Furthermore, by
exploiting column-wise clustering of parameter sensitivity, the method
introduces a row-parallel quantization framework with a globally shared inverse
Hessian matrix update mechanism, reducing computational complexity by an order
of magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a
20-200-fold quantization speedup over the Optimal Brain Quantization baseline,
with mean accuracy loss below 0.3%, confirming the method's efficacy in
balancing efficiency and accuracy.

</details>


### [29] [Reconstruction and Reenactment Separated Method for Realistic Gaussian Head](https://arxiv.org/abs/2509.05582)
*Zhiling Ye,Cong Zhou,Xiubao Zhang,Haifeng Shen,Weihong Deng,Quan Lu*

Main category: cs.CV

TL;DR: 提出一个重建与驱动解耦的3D高斯头像框架，单张人像即可生成可控化身；采用大规模一次性生成器与两阶段训练，既泛化好又能还原高频纹理；推理时用超轻量驱动实现512×512下90FPS；模型随规模提升而性能按缩放律提升，且驱动效率不受影响；实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯或神经表示的头像方法常需多视角/视频数据、推理慢或难以兼顾高保真纹理与实时驱动。作者希望：1) 仅凭单张图实现可控3D头像；2) 提高泛化与高频细节重建；3) 在保持高质量的同时实现高帧率渲染；4) 验证模型规模扩大能系统性带来性能提升且不牺牲驱动效率。

Method: - 架构：重建(生成)与重演(驱动)分离的3D高斯头像框架。
- 重建模块：基于WebSSL的大规模一次性(one-shot)高斯头部生成器；两阶段训练策略以提升泛化与高频纹理重建能力。
- 驱动模块：超轻量的高斯头像表示由控制信号驱动，实现快速渲染与动作重演。
- 推理与效率：512×512分辨率下可达90FPS。
- 缩放实验：增加重建模块参数规模以检验缩放律对性能的正向影响，同时验证驱动效率不受影响。

Result: - 定量与定性实验均显示优于现有SOTA方法。
- 两阶段训练提高高频纹理重建与泛化能力。
- 在512×512达到约90FPS的实时渲染。
- 随着重建模块参数规模增加，重建质量随之提升(符合缩放律)，驱动效率基本保持不变。

Conclusion: 解耦式3D高斯头像框架可在单张输入下生成可控高质量化身，兼顾高频细节与实时性；通过两阶段训练与模型规模扩展实现更好性能，且保持高驱动效率；整体性能超过现有方法。

Abstract: In this paper, we explore a reconstruction and reenactment separated
framework for 3D Gaussians head, which requires only a single portrait image as
input to generate controllable avatar. Specifically, we developed a large-scale
one-shot gaussian head generator built upon WebSSL and employed a two-stage
training approach that significantly enhances the capabilities of
generalization and high-frequency texture reconstruction. During inference, an
ultra-lightweight gaussian avatar driven by control signals enables high
frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further
demonstrate that the proposed framework follows the scaling law, whereby
increasing the parameter scale of the reconstruction module leads to improved
performance. Moreover, thanks to the separation design, driving efficiency
remains unaffected. Finally, extensive quantitative and qualitative experiments
validate that our approach outperforms current state-of-the-art methods.

</details>


### [30] [MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios](https://arxiv.org/abs/2509.05592)
*Changtao Miao,Yi Zhang,Man Luo,Weiwei Feng,Kaiyuan Zheng,Qi Chu,Tao Gong,Jianshu Li,Yunfeng Diao,Wei Zhou,Joey Tianyi Zhou,Xiaoshuai Hao*

Main category: cs.CV

TL;DR: 提出MFFI多维度人脸伪造图像数据集，覆盖50种伪造方法与1024K样本，强调更广伪造手段、更多场景、更多真实数据与多级传播退化，并在基准上展现更强的跨域泛化与检测难度梯度。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测受限于数据集：缺乏对先进伪造技术的覆盖、面部场景多样性不足、真实数据不丰富、以及真实传播过程中的退化缺失，导致模型在真实世界中的泛化与鲁棒性较差。

Method: 构建MFFI数据集，围绕四个维度增强现实性：1）更广的伪造方法（共50种）；2）多样化面部场景；3）多元丰富的真实数据；4）多级真实世界传播退化操作。随后在多种检测基线和评测设置下进行基准评测与对比。

Result: MFFI包含约1024K图像样本，整合50种伪造方法。基准评测显示，相较已有公开数据集，MFFI在场景复杂度、跨域泛化能力、以及可控的检测难度梯度方面更优。

Conclusion: MFFI更贴近真实世界条件，可作为更具挑战与实用性的检测基准，促进深度伪造检测模型的泛化与鲁棒性提升。数据集与细节已公开于项目主页。

Abstract: Rapid advances in Artificial Intelligence Generated Content (AIGC) have
enabled increasingly sophisticated face forgeries, posing a significant threat
to social security. However, current Deepfake detection methods are limited by
constraints in existing datasets, which lack the diversity necessary in
real-world scenarios. Specifically, these data sets fall short in four key
areas: unknown of advanced forgery techniques, variability of facial scenes,
richness of real data, and degradation of real-world propagation. To address
these challenges, we propose the Multi-dimensional Face Forgery Image
(\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances
realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied
Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation
Operations. MFFI integrates $50$ different forgery methods and contains $1024K$
image samples. Benchmark evaluations show that MFFI outperforms existing public
datasets in terms of scene complexity, cross-domain generalization capability,
and detection difficulty gradients. These results validate the technical
advance and practical utility of MFFI in simulating real-world conditions. The
dataset and additional details are publicly available at
{https://github.com/inclusionConf/MFFI}.

</details>


### [31] [Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization](https://arxiv.org/abs/2509.05604)
*Jungin Park,Jiyoung Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: 提出VideoGraph：把视频摘要表述为语言引导的时空图学习，节点为对象与帧，借助语言查询注入语义，递归地精炼图并判定关键帧，泛化到监督/无监督与通用/查询式任务均达SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往方法多依赖时间建模的全局帧间关系，忽视细粒度视觉实体（对象）及其语义关系；而语言引导的摘要需要对复杂视频的语义理解，单靠视觉相似度建边会偏离语义相关性。因此需要一种将对象与帧的语义关系统一建模、并结合语言信息指导的框架。

Method: 将视频摘要建模为语言引导的时空图问题：构建空间图（对象为节点）与时间图（帧为节点），通过图边编码节点间语义关系；从视频中提取语言查询并注入到节点表示，避免仅凭视觉相似度连边；设计递归的图精炼机制，迭代更新节点与边表示，最终对帧节点进行关键帧分类。方法适用于监督和无监督设置，并覆盖通用与查询式摘要。

Result: 在多个基准数据集上（通用与查询式任务）实现SOTA性能，且在监督与无监督两种设置下均优于现有方法。

Conclusion: 语言引导的时空图递归建模能有效捕获对象与帧的语义关系，提升关键帧选择的准确性与泛化能力，证明结合语言与图结构是视频摘要的有效范式。

Abstract: Video summarization aims to select keyframes that are visually diverse and
can represent the whole story of a given video. Previous approaches have
focused on global interlinkability between frames in a video by temporal
modeling. However, fine-grained visual entities, such as objects, are also
highly related to the main content of the video. Moreover, language-guided
video summarization, which has recently been studied, requires a comprehensive
linguistic understanding of complex real-world videos. To consider how all the
objects are semantically related to each other, this paper regards video
summarization as a language-guided spatiotemporal graph modeling problem. We
present recursive spatiotemporal graph networks, called VideoGraph, which
formulate the objects and frames as nodes of the spatial and temporal graphs,
respectively. The nodes in each graph are connected and aggregated with graph
edges, representing the semantic relationships between the nodes. To prevent
the edges from being configured with visual similarity, we incorporate language
queries derived from the video into the graph node representations, enabling
them to contain semantic knowledge. In addition, we adopt a recursive strategy
to refine initial graphs and correctly classify each frame node as a keyframe.
In our experiments, VideoGraph achieves state-of-the-art performance on several
benchmarks for generic and query-focused video summarization in both supervised
and unsupervised manners. The code is available at
https://github.com/park-jungin/videograph.

</details>


### [32] [Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning](https://arxiv.org/abs/2509.05606)
*Juan Yeo,Ijun Jang,Taesup Kim*

Main category: cs.CV

TL;DR: 提出PaKA：通过教师-学生的密集特征分布对齐来提升自监督密集表示，利用补丁级核对齐和针对密集学习的增强策略，在多项密集视觉基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自监督多聚焦于全局表示，难以捕获密集预测所需的局部语义与空间精度；需要把已学到的语义知识迁移到密集特征空间。

Method: 在预训练表征上继续进行自监督学习，采用教师-学生框架对齐密集特征分布；提出补丁级核对齐（PaKA）目标，匹配补丁间统计依赖与结构关系；并设计适配密集表示学习的数据增强策略。

Result: 在多种密集视觉任务基准上取得最新最优（SOTA）性能，验证方法有效。

Conclusion: 通过PaKA对齐与专门的增强，可将全局预训练的语义知识成功迁移至密集特征，显著提升密集预测任务表现。

Abstract: Dense representations are essential for vision tasks that require spatial
precision and fine-grained detail. While most self-supervised representation
learning methods focus on global representations that summarize the image as a
whole, such approaches often fall short in capturing the localized semantics
necessary for dense prediction tasks. To overcome these limitations, we propose
a framework that builds on pretrained representations through additional
self-supervised learning, aiming to transfer existing semantic knowledge into
the dense feature space. Our method aligns the distributions of dense features
between a teacher and a student model. Specifically, we introduce Patch-level
Kernel Alignment (PaKA), a simple yet effective alignment objective that
captures statistical dependencies, thereby matching the structural
relationships of dense patches across the two models. In addition, we
investigate augmentation strategies specifically designed for dense
representation learning. Our framework achieves state-of-the-art results across
a variety of dense vision benchmarks, demonstrating the effectiveness of our
approach.

</details>


### [33] [SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](https://arxiv.org/abs/2509.05614)
*Hanzhen Wang,Jiaming Xu,Jiayi Pan,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 提出SpecPrune-VLA：在不改训练的前提下，利用局部(当前动作)与全局(历史动作)信息进行两级token剪枝，并配合轻量控制器，提升VLA推理速度且几乎不损成功率。


<details>
  <summary>Details</summary>
Motivation: 现有对VLA的token剪枝多仅依赖当前动作的局部信息，忽略历史动作的全局上下文，导致成功率大幅下降(>20%)且加速有限。观察到连续动作间高度相似性，动机是利用这种时序冗余实现更聪明的剪枝。

Method: 训练无关(Training-free)的两级剪枝与启发式控制：1) 动作级静态剪枝：结合历史全局与当前局部上下文，预先减少每个动作的视觉token；2) 层级动态剪枝：依据各层重要性在前向时对token自适应裁剪；3) 轻量动作感知控制器：按动作粗/细粒度(速度 proxy)调整剪枝力度，因细粒度动作对剪枝更敏感。

Result: 在LIBERO基准上，相比OpenVLA-OFT，在A800获得1.46×加速、在RTX 3090获得1.57×加速，成功率几乎无损。

Conclusion: 结合全局历史与局部上下文的两级剪枝并用动作敏感控制，可在保持性能的同时显著加速VLA推理；方法简单、训练无关，具备实际部署价值。

Abstract: Pruning accelerates compute-bound models by reducing computation. Recently
applied to Vision-Language-Action (VLA) models, existing methods prune tokens
using only local info from current action, ignoring global context from prior
actions, causing >20% success rate drop and limited speedup. We observe high
similarity across consecutive actions and propose leveraging both local
(current) and global (past) info for smarter token selection. We introduce
SpecPrune-VLA, a training-free method with two-level pruning and heuristic
control: (1) Static pruning at action level: uses global history and local
context to reduce visual tokens per action; (2) Dynamic pruning at layer level:
prunes tokens per layer based on layer-specific importance; (3) Lightweight
action-aware controller: classifies actions as coarse/fine-grained (by speed),
adjusting pruning aggressiveness since fine-grained actions are
pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times
speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.
OpenVLA-OFT, with negligible success rate loss.

</details>


### [34] [SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.05625)
*Kien Nguyen,Anh Tran,Cuong Pham*

Main category: cs.CV

TL;DR: 提出SuMa方法，通过将待擦除概念的目标子空间映射到参考子空间来中和，从而在文本到图像扩散模型中稳健移除窄概念（如版权角色、名人）且保持图像质量；在四类任务上优于或匹配SOTA的稳健性与效果。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法难以同时保证稳健移除（不被prompt绕过）与生成质量，尤其对“窄概念”（名人、特定角色、具体风格/实例），因其与相邻非目标概念距离近、需要细粒度操控；为应对版权/合规需求，需要能精准且高质量地擦除这些概念。

Method: 提出Subspace Mapping（SuMa）：1）从模型中提取代表目标概念的子空间；2）构建参考子空间，使与目标子空间的距离最小；3）通过映射将目标子空间中和到参考子空间，从而在扩散过程中抑制目标概念激活，同时尽量不影响其他特征，达到稳健擦除与质量保持的平衡。

Result: 在四项任务（子类擦除、名人擦除、艺术风格擦除、实例擦除）上，SuMa在图像质量上可与以“效果”为导向的方法相当，同时在“完整性/稳健性”指标上与以完整擦除为目标的方法相当或更好，整体优于当前SOTA。

Conclusion: SuMa通过子空间映射实现对窄概念的稳健、有效擦除，兼顾合规与生成质量，填补了现有方法在窄概念处理上的空白，并在多任务上展示了与SOTA相当或更优的表现。

Abstract: The rapid growth of text-to-image diffusion models has raised concerns about
their potential misuse in generating harmful or unauthorized contents. To
address these issues, several Concept Erasure methods have been proposed.
However, most of them fail to achieve both robustness, i.e., the ability to
robustly remove the target concept., and effectiveness, i.e., maintaining image
quality. While few recent techniques successfully achieve these goals for NSFW
concepts, none could handle narrow concepts such as copyrighted characters or
celebrities. Erasing these narrow concepts is critical in addressing copyright
and legal concerns. However, erasing them is challenging due to their close
distances to non-target neighboring concepts, requiring finer-grained
manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel
method specifically designed to achieve both robustness and effectiveness in
easing these narrow concepts. SuMa first derives a target subspace representing
the concept to be erased and then neutralizes it by mapping it to a reference
subspace that minimizes the distance between the two. This mapping ensures the
target concept is robustly erased while preserving image quality. We conduct
extensive experiments with SuMa across four tasks: subclass erasure, celebrity
erasure, artistic style erasure, and instance erasure and compare the results
with current state-of-the-art methods. Our method achieves image quality
comparable to approaches focused on effectiveness, while also yielding results
that are on par with methods targeting completeness.

</details>


### [35] [Self-supervised Learning for Hyperspectral Images of Trees](https://arxiv.org/abs/2509.05630)
*Moqsadur Rahman,Saurav Kumar,Santosh S. Palmate,M. Shahriar Hossain*

Main category: cs.CV

TL;DR: 论文提出一种自监督方法，从作物田间的航拍高光谱影像中学习反映植被性质的树木表征；在下游任务中，这种学得的嵌入优于直接使用手工提取的高光谱植被属性。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感在精准农业中重要，但标注稀缺导致监督学习困难；直接使用手工计算的植被属性作为树木表征在机器学习任务中效果有限，需要一种无需大量标签、能更充分挖掘高光谱信息的学习方式。

Method: 采用自监督学习框架，从航拍高光谱影像中学习神经网络嵌入，使嵌入空间与植被性质相关（如通过对比/预文本任务等）以构建树木表征；将学得嵌入作为树的特征用于后续任务。

Result: 实验显示，基于所学习的植被性质相关嵌入构建的树表示，在多个下游机器学习任务上优于直接使用高光谱植被属性的表示。

Conclusion: 自监督学习可在无/少标签条件下从高光谱数据中提取更有效的植被相关表示，提升精准农业相关下游任务表现，优于传统基于手工属性的做法。

Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a
critical impetus to precision agriculture. Analysis of the hyperspectral images
with limited or no labels is challenging. This paper focuses on self-supervised
learning to create neural network embeddings reflecting vegetation properties
of trees from aerial hyperspectral images of crop fields. Experimental results
demonstrate that a constructed tree representation, using a vegetation
property-related embedding space, performs better in downstream machine
learning tasks compared to the direct use of hyperspectral vegetation
properties as tree representations.

</details>


### [36] [Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh](https://arxiv.org/abs/2509.05652)
*Ha Meem Hossain,Pritam Nath,Mahitun Nesa Mahi,Imtiaz Uddin,Ishrat Jahan Eiste,Syed Nasibur Rahman Ratul,Md Naim Uddin Mozumdar,Asif Mohammed Saad*

Main category: cs.CV

TL;DR: 在孟加拉本地道路环境与车种下，比较6个YOLO变体在自建29类车辆数据集上的检测效果：YOLOv11x精度最高但推理最慢，中等模型（YOLOv8m/11m）精度-速度更均衡；罕见类别因数据不平衡表现极差，存在相似车辆的混淆问题。


<details>
  <summary>Details</summary>
Motivation: 通用（非孟加拉）训练的车辆检测模型难以识别当地特有车辆与复杂路况，影响自动驾驶在发展中地区的部署与安全，亟需面向本地车种与环境的专门评测与基线。

Method: 构建含29类（含“Desi Nosimon”“Leguna”“Battery Rickshaw”“CNG”等）的高分辨率(1920x1080)本地数据集，手机拍摄、LabelImg标注为YOLO框；评估6个YOLO变体（含v8与v11、不同尺寸），对比mAP@0.5、mAP@0.5:0.95、召回、F1与推理时延，并分析混淆矩阵与长尾类别表现。

Result: YOLOv11x最佳精度：mAP@0.5=63.7%、mAP@0.5:0.95=43.8%、召回61.4%、F1=61.6%，但推理45.8ms/图；中等模型YOLOv8m与YOLOv11m的mAP@0.5分别为62.5%与61.8%，时延约14–15ms，达到性能-速度折中；长尾类（如Construction Vehicles、Desi Nosimon）近乎零准确率；相似外观类（Mini Truck vs Mini Covered Van）误判频繁。

Conclusion: 针对孟加拉交通场景的本地化数据与评测可显著提升适配性；在实际部署中宜选择中等尺寸YOLO以兼顾实时性与精度；需通过数据重采样/增广、类别重加权与细粒度区分策略缓解长尾与相似类混淆，为发展中地区自动驾驶感知奠定基线。

Abstract: Vehicle detection systems trained on Non-Bangladeshi datasets struggle to
accurately identify local vehicle types in Bangladesh's unique road
environments, creating critical gaps in autonomous driving technology for
developing regions. This study evaluates six YOLO model variants on a custom
dataset featuring 29 distinct vehicle classes, including region-specific
vehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and
``CNG''. The dataset comprises high-resolution images (1920x1080) captured
across various Bangladeshi roads using mobile phone cameras and manually
annotated using LabelImg with YOLO format bounding boxes. Performance
evaluation revealed YOLOv11x as the top performer, achieving 63.7\% mAP@0.5,
43.8\% mAP@0.5:0.95, 61.4\% recall, and 61.6\% F1-score, though requiring 45.8
milliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m)
struck an optimal balance, delivering robust detection performance with mAP@0.5
values of 62.5\% and 61.8\% respectively, while maintaining moderate inference
times around 14-15 milliseconds. The study identified significant detection
challenges for rare vehicle classes, with Construction Vehicles and Desi
Nosimons showing near-zero accuracy due to dataset imbalances and insufficient
training samples. Confusion matrices revealed frequent misclassifications
between visually similar vehicles, particularly Mini Trucks versus Mini Covered
Vans. This research provides a foundation for developing robust object
detection systems specifically adapted to Bangladesh traffic conditions,
addressing critical needs in autonomous vehicle technology advancement for
developing regions where conventional generic-trained models fail to perform
adequately.

</details>


### [37] [EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation](https://arxiv.org/abs/2509.05659)
*Guandong Li,Zhaobin Chu*

Main category: cs.CV

TL;DR: EditIDv2是一种无需微调的角色编辑方法，针对长文本和高复杂叙事，借助对PerceiverAttention的分解、引入ID损失、与扩散模型的联合动态训练，以及离线融合的ID特征集成模块，在极少数据“润滑”下实现多层语义深度编辑并保持身份一致性，在IBench上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有角色编辑在简单提示下有效，但在长文本、多语义层次与时序逻辑的复杂叙事中，易出现编辑能力下降、语义理解偏差与身份一致性崩溃。因此需要一种能在复杂语境下稳定编辑且保持角色身份一致、并能处理长提示的方案。

Method: 围绕ID特征集成模块对“可编辑性注入”的影响进行深入探讨与改进：1) 细致分解PerceiverAttention；2) 设计ID loss以约束身份一致性；3) 与扩散模型进行联合动态训练；4) 对集成模块采用离线融合策略；同时强调在极少数据条件下完成训练/适配。

Result: 在复杂叙事与长提示场景中，实现了深度、多层次语义的稳定编辑，同时保持身份一致性；在IBench评测中获得优异成绩。

Conclusion: EditIDv2在少量数据与无需调参的设定下，解决了长文本复杂叙事中的编辑衰减与身份崩溃问题，提升了多层语义理解与一致性，适用于高质量图像生成与长提示任务。

Abstract: We propose EditIDv2, a tuning-free solution specifically designed for
high-complexity narrative scenes and long text inputs. Existing character
editing methods perform well under simple prompts, but often suffer from
degraded editing capabilities, semantic understanding biases, and identity
consistency breakdowns when faced with long text narratives containing multiple
semantic layers, temporal logic, and complex contextual relationships. In
EditID, we analyzed the impact of the ID integration module on editability. In
EditIDv2, we further explore and address the influence of the ID feature
integration module. The core of EditIDv2 is to discuss the issue of editability
injection under minimal data lubrication. Through a sophisticated decomposition
of PerceiverAttention, the introduction of ID loss and joint dynamic training
with the diffusion model, as well as an offline fusion strategy for the
integration module, we achieve deep, multi-level semantic editing while
maintaining identity consistency in complex narrative environments using only a
small amount of data lubrication. This meets the demands of long prompts and
high-quality image generation, and achieves excellent results in the IBench
evaluation.

</details>


### [38] [OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation](https://arxiv.org/abs/2509.05661)
*Xiaomeng Zhu,Changwei Wang,Haozhe Wang,Xinyu Liu,Fangzhen Lin*

Main category: cs.CV

TL;DR: 提出将场景图预测任务分解为两步，并用纯文本的语言模型进行“语言化的场景图预测”（LSGA）。核心方法 OOTSM 先预测物体的出现/消失，再生成细粒度的人-物关系；在基准上优于零样本闭源 API，并与视觉模型结合在 SGA 上达成新的 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有场景图预期（SGA）方法主要依赖视觉线索，难以显式注入常识知识，导致对长时预测鲁棒性不足。作者希望显式利用常识来理解对象、概念与关系，从而提升长期预测性能与可解释性。

Method: 将 SGA 解耦为两步：(1) 用场景图捕获模型把视频片段转为场景图序列；(2) 用纯文本模型预测未来帧的场景图（LSGA）。为 LSGA 提出面向对象的两阶段方法 OOTSM：第一阶段由大型语言模型预测对象的出现与消失；第二阶段再生成详细的人-物关系。使用开源 LLM 微调，并与零样本 API（GPT-4o/mini、DeepSeek-V3）对比；同时将 OOTSM 与视觉方法 STTran++ 组合进行端到端 SGA 实验。

Result: 在 LSGA 基准（基于 Action Genome 标注构建）上，微调的开源 LLM 在预测质量上优于多种零样本 API。在完整 SGA 设定下，与 STTran++ 结合后达到新的 SOTA：短期 mR@10 提升 3.4%，长期 mR@50 大幅提升 21.9%。

Conclusion: 显式利用语言模型的常识推理能力并将对象与关系预测解耦，可以显著提升场景图的长期与短期预测性能；LSGA 本身具有独立研究价值，且与视觉模型结合能带来可观增益。

Abstract: A scene graph is a structured represention of objects and their relationships
in a scene. Scene Graph Anticipation (SGA) involves predicting future scene
graphs from video clips, enabling applications as intelligent surveillance and
human-machine collaboration. Existing SGA approaches primarily leverage visual
cues, often struggling to integrate valuable commonsense knowledge, thereby
limiting long-term prediction robustness. To explicitly leverage such
commonsense knowledge, we propose a new approach to better understand the
objects, concepts, and relationships in a scene graph. Our approach decouples
the SGA task in two steps: first a scene graph capturing model is used to
convert a video clip into a sequence of scene graphs, then a pure text-based
model is used to predict scene graphs in future frames. Our focus in this work
is on the second step, and we call it Linguistic Scene Graph Anticipation
(LSGA) and believes it should have independent interest beyond the use in SGA
discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method
(OOTSM) where an Large Language Model (LLM) first forecasts object appearances
and disappearances before generating detailed human-object relations. We
conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we
evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o,
GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome
annotations. For SGA, we combine our OOTSM with STTran++ from, and our
experiments demonstrate effective state-of-the-art performance: short-term
mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves
dramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.

</details>


### [39] [WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising](https://arxiv.org/abs/2509.05662)
*Wasikul Islam*

Main category: cs.CV

TL;DR: 论文将高能物理中“pileup”减污的物理先验（守恒、局域、隔离）嵌入到神经网络去做图像去噪，提出一套PU启发的架构（受约束残差CNN、其高斯噪声变体、以及融合这些先验的WIPUNet），在高噪声下展现更强鲁棒性，尤其在σ高时优于常规模型。


<details>
  <summary>Details</summary>
Motivation: 图像去噪在强噪声/强腐蚀情形下，纯数据驱动模型易退化；而在高能物理里，处理pileup的策略依赖明确的物理约束（如能量守恒、局部性、隔离性）。作者动机是把这些物理启发的归纳偏置转译到视觉去噪中，检验其是否能提升在强噪声下的稳健性，而非追求SOTA。

Method: 1) 定义一族PU启发的去噪器：带守恒约束的残差CNN；其适配高斯噪声的变体；以及将这些模块化先验整合进UNet骨干的WIPUNet。2) 在网络结构/损失层面嵌入守恒、局域、隔离等约束；3) 在CIFAR-10与BSD500上对不同高斯噪声强度进行比较评测。

Result: 在CIFAR-10上，PU启发的CNN与标准基线相当，但随着噪声σ增大（15→100），WIPUNet与基线的性能差距逐步扩大；在BSD500上观察到相同趋势，说明物理先验带来高噪声情形下的稳定性提升。

Conclusion: 将pileup缓解的物理原则转化为网络的归纳偏置并整合进UNet，可在强噪声下提升鲁棒性而无需重型SOTA技巧。贡献：提出模块化物理先验、完成与UNet的集成、并实证显示高噪声下的稳健增益。

Abstract: In high-energy particle physics, collider measurements are contaminated by
"pileup", overlapping soft interactions that obscure the hard-scatter signal of
interest. Dedicated subtraction strategies exploit physical priors such as
conservation, locality, and isolation. Inspired by this analogy, we investigate
how such principles can inform image denoising by embedding physics-guided
inductive biases into neural architectures. This paper is a proof of concept:
rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether
physics-inspired priors improve robustness under strong corruption.
  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with
conservation constraints, its Gaussian-noise variants, and the Weighted
Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which
integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at
$\sigma\in\{15,25,50,75,100\}$, PU-inspired CNNs are competitive with standard
baselines, while WIPUNet shows a \emph{widening margin} at higher noise.
Complementary BSD500 experiments show the same trend, suggesting
physics-inspired priors provide stability where purely data-driven models
degrade. Our contributions are: (i) translating pileup-mitigation principles
into modular inductive biases; (ii) integrating them into UNet; and (iii)
demonstrating robustness gains at high noise without relying on heavy SOTA
machinery.

</details>


### [40] [Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance](https://arxiv.org/abs/2509.05669)
*Weijie Shen,Xinrui Wang,Yuanqi Nie,Apiradee Boonmee*

Main category: cs.CV

TL;DR: 提出CAMVR框架，通过记忆与自适应视觉聚焦，提升LVLM在多轮视觉推理中的连贯性与准确性，并在VisDial、A-OKVQA(改)与新MTIF数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LLM/LVLM在多轮对话与复杂视觉推理中易出现碎片化推理、上下文丢失与幻觉，缺乏稳健的跨回合上下文建模与与视觉注意的动态调节。

Method: 提出CAMVR框架：1) 视觉-文本上下文记忆单元(VCMU)，动态读写各轮的关键视觉特征、文本语义与跨模态对应；2) 自适应视觉聚焦引导(AVFG)，基于VCMU的上下文动态调节视觉编码器对相关区域的注意；3) 多层级推理整合，将当前输入与历史记忆协同用于生成，确保语义连贯。

Result: 在VisDial、改版A-OKVQA及新提出的多轮指令跟随(MTIF)数据集上进行大量实验，CAMVR在多项指标上稳定超过现有方法，达到SOTA表现。

Conclusion: 通过显式跨回合记忆与基于上下文的视觉注意调控，CAMVR显著缓解多轮视觉推理中的上下文丢失与幻觉问题，提升连贯性与推理深度，具有广泛可迁移性。

Abstract: Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)
excel in single-turn tasks but face significant challenges in multi-turn
interactions requiring deep contextual understanding and complex visual
reasoning, often leading to fragmented reasoning, context loss, and
hallucinations. To address these limitations, we propose Context-Aware
Multi-Turn Visual Reasoning (CAMVR), a novel framework designed to empower
LVLMs with robust and coherent multi-turn visual-textual inference
capabilities. CAMVR introduces two key innovations: a Visual-Textual Context
Memory Unit (VCMU), a dynamic read-write memory network that stores and manages
critical visual features, textual semantic representations, and their
cross-modal correspondences from each interaction turn; and an Adaptive Visual
Focus Guidance (AVFG) mechanism, which leverages the VCMU's context to
dynamically adjust the visual encoder's attention to contextually relevant
image regions. Our multi-level reasoning integration strategy ensures that
response generation is deeply coherent with both current inputs and accumulated
historical context. Extensive experiments on challenging datasets, including
VisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following
(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art
performance.

</details>


### [41] [MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics](https://arxiv.org/abs/2509.05670)
*Gašper Podobnik,Tomaž Vrtovec*

Main category: cs.CV

TL;DR: 论文指出分割评估中的距离类指标在不同实现之间存在巨大偏差，并提出基于网格面的MeshMetrics框架以更精确稳健地计算这些指标。


<details>
  <summary>Details</summary>
Motivation: 图像分割研究繁荣但复现性危机凸显，其中性能评估环节的指标选择与实现是关键痛点。现有工作多关注指标选择，忽视实现可靠性；距离类指标在常用工具中实现存在陷阱，导致同一分割对上结果可相差巨大（如HD>100mm、NSD差30个百分点）。

Method: 提出MeshMetrics：基于三角网格/曲面而非体素栅格计算距离类指标，通过理论分析与实证验证，减少离散化伪影（如距离量化），提升计算精度与一致性。

Result: 相较已有开源工具，MeshMetrics在精度与稳定性更高，对离散化误差显著不敏感；实验证明传统工具在HD与NSD上可能产生大偏差，而MeshMetrics给出更准确一致的结果。

Conclusion: MeshMetrics缓解了距离指标实现不一致带来的复现性问题，为分割评估提供更可靠的度量框架；已开源发布，便于社区采用与验证。

Abstract: The surge of research in image segmentation has yielded remarkable
performance gains but also exposed a reproducibility crisis. A major
contributor is performance evaluation, where both selection and implementation
of metrics play critical roles. While recent efforts have improved the former,
the reliability of metric implementation has received far less attention.
Pitfalls in distance-based metric implementation can lead to considerable
discrepancies between common open-source tools, for instance, exceeding 100 mm
for the Hausdorff distance and 30%pt for the normalized surface distance for
the same pair of segmentations. To address these pitfalls, we introduce
MeshMetrics, a mesh-based framework that provides a more precise computation of
distance-based metrics than conventional grid-based approaches. Through
theoretical analysis and empirical validation, we demonstrate that MeshMetrics
achieves higher accuracy and precision than established tools, and is
substantially less affected by discretization artifacts, such as distance
quantization. We release MeshMetrics as an open-source Python package,
available at https://github.com/gasperpodobnik/MeshMetrics.

</details>


### [42] [Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization](https://arxiv.org/abs/2509.05695)
*Jingwei Peng,Zhixuan Qiu,Boyu Jin,Surasakdi Siripong*

Main category: cs.CV

TL;DR: 提出LVLM-VAR：将视频转为语义动作token并配合指令，用LoRA微调的LVLM进行动作分类与解释，性能与可解释性兼顾，在NTU系列数据集达SOTA或强竞争。


<details>
  <summary>Details</summary>
Motivation: 传统视频动作识别在深层语义理解、复杂上下文建模与细粒度区分上受限；希望借助预训练多模态大模型的语义推理与语言生成能力，提升准确率并提供可解释的自然语言理由。

Method: 构建Video-to-Semantic-Tokens (VST) 模块，将原始视频序列转换为语义与时间一致的离散“语义动作token”，形成可被LVLM理解的“动作叙事”；再将这些token与自然语言指令一同输入经LoRA微调的LVLM（如LLaVA-13B）进行动作分类与语义推理，并生成解释。

Result: 在NTU RGB+D与NTU RGB+D 120等基准上取得SOTA或强竞争结果：如NTU RGB+D X-Sub 94.1%、NTU RGB+D 120 X-Set 90.0%，同时生成自然语言解释，显著提升可解释性。

Conclusion: 利用VST将视频语义结构化为token并与指令驱动的LVLM结合，可同时提升视频动作识别的准确率与可解释性；LVLM-VAR证明了LVLM在视频动作理解中的有效性。

Abstract: Human action recognition often struggles with deep semantic understanding,
complex contextual information, and fine-grained distinction, limitations that
traditional methods frequently encounter when dealing with diverse video data.
Inspired by the remarkable capabilities of large language models, this paper
introduces LVLM-VAR, a novel framework that pioneers the application of
pre-trained Vision-Language Large Models (LVLMs) to video action recognition,
emphasizing enhanced accuracy and interpretability. Our method features a
Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video
sequences into discrete, semantically and temporally consistent "semantic
action tokens," effectively crafting an "action narrative" that is
comprehensible to an LVLM. These tokens, combined with natural language
instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)
for robust action classification and semantic reasoning. LVLM-VAR not only
achieves state-of-the-art or highly competitive performance on challenging
benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant
improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),
but also substantially boosts model interpretability by generating natural
language explanations for its predictions.

</details>


### [43] [JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization](https://arxiv.org/abs/2509.05696)
*Hongyu Zhou,Yunzhou Zhang,Tingsong Huang,Fawei Ge,Man Qi,Xichen Zhang,Yizhong Zhang*

Main category: cs.CV

TL;DR: 提出JRN-Geo，通过融合RGB语义与法线几何结构应对跨视角无人机-卫星地理定位；引入差异感知融合与联合约束交互，并用3D地理增广模拟视角变化，在University-1652与SUES-200上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 跨视角（如UAV对地/卫星）图像存在剧烈视角与外观差异，单纯依赖RGB语义特征难以获得视角不变表征；需要引入对视角稳健的空间几何结构信息并实现有效的多模态深度融合。

Method: - 输入模态：RGB图与法线（Normal）图。
- 框架：双分支特征提取，分别学习语义与几何结构表征。
- 差异感知融合模块（DAFM）：感知并对齐两模态间差异，进行深层次特征融合。
- 联合约束交互聚合（JCIA）：在交互与聚合阶段引入联合约束，强化语义-结构一致性与互补性。
- 3D地理增广：基于地理/几何先验生成潜在视角变化样本，促进学习视角不变特征。

Result: 在University-1652与SUES-200数据集上，对抗复杂视角变化表现稳健，达到或超过现有方法的最新水平（SOTA）。

Conclusion: 融合法线几何与RGB语义，并通过差异感知与联合约束机制以及3D增广，可显著提升跨视角地理定位的鲁棒性与准确性。

Abstract: Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle
(UAV) localization and navigation. However, significant challenges arise from
the drastic viewpoint differences and appearance variations between images.
Existing methods predominantly rely on semantic features from RGB images, often
neglecting the importance of spatial structural information in capturing
viewpoint-invariant features. To address this issue, we incorporate geometric
structural information from normal images and introduce a Joint perception
network to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a
dual-branch feature extraction framework, leveraging a Difference-Aware Fusion
Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to
enable deep fusion and joint-constrained semantic and structural information
representation. Furthermore, we propose a 3D geographic augmentation technique
to generate potential viewpoint variation samples, enhancing the network's
ability to learn viewpoint-invariant features. Extensive experiments on the
University-1652 and SUES-200 datasets validate the robustness of our method
against complex viewpoint ariations, achieving state-of-the-art performance.

</details>


### [44] [Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis](https://arxiv.org/abs/2509.05703)
*Ragib Amin Nihal,Benjamin Yen,Takeshi Ashizawa,Kazuhiro Nakadai*

Main category: cs.CV

TL;DR: 探索VLM能否直接从海洋哺乳动物声谱图中提取模式，结合LLM验证以构建领域知识，从而无需手工标注或再训练即可适配声学数据。


<details>
  <summary>Details</summary>
Motivation: 海洋哺乳动物声学研究依赖声谱图解读，但现有VLM未在此类专业可视化上训练，导致迁移困难与高标注成本。需要一种无需昂贵标注/再训练、能从声谱图中提取有用模式并逐步积累领域知识的方法。

Method: 提出一个框架：用通用VLM对声谱图进行视觉解释（描述结构、模式、事件），再由LLM基于规则/知识库进行文本层面的校验与一致性检查，循环整合为可扩展的领域知识。核心在VLM视觉感知＋LLM验证/过滤/归纳，无需对模型再训练或人工逐帧标注。

Result: 实验证明VLM可在一定程度上识别声谱图中的显著结构，并在LLM验证与知识整合的帮助下，形成对声学数据的有用描述，展示了在无标注、无再训练条件下的适配潜力。

Conclusion: 通用VLM结合LLM验证可为海洋生物声学提供零/少样本的谱图解读路径，减少标注与再训练需求；该流程可逐步构建领域知识，提升对声学数据的适配性。

Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic
spectrograms. Vision Language Models (VLMs) are not trained on these
domain-specific visualizations. We investigate whether VLMs can extract
meaningful patterns from spectrograms visually. Our framework integrates VLM
interpretation with LLM-based validation to build domain knowledge. This
enables adaptation to acoustic data without manual annotation or model
retraining.

</details>


### [45] [LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction](https://arxiv.org/abs/2509.05728)
*Niels Balemans,Ali Anwar,Jan Steckel,Siegfried Mercelis*

Main category: cs.CV

TL;DR: 论文提出LiDAR-BIND-T，在原多模态传感器到LiDAR潜空间绑定框架上，引入显式时间一致性：连续潜向量对齐、运动对齐变换损失、窗口化时间融合模块，并改进架构保留空间结构；在雷达/声呐到LiDAR翻译与基于Cartographer的SLAM中显著降低轨迹误差、提升占据图与时空一致性；还提出基于FVMD与相关峰距的新时间质量指标。


<details>
  <summary>Details</summary>
Motivation: 原LiDAR-BIND虽能将异构传感器映射到LiDAR潜空间，但时间一致性不足，导致在SLAM等下游任务中运动估计与地图构建易受抖动与漂移影响。需要一种既保持即插即用的跨模态融合，又显式增强时序稳定性的方案，并提供衡量时序质量的实用指标。

Method: 在多模态到LiDAR潜空间的翻译框架中加入三项：1) 时间嵌入相似度，约束相邻帧潜表征对齐；2) 运动对齐变换损失，使预测位移与LiDAR真值位移匹配；3) 窗口化时间融合，利用专门的时序模块在时间窗口内融合信息；并更新网络架构以更好保留空间结构。同时提出基于Fréchet Video Motion Distance (FVMD) 的指标与相关峰距离指标评估时序质量与SLAM表现。

Result: 在雷达/声呐到LiDAR的翻译任务与Cartographer-SLAM上，相比基线获得更低绝对轨迹误差（ATE）、更好的占据栅格地图准确度与更高的时间与空间一致性；新提出的FVMD变体与相关峰距指标能有效反映时序质量并与SLAM性能相关。

Conclusion: LiDAR-BIND-T在保持模块化、可插拔多模态融合的同时，显著增强时序稳定性与空间一致性，改善SLAM鲁棒性与性能；提出的时序指标为实际系统提供有用的评估工具。

Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that
binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,
with mechanisms that explicitly enforce temporal consistency. We introduce
three contributions: (i) temporal embedding similarity that aligns consecutive
latents, (ii) a motion-aligned transformation loss that matches displacement
between predictions and ground truth LiDAR, and (iii) windows temporal fusion
using a specialised temporal module. We further update the model architecture
to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR
translation demonstrate improved temporal and spatial coherence, yielding lower
absolute trajectory error and better occupancy map accuracy in
Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose
different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a
correlation-peak distance metric providing practical temporal quality
indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or
LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially
enhancing temporal stability, resulting in improved robustness and performance
for downstream SLAM.

</details>


### [46] [Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras](https://arxiv.org/abs/2509.05740)
*Xinyu Zhang,Kai Huang,Junqiao Zhao,Zihan Yuan,Tiantian Feng*

Main category: cs.CV

TL;DR: 提出Multi-LVI-SAM：以全景视觉特征模型统一多鱼眼相机，与LiDAR与IMU在因子图中紧耦合，实现更高精度与鲁棒位姿估计；并通过外参补偿解决多视角三角化不一致，提升回环与全局优化效果。


<details>
  <summary>Details</summary>
Motivation: 多相机+LiDAR+IMU的里程计能提升覆盖与鲁棒性，但传统做法需分别处理多路相机，带来冗余、约束不一致与回环困难；鱼眼相机宽视场但多视角联合三角化存在帧系不一致导致误差。需要统一的视觉表示与补偿机制，提升多相机约束质量并简化系统。

Method: 1) 提出全景视觉特征模型，将多鱼眼相机观测统一到单一全景表示，作为全局几何优化载体，支持无缝回环与全局位姿优化；2) 为解决各相机坐标与全景模型坐标错配导致的三角化不一致，提出外参补偿方法，提高跨视角特征一致性并降低三角化与优化误差；3) 将上述视觉模块与LiDAR、IMU在因子图框架中紧耦合，形成Multi-LVI-SAM。

Result: 在公开数据集上进行大量实验，表明全景视觉特征模型显著提升多相机约束的质量与一致性，整体系统在精度与鲁棒性上优于现有多相机LVI系统。

Conclusion: 全景特征统一与外参补偿有效解决多鱼眼多视角融合中的一致性问题，结合因子图的紧耦合多传感器融合，可实现更准确、鲁棒的位姿估计，并简化系统设计与回环优化。

Abstract: We propose a multi-camera LiDAR-visual-inertial odometry framework,
Multi-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and
inertial sensors for highly accurate and robust state estimation. To enable
efficient and consistent integration of visual information from multiple
fisheye cameras, we introduce a panoramic visual feature model that unifies
multi-camera observations into a single representation. The panoramic model
serves as a global geometric optimization framework that consolidates
multi-view constraints, enabling seamless loop closure and global pose
optimization, while simplifying system design by avoiding redundant handling of
individual cameras. To address the triangulation inconsistency caused by the
misalignment between each camera's frame and the panoramic model's frame, we
propose an extrinsic compensation method. This method improves feature
consistency across views and significantly reduces triangulation and
optimization errors, leading to more accurate pose estimation. We integrate the
panoramic visual feature model into a tightly coupled LiDAR-visual-inertial
system based on a factor graph. Extensive experiments on public datasets
demonstrate that the panoramic visual feature model enhances the quality and
consistency of multi-camera constraints, resulting in higher accuracy and
robustness than existing multi-camera LiDAR-visual-inertial systems.

</details>


### [47] [Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation](https://arxiv.org/abs/2509.05746)
*Tianhao Guo,Bingjie Lu,Feng Wang,Zhengyang Lu*

Main category: cs.CV

TL;DR: 提出一个距离自适应的单幅图像超分辨框架：以变分模型为本、将退化建模为随深度变化的伪微分算子，网络实现离散梯度流并用深度条件卷积与光谱约束，显著提升含深度变化场景的SR性能。


<details>
  <summary>Details</summary>
Motivation: 现实成像退化并非空间不变：大气散射、景深、透视等使退化随距离变化。传统SR忽略几何与深度，导致在远近场、不同景深区域恢复欠佳。需要一个能在理论上描述并在实践中利用“深度-频谱”关系的自适应方法，以获得更稳健、可解释且性能更好的SR。

Method: 1) 变分框架：将SR表述为空间变异的逆问题，退化算子建模为具有距离相关谱特性的伪微分算子，给出跨深度范围的可分析重建界限。2) 神经实现：用级联残差块模拟离散梯度流，确保能量泛函的收敛到驻点；卷积核由深度条件化（depth-conditional）生成，实现动态、几何感知的正则。3) 光谱与正则：依据大气散射理论施加带宽/放大量限制，抑制远场频谱外推与噪声放大；自适应正则根据局部几何结构调整平滑强度。4) 核生成网络：学习从深度到重建滤波器的连续映射。5) 训练与评估：在多基准数据集上训练/验证。

Result: 在KITTI室外场景上，×2与×4获得36.89/0.9516与30.54/0.8721（PSNR/SSIM），分别较SOTA提升0.44 dB与0.36 dB；在深度变化场景显著优于现有方法，同时在传统基准上保持竞争力。

Conclusion: 首次给出具理论支撑的距离自适应SR框架：以伪微分算子与光谱约束刻画深度相关退化，并以梯度流型网络与深度条件核实现；在深度可变场景显著提升重建质量并保证稳定性与泛化。

Abstract: Single image super-resolution traditionally assumes spatially-invariant
degradation models, yet real-world imaging systems exhibit complex
distance-dependent effects including atmospheric scattering, depth-of-field
variations, and perspective distortions. This fundamental limitation
necessitates spatially-adaptive reconstruction strategies that explicitly
incorporate geometric scene understanding for optimal performance. We propose a
rigorous variational framework that characterizes super-resolution as a
spatially-varying inverse problem, formulating the degradation operator as a
pseudodifferential operator with distance-dependent spectral characteristics
that enable theoretical analysis of reconstruction limits across depth ranges.
Our neural architecture implements discrete gradient flow dynamics through
cascaded residual blocks with depth-conditional convolution kernels, ensuring
convergence to stationary points of the theoretical energy functional while
incorporating learned distance-adaptive regularization terms that dynamically
adjust smoothness constraints based on local geometric structure. Spectral
constraints derived from atmospheric scattering theory prevent bandwidth
violations and noise amplification in far-field regions, while adaptive kernel
generation networks learn continuous mappings from depth to reconstruction
filters. Comprehensive evaluation across five benchmark datasets demonstrates
state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM
at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by
0.44dB and 0.36dB respectively. This work establishes the first
theoretically-grounded distance-adaptive super-resolution framework and
demonstrates significant improvements on depth-variant scenarios while
maintaining competitive performance across traditional benchmarks.

</details>


### [48] [InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios](https://arxiv.org/abs/2509.05747)
*Leo Ho,Yinghao Huang,Dafei Qin,Mingyi Shi,Wangpok Tse,Wei Liu,Junichi Yamagishi,Taku Komura*

Main category: cs.CV

TL;DR: 提出InterAct数据集与扩散模型方法，从语音估计双人交互中的全身与面部动态；数据多模态、长时长、角色与情绪标注完备。


<details>
  <summary>Details</summary>
Motivation: 现有工作多仅单人或仅对话手势，假设朝向/位置基本不变，难以刻画真实场景中目标驱动、动态且语义一致的长时双人交互。

Method: 1) 采集InterAct多模态数据集：241段≥1分钟的双人互动，含语音、全身动作、面部表情，带角色与情绪标签；覆盖更大空间与复杂协作模式。2) 提出基于扩散模型的语音驱动双人交互生成：层级式回归躯干/四肢等身体动作；并引入新微调机制以提升嘴唇同步与表情精准度。

Result: InterAct展示了丰富多样、长时且复杂的个体动作与互动模式，超出既有数据集。所提扩散方法能从语音估计两人的面部与身体运动，并通过层级与微调策略提升身体动作结构性与唇形准确性。

Conclusion: 提供首个面向动态、目标驱动的长时双人交互多模态基准与方法；证明语音即可驱动生成语义一致的双人肢体与表情。数据与代码公开以促进后续研究。

Abstract: We address the problem of accurate capture of interactive behaviors between
two people in daily scenarios. Most previous works either only consider one
person or solely focus on conversational gestures of two people, assuming the
body orientation and/or position of each actor are constant or barely change
over each interaction. In contrast, we propose to simultaneously model two
people's activities, and target objective-driven, dynamic, and semantically
consistent interactions which often span longer duration and cover bigger
space. To this end, we capture a new multi-modal dataset dubbed InterAct, which
is composed of 241 motion sequences where two people perform a realistic and
coherent scenario for one minute or longer over a complete interaction. For
each sequence, two actors are assigned different roles and emotion labels, and
collaborate to finish one task or conduct a common interaction activity. The
audios, body motions, and facial expressions of both persons are captured.
InterAct contains diverse and complex motions of individuals and interesting
and relatively long-term interaction patterns barely seen before. We also
demonstrate a simple yet effective diffusion-based method that estimates
interactive face expressions and body motions of two people from speech inputs.
Our method regresses the body motions in a hierarchical manner, and we also
propose a novel fine-tuning mechanism to improve the lip accuracy of facial
expressions. To facilitate further research, the data and code is made
available at https://hku-cg.github.io/interact/ .

</details>


### [49] [Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation](https://arxiv.org/abs/2509.05751)
*Bingrui Zhao,Lin Yuanbo Wu,Xiangtian Fan,Deyin Liu,Lu Zhang,Ruyi He,Jialie Shen,Ximing Li*

Main category: cs.CV

TL;DR: PARSE-VOS提出一个无需训练、由大语言模型驱动的分层（由粗到细）跨文本与视频的推理框架，用于参考视频目标分割；通过将语言解析为结构化语义指令、生成候选时空轨迹并分两阶段（运动→姿态）识别目标，最终实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RVOS方法在将静态文本与动态视频对齐方面困难，尤其面对外观相似但运动/姿态不一致的多对象场景；整体式视觉-语言融合对复杂、组合式描述处理力不足，易产生歧义和错配。

Method: 1) 语义解析：用LLM将自然语言查询解析为结构化语义命令；2) 时空定位：依据解析语义生成所有潜在目标的候选轨迹（全候选集合）；3) 分层识别：先以LLM进行粗粒度的运动层面推理缩小候选；若仍有歧义，则触发细粒度姿态验证以最终判定；4) 输出目标的像素级分割掩码。框架为training-free。

Result: 在Ref-YouTube-VOS、Ref-DAVIS17与MeViS三个主流基准上取得SOTA表现（具体指标未给出）。

Conclusion: 分层、解析驱动的跨模态推理能有效缓解复杂语言描述与动态视觉对齐难题；通过先运动后姿态的两阶段判别，在无需额外训练的前提下提升RVOS精度与鲁棒性。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment an object of
interest throughout a video based on a language description. The prominent
challenge lies in aligning static text with dynamic visual content,
particularly when objects exhibiting similar appearances with inconsistent
motion and poses. However, current methods often rely on a holistic
visual-language fusion that struggles with complex, compositional descriptions.
In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework
powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine
reasoning across text and video domains. Our approach begins by parsing the
natural language query into structured semantic commands. Next, we introduce a
spatio-temporal grounding module that generates all candidate trajectories for
all potential target objects, guided by the parsed semantics. Finally, a
hierarchical identification module select the correct target through a
two-stage reasoning process: it first performs coarse-grained motion reasoning
with an LLM to narrow down candidates; if ambiguity remains, a fine-grained
pose verification stage is conditionally triggered to disambiguate. The final
output is an accurate segmentation mask for the target object.
\textbf{PARSE-VOS} achieved state-of-the-art performance on three major
benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.

</details>


### [50] [PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters](https://arxiv.org/abs/2509.05773)
*Zijian Chen,Wenjie Hua,Jinhao Li,Lirong Deng,Fan Du,Tingzhu Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出PictOBI-20k数据集与评测基准，用于评估多模态大模型在甲骨文象形字的“视觉释读”能力；实验发现通用LMM有初步能力但多依赖语言先验，视觉信息利用不足。


<details>
  <summary>Details</summary>
Motivation: 甲骨文是最早的中文书写形式，释读对理解早期人类生产方式关键。但受限于考古出土零散与语料稀缺，传统方法进展缓慢。LMM具备强视觉理解潜力，有望用于“以形释义”的视觉辨识与联想，需有专门数据与评测验证其可行性与不足。

Method: 构建PictOBI-20k：包含2万张甲骨文与实物图像，设计1.5万+多项选择题，用于视觉匹配与释读评测；并进行主观标注以分析人类与模型在视觉推理“参照点”的一致性。以通用LMM进行实验评测其视觉释读能力与注意力使用情况。

Result: 通用LMM在任务上表现出初步视觉释读能力，但总体上并未有效利用视觉线索，往往被语言先验主导；人机在视觉参照点上存在不一致。

Conclusion: PictOBI-20k为OBC视觉释读提供标准化评测资源与分析工具，可用于诊断与优化LMM对视觉注意与对齐；未来需针对OBC的视觉注意机制与训练策略进行改进。

Abstract: Deciphering oracle bone characters (OBCs), the oldest attested form of
written Chinese, has remained the ultimate, unwavering goal of scholars,
offering an irreplaceable key to understanding humanity's early modes of
production. Current decipherment methodologies of OBC are primarily constrained
by the sporadic nature of archaeological excavations and the limited corpus of
inscriptions. With the powerful visual perception capability of large
multimodal models (LMMs), the potential of using LMMs for visually deciphering
OBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed
to evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It
includes 20k meticulously collected OBC and real object images, forming over
15k multi-choice questions. We also conduct subjective annotations to
investigate the consistency of the reference point between humans and LMMs in
visual reasoning. Experiments indicate that general LMMs possess preliminary
visual decipherment skills, and LMMs are not effectively using visual
information, while most of the time they are limited by language priors. We
hope that our dataset can facilitate the evaluation and optimization of visual
attention in future OBC-oriented LMMs. The code and dataset will be available
at https://github.com/OBI-Future/PictOBI-20k.

</details>


### [51] [Posterior shape models revisited: Improving 3D reconstructions from partial data using target specific models](https://arxiv.org/abs/2509.05776)
*Jonathan Aellen,Florian Burkhardt,Thomas Vetter,Marcel Lüthi*

Main category: cs.CV

TL;DR: 提出一种在医学影像部分形状重建中，针对训练形状与目标形状姿态不一致而导致偏差的问题，对现有线性点分布模型进行目标特异的对齐与调整的方法；无需原始训练数据，预处理即可提升重建准确度与方差估计。


<details>
  <summary>Details</summary>
Motivation: 部分形状重建常依赖全形状的统计模型（PDM），但训练数据与目标局部形状若在位姿上未对齐，会造成系统性偏差，尤其当观察到的形状片段很小时更为严重。现有流程往往忽略这一点，导致重建与不确定性评估不可靠。需要一种在不重新训练、且计算高效的条件下，能将既有模型与目标对齐的策略。

Method: 在不访问原始训练数据的前提下，对既有线性形状模型进行一次性目标特异的姿态调整（预处理）。方法在理论上对平移具有精确对齐不变性，并对小角度旋转给出良好近似；保持线性模型的计算效率，同时修正由姿态错配引入的偏差与方差估计问题。

Result: 与未对齐的基线线性模型相比，该方法显著提升了部分形状重建的准确性与预测方差的合理性；在平移情形下可精确恢复“应当”的对齐模型，在小幅旋转下给出接近最优的近似。

Conclusion: 姿态对齐是部分形状重建中不可忽视的关键因素。所提方法可作为即插即用的预处理步骤，无需原始训练集，即可在保持线性效率的同时提升重建质量与不确定性估计，适合广泛集成到现有重建管线。

Abstract: In medical imaging, point distribution models are often used to reconstruct
and complete partial shapes using a statistical model of the full shape. A
commonly overlooked, but crucial factor in this reconstruction process, is the
pose of the training data relative to the partial target shape. A difference in
pose alignment of the training and target shape leads to biased solutions,
particularly when observing small parts of a shape. In this paper, we
demonstrate the importance of pose alignment for partial shape reconstructions
and propose an efficient method to adjust an existing model to a specific
target. Our method preserves the computational efficiency of linear models
while significantly improving reconstruction accuracy and predicted variance.
It exactly recovers the intended aligned model for translations, and provides a
good approximation for small rotations, all without access to the original
training data. Hence, existing shape models in reconstruction pipelines can be
adapted by a simple preprocessing step, making our approach widely applicable
in plug-and-play scenarios.

</details>


### [52] [3DPillars: Pillar-based two-stage 3D object detection](https://arxiv.org/abs/2509.05780)
*Jongyoun Noh,Junghyup Lee,Hyekang Park,Bumsub Ham*

Main category: cs.CV

TL;DR: 提出在PointPillars框架上实现首个两阶段3D检测：用3DPillars在伪图像上高效学习体素级特征，并结合稀疏场景上下文的RoI head，兼顾速度与精度，在KITTI与Waymo上验证有效。


<details>
  <summary>Details</summary>
Motivation: PointPillars虽快但精度落后，原因在于伪图像难以保留精确3D结构且不利于两阶段检测。作者旨在保留其效率同时缩小与SOTA的性能差距。

Method: 1) 设计3DPillars：将体素3D特征视为“伪图像堆栈”，通过可分离体素特征模块用2D卷积高效提取体素级3D特征，无需3D卷积；2) 设计带稀疏场景上下文特征模块的RoI head，从3DPillars聚合多尺度特征形成稀疏场景特征，实现有效两阶段提议精炼并利用上下文。

Result: 在KITTI与Waymo Open数据集上，方法在速度与精度上取得良好折中，相比PointPillars显著提升精度，逼近SOTA，同时保持高效率。

Conclusion: 通过3DPillars与稀疏场景上下文RoI head，首次将两阶段流程有效引入基于伪图像的3D检测，缓解结构表达不足与难以两阶段的问题，达到更优的精度-速度平衡。

Abstract: PointPillars is the fastest 3D object detector that exploits pseudo image
representations to encode features for 3D objects in a scene. Albeit efficient,
PointPillars is typically outperformed by state-of-the-art 3D detection methods
due to the following limitations: 1) The pseudo image representations fail to
preserve precise 3D structures, and 2) they make it difficult to adopt a
two-stage detection pipeline using 3D object proposals that typically shows
better performance than a single-stage approach. We introduce in this paper the
first two-stage 3D detection framework exploiting pseudo image representations,
narrowing the performance gaps between PointPillars and state-of-the-art
methods, while retaining its efficiency. Our framework consists of two novel
components that overcome the aforementioned limitations of PointPillars: First,
we introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3D
voxel-based features from the pseudo image representation efficiently using 2D
convolutions. The basic idea behind 3DPillars is that 3D features from voxels
can be viewed as a stack of pseudo images. To implement this idea, we propose a
separable voxel feature module that extracts voxel-based features without using
3D convolutions. Second, we introduce an RoI head with a sparse scene context
feature module that aggregates multi-scale features from 3DPillars to obtain a
sparse scene feature. This enables adopting a two-stage pipeline effectively,
and fully leveraging contextual information of a scene to refine 3D object
proposals. Experimental results on the KITTI and Waymo Open datasets
demonstrate the effectiveness and efficiency of our approach, achieving a good
compromise in terms of speed and accuracy.

</details>


### [53] [CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation](https://arxiv.org/abs/2509.05785)
*In-Jae Lee,Sihwan Hwang,Youngseok Kim,Wonjune Kim,Sanmin Kim,Dongsuk Kum*

Main category: cs.CV

TL;DR: 提出CRAB：一种基于相机-雷达融合的BEV 3D检测/分割模型，采用后向投影并用雷达占据信息缓解深度歧义；通过将图像上下文聚合到BEV查询、融合图像密集但不可靠的深度分布与雷达稀疏但精确的深度，辅以含雷达上下文的空间交互注意力，在nuScenes上取得SOTA（NDS 62.4%、mAP 54.0%）。


<details>
  <summary>Details</summary>
Motivation: 现有前向投影方法在BEV生成上稀疏，后向投影虽然高效但易受深度歧义影响产生误检；需利用雷达的精确深度来弥补相机深度不确定性，同时保持成本效益。

Method: - 采用后向投影的视角变换：以BEV查询聚合透视图像上下文。
- 深度融合：将图像提供的密集但不可靠深度分布与雷达占据提供的稀疏且精确深度结合，提升同一射线上不同查询的深度可分性，降低深度歧义。
- 空间交叉注意力：引入包含雷达上下文的信息特征图，与BEV查询进行跨注意力交互，增强3D场景理解。
- 目标：同时进行3D目标检测与分割（BEV语义/实例，摘要暗示）。

Result: 在nuScenes公开数据集上，作为后向投影的相机-雷达融合方法取得SOTA：NDS 62.4%、mAP 54.0%（3D检测）。

Conclusion: CRAB有效结合相机与雷达优势，在后向投影框架下缓解深度歧义，提升BEV 3D检测/分割性能，并在nuScenes上达到领先水平。

Abstract: Recently, camera-radar fusion-based 3D object detection methods in bird's eye
view (BEV) have gained attention due to the complementary characteristics and
cost-effectiveness of these sensors. Previous approaches using forward
projection struggle with sparse BEV feature generation, while those employing
backward projection overlook depth ambiguity, leading to false positives. In
this paper, to address the aforementioned limitations, we propose a novel
camera-radar fusion-based 3D object detection and segmentation model named CRAB
(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based
view transformation), using a backward projection that leverages radar to
mitigate depth ambiguity. During the view transformation, CRAB aggregates
perspective view image context features into BEV queries. It improves depth
distinction among queries along the same ray by combining the dense but
unreliable depth distribution from images with the sparse yet precise depth
information from radar occupancy. We further introduce spatial cross-attention
with a feature map containing radar context information to enhance the
comprehension of the 3D scene. When evaluated on the nuScenes open dataset, our
proposed approach achieves a state-of-the-art performance among backward
projection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in
3D object detection.

</details>


### [54] [Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance](https://arxiv.org/abs/2509.05796)
*Julio Zanon Diaz,Georgios Siogkas,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出两种注意力引导自编码器用于医疗器械制造的缺陷异常检测：一种基于4-MS-SSIM的轻量实时检测，高ACC；另一种在降维潜特征上用马氏距离，侧重分布漂移监控，二者互补，优于基线并符合监管要求。


<details>
  <summary>Details</summary>
Motivation: 医疗器械制造的视觉检验面临小且不平衡数据集、超高分辨率图像以及严格合规要求（如EU AI Act）的挑战；需要既高效又可监管落地的深度异常检测方案。

Method: 提出两种注意力引导自编码器：1) 结构相似度多尺度（4-MS-SSIM）重构误差作为异常分数，支持无/有监督阈值，面向在线实时检测与轻量部署；2) 对潜在空间进行降维后，采用马氏距离度量特征分布偏离，作为异常分数，适合对分布漂移和监管监控的高敏感场景。两者均在高分辨率图像与样本不平衡条件下设计与评估，并重实现对比基线。

Result: 在Surface Seal Image测试集（仅10%缺陷样本）上：方法1 ACC=0.903（无监督阈值）/0.931（有监督阈值）；方法2 有监督阈值ACC=0.722；均优于重实现基线。方法1具备实时性与准确性；方法2对分布漂移更敏感，适合监控。

Conclusion: 两种方法在功能上互补：方法1用于可靠的产线在线检测，方法2用于生产后规模化监测与合规监督。实验显示二者在准确性与效率上平衡，满足受监管环境（如EU AI Act对高风险AI）的部署需求，提供将深度异常检测落地制造的可行路径。

Abstract: Automating visual inspection in medical device manufacturing remains
challenging due to small and imbalanced datasets, high-resolution imagery, and
stringent regulatory requirements. This work proposes two attention-guided
autoencoder architectures for deep anomaly detection designed to address these
constraints. The first employs a structural similarity-based anomaly score
(4-MS-SSIM), offering lightweight and accurate real-time defect detection,
yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised
thresholding) on the - Surface Seal Image - Test split with only 10% of
defective samples. The second applies a feature-distance approach using
Mahalanobis scoring on reduced latent features, providing high sensitivity to
distributional shifts for supervisory monitoring, achieving ACC 0.722 with
supervised thresholding. Together, these methods deliver complementary
capabilities: the first supports reliable inline inspection, while the second
enables scalable post-production surveillance and regulatory compliance
monitoring. Experimental results demonstrate that both approaches surpass
re-implemented baselines and provide a practical pathway for deploying deep
anomaly detection in regulated manufacturing environments, aligning accuracy,
efficiency, and the regulatory obligations defined for high-risk AI systems
under the EU AI Act.

</details>


### [55] [A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation](https://arxiv.org/abs/2509.05809)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出“Probabilistic SAM”，在SAM上加入潜变量与变分训练，使分割从单一确定性变为可采样的概率分布，能输出多样且与专家分歧一致的掩膜，并在LIDC-IDRI上优于现有概率基线。


<details>
  <summary>Details</summary>
Motivation: 现有可提示分割（如SAM）对每个提示仅给出单一掩膜，无法表达真实任务中的标注歧义与不确定性，尤其在医学影像中存在多种合理分割与专家间差异。需要能建模并量化这种不确定性的模型。

Method: 在SAM框架中引入潜变量空间，构建先验与后验网络并通过变分目标（类似VAE）进行训练。潜码在推理时调制提示嵌入，实现条件于图像与提示的分割分布建模；通过对潜变量采样生成多样掩膜，推理开销小。

Result: 在LIDC-IDRI肺结节数据集上，模型生成与专家分歧一致的多样分割，并在不确定性感知指标上优于现有概率分割基线。

Conclusion: Probabilistic SAM能高效产生多样、可信的不确定性分割，弥补SAM确定性缺陷，特别适用于需要表达标注不确定性的医学场景。

Abstract: Recent advances in promptable segmentation, such as the Segment Anything
Model (SAM), have enabled flexible, high-quality mask generation across a wide
range of visual domains. However, SAM and similar models remain fundamentally
deterministic, producing a single segmentation per object per prompt, and fail
to capture the inherent ambiguity present in many real-world tasks. This
limitation is particularly troublesome in medical imaging, where multiple
plausible segmentations may exist due to annotation uncertainty or inter-expert
variability. In this paper, we introduce Probabilistic SAM, a probabilistic
extension of SAM that models a distribution over segmentations conditioned on
both the input image and prompt. By incorporating a latent variable space and
training with a variational objective, our model learns to generate diverse and
plausible segmentation masks reflecting the variability in human annotations.
The architecture integrates a prior and posterior network into the SAM
framework, allowing latent codes to modulate the prompt embeddings during
inference. The latent space allows for efficient sampling during inference,
enabling uncertainty-aware outputs with minimal overhead. We evaluate
Probabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate
its ability to produce diverse outputs that align with expert disagreement,
outperforming existing probabilistic baselines on uncertainty-aware metrics.
Our code is available at: https://github.com/tbwa233/Probabilistic-SAM/.

</details>


### [56] [Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data](https://arxiv.org/abs/2509.05887)
*Caleb Gates,Patrick Moorhead,Jayden Ferguson,Omar Darwish,Conner Stallman,Pablo Rivas,Paapa Quansah*

Main category: cs.CV

TL;DR: 提出一个基于MODIS多光谱数据的近实时沙尘像元级检测系统，使用3D卷积网络跨36个波段（含拆分热红外）学习光谱-空间特征，简易缺失值处理；改进版训练加速21倍，整景快速处理；在17景独立数据上约0.92准确率、MSE=0.014，核心羽流区域一致性高，边缘有遗漏；建议更大输入窗口或注意力模型以改进边缘。


<details>
  <summary>Details</summary>
Motivation: 沙尘暴影响健康与能见度，需从卫星快速、可靠地检测沙尘，实现全球尺度、近实时预警，同时要区分沙尘与云和地表背景并处理多波段与缺测问题。

Method: 构建以MODIS Terra/Aqua 36个波段（含拆分热红外）为输入的3D卷积神经网络，联合学习光谱与空间信息进行像元级二分类；采用简单归一化与局部插补处理缺测；推出改进版以提升训练与整景推理速度（21倍）。

Result: 在17幅独立MODIS场景上达到约0.92准确率、MSE=0.014；生成的沙尘概率/掩膜与实际沙尘羽流核心区域高度一致，主要误差集中在羽流边缘。

Conclusion: 联合跨波段与空间的学习能够实现全球尺度、近实时的沙尘预警；系统已具备快速训练与推理能力，但边缘检测仍不足，扩大输入窗口或引入注意力机制有望进一步提升边界表现。

Abstract: Dust storms harm health and reduce visibility; quick detection from
satellites is needed. We present a near real-time system that flags dust at the
pixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D
convolutional network learns patterns across all 36 bands, plus split thermal
bands, to separate dust from clouds and surface features. Simple normalization
and local filling handle missing data. An improved version raises training
speed by 21x and supports fast processing of full scenes. On 17 independent
MODIS scenes, the model reaches about 0.92 accuracy with a mean squared error
of 0.014. Maps show strong agreement in plume cores, with most misses along
edges. These results show that joint band-and-space learning can provide timely
dust alerts at global scale; using wider input windows or attention-based
models may further sharpen edges.

</details>


### [57] [Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets](https://arxiv.org/abs/2509.05892)
*Phongsakon Mark Konrad,Andrei-Alexandru Popa,Yaser Sabzehmeidani,Liang Zhong,Elisa A. Liehn,Serkan Ayvaz*

Main category: cs.CV

TL;DR: 在小样本心血管组织病理图像上，对多种主流分割模型（U‑Net、DeepLabV3+、SegFormer、SAM/MedSAM/MedSAM+UNet）进行系统评测与贝叶斯超参优化，结果显示性能对数据划分极其敏感，模型间差异多属统计噪声，传统基准在低数据临床场景的可靠性受质疑。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病研究/诊断需要精准分割颈动脉组织结构，但高质量标注数据稀缺。现有关于在低数据条件下不同深度分割范式（CNN、ViT、基础模型）稳定性与可比性的证据不足，亟需系统性评估与对基准实践的反思。

Method: 收集有限规模的心血管组织病理图像数据；对U‑Net、DeepLabV3+、SegFormer以及SAM、MedSAM、MedSAM+UNet进行训练/推理；采用贝叶斯搜索进行广泛超参优化；在不同数据划分下重复评测，比较性能分布与稳定性，分析差异是否超越统计波动。

Result: 在严格调参后，各模型在不同数据划分下的表现波动显著；模型间平均指标差距很小，且经多次划分/重复实验后，差异多被统计噪声解释，无法稳定地确认“最佳”模型。

Conclusion: 在低数据临床场景下，标准单次划分的基准评测不可靠，难以支撑对模型优劣与临床实用性的结论。应采用更加稳健的评测方案（多划分/多次重复、置信区间、功效分析、外部验证）并警惕对榜单式排名的过度解读。

Abstract: Accurate segmentation of carotid artery structures in histopathological
images is vital for advancing cardiovascular disease research and diagnosis.
However, deep learning model development in this domain is constrained by the
scarcity of annotated cardiovascular histopathological data. This study
investigates a systematic evaluation of state-of-the-art deep learning
segmentation models, including convolutional neural networks (U-Net,
DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models
(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology
images. Despite employing an extensive hyperparameter optimization strategy
with Bayesian search, our findings reveal that model performance is highly
sensitive to data splits, with minor differences driven more by statistical
noise than by true algorithmic superiority. This instability exposes the
limitations of standard benchmarking practices in low-data clinical settings
and challenges the assumption that performance rankings reflect meaningful
clinical utility.

</details>


### [58] [BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model](https://arxiv.org/abs/2509.05895)
*Yujie Li,Wenjia Xu,Yuanben Zhang,Zhiwei Wei,Mugen Peng*

Main category: cs.CV

TL;DR: 提出BTCChat，一种面向双时相遥感图像的多模态大语言模型，通过显式建模时间相关与空间语义变化，实现更好的变化字幕生成与VQA表现，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM对双时相图像多用简单拼接，难以充分捕捉时间相关与空间语义变化，导致视觉-语义对齐不足，从而限制了城市监测、灾害评估等任务效果。

Method: 构建BTCChat：1) 设计Change Extraction模块，从成对影像中抽取时序与空间变化特征；2) 提出Prompt Augmentation机制，在提示中注入上下文线索以强化对细粒度空间细节的关注；同时兼容双时相变化描述与单幅图像理解。

Result: 在变化字幕生成与视觉问答任务上取得SOTA性能，实验验证所提模块与机制有效。

Conclusion: 显式建模时序与空间变化并结合提示增强，可显著提升双时相变化理解与跨模态对齐能力；BTCChat在多项评测中优于现有方法，并保持单图像解读能力。

Abstract: Bi-temporal satellite imagery supports critical applications such as urban
development monitoring and disaster assessment. Although powerful multimodal
large language models (MLLMs) have been applied in bi-temporal change analysis,
previous methods process image pairs through direct concatenation, inadequately
modeling temporal correlations and spatial semantic changes. This deficiency
hampers visual-semantic alignment in change understanding, thereby constraining
the overall effectiveness of current approaches. To address this gap, we
propose BTCChat, a multi-temporal MLLM with advanced bi-temporal change
understanding capability. BTCChat supports bi-temporal change captioning and
retains single-image interpretation capability. To better capture temporal
features and spatial semantic changes in image pairs, we design a Change
Extraction module. Moreover, to enhance the model's attention to spatial
details, we introduce a Prompt Augmentation mechanism, which incorporates
contextual clues into the prompt to enhance model performance. Experimental
results demonstrate that BTCChat achieves state-of-the-art performance on
change captioning and visual question answering tasks.

</details>


### [59] [A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features](https://arxiv.org/abs/2509.05913)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Tamanna Shermin,Md Rafiqul Islam,Mukhtar Hussain,Sami Azam*

Main category: cs.CV

TL;DR: 提出ViSK-GAT多模态深度模型，用视觉图像与骨架坐标联合，通过细粒度跨模态注意与几何对齐模块，在REBA八级风险分类上显著优于多种迁移学习基线，准确率约94%。


<details>
  <summary>Details</summary>
Motivation: 现有运动员肌骨风险评估多依赖单一模态且在非受控复杂环境下鲁棒性差，需要一种能融合多模态并捕捉时空依赖的模型以实现早期预警与干预。

Method: 构建视觉+骨架坐标的自建多模态数据集，按REBA将样本标注为8类风险。模型ViSK-GAT由残差块与轻量级Transformer联合学习时空特征，并引入两大新模块：1）细粒度注意模块（FGAM），通过视觉与骨架跨注意实现精细的跨模态特征互补；2）多模态几何对应模块（MGCM），将图像特征与坐标表征对齐以增强跨模态一致性。整体为视觉-骨架几何注意Transformer框架，做分类与分布回归评估。

Result: 在验证/测试集上准确率93.55%/93.89%，精度93.86%，F1=93.85%，Cohen’s Kappa与MCC均为93%。概率分布回归的RMSE=0.1205、MAE=0.0156。对比9种流行迁移学习骨干网络，ViSK-GAT稳定优胜。

Conclusion: ViSK-GAT通过跨模态注意与几何对齐，有效融合视觉与骨架信息，在复杂环境下实现高精度的肌骨风险八类分类，相比现有方法更鲁棒，可支持体育场景中的早期风险识别与干预应用。

Abstract: Musculoskeletal disorders pose significant risks to athletes, and assessing
risk early is important for prevention. However, most existing methods are
designed for controlled settings and fail to reliably assess risk in complex
environments due to their reliance on a single type of data. This research
proposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel
multimodal deep learning framework designed to classify musculoskeletal risk
using visual and skeletal coordinate-based features. In addition, a custom
multimodal dataset is constructed by combining visual data and skeletal
coordinates for risk assessment. Each sample is labeled into eight risk
categories based on the Rapid Entire Body Assessment system. ViSK-GAT combines
a Residual Block with a Lightweight Transformer Block to learn spatial and
temporal dependencies jointly. It incorporates two novel modules: the
Fine-Grained Attention Module (FGAM), which enables precise inter-modal feature
refinement through cross-attention between visual and skeletal inputs, and the
Multimodal Geometric Correspondence Module (MGCM), which enhances cross-modal
coherence by aligning image features with coordinate-based representations.
ViSK-GAT achieved strong performance with validation and test accuracies of
93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of
93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. The
regression results also indicated a low Root Mean Square Error of the predicted
probability distribution of 0.1205 and a corresponding Mean Absolute Error of
0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT
consistently outperformed previous methods. The ViSK-GAT model advances
artificial intelligence implementation and application, transforming
musculoskeletal risk classification and enabling impactful early interventions
in sports.

</details>


### [60] [Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models](https://arxiv.org/abs/2509.05925)
*Ruiqi Shen,Haotian Wu,Wenjing Zhang,Jiangjing Hu,Deniz Gunduz*

Main category: cs.CV

TL;DR: 提出一种基于CLIP特征的语义压缩：不还原像素，而是压缩CLIP嵌入至极低码率（约2–3×10^-3 bpp），在多数据分布与下游任务上保持语义与零样本鲁棒性，所需码率低于主流方法的5%。


<details>
  <summary>Details</summary>
Motivation: 传统有损图像压缩追求像素级失真最小化，但新应用更重视语义保持与跨分布/跨任务的稳健性。现有端到端压缩对分布敏感且以重建为目标，难以满足“语义优先”。多模态基础模型（如CLIP）具备零样本与强表示能力，激发以其特征为对象进行语义压缩。

Method: 以CLIP作为语义表示器：提取图像的CLIP特征嵌入，并设计一种特征压缩方案，将嵌入量化与编码至最少比特，而非压缩像素。评价中通过下游任务与跨分布测试检验语义保持与鲁棒性，而非重建质量。

Result: 在多个基准数据集上，平均码率约2–3×10^-3 bpp，达到与主流图像压缩在语义表现相当的效果，码率仅为其<5%。在极端压缩下依然展现零样本鲁棒性，能泛化到多种数据分布与下游任务。

Conclusion: 基于CLIP嵌入的语义压缩能够以极低码率保留跨任务语义，一举改善分布泛化与零样本能力，相较传统像素重建导向的压缩显著更高效，适用于语义优先的应用场景。

Abstract: Recent deep learning-based methods for lossy image compression achieve
competitive rate-distortion performance through extensive end-to-end training
and advanced architectures. However, emerging applications increasingly
prioritize semantic preservation over pixel-level reconstruction and demand
robust performance across diverse data distributions and downstream tasks.
These challenges call for advanced semantic compression paradigms. Motivated by
the zero-shot and representational capabilities of multimodal foundation
models, we propose a novel semantic compression method based on the contrastive
language-image pretraining (CLIP) model. Rather than compressing images for
reconstruction, we propose compressing the CLIP feature embeddings into minimal
bits while preserving semantic information across different tasks. Experiments
show that our method maintains semantic integrity across benchmark datasets,
achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This
is less than 5% of the bitrate required by mainstream image compression
approaches for comparable performance. Remarkably, even under extreme
compression, the proposed approach exhibits zero-shot robustness across diverse
data distributions and downstream tasks.

</details>


### [61] [AttriPrompt: Dynamic Prompt Composition Learning for CLIP](https://arxiv.org/abs/2509.05949)
*Qiqi Zhan,Shiwei Li,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: AttriPrompt 利用 CLIP 视觉编码器的中间层特征来动态检索并插入属性化文本提示，通过层级视觉信息引导细粒度对齐，并配合双流对比学习与自正则化，在多基准上显著超越现有方法（base-to-novel 提升最高达 7.37%）。


<details>
  <summary>Details</summary>
Motivation: 现有深层文本提示方法存在两点不足：1) 过度依赖仅做高层语义对齐的对比学习，忽视细粒度特征优化；2) 对所有类别使用静态统一的提示，无法依据输入内容自适应调整，限制了泛化与跨域迁移能力。

Method: 提出 AttriPrompt：利用 CLIP 视觉编码器各中间层的视觉特征进行“属性检索”。具体做法：对每一层的视觉特征进行聚类，得到聚合的层级视觉表示；以此从提示池中检索语义相似的文本提示，并将这些提示拼接进文本编码器的每一层输入，从而在不同层注入与视觉属性匹配的文本引导。为实现细粒度对齐，设计双流对比学习（dual-stream contrastive），利用层级视觉信息监督文本表征；同时加入自正则化机制，对带提示与不带提示的文本特征施加显式约束，抑制小样本过拟合。

Result: 在三个基准上全面优于 SOTA，在 base-to-novel 设定下最高提升 7.37%；并展示了更强的跨域知识迁移能力，说明方法在真实场景中的可用性更高。

Conclusion: 基于中间层视觉特征的属性化提示检索与层级对齐策略，能够实现内容自适应、细粒度的文本表示增强；结合双流对比学习与自正则化，有效提升零/小样本与跨域表现，使 VLP 模型更具实用价值。

Abstract: The evolution of prompt learning methodologies has driven exploration of
deeper prompt designs to enhance model performance. However, current deep text
prompting approaches suffer from two critical limitations: Over-reliance on
constrastive learning objectives that prioritize high-level semantic alignment,
neglecting fine-grained feature optimization; Static prompts across all input
categories, preventing content-aware adaptation. To address these limitations,
we propose AttriPrompt-a novel framework that enhances and refines textual
semantic representations by leveraging the intermediate-layer features of
CLIP's vision encoder. We designed an Attribute Retrieval module that first
clusters visual features from each layer. The aggregated visual features
retrieve semantically similar prompts from a prompt pool, which are then
concatenated to the input of every layer in the text encoder. Leveraging
hierarchical visual information embedded in prompted text features, we
introduce Dual-stream Contrastive Learning to realize fine-grained alignment.
Furthermore, we introduce a Self-Regularization mechanism by applying explicit
regularization constraints between the prompted and non-prompted text features
to prevent overfitting on limited training data. Extensive experiments across
three benchmarks demonstrate AttriPrompt's superiority over state-of-the-art
methods, achieving up to 7.37\% improvement in the base-to-novel setting. The
observed strength of our method in cross-domain knowledge transfer positions
vision-language pre-trained models as more viable solutions for real-world
implementation.

</details>


### [62] [Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching](https://arxiv.org/abs/2509.05952)
*Feng Wang,Zihao Yu*

Main category: cs.CV

TL;DR: 论文提出在流匹配（Flow Matching）模型上进行在线强化学习时，SDE引入的随机性会在采样中造成显著噪点，干扰奖励学习。作者提出系数保持采样（CPS），借鉴DDIM思想，在保持生成系数的同时去除多余随机性，从而减少噪点、提升奖励建模准确性，并使Flow-GRPO、Dance-GRPO等RL优化器更快更稳收敛。


<details>
  <summary>Details</summary>
Motivation: 在扩散/流匹配生成中引入RL可显著提升与提示词对齐和视觉质量，但现有把SDE加到流匹配中的做法引入过度随机性，导致图像噪点明显，进而破坏奖励信号的可靠性和RL收敛。需要一种既能支持在线RL又不牺牲采样质量的随机性处理方式。

Method: 对SDE化流匹配采样引入噪点的机理进行理论分析，指出推断阶段注入的随机性过量。借鉴DDIM确定性采样范式，提出Coefficients-Preserving Sampling（CPS）：在采样更新中保持关键系数不变、去除导致额外噪声的项，从而在不依赖过度随机性的前提下完成采样流程，便于在线RL。

Result: CPS消除了SDE采样带来的显著噪点，提升了奖励模型对图像质量/对齐的判别准确性，使基于RL的优化器（Flow-GRPO、Dance-GRPO）收敛更快、更稳定。实验（按摘要描述）显示更好的稳定性与最终质量。

Conclusion: SDE式的随机化会在流匹配的RL训练中引入有害噪点。CPS通过DDIM式的系数保持重构采样过程，缓解噪声并改进奖励学习与优化收敛，为在流匹配中有效开展在线RL提供了更可靠的采样机制。

Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for
improving image and video generation in Diffusion and Flow Matching models,
specifically for enhancing output quality and alignment with prompts. A
critical step for applying online RL methods on Flow Matching is the
introduction of stochasticity into the deterministic framework, commonly
realized by Stochastic Differential Equation (SDE). Our investigation reveals a
significant drawback to this approach: SDE-based sampling introduces pronounced
noise artifacts in the generated images, which we found to be detrimental to
the reward learning process. A rigorous theoretical analysis traces the origin
of this noise to an excess of stochasticity injected during inference. To
address this, we draw inspiration from Denoising Diffusion Implicit Models
(DDIM) to reformulate the sampling process. Our proposed method,
Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This
leads to more accurate reward modeling, ultimately enabling faster and more
stable convergence for reinforcement learning-based optimizers like Flow-GRPO
and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS

</details>


### [63] [Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation](https://arxiv.org/abs/2509.05953)
*Jeonghyun Noh,Wangsu Jeon,Jinsun Park*

Main category: cs.CV

TL;DR: 提出一种用于医学图像分割的双向交互式融合模块（DIFM），通过跨注意力双向融合原始与增强图像特征，并配合多尺度边界损失提升边界分割精度，在ACDC与Synapse上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像常存在噪声、模糊、低对比度等问题；传统增强虽能改善可见性，但可能改变诊断关键信息。常规融合（如特征拼接）难以同时充分利用原始与增强图像优势并抑制增强副作用，需要更有效的互补信息融合机制与边界敏感训练目标。

Method: 提出DIFM：对原始与增强图像的特征采用双向跨注意力，互相对齐并选取对应空间位置信息；随后用全局空间注意力对互补特征进行细化，显式利用从低到高层的结构属性（边缘、纹理块、形状）。此外设计基于梯度提取的多尺度边界损失，强化目标边界处的分割学习。

Result: 在ACDC与Synapse数据集上，定量与定性结果均优于对比方法，证明所提方法的有效性（摘要未给出具体数值）。

Conclusion: 双向跨注意力驱动的原始/增强图像互补融合结合多尺度边界损失，可在不牺牲关键信息的前提下提升医学图像分割，尤其在边界处表现更佳；代码已开源。

Abstract: Medical image segmentation is a crucial method for assisting professionals in
diagnosing various diseases through medical imaging. However, various factors
such as noise, blurriness, and low contrast often hinder the accurate diagnosis
of diseases. While numerous image enhancement techniques can mitigate these
issues, they may also alter crucial information needed for accurate diagnosis
in the original image. Conventional image fusion strategies, such as feature
concatenation can address this challenge. However, they struggle to fully
leverage the advantages of both original and enhanced images while suppressing
the side effects of the enhancements. To overcome the problem, we propose a
dual interactive fusion module (DIFM) that effectively exploits mutual
complementary information from the original and enhanced images. DIFM employs
cross-attention bidirectionally to simultaneously attend to corresponding
spatial information across different images, subsequently refining the
complementary features via global spatial attention. This interaction leverages
low- to high-level features implicitly associated with diverse structural
attributes like edges, blobs, and object shapes, resulting in enhanced features
that embody important spatial characteristics. In addition, we introduce a
multi-scale boundary loss based on gradient extraction to improve segmentation
accuracy at object boundaries. Experimental results on the ACDC and Synapse
datasets demonstrate the superiority of the proposed method quantitatively and
qualitatively. Code available at: https://github.com/JJeong-Gari/DIN

</details>


### [64] [StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud](https://arxiv.org/abs/2509.05954)
*Weichao Wang,Wendong Mao,Zhongfeng Wang*

Main category: cs.CV

TL;DR: 提出StripDet，一种轻量级3D点云目标检测框架，通过条带注意力块与硬件友好骨干，在线性复杂度下建模长程依赖，实现端侧高效高精度检测，在KITTI上以0.65M参数达79.97% mAP（Car），优于PointPillars且参数降7倍。


<details>
  <summary>Details</summary>
Motivation: 现有高精度3D检测模型计算与内存开销大，难以上端设备部署；需要在保持精度的同时显著降低复杂度与参数量，提供可实用的边缘端方案。

Method: 1) 提出Strip Attention Block（SAB）：将标准2D卷积分解为非对称条带卷积，方向性提取特征，复杂度由二次降为线性，捕获长程空间依赖。2) 设计层次化、硬件友好的骨干：将SAB与深度可分离卷积结合，并配合简洁的多尺度融合策略，实现端到端高效推理。

Result: 在KITTI数据集上，模型仅0.65M参数即可在汽车类别达到79.97% mAP；相较PointPillars参数量减少约7倍且精度更高；同时优于近期轻量化与蒸馏方法，在精度-效率权衡上占优。

Conclusion: StripDet通过SAB与高效骨干在保持较高精度的同时极大降低计算与参数，适用于边缘设备的真实场景3D检测部署，并相较主流轻量化与蒸馏方案展现更优权衡与实用性。

Abstract: The deployment of high-accuracy 3D object detection models from point cloud
remains a significant challenge due to their substantial computational and
memory requirements. To address this, we introduce StripDet, a novel
lightweight framework designed for on-device efficiency. First, we propose the
novel Strip Attention Block (SAB), a highly efficient module designed to
capture long-range spatial dependencies. By decomposing standard 2D
convolutions into asymmetric strip convolutions, SAB efficiently extracts
directional features while reducing computational complexity from quadratic to
linear. Second, we design a hardware-friendly hierarchical backbone that
integrates SAB with depthwise separable convolutions and a simple multiscale
fusion strategy, achieving end-to-end efficiency. Extensive experiments on the
KITTI dataset validate StripDet's superiority. With only 0.65M parameters, our
model achieves a 79.97% mAP for car detection, surpassing the baseline
PointPillars with a 7x parameter reduction. Furthermore, StripDet outperforms
recent lightweight and knowledge distillation-based methods, achieving a
superior accuracy-efficiency trade-off while establishing itself as a practical
solution for real-world 3D detection on edge devices.

</details>


### [65] [Neural Bloom: A Deep Learning Approach to Real-Time Lighting](https://arxiv.org/abs/2509.05963)
*Rafal Karp,Dawid Gruszka,Tomasz Trzcinski*

Main category: cs.CV

TL;DR: 提出两种基于神经网络的实时泛光（bloom）方法，在常规3D场景中生成亮度掩码并合成高质量泛光，速度较SOTA提升12%（NBL）与28%（FastNBL）。


<details>
  <summary>Details</summary>
Motivation: 传统泛光依赖多次模糊与纹理采样，并包含条件分支，计算开销大、成为实时渲染瓶颈，影响高帧率与沉浸感；需要更快同时保持质量的方案。

Method: 设计两种网络：Neural Bloom Lighting（NBL）与 Fast Neural Bloom Lighting（FastNBL）。从输入的3D场景视图预测亮度掩码（替代多级模糊管线），随后合成泛光。通过网络结构与推理优化以权衡质量与速度，对多种3D场景测试，衡量亮度掩码精度与推理速度。

Result: 两方法均生成高质量泛光并优于标准SOTA实现：FastNBL 速度提升约28%，NBL 提升约12%，在多场景中保持较高亮度掩码准确率与实时性能。

Conclusion: 神经网络可在保持或提升视觉质量的同时显著加速实时泛光，缓解实时渲染的计算瓶颈，提升高FPS场景的流畅度与沉浸感，为未来更真实的实时环境铺路。

Abstract: We propose a novel method to generate bloom lighting effect in real time
using neural networks. Our solution generate brightness mask from given 3D
scene view up to 30% faster than state-of-the-art methods. The existing
traditional techniques rely on multiple blur appliances and texture sampling,
also very often have existing conditional branching in its implementation.
These operations occupy big portion of the execution time. We solve this
problem by proposing two neural network-based bloom lighting methods, Neural
Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on
their quality and performance. Both methods were tested on a variety of 3D
scenes, with evaluations conducted on brightness mask accuracy and inference
speed. The main contribution of this work is that both methods produce
high-quality bloom effects while outperforming the standard state-of-the-art
bloom implementation, with FastNBL being faster by 28% and NBL faster by 12%.
These findings highlight that we can achieve realistic bloom lighting phenomena
faster, moving us towards more realism in real-time environments in the future.
This improvement saves computational resources, which is a major bottleneck in
real-time rendering. Furthermore, it is crucial for sustaining immersion and
ensuring smooth experiences in high FPS environments, while maintaining
high-quality realism.

</details>


### [66] [Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks](https://arxiv.org/abs/2509.05967)
*Yiqin Zhang,Meiling Chen,Zhengjie Zhang*

Main category: cs.CV

TL;DR: 提出一种可解释的自监督三维医学成像预训练方法，通过三个子任务建模多粒度空间关系，在保持训练稳定与性能可比的同时，使学习过程更直观可解释。


<details>
  <summary>Details</summary>
Motivation: 现有自监督方法多源自通用2D视觉，难以体现3D空间知识的学习过程，导致在医学场景中可解释性不足；同时医疗数据稀缺，需要能有效利用未标注数据的方法。

Method: 设计三个遵循可观测原则的自监督子任务，专门捕获3D医学影像中的空间相关语义；利用3D多一维的语义深度进行多粒度空间关系建模，以增强训练稳定性并减少因可解释设计带来的性能损失。

Result: 实验表明该方法在性能上与当前主流方法相当，同时提供直观的自监督学习过程理解。

Conclusion: 该方法在不牺牲性能的前提下，提高了三维医学影像自监督学习的可解释性与训练稳定性，验证了多粒度空间关系建模与可观测子任务设计的有效性。

Abstract: The application of self-supervised techniques has become increasingly
prevalent within medical visualization tasks, primarily due to its capacity to
mitigate the data scarcity prevalent in the healthcare sector. The majority of
current works are influenced by designs originating in the generic 2D visual
domain, which lack the intuitive demonstration of the model's learning process
regarding 3D spatial knowledge. Consequently, these methods often fall short in
terms of medical interpretability. We propose a method consisting of three
sub-tasks to capture the spatially relevant semantics in medical 3D imaging.
Their design adheres to observable principles to ensure interpretability, and
minimize the performance loss caused thereby as much as possible. By leveraging
the enhanced semantic depth offered by the extra dimension in 3D imaging, this
approach incorporates multi-granularity spatial relationship modeling to
maintain training stability. Experimental findings suggest that our approach is
capable of delivering performance that is on par with current methodologies,
while facilitating an intuitive understanding of the self-supervised learning
process.

</details>


### [67] [OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization](https://arxiv.org/abs/2509.05970)
*Ye Wang,Zili Yi,Yibo Zhang,Peng Zheng,Xuping Xie,Jiang Lin,Yilin Wang,Rui Ma*

Main category: cs.CV

TL;DR: OmniStyle2将风格迁移重塑为“数据问题”：通过去风格化生成成对数据集DST-100K，并据此训练一个简单但强大的前馈模型，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 艺术风格迁移长期缺乏真实的成对监督数据（同一内容的“无风格真实图像”与“艺术风格图像”），导致评估和训练受限。作者认为问题核心在于数据稀缺，而非仅模型设计，于是提出通过“去风格化”从现有艺术作品中恢复内容，构造可规模化的高质量监督数据。

Method: 1) 提出DST（text-guided destylization）：借助文本引导，从艺术图像中去除风格成分以重建接近自然图像的内容；2) 提出DST-Filter：多阶段、链式思考评估器，自动筛除质量差的样本，兼顾内容一致性与风格准确性；3) 基于上述流程生成大规模配对数据集DST-100K；4) 用FLUX.1-dev为骨干，训练简单的前馈式风格迁移模型OmniStyle2。

Result: 在定性与定量基准上持续超越SOTA；证明了利用去风格化进行可扩展的数据生成，能提供可靠监督；OmniStyle2尽管结构简单，但凭借DST-100K显著提升性能。

Conclusion: 通过“去风格化→大规模成对数据→简单前馈模型”这一路线，克服了风格迁移缺乏真值配对的根本难题；数据质量与规模成为关键杠杆，优于单纯堆砌复杂模型。

Abstract: OmniStyle2 introduces a novel approach to artistic style transfer by
reframing it as a data problem. Our key insight is destylization, reversing
style transfer by removing stylistic elements from artworks to recover natural,
style-free counterparts. This yields DST-100K, a large-scale dataset that
provides authentic supervision signals by aligning real artistic styles with
their underlying content. To build DST-100K, we develop (1) DST, a text-guided
destylization model that reconstructs stylefree content, and (2) DST-Filter, a
multi-stage evaluation model that employs Chain-of-Thought reasoning to
automatically discard low-quality pairs while ensuring content fidelity and
style accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward
model based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently
surpasses state-of-the-art methods across both qualitative and quantitative
benchmarks. Our results demonstrate that scalable data generation via
destylization provides a reliable supervision paradigm, overcoming the
fundamental challenge posed by the lack of ground-truth data in artistic style
transfer.

</details>


### [68] [ConstStyle: Robust Domain Generalization with Unified Style Transformation](https://arxiv.org/abs/2509.05975)
*Nam Duong Tran,Nam Nguyen Phuong,Hieu H. Pham,Phi Le Nguyen,My T. Thai*

Main category: cs.CV

TL;DR: 提出ConstStyle：通过将训练与测试样本映射到统一域，学习域不变特征以缩小域间差距，显著提升少域与大域差场景下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有DG方法在训练域有限或训练-测试域差距大时效果欠佳；作者认为若能让模型在与未见域相似的数据上训练会更鲁棒，但未知未见域使之困难，故引入“统一域”以间接逼近未见域并提取域不变特征。

Method: 提出ConstStyle框架：训练时将所有样本投射到一个优化过的统一域上进行学习；测试时将未见域样本以同样方式投射再预测。通过理论分析论证统一域能捕获域不变特征并桥接域间差距，从而缓解分布偏移。

Result: 在多种DG场景中均优于现有方法；在仅有少量可见域时，相比次优方法最高提升19.82%的准确率。

Conclusion: 通过统一域映射对齐训练与测试数据分布，可显著减轻域偏移并提升DG鲁棒性，尤其在可见域少或域差距大时效果突出。

Abstract: Deep neural networks often suffer performance drops when test data
distribution differs from training data. Domain Generalization (DG) aims to
address this by focusing on domain-invariant features or augmenting data for
greater diversity. However, these methods often struggle with limited training
domains or significant gaps between seen (training) and unseen (test) domains.
To enhance DG robustness, we hypothesize that it is essential for the model to
be trained on data from domains that closely resemble unseen test domains-an
inherently difficult task due to the absence of prior knowledge about the
unseen domains. Accordingly, we propose ConstStyle, a novel approach that
leverages a unified domain to capture domain-invariant features and bridge the
domain gap with theoretical analysis. During training, all samples are mapped
onto this unified domain, optimized for seen domains. During testing, unseen
domain samples are projected similarly before predictions. By aligning both
training and testing data within this unified domain, ConstStyle effectively
reduces the impact of domain shifts, even with large domain gaps or few seen
domains. Extensive experiments demonstrate that ConstStyle consistently
outperforms existing methods across diverse scenarios. Notably, when only a
limited number of seen domains are available, ConstStyle can boost accuracy up
to 19.82\% compared to the next best approach.

</details>


### [69] [Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction](https://arxiv.org/abs/2509.05992)
*Zekun Zhou,Yanru Gong,Liu Shi,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出STRIDE扩散模型用于稀疏视角CT重建，通过稀疏条件引导、时间重加权、线性回归校正分布偏移与双网络并行频域优化，实现更好细节与结构复原，在公开与真实数据上优于现有方法（PSNR+2.58 dB、SSIM+2.37%、MSE-0.236）。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角CT因投影数少导致条纹伪影、细节丢失与结构失真，传统方法或单纯深度网络难以同时兼顾全局结构建模与细节恢复；现有扩散模型虽强大，但对条件利用不足、指导过程分布不一致，且难以动态适配从噪声到真实图像的演化。

Method: 1) 设计稀疏条件概率引导的联合训练机制，联合学习缺失投影补全与全局信息建模；2) 提出时间变化的稀疏条件重加权策略，在扩散反演各步动态调整条件权重，使模型逐步感知稀疏信息；3) 用线性回归校正指导过程中的已知与生成数据的分布偏移；4) 构建双网络并行架构，跨多子频段进行全局校正与优化，兼顾细节恢复与结构保持。

Result: 在公共与真实CT数据上，相比最佳基线，PSNR提升2.58 dB、SSIM提升2.37%、MSE降低0.236；重建图在结构一致性、细节还原和伪影抑制方面表现更好并具备较强泛化与鲁棒性。

Conclusion: 通过稀疏条件时间重加权与分布偏移校正，以及双网络多频段并行优化，STRIDE显著提升稀疏视角CT重建质量，兼顾细节与结构，优于现有基线并具备良好泛化与鲁棒性。

Abstract: Diffusion models have demonstrated remarkable generative capabilities in
image processing tasks. We propose a Sparse condition Temporal Rewighted
Integrated Distribution Estimation guided diffusion model (STRIDE) for
sparse-view CT reconstruction. Specifically, we design a joint training
mechanism guided by sparse conditional probabilities to facilitate the model
effective learning of missing projection view completion and global information
modeling. Based on systematic theoretical analysis, we propose a temporally
varying sparse condition reweighting guidance strategy to dynamically adjusts
weights during the progressive denoising process from pure noise to the real
image, enabling the model to progressively perceive sparse-view information.
The linear regression is employed to correct distributional shifts between
known and generated data, mitigating inconsistencies arising during the
guidance process. Furthermore, we construct a dual-network parallel
architecture to perform global correction and optimization across multiple
sub-frequency components, thereby effectively improving the model capability in
both detail restoration and structural preservation, ultimately achieving
high-quality image reconstruction. Experimental results on both public and real
datasets demonstrate that the proposed method achieves the best improvement of
2.58 dB in PSNR, increase of 2.37\% in SSIM, and reduction of 0.236 in MSE
compared to the best-performing baseline methods. The reconstructed images
exhibit excellent generalization and robustness in terms of structural
consistency, detail restoration, and artifact suppression.

</details>


### [70] [S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion](https://arxiv.org/abs/2509.05999)
*Diana-Alexandra Sas,Florin Oniga*

Main category: cs.CV

TL;DR: 提出一种在单目3D检测中将预计算语义分割先验直接注入特征空间、且与检测头解耦的策略，在不增加预测分支或联合训练的前提下提升小目标（行人/骑行者）检测表现。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测仅有RGB图像，缺乏深度线索，深度估计病态；现有方法多仅依赖CNN/Transformer提特征并用专门检测头回归3D参数，难以充分利用场景语义。作者希望在不扩大模型或额外联合学习的情况下，引入对深度与几何有帮助的语义先验以提升检测，特别是对小目标。

Method: 采用解耦策略：先离线计算语义分割结果，作为先验在检测网络的特征空间中进行融合，引导3D检测；不增加新的预测分支，也不与分割进行联合训练，仅将分割信息作为输入特征增强模块注入，评估其对现有单目检测流水线的影响。

Result: 在KITTI 3D检测基准上，相比仅使用RGB特征的等价架构，方法在小目标类别（行人、骑行者）上取得更好表现。

Conclusion: 无需增加传感器或额外联合训练，通过理解并注入语义先验可显著改善单目3D检测，特别是小目标场景，说明语义分割先验能有效弥补单目深度信息不足。

Abstract: Monocular 3D Object Detection represents a challenging Computer Vision task
due to the nature of the input used, which is a single 2D image, lacking in any
depth cues and placing the depth estimation problem as an ill-posed one.
Existing solutions leverage the information extracted from the input by using
Convolutional Neural Networks or Transformer architectures as feature
extraction backbones, followed by specific detection heads for 3D parameters
prediction. In this paper, we introduce a decoupled strategy based on injecting
precomputed segmentation information priors and fusing them directly into the
feature space for guiding the detection, without expanding the detection model
or jointly learning the priors. The focus is on evaluating the impact of
additional segmentation information on existing detection pipelines without
adding additional prediction branches. The proposed method is evaluated on the
KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture
that relies only on RGB image features for small objects in the scene:
pedestrians and cyclists, and proving that understanding the input data can
balance the need for additional sensors or training data.

</details>


### [71] [Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation](https://arxiv.org/abs/2509.06000)
*Jose Sosa,Dan Pineau,Arunkumar Rathinam,Abdelrahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一种融合运动信息的单目航天器6-DoF位姿估计方法：用ViT提取图像特征，结合预训练光流与运动感知热力图定位2D关键点，再用PnP恢复位姿；在SPADES-RGB训练评估，并在SPARK-2024上测试泛化，优于单帧基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖单张图像和静态关键点定位，忽视了空间任务中天然存在的时间与运动线索，导致关键点不稳定、位姿估计误差较大与泛化性不足。

Method: 借鉴人体姿态估计框架：以ViT编码器提取图像特征；引入预训练光流网络生成的光流与运动感知热力图，融合到关键点检测头以捕获时序/运动动态；根据得到的2D关键点与先验3D模型建立对应关系，使用PnP求解6-DoF位姿。

Result: 在SPADES-RGB数据集上，相比单帧基线，2D关键点定位与6-DoF位姿精度都有所提升；在SPARK-2024（含真实与合成数据）上进行跨分布测试，表现出较好的泛化能力。

Conclusion: 利用运动线索（光流、运动热力图）与ViT特征进行时序感知的关键点定位，可显著提升单目航天器位姿估计效果，并在跨数据分布场景中保持较强的鲁棒性与泛化潜力。

Abstract: Monocular 6-DoF pose estimation plays an important role in multiple
spacecraft missions. Most existing pose estimation approaches rely on single
images with static keypoint localisation, failing to exploit valuable temporal
information inherent to space operations. In this work, we adapt a deep
learning framework from human pose estimation to the spacecraft pose estimation
domain that integrates motion-aware heatmaps and optical flow to capture motion
dynamics. Our approach combines image features from a Vision Transformer (ViT)
encoder with motion cues from a pre-trained optical flow model to localise 2D
keypoints. Using the estimates, a Perspective-n-Point (PnP) solver recovers
6-DoF poses from known 2D-3D correspondences. We train and evaluate our method
on the SPADES-RGB dataset and further assess its generalisation on real and
synthetic data from the SPARK-2024 dataset. Overall, our approach demonstrates
improved performance over single-image baselines in both 2D keypoint
localisation and 6-DoF pose estimation. Furthermore, it shows promising
generalisation capabilities when testing on different data distributions.

</details>


### [72] [Khana: A Comprehensive Indian Cuisine Dataset](https://arxiv.org/abs/2509.06006)
*Omkar Prabhu*

Main category: cs.CV

TL;DR: 提出Khana：面向印度菜肴的多任务基准数据集（分类/分割/检索），含约13.1万张500x500图像、80类，并建立菜系分类法；给出基线评测以推动研究与应用。


<details>
  <summary>Details</summary>
Motivation: 现有食物数据集难以覆盖印度菜的区域多样性与复杂烹饪细节，且缺乏完备标签与统一分类体系，限制了食物识别、食谱推荐、饮食追踪与自动配餐等应用的发展。

Method: 构建印度菜系分类法；收集并清洗约131K图像（80类，500x500分辨率）；制作适用于分类、语义/实例分割与检索的标注；在数据集上评测多种SOTA模型，报告分类、分割与检索的基线结果与难点分析。

Result: 形成Khana数据集：80标签、约131K张、500x500像素；对SOTA模型进行三任务基线评测，显示任务具有挑战性且存在改进空间；为研究者与开发者提供统一评测基准与资源。

Conclusion: Khana弥补印度菜图像数据缺口，建立系统分类法并提供大规模、多任务数据与基线，促进食物理解研究并支持现实应用（识别、推荐、追踪、配餐等）。

Abstract: As global interest in diverse culinary experiences grows, food image models
are essential for improving food-related applications by enabling accurate food
recognition, recipe suggestions, dietary tracking, and automated meal planning.
Despite the abundance of food datasets, a noticeable gap remains in capturing
the nuances of Indian cuisine due to its vast regional diversity, complex
preparations, and the lack of comprehensive labeled datasets that cover its
full breadth. Through this exploration, we uncover Khana, a new benchmark
dataset for food image classification, segmentation, and retrieval of dishes
from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian
cuisine and offering around 131K images in the dataset spread across 80 labels,
each with a resolution of 500x500 pixels. This paper describes the dataset
creation process and evaluates state-of-the-art models on classification,
segmentation, and retrieval as baselines. Khana bridges the gap between
research and development by providing a comprehensive and challenging benchmark
for researchers while also serving as a valuable resource for developers
creating real-world applications that leverage the rich tapestry of Indian
cuisine. Webpage: https://khana.omkar.xyz

</details>


### [73] [BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users](https://arxiv.org/abs/2509.06010)
*Wanyin Cheng,Zanxi Ruan*

Main category: cs.CV

TL;DR: BLaVe-CoT提出面向盲人/低视力用户场景的VQA框架，针对含糊提问与模糊图像导致的一题多解与多区域对齐问题，通过生成多候选答案、空间指认并用思维链判断答案一致性，显著提升在VQA-AnswerTherapy上的鲁棒性与表现。


<details>
  <summary>Details</summary>
Motivation: 现实助残场景中，BLV用户拍摄往往模糊、构图不佳且难以提出明确问题，导致问题语义含混、不同用户理解不同、同一图像可能存在多个合理答案且各自对应不同图像区域；传统VQA假设单一正确答案与单一区域监督，难以适配这种不确定性与噪声。

Method: 1) 使用LoRA调优的BLIP-2生成多样化候选答案；2) 采用PolyFormer为每个候选答案进行空间定位/指认（grounding）；3) 引入链式思维（CoT）模块，推理这些答案是否指向相同或不同的图像区域，从而评估答案间一致性与歧义来源；4) 在VQA-AnswerTherapy基准上进行评测。

Result: 在VQA-AnswerTherapy数据集上优于既有方法，对含糊问题与视觉噪声更为稳健，表现出更好的现实助残适用性。

Conclusion: VQA系统应显式建模人类不确定性与多解性；BLaVe-CoT通过“多候选答案+空间指认+思维链一致性判定”有效缓解BLV场景中的歧义问题并提升鲁棒性。代码已开源，促进后续研究与可及性应用。

Abstract: Visual Question Answering (VQA) holds great potential for assisting Blind and
Low Vision (BLV) users, yet real-world usage remains challenging. Due to visual
impairments, BLV users often take blurry or poorly framed photos and face
difficulty in articulating specific questions about what they cannot fully see.
As a result, their visual questions are frequently ambiguous, and different
users may interpret them in diverse ways. This leads to multiple valid answers,
each grounded in different image regions-posing a mismatch with conventional
VQA systems that assume a single answer and region. To bridge this gap, we
present BLaVe-CoT, a VQA framework designed to reason about answer consistency
in the face of ambiguity. Our method proposes diverse candidate answers using a
LoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer,
and finally applies a chain-of-thought reasoning module to assess whether the
answers refer to the same or different regions. Evaluated on the
VQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves
more robust to the ambiguity and visual noise common in assistive settings.
This work highlights the need for VQA systems that can adapt to real human
uncertainty and provide inclusive support for BLV users. To foster further
research and accessibility applications, we have made the code publicly
available at https://github.com/Accecwan/BLaVe-CoT.

</details>


### [74] [Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection](https://arxiv.org/abs/2509.06011)
*Zhenhai Weng,Zhongliang Yu*

Main category: cs.CV

TL;DR: 为无人机场景定制开放词汇目标检测：构建大规模UAV数据与新融合模块CAGE，提升在VisDrone/SIMD上的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测主要在地面自然图像上预训练，与无人机视角存在显著域间差异，导致在UAV影像上性能大幅下降，需要面向UAV的标注与模型改进。

Method: 1) 提出改进的UAV-Label标注引擎；2) 构建两个UAV数据资源：UAVDE-2M（200万+实例、1800类）与UAVCAP-15k（15k图像）；3) 设计跨注意力门控增强融合（CAGE）模块并集成到YOLO-World-v2架构中，用于提升多模态/多尺度特征融合能力；4) 在UAV数据上预训练/微调并进行系统评测。

Result: 在VisDrone与SIMD数据集上进行大量实验，所提方法优于现有方案，显著提升UAV场景的开放词汇检测性能（摘要未给出具体数值）。

Conclusion: 面向UAV的专用数据构建与CAGE模块有效缓解地面图像到UAV影像的域差距，增强YOLO-World-v2在遥感与无人机应用中的开放词汇检测能力。

Abstract: Open-Vocabulary Object Detection (OVD) has emerged as a pivotal technology
for applications involving Unmanned Aerial Vehicles (UAVs). However, the
prevailing large-scale datasets for OVD pre-training are predominantly composed
of ground-level, natural images. This creates a significant domain gap, causing
models trained on them to exhibit a substantial drop in performance on UAV
imagery. To address this limitation, we first propose a refined UAV-Label
engine. Then we construct and introduce UAVDE-2M(contains over 2,000,000
instances and 1800 categories) and UAVCAP-15k(contains over 15,000 images).
Furthermore, we propose a novel Cross-Attention Gated Enhancement Fusion (CAGE)
module and integrate it into the YOLO-World-v2 architecture. Finally, extensive
experiments on the VisDrone and SIMD datasets verify the effectiveness of our
proposed method for applications in UAV-based imagery and remote sensing.

</details>


### [75] [Micro-Expression Recognition via Fine-Grained Dynamic Perception](https://arxiv.org/abs/2509.06015)
*Zhiwen Shao,Yifan Cheng,Fan Zhang,Xuehuai Shi,Canlin Li,Lizhuang Ma,Dit-yan Yeung*

Main category: cs.CV

TL;DR: 提出FDP框架，通过对时间序列帧特征进行排序并汇聚，联合MER分类与动态图像重建两任务，以缓解小样本与细微动态建模难题，在多数据集F1上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: MER任务受限于微表情瞬时、幅度小、动态弱，以及训练数据规模小、多样性不足；手工特征需关键帧、深度方法易过拟合。需要一种能细粒度建模时序动态、同时提升数据利用效率的策略。

Method: 1) 提出细粒度动态感知（FDP）框架：将原始帧的帧级特征按时间顺序进行“排序（ranking）”，以显式编码外观与运动的动态演化；2) 设计局部-全局特征感知Transformer进行帧表示学习；3) 采用rank scorer为每帧特征打分，获得rank特征；4) 在时间维度对rank特征进行池化，形成动态表示；5) 动态表示共享给两个头：MER分类头与动态图像构建头（编码器-解码器）。

Result: 在CASME II、SAMM、CAS(ME)^2、CAS(ME)^3上F1分别较此前最佳提升4.05%、2.50%、7.71%、2.11%；同时动态图像构建效果良好。

Conclusion: FDP通过排序驱动的动态建模与多任务学习有效捕捉微表情细微动作，并缓解数据稀缺导致的泛化问题，取得SOTA性能与良好重建质量。

Abstract: Facial micro-expression recognition (MER) is a challenging task, due to the
transience, subtlety, and dynamics of micro-expressions (MEs). Most existing
methods resort to hand-crafted features or deep networks, in which the former
often additionally requires key frames, and the latter suffers from small-scale
and low-diversity training data. In this paper, we develop a novel fine-grained
dynamic perception (FDP) framework for MER. We propose to rank frame-level
features of a sequence of raw frames in chronological order, in which the rank
process encodes the dynamic information of both ME appearances and motions.
Specifically, a novel local-global feature-aware transformer is proposed for
frame representation learning. A rank scorer is further adopted to calculate
rank scores of each frame-level feature. Afterwards, the rank features from
rank scorer are pooled in temporal dimension to capture dynamic representation.
Finally, the dynamic representation is shared by a MER module and a dynamic
image construction module, in which the former predicts the ME category, and
the latter uses an encoder-decoder structure to construct the dynamic image.
The design of dynamic image construction task is beneficial for capturing
facial subtle actions associated with MEs and alleviating the data scarcity
issue. Extensive experiments show that our method (i) significantly outperforms
the state-of-the-art MER methods, and (ii) works well for dynamic image
construction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%
over the previous best results in terms of F1-score on the CASME II, SAMM,
CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at
https://github.com/CYF-cuber/FDP.

</details>


### [76] [DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion](https://arxiv.org/abs/2509.06023)
*Mengmeng Liu,Michael Ying Yang,Jiuming Liu,Yunpeng Zhang,Jiangtao Li,Sander Oude Elberink,George Vosselman,Hao Cheng*

Main category: cs.CV

TL;DR: DVLO4D提出一种稀疏时空融合的视觉-激光里程计算法，通过稀疏查询融合、时序交互更新以及时序片段训练+集体平均损失，显著提升位姿精度与鲁棒性，在KITTI与Argoverse上达SOTA并具实时潜力（82 ms）。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-激光里程计易受传感器错位影响，时序信息利用不足，且对不同传感器配置需要大量手工调参，难以在精度、鲁棒性与通用性上兼顾。

Method: 1) 稀疏查询融合：以稀疏LiDAR查询为核心进行多模态融合，降低计算并提升关键结构对齐；2) 时序交互与更新：将时间预测的位置与当前帧信息融合，提供更优位姿初始化，抑制累计误差；3) 时序片段训练+集体平均损失：跨多帧聚合梯度与损失，实现全局优化，缓解长序列尺度漂移；4) 整体以4D（空间+时间）稀疏表示与高效推理实现实时性。

Result: 在KITTI与Argoverse Odometry上取得SOTA的位姿精度与鲁棒性；推理时间82 ms，具备实时部署潜力。

Conclusion: DVLO4D通过稀疏时空融合与时序优化策略，有效提升了视觉-激光里程计的精度、稳定性和效率，减少尺度漂移与累积误差，适用于多传感器配置并具实时应用前景。

Abstract: Visual-LiDAR odometry is a critical component for autonomous system
localization, yet achieving high accuracy and strong robustness remains a
challenge. Traditional approaches commonly struggle with sensor misalignment,
fail to fully leverage temporal information, and require extensive manual
tuning to handle diverse sensor configurations. To address these problems, we
introduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse
spatial-temporal fusion to enhance accuracy and robustness. Our approach
proposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse
LiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction
and Update module that integrates temporally-predicted positions with current
frame data, providing better initialization values for pose estimation and
enhancing model's robustness against accumulative errors; and (3) a Temporal
Clip Training strategy combined with a Collective Average Loss mechanism that
aggregates losses across multiple frames, enabling global optimization and
reducing the scale drift over long sequences. Extensive experiments on the
KITTI and Argoverse Odometry dataset demonstrate the superiority of our
proposed DVLO4D, which achieves state-of-the-art performance in terms of both
pose accuracy and robustness. Additionally, our method has high efficiency,
with an inference time of 82 ms, possessing the potential for the real-time
deployment.

</details>


### [77] [Analysis of Blood Report Images Using General Purpose Vision-Language Models](https://arxiv.org/abs/2509.06033)
*Nadia Bakhsheshi,Hamid Beigy*

Main category: cs.CV

TL;DR: 评估三种通用视觉-语言模型在100张血检报告图像上的问答表现，表明其可用于面向患者的初步化验单解读，但受限于数据集较小，结论需谨慎。


<details>
  <summary>Details</summary>
Motivation: 个人难以准确解读血检报告，易产生焦虑并忽视问题；希望利用通用VLM从报告图像直接给出清晰解释，提升健康素养与可及性。

Method: 选取Qwen-VL-Max、Gemini 2.5 Pro、Llama 4 Maverick三种VLM；针对每份报告设计临床相关问题并提示模型作答；用Sentence-BERT对回答进行相似度比较与评估，基于100张多样化血检报告图像的数据集进行对比实验。

Result: 三种通用VLM均能从图像直接生成较清晰的血检解释，表现总体可行且有前景；相似度评估显示其回答与期望方向较接近，但未给出精确指标且受样本量限制。

Conclusion: 通用VLM可作为面向患者的初步血检报告分析工具的有效基础，能提升健康信息可理解性并减少认知门槛；然而由于数据集较小、评估方式间接，结果需谨慎解读，并需要更大规模与更严格的临床验证以实现可靠的AI医疗应用。

Abstract: The reliable analysis of blood reports is important for health knowledge, but
individuals often struggle with interpretation, leading to anxiety and
overlooked issues. We explore the potential of general-purpose Vision-Language
Models (VLMs) to address this challenge by automatically analyzing blood report
images. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini
2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of
100 diverse blood report images. Each model was prompted with clinically
relevant questions adapted to each blood report. The answers were then
processed using Sentence-BERT to compare and evaluate how closely the models
responded. The findings suggest that general-purpose VLMs are a practical and
promising technology for developing patient-facing tools for preliminary blood
report analysis. Their ability to provide clear interpretations directly from
images can improve health literacy and reduce the limitations to understanding
complex medical information. This work establishes a foundation for the future
development of reliable and accessible AI-assisted healthcare applications.
While results are encouraging, they should be interpreted cautiously given the
limited dataset size.

</details>


### [78] [TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection](https://arxiv.org/abs/2509.06035)
*Jiaming Cui*

Main category: cs.CV

TL;DR: 提出TinyDef-DETR用于输电线路无人机小缺陷检测：无损下采样、边缘增强、跨阶段双域多尺度注意力与改进回归损失，显著提升小目标精度召回，代价适中，并在VisDrone验证泛化。


<details>
  <summary>Details</summary>
Motivation: 传统检测在复杂背景下对小而模糊的缺陷识别困难；轻量骨干边界敏感性弱、步幅下采样造成细节丢失、全局与局部信息融合不足，导致小目标定位与分类不佳。

Method: 基于DETR框架，提出：1) stride-free space-to-depth无损下采样；2) Edge-Enhanced Convolution用于边界感知特征提取；3) Cross-Stage Dual-Domain Multi-Scale Attention联合捕获全局与局部信息（跨阶段、频域/时空域或特征域融合）；4) Focaler-Wise-SIoU回归损失，难例自适应、提升小目标定位。

Result: 在CSG-ADCD数据集上，相比强基线在精度与召回均有显著提升，尤其小目标子集收益明显，计算开销仅小幅增加；在VisDrone上验证出良好泛化。

Conclusion: 结合保细节下采样、边缘敏感表示、双域多尺度注意力与难度自适应回归，可为电网无人机小缺陷检测提供高效、实用且可泛化的解决方案。

Abstract: Automated inspection of transmission lines using UAVs is hindered by the
difficulty of detecting small and ambiguous defects against complex
backgrounds. Conventional detectors often suffer from detail loss due to
strided downsampling, weak boundary sensitivity in lightweight backbones, and
insufficient integration of global context with local cues. To address these
challenges, we propose TinyDef-DETR, a DETR-based framework designed for
small-defect detection. The method introduces a stride-free space-to-depth
module for lossless downsampling, an edge-enhanced convolution for
boundary-aware feature extraction, a cross-stage dual-domain multi-scale
attention module to jointly capture global and local information, and a
Focaler-Wise-SIoU regression loss to improve localization of small objects.
Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR
achieves substantial improvements in both precision and recall compared to
competitive baselines, with particularly notable gains on small-object subsets,
while incurring only modest computational overhead. Further validation on the
VisDrone benchmark confirms the generalization capability of the proposed
approach. Overall, the results indicate that integrating detail-preserving
downsampling, edge-sensitive representations, dual-domain attention, and
difficulty-adaptive regression provides a practical and efficient solution for
UAV-based small-defect inspection in power grids.

</details>


### [79] [BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models](https://arxiv.org/abs/2509.06040)
*Yuming Li,Yikai Wang,Yuying Zhu,Zhongyu Zhao,Ming Lu,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: 提出BranchGRPO：在扩散/随机微分方程(SDE)采样过程中进行“分支”采样与剪枝，复用前缀计算、注入过程级稠密奖励、并用树式优势估计与深度/路径冗余剪枝，较强基线对齐分数+16%，训练时间减半。


<details>
  <summary>Details</summary>
Motivation: 现有用GRPO对齐图像/视频生成虽提升人偏好，但代价高：需大量on-policy rollout与过多SDE步，且奖励稀疏导致训练不稳与探索低效。需要在不牺牲(甚至提升)探索多样性的同时降低算力与加速收敛。

Method: 在SDE采样阶段引入分支采样策略：共享公共前缀的计算，对低回报路径和冗余深度进行剪枝；构建树结构的优势估计，用过程级(稠密)奖励替代仅终端奖励；结合多种剪枝准则(路径评分与深度冗余)以减少无效采样与训练不稳定。

Result: 在图像与视频偏好对齐实验中，BranchGRPO相对强基线提升对齐分数约16%，同时训练时间减少约50%，且保持或提升探索多样性与稳定性。

Conclusion: BranchGRPO通过分支采样+树式优势估计+剪枝策略有效缓解GRPO的算力与稳定性瓶颈，实现更快、更稳且更高质量的人偏好对齐，可作为对扩散式生成对齐的高效替代方案。

Abstract: Recent advancements in aligning image and video generative models via GRPO
have achieved remarkable gains in enhancing human preference alignment.
However, these methods still face high computational costs from on-policy
rollouts and excessive SDE sampling steps, as well as training instability due
to sparse rewards. In this paper, we propose BranchGRPO, a novel method that
introduces a branch sampling policy updating the SDE sampling process. By
sharing computation across common prefixes and pruning low-reward paths and
redundant depths, BranchGRPO substantially lowers the per-update compute cost
while maintaining or improving exploration diversity. This work makes three
main contributions: (1) a branch sampling scheme that reduces rollout and
training cost; (2) a tree-based advantage estimator incorporating dense
process-level rewards; and (3) pruning strategies exploiting path and depth
redundancy to accelerate convergence and boost performance. Experiments on
image and video preference alignment show that BranchGRPO improves alignment
scores by 16% over strong baselines, while cutting training time by 50%.

</details>


### [80] [Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities](https://arxiv.org/abs/2509.06041)
*Mohammad Ahangarkiasari,Hassan Pouraria*

Main category: cs.CV

TL;DR: 提出一种多阶段GNN，通过层次化pool/unpool跨尺度建模，提升对自然对流封闭腔体的温度场预测精度、训练效率与长期稳定性，优于现有GNN。


<details>
  <summary>Details</summary>
Motivation: 传统CFD虽精确但依赖专家建模、细网格和高算力，难以快速迭代；数据驱动方法（尤其GNN）适配不规则网格，但常难以捕获高分辨率图上的长程依赖，需要新的架构提升跨尺度建模能力。

Method: 设计多阶段GNN架构，采用层次化pooling/unpooling，自上而下逐级建立全局到局部的交互，适配不规则网格CFD数据；在自建的自然对流矩形腔体数据集（不同纵横比，底热顶冷，侧壁绝热）上训练与评估。

Result: 相较SOTA GNN基线，该模型在预测精度更高、训练效率更佳，并在长期滚动预测中误差累积更小。

Conclusion: 多阶段、层次化的GNN能够有效建模网格流体热传递中的跨尺度与长程依赖，对复杂热流动的快速、准确数据驱动建模具有潜力。

Abstract: Buoyancy-driven heat transfer in closed cavities serves as a canonical
testbed for thermal design High-fidelity CFD modelling yields accurate thermal
field solutions, yet its reliance on expert-crafted physics models, fine
meshes, and intensive computation limits rapid iteration. Recent developments
in data-driven modeling, especially Graph Neural Networks (GNNs), offer new
alternatives for learning thermal-fluid behavior directly from simulation data,
particularly on irregular mesh structures. However, conventional GNNs often
struggle to capture long-range dependencies in high-resolution graph
structures. To overcome this limitation, we propose a novel multi-stage GNN
architecture that leverages hierarchical pooling and unpooling operations to
progressively model global-to-local interactions across multiple spatial
scales. We evaluate the proposed model on our newly developed CFD dataset
simulating natural convection within a rectangular cavities with varying aspect
ratios where the bottom wall is isothermal hot, the top wall is isothermal
cold, and the two vertical walls are adiabatic. Experimental results
demonstrate that the proposed model achieves higher predictive accuracy,
improved training efficiency, and reduced long-term error accumulation compared
to state-of-the-art (SOTA) GNN baselines. These findings underscore the
potential of the proposed multi-stage GNN approach for modeling complex heat
transfer in mesh-based fluid dynamics simulations.

</details>


### [81] [Home-made Diffusion Model from Scratch to Hatch](https://arxiv.org/abs/2509.06068)
*Shih-Ying Yeh*

Main category: cs.CV

TL;DR: 提出HDM，一种面向消费级硬件优化的高效文本到图像扩散模型：在4张RTX5090上，约$535–620成本即可训练到具有竞争力的1024×1024质量；关键在于XUT骨干、加速与数据/分辨率训练策略，以及小模型精巧架构带来的涌现能力。


<details>
  <summary>Details</summary>
Motivation: 主流高质量文生图扩散模型训练和推理成本高，限制了个人与中小团队的参与。作者希望在显著降低计算资源的同时，保持或接近SOTA的生成质量与可控性，实现“可负担的高质量”与方法可复现、可扩展的范式。

Method: 1) 结构：提出Cross-U-Transformer（XUT），U形Transformer，用跨注意力替代传统U-Net式跳跃连接以强化多尺度特征融合与构图一致性。2) 训练配方：引入TREAD加速；提出“shifted square crop”以高效支持任意纵横比；采用渐进式分辨率提升。3) 模型规模：约343M参数的小模型，结合上述设计实现高质生成与相机控制等能力。

Result: 在4×RTX5090上以约$535–620成本完成训练，能生成1024×1024高质量图像，表现具有竞争力；模型展现良好的构图一致性与可控性，并出现直觉式相机控制等涌现能力。

Conclusion: 精心设计的紧凑架构与高效训练策略，可在消费级硬件上实现高质量文生图，提供了有别于“简单堆算力/参数规模”的可扩展范式，为个人与小型组织民主化高质量生成铺路。

Abstract: We introduce Home-made Diffusion Model (HDM), an efficient yet powerful
text-to-image diffusion model optimized for training (and inferring) on
consumer-grade hardware. HDM achieves competitive 1024x1024 generation quality
while maintaining a remarkably low training cost of $535-620 using four RTX5090
GPUs, representing a significant reduction in computational requirements
compared to traditional approaches. Our key contributions include: (1)
Cross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer
(XUT), that employs cross-attention for skip connections, providing superior
feature integration that leads to remarkable compositional consistency; (2) a
comprehensive training recipe that incorporates TREAD acceleration, a novel
shifted square crop strategy for efficient arbitrary aspect-ratio training, and
progressive resolution scaling; and (3) an empirical demonstration that smaller
models (343M parameters) with carefully crafted architectures can achieve
high-quality results and emergent capabilities, such as intuitive camera
control. Our work provides an alternative paradigm of scaling, demonstrating a
viable path toward democratizing high-quality text-to-image generation for
individual researchers and smaller organizations with limited computational
resources.

</details>


### [82] [High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization](https://arxiv.org/abs/2509.06082)
*Anuraag Mishra,Andrea Gilch,Benjamin Apeleo Zubiri,Jan Rolfes,Frauke Liers*

Main category: cs.CV

TL;DR: 提出一种将边缘检测神经网络嵌入重建优化模型的纳米/微断层成像重建方法，利用相位均一性与锐利界面先验抑制伪影与模糊，实验显著提升界面清晰度与材料均匀性。


<details>
  <summary>Details</summary>
Motivation: 传统投影式纳米/微层析重建在样品由少数均匀材料相且存在尖锐边界时，易出现模糊与伪影；现有算法难以同时保持边缘锐利与区域均一，需要一种能将“锐利边缘+相内均一”的先验有效注入重建过程的技术。

Method: 先训练一个神经网络在子图上预测边缘位置；将该网络的预测作为先验嵌入一个数学优化模型中，使优化解倾向于遵循学得的边缘/均一性结构，但在原始投影数据强力支持下仍可偏离，从而在数据一致性与先验之间平衡，减少传统重建产生的伪影与模糊。

Result: 在实验数据集上，相比基线算法，重建结果的界面更锐利、相内更均匀、伪影减少，整体重建质量显著提升。

Conclusion: 该方法有效融合学习到的边缘先验与数据一致性，显著提升纳米/微断层成像的重建质量，具有推动断层成像技术发展的潜力。

Abstract: In this work, we develop a novel technique for reconstructing images from
projection-based nano- and microtomography. Our contribution focuses on
enhancing reconstruction quality, particularly for specimen composed of
homogeneous material phases connected by sharp edges. This is accomplished by
training a neural network to identify edges within subpictures. The trained
network is then integrated into a mathematical optimization model, to reduce
artifacts from previous reconstructions. To this end, the optimization approach
favors solutions according to the learned predictions, however may also
determine alternative solutions if these are strongly supported by the raw
data. Hence, our technique successfully incorporates knowledge about the
homogeneity and presence of sharp edges in the sample and thereby eliminates
blurriness. Our results on experimental datasets show significant enhancements
in interface sharpness and material homogeneity compared to benchmark
algorithms. Thus, our technique produces high-quality reconstructions,
showcasing its potential for advancing tomographic imaging techniques.

</details>


### [83] [MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.06096)
*Yiwen Ye,Yicheng Wu,Xiangde Luo,He Zhang,Ziyang Chen,Ting Dang,Yanning Zhang,Yong Xia*

Main category: cs.CV

TL;DR: 提出MedSeqFT顺序微调框架，通过样本选择与知识蒸馏式LoRA微调，在不断接入新分割任务时保留预训练知识并提升泛化，平均Dice提升约3%。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割任务不断新增，现有并行微调无法共享知识，多任务微调需同时访问全部数据、难以增量接入，且容易遗忘预训练能力，需要一种既能顺序适配新任务又保留/提升泛化的方案。

Method: 提出MedSeqFT，包含两部分：1) 最大数据相似度（MDS）选择，从下游数据中挑选最接近预训练分布的样本以巩固通用知识；2) 知识与泛化保持微调（K&G RFT），基于LoRA的知识蒸馏策略，在任务特异适配与保留预训练知识之间权衡，逐任务顺序更新。

Result: 在两个多任务3D分割数据集、共十个任务上，较SOTA微调策略稳定领先，平均Dice约+3.0%；在两个未见任务（COVID-19-20与Kidney）上表现更强，尤其肿瘤分割迁移性更好；损失景观与参数变化分析显示方法更稳健。

Conclusion: 顺序微调可作为面向演化临床任务的有效范式；MedSeqFT通过样本选择与LoRA蒸馏实现知识保持与性能提升，具备更强的可迁移性与鲁棒性；代码将开源。

Abstract: Foundation models have become a promising paradigm for advancing medical
image analysis, particularly for segmentation tasks where downstream
applications often emerge sequentially. Existing fine-tuning strategies,
however, remain limited: parallel fine-tuning isolates tasks and fails to
exploit shared knowledge, while multi-task fine-tuning requires simultaneous
access to all datasets and struggles with incremental task integration. To
address these challenges, we propose MedSeqFT, a sequential fine-tuning
framework that progressively adapts pre-trained models to new tasks while
refining their representational capacity. MedSeqFT introduces two core
components: (1) Maximum Data Similarity (MDS) selection, which identifies
downstream samples most representative of the original pre-training
distribution to preserve general knowledge, and (2) Knowledge and
Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge
distillation scheme that balances task-specific adaptation with the retention
of pre-trained knowledge. Extensive experiments on two multi-task datasets
covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently
outperforms state-of-the-art fine-tuning strategies, yielding substantial
performance gains (e.g., an average Dice improvement of 3.0%). Furthermore,
evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT
enhances transferability, particularly for tumor segmentation. Visual analyses
of loss landscapes and parameter variations further highlight the robustness of
MedSeqFT. These results establish sequential fine-tuning as an effective,
knowledge-retentive paradigm for adapting foundation models to evolving
clinical tasks. Code will be released.

</details>


### [84] [PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology](https://arxiv.org/abs/2509.06105)
*Yating Huang,Ziyan Huang,Lintao Xiang,Qijun Yang,Hujun Yin*

Main category: cs.CV

TL;DR: 提出PathoHR-Bench评估集与病理专用VL训练方案，提升病理图像-文本层级语义与组合推理能力，并在多个数据集上SOTA。


<details>
  <summary>Details</summary>
Motivation: 病理图像结构相似、形态差异细微，现有视觉-语言模型难以进行层级语义理解与复杂推理，限制临床应用，需要一个针对病理领域的评测基准与有效训练策略。

Method: 1) 构建PathoHR-Bench：聚焦层级语义理解与组合推理，评估跨模态关系建模能力。2) 提出病理特定VL训练方案：通过生成增强与扰动样本，用于多模态对比学习，强化精细病理表征与鲁棒推理。

Result: 基准结果显示现有VL模型在跨模态复杂关系建模上明显不足。采用所提训练方案后，在PathoHR-Bench与6个额外病理数据集上取得SOTA性能。

Conclusion: PathoHR-Bench揭示了通用VL模型在病理场景的局限；定制化的对比学习与样本增强/扰动策略有效提升了细粒度病理表征与组合推理能力，具有临床应用潜力。

Abstract: Accurate analysis of pathological images is essential for automated tumor
diagnosis but remains challenging due to high structural similarity and subtle
morphological variations in tissue images. Current vision-language (VL) models
often struggle to capture the complex reasoning required for interpreting
structured pathological reports. To address these limitations, we propose
PathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in
hierarchical semantic understanding and compositional reasoning within the
pathology domain. Results of this benchmark reveal that existing VL models fail
to effectively model intricate cross-modal relationships, hence limiting their
applicability in clinical setting. To overcome this, we further introduce a
pathology-specific VL training scheme that generates enhanced and perturbed
samples for multimodal contrastive learning. Experimental evaluations
demonstrate that our approach achieves state-of-the-art performance on
PathoHR-Bench and six additional pathology datasets, highlighting its
effectiveness in fine-grained pathology representation.

</details>


### [85] [CARDIE: clustering algorithm on relevant descriptors for image enhancement](https://arxiv.org/abs/2509.06116)
*Giulia Bonino,Luca Alberto Rizzo*

Main category: cs.CV

TL;DR: 提出CARDIE：基于颜色与亮度内容的无监督图像聚类，用以改进图像增强流程，并展示其对重采样数据集与增强算法评测的价值。


<details>
  <summary>Details</summary>
Motivation: 传统图像聚类更多面向语义任务；用于图像增强时，如何定义“对增强有意义”的簇很困难。现有按语义属性聚类与增强目标（如亮度分布、局部方差变化）不匹配，限制了增强算法训练与评估。

Method: 1) 提出CARDIE：以图像的颜色与亮度分布为特征进行无监督聚类；2) 提出量化增强算法对亮度分布与局部方差影响的指标/方法；3) 用上述指标比较CARDIE与语义聚类的相关性与有效性；4) 利用CARDIE簇对增强数据集进行重采样/再平衡，并在具体任务（色调映射、去噪）中验证。

Result: CARDIE得到的聚类与图像增强相关性更强，优于基于语义属性的聚类；基于CARDIE的簇进行数据集重采样可提升色调映射与去噪算法的性能。代码已在GitHub开源。

Conclusion: 围绕颜色与亮度内容进行无监督聚类能更好服务图像增强；所提评估指标可揭示增强算法对亮度分布与局部方差的影响；通过CARDIE簇重采样可以切实提升增强模型表现，并具备可复现性与可推广性。

Abstract: Automatic image clustering is a cornerstone of computer vision, yet its
application to image enhancement remains limited, primarily due to the
difficulty of defining clusters that are meaningful for this specific task. To
address this issue, we introduce CARDIE, an unsupervised algorithm that
clusters images based on their color and luminosity content. In addition, we
introduce a method to quantify the impact of image enhancement algorithms on
luminance distribution and local variance. Using this method, we demonstrate
that CARDIE produces clusters more relevant to image enhancement than those
derived from semantic image attributes. Furthermore, we demonstrate that CARDIE
clusters can be leveraged to resample image enhancement datasets, leading to
improved performance for tone mapping and denoising algorithms. To encourage
adoption and ensure reproducibility, we publicly release CARDIE code on our
GitHub.

</details>


### [86] [SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks](https://arxiv.org/abs/2509.06122)
*Tang Sui,Songxi Yang,Qunying Huang*

Main category: cs.CV

TL;DR: 提出SpecSwin3D，一种基于3D shifted-window Transformer的多光谱到高光谱生成模型，结合级联光谱扩展训练与优化输入带序策略，在保持空间细节与光谱一致性的同时，将5个MSI波段重建为224个HSI波段，并在PSNR、SAM、SSIM等指标大幅优于MHF-Net；同时在土地利用分类与烧毁区分割任务上验证了下游效益。


<details>
  <summary>Details</summary>
Motivation: 传统从多光谱生成高光谱的方法（如类泛锐化、矩阵分解、CNN）难以同时保留高空间细节与高光谱保真，且谱距远离的波段重建误差显著增大，影响实际应用（农业、环境监测、城市规划）中的可靠性与通用性。

Method: 1) 架构：采用3D shifted-window Transformer（类似Swin但扩展到光谱维）以联合建模空间-光谱相关性；2) 输入与输出：以5个多光谱带为输入，重建224个同分辨率的高光谱带；3) 级联训练：按光谱距离逐步扩展重建范围，先学近邻后覆盖全谱，稳定训练并降低远距波段误差；4) 优化带序：对5个选定MS带进行重复与排序设计，增强成对关系的捕获，提升3D窗口内自注意力效果；5) 评估：对比MHF-Net等，报告PSNR/SAM/SSIM/ERGAS及下游任务表现。

Result: 在重建任务上，PSNR=35.82 dB、SAM=2.40度、SSIM=0.96，相比MHF-Net提升PSNR约+5.6 dB，ERGAS减半以上；在下游任务（土地利用分类、烧毁区分割）中也取得更好效果，显示生成HSI对应用有益。

Conclusion: SpecSwin3D通过3D窗口Transformer、级联光谱扩展训练及带序优化，有效兼顾空间细节与光谱保真，显著优于现有方法，并在实际遥感下游任务中展现出实用价值。

Abstract: Multispectral and hyperspectral imagery are widely used in agriculture,
environmental monitoring, and urban planning due to their complementary spatial
and spectral characteristics. A fundamental trade-off persists: multispectral
imagery offers high spatial but limited spectral resolution, while
hyperspectral imagery provides rich spectra at lower spatial resolution. Prior
hyperspectral generation approaches (e.g., pan-sharpening variants, matrix
factorization, CNNs) often struggle to jointly preserve spatial detail and
spectral fidelity. In response, we propose SpecSwin3D, a transformer-based
model that generates hyperspectral imagery from multispectral inputs while
preserving both spatial and spectral quality. Specifically, SpecSwin3D takes
five multispectral bands as input and reconstructs 224 hyperspectral bands at
the same spatial resolution. In addition, we observe that reconstruction errors
grow for hyperspectral bands spectrally distant from the input bands. To
address this, we introduce a cascade training strategy that progressively
expands the spectral range to stabilize learning and improve fidelity.
Moreover, we design an optimized band sequence that strategically repeats and
orders the five selected multispectral bands to better capture pairwise
relations within a 3D shifted-window transformer framework. Quantitatively, our
model achieves a PSNR of 35.82 dB, SAM of 2.40{\deg}, and SSIM of 0.96,
outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by
more than half. Beyond reconstruction, we further demonstrate the practical
value of SpecSwin3D on two downstream tasks, including land use classification
and burnt area segmentation.

</details>


### [87] [RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving](https://arxiv.org/abs/2509.06142)
*Zhengquan Luo,Chi Liu,Dongfu Xiao,Zhen Yu,Yueye Wang,Tianqing Zhu*

Main category: cs.CV

TL;DR: 提出RetinaGuard：在眼底图像上用特征级对抗遮蔽与多源蒸馏，实现对“视网膜年龄”这一生物标志的通用隐私防护，同时尽量保持图像质量与疾病诊断性能。


<details>
  <summary>Details</summary>
Motivation: AI可从眼底图像推断“视网膜年龄”等敏感生物特征，已与疾病风险、行为、衰老和死亡率相关，但这也带来隐私泄露风险。需要一种方法在不损害临床有用性的前提下，阻断对敏感生物标志的推断。

Method: 提出RetinaGuard框架：1) 在特征层进行生成对抗式遮蔽，专门压制与年龄相关的表示；2) 采用多对一知识蒸馏，将“视网膜基础模型”和多个异构的替身年龄编码器的知识蒸馏到单一防护模型，形成对黑盒年龄预测器的通用防御；3) 同时优化保持图像视觉质量与病理特征可判别性。

Result: 在全面实验中，RetinaGuard显著降低第三方对眼底图像进行年龄预测的准确性/相关性，同时对图像质量和病灶表征的影响最小，维持疾病诊断任务的性能。

Conclusion: RetinaGuard可有效隐匿视网膜年龄等敏感生物标志，并具有良好的通用性与可扩展性，可推广到其他医学影像衍生生物标志的隐私保护。

Abstract: The integration of AI with medical images enables the extraction of implicit
image-derived biomarkers for a precise health assessment. Recently, retinal
age, a biomarker predicted from fundus images, is a proven predictor of
systemic disease risks, behavioral patterns, aging trajectory and even
mortality. However, the capability to infer such sensitive biometric data
raises significant privacy risks, where unauthorized use of fundus images could
lead to bioinformation leakage, breaching individual privacy. In response, we
formulate a new research problem of biometric privacy associated with medical
images and propose RetinaGuard, a novel privacy-enhancing framework that
employs a feature-level generative adversarial masking mechanism to obscure
retinal age while preserving image visual quality and disease diagnostic
utility. The framework further utilizes a novel multiple-to-one knowledge
distillation strategy incorporating a retinal foundation model and diverse
surrogate age encoders to enable a universal defense against black-box age
prediction models. Comprehensive evaluations confirm that RetinaGuard
successfully obfuscates retinal age prediction with minimal impact on image
quality and pathological feature representation. RetinaGuard is also flexible
for extension to other medical image derived biomarkers. RetinaGuard is also
flexible for extension to other medical image biomarkers.

</details>


### [88] [UniVerse-1: Unified Audio-Video Generation via Stitching of Experts](https://arxiv.org/abs/2509.06155)
*Duomin Wang,Wei Zuo,Aojie Li,Ling-Hao Chen,Xinyao Liao,Deyu Zhou,Zixin Yin,Xili Dai,Daxin Jiang,Gang Yu*

Main category: cs.CV

TL;DR: 提出UniVerse-1：一个类Veo-3的统一音视频生成模型，可同步生成协调的画面与声音；通过“专家拼接”(SoE)深度融合预训练视频/音乐专家，并用在线标注流水线保障音频（环境音与语音）与视频的时间对齐；在约7600小时数据上微调，达到良好音画协调与语音对齐；并发布评测集Verse-Bench与开源模型代码。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生成模型在音画同步、标注对齐与训练成本上存在瓶颈；从零训练昂贵且难以利用已有专家模型能力，文本标注错配会显著损伤音画对齐与质量；需要一种有效融合专家并确保时间对齐的数据与训练方案，以缩小与SOTA（如Veo-3）的差距。

Method: 1) Stitching of Experts(SoE)：将预训练视频与音乐/音频生成专家的对应模块深度融合，形成统一生成器；2) 在线标注流水线：在训练时动态处理视频、环境声与语音，生成精确标签并做时间对齐，避免离线文本标注失配；3) 在约7600小时音视频数据上微调，面向环境音生成与语音生成两类任务；4) 构建Verse-Bench基准对模型系统评测。

Result: 模型能生成音画协调的环境声音，并在语音生成与视频内容上实现强时间对齐；相较基线，显著降低因标注错配带来的性能退化；提供了可复现实验与新基准供比较。

Conclusion: 通过SoE融合与在线标注对齐策略，UniVerse-1在有限微调数据下实现高质量同步音视频生成，缩小与SOTA（如Veo-3）的性能差距；公开模型、代码与Verse-Bench以促进社区研究。

Abstract: We introduce UniVerse-1, a unified, Veo-3-like model capable of
simultaneously generating coordinated audio and video. To enhance training
efficiency, we bypass training from scratch and instead employ a stitching of
experts (SoE) technique. This approach deeply fuses the corresponding blocks of
pre-trained video and music generation experts models, thereby fully leveraging
their foundational capabilities. To ensure accurate annotations and temporal
alignment for both ambient sounds and speech with video content, we developed
an online annotation pipeline that processes the required training data and
generates labels during training process. This strategy circumvents the
performance degradation often caused by misalignment text-based annotations.
Through the synergy of these techniques, our model, after being finetuned on
approximately 7,600 hours of audio-video data, produces results with
well-coordinated audio-visuals for ambient sounds generation and strong
alignment for speech generation. To systematically evaluate our proposed
method, we introduce Verse-Bench, a new benchmark dataset. In an effort to
advance research in audio-video generation and to close the performance gap
with state-of-the-art models such as Veo3, we make our model and code publicly
available. We hope this contribution will benefit the broader research
community. Project page: https://dorniwang.github.io/UniVerse-1/.

</details>


### [89] [UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning](https://arxiv.org/abs/2509.06165)
*Huy Le,Nhat Chung,Tung Kieu,Jingkang Yang,Ngan Le*

Main category: cs.CV

TL;DR: 提出UNO，一个统一的端到端框架，用单阶段方式同时完成盒级与全景像素级视频场景图生成，通过扩展的slot attention分解对象与关系、时间一致性学习与动态三元组预测，实现高效且具竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VidSGG方法往往针对盒级或像素级之一，需任务特定架构与多阶段训练，难以共享参数与泛化。需要一个最小化任务特化改动、可跨粒度级别统一建模且高效的方案。

Method: - 统一单阶段端到端框架UNO，最大化参数共享；
- 扩展的slot attention，将视觉特征解耦为“对象槽”和“关系槽”；
- 对象时间一致性学习：无显式跟踪，跨帧约束对象表示一致；
- 动态三元组预测：将关系槽动态绑定到对象对，建模随时间演化的交互；
- 同时适配盒级与像素级VidSGG任务，最小任务特定改动。

Result: 在标准盒级与像素级VidSGG基准上达到有竞争力的结果，并在效率上优于多阶段/多模型方案。

Conclusion: UNO以对象为中心、统一且高效地处理不同粒度的VidSGG任务，通过时间一致性与动态关系建模，实现性能与效率的兼顾，显示出良好的泛化与实用潜力。

Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual
content by detecting objects and modeling their temporal interactions as
structured graphs. Prior studies typically target either coarse-grained
box-level or fine-grained panoptic pixel-level VidSGG, often requiring
task-specific architectures and multi-stage training pipelines. In this paper,
we present UNO (UNified Object-centric VidSGG), a single-stage, unified
framework that jointly addresses both tasks within an end-to-end architecture.
UNO is designed to minimize task-specific modifications and maximize parameter
sharing, enabling generalization across different levels of visual granularity.
The core of UNO is an extended slot attention mechanism that decomposes visual
features into object and relation slots. To ensure robust temporal modeling, we
introduce object temporal consistency learning, which enforces consistent
object representations across frames without relying on explicit tracking
modules. Additionally, a dynamic triplet prediction module links relation slots
to corresponding object pairs, capturing evolving interactions over time. We
evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results
demonstrate that UNO not only achieves competitive performance across both
tasks but also offers improved efficiency through a unified, object-centric
design.

</details>


### [90] [AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models](https://arxiv.org/abs/2509.06228)
*Amna Hassan,Ilsa Afzaal,Nouman Muneeb,Aneeqa Batool,Hamail Noor*

Main category: cs.CV

TL;DR: 提出一套基于轻量自定义CNN的X光骨折自动检测方法，在FracAtlas（4083张）上获得约96%准确率，优于其在同设定下的EfficientNetB0/MobileNetV2/ResNet50迁移学习基线；但受类别不平衡与数据集限制，需谨慎解读并做外部验证。


<details>
  <summary>Details</summary>
Motivation: 骨折在全球范围内，尤其是资源匮乏地区，因缺乏放射科专家、影像成本高与辐射暴露等问题，导致诊断滞后与功能受损。需要一种成本低、自动化、可在低资源环境部署的骨折检测方法，以减少对专家解读的依赖。

Method: 构建一套自定义轻量CNN用于X光骨折二分类/检测（文中强调分类指标），并与三种常用迁移学习骨干（EfficientNetB0、MobileNetV2、ResNet50）在同一训练流程与数据（FracAtlas，4083张匿名肌骨X光）上基准对比；评估指标包括accuracy、precision、recall、F1。

Result: 自定义CNN在FracAtlas测试上取得Accuracy 95.96%、Precision 0.94、Recall 0.88、F1 0.91。迁移学习模型在该设定下表现较差。作者指出结果可能受类别不平衡与数据集规模/多样性影响。

Conclusion: 轻量自定义CNN在该数据集上显示出良好骨折检测潜力，但需注意公平基准设定、类别平衡、数据多样性，并进行外部验证以推动临床转化。

Abstract: Bone fractures present a major global health challenge, often resulting in
pain, reduced mobility, and productivity loss, particularly in low-resource
settings where access to expert radiology services is limited. Conventional
imaging methods suffer from high costs, radiation exposure, and dependency on
specialized interpretation. To address this, we developed an AI-based solution
for automated fracture detection from X-ray images using a custom Convolutional
Neural Network (CNN) and benchmarked it against transfer learning models
including EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on
the publicly available FracAtlas dataset, comprising 4,083 anonymized
musculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94
precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.
Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)
performed poorly in this specific setup, these results should be interpreted in
light of class imbalance and data set limitations. This work highlights the
promise of lightweight CNNs for detecting fractures in X-rays and underscores
the importance of fair benchmarking, diverse datasets, and external validation
for clinical translation

</details>


### [91] [Exploring Light-Weight Object Recognition for Real-Time Document Detection](https://arxiv.org/abs/2509.06246)
*Lucas Wojcik,Luiz Coelho,Roger Granada,David Menotti*

Main category: cs.CV

TL;DR: 提出一种高效的文档检测与矫正管线：将车牌检测网络IWPOD‑Net改造用于身份证/证件图像，结合数据增强与跨数据集验证，以OCR质量为核心指标，取得在速度与模型体量上优于现有方法、且OCR表现具竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 实时文档检测与几何矫正对自动信息检索（OCR前处理）至关重要，但该细分方向研究相对不足；现有方法常在“大模型提精度”与“小模型提效率”间权衡，缺少同时兼顾的轻量级方案。

Method: 改造IWPOD‑Net从车牌转用于文档角点/四边形检测；在合成证件数据集NBID上训练，并与MIDV做跨数据集验证；探索数据增强策略；对比目标检测与倾斜估计SOTA方法，将各方法的检测结果用于透视矫正后送入OCR；提出基于Levenshtein距离的新OCR质量指标，统一评估端到端信息检索效果。

Result: 所提模型在模型尺寸更小、推理更快的同时，OCR质量指标与现有SOTA相当或具竞争力；实验表明几何矫正无需“完美”也能达到SOTA级别OCR分数。

Conclusion: 以OCR质量为目标优化的轻量级文档检测与矫正管线可在效率与准确度间取得更优折中；改造的IWPOD‑Net在实时性与资源占用上优于SOTA且保持竞争性OCR表现；代码已开源以便复现与扩展。

Abstract: Object Recognition and Document Skew Estimation have come a long way in terms
of performance and efficiency. New models follow one of two directions:
improving performance using larger models, and improving efficiency using
smaller models. However, real-time document detection and rectification is a
niche that is largely unexplored by the literature, yet it remains a vital step
for automatic information retrieval from visual documents. In this work, we
strive towards an efficient document detection pipeline that is satisfactory in
terms of Optical Character Recognition (OCR) retrieval and faster than other
available solutions. We adapt IWPOD-Net, a license plate detection network, and
train it for detection on NBID, a synthetic ID card dataset. We experiment with
data augmentation and cross-dataset validation with MIDV (another synthetic ID
and passport document dataset) to find the optimal scenario for the model.
Other methods from both the Object Recognition and Skew Estimation
state-of-the-art are evaluated for comparison with our approach. We use each
method to detect and rectify the document, which is then read by an OCR system.
The OCR output is then evaluated using a novel OCR quality metric based on the
Levenshtein distance. Since the end goal is to improve automatic information
retrieval, we use the overall OCR quality as a performance metric. We observe
that with a promising model, document rectification does not have to be perfect
to attain state-of-the-art performance scores. We show that our model is
smaller and more efficient than current state-of-the-art solutions while
retaining a competitive OCR quality metric. All code is available at
https://github.com/BOVIFOCR/iwpod-doc-corners.git

</details>


### [92] [Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes](https://arxiv.org/abs/2509.06266)
*Mohsen Gholami,Ahmad Rezaei,Zhou Weimin,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: 提出Ego3D-Bench用于评估VLM在自车视角多视图户外场景的3D空间推理，并给出Ego3D-VLM后训练框架，通过认知地图与全局3D坐标提升空间理解，显著优于基线但仍低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多基于单图或室内视频的空间QA，难以覆盖真实机器人/自动驾驶的自中心多视角户外场景需求；需要标准化基准与可泛化的方法提升3D空间推理。

Method: 1) 构建Ego3D-Bench：基于自中心多视图户外数据，人工深度参与标注，形成8600+ QA对，并用于评测16个SOTA VLM；2) 提出Ego3D-VLM：在推理/后训练阶段，从多视图估计全局3D坐标，生成认知地图（cognitive map），将地图与VLM结合进行空间问答与距离估计；方法模块化，可接入任意VLM。

Result: 实验显示人类与VLM存在明显差距；采用Ego3D-VLM后，多选QA平均提升12%，绝对距离估计平均提升56%，在Ego3D-Bench上对多种模型有效。

Conclusion: 当前VLM在真实多视图户外3D空间理解上明显落后于人类。Ego3D-Bench提供评测标准，Ego3D-VLM通过认知地图与全局坐标注入显著提升表现，作为可插拔后训练框架，有助于推动朝人类水平的空间理解发展。

Abstract: Understanding 3D spatial relationships remains a major limitation of current
Vision-Language Models (VLMs). Prior work has addressed this issue by creating
spatial question-answering (QA) datasets based on single images or indoor
videos. However, real-world embodied AI agents such as robots and self-driving
cars typically rely on ego-centric, multi-view observations. To this end, we
introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial
reasoning abilities of VLMs using ego-centric, multi-view outdoor data.
Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement
from human annotators to ensure quality and diversity. We benchmark 16 SOTA
VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results
reveal a notable performance gap between human level scores and VLM
performance, highlighting that current VLMs still fall short of human level
spatial understanding. To bridge this gap, we propose Ego3D-VLM, a
post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM
generates cognitive map based on estimated global 3D coordinates, resulting in
12% average improvement on multi-choice QA and 56% average improvement on
absolute distance estimation. Ego3D-VLM is modular and can be integrated with
any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for
advancing toward human level spatial understanding in real-world, multi-view
environments.

</details>


### [93] [AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution](https://arxiv.org/abs/2509.06282)
*Cecelia Soh,Rizhao Cai,Monalisha Paul,Dennis Sng,Alex Kot*

Main category: cs.CV

TL;DR: 提出用智能手机自拍人脸图像远程估计皮肤水分（SH）与经皮失水（TEWL），并以含皮肤先验的自适应ViT与对称对比正则化缓解标注不平衡，实现无物理测量的皮肤屏障评估。


<details>
  <summary>Details</summary>
Motivation: SH与TEWL是量化皮肤屏障功能的核心指标，关系到皮肤健康与疾病防护；但现有测量依赖专业仪器与门诊，公众难以常规获取。因此需要一种大众可及、远程、低成本的评估方案。

Method: 构建完整流水线：1) 收集SH/TEWL标注的人脸自拍数据并预处理；2) 设计皮肤先验自适应Vision Transformer用于回归SH与TEWL；3) 发现标注分布不均衡导致偏置，提出基于对称性的对比正则化以缓解不平衡并提升鲁棒性。

Result: 实验显示所提方法能从自拍图像有效回归SH/TEWL，且对称对比正则化显著减小因标注不均带来的模型偏差，优于不含该正则的基线。

Conclusion: 首次在无物理测量条件下，通过自拍实现皮肤屏障关键指标估计，连接计算机视觉与护肤研究，具备面向大众的AI皮肤分析应用潜力。

Abstract: Skin health and disease resistance are closely linked to the skin barrier
function, which protects against environmental factors and water loss. Two key
physiological indicators can quantitatively represent this barrier function:
skin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH
and TEWL is valuable for the public to monitor skin conditions regularly,
diagnose dermatological issues, and personalize their skincare regimens.
However, these measurements are not easily accessible to general users unless
they visit a dermatology clinic with specialized instruments. To tackle this
problem, we propose a systematic solution to estimate SH and TEWL from selfie
facial images remotely with smartphones. Our solution encompasses multiple
stages, including SH/TEWL data collection, data preprocessing, and formulating
a novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression.
Through experiments, we identified the annotation imbalance of the SH/TEWL data
and proposed a symmetric-based contrastive regularization to reduce the model
bias due to the imbalance effectively. This work is the first study to explore
skin assessment from selfie facial images without physical measurements. It
bridges the gap between computer vision and skin care research, enabling
AI-driven accessible skin analysis for broader real-world applications.

</details>


### [94] [Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding](https://arxiv.org/abs/2509.06291)
*Jiangnan Xie,Xiaolong Zheng,Liang Zheng*

Main category: cs.CV

TL;DR: 提出PAML框架，结合跨模态对齐、判别式视觉编码、原型发现与继承、以及多阶段解码器，实现强鲁棒视觉指代引导，标准场景具竞争力，开放词汇场景达SOTA。


<details>
  <summary>Details</summary>
Motivation: Transformer式VG在开放词汇场景性能明显下滑，主要因视觉-语言对齐不充分、跨模态融合不足、以及语义原型利用低效，需系统性方法提升对新类别的识别与定位。

Method: 1) 采用ALBEF进行初始跨模态对齐；2) 视觉判别特征编码器强化显著目标、抑制无关背景；3) 原型发现与继承：从多邻域抽取并聚合语义原型，支持开放词汇识别；4) 多阶段解码器进行充分的多模态融合后回归框。

Result: 在5个基准上全面评测：标准场景达到有竞争力表现；开放词汇场景取得SOTA。

Conclusion: PAML通过更强对齐、判别编码、与语义原型机制，显著提升开放词汇视觉指代能力，同时保持标准场景性能，验证了原型感知的多模态学习范式的有效性。

Abstract: Visual Grounding (VG) aims to utilize given natural language queries to
locate specific target objects within images. While current transformer-based
approaches demonstrate strong localization performance in standard scene (i.e,
scenarios without any novel objects), they exhibit notable limitations in
open-vocabulary scene (i.e, both familiar and novel object categories during
testing). These limitations primarily stem from three key factors: (1)
imperfect alignment between visual and linguistic modalities, (2) insufficient
cross-modal feature fusion, and (3) ineffective utilization of semantic
prototype information. To overcome these challenges, we present Prototype-Aware
Multimodal Learning (PAML), an innovative framework that systematically
addresses these issues through several key components: First, we leverage ALBEF
to establish robust cross-modal alignment during initial feature encoding.
Subsequently, our Visual Discriminative Feature Encoder selectively enhances
salient object representations while suppressing irrelevant visual context. The
framework then incorporates a novel prototype discovering and inheriting
mechanism that extracts and aggregates multi-neighbor semantic prototypes to
facilitate open-vocabulary recognition. These enriched features undergo
comprehensive multimodal integration through our Multi-stage Decoder before
final bounding box regression. Extensive experiments across five benchmark
datasets validate our approach, showing competitive performance in standard
scene while achieving state-of-the-art results in open-vocabulary scene. Our
code is available at https://github.com/plankXie/PAML.

</details>


### [95] [Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning](https://arxiv.org/abs/2509.06306)
*Zhang Jing,Pu Nan,Xie Yu Xiang,Guo Yanming,Lu Qianqi,Zou Shiwei,Yan Jie,Chen Yan*

Main category: cs.CV

TL;DR: 提出视频领域的通用类别发现（Video-GCD）新设定与基准，并用记忆引导的一致性感知对比学习（MCCL）融合时空线索，显著优于将图像方法直接迁移到视频的做法。


<details>
  <summary>Details</summary>
Motivation: 现有GCD多基于静态图像，难以可靠发现新类别；视频包含跨时间的多视角信息，可提供更稳健的类别线索，但缺乏相应方法与基准，因此扩展到视频并设计能有效整合时空信息的学习框架。

Method: 提出MCCL框架，含两部分：1) CACL：从多视角时序特征估计未标注样本间一致性分数，并据此加权对比损失；2) MGRE：引入特征级与logit级双层记忆缓冲，提供全局上下文以增强类内紧致、类间分离，并反哺改进CACL的一致性估计，形成表示学习与一致性建模的正反馈。

Result: 在新构建的视频GCD基准（含动作识别与鸟类分类数据集）上，方法显著优于从图像设定改造的竞争方法，证明利用时间信息能更好地发现视频中的新类别。

Conclusion: 面向视频的GCD需要显式建模时空一致性与全局记忆。MCCL通过一致性加权对比学习与记忆增强实现两者的耦合，在多个数据集上取得明显提升，强调了时序信息对新类别发现的重要性；代码将公开。

Abstract: Generalized Category Discovery (GCD) is an emerging and challenging
open-world problem that has garnered increasing attention in recent years. Most
existing GCD methods focus on discovering categories in static images. However,
relying solely on static visual content is often insufficient to reliably
discover novel categories. To bridge this gap, we extend the GCD problem to the
video domain and introduce a new setting, termed Video-GCD. Thus, effectively
integrating multi-perspective information across time is crucial for accurate
Video-GCD. To tackle this challenge, we propose a novel Memory-guided
Consistency-aware Contrastive Learning (MCCL) framework, which explicitly
captures temporal-spatial cues and incorporates them into contrastive learning
through a consistency-guided voting mechanism. MCCL consists of two core
components: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided
Representation Enhancement (MGRE). CACL exploits multiperspective temporal
features to estimate consistency scores between unlabeled instances, which are
then used to weight the contrastive loss accordingly. MGRE introduces a
dual-level memory buffer that maintains both feature-level and logit-level
representations, providing global context to enhance intra-class compactness
and inter-class separability. This in turn refines the consistency estimation
in CACL, forming a mutually reinforcing feedback loop between representation
learning and consistency modeling. To facilitate a comprehensive evaluation, we
construct a new and challenging Video-GCD benchmark, which includes action
recognition and bird classification video datasets. Extensive experiments
demonstrate that our method significantly outperforms competitive GCD
approaches adapted from image-based settings, highlighting the importance of
temporal information for discovering novel categories in videos. The code will
be publicly available.

</details>


### [96] [Text4Seg++: Advancing Image Segmentation via Generative Language Modeling](https://arxiv.org/abs/2509.06321)
*Mengcheng Lan,Chaofeng Chen,Jiaxing Xu,Zongrui Li,Yiping Ke,Xudong Jiang,Yingchen Yu,Yunqing Zhao,Song Bai*

Main category: cs.CV

TL;DR: 提出将图像分割转化为文本生成：用“语义描述符”把每个图像补丁映射为文本标签，并通过R-RLE压缩和“语义砖块”提升效率与粒度，构建Text4Seg与Text4Seg++，在无需任务特定微调下于多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉-语言任务强，但把高分辨率、密集预测的分割无缝接入仍困难：传统方案需额外分割头/解码器、训练复杂、推理慢且与语言管线割裂。作者希望用统一的语言建模把分割纳入生成框架，提升兼容性、可扩展性与通用性。

Method: 1) 文本即掩码：把分割表述为文本序列生成。2) 语义描述符：为图像补丁赋文本标签，得到与补丁对齐的序列。3) R-RLE行级游程编码：压缩重复标签序列，平均缩短74%，推理加速3倍。4) Text4Seg框架：在MLLM上直接生成语义描述符以完成分割。5) 盒级语义描述符：用检测框定位ROI，并用结构化“语义砖块”作为掩码令牌，形成下一个砖块预测任务。6) Text4Seg++：在上述基础上实现更细粒度与紧凑表示，保持与各类MLLM主干兼容。

Result: 在自然图像与遥感数据集上，Text4Seg与尤其是Text4Seg++在多种基准上稳定超越SOTA；R-RLE在不降性能前提下将文本长度降低约74%，推理加速约3倍；无需任务特定微调即可获得强泛化。

Conclusion: 将分割统一为文本生成是可行且高效的：语义描述符与R-RLE/语义砖块使得模型兼具精度、可扩展与生成效率；Text4Seg++展示了在不同MLLM主干与多领域基准上的优越性与通用性。

Abstract: Multimodal Large Language Models (MLLMs) have shown exceptional capabilities
in vision-language tasks. However, effectively integrating image segmentation
into these models remains a significant challenge. In this work, we propose a
novel text-as-mask paradigm that casts image segmentation as a text generation
problem, eliminating the need for additional decoders and significantly
simplifying the segmentation process. Our key innovation is semantic
descriptors, a new textual representation of segmentation masks where each
image patch is mapped to its corresponding text label. We first introduce
image-wise semantic descriptors, a patch-aligned textual representation of
segmentation masks that integrates naturally into the language modeling
pipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding
(R-RLE), which compresses redundant text sequences, reducing the length of
semantic descriptors by 74% and accelerating inference by $3\times$, without
compromising performance. Building upon this, our initial framework Text4Seg
achieves strong segmentation performance across a wide range of vision tasks.
To further improve granularity and compactness, we propose box-wise semantic
descriptors, which localizes regions of interest using bounding boxes and
represents region masks via structured mask tokens called semantic bricks. This
leads to our refined model, Text4Seg++, which formulates segmentation as a
next-brick prediction task, combining precision, scalability, and generative
efficiency. Comprehensive experiments on natural and remote sensing datasets
show that Text4Seg++ consistently outperforms state-of-the-art models across
diverse benchmarks without any task-specific fine-tuning, while remaining
compatible with existing MLLM backbones. Our work highlights the effectiveness,
scalability, and generalizability of text-driven image segmentation within the
MLLM framework.

</details>


### [97] [Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap](https://arxiv.org/abs/2509.06329)
*Ruiming Du,Guangxun Zhai,Tian Qiu,Yu Jiang*

Main category: cs.CV

TL;DR: 综述植物3D点云分割现状与挑战，汇总数据集与方法，发布开源基准平台PSS，并通过实验验证稀疏卷积与Transformer实例分割的有效性，以及合成数据在降标注成本上的作用，提出面向数据高效与可泛化的路线图。


<details>
  <summary>Details</summary>
Motivation: 植物表型研究需要精确表征植物形态，但当前3D分割在该领域受限于标注数据稀缺、将先进网络迁移到植物点云的技术难题、以及缺乏面向植物的标准化基准与评测协议。

Method: 系统性综述：盘点植物与通用3D分割数据集；总结点云语义与实例分割的深度学习方法；提出开源框架Plant Segmentation Studio（PSS）用于可复现基准；开展代表性网络与仿真到真实（sim-to-real）学习策略的广泛量化实验。

Result: 实验表明稀疏卷积骨干和基于Transformer的实例分割在植物点云上表现优异；基于建模和增强的合成数据在sim-to-real中互补，能有效降低标注需求。

Conclusion: 该研究搭建了连接算法前沿与实际部署的桥梁，提供数据与代码（PSS），为构建数据高效、可泛化的3D植物表型深度学习方案提供工具与路线图。

Abstract: The precise characterization of plant morphology provides valuable insights
into plant environment interactions and genetic evolution. A key technology for
extracting this information is 3D segmentation, which delineates individual
plant organs from complex point clouds. Despite significant progress in general
3D computer vision domains, the adoption of 3D segmentation for plant
phenotyping remains limited by three major challenges: i) the scarcity of
large-scale annotated datasets, ii) technical difficulties in adapting advanced
deep neural networks to plant point clouds, and iii) the lack of standardized
benchmarks and evaluation protocols tailored to plant science. This review
systematically addresses these barriers by: i) providing an overview of
existing 3D plant datasets in the context of general 3D segmentation domains,
ii) systematically summarizing deep learning-based methods for point cloud
semantic and instance segmentation, iii) introducing Plant Segmentation Studio
(PSS), an open-source framework for reproducible benchmarking, and iv)
conducting extensive quantitative experiments to evaluate representative
networks and sim-to-real learning strategies. Our findings highlight the
efficacy of sparse convolutional backbones and transformer-based instance
segmentation, while also emphasizing the complementary role of modeling-based
and augmentation-based synthetic data generation for sim-to-real learning in
reducing annotation demands. In general, this study bridges the gap between
algorithmic advances and practical deployment, providing immediate tools for
researchers and a roadmap for developing data-efficient and generalizable deep
learning solutions in 3D plant phenotyping. Data and code are available at
https://github.com/perrydoremi/PlantSegStudio.

</details>


### [98] [Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users](https://arxiv.org/abs/2509.06331)
*Md Sultanul Islam Ovi,Mainul Hossain,Md Badsha Biswas*

Main category: cs.CV

TL;DR: 提出一个统一的货币评估框架，融合面额识别（轻量CNN）、损伤量化（UCDI指标）与伪钞检测（基于特征的模板匹配），在8.2万张多状态纸币上验证，可在低资源、离线、视障用户场景下实现实时、可解释、紧凑的部署。


<details>
  <summary>Details</summary>
Motivation: 现有研究多只做面额分类，忽视纸币的物理损伤与真伪验证，尤其在低资源环境与视障用户离线使用场景下缺乏可用、可靠的整体验证方案。

Method: 构建三模块统一框架：1) 轻量级CNN（如自定义Custom_CNN）进行面额分类；2) 提出统一货币损伤指数UCDI，综合二值掩码损失、色彩失真与结构特征损失给出连续可用性评分；3) 采用基于特征的模板匹配进行伪钞检测，适应多种成像条件。提供超过8.2万张含干净/损伤/伪造纸币的标注数据用于训练评测，并面向端侧实时推理。

Result: Custom_CNN在较低参数量下取得高分类性能；UCDI能稳定输出可解释的连续可用性评分；伪钞检测模块在多样成像条件下可靠识别伪钞；整体框架满足实时、设备端、受限环境部署需求。

Conclusion: 通过轻量、可解释、端侧可部署的统一框架，兼顾面额识别、损伤评估与伪钞检测，提升货币评估在真实、资源受限场景的实用性与包容性。

Abstract: Currency recognition systems often overlook usability and authenticity
assessment, especially in low-resource environments where visually impaired
users and offline validation are common. While existing methods focus on
denomination classification, they typically ignore physical degradation and
forgery, limiting their applicability in real-world conditions. This paper
presents a unified framework for currency evaluation that integrates three
modules: denomination classification using lightweight CNN models, damage
quantification through a novel Unified Currency Damage Index (UCDI), and
counterfeit detection using feature-based template matching. The dataset
consists of over 82,000 annotated images spanning clean, damaged, and
counterfeit notes. Our Custom_CNN model achieves high classification
performance with low parameter count. The UCDI metric provides a continuous
usability score based on binary mask loss, chromatic distortion, and structural
feature loss. The counterfeit detection module demonstrates reliable
identification of forged notes across varied imaging conditions. The framework
supports real-time, on-device inference and addresses key deployment challenges
in constrained environments. Results show that accurate, interpretable, and
compact solutions can support inclusive currency evaluation in practical
settings.

</details>


### [99] [Multi-Modal Camera-Based Detection of Vulnerable Road Users](https://arxiv.org/abs/2509.06333)
*Penelope Brown,Julie Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 提出一种融合RGB与热红外成像并微调YOLOv8的多模态VRU检测框架；在多数据集训练下，通过类重加权、轻量增广、分辨率与冻结策略，提升少数类召回与总体效率；热模态精度最高，RGB→热增广可进一步提升召回，适用于交叉口安全场景。


<details>
  <summary>Details</summary>
Motivation: VRU（行人、骑行者、摩托车手）在低照度、恶劣天气和类别不平衡下难以检测，导致高致死率；现有单模态与常规训练策略不足以应对上述挑战，因此需要一个稳健的多模态、对少数类友好的检测方案。

Method: 构建融合RGB与热红外的多模态检测框架，基于YOLOv8进行微调；训练数据涵盖KITTI、BDD100K与Teledyne FLIR；采用类重加权损失与轻量数据增广；实验探索输入分辨率（640像素）、部分骨干冻结以权衡精度与速度，并尝试RGB→热域增广以提升跨模态泛化。

Result: 640像素输入与部分骨干冻结可同时优化准确率与效率；类加权损失显著提升少数类（罕见VRU）的召回；热红外模型在精度上领先；引入RGB→热增广后，召回进一步提升。

Conclusion: 多模态（RGB+热）结合适当训练策略能在复杂环境下显著改善VRU检测，特别是在交叉口场景；热模态关键提升精度，类重加权与跨模态增广有效增强少数类与低照条件下的召回与鲁棒性。

Abstract: Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists
represent more than half of global traffic deaths, yet their detection remains
challenging in poor lighting, adverse weather, and unbalanced data sets. This
paper presents a multimodal detection framework that integrates RGB and thermal
infrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,
BDD100K, and Teledyne FLIR datasets, with class re-weighting and light
augmentations to improve minority-class performance and robustness, experiments
show that 640-pixel resolution and partial backbone freezing optimise accuracy
and efficiency, while class-weighted losses enhance recall for rare VRUs.
Results highlight that thermal models achieve the highest precision, and
RGB-to-thermal augmentation boosts recall, demonstrating the potential of
multimodal detection to improve VRU safety at intersections.

</details>


### [100] [Harnessing Object Grounding for Time-Sensitive Video Understanding](https://arxiv.org/abs/2509.06335)
*Tz-Ying Wu,Sharath Nittur Sridhar,Subarna Tripathi*

Main category: cs.CV

TL;DR: 提出GO-Tokenizer，把检测到的对象信息以紧凑形式编码进Video-LLM，以提升时间敏感视频理解（TSV）；相比在提示中加入对象文本描述，能更短、更稳、更准，跨模型/数据集/任务泛化良好。


<details>
  <summary>Details</summary>
Motivation: TSV任务需要精确对齐时间与视觉事件。仅靠视频帧或在提示中拼接对象文本会带来冗长token和对象噪声敏感性。作者观察在LITA上加入对象级信息能提升时序定位，但文本化会引入长度和噪声问题，因此希望以更紧凑、鲁棒的方式把对象线索注入Video-LLM。

Method: 利用现成目标检测器在帧内抽取对象（类别、位置、置信度等），设计轻量的GO-Tokenizer模块，将对象级信息在线编码为紧凑向量/标记，与视频或语言特征对齐，用于预训练与下游推理；与两种基线对比：原生Video-LLM（无对象）与把对象描述作为文本加入提示。

Result: 在多模型、多数据集、多任务（如时序推理定位与密集字幕）上，GO-Tokenizer预训练的模型优于原生Video-LLM与“对象文本提示”方案；表现更高且更稳健、对噪声不敏感、序列更短。

Conclusion: 将具身的对象信息以紧凑编码注入Video-LLM可显著提升TSV与相关视频理解任务，优于简单的文本增强方案，且具有良好的跨设置泛化与效率优势。

Abstract: We propose to improve the time-sensitive video understanding (TSV) capability
of video large language models (Video-LLMs) with grounded objects (GO). We
hypothesize that TSV tasks can benefit from GO within frames, which is
supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM
for reasoning temporal localization. While augmenting prompts with textual
description of these object annotations improves the performance of LITA, it
also introduces extra token length and susceptibility to the noise in object
level information. To address this, we propose GO-Tokenizer, a lightweight
add-on module for Video-LLMs leveraging off-the-shelf object detectors to
encode compact object information on the fly. Experimental results demonstrate
that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its
counterpart utilizing textual description of objects in the prompt. The gain
generalizes across different models, datasets and video understanding tasks
such as reasoning temporal localization and dense captioning.

</details>


### [101] [Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing](https://arxiv.org/abs/2509.06336)
*Jeongmin Yu,Susang Kim,Kisu Lee,Taekyoung Kwon,Won-Yong Shin,Ha Young Kim*

Main category: cs.CV

TL;DR: 提出MVP-FAS，利用多视角文本和patch级对齐提升CLIP式人脸反欺骗跨域泛化，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的FAS方法未充分利用patch嵌入，忽略关键伪造线索；且每类仅用单一文本提示，导致对域文本依赖强、泛化性差。

Method: 构建包含两大模块的框架：1) 多视角Slot注意力（MVS）：以多种语义改写文本引导，对CLIP的patch tokens进行slot attention，联合提取局部细节与全局上下文；2) 多文本-补丁对齐（MTPA）：将图像patch与多种文本表示进行对齐，提升语义鲁棒性与泛化。整体以多组释义文本作为监督/引导，减少域特定文本偏置。

Result: 在多组跨域数据集上取得优于以往SOTA的泛化性能，实验广泛验证了方法有效。

Conclusion: 多视角文本引导与patch级多文本对齐能充分挖掘CLIP patch信息，提升FAS跨域鲁棒性；MVP-FAS在跨域评测中表现领先，并提供代码实现。

Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain
performance by employing vision-language models like CLIP. However, existing
CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,
failing to detect critical spoofing clues. Moreover, these models rely on a
single text prompt per class (e.g., 'live' or 'fake'), which limits
generalization. To address these issues, we propose MVP-FAS, a novel framework
incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text
Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to
generate generalized features and reduce dependence on domain-specific text.
MVS extracts local detailed spatial features and global context from patch
embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns
patches with multiple text representations to improve semantic robustness.
Extensive experiments demonstrate that MVP-FAS achieves superior generalization
performance, outperforming previous state-of-the-art methods on cross-domain
datasets. Code: https://github.com/Elune001/MVP-FAS.

</details>


### [102] [A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study](https://arxiv.org/abs/2509.06351)
*Krithik Ramesh,Ritvik Koneru*

Main category: cs.CV

TL;DR: 提出一个统一的深度学习管线，用ResNet‑50同时对结直肠组织病理切片和结肠镜视频帧进行分类，结合类平衡、增强与校准，展示可解释、可复现的多模态诊断原型。


<details>
  <summary>Details</summary>
Motivation: 传统诊断流程在组织病理和内镜影像间相互独立，准备复杂、评估分散，易引入变异与低效。需要一种能同时处理多来源影像、提高一致性与效率的统一模型。

Method: 构建单一CNN管线（ResNet‑50）对两种模态进行分类；采用类不平衡处理（类平衡学习）、强数据增强与模型校准；数据来自PathMNIST（结肠组织病理静态图）与HyperKvasir（下消化道/结肠镜视频帧）。强调可解释性与可复现性。

Result: 在两个公开数据集上实现对病理切片与结肠镜帧的分类，验证了统一管线的可行性与稳定性（具体数值未在摘要中给出）。

Conclusion: 统一的、多模态CNN诊断管线可简化流程、降低变异并提升结直肠疾病检测的可解释性与可复现性，为临床集成提供初步证据（仍为试点研究）。

Abstract: Colorectal diseases, including inflammatory conditions and neoplasms, require
quick, accurate care to be effectively treated. Traditional diagnostic
pipelines require extensive preparation and rely on separate, individual
evaluations on histological images and colonoscopy footage, introducing
possible variability and inefficiencies. This pilot study proposes a unified
deep learning network that uses convolutional neural networks (CN N s) to
classify both histopathological slides and colonoscopy video frames in one
pipeline. The pipeline integrates class-balancing learning, robust
augmentation, and calibration methods to ensure accurate results. Static colon
histology images were taken from the PathMNIST dataset, and the lower
gastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.
The CNN architecture used was ResNet-50. This study demonstrates an
interpretable and reproducible diagnostic pipeline that unifies multiple
diagnostic modalities to advance and ease the detection of colorectal diseases.

</details>


### [103] [MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification](https://arxiv.org/abs/2509.06367)
*Aswini Kumar Patra,Lingaraj Sahoo*

Main category: cs.CV

TL;DR: 提出一种轻量级混合CNN用于干旱胁迫识别，参数量比常规CNN/ViT减少约15倍且精度可比，并引入基于梯度范数影响函数的“机器遗忘”机制以按需移除特定训练数据影响；在马铃薯航拍数据上验证，兼顾高准确与低算力，适合资源受限的精准农业场景。


<details>
  <summary>Details</summary>
Motivation: 传统干旱胁迫检测方法耗时耗力；深度模型（CNN/ViT）虽有效但参数多、算力需求高，不利于资源受限与实时应用。需要既轻量又准确、还能适应数据变化/隐私删除需求的模型。

Method: 设计受ResNet、DenseNet、MobileNet启发的轻量混合CNN框架，通过残差/密集连接与深度可分离卷积等策略显著减少参数；并提出基于梯度范数的影响函数实现机器遗忘，可选择性削弱/移除特定训练样本对模型的影响。使用含专家标注（健康/干旱区）的马铃薯田航拍数据进行训练与评估。

Result: 在该数据集上，所提模型以约15倍更少的可训练参数实现与主流CNN/ViT相当的准确率，显著降低计算成本；机器遗忘机制展现出在不完全重训的情况下调整模型的能力。

Conclusion: 该轻量框架在精准农业干旱监测中具备实际落地价值，特别适合资源受限与实时需求场景；结合机器遗忘提升了模型的可适应性与合规性，具备可扩展与推广潜力。

Abstract: Drought stress is a major threat to global crop productivity, making its
early and precise detection essential for sustainable agricultural management.
Traditional approaches, though useful, are often time-consuming and
labor-intensive, which has motivated the adoption of deep learning methods. In
recent years, Convolutional Neural Network (CNN) and Vision Transformer
architectures have been widely explored for drought stress identification;
however, these models generally rely on a large number of trainable parameters,
restricting their use in resource-limited and real-time agricultural settings.
To address this challenge, we propose a novel lightweight hybrid CNN framework
inspired by ResNet, DenseNet, and MobileNet architectures. The framework
achieves a remarkable 15-fold reduction in trainable parameters compared to
conventional CNN and Vision Transformer models, while maintaining competitive
accuracy. In addition, we introduce a machine unlearning mechanism based on a
gradient norm-based influence function, which enables targeted removal of
specific training data influence, thereby improving model adaptability. The
method was evaluated on an aerial image dataset of potato fields with
expert-annotated healthy and drought-stressed regions. Experimental results
show that our framework achieves high accuracy while substantially lowering
computational costs. These findings highlight its potential as a practical,
scalable, and adaptive solution for drought stress monitoring in precision
agriculture, particularly under resource-constrained conditions.

</details>


### [104] [Your Super Resolution Model is not Enough for Tackling Real-World Scenarios](https://arxiv.org/abs/2509.06387)
*Dongsik Yoon,Jongeun Kim*

Main category: cs.CV

TL;DR: 提出可插拔的尺度感知注意模块（SAAM），为固定倍率的SISR模型赋予任意倍率超分能力，兼顾精度与开销。


<details>
  <summary>Details</summary>
Motivation: 现有单图超分模型多为固定放大倍率，跨尺度泛化差，难以在真实场景中应对多样倍率需求，需要一种能在不显著增加计算量的前提下实现任意倍率且性能稳定的方法。

Method: 设计SAAM作为可插拔模块：1）轻量级、随尺度自适应的特征提取与上采样路径；2）引入无参数的SimAM注意力作为高效引导；3）提出梯度方差损失以增强边缘与细节锐度；4）无缝集成到多种SOTA骨干（如SCNet、HiT-SR、OverNet），从固定尺度扩展为任意尺度。

Result: 在多基准数据集和广泛的整数/非整数放大倍率上取得与SOTA相当或更优的重建精度，同时计算开销增量很小；展现出强鲁棒的多尺度上采样能力。

Conclusion: SAAM为固定尺度SR模型提供了通用的任意尺度升级路径，在保持轻量与高效的同时提升细节与泛化，适合真实应用落地。

Abstract: Despite remarkable progress in Single Image Super-Resolution (SISR),
traditional models often struggle to generalize across varying scale factors,
limiting their real-world applicability. To address this, we propose a plug-in
Scale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR
models with the ability to perform arbitrary-scale SR. SAAM employs
lightweight, scale-adaptive feature extraction and upsampling, incorporating
the Simple parameter-free Attention Module (SimAM) for efficient guidance and
gradient variance loss to enhance sharpness in image details. Our method
integrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet,
HiT-SR, OverNet), delivering competitive or superior performance across a wide
range of integer and non-integer scale factors. Extensive experiments on
benchmark datasets demonstrate that our approach enables robust multi-scale
upscaling with minimal computational overhead, offering a practical solution
for real-world scenarios.

</details>


### [105] [AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery](https://arxiv.org/abs/2509.06396)
*Lorenz Achim Kuhn,Daniel Abler,Jonas Richiardi,Andreas F. Hottinger,Luis Schiappacasse,Vincent Dunet,Adrien Depeursinge,Vincent Andrearczyk*

Main category: cs.CV

TL;DR: 构建自动化纵向MRI数据整理与分析管线，基于896个转移灶/177名患者，识别5类生长轨迹，并用传统模型与图机器学习预测12个月疗效，AUC最高0.90（梯度提升）与0.88（GML）。


<details>
  <summary>Details</summary>
Motivation: 脑转移灶SRS后需要长期MRI随访，但人工标注与量化负担极大，临床多为目测评估，限制了对生长轨迹与疗效/毒性的早期判断与个体化决策支持。需要自动化、可扩展的方法来整理纵向数据、刻画轨迹并早期预测疗效。

Method: 1) 搭建自动化数据管线，整理>360天、约每两月一次随访的纵向MRI与SRS数据，得到CHUV队列（896灶/177例）。2) 数据驱动聚类识别典型生长轨迹。3) 使用仅基线与首随访MRI的特征训练传统机器学习（如梯度提升）预测12个月病灶层面反应。4) 引入图机器学习（GML），以单一模型适配多种时间点输入配置进行预测。

Result: - 聚类得到5种主导生长轨迹，并与不同最终反应类别对应。- 仅用术前与首随访两时点，梯度提升预测12个月反应AUC最高达0.90（95%CI 0.88–0.92）。- GML在多时间点配置下取得稳健性能，AUC最高0.88（95%CI 0.86–0.90），模型更具输入灵活性。

Conclusion: 自动化管线可规模化整理纵向SRS-MRI数据，识别脑转移灶生长模式并早期预测疗效，显示出提升随访评估自动化与精确度的潜力，为构建个体化临床决策支持系统奠定基础。

Abstract: Brain Metastases (BM) are a large contributor to mortality of patients with
cancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored
with Magnetic Resonance Imaging (MRI) at regular follow-up intervals according
to treatment guidelines. Analyzing and quantifying this longitudinal imaging
represents an intractable workload for clinicians. As a result, follow-up
images are not annotated and merely assessed by observation. Response to
treatment in longitudinal imaging is being studied, to better understand growth
trajectories and ultimately predict treatment success or toxicity as early as
possible. In this study, we implement an automated pipeline to curate a large
longitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in
177 patients who were monitored for >360 days at approximately two-month
intervals at Lausanne University Hospital (CHUV). We use a data-driven
clustering to identify characteristic trajectories. In addition, we predict 12
months lesion-level response using classical as well as graph machine learning
Graph Machine Learning (GML). Clustering revealed 5 dominant growth
trajectories with distinct final response categories. Response prediction
reaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first
follow-up MRI with gradient boosting. Similarly, robust predictive performance
of up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more
flexibility with a single model for multiple input time-points configurations.
Our results suggest potential automation and increased precision for the
comprehensive assessment and prediction of BM response to SRS in longitudinal
MRI. The proposed pipeline facilitates scalable data curation for the
investigation of BM growth patterns, and lays the foundation for clinical
decision support systems aiming at optimizing personalized care.

</details>


### [106] [3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom](https://arxiv.org/abs/2509.06400)
*Matthieu Gendrin,Stéphane Pateux,Théo Ladune*

Main category: cs.CV

TL;DR: 论文针对3D Gaussian Splatting在大场景、3DoF+（位置小范围移动）设置下的坐标量化问题，推导位置误差到像素投影误差的关系（误差与点距相机距离的平方反比），据此提出基于球坐标的量化方案，并在Garden数据上展示更优的率失真表现。


<details>
  <summary>Details</summary>
Motivation: 3DGS在大场景中通常只允许相机在一个小区域内移动（3DoF+），此时坐标量化误差会以不均匀方式影响成像质量，尤其对近景点更敏感。现有笛卡尔坐标量化未充分利用这种距离相关性，导致在相同码率下投影误差次优。

Method: 1) 从透视投影几何出发，分析位置量化误差对像素投影误差的传播，证明投影误差与点到相机距离的平方成反比。2) 基于该结论设计球坐标量化：围绕观测中心对半径、方位和俯仰分别量化，使近处点获得更精细的径向分辨率，从而在给定码率下控制投影误差。3) 在Garden场景上进行率失真评估，与基线（如笛卡尔均匀量化）对比。

Result: 理论上给出误差传播公式并得到投影误差∝1/距离^2；实验上在Garden场景上，所提球坐标量化在相同比特率下获得更低的重建失真/更高的视图质量（细节未给出具体数值，但表明优于基线）。

Conclusion: 在3DoF+设置下，采用与距离相关的球坐标量化更匹配投影误差特性，相对传统笛卡尔量化具备更优的率失真性能；这一思路可推广到受限视点范围的大场景3DGS压缩/量化中。

Abstract: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene
reconstruction. With a number of views of a given object or scene, the
algorithm trains a model composed of 3D gaussians, which enables the production
of novel views from arbitrary points of view. This freedom of movement is
referred to as 6DoF for 6 degrees of freedom: a view is produced for any
position (3 degrees), orientation of camera (3 other degrees). On large scenes,
though, the input views are acquired from a limited zone in space, and the
reconstruction is valuable for novel views from the same zone, even if the
scene itself is almost unlimited in size. We refer to this particular case as
3DoF+, meaning that the 3 degrees of freedom of camera position are limited to
small offsets around the central position. Considering the problem of
coordinate quantization, the impact of position error on the projection error
in pixels is studied. It is shown that the projection error is proportional to
the squared inverse distance of the point being projected. Consequently, a new
quantization scheme based on spherical coordinates is proposed. Rate-distortion
performance of the proposed method are illustrated on the well-known Garden
scene.

</details>


### [107] [VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results](https://arxiv.org/abs/2509.06413)
*Yixiao Li,Xin Li,Chris Wei Zhou,Shuo Xing,Hadi Amirpour,Xiaoshuai Hao,Guanghui Yue,Baoquan Zhao,Weide Liu,Xiaoyuan Yang,Zhengzhong Tu,Xinyu Li,Chuanbiao Song,Chenqi Zhang,Jun Lan,Huijia Zhu,Weiqiang Wang,Xiaoyan Sun,Shishun Tian,Dongyang Yan,Weixia Zhang,Junlin Chen,Wei Sun,Zhihua Wang,Zhuohang Shi,Zhizun Luo,Hang Ouyang,Tianxin Xiao,Fan Yang,Zhaowang Wu,Kaixin Deng*

Main category: cs.CV

TL;DR: 介绍了ISRGC-Q挑战赛，基于ISRGen-QA数据集，聚焦评估GAN与扩散模型生成的超分辨率图像质量与伪影；共有108人报名、4队提交有效方案，取得SOTA表现，项目已开源。


<details>
  <summary>Details</summary>
Motivation: 现有SR-IQA数据集与评测方法多基于传统或非生成式SR方法，难以有效反映GAN/扩散式SR产生的独特伪影与感知质量差异，因此需要构建面向生成式SR的新数据与基准来推动评估研究与公平对比。

Method: 搭建ISRGC-Q挑战赛：以ISRGen-QA数据集为核心，收集包含GAN与扩散模型生成的SR图像，面向参与者开放训练与评测；在最终测试阶段收集各队方法与fact sheet，统一在同一数据与协议下评测感知质量预测性能。

Result: 108名参赛者注册，最终4支队伍提交有效方案并在ISRGen-QA上达到SOTA水平，证明该基准能有效区分方法优劣并促进性能提升。

Conclusion: ISRGC-Q挑战聚焦生成式SR的感知质量评估，填补了现有SR-IQA在生成伪影方面的评测空白；开源项目与竞赛结果为后续研究提供了标准化数据与强基线。

Abstract: This paper presents the ISRGC-Q Challenge, built upon the Image
Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and
organized as part of the Visual Quality Assessment (VQualA) Competition at the
ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment
(SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated
by the latest generative approaches, including Generative Adversarial Networks
(GANs) and diffusion models. The primary goal of this challenge is to analyze
the unique artifacts introduced by modern super-resolution techniques and to
evaluate their perceptual quality effectively. A total of 108 participants
registered for the challenge, with 4 teams submitting valid solutions and fact
sheets for the final testing phase. These submissions demonstrated
state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is
publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.

</details>


### [108] [Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models](https://arxiv.org/abs/2509.06415)
*Jaemin Son,Sujin Choi,Inyong Yun*

Main category: cs.CV

TL;DR: 提出一种轻量级文档图像token裁剪框架：先用二分类补丁级分类器剔除非文本背景，再用最大池化恢复碎片化文本区域，保持空间连贯。在真实数据上显著降算力，精度基本不损失。


<details>
  <summary>Details</summary>
Motivation: VLM在文档理解上效果好但计算开销大（图像token多、包含大量无信息背景）。需要在保持准确性的前提下减少输入token数以降低推理成本。

Method: 两阶段token裁剪：1）补丁级二分类器检测并移除非文本区域（背景token）；2）最大池化式的空间连通性修复，对被切碎的文本块进行扩张/合并，恢复文本区域的完整性与连贯性，然后将保留的补丁映射为VLM输入token。

Result: 在真实文档数据集上，计算成本显著下降（例如token数/算力需求明显减少），同时任务准确率与不裁剪时相当（“maintaining comparable accuracy”）。

Conclusion: 在不显著牺牲精度的条件下，大幅降低VLM处理文档图像的计算成本；简单高效、可作为上游预处理模块集成到现有文档VLM流程中。

Abstract: Recent progress in vision-language models (VLMs) has led to impressive
results in document understanding tasks, but their high computational demands
remain a challenge. To mitigate the compute burdens, we propose a lightweight
token pruning framework that filters out non-informative background regions
from document images prior to VLM processing. A binary patch-level classifier
removes non-text areas, and a max-pooling refinement step recovers fragmented
text regions to enhance spatial coherence. Experiments on real-world document
datasets demonstrate that our approach substantially lowers computational
costs, while maintaining comparable accuracy.

</details>


### [109] [Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM](https://arxiv.org/abs/2509.06422)
*Hua Zhang,Changjiang Luo,Ruoyu Chen*

Main category: cs.CV

TL;DR: 提出Phantom-Insight，一种结合SAM与多模态大模型的VCOD方法，通过时间-空间线索融合、动态前景可视token评分与提示网络、以及前景-背景解耦学习，显著提升伪装目标边缘与前景/背景可分性，在MoCA-Mask上SOTA，并在CAD2016上具备强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VCOD方法在动态场景中难分离伪装目标：冻结的SAM难以刻画细微边缘，MLLM方法因语言模型倾向混并前景/背景导致可分性差，亟需一种同时增强边缘细节与前后景分离的新框架。

Method: 1) 用时间与空间线索表征视频，经LLM进行特征融合以提升信息密度；2) 设计动态前景可视token评分模块与提示网络，生成多重线索自适应引导并微调SAM，使其适配细微纹理；3) 提出前景-背景解耦学习策略，分别生成并训练前/背景线索，使可视token独立整合两类信息，提升分割可分性。

Result: 在MoCA-Mask数据集上取得多项指标SOTA；在未见过的CAD2016数据集上仍能有效检测伪装目标，显示良好泛化。

Conclusion: 融合SAM与MLLM并采用解耦学习与动态提示，可显著提升视频伪装目标分割的边缘细节与前后景可分性，实现SOTA性能与跨数据集泛化。

Abstract: Video camouflaged object detection (VCOD) is challenging due to dynamic
environments. Existing methods face two main issues: (1) SAM-based methods
struggle to separate camouflaged object edges due to model freezing, and (2)
MLLM-based methods suffer from poor object separability as large language
models merge foreground and background. To address these issues, we propose a
novel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the
separability of object edge details, we represent video sequences with temporal
and spatial clues and perform feature fusion via LLM to increase information
density. Next, multiple cues are generated through the dynamic foreground
visual token scoring module and the prompt network to adaptively guide and
fine-tune the SAM model, enabling it to adapt to subtle textures. To enhance
the separability of objects and background, we propose a decoupled
foreground-background learning strategy. By generating foreground and
background cues separately and performing decoupled training, the visual token
can effectively integrate foreground and background information independently,
enabling SAM to more accurately segment camouflaged objects in the video.
Experiments on the MoCA-Mask dataset show that Phantom-Insight achieves
state-of-the-art performance across various metrics. Additionally, its ability
to detect unseen camouflaged objects on the CAD2016 dataset highlights its
strong generalization ability.

</details>


### [110] [When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection](https://arxiv.org/abs/2509.06427)
*Rabin Dulal,Lihong Zheng,Muhammad Ashad Kabir*

Main category: cs.CV

TL;DR: 提出一种基于Grounding DINO的零样本牛只口鼻部（muzzle）检测框架，通过自然语言提示实现无需标注与任务特定训练的检测，mAP@0.5达76.8%。


<details>
  <summary>Details</summary>
Motivation: 传统靠人工标注或监督式模型（如YOLO）进行口鼻部检测，标注成本高、跨场景泛化差，尤其在新牛只与复杂环境下性能下降。需要一种可扩展、对标注依赖低、适应多品种多环境的检测方案。

Method: 采用视觉-语言模型Grounding DINO，通过自然语言提示引导检测，实现零样本定位口鼻部；不使用任务特定训练或标注数据；以通用文本提示适配不同品种与场景；评估以mAP@0.5为主要指标。

Result: 在无任何任务特定标注和训练的情况下，达到mAP@0.5=76.8%的检测性能，显示良好的零样本检测能力。

Conclusion: 该框架为业界落地提供了无需标注、部署便捷、跨品种与环境更具适应性的替代方案，是首个面向真实场景的零样本牛只口鼻部检测研究，具备实用性与可扩展性。

Abstract: Muzzle patterns are among the most effective biometric traits for cattle
identification. Fast and accurate detection of the muzzle region as the region
of interest is critical to automatic visual cattle identification.. Earlier
approaches relied on manual detection, which is labor-intensive and
inconsistent. Recently, automated methods using supervised models like YOLO
have become popular for muzzle detection. Although effective, these methods
require extensive annotated datasets and tend to be trained data-dependent,
limiting their performance on new or unseen cattle. To address these
limitations, this study proposes a zero-shot muzzle detection framework based
on Grounding DINO, a vision-language model capable of detecting muzzles without
any task-specific training or annotated data. This approach leverages natural
language prompts to guide detection, enabling scalable and flexible muzzle
localization across diverse breeds and environments. Our model achieves a mean
Average Precision (mAP)@0.5 of 76.8\%, demonstrating promising performance
without requiring annotated data. To our knowledge, this is the first research
to provide a real-world, industry-oriented, and annotation-free solution for
cattle muzzle detection. The framework offers a practical alternative to
supervised methods, promising improved adaptability and ease of deployment in
livestock monitoring applications.

</details>


### [111] [Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment](https://arxiv.org/abs/2509.06442)
*Yixiao Li,Xiaoyuan Yang,Guanghui Yue,Jun Fu,Qiuping Jiang,Xu Jia,Paul L. Rosin,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: 提出PBAN用于超分辨率图像全参考质量评价，通过感知导向的双向注意、分组多尺度可变形卷积和子信息激励卷积，自适应关注失真并回归质量分数，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率算法众多，但缺乏有效的全参考IQA指标来公平比较其感知质量；现有注意力型IQA方法对失真关注不充分，未与SR的生成-评价过程相一致，难以贴合人类视觉感知。

Method: 构建PBAN框架，包括：1) 图像编码器提取参考与SR图像特征；2) 感知导向的双向注意（PBA），面向失真建立由参考到失真、由失真到参考的双向注意；引入分组多尺度可变形卷积以自适应感知多尺度与非刚性失真；设计子信息激励卷积以在子像素与子通道维度上强化关键失真信息；3) 质量预测模块融合质量感知特征并回归质量分数。

Result: 在多项基准数据集上进行广泛实验，PBAN在相关性与一致性等指标上均超越现有SOTA的SR全参考IQA方法。

Conclusion: 以人类视觉特性为导向的双向注意与可变形多尺度建模能更好捕捉SR失真并提升质量评估准确性；PBAN为SR FR-IQA提供了更可靠的指标，优于当前方法。

Abstract: Many super-resolution (SR) algorithms have been proposed to increase image
resolution. However, full-reference (FR) image quality assessment (IQA) metrics
for comparing and evaluating different SR algorithms are limited. In this work,
we propose the Perception-oriented Bidirectional Attention Network (PBAN) for
image SR FR-IQA, which is composed of three modules: an image encoder module, a
perception-oriented bidirectional attention (PBA) module, and a quality
prediction module. First, we encode the input images for feature
representations. Inspired by the characteristics of the human visual system, we
then construct the perception-oriented PBA module. Specifically, different from
existing attention-based SR IQA methods, we conceive a Bidirectional Attention
to bidirectionally construct visual attention to distortion, which is
consistent with the generation and evaluation processes of SR images. To
further guide the quality assessment towards the perception of distorted
information, we propose Grouped Multi-scale Deformable Convolution, enabling
the proposed method to adaptively perceive distortion. Moreover, we design
Sub-information Excitation Convolution to direct visual perception to both
sub-pixel and sub-channel attention. Finally, the quality prediction module is
exploited to integrate quality-aware features and regress quality scores.
Extensive experiments demonstrate that our proposed PBAN outperforms
state-of-the-art quality assessment methods.

</details>


### [112] [Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark](https://arxiv.org/abs/2509.06456)
*Zongyi Xu,Zhongpeng Lang,Yilong Chen,Shanshan Zhao,Xiaoshui Huang,Yifan Zuo,Yan Zhang,Qianni Zhang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出Cross3DReg跨源点云数据集与结合图像的重叠区域预测+视觉-几何注意力匹配框架，显著提升跨源配准精度（RRE↓63.2%、RTE↓40.2%、RR↑5.4%）。


<details>
  <summary>Details</summary>
Motivation: 跨源点云配准缺少大规模真实数据集，且不同传感器导致点云分布、密度、噪声和视角差异大，造成特征提取与匹配不稳定，影响配准精度与鲁棒性。

Method: 1）构建最大规模真实多模态跨源点云数据集Cross3DReg，由旋转机械雷达与混合半固态雷达采集；2）提出基于重叠区域的跨源配准框架：利用未对齐图像预测源/目标点云重叠区域，过滤非重叠噪声点；3）设计视觉-几何注意力引导的匹配模块，融合图像与几何信息，提升跨源特征一致性与对应关系可靠性，从而完成鲁棒配准。

Result: 在大规模真实跨源数据上取得SOTA：相对旋转误差RRE降低63.2%，相对平移误差RTE降低40.2%，注册召回RR提升5.4%。

Conclusion: 数据+方法双管齐下：Cross3DReg为跨源配准提供训练/评测基准；重叠区域预测与视觉-几何注意力匹配有效抑制非重叠噪声并增强特征一致性，显著提升跨源点云配准的准确性与鲁棒性。

Abstract: Cross-source point cloud registration, which aims to align point cloud data
from different sensors, is a fundamental task in 3D vision. However, compared
to the same-source point cloud registration, cross-source registration faces
two core challenges: the lack of publicly available large-scale real-world
datasets for training the deep registration models, and the inherent
differences in point clouds captured by multiple sensors. The diverse patterns
induced by the sensors pose great challenges in robust and accurate point cloud
feature extraction and matching, which negatively influence the registration
accuracy. To advance research in this field, we construct Cross3DReg, the
currently largest and real-world multi-modal cross-source point cloud
registration dataset, which is collected by a rotating mechanical lidar and a
hybrid semi-solid-state lidar, respectively. Moreover, we design an
overlap-based cross-source registration framework, which utilizes unaligned
images to predict the overlapping region between source and target point
clouds, effectively filtering out redundant points in the irrelevant regions
and significantly mitigating the interference caused by noise in
non-overlapping areas. Then, a visual-geometric attention guided matching
module is proposed to enhance the consistency of cross-source point cloud
features by fusing image and geometric information to establish reliable
correspondences and ultimately achieve accurate and robust registration.
Extensive experiments show that our method achieves state-of-the-art
registration performance. Our framework reduces the relative rotation error
(RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$,
respectively, and improves the registration recall (RR) by $5.4\%$, which
validates its effectiveness in achieving accurate cross-source registration.

</details>


### [113] [IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks](https://arxiv.org/abs/2509.06459)
*Sebastian-Vasile Echim,Andrei-Alexandru Preda,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CV

TL;DR: 论文提出两种黑盒对抗攻击算法（ATA与AGA），基于仿射变换与遗传算法，在ResNet/DenseNet/ViT/Swin等模型与Tiny-ImageNet、Caltech-256、Food-101上评测，相比Pixle与Square Attack在分类任务上有更优表现（最高提升8.82%），并分析参数、数据增强、全局与定向攻击下的鲁棒性与防御启示。


<details>
  <summary>Details</summary>
Motivation: 深度网络虽性能强，但可解释性差、存在对抗脆弱性，特别是黑盒场景下很难有效攻击与评估鲁棒性。现有黑盒攻击方法在效率、迁移性或成功率上仍有限，亟需可泛化、无需模型细节的新型算法以系统比较不同视觉架构的脆弱性，并为防御提供指导。

Method: 提出两种迭代黑盒攻击：1) ATA：以随机仿射变换（旋转、缩放、平移、剪切等）为扰动空间，通过攻击评分函数迭代最大化；2) AGA：以遗传算法在包含随机噪声与仿射变换的搜索空间中进化优化。基于Tiny ImageNet、Caltech-256、Food-101，对ResNet-18、DenseNet-121、Swin V2、ViT进行评测；考察算法参数变化、数据增强设置，并进行全局（无目标）与定向（目标类别）攻击；与Pixle与Square Attack对比。

Result: 在图像分类任务上，所提算法总体优于对比黑盒方法，报告最高达8.82%的准确率提升；在多架构与多数据集上稳定有效；参数敏感性与数据增强实验显示方法在不同配置下保持较强攻击能力；在全局与定向攻击中均取得成功案例，并体现一定对抗鲁棒性分析价值。

Conclusion: 仿射变换与遗传搜索结合的黑盒攻击对多种视觉架构有效，较现有方法表现更好，并通过参数与设置变化实验提供了对成功攻击与防御策略的洞见；研究为评估与提升分类模型的对抗鲁棒性提供了实证依据。

Abstract: Deep neural networks currently dominate many fields of the artificial
intelligence landscape, achieving state-of-the-art results on numerous tasks
while remaining hard to understand and exhibiting surprising weaknesses. An
active area of research focuses on adversarial attacks, which aim to generate
inputs that uncover these weaknesses. However, this proves challenging,
especially in the black-box scenario where model details are inaccessible. This
paper explores in detail the impact of such adversarial algorithms on
ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network
architectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101
datasets, we benchmark two novel black-box iterative adversarial algorithms
based on affine transformations and genetic algorithms: 1) Affine
Transformation Attack (ATA), an iterative algorithm maximizing our attack score
function using random affine transformations, and 2) Affine Genetic Attack
(AGA), a genetic algorithm that involves random noise and affine
transformations. We evaluate the performance of the models in the algorithm
parameter variation, data augmentation, and global and targeted attack
configurations. We also compare our algorithms with two black-box adversarial
algorithms, Pixle and Square Attack. Our experiments yield better results on
the image classification task than similar methods in the literature, achieving
an accuracy improvement of up to 8.82%. We provide noteworthy insights into
successful adversarial defenses and attacks at both global and targeted levels,
and demonstrate adversarial robustness through algorithm parameter variation.

</details>


### [114] [Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning](https://arxiv.org/abs/2509.06461)
*Yuyao Ge,Shenghua Liu,Yiwei Wang,Lingrui Mei,Baolong Bi,Xuanshan Zhou,Jiayu Yao,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CV

TL;DR: 提出CARVE：一种无需训练的对比注意力细化方法，在像素级对比通用查询与任务查询的注意力，提取任务相关视觉信号，从而在复杂场景下显著提升VLM推理；实验证明在开源模型上最高提升达75%。


<details>
  <summary>Details</summary>
Motivation: 现有提升VLM视觉理解的方法常需额外训练、依赖外部分割器或只做粗粒度处理，忽视了VLM内部注意力可用性；复杂视觉环境下性能下降，亟需利用模型内在机制、低成本地增强视觉推理。

Method: 分析VLM多层注意力：发现视觉复杂度与注意力熵正相关且损害推理；注意力由浅层全局扫描到深层收敛且收敛度受复杂度影响；并从理论上证明通用查询与任务查询注意图的对比可将视觉信号分解为语义信号与视觉噪声。基于此提出CARVE：在像素级进行注意力对比提取任务相关信号，无需训练、无需外部工具。

Result: 广泛实验表明CARVE在多种开放源VLM上稳定提升表现，最高可达75%的性能增益，适用于复杂视觉场景与多种任务。

Conclusion: 视觉复杂度与注意力机制的关系可被利用，通过对比注意力进行训练免增强能有效提升VLM的视觉推理。CARVE提供了一条高效的、可普适迁移的增强路径。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across
diverse visual tasks, yet their performance degrades in complex visual
environments. While existing enhancement approaches require additional
training, rely on external segmentation tools, or operate at coarse-grained
levels, they overlook the innate ability within VLMs. To bridge this gap, we
investigate VLMs' attention patterns and discover that: (1) visual complexity
strongly correlates with attention entropy, negatively impacting reasoning
performance; (2) attention progressively refines from global scanning in
shallow layers to focused convergence in deeper layers, with convergence degree
determined by visual complexity. (3) Theoretically, we prove that the contrast
of attention maps between general queries and task-specific queries enables the
decomposition of visual signal into semantic signals and visual noise
components. Building on these insights, we propose Contrastive Attention
Refinement for Visual Enhancement (CARVE), a training-free method that extracts
task-relevant visual signals through attention contrasting at the pixel level.
Extensive experiments demonstrate that CARVE consistently enhances performance,
achieving up to 75% improvement on open-source models. Our work provides
critical insights into the interplay between visual complexity and attention
mechanisms, offering an efficient pathway for improving visual reasoning with
contrasting attention.

</details>


### [115] [A Statistical 3D Stomach Shape Model for Anatomical Analysis](https://arxiv.org/abs/2509.06464)
*Erez Posner,Ore Shtalrid,Oded Erell,Daniel Noy,Moshe Bouhnik*

Main category: cs.CV

TL;DR: 提出首个胃部三维统计形状模型（SSM），通过合成数据管线生成多样胃形，并结合真实CT网格进行半监督对齐与精炼，实现对未见解剖变异的稳健泛化，模型与数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 内部器官（如胃）的高保真参数化3D模型因数据稀缺与方法困难而匮乏，但它们对研究、诊断、术前规划至关重要；现有工作缺少系统性统计形状模型来捕捉胃形态变异。

Method: 1) 设计合成3D胃模型生成管线，依据已知胃形变异研究参数化生成多样形态；2) 基于合成数据训练低维统计形状模型（SSM）；3) 通过对公开CT数据提取的胃网格执行半监督对齐与精炼，提升对真实变异的泛化；4) 在留出真实CT测试集上评估拟合与泛化性能；5) 发布模型与数据集。

Result: 在留出真实胃CT扫描上获得稳健泛化与较高拟合精度；证明合成-现实混合训练策略有效。

Conclusion: 该工作首次提供胃部3D统计形状模型与合成数据集，能支持外科仿真、术前规划、医学教育与计算建模；合成数据+参数化建模+真实验证的管线为个性化医疗开辟新方向。

Abstract: Realistic and parameterized 3D models of human anatomy have become invaluable
in research, diagnostics, and surgical planning. However, the development of
detailed models for internal organs, such as the stomach, has been limited by
data availability and methodological challenges. In this paper, we propose a
novel pipeline for the generation of synthetic 3D stomach models, enabling the
creation of anatomically diverse morphologies informed by established studies
on stomach shape variability. Using this pipeline, we construct a dataset of
synthetic stomachs. Building on this dataset, we develop a 3D statistical shape
model of the stomach, trained to capture natural anatomical variability in a
low-dimensional shape space. The model is further refined using CT meshes
derived from publicly available datasets through a semi-supervised alignment
process, enhancing its ability to generalize to unseen anatomical variations.
We evaluated the model on a held-out test set of real stomach CT scans,
demonstrating robust generalization and fit accuracy. We make the statistical
shape model along with the synthetic dataset publicly available on GitLab:
https://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.
This work introduces the first statistical 3D shape model of the stomach, with
applications ranging from surgical simulation and pre-operative planning to
medical education and computational modeling. By combining synthetic data
generation, parametric modeling, and real-world validation, our approach
represents a significant advancement in organ modeling and opens new
possibilities for personalized healthcare solutions.

</details>


### [116] [Does DINOv3 Set a New Medical Vision Standard?](https://arxiv.org/abs/2509.06467)
*Che Liu,Yinda Chen,Haoyuan Shi,Jinpeng Lu,Bailiang Jian,Jiazhen Pan,Linghan Cai,Jiayi Wang,Yundi Zhang,Jun Li,Cosmin I. Bercea,Cheng Ouyang,Chen Chen,Zhiwei Xiong,Benedikt Wiestler,Christian Wachinger,Daniel Rueckert,Wenjia Bai,Rossella Arcucci*

Main category: cs.CV

TL;DR: 评估DINOv3作为统一医学视觉编码器的可迁移性：在多模态2D/3D分类与分割任务上表现强劲，部分超越医学专用基座，但在WSI、EM、PET等高度专业场景退化；规模与分辨率扩展不呈稳定增益。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉基座模型在自然图像上取得突破，但其在医学影像等专业领域的零/少样本或轻微适配能力尚不明确，需要系统评估其可迁移性、可扩展性与局限。

Method: 以DINOv3（自监督ViT）为统一编码器，在多种医学成像模态与任务（2D/3D分类、分割）上进行基准测试；系统改变模型规模与输入分辨率，分析缩放规律与跨任务表现。与BiomedCLIP、CT-Net等医学专用模型对比。

Result: DINOv3在多项医学任务上表现优异，建立强基线，部分任务上超越BiomedCLIP、CT-Net；但在需要深度领域知识的WSI、EM、PET上特征退化。观察到在医学领域不总遵循规模定律：更大模型/更细特征不必然带来更好表现，跨任务呈多样化缩放行为。

Conclusion: DINOv3可作为强健的通用视觉先验，覆盖多类复杂医学任务，但在高度专业场景存在短板；未来可利用其特征用于多视角一致性等方向（如3D重建），并需探索面向医学的适配与缩放策略。

Abstract: The advent of large-scale vision foundation models, pre-trained on diverse
natural images, has marked a paradigm shift in computer vision. However, how
the frontier vision foundation models' efficacies transfer to specialized
domains remains such as medical imaging remains an open question. This report
investigates whether DINOv3, a state-of-the-art self-supervised vision
transformer (ViT) that features strong capability in dense prediction tasks,
can directly serve as a powerful, unified encoder for medical vision tasks
without domain-specific pre-training. To answer this, we benchmark DINOv3
across common medical vision tasks, including 2D/3D classification and
segmentation on a wide range of medical imaging modalities. We systematically
analyze its scalability by varying model sizes and input image resolutions. Our
findings reveal that DINOv3 shows impressive performance and establishes a
formidable new baseline. Remarkably, it can even outperform medical-specific
foundation models like BiomedCLIP and CT-Net on several tasks, despite being
trained solely on natural images. However, we identify clear limitations: The
model's features degrade in scenarios requiring deep domain specialization,
such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),
and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3
does not consistently obey scaling law in the medical domain; performance does
not reliably increase with larger models or finer feature resolutions, showing
diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3
as a strong baseline, whose powerful visual features can serve as a robust
prior for multiple complex medical tasks. This opens promising future
directions, such as leveraging its features to enforce multiview consistency in
3D reconstruction.

</details>


### [117] [FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection](https://arxiv.org/abs/2509.06482)
*Zhongxiang Xie,Shuangxi Miao,Yuhan Jiang,Zhewei Zhang,Jing Yao,Xuecao Li,Jianxi Huang,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 提出FSG-Net，通过频域与空域协同和门控融合，降低伪变化并提升边界质量，在多CD基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 高分遥感变化检测常受两难题困扰：1) 时相辐射差异（光照、季节等）导致模型把伪变化当真变化；2) 深层语义与浅层细节的语义鸿沟阻碍有效融合，造成边界粗糙。亟需一种方法能区分真实语义变化与干扰、并高效融合多层特征。

Method: 提出频-空协同的FSG-Net：先在频域用DAWIM（小波交互）区分并自适应处理不同频段以抑制伪变化；再在空域用STSAM（时空协同注意）强化真实变化显著性；最后用轻量LGFU门控融合单元，以高层语义选择性引导浅层细节的整合，缩小语义鸿沟并优化边界。

Result: 在CDD、GZ-CD、LEVIR-CD上取得新的SOTA，F1分别为94.16%、89.51%、91.27%。

Conclusion: 频域抑伪与空域显著性增强相结合，并以语义引导的门控融合有效弥合语义-细节鸿沟，可显著提升高分遥感变化检测的准确度与边界质量；代码将于发表后开源。

Abstract: Change detection from high-resolution remote sensing images lies as a
cornerstone of Earth observation applications, yet its efficacy is often
compromised by two critical challenges. First, false alarms are prevalent as
models misinterpret radiometric variations from temporal shifts (e.g.,
illumination, season) as genuine changes. Second, a non-negligible semantic gap
between deep abstract features and shallow detail-rich features tends to
obstruct their effective fusion, culminating in poorly delineated boundaries.
To step further in addressing these issues, we propose the Frequency-Spatial
Synergistic Gated Network (FSG-Net), a novel paradigm that aims to
systematically disentangle semantic changes from nuisance variations.
Specifically, FSG-Net first operates in the frequency domain, where a
Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates
pseudo-changes by discerningly processing different frequency components.
Subsequently, the refined features are enhanced in the spatial domain by a
Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the
saliency of genuine change regions. To finally bridge the semantic gap, a
Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to
selectively gate and integrate crucial details from shallow layers.
Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate
the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores
of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at
https://github.com/zxXie-Air/FSG-Net after a possible publication.

</details>


### [118] [WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting](https://arxiv.org/abs/2509.06485)
*Andrea Marelli,Alberto Foresti,Leonardo Pesce,Giacomo Boracchi,Mario Grosso*

Main category: cs.CV

TL;DR: 提出“前后监督”概念：只用操作员取走物体前后的图像差异来训练分割模型，并发布多视角废品分拣数据集WS^2与基线管线，验证弱监督分割在工业分拣的可行性。


<details>
  <summary>Details</summary>
Motivation: 工业分拣（如垃圾/物料分拣）仍依赖人工在传送带上移除不需要的物体；全监督分割需要大量标注，成本高且难以覆盖多样任务。作者注意到操作员的“移除动作”天然提供了监督信号，但该方向少有系统研究。

Method: 1) 定义“前后监督”（Before-After Supervision）：利用同一场景中操作前/后的成对图像，基于其视觉差异作为弱标注来训练语义/实例分割网络。2) 构建WS^2数据集：包含多视角、超1.1万高分辨率帧的传送带视频，成对给出“before/after”。3) 提出鲁棒的端到端训练/评测管线，并用其对多种SOTA弱监督分割方法进行基准测试。

Result: 建立了首个面向废品分拣的多视角“前后监督”数据集与评测基线；在WS^2上对一系列弱监督分割方法进行了系统比较，显示仅凭前后差异即可有效学习目标分割。

Conclusion: 利用操作员自然产生的前后图像差异可显著降低标注成本并推动工业分拣自动化；WS^2与所提管线为该方向提供了标准数据与基线，鼓励社区进一步研究弱监督分割在实际生产中的应用。

Abstract: In industrial quality control, to visually recognize unwanted items within a
moving heterogeneous stream, human operators are often still indispensable.
Waste-sorting stands as a significant example, where operators on multiple
conveyor belts manually remove unwanted objects to select specific materials.
To automate this recognition problem, computer vision systems offer great
potential in accurately identifying and segmenting unwanted items in such
settings. Unfortunately, considering the multitude and the variety of sorting
tasks, fully supervised approaches are not a viable option to address this
challange, as they require extensive labeling efforts. Surprisingly, weakly
supervised alternatives that leverage the implicit supervision naturally
provided by the operator in his removal action are relatively unexplored. In
this paper, we define the concept of Before-After Supervision, illustrating how
to train a segmentation network by leveraging only the visual differences
between images acquired \textit{before} and \textit{after} the operator. To
promote research in this direction, we introduce WS$^2$ (Weakly Supervised
segmentation for Waste-Sorting), the first multiview dataset consisting of more
than 11 000 high-resolution video frames captured on top of a conveyor belt,
including "before" and "after" images. We also present a robust end-to-end
pipeline, used to benchmark several state-of-the-art weakly supervised
segmentation methods on WS$^2$.

</details>


### [119] [TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement](https://arxiv.org/abs/2509.06499)
*Jibai Lin,Bo Ma,Yating Yang,Rong Ma,Turghun Osman,Ahtamjan Ahmat,Rui Dong,Lei Wang,Xi Zhou*

Main category: cs.CV

TL;DR: 提出TIDE框架，通过目标监督与偏好学习，在无需测试时微调的情况下实现主体保持与指令服从的平衡，基于三元组对齐与DSD目标，利用自动生成的胜/负目标隐式建奖励，显著优于基线并适用于多种生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有主体驱动图像生成在“保留主体身份”与“遵循动态编辑指令”间难以兼顾，常需测试时微调、易失真或不服从指令，缺乏系统化的监督与奖励建模来平衡两者。

Method: 提出Target-Instructed Diffusion Enhancing (TIDE)：1) 目标监督三元组对齐，以(参考图、文本指令、目标图)三元组建模主体适配动态；2) Direct Subject Diffusion (DSD)训练目标，成对采样“胜出”(保留-服从平衡好)与“失败”(失真/不服从)目标；3) 基于量化指标自动生成并评估胜/负样本，实现隐式奖励建模与偏好学习；4) 训练后无需test-time微调即可推理。

Result: 在标准基准上，TIDE在主体一致性与指令遵从的多项量化指标上均超过基线；并在结构条件生成、图到图生成、文图插值等多任务上展现强泛化与鲁棒性。

Conclusion: 通过目标监督三元组与DSD目标的偏好学习，TIDE有效缓解主体保持与指令服从的张力，在无需测试时微调的前提下取得更佳的保留-服从平衡与更强的任务适用性。

Abstract: Subject-driven image generation (SDIG) aims to manipulate specific subjects
within images while adhering to textual instructions, a task crucial for
advancing text-to-image diffusion models. SDIG requires reconciling the tension
between maintaining subject identity and complying with dynamic edit
instructions, a challenge inadequately addressed by existing methods. In this
paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,
which resolves this tension through target supervision and preference learning
without test-time fine-tuning. TIDE pioneers target-supervised triplet
alignment, modelling subject adaptation dynamics using a (reference image,
instruction, target images) triplet. This approach leverages the Direct Subject
Diffusion (DSD) objective, training the model with paired "winning" (balanced
preservation-compliance) and "losing" (distorted) targets, systematically
generated and evaluated via quantitative metrics. This enables implicit reward
modelling for optimal preservation-compliance balance. Experimental results on
standard benchmarks demonstrate TIDE's superior performance in generating
subject-faithful outputs while maintaining instruction compliance,
outperforming baseline methods across multiple quantitative metrics. TIDE's
versatility is further evidenced by its successful application to diverse
tasks, including structural-conditioned generation, image-to-image generation,
and text-image interpolation. Our code is available at
https://github.com/KomJay520/TIDE.

</details>


### [120] [Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach](https://arxiv.org/abs/2509.06511)
*Daniil Tikhonov,Matheus Scatolin,Mohor Banerjee,Qiankun Ji,Ahmed Jaheen,Mostafa Salem,Abdelrahman Elsayed,Hu Wang,Sarim Hashmi,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 提出一种结合深度学习特征与放射组学/临床特征的混合框架，用于从纵向MRI自动判别胶质母细胞瘤治疗反应，在BraTS 2025任务上达到了AUC 0.81、Macro F1 0.50。


<details>
  <summary>Details</summary>
Motivation: RANO标准虽统一但应用复杂、观察者间一致性不足，临床需要客观可重复的自动化评估来辅助治疗决策和患者管理。

Method: 从四模态MRI的2D ROI用微调ResNet-18提取深度特征，并与>4800个放射组学与临床驱动特征融合，包括基于肿瘤生长/缩小掩膜的3D放射组学、相对最低点（nadir）的体积变化、肿瘤质心位移等；用CatBoost进行四分类（CR/PR/SD/PD）。

Result: 在四分类治疗反应预测上，平均ROC AUC=0.81，Macro F1=0.50，显示融合特征的有效性。

Conclusion: 将学习到的图像表征与领域针对性的放射组学相结合，可为神经肿瘤学中的自动化疗效评估提供稳健可行的方案。

Abstract: Accurate evaluation of the response of glioblastoma to therapy is crucial for
clinical decision-making and patient management. The Response Assessment in
Neuro-Oncology (RANO) criteria provide a standardized framework to assess
patients' clinical response, but their application can be complex and subject
to observer variability. This paper presents an automated method for
classifying the intervention response from longitudinal MRI scans, developed to
predict tumor response during therapy as part of the BraTS 2025 challenge. We
propose a novel hybrid framework that combines deep learning derived feature
extraction and an extensive set of radiomics and clinically chosen features.
Our approach utilizes a fine-tuned ResNet-18 model to extract features from 2D
regions of interest across four MRI modalities. These deep features are then
fused with a rich set of more than 4800 radiomic and clinically driven
features, including 3D radiomics of tumor growth and shrinkage masks,
volumetric changes relative to the nadir, and tumor centroid shift. Using the
fused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a
Macro F1 score of 0.50 in the 4-class response prediction task (Complete
Response, Partial Response, Stable Disease, Progressive Disease). Our results
highlight that synergizing learned image representations with domain-targeted
radiomic features provides a robust and effective solution for automated
treatment response assessment in neuro-oncology.

</details>


### [121] [On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''](https://arxiv.org/abs/2509.06535)
*Hua Chang Bakker,Stan Fris,Angela Madelon Bernardy,Stan Deutekom*

Main category: cs.CV

TL;DR: 论文复现与扩展FairCLIP，发现其在零样本青光眼分类上未带来性能或公平性提升。作者实现了对齐版本A-FairCLIP与多属性扩展FairCLIP+，虽能降低敏感组间的Sinkhorn距离，但未改善实际公平性与准确率。


<details>
  <summary>Details</summary>
Motivation: 检验Luo等(2024)提出的FairCLIP在提升CLIP群体公平性与性能方面的有效性与可复现性；澄清论文描述与原始实现不一致的问题；探究将目标扩展到多属性及距离最小化对模型表现与公平性的影响。

Method: 1) 复现实验环境与设置；2) 指出原论文模型描述与实现不一致，重写并发布对齐实现A-FairCLIP以控制设计选择；3) 提出FairCLIP+将公平性正则扩展至多敏感属性；4) 在Harvard-FairVLMed等数据集上进行零样本青光眼分类，测量Sinkhorn距离、性能指标与公平性指标；5) 对比官方FairCLIP、A-FairCLIP与基础CLIP。

Result: - 证实CLIP在医疗多模态零样本任务存在对特定人群的偏置；- 引入的正则项确实减少了组间Sinkhorn距离；- 然而，无论官方FairCLIP还是A-FairCLIP，都未在两套数据上带来统计显著的性能或公平性提升；- FairCLIP+虽支持多属性，但未转化为下游改进。

Conclusion: 降低分布距离（Sinkhorn）并不必然带来下游公平性与性能提升；FairCLIP在该医疗任务的主张缺乏复现实证支持。需要重新审视公平正则目标与任务对齐、度量选择及实现细节，并在多数据集上进行更系统的评估。

Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al.
(2024), for improving the group fairness of CLIP (Radford et al., 2021) by
minimizing image-text similarity score disparities across sensitive groups
using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was
reproduced to primarily investigate the research findings for FairCLIP. The
model description by Luo et al. (2024) was found to differ from the original
implementation. Therefore, a new implementation, A-FairCLIP, is introduced to
examine specific design choices. Furthermore, FairCLIP+ is proposed to extend
the FairCLIP objective to include multiple attributes. Additionally, the impact
of the distance minimization on FairCLIP's fairness and performance was
explored. In alignment with the original authors, CLIP was found to be biased
towards certain demographics when applied to zero-shot glaucoma classification
using medical scans and clinical notes from the Harvard-FairVLMed dataset.
However, the experimental results on two datasets do not support their claim
that FairCLIP improves the performance and fairness of CLIP. Although the
regularization objective reduces Sinkhorn distances, both the official
implementation and the aligned implementation, A-FairCLIP, were not found to
improve performance nor fairness in zero-shot glaucoma classification.

</details>


### [122] [Benchmarking EfficientTAM on FMO datasets](https://arxiv.org/abs/2509.06536)
*Senem Aktas,Charles Markham,John McDonald,Rozenn Dahyot*

Main category: cs.CV

TL;DR: 提出FMOX：为四个快速运动目标(FMO)开源数据集新增JSON元数据与目标尺寸真值，并用其评测EfficientTAM跟踪模型，结果以TIoU对比显示接近或优于专门管线；代码与JSON开源。


<details>
  <summary>Details</summary>
Motivation: 快速且小型的目标跟踪在FMO场景中仍具挑战，现有数据集描述与真值不统一且信息不足（如目标尺寸），限制了通用模型评测与复现。因此需要标准化、可扩展的标注与基准来公平比较基础模型与专用方法。

Method: 1) 为四个FMO开源序列构建统一的JSON元数据；2) 扩展为FMOX，加入目标尺寸等额外真值；3) 选用基础跟踪模型EfficientTAM在FMOX上评测；4) 与原FMO专用流水线进行对比，采用轨迹IoU（TIoU）作为指标；5) 开源代码与JSON以便复用。

Result: EfficientTAM在FMOX上的性能与为FMO专门设计的管线相当（以TIoU衡量），表明基础模型在高速小目标场景中具竞争力。

Conclusion: FMOX提供统一、丰富的FMO基准与标注，支持客观评测与比较；基础模型EfficientTAM在FMO场景表现良好。开源资源可促进后续算法与管线在FMO上的研究与应用。

Abstract: Fast and tiny object tracking remains a challenge in computer vision and in
this paper we first introduce a JSON metadata file associated with four open
source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we
extend the description of the FMOs datasets with additional ground truth
information in JSON format (called FMOX) with object size information. Finally
we use our FMOX file to test a recently proposed foundational model for
tracking (called EfficientTAM) showing that its performance compares well with
the pipelines originally taylored for these FMO datasets. Our comparison of
these state-of-the-art techniques on FMOX is provided with Trajectory
Intersection of Union (TIoU) scores. The code and JSON is shared open source
allowing FMOX to be accessible and usable for other machine learning pipelines
aiming to process FMO datasets.

</details>


### [123] [Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval](https://arxiv.org/abs/2509.06566)
*Emil Demić,Luka Čehovin Zajc*

Main category: cs.CV

TL;DR: 提出一种针对场景级手绘草图图像检索（Scene-level SBIR）的训练目标与设计，强调对草图固有歧义与噪声的鲁棒性，在无需增加模型复杂度的前提下达到SOTA，并在FS-COCO与SketchyCOCO上验证有效。


<details>
  <summary>Details</summary>
Motivation: 真实世界草图存在显著歧义与噪声，现有方法多依赖模型结构增广而忽视训练目标对鲁棒性的作用；需要一种能适应草图多样性、且不增加复杂度的训练策略，并改善评测方案。

Method: 结合合适的预训练、编码器架构与专门设计的鲁棒损失函数（训练目标），以提升跨模态对齐中对草图变异性的容忍度；不引入额外复杂组件，侧重训练设计。

Result: 在挑战性的FS-COCO和广泛使用的SketchyCOCO数据集上取得了SOTA性能，实验广泛验证了方法有效性。

Conclusion: 训练目标与整体训练设计在场景级SBIR中至关重要；无需增加模型复杂度即可获得显著提升，同时当前评测场景需要改进以更全面反映真实应用需求。

Abstract: The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural
images matching the overall semantics and spatial layout of a free-hand sketch.
Unlike prior work focused on architectural augmentations of retrieval models,
we emphasize the inherent ambiguity and noise present in real-world sketches.
This insight motivates a training objective that is explicitly designed to be
robust to sketch variability. We show that with an appropriate combination of
pre-training, encoder architecture, and loss formulation, it is possible to
achieve state-of-the-art performance without the introduction of additional
complexity. Extensive experiments on a challenging FS-COCO and widely-used
SketchyCOCO datasets confirm the effectiveness of our approach and underline
the critical role of training design in cross-modal retrieval tasks, as well as
the need to improve the evaluation scenarios of scene-level SBIR.

</details>


### [124] [Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition](https://arxiv.org/abs/2509.06570)
*Runqing Yang,Yimin Fu,Changyuan Wu,Zhunga Liu*

Main category: cs.CV

TL;DR: 提出RARL用于增量开放集识别（IOSR），通过等角紧框架构建的角度表示、VII训练与分层校正，稳定决策边界、缓解漂移与偏置，在CIFAR100与TinyImageNet上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有OSR多为静态设置，难以应对持续数据流中新类逐步出现的情形；增量场景又受限于无法访问旧数据，导致表示漂移与类别混淆、决策边界判别性下降。需要一种既能发现未知又能稳定更新知识的IOSR方法。

Method: 提出保留型角度表示学习（RARL）：1）在等角紧框架下构建角度空间，用“非激活原型”吸引未知类表征，使其围绕空闲原型对齐，减小更新时的表示漂移；2）虚-内在交互（VII）训练，构造边界邻近的虚拟类别以拉大已知类间隔并压紧类内分布；3）分层校正策略，从旧/新与正/负样本不均衡两层面细化决策边界，缓解表示偏置与特征空间畸变。

Result: 在CIFAR100与TinyImageNet上，跨多种增量任务设置进行全面评测，建立新的IOSR基线；各项实验显示RARL优于现有方法，达到SOTA性能。

Conclusion: RARL通过角度表示、VII训练与分层校正，在增量开放场景中有效稳定决策边界、降低表征漂移与类别混淆，显著提升IOSR性能并树立新基准。

Abstract: Existing open set recognition (OSR) methods are typically designed for static
scenarios, where models aim to classify known classes and identify unknown ones
within fixed scopes. This deviates from the expectation that the model should
incrementally identify newly emerging unknown classes from continuous data
streams and acquire corresponding knowledge. In such evolving scenarios, the
discriminability of OSR decision boundaries is hard to maintain due to
restricted access to former training data, causing severe inter-class
confusion. To solve this problem, we propose retentive angular representation
learning (RARL) for incremental open set recognition (IOSR). In RARL, unknown
representations are encouraged to align around inactive prototypes within an
angular space constructed under the equiangular tight frame, thereby mitigating
excessive representation drift during knowledge updates. Specifically, we adopt
a virtual-intrinsic interactive (VII) training strategy, which compacts known
representations by enforcing clear inter-class margins through
boundary-proximal virtual classes. Furthermore, a stratified rectification
strategy is designed to refine decision boundaries, mitigating representation
bias and feature space distortion caused by imbalances between old/new and
positive/negative class samples. We conduct thorough evaluations on CIFAR100
and TinyImageNet datasets and establish a new benchmark for IOSR. Experimental
results across various task setups demonstrate that the proposed method
achieves state-of-the-art performance.

</details>


### [125] [Approximating Condorcet Ordering for Vector-valued Mathematical Morphology](https://arxiv.org/abs/2509.06577)
*Marcos Eduardo Valle,Santiago Velasco-Forero,Joao Batista Florindo,Gustavo Jesus Angulo*

Main category: cs.CV

TL;DR: 提出一种以Condorcet秩为目标的“降维排序”学习方法，用于构建向量值形态学算子；初步实验在彩色图像上有效。


<details>
  <summary>Details</summary>
Motivation: 向量值图像（如彩色/高光谱）需要稳定一致的排序来定义形态学算子，但现有向量排序方案众多且无公认最优，导致算子构建不统一、性能不稳定。Condorcet式聚合可综合多种排序，但其直接计算或使用可能复杂。

Method: 将多个候选向量排序看作“投票者”，以Condorcet投票产生的总体秩为“金标准”，提出一种机器学习方法学习一个“精简的（reduced）排序映射”，使其近似Condorcet秩；据此定义向量值形态学算子（如膨胀/腐蚀）。

Result: 初步计算实验显示，学习到的精简排序能有效逼近Condorcet秩，并据此构建的形态学算子在彩色图像处理任务中表现良好。

Conclusion: 以学习的精简排序近似Condorcet秩的思路可为向量值形态学提供一致且有效的排序基础；早期结果积极，值得进一步拓展与验证。

Abstract: Mathematical morphology provides a nonlinear framework for image and spatial
data processing and analysis. Although there have been many successful
applications of mathematical morphology to vector-valued images, such as color
and hyperspectral images, there is still no consensus on the most suitable
vector ordering for constructing morphological operators. This paper addresses
this issue by examining a reduced ordering approximating the Condorcet ranking
derived from a set of vector orderings. Inspired by voting problems, the
Condorcet ordering ranks elements from most to least voted, with voters
representing different orderings. In this paper, we develop a machine learning
approach that learns a reduced ordering that approximates the Condorcet
ordering. Preliminary computational experiments confirm the effectiveness of
learning the reduced mapping to define vector-valued morphological operators
for color images.

</details>


### [126] [CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis](https://arxiv.org/abs/2509.06579)
*Xin Kong,Daniel Watson,Yannick Strümpler,Michael Niemeyer,Federico Tombari*

Main category: cs.CV

TL;DR: 提出CausNVS：一种自回归多视角扩散模型，支持任意输入/输出视角配置，顺序生成视图，通过因果遮罩、逐帧加噪和相对相机编码实现精确控制，并用滑动窗口+缓存+噪声条件增强缓解漂移，兼顾灵活性与高画质。


<details>
  <summary>Details</summary>
Motivation: 现有多视角扩散多为非自回归：需固定视角数、一次性去噪所有帧，导致推理慢、无法灵活扩展到开放式世界建模与任意轨迹生成。需要能按任意相机轨迹逐步生成、可长序列扩展、且控制精确的方案。

Method: 1) 模型：自回归多视角扩散框架CausNVS。2) 训练：使用因果遮罩确保仅依赖过去帧，逐帧噪声以匹配自回归去噪分布；引入成对相对相机位姿编码CaPE以提升相机控制精度。3) 推理：空间感知滑动窗口结合KV缓存提高长序列效率与一致性；噪声条件增强（NCA）用于抑制漂移、稳态化生成。

Result: 在多种相机轨迹和输入/输出配置下，模型可灵活进行自回归新视角合成，推理效率更高，视觉质量稳定且强，在多样设置中表现一致。

Conclusion: CausNVS突破非自回归多视角扩散的固定视角与并行去噪瓶颈，实现任意配置的顺序生成与精确相机控制，并通过滑动窗口、缓存与噪声增强缓解长序列漂移，达到稳健高质的NVS。

Abstract: Multi-view diffusion models have shown promise in 3D novel view synthesis,
but most existing methods adopt a non-autoregressive formulation. This limits
their applicability in world modeling, as they only support a fixed number of
views and suffer from slow inference due to denoising all frames
simultaneously. To address these limitations, we propose CausNVS, a multi-view
diffusion model in an autoregressive setting, which supports arbitrary
input-output view configurations and generates views sequentially. We train
CausNVS with causal masking and per-frame noise, using pairwise-relative camera
pose encodings (CaPE) for precise camera control. At inference time, we combine
a spatially-aware sliding-window with key-value caching and noise conditioning
augmentation to mitigate drift. Our experiments demonstrate that CausNVS
supports a broad range of camera trajectories, enables flexible autoregressive
novel view synthesis, and achieves consistently strong visual quality across
diverse settings. Project page: https://kxhit.github.io/CausNVS.html.

</details>


### [127] [Detection of trade in products derived from threatened species using machine learning and a smartphone](https://arxiv.org/abs/2509.06585)
*Ritwik Kulkarni,WU Hanqin,Enrico Di Minin*

Main category: cs.CV

TL;DR: 利用机器学习目标识别模型从图片中自动检测和定位野生动物制品（象牙、穿山甲鳞片/爪、虎皮/骨），并在网页与实体市场执法中应用；最佳模型总体准确率84.2%，手机应用端可达91.3%。


<details>
  <summary>Details</summary>
Motivation: 数字平台上野生动物非法贸易激增，人工审核难以覆盖海量内容，急需自动化图像识别手段帮助执法和监测，尤其针对象牙等重点制品。

Method: 收集象、穿山甲、老虎相关制品（原料与加工品）的违法销售或查获图片；比较多种训练策略与两种损失函数；分别训练按物种的模型与跨物种统一模型；评估检测准确率，并将最佳模型部署为智能手机应用实现实时拍照识别。

Result: 最佳模型总体准确率84.2%，分别为：大象71.1%、穿山甲90.2%、老虎93.5%；移动端应用总体准确率91.3%，可实时定位并标注疑似制品。

Conclusion: 机器学习目标识别能有效辅助线上与线下的野生动物贸易监测与执法；移动端部署提升可用性与现场实战价值，适合政府与执法机构快速筛查潜在禁售制品。

Abstract: Unsustainable trade in wildlife is a major threat to biodiversity and is now
increasingly prevalent in digital marketplaces and social media. With the sheer
volume of digital content, the need for automated methods to detect wildlife
trade listings is growing. These methods are especially needed for the
automatic identification of wildlife products, such as ivory. We developed
machine learning-based object recognition models that can identify wildlife
products within images and highlight them. The data consists of images of
elephant, pangolin, and tiger products that were identified as being sold
illegally or that were confiscated by authorities. Specifically, the wildlife
products included elephant ivory and skins, pangolin scales, and claws (raw and
crafted), and tiger skins and bones. We investigated various combinations of
training strategies and two loss functions to identify the best model to use in
the automatic detection of these wildlife products. Models were trained for
each species while also developing a single model to identify products from all
three species. The best model showed an overall accuracy of 84.2% with
accuracies of 71.1%, 90.2% and 93.5% in detecting products derived from
elephants, pangolins, and tigers, respectively. We further demonstrate that the
machine learning model can be made easily available to stakeholders, such as
government authorities and law enforcement agencies, by developing a
smartphone-based application that had an overall accuracy of 91.3%. The
application can be used in real time to click images and help identify
potentially prohibited products of target species. Thus, the proposed method is
not only applicable for monitoring trade on the web but can also be used e.g.
in physical markets for monitoring wildlife trade.

</details>


### [128] [Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising](https://arxiv.org/abs/2509.06591)
*Yichao Liu,YueYang Teng*

Main category: cs.CV

TL;DR: 提出HSANet混合Swin注意力网络用于LDCT/PET去噪，结合高效全局注意力和混合上采样，实验证明在公开数据集上优于现有方法且模型轻量，便于临床部署。


<details>
  <summary>Details</summary>
Motivation: LDCT/PET为降低辐射剂量而牺牲图像质量，噪声与伪影增多影响诊断，需要有效的去噪方法在保障辐射安全的同时提升图像质量与临床可用性。

Method: 设计Hybrid Swin Attention Network（HSANet）：在Swin架构中引入Efficient Global Attention（EGA）模块以增强空间与通道交互、捕获关键特征；并加入混合上采样模块以降低对噪声的过拟合风险。使用公开LDCT/PET数据集进行训练与验证。

Result: 在公开LDCT/PET数据集上，HSANet取得优于现有方法的去噪性能，同时保持较小模型规模，适配常规GPU内存配置。

Conclusion: HSANet在保证轻量化的同时显著提升LDCT/PET图像质量，具备实际临床部署的可行性与优势。

Abstract: Low-dose computed tomography (LDCT) and positron emission tomography (PET)
have emerged as safer alternatives to conventional imaging modalities by
significantly reducing radiation exposure. However, this reduction often
results in increased noise and artifacts, which can compromise diagnostic
accuracy. Consequently, denoising for LDCT/PET has become a vital area of
research aimed at enhancing image quality while maintaining radiation safety.
In this study, we introduce a novel Hybrid Swin Attention Network (HSANet),
which incorporates Efficient Global Attention (EGA) modules and a hybrid
upsampling module. The EGA modules enhance both spatial and channel-wise
interaction, improving the network's capacity to capture relevant features,
while the hybrid upsampling module mitigates the risk of overfitting to noise.
We validate the proposed approach using a publicly available LDCT/PET dataset.
Experimental results demonstrate that HSANet achieves superior denoising
performance compared to existing methods, while maintaining a lightweight model
size suitable for deployment on GPUs with standard memory configurations. This
makes our approach highly practical for real-world clinical applications.

</details>


### [129] [Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework](https://arxiv.org/abs/2509.06625)
*Aswini Kumar Patra*

Main category: cs.CV

TL;DR: 提出一个融合RGB、多光谱和两段红外的时空深度学习框架（CNN+LSTM），在干旱与杂草压力共存条件下，以时间序列冠层图像高精度（98%）判别作物氮素胁迫程度，显著优于仅空间CNN（80.45%）与既有ML方法（76%）。


<details>
  <summary>Details</summary>
Motivation: 现实田间中氮、干旱、杂草等胁迫相互作用，致使氮胁迫信号被掩盖且难以及早识别；早期精准诊断氮胁迫对及时管理和保产至关重要。现有方法在共胁迫场景与时间动态信息利用不足，需开发能捕捉细微生理变化的多模态、时空模型。

Method: 采集多模态冠层影像（RGB、MS、两段IR）及时间序列数据，覆盖低/中/高三档氮水平，并伴随不同水分胁迫与杂草压力。构建时空深度学习管线：CNN提取空间表型特征，LSTM建模时间依赖以分类氮胁迫严重度；并设仅空间CNN作对照。比较与先前机器学习基线。

Result: CNN-LSTM在氮胁迫分级上达98%准确率，显著高于空间CNN（80.45%）与既有ML方法（76%），显示多模态+时序建模能有效捕捉氮、干旱、杂草交互下的细微差异。

Conclusion: 多模态时空深度学习框架能在复杂共胁迫环境中实现对氮胁迫程度的高精度、早期识别，具备转化为作物管理决策支持工具的潜力，从而促进及时施肥与健康管理。

Abstract: Plants in their natural habitats endure an array of interacting stresses,
both biotic and abiotic, that rarely occur in isolation. Nutrient
stress-particularly nitrogen deficiency-becomes even more critical when
compounded with drought and weed competition, making it increasingly difficult
to distinguish and address its effects. Early detection of nitrogen stress is
therefore crucial for protecting plant health and implementing effective
management strategies. This study proposes a novel deep learning framework to
accurately classify nitrogen stress severity in a combined stress environment.
Our model uses a unique blend of four imaging modalities-RGB, multispectral,
and two infrared wavelengths-to capture a wide range of physiological plant
responses from canopy images. These images, provided as time-series data,
document plant health across three levels of nitrogen availability (low,
medium, and high) under varying water stress and weed pressures. The core of
our approach is a spatio-temporal deep learning pipeline that merges a
Convolutional Neural Network (CNN) for extracting spatial features from images
with a Long Short-Term Memory (LSTM) network to capture temporal dependencies.
We also devised and evaluated a spatial-only CNN pipeline for comparison. Our
CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively
surpassing the spatial-only model's 80.45% and other previously reported
machine learning method's 76%. These results bring actionable insights based on
the power of our CNN-LSTM approach in effectively capturing the subtle and
complex interactions between nitrogen deficiency, water stress, and weed
pressure. This robust platform offers a promising tool for the timely and
proactive identification of nitrogen stress severity, enabling better crop
management and improved plant health.

</details>


### [130] [Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery](https://arxiv.org/abs/2509.06660)
*Cailei Liang,Adrian Bodenmann,Emma J Curtis,Samuel Simmons,Kazunori Nagano,Stan Brown,Adam Riese,Blair Thornton*

Main category: cs.CV

TL;DR: 研究评估在海底图像自监督学习中引入“位置信息正则化”的效果，发现其在多种SSL框架、模型和数据集上普遍提升分类F1分数，尤其在低维潜表示和CNN中收益明显；高维ViT具备很强泛化能力，接近或匹配位置正则化SSL的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 海底监测与探索产生大量机器人采集的图像，人工标注成本高。自监督学习可减标注依赖，但如何充分利用伴随的位置信息（地理/坐标元数据）尚缺系统研究，特别是在不同SSL策略、模型结构（CNN/ViT）、潜空间维度与多数据集上的一致性与收益未明。

Method: 在三套多样性海底图像数据上，系统比较六种SOTA自监督框架（涵盖CNN与ViT、不同潜空间维度128与512等），将“位置正则化”作为附加约束融入SSL，对比标准SSL与预训练（通用数据预训练）基线，评估下游分类性能（F1）。

Result: - 位置正则化较标准SSL普遍提升F1：CNN平均+4.9±4.0%，ViT平均+6.3±8.9%。
- CNN：通用预训练在高维潜空间更有利；但在“数据集优化SSL”条件下，高维(512)与低维(128)性能相近。位置正则化SSL相对预训练提升：高维+2.7±2.7%，低维+10.1±9.4%。
- ViT：高维潜空间有益于预训练和数据集优化SSL；预训练ViT的泛化很强，F1=0.795±0.075，接近/匹配最佳位置正则化SSL的0.795±0.077。

Conclusion: 位置信息作为正则项能稳健提升海底图像SSL下游表现，尤其在低维潜表示与CNN中收益突出；而高维ViT展现强泛化，往往能匹配位置正则化SSL的最佳水平。实践上：若受限于低维表示或使用CNN，建议整合位置正则化；若可用高维ViT预训练，已具备强基线性能。

Abstract: High-throughput interpretation of robotically gathered seafloor visual
imagery can increase the efficiency of marine monitoring and exploration.
Although recent research has suggested that location metadata can enhance
self-supervised feature learning (SSL), its benefits across different SSL
strategies, models and seafloor image datasets are underexplored. This study
evaluates the impact of location-based regularisation on six state-of-the-art
SSL frameworks, which include Convolutional Neural Network (CNN) and Vision
Transformer (ViT) models with varying latent-space dimensionality. Evaluation
across three diverse seafloor image datasets finds that location-regularisation
consistently improves downstream classification performance over standard SSL,
with average F1-score gains of $4.9 \pm 4.0%$ for CNNs and $6.3 \pm 8.9%$ for
ViTs, respectively. While CNNs pretrained on generic datasets benefit from
high-dimensional latent representations, dataset-optimised SSL achieves similar
performance across the high (512) and low (128) dimensional latent
representations. Location-regularised SSL improves CNN performance over
pre-trained models by $2.7 \pm 2.7%$ and $10.1 \pm 9.4%$ for high and
low-dimensional latent representations, respectively. For ViTs,
high-dimensionality benefits both pre-trained and dataset-optimised SSL.
Although location-regularisation improves SSL performance compared to standard
SSL methods, pre-trained ViTs show strong generalisation, matching the
best-performing location-regularised SSL with F1-scores of $0.795 \pm 0.075$
and $0.795 \pm 0.077$, respectively. The findings highlight the value of
location metadata for SSL regularisation, particularly when using
low-dimensional latent representations, and demonstrate strong generalisation
of high-dimensional ViTs for seafloor image analysis.

</details>


### [131] [Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations](https://arxiv.org/abs/2509.06678)
*Cailei Liang,Adrian Bodenmann,Sam Fenton,Blair Thornton*

Main category: cs.CV

TL;DR: 提出一种用于海底图像的在线无监督聚类框架（OCF），在连续数据流上实时运行，维持代表样本实现可扩展、自适应、可合并/可分裂的聚类；在三套数据集上取得最高平均F1=0.68（std 3%），计算时间随数据增长保持低且有界，适用于长期自主海洋探测中的摘要与路径规划。


<details>
  <summary>Details</summary>
Motivation: 长航时、驻海AUV需要对海底图像进行实时、长期的自动解读，以便自适应任务与通信优化。现有离线方法依赖完整数据与人工标注，且对环境/操作条件敏感，不适用于实时流式场景，因此需要一种无需监督、能在线处理且随环境变化自适应的聚类方法。

Method: 提出在线聚类框架OCF：在数据流中维护一组能代表当前特征分布演化的代表样本库，通过常数时间的历史回顾与模式整合实现可扩展处理；支持动态聚类的合并与分裂而无需重处理全部历史；比较不同代表采样策略对精度与计算成本的影响。

Result: 在三套多样化海底图像数据集上评估，OCF在所有对比的在线聚类方法中取得最高平均F1=0.68，三条不同测线轨迹上的标准差为3%，表现出更强的聚类能力与对轨迹变化的鲁棒性；同时计算时间随数据量增长保持较低且有界。

Conclusion: OCF能在实时、流式条件下对海底图像进行无监督、自适应聚类，兼顾精度、鲁棒性与计算可扩展性；有助于生成巡航数据摘要并支持长期、持续的自主海洋探测中的信息化路径规划。

Abstract: As long-endurance and seafloor-resident AUVs become more capable, there is an
increasing need for extended, real-time interpretation of seafloor imagery to
enable adaptive missions and optimise communication efficiency. Although
offline image analysis methods are well established, they rely on access to
complete datasets and human-labelled examples to manage the strong influence of
environmental and operational conditions on seafloor image
appearance-requirements that cannot be met in real-time settings. To address
this, we introduce an online clustering framework (OCF) capable of interpreting
seafloor imagery without supervision, which is designed to operate in real-time
on continuous data streams in a scalable, adaptive, and self-consistent manner.
The method enables the efficient review and consolidation of common patterns
across the entire data history in constant time by identifying and maintaining
a set of representative samples that capture the evolving feature distribution,
supporting dynamic cluster merging and splitting without reprocessing the full
image history. We evaluate the framework on three diverse seafloor image
datasets, analysing the impact of different representative sampling strategies
on both clustering accuracy and computational cost. The OCF achieves the
highest average F1 score of 0.68 across the three datasets among all
comparative online clustering approaches, with a standard deviation of 3%
across three distinct survey trajectories, demonstrating its superior
clustering capability and robustness to trajectory variation. In addition, it
maintains consistently lower and bounded computational time as the data volume
increases. These properties are beneficial for generating survey data summaries
and supporting informative path planning in long-term, persistent autonomous
marine exploration.

</details>


### [132] [VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes](https://arxiv.org/abs/2509.06685)
*Shengkai Zhang,Yuhe Liu,Guanjun Wu,Jianhua He,Xinggang Wang,Mozi Chen,Kezhong Liu*

Main category: cs.CV

TL;DR: VIM-GS提出以单目图像驱动的大场景高质量高斯溅射新视角合成，通过将SfM稀疏而准确的深度与大型深度基础模型的稠密但粗糙深度融合，解决单目深度不稳定和远距离不准的问题，显著提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统GS依赖RGB-D/双目获得准确初始深度，但其测距有限，难以覆盖大场景；仅用单目会缺少可靠深度导致NVS质量差。虽然有单目深度LFMs，但存在跨帧不一致、远距不准确、纹理欺骗带来的歧义等缺陷。因此需要一种既能在大场景工作又能提供稠密且准确深度的方法，支撑高保真GS渲染。

Method: 以视觉-惯性SfM提供的稀疏、可靠深度为锚，结合LFMs输出的稠密但粗糙深度。为连接稀疏到稠密：1) 提出基于对象分割的深度传播算法，对具有结构性的物体像素进行深度渲染与填充；2) 设计动态深度细化模块，处理动态物体在SfM中深度残缺的问题，并对LFMs的粗糙深度进行自适应校正。最终生成高质量稠密深度以初始化和优化GS。

Result: 在公开与自建数据集上，VIM-GS在大场景NVS任务中取得更优的渲染质量；相较仅用LFMs或传统GS流程，呈现更稳定的跨帧一致性、更准确的远距离几何与更少的纹理伪影。

Conclusion: 将SfM稀疏且精准的几何与LFMs稠密深度融合，并通过对象分割传播与动态细化两模块，实现了单目输入下适用于大场景的高保真GS渲染，验证了在复杂与动态场景中的有效性与优越性。

Abstract: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for
novel-view synthesis (NVS) in large scenes. GS typically requires accurate
depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited
depth sensing range makes it difficult for GS to work in large scenes.
Monocular images, however, lack depth to guide the learning and lead to
inferior NVS results. Although large foundation models (LFMs) for monocular
depth estimation are available, they suffer from cross-frame inconsistency,
inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This
paper aims to generate dense, accurate depth images from monocular RGB inputs
for high-definite GS rendering. The key idea is to leverage the accurate but
sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the
dense but coarse depth from LFMs. To bridge the sparse input and dense output,
we propose an object-segmented depth propagation algorithm that renders the
depth of pixels of structured objects. Then we develop a dynamic depth
refinement module to handle the crippled SfM depth of dynamic objects and
refine the coarse LFM depth. Experiments using public and customized datasets
demonstrate the superior rendering quality of VIM-GS in large scenes.

</details>


### [133] [BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring](https://arxiv.org/abs/2509.06690)
*Usman Haider,Lukasz Szemet,Daniel Kelly,Vasileios Sergis,Andrew C. Daly,Karl Mason*

Main category: cs.CV

TL;DR: 提出BioLite U-Net轻量级语义分割用于生物打印过程实时监控，在树莓派4B上以335 ms/帧实现接近实时，并在自建787张三类标注数据集上达到mIoU 92.85%、Dice 96.17%，模型较MobileNetV2-DeepLabV3+小1300倍，兼顾精度与部署效率。


<details>
  <summary>Details</summary>
Motivation: 生物打印需要实时监控喷头与生物墨水的挤出状态以保证打印质量与细胞活性，但受限于成像数据有限与嵌入式硬件资源紧张，现有分割方法难以在设备端高效稳定运行，亟需高精度且低算力占用的在线分割方案。

Method: 构建包含787张RGB图像、三类标注（喷嘴、墨水、背景）的数据集；提出BioLite U-Net：在U-Net骨干中广泛采用深度可分离卷积以降低参数与计算量；与MobileNetV2/V3等轻量级分割基线（含DeepLabV3+）对比，评估指标为mIoU、Dice、像素精度，并在树莓派4B上进行端侧推理测试。

Result: BioLite U-Net在数据集上达到mIoU 92.85%、Dice 96.17%；模型体量较MobileNetV2-DeepLabV3+小超1300倍；在树莓派4B上推理延迟约335 ms/帧，显示接近实时能力，并在准确率-效率-可部署性上优于MobileNet基线。

Conclusion: 所提轻量级分割框架在资源受限环境中实现了生物打印过程的高效、准确语义分割，适合集成到智能闭环生物打印系统以提升打印质量与一致性。

Abstract: Bioprinting is a rapidly advancing field that offers a transformative
approach to fabricating tissue and organ models through the precise deposition
of cell-laden bioinks. Ensuring the fidelity and consistency of printed
structures in real-time remains a core challenge, particularly under
constraints imposed by limited imaging data and resource-constrained embedded
hardware. Semantic segmentation of the extrusion process, differentiating
between nozzle, extruded bioink, and surrounding background, enables in situ
monitoring critical to maintaining print quality and biological viability. In
this work, we introduce a lightweight semantic segmentation framework tailored
for real-time bioprinting applications. We present a novel, manually annotated
dataset comprising 787 RGB images captured during the bioprinting process,
labeled across three classes: nozzle, bioink, and background. To achieve fast
and efficient inference suitable for integration with bioprinting systems, we
propose a BioLite U-Net architecture that leverages depthwise separable
convolutions to drastically reduce computational load without compromising
accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based
segmentation baselines using mean Intersection over Union (mIoU), Dice score,
and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess
real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%
and a Dice score of 96.17%, while being over 1300x smaller than
MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,
demonstrating near real-time capability. Compared to MobileNet baselines,
BioLite U-Net offers a superior tradeoff between segmentation accuracy,
efficiency, and deployability, making it highly suitable for intelligent,
closed-loop bioprinting systems.

</details>


### [134] [STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment](https://arxiv.org/abs/2509.06693)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Qunyi Zhang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: 提出STAGE：一种结合分级扩散与显式掩码对齐的异常合成方法，利用干净背景先验与异常分支记录细粒度异常，实现更逼真且与背景一致的像素级异常生成，在MVTec与BTAD上达SOTA并提升下游分割。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常合成常出现两大问题：合成异常纹理细节不足、与背景不对齐；难以生成细粒度像素级异常，限制了下游异常分割的效果。

Method: 1) 引入基于干净背景先验的异常推理策略：在扩散去噪分布中注入背景信息，强化前景异常与背景的分离与凸显；2) 分级扩散框架+异常仅分支：在正向与反向过程中显式记录局部异常，避免细微异常被平滑或忽略；3) 显式掩码对齐(EMA)：逐步对齐合成异常与背景结构与语境，使生成结果上下文一致、结构连贯。

Result: 在MVTec与BTAD数据集上取得SIAS领域SOTA，且所生成数据显著提升下游异常分割性能。

Conclusion: 通过干净背景先验、分级扩散的异常记录与显式掩码对齐，STAGE能生成细粒度、与背景一致的异常，实现更高质量的异常合成，并切实提升下游像素级异常分割表现。

Abstract: Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal
role in enhancing the performance of downstream anomaly segmentation, as it
provides an effective means of expanding abnormal data. However, existing SIAS
methods face several critical limitations: (i) the synthesized anomalies often
lack intricate texture details and fail to align precisely with the surrounding
background, and (ii) they struggle to generate fine-grained, pixel-level
anomalies. To address these challenges, we propose Segmentation-oriented
Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed
STAGE. STAGE introduces a novel anomaly inference strategy that incorporates
clean background information as a prior to guide the denoising distribution,
enabling the model to more effectively distinguish and highlight abnormal
foregrounds. Furthermore, it employs a graded diffusion framework with an
anomaly-only branch to explicitly record local anomalies during both the
forward and reverse processes, ensuring that subtle anomalies are not
overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)
strategy to progressively align the synthesized anomalies with the background,
resulting in context-consistent and structurally coherent generations.
Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE
achieves state-of-the-art performance in SIAS, which in turn enhances
downstream anomaly segmentation.

</details>


### [135] [Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention](https://arxiv.org/abs/2509.06705)
*Mohamed Zayaan S*

Main category: cs.CV

TL;DR: Cortex Synth提出一种端到端可微框架，从单张2D图像同时合成3D骨架的几何与拓扑；通过层级图注意力+多尺度优化、可微谱拓扑优化（Laplacian特征分解）、以及对抗式几何一致性训练，实现SOTA，显著降低MPJPE与拓扑错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么只重建几何（关节点位置），要么预设/固定骨架拓扑，难以在单图像条件下同时推断可靠的3D结构与连通关系；同时缺乏可微的拓扑优化手段，训练中难以端到端优化结构一致性。

Method: 提出端到端架构，包含四模块：伪3D点云生成器、增强型PointNet编码器、骨架坐标解码器、可微图构建网络DGCN。三项关键技术：1) 层级图注意力与多尺度骨架细化；2) 基于拉普拉斯特征分解的可微谱拓扑优化，利用特征向量引导图连接更新；3) 对抗式几何一致性训练，使姿态结构与几何分布对齐。整体以单张2D图像为输入，生成3D关节点与边的拓扑。

Result: 在ShapeNet上相较此前方法：MPJPE降低18.7%，图编辑距离降低27.3%，拓扑错误率下降42%。

Conclusion: Cortex Synth能在单图像场景下联合学习3D骨架几何与拓扑，端到端可微带来显著精度与拓扑一致性提升，并具备机器人操作、医学影像与角色绑定等应用潜力。

Abstract: We present Cortex Synth, a novel end-to-end differentiable framework for
joint 3D skeleton geometry and topology synthesis from single 2D images. Our
architecture introduces three key innovations: (1) A hierarchical graph
attention mechanism with multi-scale skeletal refinement, (2) Differentiable
spectral topology optimization via Laplacian eigen decomposition, and (3)
Adversarial geometric consistency training for pose structure alignment. The
framework integrates four synergistic modules: a pseudo 3D point cloud
generator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a
novel Differentiable Graph Construction Network (DGCN). Our experiments
demonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and
27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological
errors by 42 percent compared to previous approaches. The model's end-to-end
differentiability enables applications in robotic manipulation, medical
imaging, and automated character rigging.

</details>


### [136] [MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture](https://arxiv.org/abs/2509.06713)
*Mustafa Yurdakul,Şakir Taşdemir*

Main category: cs.CV

TL;DR: 提出一个结合EfficientNetV2骨干与注意力型MLP-Mixer的可解释脑肿瘤MRI分类模型，在Figshare三类肿瘤数据集（3064张，T1对比增强）上，通过五折交叉验证取得约99.5%的准确、精确、召回与F1，并用Grad-CAM展示聚焦于病灶区域。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤致死率高且需早诊，MRI判读依赖专家且易出错，临床迫切需要高精度、可解释、可复现的自动化辅助诊断模型。

Method: 先比较九种主流CNN以选最优骨干，选出EfficientNetV2；在其上集成注意力机制的MLP-Mixer模块提升特征建模与分类能力；采用五折交叉验证评估，并与基础CNN和文献方法对比；用Grad-CAM进行可解释性可视化验证关注区域。

Result: 最终模型在三类脑肿瘤数据集上达成约99.50%准确率、99.47%精确率、99.52%召回率、99.49% F1，性能优于比较的基线与文献方法；Grad-CAM显示模型关注病灶相关区域。

Conclusion: EfficientNetV2与注意力型MLP-Mixer的结合形成了稳健且可解释的脑肿瘤分类模型，具备临床决策支持价值，兼具高准确性与可解释性，优于现有研究。

Abstract: Brain tumors are serious health problems that require early diagnosis due to
their high mortality rates. Diagnosing tumors by examining Magnetic Resonance
Imaging (MRI) images is a process that requires expertise and is prone to
error. Therefore, the need for automated diagnosis systems is increasing day by
day. In this context, a robust and explainable Deep Learning (DL) model for the
classification of brain tumors is proposed. In this study, a publicly available
Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI
images of three tumor types was used. First, the classification performance of
nine well-known CNN architectures was evaluated to determine the most effective
backbone. Among these, EfficientNetV2 demonstrated the best performance and was
selected as the backbone for further development. Subsequently, an
attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to
enhance its classification capability. The performance of the final model was
comprehensively compared with basic CNNs and the methods in the literature.
Additionally, Grad-CAM visualization was used to interpret and validate the
decision-making process of the proposed model. The proposed model's performance
was evaluated using the five-fold cross-validation method. The proposed model
demonstrated superior performance with 99.50% accuracy, 99.47% precision,
99.52% recall and 99.49% F1 score. The results obtained show that the model
outperforms the studies in the literature. Moreover, Grad-CAM visualizations
demonstrate that the model effectively focuses on relevant regions of MRI
images, thus improving interpretability and clinical reliability. A robust deep
learning model for clinical decision support systems has been obtained by
combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy
and interpretability in brain tumor classification.

</details>


### [137] [Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training](https://arxiv.org/abs/2509.06723)
*Ruicheng Zhang,Jun Zhou,Zunnan Xu,Zihao Liu,Jiehui Huang,Mingyang Zhang,Yu Sun,Xiu Li*

Main category: cs.CV

TL;DR: 提出Zo3T：一种零样本、测试时训练的轨迹引导图生视频方法，在不依赖昂贵微调的前提下兼顾3D透视与网络预测一致性，显著提升运动准确性与三维真实感。


<details>
  <summary>Details</summary>
Motivation: 现有I2V轨迹控制方法要么依赖稀缺标注与高成本微调，要么零样本但忽略3D透视并造成潜变量操控与噪声预测失配，导致不真实运动与生成退化。需要一种无需离线训练、能在测试时自适应并保证运动约束与生成保真度的方法。

Method: Zo3T包含三项关键创新：1) 3D感知运动投影：通过场景深度推断得到透视一致的仿射变换，对目标区域做符合三维视角的轨迹驱动；2) 轨迹引导的测试时LoRA：在去噪网络中动态注入、共同优化临时LoRA，与潜变量联动，并以局部特征一致性损失约束，使网络对被操控潜变量局部自适应，保持在流形上生成；3) 引导场校正：采用一步前视策略优化条件引导场，修正去噪演化路径，提高向目标轨迹收敛的效率与稳定性。

Result: 在轨迹控制的图生视频任务上，相比训练型与零样本基线，Zo3T显著提升3D真实感与运动精度，生成过程更稳定，定量与可视化均优于现有方法。

Conclusion: 通过3D投影、测试时LoRA共适应与引导场校正，Zo3T在零样本设定下实现高保真、透视一致的轨迹引导视频生成，为无需离线微调的可控I2V提供有效范式。

Abstract: Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos
that adhere to user-specified motion instructions. Existing methods typically
rely on computationally expensive fine-tuning on scarce annotated datasets.
Although some zero-shot methods attempt to trajectory control in the latent
space, they may yield unrealistic motion by neglecting 3D perspective and
creating a misalignment between the manipulated latents and the network's noise
predictions. To address these challenges, we introduce Zo3T, a novel zero-shot
test-time-training framework for trajectory-guided generation with three core
innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging
inferring scene depth to derive perspective-correct affine transformations for
target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a
mechanism that dynamically injects and optimizes ephemeral LoRA adapters into
the denoising network alongside the latent state. Driven by a regional feature
consistency loss, this co-adaptation effectively enforces motion constraints
while allowing the pre-trained model to locally adapt its internal
representations to the manipulated latent, thereby ensuring generative fidelity
and on-manifold adherence. Finally, we develop Guidance Field Rectification,
which refines the denoising evolutionary path by optimizing the conditional
guidance field through a one-step lookahead strategy, ensuring efficient
generative progression towards the target trajectory. Zo3T significantly
enhances 3D realism and motion accuracy in trajectory-controlled I2V
generation, demonstrating superior performance over existing training-based and
zero-shot approaches.

</details>


### [138] [Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation](https://arxiv.org/abs/2509.06740)
*Qing Xu,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: 提出Co-Seg协同分割框架，通过互相引导同时完成组织语义分割与细胞核实例分割，利用区域提示编码与互促掩码解码实现跨任务一致性，在PUMA数据集上于语义、实例与全景指标均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往方法将组织语义分割与细胞核实例分割割裂处理，未利用二者天然的结构与上下文关联，导致对肿瘤微环境理解不充分、泛化与一致性较弱。

Method: 1) 提出协同分割范式，使两任务互相增强；2) 设计区域感知提示编码器（RP-Encoder），生成高质量的语义与实例区域提示作为先验；3) 设计互促提示掩码解码器（MP-Decoder），通过跨指导（cross-guidance）强化上下文一致性，协同解出语义与实例掩码；4) 在PUMA数据集上进行广泛实验与对比。

Result: 在PUMA数据集上，Co-Seg在组织语义分割、细胞核实例分割以及全景分割指标上均超越现有最先进方法，表现出更好的精度与一致性。

Conclusion: 联合建模组织与细胞核的协同分割能有效提升病理图像理解；通过区域提示与互促解码实现跨任务信息交互，可作为病理分割的新范式，具有实践价值与可扩展性。

Abstract: Histopathology image analysis is critical yet challenged by the demand of
segmenting tissue regions and nuclei instances for tumor microenvironment and
cellular morphology analysis. Existing studies focused on tissue semantic
segmentation or nuclei instance segmentation separately, but ignored the
inherent relationship between these two tasks, resulting in insufficient
histopathology understanding. To address this issue, we propose a Co-Seg
framework for collaborative tissue and nuclei segmentation. Specifically, we
introduce a novel co-segmentation paradigm, allowing tissue and nuclei
segmentation tasks to mutually enhance each other. To this end, we first devise
a region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and
instance region prompts as prior constraints. Moreover, we design a mutual
prompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen
the contextual consistency of both tasks, collaboratively computing semantic
and instance segmentation masks. Extensive experiments on the PUMA dataset
demonstrate that the proposed Co-Seg surpasses state-of-the-arts in the
semantic, instance and panoptic segmentation of tumor tissues and nuclei
instances. The source code is available at https://github.com/xq141839/Co-Seg.

</details>


### [139] [Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light](https://arxiv.org/abs/2509.06741)
*Christian Geckeler,Niklas Neugebauer,Manasi Muglikar,Davide Scaramuzza,Stefano Mintchev*

Main category: cs.CV

TL;DR: 提出一种基于事件相机的“事件光谱”系统，在单一传感器上同时实现低时延高分辨率结构光深度与多光谱成像（650–850 nm），在林下等弱光环境中优于传统RGB/多光谱方案。


<details>
  <summary>Details</summary>
Motivation: 森林环境中的UAV需要在密集植被下安全导航与精确采集数据；传统被动RGB/多光谱成像存在时延大、深度分辨率差、受环境光强影响严重，尤其林冠下。需要一种轻量、鲁棒、可同时获取深度与光谱信息的方案。

Method: 构建事件相机+可调波长结构光投影系统：利用结构光实现高分辨率、低时延的深度重建；通过调制投影光的波长，在650–850 nm范围内获取受控窄带光谱信息。另提供便携RGB（三波长）版本用于真实雨林采集。进行与商用深度传感器、参考光谱仪和商用多光谱相机的对比验证。利用获取的深度与光谱数据进行彩色重建与材料（叶片/枝干）区分。

Result: 相较商用深度传感器，深度RMSE最高提升约60%；光谱精度与参考光谱仪/商用多光谱相机相当。便携RGB版本在马索阿拉雨林实测实现可靠深度与光谱采集，支持彩色图像重建与叶片/枝干材料区分；加入深度信息后，材料区分准确率较仅用颜色提升30%以上。系统在实验室与真实雨林均表现稳健。

Conclusion: 所提单传感器事件光谱结构光系统在复杂自然环境中实现低时延高精度深度与多光谱一体化感知，较现有方法更鲁棒轻量，可显著提升UAV在森林下的导航与数据采集能力，并为基于深度+光谱的生态监测与目标识别奠定基础。

Abstract: Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest
environments for tasks such as environmental monitoring and search and rescue,
which require safe navigation through dense foliage and precise data
collection. Traditional sensing approaches, including passive multispectral and
RGB imaging, suffer from latency, poor depth resolution, and strong dependence
on ambient light - especially under forest canopies. In this work, we present a
novel event spectroscopy system that simultaneously enables high-resolution,
low-latency depth reconstruction and multispectral imaging using a single
sensor. Depth is reconstructed using structured light, and by modulating the
wavelength of the projected structured light, our system captures spectral
information in controlled bands between 650 nm and 850 nm. We demonstrate up to
$60\%$ improvement in RMSE over commercial depth sensors and validate the
spectral accuracy against a reference spectrometer and commercial multispectral
cameras, demonstrating comparable performance. A portable version limited to
RGB (3 wavelengths) is used to collect real-world depth and spectral data from
a Masoala Rainforest. We demonstrate the use of this prototype for color image
reconstruction and material differentiation between leaves and branches using
spectral and depth data. Our results show that adding depth (available at no
extra effort with our setup) to material differentiation improves the accuracy
by over $30\%$ compared to color-only method. Our system, tested in both lab
and real-world rainforest environments, shows strong performance in depth
estimation, RGB reconstruction, and material differentiation - paving the way
for lightweight, integrated, and robust UAV perception and data collection in
complex natural environments.

</details>


### [140] [Pothole Detection and Recognition based on Transfer Learning](https://arxiv.org/abs/2509.06750)
*Mang Hu,Qianqian Xia*

Main category: cs.CV

TL;DR: 提出一种基于迁移学习的深度特征提取网络（融合ResNet50、EfficientNet、RegNet），用于道路坑洞图像识别，在两组测试集上分别达到97.78%与98.89%准确率，并优于RF、MLP、SVM、LightGBM且识别速度更快。


<details>
  <summary>Details</summary>
Motivation: 道路坑洞影响行车安全与维护决策，传统人工巡检效率低、主观性强。利用计算机视觉从道路图像中自动提取坑洞特征并准确分类，可提升检测效率与可靠性，服务城市道路养护与智能交通。

Method: 对原始图像进行标准化、归一化与数据增强；基于迁移学习构建并反复迭代优化一个深度特征提取网络，融合ResNet50、EfficientNet、RegNet的优势；与传统机器学习分类器（Random Forest、MLP、SVM、LightGBM）进行对比评估；采用Accuracy、Recall、Precision、F1-score与FPS衡量性能，进行参数选择与模型优化。

Result: 所提融合迁移学习模型在初始90张测试样本上达到97.78%（88/90）准确率，在扩展至900张测试样本上达到98.89%（890/900）；在速度（FPS）与准确性等指标上均优于RF、MLP、SVM、LightGBM等基线模型。

Conclusion: 融合ResNet50-EfficientNet-RegNet的迁移学习模型能够高效、准确地识别道路坑洞，兼具较高分类性能与推理速度，优于多种传统基线方法，适合实际应用场景。

Abstract: With the rapid development of computer vision and machine learning, automated
methods for pothole detection and recognition based on image and video data
have received significant attention. It is of great significance for social
development to conduct an in-depth analysis of road images through feature
extraction, thereby achieving automatic identification of the pothole condition
in new images. Consequently, this is the main issue addressed in this study.
Based on preprocessing techniques such as standardization, normalization, and
data augmentation applied to the collected raw dataset, we continuously
improved the network model based on experimental results. Ultimately, we
constructed a deep learning feature extraction network
ResNet50-EfficientNet-RegNet model based on transfer learning. This model
exhibits high classification accuracy and computational efficiency. In terms of
model evaluation, this study employed a comparative evaluation approach by
comparing the performance of the proposed transfer learning model with other
models, including Random Forest, MLP, SVM, and LightGBM. The comparison
analysis was conducted based on metrics such as Accuracy, Recall, Precision,
F1-score, and FPS, to assess the classification performance of the transfer
learning model proposed in this paper. The results demonstrate that our model
exhibits high performance in terms of recognition speed and accuracy,
surpassing the performance of other models. Through careful parameter selection
and model optimization, our transfer learning model achieved a classification
accuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%
(890/900) on the expanded test set.

</details>


### [141] [Raw2Event: Converting Raw Frame Camera into Event Camera](https://arxiv.org/abs/2509.06767)
*Zijie Ning,Enmin Lin,Sudarshan R. Iyengar,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 提出Raw2Event：用低成本原始Bayer帧相机实时生成事件数据的软硬件一体系统，绕过ISP，获得更高动态范围与分辨率，并在树莓派上实时运行，生成接近真实事件相机的事件流，支持同步采集与参数可调。


<details>
  <summary>Details</summary>
Motivation: 事件相机具备高时间分辨率、低延迟与高动态范围，但昂贵、分辨率有限且缺少自动对焦等功能，限制了早期研发与原型设计的普及；需要一种低成本、可部署、接近事件相机输出的替代方案。

Method: 1) 基于DVS-Voltmeter建立可配置的事件仿真框架，直接利用原始Bayer数据，绕过传统ISP；2) 设计数据采集管线，可同步记录原始RAW、RGB与事件流，用于评估与数据集构建；3) 嵌入式优化与参数可调接口，并在树莓派上部署实现实时运行。

Result: Raw2Event生成的事件流在统计与视觉质量上与真实事件相机相近，同时受益于更高分辨率、自动对焦与更高动态范围；系统实现实时性能，并提供用户友好的参数调节与可扩展性。

Conclusion: Raw2Event为事件视觉提供了一种可扩展、低成本、实时的替代方案，适合研究与早期系统开发，能灵活适配多种应用并促进数据集制作与评估。

Abstract: Event cameras offer unique advantages such as high temporal resolution, low
latency, and high dynamic range, making them more and more popular for vision
tasks under challenging light conditions. However, their high cost, limited
resolution, and lack of features such as autofocus hinder their broad adoption,
particularly for early-stage development and prototyping. In this work, we
present Raw2Event, a complete hardware-software system that enables real-time
event generation from low-cost raw frame-based cameras. By leveraging direct
access to raw Bayer data and bypassing traditional image signal processors
(ISP), our system is able to utilize the full potential of camera hardware,
delivering higher dynamic range, higher resolution, and more faithful output
than RGB-based frame-to-event converters.
  Built upon the DVS-Voltmeter model, Raw2Event features a configurable
simulation framework optimized for deployment on embedded platforms. We further
design a data acquisition pipeline that supports synchronized recording of raw,
RGB, and event streams, facilitating downstream evaluation and dataset
creation. Experimental results show that Raw2Event can generate event streams
closely resembling those from real event cameras, while benefiting from higher
resolution and autofocus capabilities. The system also supports user-intuitive
parameter tuning, enabling flexible adaptation to various application
requirements. Finally, we deploy the system on a Raspberry Pi for real-time
operation, providing a scalable and cost-effective solution for event-based
vision research and early-stage system development.
  The codes are available online:
https://anonymous.4open.science/r/raw2event-BFF2/README.md.

</details>


### [142] [D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning](https://arxiv.org/abs/2509.06771)
*Sai Kartheek Reddy Kasu,Mohammad Zia Ur Rehman,Shahid Shafi Dar,Rishi Bharat Junghare,Dhanvin Sanjay Namboodiri,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出首个面向“黑暗幽默”多模态梗图的数据集与方法：4379条Reddit梗图标注（是否黑暗幽默、攻击目标类别与强度三级），并以VLM生成-自校正解释+三流交互网络融合文本/OCR/图像/推理，显著优于强基线于三项任务。


<details>
  <summary>Details</summary>
Motivation: 黑暗幽默依赖隐含、敏感、文化语境线索，现有多模态检测资源与方法匮乏，难以支持内容审核与幽默理解研究。

Method: 1) 构建数据集：4379条Reddit梗图，标注黑暗幽默与否、目标类别（性别、心理健康、暴力、种族、残障、其他），强度（轻/中/重）。2) 推理增强框架：VLM生成结构化解释；通过“角色反转自环”，让VLM从作者视角迭代完善解释以提升完整性与一致性。3) 特征提取：OCR文本与自校正推理走文本编码器；图像走视觉Transformer。4) 融合：提出Tri-stream Cross-Reasoning Network（TCRNet），以成对注意力融合文本、图像、推理三流，得到统一表示用于分类。

Result: 在三项任务（黑暗幽默检测、目标识别、强度预测）上，方法均优于多种强基线；实验显示引入VLM解释与角色反转自环、以及三流跨推理融合均带来增益。

Conclusion: 数据集与代码开源，所提推理增强的三流跨推理网络有效提升黑暗幽默多模态理解，促进内容审核与幽默机制研究。

Abstract: Dark humor in online memes poses unique challenges due to its reliance on
implicit, sensitive, and culturally contextual cues. To address the lack of
resources and methods for detecting dark humor in multimodal content, we
introduce a novel dataset of 4,379 Reddit memes annotated for dark humor,
target category (gender, mental health, violence, race, disability, and other),
and a three-level intensity rating (mild, moderate, severe). Building on this
resource, we propose a reasoning-augmented framework that first generates
structured explanations for each meme using a Large Vision-Language Model
(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective
to iteratively refine its explanations, ensuring completeness and alignment. We
then extract textual features from both the OCR transcript and the self-refined
reasoning via a text encoder, while visual features are obtained using a vision
transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three
streams, text, image, and reasoning, via pairwise attention mechanisms,
producing a unified representation for classification. Experimental results
demonstrate that our approach outperforms strong baselines across three tasks:
dark humor detection, target identification, and intensity prediction. The
dataset, annotations, and code are released to facilitate further research in
multimodal humor understanding and content moderation. Code and Dataset are
available at:
https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning

</details>


### [143] [UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets](https://arxiv.org/abs/2509.06781)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 提出UrbanTwin：三套与真实路侧激光雷达数据集（LUMPI、V2X-Real-IC、TUMTraf-I）一一对应的高保真数字孪生合成数据，各含1万帧并具丰富标注；仅用合成数据训练即可在真实未见数据上取得优于实训基线的检测效果，显示其可增强甚至替代同域真实数据。


<details>
  <summary>Details</summary>
Motivation: 现实路侧激光雷达数据昂贵、受限于采集成本与场景多样性，且标注负担沉重；需要一种与现实高度对齐、可扩展且可控制的合成数据来提升检测、跟踪与分割等任务的训练效果，并能支持可定制的测试场景。

Method: 基于真实地点构建高保真的数字孪生：复原周边几何、车道级道路与交叉口拓扑及交通流模式；以仿真激光雷达在数字孪生中采样生成10K帧/数据集；提供3D框、实例分割、跟踪ID（6类）与语义分割（9类）标注。通过统计与结构相似性分析评估与真实数据的对齐程度；仅用合成数据训练3D检测模型并在真实数据上测试以验证迁移性能。

Result: 合成与真实数据在统计与结构指标上高度相似；仅用UrbanTwin训练的3D检测模型在真实未见数据上的表现优于使用真实数据训练的基线；显示样本量与场景多样性显著提升模型表现与泛化。

Conclusion: UrbanTwin可作为现实同域数据的有效增强乃至替代，提升3D检测等任务性能；数字孪生可灵活修改以测试自定义场景。作者声称其为首个可替代同域真实数据的激光雷达感知数字合成数据集，并已公开发布。

Abstract: This article presents UrbanTwin datasets - high-fidelity, realistic replicas
of three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.
Each UrbanTwin dataset contains 10K annotated frames corresponding to one of
the public datasets. Annotations include 3D bounding boxes, instance
segmentation labels, and tracking IDs for six object classes, along with
semantic segmentation labels for nine classes. These datasets are synthesized
using emulated lidar sensors within realistic digital twins, modeled based on
surrounding geometry, road alignment at lane level, and the lane topology and
vehicle movement patterns at intersections of the actual locations
corresponding to each real dataset. Due to the precise digital twin modeling,
the synthetic datasets are well aligned with their real counterparts, offering
strong standalone and augmentative value for training deep learning models on
tasks such as 3D object detection, tracking, and semantic and instance
segmentation. We evaluate the alignment of the synthetic replicas through
statistical and structural similarity analysis with real data, and further
demonstrate their utility by training 3D object detection models solely on
synthetic data and testing them on real, unseen data. The high similarity
scores and improved detection performance, compared to the models trained on
real data, indicate that the UrbanTwin datasets effectively enhance existing
benchmark datasets by increasing sample size and scene diversity. In addition,
the digital twins can be adapted to test custom scenarios by modifying the
design and dynamics of the simulations. To our knowledge, these are the first
digitally synthesized datasets that can replace in-domain real-world datasets
for lidar perception tasks. UrbanTwin datasets are publicly available at
https://dataverse.harvard.edu/dataverse/ucf-ut.

</details>


### [144] [P3-SAM: Native 3D Part Segmentation](https://arxiv.org/abs/2509.06784)
*Changfeng Ma,Yang Li,Xinhao Yan,Jiachen Xu,Yunhan Yang,Chunshi Wang,Zibo Zhao,Yanwen Guo,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: 提出P3-SAM：一个原生3D点提示可交互分割模型，可自动把任意3D对象划分为部件，基于大规模（≈370万）带部件标注的数据训练，配合自动掩码选择与合并算法，实现稳健、精确、SOTA的部件实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有3D部件分割方法在复杂对象上鲁棒性差、难以全自动化，限制了3D理解、模型复用与部件生成等应用，需要一种既可交互又能全自动的高鲁棒方案。

Method: 借鉴SAM框架，设计P3-SAM：包含特征提取器、多分割头与IoU预测器，实现点提示驱动的交互式分割；并提出自动选择与合并模型预测掩码的算法以获得部件实例分割；在新构建、含约370万模型且具合理分割标注的数据集上训练。

Result: 在多类、复杂3D对象上获得精确且鲁棒的部件分割效果，性能达SOTA；可对任何复杂对象实现稳定分割。

Conclusion: P3-SAM结合点提示交互与自动掩码选择合并，实现对任意3D对象的全自动/可交互部件实例分割，凭借大规模数据训练在精度与鲁棒性上优于现有方法，具备广泛应用潜力；代码即将开源。

Abstract: Segmenting 3D assets into their constituent parts is crucial for enhancing 3D
understanding, facilitating model reuse, and supporting various applications
such as part generation. However, current methods face limitations such as poor
robustness when dealing with complex objects and cannot fully automate the
process. In this paper, we propose a native 3D point-promptable part
segmentation model termed P3-SAM, designed to fully automate the segmentation
of any 3D objects into components. Inspired by SAM, P3-SAM consists of a
feature extractor, multiple segmentation heads, and an IoU predictor, enabling
interactive segmentation for users. We also propose an algorithm to
automatically select and merge masks predicted by our model for part instance
segmentation. Our model is trained on a newly built dataset containing nearly
3.7 million models with reasonable segmentation labels. Comparisons show that
our method achieves precise segmentation results and strong robustness on any
complex objects, attaining state-of-the-art performance. Our code will be
released soon.

</details>


### [145] [AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results](https://arxiv.org/abs/2509.06793)
*George Ciubotariu,Florin-Alexandru Vasluianu,Zhuyun Zhou,Nancy Mehta,Radu Timofte,Ke Wu,Long Sun,Lingshun Kong,Zhongbao Yang,Jinshan Pan,Jiangxin Dong,Jinhui Tang,Hao Chen,Yinghui Fang,Dafeng Zhang,Yongqi Song,Jiangbo Guo,Shuhua Jin,Zeyu Xiao,Rui Zhao,Zhuoyuan Li,Cong Zhang,Yufeng Peng,Xin Lu,Zhijing Sun,Chengjie Ge,Zihao Li,Zishun Liao,Ziang Zhou,Qiyu Kang,Xueyang Fu,Zheng-Jun Zha,Yuqian Zhang,Shuai Liu,Jie Liu,Zhuhao Zhang,Lishen Qu,Zhihao Liu,Shihao Zhou,Yaqi Luo,Juncheng Zhou,Jufeng Yang,Qianfeng Yang,Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 综述AIM 2025高帧率非均匀运动去模糊挑战：汇总方法与结果，基于新数据集MIORe评测，展示单帧高FPS去模糊的最新进展。


<details>
  <summary>Details</summary>
Motivation: 在多样且复杂的运动类型（非均匀运动）与高帧率成像条件下，单张图像去模糊仍具挑战；需要系统评测与对比能够学习代表性视觉线索、在困难场景中生成更清晰视觉效果的网络。

Method: 组织挑战赛：68人注册、9队提交有效方案；基于新数据集MIORe提供包含复杂运动模式样例；对参赛网络进行统一评测与对比分析，总结SOTA方法特点与性能。

Result: 完成对高FPS单图去模糊方法的系统评测，给出最终排行榜与经验性观察，显示多种方法在复杂运动聚合情况下取得显著进步。

Conclusion: 挑战推动了高FPS非均匀运动去模糊的发展；MIORe数据集提供了更具挑战的基准；当前方法已显著提升，但在更复杂运动与泛化方面仍有改进空间。

Abstract: This paper presents a comprehensive review of the AIM 2025 High FPS
Non-Uniform Motion Deblurring Challenge, highlighting the proposed solutions
and final results. The objective of this challenge is to identify effective
networks capable of producing clearer and visually compelling images in diverse
and challenging conditions, by learning representative visual cues for complex
aggregations of motion types. A total of 68 participants registered for the
competition, and 9 teams ultimately submitted valid entries. This paper
thoroughly evaluates the state-of-the-art advances in high-FPS single image
motion deblurring, showcasing the significant progress in the field, while
leveraging samples of the novel dataset, MIORe, that introduces challenging
examples of movement patterns.

</details>


### [146] [SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis](https://arxiv.org/abs/2509.06798)
*Zhengqing Chen,Ruohong Mei,Xiaoyang Guo,Qingjie Wang,Yubin Hu,Wei Yin,Weiqiang Ren,Qian Zhang*

Main category: cs.CV

TL;DR: 提出一种可扩展的real2sim2real系统，通过3D生成自动化资产挖掘、生成与稀有场景合成，克服CG与学习式模拟在多样性、可扩展性与通用性上的限制。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要大量包含稀有与多样化情况的传感器数据，现实采集难且昂贵。现有CG引擎（如CARLA）难以覆盖长尾稀有案例；学习式模拟（如NeuSim）受限于特定类别、需多传感器数据，难以泛化到通用目标。需要一种既可扩展又能覆盖长尾的通用传感器模拟方案。

Method: 构建一个real2sim2real流程：从真实数据自动化挖掘与提取资产（对象/场景要素），利用3D生成技术进行资产生成与增强，再将其组合合成包含稀有情形的仿真数据；最后用于训练与回到真实域的适配。强调无须大量多传感器配准数据、可泛化至多类别对象。

Result: 系统能够在无需特定类别与多传感器依赖的前提下，自动产生多样且包含稀有事件的仿真数据，缓解数据稀缺并提升感知训练的覆盖度与鲁棒性。（摘要未给出量化指标）

Conclusion: 通过3D生成驱动的real2sim2real框架，可规模化地进行资产挖掘与合成，从而显著改善自动驾驶传感器仿真的多样性与长尾覆盖，为鲁棒感知训练提供通用且可扩展的解决方案。

Abstract: In the field of autonomous driving, sensor simulation is essential for
generating rare and diverse scenarios that are difficult to capture in
real-world environments. Current solutions fall into two categories: 1)
CG-based methods, such as CARLA, which lack diversity and struggle to scale to
the vast array of rare cases required for robust perception training; and 2)
learning-based approaches, such as NeuSim, which are limited to specific object
categories (vehicles) and require extensive multi-sensor data, hindering their
applicability to generic objects. To address these limitations, we propose a
scalable real2sim2real system that leverages 3D generation to automate asset
mining, generation, and rare-case data synthesis.

</details>


### [147] [MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration](https://arxiv.org/abs/2509.06803)
*George Ciubotariu,Zhuyun Zhou,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: 提出两个高帧率、可控运动幅度的多任务数据集（MIORe 与 VAR-MIORe），用于运动模糊去除、插帧与光流等任务评测，提供高分辨率与可扩展真值，在复杂运动与不利条件下系统挑战现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有运动恢复基准在帧率、光学质量、运动多样性与可控性方面不足，难以全面评估去模糊、插帧、光流等多任务算法的鲁棒性和泛化能力。

Method: 采用1000FPS与专业光学系统采集，覆盖自运动、多人交互与景深相关模糊等丰富场景；基于光流度量自适应平均帧以生成一致的运动模糊，同时保留清晰帧用于插帧与光流；提出VAR-MIORe，通过可变运动幅度设计（从极小到极大）实现对运动量的显式控制；提供高分辨率且可扩展的多任务真值。

Result: 形成两个数据集：MIORe（一致运动模糊+清晰输入，多任务真值）与VAR-MIORe（可控运动幅度跨度基准），能够在受控与恶劣条件下全面、系统地挑战现有算法性能。

Conclusion: MIORe与VAR-MIORe填补了运动恢复评测中的关键空白，提供高质量、可控、多任务的基准，为下一代图像与视频恢复研究（去模糊、插帧、光流等）奠定基础。

Abstract: We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address
critical limitations in current motion restoration benchmarks. Designed with
high-frame-rate (1000 FPS) acquisition and professional-grade optics, our
datasets capture a broad spectrum of motion scenarios, which include complex
ego-camera movements, dynamic multi-subject interactions, and depth-dependent
blur effects. By adaptively averaging frames based on computed optical flow
metrics, MIORe generates consistent motion blur, and preserves sharp inputs for
video frame interpolation and optical flow estimation. VAR-MIORe further
extends by spanning a variable range of motion magnitudes, from minimal to
extreme, establishing the first benchmark to offer explicit control over motion
amplitude. We provide high-resolution, scalable ground truths that challenge
existing algorithms under both controlled and adverse conditions, paving the
way for next-generation research of various image and video restoration tasks.

</details>


### [148] [UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward](https://arxiv.org/abs/2509.06818)
*Yufeng Cheng,Wenxu Wu,Shaojin Wu,Mengqi Huang,Fei Ding,Qian He*

Main category: cs.CV

TL;DR: 提出UMO统一多身份优化框架，通过“多对多匹配”将多身份生成表述为全局分配优化，并在扩散模型上以强化学习训练，显著提升多参考人脸的身份一致性、降低身份混淆，达到开源方法SOTA；并构建可扩展多参考定制数据集与新的身份混淆评测指标。


<details>
  <summary>Details</summary>
Motivation: 图像定制能力增强带来广泛应用，但人脸高度敏感，多参考图像下难以同时保持身份一致且避免身份混淆，限制了定制模型的身份扩展性与可靠性。

Method: 提出UMO框架：1) 将多身份生成重构为全局分配优化问题，采用“多对多匹配”范式统一处理多身份一致性； 2) 在扩散模型上引入强化学习以优化匹配与生成过程； 3) 构建含合成与真实部分的可扩展多参考定制数据集； 4) 设计新的身份混淆度量用于训练与评估。

Result: 在多种图像定制方法上，UMO显著提升身份一致性并降低身份混淆，在身份保持维度上刷新开源SOTA。

Conclusion: UMO为多身份图像定制提供统一且可扩展的优化范式，结合数据集与新指标，普遍提升现有方法的身份保真与区分度，推动多参考人脸定制的可扩展与可靠落地。

Abstract: Recent advancements in image customization exhibit a wide range of
application prospects due to stronger customization capabilities. However,
since we humans are more sensitive to faces, a significant challenge remains in
preserving consistent identity while avoiding identity confusion with
multi-reference images, limiting the identity scalability of customization
models. To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability. With "multi-to-multi matching"
paradigm, UMO reformulates multi-identity generation as a global assignment
optimization problem and unleashes multi-identity consistency for existing
image customization methods generally through reinforcement learning on
diffusion models. To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts. Additionally, we propose a new metric to measure
identity confusion. Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving. Code and model: https://github.com/bytedance/UMO

</details>


### [149] [Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning](https://arxiv.org/abs/2509.06826)
*Dipta Neogi,Nourash Azmine Chowdhury,Muhammad Rafsan Kabir,Mohammad Ashrafuzzaman Khan*

Main category: cs.CV

TL;DR: 论文提出一种用于自动视频分级（MPAA：G/PG/PG-13/R）的对比学习框架，结合CNN+LSTM（LRCN）与Bahdanau注意力，在情境对比学习设置下达到88%准确率与0.8815 F1，并可实时部署为网页应用。


<details>
  <summary>Details</summary>
Motivation: 短视频与流媒体内容激增，平台需自动化判断年龄适宜等级；传统监督方法依赖大量标注、泛化弱、特征学习效率低，难以区分如PG-13与R等细粒度边界。

Method: 探索三类对比学习：实例判别、情境对比、与多视图对比；提出混合架构：LRCN（CNN提取空间特征+LSTM建模时序）+Bahdanau注意力以动态选帧；比较多种对比损失（NT-Xent、NT-logistic、Margin Triplet），并在情境对比框架下达到最优。最后将模型部署为实时网页应用。

Result: 在情境对比学习框架中达到SOTA：准确率88%、F1=0.8815；在细粒度边界（如PG-13 vs R）区分上表现突出；对不同对比损失具有稳健性。

Conclusion: 对比学习结合时空建模与注意力能有效提升视频分级的判别力与泛化性；所提架构在实际流媒体合规场景可实时应用，具有工程可用性与鲁棒性。

Abstract: The rapid growth of visual content consumption across platforms necessitates
automated video classification for age-suitability standards like the MPAA
rating system (G, PG, PG-13, R). Traditional methods struggle with large
labeled data requirements, poor generalization, and inefficient feature
learning. To address these challenges, we employ contrastive learning for
improved discrimination and adaptability, exploring three frameworks: Instance
Discrimination, Contextual Contrastive Learning, and Multi-View Contrastive
Learning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a
Bahdanau attention mechanism, achieving state-of-the-art performance in the
Contextual Contrastive Learning framework, with 88% accuracy and an F1 score of
0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,
and attention mechanisms for dynamic frame prioritization, the model excels in
fine-grained borderline distinctions, such as differentiating PG-13 and R-rated
content. We evaluate the model's performance across various contrastive loss
functions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating
the robustness of our proposed architecture. To ensure practical application,
the model is deployed as a web application for real-time MPAA rating
classification, offering an efficient solution for automated content compliance
across streaming platforms.

</details>


### [150] [Curia: A Multi-Modal Foundation Model for Radiology](https://arxiv.org/abs/2509.06830)
*Corentin Dancette,Julien Khlaut,Antoine Saporta,Helene Philippe,Elodie Ferreres,Baptiste Callard,Théo Danielou,Léo Alberge,Léo Machado,Daniel Tordjman,Julie Dupuis,Korentin Le Floch,Jean Du Terrail,Mariam Moshiri,Laurent Dercle,Tom Boeken,Jules Gregory,Maxime Ronot,François Legou,Pascal Roux,Marc Sapoval,Pierre Manceron,Paul Hérent*

Main category: cs.CV

TL;DR: 提出Curia放射影像基础模型，基于一家大型医院多年全量横断面影像（15万例，130TB）训练，在19项外部任务基准上实现器官识别、出血/心梗检测与肿瘤分期预后预测，跨模态与低数据场景出现涌现能力，达甚至超越放射科医师与其他FM，并开放基础权重。


<details>
  <summary>Details</summary>
Motivation: 现有AI多为狭义单任务，难以覆盖多模态、多疾病的复杂临床需求；尽管基础模型在其他领域展现泛化与小样本优势，但在放射学中尚未充分实现，需要一个能跨模态、具低数据鲁棒性的通用影像FM。

Method: 构建并训练名为Curia的基础模型，使用来自大型医院多年全量横断面影像数据（15万检查，130TB）进行预训练；在新整理的19项外部验证基准上评估，包括器官识别、急性病变（脑出血、心肌梗死）检测、肿瘤分期结局预测等；与放射科医师和其他基础模型进行对比；分析跨模态与小样本（低数据）表现。

Result: Curia在19项任务中表现优异，能准确完成器官定位与病变检测，并对肿瘤分期结局作出有效预测；其性能达到或超越放射科医师与最新FM；在跨模态迁移和低数据训练/微调场景中出现临床显著的涌现能力。

Conclusion: Curia证明了大规模真实世界横断面影像数据训练的FM在放射学中具备广泛泛化与临床实用性，能够在多任务、多模态、低数据条件下取得SOTA或超越专家的表现；作者开放基础模型权重以促进社区研究与应用。

Abstract: AI-assisted radiological interpretation is based on predominantly narrow,
single-task models. This approach is impractical for covering the vast spectrum
of imaging modalities, diseases, and radiological findings. Foundation models
(FMs) hold the promise of broad generalization across modalities and in
low-data settings. However, this potential has remained largely unrealized in
radiology. We introduce Curia, a foundation model trained on the entire
cross-sectional imaging output of a major hospital over several years, which to
our knowledge is the largest such corpus of real-world data-encompassing
150,000 exams (130 TB). On a newly curated 19-task external validation
benchmark, Curia accurately identifies organs, detects conditions like brain
hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.
Curia meets or surpasses the performance of radiologists and recent foundation
models, and exhibits clinically significant emergent properties in
cross-modality, and low-data regimes. To accelerate progress, we release our
base model's weights at https://huggingface.co/raidium/curia.

</details>


### [151] [Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis](https://arxiv.org/abs/2509.06831)
*Simon Pezold,Jérôme A. Kurylec,Jan S. Liechti,Beat P. Müller,Joël L. Lavanchy*

Main category: cs.CV

TL;DR: 研究将单模态基础模型 V-JEPA 通过迁移学习与多模态 OR 数据融合，用于微创外科任务。结果显示：在领域无标注视频上微调与引入时间分辨的补充数据流均可显著提升预测住院时长、并发症与阶段识别等任务性能，且在HeiCo上达到或超过EndoVis2017顶尖水平；代码与权重已开源。


<details>
  <summary>Details</summary>
Motivation: 外科数据科学需要能够泛化至不同术式与医院的数据高效模型。现有外科视频模型常受限于标注稀缺、领域偏移与单模态信息不足。通用基础模型与多模态集成可能缓解这些问题，但其在手术场景中的实际收益与方法学路径尚需系统验证。

Method: 以V-JEPA作为视频基础模型：(a)在无标注、域内外科视频上进行微调以实现领域自适应；(b)采用模块化决策支持网络思想，为OR的其他时间序列数据（如设备/生理/流程信号）训练独立编码器，使其与V-JEPA嵌入对齐至共享表示空间。评估三项任务：院内肝脏外科视频上预测住院时长与术后并发症；HeiCo公开数据上外科阶段识别。基线为未微调的V-JEPA。

Result: (1) 领域内无标注视频微调显著提升所有下游任务的性能；(2) 在院内数据上，加入OR的时间分辨补充数据进一步提升预测表现；(3) 在HeiCo上，未微调的单模态视频基线已与EndoVis2017最佳提交相当，微调后精度更高。

Conclusion: 通用基础视频模型可有效迁移至外科场景；领域自适应（在无标注域内视频上微调）与多模态OR数据融合均能带来稳定增益。该工作为外科数据科学提供了可复用的模型与实现，支持未来在更多数据源与任务上的拓展。

Abstract: We investigate how both the adaptation of a generic foundation model via
transfer learning and the integration of complementary modalities from the
operating room (OR) can support surgical data science. To this end, we use
V-JEPA as the single-modality foundation of a multimodal model for minimally
invasive surgery support. We analyze how the model's downstream performance can
benefit (a) from finetuning on unlabeled surgical video data and (b) from
providing additional time-resolved data streams from the OR in a multimodal
setup.
  In an in-house dataset of liver surgery videos, we analyze the tasks of
predicting hospital length of stay and postoperative complications. In videos
of the public HeiCo dataset, we analyze the task of surgical phase recognition.
As a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on
unlabeled, held-out videos to investigate its change in performance after
domain adaptation. Following the idea of modular decision support networks, we
integrate additional data streams from the OR by training a separate encoder to
form a shared representation space with V-JEPA's embeddings.
  Our experiments show that finetuning on domain-specific data increases model
performance. On the in-house data, integrating additional time-resolved data
likewise benefits the model. On the HeiCo data, accuracy of the pretrained
video-only, single-modality baseline setup is on par with the top-performing
submissions of the EndoVis2017 challenge, while finetuning on domain-specific
data increases accuracy further. Our results thus demonstrate how surgical data
science can leverage public, generic foundation models. Likewise, they indicate
the potential of domain adaptation and of integrating suitable complementary
data streams from the OR. To support further research, we release our code and
model weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.

</details>


### [152] [Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset](https://arxiv.org/abs/2509.06835)
*Nabeyou Tadessa,Balaji Iyangar,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 论文评估了交通标志分类器在LISA数据集上对抗样本（FGSM、PGD）的脆弱性，发现随着扰动强度增大，准确率急剧下降，提示需面向实用系统的防御研究。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻防研究多集中在MNIST等简化数据集，缺乏对真实世界任务（如交通标志识别）的系统评估；自动驾驶与交通安全对模型鲁棒性要求高，需量化其在常见梯度攻击下的脆弱性。

Method: 在LISA Traffic Sign数据集上训练一个卷积神经网络以分类47类交通标志；采用FGSM与PGD两种白盒对抗攻击，逐步增大扰动幅度（ε）评估模型分类准确率的变化。

Result: 随着扰动幅度增加，模型准确率出现显著、快速下滑，表明模型对两种梯度攻击均较为敏感；对不同ε值的准确率曲线呈单调下降趋势。

Conclusion: 交通标志识别模型对对抗扰动高度脆弱，现有未经防御的CNN在实际应用中存在安全隐患；该工作为面向真实交通场景的防御机制设计与评测提供基线与动机。

Abstract: Adversarial attacks pose significant threats to machine learning models by
introducing carefully crafted perturbations that cause misclassification. While
prior work has primarily focused on MNIST and similar datasets, this paper
investigates the vulnerability of traffic sign classifiers using the LISA
Traffic Sign dataset. We train a convolutional neural network to classify 47
different traffic signs and evaluate its robustness against Fast Gradient Sign
Method (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a
sharp decline in classification accuracy as the perturbation magnitude
increases, highlighting the models susceptibility to adversarial examples. This
study lays the groundwork for future exploration into defense mechanisms
tailored for real-world traffic sign recognition systems.

</details>


### [153] [ToonOut: Fine-tuned Background-Removal for Anime Characters](https://arxiv.org/abs/2509.06839)
*Matteo Muratori,Joël Seytre*

Main category: cs.CV

TL;DR: 针对动漫风格图像抠图困难（发丝、半透明等），作者收集并标注1228张动漫数据，微调开源BiRefNet，使像素准确率由95.3%提升到99.5%，并开源代码、模型与数据集。


<details>
  <summary>Details</summary>
Motivation: 现有抠图/背景移除模型在真实照片上表现佳，但在动漫风格（复杂发丝、透明度处理）上效果欠佳，缺乏专门数据与模型适配。

Method: 构建并标注1228张高质量动漫人物与物体图像数据集；在此数据上对开源BiRefNet进行微调；提出并使用Pixel Accuracy作为评估指标，对比微调前后性能。

Result: 微调后的BiRefNet在动漫图像背景移除任务上显著提升，Pixel Accuracy从95.3%提升至99.5%。

Conclusion: 面向动漫风格的定制数据与微调能大幅提升背景移除效果；作者开源代码、权重与数据集，便于复现与扩展。

Abstract: While state-of-the-art background removal models excel at realistic imagery,
they frequently underperform in specialized domains such as anime-style
content, where complex features like hair and transparency present unique
challenges. To address this limitation, we collected and annotated a custom
dataset of 1,228 high-quality anime images of characters and objects, and
fine-tuned the open-sourced BiRefNet model on this dataset. This resulted in
marked improvements in background removal accuracy for anime-style images,
increasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric.
We are open-sourcing the code, the fine-tuned model weights, as well as the
dataset at: https://github.com/MatteoKartoon/BiRefNet.

</details>


### [154] [Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice](https://arxiv.org/abs/2509.06854)
*Hajar Moradmand,Lei Ren*

Main category: cs.CV

TL;DR: 提出ARTSS深度学习框架，自动从全手X线片预测RA的TSS评分；在970例数据上分四阶段完成预处理、分割、关节检测与TSS回归，ViT取得最佳Huber loss=0.87，关节检测准确率99%，可降低读片变异并节省时间。


<details>
  <summary>Details</summary>
Motivation: TSS是评估RA骨破坏进展的金标准之一，但人工评分耗时、主观且一致性差；临床图像存在关节消失与序列长度可变，给自动化带来挑战。需要一个鲁棒的端到端系统，提升效率和一致性。

Method: 构建ARTSS四阶段流水线：I) ResNet50用于图像预处理与重定向；II) UNet.3进行手部分割；III) YOLOv7进行关节定位/识别，兼容关节缺失与数量变化；IV) 多模型比较进行TSS回归（VGG16/19、ResNet50、DenseNet201、EfficientNetB0、ViT），以两位放射科医师平均TSS为真值。训练采用3折交叉验证（452训/227验/折），外部测试291例；评价指标包括IoU、MAP、MAE、RMSE、Huber loss。

Result: 关节检测模型准确率99%；在TSS预测上，ViT表现最佳，Huber loss=0.87，其他误差指标未详述。整体系统在外部测试上保持较好性能。

Conclusion: ARTSS可自动化、客观化RA TSS评分，处理关节缺失与可变序列长度，降低读者间/内变异、节省时间，潜在提高放射科与风湿科决策质量。

Abstract: Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van
Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming
and subjective. This study introduces an Automated Radiographic Sharp Scoring
(ARTSS) framework that leverages deep learning to analyze full-hand X-ray
images, aiming to reduce inter- and intra-observer variability. The research
uniquely accommodates patients with joint disappearance and variable-length
image sequences. We developed ARTSS using data from 970 patients, structured
into four stages: I) Image pre-processing and re-orientation using ResNet50,
II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and
IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,
EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance
with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute
error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS
from two radiologists was used as the ground truth. Model training employed
3-fold cross-validation, with each fold consisting of 452 training and 227
validation samples, and external testing included 291 unseen subjects. Our
joint identification model achieved 99% accuracy. The best-performing model,
ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results
demonstrate the potential of deep learning to automate RA scoring, which can
significantly enhance clinical practice. Our approach addresses the challenge
of joint disappearance and variable joint numbers, offers timesaving benefits,
reduces inter- and intra-reader variability, improves radiologist accuracy, and
aids rheumatologists in making more informed decisions.

</details>


### [155] [Matching Shapes Under Different Topologies: A Topology-Adaptive Deformation Guided Approach](https://arxiv.org/abs/2509.06862)
*Aymen Merrouche,Stefanie Wuhrer,Edmond Boyer*

Main category: cs.CV

TL;DR: 提出一种拓扑自适应的非刚性3D网格匹配方法，通过同时优化模板拓扑与对齐，在存在拓扑伪影和强非等距变形时仍能获得高质量双射对应，且无需数据驱动先验并优于部分训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有功能映射与ARAP类方法分别依赖近等距或ARAP变形假设，在多视几何重建等实际场景中常见的拓扑伪影（裂缝、粘连、洞等）会破坏这些假设，导致匹配失败。需要一种能适应拓扑变化、鲁棒于噪声与强形变的匹配框架。

Method: 提出拓扑自适应的变形模型：在ARAP与双射约束下，允许形状拓扑发生受控改变；通过联合优化一个具有合适拓扑的模板网格及其与目标形状的配准来提取对应关系。优化过程在模板空间中同时调整拓扑与几何，使其对不同输入形状均可对齐。方法不依赖数据驱动先验。

Result: 在高度非等距形状与含拓扑伪影（含噪声的逐帧多视重建）上取得优异的3D对齐质量。实验显示在无训练数据前提下仍能超过一些在大型数据集上训练的方法。

Conclusion: 通过引入拓扑自适应的ARAP式变形与联合模板-配准优化，可在存在拓扑问题的真实场景中稳健地进行非刚性网格匹配，获得高精度双射对应，并降低对数据驱动先验的依赖。

Abstract: Non-rigid 3D mesh matching is a critical step in computer vision and computer
graphics pipelines. We tackle matching meshes that contain topological
artefacts which can break the assumption made by current approaches. While
Functional Maps assume the deformation induced by the ground truth
correspondences to be near-isometric, ARAP-like deformation-guided approaches
assume the latter to be ARAP. Neither assumption holds in certain topological
configurations of the input shapes. We are motivated by real-world scenarios
such as per-frame multi-view reconstructions, often suffering from topological
artefacts. To this end, we propose a topology-adaptive deformation model
allowing changes in shape topology to align shape pairs under ARAP and
bijective association constraints. Using this model, we jointly optimise for a
template mesh with adequate topology and for its alignment with the shapes to
be matched to extract correspondences. We show that, while not relying on any
data-driven prior, our approach applies to highly non-isometric shapes and
shapes with topological artefacts, including noisy per-frame multi-view
reconstructions, even outperforming methods trained on large datasets in 3D
alignment quality.

</details>


### [156] [A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition](https://arxiv.org/abs/2509.06868)
*Behnoud Shafiezadeh,Amir Mashmool,Farshad Eshghi,Manoochehr Kelarestaghi*

Main category: cs.CV

TL;DR: 提出一种结合选择性Deblur-GAN预处理与YOLOv5的端到端ALPR管线，实现实时且高精度的车牌检测与字符识别，尤其在模糊场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR因场景多样性（模糊、光照、运动、角度等）导致检测与识别不稳，深度学习虽有效但在模糊输入下仍受限；需一种既高效实时又能鲁棒处理模糊车牌的方案。

Method: 在预处理阶段引入“选择性”的Deblur-GAN，仅对需要的图像进行去模糊，避免不必要的处理；主体检测与识别采用YOLOv5：先进行车牌检测（LPD），再进行字符分割与识别（CS+CR），形成轻量高效的流水线；基于自建并公开的伊朗车牌模糊与ALPR数据集进行训练与评测。

Result: YOLOv5在LP与CR阶段的检测时间均为0.026秒，支持实时；LPD与CR准确率分别达95%与97%；在存在模糊的情况下，加入Deblur-GAN预处理可将检测准确率提升近40%。

Conclusion: 该选择性Deblur-GAN+YOLOv5的ALPR方案在精度与速度上兼顾，适合便携与实时应用，尤其在模糊场景中显著提升整体效果；公开的数据集提升了研究复现与扩展的可能。

Abstract: Automatic License-Plate Recognition (ALPR) plays a pivotal role in
Intelligent Transportation Systems (ITS) as a fundamental element of Smart
Cities. However, due to its high variability, ALPR faces challenging issues
more efficiently addressed by deep learning techniques. In this paper, a
selective Generative Adversarial Network (GAN) is proposed for deblurring in
the preprocessing step, coupled with the state-of-the-art You-Only-Look-Once
(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and
the integrated Character Segmentation (CS) and Character Recognition (CR)
steps. The selective preprocessing bypasses unnecessary and sometimes
counter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high
accuracy and low computing cost. As a result, YOLOv5 achieves a detection time
of 0.026 seconds for both LP and CR detection stages, facilitating real-time
applications with exceptionally rapid responsiveness. Moreover, the proposed
model achieves accuracy rates of 95\% and 97\% in the LPD and CR detection
phases, respectively. Furthermore, the inclusion of the Deblur-GAN
pre-processor significantly improves detection accuracy by nearly 40\%,
especially when encountering blurred License Plates (LPs).To train and test the
learning components, we generated and publicly released our blur and ALPR
datasets (using Iranian license plates as a use-case), which are more
representative of close-to-real-life ad-hoc situations. The findings
demonstrate that employing the state-of-the-art YOLO model results in excellent
overall precision and detection time, making it well-suited for portable
applications. Additionally, integrating the Deblur-GAN model as a preliminary
processing step enhances the overall effectiveness of our comprehensive model,
particularly when confronted with blurred scenes captured by the camera as
input.

</details>


### [157] [Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers](https://arxiv.org/abs/2509.06885)
*Morteza Kiani Haftlang,Mohammadhossein Malmir,Foroutan Parand,Umberto Michelucci,Safouane El Ghazouali*

Main category: cs.CV

TL;DR: 提出一个轻量级、实时的二值医学图像分割网络：浅层Swin式编码器+U-Net式解码器，跳跃连接，先用Barlow Twins自监督预训练编码器，再端到端微调；在基准上以更少参数和更快推理达成可比精度。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net等卷积模型感受野有限，难以建模全局上下文；Transformer融合方案虽改善全局建模，但通常很深且计算昂贵，不适合实时与资源受限的临床部署。

Method: 设计一个端到端轻量级架构：Swin Transformer风格的浅层编码器结合U-Net式解码器，通过跳跃连接保留空间细节并获取上下文。为减少对标注数据的依赖，先用Barlow Twins自监督对编码器进行预训练以去冗余、学习有判别力表征，然后在目标二值分割任务上微调整个模型。

Result: 在二值分割基准上取得与现有方法相当的精度，同时显著降低参数量并加速推理，实现实时或近实时性能。

Conclusion: 该浅层、经自监督预训练的Swin+U-Net混合架构在精度、效率与部署友好性间取得良好平衡，是资源受限和实时临床场景的实用替代方案；代码已开源可复现。

Abstract: Medical image segmentation is a critical task in clinical workflows,
particularly for the detection and delineation of pathological regions. While
convolutional architectures like U-Net have become standard for such tasks,
their limited receptive field restricts global context modeling. Recent efforts
integrating transformers have addressed this, but often result in deep,
computationally expensive models unsuitable for real-time use. In this work, we
present a novel end-to-end lightweight architecture designed specifically for
real-time binary medical image segmentation. Our model combines a Swin
Transformer-like encoder with a U-Net-like decoder, connected via skip pathways
to preserve spatial detail while capturing contextual information. Unlike
existing designs such as Swin Transformer or U-Net, our architecture is
significantly shallower and competitively efficient. To improve the encoder's
ability to learn meaningful features without relying on large amounts of
labeled data, we first train it using Barlow Twins, a self-supervised learning
method that helps the model focus on important patterns by reducing unnecessary
repetition in the learned features. After this pretraining, we fine-tune the
entire model for our specific task. Experiments on benchmark binary
segmentation tasks demonstrate that our model achieves competitive accuracy
with substantially reduced parameter count and faster inference, positioning it
as a practical alternative for deployment in real-time and resource-limited
clinical environments. The code for our method is available at Github
repository: https://github.com/mkianih/Barlow-Swin.

</details>


### [158] [Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization](https://arxiv.org/abs/2509.06890)
*Minheng Chen,Youyong Kong*

Main category: cs.CV

TL;DR: 提出一种在非欧几里得球面特征空间进行相似度学习的2D/3D注册方法，并用SO(4)上的黎曼距离近似特征测地，同时在推理用可微LM优化，提升捕获范围、几何一致性与收敛速度，实验在真实与合成数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SE(3)测地的可微相似度学习虽扩展了注册捕获范围，但用欧氏空间近似会扭曲流形结构、造成相似度度量与优化收敛不佳。需要一种更符合流形几何的特征空间与距离度量，以更敏感地区分细微位姿差异，并提升优化效率。

Method: - 用CNN-Transformer编码器提取2D/3D特征嵌入；
- 将特征投影到球面（非欧几里得）空间；
- 以SO(4)的双不变黎曼距离近似特征间测地，构造几何一致的深度相似度度量；
- 训练端以端到端可微框架学习该相似度；
- 推理时用全可微Levenberg–Marquardt替代梯度下降，加速与稳健优化。

Result: 在真实与合成数据集、患者特异与非特异两种场景中，达到更高的注册精度与更快收敛，相比基线具有更强的捕获范围与对大扰动的鲁棒性（摘要未给出具体数字）。

Conclusion: 在球面特征空间结合SO(4)黎曼距离的相似度学习，加上可微LM优化，可在2D/3D注册中提供更几何一致、表达力更强的度量与更快的收敛，优于基于欧氏近似的方案。

Abstract: Intraoperative 2D/3D registration aligns preoperative 3D volumes with
real-time 2D radiographs, enabling accurate localization of instruments and
implants. A recent fully differentiable similarity learning framework
approximates geodesic distances on SE(3), expanding the capture range of
registration and mitigating the effects of substantial disturbances, but
existing Euclidean approximations distort manifold structure and slow
convergence. To address these limitations, we explore similarity learning in
non-Euclidean spherical feature spaces to better capture and fit complex
manifold structure. We extract feature embeddings using a CNN-Transformer
encoder, project them into spherical space, and approximate their geodesic
distances with Riemannian distances in the bi-invariant SO(4) space. This
enables a more expressive and geometrically consistent deep similarity metric,
enhancing the ability to distinguish subtle pose differences. During inference,
we replace gradient descent with fully differentiable Levenberg-Marquardt
optimization to accelerate convergence. Experiments on real and synthetic
datasets show superior accuracy in both patient-specific and patient-agnostic
scenarios.

</details>


### [159] [BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration](https://arxiv.org/abs/2509.06904)
*Cem Eteke,Alexander Griessel,Wolfgang Kellerer,Eckehard Steinbach*

Main category: cs.CV

TL;DR: 提出BIR-Adapter：一种面向扩散模型的低复杂度盲图像复原适配器，直接利用预训练大模型的先验与鲁棒性，从退化图像中自提特征并扩展自注意力，辅以采样引导减少幻觉，在多种退化与真实场景中以更低复杂度达到或优于SOTA，并可无缝集成到其他扩散模型与任务。


<details>
  <summary>Details</summary>
Motivation: 盲图像复原需在未知退化下恢复高质量图像，现有方法常依赖额外特征提取器、复杂训练或对特定退化的假设，带来高计算与工程成本且泛化不足。预训练大规模扩散模型具有强先验与鲁棒性，如何在不引入额外网络的情况下高效利用其能力用于盲复原，是本文要解决的问题。

Method: 设计BIR-Adapter作为轻量适配模块：1) 直接用预训练扩散模型从退化输入中抽取中间特征；2) 将这些退化特征用于扩展/调制自注意力（例如作为键/值或引导），从而在无需外部特征提取器的前提下增强模型对退化的感知；3) 提出采样阶段的引导机制，抑制扩散采样中的幻觉；4) 以插件式适配器形式接入不同的扩散骨干（包括仅做超分的模型），实现对未知复合退化的适配。

Result: 在合成与真实世界退化基准上，BIR-Adapter以显著更低的模型与计算复杂度，取得与现有SOTA相当或更优的复原质量；同时展示了将仅用于超分的扩散模型扩展到更广泛盲复原场景的能力，并在额外未知退化下表现更佳。

Conclusion: 无需训练额外特征提取器即可高效利用预训练扩散模型先验，BIR-Adapter通过特征驱动的自注意力扩展与采样引导有效降低幻觉，兼具性能与效率；其适配器式设计具备良好可移植性，可推广到多种扩散模型与图像复原任务。

Abstract: This paper introduces BIR-Adapter, a low-complexity blind image restoration
adapter for diffusion models. The BIR-Adapter enables the utilization of the
prior of pre-trained large-scale diffusion models on blind image restoration
without training any auxiliary feature extractor. We take advantage of the
robustness of pretrained models. We extract features from degraded images via
the model itself and extend the self-attention mechanism with these degraded
features. We introduce a sampling guidance mechanism to reduce hallucinations.
We perform experiments on synthetic and real-world degradations and demonstrate
that BIR-Adapter achieves competitive or better performance compared to
state-of-the-art methods while having significantly lower complexity.
Additionally, its adapter-based design enables integration into other diffusion
models, enabling broader applications in image restoration tasks. We showcase
this by extending a super-resolution-only model to perform better under
additional unknown degradations.

</details>


### [160] [FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data](https://arxiv.org/abs/2509.06907)
*Bing Han,Chen Zhu,Dong Han,Rui Yu,Songliang Cao,Jianhui Wu,Scott Chapman,Zijian Wang,Bangyou Zheng,Wei Guo,Marie Weiss,Benoit de Solan,Andreas Hund,Lukas Roth,Kirchgessner Norbert,Andrea Visioni,Yufeng Ge,Wenjuan Li,Alexis Comar,Dong Jiang,Dejun Han,Fred Baret,Yanfeng Ding,Hao Lu,Shouyang Liu*

Main category: cs.CV

TL;DR: 提出FoMo4Wheat：在超大规模小麦领域数据集ImAg4Wheat上自监督预训练的作物视觉基础模型，在10个田间视觉任务上显著优于通用域预训练模型，并具备跨作物/杂草可迁移性。


<details>
  <summary>Details</summary>
Motivation: 通用视觉主干在田间小麦感知中泛化差，原因是细粒度、形态多变的冠层结构与复杂多变的田间光照、背景、气候条件相互作用，导致任务迁移不稳。需要面向作物领域、特别是小麦的专用基础模型与大规模同域预训练数据来提升鲁棒性与跨任务泛化。

Method: 构建ImAg4Wheat：历时十年、覆盖30个全球站点、>2000基因型、>500环境条件、250万张高分辨率图像；在此数据上采用自监督预训练得到FoMo4Wheat表征；在冠层与器官层面的十个田间视觉下游任务上进行微调/评测，并与通用域预训练SOTA模型对比；评估跨作物与杂草任务的迁移能力；开放模型与数据与演示网站。

Result: FoMo4Wheat在十个田间任务上持续优于基于通用数据预训练的SOTA模型；学到的表示对小麦任务更鲁棒，同时能迁移到其他作物与杂草场景，显示良好的跨物种与跨任务能力。

Conclusion: 作物（小麦）领域专用的视觉基础模型能显著提升田间感知的可靠性与泛化，为构建面向多作物、多任务的通用作物基础模型提供了可行路径；FoMo4Wheat与ImAg4Wheat的开放将推动农业视觉研究与应用。

Abstract: Vision-driven field monitoring is central to digital agriculture, yet models
built on general-domain pretrained backbones often fail to generalize across
tasks, owing to the interaction of fine, variable canopy structures with
fluctuating field conditions. We present FoMo4Wheat, one of the first
crop-domain vision foundation model pretrained with self-supervision on
ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5
million high-resolution images collected over a decade at 30 global sites,
spanning >2,000 genotypes and >500 environmental conditions). This
wheat-specific pretraining yields representations that are robust for wheat and
transferable to other crops and weeds. Across ten in-field vision tasks at
canopy and organ levels, FoMo4Wheat models consistently outperform
state-of-the-art models pretrained on general-domain dataset. These results
demonstrate the value of crop-specific foundation models for reliable in-field
perception and chart a path toward a universal crop foundation model with
cross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat
dataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat
and https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:
https://fomo4wheat.phenix-lab.com/.

</details>


### [161] [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
*Wenxuan Huang,Shuang Chen,Zheyong Xie,Shaosheng Cao,Shixiang Tang,Yufan Shen,Qingyu Yin,Wenbo Hu,Xiaoman Wang,Yuntian Tang,Junbo Qiao,Yue Guo,Yao Hu,Zhenfei Yin,Philip Torr,Yu Cheng,Wanli Ouyang,Shaohui Lin*

Main category: cs.CV

TL;DR: 提出“交错推理生成”（IRG），在文本思考与图像合成之间迭代：先生成文本化思考指导初图，再基于图像反思细化；配套训练法IRGL与IRGL-300K数据，显著提升T2I的指令遵循与细节保真，在多基准上获SOTA。


<details>
  <summary>Details</summary>
Motivation: 统一多模态理解-生成模型图像质量提升明显，但在指令跟随与细节保真仍落后于像GPT-4o这类“理解-生成紧耦合”系统。受“交错推理”进展启发，作者想验证将推理过程显式引入T2I是否能弥补这两类能力缺口。

Method: 提出IRG框架：在生成过程中交替进行文本化思考与图像生成/反思。（1）先输出文本思考，生成初始图像；（2）对初图进行文本反思，提出细化要点；（3）据此生成改进图，兼顾语义保持、细节与美学。为训练该流程，提出IRGL：两大子目标—强化首阶段“想-生”以搭好核心内容与基质质量；训练高质量文本反思并让模型忠实落实反思到后续图像。构建IRGL-300K，含六种分解学习模式，覆盖“文本思考学习”和完整“思考-图像轨迹”。训练采用两阶段：先在统一基础模型上学会稳健思考与反思，再在全轨迹数据上高效微调整个IRG流水线。

Result: 在GenEval、WISE、TIIF、GenAI-Bench、OneIG-EN等基准上取得SOTA，绝对提升约5–10分；同时在视觉质量与细粒度一致性上有显著改进。

Conclusion: 显式交错“文本思考—图像生成—反思—再生成”的范式有效提升T2I的指令跟随与细节保真。IRGL训练策略与IRGL-300K数据支撑该流程的有效学习。代码、模型与数据将开源。

Abstract: Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

</details>


### [162] [H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers](https://arxiv.org/abs/2509.06956)
*Wenhao Li,Mengyuan Liu,Hong Liu,Pichao Wang,Shijian Lu,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出H2OT层级“沙漏”式令牌剪枝-恢复框架，用极少代表性姿态token在中间层计算并最终恢复全长序列，以大幅降算力同时保持3D人体姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 视频姿态Transformer效果好但算力和延迟高，不适合资源受限设备；观察到视频中冗余帧多，未必需要保持全时序全token。

Method: 设计可插拔的层级式Hourglass Tokenizer框架，含两模块：1) Token Pruning Module（TPM）在浅层动态选择少量代表性姿态token，逐步剪枝冗余帧；2) Token Recovering Module（TRM）在后续阶段基于被选token恢复细粒度时空信息，扩展回原始时间长度，实现快速推理。框架可用于seq2seq与seq2frame两类VPT，并支持不同的剪枝与恢复策略。

Result: 在多项3D人体姿态基准上验证，兼具高效率与高精度；中间层仅保留少量代表性token即可达成接近或优于全序列性能，同时显著降低计算量与延迟。

Conclusion: 全程保持完整姿态序列并非必要；通过H2OT的层级剪枝与恢复机制，可在不牺牲精度的前提下大幅提升视频3D姿态Transformer的推理效率，且方法通用、易集成。

Abstract: Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a hierarchical plug-and-play pruning-and-recovering
framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient
transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with
progressively pruning pose tokens of redundant frames and ends with recovering
full-length sequences, resulting in a few pose tokens in the intermediate
transformer blocks and thus improving the model efficiency. It works with two
key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module
(TRM). TPM dynamically selects a few representative tokens to eliminate the
redundancy of video frames, while TRM restores the detailed spatio-temporal
information based on the selected tokens, thereby expanding the network output
to the original full-length temporal resolution for fast inference. Our method
is general-purpose: it can be easily incorporated into common VPT models on
both seq2seq and seq2frame pipelines while effectively accommodating different
token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that
maintaining the full pose sequence is unnecessary, and a few pose tokens of
representative frames can achieve both high efficiency and estimation accuracy.
Extensive experiments on multiple benchmark datasets demonstrate both the
effectiveness and efficiency of the proposed method. Code and models are
available at https://github.com/NationalGAILab/HoT.

</details>
