<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld提出一种实时多模态4D世界建模框架，将视频生成、动态场景重建与长期记忆闭环融合，通过“生成-重建-引导”范式与分层规划(MMPL)及高效蒸馏(DMD)，实现长时程一致性与低延迟交互式合成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成虽画质出色，但缺乏实时交互、长时程一致性与对动态场景的持久记忆，难以作为实用世界模型使用。需要一种能统一生成、重建与记忆，并在计算预算内实现低延迟、长时间一致的方案。

Method: 提出TeleWorld闭环框架：1) 生成-重建-引导：将生成的视频持续重建为4D时空表征，反过来约束后续生成以保证空间、时间与物理一致性。2) 使用自回归扩散式视频模型支撑长时生成。3) 引入“Macro-from-Micro Planning (MMPL)”的层级规划，从帧级到片段级降低误差累积。4) 采用高效的Distribution Matching Distillation (DMD) 以在有限算力下实现实时合成。5) 在统一的4D框架中无缝整合动态物体与静态场景建模。

Result: 在多项实验中，系统在静态与动态世界理解、长时一致性以及实时生成效率方面取得强表现，能够进行互动、具备记忆并在较低计算成本下运行。

Conclusion: TeleWorld将视频生成、4D重建与世界记忆闭环结合，显著提升长程一致性与实时性，是迈向可交互、具记忆且计算可及的多模态世界模型与具身智能的重要一步。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 通过优化扩散起始噪声而非改动模型或大量重采样，缓解文本到图像生成的模式坍缩，提升多样性且保持保真度。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型对同一文本常生成相似图像，存在显著模式坍缩。既有方法依赖引导策略或大规模候选池+重排序，成本高或引入偏差。需要一种简单、高效、与基模兼容的提升多样性的手段。

Method: 提出将“初始噪声”作为优化变量：定义一个简单的噪声优化目标，在不修改模型参数的前提下，对不同噪声向量进行优化与搜索；并从频域分析噪声特性，设计/选择具有不同频谱分布的噪声初始化以促进优化与探索。

Result: 噪声优化在不损害生成质量（保真度）的前提下，显著提升样本多样性；频域差异化的噪声初始化进一步改善优化效率与搜索效果。在多项实验中优于基线（指导与候选池方法）在质量与多样性上的综合表现。

Conclusion: 通过面向噪声的轻量优化与频域感知初始化，可有效缓解T2I模式坍缩，兼顾质量与多样性，且无需改动底层模型或昂贵的重采样流程。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出Spatial4D-Bench，一个约4万问答、涵盖18任务的4D空间智能基准，用于系统评估MLLM在物体/场景/空间与时空关系与推理等方面的能力；实验显示多款SOTA模型在路径规划、动作识别、物理合理性等4D推理上仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有空间智能评测要么规模小、要么任务单一，难以全面刻画人类式“随时间变化的空间（4D）”智能；需要一个结构化、大规模、多任务的统一基准来衡量并推动MLLM在4D时空理解与推理上的进展。

Method: 构建Spatial4D-Bench：约4万对QA，覆盖18个明确定义的任务，并按六大认知范畴组织（物体理解、场景理解、空间关系、时空关系、空间推理、时空推理）；用该基准系统评测多种开源与商用MLLM并进行对比分析。

Result: 跨多种最先进MLLM的基准测试显示，其在多类4D空间推理任务上性能不足，尤其在路径规划、动作识别、物理可行性推理等方面存在显著短板。

Conclusion: Spatial4D-Bench为评估MLLM的4D空间智能提供了结构化、全面且大规模的基准；当前MLLM距离人类水平仍有差距，该基准与发现可为后续模型设计与训练提供方向，促进更强的4D空间智能发展。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: 提出SMAGNet，一种以SAR为主、可自适应融入部分缺失MSI数据的多模态深度学习模型，用于洪水后的水域范围制图；在不同MSI可用度下均优于现有方法，且在MSI缺失时性能与仅用SAR的U-Net相当，提升鲁棒性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现实时序中，洪峰期或刚过后往往缺乏及时光学(MSI)影像，单一SAR虽稳定但信息受限；现有多模态方法对“部分可用/缺失”的MSI自适应集成不足，制约了洪涝应急阶段的精确制图与决策支持。

Method: 提出Spatially Masked Adaptive Gated Network (SMAGNet)：以SAR为主干输入，设计空间掩膜与自适应门控的特征融合模块，在不同空间位置按MSI的可用性与可靠性动态调节MSI对特征的贡献；当MSI缺失时退化为仅SAR路径，确保稳健。以C2S-MS Floods数据集进行训练与评估，模拟不同MSI可用比例。

Result: 在C2S-MS Floods数据集上，在多种MSI可用度场景下，SMAGNet预测性能始终优于其他多模态深度模型；当MSI完全缺失时，其表现与仅用SAR训练的U-Net在统计上可比。

Conclusion: SMAGNet在洪涝水域提取中实现对不完整MSI的有效利用，并在MSI缺失时保持稳健，提升多模态方法在真实灾害响应中的适用性。

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [5] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 提出Compressed Map Priors（CMP），从历史通行数据学习空间先验，以极低存储和几乎零额外计算开销提升3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶视觉系统在绝大多数区域都曾被车辆多次经过，但现有方法往往将每次感知当作首次观测，未有效利用历史空间信息，导致感知不稳定与资源浪费。

Method: 构建一种二值化哈希映射的压缩地图先验，从历史轨迹/遍历中学习空间占据与目标分布信息。该先验以约32KB/km^2的存储密度（较稠密存储压缩约20倍）保存，并可无缝集成到主流3D感知/检测架构中，几乎不增加计算成本。

Result: 在nuScenes数据集上，CMP作为插件式先验在多种3D检测模型上带来显著且一致的性能提升。

Conclusion: 利用轻量级压缩地图先验，可在不显著增加存储与计算的情况下稳健提升自动驾驶3D感知性能，验证了历史空间先验的实用价值。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [6] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: 提出GLASS架构，通过全局缩放视图+原始分辨率局部裁剪并以注意力汇聚，提高AI生成图像检测性能，兼顾精度与计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测模型常需下采样以适配固定输入，导致细粒度伪迹丢失；而仅用高分辨率输入又计算代价过高。需要一种既保留关键局部细节又控制计算的通用框架。

Method: 设计GLASS：1) 输入同时包含一个全局缩放视图（提供语义与布局）和多块从原图中“分层（分区）随机采样”的高分辨率局部裁剪；2) 使用注意力/打分机制对局部裁剪进行重要性聚合，与全局特征融合；3) 该前端可无缝接入ViT/ResNet/ConvNeXt等主干；4) 在给定计算预算下通过裁剪数量与大小实现可伸缩；

Result: 在多种主干与数据集上，相对标准迁移学习基线取得更高的检测准确率/ROC-AUC，并在可接受的计算量内运行；局部原分辨率裁剪+分层采样对高分辨率伪迹尤为有效。

Conclusion: 结合全局与原分辨率局部信息、用注意力进行裁剪级别聚合的GLASS，是检测高保真AI生成图像的有效通用模块，能在不同骨干与图像尺寸下稳定提升性能与效率。

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [7] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: 提出FCMBench-V1.0，一个面向金融信贷场景的多模态基准，覆盖感知、推理与鲁棒性三大维度，含4043张合规图像与8446个QA，用闭环合成-采集流程保障隐私与逼真；在23个VLM上评测，展示显著性能差异与在采集伪影下的鲁棒性下降。


<details>
  <summary>Details</summary>
Motivation: 多模态AI在信贷风控与文档审核中快速应用，但缺乏同时满足金融场景特性、信贷任务理解、真实世界鲁棒性与隐私合规的专用基准。需要一个能公平比较模型、避免训练数据泄漏、又具实用性的评测框架。

Method: 构建FCMBench：覆盖18类核心证件；三维度评测框架——感知（3个基础任务）、推理（4个面向决策的信贷推理任务）、鲁棒性（10类真实采集伪影压力测试）。采用闭环“模板合成+场景化实拍”的数据生产流程，使用虚拟内容与自有采集，规避网络公开图像带来的泄漏与合规问题。

Result: 基准含4043张隐私合规图像与8446个QA。对23个来自14家机构的SOTA VLM做广泛实验：商业模型中Gemini 3 Pro F1为64.61%，开源基线中Qwen3-VL-235B为57.27%，作者的领域专用模型Qfin-VL-Instruct总体最佳，F1为64.92%。

Conclusion: FCMBench能有效区分不同VLM在信贷文档上的能力与鲁棒性；即便顶尖模型在采集伪影下也出现显著性能下降，提示在真实部署中需重视鲁棒性与领域适配。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [8] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 提出FaceFocalDesc任务与数据集，并基于Qwen2.5-VL微调出Focal-RegionFace，可在任意脸部区域生成与识别多属性自然语言描述（AUs/情绪/年龄），通过多阶段逐步聚焦实现可解释、强性能的细粒度面部分析，在新基准与新旧指标上均SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有面部分析多聚焦整脸或单一属性，缺乏对任意局部区域的细粒度、多属性、可解释的自然语言描述与识别能力；局部聚焦可提升理解与可控性，因此需要新任务、数据与方法。

Method: 1) 构建FaceFocalDesc数据集：为任意选取的面部区域提供区域级多属性标注与自然语言描述（含FAUs、情绪、年龄）。2) 基于Qwen2.5-VL提出Focal-RegionFace：采用多阶段逐步微调策略，使模型逐渐强化对局部特征的注意与表征，从而生成/识别可解释的多属性描述。

Result: 在新建立的基准上，Focal-RegionFace在传统与广泛使用的指标以及新提出的指标上均取得最佳表现，优于现有方法。

Conclusion: 面向任意脸部区域的多属性自然语言描述与识别是可行且有效的；通过逐步聚焦微调的视觉语言模型能实现可解释的年龄估计、FAU与情绪检测，并在细粒度面部区域分析场景中具有通用性与优势。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [9] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

TL;DR: 提出DichroGAN，用多生成器cGAN结合水下成像物理，利用高光谱信息分离漫反射/镜面反射并估计传输，恢复海底“空气中颜色”，在PRISMA数据上训练并在卫星/水下数据上取得与SOTA相当的表现。


<details>
  <summary>Details</summary>
Motivation: 卫星观测海底颜色受水柱中光的指数衰减与散射影响严重，传统方法难以准确恢复“空气中”真实反射颜色；需要将物理先验与数据驱动方法结合，利用高光谱信息提升去水体退化与反射分离的精度。

Method: 构建条件GAN——DichroGAN，包含四个生成器与判别器：两生成器从高光谱立方体估计漫反射与镜面反射，得出大气场景辐亮度；第三生成器以该辐亮度（包含各波段特征）为输入恢复目标颜色；第四生成器估计水下光传输。各模块依据水下成像方程协同训练。使用从PRISMA卫星构建的小型数据集（RGB、对应光谱带与掩膜）进行端到端训练。

Result: 在卫星和水下图像数据集上进行广泛实验，DichroGAN在色彩恢复与清晰度等指标上达到与当前最优方法相当的性能，展示竞争力。

Conclusion: 多生成器cGAN结合物理约束和高光谱信息可有效补偿水下吸收与散射，恢复海底在空气中的真实颜色；方法在紧凑数据集上亦具鲁棒性，具有推广潜力。

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

</details>


### [10] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 提出MorphAny3D：无需训练、基于结构化潜表示（SLAT）的3D形变框架，通过在生成器注意力中融合源/目标特征与时间信息，实现跨类别、语义一致且时间平滑的形变。


<details>
  <summary>Details</summary>
Motivation: 现有3D morphing难以在不同类别之间保持语义对齐与时间平滑，并常需昂贵训练或先验。作者希望利用已有SLAT生成器，在不重新训练的前提下，获得高质量、跨品类、时序一致的形变序列与更多应用。

Method: 核心思想：在3D生成器的注意力层中结构化地混合源与目标的SLAT特征。1）Morphing Cross-Attention（MCA）：在交叉注意力中融合源/目标信息，确保结构一致性与语义对齐；2）Temporal-Fused Self-Attention（TFSA）：自注意力引入前一帧特征，提升时序连贯；3）姿态/朝向纠正策略，缓解形变步骤中的姿态歧义。整个流程为训练自由，可适配多种SLAT类生成模型。

Result: 大量实验显示，对同类与跨类别对象均可生成SOTA级别的平滑、语义一致的3D形变序列；还能实现解耦形变与3D风格迁移等高级应用，并能推广到其他SLAT生成器。

Conclusion: 通过在注意力机制中结构化融合源/目标与时间特征，MorphAny3D在无需训练的前提下实现高质量、跨品类、时间平滑的3D morphing，具备良好通用性与扩展应用潜力。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [11] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出一种基于NeRF与3D实例分割的多视角作物精确计数框架，利用可见性与掩码一致性评分在3D中稳健分割作物，实现对棉铃、苹果、梨的高精度计数且无需作物特定参数调优，并发布棉花数据集。


<details>
  <summary>Details</summary>
Motivation: 田间环境中作物彼此遮挡、聚簇紧密且视角变化大，导致2D图像分割难以区分个体并准确计数；现有方法往往依赖作物特定调参，泛化与稳健性不足。

Method: 从多视角采集2D图像，进行独立实例掩码并与NeRF视图合成关联；在NeRF的3D表示上引入作物可见性与掩码一致性评分，将多视角实例信息融合到3D空间进行实例级分割，从而进行精确计数；方法设计去除作物特定超参依赖。

Result: 在三套农业数据集（棉铃、苹果、梨）上验证，面对颜色、形状、大小显著差异仍获得一致且高精度的计数结果；与SOTA相比在作物计数任务上表现更优。

Conclusion: 基于NeRF与3D实例分割的框架能有效解决遮挡与聚簇带来的计数难题，具备跨作物的泛化能力与稳定性；同时发布棉花数据集以促进后续研究。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [12] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: 提出IntraStyler：一种基于示例图像的风格合成方法，在无先验的情况下学习并注入丰富的同域（目标域）风格，以改进无监督域适应的图像级对齐与下游分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有UDA多做源-目标域对齐，忽视了目标域内部风格多样性；现有多样化合成方法常需预先定义风格变化因素，实际不便。需要一种无需先验即可捕获并控制目标域内风格多样性的方式。

Method: 构建IntraStyler：以任意目标域示例图像作为风格引导，进行图像翻译，使输出风格匹配示例风格。引入风格编码器，借助对比学习学习“仅风格”特征，从而可控且可分辨地抽取多样风格；在CrossMoDA 2023跨模态场景中进行训练与评估。

Result: 在CrossMoDA 2023数据集上，方法实现可控的风格合成，并通过生成多样化的合成目标域数据，提升下游分割任务性能；实验体现方法有效性。

Conclusion: 无需预设风格先验即可利用示例图像实现目标域内多样风格合成，既增强合成数据的风格覆盖，又改善UDA下游分割效果；代码已开源。

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [13] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 论文提出用奖励驱动的强化学习，让开源多模态大模型在视觉推理任务中显式生成更长、更结构化且真正利用图像的信息，取得稳定提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在“先推理再回答”的范式中，生成的思维链往往未有效融入视觉信息，导致在需要精确视觉感知（如视觉谜题）的任务上表现欠佳。作者实证发现视觉感知是瓶颈：将图像转换为文本描述即可显著提升闭源模型性能，说明视觉信息未被充分利用。

Method: 提出在开源MLLM（如 Qwen-2.5-VL-7B）上使用基于奖励的RL训练，采用GRPO优化，设计六类奖励函数，分别针对：1) 图像理解质量；2) 思维步骤长度与结构化程度；3) 最终答案准确性；并通过奖励塑形抑制绕开图像的捷径，显式鼓励长链条视觉推理。

Result: 在Qwen-2.5-VL-7B上，相比基座模型总体提升5.56%，且在域内与跨域评测上均有一致增益。另证据显示，将图像先转为文本描述可使Claude 3.5与3.7分别提升26.7%与23.6%。

Conclusion: 视觉感知不足是当前MLLM长链推理的关键障碍。通过奖励驱动的RL与精心设计的多维奖励，可显著促进模型在视觉信息上的依赖与长程推理能力，带来稳健性能提升且无需昂贵监督。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [14] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: 提出LooC：用低维可组分的码本实现高容量且紧凑的向量量化，并通过插值式外推平滑增强特征，充分利用码本、避免坍塌，作为即插即用模块在多任务上以更小码本达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统VQ在数据与模型多样性增长下需要更高表达能力，但增大码本会带来参数/存储/训练负担与码本利用不足（坍塌）问题，迫切需要同时“高容量”和“更紧凑”的量化方案。

Method: 1）重构码向量与特征的对应关系：将码向量视为低维组成单元在特征内进行组合（compositional），以小码本覆盖更大解空间；2）引入无需参数的“由插值实现的外推”机制，在量化过程中增强与平滑特征，提升细节保真与稳定性；3）设计保证码本全量使用，避免坍塌；4）作为即插即用模块适配各种VQ下游任务与架构。

Result: 在多任务、多数据集、多架构上系统评测，LooC以显著更小的码本取得优于现有VQ方法的表现，达到SOTA。

Conclusion: 低维可组合码本加上参数自由的插值外推，使LooC在保持紧凑的同时提供高容量与稳定训练，广泛适配并提升VQ相关任务表现。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [15] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: 提出SynDR-IQA，通过重塑合成数据分布提升无参考图像质量评估的跨数据集泛化，核心为“多样化上采样+冗余簇下采样”，缓解特征离散聚类导致的回归性能受限。


<details>
  <summary>Details</summary>
Motivation: 深度学习BIQA受限于大规模标注数据稀缺。直接用合成失真数据训练的模型在跨数据集（尤其真实失真）上泛化差。作者观察到合成数据训练得到的表征呈离散簇：高质量靠近参考图，低质量按失真类型聚成簇，妨碍连续质量回归。这种问题来自数据分布而非模型结构，需要从数据层面重塑分布。

Method: 提出SynDR-IQA框架，基于对“样本多样性与冗余度”对泛化误差影响的理论推导，设计两种数据重塑策略：1) 分布感知的多样内容上采样（distribution-aware diverse content upsampling），在保持内容分布的同时增加视觉多样性；2) 密度感知的冗余簇下采样（density-aware redundant cluster downsampling），对密集聚簇区域降采样以平衡样本密度，减轻按失真类型聚类带来的偏置。整体目标是让特征空间从离散簇转向更连续、可回归的分布。

Result: 在三种跨数据集设置下（合成→真实、合成→算法生成、合成→合成）进行大量实验，SynDR-IQA显著提升泛化性能，优于现有使用合成数据训练的BIQA方法。

Conclusion: 合成数据导致的离散聚类分布是限制BIQA回归与泛化的关键因素。通过分布重塑（多样化上采样与冗余下采样），可有效缓解该问题并提升跨数据集表现。代码已开源。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [16] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 提出一种结合CycleGAN与YOLOv8的跨模态数据增强框架，将可见光PCB图像无配对地翻译为红外伪样本，并与少量真实IR数据联合训练，从而在低样本条件下显著提升缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业检测中IR数据采集昂贵且稀缺，导致基于IR的PCB缺陷检测难以充分训练；传统配对监督方法受限于配对数据难获取，亟需一种无需配对、能充分利用丰富可见光数据来提升IR检测的方案。

Method: 利用CycleGAN进行无配对图像到图像翻译，将大量可见光PCB图像映射到IR域，生成保留缺陷结构语义且逼真热分布的伪IR样本；再将伪IR与少量真实IR样本构成异质训练集，训练轻量化YOLOv8检测器，实现跨模态数据增强与目标检测融合。

Result: 在低数据设置下，使用伪IR增强的数据训练的YOLOv8显著优于仅用少量真实IR数据的基线，且接近全监督（充足IR数据）性能基准，表明伪IR合成有效提升特征学习与检测精度。

Conclusion: 跨模态伪IR合成是有效的工业视觉增强策略；结合CycleGAN与YOLOv8可在IR数据稀缺场景下接近全监督效果，为PCB缺陷检测提供了实用与可扩展的解决方案。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [17] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: 提出一套在低算力设备上运行的轻量级虫害检测与农药推荐框架，结合轻量CNN与原型式元学习，在小样本下仍能高准确识别，并依据作物与生育期等环境因素给出更安全、环保的用药建议；在整合构建的多源虫害图像数据集上，精度接近SOTA且计算成本显著降低，适用于实时精准农业场景。


<details>
  <summary>Details</summary>
Motivation: 传统虫害管理依赖人工巡田与化学农药，成本高、效率低、对环境有负面影响；小农户与边缘地区缺乏高性能设备与充足标注数据，亟需能在手机、无人机等低资源平台运行、在小样本条件下也可靠的智能虫害识别与用药决策系统。

Method: 1) 轻量级CNN主干，配合原型式元学习，实现小样本虫害识别；2) 决策支持模块融合作物类型与生育期等环境因子，生成安全、环保的农药推荐；3) 构建并整合多公共数据集，覆盖视角、目标尺度、背景多样性，用于训练与评测。

Result: 在整合数据集上，所提轻量CNN以显著更低计算复杂度达到与SOTA相近的识别精度；决策支持系统在实验中表现出减少对传统化学农药依赖、促进可持续实践的潜力，并满足实时应用需求。

Conclusion: 该框架兼顾准确性与算力友好，适配移动与无人机平台，能在小样本下可靠检测虫害并提供环境友好型用药建议，具备在精准农业中落地与推广的可行性。

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [18] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: TotalFM提出一种面向放射学的3D-CT基础模型，在器官分割与语言配对的框架下，用自监督与对比学习高效对齐体数据与文本，零样本任务显著优于或可比现有方法，并展示出临床可泛化性。


<details>
  <summary>Details</summary>
Motivation: 3D-CT体数据训练计算开销大，且通用VLM在医疗语境中难以高效对齐细粒度器官/病灶语义；需要一种既计算可承受、又能支撑临床多任务（检出、分类、报告生成）的基础模型方案。

Method: 提出TotalFM：1）器官分离思想：通过自动分割生成器官体积并与报告中器官相关语句进行配对；2）大规模数据（14万序列）自动构建体-文本对，文本由LLM处理报告抽取“发现-器官”句；3）两阶段训练：VideoMAE式自监督预训练以学习体表征；随后采用体-文本对进行对比学习实现跨模态对齐；4）以器官为粒度的学习与评测，兼顾效率与表达力。

Result: 零样本器官级病灶分类中，较CT-CLIP在83%(5/6)器官F1更高，较Merlin在64%(9/14)器官F1更高；零样本发现级分类在83%(25/30)类别AUROC高于Merlin；在报告生成任务上与现有VLM性能相当。

Conclusion: 器官分离的学习框架能在保证计算效率的同时提升3D-CT多任务泛化性能，为3D放射学基础模型的实用化提供有效设计指南。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [19] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: 提出S1-MMAlign：一个涵盖多学科、1550万对科学图像-文本的大规模多模态数据集，并通过Qwen-VL驱动的语义增强重述显著提升图文对齐与语义清晰度。


<details>
  <summary>Details</summary>
Motivation: 通用多模态方法在科学发现受限：科学图像复杂而文本描述稀疏，原始论文图注常存在语义弱对齐与歧义，阻碍跨模态推理与AI for Science应用。

Method: 构建来自250万开放获取论文的跨学科数据集，涵盖实验装置、热图、显微图等多类视觉形态；设计AI-ready语义增强管线，利用Qwen-VL系列结合论文摘要与引文上下文对图像进行重述（recaption）；用SciBERT伪困惑度评估语义歧义、用CLIP分数评估图文对齐。

Result: 增强后数据显著提升：SciBERT伪困惑度表明语义歧义下降；CLIP对齐分数提升18.21%。

Conclusion: S1-MMAlign作为高质、跨学科的科学图文数据基座，有望推动科学推理与跨模态理解；数据已公开发布于Hugging Face。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [20] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: 提出ActErase：一种无需训练的概念消除方法，通过前向传播时动态替换激活来高效移除敏感概念，同时保持扩散模型的生成能力，并在裸体、风格与对象移除任务上达SOTA且具对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成能力强但存在安全、版权与伦理风险；现有概念消除多依赖昂贵的微调，数据与算力成本高，且可能损伤通用生成质量。作者观察到模型激活以通用概念为主，目标概念仅占很小成分，启发了更轻量的消除策略。

Method: 训练无关（training-free）的ActErase：1) 通过成对提示（含/不含目标概念）分析定位激活差异区域；2) 从中提取目标概念相关激活；3) 在推理前向过程中动态替换输入激活以抑制目标概念。该流程可即插即用，无需微调与额外数据。

Result: 在三类关键任务（裸体、艺术风格、对象移除）上实现SOTA消除效果，显著降低目标概念生成，同时较好保持整体生成质量，并在对抗攻击场景下表现稳健。

Conclusion: ActErase以训练免、轻量、可插拔的方式实现有效的概念操控与消除，为扩散模型的安全部署提供高效实用的解决方案。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [21] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

TL;DR: 提出FaithSCAN，通过利用VLM内部信号（解码不确定性、中间视觉表示、跨模态对齐）高效检测VQA幻觉，配合自动化模型依赖监督，较现有方法更有效且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有VQA幻觉检测要么依赖外部验证（开销大、受外部资源质量限制），要么基于不确定性（只覆盖有限失效面，忽略丰富内部信号），在效率、鲁棒性和性能上均受限。

Method: 设计轻量级检测网络FaithSCAN，提取并融合VLM内部多源信号：1）词元级解码不确定性；2）中间视觉表征；3）跨模态对齐特征。采用分支证据编码与不确定性感知注意力进行特征融合。并将“LLM-as-a-Judge”扩展到VQA幻觉，提出低成本自动生成与模型相关的监督信号，实现无需人工标注的监督训练。

Result: 在多个VQA基准上显著优于现有方法，兼具更高的检测效果与效率。消融与深入分析表明不同内部信号提供互补诊断信息。

Conclusion: VQA幻觉源于视觉感知、跨模态推理与语言解码的系统性内部状态变化。利用VLM内部多维信号并进行不确定性感知融合，可高效、鲁棒地检测幻觉；不同架构呈现不同幻觉模式，为理解多模态幻觉机理提供新视角。

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [22] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 提出DUAL：一种基于证据深度学习的模型无关不确定性感知框架，将预测不确定性分解为认知不确定性(EU)与偶然不确定性(AU)，分别用于尾部样本重加权与自适应标签平滑抑噪，在遥感长尾分布任务上优于TGN、SADE等强基线。


<details>
  <summary>Details</summary>
Motivation: 遥感场景中长尾分布普遍存在，尾部类别样本稀少且常伴随噪声与歧义。现有做法通常把所有低置信样本一视同仁，导致对含噪样本过拟合。需要一种方法区分“难而稀缺”的尾部样本与“噪声/模糊”的样本，分别处理。

Method: 基于Evidential Deep Learning对每个样本的预测不确定性进行动态分解：将认知不确定性(EU)作为样本稀缺与可学性指标，用于对尾部难样本进行重加权训练；将偶然不确定性(AU)作为数据噪声/歧义度量，用自适应标签平滑机制减小其损害。该框架与骨干无关，可接入多种模型与数据集。

Result: 在多数据集与多种骨干网络上进行实验，DUAL性能稳定提升，超过TGN、SADE等强基线；消融实验验证EU用于重加权与AU驱动的自适应标签平滑等设计选择的关键性。

Conclusion: 通过将不确定性拆分为EU与AU并分别针对性处理，DUAL有效缓解长尾学习中“难样本”与“噪声样本”混杂的问题，提升泛化与鲁棒性，适用于多种遥感长尾识别场景。

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [23] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: 提出SV-GS：在时间与视角稀疏观测下，同时估计对象骨架驱动的形变与随时间变化的运动，实现平滑插值与细节保持，较现有方法在合成数据上PSNR最高提升34%，并可用扩散先验替代初始静态重建，提升实用性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中常见的是来自稀疏时间点和分散视角（如安防摄像头）的观测，传统动态重建需要密集多视视频，难以部署；因此需要一种在稀疏观测下仍能稳定重建动态目标的方法。

Method: 以粗骨架图与初始静态重建为先验（后可被扩散生成先验替代），优化一个骨架驱动的形变场：包含时间相关的关节姿态估计器（coarse）和时间不变的细粒度形变模块（fine）。仅令关节姿态随时间变化，实现运动插值同时保持几何细节；联合估计运动与形变。

Result: 在合成数据上，相比现有稀疏观测方法PSNR最高提升34%；在真实数据上，即使用显著更少帧，也能达到与密集单目视频方法相当的重建质量。

Conclusion: SV-GS在时空稀疏观测下实现高质量动态重建；骨架驱动+时间解耦带来平滑插值与细节保留；用扩散先验替代初始静态重建提升了现实可用性。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [24] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 基于Swin Transformer的皮肤病图像分类模型，经公开数据预训练与数据增强优化，在ISIC2019的8类任务上达到87.71%准确率，展示了作为临床辅助与患者自评工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 皮肤病患病率上升而皮科医生稀缺，临床需要自动化、可靠的图像诊断辅助工具以提升诊断及时性与准确性，减轻医生负担并赋能患者自我评估。

Method: 采用深度学习图像分类方案：利用公开皮肤病图像数据进行预训练以获取通用视觉特征，选用Swin Transformer作为主干；在项目过程中迭代优化网络结构、数据预处理流程，并使用有针对性的数据增强以缓解类间/类内差异与数据不平衡问题，最终在ISIC2019八分类任务上训练与评估。

Result: 最终模型（Swin Transformer）在ISIC2019八类皮损分类任务上取得87.71%的准确率，表现优于基础设置；能较准确地区分多种皮肤病变类型。

Conclusion: 该方法具备作为临床决策支持与患者自我评估工具的潜力；在有限专家资源背景下可提升诊断效率与准确性，后续可在更大规模、多中心与真实世界数据上验证并优化。

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [25] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor 是一种面向草图的视频上色扩散模型，能接收数量可变、异质的参考（角色设定稿、背景图、任意上色帧），通过显式的“每参考—区域分配”和把参考编码成时间拼接的潜在帧，在不增加参数的前提下并行利用多参考；结合时空对应掩蔽注意力与模态分离的 RoPE 索引，抑制捷径学习与跨身份调色泄漏，在 SAKUGA-42M 的单/多参考设置下提升色彩保真、身份一致与时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视频上色多仅依赖单一参考（多为首帧），忽视了动画制作中常见的多源参考（角色表、背景、其他彩色帧），导致颜色不稳定、身份漂移与参考利用不足。需要一个能高效整合多参考且稳固绑定主体—参考关系的模型。

Method: 1) 参考编码为额外“潜在帧”，在时间维度与视频潜在序列拼接，使扩散每步可并行处理所有参考且无需增参；2) 显式的按参考—区域分配，指定每个参考影响的空间区域；3) 时空对应掩蔽注意力（correspondence-masked attention），仅在匹配的时空位置允许注意力交互，强化主体—参考绑定；4) 模态互斥的 RoPE 索引（对参考与视频采用不同位置编码通道），降低参考与目标序列的混淆；整体用于草图到彩色视频的扩散框架。

Result: 在 SAKUGA-42M 数据集的单参考与多参考协议下，相比现有基线，TimeColor 在色彩保真度、身份一致性与时间稳定性三方面均取得显著提升。

Conclusion: 通过将多参考作为时间拼接潜在帧、结合时空掩蔽注意与模态分离编码，TimeColor 能在不增加参数的情况下有效利用异质多参考，显著缓解调色泄漏与身份漂移，提升动画草图视频上色质量。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [26] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: 提出VisNet，一种高效低算力的人体重识别模型，在Market-1501上达Rank-1 87.05%、mAP 77.65%，仅32.41M参数、4.601 GFLOPs，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有ReID方法准确率高但计算开销大，不利于监控与移动端等资源受限场景，需要在精度与效率间取得更好平衡的模型。

Method: 构建VisNet：1) 多尺度特征融合——融合ResNet50的stage1-4，且在各尺度上引入自动注意力；2) 语义聚类结合解剖式身体分区——通过基于规则的伪标签施加空间约束；3) 动态权重平均(DWA)平衡分类语义正则与其他目标；4) 采用FIDI损失以强化度量学习。整体不使用并行分支，保持计算简洁。

Result: 在Market-1501上达成Rank-1 87.05%、mAP 77.65%，模型规模32.41M参数、计算量4.601 GFLOPs。

Conclusion: VisNet在保持较高精度的同时显著降低计算成本，适合监控与移动端的实时人重识别部署；多尺度注意融合、语义聚类与FIDI损失的组合是性能与效率提升的关键。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [27] [ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition](https://arxiv.org/abs/2601.00311)
*Feng-Qi Cui,Jinyang Huang,Sirui Zhao,Jinglong Guo,Qifan Cai,Xin Yan,Zhi Liu*

Main category: cs.CV

TL;DR: 提出一种用于视频行为识别的数据增强策略ReMA，通过受控的“混合替换”在保持类内稳定性的同时扩展表征，提升跨时空尺度的鲁棒性和泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视频增强多为无约束扰动，易引入与判别无关的变化，导致类内分布被削弱、表征漂移，且不同时间尺度收益不一致，需要一种既能扩展数据又能维护类条件稳定性的增强方法。

Method: ReMA为即插即用的表示感知混合增强，核心包含两机制：1）表示对齐机制（RAM）：在分布对齐约束下进行结构化的类内混合，抑制无关漂移并提高统计可靠性；2）动态选择机制（DSM）：生成基于运动的时空掩码，定位扰动位置，避开判别敏感区域并保持时间连贯。二者共同控制“如何混合”和“在哪里混合”，无额外监督或可训练参数。

Result: 在多种视频行为识别基准上，ReMA在不同时空粒度上都能稳定提升泛化与鲁棒性。

Conclusion: 受控、表示感知的混合替换可在不增加训练复杂度的前提下，有效增强视频表征的稳定性与判别性，适用于广泛的视频识别任务。

Abstract: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

</details>


### [28] [Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation](https://arxiv.org/abs/2601.00322)
*Siyan Fang,Long Peng,Yuntao Wang,Ruonan Wei,Yuehuan Wang*

Main category: cs.CV

TL;DR: 提出DMDNet用于图像反射分离，通过深度引导的状态空间建模与记忆补偿，显著提升昼/夜场景中透射与反射的解耦，并发布夜间数据集NightIRS。


<details>
  <summary>Details</summary>
Motivation: 单幅图像反射分离在透射与反射对比度相近时容易混淆，夜间更严重；现有方法缺乏稳健的结构线索与跨图像记忆支撑。

Method: 1) 深度感知扫描DAScan，引导Mamba沿语义一致方向传播信息以构建稳定状态；2) 深度协同状态空间模型DS-SSM，用深度调制状态激活敏感度，抑制含糊特征扩散；3) 记忆专家补偿MECM，利用跨图像历史知识提供面向层的补偿；4) 构建夜间反射分离数据集NightIRS。

Result: 在昼夜数据上广泛实验，DMDNet在定量与定性指标上均优于现有SOTA；对夜间场景尤其鲁棒。

Conclusion: 深度引导的状态空间建模与跨图像记忆可有效提升反射/透射解耦能力；DMDNet与NightIRS为夜间反射分离提供了新的基线与数据支持。

Abstract: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.

</details>


### [29] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

TL;DR: 提出 HarmoniAD：一种频域引导的双分支异常检测框架，将CLIP特征变换到频域后拆分高/低频，分别用FSAM强化细节、GSCM建模全局语义，在MVTec-AD、VisA、BTAD上达SOTA，兼具灵敏度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 工业质检中微小缺陷漏检风险高；结构导向方法对噪声敏感、语义导向方法易忽视细节，二者存在结构-语义权衡，需要兼顾细粒度结构与全局语义的统一方法。

Method: 以CLIP图像编码器提取特征并进行频域变换，按频率解耦为高频与低频两路：高频分支引入细粒度结构注意力模块(FSAM)增强纹理与边缘以捕捉小异常；低频分支采用全局结构上下文模块(GSCM)建模长程依赖以保持语义一致性；并采用多类别联合训练策略。

Result: 在MVTec-AD、VisA、BTAD数据集上达到最新水平（SOTA），表现出更高的灵敏度和更强的鲁棒性。

Conclusion: 频域引导的双分支解耦与FSAM/GSCM的互补建模有效平衡细节与语义，实现对微小缺陷的敏感检测与稳健泛化，适用于多类别工业质检场景。

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [30] [Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion](https://arxiv.org/abs/2601.00328)
*Yingzhi Tang,Qijian Zhang,Junhui Hou*

Main category: cs.CV

TL;DR: JGA-LBD提出将单张RGB图重建3D数字人几何与外观统一到一个联合潜变量，并用桥扩散从部分潜码补全缺失，最终同时恢复高保真几何与外观，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张图像重建3D人存在几何与外观解耦训练导致不一致与统一重建困难；异构条件（深度、SMPL等）直接融合训练不稳、难优化。需要一种能统一条件表征并一致生成几何与外观的框架。

Method: 1) 将所有条件统一为3D Gaussian表征；2) 用共享稀疏VAE压缩到统一潜空间，得到联合几何+外观潜码；3) 采用专门的桥扩散，从部分观测的目标潜码出发，仅推断缺失部分；4) 通过专用解码模块从潜码解码出完整3D人体几何并渲染新视角。

Result: 在基准与复杂野外场景上，相比SOTA在几何保真与外观质量均有提升；实现单图驱动的高一致性重建与新视角渲染。

Conclusion: 统一的3D Gaussian条件与联合潜空间配合桥扩散可稳定且一致地重建3D数字人的几何与外观，解决了异构条件融合与解耦带来的不一致问题，并在实际场景中表现优越。

Abstract: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.

</details>


### [31] [Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation](https://arxiv.org/abs/2601.00344)
*Bruce Mugizi,Sudi Murindanyi,Olivia Nakacwa,Andrew Katumba*

Main category: cs.CV

TL;DR: 提出一套面向资源受限地区（如乌干达）的实时交通监控系统，集成车辆检测、车牌识别与速度估计；YOLOv8车牌检测mAP 97.9%，Transformer字符识别CER 1.79%，速度估计误差约±10 km/h，并联通数据库与短信API实现自动罚单发放。


<details>
  <summary>Details</summary>
Motivation: 发展中国家道路安全基础薄弱、超速致死率高，亟需低成本、自动化、可部署的执法与监测方案，以弥补人力与基础设施不足。

Method: 构建多源数据集（测速枪、佳能相机、手机）；采用YOLOv8进行车牌检测；比较CNN与Transformer用于车牌字符识别；以源/目标ROI进行速度估计；搭建数据库关联车辆与用户信息，并通过Africa’s Talking API自动发送罚单短信。

Result: 车牌检测mAP 97.9%；字符识别：CNN CER 3.85%，Transformer CER 1.79%（显著提升）；速度估计达到约±10 km/h误差；系统可联通数据库与短信平台形成闭环执法流程。

Conclusion: 所提系统在资源受限环境下可行，能支持自动化超速执法与交通管理；提高车牌识别精度与可用性，具减事故潜力。仍需进一步降低速度误差、扩展数据多样性并验证大规模部署效果。

Abstract: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.

</details>


### [32] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出OmniVaT框架，面向单域泛化的视觉-触觉学习，通过频域适配与离散树生成，实现跨未见域的强泛化。


<details>
  <summary>Details</summary>
Motivation: VTL存在两大难题：视觉与触觉模态间表征差异大；不同触觉传感器与采集流程导致域间分布不一致，缺少多域数据时难以泛化。

Method: 提出OmniVaT框架，包含两核心组件：1) 多模态分数阶傅里叶适配器（MFFA），将VIS与TAC嵌入映射到统一的嵌入-频率空间，减少模态鸿沟，无需多域训练或复杂跨模态融合；2) 离散树生成（DTG）模块，以层级树结构生成多样且可靠的多模态分数阶表示，以适应未知域的多样化分布偏移。

Result: 在SDG-VTL任务上进行大量实验，OmniVaT的跨域泛化性能优于现有方法。

Conclusion: 通过统一频域适配与树式多样化表示，OmniVaT显著提升单域训练下VTL的跨域鲁棒性，为多模态触觉-视觉融合的泛化提供新范式。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [33] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出DVEFormer：一种高效RGB-D Transformer，通过蒸馏Alpha-CLIP生成稠密文本对齐视觉嵌入，既可线性探测做语义分割，又支持自然语言查询与3D建图，且在Jetson Orin上实时运行。


<details>
  <summary>Details</summary>
Motivation: 家庭/室内机器人需要对环境有细粒度、可与自然语言对齐的理解，传统以固定标签的分割方法难以覆盖开放词汇、难以灵活交互与跨模态应用（如3D建图）。

Method: 以Alpha-CLIP作为教师，监督学生网络DVEFormer从RGB-D输入学习像素级文本对齐嵌入（DVE）；采用高效Transformer架构，训练后可通过线性探测实现传统分割，也可直接用文本查询；并将结果用于构建更全面的3D地图。

Result: 在常见室内数据集上性能具有竞争力；实时性强：完整模型26.3 FPS、轻量版77.0 FPS（NVIDIA Jetson AGX Orin）；给出多种定性示例展示实际应用效果。

Conclusion: DVEFormer可作为传统分割的即插即用替代，提供开放词汇自然语言查询能力，并无缝融入移动机器人3D建图流程，兼顾精度与实时性，适用于真实室内场景。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [34] [Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting](https://arxiv.org/abs/2601.00368)
*Aarya Sumuk*

Main category: cs.CV

TL;DR: 提出一个轻量两阶段框架，在体素网格上对损坏3D文物同时进行几何与颜色修复：先定位损坏，再用扩散式3D U-Net做掩膜条件修复。实验在32^3分辨率下优于基于对称性的基线，重建更完整、颜色更连贯。


<details>
  <summary>Details</summary>
Motivation: 数字文物修复需要同时补全几何与纹理颜色，现有方法往往分离或依赖对称等先验，难以在体素级别稳定处理大面积破损。作者希望通过显式损坏掩膜指导扩散模型，实现可控、联合的3D几何与颜色补全。

Method: 两阶段：1) 将体素化对象切片为RGB图像，用2D卷积网络预测每片的损坏掩膜，并聚合成立体体素掩膜；2) 基于扩散的3D U-Net在体素网格上进行掩膜条件的修复，联合预测占据（geometry）与颜色。损失由占据重建、被掩蔽区域的颜色重建以及感知正则组成，保证未损区域保真。

Result: 在含合成损坏的纹理化文物数据集上，用标准几何与颜色指标评估，相比对称性基线，在固定32^3分辨率下得到更完整的几何与更一致的颜色重建。

Conclusion: 显式掩膜条件能有效引导体素扩散模型，实现联合的3D几何与颜色修复；该轻量两阶段管线在低分辨率下已显示优越性，适合文物数字化修复场景。

Abstract: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.

</details>


### [35] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 提出一个概率化双流框架，结合不依赖校准的预处理、无监督置信的Noisy-OR融合与骨架-RGB跨模态集成，显著提升细粒度（尤其手部）动作识别的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有骨架HAR多以“身体大幅度动作”为中心，忽略手部等细粒度关节的微小变化；同时对多模态融合与不确定性/可靠性的处理不足，导致在噪声和异构数据下性能不稳。

Method: 1) 取消规范空间对齐的前处理，直接在原生坐标上学习，避免校准依赖；2) 在双流（如骨架内部与跨模态）中引入概率化Noisy-OR融合，以显式建模分支可靠性但无需显式置信监督；3) 从骨架内四种模态（关节/骨骼/关节运动/骨骼运动）扩展到与RGB的跨模态集成，统一结构与视觉运动线索；并构建手中心基准进行评测。

Result: 在NTU RGB+D 60/120、PKU-MMD、N-UCLA及新手部基准上取得一致性提升；在噪声和异构条件下表现出更高鲁棒性与稳定性。

Conclusion: 概率化、可靠性感知的双流与跨模态统一框架能在无需显式置信监督和校准的条件下，有效弥补细粒度（手部）动作信息缺失，提升骨架HAR的精度与稳健性。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [36] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse 提出一个可扩展的通用4D世界模型，支持无位姿先验的快速重建、按新轨迹生成视频，并在多项基准上达SOTA。其关键在于面向单目野外视频的可扩展训练与推理设计。


<details>
  <summary>Details</summary>
Motivation: 现有4D世界建模在可扩展性上受限：要么依赖昂贵、专业的多视角4D数据集，要么需要繁琐的预处理（如精确相机位姿/几何），难以适配大规模、野外的单目视频。

Method: 以“面向单目、端到端可扩展”的理念设计NeoVerse：1) 无需位姿的前馈式4D重建（pose-free feed-forward）；2) 在线模拟单目退化模式（monocular degradation pattern simulation），提升对真实野外视频噪声/遮挡/模糊的鲁棒性；3) 其他配套技术以统一训练与推理流程，使模型可泛化并高效扩展至多领域与大规模数据。

Result: 在标准4D重建与新轨迹视频生成基准上取得SOTA，同时展现对多领域数据的良好泛化与多种下游任务适用性。

Conclusion: 通过围绕单目视频可扩展性的系统化设计，NeoVerse兼顾通用性、鲁棒性与性能，缓解了4D世界建模对多视角数据与繁琐预处理的依赖，推动了4D重建与生成在野外场景的实用化。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [37] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: 提出RoLID-11K：首个基于行车记录仪的路侧垃圾检测大规模数据集（11k标注图），用于极小目标、长尾分布与动态驾驶场景；评测Transformer与YOLO等检测器，Transformer（如 CO-DETR）定位更准，实时模型受限于粗粒度特征；为低成本路侧垃圾监测提供基准与推动。


<details>
  <summary>Details</summary>
Motivation: 现有路侧垃圾监测依赖人工与公众上报，覆盖面有限；现有视觉数据集多为街景静态、航拍或水域场景，不符合行车记录仪视频中“极小、稀疏、杂乱背景”的特征，迫切需要针对该场景的数据与基准。

Method: 构建RoLID-11K数据集：来自英国多样驾驶条件的11k+标注图，呈现显著的长尾类分布与极小目标；系统基准评测从精度导向的Transformer到实时YOLO家族，分析不同模型在小目标与实时性上的取舍。

Result: CO-DETR及相关Transformer在定位精度上最好；实时模型因特征层级粗糙在小目标上受限；数据集对小目标检测具有高难度特性。

Conclusion: RoLID-11K为动态驾驶场景下极小目标检测提供挑战性基准，促进可扩展、低成本的路侧垃圾监测系统发展；数据与基准已开放获取。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [38] [ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis](https://arxiv.org/abs/2601.00416)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出ABFR-KAN，一种结合Transformer与KAN的新型FC分类框架，在ABIDE I上进行跨站点与消融实验，ASD分类优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统基于脑区图谱（atlas）的功能连接分析存在选择偏倚与忽视个体差异的问题，影响FC估计的可靠性与解剖一致性，进而限制脑障碍辅助诊断性能。

Method: 构建ABFR-KAN：以Transformer为分类骨干，融合“高级脑功能表征”组件，并引入Kolmogorov–Arnold Networks（KAN）以减轻结构偏置、提升与解剖结构的契合与FC估计稳定性；在ABIDE I上进行跨站点评测与多种骨干及KAN配置的消融分析。

Result: 在ABIDE I数据集上，ABFR-KAN在跨站点设置与多种消融场景中，分类准确度等指标稳定超过现有SOTA基线。

Conclusion: ABFR-KAN能更好地表征个体化功能连接，提升ASD分类可靠性与泛化性，证明融合KAN与Transformer的可行性与优势；代码已开源以促进复现与扩展。

Abstract: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.

</details>


### [39] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: 提出Anomaly Quadruplet-Net，用四元组度量学习与定制采样策略，在小数据、遮挡与细微外观变化下更稳健地估计装配进度；在台式机装配数据集上较现有方法提升准确率1.3%，相邻任务误判降低1.9%。


<details>
  <summary>Details</summary>
Motivation: 人工装配跨多天进行且任务间外观变化细微，导致基于视觉的进度估计易错，阻碍智能工厂落地；已有三元组度量学习方法在相邻任务易混淆、遮挡场景鲁棒性不足，需更可靠、少样本可用的方案。

Method: 基于异常检测与度量学习，提出以Quadruplet Loss训练的Anomaly Quadruplet-Net，引入包含(anchor, positive, hard-negative, auxiliary/anomaly)的四元组结构，扩大类间间隔、压缩类内距离；并设计定制数据加载器，按策略选择难例与相邻阶段样本，强化对细微变化与遮挡的判别；在小规模台式机装配图像上训练与评估。

Result: 在桌面PC装配数据集上，相比现有方法总体估计准确率提升1.3%，相邻任务之间的误分类率降低1.9%，对遮挡和细微变化场景表现更稳健。

Conclusion: 四元组损失结合策略性样本选择能在小数据、细微变化与遮挡下显著改进装配进度估计，为人工装配场景的智能监控提供更可靠的解决方案。

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [40] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: 提出CPPO，通过对感知相关token的对比式约束来增强VLM的感知与推理，超越以感知奖励为主的RL方法且更高效。


<details>
  <summary>Details</summary>
Motivation: RL已提升纯文本LLM的推理，但多模态推理同时依赖感知与推理能力。现有方法用显式感知奖励，但难以从输出中分离“感知token”和“推理token”，常需额外LLM/标注或对全序列一刀切施加奖励，带来成本高、误导信号的问题。

Method: 提出CPPO：1) 在对输入图像做信息保持或信息移除的扰动下，检测哪些输出token对感知变化敏感，通过比较分布熵的变化来识别“感知token”；2) 在PPO目标上加入对比式感知损失CPL：对信息保持扰动约束这些token输出保持一致，对信息移除扰动鼓励产生差异，从而强化对关键信息的感知而不过度惩罚推理token。无需额外模型或强制模块分离。

Result: 在多模态任务上优于以感知奖励为主的既有方法，表现更好且训练更高效、可扩展；避免额外模型带来的开销。

Conclusion: 通过基于熵移位自动定位感知token并施加对比式感知约束，CPPO在不增加外部依赖的情况下有效提升VLM的感知与推理，提供更高效的多模态RL微调框架。

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [41] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics 是一个端到端可微框架，从自然语言提示为给定3D场景自动推断物理参数，并通过视频扩散模型蒸馏的运动先验来引导仿真，实现不依赖真值轨迹或标注视频的逼真动态模拟。


<details>
  <summary>Details</summary>
Motivation: 现有3D对象与多样材料的真实感动态仿真常需专家经验与繁琐的物理参数调谐，且缺乏无需真值监督即可从高层描述获得合理动力学行为的方法。

Method: (1) 使用多模态大语言模型从自然语言与场景中初估材料参数，并约束其在物理可行范围；(2) 提出可学习的运动蒸馏损失，从预训练视频扩散模型提取稳健的运动先验，同时抑制外观与几何带来的偏置；(3) 在可微物理引擎中端到端优化以得到符合提示的物理参数与动态。

Result: 在30+个场景（真实、人工设计、AI生成物体）与多种材料（弹性体、金属、泡沫、砂、牛顿/非牛顿流体）上，生成视觉上逼真的动态模拟，自动确定物理可信参数，效果优于现有方法。

Conclusion: 结合多模态LLM参数估计与视频扩散模型运动先验的可微物理框架，可由自然语言高层约束实现高逼真度的动态仿真与参数识别，消除对轨迹/标注监督与专家调参的依赖。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [42] [All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations](https://arxiv.org/abs/2601.00533)
*Wenrui Li,Hongtao Chen,Yao Xiao,Wangmeng Zuo,Jiantao Zhou,Yonghong Tian,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出SEUD场景与ORCANet：针对视频中随时间平滑演化的未知、复合退化，构建时序一致的合成数据，并以递归条件与自适应提示网络实现高质量、时序稳定的恢复。


<details>
  <summary>Details</summary>
Motivation: 现有“一体化”图像复原方法难以扩展到视频：它们多按帧处理、忽视现实世界退化在时间上的平滑连续性（类型与强度都会缓慢变化并可共存/过渡），导致时序不一致与对复杂退化的适应性不足。

Method: 1) 定义SEUD场景：退化集合与强度随时间连续变化；并提出灵活的数据合成管线，生成单一、复合与演化退化的视频。2) 模型ORCANet：- CIED模块基于物理先验估计雾强度，产生粗去雾特征作初始化；- FPG模块提取退化特征并生成两类提示：静态提示捕获片段级退化类型，动态提示适应帧级强度变化；- 标签感知监督增强静态提示在不同退化下的可分辨性；整体为递归式、条件化与自适应提示的网络。

Result: 在多组实验中，ORCANet在恢复质量、时序一致性与鲁棒性方面优于图像与视频基线方法。

Conclusion: 建模退化的时序平滑演化并结合物理先验与提示学习，可显著提升视频一体化复原的性能与稳定性；所提SEUD基准与ORCANet为后续研究提供了有效范式。

Abstract: All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.

</details>


### [43] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText是一个无需训练、可插拔的方法，面向DiT类扩散模型，显著提升多行/密集/长尾文字（含中文）渲染准确性，同时基本保持语义与美学质量。其关键在于将“写在哪儿”和“写什么”解耦：前者用内生图文注意力定位稳定书写区域，后者用频域调制的字形注入以强化字形、抑制语义泄漏。跨多模型多基准验证有效，开销较小。


<details>
  <summary>Details</summary>
Motivation: 现有大规模T2I扩散模型在开放域生成上强，但文本渲染仍薄弱，尤其多行排版、密集排版和中文等长尾脚本。以往方案多依赖再训练或外部硬约束，成本高且影响美观与灵活性，亟需一种训练免疫、通用且美学友好的提升方案。

Method: 提出FreeText，面向DiT模型的训练免疫、即插即用框架，拆分为两子问题：1）Where to write：利用模型内生的图→文注意力得到token级空间归因，选取“sink-like”稳定token作空间锚点，并结合拓扑感知的细化，获得高置信写字区域掩膜。2）What to write：提出频谱调制字形注入（SGMI），将与噪声对齐的字形先验在频域进行带通调制，既增强字形结构，又抑制语义泄漏（把概念画成物而非字）。整体仅在推理阶段介入，开销适中。

Result: 在Qwen-Image、FLUX.1-dev、SD3等多种DiT系模型上，于longText-Benchmark、CVTG与自建CLT-Bench均显著提升文本可读性；同时基本保持文本-图像语义一致性和审美质量；推理时间开销温和。

Conclusion: 无需再训练即可显著改进DiT家族模型的文本渲染，尤其对多行、密集与中文场景有效；通过注意力引导的空间定位与频域字形注入联合，兼顾可读性、语义与美学。该思路具有通用性与实用性，适合作为文本生成图像系统的默认增强模块。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [44] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: 提出VNS-SAM，通过挖掘低层特征与边缘信息，显著提升SAM在低显著（前景/背景对比度低）场景的零样本分割表现，并发布35K规模的VNS-SEG数据集；仅少量参数与计算开销，训练<4小时。


<details>
  <summary>Details</summary>
Motivation: SAM在零样本分割强大，但在前景与背景低对比、轮廓不明显的“视觉非显著”场景表现欠佳，现有方法难以捕获精确轮廓且常为单任务适配，泛化差。

Method: 在SAM上引入两项设计以利用低层特征：1) Mask-Edge Token Interactive decoder：让掩码与边缘token交互，加强轮廓与细节建模；2) Non-Salient Feature Mining模块：挖掘非显著特征，提升对低对比区域的感知；同时构建覆盖多类VNS场景的统一数据集VNS-SEG（>35K图像）。

Result: VNS-SAM在多种视觉非显著分割任务上取得更优结果，特别是在零样本设定下显著超越基线，同时新增参数与算力开销很小，额外参数可在约4小时内优化完成。

Conclusion: 通过低层特征与边缘交互解码器及非显著特征挖掘模块，VNS-SAM在保持SAM零样本泛化的同时显著提升低显著场景分割能力；VNS-SEG为统一评测与训练提供了数据基础，方法具备实用性与广泛应用潜力。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [45] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: 提出DynaDrag：一种在“预测-再移动”框架下的拖拽式图像编辑方法，以迭代预测点位移动并监督执行，动态选择有效控制点，在人脸与人体数据上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑多采用“先移动再跟踪”的范式，常出现丢跟与歧义跟踪；其他框架又存在源–目标差距大、产生不合理中间点、可编辑性低等问题，亟需一种更稳健、编辑可控性更强的方案。

Method: 提出首个“预测并移动”(predict-and-move)框架DynaDrag：在每次迭代中先进行Motion Prediction预测控制点应移动到的位置，再由Motion Supervision将其拖拽到预测位置；并引入动态有效控制点选择机制以提升稳定性与效果。

Result: 在脸部与人体数据集上的实验表明，DynaDrag在编辑质量与稳定性上优于现有方法。

Conclusion: 通过以预测驱动的拖拽迭代与动态控制点选择，DynaDrag规避了传统移动-跟踪范式的跟踪问题与编辑性不足，实现更可靠的像素级编辑，具备更强的通用性与性能。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [46] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: 提出SlingBAG Pro，一种适用于任意不规则换能器阵列的点云迭代式3D光声重建方法，通过分层优化（零梯度滤波+递增时间采样）快速剔除冗余点云，加速收敛，在保持高质量的同时减少换能器数量，相比原SlingBAG最高提速约2.2倍，并在仿真与小鼠在体实验中验证。


<details>
  <summary>Details</summary>
Motivation: 临床3D光声成像需要高质量、低成本与小型化。为贴合受限空间，可采用不规则几何阵列以减少探头数，但传统迭代重建在此类阵列下计算与存储开销大、收敛慢，限制实用化。因此需要一种能兼容任意阵列、快速且高质的重建算法。

Method: 基于SlingBAG的点云迭代思想，提出SlingBAG Pro：1) 兼容任意不规则阵列几何；2) 分层优化策略：引入零梯度滤波并在迭代中逐步提高时间采样率，快速剔除冗余空间点云并加速收敛；3) 降低换能器数量需求，同时维持重建质量。

Result: 在不规则阵列几何下，SlingBAG Pro用于点云式3D PA重建较原SlingBAG实现最高约2.2倍速度提升；重建质量保持较高。通过数值仿真与在体小鼠实验验证方法有效。源码已开放。

Conclusion: SlingBAG Pro实现对任意阵列几何的高效高质3D光声重建，降低探头数量需求并显著缩短重建时间，为受限空间与低成本临床应用提供可行路径。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [47] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: 提出MS COCOAI数据集：基于MS COCO构建的9.6万张真实与合成图像，用于检测与溯源多模型AIGC图像（Stable Diffusion 3/2.1、SDXL、DALL·E 3、MidJourney v6）；包含两项任务：真假分类与生成器识别。


<details>
  <summary>Details</summary>
Motivation: 生成式多模态模型使合成图像更逼真、传播更广，带来误导信息与操纵媒介的风险，迫切需要可靠的检测与溯源基准资源。现有数据集在规模、模型覆盖与任务设置上不足，难以支持鲁棒检测研究。

Method: 从MS COCO采样真实图像；使用五种主流生成器在相同或相关文本/条件下生成合成图像；构建总计约96,000个样本的数据集；定义两项任务：(1) 真实 vs 合成二分类；(2) 对合成图像进行生成模型归因。公开数据集下载链接。

Result: 得到覆盖五大主流生成器、规模达9.6万的混合真伪图像数据集，并可直接用于二分类检测与多类归因任务的训练与评测。

Conclusion: MS COCOAI为AI生成图像检测与溯源提供统一基准，助力研究社区开发更准确、鲁棒的检测器与模型归因方法，以应对AIGC带来的误导与操纵风险。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [48] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: AEGIS 是一个评估统一多模态模型（UMMs）世界知识与多任务能力的综合基准，配合确定性清单评测（DCE）以“是/否”原子判断替代主观评分。实验显示主流 UMMs 在世界知识与复杂推理上明显不足，简单插件式推理模块可一定程度缓解。


<details>
  <summary>Details</summary>
Motivation: 现有多模态评测多为单任务、割裂且指标含糊，难以诊断 UMMs 是否能在不同任务中调动一致的世界知识与推理能力。作者希望建立覆盖更广任务形态与知识维度、且评分可复现的基准与协议。

Method: 构建 AEGIS 基准：包含理解、生成、编辑、交错生成四类任务，共 1,050 道人工标注高难度问题，覆盖 21 个主题与 6 种推理类型；并提出 DCE 协议，以确定性清单将复杂回答分解为可核验的“Y/N”原子项，减少提示敏感与主观性。

Result: 在 AEGIS 上，多数 UMMs 在世界知识相关任务上表现不佳，且随推理复杂度上升性能显著下降；引入简单的外接/插件式推理模块后可部分提升表现。

Conclusion: 世界知识驱动的推理是 UMMs 的关键短板与未来突破口。AEGIS+DCE 提供了可靠、可复现的多任务测评框架，提示结合外部推理模块是可行方向。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [49] [A Cascaded Information Interaction Network for Precise Image Segmentation](https://arxiv.org/abs/2601.00562)
*Hewen Xiao,Jie Mei,Guangfu Ma,Weiren Wu*

Main category: cs.CV

TL;DR: 提出一种带全局信息引导模块（GIGM）的级联卷积网络，融合多层次低纹理与高语义特征，显著提升在复杂场景下的图像分割精度，超越SOTA，并具备机器人应用潜力。


<details>
  <summary>Details</summary>
Motivation: 视觉感知是低成本高效的自主系统关键，但在视觉杂乱、模糊等复杂场景中，鲁棒分割仍困难；单尺度特征提取难以兼顾细节与语义，导致性能下降。

Method: 构建级联CNN，并引入全新的全局信息引导模块（GIGM）。该模块跨多层聚合全局与局部信息，将低层纹理细节与高层语义特征有效融合，从而缓解单尺度特征表达不足的问题；整体采用多层次特征融合与引导机制以提升分割。

Result: 在基准分割数据集上，所提框架在精度上优于现有SOTA，尤其在视觉杂乱或模糊环境中表现突出。

Conclusion: 全局信息引导融合策略能有效提升复杂场景下的分割鲁棒性和精度，方法具有实际机器人部署的潜力。

Abstract: Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.

</details>


### [50] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: 提出Granularity-Aware Alignment(GranAlign)，通过无训练的粒度对齐策略（查询重写+查询感知字幕）缓解文本-视频语义粒度不匹配，显著提升零样本视频片段定位性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频时刻检索中，文本查询与视频视觉表征存在语义粒度不一致（文本可粗或细，视频表征亦然），即使使用高质量预训练跨模态表示也会因粒度错配导致检索不准。需要在不额外训练的前提下对齐两种模态的语义粒度。

Method: 提出无训练框架GranAlign：1) 粒度化查询重写（granularity-based query rewriting），将原始查询改写为多层次语义粒度的若干版本；2) 查询感知字幕生成（query-aware caption generation），在视频内容生成字幕时显式注入查询意图，得到既与查询相关又保留视频语义的描述；3) 将多粒度查询与两类字幕（与查询无关的通用caption与查询感知caption）进行多对多匹配与检索，缓解粒度错配。

Result: 在三大基准（QVHighlights、Charades-STA、ActivityNet-Captions）上实现SOTA；在更具挑战的QVHighlights上mAP@avg提升3.23%。

Conclusion: 通过无训练的粒度对齐与查询注入机制，有效弥合文本-视频语义粒度差异，提升ZVMR精度，显示出无需任务特定训练即可显著改进跨模态检索的潜力。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [51] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: 提出SafeMo：在连续空间进行“最小运动遗忘（MMU）”的安全文本到动作生成框架，并发布安全数据集SafeMoVAE-29K；在保持良好自然过渡与对齐的同时，大幅提升对不安全提示的遗忘效果且不显著损害良性性能。


<details>
  <summary>Details</summary>
Motivation: 现有T2M安全化多采用离散VQ-VAE码本替换来避开不安全动作，但会带来两大问题：1）码本条目在良性与不安全场景复用，替换会拖累日常任务表现；2）离散量化造成平滑性与连续运动学损失，产生瑕疵与生硬过渡。此外，公开T2M数据集包含不安全意图与动作，不适合直接做安全学习。

Method: 提出SafeMo框架与最小运动遗忘（MMU）两阶段机器遗忘策略：在连续表示空间对不安全行为进行目标化遗忘，避免离散码本量化带来的损伤；同时构建SafeMoVAE-29K数据集，包含重写的安全文本与连续精炼动作，支撑可信的人体动作遗忘训练。模型建立在DiP骨干之上，高效生成具有自然过渡的安全动作。

Result: 在HumanML3D与Motion-X上，相比SOTA遗忘方法LCR，SafeMo在不安全提示上的遗忘更强，forget-set FID分别提升至其2.5倍与14.4倍；同时对安全提示的良性性能持平或更优，生成动作更平滑、过渡更自然。

Conclusion: 在连续空间进行最小化、定向的遗忘可显著改善T2M安全化的“安全-效用”权衡。SafeMo与SafeMoVAE-29K为可信的人体动作生成提供了有效范式与数据基础，避免离散码本方法的性能漂移与运动伪影，并在多基准上验证了更强的安全遗忘与保真度。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [52] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 提出MDI指标衡量RGB-IR多模态训练中的“模态主导”，并据此构建MDACL框架，通过层级跨模态引导与对抗均衡正则，缓解优化偏置并在多基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 在RGB与红外融合检测中，两模态信息密度与特征质量不对称，训练往往偏向占优模态，导致融合无效或次优。现有方法多关注特征融合结构，较少刻画并调控这种由优化动态引发的“模态主导”偏置。

Method: 1) 提出模态主导指数MDI，联合特征熵（表征信息密度/不确定性）与梯度贡献（表征优化驱动力）来量化每步训练中各模态的主导程度；2) 基于MDI设计MDACL框架：包含(a) 层级跨模态引导HCG，在多尺度/多层面进行对齐与信息引导，提升跨模态一致性；(b) 对抗均衡正则AER，引入对抗式约束调节两模态的梯度与更新幅度，实现训练动态的平衡；3) 将MDACL集成到RGB-IR检测网络并端到端训练。

Result: 在三个RGB-IR检测基准上，MDACL显著降低模态主导度（按MDI统计），带来稳定的融合学习，并在检测性能上达到或超过现有SOTA。

Conclusion: 优化偏置是多模态融合性能受限的重要根源。通过显式度量（MDI）与有针对性的训练调控（HCG+AER），可有效缓解模态主导，提升RGB-IR检测的鲁棒性与精度。

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [53] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: 提出TOLF，用正态化流建模定位误差并结合不确定性引导优化，提升小目标检测在噪声标注下的鲁棒性，在AI-TOD上相对DINO提升1.2% AP。


<details>
  <summary>Details</summary>
Motivation: 小目标对标注噪声极其敏感，传统严格定位损失易过拟合噪声，导致与常规尺度目标相比存在显著性能差距，亟需一种对噪声更鲁棒的定位学习框架。

Method: 构建Tiny Object Localization with Flows (TOLF)：1) 用normalizing flows对预测与真值之间的定位误差分布进行灵活建模，捕获非高斯、复杂多模态分布；2) 基于流模型得到样本级不确定性，设计不确定性感知的梯度调制机制，抑制高不确定（更可能含噪）的样本对参数更新的影响，从而减少过拟合并稳定训练。

Result: 在三个数据集上进行大量实验验证，整体提升鲁棒性；在AI-TOD数据集上，相比DINO基线提升1.2% AP。

Conclusion: 通过流式误差建模与不确定性引导优化，TOLF在标注噪声下实现更加稳健的小目标定位学习，缩小小目标检测性能差距。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [54] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: RePose提出一套面向康复训练的实时多相机RGB输入的人体3D姿态估计与运动分析系统，支持毫秒级跟踪、实时去抖平滑与误差降低，并在Unity中可视化评估与肌肉受力显示，提供即时反馈与动作纠正。


<details>
  <summary>Details</summary>
Motivation: 康复训练需要对患者动作进行持续、准确、低延迟的监测与评估，以便及时纠正动作、提升训练质量并促进功能恢复。现有方法在多目标干扰、实时性、姿态稳健性和可视化反馈方面不足。

Method: 1) 统一端到端流水线：多摄像头RGB输入→3D姿态估计→运动分析→Unity可视化与评估；2) 提出面向康复场景的快速多目标跟踪方法，单帧耗时<1ms；3) 修改SmoothNet以实现实时姿态时序平滑与误差降低，使估计更贴近真实运动并视觉更顺滑；4) 在Unity中集成实时监控、评分与肌肉受力可视化。

Result: 实现实时3D姿态估计和运动分析，能在多人的干扰环境下稳定跟踪，显著降低姿态估计误差并提升视觉平滑度；系统可即时反馈与评估，并展示肌肉受力状态。

Conclusion: RePose可在康复训练中提供实时、稳定且直观的动作监测与纠正支持，有助于患者恢复肌力与运动功能。

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [55] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: 提出HyperPriv-EPN：用超图与LUPI范式，把术后文本特权信息蒸馏到仅用术前MRI的模型里，达成SOTA诊断与生存分层。


<details>
  <summary>Details</summary>
Motivation: 术前MRI缺乏语义细节，难以做准确预后；术后手术/病理报告含丰富语义但推理时不可用。现有多模态方法在测试时无法利用这类“特权信息”，导致术前决策支持受限。

Method: 构建HyperPriv-EPN框架：以超图建模患者群体关系，采用共享编码器与“断裂图”策略，形成包含特权术后文本的Teacher图与仅含术前数据的Student图。通过双流蒸馏，使Student在无文本时从视觉特征“幻化”出语义社区结构，实现从Teacher到Student的结构与表示迁移。

Result: 在311例多中心队列上验证，模型在诊断准确率和生存分层上达到当前最优（SOTA）。

Conclusion: 将历史术后文本知识迁移到术前场景，使新患者在无文本情况下也能受益，提升术前诊断与预后评估能力。

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [56] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 利用图像深度学习监测马铃薯贮藏质量：构建萌芽二分类与重量损失/保质期多分类模型；DenseNet在萌芽检测达98.03%准确率；保质期粗粒度分级（2–5类）准确率>89.83%，细粒度（6–8类）因视觉差异微弱与数据稀疏而下降；可用于自动分选与库存管理，减少浪费。


<details>
  <summary>Details</summary>
Motivation: 传统质量监测依赖人工与破坏性称重，难以规模化、连续化；贮藏中关键痛点为早期萌芽识别、重量损失评估与保质期预测，需低成本、非侵入、可部署的智能方法以优化供应链与减少食物浪费。

Method: 在控温控湿条件下连续200天采集图像与重量数据；基于预训练ResNet/VGG/DenseNet/ViT，构建两类模型：1）高精度萌芽二分类器；2）多分类模型同时估计重量损失与预测剩余保质期。比较不同网络与不同类别划分粒度（2–8类）的表现。

Result: DenseNet在萌芽检测中最优，准确率98.03%；保质期预测在粗粒度分级（2–5类）效果最好，准确率≥89.83%；随着类别细化到6–8类，因类内差异小与每类数据不足导致准确率下降。

Conclusion: 图像驱动方法可行且具可扩展性，可集成至自动分选与库存系统，支持早期剔除萌芽薯与按贮藏阶段动态分级，促进差异化定价与降损增效。精确到细粒度保质期区间仍具挑战，建议采用粗粒度分级；未来需在多品种与多环境数据上训练通用化模型以提升适应性与可扩展性。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [57] [Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network](https://arxiv.org/abs/2601.00658)
*Zhaiyu Chen,Yuanyuan Wang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出一个学习框架，将噪声大、稀疏且不规则的TomoSAR点云直接转换为高分辨率建筑高度图；通过点-栅格双拓扑网络实现去噪与补洞，在慕尼黑与柏林数据上验证有效，并可融合光学影像增强重建。


<details>
  <summary>Details</summary>
Motivation: TomoSAR能在任意天气下获取侧视、包含立面信息的数据，理论上有利于城市建筑高度估计；但原始TomoSAR点云存在噪声、各向异性分布和在非相干表面的数据空洞，导致传统方法难以稳定、连续地恢复高度。需要一种能在不规则点和规则栅格间协同建模、同时实现去噪与插补的方案。

Method: 提出双拓扑网络：交替处理两种表示。点分支针对不规则散射体特征（处理点云噪声、稀疏和各向异性），栅格分支在规则网格上建模空间一致性；二者联合学习，既去噪又对缺失区域进行inpainting，最终输出连续的建筑高度图。框架还能扩展融合光学卫星影像以进一步提升质量。

Result: 在慕尼黑与柏林的大规模数据上进行大量实验，证实该方法能从原始TomoSAR点云产生高分辨率、连续的建筑高度图，优于传统或仅点/仅栅格的替代方案；并展示了融入光学影像后的性能提升。

Conclusion: 首次证明可直接从TomoSAR点云进行大范围城市高度制图的可行性。双拓扑联合建模有效缓解噪声和数据空洞问题，生成连续高质量高度估计，并具备与光学数据融合的可扩展性。

Abstract: Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.

</details>


### [58] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: 提出CRoPS：一种训练免（training-free）的幻觉缓解框架，通过“广义对比解码”融合多种构造的幻觉模型（包含移除视觉与关键文本token），在多个基准与模型族上显著降低LVLM幻觉（CHAIR提升约20%）。


<details>
  <summary>Details</summary>
Motivation: 现有训练免方法主要通过移除视觉token来近似“无视觉”状态，但假设过于狭窄且在生成后段效果下降；同时视觉信息仍会渗透到生成文本导致对比解码不足，需更全面建模幻觉来源并在生成后段保持抑制能力。

Method: 1) 构造新的“幻觉模型”：不仅移除视觉token，还选择性移除关键文本token，以刻画文本驱动的幻觉效应；2) 广义对比解码（Generalized Contrastive Decoding, GCD）：将多个不同来源的幻觉模型并行对比，与原模型联合解码，动态抑制与这些幻觉模型一致的生成倾向；3) 训练免、可插拔，在推理时替换/组合不同token移除策略。

Result: 在六个基准与三类LVLM中取得一致提升，相比SOTA训练免方法整体更优；CHAIR指标提升约20%，表明物体级与属性级幻觉显著减少；末尾生成段的幻觉也得到更强抑制。

Conclusion: 仅依赖“去视觉”的幻觉建模不足；通过进一步构造“去关键文本”的幻觉模型并在广义对比解码中融合多源幻觉信号，CRoPS在无需再训练的前提下广泛且稳定地降低LVLM幻觉。

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [59] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 从单张图像出发，构建3D高斯场并一次前向同时采样物体运动，实现可按相机轨迹控制的高效视频生成，兼顾时间一致性与几何一致性，性能与效率达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有单图像视频生成虽提升了时序与三维一致性，但用户可控性差，尤其难以精确控制相机路径；基于点云的两阶段方法虽能控相机，却难以保持完整时序一致与几何完整，同时推理效率受限。

Method: 提出单次前向的“像素到4D”框架：从单张图像重建场景为3D高斯表示（Gaussian Splatting），并在同一过程内对可行的物体运动进行采样；随后沿给定相机轨迹进行可微渲染得到视频，无需迭代去噪注入运动。关键在于显式3D表示与运动采样的联合建模，实现快速、可控、几何一致的生成。

Result: 在KITTI、Waymo、RealEstate10K、DL3DV-10K上实现SOTA的视频质量与推理效率，相比相机可控的现有方法，时序一致性、几何保真与相机运动建模更优，且生成更快。

Conclusion: 显式3D高斯表示结合单次前向的运动采样，能在单图像条件下实现快速、可控、时间与几何一致的相机引导视频生成，为实际应用提供更强的可控性与效率。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [60] [Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks](https://arxiv.org/abs/2601.00703)
*Cory Fan,Wenchao Zhang*

Main category: cs.CV

TL;DR: 本文探讨在用于图像去马赛克与联合去马赛克-去噪（JDD）的等各向（残差套残差）网络中引入显著空间下采样是否能在移动端约束下提升效率与性能。作者设计对照的全卷积网络（有/无下采样），并基于从 DeepMAD 改编的数学化架构设计方法进行搜索与验证，结果显示下采样版本（称为 JD3Net）在多项任务与数据集上表现优异，同时计算更高效。


<details>
  <summary>Details</summary>
Motivation: 移动端成像广泛依赖深度学习去马赛克，但主流等各向网络通常完全避免下采样，导致计算/内存成本过高，不利于移动部署。作者质疑“无下采样更好”的既有范式，探究显著下采样是否反而有利于效率与精度。

Method: - 采用从 DeepMAD 改编的数学化架构设计（约束驱动的网络规模与算力配置）来构建一组结构相同但是否下采样的全卷积等各向网络。
- 对比不下采样基线与含显著下采样的变体；后者在低分辨率特征空间中进行大部分计算，再上采样重建。
- 在图像去马赛克与 JDD 任务上进行系统实证评测。

Result: 引入显著下采样的等各向网络在相同或更低算力下取得更好或相当的重建质量；具体下采样模型 JD3Net 在多种数据集/任务上展现强竞争力与更高效率。

Conclusion: 与既有设计相反，等各向网络在去马赛克/JDD 中进行充分下采样不仅不会损害效果，反而提升效率并能改进性能。基于 DeepMAD 的架构设计为此提供了可复现的构建路径，JD3Net 证明了该策略的实用价值。

Abstract: In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.

</details>


### [61] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: RGS-SLAM 以一次性三角化多视图稠密对应替代残差驱动加密，提供结构感知的高斯初始化，稳定早期建图、加速约20%，在TUM/Replica上实现实时（最高约925 FPS）且具竞争力的定位与重建精度。


<details>
  <summary>Details</summary>
Motivation: GS-SLAM 的残差驱动逐步加密在早期建图不稳定、收敛慢、易受纹理复杂与遮挡影响；需要一种无需训练、能快速且结构一致地初始化高斯以提升稳定性与效率的方法。

Method: 利用DINOv3提取的多视图描述子建立稠密对应，通过带置信度感知的内点分类器筛选可靠对应；对内点进行一次性多视图三角化，直接生成分布良好、结构感知的高斯种子；随后沿用GS-SLAM优化流程进行联合位姿-几何-外观优化。

Result: 相较GS-SLAM，早期映射更稳定、收敛提速约20%，在纹理丰富与杂乱场景具更高渲染保真度；在TUM RGB-D与Replica上，定位与重建精度达到或优于当前高斯/点云SLAM方法；保持实时映射，最高约925 FPS。

Conclusion: 训练无关的一次性稠密对应三角化可作为高斯SLAM的有效初始化，提升稳定性、速度与渲染质量，并与现有GS-SLAM管线兼容，具备即插即用的实用价值。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [62] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 提出在病理领域的VLM中，结合输入分布漂移检测与无标签、基于置信度的输出退化指标，可更可靠地发现数据分布变化下的性能退化；并发布轻量工具DomainSAT以系统分析输入漂移。


<details>
  <summary>Details</summary>
Motivation: VLM在临床部署后常遭遇数据分布迁移，导致性能下滑；大模型在无标注环境下的退化检测困难且对临床可靠性关键。

Method: 1）从两条线索研究退化监控：输入侧的分布漂移检测（集成代表性算法，GUI工具DomainSAT用于可视化与诊断）；2）输出侧的无标签监控，提出基于模型预测置信度的退化指示器，直接度量置信度变化。通过在大规模病理肿瘤分类数据上实验，比较单独与组合方案的效果。

Result: 输入漂移检测能有效识别分布变更并提供早期信号，但与真实性能退化并非总是强相关；新提出的置信度指标与性能退化高度相关，作为输入检测的有效补充。两者结合显著提升退化检测的可靠性与可解释性。

Conclusion: 输入漂移检测与输出置信度监控是互补的：前者善于发现环境变化，后者更贴近性能变化。联合使用构成一套实用框架，可在数字病理等场景下更稳健地监控基础模型的可靠性。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [63] [Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection](https://arxiv.org/abs/2601.00725)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: 提出多层特征融合方法，使质量检测的持续学习快速适配、少参数、少遗忘、强泛化。


<details>
  <summary>Details</summary>
Motivation: 制造业质量检测中，重制造等场景变化频繁，产品与缺陷模式经常更替，已部署的深度模型需要不断适配新条件。传统端到端再训练耗时耗算力，且易灾难性遗忘，亟需兼顾高效训练与稳定迁移的方案。

Method: 在预训练网络中，从不同深度提取多层表征，并进行多级特征融合（MLFF），通过只训练较少的融合/适配模块来完成任务更新，从而避免对全网大规模微调；用于持续学习情境下的新产品/新缺陷适配。

Result: 在多种质量检测任务上，MLFF以显著更少的可训练参数达到与端到端训练相当的性能；在增量任务中降低灾难性遗忘，并在面对新产品类型或新缺陷时表现出更强的泛化鲁棒性。

Conclusion: 融合不同深度的预训练特征可在持续学习中实现高效适配与性能稳定：既减少训练参数和计算，又缓解遗忘并提升对分布变化的泛化。

Abstract: Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.

</details>


### [64] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 提出一个端到端流程，用多模态大模型对扫描的手写工程测验进行评分：用手写参考答案+简短评分规则指引，采用多阶段校验、评审集成与可审计模板，实测在真实课程小测上与教师评分平均相差约8分，并触发人工复核约17%。结构化提示与参考对齐是准确性的关键。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试包含开放式推理与图示，人工批改耗时且难以扩展；现有自动评分多依赖数字化或受限输入，难以保持传统纸考流程。需要一种既保留A4纸手写与不受限作答，又能可靠、可审计地自动评分的方案。

Method: 构建多模态LLM评分流水线：教师仅提供手写满分参考解与简短评分规则；先将参考解转为纯文本摘要用于条件化评分（不直接暴露参考扫描）。流水线包含：格式/作答存在性检查防止空白误判；多名独立评分器（ensemble）；监督者聚合；严格模板与确定性校验，输出可审计、可机器解析的报告。在斯洛文尼亚语真实课程小测（含手绘电路图）上，以SOTA后端（GPT-5.2、Gemini-3 Pro）进行“洁净室”评估与消融。

Result: 完整流水线在保留集测验上达到与教师评分约8分的平均绝对差（低偏差），在D_max=40阈值下估计约17%的答卷需人工复核。消融显示：用朴素提示或去除参考解会显著降低准确度并产生系统性高分偏差。

Conclusion: 多阶段、结构化的提示与参考对齐，加上集成与可审计模板，可在传统纸考场景下实现可扩展、较可靠的手写工程测验自动评分；参考驱动与严格流程对准确性与偏差控制至关重要。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [65] [Unified Primitive Proxies for Structured Shape Completion](https://arxiv.org/abs/2601.00759)
*Zhaiyu Chen,Yuqing Wang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: UniCo提出一种统一的结构化形状补全：用“primitive代理”在单次前向中同时预测几何、语义与内点归属，借助与点云的联合训练与在线目标更新，在多基准与多装配求解器上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有结构化补全多采用级联：先点/特征，再拟合primitive，导致信息割裂、优化不一致、装配不稳定。作者希望直接在共享形状特征上专门解码primitive，使其与点等表征协同，并获得可直接用于装配的输出。

Method: 提出UniCo：1) 专用primitive解码通道，从共享形状特征中通过“primitive proxies”（可学习查询）生成一组带完整几何、语义与内点成员的primitive；2) 统一表征与训练，将primitive与点通过在线目标更新耦合优化，确保二者一致；3) 单次前向输出“装配就绪”的primitive集合；4) 与四种独立装配求解器无缝对接。

Result: 在合成与真实数据集上，相对近期基线，Chamfer距离最高降低约50%，法向一致性提升最高约7%，跨四个装配求解器均保持领先且稳定。

Conclusion: 以primitive为中心的统一补全路径、代理查询与耦合优化，提供了从不完整数据到结构化3D理解的有效范式，能够直接支持下游装配与重建，并在多场景稳健超越现有方法。

Abstract: Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.

</details>


### [66] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: 以自监督作为辅助任务，融合其特征表示以提升泛化型深度伪造检测；经多数据集跨库评测优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 通用深度伪造检测在跨数据集场景中易失效，现有方法对分布偏移与多样伪造手法的鲁棒性不足。作者希望利用自监督学习的任务不依赖标签、能学到更通用的表征，来提升主任务的泛化能力。

Method: 将自监督学习设为辅助任务，探索多种主-辅训练方案与特征融合策略；核心做法是将来自自监督辅助任务的特征与主任务特征进行融合，形成更具判别力与通用性的表示，用于深度伪造检测。

Result: 在DF40、FF++、Celeb-DF、DFD、FaceShifter、UADFV等多数据集上进行实验，尤其在跨数据集评测中，相比当前SOTA检测器取得更好泛化性能。

Conclusion: 自监督辅助任务的特征在与主任务表征融合后，可显著提升深度伪造检测的跨库泛化能力；特征融合是关键。

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [67] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 提出两种用于心脏短轴cine MRI左心室分割的改进U-Net：LNU-Net（在每个卷积块使用LayerNorm）与IBU-Net（首个卷积块融合Instance+Batch Norm），在Dice和平均垂直距离上优于基线和主流方法。


<details>
  <summary>Details</summary>
Motivation: 左心室精准分割对心功能定量与诊断关键，但传统或标准U‑Net在不同对比/患者间的域移、强度分布变化下鲁棒性有限，需改进归一化与正则化策略以提升泛化与精度。

Method: 以U‑Net为骨架，保持编码-解码（下采样提特征、上采样做定位）的对称结构；提出两种归一化策略：1) LNU-Net在每个卷积块中采用Layer Normalization；2) IBU-Net在首个卷积块联合使用Instance+Batch Normalization并将其输出传递至下一层。训练时采用数据增强（仿射变换与弹性形变）。在包含45名患者共805张短轴cine MRI图像的数据集上评估，并与原始U‑Net及SOTA方法比较。

Result: 两种模型在左心室分割上取得更高Dice系数和更低平均垂直距离（APD），优于基线U‑Net及其他对比方法（摘要未给出具体数值）。

Conclusion: 改进的归一化设计（LN或IB联合）能在U‑Net中提升左心室分割的精度与边界定位，结合常规数据增强即可在小规模临床数据上取得SOTA水平，显示对心脏MRI分割的有效性与鲁棒性。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [68] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: AdaGaR 提出一种用于单目视频动态三维场景重建的显式表示：在高频细节与时间连续性间取得平衡，通过自适应Gabor表示与样条时序约束实现稳定、高保真重建，并在多任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 单高斯表示具有低通特性难以捕获高频细节，标准Gabor虽能提升频率表达但存在能量不稳定；此外缺乏时间连续性约束会在插帧时产生运动伪影。需要一种同时具备频率自适应与时序平滑的显式动态场景建模方法。

Method: 1) 自适应Gabor表示：在高斯基础上引入可学习频率权重与自适应能量补偿，提升细节并保持数值稳定；2) 时间连续性：使用三次Hermite样条描述轨迹，并加入时间曲率正则以平滑运动；3) 自适应初始化：结合深度估计、点跟踪与前景掩码，在早期训练形成稳定点云分布。

Result: 在 Tap-Vid DAVIS 上取得 SOTA：PSNR 35.49、SSIM 0.9433、LPIPS 0.0723；并展现出在插帧、深度一致性、视频编辑与双目视图合成等任务上的强泛化能力。

Conclusion: AdaGaR 同时解决了动态场景重建中的频率表达与时序连续性问题，带来稳定且细节丰富的重建，并在多任务上表现优异，具有通用的显式动态表示潜力。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>
