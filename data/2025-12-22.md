<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 109]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [V-Agent: An Interactive Video Search System Using Vision-Language Models](https://arxiv.org/abs/2512.16925)
*SunYoung Park,Jong-Hyeon Lee,Youngjune Kim,Daegyu Sung,Younghyun Yu,Young-rok Cha,Jeongho Ju*

Main category: cs.CV

TL;DR: V-Agent是一个多智能体的视频检索与对话系统：用小规模视频偏好数据微调VLM，并融合图像-文本检索向量；将视频帧与ASR转录独立嵌入同一多模态空间；由路由、搜索、聊天三代理协作，搜索代理含重排序模块；在MultiVENT 2.0上零样本达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统文本检索在多模态（视觉+语音）场景下表达受限；现有系统难以同时理解视频视觉内容与口语语义并进行互动对话式检索与结果优化。

Method: 1) 用小规模视频偏好数据对VLM进行微调以对齐检索偏好；2) 将外部图像-文本检索模型的向量作为额外检索信号与VLM融合；3) 将视频帧与ASR语音转写分别独立编码到共享多模态表征空间；4) 多智能体架构：路由代理理解意图与任务拆分，搜索代理用VLM检索+重排序提升结果，聊天代理与用户交互、细化查询与解释结果。

Result: 在MultiVENT 2.0基准上实现零样本SOTA表现，显示出优于传统文本检索与先前多模态方法的检索质量。

Conclusion: 通过VLM微调+外部检索向量融合和多代理协作，系统能够在理解视觉与语音内容的同时进行上下文感知的视频检索与交互，具有研究与应用潜力。

Abstract: We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.

</details>


### [2] [Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content](https://arxiv.org/abs/2512.16947)
*Reza Chandra,Adang Suhendra,Lintang Yuniar Banowosari,Prihandoko*

Main category: cs.CV

TL;DR: 研究比较CNN与VGG-16用于快速识别含色情图像的网站，CNN在特定训练设置下表现最佳（准确率94.87%）。


<details>
  <summary>Details</summary>
Motivation: 因印尼政府大量封锁含负面/色情内容网站，但公众仍可通过VPN访问，亟需自动、快速识别含色情图像的网站以辅助监管。

Method: 构建两种深度学习图像分类模型：自定义CNN与预训练/或迁移的VGG-16；通过多组实验（不同epoch与学习率等）对比其在识别色情图像上的性能与速度，选取最优配置。

Result: 在多次实验中，第八组实验（CNN，epoch=50，learning rate=0.001）取得最佳测试准确率0.9487（94.87%）；VGG-16相对较低。

Conclusion: 在本研究数据与设定下，CNN较VGG-16在快速且准确地检测色情内容方面更有效，适合作为网站色情图像识别的核心模型。

Abstract: In 2020, a total of 59,741 websites were blocked by the Indonesian government due to containing negative content, including pornography, with 14,266 websites falling into this category. However, these blocked websites could still be accessed by the public using virtual private networks (VPNs). This prompted the research idea to quickly identify pornographic content. This study aims to develop a system capable of identifying websites suspected of containing pornographic image content, using a deep learning approach with convolutional neural network (CNN) and visual geometry group 16 (VGG-16) model. The two models were then explored comprehensively and holistically to determine which model was most effective in detecting pornographic content quickly. Based on the findings of the comparison between testing the CNN and VGG-16 models, research results showed that the best test results were obtained in the eighth experiment using the CNN model at an epoch value level of 50 and a learning rate of 0.001 of 0.9487 or 94.87%. This can be interpreted that the CNN model is more effective in detecting pornographic content quickly and accurately compared to using the VGG-16 model.

</details>


### [3] [AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals](https://arxiv.org/abs/2512.16948)
*Qi Xu,Shuai Gong,Xuming Ran,Haihua Luo,Yangfan Hu*

Main category: cs.CV

TL;DR: 提出AVM框架：冻结ViT作为稳定视觉编码，增设可条件化的调制子网络分别适配刺激内容与个体差异；在多种设置下比SOTA V1T更准，跨数据集FEVE提升9.1%。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型虽能拟合神经反应，但难以区分稳定的视觉编码与由刺激或个体引起的适应，从而限制跨刺激、跨个体与跨数据集的泛化与可解释性。

Method: 结构保持与条件化适配相结合：将Vision Transformer编码器完全冻结以捕捉稳定视觉特征；并行的“调制路径”作为模块化子网络，针对两类条件（刺激内容、被试身份）学习对神经反应的可分解调制；在三类实验（刺激层级变化、跨被试泛化、跨数据集适配）上训练/评估，比较SOTA模型V1T。

Result: 在两套大规模小鼠V1数据上，AVM在预测相关性上优于V1T约2%，具有稳健泛化、按条件可解释的调制与高效架构；在跨数据集适配场景中，解释方差（FEVE）提升9.1%。

Conclusion: AVM提供了统一的、可扩展的神经建模方案：在不改变核心表示的前提下，通过条件化调制实现对生物与实验条件的适配；该设计可启发未来的大脑皮层建模与类脑AI方法。

Abstract: While deep learning models have shown strong performance in simulating neural responses, they often fail to clearly separate stable visual encoding from condition-specific adaptation, which limits their ability to generalize across stimuli and individuals. We introduce the Adaptive Visual Model (AVM), a structure-preserving framework that enables condition-aware adaptation through modular subnetworks, without modifying the core representation. AVM keeps a Vision Transformer-based encoder frozen to capture consistent visual features, while independently trained modulation paths account for neural response variations driven by stimulus content and subject identity. We evaluate AVM in three experimental settings, including stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all of which involve structured changes in inputs and individuals. Across two large-scale mouse V1 datasets, AVM outperforms the state-of-the-art V1T model by approximately 2% in predictive correlation, demonstrating robust generalization, interpretable condition-wise modulation, and high architectural efficiency. Specifically, AVM achieves a 9.1% improvement in explained variance (FEVE) under the cross-dataset adaptation setting. These results suggest that AVM provides a unified framework for adaptive neural modeling across biological and experimental conditions, offering a scalable solution under structural constraints. Its design may inform future approaches to cortical modeling in both neuroscience and biologically inspired AI systems.

</details>


### [4] [Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections](https://arxiv.org/abs/2512.16950)
*Adrian Straker,Paul Magdon,Marco Zullich,Maximilian Freudenberg,Christoph Kleinn,Johannes Breidenbach,Stefano Puliti,Nils Nölke*

Main category: cs.CV

TL;DR: 提出一种将Finer-CAM显著性与TLS投影结构片段相链接的新方法，分析YOLOv8树种分类模型的判别依据；在2445株七个欧洲树种上达到约96%准确率，发现模型主要依赖冠部（尤其细枝）特征，不同树种对茎干/冠部依赖有差异，模型的相似性判断与人类专家一致，强调解释性对发现偏差和提升信任的重要性。


<details>
  <summary>Details</summary>
Motivation: TLS与深度学习已能高精度识别树种，但其决策过程不透明；现有可解释方法如Finer-CAM能在投影中高亮贡献区域，却缺乏将这些高亮与具体结构特征（冠、枝、干等）系统关联的机制，从而难以量化哪个结构驱动类间区分。

Method: 利用七个欧洲树种（2445株）的TLS数据训练五个YOLOv8模型并交叉验证；生成Finer-CAM显著图并将其与TLS投影中的结构化分割片段（冠部、茎干、枝条/细枝等）对齐，统计630张显著图中不同结构区域对分类贡献的频率与强度，并分析模型间与树种间差异及与人类专家相似性。

Result: 五个YOLOv8模型平均准确率约96%（SD=0.24%）；显著性分析显示分类主要依赖冠部特征，尤其细枝表征；银桦、山毛榉、英国栎、挪威云杉更依赖冠部，而欧洲白蜡、苏格兰松、道格拉斯杉中茎干特征贡献更频繁；模型在树种相似性判断上与专家一致。

Conclusion: 将可解释性与结构片段绑定能系统揭示深度模型的判别依据，发现不同树种依赖的结构特征差异；结果提示需深化对模型决策机制的理解，以识别数据集/模型局限与偏差，并增强对预测的信任。

Abstract: Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.

</details>


### [5] [Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories](https://arxiv.org/abs/2512.16954)
*Chayan Jain,Rishant Sharma,Archit Garg,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: 提出一种分阶段、类电影制作流程的长视频生成方法：先由大语言模型产出制作脚本，再用文生图生成角色一致的视觉锚点，最后按场景用视频生成模型合成，显著提升角色一致性，并揭示跨文化偏差。


<details>
  <summary>Details</summary>
Motivation: 现有文生视频在长篇叙事与角色一致性上表现不佳，一步式生成难以保持视觉身份与连贯性；需要一种可控且可扩展的流程来像电影制作那样分工协作，提升一致性与叙事质量，并检视模型在不同文化主题下的偏差。

Method: 采用多阶段流水线：1) 用大语言模型生成细粒度制作脚本（场景划分、镜头设定、角色指示等）；2) 基于脚本用文生图为每个角色生成一致的视觉形象，作为视觉锚点；3) 以这些锚点为先验，逐场景调用视频生成模型合成片段；4) 与无锚点或一步式基线比较，并进行跨文化（印度 vs 西方）主题分析。

Result: 移除视觉锚点会导致角色一致性评分从7.99骤降到0.55，证明视觉先验对身份保持至关重要；同时发现当前模型在不同文化主题上呈现显著差异，包括主体一致性与动态程度的偏差。

Conclusion: 多阶段、脚本驱动并结合视觉锚点的生成流程能显著提升长视频角色一致性，是优于一步式的有效策略；同时需要关注并纠正模型在不同文化题材上的系统性偏差。

Abstract: Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.

</details>


### [6] [InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression](https://arxiv.org/abs/2512.16975)
*Haotian Ye,Qiyuan He,Jiaqi Han,Puheng Li,Jiaojiao Fan,Zekun Hao,Fitsum Reda,Yogesh Balaji,Huayu Chen,Sheng Liu,Angela Yao,James Zou,Stefano Ermon,Haoxiang Wang,Ming-Yu Liu*

Main category: cs.CV

TL;DR: InfoTok提出自适应视频离散标记化框架，基于信息论与ELBO优化，按信息密度分配token。在保持性能的同时平均节省20% token，并在2.3x压缩下仍优于先前启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频tokenizer以固定压缩率对所有片段一刀切，忽略视频时空信息密度的变化，导致冗余或信息丢失；缺乏从信息论出发的最优表示长度分析与可实现算法。

Method: 提出基于香农信息论的自适应token化理论框架，证明数据不可知训练策略在表示长度上非最优；建立ELBO驱动的训练目标，设计Transformer式自适应压缩器，根据“信息丰富度”动态分配token数，实现自适应离散视频表示。

Result: 在多项实验中达到SOTA压缩-性能权衡：在不降性能的前提下平均节省约20%的token；在2.3倍压缩时仍优于先前的启发式自适应方法。

Conclusion: 以信息论为基础的自适应视频token化可在更高压缩率下保持或提升性能；InfoTok提供了接近理论最优的训练与模型设计范式，为长视频处理与表示学习带来新思路。

Abstract: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.

</details>


### [7] [Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video](https://arxiv.org/abs/2512.16977)
*Hao Li,Daiwei Lu,Xing Yao,Nicholas Kavoussi,Ipek Oguz*

Main category: cs.CV

TL;DR: 提出Endo-SemiS：面向内镜视频的半监督分割框架，结合双网络互监督、基于不确定性的伪标签、双伪标签联合筛选、特征与图像层面的互学习，并配以时空校正网络；在肾结石碎石与息肉检测数据集上，在少标注条件下优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 内镜视频分割在临床决策与导航中关键，但高质量像素级标注昂贵且稀缺；现有半监督/一致性方法对复杂内镜场景（光照变化、运动模糊、器械遮挡）与视频时空信息利用不足，难以在少标注下稳健泛化。

Method: 搭建双分支半监督框架：1）交叉监督：两网络互为教师学生；2）不确定性引导伪标签：对未标注帧按置信度筛选高可靠区域；3）联合伪标签监督：融合两网络伪标签的交集/可靠像素以提高监督精度；4）互学习：在特征层与图像层进行一致性/蒸馏以减小方差并收敛至一致解；另设独立校正网络，利用视频时空线索对分割结果进行时序平滑与纠错。

Result: 在两项临床任务（输尿管镜肾结石激光碎石、结肠镜息肉筛查）上，相比SOTA分割方法，在有限标注比例下取得显著更优的性能指标（文中未给出具体数值于摘要）；公开代码提供可复现性。

Conclusion: 通过多重互监督与不确定性驱动的伪标签策略，并结合时空校正，Endo-SemiS能高效利用未标注内镜视频数据，在低标注场景实现更可靠的分割并优于现有方法。

Abstract: In this paper, we present Endo-SemiS, a semi-supervised segmentation framework for providing reliable segmentation of endoscopic video frames with limited annotation. EndoSemiS uses 4 strategies to improve performance by effectively utilizing all available data, particularly unlabeled data: (1) Cross-supervision between two individual networks that supervise each other; (2) Uncertainty-guided pseudo-labels from unlabeled data, which are generated by selecting high-confidence regions to improve their quality; (3) Joint pseudolabel supervision, which aggregates reliable pixels from the pseudo-labels of both networks to provide accurate supervision for unlabeled data; and (4) Mutual learning, where both networks learn from each other at the feature and image levels, reducing variance and guiding them toward a consistent solution. Additionally, a separate corrective network that utilizes spatiotemporal information from endoscopy video to improve segmentation performance. Endo-SemiS is evaluated on two clinical applications: kidney stone laser lithotomy from ureteroscopy and polyp screening from colonoscopy. Compared to state-of-the-art segmentation methods, Endo-SemiS substantially achieves superior results on both datasets with limited labeled data. The code is publicly available at https://github.com/MedICL-VU/Endo-SemiS

</details>


### [8] [A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos](https://arxiv.org/abs/2512.16978)
*Mohammed Irfan Kurpath,Jaseel Muhammad Kaithakkodan,Jinxing Zhou,Sahal Shaji Mullappilly,Mohammad Almansoori,Noor Ahsan,Beknur Kalmakhanbet,Sambal Shikhar,Rishabh Lalla,Jean Lahoud,Mariette Awad,Fahad Shahbaz Khan,Salman Khan,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 提出LongShOTBench与LongShOTAgent，用于长时多模态视频理解的可解释评测与基线。基准覆盖开放式意图驱动问答、单/多轮对话、跨视觉-语音-环境音的工具化推理，配套参考答案与分级评分细则。人类校验的数据与可复现实验显示当前SOTA模型存在显著性能差距，凸显任务难度。


<details>
  <summary>Details</summary>
Motivation: 现有基准要么强调时长（长序列），要么强调多模态丰富性，但很少同时兼顾；且多依赖单一分数，难以暴露失败模式与推理缺陷。需要一个可诊断、可追踪、可复现、覆盖真实长视频多模态理解挑战的评测框架。

Method: 1) 构建LongShOTBench：含开放式、意图驱动问题；单/多轮对话；需要结合视频、语音、环境音的多模态推理与工具使用任务。为每个样本提供参考答案与分级评分rubric，并以可扩展、经人工验证与纠正的流水线生产数据，保证覆盖度与可复现性。2) 提出LongShOTAgent：包含预处理、检索/搜索与迭代式细化的代理系统，用于长视频分析。

Result: 在LongShOTBench上，Gemini-2.5-Flash得分52.95%，开源MLLM低于30%，LongShOTAgent为44.66%。显示显著性能差距与任务难度。

Conclusion: LongShOTBench提供一个实用、可诊断、可复现的长时多模态视频理解评测基础，揭示当前MLLM在真实长视频场景中的明显短板；LongShOTAgent作为基线展示了代理式流程的潜力。资源在GitHub公开。

Abstract: Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.

</details>


### [9] [4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation](https://arxiv.org/abs/2512.17012)
*Chiao-An Yang,Ryo Hachiuma,Sifei Liu,Subhashree Radhakrishnan,Raymond A. Yeh,Yu-Chiang Frank Wang,Min-Hung Chen*

Main category: cs.CV

TL;DR: 提出4D-RGPT模型、P4D蒸馏框架与R4D-Bench基准，强化视频中的4D（时空+深度）感知与推理，在现有与新基准上均显著提升4D VQA表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在3D结构与时间动态推理薄弱；现有3D/4D VQA数据集偏静态、缺少区域级提示，难以评测与驱动模型获得强4D表征与时序理解。

Method: (a) 设计4D-RGPT，面向视频输入，增强时间感知以捕获4D表征；(b) 提出P4D感知蒸馏，将冻结的4D专家模型的表征迁移至4D-RGPT，实现更全面的4D感知；(c) 构建R4D-Bench：面向深度感知的动态场景、支持区域级提示，采用自动化+人工校验的混合流水线生成。

Result: 4D-RGPT在已有4D VQA基准与新提出的R4D-Bench上均取得显著改进。

Conclusion: 通过专用模型、感知蒸馏与新基准的结合，显著提升MLLM在4D理解与推理上的能力，并提供更贴合动态深度与区域级问答的评测框架。

Abstract: Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.

</details>


### [10] [FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring](https://arxiv.org/abs/2512.17021)
*Martin Schwartz,Fajwel Fogel,Nikola Besic,Damien Robert,Louis Geist,Jean-Pierre Renaud,Jean-Matthieu Monnet,Clemens Mosig,Cédric Vega,Alexandre d'Aspremont,Loic Landrieu,Philippe Ciais*

Main category: cs.CV

TL;DR: FORMSpoT 构建了2014-2024年、法国本土范围内、1.5米分辨率的年际森林冠层高度与扰动图（FORMSpoT-Δ），可在单株尺度监测森林变化，并在复杂山区显著优于现有产品。


<details>
  <summary>Details</summary>
Motivation: 欧洲森林碳汇近年下滑，迫切需要高时空分辨率、可频繁更新的监测工具；现有卫星扰动产品空间过粗，难以捕捉<100 m²的树木级变化。

Method: 以SPOT-6/7年度合成影像为基础，使用分层Transformer（PVTv2）在高分辨率机载激光雷达（ALS）真值上训练，反演年际冠层高度；为保证多时相、多来源数据的稳健变化检测，提出配准与时空总变分去噪的后处理流程；产出全国级1.5 m冠高与年度扰动多边形（FORMSpoT-Δ）。

Result: 在19个ALS复访站点和5,087个国家森林清查样地验证下，FORMSpoT-Δ显著优于现有扰动产品；尤其在扰动小且破碎的山区林分，F1=0.44，较基线方法提升约一个数量级。

Conclusion: FORMSpoT-Δ实现了国家尺度的树木级动态监测，可用于分析经营实践、早期衰退信号检测，以及量化间伐/选择性采伐等细微扰动导致的碳损失；结果强调维持SPOT等超高分辨率卫星任务与DINAMIS等开放数据计划对气候变化情境下森林监测的关键性。

Abstract: The recent decline of the European forest carbon sink highlights the need for spatially explicit and frequently updated forest monitoring tools. Yet, existing satellite-based disturbance products remain too coarse to detect changes at the scale of individual trees, typically below 100 m$^{2}$. Here, we introduce FORMSpoT (Forest Mapping with SPOT Time series), a decade-long (2014-2024) nationwide mapping of forest canopy height at 1.5 m resolution, together with annual disturbance polygons (FORMSpoT-$Δ$) covering mainland France. Canopy heights were derived from annual SPOT-6/7 composites using a hierarchical transformer model (PVTv2) trained on high-resolution airborne laser scanning (ALS) data. To enable robust change detection across heterogeneous acquisitions, we developed a dedicated post-processing pipeline combining co-registration and spatio-temporal total variation denoising. Validation against ALS revisits across 19 sites and 5,087 National Forest Inventory plots shows that FORMSpoT-$Δ$ substantially outperforms existing disturbance products. In mountainous forests, where disturbances are small and spatially fragmented, FORMSpoT-$Δ$ achieves an F1-score of 0.44, representing an order of magnitude higher than existing benchmarks. By enabling tree-level monitoring of forest dynamics at national scale, FORMSpoT-$Δ$ provides a unique tool to analyze management practices, detect early signals of forest decline, and better quantify carbon losses from subtle disturbances such as thinning or selective logging. These results underscore the critical importance of sustaining very high-resolution satellite missions like SPOT and open-data initiatives such as DINAMIS for monitoring forests under climate change.

</details>


### [11] [Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation](https://arxiv.org/abs/2512.17040)
*Min-Jung Kim,Jeongho Kim,Hoiyeong Jin,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: InfCam 提出一种无需深度估计的相机可控视频到视频生成方法，通过在扩散模型潜空间中直接编码相机旋转，实现高姿态一致性与高视觉质量，并配合合成多视角数据增强以覆盖多样轨迹与焦距，优于现有基线且可从合成数据泛化到真实场景。


<details>
  <summary>Details</summary>
Motivation: 现有相机可控视频生成难以同时保证对给定相机位姿的忠实性与跨视角一致性，主要受限于：1）基于重投影的方法对深度误差高度敏感，导致伪影与姿态偏差；2）可用数据集中相机轨迹多样性不足，限制了模型对复杂运动与焦距变化的学习与泛化。

Method: 提出 InfCam：
- 无限单应（infinite homography）变换将三维相机旋转直接编码到视频扩散模型的2D潜空间，作为无噪声的条件输入；
- 通过端到端训练预测剩余视差（parallax）残差项，以补偿仅旋转单应无法解释的运动，实现高姿态一致性；
- 设计数据增强管线，将现有合成多视角数据转化为具有多样轨迹与焦距的序列，提升模型对相机运动与镜头变化的覆盖。

Result: 在相机姿态精度与视觉保真度上优于基线方法；对深度误差不敏感，能从合成数据有效泛化到真实世界视频。

Conclusion: 通过在潜空间中显式编码相机旋转并学习残余视差，结合多样化轨迹数据增强，InfCam 实现了无需深度的高姿态忠实相机可控视频生成，为后期制作中的电影级机位控制提供了更稳定与可泛化的方案。

Abstract: Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/

</details>


### [12] [Interpretable Similarity of Synthetic Image Utility](https://arxiv.org/abs/2512.17080)
*Panagiota Gatoula,George Dimas,Dimitris K. Iakovidis*

Main category: cs.CV

TL;DR: 提出IUS（可解释的效用相似度）指标，用于量化评估合成医疗影像与真实影像在CDS任务中的“效用相似性”，并验证可显著提升基于合成数据的分类性能。


<details>
  <summary>Details</summary>
Motivation: 目前评估合成医疗影像质量多依赖人工评测、Inception类分数或在合成数据上的分类精度，难以直接反映其在特定临床决策支持（CDS）任务中的“可用性”，且缺乏可解释性与跨模态泛化指标。因此需要一种既与任务效用对齐、又具有可解释性的量化评估方法。

Method: 受广义神经可加模型（GNAM）启发，构建IUS指标：通过基于临床相关图像特征的可解释模型，将合成集与真实集的“效用”进行对齐比较，度量其在训练DL型CDS系统时的相对贡献。不同于以Inception为基的分布相似度，IUS强调特征层面的可解释性，能指出为何某合成数据集比另一个更有用。

Result: 在多种彩色医疗影像数据集（内镜、皮肤镜、眼底）上，按IUS选择高效用相似性的合成图像可带来最高达54.6%的相对分类性能提升；同时在灰度X光与超声模态上也显示出方法的通用性。

Conclusion: IUS为评估合成医疗影像的任务导向、可解释相似度提供了通用工具，可帮助选择更“有用”的合成数据，从而显著提升DL型CDS系统性能。实现代码已开源（https://github.com/innoisys/ius）。

Abstract: Synthetic medical image data can unlock the potential of deep learning (DL)-based clinical decision support (CDS) systems through the creation of large scale, privacy-preserving, training sets. Despite the significant progress in this field, there is still a largely unanswered research question: "How can we quantitatively assess the similarity of a synthetically generated set of images with a set of real images in a given application domain?". Today, answers to this question are mainly provided via user evaluation studies, inception-based measures, and the classification performance achieved on synthetic images. This paper proposes a novel measure to assess the similarity between synthetically generated and real sets of images, in terms of their utility for the development of DL-based CDS systems. Inspired by generalized neural additive models, and unlike inception-based measures, the proposed measure is interpretable (Interpretable Utility Similarity, IUS), explaining why a synthetic dataset could be more useful than another one in the context of a CDS system based on clinically relevant image features. The experimental results on publicly available datasets from various color medical imaging modalities including endoscopic, dermoscopic and fundus imaging, indicate that selecting synthetic images of high utility similarity using IUS can result in relative improvements of up to 54.6% in terms of classification performance. The generality of IUS for synthetic data assessment is demonstrated also for greyscale X-ray and ultrasound imaging modalities. IUS implementation is available at https://github.com/innoisys/ius

</details>


### [13] [DGH: Dynamic Gaussian Hair](https://arxiv.org/abs/2512.17094)
*Junying Wang,Yuanlu Xu,Edith Tretschk,Ziyan Wang,Anastasia Ianina,Aljaz Bozic,Ulrich Neumann,Tony Tung*

Main category: cs.CV

TL;DR: DGH提出以动态3D高斯为核心的、端到端可微渲染的数据驱动方法，实现多发型、多运动下的写实动态头发重建与渲染，取代大量手调与重计算的物理仿真流程。


<details>
  <summary>Details</summary>
Motivation: 现有动态头发生成依赖静态捕获+物理仿真：需要繁琐的参数调优、难以覆盖多样发型与运动，且高质量外观渲染计算昂贵；在复杂遮挡与多次散射下保持时序一致与视角一致更难。需要一种可扩展、数据驱动、训练可泛化的方法。

Method: 提出Dynamic Gaussian Hair (DGH)：1) 粗到细的时序一致动力学模型，学习跨多发型的头发运动；2) 基于发丝引导的优化模块，学习可微渲染的动态3D高斯表示以表征头发外观，实现在运动下的视角一致外观；可无缝集成到3D高斯头像框架，端到端梯度学习。

Result: 在几何与外观上取得有竞争力/优异效果，能跨多发型与头动序列泛化；较仿真法更高效、可扩展、无需繁重手工调参与重计算。

Conclusion: DGH提供了物理仿真的可扩展数据驱动替代方案：通过动态3D高斯与可微渲染，实现真实感、可动画的头发表示，并易于集成至高保真3D头像系统。

Abstract: The creation of photorealistic dynamic hair remains a major challenge in digital human modeling because of the complex motions, occlusions, and light scattering. Existing methods often resort to static capture and physics-based models that do not scale as they require manual parameter fine-tuning to handle the diversity of hairstyles and motions, and heavy computation to obtain high-quality appearance. In this paper, we present Dynamic Gaussian Hair (DGH), a novel framework that efficiently learns hair dynamics and appearance. We propose: (1) a coarse-to-fine model that learns temporally coherent hair motion dynamics across diverse hairstyles; (2) a strand-guided optimization module that learns a dynamic 3D Gaussian representation for hair appearance with support for differentiable rendering, enabling gradient-based learning of view-consistent appearance under motion. Unlike prior simulation-based pipelines, our approach is fully data-driven, scales with training data, and generalizes across various hairstyles and head motion sequences. Additionally, DGH can be seamlessly integrated into a 3D Gaussian avatar framework, enabling realistic, animatable hair for high-fidelity avatar representation. DGH achieves promising geometry and appearance results, providing a scalable, data-driven alternative to physics-based simulation and rendering.

</details>


### [14] [Predictive Modeling of Maritime Radar Data Using Transformer Architecture](https://arxiv.org/abs/2512.17098)
*Bjorna Qesaraku,Jan Steckel*

Main category: cs.CV

TL;DR: 该综述指出：变压器在AIS轨迹与声呐帧预测已有应用，但在海事雷达帧预测上仍是空白；系统回顾了相关时空预测方法与数据；提出以变压器用于雷达帧预测是明确而迫切的研究方向。


<details>
  <summary>Details</summary>
Motivation: 海事自主系统需要在复杂海况下稳定预判船舶运动与环境。雷达具备全天候可靠性，却缺乏基于变压器的帧预测研究；相比之下，AIS与声呐领域已有验证。弥补这一缺口可显著提升自主航行感知与规划的鲁棒性。

Method: 开展系统性文献综述：围绕与海事雷达相关的时空序列预测方法，重点梳理变压器架构；按数据类型（AIS、声呐、雷达等）、网络架构（时空注意力、编码器-解码器、混合卷积-注意力等）与预测时域（短/中/长时域）进行对比分析与归纳。

Result: 发现已有工作在AIS轨迹预测与声呐帧预测中证实了变压器的可行性与优势，但尚无针对海事雷达帧预测的变压器研究；由此明确界定了当前研究现状与差距。

Conclusion: 存在清晰研究空白：缺乏变压器用于海事雷达帧预测的研究。建议将变压器时空建模策略迁移到雷达数据上，并面向不同预测时域与数据模态开展基准、数据集与评价协议建设，以推动该方向发展。

Abstract: Maritime autonomous systems require robust predictive capabilities to anticipate vessel motion and environmental dynamics. While transformer architectures have revolutionized AIS-based trajectory prediction and demonstrated feasibility for sonar frame forecasting, their application to maritime radar frame prediction remains unexplored, creating a critical gap given radar's all-weather reliability for navigation. This survey systematically reviews predictive modeling approaches relevant to maritime radar, with emphasis on transformer architectures for spatiotemporal sequence forecasting, where existing representative methods are analyzed according to data type, architecture, and prediction horizon. Our review shows that, while the literature has demonstrated transformer-based frame prediction for sonar sensing, no prior work addresses transformer-based maritime radar frame prediction, thereby defining a clear research gap and motivating a concrete research direction for future work in this area.

</details>


### [15] [SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction](https://arxiv.org/abs/2512.17137)
*Puyang Wang,Pengfei Guo,Keyi Chai,Jinyuan Zhou,Daguang Xu,Shanshan Jiang*

Main category: cs.CV

TL;DR: 提出SDUM通用深度展开MRI重建框架，跨协议泛化强、可随级联深度可预测扩展，夺得多项基准SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习MRI重建多为特定协议定制，难以在解剖部位、对比、采样与加速因子等广泛临床场景泛化与部署，需要一个单模统一、可扩展、在异质数据上稳定表现的通用方法。

Method: 提出Scalable Deep Unrolled Model（SDUM）：(1) 以Restormer为重建主干；(2) 学习式coil sensitivity map estimator（CSME），并在每个级联中更新；(3) 采样感知的加权数据一致性（SWDC），适配不同采样策略；(4) 对级联索引与协议元数据进行通用条件编码（UC）；(5) 逐步增加级联数的渐进式训练策略，实现可扩展深度。

Result: 显示“类基础模型”缩放规律：PSNR与参数量呈对数关系，相关系数r=0.986（R^2=0.973），至18级联仍线性提升。在CMRxRecon2025四条赛道（多中心、多疾病、5T、儿科）单模型无特定微调即达SOTA，较专用基线最高+1.0 dB；在CMRxRecon2024较PromptMR+提升+0.55 dB；在fastMRI脑部较PC-RNN提升+1.8 dB。消融：SWDC较标准DC +0.43 dB；逐级CSME +0.51 dB；UC +0.38 dB。

Conclusion: SDUM以统一架构和条件化训练在异构协议上实现强泛化与可预测的规模化收益，是通向通用、可扩展MRI重建的实用方案。

Abstract: Clinical MRI encompasses diverse imaging protocols--spanning anatomical targets (cardiac, brain, knee), contrasts (T1, T2, mapping), sampling patterns (Cartesian, radial, spiral, kt-space), and acceleration factors--yet current deep learning reconstructions are typically protocol-specific, hindering generalization and deployment. We introduce Scalable Deep Unrolled Model (SDUM), a universal framework combining a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training. SDUM exhibits foundation-model-like scaling behavior: reconstruction quality follows PSNR ${\sim}$ log(parameters) with correlation $r{=}0.986$ ($R^2{=}0.973$) up to 18 cascades, demonstrating predictable performance gains with model depth. A single SDUM trained on heterogeneous data achieves state-of-the-art results across all four CMRxRecon2025 challenge tracks--multi-center, multi-disease, 5T, and pediatric--without task-specific fine-tuning, surpassing specialized baselines by up to ${+}1.0$~dB. On CMRxRecon2024, SDUM outperforms the winning method PromptMR+ by ${+}0.55$~dB; on fastMRI brain, it exceeds PC-RNN by ${+}1.8$~dB. Ablations validate each component: SWDC ${+}0.43$~dB over standard DC, per-cascade CSME ${+}0.51$~dB, UC ${+}0.38$~dB. These results establish SDUM as a practical path toward universal, scalable MRI reconstruction.

</details>


### [16] [Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps](https://arxiv.org/abs/2512.17143)
*Sandeep Mishra,Yasamin Jafarian,Andreas Lugmayr,Yingwei Li,Varsha Ramakrishnan,Srivatsan Varadharajan,Alan C. Bovik,Ira Kemelmacher-Shlizerman*

Main category: cs.CV

TL;DR: 论文提出一种将普通人像照片转化为“专业摄影风格”肖像的方法，关键在于在改变姿态、光照、服装与背景的同时保留个人身份特征。通过将人脸与身体映射到规范化UV空间并结合重姿态与多图微调个性化，方法在真实数据上获得高质量、可重姿的人像生成效果。


<details>
  <summary>Details</summary>
Motivation: 日常自拍或随拍往往缺乏专业摄影中的优质光照、姿态与整体美感；缺少成对数据（同一人在自然场景与专业棚拍中的对应照片）使得直接监督学习困难，需要无配对数据可利用且能强约束身份保持的方法。

Method: 1) 将输入脸部与人体几何统一到规范UV空间，并结合重姿态（novel view synthesis与遮挡建模）在UV域进行操作，从而利用现有无配对数据进行训练；2) 通过多图微调对特定身份进行个性化，使输出在标准服装、简洁背景、良好光照下仍能保留独特面部与身体特征。

Result: 在真实世界图像上生成高质量、可重姿的专业风格肖像；在定性与定量评估中均表现优异，说明身份保持与画面质量兼顾。

Conclusion: 利用UV空间表示结合重姿态与个性化微调，可在无成对数据条件下，把普通照片转换为专业肖像，并有效保持个人身份特征与细节。

Abstract: Photographs of people taken by professional photographers typically present the person in beautiful lighting, with an interesting pose, and flattering quality. This is unlike common photos people can take of themselves. In this paper, we explore how to create a ``professional'' version of a person's photograph, i.e., in a chosen pose, in a simple environment, with good lighting, and standard black top/bottom clothing. A key challenge is to preserve the person's unique identity, face and body features while transforming the photo. If there would exist a large paired dataset of the same person photographed both ``in the wild'' and by a professional photographer, the problem would potentially be easier to solve. However, such data does not exist, especially for a large variety of identities. To that end, we propose two key insights: 1) Our method transforms the input photo and person's face to a canonical UV space, which is further coupled with reposing methodology to model occlusions and novel view synthesis. Operating in UV space allows us to leverage existing unpaired datasets. 2) We personalize the output photo via multi image finetuning. Our approach yields high-quality, reposed portraits and achieves strong qualitative and quantitative performance on real-world imagery.

</details>


### [17] [Text-Conditioned Background Generation for Editable Multi-Layer Documents](https://arxiv.org/abs/2512.17151)
*Taewon Kang,Joseph K J,Chris Tensmeyer,Jihyung Kil,Wanrong Zhu,Ming C. Lin,Vlad I. Morariu*

Main category: cs.CV

TL;DR: 提出一个面向文档的多页一致背景生成与编辑框架：通过潜变量遮蔽保持文字可读，自动可读性优化(ARO)放置半透明底衬满足WCAG对比度，多页一致性靠摘要-指令递归引导，分层处理文本/图/背景，支持训练-free与用户风格提示。


<details>
  <summary>Details</summary>
Motivation: 现有生成式背景/排版方法难以同时满足：1) 文本区域不被破坏且可读；2) 多页主题连贯与可控演进；3) 在设计工作流中进行局部背景编辑而不影响文本/图；4) 无需复杂训练即可落地。

Method: 1) 潜变量遮蔽：在扩散模型潜空间对文本区域进行平滑抑制更新（受光滑势垒函数启发），避免文字被改写；2) ARO：自动在文本后放置半透明圆角底形，依据底图估计对比度，最小化不透明度以满足WCAG 2.2感知对比；3) 多页一致性：对每页进行摘要，作为紧凑表示递归指导后续页生成，保持主题/动机延续；4) 分层合成：将文档视为文本、图像、背景的独立层，定向只编辑背景；5) 用户提示控制：允许色彩与纹理风格化；6) 训练-free：不需再训练。

Result: 在无需训练的条件下，生成的文档背景在多页上保持风格连续；文本区域被保留且通过ARO达到标准化可读对比；可对背景进行定向编辑而不破坏前景，且可通过提示实现审美定制。

Conclusion: 该框架把扩散生成与排版设计流程连接起来，实现多页一致、文本可读且可定制的文档背景生成；以轻量、训练-free的方式提升实际文档设计工作流的效率与质量。

Abstract: We present a framework for document-centric background generation with multi-page editing and thematic continuity. To ensure text regions remain readable, we employ a \emph{latent masking} formulation that softly attenuates updates in the diffusion space, inspired by smooth barrier functions in physics and numerical optimization. In addition, we introduce \emph{Automated Readability Optimization (ARO)}, which automatically places semi-transparent, rounded backing shapes behind text regions. ARO determines the minimal opacity needed to satisfy perceptual contrast standards (WCAG 2.2) relative to the underlying background, ensuring readability while maintaining aesthetic harmony without human intervention. Multi-page consistency is maintained through a summarization-and-instruction process, where each page is distilled into a compact representation that recursively guides subsequent generations. This design reflects how humans build continuity by retaining prior context, ensuring that visual motifs evolve coherently across an entire document. Our method further treats a document as a structured composition in which text, figures, and backgrounds are preserved or regenerated as separate layers, allowing targeted background editing without compromising readability. Finally, user-provided prompts allow stylistic adjustments in color and texture, balancing automated consistency with flexible customization. Our training-free framework produces visually coherent, text-preserving, and thematically aligned documents, bridging generative modeling with natural design workflows.

</details>


### [18] [PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics](https://arxiv.org/abs/2512.17152)
*Nan Zhou,Huandong Wang,Jiahao Li,Yang Li,Xiao-Ping Zhang,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出PhysFire-WM：一个引入物理先验的世界模型，用于细粒度火势蔓延预测；通过从物理模拟器注入燃烧动力学先验，并用跨任务协同训练融合红外热辐射与火焰掩码边界信息，在提高物理一致性的同时增强几何精度，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有火灾预测多依赖二值掩码，信号稀疏难以刻画复杂火焰动力学；而通用世界模型虽能生成视频，但存在物理不一致与无法保证守恒、扩散等约束的问题，需要一种既能捕获热动力学又能保证物理合理性的模型。

Method: 构建物理引导的世界模型PhysFire-WM：1) 从物理模拟器提取结构化先验（燃烧/扩散/热辐射规律）注入模型，纠正世界模型的物理偏差；2) 设计跨任务协同训练CC-Train，以参数共享与梯度协调联合学习红外热图（连续热辐射）与火焰掩码（空间边界），缓解掩码信息稀疏；3) 在多模态细粒度数据上训练与验证。

Result: 在细粒度多模态火灾数据集上，PhysFire-WM在火势蔓延预测精度与物理一致性指标上均显著优于现有方法；消融显示物理先验和CC-Train均带来明显增益。

Conclusion: 将物理先验注入世界模型并进行跨任务协同，有助于同时提升热动力学真实性与几何边界精度，为灾害预测中的物理一致视频建模提供有效范式。

Abstract: Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.

</details>


### [19] [Can Synthetic Images Serve as Effective and Efficient Class Prototypes?](https://arxiv.org/abs/2512.17160)
*Dianxing Shi,Dingjie Fu,Yuqiao Liu,Jun Wang*

Main category: cs.CV

TL;DR: 提出LGCLIP：用LLM生成类别提示，引导扩散模型合成参考图像，作为视觉原型，仅用视觉编码器进行对比分类，实现零样本分类且无需图文配对数据。


<details>
  <summary>Details</summary>
Motivation: 现有VLM如CLIP依赖大规模高质量图文对，数据标注昂贵且存在噪声；双塔结构推理成本高、模型不够轻量。作者希望仅用类别标签、避免人工配对与双塔开销，仍能获得强零样本性能。

Method: 1) 以类别标签喂给LLM，生成类特定的高质量文字提示；2) 用该提示引导扩散模型合成该类别的“参考/原型”图像；3) 用单一视觉编码器分别提取真实图像与合成原型的特征；4) 通过对比相似度进行零样本预测；5) 优化提示生成策略以提升原型质量与判别性。

Result: 在零样本图像分类任务上取得优异结果与效率优势，验证无需图文对、仅用视觉编码器也可实现有效的跨模态对齐与识别。

Conclusion: LGCLIP提出一种以LLM+扩散模型合成视觉原型、再以单视觉塔对比判别的轻量范式，仅需类别标签即可实现零样本分类，减少数据与模型开销，为无配对监督的视觉-语言对齐提供新思路。

Abstract: Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)" framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.

</details>


### [20] [ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching](https://arxiv.org/abs/2512.17178)
*Qi Zhang,Yuxu Chen,Lei Deng,Lili Shen*

Main category: cs.CV

TL;DR: 提出无训练的ABE-CLIP，通过语义精炼与局部对齐增强CLIP的属性-对象绑定，显著提升组合匹配表现，优于需训练的方法。


<details>
  <summary>Details</summary>
Motivation: CLIP全局表征对细粒度属性绑定不敏感，常出现对象与属性错配。现有改进多依赖额外训练或大量难负样本，泛化差且未从根本上解决全局相似度忽略局部语义的问题。

Method: 在无需再训练的框架下：1) 语义精炼机制，对文本中“对象/属性”短语的token嵌入进行精炼，减少属性歧义并提升语义纯度；2) 局部token-图像patch对齐，计算精炼后的文本token与最相关图像patch的相似度，聚合这些局部相似度得到最终图文相似分数，用于图文匹配。

Result: 在多个数据集上，ABE-CLIP显著提升属性-对象绑定指标，且在组合泛化上超过需要大量训练或难负采样的方法。

Conclusion: 通过训练无关的语义精炼与局部对齐，可在不改变CLIP主干的情况下强化属性绑定，提升组合图文匹配与泛化能力，验证了细粒度局部相似度优于单纯全局表征。

Abstract: Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.

</details>


### [21] [It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities](https://arxiv.org/abs/2512.17186)
*Matias Quintana,Fangqi Liu,Jussi Torkko,Youlong Gu,Xiucheng Liang,Yujun Hou,Koichi Ito,Yihan Zhu,Mahmoud Abdelrahman,Tuuli Toivonen,Yi Lu,Filip Biljecki*

Main category: cs.CV

TL;DR: 研究比较城市绿化的客观指标（如GVI）与主观感知评分的差异，并解释这些差异的成因，发现地域位置对感知差异影响最大，而人口统计与人格因素作用很小。


<details>
  <summary>Details</summary>
Motivation: 城市绿地影响气候与福祉，规划需兼顾“实际有多少绿”与“人感觉多不多绿”。然而两者常不一致，缺乏定量刻画与成因解释，限制了基于感知的规划与评估。

Method: 基于街景图像获取客观绿量（如GVI等上下文视觉特征），并收集来自五国1000名受试者的大规模城市视觉感知调查（含人口统计与人格）。通过分析主观成对比较评分与客观指标的偏差，建模其与人因（年龄等）、地理与空间视觉特征（场景绿地空间分布等）的关系，比较跨地域一致性。

Result: 主客观差异在全球范围相近；主观感知与客观绿量总体相关，但人口统计与人格对感知差异影响不显著；影响主观感知的首要因素包括人居住地与图像来源地（地理位置相关因素），提示文化/环境/经验在塑造感知中的重要性。

Conclusion: 城市绿化评估不应仅依赖客观指标；需纳入位置与环境语境以解释和校准公众感知。跨区域的模型需考虑本地化适配，而对人口统计与人格的细分加权价值有限。

Abstract: Quantifying and assessing urban greenery is consequential for planning and development, reflecting the everlasting importance of green spaces for multiple climate and well-being dimensions of cities. Evaluation can be broadly grouped into objective (e.g., measuring the amount of greenery) and subjective (e.g., polling the perception of people) approaches, which may differ -- what people see and feel about how green a place is might not match the measurements of the actual amount of vegetation. In this work, we advance the state of the art by measuring such differences and explaining them through human, geographic, and spatial dimensions. The experiments rely on contextual information extracted from street view imagery and a comprehensive urban visual perception survey collected from 1,000 people across five countries with their extensive demographic and personality information. We analyze the discrepancies between objective measures (e.g., Green View Index (GVI)) and subjective scores (e.g., pairwise ratings), examining whether they can be explained by a variety of human and visual factors such as age group and spatial variation of greenery in the scene. The findings reveal that such discrepancies are comparable around the world and that demographics and personality do not play a significant role in perception. Further, while perceived and measured greenery correlate consistently across geographies (both where people and where imagery are from), where people live plays a significant role in explaining perceptual differences, with these two, as the top among seven, features that influences perceived greenery the most. This location influence suggests that cultural, environmental, and experiential factors substantially shape how individuals observe greenery in cities.

</details>


### [22] [Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences](https://arxiv.org/abs/2512.17188)
*Zhenbao Yu,Banglei Guan,Shunkun Liang,Zibin Liu,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种利用仿射对应、已知垂直方向的多相机-IMU系统广义相对位姿全局最优求解器，并在小旋转时给出线性快速解；在合成与真实数据上精度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多相机+IMU系统（如自动驾驶）广泛应用，精确估计相对位姿对定位与感知至关重要。现有方法在多相机广义相对位姿、利用仿射对应和已知垂直约束时，往往存在局部最优、精度或效率不足的问题，需要一个能全局最优且稳健的求解器。

Method: 1) 将广义相对位姿的旋转与平移解耦；2) 依据仿射对应的几何约束构建以相对旋转角为变量的代数误差代价函数；3) 通过特征方程与其一阶导数为零的条件，把全局优化转化为含两个未知量的两元多项式系统；4) 采用多项式特征值求解器得到相对旋转角，并由对应特征向量恢复平移；5) 在小旋转场景下提出线性闭式近似解以加速与稳健初始化。

Result: 在合成数据与真实数据集上评测，相比同类最先进方法，所提算法在相对位姿估计精度上更优，显示出更强的准确性与稳定性。

Conclusion: 基于仿射对应与已知垂直方向的广义相对位姿全局最优求解器能显著提升多相机系统的位姿估计精度；小旋转下的线性解进一步提升效率与适用性，具有实际应用潜力。

Abstract: Mobile devices equipped with a multi-camera system and an inertial measurement unit (IMU) are widely used nowadays, such as self-driving cars. The task of relative pose estimation using visual and inertial information has important applications in various fields. To improve the accuracy of relative pose estimation of multi-camera systems, we propose a globally optimal solver using affine correspondences to estimate the generalized relative pose with a known vertical direction. First, a cost function about the relative rotation angle is established after decoupling the rotation matrix and translation vector, which minimizes the algebraic error of geometric constraints from affine correspondences. Then, the global optimization problem is converted into two polynomials with two unknowns based on the characteristic equation and its first derivative is zero. Finally, the relative rotation angle can be solved using the polynomial eigenvalue solver, and the translation vector can be obtained from the eigenvector. Besides, a new linear solution is proposed when the relative rotation is small. The proposed solver is evaluated on synthetic data and real-world datasets. The experiment results demonstrate that our method outperforms comparable state-of-the-art methods in accuracy.

</details>


### [23] [Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs](https://arxiv.org/abs/2512.17189)
*Xiao Liang,Chenxi Liu,Zhi Ma,Di Wang,Bin Jing,Quan Wang,Yuanyuan Shi*

Main category: cs.CV

TL;DR: 提出一种名为ARCD的解码策略，通过解剖区域掩膜引导对比解码，在不重新训练的前提下，针对性抑制MedVLM的幻觉并提升区域理解与诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MedVLM常因依赖文本先验而忽视影像证据，产生幻觉。训练式方法需昂贵专家标注、扩展性差；训练免方法如对比解码提供的是全局且不具针对性的校正，在复杂临床环境下不稳定。需要一种数据高效、可控且能在特定解剖区域发挥作用的策略。

Method: 提出ARCD（Anatomical Region-Guided Contrastive Decoding）：利用解剖掩膜在解码时进行三层次（token、attention、logits）动态重加权，对模型注意力与输出进行区域化约束与引导，从而将视觉证据聚焦到指定解剖区域，抑制与区域无关或错误的输出。模块为即插即用、无需额外训练。

Result: 在胸部X光、CT、脑MRI、眼部超声等多数据集上，ARCD显著提升区域理解能力，减少幻觉，并提高总体诊断准确率，效果经系统实验验证。

Conclusion: 区域引导的对比解码可在无需再训练的条件下，为MedVLM提供可控且精细化的纠偏，提高临床可靠性与可推广性。

Abstract: Medical Vision-Language Models (MedVLMs) show immense promise in clinical applicability. However, their reliability is hindered by hallucinations, where models often fail to derive answers from visual evidence, instead relying on learned textual priors. Existing mitigation strategies for MedVLMs have distinct limitations: training-based methods rely on costly expert annotations, limiting scalability, while training-free interventions like contrastive decoding, though data-efficient, apply a global, untargeted correction whose effects in complex real-world clinical settings can be unreliable. To address these challenges, we introduce Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play strategy that mitigates hallucinations by providing targeted, region-specific guidance. Our module leverages an anatomical mask to direct a three-tiered contrastive decoding process. By dynamically re-weighting at the token, attention, and logits levels, it verifiably steers the model's focus onto specified regions, reinforcing anatomical understanding and suppressing factually incorrect outputs. Extensive experiments across diverse datasets, including chest X-ray, CT, brain MRI, and ocular ultrasound, demonstrate our method's effectiveness in improving regional understanding, reducing hallucinations, and enhancing overall diagnostic accuracy.

</details>


### [24] [Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening](https://arxiv.org/abs/2512.17202)
*Kai Liu,Zeli Lin,Weibo Wang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出Fose：把单步扩散与端到端模型融合，并用四阶段训练实现轻量高效的全色锐化；从50步蒸馏到1步，速度提升约7.4倍且精度更优。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在全色锐化中精度高但推理耗时、计算昂贵；端到端模型高效但缺少先验与结构简单导致性能受限。需要一种既保留扩散先验与重建质量、又具备端到端高效性的方案。

Method: 四阶段训练策略：1) 选择并增强SOTA扩散模型作为教师；2) 单步蒸馏，将原50步扩散过程压缩为1步以预测LRMSI与HRMSI的残差；3) 设计轻量级集成块，将一步DM与端到端网络融合；4) 进行联合/逐段训练与微调，形成轻量网络Fose。

Result: 在三个常用基准上，相比基线扩散模型实现约7.42倍推理加速，同时取得更优的全色锐化指标表现（具体数值未在摘要中给出），显示出效率与精度的显著提升。

Conclusion: 通过单步蒸馏和轻量融合结构，Fose兼顾扩散模型的先验与E2E模型的高效性，实现更快且更准的全色锐化；方法通用且代码已开源，有望推广至其他图像融合任务。

Abstract: Pansharpening is a significant image fusion task that fuses low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to obtain high-resolution multispectral images (HRMSI). The development of the diffusion models (DM) and the end-to-end models (E2E model) has greatly improved the frontier of pansharping. DM takes the multi-step diffusion to obtain an accurate estimation of the residual between LRMSI and HRMSI. However, the multi-step process takes large computational power and is time-consuming. As for E2E models, their performance is still limited by the lack of prior and simple structure. In this paper, we propose a novel four-stage training strategy to obtain a lightweight network Fose, which fuses one-step DM and an E2E model. We perform one-step distillation on an enhanced SOTA DM for pansharping to compress the inference process from 50 steps to only 1 step. Then we fuse the E2E model with one-step DM with lightweight ensemble blocks. Comprehensive experiments are conducted to demonstrate the significant improvement of the proposed Fose on three commonly used benchmarks. Moreover, we achieve a 7.42 speedup ratio compared to the baseline DM while achieving much better performance. The code and model are released at https://github.com/Kai-Liu001/Fose.

</details>


### [25] [Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs](https://arxiv.org/abs/2512.17206)
*Rujiao Long,Yang Li,Xingyao Zhang,Weixun Wang,Tianqianjin Lin,Xi Zhao,Yuchi Xu,Wenbo Su,Junchi Yan,Bo Zheng*

Main category: cs.CV

TL;DR: 提出“Reasoning Palette”：在生成前注入可采样的潜在上下文以调制推理轨迹，提升推理多样性、RL 探索效率与性能，并具可解释与可控性。


<details>
  <summary>Details</summary>
Motivation: 大模型在采样与RL训练中易产生重复、缺乏高层多样性的推理路径，导致探索低效、学习停滞；需要一种在生成前即可多样化“推理策略”的机制，兼顾可控性与效率。

Method: 设计潜在-调制框架：用VAE从问答对的均值池化表示中推断潜在变量z；在推理时采样z，并解码为可学习的“前缀token”，预置到输入prompt前，作为“策略上下文”调制整个生成轨迹；先经短暂SFT以适配潜在条件；在RL中通过按需注入不同z实现结构化探索。

Result: 在多项推理基准上，相比标准RL，方法实现持续学习与探索效率提升，并带来稳定的性能增益；同时体现出对策略风格/结构的可解释、可控调节。

Conclusion: 预生成阶段的潜在策略调制可有效扩大推理多样性、提升RL探索与收敛，且具可解释与可控优势，为(视觉)语言模型的推理提供通用增强手段。

Abstract: Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.

</details>


### [26] [CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency](https://arxiv.org/abs/2512.17213)
*Xiao Liang,Yuxuan An,Di Wang,Jiawei Hu,Zhicheng Jiao,Bin Jing,Quan Wang*

Main category: cs.CV

TL;DR: 提出CheXPO-v2，通过过程监督与知识图一致性奖励减少医学VLM幻觉，在MIMIC-CXR-VQA等上以仅5k样本达SOTA并生成可验证推理。


<details>
  <summary>Details</summary>
Motivation: 医学VLM常出现幻觉与冗长不可验证的思维链；基于稀疏结果奖励的GRPO等RL方法诱发“过度思考”，掩盖事实错误，存在安全风险。需从过程层面对推理进行细粒度纠偏。

Method: 设计以实体-关系匹配驱动的知识图一致性奖励：将推理步骤解析为“疾病-关系-解剖部位”三元组，按原子级一致性惩罚不连贯与幻觉；并结合困难样本挖掘。以此进行过程监督对齐，替代单纯结果导向的GRPO。

Result: 在MIMIC-CXR-VQA等基准上显著优于GRPO与SOTA；仅用约5k样本达到新的SOTA，推理更临床可信、可验证。

Conclusion: 过程监督结合知识图一致性奖励能有效抑制医学VLM的幻觉与冗余推理，提升数据效率与临床可靠性；CheXPO-v2验证了此范式并公开代码。

Abstract: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to "overthink" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured "Disease, Relation, Anatomy" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.

</details>


### [27] [DAVE: A VLM Vision Encoder for Document Understanding and Web Agents](https://arxiv.org/abs/2512.17221)
*Brandon Huang,Hang Hua,Zhuoran Yu,Trevor Darrell,Rogerio Feris,Roei Herzig*

Main category: cs.CV

TL;DR: 提出DAVE：针对文档理解与Web代理的VLM专用视觉编码器。使用两阶段训练（自监督预训练+有监督自回归预训练），并通过模型合并与特征集成提升对通用视觉与文档/Web任务的对齐。实验覆盖文档任务、VQA、Web定位与代理基准，表现强劲。


<details>
  <summary>Details</summary>
Motivation: 现有VLM常用视觉编码器在低层结构与空间信息上不足，限制了文档理解与Web代理等需要精细布局/定位的任务；同时，大规模标注成本高，需能利用海量无标数据与少量高质标注的训练方案。

Method: 训练管线两阶段：1) 自监督预训练：在海量无标图像上学习通用视觉表征；2) 有监督自回归预训练：用少量高质量数据学习解析（如版面解析）与定位等任务。为提升兼容性与表征力：i) 模型合并：将分别与不同文本解码器训练的编码器进行参数/表征层面的合并，增强对多种Web代理架构的适配；ii) 集成训练：将通用编码器（如SigLIP2）的特征与自研的文档/Web特征融合，联合优化。

Result: 在经典文档任务、VQA、Web定位及基于代理的多项基准上均取得显著效果，证明所提方法有效；显示出作为面向文档与Web应用的视觉编码器的优势。

Conclusion: DAVE通过自监督+有监督自回归两阶段训练，并结合模型合并与特征集成，有效补足VLM在结构与空间表征上的短板，兼顾通用视觉知识与文档/Web特定能力，成为面向文档与Web代理任务的强力视觉编码器。

Abstract: While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder's alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.

</details>


### [28] [Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing](https://arxiv.org/abs/2512.17224)
*Xuyang Li,Chenyu Li,Danfeng Hong*

Main category: cs.CV

TL;DR: 提出AOM通用光学遥感基础模型，能处理任意波段组合、传感器与分辨率，并在多数据集上于缺失波段、跨传感器与跨分辨率条件下达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RSFM通常在固定波段与分辨率上预训练，遇到真实场景的缺失波段、跨传感器融合与尺度变化时泛化差，限制了落地应用。

Method: 1) 频谱无关tokenizer：为每个通道分配专属band embedding，显式编码光谱身份，适应缺失或新增波段。2) 多尺度自适应patch嵌入：动态调节感受野以覆盖从亚米到百米纹理与上下文。3) 多尺度语义对齐：在不同分辨率间保持全局语义一致。4) 通道级自监督遮挡-重构：联合建模光谱-空间关系的预训练策略。

Result: 在包含Sentinel-2、Landsat、HLS等10余个公共数据集上，AOM在缺失波段、跨传感器和跨分辨率设置下均取得一致SOTA表现。

Conclusion: AOM打破固定波段/分辨率限制，实现对任意波段组合与尺度的鲁棒建模，显著提升RSFM在真实世界多样传感器与尺度条件下的泛化与实用性。

Abstract: Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.

</details>


### [29] [Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors](https://arxiv.org/abs/2512.17226)
*Son Tung Nguyen,Tobias Fischer,Alejandro Fontan,Michael Milford*

Main category: cs.CV

TL;DR: 提出一种聚合模块学习同时符合几何结构与视觉相似性的全局描述子，纠正仅基于几何的错误关联；用仅依赖重叠分数的批挖掘与改进对比损失，无需人工标签，跨场景泛化，实验证明在大规模定位中显著提升且高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的视觉定位多用全局描述子来区分外观相似地点，但常仅依赖几何线索（如共可见图）来构建监督或聚合，导致在几何噪声下判别力不足、易产生错误匹配与定位失败。需要一种同时利用外观与空间连通性的表示学习方式，以在噪声几何约束下仍保持鲁棒性。

Method: 提出一个聚合器模块，使图像全局描述子同时对齐视觉相似度与空间连通性：仅当图像在外观上相似且在空间上相邻（或有实际重叠）时，才在嵌入空间中接近。训练上，利用仅基于图像重叠分数的批挖掘策略选择正负样本，配合改进的对比损失进行弱监督学习，无需人工地点标签；通过该设计减轻不可靠重叠分数带来的错误关联。

Result: 在具有挑战性的、大规模环境基准上，方法在定位精度与召回上取得显著提升，同时保持计算与存储的高效性；展现了跨多样环境的良好泛化能力。

Conclusion: 联合几何与视觉一致性的全局描述子学习可显著提升大规模视觉定位的鲁棒性与精度；弱监督的批挖掘与改进对比损失使方法无需人工标注且具备良好泛化与效率。

Abstract: Recent learning-based visual localization methods use global descriptors to disambiguate visually similar places, but existing approaches often derive these descriptors from geometric cues alone (e.g., covisibility graphs), limiting their discriminative power and reducing robustness in the presence of noisy geometric constraints. We propose an aggregator module that learns global descriptors consistent with both geometrical structure and visual similarity, ensuring that images are close in descriptor space only when they are visually similar and spatially connected. This corrects erroneous associations caused by unreliable overlap scores. Using a batch-mining strategy based solely on the overlap scores and a modified contrastive loss, our method trains without manual place labels and generalizes across diverse environments. Experiments on challenging benchmarks show substantial localization gains in large-scale environments while preserving computational and memory efficiency. Code is available at \href{https://github.com/sontung/robust\_scr}{github.com/sontung/robust\_scr}.

</details>


### [30] [Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning](https://arxiv.org/abs/2512.17227)
*Siqi Yang,Zilve Gao,Haibo Qiu,Fanfan Liu,Peng Shi,Zhixiong Zeng,Qingmin Liao,Lin Ma*

Main category: cs.CV

TL;DR: 论文指出MLLM在长链视觉推理中会“越想越看不见”（视觉遗忘）。提出两阶段课程：先在纯文本上训练稳健“如何思考”的抽象推理，再用PG-CoT将推理锚定到视觉；随后用强化学习学习“何时看”，通过关键感知奖励让模型在不确定时触发感知，从而学会自主观看策略。


<details>
  <summary>Details</summary>
Motivation: 现有训练将抽象推理与视觉感知策略早期缠结，导致两类缺陷：1）冷启动：抽象推理骨干薄弱；2）策略性感知缺失：不会在何时查看图像，推理越长越丢视觉依据。需要解耦“如何思考”与“何时看”，以减少视觉遗忘，提升长链视觉推理稳健性。

Method: 提出课程式两阶段框架：阶段一——解耦SFT。先用纯文本数据训练强抽象推理（how-to-think），再引入PG-CoT（Perception-Grounded CoT），在推理链中显式插入与图像交互的感知步，将语言推理与视觉观测对齐与锚定。阶段二——策略学习。把“何时看”建模为强化学习问题，设计“关键感知奖励”（Pivotal Perception Reward），将感知动作与语言中的不确定性标记（如“wait”“verify”等）耦合，鼓励在不确定处触发观看，从而学得自主的视觉接入策略。

Result: 在长链、复杂视觉推理任务上显著减轻“视觉遗忘”，推理链更稳定且更具视觉支撑；模型从被动启发式观察转为主动、策略性地查看图像。

Conclusion: 通过先练“会思考”再练“会看”、并用RL学习观看时机，系统性缓解MLLM长链推理中的视觉遗忘，使模型从启发式观察者转变为战略性、具备自我落地能力的推理者；代码已开源。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate significant potential but remain brittle in complex, long-chain visual reasoning tasks. A critical failure mode is "visual forgetting", where models progressively lose visual grounding as reasoning extends, a phenomenon aptly described as "think longer, see less". We posit this failure stems from current training paradigms prematurely entangling two distinct cognitive skills: (1) abstract logical reasoning "how-to-think") and (2) strategic visual perception ("when-to-look"). This creates a foundational cold-start deficiency -- weakening abstract reasoning -- and a strategic perception deficit, as models lack a policy for when to perceive. In this paper, we propose a novel curriculum-based framework to disentangle these skills. First, we introduce a disentangled Supervised Fine-Tuning (SFT) curriculum that builds a robust abstract reasoning backbone on text-only data before anchoring it to vision with a novel Perception-Grounded Chain-of-Thought (PG-CoT) paradigm. Second, we resolve the strategic perception deficit by formulating timing as a reinforcement learning problem. We design a Pivotal Perception Reward that teaches the model when to look by coupling perceptual actions to linguistic markers of cognitive uncertainty (e.g., "wait", "verify"), thereby learning an autonomous grounding policy. Our contributions include the formalization of these two deficiencies and the development of a principled, two-stage framework to address them, transforming the model from a heuristic-driven observer to a strategic, grounded reasoner. \textbf{Code}: \url{https://github.com/gaozilve-max/learning-when-to-look}.

</details>


### [31] [Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos](https://arxiv.org/abs/2512.17229)
*Henghui Du,Chang Zhou,Chunjie Zhang,Xi Chen,Di Hu*

Main category: cs.CV

TL;DR: 提出VideoDetective：基于“问题感知记忆”的迭代压缩与检索机制，使MLLM在32K上下文限制下高效处理小时级长视频，并在多个基准上更好抓取关键线索。


<details>
  <summary>Details</summary>
Motivation: LVQA面临超长上下文与信息冗余，直接延长上下文或减少视觉token要么耗算力、要么丢关键信息；回答问题实际只需少量关键线索，需有问题驱动的高效记忆/检索机制。

Method: 将长视频切为子片段，针对每个子片段引入少量“问题感知”记忆token进行目的性压缩；把得到的记忆token递归聚合并作为历史上下文供后续子片段使用，从而在迭代过程中持续定位关键线索并减少视觉token；并提出GLVC数据集，用于评测跨整段视频的关键线索定位与问答能力。

Result: 在32K上下文的MLLM上，能高效处理约100K token（约3600帧、1小时@1fps），推理约2分钟、显存约37GB；在多个长视频基准上优于现有方法，能更有效寻找到关键信息。

Conclusion: 问题感知记忆与迭代压缩使MLLM在有限上下文下实现高效长视频理解与问答；GLVC为评估长视频关键信息定位能力提供了新的基准。

Abstract: Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. While existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation. In fact, when answering given questions, only a small amount of crucial information is required. Therefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens. Then, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. Furthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos. Experimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.

</details>


### [32] [Mitty: Diffusion-based Human-to-Robot Video Generation](https://arxiv.org/abs/2512.17253)
*Yiren Song,Cheng Liu,Weijia Mao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Mitty是一种基于扩散Transformer的视频“类上下文学习”方法，直接把人类演示视频翻译成机器人执行视频，不依赖关键点/轨迹等中间表征，并通过自动合成的人机配对数据缓解配对数据稀缺，达到SOTA并具备强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有从人类演示学习的方法常借助关键点、轨迹或动作标签等中间表示，带来信息丢失和误差累积，破坏时空一致性；且高质量配对数据匮乏，限制了端到端从人类视频到机器人控制的可扩展性。

Method: 提出Mitty：在预训练视频扩散模型基础上，采用Diffusion Transformer实现端到端Human2Robot视频生成。将人类演示视频编码为条件tokens，在扩散去噪过程中与机器人视频的去噪tokens通过双向注意力融合，无需动作标签或中间抽象。为缓解配对数据稀缺，构建自动合成流水线，从大规模第一人称数据集中生成高质量的人-机配对视频。

Result: 在Human2Robot与EPIC-Kitchens基准上达到SOTA，在未见过的环境中表现出强泛化能力；实验表明该范式对可扩展的从人类观察学习机器人策略提供新证据。

Conclusion: Mitty利用视频扩散与Transformer的时空先验，实现无需中间表示的端到端人到机视频翻译；结合自动合成配对数据，既提升精度与一致性，又增强泛化，为可扩展的机器人从人类观测学习提供了一条有效路径。

Abstract: Learning directly from human demonstration videos is a key milestone toward scalable and generalizable robot learning. Yet existing methods rely on intermediate representations such as keypoints or trajectories, introducing information loss and cumulative errors that harm temporal and visual consistency. We present Mitty, a Diffusion Transformer that enables video In-Context Learning for end-to-end Human2Robot video generation. Built on a pretrained video diffusion model, Mitty leverages strong visual-temporal priors to translate human demonstrations into robot-execution videos without action labels or intermediate abstractions. Demonstration videos are compressed into condition tokens and fused with robot denoising tokens through bidirectional attention during diffusion. To mitigate paired-data scarcity, we also develop an automatic synthesis pipeline that produces high-quality human-robot pairs from large egocentric datasets. Experiments on Human2Robot and EPIC-Kitchens show that Mitty delivers state-of-the-art results, strong generalization to unseen environments, and new insights for scalable robot learning from human observations.

</details>


### [33] [AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning](https://arxiv.org/abs/2512.17263)
*Dong Zifei,Wu Wenjie,Hao Jinkui,Chen Tianqi,Weng Ziqiao,Zhou Bo*

Main category: cs.CV

TL;DR: AnyCXR 以全合成监督训练，通过多阶段域随机化生成大规模、解剖一致且角度多样的合成胸片，并以条件联合标注正则化利用不完整/不完美标签，最终在真实PA/侧位/斜位数据上零样本实现54个结构的稳健分割，并提升下游临床任务表现。


<details>
  <summary>Details</summary>
Motivation: 现实CXR解剖分割受限于标注稀缺与采集条件多变；多角度、多器官的一致分割尤其困难，且获取全面高质量标注代价高。作者希望用可扩展的方式减少标注负担，同时在复杂真实域中保持鲁棒与泛化。

Method: 提出AnyCXR框架：1) 多阶段域随机化(MSDR)从3D CT体数据生成>10万张解剖可信、角度多样的合成X光（PA、侧位、斜位等），增强外观与几何多样性；2) 条件联合标注正则化(CAR)在潜在空间施加解剖一致性约束，能利用部分或不完美标签进行联合学习；整体仅用合成数据训练多器官分割网络。

Result: 在多套真实数据上实现强零样本泛化，准确分割54个解剖结构（跨PA/侧位/斜位）；生成的分割图用于心胸比估计、脊柱曲度评估、疾病分类等任务，注入解剖先验后下游诊断性能提升。

Conclusion: AnyCXR用纯合成监督构建可扩展、鲁棒的胸片解剖分割基础设施，减少真实标注需求，并在多种成像条件下保持可靠性，为解剖感知的CXR分析与临床应用提供实用路径。

Abstract: Robust anatomical segmentation of chest X-rays (CXRs) remains challenging due to the scarcity of comprehensive annotations and the substantial variability of real-world acquisition conditions. We propose AnyCXR, a unified framework that enables generalizable multi-organ segmentation across arbitrary CXR projection angles using only synthetic supervision. The method combines a Multi-stage Domain Randomization (MSDR) engine, which generates over 100,000 anatomically faithful and highly diverse synthetic radiographs from 3D CT volumes, with a Conditional Joint Annotation Regularization (CAR) learning strategy that leverages partial and imperfect labels by enforcing anatomical consistency in a latent space. Trained entirely on synthetic data, AnyCXR achieves strong zero-shot generalization on multiple real-world datasets, providing accurate delineation of 54 anatomical structures in PA, lateral, and oblique views. The resulting segmentation maps support downstream clinical tasks, including automated cardiothoracic ratio estimation, spine curvature assessment, and disease classification, where the incorporation of anatomical priors improves diagnostic performance. These results demonstrate that AnyCXR establishes a scalable and reliable foundation for anatomy-aware CXR analysis and offers a practical pathway toward reducing annotation burdens while improving robustness across diverse imaging conditions.

</details>


### [34] [WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.17278)
*Guoping Cai,Houjin Chen,Yanfeng Li,Jia Sun,Ziwei Chen,Qingzi Geng*

Main category: cs.CV

TL;DR: 提出WDFFU-Mamba：在U形Mamba架构中结合小波域增强与双注意力特征融合，用于BUS肿瘤分割；在两公共数据集上Dice与HD95显著优于现有方法，且高效泛化好。


<details>
  <summary>Details</summary>
Motivation: BUS分割受斑点噪声、伪影、病灶形态不规则与边界模糊影响，现有方法要么鲁棒性不足、上下文建模弱，要么计算开销大，亟需一种兼具鲁棒性、准确性与效率的模型。

Method: 构建U形Mamba主干，引入两大模块：1) WHF模块：利用小波去噪后的高频引导增强低层特征，抑制噪声并突出结构细节；2) DAFF模块：在跳连与深层语义间进行双注意力（通道/空间或Query-Key样式）融合，提升上下文一致性与边界辨识。整体保持轻量高效。

Result: 在两套公共BUS数据集上，相比SOTA模型，Dice更高、HD95更低，表明分割更准确且边界更稳定；同时推理/参数量保持较低，显示计算效率优势。

Conclusion: 小波域高频引导与注意力融合在Mamba式U-Net中互补，显著提升BUS分割的准确性与鲁棒性并具良好跨数据集泛化，具备临床应用潜力。

Abstract: Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.

</details>


### [35] [Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge](https://arxiv.org/abs/2512.17279)
*Zehui Lin,Luyi Han,Xin Wang,Ying Zhou,Yanming Zhang,Tianyu Zhang,Lingyun Bao,Shandong Wu,Dong Xu,Tao Tan,the UUSIC25 Challenge Consortium*

Main category: cs.CV

TL;DR: 评估通用深度学习模型在超声多器官分割与分类中的准确性与效率：在独立多中心测试集上表现优异但在未见域上泛化受限。


<details>
  <summary>Details</summary>
Motivation: 现有超声AI多为单任务工具，难以匹配临床中一机多能的使用需求；需要验证单一架构能否在多任务、多器官下达到可靠诊断性能与效率。

Method: 组织UUSIC25挑战：在11,644张训练图像（含公开/私有）上开发模型；用来自多中心、含一完全未参与训练中心的2,479张独立测试图像评估；以DSC与AUC衡量诊断性能，并记录推理时间与GPU显存作为效率指标。

Result: 15个有效算法中最佳模型SMART在5项分割任务宏平均DSC为0.854，二分类AUC为0.766；在分割任务上总体较强（如胎头DSC 0.942），但在受域迁移影响的复杂任务上表现波动；乳腺癌分子分型从内部AUC 0.571降至外部未见中心AUC 0.508，暴露显著泛化问题。

Conclusion: 单一通用模型可在多任务中实现较高准确性与效率，但在未见数据域上性能下降明显，域泛化能力是临床落地的关键瓶颈与未来研究重点。

Abstract: IMPORTANCE: Current ultrasound AI remains fragmented into single-task tools, limiting clinical utility compared to versatile modern ultrasound systems.
  OBJECTIVE: To evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation.
  DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images (public/private). Evaluation used an independent, multi-center test set of 2,479 images, including data from a center completely unseen during training to assess generalization.
  OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory).
  RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models showed high capability in segmentation (e.g., fetal head DSC: 0.942) but variability in complex tasks subject to domain shift. Notably, in breast cancer molecular subtyping, the top model's performance dropped from AUC 0.571 (internal) to 0.508 (unseen external center), highlighting generalization challenges.
  CONCLUSIONS: General-purpose AI models achieve high accuracy and efficiency across multiple tasks using a single architecture. However, performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.

</details>


### [36] [Vision-Language Model Guided Image Restoration](https://arxiv.org/abs/2512.17292)
*Cuixin Yang,Rongkang Dong,Kin-Man Lam*

Main category: cs.CV

TL;DR: 提出VLMIR：用VLM（如CLIP）的视觉与语言先验，结合扩散模型，通过跨注意力引导，实现兼顾像素保真与语义一致的通用与特定退化图像复原，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有IR方法难以同时利用视觉细节与语言语义；虽有把VLM引入IR的尝试，但未充分利用语言先验来保证复原过程的语义连贯与一致。

Method: 两阶段框架：1) 基于VLM特征提取：从输入低质图像提取互补的视觉与语言表示。通过LoRA微调，用余弦相似度损失对低质/高质图像的caption嵌入进行对齐；引入退化预测器将嵌入分解为退化成分与干净内容。2) 扩散式复原：将上述视觉与文本嵌入通过跨注意力注入扩散模型，引导重建。

Result: 在通用与特定退化的多种IR任务上取得SOTA或优于现有方法的性能；消融实验证实视觉-语言融合与嵌入对齐、退化分解等模块的有效性。

Conclusion: 融合VLM的视觉与语言先验能显著提升图像复原的感知与语义一致性；所提VLMIR为IR提供了有效范式，可广泛适用于多种退化场景。

Abstract: Many image restoration (IR) tasks require both pixel-level fidelity and high-level semantic understanding to recover realistic photos with fine-grained details. However, previous approaches often struggle to effectively leverage both the visual and linguistic knowledge. Recent efforts have attempted to incorporate Vision-language models (VLMs), which excel at aligning visual and textual features, into universal IR. Nevertheless, these methods fail to utilize the linguistic priors to ensure semantic coherence during the restoration process. To address this issue, in this paper, we propose the Vision-Language Model Guided Image Restoration (VLMIR) framework, which leverages the rich vision-language priors of VLMs, such as CLIP, to enhance IR performance through improved visual perception and semantic understanding. Our approach consists of two stages: VLM-based feature extraction and diffusion-based image restoration. In the first stage, we extract complementary visual and linguistic representations of input images by condensing the visual perception and high-level semantic priors through VLMs. Specifically, we align the embeddings of captions from low-quality and high-quality images using a cosine similarity loss with LoRA fine-tuning, and employ a degradation predictor to decompose degradation and clean image content embeddings. These complementary visual and textual embeddings are then integrated into a diffusion-based model via cross-attention mechanisms for enhanced restoration. Extensive experiments and ablation studies demonstrate that VLMIR achieves superior performance across both universal and degradation-specific IR tasks, underscoring the critical role of integrated visual and linguistic knowledge from VLMs in advancing image restoration capabilities.

</details>


### [37] [Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \\ via Self-Supervised Image Reconstruction](https://arxiv.org/abs/2512.17296)
*Wuyi Liu,Le Jin,Junxian Yang,Yuanchao Yu,Zishuo Peng,Jinfeng Xu,Xianzhi Li,Jun Zhou*

Main category: cs.CV

TL;DR: 提出HiSIR-Net用于高分辨率PCBA像素级缺陷定位：通过SIR-Gate在输入与重建之间自适应选择、结合带位置信息的ROPS跨分辨率一致地融合重叠patch重建，得到更干净的异常图并降低误报；并发布自采集SIPCBA-500数据集，实验显示在自有与公开数据集上更优且速度可用。


<details>
  <summary>Details</summary>
Motivation: PCBA装配板检测面临高分辨率、复杂纹理、标注稀缺与微小缺陷（仅数个像素）的问题。自监督重建方法常引入重建伪影导致误报，且在4K图像上patch融合不一致、定位不稳定，缺乏高分辨率PCBA数据集作为评测基准。

Method: 提出HiSIR-Net自监督重建框架，包含两大轻量模块：1) Selective Input-Reconstruction Gate (SIR-Gate)，让模型在像素/区域层面学会在原始输入与重建结果之间做选择，抑制与缺陷无关的重建伪影、降低FP；2) Region-level Optimized Patch Selection (ROPS) 带位置提示，对重叠patch重建进行区域级一致选择与融合，适配任意分辨率（含4K），生成连续、干净的异常图。并构建SIPCBA-500（500张）高分辨率PCBA数据集。

Result: 在SIPCBA-500与公开基准上进行大量实验，HiSIR-Net在像素级缺陷定位上优于现有方法，显著降低误报，且推理速度满足实际4K板卡检测需求。

Conclusion: 通过SIR-Gate与ROPS的有机结合，HiSIR-Net在高分辨率、少标注PCBA场景下实现更稳健的像素级异常定位；所发布的数据集补齐高分辨率PCBA评测空缺，为后续研究提供基准。

Abstract: Automated defect inspection of assembled Printed Circuit Board Assemblies (PCBA) is quite challenging due to the insufficient labeled data, micro-defects with just a few pixels in visually-complex and high-resolution images. To address these challenges, we present HiSIR-Net, a High resolution, Self-supervised Reconstruction framework for pixel-wise PCBA localization. Our design combines two lightweight modules that make this practical on real 4K-resolution boards: (i) a Selective Input-Reconstruction Gate (SIR-Gate) that lets the model decide where to trust reconstruction versus the original input, thereby reducing irrelevant reconstruction artifacts and false alarms; and (ii) a Region-level Optimized Patch Selection (ROPS) scheme with positional cues to select overlapping patch reconstructions coherently across arbitrary resolutions. Organically integrating these mechanisms yields clean, high-resolution anomaly maps with low false positive (FP) rate. To bridge the gap in high-resolution PCBA datasets, we further contribute a self-collected dataset named SIPCBA-500 of 500 images. We conduct extensive experiments on our SIPCBA-500 as well as public benchmarks, demonstrating the superior localization performance of our method while running at practical speed. Full code and dataset will be made available upon acceptance.

</details>


### [38] [ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration](https://arxiv.org/abs/2512.17298)
*Fanpu Cao,Yaofo Chen,Zeng You,Wei Luo,Cen Chen*

Main category: cs.CV

TL;DR: 提出ProCache：针对Diffusion Transformer的训练免缓存加速框架，通过非均匀激活调度与选择性计算，在PixArt-alpha与DiT上实现最高≈1.96×与2.90×加速且质量几乎不降。


<details>
  <summary>Details</summary>
Motivation: DiT推理成本高、难以实时部署。现有训练免缓存方法用固定间隔复用特征，既不能适配DiT在时间与层深上非均匀的特征演化，又在大间隔下易累积误差，导致画质下降。

Method: 1) 特征演化分析：揭示DiT在去噪过程中，特征变化与误差传播均随时间步与网络深度显著变化。2) 约束感知的缓存模式搜索：离线受约束采样，生成与时间动态匹配的非均匀激活/缓存调度（哪些步、哪些层重新计算）。3) 选择性计算：在被缓存的片段内，仅对深层块和高重要度token进行局部重算，抑制误差积累且开销很小。方法完全训练免。

Result: 在PixArt-alpha与DiT模型上，相比现有缓存类加速方法，以几乎无质量损失，实现最高约1.96×与2.90×的加速；在相同计算预算下，图像质量更优。

Conclusion: DiT特征与误差具有强时深非均匀性。利用离线非均匀调度与在线选择性重算，可在训练免的前提下显著加速且保持质量；ProCache优于以往统一间隔缓存方案。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.

</details>


### [39] [MatLat: Material Latent Space for PBR Texture Generation](https://arxiv.org/abs/2512.17302)
*Kyeongmin Yeo,Yunhong Min,Jaihoon Kim,Minhyuk Sung*

Main category: cs.CV

TL;DR: 提出一种在给定3D网格上生成高质量PBR纹理的生成式框架，通过微调预训练VAE学习材料潜空间MatLat，并引入局部性保持与对应感知注意，显著提升跨视角一致性与纹理保真度，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 大规模PBR纹理数据稀缺，直接训练困难；既有方法冻结嵌入网络，添加多通道(PBR)时产生潜空间分布偏移，破坏扩散先验训练；跨视角一致性常不足。需要一种能有效利用预训练扩散/潜空间先验并适配多材料通道且保持局部一致性的方案。

Method: 在预训练潜空间扩散模型基础上，针对PBR任务微调VAE，学习材料潜空间MatLat，使新增PBR通道(如法线、粗糙度等)被编码而潜分布最小偏移；提出对应感知注意力用于跨视角一致性；发现仅靠注意力不足，因潜到像素映射需保持局部性，于是引入局部性正则：裁剪潜变量patch、解码后与对应图像区域对齐，强化像素-潜变量的空间对应；整体框架以此支持在给定3D网格上生成高保真PBR纹理。

Result: 消融与基线对比显示：微调VAE以纳入PBR通道可减小分布漂移并提升扩散训练稳定性与质量；加入局部性正则显著改善跨视角一致性与细节保真；整体方法在PBR纹理保真度上达到SOTA。

Conclusion: 通过学习材料潜空间并在VAE中显式保持潜-像素局部对应，可更好地利用预训练扩散先验，解决多通道引发的分布偏移和跨视角一致性问题，从而在给定网格上生成更高质量的PBR纹理。

Abstract: We propose a generative framework for producing high-quality PBR textures on a given 3D mesh. As large-scale PBR texture datasets are scarce, our approach focuses on effectively leveraging the embedding space and diffusion priors of pretrained latent image generative models while learning a material latent space, MatLat, through targeted fine-tuning. Unlike prior methods that freeze the embedding network and thus lead to distribution shifts when encoding additional PBR channels and hinder subsequent diffusion training, we fine-tune the pretrained VAE so that new material channels can be incorporated with minimal latent distribution deviation. We further show that correspondence-aware attention alone is insufficient for cross-view consistency unless the latent-to-image mapping preserves locality. To enforce this locality, we introduce a regularization in the VAE fine-tuning that crops latent patches, decodes them, and aligns the corresponding image regions to maintain strong pixel-latent spatial correspondence. Ablation studies and comparison with previous baselines demonstrate that our framework improves PBR texture fidelity and that each component is critical for achieving state-of-the-art performance.

</details>


### [40] [EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance](https://arxiv.org/abs/2512.17303)
*Ankit Yadav,Ta Duc Huy,Lingqiao Liu*

Main category: cs.CV

TL;DR: 提出一种在扩散Transformer推断阶段的无训练新指导法EMAG，用统计驱动的自适应层选择与注意力修改，生成更“困难且语义一致”的负样本，细粒度修复伪影；相比CFG显著提升质量与人偏好分数，并可与APG、CADS等先进指导叠加增益。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比负样本的推断期指导方法虽能提升质量，但负样本“难度/粒度”不可控、目标层固定或经验性，难以稳定暴露细微失败模式，限制进一步提升与可组合性。

Method: 提出Exponential Moving Average Guidance（EMAG）：在扩散/流匹配的Transformer里，于推断期对自注意力进行修改。利用指数滑动平均统计构造更难但语义保真的负样本信号，并通过统计驱动的自适应层选择规则，动态决定在哪些层应用指导；无需额外训练，可与其他指导（APG、CADS）并行组合。

Result: 相较CFG，EMAG能产生更细粒度退化的对比样本，使去噪器聚焦微小伪影；在质量和人类偏好评分（HPS）上提升，报告+0.46 HPS；与APG、CADS叠加进一步提升。

Conclusion: EMAG是一种训练免、可自适应选择层位的注意力级指导方法，能够构造更“困难且语义一致”的负样本，显著提升扩散Transformer生成质量与人偏好，并具备与现有高级指导方法的可组合性。

Abstract: In diffusion and flow-matching generative models, guidance techniques are widely used to improve sample quality and consistency. Classifier-free guidance (CFG) is the de facto choice in modern systems and achieves this by contrasting conditional and unconditional samples. Recent work explores contrasting negative samples at inference using a weaker model, via strong/weak model pairs, attention-based masking, stochastic block dropping, or perturbations to the self-attention energy landscape. While these strategies refine the generation quality, they still lack reliable control over the granularity or difficulty of the negative samples, and target-layer selection is often fixed. We propose Exponential Moving Average Guidance (EMAG), a training-free mechanism that modifies attention at inference time in diffusion transformers, with a statistics-based, adaptive layer-selection rule. Unlike prior methods, EMAG produces harder, semantically faithful negatives (fine-grained degradations), surfacing difficult failure modes, enabling the denoiser to refine subtle artifacts, boosting the quality and human preference score (HPS) by +0.46 over CFG. We further demonstrate that EMAG naturally composes with advanced guidance techniques, such as APG and CADS, further improving HPS.

</details>


### [41] [Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images](https://arxiv.org/abs/2512.17306)
*Wenhao Yang,Yu Xia,Jinlong Huang,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Yuanyu Wan,Lijun Zhang*

Main category: cs.CV

TL;DR: DRIM提出一种让VLM在多模态CoT中进行多轮、可校正推理的方法，通过高难度可验证数据、冷启动SFT的工具轨迹学习，以及在RL中加入“冗余惩罚”的策略优化，促成自反思与多尺度探索，最终在视觉理解基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM虽可通过工具在图像上做推理，但一旦走入错误轨迹，缺乏自我反思与纠偏能力，导致复杂任务中的稳定性与可靠性不足。需要一种机制让模型在多轮推理中自评判路径质量并进行修正。

Method: 三阶段管线：1) 数据构建：基于高分辨率图像生成高难度、可验证的问答，每题需要多轮工具调用才能得出正确答案；2) 冷启动SFT：收集并监督工具使用的多轮轨迹，教会模型多步推理范式；3) 强化学习：提出带冗余惩罚的策略优化，对在缺乏充分多尺度探索却产生错误答案的轨迹进行惩罚，鼓励自反思、纠错与更有效的工具调用。

Result: 在多项视觉理解基准上取得优于现有VLM的成绩；实验表明模型在长链条、多轮、多工具情境下更稳健，错误率更低，能更好地自我评估与修正推理路径。

Conclusion: 通过构造可验证的多轮工具任务并在RL中引入冗余惩罚，DRIM实现了深度而可靠的多模态CoT推理，显著提升VLM在复杂视觉任务中的自反思与纠错能力。

Abstract: Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.

</details>


### [42] [CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning](https://arxiv.org/abs/2512.17312)
*Qi Song,Honglin Li,Yingchen Yu,Haoyi Zhou,Lin Yang,Song Bai,Qi She,Zilong Huang,Yunqing Zhao*

Main category: cs.CV

TL;DR: CodeDance 使用可执行代码作为通用“视觉推理器”，编排多种工具并渲染中间可视化，以实现可解释、可自检的多步视觉推理；通过平衡探索与效率的奖励在强化学习中涌现出新型工具调用与跨任务迁移，实验在视觉搜索、数学、图表问答等上超越模式化/纯文本基线与部分闭源模型（如 GPT-4o）。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态推理多依赖文本链、固定视觉模式或单步流程，缺乏灵活性、可解释性与跨任务可迁移性；而最新闭源趋势显示“带图思考”需要结构化工具调用与逐步验证。作者欲提供一种通用、可执行、可自检的视觉推理框架。

Method: 提出 CodeDance：以可执行代码为核心，将工具调用、计算中间结果、以及渲染可视化（框、线、图）纳入同一可编排流程；引入“平衡且自适应的工具调用奖励”，在强化学习中兼顾探索与效率、缓解工具滥用；采用原子级监督作为基础，再通过 RL 促使策略涌现更复杂的工具组合与跨任务行为。

Result: 在视觉搜索、数学题、图表问答等基准上，CodeDance 一致优于基于固定 schema 或纯文本的开源基线，并超过部分先进闭源（GPT-4o）与更大开源模型；训练中观察到新颖工具调用、未见过的工具组合及跨任务迁移，无需任务特定微调。

Conclusion: 可执行代码作为通用视觉推理中枢可带来透明、可验证、可迁移的多步推理能力；配合平衡工具使用的奖励与 RL，可在不依赖任务特定微调下实现性能与泛化的双提升，展示了一条可扩展的可执行视觉推理范式。

Abstract: Recent releases such as o3 highlight human-like "thinking with images" reasoning that combines structured tool use with stepwise verification, yet most open-source approaches still rely on text-only chains, rigid visual schemas, or single-step pipelines, limiting flexibility, interpretability, and transferability on complex tasks. We introduce CodeDance, which explores executable code as a general solver for visual reasoning. Unlike fixed-schema calls (e.g., only predicting bounding-box coordinates), CodeDance defines, composes, and executes code to orchestrate multiple tools, compute intermediate results, and render visual artifacts (e.g., boxes, lines, plots) that support transparent, self-checkable reasoning. To guide this process, we introduce a reward for balanced and adaptive tool-call, which balances exploration with efficiency and mitigates tool overuse. Interestingly, beyond the expected capabilities taught by atomic supervision, we empirically observe novel emergent behaviors during RL training: CodeDance demonstrates novel tool invocations, unseen compositions, and cross-task transfer. These behaviors arise without task-specific fine-tuning, suggesting a general and scalable mechanism of executable visual reasoning. Extensive experiments across reasoning benchmarks (e.g., visual search, math, chart QA) show that CodeDance not only consistently outperforms schema-driven and text-only baselines, but also surpasses advanced closed models such as GPT-4o and larger open-source models.

</details>


### [43] [Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model](https://arxiv.org/abs/2512.17313)
*SuBeen Lee,GilHan Park,WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出ADK框架：用LLM离线生成类别描述，作为组合知识与实例选择知识，参数无增加、推理高效，提升VLM少样本适配性能，刷新多场景SOTA。


<details>
  <summary>Details</summary>
Motivation: VLM零样本强但遇到分布偏移与少样本适配时表现下降。现有PEFT依赖固定人工prompt，难覆盖类别语义；基于图像动态prompt方案在推理时计算开销大。需要一种既丰富语义又不增加推理负担的方法。

Method: ADK离线用LLM为每个类别生成多样化描述性文本并编码成特征；在适配与推理时：1) 组合知识：将这些描述平均得到类别的丰富语义表示，缓解类名歧义或陌生词问题；2) 实例特定知识：使用轻量、非参数化注意力，根据输入图像与描述特征的相似度选择最相关描述并聚合；两路知识与人工prompt共同作为文本侧增强；ADK为参数无增、可插拔组件，可与多种PEFT方法结合。

Result: 在多数据集与多种PEFT基线上，ADK稳定提升性能，在不同分布移位与少样本设置中达到新的SOTA；相比基于图像动态prompt方法，推理时几乎无额外开销。

Conclusion: 通过离线生成并高效利用描述性知识，ADK在不牺牲推理效率的前提下显著增强VLM的少样本适配能力，可作为通用、可插拔增强模块广泛应用于PEFT方法。

Abstract: Despite the impressive zero-shot capabilities of Vision-Language Models (VLMs), they often struggle in downstream tasks with distribution shifts from the pre-training data. Few-Shot Adaptation (FSA-VLM) has emerged as a key solution, typically using Parameter-Efficient Fine-Tuning (PEFT) to adapt models with minimal data. However, these PEFT methods are constrained by their reliance on fixed, handcrafted prompts, which are often insufficient to understand the semantics of classes. While some studies have proposed leveraging image-induced prompts to provide additional clues for classification, they introduce prohibitive computational overhead at inference. Therefore, we introduce Auxiliary Descriptive Knowledge (ADK), a novel framework that efficiently enriches text representations without compromising efficiency. ADK first leverages a Large Language Model to generate a rich set of descriptive prompts for each class offline. These pre-computed features are then deployed in two ways: (1) as Compositional Knowledge, an averaged representation that provides rich semantics, especially beneficial when class names are ambiguous or unfamiliar to the VLM; and (2) as Instance-Specific Knowledge, where a lightweight, non-parametric attention mechanism dynamically selects the most relevant descriptions for a given image. This approach provides two additional types of knowledge alongside the handcrafted prompt, thereby facilitating category distinction across various domains. Also, ADK acts as a parameter-free, plug-and-play component that enhances existing PEFT methods. Extensive experiments demonstrate that ADK consistently boosts the performance of multiple PEFT baselines, setting a new state-of-the-art across various scenarios.

</details>


### [44] [A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs](https://arxiv.org/abs/2512.17319)
*Yunkai Dang,Meiyi Zhu,Donghao Wang,Yizhuo Zhang,Jiacheng Yang,Qi Fan,Yuekun Yang,Wenbin Li,Feng Miao,Yang Gao*

Main category: cs.CV

TL;DR: 提出RSHR-Bench：面向遥感的超高分辨率视觉理解与推理基准，涵盖4K+长边、最高3×10^8像素图像与四类任务，经对抗过滤与人工校验构建近8,300项标注；实验证明现有多模态模型在超高分辨率场景存在显著性能缺口，而仅文本LLM在旧基准上的“越级”表现暴露了基准与视觉理解目标的不匹配。


<details>
  <summary>Details</summary>
Motivation: 现有遥感多模态评测多为低分辨率或存在不严谨推理设计，导致语言先验即可取得高分，无法真实反映视觉理解能力；需要一个能考验超高分辨率感知与推理、并有效抑制语言捷径的新基准。

Method: 收集5,329张超高分辨率全景RS图像（长边≥4,000像素，最高约3×10^8像素）来自通用RS与UAV数据；设计四类任务（多选VQA、开放式VQA、图像描述、单图评估）覆盖九类感知与四类推理，支持多轮/多图对话；用强LLM做对抗式过滤以削弱语言先验，再由人工严格核验与撰写，最终形成3,864个VQA、3,913个描述、500个高质量单图评估对。

Result: 在开源、闭源及RS专用VLM上评测显示：在超高分辨率条件下性能普遍下降且差距明显；同时验证了仅文本LLM在旧基准上可与多模态竞争，佐证旧基准存在语言偏置。

Conclusion: RSHR-Bench能更忠实地评估遥感场景中的视觉理解与推理，显著降低语言先验影响并揭示当前VLM在超高分辨率上的不足，为未来模型设计与训练提供更可靠的测试场与改进方向。

Abstract: Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR

</details>


### [45] [EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories](https://arxiv.org/abs/2512.17320)
*Lu Wei,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 论文提出EMMA基准，系统评测T2I模型“概念擦除”方法在多维度、多场景下的有效性与副作用，发现现有方法在隐式提示与相似非目标概念上失效，并可能放大性别/族裔偏见。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法多在少量直白提示与简单场景上评估，难以回答“是否真的从表示中移除了目标概念”，且缺乏对社会偏见与鲁棒性的系统检验。

Method: 构建EMMA基准：覆盖5个域（物体、名人、艺术风格、NSFW、版权），设5个关键维度、12项指标，超越图像质量与效率，重点考察在隐式描述、与目标概念近似的非目标概念、以及性别和族裔偏见等挑战条件下的方法表现。用EMMA评测五种主流概念擦除方法。

Result: 所有被测方法在两方面普遍薄弱：（1）隐式提示下仍生成被擦除概念；（2）对视觉相近的非目标概念出现过度抑制，导致想要的输出失败。此外，一些方法相较原模型放大性别与族裔偏见。

Conclusion: 单纯依赖现有擦除技术难以安全可靠地移除不当概念；未来需面向隐式提示鲁棒性、相似概念可分辨性与社会偏见控制进行方法设计与评测。

Abstract: The widespread adoption of text-to-image (T2I) generation has raised concerns about privacy, bias, and copyright violations. Concept erasure techniques offer a promising solution by selectively removing undesired concepts from pre-trained models without requiring full retraining. However, these methods are often evaluated on a limited set of concepts, relying on overly simplistic and direct prompts. To test the boundaries of concept erasure techniques, and assess whether they truly remove targeted concepts from model representations, we introduce EMMA, a benchmark that evaluates five key dimensions of concept erasure over 12 metrics. EMMA goes beyond standard metrics like image quality and time efficiency, testing robustness under challenging conditions, including indirect descriptions, visually similar non-target concepts, and potential gender and ethnicity bias, providing a socially aware analysis of method behavior. Using EMMA, we analyze five concept erasure methods across five domains (objects, celebrities, art styles, NSFW, and copyright). Our results show that existing methods struggle with implicit prompts (i.e., generating the erased concept when it is indirectly referenced) and visually similar non-target concepts (i.e., failing to generate non-targeted concepts resembling the erased one), while some amplify gender and ethnicity bias compared to the original model.

</details>


### [46] [Rotterdam artery-vein segmentation (RAV) dataset](https://arxiv.org/abs/2512.17322)
*Jose Vargas Quiros,Bart Liefers,Karin van Garderen,Jeroen Vermeulen,Eyened Reading Center,Caroline Klaver*

Main category: cs.CV

TL;DR: 提出一个涵盖多设备、多条件的彩色眼底图像（CFI）动静脉（A/V）分割数据集，带连通性校验的高质量标注，用于机器学习模型训练与评估。


<details>
  <summary>Details</summary>
Motivation: 现有A/V分割数据集规模小、同质化、且缺乏严格连通性验证，限制了算法在真实复杂条件下的泛化与临床可用性。

Method: 从鹿特丹纵向队列研究中抽取多年龄段、跨设备与成像条件的CFI。使用定制标注界面在初始血管分割掩膜基础上分层标注动脉、静脉与未知血管，并借助连通成分可视化工具显式检查与修正血管连通性。输出包含原始RGB、对比度增强图及RGB编码A/V掩膜。

Result: 构建了分辨率为1024×1024的PNG数据集，质量分布广泛，含常被自动质控剔除但仍具血管信息的困难样本；所有A/V标注均经过连通性验证与修正。

Conclusion: 数据集为在真实世界变异下训练与基准测试A/V分割与血管分析模型提供了高质量、异质性强的资源。支持开发更鲁棒、可推广的临床级算法。

Abstract: Purpose: To provide a diverse, high-quality dataset of color fundus images (CFIs) with detailed artery-vein (A/V) segmentation annotations, supporting the development and evaluation of machine learning algorithms for vascular analysis in ophthalmology.
  Methods: CFIs were sampled from the longitudinal Rotterdam Study (RS), encompassing a wide range of ages, devices, and capture conditions. Images were annotated using a custom interface that allowed graders to label arteries, veins, and unknown vessels on separate layers, starting from an initial vessel segmentation mask. Connectivity was explicitly verified and corrected using connected component visualization tools.
  Results: The dataset includes 1024x1024-pixel PNG images in three modalities: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. Image quality varied widely, including challenging samples typically excluded by automated quality assessment systems, but judged to contain valuable vascular information.
  Conclusion: This dataset offers a rich and heterogeneous source of CFIs with high-quality segmentations. It supports robust benchmarking and training of machine learning models under real-world variability in image quality and acquisition settings.
  Translational Relevance: By including connectivity-validated A/V masks and diverse image conditions, this dataset enables the development of clinically applicable, generalizable machine learning tools for retinal vascular analysis, potentially improving automated screening and diagnosis of systemic and ocular diseases.

</details>


### [47] [DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training](https://arxiv.org/abs/2512.17323)
*Jiyun Kong,Jun-Hyuk Kim,Jong-Seok Lee*

Main category: cs.CV

TL;DR: 提出DESSERT：一种基于扩散模型、以事件数据为条件的单帧合成框架，通过残差学习实现时序一致且清晰的未来帧预测，优于现有重建/预测/插帧方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频帧预测在动态场景中因缺乏未来帧信息而易产生误差；事件相机可提供高时间分辨率的像素级变化，但以往方法多依赖事件流估计光流并做像素扭曲，位移不准会带来空洞和模糊，难以获得清晰且时序一致的结果。

Method: 利用预训练Stable Diffusion，采用“事件驱动+残差学习”的两阶段：1) ER-VAE将锚帧与目标帧之间的事件帧对齐到对应的帧间残差，实现事件到残差的对齐编码；2) 扩散模型在事件条件下对残差潜变量去噪，推理时将预测残差叠加到锚帧得到目标帧。并提出DLT多时长时序增强，在不同时间跨度的帧段上训练以增强鲁棒性和泛化。

Result: 在事件重建、图像式帧预测、事件式帧预测以及单向事件插帧等基线之上取得更好表现，生成的帧更锐利、空洞更少、且时序一致性更强。

Conclusion: 以事件数据为条件、通过残差扩散建模能有效替代光流扭曲路径，缓解位移误差引发的空洞与模糊；DLT增强进一步提升不同时间跨度下的稳健性，使框架在多类任务上达到先进性能。

Abstract: Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.

</details>


### [48] [Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling](https://arxiv.org/abs/2512.17326)
*Sander Moonemans,Sebastiaan Ram,Frédérique Meeuwsen,Carlijn Lems,Jeroen van der Laak,Geert Litjens,Francesco Ciompi*

Main category: cs.CV

TL;DR: 提出Polysome用于合成指令、构建HISTAI-Instruct（24,259张WSI与110万+指令-回答对），并训练VLM ANTONI-α，在全视野病理VQA任务上优于MedGemma，代码与数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在病理WSI场景存在：仅关注小区域、仅给静态整片级输出、或依赖非公开数据导致不可复现；同时缺乏成对的WSI—临床详细报告训练数据，限制了可解释、可泛化VLM的进展。

Method: 1) 提出Polysome：标准化的合成指令生成工具；2) 将Polysome应用于公开HISTAI数据集，生成HISTAI-Instruct（大规模WSI指令微调数据集）；3) 基于该数据训练VLM ANTONI-α，用于WSI级VQA；4) 在组织识别、肿瘤检出、鉴别诊断等任务评测，并比较不同训练数据规模的模型变体。

Result: ANTONI-α在WSI级VQA（组织类型识别、肿瘤检测、鉴别诊断）上优于MedGemma；不同数据量训练的版本呈现随数据增多而性能提升的趋势；全部方法、数据与代码公开以支持复现。

Conclusion: 通过合成指令工具与公开大规模WSI指令数据集，推动可复现、可泛化的病理VLM研究；ANTONI-α验证了在公开数据和指令调优下可在多类WSI任务上达SOTA或优于强基线，展示了利用开源生态构建病理共驾VLM的可行性。

Abstract: Vision-language models (VLMs) have the potential to become co-pilots for pathologists. However, most VLMs either focus on small regions of interest within whole-slide images, provide only static slide-level outputs, or rely on data that is not publicly available, limiting reproducibility. Furthermore, training data containing WSIs paired with detailed clinical reports is scarce, restricting progress toward transparent and generalisable VLMs. We address these limitations with three main contributions. First, we introduce Polysome, a standardised tool for synthetic instruction generation. Second, we apply Polysome to the public HISTAI dataset, generating HISTAI-Instruct, a large whole-slide instruction tuning dataset spanning 24,259 slides and over 1.1 million instruction-response pairs. Finally, we use HISTAI-Instruct to train ANTONI-α, a VLM capable of visual-question answering (VQA). We show that ANTONI-α outperforms MedGemma on WSI-level VQA tasks of tissue identification, neoplasm detection, and differential diagnosis. We also compare the performance of multiple incarnations of ANTONI-α trained with different amounts of data. All methods, data, and code are publicly available.

</details>


### [49] [SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation](https://arxiv.org/abs/2512.17331)
*Shihang Li,Zhiqiang Gong,Minming Ye,Yue Gao,Wen Yao*

Main category: cs.CV

TL;DR: SynergyWarpNet提出一个三阶段、注意力引导的协同变形框架，把显式3D光流对齐、参考图跨注意力补全、以及置信度引导的自适应融合结合起来，实现高保真说话人头像合成并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 显式形变在复杂表情/视角下难以精准传递动作且易丢失遮挡区域；纯注意力变形虽能补全但计算复杂、几何约束弱。需要一种既有几何对齐、又能利用多参考语义补全、并能稳健融合的方案。

Method: 三阶段流程：1) 显式变形：利用3D稠密光流对源像与驱动像进行粗对齐；2) 参考增强纠正：对多张参考图的3D关键点与纹理特征进行跨注意力，语义地补全/校正遮挡或失真的区域；3) 置信度引导融合：学习置信度图，在结构对齐与视觉一致性之间进行空间自适应融合，输出最终动画。

Result: 在标准基准上进行全面评测，指标与可视质量均达到或超过现有最优方法（SOTA）。

Conclusion: 协同利用显式几何对齐与注意力补全，并通过置信度自适应融合，可显著提升头像驱动的保真度与稳健性。

Abstract: Recent advances in neural portrait animation have demonstrated remarked potential for applications in virtual avatars, telepresence, and digital content creation. However, traditional explicit warping approaches often struggle with accurate motion transfer or recovering missing regions, while recent attention-based warping methods, though effective, frequently suffer from high complexity and weak geometric grounding. To address these issues, we propose SynergyWarpNet, an attention-guided cooperative warping framework designed for high-fidelity talking head synthesis. Given a source portrait, a driving image, and a set of reference images, our model progressively refines the animation in three stages. First, an explicit warping module performs coarse spatial alignment between the source and driving image using 3D dense optical flow. Next, a reference-augmented correction module leverages cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions. Finally, a confidence-guided fusion module integrates the warped outputs with spatially-adaptive fusing, using a learned confidence map to balance structural alignment and visual consistency. Comprehensive evaluations on benchmark datasets demonstrate state-of-the-art performance.

</details>


### [50] [Multi-level distortion-aware deformable network for omnidirectional image super-resolution](https://arxiv.org/abs/2512.17343)
*Cuixin Yang,Rongkang Dong,Kin-Man Lam,Yuhang Zhang,Guoping Qiu*

Main category: cs.CV

TL;DR: 提出一种多级畸变感知可变形网络（MDDN）用于等距矩形投影(ERP)的全景图像超分辨，利用多分支可变形注意+空洞可变形卷积扩大全景畸变区域的采样范围与感受野，并通过多级自适应融合与低秩分解在保证性能的同时降低计算量；在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ERP 将球面全景展开到平面会造成纬度相关畸变，极区被拉伸严重。现有ODISR方法采样范围有限、特征提取与畸变建模不足，难以覆盖大范围失真模式，影响重建质量。需要一种既能扩大有效感受野、又能针对畸变自适应采样的架构。

Method: 提出 MDDN：特征提取器含三条并行分支——(1) 可变形注意（等效膨胀率1），(2) 膨胀率2的空洞可变形卷积，(3) 膨胀率3的空洞可变形卷积；三分支产生密集且覆盖更广的畸变感知特征。随后采用多级特征融合模块自适应整合多尺度/多畸变级别表示。此外，对空洞可变形卷积进行低秩分解以降低计算量。

Result: 在公开ODI数据集上进行大量实验，MDDN在客观指标与视觉质量上均超过最新方法，展示更强的畸变建模与重建能力。

Conclusion: 面向ERP畸变的ODISR，扩展采样范围与构建畸变感知特征至关重要。MDDN通过多级并行可变形分支与自适应融合，在提升性能的同时控制了计算成本，是一种有效且优于现有方法的解决方案。

Abstract: As augmented reality and virtual reality applications gain popularity, image processing for OmniDirectional Images (ODIs) has attracted increasing attention. OmniDirectional Image Super-Resolution (ODISR) is a promising technique for enhancing the visual quality of ODIs. Before performing super-resolution, ODIs are typically projected from a spherical surface onto a plane using EquiRectangular Projection (ERP). This projection introduces latitude-dependent geometric distortion in ERP images: distortion is minimal near the equator but becomes severe toward the poles, where image content is stretched across a wider area. However, existing ODISR methods have limited sampling ranges and feature extraction capabilities, which hinder their ability to capture distorted patterns over large areas. To address this issue, we propose a novel Multi-level Distortion-aware Deformable Network (MDDN) for ODISR, designed to expand the sampling range and receptive field. Specifically, the feature extractor in MDDN comprises three parallel branches: a deformable attention mechanism (serving as the dilation=1 path) and two dilated deformable convolutions with dilation rates of 2 and 3. This architecture expands the sampling range to include more distorted patterns across wider areas, generating dense and comprehensive features that effectively capture geometric distortions in ERP images. The representations extracted from these deformable feature extractors are adaptively fused in a multi-level feature fusion module. Furthermore, to reduce computational cost, a low-rank decomposition strategy is applied to dilated deformable convolutions. Extensive experiments on publicly available datasets demonstrate that MDDN outperforms state-of-the-art methods, underscoring its effectiveness and superiority in ODISR.

</details>


### [51] [Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection](https://arxiv.org/abs/2512.17350)
*Chenming Zhou,Jiaan Wang,Yu Li,Lei Li,Juan Cao,Sheng Tang*

Main category: cs.CV

TL;DR: 提出一种简单的像素级映射预处理，打破AI图像中的语义捷径，使检测器聚焦更通用的高频生成痕迹，从而显著提升跨生成器的检测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在面对未见过的生成模型时泛化差，原因在于过度依赖源模型的语义特征或捷径，而非普适的生成伪迹。因此需要一种方法迫使检测器学习与具体语义无关、能跨模型迁移的通用特征。

Method: 在训练与推理前加入一个简单但有效的像素级映射预处理，扰动图像像素分布，打破非本质的语义模式与捷径信号；这样促使检测器关注更基础、可泛化的高频生成痕迹。随后在多种GAN与扩散模型数据上训练/评估现有检测器，比较跨生成器性能。

Result: 在GAN与扩散模型上进行的大量实验显示，该预处理能显著提升多种最先进检测器在跨生成器场景下的性能。分析结果支持：语义线索的扰动是实现泛化提升的关键因素。

Conclusion: 通过像素级映射破坏语义捷径，可引导检测器学习更通用的高频痕迹，从而显著改善对未见生成模型的检测泛化。该策略简单、通用，易于集成到现有检测框架中。

Abstract: The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.

</details>


### [52] [Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors](https://arxiv.org/abs/2512.17376)
*Peixuan Zhang,Shuchen Weng,Jiajun Tang,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: 提出情感图像滤镜（AIF）任务与数据集，给出两种模型AIF-B（多模态Transformer）与AIF-D（结合扩散模型先验），在内容一致性与情感保真度上优于现有方法，并通过用户研究验证其更能激发特定情感。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中文本+图像的表达常需要将文字中的抽象情感映射到图像的具体视觉效果；现有方法在情感反映深度与生成质量上不足，缺乏专门任务定义与系统数据/模型框架。

Method: 1) 定义AIF任务与构建AIF数据集；2) 提出AIF-B：基于多模态Transformer，将文本情感与图像内容对齐以生成/编辑图像；3) 提出AIF-D：在AIF-B基础上引入大规模预训练扩散模型的生成先验，实现更深层情感映射与更高保真度；4) 进行定量/定性评估与用户研究。

Result: AIF模型在内容一致性与情感保真度方面超越SOTA；用户研究显示AIF生成结果更能诱发目标情绪，主观偏好显著提升。

Conclusion: 将文本抽象情感有效注入图像是可行且有价值；结合多模态Transformer与扩散先验能显著提升情感反映与视觉质量，AIF为情感化内容创作与社交媒体应用提供潜力与方向。

Abstract: Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.

</details>


### [53] [RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering](https://arxiv.org/abs/2512.17396)
*Léo Butsanets,Charles Corbière,Julien Khlaut,Pierre Manceron,Corentin Dancette*

Main category: cs.CV

TL;DR: 提出RadImageNet-VQA，一个面向CT/MRI的放射学VQA大规模数据集：75万影像、750万问答，覆盖8个部位、97类病理，含开放/封闭/多选题；证明现有多模态模型在细粒度病理识别尤其开放式任务上仍较弱，文本-only表现接近随机，数据集无语言捷径。


<details>
  <summary>Details</summary>
Motivation: 现有医用VQA数据集规模小、以X光/示意图为主、且常被语言捷径污染，难以推动CT/MRI真实临床场景下的VQA研究与评测。需要一个大规模、专家标注、任务全面且能有效剔除文本偏置的数据基准。

Method: 基于专家策划与标注，从CT/MRI构建覆盖异常检测、解剖识别、病理识别三类任务的数据集；涵盖8个解剖区域与97种病理；生成开放式、封闭式、与多选三种问答形式；进行系统实验，评估SOTA视觉-语言模型在不同任务/设定（含微调后）的表现，并进行仅文本输入的对照分析以检验语言捷径。

Result: SOTA多模态模型在细粒度病理识别上表现不佳，尤其是开放式问答，即便微调后亦明显不足；文本-only实验结果接近随机，表明模型无法仅凭语言模式取巧，该数据集有效避免了语言捷径。

Conclusion: RadImageNet-VQA为CT/MRI放射学VQA提供了规模空前且无语言捷径的大型基准，揭示当前模型在细粒度病理识别方面的显著短板，并为后续方法改进与公平评测提供公开资源与标准化平台。

Abstract: In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.

</details>


### [54] [Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification](https://arxiv.org/abs/2512.17416)
*Martin Krebs,Jan Obdržálek,Vít Musil,Tomáš Brázdil*

Main category: cs.CV

TL;DR: 论文在前列腺癌检测系统中，用更快的可解释性方法替代遮挡（occlusion），在不降低解释质量的前提下，将解释时间提升至少10倍。


<details>
  <summary>Details</summary>
Motivation: 深度网络逐步用于临床关键任务（如病理癌症诊断），但其输出需能被病理学家理解。传统遮挡法计算代价高，影响模型开发与与专家互动效率，因此需要一种更快且同等可靠的解释方法，并缺乏统一的比较评测框架。

Method: 在现有成功的前列腺癌检测系统中：1）提出并确定比较解释方法的评估准则与相应指标；2）据此系统性比较多种解释技术与遮挡法的表现；3）选择满足质量不降且运行显著更快的替代方法。

Result: 选出的替代方法在解释质量不受影响的情况下，将生成解释所需时间至少缩短10倍，实现显著加速。

Conclusion: 通过建立评测准则与指标，可以在临床AI系统中以更快的方法替代遮挡法，显著加速迭代与调试，推动AI辅助前列腺癌检测更接近临床落地；该评估思路可推广至其他相关应用。

Abstract: Deep neural networks are starting to show their worth in critical applications such as assisted cancer diagnosis. However, for their outputs to get accepted in practice, the results they provide should be explainable in a way easily understood by pathologists. A well-known and widely used explanation technique is occlusion, which, however, can take a long time to compute, thus slowing the development and interaction with pathologists. In this work, we set out to find a faster replacement for occlusion in a successful system for detecting prostate cancer. Since there is no established framework for comparing the performance of various explanation methods, we first identified suitable comparison criteria and selected corresponding metrics. Based on the results, we were able to choose a different explanation method, which cut the previously required explanation time at least by a factor of 10, without any negative impact on the quality of outputs. This speedup enables rapid iteration in model development and debugging and brings us closer to adopting AI-assisted prostate cancer detection in clinical settings. We propose that our approach to finding the replacement for occlusion can be used to evaluate candidate methods in other related applications.

</details>


### [55] [AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments](https://arxiv.org/abs/2512.17432)
*Georgios Simantiris,Konstantinos Bacharidis,Apostolos Papanikolaou,Petros Giannakakis,Costas Panagiotakis*

Main category: cs.CV

TL;DR: AIFloodSense 发布了一个全球多样化的航拍洪水数据集（2022–2024），含470张高分辨率图像、三大任务（分类/分割/VQA）及基线，旨在推进面向气候韧性的通用视觉模型。


<details>
  <summary>Details</summary>
Motivation: 洪水视觉分割数据集稀缺，现有数据在地理覆盖与标注精细度上受限，制约了可泛化的计算机视觉方法与应急/风控应用发展。

Method: 构建并公开 AIFloodSense 数据集：收集来自64个国家、六大洲、230起洪水事件的470张航拍图像（2022–2024）；设计三类任务：图像分类（含环境类型、相机角度、大洲识别子任务）、语义分割（洪水/天空/建筑精细掩膜）、VQA（支持灾情推理）；并以SOTA模型提供各任务的基线实验，验证难度与实用性。

Result: 形成具有全球与时间多样性的高质量数据集，并给出多任务SOTA基线表现，显示任务具有挑战性、能促进跨域泛化研究。

Conclusion: AIFloodSense 弥补了洪水遥感/航拍视觉数据的空缺，为分类、分割与VQA等多模态任务提供统一基准，有助于发展更具鲁棒性和可泛化的灾害评估AI工具，服务气候韧性建设。

Abstract: Accurate flood detection from visual data is a critical step toward improving disaster response and risk assessment, yet datasets for flood segmentation remain scarce due to the challenges of collecting and annotating large-scale imagery. Existing resources are often limited in geographic scope and annotation detail, hindering the development of robust, generalized computer vision methods. To bridge this gap, we introduce AIFloodSense, a comprehensive, publicly available aerial imagery dataset comprising 470 high-resolution images from 230 distinct flood events across 64 countries and six continents. Unlike prior benchmarks, AIFloodSense ensures global diversity and temporal relevance (2022-2024), supporting three complementary tasks: (i) Image Classification with novel sub-tasks for environment type, camera angle, and continent recognition; (ii) Semantic Segmentation providing precise pixel-level masks for flood, sky, and buildings; and (iii) Visual Question Answering (VQA) to enable natural language reasoning for disaster assessment. We establish baseline benchmarks for all tasks using state-of-the-art architectures, demonstrating the dataset's complexity and its value in advancing domain-generalized AI tools for climate resilience.

</details>


### [56] [Xiaomi MiMo-VL-Miloco Technical Report](https://arxiv.org/abs/2512.17436)
*Jiaze Li,Jingyang Chen,Yuxun Qu,Jianzhong Ju,Zhenbo Luo,Jian Luan,Shijie Xu,Zhenru Lin,Junyou Zhu,Boshen Xu,Wenhui Tan,Pei Fu*

Main category: cs.CV

TL;DR: MiMo-VL-Miloco-7B 是面向智能家居场景强化的视觉语言模型及其量化版，兼顾家庭场景理解与通用多模态推理，凭借两阶段训练（SFT+GRPO强化学习）、CoT与令牌预算感知推理，在多项视频与语言基准上取得领先或显著提升，并已开源权重与评测工具。


<details>
  <summary>Details</summary>
Motivation: 通用多模态模型在家庭/智能家居场景（手势、活动、家务语境）上往往欠鲁棒，同时易在专门化与通用性之间失衡。作者希望构建一个既能在家庭语境中强表现、又不牺牲通用多模态与文本推理能力的模型，并可高效部署（含量化权重）与复现实验（开源工具链）。

Method: 以 MiMo-VL-7B 为骨干，采用两阶段训练：1）监督微调（SFT）用于家庭场景知识与指令对齐；2）基于 Group Relative Policy Optimization（GRPO）的强化学习以平衡专门化与通用能力。引入链式思维（CoT）监督与“令牌预算感知”推理策略，使用高效多领域数据以提升数据与推理效率。提供量化 GGUF 以便边缘/部署。

Result: 在手势识别与家庭场景理解上获领先 F1，并在 Video-MME、Video-MMMU、Charades-STA 等视频基准及 MMMU-Pro、MMLU-Pro 等语言基准上取得一致增益，超过多项强势开源与闭源基线。针对性家庭训练不仅提升活动/手势理解，也改进了纯文本推理，对文档类任务仅有小幅折衷。

Conclusion: MiMo-VL-Miloco-7B 通过SFT+GRPO、CoT和令牌预算感知推理实现家居场景与通用多模态推理的兼顾，表现强、数据与推理高效，且已开源模型权重与评测工具，为真实智能家居应用与研究提供可复现基础。

Abstract: We open-source \textbf{MiMo-VL-Miloco-7B} and its quantized variant \textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.

</details>


### [57] [LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents](https://arxiv.org/abs/2512.17445)
*Yun He,Francesco Pittaluga,Ziyu Jiang,Matthias Zwicker,Manmohan Chandraker,Zaid Tasneem*

Main category: cs.CV

TL;DR: LangDriveCTRL 是一个可用自然语言控制的真实驾驶视频编辑框架，通过3D场景分解与多智能体执行图实现对象与多目标行为可编辑，并用视频扩散模型精炼渲染；在指令对齐、结构保持和逼真度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶视频编辑在对象操控、行为编排与照片级真实感之间难以兼顾，且对复杂自然语言指令与多目标交互的对齐能力不足；需要一个能在真实场景中进行细粒度对象与行为编辑、同时保持交通合理性与视觉一致性的系统。

Method: 1) 显式3D场景分解：将驾驶视频解析为包含静态背景与动态对象的场景图；2) 代理式流水线：由Orchestrator把用户自然语言转为执行图，协同三个子代理与工具——Object Grounding Agent 将文本描述与场景图节点对齐；Behavior Editing Agent 从语言生成多目标轨迹；Behavior Reviewer Agent 迭代审阅并优化轨迹；3) 渲染与视频扩散精炼：将编辑后的场景图渲染为视频，并用视频扩散工具修复插入物体与大视角变化导致的伪影。

Result: 在定量评测中，相比此前SOTA，指令对齐度近乎提升2倍，并在结构保持、照片级真实感与交通合理性方面更优。系统支持基于单条指令的对象节点编辑（删除、插入、替换）与多对象行为编辑。

Conclusion: 通过语言驱动的多代理执行图与显式3D场景表示，LangDriveCTRL 在真实驾驶视频中实现高可控、可组合与高逼真的对象与行为编辑，显著提升指令对齐与场景一致性，为合成多样交通场景提供有效工具。

Abstract: LangDriveCTRL is a natural-language-controllable framework for editing real-world driving videos to synthesize diverse traffic scenarios. It leverages explicit 3D scene decomposition to represent driving videos as a scene graph, containing static background and dynamic objects. To enable fine-grained editing and realism, it incorporates an agentic pipeline in which an Orchestrator transforms user instructions into execution graphs that coordinate specialized agents and tools. Specifically, an Object Grounding Agent establishes correspondence between free-form text descriptions and target object nodes in the scene graph; a Behavior Editing Agent generates multi-object trajectories from language instructions; and a Behavior Reviewer Agent iteratively reviews and refines the generated trajectories. The edited scene graph is rendered and then refined using a video diffusion tool to address artifacts introduced by object insertion and significant view changes. LangDriveCTRL supports both object node editing (removal, insertion and replacement) and multi-object behavior editing from a single natural-language instruction. Quantitatively, it achieves nearly $2\times$ higher instruction alignment than the previous SoTA, with superior structural preservation, photorealism, and traffic realism. Project page is available at: https://yunhe24.github.io/langdrivectrl/.

</details>


### [58] [MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation](https://arxiv.org/abs/2512.17450)
*Jon Muhovič,Janez Perš*

Main category: cs.CV

TL;DR: 提出MULTIAQUA多模态海事数据集（RGB、热成像、IR、LiDAR等），并展示仅用白天数据训练也能在夜间近乎全黑条件下保持稳健表现的多模态方法与训练策略。


<details>
  <summary>Details</summary>
Motivation: 单靠可见光相机在恶劣天气或低光环境下表现不佳，无人艇在多样、极端视觉条件下需要更丰富的传感信息与可靠的场景理解能力。缺乏统一、标注齐全且多模态对齐的数据限制了算法发展与评测。

Method: 构建并发布同步、标定、带注释的多模态海事数据集MULTIAQUA；在困难的夜间测试集上评测多种多模态方法；提出新的训练策略，使模型更稳健，包括利用仅白天图像进行训练以泛化至夜间/低可见度场景。

Result: 多模态方法在夜间近乎全黑条件下仍能维持可靠性能；所提训练策略提升了跨光照条件的鲁棒性，即使未使用夜间数据进行训练。

Conclusion: 多模态传感与相应训练方案能显著提升海事场景理解在恶劣可见度下的可靠性；MULTIAQUA为监督式多模态方法的研究与评估提供了高质量基准，并简化数据采集与训练流程（仅用白天数据也可获得夜间稳健性）。

Abstract: Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.

</details>


### [59] [3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework](https://arxiv.org/abs/2512.17459)
*Tobias Sautter,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 提出3D-RE-GEN：从单张图像重建可编辑的纹理化网格场景（前景多对象+完整背景），通过组合多领域SOTA模型与可微4-DoF布局优化，达到SOTA的单图三维场景重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成虽好看，但不适合艺术家生产流程：网格可编辑性差、对象分解不准、空间关系失真、背景缺失，导致难以用于VFX与游戏。需要能输出可编辑的多对象网格与背景、物理一致的布局和光照基础。

Method: 提出组合式管线：资产检测→（含遮挡补全的）对象重建→对象放置→完整背景生成。把遮挡物体当作图像编辑问题，用生成模型在一致光照与几何约束下推理补全并重建；引入精确相机恢复；生成可约束优化的背景；并设计新颖的4自由度（平移x,y,尺度,朝向/高度与地平面对齐）可微优化，使对象与估计地面平面对齐，获得物理合理布局。

Result: 在单图3D场景重建任务上取得SOTA：得到连贯、可修改的多对象网格与完整背景，改进对象分解、空间关系与布局合理性，并为真实感光照/模拟提供基础。

Conclusion: 组合多专用SOTA模型与可微空间优化能显著提升单图场景到可编辑网格的重建质量；完整背景与地面对齐约束是关键，使结果适配艺术家工作流与后续VFX/游戏应用。

Abstract: Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.
  Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.

</details>


### [60] [TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis](https://arxiv.org/abs/2512.17488)
*Almustapha A. Wakili,Adamu Hussaini,Abubakar A. Musa,Woosub Jung,Wei Yu*

Main category: cs.CV

TL;DR: 提出TwinSegNet：结合混合ViT-UNet与联邦学习和个性化数字孪生，实现在多机构MRI上的隐私保护脑肿瘤分割，跨数据异质性仍保持高Dice与高敏感/特异。


<details>
  <summary>Details</summary>
Motivation: 集中式深度学习需要汇聚多机构医疗影像，存在隐私风险与域间分布差异导致泛化差；需在严格保密前提下实现可扩展、个性化且高精度的脑肿瘤分割。

Method: 设计TwinSegNet联邦框架：以卷积编码器+ViT瓶颈的混合UNet获取局部/全局上下文；服务器聚合全局模型，各机构在本地私有数据上微调形成数字孪生（个性化模型）。在九个异质MRI数据集（含BraTS2019-2021与自定义集合）上评估；与集中式模型（如TumorVisNet）比较。

Result: 在非IID客户端分布下仍取得高性能：Dice最高约0.90，敏感度/特异度均超过90%，优于或可比集中式方法；表现稳健且跨机构泛化良好。

Conclusion: TwinSegNet在不共享原始数据的前提下实现了可扩展的个性化脑肿瘤分割，兼顾隐私与性能，适用于多机构临床部署。

Abstract: Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.

</details>


### [61] [LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models](https://arxiv.org/abs/2512.17489)
*Muhammad Atif Butt,Kai Wang,Javier Vazquez-Corral,Joost Van De Weijer*

Main category: cs.CV

TL;DR: 提出LumiCtrl：从单张目标图像学习“照明提示词”，实现对T2I生成中光源/色温的精确可控与个性化；通过物理增广、结构解耦与掩膜重建，提高光照忠实度、美学与场景一致性，并获人类偏好验证。


<details>
  <summary>Details</summary>
Motivation: 现有T2I虽能创作多样图像，但难以精确控制场景照明（色温/光源），限制了设计师对情绪、氛围和审美的操控需求；个性化方法常混淆形状内容与光照，导致照明不稳与失真。

Method: 给定单张对象图像，学习对象专属“照明提示词”。三组件：1) 物理驱动的照明增广：沿普朗克轨迹（Planckian locus）合成标准光源下的微调样本，实现色温有序变化；2) 基于冻结ControlNet的边缘引导提示解耦，使学习聚焦照明而非结构；3) 前景掩膜重建损失，仅对前景强约束，同时允许背景自适应，从而实现“情境光适配”。

Result: 与多种T2I个性化基线定性定量对比，LumiCtrl在照明忠实度、美学质量和场景连贯性上显著更优；用户偏好实验显示人类明显更偏好LumiCtrl输出。

Conclusion: LumiCtrl可从单图像学习可控的照明提示，实现细粒度光源控制与情境适配，优于现有个性化方法；代码与数据将在发表后开源。

Abstract: Current text-to-image (T2I) models have demonstrated remarkable progress in creative image generation, yet they still lack precise control over scene illuminants, which is a crucial factor for content designers aiming to manipulate the mood, atmosphere, and visual aesthetics of generated images. In this paper, we present an illuminant personalization method named LumiCtrl that learns an illuminant prompt given a single image of an object. LumiCtrl consists of three basic components: given an image of the object, our method applies (a) physics-based illuminant augmentation along the Planckian locus to create fine-tuning variants under standard illuminants; (b) edge-guided prompt disentanglement using a frozen ControlNet to ensure prompts focus on illumination rather than structure; and (c) a masked reconstruction loss that focuses learning on the foreground object while allowing the background to adapt contextually, enabling what we call contextual light adaptation. We qualitatively and quantitatively compare LumiCtrl against other T2I customization methods. The results show that our method achieves significantly better illuminant fidelity, aesthetic quality, and scene coherence compared to existing personalization baselines. A human preference study further confirms strong user preference for LumiCtrl outputs. The code and data will be released upon publication.

</details>


### [62] [MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding](https://arxiv.org/abs/2512.17492)
*Oskar Kristoffersen,Alba R. Sánchez,Morten R. Hannemose,Anders B. Dahl,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 提出MMLANDMARKS，一个覆盖图像、文本与地理坐标的多模态地标数据集，统一对齐四种模态，用于多类地理空间检索与定位任务，并以简单CLIP式基线在多任务上获得强泛化与竞争力表现，凸显多模态数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间基准在模态覆盖上有限，无法在统一框架下整合图像（空/地视）、文本与坐标等多源信息，制约了跨模态地理理解、检索与定位能力的进展。

Method: 构建MMLANDMARKS数据集：为美国18,557个地标收集并一一对齐四种模态数据（197k高分辨率航拍图、329k地面图、文本描述与GPS坐标）。基于该数据集，采用一个简单的CLIP风格对比学习基线，进行多任务训练与评测（地-空跨视图检索、地/空定位、文本到图像与文本到GPS检索）。

Result: 所提CLIP式基线在多项任务上展现广泛泛化能力与与现有基础模型和特定SOTA方法相当的竞争性能。

Conclusion: 统一、多模态对齐的数据集对于实现广义地理空间理解至关重要；MMLANDMARKS为跨模态检索与定位提供了标准化基准与强健起点，能推动方法在多任务上的一致进步。

Abstract: Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.

</details>


### [63] [GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation](https://arxiv.org/abs/2512.17495)
*Rang Li,Lei Li,Shuhuai Ren,Hao Tian,Shuhao Gu,Shicheng Li,Zihao Yue,Yudong Wang,Wenhan Ma,Zhe Yang,Jingyuan Ma,Zhifang Sui,Fuli Luo*

Main category: cs.CV

TL;DR: 提出GroundingME基准，系统评估MLLM在视觉指代落地的真实能力，发现显著能力缺口（最佳仅45.1%，拒识几乎为0%）；并给出测试时扩展与数据混合训练两条改进路径。


<details>
  <summary>Details</summary>
Motivation: 现有视觉指代基准过于简化，无法反映人类在真实场景中处理歧义、复杂空间关系、遮挡与无法落地情形的能力，难以回答MLLM是在“真正落地”还是“模式匹配”。需要一个能系统刻画这些维度并含拒识能力的严格评测。

Method: 构建GroundingME：沿四个维度设计样例——（1）判别性：区分高度相似目标；（2）空间性：理解复杂关系描述；（3）受限性：处理遮挡/微小目标；（4）拒识性：识别无法落地查询。通过自动生成+人工校验得到1005个高难例，评测25个SOTA MLLM；并探索两类改进：（a）测试时扩展（思维轨迹多样化，选择最优响应）；（b）数据混合训练（加入无法落地样本以教会拒识）。

Result: 在GroundingME上，最佳模型仅45.1%准确率；在拒识任务上多数模型为0%，倾向幻觉而非承认缺失，存在安全隐患。测试时扩展带来最多约+2.9%的复杂落地提升；数据混合训练将拒识准确率从0%提升到27.9%。

Conclusion: GroundingME揭示MLLM在真实视觉落地上的明显短板，尤其是拒识能力。该基准既是诊断工具也是改进路线图：通过推理时扩展与面向无法落地数据的训练可部分缓解，但距离类人水平仍有显著差距。

Abstract: Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.

</details>


### [64] [Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort](https://arxiv.org/abs/2512.17499)
*Peshawa J. Muhammad Ali,Navin Vincent,Saman S. Abdulla,Han N. Mohammed Fadhl,Anders Blilie,Kelvin Szolnoky,Julia Anna Mielcarz,Xiaoyi Ji,Nita Mulliqi,Abdulbasit K. Al-Talabani,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 该研究在伊拉克库尔德斯坦收集339例前列腺活检数字病理，验证专用端到端AI与两种基础模型在癌症诊断与Gleason分级上的表现，显示AI与病理医师一致性达病理医师间水平，且跨三种扫描仪（含低成本机型）一致性高。


<details>
  <summary>Details</summary>
Motivation: 以往病理AI多在欧美大型中心数据上评估，缺乏对中东等代表性不足人群与资源受限环境的验证；为实现全球公平采用，需要在这些地区开展外部验证，尤其关注扫描设备差异与低成本可及性。

Method: 收集2013-2024年间185名患者的339份前列腺活检切片并数字化（Hamamatsu、Leica、Grundium三款扫描仪）。比较一款任务定制的端到端AI与两种基础模型在诊断/分级上与病理医师的一致性（Cohen二次加权κ），并评估跨扫描仪一致性。

Result: AI与病理医师在Gleason分级上的一致性与病理医师间一致性相当（κ=0.801 vs 0.799，p=0.9824）。三款扫描仪任意配对下，所有AI模型跨设备一致性均高（二次加权κ>0.90），包括低成本紧凑型扫描仪。

Conclusion: AI在前列腺组织学评估已达病理医师水平；低成本紧凑型扫描仪可用于未数字化环境的验证，并为低量实验室提供具成本效益的AI落地路径；该首个中东公开数字病理数据集为全球公平的病理AI研究奠定基础。

Abstract: Background: Artificial intelligence (AI) is improving the efficiency and accuracy of cancer diagnostics. The performance of pathology AI systems has been almost exclusively evaluated on European and US cohorts from large centers. For global AI adoption in pathology, validation studies on currently under-represented populations - where the potential gains from AI support may also be greatest - are needed. We present the first study with an external validation cohort from the Middle East, focusing on AI-based diagnosis and Gleason grading of prostate cancer.
  Methods: We collected and digitised 339 prostate biopsy specimens from the Kurdistan region, Iraq, representing a consecutive series of 185 patients spanning the period 2013-2024. We evaluated a task-specific end-to-end AI model and two foundation models in terms of their concordance with pathologists and consistency across samples digitised on three scanner models (Hamamatsu, Leica, and Grundium).
  Findings: Grading concordance between AI and pathologists was similar to pathologist-pathologist concordance with Cohen's quadratically weighted kappa 0.801 vs. 0.799 (p=0.9824). Cross-scanner concordance was high (quadratically weighted kappa > 0.90) for all AI models and scanner pairs, including low-cost compact scanner.
  Interpretation: AI models demonstrated pathologist-level performance in prostate histopathology assessment. Compact scanners can provide a route for validation studies in non-digitalised settings and enable cost-effective adoption of AI in laboratories with limited sample volumes. This first openly available digital pathology dataset from the Middle East supports further research into globally equitable AI pathology.
  Funding: SciLifeLab and Wallenberg Data Driven Life Science Program, Instrumentarium Science Foundation, Karolinska Institutet Research Foundation.

</details>


### [65] [InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion](https://arxiv.org/abs/2512.17504)
*Hoiyeong Jin,Hyojin Jang,Jeongho Kim,Junha Hyung,Kinam Kim,Dongjin Kim,Huijin Choi,Hyeonji Kim,Jaegul Choo*

Main category: cs.CV

TL;DR: InsertAnywhere提出用于视频目标插入的4D感知编辑框架：先用4D一致的掩码/几何重建实现时空与遮挡一致的放置，再在扩散式视频生成器上联合合成插入物体及其局部光照/阴影变化；并构建含光照信息的合成训练集ROSE++。实验显示在真实场景中几何与视觉一致性优于学术与商用方法。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频编辑虽有进展，但VOI难点在于：1) 对场景的4D（空间+时间）理解不足，导致时序不一致与几何失真；2) 遮挡处理不可靠；3) 光照与阴影匹配欠佳，生成结果“贴图感”强。作者旨在实现几何一致且外观/光照可信的真实物体插入。

Method: 两阶段框架：1) 4D感知掩码生成模块：重建场景几何，基于用户指定初始位置，将目标在全视频中传播，确保时序连贯与遮挡一致。2) 扩展扩散式视频生成模型：在插入区域联合建模物体与周边局部外观变化（照明、阴影、反射等），实现外观与环境匹配的共同合成。为监督训练，构建ROSE++数据集：从ROSE物体移除数据集生成三元组（移除视频、含物体视频、由VLM生成的参考图像），并注入光照感知标注。

Result: 在多种真实世界场景上进行广泛实验，InsertAnywhere在几何合理性、时序一致性、遮挡处理与光照匹配等指标上显著优于现有学术与商用基线；合成结果更连贯、逼真。

Conclusion: 通过4D一致的掩码/几何建模与扩散模型的联合局部外观合成，InsertAnywhere实现了逼真的视频物体插入。ROSE++为监督训练提供了光照感知数据支撑。整体方法在各维度均优于现有方法，适用于多样真实场景。

Abstract: Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.

</details>


### [66] [Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection](https://arxiv.org/abs/2512.17514)
*Sairam VCR,Rishabh Lalla,Aveen Dayal,Tejal Kulkarni,Anuj Lalla,Vineeth N Balasubramanian,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出FALCON-SFOD，通过加强特征空间的“目标聚焦”来改进源无关目标检测，在域偏移下减少背景误激活，结合SPAR与IRPL两模块，兼顾前景聚焦与噪声/不均衡鲁棒，理论上收紧定位与分类误差界，并在基准上取得有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 现有SFOD多依赖Mean-Teacher自标注，但域偏移使检测器对背景产生高置信误激活，削弱目标聚焦，导致伪标签不可靠。既有工作多在伪标签端修补，忽视应从特征空间本身增强“前景对齐/结构化表示”。

Method: 提出FALCON-SFOD框架，由两部分组成：1) SPAR：利用开放词汇分割/基础模型（如OV-SAM）生成类无关二值前景掩码，对检测特征施加空间先验正则，使激活集中于前景、结构化；2) IRPL：在严重前景-背景不均衡场景下进行“失衡感知、噪声鲁棒”的伪标签学习，缓解噪声与类别/前景稀缺带来的偏置。并提供理论分析连接到更紧的定位与分类误差上界。

Result: 在多个SFOD基准上取得有竞争力的性能（优于或可比SOTA），表现出更强的域内外泛化、前景聚焦与鲁棒性。

Conclusion: 通过将基础模型的空间先验与噪声鲁棒、失衡感知伪标签学习相结合，FALCON-SFOD有效缓解域偏移下的背景干扰与伪标签不可靠问题，从特征空间层面提升源无关目标检测的适应能力。

Abstract: Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.

</details>


### [67] [PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology](https://arxiv.org/abs/2512.17517)
*Siemen Brussee,Pieter A. Valkema,Jurre A. J. Weijer,Thom Doeleman,Anne M. R. Schrader,Jesper Kers*

Main category: cs.CV

TL;DR: PathBench-MIL 是一个面向病理图像 MIL 的开源 AutoML 与基准框架，自动化从预处理、特征提取到聚合的全流程，并提供可复现实验、可视化与统一配置，实现快速实验与标准化对比。


<details>
  <summary>Details</summary>
Motivation: 病理图像 MIL 研究生态分散、实现细节繁琐、结果难复现、跨模型与特征提取器的对比不统一，缺乏端到端自动化与标准化工具。因此需要一个可扩展、统一配置、可视化友好且支持广泛模型的 AutoML/benchmark 平台。

Method: 构建一个模块化的 MIL 自动化与评测框架：包括数据预处理、特征提取（多种特征提取器）、MIL 聚合（多种 MIL 模型）、统一配置系统、可视化工具；将数十种 MIL 模型与特征提取器封装，支持端到端流水线自动构建与可复现实验；提供公开代码库与扩展接口。

Result: 实现对数十种 MIL 模型与特征提取器的可复现实验与基准评测，支持快速实验迭代、跨数据集/任务的标准化比较，并提供可视化输出。

Conclusion: PathBench-MIL 作为开源平台，提升了病理 MIL 流水线的自动化、标准化与可扩展性，为研究与应用提供统一基准与快速实验能力，并已在 GitHub 开源。

Abstract: We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL

</details>


### [68] [Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding](https://arxiv.org/abs/2512.17532)
*Jiaqi Tang,Jianmin Chen,Wei Wei,Xiaogang Xu,Runtao Liu,Xiangyu Wu,Qipeng Xie,Jiafei Wu,Lei Zhang,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出Robust-R1：用显式“退化链式推理”增强MLLM在极端真实视觉退化下的鲁棒性，结合SFT、奖励对齐和动态推理深度，并配套11K带结构化标注数据；在R-Bench等多基准上达SOTA鲁棒表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在雾、噪声、压缩、运动模糊等真实退化下性能脆弱。现有方法多通过隐式训练/适配仅提升视觉编码器泛化，导致可解释性差、优化各自为政、难以精确感知退化强度与类型。需要一种可解释、端到端协同、能感知与适配退化强度的框架。

Method: 显式建模“退化→感知影响→语义推理→结论”的结构化推理链：
1) 监督微调：构建退化感知的推理基础，使模型学会输出带退化要素的推理链；
2) 奖励驱动对齐：通过奖励信号对齐，强化对退化参数（类型/强度等）的准确感知与表述；
3) 动态推理深度：根据退化强度自适应扩展/收缩推理步数。配套11K数据，覆盖四个真实视觉处理阶段的退化，含退化参数、感知影响、无退化语义链与结论的结构化标注。

Result: 在R-Bench上超越通用与鲁棒基线；在MMMB、MMStar、RealWorldQA上面对多强度对抗式退化保持更强的抗退化性能，整体达SOTA鲁棒性。

Conclusion: 显式的退化链式推理与奖励对齐、动态深度结合，较仅提升编码器泛化的方法更可解释、更协同，显著提升MLLM在真实与对抗退化下的鲁棒性。配套数据与方法为鲁棒多模态推理提供可扩展范式。

Abstract: Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.

</details>


### [69] [FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views](https://arxiv.org/abs/2512.17541)
*Qijian Tian,Xin Tan,Jiayu Ying,Xuhong Wang,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: FLEG 提出一种前馈式网络，从任意视角重建带语言嵌入的3D高斯表示；无需3D标注，可用大规模视频数据进行训练，并通过实例引导对比学习与层级稀疏化，实现高效、准确的几何、外观与语义联合重建，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的前馈重建方法通常假设固定输入视角，且受限于缺乏大规模3D标注数据，导致对任意视角与语言语义的泛化能力不足、训练成本高。需要一种无需3D标注、能处理未标定/未配准多视图、并与语言语义对齐的高效重建方案。

Method: 提出无3D标注的2D→3D提升训练框架：1）利用大规模视频与可得的2D实例信息进行语义丰富化；2）实例引导对比学习，将2D语义嵌入与3D高斯表示对齐；3）几何-语义层级稀疏化，降低密集视图的存算开销；4）前馈网络FLEG从任意稀疏/密集视角直接重建语言嵌入的3D高斯（同时预测几何与外观与语义）。

Result: 在多项相关任务上达到或超过现有方法，能在任意未标定/未对齐的多视角下高效重建，生成准确几何、逼真外观与语言对齐语义。

Conclusion: FLEG通过无3D标注训练、实例对比对齐与层级稀疏化，实现从任意视角的高效前馈式语言嵌入3D高斯重建，兼顾精度、外观质量与语义一致性，优于现有方法。

Abstract: We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.

</details>


### [70] [ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image](https://arxiv.org/abs/2512.17545)
*Yunqi Gao,Leyuan Liu,Yuhan Li,Changxin Gao,Yuanyuan Liu,Jingying Chen*

Main category: cs.CV

TL;DR: 提出ClothHMR，通过“裁衣贴体+大模型视觉对齐”两步，显著提升穿着多样/宽松服装下的3D人体网格重建效果，并在基准与野外场景超越SOTA，提供实用Web应用与开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体网格恢复方法假设紧身衣，遇到宽松、多样服饰时形体与姿态估计显著退化，泛化不足；需要既抑制服装干扰，又提升对复杂视觉场景的泛化能力。

Method: 提出ClothHMR，含两模块：1) Clothing Tailoring(CT)：通过人体语义估计与人体边缘预测，将服装“裁剪/贴体”，使外观轮廓贴合真实身形，降低服装对重建的干扰；2) FHVM-based Mesh Recovering(MR)：借助基础人类视觉模型(FHVM)的中间表征，对3D人体参数持续优化与对齐，迭代细化网格，使估计更稳健、可泛化。

Result: 在多个基准数据集与真实野外图像上显著优于现有SOTA，能精确恢复穿着多样（尤其宽松）服饰的人体形体与姿态；并开发在线时尚/购物Web应用展示其实用性。

Conclusion: 将“贴体化处理”与“基础视觉模型表征对齐”结合，可在复杂服饰下实现高精度、强泛化的3D人体网格恢复；方法有效且可落地，代码与模型已开源。

Abstract: With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.

</details>


### [71] [G3Splat: Geometrically Consistent Generalizable Gaussian Splatting](https://arxiv.org/abs/2512.17547)
*Mehdi Hosseinzadeh,Shin-Fang Chng,Yi Xu,Simon Lucey,Ian Reid,Ravi Garg*

Main category: cs.CV

TL;DR: 提出G3Splat，在仅有视图合成监督易致几何歧义的场景下，通过几何先验与自监督约束学习每像素3D高斯，达成更一致几何、相对位姿与新视角合成的SOTA，并在ScanNet零样本泛化强。


<details>
  <summary>Details</summary>
Motivation: 现有把多视图结构网络扩展到回归每像素3D高斯的方法，多依赖视图合成损失来学习额外参数（朝向、尺度、不透明度、外观），但仅靠视图合成无法保证几何可辨识与一致性，导致“好看但不对”的高斯喷溅。需要在无位姿标注的自监督/可泛化设定下，消除几何/外观/不透明度之间的歧义，获得几何有意义的表示。

Method: 提出G3Splat：在pose-free、自监督框架下，从图像回归每像素3D高斯（位置、朝向、尺度、透明度、外观），并显式引入几何先验与约束来打破歧义并保持跨视角一致性。核心做法包括：对高斯形状与朝向的正则化、稀疏性/体素一致性或深度一致性约束、可见性/不透明度物理先验、以及用于相对位姿估计的几何一致性损失；整体以可微splatting进行前向渲染，联合优化高斯参数与相机相对位姿。

Result: 在RE10K上，G3Splat在三方面取得SOTA：几何一致重建、相对位姿估计与新视角合成质量；在ScanNet上零样本泛化显著优于以往方法，尤其在几何恢复与相对位姿上有大幅提升。

Conclusion: 仅用视图合成监督不足以学到几何可信的3D高斯；通过引入几何先验与一致性自监督，G3Splat可在无位姿标注下学习到几何一致的3D表征，同时提升位姿估计与视图合成，并具备强零样本泛化能力。

Abstract: 3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).

</details>


### [72] [A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points](https://arxiv.org/abs/2512.17566)
*Mathilde Gajda Faanes,David Bouget,Asgeir S. Jakola,Timothy R. Smith,Vasileios K. Kavouridis,Francesco Latini,Margret Jensdottir,Peter Milos,Henrietta Nittby Redebrandt,Rickard L. Sjöberg,Rupavathana Mahesparan,Lars Kjelsberg Pedersen,Ole Solheim,Ingerid Reinertsen*

Main category: cs.CV

TL;DR: 基于约5000例多中心FLAIR MRI，训练统一的注意力U-Net模型自动分割FLAIR高信号区，在多肿瘤类型与时相上取得与专用数据集模型相当的性能，并整合进Raidionics以便临床应用。


<details>
  <summary>Details</summary>
Motivation: 临床上不同脑肿瘤的体积评估与水肿监测高度依赖FLAIR高信号区域，但跨中心、跨肿瘤类型与不同手术时相的差异导致通用自动分割困难，急需一个可泛化的统一模型以支持诊断与随访。

Method: 收集多中心、不同肿瘤类型与时相的约5000份FLAIR影像，采用Attention U-Net训练单一统一分割器；与按数据集分别训练的专用模型比较，并在多肿瘤类型、不同时间点及BraTS数据上验证；以Dice评分衡量性能；将模型集成到开源Raidionics平台。

Result: 统一模型在多任务上取得Dice：术前脑膜瘤88.65%，术前转移瘤80.08%，BraTS术前胶质瘤90.92%、术后84.60%，术前低级别胶质瘤84.47%、术后61.27%；对比各数据集专用模型表现相当，展现良好跨类型与跨时相泛化能力。

Conclusion: 统一的Attention U-Net能在不同脑肿瘤与获取时相上稳定分割FLAIR高信号，性能接近专用模型并具备更好的泛化性，适合临床部署，已在Raidionics中提供。

Abstract: T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.

</details>


### [73] [RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis](https://arxiv.org/abs/2512.17573)
*Qilong Wang,Xiaofan Ming,Zhenyi Lin,Jinwen Li,Dongwei Ren,Wangmeng Zuo,Qinghua Hu*

Main category: cs.CV

TL;DR: 提出RoomBench++数据集与RoomEditor++扩散式架构，实现高保真虚拟家具合成，兼顾几何一致与背景完整，并在多项评测与人偏好上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 虚拟家具合成对家装与电商有价值，但缺少可复现实验基准，且现有图像合成/抠图/修补方法难以在高保真合成同时保持背景完整与几何一致。

Method: 1) 构建RoomBench++：来自真实室内视频与真实感渲染的112,851训练对与1,832测试对；2) 提出RoomEditor++：扩散式、参数共享的双分支骨干（兼容U-Net与DiT），统一参考物体与背景图的特征提取与修复（inpainting）；参数共享使特征对齐，促进几何变换、纹理保真与无缝融合。

Result: 在定量指标、定性对比与人主观偏好上均优于现有方法；对未见室内与通用场景具备强泛化，无需任务特定微调。

Conclusion: RoomBench++为任务提供标准化训练/评测基准；RoomEditor++通过参数共享的双扩散骨干实现更好的融合质量与泛化能力，推动虚拟家具合成走向实用。

Abstract: Virtual furniture synthesis, which seamlessly integrates reference objects into indoor scenes while maintaining geometric coherence and visual realism, holds substantial promise for home design and e-commerce applications. However, this field remains underexplored due to the scarcity of reproducible benchmarks and the limitations of existing image composition methods in achieving high-fidelity furniture synthesis while preserving background integrity. To overcome these challenges, we first present RoomBench++, a comprehensive and publicly available benchmark dataset tailored for this task. It consists of 112,851 training pairs and 1,832 testing pairs drawn from both real-world indoor videos and realistic home design renderings, thereby supporting robust training and evaluation under practical conditions. Then, we propose RoomEditor++, a versatile diffusion-based architecture featuring a parameter-sharing dual diffusion backbone, which is compatible with both U-Net and DiT architectures. This design unifies the feature extraction and inpainting processes for reference and background images. Our in-depth analysis reveals that the parameter-sharing mechanism enforces aligned feature representations, facilitating precise geometric transformations, texture preservation, and seamless integration. Extensive experiments validate that RoomEditor++ is superior over state-of-the-art approaches in terms of quantitative metrics, qualitative assessments, and human preference studies, while highlighting its strong generalization to unseen indoor scenes and general scenes without task-specific fine-tuning. The dataset and source code are available at \url{https://github.com/stonecutter-21/roomeditor}.

</details>


### [74] [3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging](https://arxiv.org/abs/2512.17578)
*Ge Wang,Xing Liu,Xin Yuan*

Main category: cs.CV

TL;DR: 论文提出基于one-hot调制的视频快照压缩成像（SCI）重建新方法：把重建转化为生成式视频补洞，并以与硬件压缩一致的SDE建模前向过程，采用“一步回归初始化+一步扩散细化”的框架，并通过双光路获取互补信息缓解空间退化；在合成与真实数据上有效。


<details>
  <summary>Details</summary>
Motivation: 随机二值调制的主流SCI会产生时间混叠；one-hot调制可完美时间解耦但缺乏匹配的重建算法。需要一种能充分发挥one-hot掩膜优势、同时解决空间退化并提升重建质量与稳定性的方法。

Method: 1) 将one-hot调制下的重建等价为生成式视频补洞问题；2) 设计与硬件压缩一致的SDE作为前向过程；3) 发现纯扩散用于SCI的不足，提出“一步回归初始化+一步扩散细化”的两阶段框架；4) 硬件层面引入双光路，利用第二路的互补信息提升空间细节。

Result: 在合成数据与真实场景实验中，所提方法优于现有方法（摘要称“有效”，暗示更好的重建质量与抗混叠能力），实现对one-hot调制的高质量视频重建。

Conclusion: 首次将扩散模型引入视频SCI重建，并与one-hot调制、SDE建模及双光路硬件协同，显著缓解时间混叠与空间退化，验证了方法在多种数据上的有效性。

Abstract: Video snapshot compressive imaging (SCI) captures dynamic scene sequences through a two-dimensional (2D) snapshot, fundamentally relying on optical modulation for hardware compression and the corresponding software reconstruction. While mainstream video SCI using random binary modulation has demonstrated success, it inevitably results in temporal aliasing during compression. One-hot modulation, activating only one sub-frame per pixel, provides a promising solution for achieving perfect temporal decoupling, thereby alleviating issues associated with aliasing. However, no algorithms currently exist to fully exploit this potential. To bridge this gap, we propose an algorithm specifically designed for one-hot masks. First, leveraging the decoupling properties of one-hot modulation, we transform the reconstruction task into a generative video inpainting problem and introduce a stochastic differential equation (SDE) of the forward process that aligns with the hardware compression process. Next, we identify limitations of the pure diffusion method for video SCI and propose a novel framework that combines one-step regression initialization with one-step diffusion refinement. Furthermore, to mitigate the spatial degradation caused by one-hot modulation, we implement a dual optical path at the hardware level, utilizing complementary information from another path to enhance the inpainted video. To our knowledge, this is the first work integrating diffusion into video SCI reconstruction. Experiments conducted on synthetic datasets and real scenes demonstrate the effectiveness of our method.

</details>


### [75] [Medical Imaging AI Competitions Lack Fairness](https://arxiv.org/abs/2512.17581)
*Annika Reinke,Evangelia Christodoulou,Sthuthi Sadananda,A. Emre Kavur,Khrystyna Faryna,Daan Schouten,Bennett A. Landman,Carole Sudre,Olivier Colliot,Nick Heller,Sophie Loizillon,Martin Maška,Maëlys Solal,Arya Yazdan-Panah,Vilma Bozgo,Ömer Sümer,Siem de Jong,Sophie Fischer,Michal Kozubek,Tim Rädsch,Nadim Hammoud,Fruzsina Molnár-Gábor,Steven Hicks,Michael A. Riegler,Anindo Saha,Vajira Thambawita,Pal Halvorsen,Amelia Jiménez-Sánchez,Qingyang Yang,Veronika Cheplygina,Sabrina Bottazzi,Alexander Seitel,Spyridon Bakas,Alexandros Karargyris,Kiran Vaidhya Venkadesh,Bram van Ginneken,Lena Maier-Hein*

Main category: cs.CV

TL;DR: 该研究系统评估了241个医学影像AI挑战赛（458个任务、19种成像模态），发现数据集在地理、模态与任务类型上存在显著偏倚，且在可获取性与许可合规性上不充分，导致基准榜单成绩与临床相关性脱节。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI的基准竞赛深刻影响方法发展与性能标尺，但尚不清楚其数据是否足够代表真实临床多样性，并且是否符合FAIR（可发现、可获取、可互操作、可复用）原则，从而支撑具有临床意义的AI。

Method: 开展大规模系统性综述/审查，涵盖241个生物医学图像分析挑战、458项任务、19种成像模态；沿两条维度评估：1）数据集是否代表真实世界临床多样性；2）是否在可获取性、许可与复用性方面符合FAIR原则。

Result: 发现显著的数据集组成偏倚（地理来源、成像模态与问题类型偏倚）；数据访问常受限或不明确，许可证实践不一致或不合规，文档不完整，影响可重复性与长期复用。

Conclusion: 当前基准挑战存在基础性公平性限制，数据代表性与可复用性不足，造成“排行榜成功”与“临床相关性”之间的鸿沟；未来需要改进数据代表性、开放获取与许可规范化及完善文档，以提升临床意义。

Abstract: Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress. However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI. In this work, we assess fairness along two complementary dimensions: (1) whether challenge datasets are representative of real-world clinical diversity, and (2) whether they are accessible and legally reusable in line with the FAIR principles. To address this question, we conducted a large-scale systematic study of 241 biomedical image analysis challenges comprising 458 tasks across 19 imaging modalities. Our findings show substantial biases in dataset composition, including geographic location, modality-, and problem type-related biases, indicating that current benchmarks do not adequately reflect real-world clinical diversity. Despite their widespread influence, challenge datasets were frequently constrained by restrictive or ambiguous access conditions, inconsistent or non-compliant licensing practices, and incomplete documentation, limiting reproducibility and long-term reuse. Together, these shortcomings expose foundational fairness limitations in our benchmarking ecosystem and highlight a disconnect between leaderboard success and clinical relevance.

</details>


### [76] [HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection](https://arxiv.org/abs/2512.17601)
*Zhaolin Cai,Fan Li,Ziwei Zheng,Haixia Bi,Lijun He*

Main category: cs.CV

TL;DR: 提出HeadHunt-VAD：在冻结的多模态大语言模型(MLLM)内直接搜寻对异常最敏感的注意力头，绕过文本生成，实现调参-free视频异常检测，效率高、可解释，并在两大基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 调参/监督方法成本高、数据依赖强；基于MLLM的免调参方法虽具世界知识，但依赖文本输出导致信息损失、正常性偏置与提示敏感，难以捕捉细微异常。

Method: 在冻结的MLLM中进行“头猎”：1)鲁棒头识别模块对所有注意力头做多准则评估（显著性+稳定性），在多提示下筛选出对异常判别稳定、稀疏的专家头；2)将这些头的特征输入轻量级异常评分器与时间定位器，实现视频帧/片段级异常检测与定位；3)输出具可解释性（头级别）的结果。

Result: 在两大VAD基准上，较现有免调参方法取得SOTA，同时保持高效率；证明头级探测能稳定跨提示工作并捕捉微弱异常线索。

Conclusion: 绕过文本生成、直接在MLLM内部注意力头层面探测是有效且实用的VAD路径；鲁棒头识别+轻量评分/定位器带来高效、可解释且强泛化的异常检测效果。

Abstract: Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.

</details>


### [77] [MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration](https://arxiv.org/abs/2512.17605)
*Svetlana Krasnova,Emiliya Starikova,Ilia Naletov,Andrey Krylov,Dmitry Sorokin*

Main category: cs.CV

TL;DR: 提出MGRegBench：一个包含5000+乳腺X线配准图像对（其中100对具有人为标注点与分割掩膜）的公共基准，用于公平评测多种配准方法，并公开代码与数据。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线配准对疾病进展追踪与纵向监测至关重要，但缺乏公开数据集与统一评测标准，导致研究不可比、难复现。

Method: 构建并发布MGRegBench数据集（>5000对图像，100对具人工解剖标注与掩膜），并在该基准上对多类方法进行同台评测：经典（ANTs）、学习型（VoxelMorph、TransMorph）、隐式神经表示（IDIR）、经典乳腺特定方法、以及SOTA深度方法MammoRegNet；实现来自作者代码的移植或从零重实现，统一评估框架。

Result: MGRegBench成为少数且规模最大的具人工标注的2D乳腺配准公共数据之一，实现了首次在该模态上“同规格”比较多种方法，并对深度学习配准进行系统实证分析；提供可复用代码与基准。

Conclusion: MGRegBench为乳腺X线配准提供了标准化数据与评测基线，促进公平比较与未来研究发展；代码与数据已公开（GitHub链接）。

Abstract: Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.

</details>


### [78] [Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR](https://arxiv.org/abs/2512.17610)
*Denis Mikhailapov,Vladimir Berikov*

Main category: cs.CV

TL;DR: 提出一种面向多输出CNN的半监督分割方法，在TBAD三类（TL/FL/FLT）3D CTA任务上，通过旋转与翻转一致性训练，无需概率假设、可泛化到独立分支输出的架构。


<details>
  <summary>Details</summary>
Motivation: TBAD诊断需精确分割TL/FL/FLT，但ImageTBDA仅100例且类不均衡（68有FL、32无），三维精标代价高，限制了多输出CNN（每类独立分支）在实际中的性能与可用性。现有半监督多基于概率化输出（如softmax/互信息等），对独立分支/非概率式输出兼容性差。

Method: 针对多输出分支的分割网络，构建基于几何变换（额外旋转与翻转）的预测一致性正则：在无标签数据上，对输入施加变换并强制原预测与逆变换后的预测一致；不依赖概率归一化或类间互斥假设，适配每类独立通道输出。与有标签监督联合训练；共享编码器、独立解码/头部。

Result: 在TBAD三类分割上取得精度提升（文摘未给出具体数值），尤其在标注数据有限情形下，半监督训练提升多输出架构的分割质量与鲁棒性。

Conclusion: 提出一种无需概率假设、适配多输出分支的通用半监督框架，利用旋转/翻转一致性提高3D CTA中TL/FL/FLT分割的表现，缓解高成本标注问题，并可推广到其他独立分支分割架构。

Abstract: Convolutional neural networks (CNN) for multi-class segmentation of medical images are widely used today. Especially models with multiple outputs that can separately predict segmentation classes (regions) without relying on a probabilistic formulation of the segmentation of regions. These models allow for more precise segmentation by tailoring the network's components to each class (region). They have a common encoder part of the architecture but branch out at the output layers, leading to improved accuracy.
  These methods are used to diagnose type B aortic dissection (TBAD), which requires accurate segmentation of aortic structures based on the ImageTBDA dataset, which contains 100 3D computed tomography angiography (CTA) images. These images identify three key classes: true lumen (TL), false lumen (FL), and false lumen thrombus (FLT) of the aorta, which is critical for diagnosis and treatment decisions. In the dataset, 68 examples have a false lumen, while the remaining 32 do not, creating additional complexity for pathology detection.
  However, implementing these CNN methods requires a large amount of high-quality labeled data. Obtaining accurate labels for the regions of interest can be an expensive and time-consuming process, particularly for 3D data. Semi-supervised learning methods allow models to be trained by using both labeled and unlabeled data, which is a promising approach for overcoming the challenge of obtaining accurate labels. However, these learning methods are not well understood for models with multiple outputs.
  This paper presents a semi-supervised learning method for models with multiple outputs. The method is based on the additional rotations and flipping, and does not assume the probabilistic nature of the model's responses. This makes it a universal approach, which is especially important for architectures that involve separate segmentation.

</details>


### [79] [Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution](https://arxiv.org/abs/2512.17612)
*Alireza Samadifardheris,Dirk H. J. Poot,Florian Wiesinger,Stefan Klein,Juan A. Hernandez-Tamames*

Main category: cs.CV

TL;DR: 提出一种物理先验、无需高分辨率qMRI标注的自监督超分辨学习框架，用常规高分辨率加权MRI作引导，通过贝叶斯MAP最小化物理一致性误差，实现从低分辨率qMRI快速恢复高分辨率松弛参数图；在模拟与跨序列体内数据上验证，1分钟采集可达接近5分钟质量。


<details>
  <summary>Details</summary>
Motivation: qMRI松弛成像能定量表征组织，但临床采集时间长且高分辨率数据难以获得，限制了应用。需要一种不依赖HR qMRI真值、可利用临床常规wMRI的学习方法来加速并提升qMRI分辨率。

Method: 将超分辨表述为贝叶斯MAP问题：用深度网络预测HR qMRI参数图；物理前向模型将预测的参数合成HR加权图像，与实际HR wMRI引导对齐（误差1）；同时将预测的HR qMRI下采样，与采集到的LR qMRI匹配（误差2）。训练数据通过从HR qMRI用信号方程合成wMRI并对qMRI做k-space截断构造；多种引导（T1w、T2w、组合）做消融；在独立序列的体内数据上做泛化验证。

Result: 消融显示：T1w主要提升T1图，T2w主要提升T2图，组合引导可同时最优提升多参数。用合成数据训练的模型在独立采集的不同qMRI序列上保持泛化，1分钟采集经超分辨后可接近5分钟参考质量。

Conclusion: 物理一致性自监督框架能在无需HR qMRI标注的情况下，利用常规临床wMRI引导实现qMRI超分辨，减少采集时间并保持跨序列、跨设备的适用性，为将定量松弛成像更实际地融入临床流程提供可行路径。

Abstract: High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions. This physics-informed objective allows the models to learn from clinical wMRI without HR qMRI supervision. To validate the concept, we generate training data by synthesizing wMRI guides from HR qMRI using signal equations, then degrading qMRI resolution via k-space truncation. A deep neural network learns the super-resolution mapping. Ablation experiments demonstrate that T1-weighted images primarily enhance T1 maps, T2-weighted images improve T2 maps, and combined guidance optimally enhances all parameters simultaneously. Validation on independently acquired in-vivo data from a different qMRI sequence confirms cross-qMRI sequence generalizability. Models trained on synthetic data can produce super-resolved maps from a 1-minute acquisition with quality comparable to a 5-minute reference scan, leveraging the scanner-independent nature of relaxometry parameters. By decoupling training from HR qMRI requirement, our framework enables fast qMRI acquisitions enhanced via routine clinical images, offering a practical pathway for integrating quantitative relaxometry into clinical workflows with acceptable additional scan time.

</details>


### [80] [StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection](https://arxiv.org/abs/2512.17620)
*Di Wu,Feng Yang,Wenhui Zhao,Jinwen Yu,Pan Liao,Benlian Xu,Dingwen Zhang*

Main category: cs.CV

TL;DR: 提出StereoMV2D：在多视角稀疏查询3D检测中，利用相邻帧的时间立体信息改进深度估计与查询初始化，在2D RoI内高效计算，并通过动态置信门控提升鲁棒性，于nuScenes与Argoverse2上以较小开销取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有多视角3D检测需在精度与效率间权衡；稀疏查询方法高效但依赖良好查询先验。MV2D用2D检测初始化查询虽提升召回与精度，但单帧2D存在深度歧义，限制3D先验质量。因此需要能在不显著增算的前提下缓解深度歧义、稳健生成高质量3D查询的方法。

Method: 在MV2D框架上引入时间立体建模：对相邻帧同一目标的跨时间视差进行估计，从而增强深度感知并细化3D查询先验。计算限制在2D RoI内以保持效率；并设计动态置信门控机制，依据帧间匹配矩阵的统计模式与外观一致性自适应评估与选择时间立体线索的可靠性，以在遮挡与外观变化下保持稳健。

Result: 在nuScenes与Argoverse 2数据集上，StereoMV2D在几乎不增加计算开销的情况下优于现有方法（相较MV2D与其他多视角稀疏查询基线取得更高检测性能，文中报告“显著/更优”的指标提升）。

Conclusion: 时间立体线索可有效缓解单帧2D深度歧义，结合RoI内高效计算与动态置信门控，实现对多视角稀疏查询3D检测的精度与鲁棒性提升且保持高效率；方法通用且可扩展，代码开源。

Abstract: Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.

</details>


### [81] [PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology](https://arxiv.org/abs/2512.17621)
*Fengchun Liu,Songhan Jiang,Linghan Cai,Ziyue Wang,Yongbing Zhang*

Main category: cs.CV

TL;DR: PathFLIP提出将WSI的整题级文字描述拆解为区域级子描述，并生成文本条件的区域嵌入，实现更精细的图文对齐；在少量数据下于多基准上优于现有病理VLM，支持分类、检索、病灶定位与指令跟随。


<details>
  <summary>Details</summary>
Motivation: 现有病理VLM在超大尺度、强异质性的WSI上难以建立细粒度的图文对应：一张切片包含成千上万patch，简单的全局对齐导致下游分类、检索与定位性能受限。需要一种既能保持整体语义，又能将文本精确落地到局部区域的对齐框架，并能适配临床多样的指令语境。

Method: 提出PathFLIP：1) 将切片级caption自动分解为区域级subcaption；2) 基于这些subcaption生成文本条件的区域级嵌入，实现视觉-语言细粒度对齐与定位；3) 借助LLM进行指令解析与语义分解，使模型能跟随多样临床指令、适配不同诊断情境；4) 统一训练使其可兼容多范式任务（分类、检索、定位、指令跟随）。

Result: 在四个具有代表性的病理多模态基准上，PathFLIP在使用显著更少训练数据的情况下，整体性能超过现有大规模病理VLM；同时展示了高效的切片级分类/检索、细粒度病灶定位与指令跟随能力。

Conclusion: 细粒度、指令感知的图文预训练能显著提升WSI多模态理解。通过区域级文本-视觉对齐与LLM驱动的指令解析，PathFLIP为临床落地提供更高效、可扩展的WSI解释框架。

Abstract: While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.

</details>


### [82] [Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs](https://arxiv.org/abs/2512.17640)
*Zhaolin Cai,Huiyu Duan,Zitong Xu,Fan Li,Zhi Liu,Jing Liu,Wei Shen,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出GRASP-HOI：将HOI检测从封闭集分类转为开放词汇生成，通过轻量认知引导模块把视觉证据注入冻结的多模态大模型，实现可解释、可泛化的检测；混合训练目标兼顾生成与判别，达成SOTA与强零样本表现。


<details>
  <summary>Details</summary>
Motivation: 现有HOI方法依赖小规模预定义动词集合，难以覆盖真实世界长尾与歧义交互；直接微调MLLM成本高且与现有检测器割裂，需一种既能利用MLLM知识又保持可训练性的框架。

Method: 1) 将HOI从封闭集分类重构为开放词汇生成任务；2) 提取“混合交互表征”（融合细粒度视觉证据与候选对信息）；3) 设计轻量可学习的认知引导通道CSC，把视觉证据注入冻结的MLLM进行生成式推理；4) 提出“混合引导”训练：语言建模损失+辅助分类损失，既保证生成灵活性又提供判别式定位/对齐信号。

Result: 在标准封闭集评测上达到SOTA；在零样本设置下表现强劲，显示良好的开放词汇泛化能力；实现统一范式，将判别式感知与生成式推理有效结合。

Conclusion: GRASP-HOI通过可控生成范式与轻量认知引导，成功把冻结的MLLM与视觉检测耦合，兼顾精确定位与开放词汇表达，为开放世界HOI提供统一、可扩展的解决方案。

Abstract: Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.

</details>


### [83] [Region-Constraint In-Context Generation for Instructional Video Editing](https://arxiv.org/abs/2512.17650)
*Zhongwei Zhang,Fuchen Long,Wei Li,Zhaofan Qiu,Wu Liu,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 提出ReCo：一种面向指令的视频编辑的“对齐约束”式上下文生成范式，通过并排拼接源/目标视频联合去噪，并在潜空间与注意力层引入正则，显著提升编辑区域精准性并减少非编辑区干扰；并发布50万指令-视频对数据集ReCo-Data。


<details>
  <summary>Details</summary>
Motivation: 图像领域的In-context指令编辑高效且质好，但直接迁移到视频会出现：1) 未显式标注编辑区域导致编辑范围不准；2) 去噪过程中编辑与非编辑区域token相互干扰。需要一种在上下文生成中显式建模编辑/非编辑区域约束的方法。

Method: - 设计ReCo：将源视频与目标视频在宽度方向拼接，进行联合扩散去噪。
- 两个正则：
  1) 潜空间正则（latent regularization）：在一步反向去噪后的潜表征上，拉大编辑区源/目标差异，缩小非编辑区差异，突出编辑、抑制越界生成。
  2) 注意力正则（attention regularization）：抑制编辑区token对源视频对应位置token的注意力，减少在目标新物体生成时的干扰。
- 构建ReCo-Data：50万条高质量指令-视频对用于训练。

Result: 在四类主流指令式视频编辑任务上进行大量实验，ReCo在编辑准确性、视觉质量与稳健性方面优于现有方法（文中实证）。

Conclusion: 通过在上下文生成框架中引入“编辑/非编辑区域约束”，ReCo有效缓解区域不准与token干扰问题，显著提升指令式视频编辑性能，并以大规模数据集进一步增强训练与泛化。

Abstract: The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.

</details>


### [84] [Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos](https://arxiv.org/abs/2512.17655)
*Evangelos Sariyanidi,Gokul Nair,Lisa Yankowitz,Casey J. Zampella,Mohan Kashyap Pargi,Aashvi Manakiwala,Maya McNealis,John D. Herrington,Jeffrey Cohn,Robert T. Schultz,Birkan Tunc*

Main category: cs.CV

TL;DR: Bitbox 是一个面向行为与临床科研的开源视频行为计算测量工具包，提供可复现、可解释、模块化的高层行为指标抽取接口，降低非工程背景研究者使用 AI 行为分析的门槛，并为算法研究者提供落地与传播渠道。


<details>
  <summary>Details</summary>
Motivation: AI 已能从视频精细量化面部、头部与身体行为，但现有工具多面向工程人群、依赖复杂技术栈、缺乏直接可用于假设驱动研究的高层指标，导致心理/精神/神经与临床研究领域采用缓慢、门槛高。

Method: 构建开源工具包 Bitbox：遵循可复现性、模块化、可解释性；提供标准化接口聚合多种人脸、头部与身体处理器；输出高层行为度量；核心模块在临床样本上测试与验证，并支持低成本扩展新测度。

Result: 形成一个即插即用的 standardized 管道，可稳健地从视频提取高层行为指标；已在临床样本验证核心模块可用性；为研究者提供无需工程背景的使用体验，同时为计算机科学家提供方法发布与迁移路径。

Conclusion: Bitbox 降低了计算行为测量在行为、临床与心理健康研究中的进入门槛与转化阻力，预期将加速该领域的整合与应用，并作为社区驱动项目持续演进。

Abstract: Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.

</details>


### [85] [Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation](https://arxiv.org/abs/2512.17673)
*Alexandre Personnic,Mihai Bâce*

Main category: cs.CV

TL;DR: 提出ST-Gaze，将CNN骨干与通道注意力和自注意力结合，先在帧内以空间序列建模再跨时间传播，在EVE数据集上达SOTA，优于提前空间池化；适用于通用相机场景。


<details>
  <summary>Details</summary>
Motivation: 视频凝视估计需要同时捕获帧内（空间）与帧间（时间）关系，现有方法受限于特征表达不足与对空间上下文的过早聚合，导致时序建模受损。

Method: 设计ST-Gaze：1）CNN提取脸部与眼部特征；2）通道注意力与自注意力模块对眼/脸特征进行最优融合；3）将融合后的特征视为“空间序列”以建模帧内上下文；4）通过时序递归/传播机制在多帧间建模帧间动态；并进行消融分析。

Result: 在EVE数据集上，无论是否进行个体自适应，均达到SOTA性能；消融显示保留并建模帧内空间上下文且再进行时序传播，显著优于提前做空间池化。

Conclusion: 通过将帧内空间上下文与跨帧时序动态联合建模，ST-Gaze提升了视频凝视估计的鲁棒性与精度，为使用常见相机的实际应用铺路。

Abstract: Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.

</details>


### [86] [An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution](https://arxiv.org/abs/2512.17675)
*Yudhistira Arief Wibowo*

Main category: cs.CV

TL;DR: 本文对扩散模型在单图像超分中的条件化策略做经验消融，发现“条件步长”比“扩散步数”更关键，最佳步长约在2.0–3.0。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为无条件先验在逆问题（如超分）中表现强，但现有条件化方法（如DPS、MCG）需新增超参数，调参成本高且影响大。作者希望厘清哪些因素主导性能，以减轻调参负担并给出实用指导。

Method: 在FFHQ超分任务上，基于预训练的无条件扩散模型，系统比较不同条件化方法的关键超参数。通过固定其他设置、逐一改变扩散步数与条件步长等，进行消融实验与量化评估，以隔离各因素的影响。

Result: 实验表明：条件步长对重建质量影响远大于扩散步数；当条件步长取2.0–3.0区间时，整体性能最佳；增加扩散步数带来的收益相对有限。

Conclusion: 在对预训练扩散模型进行条件化以解决超分问题时，应优先调参条件步长而非盲目增加扩散步数。实际应用中建议首先在步长2.0–3.0范围内搜索，可获得更稳健、成本更低的性能提升。

Abstract: Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.

</details>


### [87] [FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation](https://arxiv.org/abs/2512.17717)
*Cheng Peng,Zhuo Su,Liao Wang,Chen Guo,Zhaohu Li,Chengjiang Long,Zheng Lv,Jingxiang Sun,Chenyangguang Zhang,Yebin Liu*

Main category: cs.CV

TL;DR: FlexAvatar是一种无需相机位姿与表情标签、可从单/少量图像重建高保真可动画3D头部化身的大模型。其以Transformer与结构化头部查询作为规范锚点聚合多视输入，并配合UV条件的轻量UNet实现实时细节表情形变，辅以数据分布重加权与10秒个性化微调以增强极端身份与罕见表情细节，整体在3D一致性与动态真实感上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D头部化身重建常依赖多视/视频、精确相机位姿或表情标签，且难以兼顾单/稀疏输入下的三维一致性、动态细节（皱纹、露齿等）与实时性；同时个体化极端身份细节易丢失。需要通用、输入数目无关、无需先验的重建方法，并能捕捉表情驱动的细微变形。

Method: 1) 以Transformer重建器为核心，引入结构化头部查询token作为规范空间锚点，聚合任意数量、无位姿与无表情标签的图像，获得稳健的规范3D表示；2) 设计以UV空间位置图为条件的轻量UNet解码器，实时预测与表情相关的细节动态形变；3) 训练时对数据分布进行调整，提升皱纹、露齿等稀有但关键表情的覆盖；4) 推理端提供约10秒的轻量个性化微调，增强极端身份的细节而不损伤形变质量。

Result: 在大量实验中，相较现有方法，FlexAvatar展现出更强的三维一致性、更逼真的动态细节与更好的可动画性，并实现实时形变生成；在罕见表情与极端身份上也有明显改进。

Conclusion: FlexAvatar以输入数量无关、无需位姿/表情标签的重建框架与UV条件UNet实现高保真、可动画的3D头部化身，结合数据分布重加权与快速个性化微调，取得优于现有方法的3D一致性与动态真实感，具备实用价值。

Abstract: We present FlexAvatar, a flexible large reconstruction model for high-fidelity 3D head avatars with detailed dynamic deformation from single or sparse images, without requiring camera poses or expression labels. It leverages a transformer-based reconstruction model with structured head query tokens as canonical anchor to aggregate flexible input-number-agnostic, camera-pose-free and expression-free inputs into a robust canonical 3D representation. For detailed dynamic deformation, we introduce a lightweight UNet decoder conditioned on UV-space position maps, which can produce detailed expression-dependent deformations in real time. To better capture rare but critical expressions like wrinkles and bared teeth, we also adopt a data distribution adjustment strategy during training to balance the distribution of these expressions in the training set. Moreover, a lightweight 10-second refinement can further enhances identity-specific details in extreme identities without affecting deformation quality. Extensive experiments demonstrate that our FlexAvatar achieves superior 3D consistency, detailed dynamic realism compared with previous methods, providing a practical solution for animatable 3D avatar creation.

</details>


### [88] [SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses](https://arxiv.org/abs/2512.17724)
*Shaoyan Zhai,Mohamed Abdel-Aty,Chenzhu Wang,Rodrigo Vena Garcia*

Main category: cs.CV

TL;DR: SAVeD 提供首个大规模、聚焦ADAS真实高风险情境（碰撞、险情、接管）的社媒视频数据集，并配套标注与评测框架，证明可用于TTC估计、极端风险建模与VLLM基线提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为仿真或人工驾驶，缺乏含ADAS在真实高风险边缘工况下的行为数据与细粒度标注，限制了对感知/决策失效与安全研究。

Method: 从公开社媒收集并筛选ADAS相关第一人称视频，形成2119段多场景、多光照与天气的数据；提供帧级标注（碰撞、规避、接管）。提出语义分割+单目深度融合的实时TTC计算框架；采用GEV分布建模碰撞/险情极端风险；在VideoLLaMA2与InternVL2.5 HiCo R16上进行域自适应训练与基准评测。

Result: SAVeD使得对感知与决策失败的细粒度分析成为可能；所提TTC框架可对动态目标进行实时风险量化；GEV模型刻画不同道路类型下的极端风险差异；利用SAVeD标注进行域自适应显著提升VLLM在复杂险情上的表现。

Conclusion: SAVeD填补ADAS真实高风险数据空白，提供可复用的标注与评测基线，既支持安全机理研究（TTC与极端风险）也促进多模态视频理解模型在近失事故场景的适应与性能提升。

Abstract: The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.

</details>


### [89] [MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image](https://arxiv.org/abs/2512.17726)
*Qian Zeng,Yihui Wang,Shu Yang,Yingxue Xu,Fengtao Zhou,Jiabo Ma,Dejia Cai,Zhengyu Zhang,Lijuan Qu,Yu Wang,Li Liang,Hao Chen*

Main category: cs.CV

TL;DR: 提出MambaMIL+用于WSI多实例学习，通过重叠扫描、选择性条带位置编码(S2PE)与上下文token选择(CTS)解决长序列空间建模与记忆衰减问题，在20个基准与三种特征提取器下达SOTA。


<details>
  <summary>Details</summary>
Motivation: WSI为千兆像素、标注稀缺，MIL可行但需处理超长序列与丰富空间上下文。现有长序列模型如Mamba虽线性扩展高效，但空间上下文弱、存在记忆衰减，限制在WSI中的表现。

Method: 设计MambaMIL+：1) 重叠扫描，将patch按重叠轨迹串联以嵌入空间连续性与实例相关性；2) S2PE（选择性条带位置编码），在条带/行列粒度编码位置信息，缓解固定扫描顺序偏置；3) CTS（上下文token选择），基于监督信号动态扩容上下文记忆，稳定远程依赖建模。整体以Mamba为骨干的MIL框架，兼顾线性复杂度与空间建模。

Result: 在诊断分类、分子预测、生存分析三类任务、20个公开基准上，配合ResNet-50、PLIP、CONCH三种特征，均取得一致SOTA；表现稳健、泛化性强。

Conclusion: 通过融合重叠空间扫描、条带位置编码与监督驱动的上下文选择，MambaMIL+克服Mamba在WSI的空间与记忆瓶颈，实现长程依赖且不过度遗忘，适用于大规模计算病理学。

Abstract: Whole-slide images (WSIs) are an important data modality in computational pathology, yet their gigapixel resolution and lack of fine-grained annotations challenge conventional deep learning models. Multiple instance learning (MIL) offers a solution by treating each WSI as a bag of patch-level instances, but effectively modeling ultra-long sequences with rich spatial context remains difficult. Recently, Mamba has emerged as a promising alternative for long sequence learning, scaling linearly to thousands of tokens. However, despite its efficiency, it still suffers from limited spatial context modeling and memory decay, constraining its effectiveness to WSI analysis. To address these limitations, we propose MambaMIL+, a new MIL framework that explicitly integrates spatial context while maintaining long-range dependency modeling without memory forgetting. Specifically, MambaMIL+ introduces 1) overlapping scanning, which restructures the patch sequence to embed spatial continuity and instance correlations; 2) a selective stripe position encoder (S2PE) that encodes positional information while mitigating the biases of fixed scanning orders; and 3) a contextual token selection (CTS) mechanism, which leverages supervisory knowledge to dynamically enlarge the contextual memory for stable long-range modeling. Extensive experiments on 20 benchmarks across diagnostic classification, molecular prediction, and survival analysis demonstrate that MambaMIL+ consistently achieves state-of-the-art performance under three feature extractors (ResNet-50, PLIP, and CONCH), highlighting its effectiveness and robustness for large-scale computational pathology

</details>


### [90] [AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection](https://arxiv.org/abs/2512.17730)
*Yichen Jiang,Mohammed Talha Alam,Sohail Ahmed Khan,Duc-Tien Dang-Nguyen,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出利用CLIP与参数高效迁移学习检测多种生成模型的深伪，含新数据集Diff-Gen与方法AdaptPrompt，并通过剪枝视觉编码器末层提升跨域与少样本泛化，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 深伪检测在遇到未见过的生成器时易失效，现有以GAN为主的数据集与方法对扩散模型等新范式泛化差；需要统一、鲁棒、跨域与少样本能力强的检测框架与基准。

Method: 1) 构建Diff-Gen：10万张扩散模型生成图像，包含更广谱的频域/纹理伪影，作为训练与评测基准。2) 提出AdaptPrompt：在冻结CLIP主干下，联合学习文本侧任务提示词与视觉侧adapter，参数高效迁移。3) 视觉编码器剪枝：通过层消融，裁剪最后一个Transformer block，以更好保留高频生成伪影信号，提升检测。4) 进行跨域与少样本设置下的训练与评估，并扩展到来源归因任务。

Result: 在25个具有挑战性的测试集（覆盖GAN、扩散、商业工具）上，标准与跨域检测均达成新的SOTA；在少样本场景（仅320张图）仍具强泛化；在闭集条件下可准确归因生成器架构来源。

Conclusion: 基于CLIP的参数高效框架结合Diff-Gen与末层剪枝，可稳健捕捉多范式生成伪影，实现强跨域与少样本深伪检测与来源归因，为面向新型生成模型的通用检测提供有效路径。

Abstract: Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.

</details>


### [91] [Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image](https://arxiv.org/abs/2512.17773)
*Simon Giebenhain,Tobias Kirschstein,Liam Schoneveld,Davide Davoli,Zhe Chen,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出Pix2NPHM：用单张图像通过ViT直接回归NPHM参数，实现高保真、可交互速率的人脸3D重建，并可在推理时进一步优化几何细节。


<details>
  <summary>Details</summary>
Motivation: NPHM相比传统3DMM能表示更精细几何，但其潜变量空间表达力强、难以从视觉输入稳健拟合。需要一种既能广泛泛化、又能高效准确地从单张图像得到NPHM参数的方案。

Method: - 设计Pix2NPHM：以领域特化、在几何任务上预训练的ViT为骨干，直接回归NPHM参数。
- 训练数据混合：>100K NPHM注册提供SDF空间的直接监督；大规模2D视频数据通过法线估计作为伪几何真值。
- 推理速度快，可选推理时优化：基于估计的表面法线与canonical point maps做能量最小化，进一步提升几何保真度。

Result: 在单张图人脸重建上获得更可辨识的面部几何与更准确的表情；达到交互级帧率；在野外数据上规模化运行并实现前所未有的重建质量。

Conclusion: 直接回归NPHM参数结合多源监督与推理时几何一致性优化，能在保持实时性的同时显著提升人脸3D重建的精细度与鲁棒性，相比现有方法更实用、可扩展。

Abstract: Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.

</details>


### [92] [LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence](https://arxiv.org/abs/2512.17781)
*Yohanes Yudhi Adikusuma,Qixing Huang,Ying He*

Main category: cs.CV

TL;DR: LiteGE是一种轻量级的3D表面测地距离估计方法，通过在信息性体素处对UDF采样做PCA得到紧凑的、类别感知的形状描述子，在稀疏点云上仍稳健，显著降低内存与时延，并可用于快速准确的形状对应。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的测地距离估计依赖大型3D骨干网络，内存开销与推理延迟高，不适合交互式或资源受限场景；同时测地距离与形状对应紧密相关，若能以更轻的表征替代大模型，可同时提升测距与匹配效率。

Method: 提出LiteGE：对形状的无符号距离场（UDF）在“信息性”体素处进行采样，使用PCA学习低维、类别感知的形状描述子；以此描述子替代高容量网络进行测地距离估计与形状匹配。方法可直接处理点云（稀疏至约300点）。

Result: 相较现有神经方法，内存和推理时间最高降至1/300；在形状匹配任务中，较网格为基础的SOTA方法实现最高1000倍加速，同时在非等距形状对及点云输入上保持相当精度。

Conclusion: 基于UDF+PCA的轻量描述子足以支撑高质量的测地距离估计与形状对应，在资源受限与交互式应用中表现优越，并在稀疏点云场景下具备强鲁棒性。

Abstract: Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.

</details>


### [93] [UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover](https://arxiv.org/abs/2512.17782)
*Arya Chavoshi,Hassan Dashtian,Naveen Sudharsan,Dev Niyogi*

Main category: cs.CV

TL;DR: 提出UrbanDIFF：一种仅用空间信息的扩散去噪图像修复模型，用于在云覆盖条件下重建城市LST；在高缺失率下优于插值基线，性能退化更慢。


<details>
  <summary>Details</summary>
Motivation: LST对监测城市热岛至关重要，但云覆盖导致观测缺失。现有方法多依赖时序或多源数据，在持续多云条件下难以获得或可靠；仅空间填充在大面积缺失时效果差，深度模型也随缺失率升高而快速退化。扩散式修复在高缺失率下更稳健，值得用于LST重建。

Method: 提出UrbanDIFF：纯空间的去噪扩散（diffusion）图像修复模型。以静态城市结构（如建成区数据和DEM）作为条件；推理阶段加入监督的像素引导精修步骤，强制与无云像素保持一致。训练与评估基于2002–2025年7个美国大都市的MODIS Terra LST，使用合成云掩膜（20–85%覆盖）进行实验。

Result: 与插值基线相比在不同云密度下均优，尤其在85%云覆盖时仍达SSIM 0.89、RMSE 1.2 K、R² 0.84，并显示随云密度增加性能下降更慢。

Conclusion: 基于扩散的纯空间LST重建在高缺失率和大范围云遮挡下具有明显鲁棒性；结合静态城市先验与像素一致性约束可提升SUHI连续监测的可用性。

Abstract: Satellite-derived Land Surface Temperature (LST) products are central to surface urban heat island (SUHI) monitoring due to their consistent grid-based coverage over large metropolitan regions. However, cloud contamination frequently obscures LST observations, limiting their usability for continuous SUHI analysis. Most existing LST reconstruction methods rely on multitemporal information or multisensor data fusion, requiring auxiliary observations that may be unavailable or unreliable under persistent cloud cover. Purely spatial gap-filling approaches offer an alternative, but traditional statistical methods degrade under large or spatially contiguous gaps, while many deep learning based spatial models deteriorate rapidly with increasing missingness.
  Recent advances in denoising diffusion based image inpainting models have demonstrated improved robustness under high missingness, motivating their adoption for spatial LST reconstruction. In this work, we introduce UrbanDIFF, a purely spatial denoising diffusion model for reconstructing cloud contaminated urban LST imagery. The model is conditioned on static urban structure information, including built-up surface data and a digital elevation model, and enforces strict consistency with revealed cloud free pixels through a supervised pixel guided refinement step during inference.
  UrbanDIFF is trained and evaluated using NASA MODIS Terra LST data from seven major United States metropolitan areas spanning 2002 to 2025. Experiments using synthetic cloud masks with 20 to 85 percent coverage show that UrbanDIFF consistently outperforms an interpolation baseline, particularly under dense cloud occlusion, achieving SSIM of 0.89, RMSE of 1.2 K, and R2 of 0.84 at 85 percent cloud coverage, while exhibiting slower performance degradation as cloud density increases.

</details>


### [94] [Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras](https://arxiv.org/abs/2512.17784)
*Ami Pandat,Punna Rajasekhar,G. Aravamuthan,Gopika Vinod,Rohit Shukla*

Main category: cs.CV

TL;DR: 提出一种用于远距离（三维定位达5公里）的摄像机畸变建模与校准框架：先扩展传统畸变模型的高阶项，再用神经网络做残差校正，显著提升长距离立体定位精度与稳健性，并将结果映射到GIS可视化。


<details>
  <summary>Details</summary>
Motivation: 传统立体视觉/摄影测量在长距离（>几百米）下定位精度受限，关键瓶颈在于镜头非线性畸变的建模不足。直接用神经网络去拟合畸变函数常常不收敛、参数不可辨识，导致相机参数估计失败。需要一种既能表达复杂非线性、又可稳健收敛和可解释的畸变模型与校准流程。

Method: 提出混合畸变建模框架：1) 在常规的径向/切向畸变模型基础上加入更高阶项进行扩展；2) 在此基础上学习一个神经网络作为残差校正器，仅拟合传统模型未能解释的误差；3) 通过该混合模型完成相机标定与立体几何解算；4) 将估计的3D坐标转换为GIS坐标并在GIS地图上可视化。

Result: 与仅用传统模型或直接用神经网络相比，混合方法在远距离场景中显著提升3D定位精度与鲁棒性，实现最长约5公里目标位置估计；实验验证显示该框架能稳定收敛并有效校准CCTV相机。

Conclusion: 通过“高阶传统模型+神经网络残差”的混合畸变框架，可在长距离摄影测量中获得高精度、稳健的相机标定与物体三维定位，并可无缝对接GIS坐标与可视化，具有实际部署价值（如CCTV长距监测与测绘）。

Abstract: Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances. Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range. This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens. This paper presents a framework for modeling a suitable distortion model that can be used for localizing the objects at longer distances. It is well known that neural networks can be a better alternative to model a highly complex non-linear lens distortion function; on contrary, it is observed that a direct application of neural networks to distortion models fails to converge to estimate the camera parameters. To resolve this, a hybrid approach is presented in this paper where the conventional distortion models are initially extended to incorporate higher-order terms and then enhanced using neural network based residual correction model. This hybrid approach has substantially improved long-range localization performance and is capable of estimating the 3D position of objects at distances up to 5 kilometres. The estimated 3D coordinates are transformed to GIS coordinates and are plotted on a GIS map for visualization. Experimental validation demonstrates the robustness and effectiveness of proposed framework, offering a practical solution to calibrate CCTV cameras for long-range photogrammetry applications.

</details>


### [95] [Animate Any Character in Any World](https://arxiv.org/abs/2512.17796)
*Yitong Wang,Fangyun Wei,Hongyang Zhang,Bo Dai,Yan Lu*

Main category: cs.CV

TL;DR: AniX提出一种结合静态世界生成的真实感与结构约束，同时扩展到用户可控角色执行开放式动作的交互式视频生成框架。给定3DGS场景与角色，通过自然语言指令生成时间连贯、角色一致、动作受控的高保真视频片段，并在预训练视频生成器上以条件自回归方式训练，显著提升运动动态与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么只能生成无智能体的静态3D环境，要么仅支持单一可控实体在基本不可控环境中做有限动作，难以满足“给定真实场景与角色、以自然语言驱动开放式交互行为”的需求。需要一种既保持场景与角色真实一致，又能长时序、可控地执行多样动作的生成方法。

Method: 提出AniX：以3DGS场景与用户指定角色为条件，把视频合成建模为条件自回归视频生成。基于预训练视频生成器，设计训练策略强化运动动力学与长期一致性，同时保持动作与角色跨域泛化。系统支持自然语言指令驱动，从基础移动到面向物体的交互；输出为时间连贯、视觉保真度高的视频片段。

Result: 在多个维度进行评估，包括视觉质量、角色一致性、动作可控性与长时程连贯性，实验表明AniX在这些方面取得显著提升，能在多动作与多角色场景中保持高保真与可控。

Conclusion: AniX有效融合静态世界生成的真实感与可控实体模型的交互性，实现对用户指定角色在真实3D场景中的开放式动作控制与视频合成，并在运动动态与泛化方面优于现有方法。

Abstract: Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.

</details>


### [96] [Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding](https://arxiv.org/abs/2512.17817)
*Yue Li,Qi Ma,Runyi Yang,Mengjiao Ma,Bin Ren,Nikola Popovic,Nicu Sebe,Theo Gevers,Luc Van Gool,Danda Pani Paudel,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出Chorus：用多教师蒸馏预训练一个可前馈的3DGS场景编码器，将2D大模型的语言、通用、目标感知信号对齐到共享3D嵌入，广泛任务上有效并可迁移到仅点云设置；还给出渲染-蒸馏适配实现域外微调。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼洒(3DGS)具备高保真场景表示，但如何直接从其原语学习通用且语义丰富的特征仍欠研究。需要一个能统一吸收多源高层语义与细粒度几何的3D编码器，提升下游开放词汇分割与高效监督等任务表现与跨域泛化。

Method: 提出Chorus多教师预训练框架：使用共享3D编码器+教师特定投影头，从多个2D基础模型蒸馏，包括语言对齐、通用视觉与目标感知教师；学习一个共享嵌入空间，覆盖从高层语义到细粒度结构的信号。除完整3DGS输入外，还训练仅用高斯中心、颜色、法向的点云变体；并提出render-and-distill适配策略用于域外微调。

Result: 在开放词汇语义/实例分割、线性与解码器探测、数据高效监督等多任务上取得强表现。点云变体在仅支持点云的基准上具有更强迁移性，且以少39.9倍训练场景超越点云基线。

Conclusion: 多教师蒸馏可为3DGS学习到统一通用特征，既支持多下游任务又具数据与域外适配效率；方法可推广到点云设定并优于传统点云基线。

Abstract: While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.
  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.

</details>


### [97] [ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges](https://arxiv.org/abs/2512.17838)
*Roshan Kenia,Xiaoman Zhang,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 提出ReX-MLE基准，评测LLM自主编码代理在医学影像端到端任务上的真实能力，发现现有顶级代理在该领域几乎全面失效，为构建领域感知的自主AI指明改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM编码代理在通用软件/ML任务上表现不错，但在复杂、领域特定的科学问题（尤其医学影像）上缺乏有效评测与能力；医学影像涉及长训练周期、高维数据、复杂预处理与严格验证，现有基准未覆盖这些关键环节。

Method: 构建ReX-MLE基准：从高影响力医学影像竞赛中精选20个跨模态、跨任务的挑战；要求代理在现实算力与时间约束下完成从数据预处理、模型训练到结果提交的端到端流程；对多种SOTA代理（AIDE、ML-Master、R&D-Agent）及不同LLM后端（GPT-5、Gemini、Claude）进行系统评测。

Result: 大多数代理提交的结果在人类专家基线中处于第0百分位，显示显著性能鸿沟；失败主要由领域知识匮乏与工程能力不足导致。

Conclusion: ReX-MLE揭示了LLM自主代理在医学影像端到端工作流中的关键瓶颈，为开发具备领域知识与工程鲁棒性的自主AI系统提供了基准与研究方向。

Abstract: Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.

</details>


### [98] [InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.17851)
*Sarah Rastegar,Violeta Chatalbasheva,Sieger Falkena,Anuj Singh,Yanbo Wang,Tejas Gokhale,Hamid Palangi,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: InfSplign是一种无需训练、在推理时通过复合损失调控扩散去噪过程的T2I空间对齐方法，利用多层跨注意力图同时约束物体位置与存在性，轻量、可插拔、兼容任意扩散骨干，并在VISOR与T2I-CompBench上超越现有推理与微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型难以准确遵循文本中的空间关系，原因在于：训练数据缺乏细粒度空间监督，以及文本嵌入本身难以编码空间语义。因此需要一种无需额外训练、能在推理阶段提升空间对齐的通用方法。

Method: 提出InfSplign：在每个去噪步骤引入复合损失以调整噪声。损失基于从骨干解码器提取的多尺度跨注意力图，既约束精确的物体位置（空间关系）又维持物体出现的平衡。方法轻量、即插即用，可与任意扩散骨干兼容。

Result: 在VISOR与T2I-CompBench基准上，较最强的现有推理时基线取得显著提升，并且优于需要微调的训练方法，达到新的SOTA（据作者所知）。

Conclusion: 通过在推理时利用跨注意力图施加复合空间约束，InfSplign无需训练即可显著提升T2I的空间对齐能力，具备通用性与实用性，并已开源。

Abstract: Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.

</details>


### [99] [Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions](https://arxiv.org/abs/2512.17852)
*Mengkun Chen,Sanidhya D. Tripathi,James W. Tunnell*

Main category: cs.CV

TL;DR: 提出一个基于物理统计噪声建模与深度学习结合的去噪框架，用于在强荧光背景下提升拉曼光谱质量，并在模拟的人体皮肤数据上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 拉曼光谱在生物医学诊断中具有高特异性且无标记、无损伤优势，但生物组织测量常受弱拉曼信号与强荧光背景及探测器随机噪声的影响，导致谱质下降、定量与判别困难。需要一种能在荧光主导条件下有效提升信噪比与去基线的方法，以加速更准确的组织分析。

Method: 建立统计学扎实的噪声物理模型，全面涵盖主要噪声源；基于该模型生成生物学真实感的合成拉曼光谱数据；设计级联深度神经网络，联合抑制随机探测器噪声与荧光基线干扰；以来自真实实验的人体皮肤光谱为基础进行仿真并作为验证案例。

Result: 仿真与验证显示，所提物理先验驱动的学习方法能显著改善在荧光主导条件下的光谱质量，相较原始数据获得更清晰的拉曼特征峰与更高的有效信噪比，从而支持更快、更准确的组织分析。

Conclusion: 物理信息指导的深度学习去噪在强荧光背景下对拉曼光谱提升显著，可作为加速、提高准确度的组织诊断工具；为今后在真实生物样本与成像场景中的应用奠定基础。

Abstract: Raman spectroscopy enables non-destructive, label-free molecular analysis with high specificity, making it a powerful tool for biomedical diagnostics. However, its application to biological tissues is challenged by inherently weak Raman scattering and strong fluorescence background, which significantly degrade signal quality. In this study, we present a simulation-driven denoising framework that combines a statistically grounded noise model with deep learning to enhance Raman spectra acquired under fluorescence-dominated conditions. We comprehensively modeled major noise sources. Based on this model, we generated biologically realistic Raman spectra and used them to train a cascaded deep neural network designed to jointly suppress stochastic detector noise and fluorescence baseline interference. To evaluate the performance of our approach, we simulated human skin spectra derived from real experimental data as a validation case study. Our results demonstrate the potential of physics-informed learning to improve spectral quality and enable faster, more accurate Raman-based tissue analysis.

</details>


### [100] [Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN](https://arxiv.org/abs/2512.17864)
*Balram Singh,Ram Prakash Sharma,Somnath Dey*

Main category: cs.CV

TL;DR: 提出一种在VGG16各卷积阶段嵌入CBAM的可解释注意力引导CNN（CBAM‑VGG16），用于植物叶片病害检测，在五个数据集上最高达98.87%准确率并具良好泛化；结合CBAM注意力图、Grad‑CAM/++与LRP展示可解释性；代码开源。


<details>
  <summary>Details</summary>
Motivation: 植物病害严重威胁粮食安全，现有方法需兼顾高准确率与可解释性，以便在农业诊断中可信部署与决策。

Method: 在标准VGG16的每个卷积阶段集成Convolutional Block Attention Module（通道+空间注意力）形成CBAM‑VGG16；在五个不同植物病害数据集上训练与评估；采用CBAM注意力图、Grad‑CAM、Grad‑CAM++、LRP进行可解释性与定位分析。

Result: 在多数据集上优于近期方法，最高准确率达98.87%，表现出较强泛化与鲁棒性；可视化结果显示模型关注病斑区域、定位合理。

Conclusion: CBAM‑VGG16在植物病害检测中实现高精度与可解释性，适用于智慧农业场景；开放源码便于复现与扩展，推进可解释AI在农业诊断的应用。

Abstract: Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.

</details>


### [101] [InSPECT: Invariant Spectral Features Preservation of Diffusion Models](https://arxiv.org/abs/2512.17873)
*Baohua Yan,Qingyuan Liu,Jennifer Kava,Xuan Di*

Main category: cs.CV

TL;DR: 论文提出InSPECT，一种在扩散前向/反向过程中保留不变谱特征（Fourier系数）的扩散模型，通过让频域系数平滑趋近随机噪声以兼顾特征保留与多样性，带来更快收敛、更平滑扩散和更好生成质量；在CIFAR‑10、CelebA、LSUN上较DDPM在10K迭代下平均FID降39.23%、IS升45.80%。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型将数据扩散到纯白噪声再重建，导致预测任务极难、计算代价高、收敛慢，且可能损失对生成质量关键的结构性信息。作者观察到图像在频域存在可稳定/不变的谱特征，若能在扩散过程中保持这些特征，可能缓解重建难度并提升效率与质量。

Method: 提出InSPECT：在前向/反向过程中显式约束并保留图像的“不变谱特征”。具体做法是在频域（Fourier域）对系数进行调度，使其在前向末端平滑收敛到指定的随机噪声分布，同时保持若干频谱统计或系数不变，从而兼顾随机性与特征保留；在反向采样中施加相应的约束或正则，使模型重构时遵循这些不变谱特性。整体仍是扩散框架，但在噪声注入与去噪步骤中加入谱域保持机制。

Result: 在CIFAR‑10、Celeb‑A、LSUN上，与DDPM在10K迭代、给定相同参数设置下相比：平均FID下降39.23%，IS提升45.80%；观察到更高的视觉多样性、更快的收敛速度和更平滑的扩散轨迹。

Conclusion: 在扩散过程中保留不变的频域特征能显著降低重建难度并提升效率与生成质量。InSPECT首次系统分析并保留扩散模型中的不变谱特征，带来更低FID、更高IS、更快收敛，提示未来可在频域或其他先验约束上改进扩散模型。

Abstract: Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.

</details>


### [102] [Visually Prompted Benchmarks Are Surprisingly Fragile](https://arxiv.org/abs/2512.17875)
*Haiwen Feng,Long Lian,Lisa Dunlap,Jiahao Shu,XuDong Wang,Renhao Wang,Trevor Darrell,Alane Suhr,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 论文揭示：视觉提示式评测对“无关细节”高度敏感，小改动（标记颜色、尺寸、JPEG压缩等）即可颠覆VLM榜单。作者据此发布含16种标记变体的大规模基准VPBench以提升稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM评测想检验模型是否能独立于文本先验进行视觉理解。视觉提示（在图像中以坐标+可视标记指向目标）是重要路线，但作者观察到现有基准对标记外观等细节异常脆弱，导致排名不稳定与可被“操纵”。需要系统性分析这些因素并提供更稳健的评测资源。

Method: - 选取9个常用开源与闭源VLM。
- 在两类视觉提示任务上系统改变基准设置：标记颜色、尺寸、设计样式及数据集规模；并考察推理层面的低级因素（如API调用的JPEG压缩等级）。
- 量化这些变量对准确率与排行榜的影响，并与常规语义类VLM评测对比。
- 基于现有数据集整理与扩充，构建包含16种视觉标记变体的VPBench，并提供分析工具。

Result: - 模型对标记细节极其敏感：仅将红色改为蓝色即可显著改变模型相对排名。
- 稍增标记尺寸即可使较小的开源模型（如 InternVL3-8B）在名次上追平或超越更大的商用模型（如 Gemini 2.5 Pro）。
- 低级推理设置（如JPEG压缩水平）亦会改变模型排列。
- 这种不稳定性在视觉提示式评测中远大于传统语义类评测。

Conclusion: 视觉提示评测当前存在显著的不稳健与可操纵性，排名对标记与推理细节高度依赖。为缓解问题，作者推出更大、覆盖16种标记变体的VPBench与工具，以期提供更可靠的VLM视觉感知评测框架。

Abstract: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.

</details>


### [103] [Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training](https://arxiv.org/abs/2512.17891)
*Kristoffer Wickstrøm,Teresa Dorszewski,Siyan Chen,Michael Kampffmeyer,Elisabeth Wetzer,Robert Jenssen*

Main category: cs.CV

TL;DR: 提出一种无需重训即可把已训练的ViT模型转为可自解释模型的方法：关键点计数分类器（KCC），通过ViT自动匹配的关键点，构建可视化、可解释的决策；实验显示相较基线提升人机沟通与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自解释模型需要特殊结构与复杂训练流程，不实用；而ViT为核心的基础模型普及，缺乏透明性与可靠性更成问题，需新方法赋予其可解释能力且不破坏现有模型。

Method: 利用ViT近期被发现的高精度图像间关键点匹配能力，提出KCC：在不重训练的前提下，对任意已训练的ViT模型进行包装，将预测过程转化为对与类别原型/参考的匹配关键点的计数与聚合；该过程天然可在输入上可视化，形成直观的解释。

Result: 广泛实验表明，KCC在可解释性与人机沟通质量方面优于近期基线；在保持原有模型的性能前提下，提供更清晰的决策证据展示。

Conclusion: KCC为将ViT基础模型转化为透明、可靠的自解释模型提供了简单实用的路径，推进了无重训可解释化方向的发展。

Abstract: Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.

</details>


### [104] [RadarGen: Automotive Radar Point Cloud Generation from Cameras](https://arxiv.org/abs/2512.17897)
*Tomer Borreda,Fangqiang Ding,Sanja Fidler,Shengyu Huang,Or Litany*

Main category: cs.CV

TL;DR: RadarGen是一种将多视角相机图像条件输入的扩散模型，用BEV雷达表示（含RCS与多普勒）合成逼真的车载雷达点云，并用轻量重建生成点云；利用预训练模型提供的深度/语义/运动BEV提示对齐视觉场景，显著缩小游戏到真实的分布差距。


<details>
  <summary>Details</summary>
Motivation: 现有雷达仿真难以兼顾物理真实性、可扩展性与与视觉数据的对齐；需要一种可利用丰富视觉数据、能生成与真实分布一致的雷达测量的通用生成式模拟方法，以支持多模态感知训练与评估。

Method: 1) 将雷达测量编码为BEV栅格，联合表示空间结构、RCS与多普勒；2) 采用高效图像潜空间扩散进行生成；3) 从预训练基础模型提取BEV对齐的深度、语义、运动先验作为条件，引导随机生成过程以满足物理合理性；4) 通过轻量的后处理将生成的BEV图映射回雷达点云。

Result: 在大规模自动驾驶数据上，生成的雷达分布与真实数据更匹配；在下游感知模型训练时，相比传统模拟更接近用真实雷达数据训练的性能。

Conclusion: RadarGen证明了以相机为条件、BEV表达与多模态先验引导的扩散生成能有效合成物理合理的雷达点云，推动跨模态统一生成式仿真的可行性与可扩展性。

Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.

</details>


### [105] [Diffusion Forcing for Multi-Agent Interaction Sequence Modeling](https://arxiv.org/abs/2512.17900)
*Vongani H. Maluleke,Kie Horiuchi,Lea Wilken,Evonne Ng,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: MAGNet 提出一个统一的自回归扩散-Transformer 框架，可在单一模型内完成双人预测、搭档补全与多人的整段动作生成，并能生成数百帧的超长序列，同时显式建模多智能体之间的耦合，以实现协调一致的群体动作。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成方法多为任务特定，难以泛化到人数可变、时间跨度长、强交互依赖的多主体场景；需要一个能统一支持多种交互任务、并在长时序中保持多体协调性的模型。

Method: 基于“Diffusion Forcing”的自回归扩散框架，结合Transformer 作为序列建模骨干：在自回归去噪过程中引入显式的跨主体耦合建模与灵活条件控制（用于双人预测、伙伴补全、全多体生成等），架构对主体数量无关并可扩展到三人及以上；通过条件采样与自回归滚动生成实现超长序列。

Result: 在双人基准上达到与专用方法相当的性能，并自然扩展到三人及以上的多主体交互；能够生成从高度同步（如舞蹈、拳击）到松散社交互动的连贯协调动作，时空一致性良好；可生成数百帧的长序列。

Conclusion: MAGNet 作为统一的多主体动作生成框架，实现了灵活条件控制与显式跨主体耦合，在不牺牲双人任务性能的前提下，具备可扩展到多人的通用性与长时序稳定性；适用于机器人与社交计算中的多体交互合成与补全场景。

Abstract: Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/

</details>


### [106] [Adversarial Robustness of Vision in Open Foundation Models](https://arxiv.org/abs/2512.17902)
*Jonathon Fox,William J Buchanan,Pavlos Papadopoulos*

Main category: cs.CV

TL;DR: 论文评估两种开源视觉-语言模型（LLaVA-1.5-13B 与 Llama 3.2 Vision-8B-2）在视觉通道遭受PGD对抗扰动下的鲁棒性，并在VQA v2子集上以VQA标准准确率量化性能下降；发现Llama 3.2 Vision尽管基线准确率更低，但在高扰动强度下精度下降更小，提示鲁棒性与常规基准性能并不正相关。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型可解释性不足使其易受对抗样本攻击，尤其在视觉输入端加入细微扰动即可误导识别。随着开源VLM广泛应用，评估其在现实攻击面（视觉模态）下的鲁棒性至关重要，以指导模型选择与加固。

Method: 对LLaVA-1.5-13B与Llama 3.2 Vision-8B-2在VQA v2子集上实施无目标PGD攻击，仅作用于视觉输入；在不同扰动强度下生成对抗样本，并用VQA标准准确率评估；比较各模型基线精度与在攻击下的精度下降幅度。

Result: 两模型在PGD攻击下准确率均显著下降；Llama 3.2 Vision的基线准确率较低，但随扰动增强其性能下降幅度小于LLaVA，尤其在高epsilon条件下更稳健。

Conclusion: 视觉模态是攻击开源VLM的有效入口；对抗鲁棒性与常规基线精度并非正相关，可能由架构与训练差异驱动。对VLM的部署应考虑专门的鲁棒性评测与防护策略。

Abstract: With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.

</details>


### [107] [Dexterous World Models](https://arxiv.org/abs/2512.17907)
*Byungjun Kim,Taeksoo Kim,Junyoung Lee,Hanbyul Joo*

Main category: cs.CV

TL;DR: 提出Dexterous World Model (DWM)，一种场景+动作条件的视频扩散模型，可从静态3D场景渲染和自我视角手部动作生成连贯的人-物交互视频，实现交互式数字孪生的初步能力。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生大多静态，只支持导航/视图合成，缺乏具身交互。需要能将人类灵巧操作引发的场景动态引入静态3D环境，实现物体操控、开合、移动等交互，并保持相机/场景一致性。

Method: 提出DWM：视频扩散生成框架，条件包括(1)沿指定相机轨迹的静态场景渲染以保证空间一致性；(2)自我视角手部网格渲染，编码几何与运动，用于建模动作驱动的动态。训练使用混合交互视频数据集：合成的自我视角交互提供对齐监督用于联合移动与操控学习；真实固定机位视频提供多样且逼真的物体动力学。

Result: 实验显示DWM能生成抓取、开合、移动物体等逼真且物理可行的交互视频，同时保持相机与场景一致性，时间上连贯。

Conclusion: DWM迈出以视频扩散实现交互式数字孪生的第一步，使能从自我视角动作进行具身仿真，为将静态3D场景扩展为可交互环境提供路径。

Abstract: Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.
  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.
  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.

</details>


### [108] [Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting](https://arxiv.org/abs/2512.17908)
*Ananta R. Bhattarai,Helge Rhodin*

Main category: cs.CV

TL;DR: 提出Re-Depth Anything：在测试时将DA-V2与2D扩散模型先验融合，通过重光照与输入增强实现无标注深度细化，采用SDS基于形状-明暗线索进行自监督优化，并以“冻结编码器+更新中间嵌入+微调解码器”的定向策略避免坍塌，显著提升跨域深度精度与真实感。


<details>
  <summary>Details</summary>
Motivation: DA-V2等单目深度基础模型在分布外真实场景中性能下降，传统基于光度一致性的自监督在复杂外观与照明变化下不稳。需要一种能在测试时自适应、利用大模型先验并减少标注依赖的方法来弥合域间差距。

Method: 将DA-V2的预测深度通过“重光照”再合成：利用大规模2D扩散模型的生成先验与SDS对输入图像进行再渲染/增强，使深度受形状-明暗（SfS）约束；不直接优化深度或整体微调，而是冻结编码器，优化中间特征嵌入并微调解码器，实现稳定的无标注测试时细化。

Result: 在多个基准上，相较DA-V2，Re-Depth Anything在深度准确性和视觉真实感上取得显著提升，特别是在分布外真实图像上表现更优。

Conclusion: 通过在测试时用扩散先验与SDS进行SfS式重合成约束，并采用定向优化策略，可有效提升单目深度在跨域场景的鲁棒性与质量，展示了将生成式先验用于几何推理自监督的新方向。

Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.

</details>


### [109] [Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing](https://arxiv.org/abs/2512.17909)
*Shilong Zhang,He Zhang,Zhifei Zhang,Chongjian Ge,Shuchen Xue,Shaoteng Liu,Mengwei Ren,Soo Ye Kim,Yuqian Zhou,Qing Liu,Daniil Pakhomov,Kai Zhang,Zhe Lin,Ping Luo*

Main category: cs.CV

TL;DR: 论文提出将理解型表征编码器的高维特征改造成可生成的紧凑潜空间，通过语义-像素联合重建目标，得到既语义丰富又细节精确的96×16×16潜表征，并据此构建统一的文生图与编辑扩散模型，取得重建与生成的SOTA与更快收敛。


<details>
  <summary>Details</summary>
Motivation: 现有LDM多在为像素重建优化的VAE低级潜空间中工作，难以统一理解与生成。直接用判别式表征特征作潜变量又面临两难：其一，判别特征缺少紧致正则，扩散易采样到流形外，造成结构失真；其二，编码器像素级重建能力弱，导致细粒度几何与纹理学习不足。

Method: 提出系统框架将理解型编码器的特征适配为生成潜空间：核心是语义-像素重建目标，既压缩高层语义又保留细节，得到高度紧凑的潜表示（通道96，空间下采样16×）。在此潜空间上训练统一的T2I与图像编辑扩散模型。并对比多种特征空间，验证正则化与重建设计的有效性。

Result: 所获潜空间在图像重建上达SOTA；在T2I与编辑任务中显著提升质量与结构准确性，并带来更快的训练收敛；相较其他高维表征潜空间表现更稳健，减少“离流形”采样导致的物体结构错误。

Conclusion: 理解型表征编码器可通过语义-像素联合重建正则被有效转化为稳健的生成组件，形成紧凑且语义丰富的潜空间，实现统一的生成与编辑，并在性能与效率上优于现有方案。

Abstract: Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.

</details>
