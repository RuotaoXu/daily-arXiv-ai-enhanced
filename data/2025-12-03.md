<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 126]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale](https://arxiv.org/abs/2512.02055)
*Mirela G. Tulbure,Julio Caineta,Mark Broich,Mollie D. Gaines,Philippe Rufin,Leon-Friedrich Thomas,Hamed Alemohammad,Jan Hemmerling,Patrick Hostert*

Main category: cs.CV

TL;DR: 作者用FloodsNet数据集微调TerraMind地理空间基础模型进行全球洪水淹没范围分割，比较不同规模与冻结策略，并与U-Net和现有基线对比：多模态（SAR+光学）+微调可提升近实时洪水制图；base-不冻结性价比最佳，large-不冻结召回最高；U-Net召回更高但精度略低。


<details>
  <summary>Details</summary>
Motivation: 洪水频发且危害巨大，2024年极端事件全球多地发生。EO卫星能快速覆盖，但依赖标注与模型泛化能力。新近GFMs（如TerraMind）通过自监督预训练具备更强泛化，但其在全球多样洪水上的表现缺乏系统评估。

Method: 使用涵盖全球85起洪水事件、配准的Sentinel-1 SAR与Sentinel-2光学多模态数据集FloodsNet，对TerraMind进行微调；测试四种配置（base/large × 骨干冻结/不冻结），并与TerraMind基线（基于Sen1Floods11示例）以及在FloodsNet与Sen1Floods11上训练的U-Net比较，评估准确率、精确率、召回率与计算成本。

Result: base-不冻结在精度、准确率、召回之间达到最佳平衡且算力成本显著低于large；large-不冻结召回最高；基于FloodsNet训练的模型在召回上优于Sen1Floods11示例且总体准确率相近；U-Net的召回高于所有GFM配置，但准确率与精确率略低。

Conclusion: 多模态（SAR+光学）与对GFM的微调能增强近实时洪水制图能力；本研究提供了首批对GFM进行全球尺度洪水分割评估的证据，展示其潜力与当前局限，为气候适应与灾害韧性应用奠定基础。

Abstract: Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.
  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.
  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.

</details>


### [2] [Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework](https://arxiv.org/abs/2512.02152)
*Haojin Deng,Yimin Yang*

Main category: cs.CV

TL;DR: 提出一种“上下文增强”的对比损失，兼顾类间可分性与同源增广一致性，在8个大规模数据集上优于16种SOTA，并在BiasedMNIST上带来22.9%提升。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习依赖数据增强（旋转、裁剪）构造正负样本，但会引入“信息扭曲”：模型过度依赖同标签样本而忽视来自同一原图的增广正对，尤其在大规模数据集下，导致学习偏差与收敛低效。

Method: 设计一个含两条收敛目标的上下文增强对比损失：1) 标签对比敏感项，强化同类聚合、异类分离，加速区分类间边界；2) 同源一致性项，将来自同一原图的所有增广样本聚拢，并与其他样本拉远，减轻数据增强引入的语义扭曲。方法可无缝替换原有对比损失，用于标准图像分类预训练/微调流程。

Result: 在CIFAR10/100、Caltech-101/256、ImageNet、BiasedMNIST、UTKFace、CelebA上，较16种SOTA取得更好泛化与更快收敛；在系统性偏置任务BiasedMNIST上，相比原始对比损失的下游表现提升22.9%。

Conclusion: 同时建模“类判别性”和“同源一致性”的对比损失能缓解增强导致的信息扭曲并提高效率与公平性，对含系统性偏置的数据尤其有效。

Abstract: Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.

</details>


### [3] [FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges](https://arxiv.org/abs/2512.02161)
*Kevin David Hayes,Micah Goldblum,Vikash Sehwag,Gowthami Somepalli,Ashwinee Panda,Tom Goldstein*

Main category: cs.CV

TL;DR: 提出一个分层评测框架，利用VLM识别T2I在困难提示下的27类失效模式，并构建含5个T2I与3个VLM的跨模型数据集，显示现有指标难以捕捉细粒度错误。


<details>
  <summary>Details</summary>
Motivation: T2I常在颜色、数量、属性绑定等细节上偏离提示；同时VLM基准未覆盖真实复杂场景，缺乏系统化方式比较不同T2I与VLM对提示遵循与错误识别的能力。

Method: 1) 设计层次化失效模式本体（27类）覆盖属性忠实度与目标表征错误；2) 以具有挑战性的提示生成图像，来自5个T2I模型（Flux、SD3-Medium/Large、SD3.5-Medium/Large）；3) 用3个VLM（Molmo, InternVL3, Pixtral）对图像进行失效模式判别；4) 通过LLM（Llama3）对VLM输出进行标准化与标注以判断是否正确识别失效；5) 统计各模型在不同失效类型上的表现，形成联合评测基准。

Result: 在精心挑选的提示集上揭示出系统性错误：属性绑定与对象表示方面的普遍失配（如数量、颜色、位置、关系），且不同T2I在某些模式上各有弱点；同时VLM在复杂场景下对失效识别并不稳定。

Conclusion: 现有通用指标无法反映细粒度的失效差异，需要针对性、结构化的基准来提升生成模型的可靠性与可解释性。本工作提供了方法与数据集，为联合评测T2I与VLM提供参考。

Abstract: Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.

</details>


### [4] [Mapping of Lesion Images to Somatic Mutations](https://arxiv.org/abs/2512.02162)
*Rahul Mehta*

Main category: cs.CV

TL;DR: 提出LLOST：用双VAE与共享潜空间，将病灶点云影像与体细胞突变计数对齐，从影像预测突变谱；在TCIA影像+TCGA突变上，能预测特定突变计数与发生情况，揭示跨模态共享癌种相关模式。


<details>
  <summary>Details</summary>
Motivation: 临床上影像先行、基因随后，但早诊早治关键，若能仅凭影像推断患者体细胞突变谱，可加速分型与用药决策、缓解测序成本与时延，并捕捉影像-基因的共享肿瘤学模式。

Method: 将病灶影像表示为点云以实现对成像模态的不变性；提出LLOST：两个VAE分别编码影像点云与突变计数，通过一个独立的共享潜空间耦合三者。每个潜空间均配备条件正规化流作为先验，以适配各域复杂分布。以TCIA影像配对TCGA Pan-Cancer突变数据进行训练与评估。

Result: 定性与定量实验显示，可预测特定基因体细胞突变的计数及其发生概率，并在共享潜空间中学到反映癌种的跨模态模式。

Conclusion: LLOST能从影像推断体细胞突变信息并揭示影像-基因共享结构；未来可改进模型并扩展到其他遗传层面数据。

Abstract: Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains.

</details>


### [5] [SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting](https://arxiv.org/abs/2512.02172)
*Pranav Asthana,Alex Hanson,Allen Tu,Tom Goldstein,Matthias Zwicker,Amitabh Varshney*

Main category: cs.CV

TL;DR: 提出SplatSuRe：基于3DGS渲染中相机位姿与场景几何关系，选择性地对欠采样区域添加超分辨细节，从而在保持多视一致性的同时提升清晰度与感知质量。


<details>
  <summary>Details</summary>
Motivation: 直接对每个低分辨率视图做超分会造成多视不一致，导致新视角渲染模糊；现有方法要么依赖复杂神经组件/视频先验/联合优化，且对所有图像一刀切地应用SR，忽略了不同视角间可共享的高频信息。

Method: 利用相机与场景几何（由3DGS提供）推断各视图区域是否已有来自近景视图的高频监督；仅在远景等欠采样、缺乏高频监督的区域选择性注入SR内容（而非全局SR），以减少跨视不一致。实现上在3DGS框架中基于位姿与可见性/采样密度判定区域，并将SR输出限制到这些区域。

Result: 在Tanks & Temples、Deep Blending、Mip-NeRF 360等数据集上，相比基线在保真度与感知指标均有提升，尤其在局部前景高细节区域提升最明显，渲染更锐利且一致。

Conclusion: 选择性区域超分结合3D几何与位姿引导，可在无需对所有图像统一超分的情况下，提升新视角渲染的清晰度与多视一致性，优于现有统一SR策略。

Abstract: 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.

</details>


### [6] [RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation](https://arxiv.org/abs/2512.02188)
*Mansoor Ali,Maksim Richards,Gilberto Ochoa-Ruiz,Sharib Ali*

Main category: cs.CV

TL;DR: 提出RobustSurg，通过风格/内容解耦与特征协方差映射+实例归一化，并在ResNet中加入“补偿/复原”模块，增强手术场景分割在跨中心与跨模态的泛化；还发布新多类别多中心数据集；在HeiCholSeg与EndoUDA目标集上对mIoU显著优于DeepLabv3+与多种SOTA。


<details>
  <summary>Details</summary>
Motivation: 手术场景语义分割在单中心/单模态上有效，但对未见分布（跨中心）与跨模态迁移泛化差；现有自然场景的OOD/域泛化方法不适用于手术视频，因视觉线索少、场景多变（血液、伪影等）。需要一种能降低外观变化影响且不丢失任务相关特征的鲁棒方法与相应数据资源。

Method: 1) 利用实例归一化与特征协方差映射，分离并对齐风格与内容，获得更稳健的表征；2) 在ResNet骨干中插入“复原/补偿（restitution）”模块，防止归一化等操作抹除与目标相关的判别特征；3) 构建并公开多类别、多中心的手术场景分割数据集，用于评估泛化能力；4) 以DeepLabv3+为基线，跨数据集/模态验证。

Result: 在未见中心HeiCholSeg上（训练集为CholecSeg8K），mIoU较DeepLabv3+提升约23%，较多种SOTA提升10–32%；在EndoUDA息肉目标集上，较基线提升约22%，较近期SOTA再提升约11%。

Conclusion: 通过风格-内容建模、特征统计对齐与复原模块，可在不牺牲判别性的前提下显著提升手术场景分割的跨中心与跨模态泛化；所提数据集为该方向提供了基准与推动。

Abstract: While recent advances in deep learning for surgical scene segmentation have demonstrated promising results on single-centre and single-imaging modality data, these methods usually do not generalise to unseen distribution (i.e., from other centres) and unseen modalities. Current literature for tackling generalisation on out-of-distribution data and domain gaps due to modality changes has been widely researched but mostly for natural scene data. However, these methods cannot be directly applied to the surgical scenes due to limited visual cues and often extremely diverse scenarios compared to the natural scene data. Inspired by these works in natural scenes to push generalisability on OOD data, we hypothesise that exploiting the style and content information in the surgical scenes could minimise the appearances, making it less variable to sudden changes such as blood or imaging artefacts. This can be achieved by performing instance normalisation and feature covariance mapping techniques for robust and generalisable feature representations. Further, to eliminate the risk of removing salient feature representation associated with the objects of interest, we introduce a restitution module within the feature learning ResNet backbone that can enable the retention of useful task-relevant features. To tackle the lack of multiclass and multicentre data for surgical scene segmentation, we also provide a newly curated dataset that can be vital for addressing generalisability in this domain. Our proposed RobustSurg obtained nearly 23% improvement on the baseline DeepLabv3+ and from 10-32% improvement on the SOTA in terms of mean IoU score on an unseen centre HeiCholSeg dataset when trained on CholecSeg8K. Similarly, RobustSurg also obtained nearly 22% improvement over the baseline and nearly 11% improvement on a recent SOTA method for the target set of the EndoUDA polyp dataset.

</details>


### [7] [Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation](https://arxiv.org/abs/2512.02198)
*Miguel L. Martins,Miguel T. Coimbra,Francesco Renna*

Main category: cs.CV

TL;DR: 提出将多重分形统计作为通道注意力的先验，嵌入U-Net以改进医学图像分割，较其他高阶统计注意力显著提升性能，并揭示跳连抑制了随深度而增强的专化趋势。


<details>
  <summary>Details</summary>
Motivation: 传统端到端多重分形方法依赖强下采样/池化，难以适配像语义分割这类需高分辨率特征的任务；同时，多重分形在刻画自组织/病灶不规则性方面已被验证有效，但尚未在现代深度网络中充分利用。

Method: 提出两种归纳先验：单分形与多重分形再校准（Recalibration）。基于特征编码的指数概率质量与多重分形谱的关系，构建统计摘要，并以通道注意力函数实现，集成于卷积网络（U-Net框架）。与含其他高阶统计注意力的基线对比评估；在ISIC18、Kvasir-SEG、BUSI上验证。

Result: 在U-Net上，多重分形再校准较含其他高阶统计注意力的基线取得显著性能提升（用于医学图像分割三数据集）。实证分析显示：由于U-Net跳连，注意力激活随编码器深度并未更专化；其有效性可能与实例全局统计变异性相关。

Conclusion: 多重分形先验可作为有效的通道注意力机制嵌入分割网络，在医学影像中带来稳定增益；网络结构（如跳连）影响注意力层的专化行为，提示在设计中应考虑全局变异统计与架构交互。

Abstract: Multifractal analysis has revealed regularities in many self-seeding phenomena, yet its use in modern deep learning remains limited. Existing end-to-end multifractal methods rely on heavy pooling or strong feature-space decimation, which constrain tasks such as semantic segmentation. Motivated by these limitations, we introduce two inductive priors: Monofractal and Multifractal Recalibration. These methods leverage relationships between the probability mass of the exponents and the multifractal spectrum to form statistical descriptions of encoder embeddings, implemented as channel-attention functions in convolutional networks.
  Using a U-Net-based framework, we show that multifractal recalibration yields substantial gains over a baseline equipped with other channel-attention mechanisms that also use higher-order statistics. Given the proven ability of multifractal analysis to capture pathological regularities, we validate our approach on three public medical-imaging datasets: ISIC18 (dermoscopy), Kvasir-SEG (endoscopy), and BUSI (ultrasound).
  Our empirical analysis also provides insights into the behavior of these attention layers. We find that excitation responses do not become increasingly specialized with encoder depth in U-Net architectures due to skip connections, and that their effectiveness may relate to global statistics of instance variability.

</details>


### [8] [Towards Unified Video Quality Assessment](https://arxiv.org/abs/2512.02224)
*Chen Feng,Tianhao Peng,Fan Zhang,David Bull*

Main category: cs.CV

TL;DR: 提出Unified-VQA：把通用视频质量评估重构为诊断型专家混合（MoE）问题，利用多专家和多任务头，在无需微调情况下跨多格式、多数据库优于18+基线，并输出全局质量与可解释伪影向量。


<details>
  <summary>Details</summary>
Motivation: 现有VQA多为单一分数、缺乏可解释诊断，且常对特定格式/失真专用，难以泛化到多种感知域与多视频格式。需要一个统一、可解释、可泛化的VQA框架。

Method: 将通用VQA建模为诊断型MoE：为不同感知域设置“感知专家”，通过多代理（proxy）指标引导、基于排序的损失来训练各专家；加入诊断型多任务头，同时预测全局质量分数与多维伪影向量。伪影向量用弱监督训练，利用自建大规模训练库的已知属性。模型推理时参数固定，无需再训练或微调。

Result: 在HD、UHD、HDR、HFR等多格式、含多种流媒体伪影的17个数据库上，对通用VQA与诊断伪影检测两类任务，相比18+基准方法取得一致且更优的性能。

Conclusion: Unified-VQA实现了统一、可解释且具诊断能力的VQA，在多格式、多失真场景中强泛化、免微调，向实用且可行动的视频质量评估迈出重要一步。

Abstract: Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.

</details>


### [9] [See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2512.02231)
*Le Thien Phuc Nguyen,Zhuoran Yu,Samuel Low Yu Hang,Subin An,Jeongik Lee,Yohan Ban,SeungEun Chung,Thanh-Huy Nguyen,JuWan Maeng,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: AV-SpeakerBench 提出一个专注“谁在说、说了什么、何时说”的多模态视听基准，用专家标注与跨模态对齐来严测细粒度语音-视觉推理；评测显示 Gemini 系列领先，开源模型主要败在视听融合弱。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准多可被纯视觉线索解决或仅粗粒度评估语音，难以检验模型是否能准确对齐说话人、语内容与时间点。需要一个以说话者为中心、真正依赖视听融合且时间精确的评测框架。

Method: 构建 AV-SpeakerBench：包含3,212道多选题，来自真实世界视频。设计三项关键：1) 以“说话者”为核心的任务表述；2) 在题目语义中显式嵌入视听依赖，要求跨模态融合；3) 专家策划与标注，确保时间精度与跨模态有效性。对多款闭源与开源 MLLM 进行系统评测。

Result: Gemini 家族整体领先，其中 Gemini 2.5 Pro 表现最佳；开源中 Qwen3-Omni-30B 接近 Gemini 2.0 Flash，但与 Gemini 2.5 Pro差距明显。误差分析表明主要短板在视听融合能力，而非纯视觉感知。

Conclusion: AV-SpeakerBench 为细粒度说话者中心的视听推理提供严格评测基准，可驱动未来多模态系统在“谁说-说啥-何时”对齐与融合上的实质性进展。

Abstract: Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.

</details>


### [10] [Exploring the Potentials of Spiking Neural Networks for Image Deraining](https://arxiv.org/abs/2512.02258)
*Shuang Chen,Tomas Krajnik,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 提出一种适用于图像去雨的生物可解释、节能脉冲视觉框架：以新型Visual LIF（VLIF）神经元为核心，配合脉冲分解增强模块与轻量多尺度单元，实现层级多尺度表示学习；在5个基准上显著优于现有SNN去雨方法，仅耗其约13%的能耗。


<details>
  <summary>Details</summary>
Motivation: 传统SNN在低层视觉任务（如去雨）探索不足，主要因其神经元在频域表现饱和、空间上下文理解缺失，难以有效刻画雨纹等高频退化与背景结构关系。需要一种既保留SNN高通特性与能效优势，又具备空间上下文建模能力的神经元与模块设计。

Method: 提出Visual LIF（VLIF）神经元，增强传统LIF的空间上下文与频域表达；基于VLIF设计：1）脉冲分解与增强模块（Spiking Decomposition and Enhancement）以缓解传统SNN的频域饱和并突出有效高频；2）轻量级脉冲多尺度单元（Spiking Multi-scale Unit）进行层级多尺度表示学习；整体构成面向去雨的SNN框架。

Result: 在5个去雨基准数据集上，方法显著超过现有SNN去雨SOTA，同时能耗仅为其约13%，体现出性能与能效兼优。

Conclusion: VLIF神经元与配套模块有效克服传统SNN在频域饱和与空间上下文缺失的问题，使SNN在低层视觉（以去雨为例）中实现高性能与高能效，为SNN在更多低层视觉任务部署奠定基础。

Abstract: Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.

</details>


### [11] [Spatiotemporal Pyramid Flow Matching for Climate Emulation](https://arxiv.org/abs/2512.02268)
*Jeremy Andrew Irvin,Jiaqi Han,Zikui Wang,Abdulaziz Alharbi,Yufei Zhao,Nomin-Erdene Bayarsaikhan,Daniele Visioni,Andrew Y. Ng,Duncan Watson-Parris*

Main category: cs.CV

TL;DR: 提出Spatiotemporal Pyramid Flows (SPF)，一种分层时空流匹配生成模型，可在多时间尺度高效并行地模拟气候；在ClimateBench上优于强基线，并配套发布大规模数据集ClimateSuite以支持可扩展训练与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有生成式气候模拟多依赖逐步自回归（天气尺度）方法，长时段推演效率低、对非平稳外强迫（如温室气体变化）下长期稳定性欠佳；需要一种既能高效长时模仿、又能处理不同时间尺度并可条件化物理强迫的生成框架。

Method: 提出SPF：将生成轨迹划分为时空金字塔，逐级提升空间分辨率并为每级绑定相应时间尺度；各级采用flow matching进行训练，并对物理外强迫（温室气体、气溶胶等）进行条件化，使得任一时间层级可直接采样；设计受级联视频模型启发，实现跨层并行、快速采样。

Result: 在ClimateBench上，SPF在年尺度和月尺度均优于强flow matching基线与预训练模型；在粗时间层级采样显著更快。作者构建ClimateSuite（>3.3万模拟年、10个气候模型，含气候干预情景），用以扩展训练后，SPF对跨模型、留出情景表现出良好泛化。

Conclusion: SPF与ClimateSuite共同提供了一个可准确、高效、具不确定性量化能力的多时间尺度气候仿真基础；方法能在现实未来情景下并行生成并稳定外推，开源数据与代码促进后续研究与应用。

Abstract: Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .

</details>


### [12] [Progressive Image Restoration via Text-Conditioned Video Generation](https://arxiv.org/abs/2512.02273)
*Peng Kang,Xijun Wang,Yu Yuan*

Main category: cs.CV

TL;DR: 将 CogVideo 由“生成自然运动视频”改造为“生成逐步复原轨迹”，在超分、去模糊、弱光增强等任务上，通过合成的从劣化到干净的序列进行微调，使时间推进对应画质提升，取得更好的 PSNR/SSIM/LPIPS，并在 ReLoBlur 上零样本泛化良好。


<details>
  <summary>Details</summary>
Motivation: 文本到视频模型具备强时序建模与生成能力，但其在图像/视频复原中的潜能未被充分挖掘。若能把“时间演进”重新解释为“复原程度递进”，则可利用大模型的时序一致性优势来提升复原质量与稳定性，并增强对真实世界劣化的鲁棒性和可解释性。

Method: 以 CogVideo 为基础，构造超分、去模糊、弱光增强的合成数据集，每个样本由从劣化到干净的逐帧过渡序列组成。对比两种文本提示：通用统一提示与基于 LLaVA 生成、经 ChatGPT 精修的场景特定提示。对 CogVideo 进行微调，使其学习“时间=复原进度”的映射，生成随时间逐步提升质量的序列。评估时用 PSNR、SSIM、LPIPS 及时序一致性指标。

Result: 微调后的模型能在序列中逐帧提升空间细节与光照一致性，同时保持时序连贯，显著改善 PSNR/SSIM/LPIPS。即便在真实数据集 ReLoBlur 上也无需额外训练即可取得良好表现，显示出强零样本泛化能力；场景特定提示相较统一提示进一步提升表现与稳定性。

Conclusion: 将文本到视频模型的时间维度重解释为复原轨迹是有效范式。CogVideo 可通过少量改动胜任渐进式复原，兼顾空间质量与时间一致性，并具有良好的零样本泛化与可解释性。未来可扩展至更多复原任务与更细粒度的提示/控制。

Abstract: Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.

</details>


### [13] [Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation](https://arxiv.org/abs/2512.02290)
*Andre Juarez,Luis Salsavilca,Frida Coaquira,Celso Gonzales*

Main category: cs.CV

TL;DR: 提出 MORP–Synth：用于SAR油污分割跨区域迁移的两阶段合成数据增强框架，从地中海迁移到秘鲁海域，利用形态学曲率引导的标签扰动与条件生成模型渲染SAR纹理，显著提升少数类与总体mIoU（最高+6 mIoU，油类+10.8 IoU，类油污+14.6）。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的SAR油污分割模型在跨海域迁移时泛化性差，原因在于海况、后向散射统计、油膜形态差异显著，且秘鲁沿岸缺乏带标注的Sentinel-1数据，导致直接迁移性能大幅下降。需要一种无需大量目标域标注、能生成域特定外观与形状变化的增强方法以缩小域差距。

Method: 提出 MORP–Synth 两阶段合成增强：A) 形态学区域扰动（MORP），在标签空间基于曲率引导对“油污/类油污”区域进行几何变形与拓扑微调，产生真实感的形状多样性；B) 条件生成 INADE 模型，根据编辑后的掩膜渲染具有SAR外观统计的纹理，实现与秘鲁海况匹配的强逼真度数据合成。构建并与CleanSeaNet统一规范的秘鲁数据集（2112个512×512补丁，40景，2014–2024），在七种分割架构上评测从地中海到秘鲁的迁移。

Result: 在不增强的跨域迁移中，mIoU从67.8%降至51.8%；使用 MORP–Synth 后，最高提升约+6 mIoU，并显著改善少数类：油污IoU提升+10.8，类油污提升+14.6。

Conclusion: 通过在标签空间进行形状扰动并用条件生成渲染SAR纹理，MORP–Synth有效缓解跨海域域偏移，尤其提升少数类识别，对缺乏标注的秘鲁场景实现更稳健的油污分割迁移。

Abstract: Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\% to 51.8\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).

</details>


### [14] [Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision](https://arxiv.org/abs/2512.02339)
*Chenshuang Zhang,Kang Zhang,Joon Son Chung,In So Kweon,Junmo Kim,Chengzhi Mao*

Main category: cs.CV

TL;DR: 利用预训练视频扩散模型的去噪早期阶段所蕴含的“运动先分离”特性，无需标注即可获得有效运动表征，从而显著提升自监督跟踪对外观极其相似目标的区分与追踪能力，较现有自监督方法最高提升约6点。


<details>
  <summary>Details</summary>
Motivation: 自监督跟踪在视觉线索模糊、外观高度相似的目标场景中易混淆、掉跟，需要无需大量标注却能可靠区分相似目标的运动表征；现有方法在可扩展性与泛化上受限。

Method: 观察并利用视频扩散模型的去噪过程：高噪声早期阶段主要学习并隔离运动信息，后期才细化外观。基于此，提出一种自监督跟踪器，从预训练视频扩散模型中提取早期阶段的运动表征用于数据关联与目标区分，无需任务特定训练；并构建针对相似物体的新测试集。

Result: 在既有基准与新引入的“相似物体追踪”评测上，相比最新自监督方法最高提升约6个点；可视化显示模型能在视角变化与形变下稳定区分并跟踪外观相同的多个目标。

Conclusion: 视频扩散模型天然学习到对跟踪有用的运动表征，特别是在去噪早期阶段；将其用于自监督跟踪可在相似外观目标场景显著提升鲁棒性与精度，减少对标注的依赖，并为未来基于扩散模型的无监督/弱监督多目标跟踪提供了新方向。

Abstract: Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.

</details>


### [15] [TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction](https://arxiv.org/abs/2512.02341)
*Fengyi Zhang,Tianjun Zhang,Kasra Khosoussi,Zheng Zhang,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: 提出TALO：一种基于薄板样条（TPS）的高自由度、长时窗对齐框架，用于在在线场景中让3D视觉大模型的连续预测时序一致、更稳健。通过全局传播控制点与点无关的子图配准，显著提升几何一致性与轨迹精度，适配多种模型与相机设置。


<details>
  <summary>Details</summary>
Motivation: 3D基础模型在单帧重建上具备强泛化，但在线应用（如自动驾驶）需要在时间序列上保持一致性。现有方法多通过求解全局刚体/相似变换对齐相邻帧，存在：假设过强（刚体/低自由度不适配空间变化误差）、仅局部对齐（短时窗，误差难以长期抑制）、对噪声几何不鲁棒等问题。

Method: 构建基于薄板样条（TPS）的高自由度、长时窗时空对齐：1) 设计全局传播的控制点，将时序内的空间不一致通过非刚性形变统一校正；2) 提出点无关（point-agnostic）的子图（submap）配准，避免依赖逐点对应，天然应对噪声几何；3) 完全即插即用，兼容不同3D基础模型与相机（单目/环视）。

Result: 在多数据集、不同骨干模型、不同相机设置上，持续获得更一致的几何与更低的轨迹误差，相比近期对齐策略表现更稳健、泛化更强。

Conclusion: 利用TPS实现高自由度、长时窗的非刚性对齐，并结合点无关子图配准，可显著改善在线3D重建的时序一致性与鲁棒性；方法通用、可插拔，适用于多类3D基础模型与多种相机配置。

Abstract: 3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.

</details>


### [16] [A multi-weight self-matching visual explanation for cnns on sar images](https://arxiv.org/abs/2512.02344)
*Siyuan Sun,Yongping Zhang,Hongcheng Zeng,Yamin Wang,Wei Yang,Wanting Yang,Jie Chen*

Main category: cs.CV

TL;DR: 提出MS-CAM，可同时利用通道权重与元素级权重，对SAR任务中的CNN决策进行更精细可视化解释，并在自建SAR目标分类数据集上验证其在兴趣区域定位与细粒度特征呈现上的优势，且可用于弱监督目标定位并分析阈值等关键因素。


<details>
  <summary>Details</summary>
Motivation: CNN在SAR任务上表现突出，但黑箱性与可解释性不足限制高可靠应用与部署；需要一种能准确揭示CNN在SAR图像中决策依据的可视化解释方法，提升可信度并支持下游定位任务。

Method: 提出多权重自匹配类激活映射（MS-CAM）：将SAR原图与CNN提取的特征图及其梯度进行自匹配，联合通道级权重与像素/元素级权重生成显著图，直观展示模型关注的区域与细节特征；并探讨弱监督定位流程及像素阈值等参数设定。

Result: 在自建的SAR目标分类数据集上，MS-CAM相比现有方法更准确地突出网络兴趣区域，捕获更细粒度目标特征，提升了可解释性；在弱监督目标定位上可行，并通过实验系统评估影响定位精度的关键因素（如像素阈值）。

Conclusion: MS-CAM能更好可视化CNN在SAR中的决策依据，提升解释质量并支持弱监督定位；阈值等参数对定位性能影响显著，为今后优化提供方向。

Abstract: In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.

</details>


### [17] [Understanding and Harnessing Sparsity in Unified Multimodal Models](https://arxiv.org/abs/2512.02351)
*Shwai He,Chaorui Deng,Ang Li,Shen Yan*

Main category: cs.CV

TL;DR: 论文系统分析统一多模态模型（理解+生成）在推理中的低效来源，发现理解模块可大幅压缩而生成模块对压缩极为敏感；提出将生成模块做MoE稀疏激活的适配方案，几乎不损性能但只激活约一半参数。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型虽兼具理解与生成，但不同任务/样本不一定需要全模型容量，导致推理低效。现有工作缺乏对模型各组件在压缩下敏感性的系统性认识，难以有针对性地提升效率。

Method: 以“训练无关”的剪枝作为探针：对统一模型的理解与生成组件做深度剪枝（层数）与宽度缩减（隐藏/通道等），评估在理解与生成任务上的鲁棒性；据发现的动态激活模式，提出Mixture-of-Experts (MoE) Adaptation：将生成模块划分为多个专家，采用稀疏激活；验证两种设置——冻结专家的微调（expert-frozen tuning）与全可训练的适配。

Result: 观察：理解组件在理解与生成任务上均具显著可压缩性，且在生成任务上更明显；生成组件对压缩高度敏感，中等压缩即显著退化。方法：MoE稀疏激活可恢复/保持生成质量；全可训练适配比冻结专家进一步提升。BAGEL适配后，仅激活约一半参数即可达到与完整模型相当的性能。

Conclusion: 统一多模态模型的推理低效主要来自生成组件的不可压缩性；通过对生成模块进行MoE化并稀疏激活，可在保持性能的同时显著降低有效激活参数与推理成本。

Abstract: Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.

</details>


### [18] [WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting](https://arxiv.org/abs/2512.02359)
*Bin Li,Daijie Chen,Qi Zhang*

Main category: cs.CV

TL;DR: 提出一种弱监督、免标定的多视角人群计数方法WSCF-MVCC，仅用总人数计数而非密度图监督单视图模块，并结合自监督排序损失与语义匹配，三数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 多视角可缓解单图像计数的遮挡问题，但现有方法依赖昂贵的相机标定与密集点注释；即便免标定方法也常需图像级密度监督，训练成本高、部署受限。

Method: 1) 弱监督：以总人数计数而非密度图监督单视图计数分支；2) 自监督排序损失：利用多尺度先验构建排名约束，提升感知与泛化，无额外标注；3) 语义引导的视图匹配：利用语义信息跨视角对齐，进行更精确的场景级融合与总数估计；4) 完整的无标定流程，避免相机参数。

Result: 在三个人群多视角数据集的弱监督设定下，整体性能超过当前SOTA，验证了该方法在精度与成本之间的优势。

Conclusion: WSCF-MVCC在无需相机标定与密度图注释的前提下，实现更准确的多视角人群计数，训练成本低、部署更实用，具备推广潜力；代码已开源。

Abstract: Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.

</details>


### [19] [VACoT: Rethinking Visual Data Augmentation with VLMs](https://arxiv.org/abs/2512.02361)
*Zhengzhuo Xu,Chong Sun,SiNan Du,Chen Li,Jing Lyu,Chun Yuan*

Main category: cs.CV

TL;DR: VACoT在推理时动态调用图像增强，通过后处理变换与高效强化学习策略，显著提升VLM在OCR等易受扰任务上的鲁棒性与简洁推理。


<details>
  <summary>Details</summary>
Motivation: VLM主要依赖海量真实或合成数据，缺乏像纯视觉模型那样系统化的数据增强支持；大模型再训练成本高且增益递减，导致在基础感知与分布外场景鲁棒性不足，尤其是OCR对抗干扰。

Method: 提出VACoT：在推理阶段以“链式思维”的代理框架动态选择与调用一组通用视觉增强（如去噪等），形成多视角查询；使用高效的代理式强化学习训练决策策略，并设计条件化奖励，既鼓励必要增强又惩罚冗长回答，降低训练与计算开销。

Result: 在13个感知基准上优于现有方法，特别是在OCR相关的对抗/分布外输入上获得显著鲁棒性提升；同时提出AdvOCR基准以展示后处理视觉增强在对抗场景下的泛化优势。

Conclusion: 无需昂贵再训练，推理期的结构化视觉增强与RL策略可大幅提升VLM在困难与分布外感知任务中的稳健与简洁性，提供低成本、可泛化的实用方案。

Abstract: While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.

</details>


### [20] [Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection](https://arxiv.org/abs/2512.02364)
*Daanish Hindustani,Sanober Hindustani,Preston Nguyen*

Main category: cs.CV

TL;DR: 对比ResNet-50与SqueezeNet在胸部X光片结核检测的效果：SqueezeNet明显优于ResNet-50（Acc 89% vs 73%，F1 87% vs 65%），显示轻量模型在资源受限环境的潜力，但仍需更快更小更准的模型与更严谨验证。


<details>
  <summary>Details</summary>
Motivation: 结核在资源匮乏地区诊断困难，传统方法效率低，期望利用深度学习与计算机视觉提升早期检测与可及性，并探索能在移动设备上部署的轻量模型可行性。

Method: 使用Kaggle 4200张胸片数据；数据预处理含划分、增强、重采样/重缩放；比较预训练ResNet-50与通用SqueezeNet；采用准确率、精确率、召回率、F1和混淆矩阵评估。

Result: SqueezeNet：loss 32%、Acc 89%、Precision 98%、Recall 80%、F1 87%；ResNet-50：loss 54%、Acc 73%、Precision 88%、Recall 52%、F1 65%。SqueezeNet整体表现更佳。

Conclusion: 轻量深度模型可用于TB胸片筛查并具移动端部署潜力，有助于早识别与治疗启动；但需继续改进模型大小、速度与精度，并开展更严格的验证以促进实际应用。

Abstract: This study explores the application of machine learning models, specifically a pretrained ResNet-50 model and a general SqueezeNet model, in diagnosing tuberculosis (TB) using chest X-ray images. TB, a persistent infectious disease affecting humanity for millennia, poses challenges in diagnosis, especially in resource-limited settings. Traditional methods, such as sputum smear microscopy and culture, are inefficient, prompting the exploration of advanced technologies like deep learning and computer vision. The study utilized a dataset from Kaggle, consisting of 4,200 chest X-rays, to develop and compare the performance of the two machine learning models. Preprocessing involved data splitting, augmentation, and resizing to enhance training efficiency. Evaluation metrics, including accuracy, precision, recall, and confusion matrix, were employed to assess model performance. Results showcase that the SqueezeNet achieved a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and an F1 score of 87%. In contrast, the ResNet-50 model exhibited a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and an F1 score of 65%. This study emphasizes the potential of machine learning in TB detection and possible implications for early identification and treatment initiation. The possibility of integrating such models into mobile devices expands their utility in areas lacking TB detection resources. However, despite promising results, the need for continued development of faster, smaller, and more accurate TB detection models remains crucial in contributing to the global efforts in combating TB.

</details>


### [21] [Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention](https://arxiv.org/abs/2512.02368)
*Wenyi Xiong,Jian Chen*

Main category: cs.CV

TL;DR: 提出一种无需高清地图的轨迹预测方法，在时域、空域与频域协同建模：用MoE自适应选择关键频率并融合多尺度时间特征，以选择性注意筛除冗余的时序与交互信息，最后用多模态解码器在patch级与点级联合损监督下生成高质量轨迹；在NuScenes上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶轨迹预测在复杂交互场景下难以从冗余数据中高效提取有效信息，导致计算效率与精度下降；高清地图依赖重，鲁棒性与部署成本高。需要一种能在多域联合建模、减少冗余、提升效率且不依赖地图的算法。

Method: 1) 时域/频域：引入Mixture of Experts选择关键频率分量，并提取与融合多尺度时间特征；2) 空域/交互：提出选择性注意模块，过滤时序与空间交互中的冗余信息；3) 解码：设计多模态解码器输出多种可能轨迹；4) 训练：采用patch-level与point-level联合损失进行监督。

Result: 在NuScenes数据集上优于现有方法（摘要暗示SOTA或显著提升），证明在复杂交互场景下的有效性与效率提升。

Conclusion: 通过跨时-空-频域的联合建模、MoE频率选择与选择性注意去冗余，并辅以多模态解码与多粒度损失监督，方法能在不依赖地图的前提下实现更准确高效的轨迹预测，适用于复杂交互场景。

Abstract: Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.

</details>


### [22] [SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains](https://arxiv.org/abs/2512.02369)
*Qingmei Li,Yang Zhang,Peifeng Zhang,Haohuan Fu,Juepeng Zheng*

Main category: cs.CV

TL;DR: 提出SAGE：在无法改动模型权重与结构的隐私约束下，通过输入级“风格自适应视觉提示”提升语义分割的跨域泛化。利用多风格表示与自适应融合，对每张输入生成动态风格提示以对齐特征分布，在五个基准上优于或匹配SOTA，并在所有设置中超越全量微调。


<details>
  <summary>Details</summary>
Motivation: 现实中因隐私与安全限制，常无法访问模型参数与结构，导致传统微调/自适应不可行；但域移引起的性能下降亟需缓解，促使探索不改权重的输入级泛化策略。

Method: 1) 用风格迁移从源域构建多样化风格表示，学习覆盖广泛视觉特征的风格特征簇；2) 针对每个输入，根据其视觉上下文自适应融合这些风格线索，合成动态的视觉提示（prompt），在不触碰模型内部的前提下调整输入外观，从而隐式对齐跨风格特征分布；3) 形成闭环：风格库—自适应融合—提示注入—性能反馈，持续提升对未见域的鲁棒性。

Result: 在五个公开基准上，SAGE在隐私约束场景下达到与或优于当前SOTA的效果，并在所有设置中超越全量微调基线。

Conclusion: 输入级、风格自适应的提示生成可在冻结模型条件下有效缩小跨域差距；SAGE在无需访问模型权重的前提下实现强泛化，适合隐私敏感应用。

Abstract: Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \textbf{S}tyle-\textbf{A}daptive \textbf{GE}neralization framework (\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.

</details>


### [23] [On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning](https://arxiv.org/abs/2512.02375)
*Liyuan Lou,Wanyun Li,Wentian Gan,Yifei Yu,Tengfei Wang,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: 提出一种用于无人机实时摄影测量的“On-the-fly Feedback SfM”框架，在重建的同时评估质量并给出可行动反馈，结合增量网格、质量评估和预测路径规划，实现近实时的探索-利用式数据采集与航迹优化，减少覆盖空洞与返航成本。


<details>
  <summary>Details</summary>
Motivation: 传统离线无人机摄影测量无法满足灾害响应、数字孪生维护等对时效性和现场闭环的需求；现有实时方法多仅处理图像/序列重建，缺乏对重建质量的在线评估与针对性的采集引导，导致覆盖不足和重复飞行。

Method: 在“SfM on-the-fly”基础上，整合三大在线模块：1) 增量粗网格生成，随稀疏点云扩展构建动态粗网格；2) 网格质量评估与可行动指标输出，用以量化重建可靠性和覆盖状况；3) 预测路径规划，基于质量评估对航迹进行在线优化，实现对未观测区域的探索和已观测区域的精化。形成“探索-利用”的闭环。

Result: 在综合实验中实现近实时的原位重建与评估；通过在线反馈显著减少覆盖空洞与返航/补飞成本，相比基线具有更好的覆盖与效率表现。

Conclusion: 该方法将采集、处理、三维重建与评估、在线反馈一体化，提供从被动到智能自适应探索流程的可行替代方案，适合时间敏感的地理空间应用；代码已开源。

Abstract: Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance. However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area. This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time. Built upon SfM on-the-fly , the proposed method integrates three modules: (1) online incremental coarse-mesh generation for dynamically expanding sparse 3D point cloud; (2) online mesh quality assessment with actionable indicators; and (3) predictive path planning for on-the-fly trajectory refinement. Comprehensive experiments demonstrate that our method achieves in-situ reconstruction and evaluation in near real time while providing actionable feedback that markedly reduces coverage gaps and re-flight costs. Via the integration of data collection, processing, 3D reconstruction and assessment, and online feedback, our on the-fly feedback SfM could be an alternative for the transition from traditional passive working mode to a more intelligent and adaptive exploration workflow. Code is now available at https://github.com/IRIS-LAB-whu/OntheflySfMFeedback.

</details>


### [24] [From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking](https://arxiv.org/abs/2512.02392)
*Yuqing Shao,Yuchen Yang,Rui Yu,Weilong Li,Xu Guo,Huaicheng Yan,Wei Wang,Xiao Sun*

Main category: cs.CV

TL;DR: 提出FDTA框架，通过空间、时间与身份三个适配器显式强化嵌入判别性，显著提升端到端MOT的关联精度并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 端到端MOT虽统一检测与关联并在检测上强，但关联准确率偏低。分析发现共享DETR产生的目标嵌入相似度过高，只在单帧做类别区分，缺乏跨帧、实例级的判别与时空连续性建模。

Method: 在DETR式骨干上增加显式特征精炼模块FDTA：1) 空间适配器（SA）融合深度感知线索以强化空间连续性；2) 时间适配器（TA）聚合历史信息以建模时间依赖；3) 身份适配器（IA）引入质量感知的对比学习，提升实例级可分性。三者互补，端到端训练。

Result: 在多个挑战性MOT基准（DanceTrack、SportsMOT、BFT）上取得SOTA，显著提升关联相关指标，同时保持强检测性能。

Conclusion: 通过对嵌入的空间、时间与身份维度的显式增强，可有效缓解DETR端到端MOT的高相似度问题，提升跨帧关联的鲁棒性与精度；FDTA为统一检测-关联框架提供通用、可扩展的改进路径。

Abstract: End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.

</details>


### [25] [Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels](https://arxiv.org/abs/2512.02394)
*Kejia Hu,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 提出一个可复现的、无需人工标注的相机引导雷达语义标注流程，重现RaDelft结果并在不同雾级下量化标注性能影响。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在恶劣环境下感知鲁棒，但语义分割研究受限于缺少开源数据与雷达标签。RaDelft仅提供激光雷达标注且无公开雷达标注代码，阻碍复现与后续研究。

Method: 将4D雷达点云投影到相机语义分割结果上，结合空间聚类生成雷达点级标签；复现实验以对齐RaDelft数值结果，并在不同雾强条件下评估标注质量。

Result: 相机引导+聚类的自动标注显著提升雷达标签准确度，成功复现RaDelft组的数值表现；给出在多种雾级下的标注性能量化结果。

Conclusion: 提供了一个可复现、无需人工标注的4D雷达语义标注框架，促进社区训练与评测；并揭示雾度对标注性能的影响规律。

Abstract: Recent advances in 4D radar highlight its potential for robust environment perception under adverse conditions, yet progress in radar semantic segmentation remains constrained by the scarcity of open source datasets and labels. The RaDelft data set, although seminal, provides only LiDAR annotations and no public code to generate radar labels, limiting reproducibility and downstream research. In this work, we reproduce the numerical results of the RaDelft group and demonstrate that a camera-guided radar labeling pipeline can generate accurate labels for radar point clouds without relying on human annotations. By projecting radar point clouds into camera-based semantic segmentation and applying spatial clustering, we create labels that significantly enhance the accuracy of radar labels. These results establish a reproducible framework that allows the research community to train and evaluate the labeled 4D radar data. In addition, we study and quantify how different fog levels affect the radar labeling performance.

</details>


### [26] [Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch](https://arxiv.org/abs/2512.02395)
*Yifan Zhang,Liang Hu,Haofeng Sun,Peiyu Wang,Yichen Wei,Shukang Yin,Jiangbo Pei,Wei Shen,Peng Xia,Yi Peng,Tianyidan Xie,Eric Li,Yang Liu,Xuchen Song,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork-R1V4 是一款30B参数的多模态“代理型”模型，通过统一规划、主动图像操作与深度多模态搜索，实现交替进行的视觉操作与外部检索，并仅用少量高质量SFT轨迹（<3万）在多项基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前多模态代理系统往往将图像操作与网页搜索割裂处理、过度依赖昂贵的强化学习、且缺乏基于真实工具执行轨迹的规划训练，限制了可泛化的多步推理与工具编排能力。

Method: 构建30B（A3B）多模态模型，统一四类能力：多模态规划、主动图像编辑/思考、深度多模态搜索，以及可交替进行的视觉-检索交织推理。训练完全采用监督微调：基于少于3万条高质量、规划与执行一致的轨迹，并通过逐步一致性过滤确保数据质量。

Result: 在MMSearch得分66.1、在FVQA得分67.2，11项指标全面超越Gemini 2.5 Flash；推理时涌现长程规划能力，可稳定编排10+次工具调用处理复杂多步任务。

Conclusion: 精心策划的高质量监督数据即可训练出具备复杂工具编排与长程推理的多模态代理，无需依赖强化学习；统一的交织推理范式显著提升感知与检索任务表现。

Abstract: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation ("thinking with images"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.

</details>


### [27] [Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation](https://arxiv.org/abs/2512.02400)
*Wentao Xiang,Haokang Zhang,Tianhang Yang,Zedong Chu,Ruihang Chu,Shichao Xie,Yujian Yuan,Jian Sun,Zhining Gu,Junjie Wang,Xiaolong Wu,Mu Xu,Yujiu Yang*

Main category: cs.CV

TL;DR: Nav-R^2 通过结构化CoT推理与相似度感知记忆（SA-Mem），在开放词汇目标导航中显式建模“目标-环境”和“环境-动作”关系，提升对未见物体的定位成功率并保持实时性。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标导航需要在未见环境中找到新目标，但现有方法决策过程不透明、对未见类泛化差且成功率低。需要一种既可解释又能提升定位未见物体能力的方法。

Method: 提出Nav-R^2：1) 结构化Chain-of-Thought，将流程分为环境感知、目标相关上下文聚焦、未来动作规划；2) 构建NavR^2-CoT数据集以监督上述推理链；3) 设计Similarity-Aware Memory（SA-Mem），从时间与语义双维度压缩视频帧、融合历史观测，保留与目标和当前观测最相关特征且不引入额外参数；4) 形成简洁高效的导航管线，避免对已见类别过拟合。

Result: 在未见物体定位任务上达到SOTA，并保持2Hz实时推理，相比以往方法成功率更高、泛化更好且具可解释性。

Conclusion: 显式关系建模与结构化推理结合相似度感知记忆，可在开放词汇目标导航中有效提升对未见目标的定位与泛化，并维持实时与简洁的推理管线。

Abstract: Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \href{https://github.com/AMAP-EAI/Nav-R2}{github link}.

</details>


### [28] [WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate](https://arxiv.org/abs/2512.02405)
*Anoop Cherian,River Doyle,Eyal Ben-Dov,Suhas Lohit,Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: 提出WISE：一种用于多模态推理的加权迭代“专家社会”多智能体辩论框架，将解答与反思分工，并结合改进的Dawid-Skene聚合法，较现有MAD方法在多数据集上提升2-7%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论主要集中在纯文本任务，尚不清楚在视觉-语言推理上的有效性；不同LLM具备互补能力，需要一个通用、可扩展的框架来协调单模态与多模态专家，稳定聚合多轮辩论中噪声与不一致的答案。

Method: 构建WISE框架：将代理分为Solvers（生成解答）与Reflectors（进行校验、赋权并提供自然语言反馈）；进行多轮迭代，Reflectors基于正确性给出权重指导Solvers更新；最终使用改进的Dawid-Skene算法进行后处理，结合反思权重与跨轮方差实现鲁棒聚合；支持异构单/多模态专家与两阶段辩论流程。

Result: 在SMART-840、VisualPuzzles、EvoChart-QA以及新构造的SMART-840++（可控难度）上评测，WISE在多种多模态任务与不同LLM组合下，相比现有MAD与聚合方法稳定提升2-7%准确率。

Conclusion: MAD在多模态推理中同样有效；通过角色分工与带权聚合的WISE能更好利用异构专家的互补性，显著提升稳健性与准确率，提供了可泛化、模块化的多模态辩论求解范式。

Abstract: Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.

</details>


### [29] [MitUNet: Enhancing Floor Plan Recognition using a Hybrid Mix-Transformer and U-Net Architecture](https://arxiv.org/abs/2512.02413)
*Dmitriy Parashchuk,Alexey Kapshitskiy,Yuriy Karyakin*

Main category: cs.CV

TL;DR: 提出MitUNet：针对平面图墙体语义分割，结合Mix-Transformer编码器、U-Net解码器与scSE注意力，并用可调Tversky损优化，显著提升细薄结构与边界精度，利于后续3D重建矢量化。


<details>
  <summary>Details</summary>
Motivation: 现有方法以常规分割指标优化，难以准确检测细长墙体、边界粗糙，导致3D重建矢量化阶段几何精度不足；需要一种既兼顾全局上下文又能精细恢复边界的模型与损失设计。

Method: 构建混合架构MitUNet：编码端用分层Mix-Transformer获取全局上下文；解码端为U-Net并加入scSE注意力以强化通道与空间特征、恢复边界。训练时采用Tversky损并调节α/β以在精准率与召回率间权衡，重点抑制墙体边界的假阳性同时保持对细薄结构的敏感度。

Result: 在CubiCasa5k与自有区域数据集上，MitUNet产生结构正确、边界精度高的墙体掩膜，性能优于标准单任务基线；特别在薄结构、边界连续性与噪声抑制方面有显著提升。

Conclusion: MitUNet为室内平面图墙体分割提供鲁棒方案，通过Transformer+U-Net+scSE与Tversky损优化，生成几何精确的掩膜，适配自动3D重建流程的数据准备阶段。

Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans requires high-precision semantic segmentation of structural elements, particularly walls. However, existing methods optimized for standard metrics often struggle to detect thin structural components and yield masks with irregular boundaries, lacking the geometric precision required for subsequent vectorization. To address this issue, we introduce MitUNet, a hybrid neural network architecture specifically designed for wall segmentation tasks in the context of 3D modeling. In MitUNet, we utilize a hierarchical Mix-Transformer encoder to capture global context and a U-Net decoder enhanced with scSE attention blocks for precise boundary recovery. Furthermore, we propose an optimization strategy based on the Tversky loss function to effectively balance precision and recall. By fine-tuning the hyperparameters of the loss function, we prioritize the suppression of false positive noise along wall boundaries while maintaining high sensitivity to thin structures. Our experiments on the public CubiCasa5k dataset and a proprietary regional dataset demonstrate that the proposed approach ensures the generation of structurally correct masks with high boundary accuracy, outperforming standard single-task models. MitUNet provides a robust tool for data preparation in automated 3D reconstruction pipelines.

</details>


### [30] [Generalizing Vision-Language Models with Dedicated Prompt Guidance](https://arxiv.org/abs/2512.02421)
*Xinyao Li,Yinjie Min,Hongbo Chen,Zhekai Du,Fengling Li,Jingjing Li*

Main category: cs.CV

TL;DR: 提出GuiDG：通过训练多个参数高效的领域专家并自适应集成，以在微调VLM时提升域外泛化；在标准DG基准与ImageNet-DG上优于SOTA且高效。


<details>
  <summary>Details</summary>
Motivation: 传统对大规模视觉-语言模型的微调多在整数据上统一微调，易在域专属性与域泛化之间失衡，导致对未见域泛化差。作者希望从理论与方法两方面改善微调时的域外泛化能力，尤其在少样本DG场景。

Method: 给出理论分析：将源域划分并训练多个参数高效的“专家”比单一通用模型更有利于泛化。基于此提出GuiDG两步框架：1) 通过prompt tuning在各源域获得领域专家；2) 设计跨模态注意力模块，对视觉编码器进行引导式微调，按样本自适应集成不同专家的知识。同时构建ImageNet-DG数据集用于少样本DG评测。

Result: 在标准DG基准与新建的ImageNet-DG上进行大量实验，GuiDG在保持参数与计算效率的同时，整体性能超过现有微调方法。

Conclusion: 通过领域划分+参数高效专家与跨模态自适应集成，可缓解VLM微调中的专用性-泛化性权衡，提升少样本与跨域泛化，实践上优于SOTA且高效。

Abstract: Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.

</details>


### [31] [GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.02423)
*Haolong Yan,Yeqing Shen,Xin Huang,Jia Wang,Kaijun Tan,Zhixuan Liang,Hongxin Li,Zheng Ge,Osamu Yoshie,Si Li,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CV

TL;DR: 提出GUI Exploration Lab，一个可组合屏幕/图标/导航图的GUI导航仿真引擎，并用分阶段训练范式（SFT→单轮RL→多轮RL）显著提升LVLM驱动的GUI代理在静态与交互式基准及真实场景中的导航泛化与性能。


<details>
  <summary>Details</summary>
Motivation: 现实PC/移动端GUI复杂且多为闭源，难以获取完整环境信息，限制了系统化研究与可复现实验与基准构建，尤其在从单屏到复杂多屏导航的迁移中凸显。需要一个可控、可观测、可组合的环境来训练与评估GUI代理。

Method: 1) 设计GUI Exploration Lab：模块化定义屏幕、图标与导航图，支持灵活组合与完全可观测信息输出，用于训练与评估。2) 训练范式：先用监督微调（SFT）记忆基本知识；再用单轮强化学习提升对未见场景的泛化；最后用多轮强化学习通过交互试错学习探索策略，进一步提升复杂导航能力。3) 在静态与交互式基准以及真实应用上进行对比与验证。

Result: 实验显示：SFT有效固定基础知识；在此基础上，单轮RL显著提升对未见场景的泛化；多轮RL通过探索策略进一步提高复杂屏幕导航表现。方法在静态和交互式基准上均取得明显提升，并能迁移到真实GUI任务。

Conclusion: 开放可控的GUI仿真环境结合分阶段RL训练能系统性提升GUI代理的导航能力与泛化，证实RL在GUI导航上的优势，并为构建更强更通用的GUI代理提供实践路径与经验。

Abstract: With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.

</details>


### [32] [WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning](https://arxiv.org/abs/2512.02425)
*Woongyeong Yeo,Kangsan Kim,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出WorldMM，一个多模态记忆智能体，通过多粒度时间尺度、文本与视觉双通道记忆与自适应检索，显著提升长视频问答表现（较SOTA平均+8.4%）。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型擅长短视频理解，但在小时/天级长视频上受限于上下文窗口和抽象过程中的视觉细节损失。基于文本摘要的记忆方法忽视视觉证据，且固定时间尺度检索难以覆盖变长事件。需要一种能同时利用文本与视觉、多时间粒度且可自适应检索的记忆机制。

Method: 提出WorldMM多模态记忆框架：构建三类互补记忆——(1)情景记忆（episodic）：跨多时间尺度索引事实事件；(2)语义记忆（semantic）：持续更新高层概念知识；(3)视觉记忆（visual）：保留场景细节。推理阶段由自适应检索代理根据查询迭代选择合适记忆源与时间粒度，直到判定信息充分为止。

Result: 在五个长视频问答基准上显著优于现有方法，较前SOTA平均提升8.4%。

Conclusion: 多模态、多粒度、可自适应的记忆与检索机制能有效缓解长视频推理中的上下文限制与细节丢失问题，提升复杂场景推理能力。

Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.

</details>


### [33] [LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework](https://arxiv.org/abs/2512.02437)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出LightHCG：一种极轻量的因果表示驱动青光眼检测模型，通过卷积VAE潜表示、HSIC解耦与图自编码器学习因果结构，在保持或提升检测性能的同时将参数量减少93~99%，并支持干预/模拟分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于CV的青光眼检测（VGG/ViT等）虽表现优异，但存在可靠性不足、参数冗余、易产生虚假相关、难以用于干预分析与临床模拟的问题，需一种能体现病理因果机制、轻量且可解释的模型。

Method: 构建LightHCG：以轻量级卷积VAE学习视盘区域的潜在表示；利用HSIC约束实现潜变量解耦，减少混杂与虚假相关；再以无监督图自编码器在潜空间中学习青光眼相关物理因素间的因果关系，用于分类与干预推断。与InceptionV3、MobileNetV2、VGG16等进行比较。

Result: 在基于眼底或OCT数据的实验中，LightHCG以远少于对比模型（参数量减少93~99%）的权重规模实现相当或更高的青光眼分类性能，并呈现更稳定、更具因果含义的潜变量表示。

Conclusion: LightHCG在保持高准确度的同时显著压缩模型规模，并通过因果表示与解耦提升可靠性与可解释性，增强对干预分析和临床模拟的适用性，相较主流深度视觉模型更具实用潜力。

Abstract: As a representative optic degenerative condition, glaucoma has been a threat to millions due to its irreversibility and severe impact on human vision fields. Mainly characterized by dimmed and blurred visions, or peripheral vision loss, glaucoma is well known to occur due to damages in the optic nerve from increased intraocular pressure (IOP) or neovascularization within the retina. Traditionally, most glaucoma related works and clinical diagnosis focused on detecting these damages in the optic nerve by using patient data from perimetry tests, optic papilla inspections and tonometer-based IOP measurements. Recently, with advancements in computer vision AI models, such as VGG16 or Vision Transformers (ViT), AI-automatized glaucoma detection and optic cup segmentation based on retinal fundus images or OCT recently exhibited significant performance in aiding conventional diagnosis with high performance. However, current AI-driven glaucoma detection approaches still have significant room for improvement in terms of reliability, excessive parameter usage, possibility of spurious correlation within detection, and limitations in applications to intervention analysis or clinical simulations. Thus, this research introduced a novel causal representation driven glaucoma detection model: LightHCG, an extremely lightweight Convolutional VAE-based latent glaucoma representation model that can consider the true causality among glaucoma-related physical factors within the optic nerve region. Using HSIC-based latent space disentanglement and Graph Autoencoder based unsupervised causal representation learning, LightHCG not only exhibits higher performance in classifying glaucoma with 93~99% less weights, but also enhances the possibility of AI-driven intervention analysis, compared to existing advanced vision models such as InceptionV3, MobileNetV2 or VGG16.

</details>


### [34] [Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources](https://arxiv.org/abs/2512.02438)
*Phuc Pham,Nhu Pham,Ngoc Quoc Ly*

Main category: cs.CV

TL;DR: 提出结合动量自蒸馏与梯度累积的高效医药领域视觉-语言对比学习框架，在单卡低资源条件下仍具备强零样本与小样本表现。


<details>
  <summary>Details</summary>
Motivation: 医疗数据精细标注昂贵且难以获得，VLM 需在小数据或零样本下仍保持性能；传统对比学习依赖大批量训练，计算开销大且门槛高，因此需要一种既能高效训练又能充分挖掘数据与模型知识的方法。

Method: 引入“动量+蒸馏”的联合策略：以动量编码器作为教师，对学生进行自蒸馏，增强多模态表示学习；并将动量机制与梯度累积结合，在不增加显存与硬件消耗的情况下扩大等效批量，维持对比学习的稳定与效果。

Result: 在零样本分类上达到与SOTA可比的性能，小样本适配显著提升（AUC-ROC>90%），检索任务提升约2-3%；可在单块GPU上以合理训练时间获得高训练效率。

Conclusion: 动量自蒸馏与梯度累积的结合缓解了对比学习对大批次与大算力的依赖，同时提升了VLM在医疗场景的小/零样本能力，提供了资源友好且性能出色的多模态学习方案；代码已开源。

Abstract: In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .

</details>


### [35] [Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation](https://arxiv.org/abs/2512.02441)
*Junghwan Park,Woojin Cho,Junhyuk Heo,Darongsae Kwon,Kookjin Lee*

Main category: cs.CV

TL;DR: BOLT：利用已微调模型的“正交任务子空间”进行低秩迁移。离线从多任务向量提取并正交化奇异方向形成每层可复用基；在线冻结基，仅学习每层少量对角系数，实现极少参数、可控秩的适配。提供无训练初始化与高效PEFT路径，在多基线与元学习初始化上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 在数据与算力受限时将大预训练模型适配到新任务很难。元学习虽能给出好初始化，但需要昂贵且不稳定的跨任务元训练。同时，任务专用微调模型越来越多，却缺乏如何以极少额外训练把它们迁移到新任务的方法。作者希望利用现有多任务模型的信息，在不再进行高成本合并或元训练的前提下，为新任务提供强初始化与参数高效微调方案。

Method: 提出BOLT：1) 离线阶段：收集多任务的task vector（如微调权重差），对每层做SVD提取主奇异方向，并进行正交化，得到可复用的、任务感知的正交基。2) 在线阶段：冻结这些基，只在基坐标系内学习每层的对角缩放系数（小量可训练参数），形成低秩、可控秩的更新。3) 训练免费初始化：通过汇聚源任务的系数并进行轻量重标定，在共享基上生成对未见任务的起点。4) 与常见PEFT和元学习初始化对比评测。

Result: BOLT在多个任务上较主流PEFT（如LoRA等）与具代表性的元学习初始化取得稳健和强竞争力的效果；在参数效率与训练稳定性方面具有优势。把适配限制在任务感知的正交子空间内，有助于提升未见任务的迁移表现。

Conclusion: 利用从现有微调模型提取的、逐层正交的任务谱基，并在该子空间内进行低秩适配，可以在极少参数与训练成本下实现对新任务的有效迁移；这是权重合并与元学习之外的一条高效可行路径。

Abstract: Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.

</details>


### [36] [Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors](https://arxiv.org/abs/2512.02447)
*Fan Luo,Zeyu Gao,Xinhao Luo,Kai Zhao,Yanfeng Lu*

Main category: cs.CV

TL;DR: 提出Temporal Dynamics Enhancer（TDE），通过生成时间多样化刺激与注意门控，增强SNN对时序信息的建模，在检测任务上显著提升精度并降低注意能耗。


<details>
  <summary>Details</summary>
Motivation: 现有SNN常将输入逐帧复制或固定聚合，导致跨时间步刺激近似相同，限制了时序表征与复杂任务（如目标检测）的表达力与性能；此外，常规注意力引入高能耗乘法。

Method: 构建TDE，包括：1）Spiking Encoder（SE）：在时间步上生成多样化输入刺激，提升时序辨识度；2）Attention Gating Module（AGM）：依据跨时间依赖引导SE的生成；并提出Spike-Driven Attention（SDA），以脉冲驱动的方式替代高能乘法的注意力计算，降低能耗。TDE可无缝集成到现有SNN检测器中。

Result: 在PASCAL VOC（静态）上mAP50-95达57.7%，在EvDET200K（类神经形态）上达47.6%；SDA注意模块能耗为常规注意力的0.240倍。

Conclusion: 通过TDE显著增强SNN的时序建模与检测性能，同时用SDA大幅降低注意能耗；方法通用、可插拔，适用于多种SNN检测框架。

Abstract: Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.

</details>


### [37] [nuScenes Revisited: Progress and Challenges in Autonomous Driving](https://arxiv.org/abs/2512.02448)
*Whye Kit Fong,Venice Erin Liong,Kok Seang Tan,Holger Caesar*

Main category: cs.CV

TL;DR: 论文回顾并系统梳理 nuScenes 数据集及其扩展（nuImages、Panoptic nuScenes）的创建过程、影响与任务生态，概述其对自动驾驶多模态基准与方法发展的推动。


<details>
  <summary>Details</summary>
Motivation: 深度学习驱动的自动驾驶需要大规模、高质量、细粒度标注的多模态数据集；nuScenes 作为早期且有影响力的公开数据集，已成为学界与业界基准，但其创建细节、扩展数据集、对后续数据集和研究范式的影响尚缺系统化总结。

Method: 对 nuScenes 的数据采集平台与流程、传感器设置（含首个引入雷达）、跨洲多样化城市场景、完全自动驾驶车辆在公开道路采集等进行详述；梳理其基准与任务（感知、定位与建图、预测、规划）设计；汇总扩展集 nuImages 与 Panoptic nuScenes；对后续数据集与研究进行影响追踪与横向比较，归纳官方与非官方任务并回顾方法学进展。

Result: 提供未在既有论文公开的技术细节与数据集构建内幕；明确 nuScenes 在多模态融合、标准化评测与任务覆盖方面树立的规范；展示其对后续众多数据集设计与研究方向的显著影响，并给出围绕 nuScenes 的任务与方法的系统性综述。

Conclusion: nuScenes 通过率先引入雷达、跨地域多样场景、完善基准与任务设计，奠定了自动驾驶数据集的多项标准；其生态（含扩展集与任务）持续影响社区，成为自动驾驶感知到规划全链路研究的重要基石。

Abstract: Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \& mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.

</details>


### [38] [HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild](https://arxiv.org/abs/2512.02450)
*Valentin Bieri,Marie-Julie Rakotosaona,Keisuke Tateno,Francis Engelmann,Leonidas Guibas*

Main category: cs.CV

TL;DR: 提出HouseLayout3D真实世界多楼层建筑布局评测基准，并给出无需训练的基线MultiFloor3D，能在新基准和既有数据上超越现有模型，呼吁推动全建筑尺度3D布局估计研究。


<details>
  <summary>Details</summary>
Motivation: 现有3D布局估计主要在合成、单房间/单楼层数据上训练，无法原生处理大型多楼层建筑；实际使用需先按楼层切分，丢失跨楼层（如楼梯）全局空间上下文，限制模型对复杂结构的推理与应用。

Method: 1) 发布HouseLayout3D：覆盖多楼层与复杂建筑空间的真实世界基准，用于评测全建筑尺度布局估计。2) 提出MultiFloor3D：一种简单、无需训练的流程，整合近期场景理解方法，实现端到端多楼层布局推断。

Result: MultiFloor3D在HouseLayout3D基准和以往数据集上都优于现有3D布局估计模型，显示当前方法在多楼层、全建筑尺度任务上的不足。

Conclusion: 全建筑尺度、多楼层3D布局估计亟需新基准与方法支持；所提基准与无训练基线证明该方向可行且有改进空间，为后续研究提供数据与起点（数据与代码公开）。

Abstract: Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.

</details>


### [39] [ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation](https://arxiv.org/abs/2512.02453)
*Kerui Chen,Jianrong Zhang,Ming Li,Zhonglong Zheng,Hehe Fan*

Main category: cs.CV

TL;DR: 提出ClusterStyle：用聚类原型建模全局与局部风格多样性，并通过SMA适配器将风格特征注入文本到动作模型，实现更好风格化动作生成与风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有风格化动作生成能注入风格但难以体现同一风格下的多样化表现（intra-style diversity），导致生成动作单一、风格刻板。

Method: 以聚类为核心：为每个风格类别学习一组非可学习的原型锚点，分别建模两种多样性——跨样本的全局多样性与序列时间维的局部多样性；通过与这些原型的对齐学习两个结构化风格嵌入空间（global/local）。在预训练文本到动作生成器上加入Stylistic Modulation Adapter（SMA），将风格嵌入调制到生成过程。

Result: 在风格化动作生成与风格迁移任务上，较现有SOTA取得更优性能（定量与定性均优），展现更丰富且一致的风格变体。

Conclusion: 结构化的原型对齐可有效捕捉风格内部多样性；结合SMA进行风格注入能提升文本到动作系统的风格表达与可控性，优于现有方法。

Abstract: Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.

</details>


### [40] [See, Think, Learn: A Self-Taught Multimodal Reasoner](https://arxiv.org/abs/2512.02456)
*Sourabh Sharma,Sonam Gupta,Sadbhawna*

Main category: cs.CV

TL;DR: 提出See-Think-Learn (STL) 自训练框架，通过“先看后想”的结构化模板，先抽取视觉属性再进行推理，并引入负向解释以提升区分能力，显著增强VLM的多模态感知与推理，且成本低于依赖人工/专有CoT的方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM要在多模态推理上表现好，既需准确感知也需稳健推理；但增强推理常依赖高质量CoT，获取成本高，且不少自训练忽视视觉感知环节，导致整体能力受限。

Method: 提出STL自训练：1) 结构化“See-Think”模板，先以文本形式抽取图像关键属性（See），再据此展开推理（Think）；2) 模型生成并学习自身结构化理由（rationales），形成自训练闭环；3) 数据增强引入负向理由（解释错误选项为何错），促进更强的判别与鲁棒学习。

Result: 在多种领域上的实验，STL均优于仅用答案监督或仅用自生成推理的基线；质化分析显示其理由质量更高。

Conclusion: STL在不依赖昂贵人工/专有CoT的前提下，同时提升VLM的感知与推理，为多模态推理提供一种简洁、有效、成本友好的方案。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.

</details>


### [41] [Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation](https://arxiv.org/abs/2512.02457)
*Jianzong Wu,Hao Lian,Dachao Hao,Ye Tian,Qingyu Shi,Biaolong Chen,Hao Jiang*

Main category: cs.CV

TL;DR: 论文提出在联合音频-视频去噪训练下，即使只关心视频质量，视频生成也能受益。通过AVFullDiT架构将预训练T2V与T2A模块联合训练，实验显示在大幅运动与物体接触等复杂场景中，视频质量稳定提升，表明音频预测作为“特权信号”能正则化视频动力学并加强物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态生成表明音视频耦合可提升同步性，但尚不清楚当目标仅为视频质量时，联合训练是否仍然有益；作者欲系统验证音频作为辅助任务能否提升视频生成的动态与物理一致性。

Method: 提出参数高效的AVFullDiT：在扩散式DiT框架中，复用预训练T2V与T2A模块进行联合去噪；训练两组模型：(i) T2AV联合去噪模型，(ii) 在相同设置下的仅视频基线；比较在挑战子集（大运动、接触/碰撞等）上的表现以隔离联合训练的收益。

Result: 首次系统性证据表明：联合音视频去噪不仅带来同步，更提高视频质量；在大幅运动与物体接触场景中一致优于仅视频模型。

Conclusion: 跨模态协同训练为更强、更具物理约束的世界模型提供有效路径；音频预测作为特权信号帮助模型内化视觉事件与声学后果的因果关系，从而正则化视频动力学；代码与数据将开源。

Abstract: Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.

</details>


### [42] [Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration](https://arxiv.org/abs/2512.02458)
*Zhongyi Cai,Yi Du,Chen Wang,Yu Kong*

Main category: cs.CV

TL;DR: 提出SEER-Bench评估“顺序化”的具身任务场景，并提出3DSPMR方法，用关系/视觉/几何线索构建3D空间记忆，增强MLLM在连续EQA与EMN中的推理与探索，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，代理常面对连续子任务，且可能出现不可行子任务（如寻找不存在的物体）。与单次任务不同，关键在于如何复用先前探索获得的空间知识来指导后续推理与探索；现有工作缺乏对此系统化评测与方法支持。

Method: 1) 构建SEER-Bench：覆盖顺序化的EQA与EMN场景，要求在多轮任务中复用空间记忆；2) 提出3DSPMR：将已探索区域的关系、视觉与几何信息融合为3D空间记忆，显式把几何线索纳入MLLM输入，提升空间理解与推理；3) 在顺序任务中以记忆增强方式引导后续探索与决策。

Result: 在SEER-Bench的顺序EQA与EMN上，3DSPMR取得显著优于基线的性能增益，显示显式几何信息与空间记忆融合对连续任务有效。

Conclusion: 顺序化具身任务需要可复用的空间记忆与显式几何推理。SEER-Bench为评测提供基准，3DSPMR首次在MLLM框架中引入显式几何信息进行空间理解，显著提升顺序EQA与EMN表现。

Abstract: Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.

</details>


### [43] [TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution](https://arxiv.org/abs/2512.02469)
*Fengli Ran,Xiao Pu,Bo Liu,Xiuli Bi,Bin Xiao*

Main category: cs.CV

TL;DR: 提出TGDD：沿训练轨迹动态对齐特征分布并加正则以减小类重叠，在不增加优化开销下显著提升数据蒸馏效果，尤其在高分辨率数据上提升5%。


<details>
  <summary>Details</summary>
Motivation: 现有基于分布匹配（DM）的数据集蒸馏方法高效但忽视训练过程中表征不断演化，导致合成数据语义表达力不足、类间混淆、下游性能受限。

Method: 将分布匹配重构为沿模型训练轨迹的动态对齐：在每个训练阶段，匹配合成数据与原始数据的特征分布；并引入分布约束正则，降低类间重叠、提升判别性。该设计不引入额外优化开销。

Result: 在10个数据集上达SOTA，特别是在高分辨率基准上带来约5.0%的准确率提升；总体在性能与效率间取得更优平衡。

Conclusion: 通过轨迹引导的阶段化分布对齐与正则化，TGDD能更好保留语义多样性与代表性，提升下游任务表现，同时保持效率。

Abstract: Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.

</details>


### [44] [WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling](https://arxiv.org/abs/2512.02473)
*Yuta Oshima,Yusuke Iwasawa,Masahiro Suzuki,Yutaka Matsuo,Hiroki Furuta*

Main category: cs.CV

TL;DR: 提出WorldPack：一种具压缩记忆的视频世界模型，在显著缩短上下文长度的同时提升长时空一致性、保真度与质量，并在Minecraft的LoopNav基准上优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 长时间、时空一致的视频世界建模仍受长上下文计算开销巨大所限，现有SOTA在长期一致性上仍不稳定，需要更高效的记忆机制以支撑长期推演与空间推理。

Method: 引入高效压缩记忆框架WorldPack，包含：1) 轨迹打包（trajectory packing）以提升上下文利用效率；2) 记忆检索（memory retrieval）在生成过程中检索关键记忆，保持滚动生成的一致性并支撑需要空间推理的长时生成。

Result: 在Minecraft的LoopNav长期一致性基准上，WorldPack显著超越强力SOTA模型，展现更好的空间一致性、保真度与长程生成质量。

Conclusion: 高效压缩记忆（轨迹打包+记忆检索）可在较短上下文下实现更优的长期视频世界建模，一举改善时空一致性与质量，并在标准基准上取得领先。

Abstract: Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.

</details>


### [45] [G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline](https://arxiv.org/abs/2512.02482)
*Vishwesh Nath,Javier G. Tejero,Ruilong Li,Filippo Filicori,Mahdi Azizian,Sean D. Huver*

Main category: cs.CV

TL;DR: G-SHARP是基于Apache-2.0许可的GSplat实现的实时手术场景三维重建框架，面向内镜等微创手术，对可变形组织进行高保真、可商用、低延迟重建，并已在NVIDIA边缘硬件上以Holoscan应用形式部署。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅用于内镜重建的方法多依赖非商业许可或衍生实现，限制临床部署；同时手术场景需要对组织形变、遮挡与实时性的综合处理。

Method: 以GSplat可微高斯光栅化器为底座，构建原生、可商用的手术重建管线；引入可解释的形变建模、稳健遮挡处理与速度-精度权衡策略；在EndoNeRF pulling基准上评估；并通过Holoscan SDK将系统部署到NVIDIA IGX Orin与Thor实现实时可视化。

Result: 在EndoNeRF pulling基准上获得最先进重建质量，同时实现适用于术中应用的速度-精度折中；系统在NVIDIA边缘硬件上实时运行。

Conclusion: G-SHARP在遵循商业许可的前提下，实现了对手术场景的实时高保真重建与形变/遮挡鲁棒性，并提供可落地的OR级部署途径。

Abstract: We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.

</details>


### [46] [UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making](https://arxiv.org/abs/2512.02485)
*Qianhan Feng,Zhongzhen Huang,Yakun Zhu,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: UCAgents 是一种面向医学视觉问答/诊断的层级式多智能体框架，通过“单向收敛 + 结构化证据审计”抑制语言漂移并强化图像证据提取，在降低87.7%令牌成本的同时，在多基准上取得SOTA或显著提升（如PathVQA 71.3%，+6.0%）。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在医学场景常出现“推理脱节”：解释流畅但与可验证影像证据不一致，削弱临床信任。多代理MDT式辩论虽可减轻单模型偏差，但开放式长对话带来文本噪声、成本高，且仍缺乏对视觉证据的锚定。需要一种既能抑制语言/交互噪声，又能强化对影像证据抽取与核验的机制。

Method: 提出UCAgents：层级式多智能体，强制“单向收敛”（禁止立场反转），以结构化证据审计替代开放式辩论；仅允许面向证据的定向交互与一次性询证讨论，用以暴露视觉-文本错配风险。以信息论刻画“视觉歧义+文本噪声”的双噪声瓶颈，并通过流程设计联合收缩两类不确定性，实现受控、低成本的证据核验与结论生成。

Result: 在四个医学VQA基准上，UCAgents取得更高准确率与更低成本：例如PathVQA准确率71.3%，较现SOTA提升+6.0%，同时令牌成本降低87.7%。评估显示其能在更多挖掘视觉证据的同时避免混淆性文本干扰。

Conclusion: 结构化、单向收敛的多代理证据审计相较开放式MDT辩论更能稳健地锚定视觉证据，提升诊断可靠性并兼顾计算效率，适合临床落地。

Abstract: Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.

</details>


### [47] [Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding](https://arxiv.org/abs/2512.02487)
*Yerim Jeon,Miso Lee,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出3D-SLIM，用空间自适应注意力掩码替代因果掩码，缓解3D场景理解中序列偏置与指令注意受限的问题，在多基准与多LLM上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: LLM被用于3D场景-语言推理，但常规语言解码器使用因果注意掩码：1) 把无序的3D物体强行施加顺序导致序列偏置；2) 物体token难以充分访问任务指令，限制特定任务推理。需要一种更契合3D空间结构且指令可及的注意力机制。

Method: 提出3D Spatial Language Instruction Mask (3D-SLIM)，不改网络结构、不加参数，用自适应注意力掩码替代因果掩码：- Geometry-adaptive Mask：依据空间密度/几何邻近约束注意力，而非token顺序；- Instruction-aware Mask：允许物体token直接访问指令上下文，从而在空间关系指导下进行任务相关推理。

Result: 在多种3D场景-语言任务、多个基准与不同LLM作为解码器的设置下，3D-SLIM带来显著性能提升，实验充分验证其有效性。

Conclusion: 解码器中的注意力掩码设计对3D多模态推理至关重要；用空间与指令感知的掩码替代因果掩码能有效消除序列偏置、增强任务对齐，在无需改结构或增加参数的前提下显著提升3D场景-语言理解。

Abstract: Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.

</details>


### [48] [YingVideo-MV: Music-Driven Multi-Stage Video Generation](https://arxiv.org/abs/2512.02492)
*Jiahui Chen,Weida Wang,Runhua Shi,Huan Yang,Chaofan Ding,Zihao Chen*

Main category: cs.CV

TL;DR: 提出YingVideo-MV：首个面向音乐驱动长视频的级联扩散框架，结合音频语义解析、可解释镜头规划、时序感知扩散Transformer与一致性建模，并引入相机适配器与动态时间窗，实现音乐-动作-相机同步的高质量演奏视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动头像视频在长序列和同步方面进展显著，但对“音乐表演+相机运动”的长视频生成缺乏研究与显式相机控制，且跨片段连续性与多样性数据资源不足。

Method: 1) 级联框架：从音频出发进行语义分析；2) MV-Director镜头规划：可解释的镜头/分镜计划；3) 时序感知扩散Transformer用于长序列生成与一致性建模；4) Music-in-the-Wild大规模数据集构建；5) 相机适配器：将相机位姿嵌入潜空间噪声以实现显式相机运动控制；6) 动态时间窗策略：依据音频嵌入自适应调整去噪窗口以增强跨片段连续性。

Result: 在综合基准上优于现有长视频生成方法，实现连贯、富表现力的音乐视频，并达到精确的音乐-动作-相机同步；展示多样高质量样例。

Conclusion: YingVideo-MV有效填补音乐驱动长视频与相机运动控制的空白，借助相机适配器与动态时间窗实现稳定连贯的长序列生成，具备实用性与可扩展性。

Abstract: While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .

</details>


### [49] [Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration](https://arxiv.org/abs/2512.02496)
*Mizuki Kikkawa,Tatsuya Yatagawa,Yutaka Ohtake,Hiromasa Suzuki*

Main category: cs.CV

TL;DR: 提出ARPS层，通过注意力机制为局部到局部点云配准找到共同参考点，获得变换不变特征，从而显著提升基于GMM的深度方法（如DeepGMR、UGMMReg）的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习与GMM的点云配准方法（代表为DeepGMR）在处理局部到局部配准时，常依赖特征对平移和旋转的不变性。但这类不变性在理论与实践上存在问题，导致在部分重叠、参考点选择与对齐时性能不稳。作者希望揭示问题根源，并给出可解释的改进。

Method: 提出注意力驱动的参考点平移（ARPS）层：通过注意力模块在两组部分点集间寻找共同“参考点”（而非直接估计重叠区域），以此将点集对齐到统一参考，使提取到的特征对平移与旋转具有不变性。将ARPS无缝集成到DeepGMR及其变体UGMMReg中。

Result: 加入ARPS后，DeepGMR和UGMMReg在局部到局部配准上的表现显著提升，并超过先前利用注意力或Transformer显式提取重叠区域或公共参考点的深度方法。

Conclusion: 通过把注意力用于寻找公共参考点并进行参考点平移，可在不直接依赖重叠区域检测的情况下获得稳健的变换不变特征，从而提高基于深度学习+GMM的点集配准性能；为此类方法的设计提供了新的理解与方向。

Abstract: This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.

</details>


### [50] [A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation](https://arxiv.org/abs/2512.02497)
*Wenjing Yu,Shuo Jiang,Yifei Chen,Shuo Chang,Yuanhan Wang,Beining Wu,Jie Dong,Mingxuan Liu,Shenghao Zhu,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: MedSeg-TTA提出一个覆盖7种成像模态、20种代表性方法的统一测试时自适应（TTA）分割基准，系统比较四大范式，并给出不同域移类型下的性能与稳定性结论，提供标准数据与榜单以促进临床可用的稳健TTA研究。


<details>
  <summary>Details</summary>
Motivation: 现有医学分割TTA研究在模态覆盖、任务多样性与方法评估一致性方面不足，缺乏跨模态、跨范式的系统性比较，难以指导临床部署中的方法选择。

Method: 构建统一的基准MedSeg-TTA：标准化数据预处理、统一骨干网络与测试时协议；覆盖MRI/CT/超声/病理/皮肤镜/OCT/胸片七种模态；纳入输入级变换、特征级对齐、输出级正则与先验估计四类共20种方法；按多种域移场景（含中心/设备差异与外观变化）进行系统评测，并提供代码、数据与公共榜单。

Result: 无单一范式在所有条件下最优。输入级方法在轻度外观迁移下更稳定；特征级与输出级方法在边界相关指标上更有优势；基于先验的方法高度依赖模态。若存在较大的跨中心/跨设备差异，部分方法性能显著下降。

Conclusion: TTA方法需按场景与模态谨慎选择；MedSeg-TTA通过标准化基准与公开实现为后续研究与临床稳健部署奠定基础，并鼓励在不同域移强度与模态下进行有原则的算法选型与改进。

Abstract: Test time Adaptation is a promising approach for mitigating domain shift in medical image segmentation; however, current evaluations remain limited in terms of modality coverage, task diversity, and methodological consistency. We present MedSeg-TTA, a comprehensive benchmark that examines twenty representative adaptation methods across seven imaging modalities, including MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray, under fully unified data preprocessing, backbone configuration, and test time protocols. The benchmark encompasses four significant adaptation paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, enabling the first systematic cross-modality comparison of their reliability and applicability. The results show that no single paradigm performs best in all conditions. Input-level methods are more stable under mild appearance shifts. Feature-level and Output-level methods offer greater advantages in boundary-related metrics, whereas prior-based methods exhibit strong modality dependence. Several methods degrade significantly under large inter-center and inter-device shifts, which highlights the importance of principled method selection for clinical deployment. MedSeg-TTA provides standardized datasets, validated implementations, and a public leaderboard, establishing a rigorous foundation for future research on robust, clinically reliable test-time adaptation. All source codes and open-source datasets are available at https://github.com/wenjing-gg/MedSeg-TTA.

</details>


### [51] [dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model](https://arxiv.org/abs/2512.02498)
*Yumeng Li,Guang Yang,Hao Liu,Bowen Wang,Colin Zhang*

Main category: cs.CV

TL;DR: 提出dots.ocr，一个端到端单一视觉-语言模型，联合学习版面检测、文本识别与关系理解，在OmniDocBench达SOTA，并在覆盖126种语言的新基准XDocParse上领先次优方法7.4分。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析依赖多阶段流水线，容易误差累积且难以进行联合训练，限制了面向多语言、多版式、多领域的通用文档智能与下一代视觉-语言模型能力。

Method: 构建可扩展的数据引擎合成大规模多语言语料；以单一VLM统一建模版面检测、OCR文本识别与结构关系推断，在端到端框架中联合训练，覆盖多语言、多版式与多领域场景；在综合基准上验证。

Result: 在OmniDocBench上达到最新最优；在新提出的XDocParse（126种语言）上，相对次优方法取得+7.4分优势，展现出强大的多语言泛化。

Conclusion: 端到端统一VLM进行文档布局解析可显著优于分阶段方案；大规模多语言数据引擎与联合训练带来跨语言、跨版式、跨任务的鲁棒提升，并为全球文档智能研究奠定新基准。

Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.

</details>


### [52] [GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding](https://arxiv.org/abs/2512.02505)
*Jiaqi Liu,Ronghao Fu,Haoran Liu,Lang Sun,Bo Yang*

Main category: cs.CV

TL;DR: 论文提出GeoDiT：将地理空间生成从自回归改为并行扩散式细化，显著提升结构化任务（描述、指代、检测）的表现。


<details>
  <summary>Details</summary>
Motivation: 地理空间理解本质是并行、结构化的，但自回归方法强加序列化叙述，难以同时保持全局一致性与对象级结构，导致输出不连贯、结构差。

Method: 将地理空间生成表述为并行的粗到细（coarse-to-fine）整体细化过程，基于扩散模型构建首个面向地理空间的视觉-语言模型GeoDiT，实现对所有语义元素的同步建模与生成。

Result: 在需要结构化、以对象为中心的基准上达到SOTA：图像描述、视觉指代（grounding）、多目标检测均显著优于自回归模型。

Conclusion: 生成过程若与数据的内在结构对齐（并行、整体细化），即可在复杂地理空间分析中解锁更优性能；扩散式并行范式优于自回归序列范式。

Abstract: Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.

</details>


### [53] [Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling](https://arxiv.org/abs/2512.02512)
*Aditya Chaudhary,Prachet Dev Singh,Ankit Jha*

Main category: cs.CV

TL;DR: 提出ViT‑SR：用自监督预训练+微调的两阶段策略，将ViT用于4×单图超分，通过预测高频残差叠加双三次上采样，DIV2K上得PSNR 22.90 dB、SSIM 0.712，显示自监督预训练对图像复原有效。


<details>
  <summary>Details</summary>
Motivation: SISR仍具挑战，尤其在有限标注与泛化能力方面。希望利用ViT的全局建模优势与自监督学习，从未标注数据中学到可迁移视觉表征，提升超分效果与训练稳定性。

Method: 两阶段：1) 自监督预训练：以图像上色为预文本任务，促使ViT学习语义与结构感知的特征；2) 任务微调：将预训练ViT适配为4×超分，通过预测高频残差并加到双三次插值结果上，简化残差学习与收敛。训练与评测在DIV2K进行。

Result: 在DIV2K上获得PSNR 22.90 dB、SSIM 0.712。

Conclusion: 两阶段ViT‑SR在SISR上有效，说明自监督预训练可增强图像复原能力。更大ViT或替代预训练任务可能进一步提升性能。

Abstract: In computer vision, Single Image Super-Resolution (SISR) is still a difficult problem. We present ViT-SR, a new technique to improve the performance of a Vision Transformer (ViT) employing a two-stage training strategy. In our method, the model learns rich, generalizable visual representations from the data itself through a self-supervised pretraining phase on a colourization task. The pre-trained model is then adjusted for 4x super-resolution. By predicting the addition of a high-frequency residual image to an initial bicubic interpolation, this design simplifies residual learning. ViT-SR, trained and evaluated on the DIV2K benchmark dataset, achieves an impressive SSIM of 0.712 and PSNR of 22.90 dB. These results demonstrate the efficacy of our two-stage approach and highlight the potential of self-supervised pre-training for complex image restoration tasks. Further improvements may be possible with larger ViT architectures or alternative pretext tasks.

</details>


### [54] [SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts](https://arxiv.org/abs/2512.02517)
*Jiaqi Liu,Ronghao Fu,Lang Sun,Haoran Liu,Xiao Yang,Weipeng Zhang,Xu Na,Zhuoran Duan,Bo Yang*

Main category: cs.CV

TL;DR: SkyMoE是一种面向遥感多模态多任务的MoE视觉-语言模型，通过任务与粒度感知的路由和上下文解耦增强，实现局部细节与全局语义的平衡；并构建MGRS-Bench验证其在21个数据集上的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 通用VLM在遥感任务上效果不佳，现有地学VLM常采用统一建模，难以区分任务类型与解释粒度，导致对局部与全局信息的权衡不足，限制泛化与可扩展性。

Method: 提出SkyMoE：1) 自适应路由器生成“任务与粒度感知”的路由指令，将不同子任务分配给专门LLM专家；2) 设计上下文解耦的数据增强，构造局部-全局对比对，引导专家学习粒度特定表示；3) 构建覆盖多任务、多粒度的MGRS-Bench用于评测。

Result: 在21个公开数据集上进行广泛实验，SkyMoE在多类遥感解释任务上取得SOTA或领先结果，表现出更好的多粒度理解、适应性与可扩展性。

Conclusion: MoE与任务/粒度感知路由结合上下文解耦，有效提升遥感VLM在局部-全局平衡和跨任务泛化上的能力；MGRS-Bench为复杂场景下的通用性评估提供了标准化基准。

Abstract: The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.

</details>


### [55] [On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection](https://arxiv.org/abs/2512.02520)
*Tai Le-Gia*

Main category: cs.CV

TL;DR: 论文提出零样本异常分类/分割的新理论与框架：揭示一致性异常导致的相似性缩放与邻域“烧蚀”两大现象；据此设计CoDeGraph图方法抑制一致性异常；扩展到3D MRI并提出无需训练的体素化分词，实现零样本3D异常分割；最后用CoDeGraph伪掩码监督提示驱动的视觉-语言模型，联通批量式与文本式零样本方法。


<details>
  <summary>Details</summary>
Motivation: 工业检测与医学影像中常缺少标注或训练数据，需在零样本条件下发现异常。但现有基于距离/相似度的方法在“重复出现的相似异常”场景下会系统性偏置，缺乏理论解释与稳健算法；同时3D医学场景与多模态文本提示的结合仍未解决。

Method: 1) 对预训练ViT补丁表示进行统计与几何分析，提出并刻画“相似性缩放”和“邻域烧蚀”两种现象，形式化“一致性异常”问题。2) 基于上述现象构建CoDeGraph：多阶段相似图构建—社区检测—结构化细化，过滤一致性异常对邻域结构的干扰。3) 面向3D MRI提出训练自由的高效体素化分词策略，形成真正零样本3D异常检测/分割流水线。4) 以CoDeGraph生成的伪掩码监督提示驱动的视觉-语言模型，实现批量式与文本式零样本方法的桥接。

Result: CoDeGraph能有效抑制一致性异常引起的偏置，在零样本AC/AS上性能提升；提出的体素化分词实现无需3D训练样本的3D异常分割；利用伪掩码对VL模型进行弱监督，提升文本驱动零样本性能并展示两类方法的互补性。

Conclusion: 从理论到算法系统刻画并解决零样本异常检测/分割中的一致性异常难题；给出可扩展到3D医学影像的训练自由方案，并通过伪掩码把批量式与文本式零样本方法连接起来，提供了零样本AC/AS的可行且实用的整体路径。

Abstract: Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.
  We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.
  We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.
  Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.
  Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.

</details>


### [56] [WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens](https://arxiv.org/abs/2512.02536)
*Jian Yang,Dacheng Yin,Xiaoxuan He,Yong Li,Fengyun Rao,Jing Lyu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出“噪声查询token”与额外VAE分支，使VLM与扩散模型高效衔接，缓解固定查询token方法的任务泛化崩溃，并支持稳定的持续学习与细粒度细节重建。


<details>
  <summary>Details</summary>
Motivation: 现有用固定数量可学习查询token连接VLM与扩散模型的方法虽高效，但在遇到与预训练任务分布差异大的新任务时出现泛化崩溃，且难以持续学习，还容易丢失细节。需要一种既高效又具备跨任务与持续学习能力的桥接机制。

Method: 1) 设计“噪声查询token”：在VLM与扩散模型之间通过带噪分布式表示空间进行端到端优化，使查询token在训练中覆盖更广的任务分布并逐步对齐两端特征；2) 引入带线性投影的VAE分支，提供细粒度重建路径以补偿扩散生成中的细节缺失。

Result: 在多样化任务上实验显示：相比固定查询token方案，该方法显著缓解泛化崩溃，且在持续学习场景中训练稳定，生成质量和细节恢复更好。

Conclusion: 噪声查询token与VAE线性投影分支可作为高效稳健的VLM—扩散模型桥接策略，提升跨任务泛化与持续学习能力，并改善细节重建。

Abstract: Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.

</details>


### [57] [AVGGT: Rethinking Global Attention for Accelerating VGGT](https://arxiv.org/abs/2512.02541)
*Xianbing Sun,Zhikai Zhu,Zhengyu Lou,Bo Yang,Jinyang Tang,Liqing Zhang,He Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 论文针对多视图3D模型（如 VGGT、π^3）中全局自注意力的高计算开销，系统性剖析其作用并提出无需再训练的两步加速：将早期全局层改为帧内注意力、对全局注意力做带对角保留与均值填充的K/V子采样，在保持或略提精度的同时将推理提速8–10倍，且在极密集多视图下依然稳健。


<details>
  <summary>Details</summary>
Motivation: 现有强性能多视图3D模型高度依赖全局自注意力，计算昂贵；稀疏注意力虽有加速，但缺乏关于“全局注意力真正在哪些层面起作用”的系统分析，导致加速与性能折衷不可控。作者动机是辨析各层全局注意力的具体职责，以指导更有针对性的、无需再训练的高效替代方案。

Method: 1) 分析：对VGGT与π^3的交替式全局-帧架构进行层级作用剖析，发现早期全局层不建立有效对应、中期负责跨视图对齐、后期仅微调。2) 训练免加速方案：a) 早期层由全局注意力改为帧内注意力；b) 对保留的全局注意力进行K/V子采样，采用斜对角（自相似/同位邻近）保留与均值填充以补全信息。3) 将方案直接应用于VGGT与π^3并在位姿与点图基准上验证。

Result: 在标准位姿估计与点图重建基准上，推理时间加速约8–10倍，同时与原模型精度持平或略有提升；在极密集多视图场景下依然稳定，而先前稀疏注意力方法会失效。

Conclusion: 全局注意力在多视图推理中分工明确：中期层最关键。基于此可进行有针对性的结构替换与子采样，无需再训练即可显著加速且维持精度，为多视图3D模型提供通用、稳健的高效化路径。

Abstract: Since DUSt3R, models such as VGGT and $π^3$ have shown strong multi-view 3D performance, but their heavy reliance on global self-attention results in high computational cost. Existing sparse-attention variants offer partial speedups, yet lack a systematic analysis of how global attention contributes to multi-view reasoning. In this paper, we first conduct an in-depth investigation of the global attention modules in VGGT and $π^3$ to better understand their roles. Our analysis reveals a clear division of roles in the alternating global-frame architecture: early global layers do not form meaningful correspondences, middle layers perform cross-view alignment, and last layers provide only minor refinements. Guided by these findings, we propose a training-free two-step acceleration scheme: (1) converting early global layers into frame attention, and (2) subsampling global attention by subsampling K/V over patch tokens with diagonal preservation and a mean-fill component. We instantiate this strategy on VGGT and $π^3$ and evaluate across standard pose and point-map benchmarks. Our method achieves up to $8$-$10\times$ speedup in inference time while matching or slightly improving the accuracy of the original models, and remains robust even in extremely dense multi-view settings where prior sparse-attention baselines fail.

</details>


### [58] [OmniPerson: Unified Identity-Preserving Pedestrian Generation](https://arxiv.org/abs/2512.02554)
*Changxiao Ma,Chao Yuan,Xincheng Shi,Yuzhuo Ma,Yongfei Zhang,Longkun Zhou,Yujia Zhang,Shangze Li,Yifan Xu*

Main category: cs.CV

TL;DR: OmniPerson提出一个统一的、可控且保身份一致的行人生成管线，兼容RGB/IR图像与视频，并以多参考融合与新数据集PersonSyn支撑，在视觉质量与身份一致性上达SOTA，并能稳健提升ReID性能。


<details>
  <summary>Details</summary>
Motivation: 现有ReID训练数据受隐私与标注成本限制且难扩展；已有行人生成方法在身份一致性与可控性方面不足，限制了其作为数据增强的效果。

Method: 1) 统一生成模型OmniPerson：支持RGB/IR图像与视频生成、多张参考图、两类姿态条件与文本控制，并具备RGB→IR迁移与超分能力；2) Multi-Refer Fuser：融合任意数量、多视角参考图以蒸馏统一身份表征，提升身份保真；3) PersonSyn数据集与自动化构建流程：把公开仅有ID标注的ReID数据集转化为具备稠密多模态监督的可控行人生成数据。

Result: 在行人生成的视觉保真与身份一致性上达到SOTA；用生成数据增强现有数据集能持续提升多种ReID模型的效果。

Conclusion: OmniPerson作为首个统一、强可控且保身份一致的行人生成方案，在可见光/红外图像与视频任务上表现优异，并通过PersonSyn与开源生态推动ReID数据规模与质量提升。

Abstract: Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.

</details>


### [59] [From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature](https://arxiv.org/abs/2512.02566)
*Kun Yuan,Min Woo Sun,Zhen Chen,Alejandro Lozano,Xiangteng He,Shi Li,Nassir Navab,Xiaoxiao Sun,Nicolas Padoy,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: Panel2Patch从生物医学论文中的多面板、带标记的图和说明文字中挖掘层级结构，将图-文对应由粗粒度的“整幅图—整段说明”细化为“图/面板/局部补丁—相应文本”，并据此开展粒度感知预训练，以更少数据取得更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学视觉-语言预训练多以整图与整段文字配对，忽略临床实践需要的细粒度局部结构与标注对应，导致监督信号稀释、泛化有限。作者希望利用论文图中天然存在的层级与视觉标记，保留局部语义对齐以提升表征质量与数据效率。

Method: 提出Panel2Patch数据管线：对科学图解析版式、面板和可视化标记，结合题注文本，构建图-面板-补丁多层级对齐的图文对；在此语料上设计粒度感知预训练策略，统一从粗到细的异构目标（如教学式描述与区域短语），实现层级监督学习。

Result: 在仅使用少量论文图的情况下，所提数据与训练策略相比以往粗粒度管线提供更有效监督，显著提升下游表现，并以更少的预训练数据达到更优效果。

Conclusion: 保留并利用科学图的层级与局部标记能显著增强生物医学视觉-语言模型的学习效率与性能；多粒度对齐与训练是提升表征质量、减少数据需求的有效途径。

Abstract: There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.

</details>


### [60] [Co-speech Gesture Video Generation via Motion-Based Graph Retrieval](https://arxiv.org/abs/2512.02576)
*Yafei Song,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: 提出一个结合扩散模型与基于图的检索与拼接的共语手势视频生成框架，相比现有方法在同步性与自然度上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于动作图的检索方法通常依赖音频与动作的一对一特征距离或共享空间嵌入，但音频与手势之间存在多对多对应关系，导致匹配不充分、动作不自然、时序不同步。

Method: 1) 训练扩散模型以学习音频与动作的联合分布，用输入音频先生成手势运动；同时从音频提取低层与高层特征以丰富扩散模型的条件信号。2) 在动作图上进行运动驱动的检索：计算全局与局部运动相似度，选择最合适的路径。3) 对检索得到且不完全连续的节点序列进行无缝拼接，生成连贯视频。

Result: 实验显示相较以往方法在同步准确性与生成手势自然度上有显著提升。

Conclusion: 通过先用扩散模型生成上下文合适的手势，再用运动图检索与拼接优化时序与连贯性，可有效克服音频-手势多对多映射带来的局限，提升共语手势视频的质量。

Abstract: Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.

</details>


### [61] [Content-Aware Texturing for Gaussian Splatting](https://arxiv.org/abs/2512.02621)
*Panagiotis Papantonakis,Georgios Kopanas,Fredo Durand,George Drettakis*

Main category: cs.CV

TL;DR: 他们将高斯点云渲染与可适配分辨率的纹理贴图结合，通过在优化过程中自适应调整每个高斯基元的纹理分辨率，既提升细节还减少参数量。


<details>
  <summary>Details</summary>
Motivation: 纯高斯点（Gaussian Splatting）在表示高频外观细节时需要大量小高斯，参数冗余且低效；而传统纹理在表现外观细节方面更经济。作者希望在不牺牲实时性的前提下，用纹理承担外观高频、让高斯负责几何/低频，从而降低参数并提高质量。

Method: 提出为二维高斯基元配套的逐基元纹理表示，并在优化过程中依据图像采样频率与内容自适应调整纹理分辨率（可上/下采样），约束纹素尺寸不超过图像采样频率；同时通过纹理分辨率控制基元数量的增长或合并。与现有“加纹理的高斯”方案相比，其纹理大小与基元分辨率动态联动。

Result: 在多个场景中，相比替代的纹理化高斯方案，以更少的总参数达到更高或相当的成像质量；还能在优化中稳定地控制基元数量。项目页提供演示与代码。

Conclusion: 自适应纹理分辨率与高斯基元结合能更高效地表示外观高频细节，在保证实时渲染的同时减少参数量并提升图像质量；方法在优化阶段即可联合决定纹理与基元规模。

Abstract: Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.
  Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/

</details>


### [62] [RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence](https://arxiv.org/abs/2512.02622)
*Xuming He,Zehao Fan,Hengjia Li,Fan Zhuo,Hankun Xu,Senlin Cheng,Di Weng,Haifeng Liu,Can Ye,Boxi Wu*

Main category: cs.CV

TL;DR: 提出RULER-Bench，对视频生成模型的“规则型推理”能力进行系统评测；覆盖T2V与I2V两范式、6类规则、40任务、622实例；用四维度清单+GPT-o3自动打分（与人类85%一致）；SOTA在规则一致性仅48.87%，显示推理仍有巨大改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评测集中在视觉审美、指令遵循、时序一致等感知层面，缺乏对“基于认知规则的推理”能力的细粒度分解与系统评测；零样本学习探索不足以揭示推理结构化能力，因而需要专门基准。

Method: 构建RULER-Bench：面向文本到视频与图像到视频两种范式；设计6类规则、40代表性任务、622标注实例；为每个生成视频制定四维度检查清单并用GPT-o3自动评分，验证与人工评判85%一致；据此对多模型进行大规模评测。

Result: 在该基准上，当前最强视频生成模型在“规则一致性（rule coherence）”指标仅得48.87%；自动评测框架与人工评估达85%一致性，显示量化有效。

Conclusion: 视频生成模型在规则型推理方面明显不足；RULER-Bench提供系统化、可扩展的评测协议，有望推动“面向推理的”视频生成研究，迈向具备基础视觉智能的模型。

Abstract: Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.

</details>


### [63] [PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding](https://arxiv.org/abs/2512.02624)
*Zheng Huang,Xukai Liu,Tianyu Hu,Kai Zhang,Ye Liu*

Main category: cs.CV

TL;DR: PPTBench提出一个面向PPT任务的多模态基准，覆盖检测、理解、修改、生成四类共4439个样本，揭示MLLM在语义理解强但布局推理弱的显著差距，并公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有评测多集中于狭窄子任务，忽略PPT场景中最关键的布局与视觉-结构推理，无法反映真实的幻灯片制作与编辑需求，因此需要一个综合、布局中心的基准。

Method: 从958个PPTX文件构建数据，设计四大任务类别（Detection/Understanding/Modification/Generation），将视觉线索与JSON结构化布局结合，制定评测协议与消融分析，进行模型比较与案例可视化。

Result: 实验显示当前MLLM能理解语义内容但无法生成连贯空间布局；在结合视觉线索与JSON布局、以及将视觉信息用于API规划方面表现不足；案例揭示系统性对齐错误与元素重叠等问题。

Conclusion: PPTBench为评估VLLM在PPT任务上的视觉-结构推理与连贯生成提供了全面基准，暴露现有模型布局能力短板并指明改进方向；数据与代码已完全开源，便于复现与后续研究。

Abstract: PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.

</details>


### [64] [Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening](https://arxiv.org/abs/2512.02643)
*Yongchuan Cui,Peng Liu,Yi Zeng*

Main category: cs.CV

TL;DR: 提出一种利用大规模模拟数据预训练的基础模型策略，学习稳健的空间-光谱先验，以提升遥感影像融合（尤其是跨传感器的全色锐化）在零样本与小样本场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习融合方法因真实标注数据稀缺且跨卫星传感器存在域差导致泛化差；需要一种能跨域迁移、减少对真实配准数据依赖的训练范式。

Method: 构建大规模模拟数据：对ImageNet与SkyScript图像施加退化（模糊、噪声、降采样）与增强（波段生成、通道打乱、高通滤波、颜色抖动等），合成多样化的空间-光谱映射；在此上预训练多种融合网络（CNN/Transformer/Mamba），学习通用空间-光谱表征；在六个真实数据集（WV2/3/4、IKONOS、QuickBird、GF-2）上以零样本与一次样本，配合全量或冻结微调进行评测。

Result: 预训练显著提升跨传感器与成像条件下的泛化；在零样本情形优于现有方法，在一次样本（极少真实数据）下适应性强；对不同架构均有效，建立新的跨域融合基线。

Conclusion: 基于模拟数据的大规模预训练可有效学习空间-光谱先验，为跨域全色锐化提供实用方案并树立新基准，展示了将基础模型与先进训练策略用于遥感融合任务的潜力。

Abstract: Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.

</details>


### [65] [PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking](https://arxiv.org/abs/2512.02648)
*Dong Li,Jiahao Xiong,Yingda Huang,Le Chang*

Main category: cs.CV

TL;DR: PoreTrack3D 提出首个用于非刚性人脸细粒度运动的动态3D高斯泼溅基准与数据集，含44万+轨迹，并给出现有方法在其上的首个性能基线。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D重建与表情分析多聚焦于稀疏人脸关键点或宏观表情，缺乏能刻画毛孔级别细微皮肤表面运动的高保真数据与统一评测基准，阻碍对细粒度表情与真实人脸动态的研究。

Method: 构建PoreTrack3D数据集：采集非刚性人脸动态序列，标注/追踪包含传统面部地标与毛孔级关键点的3D轨迹（总计44万+，其中5.2万+>10帧，68条覆盖150帧并经人工复核）；基于动态3D高斯泼溅的管线完成高保真运动捕捉与动态重建；在数据集上系统评测多种SOTA动态3D高斯泼溅方法，建立基线。

Result: 获得覆盖广、时长足的高质量3D人脸轨迹数据；首次提供动态3D高斯泼溅在人脸细粒度运动上的系统评测与基线表现；验证所提数据生成与评测管线可实现高保真动态重建与跟踪。

Conclusion: PoreTrack3D填补了细粒度人脸动力学数据与评测的空白，为动态3D高斯泼溅在人脸领域提供首个标准基准，并提出可复用的高保真构建与评测框架；数据已开源，可促进精细表情分析与相关方法发展。

Abstract: We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D

</details>


### [66] [Hear What Matters! Text-conditioned Selective Video-to-Audio Generation](https://arxiv.org/abs/2512.02650)
*Junwon Lee,Juhan Nam,Jiyoung Lee*

Main category: cs.CV

TL;DR: 提出SelVA：一种文本条件的选择性视频转音频生成模型，可在多物体视频中只生成用户意图的目标声源，并在语义与时间上与视频对齐。


<details>
  <summary>Details</summary>
Motivation: 多媒体制作需要对每个声源单独处理（编辑、混音、创作控制），但现有V2A方法往往一次性输出混合的单通道声音，且视觉表征纠缠、区域线索或提示难以明确定位目标声源，缺乏对“只生成某一源”的能力。

Method: 将文本提示显式作为目标选择器：对视频编码器进行调制，使其聚焦提取与文本相关的视觉特征；引入补充token以促进跨注意力，并抑制与文本无关的激活，从而增强语义与时间落地；通过自增广策略在缺乏单源音轨监督的情况下进行训练；在VGG-MONOAUDIO这一经整理的单源干净视频基准上进行训练与评测。

Result: 在音频质量、语义对齐和时间同步等指标与消融实验上均显著优于对比方法，验证了选择性V2A任务上的有效性与稳健性。

Conclusion: SelVA能以文本选择目标声源，从多物体视频中生成干净、对齐的单源音频；补充token与高效参数调优提升跨模态选择性与时序对齐，自增广缓解监督不足，方法在新建基准上表现优越。

Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.

</details>


### [67] [Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation](https://arxiv.org/abs/2512.02660)
*Agathoklis Georgiou*

Main category: cs.CV

TL;DR: 该论文提出一种混合检索方法：用ColPali等视觉-语言模型的补丁级相似度作为空间过滤器，去筛选OCR提取的文本框，从而在不训练的前提下实现页面内精确片段级检索，适用于RAG。


<details>
  <summary>Details</summary>
Motivation: VLM基于图像补丁与查询token的细粒度相似度可达SOTA，但只能返回整页上下文，不利于RAG的精确引用；OCR能给出结构化文本及坐标，却缺乏语义相关性评估。需要将两者统一，既保留语义判别力，又获得精确空间定位。

Method: 1) 将ColPali产生的补丁级（Vision Transformer网格）相似度分布映射到OCR文本框坐标系；2) 设计补丁网格与OCR框之间的坐标对应关系与交并度量（如IoU/面积交集比例）以传播相关性；3) 依据交集权重聚合补丁分数，得到每个OCR区域的相关性；4) 理论上给出检索精度的上界/下界；5) 推理时集成，无需额外训练；6) 发布开源实现Snappy。

Result: 方法可在推理阶段直接应用，能将页面级检索细化到区域级，理论分析提供精度保证；实证评估仍在进行中，但开源实现显示出可用性与实用性。

Conclusion: 混合VLM+OCR框架在不训练的条件下实现精确的片段级检索，弥补VLM只能返页、OCR缺乏语义的缺点，适合RAG场景；未来工作包括完善实证、优化映射与传播策略。

Abstract: Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.

</details>


### [68] [PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes](https://arxiv.org/abs/2512.02664)
*Derui Shan,Qian Qiao,Hao Lu,Tao Du,Peng Lu*

Main category: cs.CV

TL;DR: 提出PolarGuide-GSDR：将偏振先验与3D高斯点渲染（3DGS）双向耦合，实现无需环境贴图与材料假设的实时高保真反射分离与场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有偏振NeRF可重建镜面反射但训练慢、渲染低效且依赖材料/视角假设；3DGS虽能实时渲染，但反射与几何耦合导致反射重建不准，引入延迟反射模块又依赖环境图。需要一种既保留3DGS实时性，又准确处理镜面反射且不依赖强先验的方法。

Method: 提出PolarGuide-GSDR，建立偏振与3DGS的双向耦合：先利用3DGS提供的几何先验消解偏振歧义（如法线方向/反射分量分解）；再用经校正的偏振信息反向指导3DGS的法线与球谐表示优化，实现反射分离与全场景重建。无需环境图或严格材料假设。

Result: 在公开与自采数据上，相比现有方法在镜面重建、法线估计与新视角合成上取得SOTA，同时保持实时渲染能力。

Conclusion: 这是首个将偏振先验直接嵌入3DGS优化的框架，通过双向耦合实现对复杂反射场景的可解释、实时、高保真建模与渲染。

Abstract: Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.

</details>


### [69] [UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking](https://arxiv.org/abs/2512.02668)
*Qionglin Ren,Dawei Zhang,Chunxu Tian,Dan Zhang*

Main category: cs.CV

TL;DR: 提出UAUTrack：单流、单阶段、端到端的统一多模态（RGB/TIR/RGB-T）单目标Anti-UAV跟踪框架，借助文本先验提示聚焦无人机，达到SOTA并兼顾速度与精度。


<details>
  <summary>Details</summary>
Motivation: 现有Anti-UAV跟踪多基于单一模态或为每种任务独立建模，缺乏跨模态协同的统一框架，且多模态融合效果有限；需要一种能高效共享跨模态信息、提升鲁棒性的统一方法。

Method: 设计UAUTrack：单流、单阶段、端到端架构，统一处理RGB、TIR及其融合；核心引入文本先验提示策略，引导模型在不同场景中关注“无人机”目标，实现跨模态信息有效整合与关注对齐。

Result: 在Anti-UAV与DUT Anti-UAV数据集上取得SOTA；在Anti-UAV410上实现精度与速度的良好折中，展现高准确性与实用效率。

Conclusion: UAUTrack证明了文本提示结合统一单流架构能有效实现跨模态协同与高效Anti-UAV跟踪，为多模态小目标安防跟踪提供了可推广的解决方案。

Abstract: Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.

</details>


### [70] [PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution](https://arxiv.org/abs/2512.02681)
*Zhongbao Yang,Jiangxin Dong,Yazhou Yao,Jinhui Tang,Jinshan Pan*

Main category: cs.CV

TL;DR: PGP-DiffSR 提出一种轻量化扩散式超分方法：通过渐进剪枝去掉冗余扩散块，并用相位交换适配器利用输入图像的相位信息弥补细节，达到在显著降算力/显存的同时保持接近的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散超分依赖SDXL、DiT等大型骨干，训练推理成本高。作者观察到扩散骨干内部存在冗余，且被剪枝后的模型在相位（结构边缘/纹理）恢复上不足，需在不牺牲质量的前提下降低开销。

Method: 1) 渐进式剪枝：识别扩散网络的intra-block冗余，逐步移除冗余块，同时保持恢复能力。2) 相位交换适配器：从输入图像提取相位信息，并与剪枝后的扩散模型特征交互，纠正相位估计偏差。3) 将剪枝策略与相位适配器统一训练/集成。

Result: 在多项实验中，PGP-DiffSR显著降低计算量与显存占用，同时在重建质量（感知与PSNR等指标）上与大型扩散基线竞争。公开代码：https://github.com/yzb1997/PGP-DiffSR。

Conclusion: 通过有指导的渐进剪枝与相位信息引导，PGP-DiffSR在保持高质量超分的同时显著提升效率，为扩散式超分提供了实用的轻量化方案。

Abstract: Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.

</details>


### [71] [Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance](https://arxiv.org/abs/2512.02685)
*Huankun Sheng,Ming Li,Yixiang Wei,Yeying Fan,Yu-Hui Wen,Tieliang Gong,Yong-Jin Liu*

Main category: cs.CV

TL;DR: 提出FASA（Foreground-Aware Slot Attention），通过显式前景/背景分离与伪掩码引导，提升无监督目标发现与场景分解性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于slot attention的无监督目标发现方法对前景与背景一视同仁，导致背景干扰、前景过分割，尤其在真实数据上实例发现效果欠佳。需要一种能显式建模前景并抑制背景干扰的机制。

Method: 两阶段框架：1）前景/背景粗分解：引入双槽竞争机制（dual-slot competition），并用聚类初始化策略获得突出区域的良好初始表示，从而将前景与背景区分开。2）掩码化slot attention：固定第一个槽捕获背景，其他槽竞争表示各个前景实例；为缓解前景过分割，基于自监督特征构建patch亲和图，生成伪掩码对前景槽进行监督引导。

Result: 在合成与真实数据集上，FASA在对象发现与场景分解指标上持续优于当前SOTA，显示显式前景建模与伪掩码引导的有效性和鲁棒性。

Conclusion: 显式前景/背景分离与伪掩码引导能显著提升无监督对象中心表示学习中的实例发现与对象一致性表示；FASA为现实场景中的稳健场景分解提供了有效方案，代码将开源。

Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.

</details>


### [72] [ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data](https://arxiv.org/abs/2512.02686)
*Yuxing Liu,Yong Liu*

Main category: cs.CV

TL;DR: 提出ClimaDrive，一个语义引导的图像到图像框架，生成语境一致、物理真实、跨天气的异常驾驶数据，并构建ClimaOoD基准；在多种SOTA上显著提升异常分割泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有异常分割受限于真实异常数据稀缺与多样性不足，常用的合成方法（拷贝粘贴、扩散修补）易造成语境不一致与物理不真实，导致合成与真实分布存在域间隙，影响开放环境泛化。

Method: 提出ClimaDrive：语义引导的图像到图像管线，统一结构引导的多天气生成与提示词驱动的异常修补（inpainting），保证与场景语义一致、跨天气多样且物理可行的OoD样本；据此构建大规模ClimaOoD基准，覆盖六类驾驶场景与晴/恶劣天气。

Result: 在四个SOTA异常分割方法上，使用ClimaOoD训练显著提高AUROC、AP并降低FPR95；如Fishyscapes LAF上，RbA的FPR95由3.97降至3.52，整体表现更稳健。

Conclusion: 语义一致、物理合理、跨天气的合成OoD数据能有效缩小域间隙，显著提升异常分割的鲁棒性与泛化；ClimaOoD为开放世界异常检测提供高价值训练数据与评测基准。

Abstract: Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.

</details>


### [73] [ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection](https://arxiv.org/abs/2512.02696)
*Omid Reza Heidari,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: 提出在安检X光目标检测中应用ALDI++进行跨域自适应，结合自蒸馏、特征对齐与强化训练，在EDS数据集多种适配场景下全面超越SOTA，尤其搭配ViTDet取得最高mAP，类别层面也稳定提升。


<details>
  <summary>Details</summary>
Motivation: 安检X光成像在不同设备与环境下存在显著域偏移，导致检测性能下降；需要一种稳健的跨域自适应方法以提升真实应用中的泛化与稳定性。

Method: 采用ALDI++域自适应框架：1) 自蒸馏以稳定目标域学习并缓解伪标签噪声；2) 特征对齐（可能含全局/局部/对抗式或统计匹配）以缩小源/目标域分布差异；3) 强化训练策略（如数据增广、优化与正则）提升鲁棒性；并与ViTDet主干结合以增强跨域表征能力。

Result: 在EDS数据集的多种源→目标适配场景中，ALDI++在mAP上优于现有SOTA方法；使用ViTDet主干时获得最高mAP；类别级分析显示各类目标检测精度一致性提升。

Conclusion: ALDI++为安检X光跨域目标检测提供高效稳健的解决方案，树立新的性能与稳定性基准；Transformer主干（ViTDet）在跨域检测中尤为有效，模型对不同类别具备良好泛化。

Abstract: Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.

</details>


### [74] [GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization](https://arxiv.org/abs/2512.02697)
*Zixuan Song,Jing Zhang,Di Wang,Zidie Zhou,Wenbin Liu,Haonan Guo,En Wang,Bo Du*

Main category: cs.CV

TL;DR: GeoBridge 提出一种跨视角与跨模态的通用地理定位模型，利用“语义锚点”把无人机、街景、卫星图像与文本描述对齐，实现双向图像检索与文到图检索；并发布了大规模多视角跨模态数据集 GeoLoc。预训练显著提升定位精度与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角定位多依赖高质量、最新的卫星图像，并以卫星为中心进行匹配，导致在卫星分辨率低、更新滞后或域偏移场景下鲁棒性差；同时没有充分利用无人机、街景等视角及语言描述等互补信息。

Method: 提出 GeoBridge：1) 引入“语义锚点”机制，将多视角视觉特征通过与文本描述对齐来桥接，实现在不同视角间的稳健共享语义空间；2) 设计可双向匹配的检索框架（跨视角图像-图像与语言-图像）；3) 构建 GeoLoc 数据集，包含5万余组三元视角（无人机/街景全景/卫星）与文本描述，覆盖36个国家，保证地理与语义对齐；4) 在该数据集上进行预训练以促进跨域泛化和跨模态知识迁移。

Result: 基于 GeoLoc 的预训练显著提升 GeoBridge 的定位准确率；在多项任务与不同域设置上均表现出更好的泛化与跨模态迁移能力，相比传统卫星中心方法更稳健。

Conclusion: 通过语义锚点将多视角与文本统一到共享语义空间，GeoBridge 实现了更灵活、鲁棒的跨视角地理定位；配套的 GeoLoc 数据集为跨模态、多视角研究提供了基础，代码与模型已开源。

Abstract: Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.

</details>


### [75] [VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm](https://arxiv.org/abs/2512.02700)
*Zhenkai Wu,Xiaowen Ma,Zhenliang Ni,Dengming Zhang,Han Shu,Xin Jiang,Xinghao Chen*

Main category: cs.CV

TL;DR: 提出VLM-Pruner：一种训练免调的视觉token裁剪方法，显著减少视觉token（约88.9%裁剪率），同时保持/提升VLM性能并加速端到端推理。核心在于兼顾冗余与空间稀疏：采用“离心式”近到远选择与BSS缓冲准则，配合并行贪心选择与被丢弃token的信息融合。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在移动端部署受限于大量视觉token带来的计算开销。传统基于重要性的裁剪忽视token间冗余，保留了重复信息；冗余感知方法又常忽略空间关系，导致保留token过于分散，目标区域覆盖不足。需要一种既考虑冗余又考虑空间稀疏的裁剪策略，以在高裁剪率下仍保持目标细节与性能。

Method: 提出训练免调的VLM-Pruner：1）离心式（centrifugal）token裁剪范式：从中心向外“近到远”选择，优先保留细粒度目标细节；2）BSS（Buffering for Spatial Sparsity）准则：对空间上距离较远的token延迟选择，避免过早导致空间稀疏；3）并行贪心策略：高效进行token选择；4）信息融合：对被丢弃token的显著信息进行选择性融合到保留token，降低信息损失。

Result: 在5个不同VLM上进行全面对比，在约88.9%的裁剪率下，性能持续优于强基线，同时实现端到端推理加速。

Conclusion: VLM-Pruner在无需训练的前提下，通过兼顾冗余与空间稀疏的裁剪与信息融合，在高裁剪率下仍保持乃至提升VLM性能，并带来推理速度收益，适用于资源受限的设备部署。

Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.

</details>


### [76] [Tissue-mask supported inter-subject whole-body image registration in the UK Biobank -- A method benchmarking study](https://arxiv.org/abs/2512.02702)
*Yasemin Utkueri,Elin Lundström,Håkan Ahlström,Johan Öfverstedt,Joel Kullberg*

Main category: cs.CV

TL;DR: 提出一种性别分层、结合皮下脂肪与肌肉掩膜的全身MR互注册方法，在UK Biobank 4000例上较强基线显著提升Dice与标签误差，且生成更干净的年龄相关性图谱。


<details>
  <summary>Details</summary>
Motivation: UK Biobank拥有大规模全身MR与非影像健康数据，但跨受试者的稳健高精度配准仍不足，限制了将非影像表型与体素/区域影像指标进行空间对齐和关联分析的能力。现有仅基于强度或通用配准方法在全身、多组织尺度上易失配，导致相关性分析噪声大、解剖一致性差。

Method: 提出性别分层的互注册框架：在强度驱动的graph-cut配准中，融合VIBESegmentator生成的皮下脂肪（SAT）与肌肉掩膜作为结构先验，约束与引导配准；并与强度仅方法、uniGradICON、MIRTK对比。评估在4000例、71个组织/结构掩膜上，使用Dice与体素级标签误差频率；并生成年龄与脂肪含量/组织体积的体素级相关图作为应用示例。

Result: 在男性/女性上平均Dice达0.77/0.75。相较强度仅配准提升6个百分点；相较uniGradICON提升9/8个百分点；相较MIRTK提升12/13个百分点；多数组织的标签误差频率降低。采用该方法得到的年龄相关性图更少噪声且解剖对齐更好。

Conclusion: 利用SAT与肌肉掩膜增强的性别分层graph-cut配准显著改进UK Biobank全身MR互注册质量，并提升体素级表型-影像关联分析的稳定性与可解释性。

Abstract: The UK Biobank is a large-scale study collecting whole-body MR imaging and non-imaging health data. Robust and accurate inter-subject image registration of these whole-body MR images would enable their body-wide spatial standardization, and region-/voxel-wise correlation analysis of non-imaging data with image-derived parameters (e.g., tissue volume or fat content). We propose a sex-stratified inter-subject whole-body MR image registration approach that uses subcutaneous adipose tissue- and muscle-masks from the state-of-the-art VIBESegmentator method to augment intensity-based graph-cut registration. The proposed method was evaluated on a subset of 4000 subjects by comparing it to an intensity-only method as well as two previously published registration methods, uniGradICON and MIRTK. The evaluation comprised overlap measures applied to the 71 VIBESegmentator masks: 1) Dice scores, and 2) voxel-wise label error frequency. Additionally, voxel-wise correlation between age and each of fat content and tissue volume was studied to exemplify the usefulness for medical research. The proposed method exhibited a mean dice score of 0.77 / 0.75 across the cohort and the 71 masks for males/females, respectively. When compared to the intensity-only registration, the mean values were 6 percentage points (pp) higher for both sexes, and the label error frequency was decreased in most tissue regions. These differences were 9pp / 8pp against uniGradICON and 12pp / 13pp against MIRTK. Using the proposed method, the age-correlation maps were less noisy and showed higher anatomical alignment. In conclusion, the image registration method using two tissue masks improves whole-body registration of UK Biobank images.

</details>


### [77] [GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding](https://arxiv.org/abs/2512.02715)
*Peirong Zhang,Yidan Zhang,Luxiao Xu,Jinliang Lin,Zonghao Guo,Fengxiang Wang,Xue Yang,Kaiwen Wei,Lei Wang*

Main category: cs.CV

TL;DR: GeoViS将遥感视觉指代从“一步定位”改为“逐步搜索+推理”，用树状线索探索全局图像，并以奖励引导迭代缩小目标范围，在五个基准上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: MLLM在自然图像上可做细粒度视觉指代，但在遥感场景中目标极小且查询含复杂地理关系（相对方位、空间层级、跨远距离语境），导致直接迁移效果差，亟需兼顾全局场景感知与微小目标定位的机制。

Method: 提出GeoViS：把遥感视觉指代重构为树状的“搜索-感知-推理-反馈”过程。模型在全局图上主动探索，逐层生成视觉线索与地理假设，通过多模态感知与空间推理迭代细化；引入基于地理对齐的奖励来引导探索策略，提升对小目标与长距离关系的捕捉能力与可解释性。

Result: 在五个遥感指代基准上，GeoViS在关键指标上持续领先，能精确理解地理关系、定位细微目标，并展现跨域泛化能力与有迹可循的推理过程。

Conclusion: 将指代任务转化为奖励驱动的渐进式视觉搜索能有效应对遥感中小目标与复杂地理关系难题，兼顾全局与细节，优于现有方法且更具可解释与泛化能力。

Abstract: Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.

</details>


### [78] [DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions](https://arxiv.org/abs/2512.02727)
*Yifan Zhou,Takehiko Ohkawa,Guwenxiao Zhou,Kanoko Goto,Takumi Hirose,Yusuke Sekikawa,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 提出DF-Mamba，用状态空间模型与可变形扫描增强全局上下文建模，显著提升3D手部姿态估计在遮挡场景的准确性，速度可比ResNet-50，并在多数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多手重叠、手物交互等强遮挡导致局部特征缺失，传统基于ResNet的CNN难以高效建模全局上下文，影响3D手部姿态估计的鲁棒性与精度。需要一种既能捕捉长程依赖又高效的视觉特征抽取框架。

Method: 以状态空间模型Mamba为核心，提出Deformable Mamba（DF-Mamba）：1）选择性状态建模用于全局上下文聚合；2）可变形状态扫描（deformable scanning）在图像域自适应聚合局部卷积后的特征，选择性保留对全局结构有用的线索；3）作为图像主干替代ResNet进行3D HPE特征提取。

Result: 在五个异构数据集（单手/双手、纯手/手物交互、RGB/深度）上，DF-Mamba优于包括VMamba、Spatial-Mamba在内的最新图像主干；在精度上达SOTA，同时推理速度与ResNet-50相当。

Conclusion: 通过将Mamba的选择性状态建模与可变形扫描结合，DF-Mamba有效弥补CNN对全局上下文建模不足，显著提升遮挡条件下的结构化3D手部姿态估计性能，并保持高效率。

Abstract: Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.

</details>


### [79] [Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone](https://arxiv.org/abs/2512.02737)
*Tristan Amadei,Enric Meinhardt-Llopis,Benedicte Bascle,Corentin Abgrall,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 提出CAEVL，无需UAV训练图像，仅用卫星图像通过专门增强模拟视角域差，实现GNSS拒止环境下UAV图像定位；在新发布的ViLD上达到与成对数据方法相当的表现，具备强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有UAV→卫星匹配方法依赖大规模成对UAV-卫星数据，获取昂贵且常不可得，限制在GNSS拒止环境中的实用性，需要一种不依赖成对训练数据且能跨域泛化的方案。

Method: 提出仅用卫星参考图像进行训练的范式：设计专用数据增强策略来模拟卫星视角到真实UAV视角的视觉域迁移；并提出高效模型CAEVL以利用该范式进行图像定位学习。

Result: 在作者新构建并公开的真实UAV图像数据集ViLD上验证，方法在无需成对训练数据的条件下取得与需成对数据的SOTA方法相竞争的性能。

Conclusion: 通过域迁移增强与CAEVL模型，可在无UAV训练数据情况下实现有效的图像定位，展现强泛化与实用价值，并为GNSS拒止环境下UAV自主定位提供可行路径。

Abstract: Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.

</details>


### [80] [Reasoning-Aware Multimodal Fusion for Hateful Video Detection](https://arxiv.org/abs/2512.02743)
*Shuonan Yang,Tailin Chen,Jiangbei Yue,Guangliang Cheng,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出RAMF框架，结合局部-全局融合与语义交叉注意，并引入“对抗式推理”三阶段生成解释，显著提升视频仇恨言论检测（Macro-F1+3%，仇恨召回+7%）。


<details>
  <summary>Details</summary>
Motivation: 多模态视频中的仇恨言论具备跨模态、时序与语境依赖的细粒度特征，现有方法难以充分融合模态语义与理解隐晦仇恨意图，导致泛化与召回不足。

Method: 提出Reasoning-Aware Multimodal Fusion (RAMF)。(1) Local-Global Context Fusion（LGCF）捕捉局部显著线索与全局时序结构；(2) Semantic Cross Attention（SCA）实现细粒度跨模态语义交互；(3) 对抗式推理：用视觉-语言模型生成三类文本视角——客观描述、基于仇恨假设的推断、基于非仇恨假设的推断，以丰富语境理解并作为模型输入/监督信号。

Result: 在两个真实世界仇恨视频数据集上，相比SOTA，Macro-F1提升约3%，仇恨类召回提升约7%，表现出更强泛化与鲁棒性。

Conclusion: 结合LGCF与SCA的多模态融合，叠加三阶段“对抗式推理”可有效捕捉细粒度、语境化的仇恨意图，显著提升视频仇恨检测性能；代码将在匿名期后开源。

Abstract: Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.

</details>


### [81] [AttMetNet: Attention-Enhanced Deep Neural Network for Methane Plume Detection in Sentinel-2 Satellite Imagery](https://arxiv.org/abs/2512.02751)
*Rakib Ahsan,MD Sadik Hossain Shanto,Md Sultanul Arifin,Tanzima Hashem*

Main category: cs.CV

TL;DR: 提出AttMetNet：将NDMI与注意力增强U-Net融合，用焦点损失处理类不平衡，在Sentinel-2 B11/B12上实现更稳健的甲烷羽流检测，显著降低误报并提升精度、召回与IoU。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，需及时可靠地检测排放。现有以B11/B12差异/比值的传统方法误报高、需专家复核；CNN方法改进有限，缺乏对甲烷特征的显式优先机制，且面对多样地表背景易混淆，类不平衡严重。

Method: 构建甲烷感知的注意力增强U-Net，将对羽流敏感的NDMI作为引导特征与编码-解码网络融合；通过注意力模块选择性放大甲烷吸收相关特征并抑制背景噪声；使用焦点损失缓解正样本稀缺与像素级稀疏导致的类不平衡；在真实甲烷羽流数据上训练与评估。

Result: 在广泛实验中，相比近期方法，AttMetNet实现更低误报率，更佳的Precision-Recall平衡以及更高的IoU。

Conclusion: 通过将NDMI先验与注意力机制整合进U-Net，AttMetNet能更稳健地从Sentinel-2影像中检测甲烷羽流，适用于真实场景，并在多项指标上优于现有方法。

Abstract: Methane is a powerful greenhouse gas that contributes significantly to global warming. Accurate detection of methane emissions is the key to taking timely action and minimizing their impact on climate change. We present AttMetNet, a novel attention-enhanced deep learning framework for methane plume detection with Sentinel-2 satellite imagery. The major challenge in developing a methane detection model is to accurately identify methane plumes from Sentinel-2's B11 and B12 bands while suppressing false positives caused by background variability and diverse land cover types. Traditional detection methods typically depend on the differences or ratios between these bands when comparing the scenes with and without plumes. However, these methods often require verification by a domain expert because they generate numerous false positives. Recent deep learning methods make some improvements using CNN-based architectures, but lack mechanisms to prioritize methane-specific features. AttMetNet introduces a methane-aware architecture that fuses the Normalized Difference Methane Index (NDMI) with an attention-enhanced U-Net. By jointly exploiting NDMI's plume-sensitive cues and attention-driven feature selection, AttMetNet selectively amplifies methane absorption features while suppressing background noise. This integration establishes a first-of-its-kind architecture tailored for robust methane plume detection in real satellite imagery. Additionally, we employ focal loss to address the severe class imbalance arising from both limited positive plume samples and sparse plume pixels within imagery. Furthermore, AttMetNet is trained on the real methane plume dataset, making it more robust to practical scenarios. Extensive experiments show that AttMetNet surpasses recent methods in methane plume detection with a lower false positive rate, better precision recall balance, and higher IoU.

</details>


### [82] [Rethinking Surgical Smoke: A Smoke-Type-Aware Laparoscopic Video Desmoking Method and Dataset](https://arxiv.org/abs/2512.02780)
*Qifan Liang,Junlin Li,Zhen Han,Xihao Wang,Zhongyuan Wang,Bin Mei*

Main category: cs.CV

TL;DR: 论文提出首个“烟型感知”的腹腔镜视频去烟网络STANet，将手术烟雾按运动模式划分为扩散烟与环境烟，并据此在分割与重建中分类型处理；通过粗到细的解缠模块与跨注意力实现两类烟雾掩模的准确解耦，配合新构建的大规模带烟型标注的合成数据集，在多项指标与下游任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有腹腔镜去烟方法忽略了不同烟雾类型（由运动模式差异导致）的时空特性差别，导致去烟不足或副作用（细节丢失、伪影）。需要一个能识别并分类型处理烟雾的模型，以提升去烟效果与对外科下游任务的泛化。

Method: 1) 定义两类烟：扩散烟(Diffusion)与环境烟(Ambient)。2) 设计双分支：a) 烟掩模分割子网，基于注意加权的掩模聚合，同时预测烟掩模与烟型；并嵌入粗到细的解缠模块，通过烟型感知的跨注意力在“未缠结/已缠结”区域间交互，得到更准确、彼此解耦的两类烟掩模。b) 无烟视频重建子网，利用两类掩模引导，对烟特征进行针对性去烟与视频重建。3) 构建首个带烟型标注的大规模合成视频去烟数据集。4) 端到端训练与评估。

Result: 在合成与真实腹腔镜数据上，相较SOTA方法取得更高的图像/视频质量指标，并在多种下游外科任务（如器官/器械分割、导航等）上表现出更强的泛化与鲁棒性。

Conclusion: 烟型感知建模和掩模解缠是提升腹腔镜视频去烟的关键；STANet通过分类型掩模与重建显著优于现有方法，并凭借新数据集推动领域基准建设。

Abstract: Electrocautery or lasers will inevitably generate surgical smoke, which hinders the visual guidance of laparoscopic videos for surgical procedures. The surgical smoke can be classified into different types based on its motion patterns, leading to distinctive spatio-temporal characteristics across smoky laparoscopic videos. However, existing desmoking methods fail to account for such smoke-type-specific distinctions. Therefore, we propose the first Smoke-Type-Aware Laparoscopic Video Desmoking Network (STANet) by introducing two smoke types: Diffusion Smoke and Ambient Smoke. Specifically, a smoke mask segmentation sub-network is designed to jointly conduct smoke mask and smoke type predictions based on the attention-weighted mask aggregation, while a smokeless video reconstruction sub-network is proposed to perform specially desmoking on smoky features guided by two types of smoke mask. To address the entanglement challenges of two smoke types, we further embed a coarse-to-fine disentanglement module into the mask segmentation sub-network, which yields more accurate disentangled masks through the smoke-type-aware cross attention between non-entangled and entangled regions. In addition, we also construct the first large-scale synthetic video desmoking dataset with smoke type annotations. Extensive experiments demonstrate that our method not only outperforms state-of-the-art approaches in quality evaluations, but also exhibits superior generalization across multiple downstream surgical tasks.

</details>


### [83] [LumiX: Structured and Coherent Text-to-Intrinsic Generation](https://arxiv.org/abs/2512.02781)
*Xu Han,Biao Zhang,Xiangjun Tang,Xianzhi Li,Peter Wonka*

Main category: cs.CV

TL;DR: LumiX 是一个面向文本到“内在属性”(intrinsics)的结构化扩散框架，可一次性生成反照率、照度、法线、深度和最终颜色等多通道、物理一致的场景表征。核心在于共享查询的注意力(确保跨图一致性)与张量化的LoRA(高效建模跨图关系)，从而实现稳定的联合训练与统一生成，显著提升一致性与用户偏好，并支持图像条件的内在分解。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像方法缺乏对场景物理内在属性的显式、统一建模，导致跨通道(如法线/深度/反照率/照度/最终颜色)不一致，难以进行可控渲染、编辑和分解。需要一个既物理一致、又能在多个内在图之间保持结构耦合的生成框架。

Method: 提出结构化扩散：在同一扩散过程内联合生成多种内在图。关键组件包括：1) Query-Broadcast Attention：在每个自注意力层共享查询向量到所有地图，使结构线索一致并跨通道对齐；2) Tensor LoRA：以张量形式对多图通道进行参数高效的适配，显式建模跨图关系，降低训练成本并提升稳定性；统一训练可处理文本条件，也可扩展到图像条件做内在分解。

Result: 与SOTA相比，LumiX在对齐度提升23%，用户偏好分数从-0.41提升到0.19；生成结果在几何/材质/光照等多通道上更一致、更具物理意义；同一框架还能进行图像条件的内在分解。

Conclusion: 结构化的联合扩散加上共享查询注意力与张量LoRA，有效实现了跨内在图的物理一致生成与高效训练，兼具文本生成与图像分解能力，推动了可控、可解释的生成式视觉建模。

Abstract: We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.

</details>


### [84] [TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking](https://arxiv.org/abs/2512.02789)
*Tang Haonan,Chen Yanjun,Jiang Lezhi*

Main category: cs.CV

TL;DR: TrackNetV5 通过显式编码运动方向与残差式时空细化，解决前代在遮挡与方向歧义上的瓶颈，在保持实时性的同时刷新小球追踪SOTA（F1 0.9859，ACC 0.9733）。


<details>
  <summary>Details</summary>
Motivation: 现有小而快目标（如体育比赛中的球）追踪面临两点：1）V1–V3仅依赖外观视觉，遮挡时易丢失；2）V4虽引入运动信息，但使用绝对差分导致丢失方向极性，产生方向歧义。需要一种既能保留运动方向先验、又能在遮挡下稳健恢复目标的架构。

Method: 提出TrackNetV5，包含两大机制：1）运动方向解耦（MDD）模块，将时间动态分解为带符号的极性场，显式编码“是否运动”和“运动方向/轨迹”；2）残差驱动的时空细化（R-STR）头，基于粗到细范式与因子化时空上下文的Transformer，估计校正残差，以恢复被遮挡目标。整体在V4基础上轻量增加计算量。

Result: 在TrackNetV2数据集上，取得F1=0.9859、Accuracy=0.9733，明显优于前代；相对V4只增加约3.7% FLOPs，仍可实时推理。

Conclusion: 显式方向建模（MDD）与残差式时空细化（R-STR）协同缓解遮挡与方向歧义，提升精度且几乎不牺牲效率，为体育小目标追踪提供新的强基线。

Abstract: The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.

</details>


### [85] [UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits](https://arxiv.org/abs/2512.02790)
*Keming Ye,Zhipeng Huang,Canmiao Fu,Qingyang Liu,Jiani Cai,Zheqi Lv,Chen Li,Jing Lyu,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 提出轻量化数据流水线与校验模型Qwen-Verify，构建千万级图像编辑数据集UnicEdit-10M，并发布覆盖空间与知识推理的评测UnicBench与新指标，用于诊断多模态编辑模型弱点。


<details>
  <summary>Details</summary>
Motivation: 开源图像编辑模型与闭源模型之间性能差距扩大，根因在于缺乏大规模高质量数据与能细粒度诊断多样编辑行为弱点的通用基准；现有数据构造在规模与质量间权衡明显：人工高质不易扩展，自动流水线易积累误差与噪声。

Method: 以端到端模型替代多工具链，并引入统一后验验证阶段；训练7B双任务专家模型Qwen-Verify，用于失败检测与指令重述（recaptioning），实现可扩展质控；由此生成覆盖基础与复杂编辑的UnicEdit-10M数据集；构建UnicBench基准，显式评测空间与知识驱动的推理；提出Non-edit Consistency与Reasoning Accuracy等新指标实现细粒度诊断。

Result: 得到规模达一千万的多样图像编辑数据集UnicEdit-10M与综合基准UnicBench；通过这些资源对主流模型进行系统评测，揭示其在空间与知识推理、编辑一致性等方面的不足。

Conclusion: 轻量化数据生成与验证框架结合专用质控模型可在保证质量前提下实现大规模数据构造；UnicBench与新指标可更全面诊断图像编辑模型弱点，为后续研究与模型改进提供明确方向。

Abstract: With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \textit{Non-edit Consistency} and \textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.

</details>


### [86] [HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval](https://arxiv.org/abs/2512.02792)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Haokun Wen,Weili Guan*

Main category: cs.CV

TL;DR: 提出HUD框架，通过分层不确定性与消歧策略改进由“参考视频+文本修改”的组合视频检索，实现细粒度对齐与目标消歧，在CVR与CIR三基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: CVR中视频信息密度通常高于文本，现有方法未显式建模这种差异，导致（1）修改主体指代歧义（如指代谁/哪个物体不清）；（2）缺乏对细节语义的关注，影响组合特征学习与检索精度。

Method: 提出分层不确定性感知的HUD框架，包含三模块：a) 全局代词消歧（Holistic Pronoun Disambiguation），通过跨模态全局交互利用视频富语义解决指代不清；b) 原子级不确定性建模（Atomistic Uncertainty Modeling），在细粒度层面对词/区域建模不确定性，提升对细节的鲁棒关注；c) 自上而下的全局到原子对齐（Holistic-to-Atomistic Alignment），先做全局重叠语义交互再进行原子级跨模态精对齐，实现精确的组合特征表示。

Result: HUD在组合视频检索与组合图像检索三套基准数据集上取得SOTA性能，表明其在对象消歧与细节对齐方面有效。

Conclusion: 利用视频与文本信息密度差异进行分层建模（全局消歧+原子级不确定性与对齐）能显著提升CVR/CIR的组合特征学习与检索效果；方法通用且代码已开源。

Abstract: Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.

</details>


### [87] [IC-World: In-Context Generation for Shared World Modeling](https://arxiv.org/abs/2512.02793)
*Fan Wu,Jiacheng Wei,Ruibo Li,Yi Xu,Junyou Li,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出IC-World：一种基于大视频模型的并行共享世界生成框架，利用上下文内生成并结合RL（GRPO）与几何/运动一致性奖励，使多视角视频在同一世界中保持一致，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型能生成丰富动态环境，但在“共享世界建模”场景下——从多张代表同一世界不同相机姿态的图像生成多段视频——常出现跨视角几何与物体运动不一致的问题。需要一种机制同时生成并约束多视角视频在同一场景下保持一致性。

Method: 1) 设计IC-World框架：激活大视频模型的“上下文内生成（in-context）”能力，实现对多输入图像的并行视频生成；2) 使用强化学习微调，采用Group Relative Policy Optimization（GRPO）；3) 构建两类奖励模型：场景级几何一致性奖励与物体级运动一致性奖励，引导生成的多视频在跨视角保持一致。

Result: 在广泛实验中，IC-World在几何一致性与运动一致性两方面显著优于最新方法，支持高质量的多视角共享世界视频生成。

Conclusion: IC-World首次系统性地用视频世界模型解决共享世界建模问题，通过并行生成与RL一致性约束，实现跨视角几何和运动的协同一致，并在实验中取得领先性能。

Abstract: Video-based world models have recently garnered increasing attention for their ability to synthesize diverse and dynamic visual environments. In this paper, we focus on shared world modeling, where a model generates multiple videos from a set of input images, each representing the same underlying world in different camera poses. We propose IC-World, a novel generation framework, enabling parallel generation for all input images via activating the inherent in-context generation capability of large video models. We further finetune IC-World via reinforcement learning, Group Relative Policy Optimization, together with two proposed novel reward models to enforce scene-level geometry consistency and object-level motion consistency among the set of generated videos. Extensive experiments demonstrate that IC-World substantially outperforms state-of-the-art methods in both geometry and motion consistency. To the best of our knowledge, this is the first work to systematically explore the shared world modeling problem with video-based world models.

</details>


### [88] [PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation](https://arxiv.org/abs/2512.02794)
*Fan Wu,Cheng Chen,Zhoujie Fu,Jiacheng Wei,Yi Xu,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出PhyCustom，对扩散模型进行微调以实现物理概念的可控生成；通过等距损失与解耦损失显式注入物理知识，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像定制方法擅长风格、形状等“具体”概念，但无法可靠表达“物理”属性（如材料、力学、光学等）。根因是训练中未显式引入物理知识，导致即使提示词包含物理术语，生成也难以满足对应物理性质。

Method: 提出微调框架PhyCustom，包含两类正则化损失：1) 等距（isometric）损失：引导扩散模型在特征空间中学习并保持物理概念之间的几何/度量关系，从而“激活”物理概念理解；2) 解耦（decouple）损失：抑制互不相关概念的混学与耦合，避免风格/形状等与物理属性相互干扰。以多样化数据集进行训练与评估。

Result: 在作者构建的多样数据集和基准上，PhyCustom在物理定制的定量与定性指标上均超越主流与SOTA方法，生成结果更准确反映物理属性。

Conclusion: 显式物理正则化（等距与解耦损失）可显著提升扩散模型的物理定制能力；该框架为在生成模型中注入物理知识提供了有效途径。

Abstract: Recent diffusion-based text-to-image customization methods have achieved significant success in understanding concrete concepts to control generation processes, such as styles and shapes. However, few efforts dive into the realistic yet challenging customization of physical concepts. The core limitation of current methods arises from the absence of explicitly introducing physical knowledge during training. Even when physics-related words appear in the input text prompts, our experiments consistently demonstrate that these methods fail to accurately reflect the corresponding physical properties in the generated results. In this paper, we propose PhyCustom, a fine-tuning framework comprising two novel regularization losses to activate diffusion model to perform physical customization. Specifically, the proposed isometric loss aims at activating diffusion models to learn physical concepts while decouple loss helps to eliminate the mixture learning of independent concepts. Experiments are conducted on a diverse dataset and our benchmark results demonstrate that PhyCustom outperforms previous state-of-the-art and popular methods in terms of physical customization quantitatively and qualitatively.

</details>


### [89] [Defense That Attacks: How Robust Models Become Better Attackers](https://arxiv.org/abs/2512.02830)
*Mohamed Awad,Mahmoud Akrm,Walid Gomaa*

Main category: cs.CV

TL;DR: 论文发现一个悖论：经过对抗训练的模型在生成可迁移对抗样本方面更强，从而提升了攻击的跨模型效果，带来新的生态风险。


<details>
  <summary>Details</summary>
Motivation: 对抗训练提升鲁棒性已成共识，但其对“攻击可迁移性”的影响缺乏系统研究。作者怀疑对抗训练可能无意中让对抗样本更容易跨模型生效，因此需要大规模实证验证。

Method: 构建由36个模型组成的多样化“模型动物园”，涵盖CNN和ViT；在这些模型上分别进行标准训练与对抗训练；系统评估从不同源模型生成的对抗扰动，在其他目标模型上的迁移成功率，进行全面对比分析并发布代码与模型以便复现。

Result: 对抗训练的源模型生成的扰动在跨模型迁移上明显优于标准训练源模型；这一现象在不同架构（CNN/ViT）与多种设置下均成立，呈现稳定的提升，揭示出鲁棒性与攻击可迁移性之间的悖论。

Conclusion: 鲁棒性评测不应仅关注模型抵御迁移攻击的能力，还应评估其作为“攻击源”的可迁移性倾向；对抗训练可能引入新的生态风险，需要在方法设计与基准评测中同时考虑“防守强度”和“攻击外溢”。

Abstract: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.

</details>


### [90] [ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning](https://arxiv.org/abs/2512.02835)
*Yifan Li,Yingda Yin,Lingting Zhu,Weikai Chen,Shengju Qian,Xin Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: ReVSeg将“推理型”视频目标分割拆成多步显式决策：先理解语义，再选取时间证据，最后做空间定位，并用强化学习优化整条链路，取得SOTA且过程可解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法把与动态、因果、时序交互相关的复杂推理压缩进潜在嵌入，导致推理链不可见、难以优化；需要一种对齐VLM原生能力、可解释且可优化的显式推理机制。

Method: 提出ReVSeg：在预训练视觉语言模型的原生接口上，将视频目标分割的推理分解为三步顺序操作——(1)语义解释：解析文本查询以明确目标语义与条件；(2)时间证据选择：在视频帧/片段中定位与查询相关的关键时刻与因果线索；(3)空间落地：在选定帧上进行像素级分割。进一步采用强化学习以结果为导向优化多步策略，使各步骤的决策质量自我改进，并输出可追踪的推理轨迹。

Result: 在标准视频目标分割基准上达到SOTA表现，并展示了可解释的多步推理过程。

Conclusion: 显式分解并配合强化学习能有效提升推理型视频目标分割的准确性与可解释性；利用VLM的原生接口进行顺序决策是高效可扩展的方案。

Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .

</details>


### [91] [Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?](https://arxiv.org/abs/2512.02846)
*Manuel Benavent-Lledo,Konstantinos Bacharidis,Victoria Manousaki,Konstantinos Papoutsakis,Antonis Argyros,Jose Garcia-Rodriguez*

Main category: cs.CV

TL;DR: 提出AAG方法：仅凭单帧的RGB+深度与语言/先验上下文进行动作预判，在多数据集上与时序聚合方法竞争。


<details>
  <summary>Details</summary>
Motivation: 人类常能通过一瞬间并结合上下文预测即将发生的动作；现有方法多依赖长时序视频聚合，成本高且可能非必要。作者探究：能否用替代模态与先验上下文在单帧层面实现可比的动作预判。

Method: 提出AAG（Action Anticipation at a Glimpse）：1）从单帧提取RGB视觉特征并融合深度信息以加强空间推理；2）引入“长期上下文”先验，两种来源：a) 由视觉-语言模型生成的文本动作摘要；b) 单帧动作识别器的预测；3）将多模态特征（RGB+深度+上下文）融合，用于预测即将发生的动作；与依赖时序聚合的基线比较。

Result: 在IKEA-ASM、Meccano、Assembly101三个装配类数据集中，单帧多模态的AAG在动作预判上达到与视频时序聚合基线和SOTA相当的性能，显示单帧+上下文可显著缩小与长时序方法的差距。

Conclusion: 在动作预判任务中，时序视频聚合并非总是必须；通过单帧的空间线索与语言/预测先验的上下文融合，能取得有竞争力的效果。方法为低延迟、低算力的预判提供了可行路径，效果依赖任务复杂度与上下文质量。

Abstract: Anticipating actions before they occur is a core challenge in action understanding research. While conventional methods rely on extracting and aggregating temporal information from videos, as humans we can often predict upcoming actions by observing a single moment from a scene, when given sufficient context. Can a model achieve this competence? The short answer is yes, although its effectiveness depends on the complexity of the task. In this work, we investigate to what extent video aggregation can be replaced with alternative modalities. To this end, based on recent advances in visual feature extraction and language-based reasoning, we introduce AAG, a method for Action Anticipation at a Glimpse. AAG combines RGB features with depth cues from a single frame for enhanced spatial reasoning, and incorporates prior action information to provide long-term context. This context is obtained either through textual summaries from Vision-Language Models, or from predictions generated by a single-frame action recognizer. Our results demonstrate that multimodal single-frame action anticipation using AAG can perform competitively compared to both temporally aggregated video baselines and state-of-the-art methods across three instructional activity datasets: IKEA-ASM, Meccano, and Assembly101.

</details>


### [92] [Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study](https://arxiv.org/abs/2512.02850)
*Vishal Dubey,Pallavi Tyagi*

Main category: cs.CV

TL;DR: 研究首次系统评估身份保持型AIGC（IP-AIGC）在人脸（尤其印度/南亚人群）检测上的稳健性与公平性。作者构建印度聚焦的数据与两套商业生成器生成的保身份测试集，评测两种SOTA检测器在预训练与微调后的跨生成器与人群内表现。结果显示：微调虽在源域显著提升，但在保身份的印度测试集上大幅退化，揭示对训练生成器特征的过拟合与对身份保持编辑的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC检测多忽视身份保持编辑场景与少数群体（印度/南亚）公平性；缺乏系统的跨生成器泛化评估与面向印度人群的基准，难以指导实际风控与内容审核。

Method: 从FairFD与HAV-DF中构建印度聚焦训练划分；使用商业Web UI（Gemini、ChatGPT）按身份保持提示生成两套保身份测试集（图像与视频）。选取两种SOTA检测器（AIDE、Effort）在预训练与微调两种设置下评估，报告AUC、AP、EER、Accuracy，并比较跨生成器与人群内表现。

Result: 微调在源域显著提升（如Effort AUC 0.739→0.944；AIDE EER 0.484→0.259），但在印度IP-AIGC的保留测试集上显著下降（AIDE AUC 0.923→0.563；Effort 0.740→0.533），表明对训练生成器线索的过拟合。对非IP的HIDF图像，预训练性能仍高，说明脆弱性特指身份保持编辑而非一般分布偏移。

Conclusion: IP-AIGC-Indian构成具有实践意义且具挑战性的检测场景。需要面向身份表示保持的自适应策略与印度语境感知的基准建设，以弥合AIGC检测在跨生成器与人群间的泛化鸿沟。

Abstract: Modern image editors can produce identity-preserving AIGC (IP-AIGC), where the same person appears with new attire, background, or lighting. The robustness and fairness of current detectors in this regime remain unclear, especially for under-represented populations. We present what we believe is the first systematic study of IP-AIGC detection for Indian and South-Asian faces, quantifying cross-generator generalization and intra-population performance. We assemble Indian-focused training splits from FairFD and HAV-DF, and construct two held-out IP-AIGC test sets (HIDF-img-ip-genai and HIDF-vid-ip-genai) using commercial web-UI generators (Gemini and ChatGPT) with identity-preserving prompts. We evaluate two state-of-the-art detectors (AIDE and Effort) under pretrained (PT) and fine-tuned (FT) regimes and report AUC, AP, EER, and accuracy. Fine-tuning yields strong in-domain gains (for example, Effort AUC 0.739 to 0.944 on HAV-DF-test; AIDE EER 0.484 to 0.259), but consistently degrades performance on held-out IP-AIGC for Indian cohorts (for example, AIDE AUC 0.923 to 0.563 on HIDF-img-ip-genai; Effort 0.740 to 0.533), which indicates overfitting to training-generator cues. On non-IP HIDF images, PT performance remains high, which suggests a specific brittleness to identity-preserving edits rather than a generic distribution shift. Our study establishes IP-AIGC-Indian as a challenging and practically relevant scenario and motivates representation-preserving adaptation and India-aware benchmark curation to close generalization gaps in AIGC detection.

</details>


### [93] [RFOP: Rethinking Fusion and Orthogonal Projection for Face-Voice Association](https://arxiv.org/abs/2512.02860)
*Abdul Hannan,Furqan Malik,Hina Jabbar,Syed Suleman Sadiq,Mubashir Noman*

Main category: cs.CV

TL;DR: 多语种人脸-声音关联挑战（FAME 2026）中，作者通过改进模态融合与正交投影，突出两模态的相关语义，针对英-德数据取得33.1%的EER，排名第3。


<details>
  <summary>Details</summary>
Motivation: 在多语种（英语-德语）场景下，人脸与声音跨模态关联更困难：语言差异、说话人变异与视觉差异会削弱关联模型的鲁棒性，需要专门的方法提升跨语言泛化与语义对齐。

Method: 重访并改进两点：1）融合策略：强调提取并融合人脸与语音中与身份相关的语义信息；2）正交投影：通过将模态特定与共享信息解耦，利用正交约束把无关或冗余成分分离，使公共子空间对齐更纯净，从而提升跨模态匹配。

Result: 在FAME 2026挑战的英-德数据划分上表现良好，等错误率（EER）为33.1%，综合成绩排名第3。

Conclusion: 专注相关语义的信息融合与正交投影解耦能在多语种人脸-声音关联任务中带来实证收益，但当前EER仍较高，仍有改进空间（如更强的跨语种对齐、数据多样化和更鲁棒的判别学习）。

Abstract: Face-voice association in multilingual environment challenge 2026 aims to investigate the face-voice association task in multilingual scenario. The challenge introduces English-German face-voice pairs to be utilized in the evaluation phase. To this end, we revisit the fusion and orthogonal projection for face-voice association by effectively focusing on the relevant semantic information within the two modalities. Our method performs favorably on the English-German data split and ranked 3rd in the FAME 2026 challenge by achieving the EER of 33.1.

</details>


### [94] [MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration](https://arxiv.org/abs/2512.02867)
*Yaqi Wang,Zhi Li,Chengyu Wu,Jun Liu,Yifan Zhang,Jialuo Chen,Jiaxue Ni,Qian Luo,Jin Liu,Can Han,Changkai Ji,Zhi Qin Tan,Ajo Babu George,Liangyu Chen,Qianni Zhang,Dahong Qian,Shuai Wang,Huiyu Zhou*

Main category: cs.CV

TL;DR: MICCAI 2025 的 STSR 挑战聚焦牙科 CBCT 与口内扫描（IOS）的半监督分割与跨模态刚性配准，提供有限标注+大量未标注数据，吸引多队用开源深度SSL方案取得SOTA成绩；数据与代码完全开源以促复现。


<details>
  <summary>Details</summary>
Motivation: 数字牙科依赖CBCT与IOS，但牙髓管分割与跨模态配准受限于标注稀缺与模态差异；缺乏标准基准阻碍方法比较与进展。因此需要一个基于半监督学习的公开挑战来推动该领域发展与可比性。

Method: 搭建双任务挑战：1) CBCT牙齿与牙髓管半监督分割；2) CBCT-IOS半监督刚性配准。提供多模态、不同FOV/分辨率的数据集（IOS：60标注/640未标注；CBCT：30标注/250未标注）。评测中，分割侧主流采用nnU-Net与类Mamba状态空间模型，结合伪标签与一致性正则；配准侧结合PointNetLK、可微SVD、几何增强，并以神经-经典混合精炼。

Result: 分割在隐藏测试集上达到Dice 0.967、Instance Affinity 0.738；配准通过PointNetLK+可微SVD与增强在模态差异下实现高精度对齐，少量标注亦有效。多支队伍开源其SSL管线。

Conclusion: 半监督学习在数字牙科的分割与跨模态配准任务中有效，可在少量标注下取得强性能。该挑战与开源资源为未来方法比较、复现与进一步研究提供了标准基准。

Abstract: Cone-Beam Computed Tomography (CBCT) and Intraoral Scanning (IOS) are essential for digital dentistry, but annotated data scarcity limits automated solutions for pulp canal segmentation and cross-modal registration. To benchmark semi-supervised learning (SSL) in this domain, we organized the STSR 2025 Challenge at MICCAI 2025, featuring two tasks: (1) semi-supervised segmentation of teeth and pulp canals in CBCT, and (2) semi-supervised rigid registration of CBCT and IOS. We provided 60 labeled and 640 unlabeled IOS samples, plus 30 labeled and 250 unlabeled CBCT scans with varying resolutions and fields of view. The challenge attracted strong community participation, with top teams submitting open-source deep learning-based SSL solutions. For segmentation, leading methods used nnU-Net and Mamba-like State Space Models with pseudo-labeling and consistency regularization, achieving a Dice score of 0.967 and Instance Affinity of 0.738 on the hidden test set. For registration, effective approaches combined PointNetLK with differentiable SVD and geometric augmentation to handle modality gaps; hybrid neural-classical refinement enabled accurate alignment despite limited labels. All data and code are publicly available at https://github.com/ricoleehduu/STS-Challenge-2025 to ensure reproducibility.

</details>


### [95] [Taming Camera-Controlled Video Generation with Verifiable Geometry Reward](https://arxiv.org/abs/2512.02870)
*Zhaoqing Wang,Xiaobo Xia,Zhuolin Bie,Jinlin Liu,Dongdong Yu,Jia-Wang Bian,Changhu Wang*

Main category: cs.CV

TL;DR: 提出一种针对视频扩散模型的在线强化学习后训练框架，通过可验证的几何奖励在片段级优化相机控制，显著优于仅用SFT的基线。


<details>
  <summary>Details</summary>
Motivation: 现有相机可控视频生成多依赖SFT，缺乏在线RL后训练，导致相机控制精度与几何一致性有限，奖励稀疏、优化低效。

Method: 在预训练视频生成器上进行在线RL：为生成视频与参考视频同时估计3D相机轨迹，将轨迹划分为短片段，计算片段相对位姿并与参考对齐；以片段对齐分数作为密集几何奖励，缓解稀疏性并提升优化效率；同时构建包含大幅度相机运动与多样主体动态的数据集以支撑训练与评测。

Result: 在多个指标上（相机控制准确度、几何一致性、画质）显著优于SFT基线，实验广泛验证了方法有效性。

Conclusion: 在线RL结合片段级可验证几何奖励可显著提升相机可控视频生成的精度与稳定性，相较单纯SFT更具优势。

Abstract: Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.

</details>


### [96] [MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm](https://arxiv.org/abs/2512.02895)
*Wei Chen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Zide Liu,Xuhao Pan,Chang Ren,Xudong Rao,Chenfeng Wang,Tao Wei,Chengjun Yu,Pengfei Yu,Yufei Zheng,Chunpeng Zhou,Pan Zhou,Xuhan Zhu*

Main category: cs.CV

TL;DR: MindGPT-4ov 提出一套针对多模态大模型的通用后训练范式，从数据生成、监督微调到多目标强化学习与高效部署全链路优化，在MMBench、MMStar、MathVision、MathVista等基准上达到或超越SOTA，并以较低成本实现更强的泛化与行业落地。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在跨域泛化、推理能力、多目标权衡（准确性、简洁性、感知保持、探索多样性）以及低成本高效部署方面存在不足，缺乏系统性的后训练与工程化范式。

Method: 三大核心：1）信息密度驱动的数据生成，结合双维度树状标签体系，实现高质量跨域数据自动化生产；2）协同课程式SFT，在注入垂直知识的同时保持通用能力；3）混合式多模态强化学习，同时优化推理、探索多样性、感知保持和回答简洁度。工程侧：5D并行训练、算子优化与推理量化，降低训练/适配成本并提升吞吐。

Result: 在MMBench、MMStar、MathVision、MathVista等多项基准上优于SOTA；在垂直领域任务中提供更佳交互体验；证明所提范式可提升MLLM基础与泛化能力并具备低成本迁移性。

Conclusion: MindGPT-4ov给出面向MLLM的通用后训练与部署范式，兼顾数据、SFT、RL与系统优化，能以较低成本带来强泛化与推理能力，适用于多种基座模型；基于Qwen3-VL的变体将开源模型、数据与代码，促进社区发展。

Abstract: We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.

</details>


### [97] [Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models](https://arxiv.org/abs/2512.02897)
*Pierpaolo Serio,Giulio Pisaneschi,Andrea Dan Ryals,Vincenzo Infantino,Lorenzo Gentilini,Valentina Donzella,Lorenzo Pollini*

Main category: cs.CV

TL;DR: 研究系统分析不同LiDAR到图像的2D投影方式，在结合前沿视觉基础模型时对度量式地点识别性能的影响，并通过模块化检索管线隔离投影本身的作用。结果显示，精心设计的投影在判别力、鲁棒性与实时性方面可显著提升，并能在实际策略中替代端到端3D学习。


<details>
  <summary>Details</summary>
Motivation: 当前LiDAR地点识别常依赖端到端3D学习，计算与数据需求高、部署复杂；而将点云投影为2D图像再用强大的视觉基础模型或许更高效，但不同投影方式对性能影响缺乏系统量化与可复现比较。

Method: 提出可控的模块化检索管线，固定骨干网络、特征聚合与评测协议，仅改变LiDAR到图像的2D投影；在多数据集与多场景下，使用统一的几何/结构通道设定，比较不同投影在判别力、环境变化鲁棒性与实时性上的差异；并将最佳方案集成到实际地点识别策略中进行验证。

Result: 识别出决定性能的关键投影特性（如通道设计、几何保持、结构纹理化等），证明某些精心设计的投影能在多数据集上取得更强判别性与鲁棒性，并满足实时要求；在真实部署的地点识别策略中同样取得实用收益。

Conclusion: 2D投影选择对基于视觉基础模型的LiDAR地点识别至关重要；得当的投影可作为端到端3D学习的有效替代，兼顾精度、鲁棒性与实时性，具有实际部署价值。

Abstract: This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model. We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself. Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy. Experiments with different datasets, including integration into an operational place recognition policy, validate the practical relevance of these findings and demonstrate that carefully designed projections can serve as an effective surrogate for end-to-end 3-D learning in LiDAR place recognition.

</details>


### [98] [Glance: Accelerating Diffusion Models with 1 Sample](https://arxiv.org/abs/2512.02899)
*Zhuobai Dong,Rui Zhao,Songjie Wu,Junchao Yi,Linjie Li,Zhengyuan Yang,Lijuan Wang,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 提出一种相位感知的扩散模型加速方法：在早期语义阶段小幅加速、在后期冗余阶段大幅加速，借助两组轻量LoRA适配器（Slow-LoRA与Fast-LoRA），在几乎不重训的前提下实现最高5倍推理加速且画质保持。训练仅用极少样本与算力，具强泛化。


<details>
  <summary>Details</summary>
Motivation: 少步蒸馏常需重训小模型且泛化变差；现有方法均匀加速忽略扩散过程阶段差异，早期语义构建敏感而后期冗余度高。需要一种既低成本又保持泛化的加速方案。

Method: 将扩散推理划分为“慢（语义敏感）”与“快（冗余较多）”两个相位：对前期采用小加速、后期采用大加速。通过给基础扩散模型插入两套轻量LoRA适配器：Slow-LoRA处理慢相位、Fast-LoRA处理快相位。无需重训学生模型，仅微调LoRA；训练数据极少（据称1个样本），单卡V100一小时内完成。

Result: 在多项基准上实现最高约5×的推理加速，同时视觉质量与原始模型相当；即便用极少数据训练的LoRA专家，在未见过的提示词上也能较好泛化。

Conclusion: 相位感知的双LoRA专家可在极低训练成本下有效加速扩散模型，兼顾速度与质量，并具良好跨提示泛化，提供较传统少步蒸馏更实用的部署路径。

Abstract: Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.

</details>


### [99] [MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding](https://arxiv.org/abs/2512.02906)
*Fan Yang,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出MRD：多分辨率检索-检测的训练免框架，通过多分辨率语义融合与开放词表检测，提升MLLM对高分辨率图像的理解与定位。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在高分辨率图像上表现不佳。基于裁剪+RAG的方法会把对象切碎，导致跨裁剪的语义相似度失真，不同尺度目标需要不同分辨率处理。需要一种既能保持目标完整性又能全局定位的方案。

Method: 1) 多分辨率语义融合：对同一图像在多个分辨率下进行裁剪-检索，得到相似度热图；将不同分辨率的热图融合，缓解对象被切碎带来的偏置，保留目标完整语义。2) 开放词表目标检测(OVD) + 滑窗：在全局尺度用OVD滑窗识别与查询相关的目标区域，实现直接定位。3) 训练免：作为通用前端，适配多种MLLM。

Result: 在多个高分辨率图像理解基准与不同MLLM组合上，MRD优于仅裁剪-RAG的方案，能更准确地抑制无关区域并定位目标（文摘未给具体数值）。

Conclusion: 多分辨率语义融合与开放词表全局检测的结合，能在不额外训练的情况下显著提升MLLM的高分辨率图像理解与目标定位效果。

Abstract: Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.

</details>


### [100] [DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation](https://arxiv.org/abs/2512.02931)
*Ying Yang,Zhengyao Lv,Tianlin Pan,Haofan Wang,Binxin Yang,Hubery Yin,Chen Li,Chenyang Si*

Main category: cs.CV

TL;DR: 提出DiverseAR，通过自适应logits尺度与能量路径搜索，缓解比特级AR视觉分词器的采样塌缩，显著提升多样性且保持画质。


<details>
  <summary>Details</summary>
Motivation: 比特级AR视觉生成由于二元分类空间受限与logits过尖锐，采样易塌缩、样本多样性差；现有方法提升多样性常牺牲保真度，亟需兼顾多样性与质量的原理化方案。

Method: 1) 自适应logits分布缩放：在采样时动态调节二元输出分布的尖锐度，平滑预测以扩大有效采样空间；2) 基于能量的生成路径搜索：在平滑后避免选取低置信比特，通过能量准则筛除不可靠token，维持高保真。

Result: 在大量实验中，DiverseAR在比特级AR图像生成上显著提升样本多样性，同时不明显降低视觉质量。

Conclusion: 定位并解决二元空间受限与过尖logits导致的多样性不足问题；DiverseAR通过自适应平滑与能量搜索实现“多样性↑、质量≈不降”，为比特级AR生成提供通用、有效的采样改进。

Abstract: In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.

</details>


### [101] [EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis](https://arxiv.org/abs/2512.02932)
*Yancheng Zhang,Guangyu Sun,Chen Chen*

Main category: cs.CV

TL;DR: EGGS提出混合2D与3D高斯的可交换表示，通过统一渲染、动态类型切换与频率解耦优化，在保证纹理细节的同时提升多视一致与几何精度，实现高效高质的NVS。


<details>
  <summary>Details</summary>
Motivation: 3DGS实时且细节好但多视不一致、几何差；2DGS几何一致但纹理受损。需要一种既保外观又保几何的一体化表示与训练/渲染框架。

Method: 提出Exchangeable Gaussian Splatting：1) Hybrid Gaussian Rasterization，在同一渲染器内支持2D/3D高斯；2) Adaptive Type Exchange，训练中可根据区域/频段在2D与3D间切换；3) Frequency-Decoupled Optimization，按频率成分分别利用2D/3D优势进行优化；并提供CUDA加速实现以提升训练与推理效率。

Result: 在多项实验中，相比现有方法获得更佳的渲染质量（纹理与细节）、更高的几何精度与更高的效率。

Conclusion: 混合并可交换的2D/3D高斯表示结合统一渲染、动态切换与频率解耦优化，为高质量、几何准确且高效的NVS提供实用方案。

Abstract: Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.

</details>


### [102] [LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization](https://arxiv.org/abs/2512.02933)
*Zhihan Xiao,Lin Liu,Yixin Gao,Xiaopeng Zhang,Haoxuan Che,Songping Mai,Qi Tian*

Main category: cs.CV

TL;DR: LoVoRA提出一种无需显式掩码与参考图的文本引导视频编辑框架，依托可学习的对象感知定位与扩散掩码预测器，实现时空一致的目标移除与添加，并通过合成数据管线获得密集监督，实验与人评显示效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导视频编辑在对象移除/添加时难以保证精确的空间与时间一致性，且常依赖外部掩码或参考图，限制可扩展性与泛化。需要一种在推理时免控制信号、同时兼顾时空一致性的通用方案。

Method: 构建一条数据与监督管线：将图像翻译为视频以生成多帧训练样本；利用光流进行掩码传播；结合视频修复获得时序一致的编辑对。核心是可学习的对象感知定位机制，提供密集时空监督，并引入Diffusion Mask Predictor在扩散过程中预测编辑区域，从而端到端地完成对象插入与移除，无需推理阶段外部掩码/参考。

Result: 在广泛实验与人主观评测中，LoVoRA相较现有方法实现更高质量、时空一致的目标移除与添加，表现出更强的通用性与稳定性。

Conclusion: LoVoRA通过对象感知定位与扩散掩码预测，实现无掩码、端到端的文本引导视频对象编辑，兼具时空一致与高质量效果，减少对外部控制信号的依赖，提升可扩展与泛化能力。

Abstract: Text-guided video editing, particularly for object removal and addition, remains a challenging task due to the need for precise spatial and temporal consistency. Existing methods often rely on auxiliary masks or reference images for editing guidance, which limits their scalability and generalization. To address these issues, we propose LoVoRA, a novel framework for mask-free video object removal and addition using object-aware localization mechanism. Our approach utilizes a unique dataset construction pipeline that integrates image-to-video translation, optical flow-based mask propagation, and video inpainting, enabling temporally consistent edits. The core innovation of LoVoRA is its learnable object-aware localization mechanism, which provides dense spatio-temporal supervision for both object insertion and removal tasks. By leveraging a Diffusion Mask Predictor, LoVoRA achieves end-to-end video editing without requiring external control signals during inference. Extensive experiments and human evaluation demonstrate the effectiveness and high-quality performance of LoVoRA.

</details>


### [103] [Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench](https://arxiv.org/abs/2512.02942)
*Lanxiang Hu,Abhilash Shankarampeta,Yixin Huang,Zilin Dai,Haoyang Yu,Yujie Zhao,Haoqiang Kang,Daniel Zhao,Tajana Rosing,Hao Zhang*

Main category: cs.CV

TL;DR: 提出VideoScience-Bench，用于评测视频生成模型在本科水平物理与化学情景中的零样本科学推理与物理一致性。基准含200个跨14主题、103概念的复合科学场景提示，结合VLM法官与专家标注，从5个维度评估T2V与I2V模型，结果与人工高度相关，填补仅“物理常识”评测的空白。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评测多停留在物理常识或表面一致性，难以检验模型是否真正理解并能推理真实世界的科学规律，尤其在零样本、多概念复合情境下的物理/化学现象生成。需要一个能衡量“生成即推理”的科学理解基准。

Method: 构建VideoScience-Bench：1) 设计200条复合科学场景提示，覆盖14大主题与103个物理、化学概念；2) 面向T2V与I2V设置；3) 五维度评测指标——提示一致性、现象符合度、动力学正确性、不变性、时空连续性；4) 七个SOTA视频模型生成；5) 采用VLM-as-a-Judge评分并与专家标注对齐，验证相关性。

Result: 在七个SOTA模型上进行系统评测，VLM法官与人类评估存在较强相关性；结果显示当前视频模型在需要复合科学推理的场景中仍存在明显差距（具体分数未在摘要中给出）。

Conclusion: VideoScience-Bench首次系统把视频模型作为“生成者+推理者”共同评估，能够检验其对物理与化学现象的科学理解与一致性；为未来具备零样本科学推理能力的视频生成研究提供标准化数据与评测工具。

Abstract: The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.

</details>


### [104] [Layout Anything: One Transformer for Universal Room Layout Estimation](https://arxiv.org/abs/2512.02952)
*Md Sohag Mia,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: 提出Layout Anything：将OneFormer通用分割架构适配到室内布局估计的Transformer框架，引入布局退化数据增广与可微几何损失，实现无需繁琐后处理、114ms高速推理，并在多数据集达到SOTA误差（LSUN: PE 5.43%/CE 4.02%，Hedau: PE 7.04%/CE 5.17%，Matterport3D-Layout: PE 4.03%/CE 3.15%）。


<details>
  <summary>Details</summary>
Motivation: 现有室内布局估计方法常依赖复杂的后处理与启发式几何约束，训练中难以直接优化几何一致性，推理速度与通用性受限；需要一个统一、端到端且具几何意识的框架，以在保持曼哈顿世界约束的同时提高精度与效率，适配AR与大规模重建场景。

Method: 基于OneFormer的任务条件查询与对比学习，提出两大模块：1) 布局退化策略（topology-aware）进行数据增广，在保留曼哈顿世界约束下做拓扑变换；2) 可微几何损失，直接在训练中约束平面一致性与边界锐利度。整体端到端训练，去除复杂后处理，快速推理。

Result: 在标准基准上达SOTA：LSUN PE 5.43%、CE 4.02%；Hedau PE 7.04%、CE 5.17%；Matterport3D-Layout PE 4.03%、CE 3.15%。推理时间114ms。

Conclusion: 将通用分割Transformer与几何感知训练相结合，可在不依赖后处理的前提下实现高精度与高效率的室内布局估计，适用于AR与大规模3D场景重建。

Abstract: We present Layout Anything, a transformer-based framework for indoor layout estimation that adapts the OneFormer's universal segmentation architecture to geometric structure prediction. Our approach integrates OneFormer's task-conditioned queries and contrastive learning with two key modules: (1) a layout degeneration strategy that augments training data while preserving Manhattan-world constraints through topology-aware transformations, and (2) differentiable geometric losses that directly enforce planar consistency and sharp boundary predictions during training. By unifying these components in an end-to-end framework, the model eliminates complex post-processing pipelines while achieving high-speed inference at 114ms. Extensive experiments demonstrate state-of-the-art performance across standard benchmarks, with pixel error (PE) of 5.43% and corner error (CE) of 4.02% on the LSUN, PE of 7.04% (CE 5.17%) on the Hedau and PE of 4.03% (CE 3.15%) on the Matterport3D-Layout datasets. The framework's combination of geometric awareness and computational efficiency makes it particularly suitable for augmented reality applications and large-scale 3D scene reconstruction tasks.

</details>


### [105] [A Lightweight Real-Time Low-Light Enhancement Network for Embedded Automotive Vision Systems](https://arxiv.org/abs/2512.02965)
*Yuhan Chen,Yicui Shi,Guofa Li,Guangrui Bai,Jinyuan Shao,Xiangfei Huang,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 提出UltraFast-LieNET，一种超轻量多尺度移位卷积网络，实现车辆低照图像实时增强，在仅180参数下于LOLI-Street达26.51 dB，超SOTA约4.6 dB。


<details>
  <summary>Details</summary>
Motivation: 车载夜间/低照场景图像退化严重，影响安全；现有增强方法计算量大、参数多，难以在资源受限的车载平台实时部署，需要一种在极小参数、低算力下仍具备良好增强质量的模型。

Method: 设计仅12个可学习参数的动态移位卷积（DSConv），通过不同移位距离构建多尺度移位残差块（MSRB）以扩大全局感受野；采用残差结构稳定训练，并提出多级梯度感知损失提升轻量网络的稳定性与细节恢复；模型可灵活配置，最小仅36参数。

Result: 在LOLI-Street数据集上以约180参数取得26.51 dB的PSNR，较现有SOTA提升约4.6 dB；在四个基准数据集上均验证了在受限资源下的实时性与增强质量的优越性。

Conclusion: UltraFast-LieNET在极低参数量与算力预算下实现高质量低照增强，兼具实时性与精度，适合车载等资源受限场景，并通过新型DSConv与MSRB有效扩展感受野、提升稳定性。

Abstract: In low-light environments like nighttime driving, image degradation severely challenges in-vehicle camera safety. Since existing enhancement algorithms are often too computationally intensive for vehicular applications, we propose UltraFast-LieNET, a lightweight multi-scale shifted convolutional network for real-time low-light image enhancement. We introduce a Dynamic Shifted Convolution (DSConv) kernel with only 12 learnable parameters for efficient feature extraction. By integrating DSConv with varying shift distances, a Multi-scale Shifted Residual Block (MSRB) is constructed to significantly expand the receptive field. To mitigate lightweight network instability, a residual structure and a novel multi-level gradient-aware loss function are incorporated. UltraFast-LieNET allows flexible parameter configuration, with a minimum size of only 36 parameters. Results on the LOLI-Street dataset show a PSNR of 26.51 dB, outperforming state-of-the-art methods by 4.6 dB while utilizing only 180 parameters. Experiments across four benchmark datasets validate its superior balance of real-time performance and enhancement quality under limited resources. Code is available at https://githubhttps://github.com/YuhanChen2024/UltraFast-LiNET

</details>


### [106] [BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection](https://arxiv.org/abs/2512.02972)
*Guowen Zhang,Chenhang He,Liyi Chen,Lei Zhang*

Main category: cs.CV

TL;DR: 提出BEVDilation：在BEV中以激光雷达为中心、以图像为隐式引导进行多模态融合，缓解深度误差带来的对齐问题，并通过稀疏体素膨胀与语义引导BEV膨胀提升检测性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-相机BEV融合常“无差别”合并，忽视两传感器几何精度差异，导致由图像深度误差带来的空间错位和性能下降；同时纯LiDAR存在稀疏性与语义不足。需要一种既保持LiDAR几何可靠性又能借助图像补充密度与语义的融合方式。

Method: 提出LiDAR-centric的隐式引导融合：不做简单拼接，而将图像BEV特征作为引导信号。核心包括两模块：1) Sparse Voxel Dilation Block：利用图像先验对前景体素进行“膨胀”以稠密化LiDAR稀疏点；2) Semantic-Guided BEV Dilation Block：在BEV中引入图像语义引导与长程上下文扩散，增强LiDAR特征传播。整体在BEV空间进行融合与扩散，强调LiDAR主导。

Result: 在nuScenes上优于SOTA且计算效率具竞争力；相较朴素融合，对图像深度噪声更鲁棒。

Conclusion: 以LiDAR为中心、图像作隐式引导的BEV融合能同时缓解对齐误差与稀疏/语义不足问题。BEVDilation通过稀疏体素膨胀与语义引导BEV膨胀实现更强的3D检测性能与鲁棒性，并具良好效率。

Abstract: Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.

</details>


### [107] [Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities](https://arxiv.org/abs/2512.02973)
*Yuan Xiong,Ziqi Miao,Lijun Li,Chen Qian,Jie Li,Jing Shao*

Main category: cs.CV

TL;DR: 论文提出一种以图像为中心的越狱攻击方法CIA，通过在看似良性的视觉场景中隐蔽嵌入有害查询，显著提升对MLLM的攻击成功率与毒性输出，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态越狱大多把图像当作辅助提示，未充分利用图像可承载丰富上下文与隐蔽信息的优势，导致攻击能力受限。

Method: 设计“上下文图像攻击（CIA）”：构建多智能体系统，采用四种可视化策略将有害意图隐蔽编码进图像上下文；并加入“上下文元素增强”和“自动毒性混淆”以提升触发率与绕过安全审查能力；以图像为主导信道诱导MLLM生成不安全响应。

Result: 在MMSafetyBench-tiny上，对GPT-4o与Qwen2.5-VL-72B分别取得毒性得分4.73/4.83，攻击成功率86.31%/91.07%，显著超过现有基线。

Conclusion: 视觉模态本身可作为强有力的越狱载体；以图像为中心的上下文嵌入与混淆策略能有效绕过先进MLLM的安全对齐，凸显需加强多模态安全防护。

Abstract: While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\% and 91.07\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.

</details>


### [108] [InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration](https://arxiv.org/abs/2512.02981)
*Zhongyu Yang,Yingfang Yuan,Xuanming Jiang,Baoyi An,Wei Pang*

Main category: cs.CV

TL;DR: 提出InEx：一种免训练的多智能体框架，通过内在反思与外部多模态协作来缓解LLM/MLLM幻觉，实验在通用与幻觉基准上带来4%-27%提升。


<details>
  <summary>Details</summary>
Motivation: 现有减轻幻觉的方法依赖人工或未充分利用智能体自我纠偏能力。受人类决策过程启发：先内省降低不确定性，再多视角外部核验以达成可靠结论。

Method: InEx为训练-free多智能体系统：1) 内部阶段：基于熵的不确定性估计引导“内省推理”，先生成初稿并在不确定部分加强推理；2) 外部阶段：跨模态多智能体协作，包括决策代理、编辑代理与自反代理，对初稿进行迭代核验与修订；3) 通过多轮验证-编辑-自反循环提高答案可靠性、降低幻觉。

Result: 在多项通用与幻觉基准上，InEx相较现有方法取得稳定领先，性能提升幅度为4%-27%，并展现出较强鲁棒性。

Conclusion: 无需额外训练即可将内省与外部多代理协同结合，有效缓解MLLM幻觉并提升可靠性，为构建更可信的多模态智能体提供实用途径。

Abstract: Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.

</details>


### [109] [U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences](https://arxiv.org/abs/2512.02982)
*Xiang Xu,Ao Liang,Youquan Liu,Linfeng Li,Lingdong Kong,Ziwei Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: U4D提出一种不确定性感知的4D激光雷达世界建模框架，通过先难后易的两阶段生成和时空混合模块，提升几何真实性与时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有4D LiDAR生成方法对空间区域一视同仁，忽略真实场景中不确定性分布不均，导致在复杂/含糊区域产生伪影、逼真度差、时序不稳定。需要一种能针对高不确定区域重点建模并保持时空一致性的框架。

Method: 1) 利用预训练分割模型估计空间不确定性图，定位语义困难的高熵区域；2) 两阶段“先难后易”扩散生成：a) 不确定区域建模，优先重建高熵区域以获得精细几何；b) 基于不确定性条件的补全，用学习到的结构先验合成剩余区域；3) 设计MoST（Mixture of Spatio-Temporal）块，在扩散过程中自适应融合空间与时间表征以增强时序一致性。

Result: 在广泛实验中，U4D生成的LiDAR序列在几何保真度与时间一致性上均优于现有方法，减少伪影并提升4D世界建模可靠性。

Conclusion: 不确定性感知与先难后易的建模范式，结合时空混合表示融合，可有效提升4D LiDAR序列的逼真度与稳定性，适用于自动驾驶与仿真等场景。

Abstract: Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a "hard-to-easy" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.

</details>


### [110] [GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection](https://arxiv.org/abs/2512.02991)
*Md Sohag Mia,Md Nahid Hasan,Tawhid Ahmed,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: 提出GraphFusion3D：融合图像与点云的统一3D目标检测框架，含自适应跨模态Transformer与基于图的推理和级联解码，实现局部几何与全局语义联合建模，在SUN RGB-D与ScanNetV2上显著提升AP。


<details>
  <summary>Details</summary>
Motivation: 点云稀疏、结构不完整且语义有限，远距对象上下文难以建模，导致3D检测性能受限。需要一种能有效融合多模态信息并同时获取局部几何与全局语义关系的方法。

Method: 1) 自适应跨模态Transformer（ACMT）：将图像特征自适应地注入点表示，增强几何与语义。2) 图推理模块（GRM）：在proposal层面构建多尺度图注意力，基于空间邻近与特征相似度动态加权，建模邻域关系以捕获局部结构与全局上下文。3) 级联解码器：多阶段逐步细化检测结果。

Result: 在SUN RGB-D上达到70.6% AP25与51.2% AP50，在ScanNetV2上达到75.1% AP25与60.8% AP50，较现有方法有显著提升。

Conclusion: 融合跨模态自适应特征学习与图推理的统一框架可有效缓解点云稀疏与语义不足问题，通过局部-全局联合建模与级联细化显著提升3D目标检测性能。

Abstract: Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.

</details>


### [111] [TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond](https://arxiv.org/abs/2512.02993)
*Yifei Zeng,Yajie Bao,Jiachen Qian,Shuang Wu,Youtian Lin,Hao Zhu,Buyu Li,Feihu Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 提出TEXTRIX：一种原生3D属性生成框架，直接在体素空间上为3D模型上色与标注，避免多视角融合带来的不一致与缺失，在纹理合成与3D零件分割上达到SOTA、无缝高保真与精确边界。


<details>
  <summary>Details</summary>
Motivation: 多视图融合方法常出现视角间不一致、复杂表面覆盖不全，导致纹理不连贯和细节缺失；同时希望统一框架下兼顾高保真纹理生成与精细3D语义分割。

Method: 构建潜在3D属性网格（latent 3D attribute grid），以带稀疏注意力的扩散Transformer（Diffusion Transformer）直接在体素空间生成与编辑属性，实现对3D模型的原生上色；同一架构通过训练预测语义属性，从而扩展到高精度3D分割。

Result: 在纹理生成与3D零件分割两项任务上取得SOTA表现：生成无缝、高保真纹理，并在分割上实现准确、边界清晰的结果。

Conclusion: 原生3D属性表示结合扩散Transformer的稀疏注意力，可从根本上规避多视图融合的局限，统一实现高质量纹理合成与精细3D分割。

Abstract: Prevailing 3D texture generation methods, which often rely on multi-view fusion, are frequently hindered by inter-view inconsistencies and incomplete coverage of complex surfaces, limiting the fidelity and completeness of the generated content. To overcome these challenges, we introduce TEXTRIX, a native 3D attribute generation framework for high-fidelity texture synthesis and downstream applications such as precise 3D part segmentation. Our approach constructs a latent 3D attribute grid and leverages a Diffusion Transformer equipped with sparse attention, enabling direct coloring of 3D models in volumetric space and fundamentally avoiding the limitations of multi-view fusion. Built upon this native representation, the framework naturally extends to high-precision 3D segmentation by training the same architecture to predict semantic attributes on the grid. Extensive experiments demonstrate state-of-the-art performance on both tasks, producing seamless, high-fidelity textures and accurate 3D part segmentation with precise boundaries.

</details>


### [112] [DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling](https://arxiv.org/abs/2512.03000)
*Kairun Wen,Yuzhi Huang,Runyu Chen,Hui Zheng,Yunlong Lin,Panwang Pan,Chenxin Li,Wenyan Cong,Jian Zhang,Junbin Lu,Chenguo Lin,Dilin Wang,Zhicheng Yan,Hongyu Xu,Justin Theiss,Yue Huang,Xinghao Ding,Rakesh Ranjan,Zhiwen Fan*

Main category: cs.CV

TL;DR: DynamicVerse提出一个物理尺度、多模态的4D世界建模框架与大规模数据集，面向真实互联网单目视频，联合静态几何、动态运动、实例掩码与文本描述，显著提升深度、位姿与内参估计的全局准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据多来自受限模拟器或传统SfM的“仅尺度一致/上到尺度”标注，缺乏丰富语义描述，难以支撑基础模型准确理解来自互联网的真实动态单目视频中的物理尺度、运动与语义。

Method: 利用大型视觉、几何与多模态模型，估计度量尺度的静态几何、真实世界动态运动、实例级掩码与整体文本描述；并将窗口化BA与全局优化结合，把长序列真实视频转化为完整的4D多模态表示。

Result: 构建含10M+帧、100K+视频、800K+掩码的大规模数据集；在视频深度、相机位姿与内参估计三项基准上，相较现有方法取得更高的物理尺度一致性与全局准确度。

Conclusion: DynamicVerse为动态真实视频提供物理尺度、跨模态一致的4D建模与数据集，推进单目视频对真实世界动态的精确理解，并在关键几何任务上显著优于现有方法。

Abstract: Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.

</details>


### [113] [DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images](https://arxiv.org/abs/2512.03004)
*Xiaoxue Chen,Ziyi Xiong,Yuantao Chen,Gen Li,Nan Wang,Hongcheng Luo,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Hongyang Li,Ya-Qin Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: DGGT提出一种无需已知位姿的端到端前馈框架，从稀疏未标定图像直接重建动态驾驶场景并联合预测相机参数与4D高斯表示，结合动态/寿命头与扩散式渲染精炼，在Waymo、nuScenes、Argoverse2上实现SOTA精度与速度，并具备零样本跨数据集泛化与随帧数扩展的可伸缩性。


<details>
  <summary>Details</summary>
Motivation: 现有动态驾驶场景的4D重建/再模拟方法多依赖逐场景优化、已知相机标定或短时窗口，导致速度慢、扩展性差且难以在大规模训练与评测中使用。因此需要一种能从稀疏、未配准长序列中快速、可扩展地重建并支持再渲染/评估的统一方法。

Method: 提出Driving Gaussian Grounded Transformer (DGGT)：将相机位姿由输入改为可学习输出，采用前馈架构同时预测每帧3D Gaussian地图与相机参数；通过轻量动态头解耦场景动态，通过寿命(lifespan)头在时间上调制可见性以保持时序一致性；引入扩散式渲染精炼以降低运动/插值伪影，在稀疏视角下提升新视角质量；支持任意视角数量与长序列单次通过。

Result: 在Waymo、nuScenes、Argoverse2大规模基准上实现SOTA的重建质量与速度；无论在各数据集内训练还是跨数据集零样本迁移均优于以往方法；随着输入帧数增加，性能良好扩展。

Conclusion: 将位姿从前提转为预测目标，使得从稀疏未定姿图像进行单次、快速的动态场景4D重建成为可能；结合高斯表示、动态/寿命建模与扩散精炼，实现兼顾精度、速度与可扩展性的统一框架，适用于大规模自动驾驶训练与评估。

Abstract: Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.

</details>


### [114] [SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting](https://arxiv.org/abs/2512.03010)
*Svenja Strobel,Matthias Innmann,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.CV

TL;DR: 提出SurfFill：基于高斯surfel的LiDAR点云补全方法，利用密度变化的歧义启发式定位缺失区域，结合受约束的高斯surfel重建进行局部致密化与优化，并通过分而治之扩展到大规模场景，在合成与真实数据上优于既有方法。


<details>
  <summary>Details</summary>
Motivation: LiDAR在平坦区域精度高但易漏检细薄/边缘结构与黑暗吸收材质，影像测量可补细节但在无特征区精度不足。需要融合两者优势，以针对性补齐LiDAR缺失。

Method: 1) 分析LiDAR捕获，指出光束发散导致在细结构和边缘处的伪影与缺失；2) 基于点云密度变化定义“歧义”启发式，定位靠近缺失的点；3) 在这些歧义区域上，约束高斯surfel重建[Huang et al. 2024]，聚焦优化和致密化；4) 从重建得到的高斯原语中抽样生成新点以补全点云；5) 引入分而治之策略以支持建筑规模的点云补全。

Result: 在合成与真实场景的LiDAR点云补全任务上，方法优于以往重建方法（定量/定性均提升）。

Conclusion: 利用密度驱动的歧义检测结合高斯surfel的局部致密化与抽样，可有效补全LiDAR在细薄结构和边缘处的缺失；通过分而治之方案，方法适用于大规模重建，并在多数据集上取得领先性能。

Abstract: LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.

</details>


### [115] [In-Context Sync-LoRA for Portrait Video Editing](https://arxiv.org/abs/2512.03013)
*Sagi Polaczek,Or Patashnik,Ali Mahdavi-Amiri,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: Sync-LoRA是一种人像视频编辑方法，通过在首帧进行编辑并利用图像到视频扩散模型将修改传播到全序列，实现高保真视觉修改与逐帧同步、身份一致性。其核心是在上下文中训练LoRA，使用具有相同运动轨迹但外观不同的成对视频，并通过基于同步性的自动筛选确保时间对齐，从而将源视频的运动与首帧编辑的外观变化稳健融合。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑往往难以同时保持编辑质量与时间一致性，尤其在人像视频中，外观/表情/物体添加等编辑容易破坏角色的原始时序与身份一致性。需要一种既能灵活编辑又能逐帧与源视频严格对齐的方法。

Method: 采用图像到视频扩散模型：先在第一帧定义编辑，再将其传播至整段视频；训练一种“in-context LoRA”，使用自动生成的成对视频（运动轨迹一致、外观不同）作为训练数据；通过基于同步的筛选流程只保留时间对齐最佳的样本，促使模型学习将源视频的运动线索与首帧的外观更改相融合，从而实现精确同步与身份保持。

Result: 在紧凑且高度精选的同步人像数据上训练后，模型对未见身份和多样化编辑（外观修改、物体添加、背景替换等）具有良好泛化能力；能在姿态、表情变化下保持高视觉保真和强时序一致性，兼顾编辑保真度与运动精确保留。

Conclusion: Sync-LoRA通过同步对齐的成对训练与首帧编辑传播，实现了人像视频编辑中高质量视觉修改与逐帧精确同步的兼顾，具备强泛化、身份一致性与稳健的时序保真，适用于多种编辑场景。

Abstract: Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.

</details>


### [116] [Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks](https://arxiv.org/abs/2512.03014)
*Matthew Dutson,Nathan Labiosa,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 提出一种将逐帧模型稳定地适配到视频上的通用方法：在任意架构中插入“稳定性适配器”，配合冻结主干、低成本训练，以同时提升时间一致性与对时变扰动的鲁棒性，同时保持或提升任务精度。


<details>
  <summary>Details</summary>
Motivation: 逐帧推理在视频上常出现时间不一致（闪烁），且输入含随时间变化的污染（压缩伪影、噪声、天气等）会放大问题。现有方法往往与任务/架构强耦合或训练成本高，需要一种通用、可插拔、训练经济的方案来统一提升稳定性与鲁棒性。

Method: 提出“稳定性适配器”（stability adapters），可插入几乎任意网络；采用冻结基础网络的资源高效训练流程。提出以“准确性-稳定性-鲁棒性”联合损失为核心的统一框架，分析该损失的理论性质并给出产生良好训练行为的条件，使适配器在不改变主干的情况下学习时序一致与抗扰动特性。

Result: 在多个视觉任务和模型上实验验证：去噪（NAFNet）、图像增强（HDRNet）、单目深度（Depth Anything v2）、语义分割（DeepLabv3+）。方法显著提升时间稳定性与对多种图像污染（压缩、噪声、恶劣天气等）的鲁棒性，同时保持或提升预测质量。

Conclusion: 稳定性适配器提供了一个通用且高效的方式，将逐帧图像模型适配为在视频上稳定且鲁棒的推理器；所提联合损失为理解与优化时间稳定性和鲁棒性提供了统一视角，并在多任务上取得了实际收益。

Abstract: When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.

</details>


### [117] [AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry](https://arxiv.org/abs/2512.03018)
*Xiang Xu,Pradeep Kumar Jayaraman,Joseph G. Lambourne,Yilin Liu,Durvesh Malpure,Pete Meltzer*

Main category: cs.CV

TL;DR: AutoBrep 提出一种自回归 Transformer，通过统一离散化将 B-Rep 的几何与拓扑编码为令牌序列，按面邻接图的广度优先顺序生成，能高质量、具水密性的 B-Rep，支持补全与可控生成，优于基线且具可扩展性与速度优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以端到端直接生成既几何精确又拓扑水密的 B-Rep。需要一种既统一表达几何/拓扑、又适配自回归生成与高效推理的表示与模型。

Method: 提出统一令牌化：将曲面/曲线几何编码为潜在几何令牌，将结构关系编码为拓扑引用令牌；以 B-Rep 面邻接图的广度优先顺序组织序列；使用自回归 Transformer 进行下一个令牌预测，推理时逐步生成相邻面、边及其拓扑结构，实现渐进构形与约束维护。

Result: 在大量实验中，AutoBrep 在质量和水密性上优于基线，具较高保真度、推理速度快，并可扩展至复杂实体；统一令牌化天然支持 B-Rep 自动补全与用户可控生成。

Conclusion: 统一几何-拓扑令牌化结合自回归生成可实现高质量、水密的 B-Rep 生成与补全，具备可扩展性与效率，适用于可控 CAD 建模场景。

Abstract: The boundary representation (B-Rep) is the standard data structure used in Computer-Aided Design (CAD) for defining solid models. Despite recent progress, directly generating B-Reps end-to-end with precise geometry and watertight topology remains a challenge. This paper presents AutoBrep, a novel Transformer model that autoregressively generates B-Reps with high quality and validity. AutoBrep employs a unified tokenization scheme that encodes both geometric and topological characteristics of a B-Rep model as a sequence of discrete tokens. Geometric primitives (i.e., surfaces and curves) are encoded as latent geometry tokens, and their structural relationships are defined as special topological reference tokens. Sequence order in AutoBrep naturally follows a breadth first traversal of the B-Rep face adjacency graph. At inference time, neighboring faces and edges along with their topological structure are progressively generated. Extensive experiments demonstrate the advantages of our unified representation when coupled with next-token prediction for B-Rep generation. AutoBrep outperforms baselines with better quality and watertightness. It is also highly scalable to complex solids with good fidelity and inference speed. We further show that autocompleting B-Reps is natively supported through our unified tokenization, enabling user-controllable CAD generation with minimal changes. Code is available at https://github.com/AutodeskAILab/AutoBrep.

</details>


### [118] [Unrolled Networks are Conditional Probability Flows in MRI Reconstruction](https://arxiv.org/abs/2512.03020)
*Kehan Qi,Saumya Gupta,Qingqiao Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 提出将流动常微分方程（flow ODE）引入MRI重建，理论上把展开网络视为条件概率流ODE的离散实现，并据此设计FLAT训练以提升稳定性与效率。实验表明在3个MRI数据集上，FLAT能以更少迭代数达到高质量重建，稳定性优于传统展开网络，效率优于扩散模型。


<details>
  <summary>Details</summary>
Motivation: MRI采集时间长，需依赖欠采样与深度重建。展开（unrolled）网络高效但中间步骤参数自由学习导致演化不稳定；扩散模型/随机微分方程具理论稳定性但计算量大。需要既稳定又高效的重建框架。

Method: 从理论上证明展开网络等价于条件概率流ODE的离散化，实现了对中间状态演化的明确刻画与参数显式表达。在此基础上提出Flow-Aligned Training（FLAT）：通过ODE离散化导出展开网络参数，并用对齐损失使各中间重建贴合理想ODE轨迹，从而提升稳定性与收敛。

Result: 在三个MRI数据集上，相比扩散生成模型，FLAT以最多3倍更少迭代获得同等或更优图像质量；相较传统展开网络，训练与推理过程中的中间状态更稳定，收敛更快、震荡更少。

Conclusion: 将展开网络与条件概率流ODE建立等价联系，并据此提出的FLAT实现了稳定且高效的MRI重建，在保持高质量的同时显著降低迭代和不稳定性，为临床可用的快速MRI重建提供了新途径。

Abstract: Magnetic Resonance Imaging (MRI) offers excellent soft-tissue contrast without ionizing radiation, but its long acquisition time limits clinical utility. Recent methods accelerate MRI by under-sampling $k$-space and reconstructing the resulting images using deep learning. Unrolled networks have been widely used for the reconstruction task due to their efficiency, but suffer from unstable evolving caused by freely-learnable parameters in intermediate steps. In contrast, diffusion models based on stochastic differential equations offer theoretical stability in both medical and natural image tasks but are computationally expensive. In this work, we introduce flow ODEs to MRI reconstruction by theoretically proving that unrolled networks are discrete implementations of conditional probability flow ODEs. This connection provides explicit formulations for parameters and clarifies how intermediate states should evolve. Building on this insight, we propose Flow-Aligned Training (FLAT), which derives unrolled parameters from the ODE discretization and aligns intermediate reconstructions with the ideal ODE trajectory to improve stability and convergence. Experiments on three MRI datasets show that FLAT achieves high-quality reconstructions with up to $3\times$ fewer iterations than diffusion-based generative models and significantly greater stability than unrolled networks.

</details>


### [119] [MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation](https://arxiv.org/abs/2512.03034)
*Youxin Pang,Jiajun Liu,Lingfeng Tan,Yong Zhang,Feng Gao,Xiang Deng,Zhuoliang Kang,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: MAViD提出一个用于音视对话理解与生成的多模态框架，通过“指挥-创作”双模块实现可控、连贯的长时交互式对话生成，并结合AR音频与扩散视频生成与新型融合模块，显著提升长时、一致、同步的视听表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为非交互式，语音不自然、表达受限；难点在于同时兼顾理解与生成、并实现音视频的无缝融合与长时一致性（身份、音色、语气）。

Method: 提出Conductor-Creator架构：Conductor负责理解、推理与将指令分解为动作与语音两部分，实现细粒度控制；Creator依据指令生成响应，采用AR模型生成音频、扩散模型生成高质量视频；并设计新的跨片段与跨模态融合模块，提升上下文连续片段之间的联系与同步，支持长时多模态生成。

Result: 实验显示该框架能产生生动、上下文连贯的长时对话互动，准确理解多模态查询，并在身份、音色、语气一致性与音视频同步方面表现优异。

Conclusion: MAViD通过模块化分工与AR+扩散的混合生成策略，以及新的融合机制，有效解决了长时多模态对话生成中的理解-生成耦合与音视频一致性问题，提升了交互自然度与可控性。

Abstract: We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.

</details>


### [120] [ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation](https://arxiv.org/abs/2512.03036)
*Mengchen Zhang,Qi Chen,Tong Wu,Zihan Liu,Dahua Lin*

Main category: cs.CV

TL;DR: 论文提出端到端从静音视频生成双耳空间音频的新任务与模型ViSAudio，并配套构建了约9.7万对视频-双耳音频的数据集BiAudio。ViSAudio基于条件流匹配与双分支音频生成架构，结合时空条件模块，直接生成时空对齐的双耳音频，避免两阶段方法的误差累积与时空不一致。在客观与主观评测中均优于现有方法，能适应视角变化、声源运动与多样声学环境。


<details>
  <summary>Details</summary>
Motivation: 现有视频转音频工作多输出单声道，缺乏空间沉浸感；现有双耳方法多为“先单声道再空间化”的两阶段流程，易产生误差累积及时空不一致，难以精准随视频时空变化呈现空间听感。需要端到端方法直接从视频生成双耳音频。

Method: 1) 数据：构建BiAudio数据集，约97K视频-双耳音频对，覆盖多类真实场景与相机旋转轨迹，半自动生成流程。
2) 模型：提出ViSAudio端到端框架，采用条件流匹配（conditional flow matching）进行生成；设计双分支音频生成架构，分别为左右声道建模音频潜在流；引入条件时空模块（conditional spacetime module）在保持两声道一致性的同时保留空间差异，实现与视频的精确时空对齐。

Result: 在全面实验中，ViSAudio在客观指标与主观听感评测上均超过现有SOTA。生成的双耳音频具备高质量空间沉浸感，并能有效适配视角变化、声源运动与多样声学环境。

Conclusion: 端到端的条件流匹配+双分支架构能避免两阶段空间化的缺陷，实现与视频严格时空对齐的双耳音频生成。BiAudio数据集与ViSAudio共同推动视频到双耳音频生成的研究前沿。

Abstract: Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.

</details>


### [121] [Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation](https://arxiv.org/abs/2512.03040)
*Zeqi Xiao,Yiwei Zhao,Lingxiao Li,Yushi Lan,Yu Ning,Rahul Garg,Roshni Cooper,Mohammad H. Taghavi,Xingang Pan*

Main category: cs.CV

TL;DR: Video4Spatial 证明仅用视频上下文条件的视频扩散模型即可执行复杂空间任务（导航与目标定位），无需深度/位姿等辅助模态，并在长上下文与OOD环境中泛化良好。


<details>
  <summary>Details</summary>
Motivation: 检验“仅凭视觉数据”的视频生成模型是否具备人类核心的视空智能，即能否在没有显式几何/位姿输入时理解三维结构、遵循指令并进行规划。

Method: 提出 Video4Spatial：以视频上下文为唯一条件的视频扩散框架。通过简单但有效的架构与数据策划，使模型在生成过程中隐式建模场景几何与语义；设计两个任务评估——(1) 场景导航：按相机位姿指令生成相机运动的视频，同时保持与场景3D几何一致；(2) 目标落地：在视频中定位并规划到目标物体，融合语义定位、指令跟随与规划能力。全程不使用深度图、相机姿态等辅助模态。

Result: 模型能端到端规划导航并完成目标落地，生成的视频与给定相机位姿指令一致且保持空间一致性；在长时上下文与域外环境上具有良好泛化。整体表现显示出强视空理解能力。

Conclusion: 视频扩散模型在仅视频条件下即可展现出非凡的视空推理与任务执行能力，向通用视空推理迈进一步。

Abstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.

</details>


### [122] [MultiShotMaster: A Controllable Multi-Shot Video Generation Framework](https://arxiv.org/abs/2512.03041)
*Qinghe Wang,Xiaoyu Shi,Baolu Li,Weikang Bian,Quande Liu,Huchuan Lu,Xintao Wang,Pengfei Wan,Kun Gai,Xu Jia*

Main category: cs.CV

TL;DR: 提出MultiShotMaster框架，将单段视频模型扩展为可控的多镜头视频生成，通过两种改进的RoPE实现灵活镜头编排、叙事顺序保持与时空参考注入，并配合自动标注数据管线，显著提升多镜头一致性与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成多擅长单镜头，难以在多个镜头间保持叙事连贯、角色/场景一致与超越文本的精细可控性，同时缺乏带有跨镜头标注的数据。

Method: 在预训练单镜头扩散/Transformer模型上加入两种RoPE变体：1) 多镜头叙事RoPE，在镜头衔接处施加显式相位偏移，允许镜头重排同时保留时间叙事顺序；2) 时空位置感知RoPE，引入参考token与落地信号（grounding），实现时空对齐的参考注入。此外构建自动化标注流程，从多镜头视频中提取镜头段、字幕、跨镜头grounding与参考图像。

Result: 实现多镜头视频生成的高可控性：文本驱动的镜头间一致性、可定制主体与运动控制、背景驱动的定制场景；镜头数量与时长可灵活设定；实验显示在质量与可控性上优于现有方法。

Conclusion: 利用RoPE的相位与位置编码扩展，结合自动标注数据，框架在保持叙事与时空一致性的同时提供强可控的多镜头生成，优于当下单镜头或弱可控方案。

Abstract: Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.

</details>


### [123] [PPTArena: A Benchmark for Agentic PowerPoint Editing](https://arxiv.org/abs/2512.03042)
*Michael Ofengenden,Yunze Man,Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: PPTArena 是一个用于评测基于自然语言的 PowerPoint 原地编辑的基准，涵盖真实幻灯片和多类型目标编辑；作者还提出了结构感知的编辑代理 PPTPilot，能在该基准上显著优于现有系统，但长程文档级任务仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于将文本生成整套幻灯片或对 PDF 图像进行操作，缺乏对真实 PPT 文件中精细、可控、可重复的“就地编辑”能力的系统评测；同时缺少既衡量指令遵循又衡量视觉质量的标准化流程。

Method: 1) 构建 PPTArena：包含100个文档、2125页、800+有标注的目标编辑，覆盖文本、图表、表格、动画、母版样式；为每个案例提供原始文件、明确目标结果，以及双VLM裁判流程，从结构差异与渲染图像两路打分指令遵循与视觉质量。2) 提出 PPTPilot：结构感知的编辑代理，先规划语义编辑序列，再在高层程序化工具与确定性的XML操作间路由，以实现精细控制，并通过“计划-编辑-检查”的迭代回路对照任务约束进行自检与修正。

Result: 在复合型、排版敏感、跨页编辑任务上，PPTPilot相较强力商用代理与前沿VLM提升10+个百分点，尤其在视觉保真度与整册一致性方面优势显著。

Conclusion: PPTArena为真实PPT就地编辑提供了可复现的评测基准；PPTPilot展示了结构化规划与可验证编辑的有效性，但当前代理在长时程、文档级任务上仍明显不足，提示未来需加强跨页依赖建模、长期计划与可靠性保障。

Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.

</details>


### [124] [OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043)
*Kaituo Feng,Manyuan Zhang,Hongyu Li,Kaixuan Fan,Shuang Chen,Yilei Jiang,Dian Zheng,Peiwen Sun,Yiyuan Zhang,Haoze Sun,Yan Feng,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.CV

TL;DR: OneThinker提出一个统一的多模态推理模型，在同一框架内处理图像与视频的多种基础任务（问答、描述、空间/时间定位、跟踪、分割），并通过多任务RL与SFT实现强泛化与知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为不同任务与模态分别训练模型，导致跨任务与跨模态的可扩展性差、知识共享受限，难以形成通用多模态推理体。

Method: 1) 构建覆盖图像与视频、多种任务的训练语料OneThinker-600k；利用商用模型生成CoT标注，得到用于冷启动微调的OneThinker-SFT-340k。2) 统一模型架构以同时处理问答、描述、定位、跟踪、分割等任务。3) 提出EMA-GRPO：在多任务强化学习中跟踪各任务回报标准差的指数移动平均，以平衡不同任务奖励尺度的异质性，实现稳定优化。

Result: 在10类基础视觉理解任务、31个基准上取得强性能；表现出跨任务知识迁移与初步的零样本泛化能力。

Conclusion: 通过统一数据与训练范式（SFT+多任务RL）并引入EMA-GRPO以处理奖励异质性，OneThinker向通用多模态推理体迈出一步，兼顾图像与视频、多个基础任务，并开源代码、模型与数据。

Abstract: Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.

</details>


### [125] [CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models](https://arxiv.org/abs/2512.03045)
*Minkyung Kwon,Jinhyeok Choi,Jiho Park,Seonghu Jeon,Jinhyuk Jang,Junyoung Seo,Minseop Kwak,Jin-Hwa Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: 提出CAMEO：用几何对应直接监督多视角扩散模型的注意力图，提升一致性、收敛速度与新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 多视角扩散模型能合成一致的新视角，但其视角一致性的来源与局限不清；注意力图虽学习到跨视角几何对应，但在大视差时不稳定、信号不完整，需要方法强化与利用该对应。

Method: 观察与实证：训练过程中跨视角注意力图逐步对齐几何对应。提出CAMEO：在训练中用外部/显式几何对应（如多视几何投影/匹配）对某一层注意力图进行监督，促使模型学到精确对应；仅监督单层注意力即可；模型无关、可插拔。

Result: 与原始训练相比，CAMEO将收敛迭代数减半；在相同迭代数下也取得更优的新视角合成质量与几何/结构保真；在大视角变化下保持更高对应精度；适用于不同多视角扩散模型。

Conclusion: 多视角扩散模型的视角一致性源于注意力中的几何对应信号，但原生不完整。通过对注意力进行几何对应监督（CAMEO），可显著提升对应精度、几何保真与收敛效率，且具模型无关性。

Abstract: Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.

</details>


### [126] [MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues](https://arxiv.org/abs/2512.03046)
*Zichen Liu,Yue Yu,Hao Ouyang,Qiuyu Wang,Shuailei Ma,Ka Leong Cheng,Wen Wang,Qingyan Bai,Yuxuan Zhang,Yanhong Zeng,Yixuan Li,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: MagicQuill V2 通过“分层合成”将生成式编辑拆为内容、空间、结构、颜色四层，并配合统一控制模块与特化数据/空间分支，实现更直观、可控的局部编辑与对象移除。


<details>
  <summary>Details</summary>
Motivation: 扩散模型整体生成强但单一提示难以表达并解耦“生成什么、放在哪、长什么样、什么颜色”等多重用户意图，导致可控性与传统图形软件的细粒度控制存在鸿沟。

Method: 提出分层表示：内容层（生成什么）、空间层（放在哪里）、结构层（形状如何）、颜色层（调色）。构建上下文感知的数据生成管线，用于训练/微调；设计统一控制模块接收并融合多种视觉线索；在空间分支上进行微调以实现精确局部编辑与对象移除。

Result: 在广泛实验中，层式方法显著提升了编辑可控性与直观性，能够精确地将内容按指定位置、形状和色彩整合到图像中，并支持高质量的局部编辑与删除。

Conclusion: 分层合成有效弥合扩散模型与传统图形编辑的意图表达差距，赋予创作者对生成过程的直接、细粒度控制。

Abstract: We propose MagicQuill V2, a novel system that introduces a \textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.

</details>
