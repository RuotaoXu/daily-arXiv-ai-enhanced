<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 107]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

TL;DR: 他们提出在下游任务模型训练中加入新型鲁棒性损失，以抵消组织病理学基础模型对扫描仪/预分析技术差异的敏感性，从而在不重训基础模型的前提下提升稳健性与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型除学习生物学特征外，还吸收了大量与生物无关的技术变异（如扫描仪、预处理批次），导致下游任务偏置与泛化差。需要一种无需重训大模型、即可抑制技术域偏差并突出生物学信号的方法。

Method: 构建面向下游任务的训练框架，在特征层面使用八个流行基础模型的嵌入，加入新型鲁棒性损失项（用于降低对预分析与扫描仪差异的敏感性），并在包含27,042张WSI、6,155名患者的大规模基准上系统评测，训练数千个下游模型比较鲁棒性与精度。

Result: 与常规下游训练相比，加入鲁棒性损失后，对技术变异的敏感性显著下降，模型在跨扫描仪/批次的泛化更好；同时因更聚焦生物学相关特征，预测准确率也有所提升。

Conclusion: 通过在下游训练中引入鲁棒性损失，可在不重训基础模型的情况下缓解其鲁棒性问题，实现在真实临床数据上的稳健、可推广的计算病理模型。

Abstract: Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

</details>


### [2] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

TL;DR: 提出MNAS-Unet：用蒙特卡洛树搜索结合NAS，优化DownSC/UpSC单元，以更快更准地搜索和调整分割网络；在多数据集上精度优于NAS-Unet与SOTA，同时将搜索预算降54%，模型仅0.6M参数、显存更省。


<details>
  <summary>Details</summary>
Motivation: 传统NAS用于医学分割常面临搜索效率低、算力开销大、结构调整不灵活的问题。在实际医疗场景中，资源受限且需要高精度与轻量化模型，因此需要一种在有限预算下高效搜索并保持竞争性精度的方法。

Method: 将蒙特卡洛树搜索(MCTS)引入架构搜索流程，动态探索并优先扩展前景较好的网络结构；设计并可调的DownSC/UpSC模块以便快速精细化网络调整；在多医学数据集上进行实验评估，与NAS-Unet和其他SOTA比较；采用早停策略（139/300 epoch）以减少搜索开销。

Result: 在PROMISE12、Ultrasound Nerve、CHAOS等数据集上分割精度优于NAS-Unet及其他SOTA；相较NAS-Unet，搜索预算降低约54%（139 vs 300 epoch），得到仅0.6M参数、显存占用更低的轻量模型。

Conclusion: MNAS-Unet通过MCTS引导的高效NAS与可调DownSC/UpSC设计，在资源受限条件下实现更快搜索与更高/相当精度，并提供轻量低显存占用的实用分割模型。

Abstract: This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

</details>


### [3] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

TL;DR: AeroDGS 提出一种面向单目无人机视频的物理先验引导的4D高斯渲染框架，通过几何提升与物理一致性优化，稳定重建大范围航拍场景中的动态与静态几何，并在自建数据集与合成/真实场景上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目航拍场景存在视角单一、覆盖范围广、目标体积小且运动差异大，导致深度与运动估计高度不确定；现有4D重建方法在此设定下易产生深度歧义与不稳定的动态估计，需要一种能从单目线索中恢复可靠几何并约束物体物理合理运动的方案。

Method: 提出 AeroDGS：1) 单目几何提升模块，从单个航拍序列中联合重建稳定的静态背景与动态几何，作为动态估计的可靠基座；2) 物理引导优化模块，将可微的地面支撑、竖直稳定与轨迹平滑等先验融入优化，把模糊图像线索转化为物理一致的运动；3) 在4D Gaussian Splatting 框架内联合细化静态与动态的时空一致表示。并构建覆盖不同高度与运动条件的真实无人机数据集用于评测。

Result: 在合成与真实的无人机场景上，相较SOTA方法取得更高的重建保真度与更稳定的动态重建表现，验证了单目航拍条件下的有效性。

Conclusion: 物理先验与单目几何提升的结合可显著缓解单目航拍中的深度与运动歧义，AeroDGS 能稳定重建大范围动态空域场景，为单目UAV 4D重建提供了实用方案并配套数据集以推动研究。

Abstract: Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

</details>


### [4] [Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention](https://arxiv.org/abs/2602.22381)
*Zhengkang Fan,Chengkun Sun,Russell Terry,Jie Xu,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 提出一种无需分割即可进行肾肿瘤恶性预测的深度学习框架，通过器官聚焦注意力（OFA）损失让器官补丁仅关注器官补丁，在UF-IDR与KiTS21数据集上优于依赖分割裁剪的传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有影像学方法在术前恶性预测准确度不足。深度学习虽有潜力，但常依赖人工肿瘤分割以降噪提升性能，代价高、耗时并依赖专家。需一种在不进行显式分割的情况下仍能提升预测准确性的方案，降低临床部署门槛。

Method: 构建以Organ Focused Attention（OFA）损失为核心的框架：将3D肾脏CT划分为图像补丁，并通过OFA约束“器官补丁只关注器官补丁”，从而在训练中学到器官内相关性、抑制背景噪声；推理阶段不需提供分割掩膜或裁剪。与传统基于分割裁剪的深度模型进行对比评估。

Result: 在UF IDR私有数据集上AUC 0.685、F1 0.872；在KiTS21上AUC 0.760、F1 0.852。两者均优于依赖分割裁剪的基线模型。

Conclusion: OFA引导的无分割框架能在不依赖显式分割的情况下实现更高或相当的恶性预测性能，简化工作流程、降低成本并提高临床可部署性，对肾癌诊断决策具有潜在价值。

Abstract: Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.

</details>


### [5] [Vision Transformers Need More Than Registers](https://arxiv.org/abs/2602.22394)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 论文指出：ViT 在大规模预训练后常出现“伪全局语义”伪影，其根源是CLS通过全局注意力偷懒地聚合与语义无关的背景patch。作者提出有选择地将patch特征注入CLS，抑制背景主导的捷径，从而在多种监督范式与12个基准上稳定提升表现。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT被广泛用于下游任务，但在多种监督方式（有标签、文本、自监督）和任务上都观察到系统性伪影，说明其内部机制与表征形成过程尚不清晰，特别是为何背景信息会不当主导全局语义表示。

Method: 对ViT伪影进行系统性实证分析，归因为“惰性聚合”：在全局注意力与粗粒度语义监督驱动下，CLS倾向用语义无关的背景patch作为表征捷径。为此，提出一种选择性整合策略，只将与前景/语义相关的patch特征注入CLS，过滤或降低背景主导影响。

Result: 在12个基准上、覆盖标签监督、文本监督和自监督三种范式，所提方法带来一致且显著的性能提升，并减少由背景驱动的伪影与错误关联。

Conclusion: ViT伪影主要源于CLS的惰性聚合与背景捷径。通过选择性地将patch特征融合进CLS可缓解该问题并提升泛化性能，为理解与改进ViT提供了新视角。

Abstract: Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.

</details>


### [6] [CLIP Is Shortsighted: Paying Attention Beyond the First Sentence](https://arxiv.org/abs/2602.22419)
*Marc-Antoine Lavoie,Anas Mahmoud,Aldo Zaimi,Arsene Fansi Tchango,Steven L. Waslander*

Main category: cs.CV

TL;DR: 提出DeBias-CLIP，通过去除摘要句、句子子采样与文本token填充，缓解长描述训练中“首句捷径”偏差，显著提升长文本检索并改进短文本检索，对句序更鲁棒，且无需新增可训练参数。


<details>
  <summary>Details</summary>
Motivation: CLIP多用短标题配图预训练，导致对复杂场景与密集描述对齐粗糙。即使用长描述微调，人/LLM生成长文往往“首句总结+细节”，训练时模型偏向前句与前几个token，削弱后续信息对齐。需要一种方法打破首句捷径，将监督均匀分布到整段文本。

Method: 提出DeBias-CLIP：1) 训练时移除开头摘要句；2) 进行句子级子采样，使不同训练步关注不同句子片段；3) 对文本序列进行token位置填充/对齐，促使监督覆盖各token位置；作为Long-CLIP的即插即用替代，不引入新参数。

Result: 在长文本检索上达到SOTA；短文本检索也有提升；对句子顺序扰动不敏感（更鲁棒）。

Conclusion: 通过结构化的数据处理与位置监督再分配，消除长描述训练中的“首句偏置”，在不增加模型容量的前提下改进跨模态对齐与检索性能，可直接替换Long-CLIP用于相关任务。

Abstract: CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.

</details>


### [7] [SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read](https://arxiv.org/abs/2602.22426)
*Yibo Peng,Peng Xia,Ding Zhong,Kaide Zeng,Siwei Han,Yiyang Zhou,Jiaqi Liu,Ruiyi Zhang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 论文提出VQ设定将文本查询直接可视化到图像上，揭示MLLM在需要“看图读字”时存在高达12.7%的性能下滑，并用SimpleOCR训练策略通过随机风格化的VQ样本消除文本提示捷径，显著提升泛化与数据效率，可与RL方法兼容。


<details>
  <summary>Details</summary>
Motivation: 探究MLLM在需要视觉文字理解时是否真正进行OCR式阅读，还是依赖提示中的参数化捷径；并诊断、缩小“能力-利用”鸿沟与“模态惰性”。

Method: 1) 设计Visualized-Question (VQ) 设定：将问题文本渲染到图片上，迫使模型走视觉通道。2) 诊断试验：在Qwen2.5-VL上对比常规与VQ设定表现。3) 提出SimpleOCR：把训练样本转为VQ格式并随机化字体/样式，结构性削弱文本提示捷径，强化视觉文字提取路径；无需改模型架构；可与RL（如NoisyRollout）组合。

Result: 发现在VQ设定下性能最多下降12.7%，显示模态惰性。采用SimpleOCR后，在四个OOD基准上较基线提升5.4%，较基于原图的GRPO提升2.7%；数据效率高，以8.5K样本就超过近端RL方法（约少30倍数据）；与NoisyRollout结合有额外收益。

Conclusion: MLLM存在在视觉文本读取上的“能力未被充分利用”。通过VQ诊断与SimpleOCR的结构化训练可有效激活视觉OCR通路、提升OOD泛化与效率，且方法简单、可即插即用并与先进RL策略互补。

Abstract: Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.

</details>


### [8] [Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge](https://arxiv.org/abs/2602.22455)
*Giuseppe Lando,Rosario Forte,Antonino Furnari*

Main category: cs.CV

TL;DR: 论文探索将多模态大模型用于实时在线情景记忆问答，在边缘端（而非云端）实施以兼顾隐私与时延，提出双线程流水：描述线程将视频流转为轻量文本记忆，问答线程基于该记忆流式回答问题；在受限算力下依然取得接近云端的准确率与更低TTFT，显示边缘方案的可行性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴助手需要持续理解第一人称视频并回答“发生了什么/在哪里”等情景记忆问题。云端推理虽强，但存在隐私泄露与网络时延、带宽成本；因此动机是在受限算力的边缘设备上实现低延迟、隐私友好的在线情景记忆问答，同时评估在严格资源约束下的MLLM性能。

Method: 设计流式约束的双异步线程架构：1) Descriptor线程持续将视频帧压缩为轻量文本“记忆”（描述/摘要）；2) QA线程在接收文本记忆的同时进行推理并回答用户查询。对比不同部署：消费级8GB GPU本地端、企业级本地服务器、以及云端；以QAEgo4D-Closed基准评测准确率与首token时间（TTFT）。

Result: 在8GB消费级GPU端到端部署：准确率51.76%，TTFT 0.41s；在本地企业级服务器：准确率54.40%，TTFT 0.88s；云端方案：准确率56.00%。在严格资源限制下，边缘端结果接近云端，且TTFT具有竞争力。

Conclusion: 将MLLM用于边缘侧在线情景记忆问答是可行且具隐私优势。所提双线程流水在资源受限下仍保持较高准确率与低延迟；边缘部署为隐私保护的情景记忆检索提供了有前景的替代方案。

Abstract: We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.

</details>


### [9] [MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation](https://arxiv.org/abs/2602.22462)
*Raiyan Jahangir,Nafiz Imtiaz Khan,Amritanand Sudheerkumar,Vladimir Filkov*

Main category: cs.CV

TL;DR: 提出MammoWise：在本地把开放源VLM改造成乳腺摄影报告生成与多任务分类器的可扩展流水线；在VinDr-Mammo与DMID上，零/少样本与RAG均能生成高质量报告，分类可行但依赖模型/数据集；对MedGemma做QLoRA微调后，BI-RADS准确率0.7545、密度0.8840、钙化0.9341，并保持报告质量。


<details>
  <summary>Details</summary>
Motivation: 临床筛查乳腺X线任务量大、时效性强、且需结构化文档；放射科医师需将细微影像征象稳定映射为BI-RADS、乳腺密度与规范化报告。现有VLM虽能图文生成，但多为闭源云端或耦合架构，限制隐私、可复现性与适配性。需要一个在本地运行、可复用、支持多模型与多数据集的开放框架。

Method: 构建本地多模型流水线MammoWise：支持任意Ollama托管VLM与任意乳腺摄影数据集；提供零/少样本与思维链提示；可选多模态RAG（向量库检索病例上下文）；统一评估报告生成（BERTScore、ROUGE-L）与多任务分类（BI-RADS、乳腺密度、关键征象）。比较MedGemma、LLaVA-Med、Qwen2.5-VL；并对MedGemma进行QLoRA参数高效微调以提升稳定性。

Result: - 报告生成：基线即表现稳健；少样本与RAG进一步提升BERTScore与ROUGE-L。
- 分类：可行但对模型与数据集敏感。
- QLoRA微调MedGemma后，达到BI-RADS准确率0.7545、密度0.8840、钙化0.9341，同时保持生成质量。

Conclusion: MammoWise作为本地、可扩展、可复现的VLM乳腺摄影工作流，能在保护隐私的前提下统一完成报告生成与多任务分类；少样本与RAG能提升生成效果，分类性能经轻量微调可显著提高，适合作为临床部署与研究的实用框架。

Abstract: Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

</details>


### [10] [Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models](https://arxiv.org/abs/2602.22469)
*Niamul Hassan Samin,Md Arifur Rahman,Abdullah Ibne Hanif,Juena Ahmed Noshin,Md Ashikur Rahman*

Main category: cs.CV

TL;DR: 论文提出一种训练时无改动、推理时应用的方法SCR，在多种VLM上显著降低“看图说话”中的虚构物体（幻觉），且几乎不损失描述质量与速度，优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: VLM常在图像中捏造不存在的物体。作者将其归因于空间信用坍缩：早期Transformer层的激活/注意力集中于少量视觉patch，抑制上下文证据，导致模型过度依赖语言先验。需要一种无需重训、可在推理期缓解该问题的方法。

Method: 提出Spatial Credit Redistribution（SCR）：在推理期、训练无改动地，对隐藏态进行基于注意力的“信用再分配”。具体做法是从高注意力的源patch向其空间上下文扩散/转移激活量，且以低熵输入为引导（低不确定度时更易发生坍缩）。同时进行受控消融：用均匀随机代替注意力引导的源选择以验证关键性。

Result: 在Chameleon、LLaVA、Qwen（含Qwen‑VL与Qwen2‑VL）7B/13B/30B上评测：POPE‑Adversarial幻觉率下降约4.7–6.0个百分点；CHAIR‑s下降3.7–5.2个百分点（相对降42–51%），CHAIR‑i下降2.7–4.4个百分点（相对降44–58%）；CIDEr基本不变（≤0.8个百分点）。推理开销仅+43–56 ms，比OPERA/VCD快3–6倍，比OVCD快1.3–1.7倍，同时在幻觉率与CIDEr上帕累托占优。消融显示若用随机源选择，仅剩2.6–3.4个百分点的改进，证明信用坍缩是主因且注意力引导至关重要。

Conclusion: SCR以简单的推理期干预缓解空间信用坍缩，显著降低VLM幻觉且几乎无质量与时延代价，跨模型与规模均有效，并在效能-质量上优于现有方法；注意力引导的源定位是效果关键，支持作者提出的理论框架。

Abstract: Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.

</details>


### [11] [Pix2Key: Controllable Open-Vocabulary Retrieval with Semantic Decomposition and Self-Supervised Visual Dictionary Learning](https://arxiv.org/abs/2602.22510)
*Guoyizhe Wei,Yang Jiao,Nan Xi,Zhishen Huang,Jingjing Meng,Rama Chellappa,Yan Gao*

Main category: cs.CV

TL;DR: Pix2Key 以“开放词汇视觉字典”表示查询与候选图像，在同一嵌入空间里进行意图约束匹配与多样性重排序，并结合仅用图像的自监督预训练模块 V-Dict-AE 强化细粒度属性理解；在 DFMM-Compose 上显著提升 Recall@10（+3.2，再加预训练再+2.3），同时更契合意图且保持结果多样性。


<details>
  <summary>Details</summary>
Motivation: 现有 CIR 方法要么依赖监督三元组融合而丢失细粒度线索，要么零样本管线通过图像描述与文本编辑合并，容易错过隐含意图且结果重复，难以同时兼顾意图一致性、细粒度属性与多样性。

Method: 将查询（参考图像+文本编辑）与候选图像都编码为开放词汇“视觉字典”（一组可对齐的视觉词/槽位）；在统一嵌入空间里进行基于意图的约束匹配，并加入面向多样性的重排序。提出 V-Dict-AE 自监督预训练，仅用图像数据训练字典式表征，提升细粒度属性理解，无需 CIR 专用监督。

Result: 在 DFMM-Compose 基准上，Pix2Key 单独提升 Recall@10 最多 3.2 个点；进一步加入 V-Dict-AE 预训练再提升 2.3 个点，同时提高意图一致性并维持候选列表的多样性。

Conclusion: 以视觉字典为核心的统一嵌入与自监督预训练，使 CIR 同时实现更好的意图对齐、细粒度属性保留与结果多样性，在标准基准上取得稳定增益。

Abstract: Composed Image Retrieval (CIR) uses a reference image plus a natural-language edit to retrieve images that apply the requested change while preserving other relevant visual content. Classic fusion pipelines typically rely on supervised triplets and can lose fine-grained cues, while recent zero-shot approaches often caption the reference image and merge the caption with the edit, which may miss implicit user intent and return repetitive results. We present Pix2Key, which represents both queries and candidates as open-vocabulary visual dictionaries, enabling intent-aware constraint matching and diversity-aware reranking in a unified embedding space. A self-supervised pretraining component, V-Dict-AE, further improves the dictionary representation using only images, strengthening fine-grained attribute understanding without CIR-specific supervision. On the DFMM-Compose benchmark, Pix2Key improves Recall@10 up to 3.2 points, and adding V-Dict-AE yields an additional 2.3-point gain while improving intent consistency and maintaining high list diversity.

</details>


### [12] [DisQ-HNet: A Disentangled Quantized Half-UNet for Interpretable Multimodal Image Synthesis Applications to Tau-PET Synthesis from T1 and FLAIR MRI](https://arxiv.org/abs/2602.22545)
*Agamdeep S. Chopra,Caitlin Neher,Tianyi Ren,Juampablo E. Heras Rivera,Mehmet Kurt*

Main category: cs.CV

TL;DR: 提出DisQ-HNet，用MRI（T1+FLAIR）合成tau-PET，同时可解释各模态贡献；通过PID引导的向量量化编码器分解潜在信息，以及基于边缘条件的Half-UNet解码器；在多基线下更好保留与AD相关信号，支持Braak分期、tau定位与分类；并用PID-Shapley进行模态归因。


<details>
  <summary>Details</summary>
Motivation: tau-PET能直接反映AD的tau病理，但昂贵且稀缺；希望用常规、可获得的MRI替代性地合成tau-PET，同时提供对不同MRI模态（T1、FLAIR）如何贡献于合成结果的可解释性，以便临床信任与下游任务使用。

Method: 1) 编码器：PID（部分信息分解）指导的向量量化，将来自T1与FLAIR的潜在表示划分为冗余、独特与互补成分，并以此约束学习；2) 解码器：Half-UNet，使用由结构边缘线索（而非直接跳连特征）条件化的“伪跳连”来保留解剖细节、减少语义泄漏；3) 训练与评估：与VAE、VQ-VAE、UNet等基线比较重建质量与AD相关信号保留度；4) 解释：基于PID的Shapley分析，量化各模态对合成tau摄取图样的贡献。

Result: 在重建保真度上与或优于基线，同时显著更好地保留与AD相关的信号；在Braak分期、tau病灶定位与AD分类等下游任务中取得更优性能；归因分析能够区分T1与FLAIR对不同区域摄取模式的具体贡献。

Conclusion: DisQ-HNet可在无需真实tau-PET的情况下，从MRI合成具有临床相关性的tau-PET，并提供模态级可解释性；其PID分解与Half-UNet设计共同提升了解剖细节与病理信号的保持，适合支持AD诊断与研究场景。

Abstract: Tau positron emission tomography (tau-PET) provides an in vivo marker of Alzheimer's disease pathology, but cost and limited availability motivate MRI-based alternatives. We introduce DisQ-HNet (DQH), a framework that synthesizes tau-PET from paired T1-weighted and FLAIR MRI while exposing how each modality contributes to the prediction. The method combines (i) a Partial Information Decomposition (PID)-guided, vector-quantized encoder that partitions latent information into redundant, unique, and complementary components, and (ii) a Half-UNet decoder that preserves anatomical detail using pseudo-skip connections conditioned on structural edge cues rather than direct encoder feature reuse. Across multiple baselines (VAE, VQ-VAE, and UNet), DisQ-HNet maintains reconstruction fidelity and better preserves disease-relevant signal for downstream AD tasks, including Braak staging, tau localization, and classification. PID-based Shapley analysis provides modality-specific attribution of synthesized uptake patterns.

</details>


### [13] [DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation](https://arxiv.org/abs/2602.22549)
*Zhechao Wang,Yiming Zeng,Lufan Ma,Zeqing Fu,Chen Bai,Ziyao Lin,Cheng Lu*

Main category: cs.CV

TL;DR: DrivePTS是一种面向自动驾驶场景合成的条件扩散生成框架，通过解耦几何条件、引入多视角语义文本指导与频域结构损失，显著提升生成画面的可控性、细节与结构保真度，并能稳定生成稀有场景，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法以HD地图与3D框作为条件进行场景生成，但(1) 多条件间隐式耦合导致当单一条件变化时生成失败；(2) 语义指导贫乏（简短、视角不变的caption），背景建模弱；(3) 统一加权的去噪损失忽略前景高频结构，致使模糊和形变。需要一种既能解耦控制条件又能丰富语义与强化结构细节的生成方法。

Method: 提出DrivePTS：1) 渐进式学习策略+显式互信息约束，降低HD地图与3D框等几何条件的相互依赖；2) 利用VLM生成覆盖六大语义维度的多视角分层描述，为扩散模型提供细粒度文本引导；3) 频率引导的结构损失，增强对高频成分（前景边缘/轮廓/纹理）的敏感度，提升结构保真。

Result: 在多项实验中达成SOTA的逼真度与可控性；对比方法在条件变化或稀有场景下失效时，DrivePTS依然能稳定合成，显示出更强的泛化与生成可靠性；生成结果在语义细节与前景结构方面显著更清晰。

Conclusion: 通过条件解耦的训练策略、多视角层次化语义引导以及频域结构损失，DrivePTS有效缓解了多条件互依赖与细节不足问题，实现对自动驾驶场景的高保真、强可控与更强泛化的合成。

Abstract: Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.

</details>


### [14] [SwiftNDC: Fast Neural Depth Correction for High-Fidelity 3D Reconstruction](https://arxiv.org/abs/2602.22565)
*Kang Han,Wei Xiang,Lu Yu,Mathew Wyatt,Gaowen Liu,Ramana Rao Kompella*

Main category: cs.CV

TL;DR: SwiftNDC提出以神经深度校正场为核心的快速通用3D重建框架，生成跨视角一致的深度与致密干净点云，作为3DGS的强几何初始化，从而以更少优化实现高质量网格与更佳新视角渲染，并在五个数据集上验证了提速与质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度引导的快速3D重建虽然较轻量，但存在尺度漂移、多视图不一致、以及为达高保真几何仍需大量后续优化的问题；亟需一种既快又稳、能提供跨视角一致几何初始化的方法，以提升网格重建和新视图合成效率与质量。

Method: 构建Neural Depth Correction（NDC）场，对多视角初始深度进行跨视角一致性校正；据此反投影生成致密点云，并结合稳健的重投影误差过滤获得干净、均匀分布的几何初始化；将其作为3D Gaussian Splatting（3DGS）的强几何先验，加速网格重建与提升渲染质量。

Result: 在五个数据集（两套用于网格重建、三套用于新视角合成）上，SwiftNDC能产生一致的深度与高质量致密点云，显著减少3DGS达到高质量网格所需的优化迭代与总运行时间，同时提高新视角渲染保真度。

Conclusion: 将神经深度精炼与稳健几何初始化相结合，可在不牺牲速度的前提下提升3D重建与视图合成质量。SwiftNDC提供跨视角一致的深度与可靠初始几何，既加速优化流程又提高最终重建与渲染表现。

Abstract: Depth-guided 3D reconstruction has gained popularity as a fast alternative to optimization-heavy approaches, yet existing methods still suffer from scale drift, multi-view inconsistencies, and the need for substantial refinement to achieve high-fidelity geometry. Here, we propose SwiftNDC, a fast and general framework built around a Neural Depth Correction field that produces cross-view consistent depth maps. From these refined depths, we generate a dense point cloud through back-projection and robust reprojection-error filtering, obtaining a clean and uniformly distributed geometric initialization for downstream reconstruction. This reliable dense geometry substantially accelerates 3D Gaussian Splatting (3DGS) for mesh reconstruction, enabling high-quality surfaces with significantly fewer optimization iterations. For novel-view synthesis, SwiftNDC can also improve 3DGS rendering quality, highlighting the benefits of strong geometric initialization. We conduct a comprehensive study across five datasets, including two for mesh reconstruction, as well as three for novel-view synthesis. SwiftNDC consistently reduces running time for accurate mesh reconstruction and boosts rendering fidelity for view synthesis, demonstrating the effectiveness of combining neural depth refinement with robust geometric initialization for high-fidelity and efficient 3D reconstruction.

</details>


### [15] [Quality-Aware Robust Multi-View Clustering for Heterogeneous Observation Noise](https://arxiv.org/abs/2602.22568)
*Peihan Wu,Guanjie Cheng,Yufei Tong,Meng Xi,Shuiguang Deng*

Main category: cs.CV

TL;DR: 提出QARMVC：通过质量感知机制应对多视图聚类中的异质噪声，利用重构差异估计样本级噪声强度，并在特征与融合两层以质量加权的方式抑噪与对齐，从而在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实多视图数据常含连续变化的异质噪声，现有方法多采用“干净/污染”二元假设，难以精细处理不同强度的污染，导致鲁棒性与聚类质量受限。

Method: 1) 信息瓶颈编码器提取语义并进行视图重构；2) 以“噪声破坏语义导致重构受阻”为依据，用重构误差刻画细粒度污染强度，得到样本级质量分数；3) 分层学习：- 特征层采用质量加权的对比学习，自适应抑制低质量样本的影响；- 融合层用质量加权聚合得到高质量全局共识，并通过最大化互信息对齐与校正各本地视图。

Result: 在五个基准数据集上，QARMVC在总体与异质噪声场景下均显著超过现有SOTA方法。

Conclusion: 细粒度质量评估结合分层质量加权学习，可有效提升深度多视图聚类在异质噪声下的鲁棒性与性能。

Abstract: Deep multi-view clustering has achieved remarkable progress but remains vulnerable to complex noise in real-world applications. Existing noisy robust methods predominantly rely on a simplified binary assumption, treating data as either perfectly clean or completely corrupted. This overlooks the prevalent existence of heterogeneous observation noise, where contamination intensity varies continuously across data. To bridge this gap, we propose a novel framework termed Quality-Aware Robust Multi-View Clustering (QARMVC). Specifically, QARMVC employs an information bottleneck mechanism to extract intrinsic semantics for view reconstruction. Leveraging the insight that noise disrupts semantic integrity and impedes reconstruction, we utilize the resulting reconstruction discrepancy to precisely quantify fine-grained contamination intensity and derive instance-level quality scores. These scores are integrated into a hierarchical learning strategy: at the feature level, a quality-weighted contrastive objective is designed to adaptively suppress the propagation of noise; at the fusion level, a high-quality global consensus is constructed via quality-weighted aggregation, which is subsequently utilized to align and rectify local views via mutual information maximization. Extensive experiments on five benchmark datasets demonstrate that QARMVC consistently outperforms state-of-the-art baselines, particularly in scenarios with heterogeneous noise intensities.

</details>


### [16] [Guidance Matters: Rethinking the Evaluation Pitfall for Text-to-Image Generation](https://arxiv.org/abs/2602.22570)
*Dian Xie,Shitong Shao,Lichen Bai,Zikai Zhou,Bojun Cheng,Shuo Yang,Jun Wu,Zeke Xie*

Main category: cs.CV

TL;DR: 论文指出：现有许多扩散模型的引导方法在常用偏好评测中看似优于CFG，但主要受“更大引导尺度更对齐”的评测偏差驱动；提出GA-Eval用于在等效引导强度下公平比较；并展示了一个能“刷榜但无用”的TDG以证明漏洞；最终发现多数方法在公平评测下并不优于标准CFG，甚至退化。


<details>
  <summary>Details</summary>
Motivation: 扩散模型条件生成广泛采用CFG；新涌现的引导方法声称在质量与人类偏好上更优。但作者质疑：这些提升是否真实而非评测偏差导致？尤其是人类偏好模型/指标是否系统性偏向更大的引导尺度，从而奖励语义对齐却忽视画质退化。

Method: 1) 揭示评测陷阱：常用人类偏好模型对大CFG尺度存在强偏置，导致过饱和、伪影的图像仍得高分。2) 提出GA-Eval：通过“引导尺度校准”，分离与CFG平行（等效放大对齐）与正交（真正新信息/改进）效应，实现与CFG的公平对比。3) 设计TDG：一种在传统评测下显著提分、但在实际观感与GA-Eval下无效的方法，用作“对抗性示例”暴露体系缺陷。4) 系统实证：在常规与GA-Eval两套框架下评测8种近作。

Result: - 在常规评测下，单纯增大CFG尺度即可与多数最新引导方法“打平或超越”。- 在GA-Eval（尺度校准、效应分解）下，所有方法相对标准CFG的胜率显著下降，显示真实改进有限或为负。- TDG能显著提高传统偏好分，但在GA-Eval及人眼实测中并无真实提升。

Conclusion: 当前扩散引导研究中的许多“改进”主要源自评测偏差而非方法本身；应采用如GA-Eval的尺度校准与效应分解来做公平比较。社区需要重新审视评测范式，避免被偏好模型对“大尺度对齐”的偏置误导。

Abstract: Classifier-free guidance (CFG) has helped diffusion models achieve great conditional generation in various fields. Recently, more diffusion guidance methods have emerged with improved generation quality and human preference. However, can these emerging diffusion guidance methods really achieve solid and significant improvements? In this paper, we rethink recent progress on diffusion guidance. Our work mainly consists of four contributions. First, we reveal a critical evaluation pitfall that common human preference models exhibit a strong bias towards large guidance scales. Simply increasing the CFG scale can easily improve quantitative evaluation scores due to strong semantic alignment, even if image quality is severely damaged (e.g., oversaturation and artifacts). Second, we introduce a novel guidance-aware evaluation (GA-Eval) framework that employs effective guidance scale calibration to enable fair comparison between current guidance methods and CFG by identifying the effects orthogonal and parallel to CFG effects. Third, motivated by the evaluation pitfall, we design Transcendent Diffusion Guidance (TDG) method that can significantly improve human preference scores in the conventional evaluation framework but actually does not work in practice. Fourth, in extensive experiments, we empirically evaluate recent eight diffusion guidance methods within the conventional evaluation framework and the proposed GA-Eval framework. Notably, simply increasing the CFG scales can compete with most studied diffusion guidance methods, while all methods suffer severely from winning rate degradation over standard CFG. Our work would strongly motivate the community to rethink the evaluation paradigm and future directions of this field.

</details>


### [17] [GIFSplat: Generative Prior-Guided Iterative Feed-Forward 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2602.22571)
*Tianyu Chen,Wei Xiang,Kang Han,Yu Lu,Di Wu,Gaowen Liu,Ramana Rao Kompella*

Main category: cs.CV

TL;DR: GIFSplat 提出一种纯前向、可迭代细化的3D高斯Splatting重建框架，从稀疏且无位姿视图中在秒级完成重建，并通过蒸馏冻结扩散先验进行无反传的场景自适应，较现有前馈方法显著提升PSNR（最高+2.1 dB）。


<details>
  <summary>Details</summary>
Motivation: 现有前馈3D重建虽较逐场优化更快，但在域外数据、稀疏视角下效果不足；一旦引入生成先验又难以保持秒级推理。根因在于“一次性预测”范式：模型受容量限制、缺乏推理时细化、难以持续注入先验。

Method: 1) 前向残差迭代：对当前3D高斯场进行少量仅前向的残差更新，利用渲染证据逐步细化；2) 先验蒸馏：从增强的新视角渲染中，将冻结扩散模型的信号蒸馏为高斯层级线索，无需反向传播与视角集合扩张，实现生成先验的逐场自适应，同时保持前馈效率；3) 无需相机位姿、无测试时梯度优化。

Result: 在 DL3DV、RealEstate10K、DTU 三个数据集上，较最先进前馈基线一致领先，PSNR 最高提升+2.1 dB，并在推理时保持秒级延迟。

Conclusion: 通过前向迭代细化与无反传的扩散先验蒸馏，GIFSplat在不依赖相机位姿和测试时优化的前提下，实现效率与质量的兼顾，显著优于现有前馈方法并具更强域外与稀疏视角鲁棒性。

Abstract: Feed-forward 3D reconstruction offers substantial runtime advantages over per-scene optimization, which remains slow at inference and often fragile under sparse views. However, existing feed-forward methods still have potential for further performance gains, especially for out-of-domain data, and struggle to retain second-level inference time once a generative prior is introduced. These limitations stem from the one-shot prediction paradigm in existing feed-forward pipeline: models are strictly bounded by capacity, lack inference-time refinement, and are ill-suited for continuously injecting generative priors. We introduce GIFSplat, a purely feed-forward iterative refinement framework for 3D Gaussian Splatting from sparse unposed views. A small number of forward-only residual updates progressively refine current 3D scene using rendering evidence, achieve favorable balance between efficiency and quality. Furthermore, we distill a frozen diffusion prior into Gaussian-level cues from enhanced novel renderings without gradient backpropagation or ever-increasing view-set expansion, thereby enabling per-scene adaptation with generative prior while preserving feed-forward efficiency. Across DL3DV, RealEstate10K, and DTU, GIFSplat consistently outperforms state-of-the-art feed-forward baselines, improving PSNR by up to +2.1 dB, and it maintains second-scale inference time without requiring camera poses or any test-time gradient optimization.

</details>


### [18] [Causal Motion Diffusion Models for Autoregressive Motion Generation](https://arxiv.org/abs/2602.22594)
*Qing Yu,Akihisa Watanabe,Kent Fujiwara*

Main category: cs.CV

TL;DR: 提出因果运动扩散模型（CMDM），在因果潜空间内用自回归扩散Transformer逐帧去噪，实现高保真、低时延的人体运动生成与流式合成。


<details>
  <summary>Details</summary>
Motivation: 现有全序列双向扩散限制因果性与实时性，纯自回归又易不稳定并积累误差；需要兼顾因果生成、稳定性、语义一致性与推理效率的统一方案。

Method: 1) 构建运动-语言对齐的因果VAE（MAC-VAE），将运动编码为时间因果的潜表示；2) 在该潜空间上训练自回归扩散Transformer，并采用“因果扩散强制”进行按时间顺序的去噪；3) 设计带因果不确定性的逐帧采样策略：每个新帧基于已部分去噪的历史帧预测，以加速推理并保持稳定。

Result: 在HumanML3D与SnapMoGen基准上，CMDM在语义一致性与时间平滑度上优于现有扩散与自回归方法，同时显著降低推理时延，支持长时程与流式生成。

Conclusion: 在因果对齐潜空间中结合自回归扩散与逐帧采样，可在保证语义与平滑性的同时实现低时延的人体运动生成，为实时与长时程应用提供统一框架。

Abstract: Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.

</details>


### [19] [Don't let the information slip away](https://arxiv.org/abs/2602.22595)
*Taozhe Li*

Main category: cs.CV

TL;DR: 提出一种利用背景上下文信息的目标检测模型Association DETR，在COCO val2017上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO与DETR家族尽管速度/精度出色，但主要关注前景目标特征，忽视背景上下文，导致信息流失；而背景先验（如车多在道路、野生动物多在森林）可显著帮助检测。

Method: 基于DETR范式，显式建模并关联前景与背景的上下文信息（“Association”），在特征提取与解码阶段融合/约束背景语义与前景查询，提高定位与分类的判别性。

Result: 在COCO val2017上超越当代YOLOv12、RT-DETR/RT-DETRv2等主流模型，取得SOTA（文中未给出具体mAP数值）。

Conclusion: 引入背景上下文关联能够减少信息流失、提升检测精度与鲁棒性；Association DETR验证了背景先验在通用目标检测中的有效性，并在标准数据集上取得领先表现。

Abstract: Real-time object detection has advanced rapidly in recent years. The YOLO series of detectors is among the most well-known CNN-based object detection models and cannot be overlooked. The latest version, YOLOv26, was recently released, while YOLOv12 achieved state-of-the-art (SOTA) performance with 55.2 mAP on the COCO val2017 dataset. Meanwhile, transformer-based object detection models, also known as DEtection TRansformer (DETR), have demonstrated impressive performance. RT-DETR is an outstanding model that outperformed the YOLO series in both speed and accuracy when it was released. Its successor, RT-DETRv2, achieved 53.4 mAP on the COCO val2017 dataset. However, despite their remarkable performance, all these models let information to slip away. They primarily focus on the features of foreground objects while neglecting the contextual information provided by the background. We believe that background information can significantly aid object detection tasks. For example, cars are more likely to appear on roads rather than in offices, while wild animals are more likely to be found in forests or remote areas rather than on busy streets. To address this gap, we propose an object detection model called Association DETR, which achieves state-of-the-art results compared to other object detection models on the COCO val2017 dataset.

</details>


### [20] [BetterScene: 3D Scene Synthesis with Representation-Aligned Generative Model](https://arxiv.org/abs/2602.22596)
*Yuci Han,Charles Toth,John E. Anderson,William J. Shuart,Alper Yilmaz*

Main category: cs.CV

TL;DR: BetterScene提出基于Stable Video Diffusion并结合3D高斯泼洒的增强式NVS方法，在极稀疏、非约束照片下生成一致、无伪影的新视角，并在DL3DV-10K上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 稀疏实景照片进行新视角合成常出现几何不完整、细节不一致、伪影多。现有扩散式方法多仅微调UNet且冻结VAE等组件，即使加入深度/语义等正则仍会产生视角不一致与纹理抖动。需要更强的先验利用与跨视角一致性约束。

Method: 以SVD为骨干：1) 探索扩散模型潜空间，在VAE上引入两项改造：a) 时间等变正则，使潜表示对相邻/变换帧保持一致性；b) 与视觉基础模型对齐的表示，提升语义稳定与泛化。2) 融合前馈式3D Gaussian Splatting渲染特征，作为SVD增强器输入，输出连续、无伪影的一致新视角。整体为推理时增强框架。

Result: 在DL3DV-10K上系统性评测，优于最新方法（SOTA），在一致性、细节保真和伪影减少方面显著提升。

Conclusion: 针对稀疏多视角NVS，除UNet外对VAE潜空间进行时间等变与基础模型对齐的优化，并结合3DGS特征渲染，可显著提升跨视角一致性与画质，成为实用的推理时增强方案。

Abstract: We present BetterScene, an approach to enhance novel view synthesis (NVS) quality for diverse real-world scenes using extremely sparse, unconstrained photos. BetterScene leverages the production-ready Stable Video Diffusion (SVD) model pretrained on billions of frames as a strong backbone, aiming to mitigate artifacts and recover view-consistent details at inference time. Conventional methods have developed similar diffusion-based solutions to address these challenges of novel view synthesis. Despite significant improvements, these methods typically rely on off-the-shelf pretrained diffusion priors and fine-tune only the UNet module while keeping other components frozen, which still leads to inconsistent details and artifacts even when incorporating geometry-aware regularizations like depth or semantic conditions. To address this, we investigate the latent space of the diffusion model and introduce two components: (1) temporal equivariance regularization and (2) vision foundation model-aligned representation, both applied to the variational autoencoder (VAE) module within the SVD pipeline. BetterScene integrates a feed-forward 3D Gaussian Splatting (3DGS) model to render features as inputs for the SVD enhancer and generate continuous, artifact-free, consistent novel views. We evaluate on the challenging DL3DV-10K dataset and demonstrate superior performance compared to state-of-the-art methods.

</details>


### [21] [LoR-LUT: Learning Compact 3D Lookup Tables via Low-Rank Residuals](https://arxiv.org/abs/2602.22607)
*Ziqi Zhao,Abhijit Mishra,Shounak Roychowdhury*

Main category: cs.CV

TL;DR: LoR-LUT 提出一种统一的低秩3D-LUT生成方法，用低秩残差校正配合少量基底LUT，在保持三线性插值复杂度不变的前提下，大幅减少参数与模型体积（亚MB），并在 FiveK 上达到接近专家的感知质量；另提供交互式可视化工具提升可解释性与用户信心。


<details>
  <summary>Details</summary>
Motivation: 现有3D-LUT方法多依赖稠密基底LUT融合，参数冗余且可解释性不足；同时在追求高感知质量时模型体积与计算成本上升。作者希望在不增加推理复杂度的条件下，提高图像增强/风格迁移的感知质量与可解释性，同时显著压缩模型。

Method: 在统一框架中引入“低秩残差校正（low-rank residual corrections）”与少量基底LUT联合表示颜色映射：将原本稠密LUT的表示分解为基底LUT + 低秩残差项；训练于 MIT-Adobe FiveK，推理阶段仍用标准三线性插值；并提供 LoR-LUT Viewer 通过滑条交互控制参数、可视化输入到输出的调整过程。

Result: 在 FiveK 数据集上重现接近专家修图的风格与高感知保真度；以显著更少的网络参数、残差与LUT参数实现亚MB模型规模；在相同三线性插值复杂度下优于传统以稠密基底融合的3D-LUT方法。

Conclusion: 低秩残差与基底LUT的统一建模能在不增加插值复杂度的情况下，提供紧凑、可解释且高质量的图像增强/风格迁移方案；配套的交互式可视化工具进一步提升了可解释性与用户信任，为未来基于LUT的方法提供了高效方向。

Abstract: We present LoR-LUT, a unified low-rank formulation for compact and interpretable 3D lookup table (LUT) generation. Unlike conventional 3D-LUT-based techniques that rely on fusion of basis LUTs, which are usually dense tensors, our unified approach extends the current framework by jointly using residual corrections, which are in fact low-rank tensors, together with a set of basis LUTs. The approach described here improves the existing perceptual quality of an image, which is primarily due to the technique's novel use of residual corrections. At the same time, we achieve the same level of trilinear interpolation complexity, using a significantly smaller number of network, residual corrections, and LUT parameters. The experimental results obtained from LoR-LUT, which is trained on the MIT-Adobe FiveK dataset, reproduce expert-level retouching characteristics with high perceptual fidelity and a sub-megabyte model size. Furthermore, we introduce an interactive visualization tool, termed LoR-LUT Viewer, which transforms an input image into the LUT-adjusted output image, via a number of slidebars that control different parameters. The tool provides an effective way to enhance interpretability and user confidence in the visual results. Overall, our proposed formulation offers a compact, interpretable, and efficient direction for future LUT-based image enhancement and style transfer.

</details>


### [22] [Spectrally Distilled Representations Aligned with Instruction-Augmented LLMs for Satellite Imagery](https://arxiv.org/abs/2602.22613)
*Minh Kha Do,Wei Xiang,Kang Han,Di Wu,Khoa Phan,Yi-Ping Phoebe Chen,Gaowen Liu,Ramana Rao Kompella*

Main category: cs.CV

TL;DR: 提出SATtxt：一种频谱感知的视觉-语言基础模型，训练期利用多光谱先验，推理期仅用RGB实现零样本与检索能力；通过光谱表示蒸馏与指令增强LLM对齐两阶段，在EuroSAT、BigEarthNet、ForestNet上零样本分类/检索/线性探针分别平均提升4.2%/5.9%/2.7%。


<details>
  <summary>Details</summary>
Motivation: 现实卫星系统常缺乏完整多光谱覆盖，部署端更偏好仅RGB输入；但多光谱虽信息丰富却存在波段冗余与错配，且CLIP式文本编码器语义表达受限、细粒度对齐不足，阻碍VLFMs在遥感中的应用。

Method: 两阶段框架SATtxt：1) 光谱表示蒸馏（SRD）：冻住的多光谱教师网络向RGB学生传递光谱先验，通过轻量投影器将教师多光谱特征蒸馏到RGB空间。2) 光谱落地对齐（SGA）与指令增强LLM：将蒸馏后的视觉特征与更具表达力的LLM嵌入空间对齐，超越传统CLIP文本编码器的限制；支持零样本与跨模态检索。

Result: 在EuroSAT、BigEarthNet、ForestNet三数据集上，相比基线：零样本分类平均+4.2%，检索+5.9%，线性探针+2.7%，表明在仅RGB推理下仍能保留训练中学习的光谱线索并提升性能。

Conclusion: 通过在训练期学习并蒸馏多光谱先验、在语义端采用指令增强LLM实现更强对齐，SATtxt为仅RGB推理的遥感VLM提供有效路径，兼顾可部署性与精细语义表达，显著优于传统CLIP式方案。

Abstract: Vision-language foundation models (VLFMs) promise zero-shot and retrieval understanding for Earth observation. While operational satellite systems often lack full multi-spectral coverage, making RGB-only inference highly desirable for scalable deployment, the adoption of VLFMs for satellite imagery remains hindered by two factors: (1) multi-spectral inputs are informative but difficult to exploit consistently due to band redundancy and misalignment; and (2) CLIP-style text encoders limit semantic expressiveness and weaken fine-grained alignment. We present SATtxt, a spectrum-aware VLFM that operates with RGB inputs only at inference while retaining spectral cues learned during training. Our framework comprises two stages. First, Spectral Representation Distillation transfers spectral priors from a frozen multi-spectral teacher to an RGB student via a lightweight projector. Second, Spectrally Grounded Alignment with Instruction-Augmented LLMs bridges the distilled visual space and an expressive LLM embedding space. Across EuroSAT, BigEarthNet, and ForestNet, SATtxt improves zero-shot classification on average by 4.2%, retrieval by 5.9%, and linear probing by 2.7% over baselines, showing an efficient path toward spectrum-aware vision-language learning for Earth observation. Project page: https://ikhado.github.io/sattxt/

</details>


### [23] [Coded-E2LF: Coded Aperture Light Field Imaging from Events](https://arxiv.org/abs/2602.22620)
*Tomoya Tsuchida,Keita Takahashi,Chihiro Tsutake,Toshiaki Fujii,Hajime Nagahara*

Main category: cs.CV

TL;DR: 提出Coded-E2LF：用编码光阑与静止事件相机获取4D光场，仅基于事件数据即可重建像素级精度的光场，并在真实硬件上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有类似系统需同时采集事件与强度图像，硬件复杂、限制多；作者希望仅用事件流完成光场重建，并厘清编码光阑设计中“黑色图样”的关键作用，以在理论与实践上提升重建质量与可实现性。

Method: 采用带有特定黑色图样的编码光阑与固定的事件相机，提出纯事件驱动的光场重建框架；给出理论分析说明黑图样在解耦与可观测性中的作用；改进重建算法以从事件流中恢复4D光场，并在真实成像硬件上实现与评估。

Result: 在真实三维场景上实现仅基于事件数据的4D光场重建，达到像素级精度；相较以往需事件+强度的方案，硬件更简化，重建质量与稳定性得到提升；提供开源代码与演示视频。

Conclusion: 纯事件相机结合经精心设计（含黑色图样）的编码光阑即可重建高精度4D光场，证明了事件数据对光场成像的充分性，并为更简洁、快速、低功耗的计算成像系统铺路。

Abstract: We propose Coded-E2LF (coded event to light field), a computational imaging method for acquiring a 4-D light field using a coded aperture and a stationary event-only camera. In a previous work, an imaging system similar to ours was adopted, but both events and intensity images were captured and used for light field reconstruction. In contrast, our method is purely event-based, which relaxes restrictions for hardware implementation. We also introduce several advancements from the previous work that enable us to theoretically support and practically improve light field reconstruction from events alone. In particular, we clarify the key role of a black pattern in aperture coding patterns. We finally implemented our method on real imaging hardware to demonstrate its effectiveness in capturing real 3-D scenes. To the best of our knowledge, we are the first to demonstrate that a 4-D light field with pixel-level accuracy can be reconstructed from events alone. Our software and supplementary video are available from our project website.

</details>


### [24] [CGSA: Class-Guided Slot-Aware Adaptation for Source-Free Object Detection](https://arxiv.org/abs/2602.22621)
*Boyang Dai,Zeng Fan,Zihao Qi,Meng Lou,Yizhou Yu*

Main category: cs.CV

TL;DR: 提出CGSA，将面向对象的槽位学习引入无源数据的领域自适应目标检测，通过层次槽感知与类引导对比实现域不变检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SF-DAOD多聚焦伪标签阈值或师生框架，忽视跨域数据中的对象级结构线索；需要一种利用对象中心表示、在不保留源数据条件下实现更稳健域适应的方法。

Method: 基于DETR，加入层次槽感知（HSA）逐级将图像解耦为槽位表示作为视觉先验；再以类引导槽对比（CGSC）将槽与类别语义对齐，保持语义一致并推动域不变适配；提供理论推导与消融验证。

Result: 在多组跨域数据集上取得优于以往SF-DAOD方法的性能，实验和理论分析证明所提模块与总体框架的有效性。

Conclusion: 对象中心设计在隐私敏感的无源域适应检测场景中具有前景；CGSA通过HSA与CGSC实现更强的域不变表示与检测性能。

Abstract: Source-Free Domain Adaptive Object Detection (SF-DAOD) aims to adapt a detector trained on a labeled source domain to an unlabeled target domain without retaining any source data. Despite recent progress, most popular approaches focus on tuning pseudo-label thresholds or refining the teacher-student framework, while overlooking object-level structural cues within cross-domain data. In this work, we present CGSA, the first framework that brings Object-Centric Learning (OCL) into SF-DAOD by integrating slot-aware adaptation into the DETR-based detector. Specifically, our approach integrates a Hierarchical Slot Awareness (HSA) module into the detector to progressively disentangle images into slot representations that act as visual priors. These slots are then guided toward class semantics via a Class-Guided Slot Contrast (CGSC) module, maintaining semantic consistency and prompting domain-invariant adaptation. Extensive experiments on multiple cross-domain datasets demonstrate that our approach outperforms previous SF-DAOD methods, with theoretical derivations and experimental analysis further demonstrating the effectiveness of the proposed components and the framework, thereby indicating the promise of object-centric design in privacy-sensitive adaptation scenarios. Code is released at https://github.com/Michael-McQueen/CGSA.

</details>


### [25] [Instruction-based Image Editing with Planning, Reasoning, and Generation](https://arxiv.org/abs/2602.22624)
*Liya Ji,Chenyang Qi,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出一个多模态链式推理的指令图像编辑框架，将“规划-区域推理-编辑”解耦，结合大语言模型与扩散模型，以提示/提示区域为线索实现复杂真实场景的高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有指令图像编辑常用LLM+分割+编辑的串联方案，但理解环节多为单模态，难以处理复杂场景与细粒度区域定位，限制了编辑质量与可控性。因此需要把理解与生成更紧密地耦合，以多模态推理支撑更精确的编辑。

Method: 将任务拆为三步：1) CoT规划：LLM基于指令与编辑网络能力生成可执行的子提示/步骤；2) 编辑区域推理：利用多模态大模型训练一个指令到编辑区域的生成网络，推断应修改的语义区域与掩码；3) 线索引导的编辑：在大规模文本到图像扩散模型上构建能接收“线索”（子提示与区域提示）的指令编辑网络，依据区域与步骤进行受控生成。

Result: 在多组复杂真实图像编辑任务上取得有竞争力的表现（定性与定量均优），对复杂指令、细粒度区域与多步骤编辑展现更强的可控性与一致性。

Conclusion: 多模态链式推理把理解与生成有效衔接，通过“规划-区域-编辑”的解耦与线索注入显著提升指令图像编辑的可控性与复杂场景适应能力。

Abstract: Editing images via instruction provides a natural way to generate interactive content, but it is a big challenge due to the higher requirement of scene understanding and generation. Prior work utilizes a chain of large language models, object segmentation models, and editing models for this task. However, the understanding models provide only a single modality ability, restricting the editing quality. We aim to bridge understanding and generation via a new multi-modality model that provides the intelligent abilities to instruction-based image editing models for more complex cases. To achieve this goal, we individually separate the instruction editing task with the multi-modality chain of thought prompts, i.e., Chain-of-Thought (CoT) planning, editing region reasoning, and editing. For Chain-of-Thought planning, the large language model could reason the appropriate sub-prompts considering the instruction provided and the ability of the editing network. For editing region reasoning, we train an instruction-based editing region generation network with a multi-modal large language model. Finally, a hint-guided instruction-based editing network is proposed for editing image generations based on the sizeable text-to-image diffusion model to accept the hints for generation. Extensive experiments demonstrate that our method has competitive editing abilities on complex real-world images.

</details>


### [26] [CRAG: Can 3D Generative Models Help 3D Assembly?](https://arxiv.org/abs/2602.22629)
*Zeyu Jiang,Sihang Li,Siqi Tan,Chenyang Xu,Juexiao Zhang,Julia Galway-Witham,Xue Wang,Scott A. Williams,Radu Iovita,Chen Feng,Jing Zhang*

Main category: cs.CV

TL;DR: 提出CRAG方法，将3D装配从纯位姿估计转为“装配+生成”的联合任务，可同时预测部件姿态并补全缺失几何，显著提升在野外多样对象上的装配与重建表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅做刚体位姿估计，无法利用整体形状先验，也难以在部件缺失或数量变化时补全几何，导致歧义与失败。人类装配依赖结构推理与整体形状感知，启发将两者结合。

Method: 提出CRAG框架：将装配提供的部件级结构先验与生成带来的整体形状上下文互相促进，联合优化/学习同时完成形状生成（补全、先验注入）与部件位姿预测，从而在存在缺失部件、不同几何与部件数量的情况下实现鲁棒装配。

Result: 在多类、在野数据上取得SOTA，能对多样几何、可变部件数与缺失件的对象实现更准确装配与更完整形状生成。

Conclusion: 装配与生成应联合建模：装配为生成提供结构约束，生成为装配提供整体上下文，CRAG据此实现同时补全与配准，优于单纯位姿估计方法；代码与模型将开源。

Abstract: Most existing 3D assembly methods treat the problem as pure pose estimation, rearranging observed parts via rigid transformations. In contrast, human assembly naturally couples structural reasoning with holistic shape inference. Inspired by this intuition, we reformulate 3D assembly as a joint problem of assembly and generation. We show that these two processes are mutually reinforcing: assembly provides part-level structural priors for generation, while generation injects holistic shape context that resolves ambiguities in assembly. Unlike prior methods that cannot synthesize missing geometry, we propose CRAG, which simultaneously generates plausible complete shapes and predicts poses for input parts. Extensive experiments demonstrate state-of-the-art performance across in-the-wild objects with diverse geometries, varying part counts, and missing pieces. Our code and models will be released.

</details>


### [27] [QuadSync: Quadrifocal Tensor Synchronization via Tucker Decomposition](https://arxiv.org/abs/2602.22639)
*Daniel Miao,Gilad Lerman,Joe Kileel*

Main category: cs.CV

TL;DR: 提出使用四焦张量进行相机同步与重建的新框架：将所有四焦张量组装为“块四焦张量”，证明其具有(4,4,4,4)的Tucker多线性秩，并由此设计首个四焦张量同步算法及与三/二焦张量的联合同步方法，实验在现代数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于成对本质矩阵（或三焦张量）的同步在信息利用上受限；四焦张量虽信息更丰富，但被认为难以实用。作者旨在打破这一认知，构建能在多视图场景中稳定、可扩展地利用四焦约束进行相机姿态/投影矩阵恢复的算法框架。

Method: 1) 定义并构造“块四焦张量”，证明其 admits Tucker 分解，因而其四个模的因子矩阵即为堆叠的相机矩阵，且多线性秩固定为(4,4,4,4)与相机数n无关；2) 基于此结构提出第一个四焦张量同步算法，结合Tucker分解、ADMM优化与IRLS稳健加权；3) 推导块四焦、三焦与二焦（本质）张量间的关系，提出联合同步算法，联合利用高/低阶张量信息以提升鲁棒性与精度。

Result: 在现代真实/合成数据集上的数值实验显示：所提四焦同步在姿态/相机矩阵估计的准确度与鲁棒性方面优于仅用二/三焦的方法；联合同步进一步提升在噪声、外点与视角稀疏情况下的性能，展现良好的可扩展性。

Conclusion: 四焦张量不仅具有理论价值，也能在实际多视图同步中发挥关键作用。通过块四焦张量的Tucker结构与相应优化方案，可高效、稳健地从四焦约束恢复多相机；与三/二焦信息的联合同步表明更高阶信息对提升重建质量具有重要意义。

Abstract: In structure from motion, quadrifocal tensors capture more information than their pairwise counterparts (essential matrices), yet they have often been thought of as impractical and only of theoretical interest. In this work, we challenge such beliefs by providing a new framework to recover $n$ cameras from the corresponding collection of quadrifocal tensors. We form the block quadrifocal tensor and show that it admits a Tucker decomposition whose factor matrices are the stacked camera matrices, and which thus has a multilinear rank of (4,~4,~4,~4) independent of $n$. We develop the first synchronization algorithm for quadrifocal tensors, using Tucker decomposition, alternating direction method of multipliers, and iteratively reweighted least squares. We further establish relationships between the block quadrifocal, trifocal, and bifocal tensors, and introduce an algorithm that jointly synchronizes these three entities. Numerical experiments demonstrate the effectiveness of our methods on modern datasets, indicating the potential and importance of using higher-order information in synchronization.

</details>


### [28] [Plug, Play, and Fortify: A Low-Cost Module for Robust Multimodal Image Understanding Models](https://arxiv.org/abs/2602.22644)
*Siqi Lu,Wanying Xu,Yongbin Zheng,Wenting Luan,Peng Sun,Jianhang Yao*

Main category: cs.CV

TL;DR: 提出以频域视角衡量并抑制多模态学习中的“模态偏置/失衡”，通过频率比率指标FRM检测主导模态，并用可插拔的多模态权重分配模块MWAM在训练中自适应重加权各模态，显著提升在缺失模态场景下的稳健性与总体性能。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在推理时常遇到部分模态缺失，现有方法容易因训练时偏好强势模态而在弱势或缺失模态上表现灾难性下降；需要一种能在训练过程中显式感知并纠正模态主导关系的机制，以提升鲁棒性和泛化。

Method: 1) 频域分析：将各模态特征转到频域，提出频率比率指标FRM量化模态主导程度与失衡；2) 依据FRM设计可插拔的多模态权重分配模块MWAM，在训练中动态调整各模态分支权重，抑制过强模态、强化被忽视模态；3) 模块可无缝集成于CNN或ViT等不同骨干与多种模态组合。

Result: 在多种任务与数据、不同架构（CNN/ViT）上，MWAM带来稳定增益；相较基础模型在缺失模态场景下显著更稳健；叠加到现有最先进缺失模态方法上还能进一步提升。

Conclusion: 频域可有效刻画模态主导关系；利用FRM引导的动态权重分配能缓解训练失衡，提升多模态模型在缺失模态下的鲁棒性与整体性能，并具有良好的通用性与可插拔性。

Abstract: Missing modalities present a fundamental challenge in multimodal models, often causing catastrophic performance degradation. Our observations suggest that this fragility stems from an imbalanced learning process, where the model develops an implicit preference for certain modalities, leading to the under-optimization of others. We propose a simple yet efficient method to address this challenge. The central insight of our work is that the dominance relationship between modalities can be effectively discerned and quantified in the frequency domain. To leverage this principle, we first introduce a Frequency Ratio Metric (FRM) to quantify modality preference by analyzing features in the frequency domain. Guided by FRM, we then propose a Multimodal Weight Allocation Module, a plug-and-play component that dynamically re-balances the contribution of each branch during training, promoting a more holistic learning paradigm. Extensive experiments demonstrate that MWAM can be seamlessly integrated into diverse architectural backbones, such as those based on CNNs and ViTs. Furthermore, MWAM delivers consistent performance gains across a wide range of tasks and modality combinations. This advancement extends beyond merely optimizing the performance of the base model; it also manifests as further performance improvements to state-of-the-art methods addressing the missing modality problem.

</details>


### [29] [Interactive Medical-SAM2 GUI: A Napari-based semi-automatic annotation tool for medical images](https://arxiv.org/abs/2602.22649)
*Woojae Hong,Jong Ha Hwang,Jiyong Chung,Joongyeon Choi,Hyunngun Kim,Yong Hwy Kim*

Main category: cs.CV

TL;DR: Medical-SAM2 GUI 是一款基于 Napari 的本地开源桌面工具，用于通过框/点提示与 SAM2 风格传播，对2D/3D 医学图像进行半自动标注，并支持批量病例的统一工作流与体素级导出。


<details>
  <summary>Details</summary>
Motivation: 3D 医学影像的体素级标注对算法开发与验证至关重要，但人工逐切片标注耗时昂贵；现有工具多强调逐切片交互，缺乏涵盖导航、传播、交互校正与定量导出的统一本地化工作流，且对多研究队列/标准格式支持不足。

Method: 在 Napari 中将3D 体数据视为切片序列，采用框/点提示启动并用 Medical-SAM2 进行 SAM2 式掩膜传播；提供“按病例顺序”本地工作流：根目录批量浏览、进行/跳过、盒子优先初始化（含首/末切片单对象传播）、点提示细化、保存前基于提示的校正；支持 N4 偏场校正；导出时用 SimpleITK 保持几何一致并输出每对象体积与3D 渲染。实现基于 Python、Napari、PyTorch。

Result: 实现了一个可对 DICOM/NIfTI 进行高效3D 半自动标注的应用，支持跨病例批处理、稀疏提示的掩膜传播、交互式校正与体积量化导出，并在本地环境下运行。项目已开源发布。

Conclusion: 该 GUI 以本地优先的一体化流程显著降低3D 医学图像体素级标注成本，兼顾准确性与效率，适用于研究场景的标注与体积学分析，但限定研究用途。

Abstract: Interactive Medical-SAM2 GUI is an open-source desktop application for semi-automatic annotation of 2D and 3D medical images. Built on the Napari multi-dimensional viewer, box/point prompting is integrated with SAM2-style propagation by treating a 3D volume as a slice sequence, enabling mask propagation from sparse prompts using Medical-SAM2 on top of SAM2. Voxel-level annotation remains essential for developing and validating medical imaging algorithms, yet manual labeling is slow and expensive for 3D scans, and existing integrations frequently emphasize per-slice interaction without providing a unified, cohort-oriented workflow for navigation, propagation, interactive correction, and quantitative export in a single local pipeline. To address this practical limitation, a local-first Napari workflow is provided for efficient 3D annotation across multiple studies using standard DICOM series and/or NIfTI volumes. Users can annotate cases sequentially under a single root folder with explicit proceed/skip actions, initialize objects via box-first prompting (including first/last-slice initialization for single-object propagation), refine predictions with point prompts, and finalize labels through prompt-first correction prior to saving. During export, per-object volumetry and 3D volume rendering are supported, and image geometry is preserved via SimpleITK. The GUI is implemented in Python using Napari and PyTorch, with optional N4 bias-field correction, and is intended exclusively for research annotation workflows. The code is released on the project page: https://github.com/SKKU-IBE/Medical-SAM2GUI/.

</details>


### [30] [Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache](https://arxiv.org/abs/2602.22654)
*Bowen Cui,Yuanbin Wang,Huajiang Xu,Biaolong Chen,Aixi Zhang,Hao Jiang,Zhengzheng Jin,Xu Liu,Pipei Huang*

Main category: cs.CV

TL;DR: DPCache将扩散采样加速视为全局路径规划：先用小校准集构建“路径感知代价张量”，再用动态规划选出最优关键步长，仅在关键步做全量计算，其余用缓存特征预测，最终在DiT/FLUX/混元视频上以3.5–4.9倍加速下维持或提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有缓存式加速方法多采用固定或局部自适应步长，未考虑去噪轨迹的全局结构，易造成误差积累与伪影。需要一种能全局权衡跳步误差、在保证轨迹保真前提下选择关键时刻的训练免加速框架。

Method: 1) 用小规模校准集评估在给定“前一关键步”条件下跳过不同步的路径依赖误差，构建“路径感知代价张量”。2) 基于该代价，以动态规划在整个采样区间选择总代价最小的关键时间步序列（全局调度）。3) 推理时仅在关键步做全量前向传播；中间步通过缓存特征进行高效预测，实现训练免的缓存式加速。

Result: 在DiT、FLUX与HunyuanVideo上，相比既有加速法，在约4.87×加速下ImageReward提升+0.031；在FLUX上以3.54×加速甚至超过全步基线+0.028 ImageReward，表明在加速的同时保持或提升生成质量。

Conclusion: 将扩散加速转化为全局路径规划并用路径感知代价与动态规划进行关键步选择，可在无需再训练的前提下，实现显著加速且抑制误差积累，优于现有缓存/跳步方案。

Abstract: Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.

</details>


### [31] [Scaling Audio-Visual Quality Assessment Dataset via Crowdsourcing](https://arxiv.org/abs/2602.22659)
*Renyu Yang,Jian Jin,Lili Meng,Meiqin Liu,Yilin Wang,Balu Adsumilli,Weisi Lin*

Main category: cs.CV

TL;DR: 提出一套面向众包的音视质量评估（AVQA）数据集构建方法，并据此发布目前规模与多样性最大的YT-NTU-AVQ数据集（1620段UGC A/V），含更丰富标注与公开代码平台。


<details>
  <summary>Details</summary>
Motivation: 现有AVQA数据集规模小、内容与质量多样性不足、仅有总体分数标注，限制了模型发展与多模态感知研究。

Method: 1) 设计可在多环境下可靠标注的众包主观实验框架，打破实验室限制；2) 采用系统化数据准备策略，覆盖广泛质量等级与语义场景；3) 扩展多种标注，支持研究多模态感知机制及其与内容的关系；4) 以此流程构建并发布YT-NTU-AVQ数据集与平台代码。

Result: 得到YT-NTU-AVQ：包含1620条用户生成音视频序列，规模与多样性优于既有数据集，并附带额外标注；众包流程验证了标注的可靠性。

Conclusion: 该实用流程可有效构建大规模、多样、可多维标注的AVQA数据集；YT-NTU-AVQ为多模态质量评估与感知机制研究提供了更强支撑与可复现平台。

Abstract: Audio-visual quality assessment (AVQA) research has been stalled by limitations of existing datasets: they are typically small in scale, with insufficient diversity in content and quality, and annotated only with overall scores. These shortcomings provide limited support for model development and multimodal perception research. We propose a practical approach for AVQA dataset construction. First, we design a crowdsourced subjective experiment framework for AVQA, breaks the constraints of in-lab settings and achieves reliable annotation across varied environments. Second, a systematic data preparation strategy is further employed to ensure broad coverage of both quality levels and semantic scenarios. Third, we extend the dataset with additional annotations, enabling research on multimodal perception mechanisms and their relation to content. Finally, we validate this approach through YT-NTU-AVQ, the largest and most diverse AVQA dataset to date, consisting of 1,620 user-generated audio and video (A/V) sequences. The dataset and platform code are available at https://github.com/renyu12/YT-NTU-AVQ

</details>


### [32] [ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals](https://arxiv.org/abs/2602.22666)
*Xuelu Li,Zhaonan Wang,Xiaogang Wang,Lei Wu,Manyi Li,Changhe Tu*

Main category: cs.CV

TL;DR: 提出ArtPro：一种自监督、面向多关节对象的高保真数字孪生重建框架，通过“过分割+运动假设”生成部件提案，并在优化中基于邻域运动一致性自适应合并，同时用碰撞感知的运动剪枝避免错误运动学；在合成与真实数据上显著优于现有方法，鲁棒、精准、稳定。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微渲染（如3D Gaussian Splatting）的自监督方法对初始部件分割极为敏感，常依赖启发式聚类或预训练模型，易在多部件复杂对象上陷入局部最优，导致重建与运动学估计不稳且不准。

Method: 1) 用几何特征与运动先验进行过分割，生成含可行运动假设的“部件提案”；2) 在优化过程中，依据空间邻域的运动一致性自适应地合并这些提案；3) 设计碰撞感知的运动剪枝机制，剔除不合理的运动估计；4) 置于自监督的可微渲染框架中进行端到端优化。

Result: 在合成与真实世界多部件对象数据上，ArtPro在重建精度、运动学估计稳定性与整体鲁棒性上均显著优于现有方法，能可靠处理复杂多部件结构。

Conclusion: 自监督地引入“可合并的运动提案”并结合碰撞约束，可有效克服对初始分割的依赖与局部最优问题，实现多关节对象的高保真数字孪生重建；ArtPro在准确性与稳定性上取得显著领先。

Abstract: Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.

</details>


### [33] [Monocular Open Vocabulary Occupancy Prediction for Indoor Scenes](https://arxiv.org/abs/2602.22667)
*Changqing Zhou,Yueru Luo,Han Zhang,Zeyu Jiang,Changhao Chen*

Main category: cs.CV

TL;DR: 提出LegoOcc：在仅用几何占据（占/空）弱监督下，实现室内场景的开放词汇3D占据与语义。基于3D语言嵌入高斯表示，提出不透明度感知的泊松体聚合与渐进温度衰减以缓解特征混合并稳定训练，在Occ-ScanNet上以59.50 IoU与21.05 mIoU显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 室内 embodied agents 需理解高密度、复杂布局与细粒度语义的环境，固定类目受限。现有开放词汇占据方法多为户外场景，难以迁移到室内；且完整语义标注昂贵。亟需在弱监督（仅占/空）下学到高质量几何与可对齐语言语义的3D占据。

Method: 采用3D Language-Embedded Gaussians作为统一中间表示，将细粒度几何与语言对齐的特征嵌入绑定。几何侧：发现现有高斯→占据算子在弱监督下不收敛，提出基于泊松方程的体聚合并引入opacity-aware机制稳定体渲染与占据推断。语义侧：直接将渲染特征与开放词汇分割特征对齐会发生特征混合，提出Progressive Temperature Decay，在splatting中逐步锐化不透明度，增强高斯-语言对齐。端到端以二值占据标签监督训练，推理时进行开放词汇查询与占据预测。

Result: 在Occ-ScanNet开放词汇设置下，整体占据IoU达59.50，语义mIoU达21.05。相较所有现有占据方法在IoU上更优，且在开放词汇mIoU上大幅领先以往方法。

Conclusion: 弱监督（仅占/空）下，通过语言嵌入高斯的统一表示与两项关键技术（泊松式不透明度感知体聚合、渐进温度衰减），可在室内复杂场景实现高质量的开放词汇3D占据与语义，对具身智能在无昂贵标注下的环境理解具有实用价值。

Abstract: Open-vocabulary 3D occupancy is vital for embodied agents, which need to understand complex indoor environments where semantic categories are abundant and evolve beyond fixed taxonomies. While recent work has explored open-vocabulary occupancy in outdoor driving scenarios, such methods transfer poorly indoors, where geometry is denser, layouts are more intricate, and semantics are far more fine-grained. To address these challenges, we adopt a geometry-only supervision paradigm that uses only binary occupancy labels (occupied vs free). Our framework builds upon 3D Language-Embedded Gaussians, which serve as a unified intermediate representation coupling fine-grained 3D geometry with a language-aligned semantic embedding. On the geometry side, we find that existing Gaussian-to-Occupancy operators fail to converge under such weak supervision, and we introduce an opacity-aware, Poisson-based approach that stabilizes volumetric aggregation. On the semantic side, direct alignment between rendered features and open-vocabulary segmentation features suffers from feature mixing; we therefore propose a Progressive Temperature Decay schedule that gradually sharpens opacities during splatting, strengthening Gaussian-language alignment. On Occ-ScanNet, our framework achieves 59.50 IoU and 21.05 mIoU in the open-vocabulary setting, surpassing all existing occupancy methods in IoU and outperforming prior open-vocabulary approaches by a large margin in mIoU. Code will be released at https://github.com/JuIvyy/LegoOcc.

</details>


### [34] [SPMamba-YOLO: An Underwater Object Detection Network Based on Multi-Scale Feature Enhancement and Global Context Modeling](https://arxiv.org/abs/2602.22674)
*Guanghao Liao,Zhen Liu,Liyuan Cao,Yonghui Yang,Qi Li*

Main category: cs.CV

TL;DR: 提出SPMamba-YOLO：在YOLOv8n上引入SPPELAN多尺度聚合、PSA注意力与基于Mamba的状态空间模块，实现对水下小目标与密集目标更稳健的检测，URPC2022上mAP@0.5提升>4.9%，精度与算力权衡良好。


<details>
  <summary>Details</summary>
Motivation: 水下成像存在强光衰减、颜色失真、背景杂波及目标尺度小等问题，导致常规检测器对小而密集目标和复杂背景表现不佳，需要兼顾多尺度特征、全局上下文与计算效率的新方法。

Method: 在YOLO框架上三处改进：1) SPPELAN：结合空间金字塔池化与ELAN式层聚合，扩大感受野、加强多尺度特征融合；2) PSA金字塔分割注意力：在多尺度上重标定通道/空间权重，突出有效区域、抑制背景干扰；3) 基于Mamba的状态空间模块：以线性复杂度建模长程依赖，注入全局上下文以增强鲁棒性。

Result: 在URPC2022数据集上，相比YOLOv8n基线，mAP@0.5提升超过4.9%，对小目标与密集场景尤为明显；同时保持较低计算成本，实现精度-效率的良好折中。

Conclusion: 融合多尺度增强（SPPELAN）、分层注意力（PSA）与Mamba全局建模，可显著提升水下目标检测的准确性与鲁棒性且具备高效性，适合复杂水下环境与小目标密集检测场景。

Abstract: Underwater object detection is a critical yet challenging research problem owing to severe light attenuation, color distortion, background clutter, and the small scale of underwater targets. To address these challenges, we propose SPMamba-YOLO, a novel underwater object detection network that integrates multi-scale feature enhancement with global context modeling. Specifically, a Spatial Pyramid Pooling Enhanced Layer Aggregation Network (SPPELAN) module is introduced to strengthen multi-scale feature aggregation and expand the receptive field, while a Pyramid Split Attention (PSA) mechanism enhances feature discrimination by emphasizing informative regions and suppressing background interference. In addition, a Mamba-based state space modeling module is incorporated to efficiently capture long-range dependencies and global contextual information, thereby improving detection robustness in complex underwater environments. Extensive experiments on the URPC2022 dataset demonstrate that SPMamba-YOLO outperforms the YOLOv8n baseline by more than 4.9\% in mAP@0.5, particularly for small and densely distributed underwater objects, while maintaining a favorable balance between detection accuracy and computational cost.

</details>


### [35] [ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport](https://arxiv.org/abs/2602.22678)
*Quoc-Khang Tran,Minh-Thien Nguyen,Nguyen-Khang Pham*

Main category: cs.CV

TL;DR: ViCLIP-OT 是一个面向越南语的视觉-语言基础模型，通过在 CLIP 对比学习上引入相似度图正则的最优传输（SIGROT）损失，显著提升图像-文本检索的跨模态对齐与检索效果，尤其在低资源与零样本场景中超越 CLIP/SigLIP。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型主要针对高资源语言优化，直接迁移到越南语等低资源语言时跨模态对齐差、模态鸿沟大、检索性能不足。需要一种既能增强全局跨模态一致性、又具可扩展性的训练机制，专为低资源语言场景提升检索表现。

Method: 在 CLIP 风格的图文对比学习框架中，加入“相似度图正则的最优传输”（SIGROT）损失：先基于样本间相似度构建图来刻画语义结构，再用最优传输在图像-文本嵌入间进行质量与结构一致性的对齐，同时以图正则约束保持全局语义邻接关系，缓解模态间分布差异。整体作为可并行、可扩展的训练目标，提升全局一致与细粒度匹配。

Result: 在三大越南语基准（UIT-OpenViIC、KTVIC、Crossmodal-3600）上，ViCLIP-OT 在域内与零样本均优于 CLIP 与 SigLIP：如 UIT-OpenViIC 平均 Recall@K 达 67.34%，较 CLIP 提升 5.75 个百分点；在 Crossmodal-3600 零样本上较 CLIP 提升 11.72 个百分点。嵌入空间分析显示跨模态对齐更好、模态鸿沟更小。

Conclusion: 将 SIGROT 融入 CLIP 式训练能够有效、可扩展地缩小模态鸿沟并提升低资源语言的跨模态检索性能。该策略对越南语与其他欠资源语言的多媒体检索具有实际应用价值。

Abstract: Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.

</details>


### [36] [SUPERGLASSES: Benchmarking Vision Language Models as Intelligent Agents for AI Smart Glasses](https://arxiv.org/abs/2602.22683)
*Zhuohang Jiang,Xu Yuan,Haohao Qu,Shanru Lin,Kanglong Liu,Wenqi Fan,Qing Li*

Main category: cs.CV

TL;DR: 论文提出智能眼镜场景下的VQA新基准SUPERGLASSES与智能体SUPERLENS。基准基于真实眼镜数据，涵盖多域多问类并含搜索轨迹与推理标注；评测26个VLM显示明显差距。SUPERLENS结合目标检测、查询解耦与多模态网页检索实现检索增强回答，SOTA，超越GPT-4o 2.19%。


<details>
  <summary>Details</summary>
Motivation: 现有为智能眼镜改造的VLM多在传统数据集训练/评测，缺乏真实佩戴视角与任务特性（先准确认定关注目标再检索外部知识）。需一套能反映真实使用情境、并促进模型在“物体定位→知识检索→回答”链条上协同优化的基准与方法。

Method: 1) 基准SUPERGLASSES：从智能眼镜真实采集的2,422个第一视角图像-问题对，覆盖14个图像域与8类查询；附完整搜索轨迹与推理标注；对26个代表性VLM进行统一评测。2) 方法SUPERLENS：面向眼镜VQA的多模态代理，管线含自动目标检测以锁定关注对象、查询解耦（将识别与知识检索子任务分离）、多模态网页搜索与检索增强生成，最终融合回答。

Result: 基准上各模型差距显著；所提SUPERLENS在该基准上取得SOTA，整体性能超过GPT-4o 2.19%。

Conclusion: 真实智能眼镜数据与任务分解对提升VQA至关重要；通用VLM在该场景存在短板，需要面向任务的代理式方案与检索增强。SUPERGLASSES为评测与分析提供依据，SUPERLENS验证了针对性设计的有效性。

Abstract: The rapid advancement of AI-powered smart glasses, one of the hottest wearable devices, has unlocked new frontiers for multimodal interaction, with Visual Question Answering (VQA) over external knowledge sources emerging as a core application. Existing Vision Language Models (VLMs) adapted to smart glasses are typically trained and evaluated on traditional multimodal datasets; however, these datasets lack the variety and realism needed to reflect smart glasses usage scenarios and diverge from their specific challenges, where accurately identifying the object of interest must precede any external knowledge retrieval. To bridge this gap, we introduce SUPERGLASSES, the first comprehensive VQA benchmark built on real-world data entirely collected by smart glasses devices. SUPERGLASSES comprises 2,422 egocentric image-question pairs spanning 14 image domains and 8 query categories, enriched with full search trajectories and reasoning annotations. We evaluate 26 representative VLMs on this benchmark, revealing significant performance gaps. To address the limitations of existing models, we further propose SUPERLENS, a multimodal smart glasses agent that enables retrieval-augmented answer generation by integrating automatic object detection, query decoupling, and multimodal web search. Our agent achieves state-of-the-art performance, surpassing GPT-4o by 2.19 percent, and highlights the need for task-specific solutions in smart glasses VQA scenarios.

</details>


### [37] [No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings](https://arxiv.org/abs/2602.22689)
*Joonsung Jeon,Woo Jae Kim,Suhyeon Ha,Sooel Son,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: MoFit是一种无需真实字幕的扩散模型成员推断攻击（MIA）方法，通过对目标模型生成流形进行“拟合”，在无文本条件下也能判断图像是否参与训练，表现优于基于VLM字幕的基线，接近依赖真字幕的方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对潜变量扩散模型的MIA多依赖真实图文对，现实中往往只有图像而无公开字幕；用VLM自动字幕替代会引入偏差，显著削弱攻击力。因此需要一种不依赖文本条件、仍能放大成员与非成员之间可分性的审计方法，以评估隐私与版权风险。

Method: MoFit分两阶段：1）模型拟合的代理优化：在查询图像上学习一个微扰，使其成为位于模型无条件先验、且更可能由成员样本塑造的“代理”输入；2）代理驱动的嵌入提取：从该代理中提取与模型匹配的嵌入，用作与原图不匹配的条件，计算条件损失。该嵌入会对成员样本诱发更大的条件损失响应，而对非成员影响较小，从而提升可分性。

Result: 在多数据集与多种扩散模型上，MoFit持续优于使用VLM字幕的基线，并与需要真字幕的最强方法取得相近的MIA性能。

Conclusion: 即便缺乏真实字幕，也可通过对生成模型流形进行显式拟合来进行有效的成员推断审计。MoFit为扩散模型隐私与版权风险评估提供了更现实且强健的无字幕方案。

Abstract: Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.

</details>


### [38] [GFRRN: Explore the Gaps in Single Image Reflection Removal](https://arxiv.org/abs/2602.22695)
*Yu Chen,Zewei He,Xingyu Liu,Zixuan Chen,Zheming Lu*

Main category: cs.CV

TL;DR: 提出GFRRN用于单幅图像反射去除，通过PEFT对齐语义、统一合成/真实反射标签，并引入G-AFLB与DAA以自适应频率学习与动态注意，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 双分支交互方法虽强，但存在两大缺口：1) 预训练特征与去反射特征的语义不匹配，影响特征交互与迁移；2) 合成与真实数据的反射标注不一致，训练监督存在域偏移。作者旨在弥合上述“语义/标签”鸿沟，进一步提升SIRR的泛化与性能。

Method: - 采用参数高效微调（PEFT），在预训练主干中插入可学习的Mona层以对齐训练方向与特征语义。
- 设计标签生成器，对合成与真实数据的反射标签进行统一（规范化/一致化），缓解监督偏差。
- 提出高斯式自适应频率学习块（G-AFLB），自适应建模与融合频率先验。
- 引入动态代理注意力（DAA），替代窗口注意力，能在跨窗口（inter）与窗口内（intra）动态分配重要性。
- 将上述组件整合为无缝差距的GFRRN框架。

Result: 在多项基准上进行广泛实验，GFRRN在客观指标与主观视觉质量上均优于现有SOTA的SIRR方法，验证了各组件有效性（含消融）。

Conclusion: 通过PEFT对齐语义、标签统一、频率自适应与动态注意联合设计，GFRRN成功弥合预训练语义和数据标注差距，实现更强的反射去除性能与泛化，优于现有方法。

Abstract: Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.

</details>


### [39] [UFO-DETR: Frequency-Guided End-to-End Detector for UAV Tiny Objects](https://arxiv.org/abs/2602.22712)
*Yuankai Chen,Kai Lin,Qihong Wu,Xinxuan Yang,Jiashuo Lai,Ruoen Chen,Haonan Shi,Minfan He,Meihua Wang*

Main category: cs.CV

TL;DR: 提出UFO-DETR，一种面向无人机(UAV)小目标的端到端检测框架，融合LSKNet主干、DAttention+AIFI多尺度关系建模与DynFreq-C3频域增强，小目标/多尺度性能与效率均优于RT-DETR-L，适配边缘计算。


<details>
  <summary>Details</summary>
Motivation: UAV图像中小目标占比高、尺度变化大且密集，通用检测器与手工设计组件难兼顾精度与复杂度，尤其在边缘端算力受限场景，需要更高效且对小目标更友好的端到端检测方案。

Method: 1) 主干：采用LSKNet以更优感受野与更少参数提取特征；2) 关系建模：引入DAttention与AIFI，灵活刻画多尺度空间关系，提高多尺度检测能力；3) 频域增强：设计DynFreq-C3进行跨空间的频率特征增强，突出小目标关键信息；4) 端到端：整体以DETR范式训练推理，面向边缘部署优化计算效率。

Result: 在实验中，UFO-DETR相较RT-DETR-L在检测精度与计算效率上均取得显著提升，表明其在UAV小目标检测与边缘部署上的优势。

Conclusion: UFO-DETR通过LSKNet、DAttention+AIFI与DynFreq-C3的协同，实现对UAV小目标的高效、鲁棒检测，为UAV边缘计算提供更优的精度-效率折中。

Abstract: Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity. To address these challenges, this paper proposes an end-to-end object detection framework, UFO-DETR, which integrates an LSKNet-based backbone network to optimize the receptive field and reduce the number of parameters. By combining the DAttention and AIFI modules, the model flexibly models multi-scale spatial relationships, improving multi-scale target detection performance. Additionally, the DynFreq-C3 module is proposed to enhance small target detection capability through cross-space frequency feature enhancement. Experimental results show that, compared to RT-DETR-L, the proposed method offers significant advantages in both detection performance and computational efficiency, providing an efficient solution for UAV edge computing.

</details>


### [40] [SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs](https://arxiv.org/abs/2602.22716)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Liangyu Yuan,Mingkai Li,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Qing Jiang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 提出SoPE，用球坐标位置编码替代RoPE，统一建模3D位置与方向，并引入多尺度频率混合，显著提升3D LVLM在多基准与真实部署中的空间理解与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有3D LVLM沿用RoPE，但RoPE对3D点云的空间结构保持不足：无法有效编码三维拓扑与角度关系，相对距离忽略方向性，导致对方向变化敏感度差，限制多模态几何理解。

Method: 将点云token索引映射到球坐标(半径r、仰角θ、方位角φ)，在注意力中以球坐标构造位置相对关系与角度依赖，实现位置与方向统一编码；并设计多尺度频率混合，将不同频域的特征进行融合，增强表达能力与尺度鲁棒性。

Result: 在多个3D场景基准上优于基线RoPE与其他位置编码；在真实部署中同样表现稳健，显示更强的泛化和更一致的几何表示。

Conclusion: 球坐标位置编码SoPE能更好保持点云几何与方向信息，提升3D多模态空间理解；配合多尺度频率混合实现更强表达与泛化，适合3D LVLM广泛应用。

Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.

</details>


### [41] [IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling](https://arxiv.org/abs/2602.22717)
*Shuoqi Chen,Yujia Wu,Geoffrey P. Luke*

Main category: cs.CV

TL;DR: 提出一种基于扩散/随机微分方程（Image Restoration SDE）框架的超声去斑方法，利用从无斑MRI合成的配对数据进行监督训练，在模拟测试集上优于传统与深度学习基线，并能用跨模型方差量化不确定性；对探头参数域移敏感，提示需多样化训练与自适应以提升临床稳健性。


<details>
  <summary>Details</summary>
Motivation: 超声成像实时无创、成本低，但散斑噪声与伪影降低对比度与边缘可辨性，影响诊断；现有滤波或学习方法常在去斑与结构保真间权衡，且缺乏不确定性评估与对域移的系统考察。

Method: 在Image Restoration SDE（扩散式复原）框架下构建去斑模型；用Matlab UltraSound Toolbox从无斑的MRI合成带斑超声，形成大规模成对（带斑/无斑）数据以做监督学习；推理阶段输出去斑图并通过多模型（或多次采样）方差估计像素级不确定性；分析对不同探头设置的敏感性以评估域移。

Result: 在留出模拟测试集上，相比经典滤波（如各向异性扩散等）与近期学习基线，方法在定量指标上持续领先，并能在边缘与对比度保持的同时抑制散斑；跨模型方差与重建误差显著正相关，可指示潜在失败区域；对探头参数变化出现性能下降，显示域移存在。

Conclusion: 基于扩散SDE的去斑在模拟数据上效果优于现有方法，并提供有用的不确定性提示；然而对成像参数的域移敏感，需通过更丰富的模拟/实采数据与自适应策略来提升临床可用性。

Abstract: Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.

</details>


### [42] [HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2602.22727)
*Yangguang Lin,Quan Fang,Yufei Li,Jiachen Sun,Junyu Gao,Jitao Sang*

Main category: cs.CV

TL;DR: 提出HulluEdit：一种对抗LVLM物体幻觉的单次前向、无参考、可控干预方法，通过正交子空间编辑抑制先验引发的幻觉，同时不伤及视觉证据；在POPE/CHAIR上SOTA，保持MME等通用能力与推理效率。


<details>
  <summary>Details</summary>
Motivation: LVLM常发生物体幻觉，现有方法要么依赖昂贵参考模型与多次解码，效率低；要么做静态全局编辑，易误伤真实视觉信号，准确性受损。需要一种同时高效、精准、对视觉证据无干扰的干预机制。

Method: 提出正交子空间编辑：将模型隐状态分解为互相正交的三个子空间——(1)视觉证据；(2)冲突先验（易致幻觉的语言/统计偏置）；(3)残余不确定性。在解码时单次前向中对先验子空间施加抑制或调整，数学上保证对视觉子空间零干扰，从而在不破坏视觉对齐的前提下降低幻觉。无需外部参考模型与对比解码。

Result: 在POPE、CHAIR等幻觉基准上取得SOTA的幻觉降低效果；在MME等通用能力评测上性能保持；推理效率高于需要多次前向或参考模型的方法。优于对比解码与静态子空间编辑基线。

Conclusion: 正交子空间的可分解与定向编辑为抑制LVLM幻觉提供了有效路径：选择性压制致幻先验即可显著减少幻觉，同时保留视觉扎根与整体能力；方法高效通用，具备跨架构适用性。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.

</details>


### [43] [Asymmetric Idiosyncrasies in Multimodal Models](https://arxiv.org/abs/2602.22734)
*Muzi Tao,Chufan Shi,Huijuan Wang,Shengbang Tong,Xuezhe Ma*

Main category: cs.CV

TL;DR: 论文提出用分类框架量化字幕（图像描述）模型的风格指纹及其对文生图系统的影响：文本端能高精度识别来源模型，但这些风格在生成图像中几乎消失，揭示文生图对提示的关键信息保真不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像字幕模型可能带有独特风格，但我们缺少系统性方法去衡量这些风格以及它们如何在跨模态（文本→图像）过程中被保留或丢失；同时也需要客观度量文生图模型的“遵循提示”能力。

Method: 构建分类任务：给定一段生成的字幕文本或其对应生成图像，训练神经网络预测字幕来源模型。比较文本分类与图像分类的准确率以评估风格指纹在跨模态转换中的可辨识度；并对失败案例做细粒度分析（细节层次、颜色纹理强调、场景中物体分布等差异）。

Result: 文本侧分类准确率极高（99.70%），说明不同字幕模型在语言上有显著且可学习的风格指纹；而在图像侧，这些指纹大幅减弱，即使是SOTA的Flux生成图像，来源分类最高仅约50%。

Conclusion: 提出的基于分类的评估框架可量化字幕模型的风格特征与文生图系统的提示保真度；当前文生图未能充分保留字幕中的关键信息维度（细节、颜色纹理、物体布局），提示需要改进跨模态对齐与提示跟随能力。

Abstract: In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.

</details>


### [44] [AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation](https://arxiv.org/abs/2602.22740)
*Tongfei Chen,Shuo Yang,Yuguang Yang,Linlin Yang,Runtang Guo,Changbai Li,He Long,Chunyu Xie,Dawei Leng,Baochang Zhang*

Main category: cs.CV

TL;DR: 提出一种用于指代图像分割（RIS）的训练策略：对像素级视觉-语言对齐进行显式估计，并在优化中屏蔽/过滤对齐差的区域，聚焦可信线索，从而取得SOTA表现并提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RIS方法常受噪声监督与语言-视觉错配影响：文本表达与图像局部区域对齐不稳定，训练时会被误导，导致分割掩码漏检/误检与对复杂描述泛化差。作者希望在训练阶段辨别并抑制这类低对齐区域，提升模型对真正相关像素的学习效率与泛化能力。

Method: 提出Alignment-Aware Masked Learning (AML)：1) 估计像素级视觉-语言对齐度（如以跨模态注意/相似度评分得到对齐图）；2) 基于阈值或排名对对齐差的像素进行掩蔽，从损失中剔除或降权，保留高对齐区域强化监督；3) 在标准RIS框架上作为训练策略无缝集成，无需修改推理流程。

Result: 在RefCOCO系列数据集上达到最新SOTA指标；在不同表述多样性和复杂场景下表现更稳健，较基线更少受噪声文本或背景干扰影响。

Conclusion: 通过在训练中引入对齐感知的像素级掩蔽，AML能有效过滤不可信监督信号，专注可靠线索，从而显著提升RIS精度与鲁棒性，并可作为通用训练策略集成到现有方法中。

Abstract: Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and scenarios

</details>


### [45] [ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control](https://arxiv.org/abs/2602.22742)
*Akihisa Watanabe,Qing Yu,Edgar Simo-Serra,Kent Fujiwara*

Main category: cs.CV

TL;DR: ProjFlow 是一种无需训练的采样器，把许多动画任务表述为线性逆问题，在采样过程中以零样本方式精确满足线性空间约束，同时保持动作自然性。核心是一个编码骨架拓扑的运动学感知度量，使投影校正能在全身一致分配，避免生硬伪影；并通过随时间衰减的伪观测处理稀疏输入（如长间隔关键帧补全）。在动作修补与2D转3D提升任务上，ProjFlow精确满足约束，真实感不逊于零样本基线，且与训练式控制器竞争。


<details>
  <summary>Details</summary>
Motivation: 现有受控人体动作生成要么需任务特定训练、要么依赖缓慢优化；硬性约束常破坏动作自然度。作者观察到大量动画任务可表述为线性逆问题，因而希望在不重新训练的前提下，既能严格满足空间约束，又能保留真实感与高效率。

Method: 提出 ProjFlow：基于扩散/流式采样的训练后控制器，通过在每一步对样本进行到线性约束集的投影实现“精确约束”。关键创新是运动学感知度量（考虑骨架拓扑与关节耦合），用于分配修正量，避免 naive 欧氏投影带来的不自然。对稀疏观测，引入随时间衰减的伪观测，使早期强约束、后期弱化，从而在长间隙补全中兼顾可行性与自然性。

Result: 在动作修补与2D到3D姿态提升上，ProjFlow实现对线性空间约束的严格满足；与零样本基线相比，动作真实感相当或更好；与需训练的控制器相比，质量具有竞争力，同时免训练、效率更高。

Conclusion: 将受控动作生成统一为线性逆问题并用训练外的投影采样器解决是有效路径。运动学感知度量使硬约束与自然度兼得；时间衰减伪观测提升稀疏条件下的稳定性。ProjFlow在多任务上实现零样本、精确约束与高真实感，显示出实用潜力。

Abstract: Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.

</details>


### [46] [SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation](https://arxiv.org/abs/2602.22745)
*Fengming Liu,Tat-Jen Cham,Chuanxia Zheng*

Main category: cs.CV

TL;DR: 提出SPATIALALIGN：面向文本生成视频模型的自改进框架，使模型更好遵循文本中描述的动态空间关系（DSR），通过零阶正则化DPO微调与几何度量DSR-SCORE评估，显著提升空间对齐。


<details>
  <summary>Details</summary>
Motivation: 现有T2V方法偏重画面美感与连贯性，却常忽视或错误执行“谁在何处、相对谁如何移动”等动态空间约束，导致语义对齐不足、可控性差与下游应用受限。作者希望在无需强依赖外部VLM打分的前提下，系统性提升T2V对DSR的遵循能力。

Method: 1) 提出SPATIALALIGN自改进框架：以偏好优化为核心，通过生成候选视频并根据自动度量选择偏好样本，迭代微调基础T2V。2) 设计零阶正则化Direct Preference Optimization（DPO）：在不可微视频生成分布下，以零阶估计与正则项稳定训练，将模型推向更高DSR一致性。3) 提出DSR-SCORE：基于几何的可解释指标，从对象检测/跟踪与位置-轨迹关系中量化“左/右、前/后、靠近/远离、环绕、穿过”等动态关系，替代依赖VLM的黑盒评估。4) 构建覆盖多样DSR的文本-视频数据集，用于训练与评测。

Result: 在多个包含DSR约束的基准与自建数据上，微调后的模型在空间关系遵循度显著高于基线；DSR-SCORE与人类偏好高度相关；整体保持或轻微牺牲美学质量的同时，明显提升空间一致性。

Conclusion: 几何可解释的度量结合零阶正则化DPO，可有效强化T2V模型对动态空间关系的执行；数据与代码将开源，为后续可控T2V与空间推理研究提供基准与工具。

Abstract: Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.

</details>


### [47] [Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval](https://arxiv.org/abs/2602.22759)
*Yuan-Chih Chen,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出统一的“隐藏编码”恢复框架，可在后处理与生成期水印两种范式下，同时支持事实检索与图像重建；在新建的ImageNet-S上验证效果良好，并与多种水印流程兼容。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于深度伪造的检测与篡改定位，却缺乏对被篡改内容的事实级恢复与可还原能力；需要一种既能从水印中取回语义事实、又能在视觉上重建原貌的通用方法与评测基准。

Method: 提出统一的隐藏编码恢复框架：将语义与感知信息压缩为紧凑的hidden-code，通过多尺度向量量化进行细粒度表征；并以条件Transformer增强上下文推理，实现从两类水印（事后添加与生成期嵌入）中进行检索与重建；框架与多种水印流水线兼容。另构建ImageNet-S基准，提供配对的图像-标签事实检索任务以系统评测。

Result: 在ImageNet-S上进行大量实验，方法在事实检索与图像重建两方面均取得有竞争力/有前景的性能，同时证明对多样水印管线保持兼容。

Conclusion: 该框架为超越“检测/定位”的通用图像恢复奠定基础，可在不同水印范式下统一实现事实级检索与视觉重建，显示出推广潜力。

Abstract: Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.

</details>


### [48] [TrajTok: Learning Trajectory Tokens enables better Video Understanding](https://arxiv.org/abs/2602.22779)
*Chenhao Zheng,Jieyu Zhang,Jianing Zhang,Weikai Huang,Ashutosh Kumar,Quan Kong,Oncel Tuzel,Chun-Liang Li,Ranjay Krishna*

Main category: cs.CV

TL;DR: 提出TrajTok：端到端、与视频模型共训的轨迹式视频分词器，动态按语义复杂度调节token粒度，独立于视频时长；在CLIP式模型与多模态任务中提升精度与长视频效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频分词多用patch化，token数随帧数线性增长，冗余大、效率与可扩展性差；近来的轨迹式分词虽可解耦时长与token数，但依赖外部分割/跟踪流水线，慢且与下游无关，难以自适应任务。

Method: 提出TrajTok模块：统一时空“分割器”，在单次前向中对像素在时空上做隐式聚类，直接生成对象轨迹token；与下游视频模型端到端联合训练，依据语义复杂度动态调整token粒度，优先优化下游适配性而非逐像素精度。基于此实现从零训练的视频CLIP（TrajViT2），并将TrajTok用作探测头（TrajAdapter）或视觉-语言对齐连接器（TrajVLM）。

Result: TrajViT2在分类与检索基准上于同尺度下达到最优精度，效率与最佳token合并方法相当；TrajTok作为适配/对齐组件在长视频推理任务上表现尤佳。

Conclusion: 端到端的轨迹式分词可在不随时长增长token的前提下，动态匹配语义复杂度，提升视频理解精度与长视频效率；TrajTok通用、轻量，可作为分词器或多模态连接件，具备良好扩展性。

Abstract: Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.

</details>


### [49] [SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation](https://arxiv.org/abs/2602.22785)
*Ling Wang,Hao-Xiang Guo,Xinzhou Wang,Fuchun Sun,Kai Sun,Pengkun Liu,Hang Xiao,Zhong Wang,Guangyuan Fu,Eric Li,Yang Liu,Yikai Wang*

Main category: cs.CV

TL;DR: SceneTransporter提出从单张图像端到端生成结构化3D场景的方法，通过在组合式DiT去噪循环中引入熵正则最优传输(OT)以全局相关分配，解决以往仅有部件级生成而无法形成清晰实例的问题，显著提升实例一致性与几何保真。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能生成部件级3D，但在开放世界场景中难以将部件组织为独立实例，分析表明症结在于模型内部的分配机制缺乏结构约束，导致补丁-潜变量纠缠与实例碎片化。

Method: 将结构化3D场景生成重构为“全局相关分配”问题：在组合式DiT去噪循环内求解带熵正则的最优传输目标。所得传输计划用于：1) 以传输矩阵门控跨注意力，强制图像patch与部件级3D潜变量一对一独占路由，避免纠缠；2) 借助OT的竞争性与基于边界的cost正则，鼓励相似patch聚合，形成连贯对象并抑制碎片化。

Result: 在开放世界场景生成上，相比现有方法显著提升实例级连贯性与几何保真；消融与实验验证OT约束在组织实例与防止碎片化方面有效。

Conclusion: 将结构化3D场景生成表述为OT驱动的全局相关分配，并在DiT去噪循环中求解，可通过跨注意力门控与聚合正则，稳健地从单张图像生成实例明确、几何一致的3D场景，优于现有方法；代码与模型将开源。

Abstract: We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.

</details>


### [50] [Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning](https://arxiv.org/abs/2602.22791)
*Taishu Arashima,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: 提出一种将自监督骨架表征融入的人体轨迹预测方法，通过掩码自编码预训练增强对关节缺失的鲁棒性，在遮挡多的场景中优于基线，且在无到中度缺失下不牺牲精度。


<details>
  <summary>Details</summary>
Motivation: 现实场景骨架数据常因遮挡导致关节缺失，现有将骨架序列与轨迹融合的方法在这种噪声下性能显著下降，亟需一种对缺失关节更鲁棒的骨架表示以提升轨迹预测的稳定性与准确性。

Method: 采用自监督的掩码自编码（MAE）对人体骨架序列进行预训练，学习能在关节部分缺失时仍可恢复/表征动作结构的鲁棒骨架特征；将该预训练的骨架表征模块与轨迹预测框架集成，实现对轨迹与骨架信息的联合建模与预测。

Result: 在高遮挡、关节缺失的场景中，该方法在不牺牲常规场景预测精度的前提下，显著提升对缺失骨架数据的鲁棒性；在“干净到中度缺失”的范围内，性能稳定且持续优于多种基线模型。

Conclusion: 基于MAE的自监督骨架表征能有效缓解关节缺失带来的影响，使轨迹预测在复杂遮挡环境中更稳健，同时保持正常条件下的精度，显示出在实际导航与监控应用中的潜力。

Abstract: Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.

</details>


### [51] [GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation](https://arxiv.org/abs/2602.22800)
*Hanliang Du,Zhangji Lu,Zewei Cai,Qijian Tang,Qifeng Yu,Xiaoli Liu*

Main category: cs.CV

TL;DR: 提出GSTurb框架，结合光流引导的倾斜校正与Gaussian Splatting建模非等相性模糊，在多帧上联合优化高斯参数，显著提升湍流退化图像恢复；在ATSyn-static与真实数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 大气湍流在远距离成像中会引起像素位移（tilt）与空间变化模糊（非等相性），现有方法难以同时精确校正两者并在合成与真实场景中稳健泛化，需要一个统一、可优化的表示与跨帧约束来提升恢复质量。

Method: 构建GSTurb：以光流指导的倾斜校正对齐多帧；用Gaussian Splatting参数化并渲染非等相性模糊，将“倾斜与模糊”以高斯参数表示；在多帧间联合优化这些参数以恢复清晰图像。

Result: 在ATSyn-static上达到PSNR 27.67 dB、SSIM 0.8735，相比SOTA提升PSNR 1.3 dB（+4.5%）、SSIM 0.048（+5.8%）；在TSRWGAN Real-World与CLEAR等真实数据集上，定性与定量均显著优于现有方法。

Conclusion: 光流引导的倾斜校正与Gaussian Splatting相结合，可有效建模并校正湍流导致的位移与非等相性模糊，在合成与真实数据中均提升图像恢复质量；代码将开源，具备实际应用潜力。

Abstract: Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.

</details>


### [52] [PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning](https://arxiv.org/abs/2602.22809)
*Mingde Yao,Zhiyuan You,Tam-King Man,Menglu Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: 论文提出PhotoAgent，一个无需逐步用户指令即可自动进行多步图像美学编辑的系统；通过树搜索规划、闭环执行与记忆反馈提升指令遵从与画面质量，并发布UGC-Edit基准与美学奖励模型，实验显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有指令式图像编辑对用户编写精细、多步指令依赖强，任务分解与顺序规划负担重，导致编辑质量与稳定性受限；缺乏贴近现实UGC场景的系统性评测与可学习的美学评价信号。

Method: 将自动图像编辑建模为长时序决策问题：1) 从用户美学意图出发进行显式美学规划；2) 采用树搜索规划多步编辑动作序列；3) 在闭环框架中结合记忆与视觉反馈迭代 refine；4) 不需逐步用户提示；5) 构建UGC-Edit基准（7000张照片）与学习型美学奖励模型；6) 制作包含1017张照片的测试集以系统评估。

Result: 在多项实验中，PhotoAgent在指令遵从（instruction adherence）与视觉质量两方面持续优于多种基线方法；基于UGC-Edit与奖励模型的评估验证其稳定改进。

Conclusion: 显式美学规划与树搜索驱动的闭环多步编辑可有效实现自治图像编辑，减少用户负担并稳定提升画质与一致性；所提出的基准与奖励模型为现实场景评测提供了可靠支撑。

Abstract: With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.

</details>


### [53] [CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation](https://arxiv.org/abs/2602.22821)
*Tong Wang,Yaolei Qi,Siwen Wang,Imran Razzak,Guanyu Yang,Yutong Xie*

Main category: cs.CV

TL;DR: 提出CMSA-Net，通过因果多尺度聚合与动态多源参考，提升视频息肉分割的准确性与稳定性，并保持实时性，在SUN-SEG上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 视频息肉分割中，息肉与周围黏膜外观相似导致语义区分弱；跨帧位置与尺度剧烈变化导致时序传播不稳、分割不稳定。需一种既能稳健利用多帧信息、又能实时推理的方法。

Method: 1) CMSA-Net框架。2) 因果多尺度聚合（CMA）：从多帧历史特征、跨多尺度聚合，并用因果注意力保证时间单向传播，降低噪声、提升特征可靠性。3) 动态多源参考（DMR）：依据语义可分性与预测置信度自适应选择信息量高、可靠的参考帧，提供强多帧引导，同时控制计算以满足实时性。

Result: 在SUN-SEG数据集上进行大量实验，CMSA-Net取得当前最先进的性能，并在分割精度与推理效率之间实现优良权衡，满足临床实时应用需求。

Conclusion: 通过因果时序约束的多尺度特征聚合与自适应参考帧选择，CMSA-Net有效缓解语义弱区分与跨帧尺度/位移变化问题，实现稳健、准确且高效的视频息肉分割。

Abstract: Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.

</details>


### [54] [Reflectance Multispectral Imaging for Soil Composition Estimation and USDA Texture Classification](https://arxiv.org/abs/2602.22829)
*G. A. S. L Ranasinghe,J. A. S. T. Jayakody,M. C. L. De Silva,G. Thilakarathne,G. M. R. I. Godaliyadda,H. M. V. R. Herath,M. P. B. Ekanayake,S. K. Navaratnarajah*

Main category: cs.CV

TL;DR: 提出一种低成本多光谱成像(MSI)+机器学习框架，用于现场快速预测土壤三相组分(砂/粉/黏)并判定USDA质地类别，在配比样本上实现R^2≈0.99与>99%分类准确率，适用于地质与农业的无损筛查。


<details>
  <summary>Details</summary>
Motivation: 实验室粒径分析耗时、费力且难以现场部署；现有传感替代方案价格高或分辨率不足，限制了田间/工程尺度的常规应用。需要一种低成本、鲁棒、可现场部署的手段实现土壤质地快速、准确表征。

Method: 自研365–940 nm、13波段的低成本MSI设备采集土壤光谱；以回归模型预测黏、粉、砂百分比；以直接分类器预测12类USDA质地；并通过将回归得到的组分映射至USDA三角图获得间接分类。以按USDA三角图比例配制的混合样本进行评估。

Result: 在混合样本实验中，组分回归的决定系数R^2最高达0.99；质地分类准确率超过99%。

Conclusion: MSI与数据驱动建模能够提供准确、无损、可现场部署的土壤质地表征，适用于岩土工程初筛与精准农业。

Abstract: Soil texture is a foundational attribute that governs water availability and erosion in agriculture, as well as load bearing capacity, deformation response, and shrink-swell risk in geotechnical engineering. Yet texture is still typically determined by slow and labour intensive laboratory particle size tests, while many sensing alternatives are either costly or too coarse to support routine field scale deployment. This paper proposes a robust and field deployable multispectral imaging (MSI) system and machine learning framework for predicting soil composition and the United States Department of Agriculture (USDA) texture classes. The proposed system uses a cost effective in-house MSI device operating from 365 nm to 940 nm to capture thirteen spectral bands, which effectively capture the spectral properties of soil texture. Regression models use the captured spectral properties to estimate clay, silt, and sand percentages, while a direct classifier predicts one of the twelve USDA textural classes. Indirect classification is obtained by mapping the regressed compositions to texture classes via the USDA soil texture triangle. The framework is evaluated on mixture data by mixing clay, silt, and sand in varying proportions, using the USDA classification triangle as a basis. Experimental results show that the proposed approach achieves a coefficient of determination R^2 up to 0.99 for composition prediction and over 99% accuracy for texture classification. These findings indicate that MSI combined with data-driven modeling can provide accurate, non-destructive, and field deployable soil texture characterization suitable for geotechnical screening and precision agriculture.

</details>


### [55] [A data- and compute-efficient chest X-ray foundation model beyond aggressive scaling](https://arxiv.org/abs/2602.22843)
*Chong Wang,Yabin Zhang,Yunhe Gao,Maya Varma,Clemence Mottez,Faidra Patsatzi,Jiaming Liu,Jin Long,Jean-Benoit Delbrouck,Sergios Gatidis,Akshay S. Chaudhari,Curtis P. Langlotz*

Main category: cs.CV

TL;DR: 提出CheXficient：通过主动数据筛选而非一味扩规模，在仅用约22.7%胸片-报告配对和27.3%算力下，获得与全量或更大模型相当/更优表现，尤其提升长尾与稀有病种的泛化。


<details>
  <summary>Details</summary>
Motivation: 传统医学影像基础模型依赖超大规模预训练，但存在样本冗余、类别极度不平衡、数据质量异质且训练代价高，导致表示学习偏向过度代表的模式和算力低效。需要一种在保证性能的同时减少数据与计算开销的策略。

Method: 提出主动、原则化的数据遴选框架CheXficient：从123.5万对CXR图像-报告中选择信息量高、质量佳且代表性强的样本进行预训练；在视觉-语言设置下进行预训练，并系统评估零样本分类、跨模态检索，以及经微调/适配的疾病预测、语义分割和报告生成等任务；分析其对长尾/稀有类别的样本优先级。

Result: 仅用22.7%的数据与不到27.3%的算力，CheXficient在20个基准、5类任务上取得与全量训练及其他大规模模型相当或更优的结果；显示对长尾和稀有病种的更好泛化能力。

Conclusion: 与其盲目扩展数据规模，不如在预训练阶段进行主动数据策展；CheXficient验证了高效预训练与下游适配的可行路径，为医学视觉-语言基础模型在数据与计算约束下的开发提供了实用指南。

Abstract: Foundation models for medical imaging are typically pretrained on increasingly large datasets, following a "scale-at-all-costs" paradigm. However, this strategy faces two critical challenges: large-scale medical datasets often contain substantial redundancy and severe class imbalance that bias representation learning toward over-represented patterns, and indiscriminate training regardless of heterogeneity in data quality incurs considerable computational inefficiency. Here we demonstrate that active, principled data curation during pretraining can serve as a viable, cost-effective alternative to brute-force dataset enlargement. We introduce CheXficient, a chest X-ray (CXR) foundation model that selectively prioritizes informative training samples. CheXficient is pretrained on only 22.7% of 1,235,004 paired CXR images and reports while consuming under 27.3% of the total compute budget, yet achieving comparable or superior performance to its full-data counterpart and other large-scale pretrained models. We assess CheXficient across 20 individual benchmarks spanning 5 task types, including non-adapted off-the-shelf evaluations (zero-shot findings classification and crossmodal retrieval) and adapted downstream tasks (disease prediction, semantic segmentation, and radiology report generation). Further analyses show that CheXficient systematically prioritizes under-represented training samples, improving generalizability on long-tailed or rare conditions. Overall, our work offers practical insights into the data and computation demands for efficient pretraining and downstream adaptation of medical vision-language foundation models.

</details>


### [56] [From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models](https://arxiv.org/abs/2602.22859)
*Hongrui Jia,Chaoya Jiang,Shikun Zhang,Wei Ye*

Main category: cs.CV

TL;DR: 提出一种用于大型多模态模型持续训练的“诊断驱动渐进进化”(DPE)框架：用多代理生成与标注高质量多模态数据，基于失误诊断动态调配数据与强化学习信号，循环迭代以弥补弱点；在多个基准上实现稳定提升。


<details>
  <summary>Details</summary>
Motivation: 现有LMM尽管借助规模化与RL取得进展，但训练依赖静态数据与固定流程，难以发现与修补能力盲点；教育与测试启示“先暴露错误、再有针对性反馈”优于机械重复，需要一个能按诊断结果动态生成数据与强化的闭环。

Method: 构建螺旋式闭环：1) 多代理协作从海量未标注多模态数据中进行标注与质控，并借助网页搜索、图像编辑等工具生成多样、贴近真实的样本；2) 对模型失败进行归因，定位具体薄弱点，动态调整数据配比并引导代理生成面向弱点的数据；3) 用这些数据进行有针对性的强化学习；4) 迭代后重新诊断，驱动下一轮改进。

Result: 在Qwen3-VL-8B-Instruct与Qwen2.5-VL-7B-Instruct上，对11个基准实现稳定、持续增益，显示方法在开放任务分布下可扩展、可持续。

Conclusion: DPE为LMM的持续训练提供了可扩展范式：以失误诊断为核心，驱动数据生成与强化的动态耦合，实现面向弱点的渐进式提升；代码、模型与数据已开源。

Abstract: As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.

</details>


### [57] [SO3UFormer: Learning Intrinsic Spherical Features for Rotation-Robust Panoramic Segmentation](https://arxiv.org/abs/2602.22867)
*Qinfeng Zhu,Yunxi Jiang,Lei Fan*

Main category: cs.CV

TL;DR: 提出SO3UFormer，使全景语义分割在任意三维旋转下保持稳健；通过去除绝对纬度、四分法一致注意力与规范感知相对位置等几何设计，并配合重采样与一致性正则，显著缓解重力对齐假设带来的崩溃，SOTA在SO(3)旋转下大幅掉点而SO3UFormer基本不掉点。


<details>
  <summary>Details</summary>
Motivation: 现实数据常存在相机任意姿态（手持抖动、航拍姿态变化），而现有球面Transformer依赖重力对齐与纬度先验，导致对全球坐标过拟合，遇到旋转时性能崩溃。需要一种与坐标系无关、对SO(3)旋转鲁棒的球面特征学习方法与评测基准。

Method: 1) 去重力的内在特征：移除绝对纬度编码，解耦表达与重力向量；2) 四分法一致的球面注意力：按球面采样非均匀性进行加权；3) 规范（gauge）感知相对位置：在切平面用投影角度与离散规范池化编码局部角度，不依赖全局轴；4) 训练时采用基于索引的球面重采样与logit级SO(3)一致性正则。并引入Pose35数据集（在Stanford2D3D上随机±35°旋转）做鲁棒性评测。

Result: 在任意SO(3)旋转极限测试下，基线SphereUFormer的mIoU从67.53跌至25.26；SO3UFormer在Pose35上达72.03 mIoU，并在全SO(3)旋转下仍有70.67 mIoU，表现稳定且显著优于现有方法。

Conclusion: 通过内在几何表征、采样一致注意力与规范感知相对位置等设计，SO3UFormer学习到与全局坐标无关的球面特征，实现对任意三维旋转的强鲁棒性，并在新基准Pose35与全旋转测试中显著超越SOTA。

Abstract: Panoramic semantic segmentation models are typically trained under a strict gravity-aligned assumption. However, real-world captures often deviate from this canonical orientation due to unconstrained camera motions, such as the rotational jitter of handheld devices or the dynamic attitude shifts of aerial platforms. This discrepancy causes standard spherical Transformers to overfit global latitude cues, leading to performance collapse under 3D reorientations. To address this, we introduce SO3UFormer, a rotation-robust architecture designed to learn intrinsic spherical features that are less sensitive to the underlying coordinate frame. Our approach rests on three geometric pillars: (1) an intrinsic feature formulation that decouples the representation from the gravity vector by removing absolute latitude encoding; (2) quadrature-consistent spherical attention that accounts for non-uniform sampling densities; and (3) a gauge-aware relative positional mechanism that encodes local angular geometry using tangent-plane projected angles and discrete gauge pooling, avoiding reliance on global axes. We further use index-based spherical resampling together with a logit-level SO(3)-consistency regularizer during training. To rigorously benchmark robustness, we introduce Pose35, a dataset variant of Stanford2D3D perturbed by random rotations within $\pm 35^\circ$. Under the extreme test of arbitrary full SO(3) rotations, existing SOTAs fail catastrophically: the baseline SphereUFormer drops from 67.53 mIoU to 25.26 mIoU. In contrast, SO3UFormer demonstrates remarkable stability, achieving 72.03 mIoU on Pose35 and retaining 70.67 mIoU under full SO(3) rotations.

</details>


### [58] [Towards Multimodal Domain Generalization with Few Labels](https://arxiv.org/abs/2602.22917)
*Hongzhao Li,Hao Dong,Hualei Wan,Shupan Li,Mingliang Xu,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出SSMDG任务与统一方法，结合共识驱动一致性、分歧感知正则与跨模态原型对齐，在多源少标注下提升跨域与缺失模态鲁棒性，并建立首个基准，优于强基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法各有短板：多模态域泛化不会用无标注数据；半监督多模态忽视域移；半监督域泛化仅限单模态。现实中标注昂贵且需跨域与缺失模态鲁棒，因此需要一种能在多源少标注下同时利用无标注与多模态信息的框架。

Method: 统一框架包含三部分：1) 共识驱动一致性：对各模态单独预测并融合，筛选高置信“共识”样本生成伪标签，做一致性训练；2) 分歧感知正则：对非共识（歧义）样本采用弱到强增强一致性、温度/熵或对比等正则，避免伪标签污染同时利用其信息；3) 跨模态原型对齐：学习类别原型并在不同域与模态间对齐，通过跨模态翻译/对比，使表征在域与模态上不变，并支持缺失模态鲁棒。

Result: 作者构建首个SSMDG基准，并在标准与缺失模态场景上，相比强基线均取得显著且稳定的性能提升。

Conclusion: SSMDG被正式定义并给出首个基准。所提三组件的统一框架有效利用少量标注与大量无标注多源多模态数据，实现跨域与缺失模态鲁棒的泛化能力，优于现有方法。

Abstract: Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.

</details>


### [59] [Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins](https://arxiv.org/abs/2602.22919)
*Haofan Wu,Nay Aung,Theodoros N. Arvanitis,Joao A. C. Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

TL;DR: 提出“Chain of Flow (COF)”：一个以ECG驱动的生成式框架，可从单个心动周期重建个体化心脏的4D解剖与运动，并支持下游CDT任务。


<details>
  <summary>Details</summary>
Motivation: 现有心脏数字孪生多为任务特定预测器，难以构建可操作、可更新、可泛化的患者级虚拟心脏；临床可用CDT需能从多模态信号更新内部状态并支持多类模拟。

Method: 在训练阶段联合使用cine-CMR与12导联ECG，学习统一表示以耦合心脏几何、传导电生理与动力学运动；推理时仅用单个心动周期的ECG即可生成全心4D结构与运动；框架为生成式，可进行体积、分区功能与虚拟cine等任务。

Result: 在多队列上验证，能准确恢复心脏解剖、分腔功能与动态运动模式；重建的4D心脏进一步支持体积测量、区域功能分析和虚拟cine合成等下游任务。

Conclusion: COF将CDT从窄任务预测转变为可生成、以患者为中心的虚拟心脏，实现从ECG直接获得完整4D器官重建，具备更广泛临床可用性。

Abstract: A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.

</details>


### [60] [OSDaR-AR: Enhancing Railway Perception Datasets via Multi-modal Augmented Reality](https://arxiv.org/abs/2602.22920)
*Federico Nesti,Gianluca D'Amico,Mauro Marinoni,Giorgio Buttazzo*

Main category: cs.CV

TL;DR: 提出一个多模态增强现实框架，将虚拟障碍物以时空一致的方式植入真实铁路视频，利用UE5、LiDAR与INS/GNSS确保精确定位与时间稳定，并通过分割引导的INS/GNSS校正提升逼真度，生成公开数据集OSDaR-AR以缩小模拟到现实差距。


<details>
  <summary>Details</summary>
Motivation: 铁路场景安全感知（如障碍物检测）缺乏高质量标注数据；纯模拟存在“仿真到现实”鸿沟，简单图像遮罩增强又缺乏时空一致性与正确几何外观，需要一种既真实又稳定的增强方法与数据资源。

Method: 构建基于Unreal Engine 5的多模态AR流水线：将虚拟目标以照片级真实感插入OSDaR23实拍序列；融合LiDAR点云与INS/GNSS进行精确位姿估计与跨帧稳定；提出基于分割的INS/GNSS数据细化策略以修正漂移与误差；产出单帧/多帧一致的增强序列。

Result: 通过对比实验表明，分割引导的INS/GNSS细化显著提升增强序列的真实感与时序稳定性；构建并发布OSDaR-AR数据集，包含精心设计的增强铁路场景，用于训练与评测下一代感知模型。

Conclusion: 多模态AR可有效缩小铁路场景感知中的sim-to-real差距；融合LiDAR与INS/GNSS并辅以分割细化能实现高逼真、时空一致的增强数据；公开OSDaR-AR为安全关键任务（如障碍物检测）提供可复用资源。

Abstract: Although deep learning has significantly advanced the perception capabilities of intelligent transportation systems, railway applications continue to suffer from a scarcity of high-quality, annotated data for safety-critical tasks like obstacle detection. While photorealistic simulators offer a solution, they often struggle with the ``sim-to-real" gap; conversely, simple image-masking techniques lack the spatio-temporal coherence required to obtain augmented single- and multi-frame scenes with the correct appearance and dimensions. This paper introduces a multi-modal augmented reality framework designed to bridge this gap by integrating photorealistic virtual objects into real-world railway sequences from the OSDaR23 dataset. Utilizing Unreal Engine 5 features, our pipeline leverages LiDAR point-clouds and INS/GNSS data to ensure accurate object placement and temporal stability across RGB frames. This paper also proposes a segmentation-based refinement strategy for INS/GNSS data to significantly improve the realism of the augmented sequences, as confirmed by the comparative study presented in the paper. Carefully designed augmented sequences are collected to produce OSDaR-AR, a public dataset designed to support the development of next-generation railway perception systems. The dataset is available at the following page: https://syndra.retis.santannapisa.it/osdarar.html

</details>


### [61] [WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents](https://arxiv.org/abs/2602.22923)
*Runwei Guan,Shaofeng Liang,Ningwei Ouyang,Weichen Fei,Shanliang Yao,Wei Dai,Chenhao Ge,Penglei Sun,Xiaohui Zhu,Tao Huang,Ryan Wen Liu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出WaterVideoQA海事视频问答基准与NaviMind多智能体神经-符号系统，面向全水域环境的视频理解与合规推理，显著超越现有基线，推动ASV从感知到认知的可解释决策。


<details>
  <summary>Details</summary>
Motivation: 当前自主航行在被动感知上进展显著，但缺乏基于知识的交互式环境认知，难以在高风险海事场景中进行安全、合规和精确的机动与决策。需构建连接视觉感知与复杂推理的评测与方法。

Method: 1) 构建WaterVideoQA：包含3029段跨6类水域的视频，覆盖多变光照与动态天气，基于五层层级认知框架设计问题以全面考核。2) 提出NaviMind：多智能体神经-符号系统，包含自适应语义路由、情景感知的层级推理、以及自我反思式验证，实现开放式海事推理与法规合规的可解释决策。

Result: 在WaterVideoQA上，NaviMind在多项指标上显著优于现有基线，展现出更强的开放式视频问答能力与可靠、可解释的航行决策。

Conclusion: 将ASV从表层模式匹配推进到基于规则合规与可解释推理的智能交互新范式；WaterVideoQA与NaviMind为动态海事环境中的可信自主导航提供关键基准与方法。

Abstract: While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.

</details>


### [62] [ToProVAR: Efficient Visual Autoregressive Modeling via Tri-Dimensional Entropy-Aware Semantic Analysis and Sparsity Optimization](https://arxiv.org/abs/2602.22948)
*Jiayu Chen,Ruoyu Lin,Zihao Zheng,Jingxin Li,Maoliang Li,Guojie Luo,Xiang chen*

Main category: cs.CV

TL;DR: 提出ToProVAR：基于注意力熵分析的三维稀疏优化框架，在不依赖启发式跳步的前提下，加速视觉自回归(VAR)生成，最高达3.4倍，质量损失极小。


<details>
  <summary>Details</summary>
Motivation: VAR在后期生成阶段效率瓶颈明显，现有FastVAR/SkipVAR多依赖启发式跳步，难以精确把握不同粒度下的语义与参数动态，容易牺牲细节或一致性，亟需更系统可解释的加速方法。

Method: 以注意力熵衡量语义投影与不确定性，刻画随token粒度、语义范围与生成尺度变化的参数动态；据此揭示token、层、scale三维的稀疏模式，并设计对应的细粒度优化策略（在重要性低的token/层/尺度处稀疏化或跳过，重要部分保留/重计算），实现自适应而非启发式的加速。

Result: 在Infinity-2B与Infinity-8B上，ToProVAR显著加速生成过程，最高达3.4×，同时保持语义保真与细节质量，整体在效率与质量上均优于传统方法。

Conclusion: 基于注意力熵的三维稀疏优化为VAR提供了可解释且高效的加速路径，可在多模型与多尺度场景下稳定提升推理速度并减少质量损失，缓解以往方法的失真与不稳问题。

Abstract: Visual Autoregressive(VAR) models enhance generation quality but face a critical efficiency bottleneck in later stages. In this paper, we present a novel optimization framework for VAR models that fundamentally differs from prior approaches such as FastVAR and SkipVAR. Instead of relying on heuristic skipping strategies, our method leverages attention entropy to characterize the semantic projections across different dimensions of the model architecture. This enables precise identification of parameter dynamics under varying token granularity levels, semantic scopes, and generation scales. Building on this analysis, we further uncover sparsity patterns along three critical dimensions-token, layer, and scale-and propose a set of fine-grained optimization strategies tailored to these patterns. Extensive evaluation demonstrates that our approach achieves aggressive acceleration of the generation process while significantly preserving semantic fidelity and fine details, outperforming traditional methods in both efficiency and quality. Experiments on Infinity-2B and Infinity-8B models demonstrate that ToProVAR achieves up to 3.4x acceleration with minimal quality loss, effectively mitigating the issues found in prior work. Our code will be made publicly available.

</details>


### [63] [OpenFS: Multi-Hand-Capable Fingerspelling Recognition with Implicit Signing-Hand Detection and Frame-Wise Letter-Conditioned Synthesis](https://arxiv.org/abs/2602.22949)
*Junuk Cha,Jihyeon Kim,Han-Mu Park*

Main category: cs.CV

TL;DR: 提出OpenFS：无需显式手别检测与CTC的手指拼写识别与合成框架，含多手识别器（SF损失+双层位置编码）、单调对齐（MA）损失以及逐帧字母条件生成器，用于OOV合成与FSNeo基准；在多项实验中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 手指拼写识别受三难点制约：签名手（主动作手）歧义导致检测失败、CTC损失的peaky行为影响对齐与稳健性、以及OOV词缺数据。现有方法依赖显式手别检测与CTC，易出错且对齐不稳，且缺乏生成OOV训练数据的机制。

Method: 1) 多手能力识别器：支持单手/多手输入；通过双层位置编码与签名手聚焦（SF）损失，在交叉注意力中隐式聚焦签名手以实现隐式手别检测。2) 去CTC化训练：提出单调对齐（MA）损失，用交叉注意力正则使输出字母顺序与输入姿态时间顺序一致。3) 合成器：逐帧、字母条件生成器生成逼真的手指拼写姿态序列以覆盖OOV；据此构建合成基准FSNeo。4) 开源实现OpenFS（识别+合成）。

Result: 在多项综合实验中，识别性能达SOTA；消融验证表明SF损失、MA损失及生成器均显著提升鲁棒性与准确率；生成的FSNeo为评测与训练提供有效数据。

Conclusion: 通过隐式签名手检测与单调对齐训练，OpenFS缓解手别歧义与CTC峰值问题，并借助字母条件生成器有效处理OOV，整体在识别上达到SOTA并促进数据与基准建设。

Abstract: Fingerspelling is a component of sign languages in which words are spelled out letter by letter using specific hand poses. Automatic fingerspelling recognition plays a crucial role in bridging the communication gap between Deaf and hearing communities, yet it remains challenging due to the signing-hand ambiguity issue, the lack of appropriate training losses, and the out-of-vocabulary (OOV) problem. Prior fingerspelling recognition methods rely on explicit signing-hand detection, which often leads to recognition failures, and on a connectionist temporal classification (CTC) loss, which exhibits the peaky behavior problem. To address these issues, we develop OpenFS, an open-source approach for fingerspelling recognition and synthesis. We propose a multi-hand-capable fingerspelling recognizer that supports both single- and multi-hand inputs and performs implicit signing-hand detection by incorporating a dual-level positional encoding and a signing-hand focus (SF) loss. The SF loss encourages cross-attention to focus on the signing hand, enabling implicit signing-hand detection during recognition. Furthermore, without relying on the CTC loss, we introduce a monotonic alignment (MA) loss that enforces the output letter sequence to follow the temporal order of the input pose sequence through cross-attention regularization. In addition, we propose a frame-wise letter-conditioned generator that synthesizes realistic fingerspelling pose sequences for OOV words. This generator enables the construction of a new synthetic benchmark, called FSNeo. Through comprehensive experiments, we demonstrate that our approach achieves state-of-the-art performance in recognition and validate the effectiveness of the proposed recognizer and generator. Codes and data are available in: https://github.com/JunukCha/OpenFS.

</details>


### [64] [MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis](https://arxiv.org/abs/2602.22955)
*Feng Guo,Jiaxiang Liu,Yang Li,Qianqian Shi,Mingkun Xu*

Main category: cs.CV

TL;DR: 提出MM-NeuroOnco多模态脑肿瘤MRI指令调优与评测数据集（约2万4千张切片、20来源、约20万多模态指令），并基于此构建评测基准与模型NeuroOnco-GPT，显著提升诊断问答表现。


<details>
  <summary>Details</summary>
Motivation: 现有公开脑肿瘤数据集多为分割/检测导向，缺乏丰富的诊断语义与可解释临床推理标注，难以评估/训练模型在“临床可解释诊断理解”上的能力。

Method: 1) 汇集多来源MRI并构建大规模多模态指令数据；2) 设计多模型协同流水线进行自动医学信息补全与质控，从掩膜拓展到诊断相关语义；3) 构建具拒答意识(rejection-aware)的人工标注评测基准MM-NeuroOnco-Bench；4) 基于数据集微调NeuroOnco-GPT并与10个代表性模型比较。

Result: 在MM-NeuroOnco-Bench上，最强基线Gemini 3 Flash在诊断类问题仅41.88%准确率；经MM-NeuroOnco微调的NeuroOnco-GPT在诊断问题上取得绝对+27%的准确率提升。

Conclusion: MM-NeuroOnco提供了丰富诊断语义与指令的多模态资源，配套的拒答评测与微调策略显著提升了模型的临床诊断推理能力，推动脑肿瘤多模态理解研究。

Abstract: Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnco, a large-scale multimodal benchmark and instruction-tuning dataset for brain tumor MRI understanding, consisting of 24,726 MRI slices from 20 data sources paired with approximately 200,000 semantically enriched multimodal instructions spanning diverse tumor subtypes and imaging modalities. To mitigate the scarcity and high cost of diagnostic semantic annotations, we develop a multi-model collaborative pipeline for automated medical information completion and quality control, enabling the generation of diagnosis-related semantics beyond mask-only annotations. Building upon this dataset, we further construct MM-NeuroOnco-Bench, a manually annotated evaluation benchmark with a rejection-aware setting to reduce biases inherent in closed-ended question formats. Evaluation across ten representative models shows that even the strongest baseline, Gemini 3 Flash, achieves only 41.88% accuracy on diagnosis-related questions, highlighting the substantial challenges of multimodal brain tumor diagnostic understanding. Leveraging MM-NeuroOnco, we further propose NeuroOnco-GPT, which achieves a 27% absolute accuracy improvement on diagnostic questions following fine-tuning. This result demonstrates the effectiveness of our dataset and benchmark in advancing clinically grounded multimodal diagnostic reasoning. Code and dataset are publicly available at: https://github.com/gfnnnb/MM-NeuroOnco

</details>


### [65] [Can Agents Distinguish Visually Hard-to-Separate Diseases in a Zero-Shot Setting? A Pilot Study](https://arxiv.org/abs/2602.22959)
*Zihao Zhao,Frederik Hauke,Juliana De Castilhos,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 研究提出对视觉上难区分疾病进行零样本诊断的多智能体对比裁决框架，并在两项影像任务上较基线显著提升准确率（如皮肤镜提高11个百分点），但离临床可用仍有差距。


<details>
  <summary>Details</summary>
Motivation: 现有医疗影像中的MLLM/智能体多聚焦于自动化常规流程，较少关注在无标注或无特定训练下对“视觉高度混淆、但临床处理差异大”的疾病进行区分；该问题具有高临床价值且更能检验零样本能力与稳健性。

Method: 构建基于“对比式裁决（contrastive adjudication）”的多智能体框架：多个代表性MLLM智能体在仅有影像输入的前提下，对两类易混任务（黑色素瘤vs非典型痣；肺水肿vs肺炎）给出判断与理由，再通过对比与裁决机制整合，形成最终诊断与不确定性评估；并与若干基线代理进行基准测试，含定量准确率与定性不当陈述分析。

Result: 在受控设置下，多智能体对比裁决带来诊断性能提升：例如在皮肤镜数据集上准确率提升约11个百分点；在定性样本上可减少无依据断言（unsupported claims）。但整体准确性与稳健性仍不足以满足临床部署要求。

Conclusion: 多智能体对比裁决在视觉强混淆、零样本诊断场景中展现潜力并能减少不当陈述，但受限于人工标注不确定性与缺乏临床上下文，当前仅为初步探索，尚需结合临床信息、扩大数据与更严格验证方可转化。

Abstract: The rapid progress of multimodal large language models (MLLMs) has led to increasing interest in agent-based systems. While most prior work in medical imaging concentrates on automating routine clinical workflows, we study an underexplored yet clinically significant setting: distinguishing visually hard-to-separate diseases in a zero-shot setting. We benchmark representative agents on two imaging-only proxy diagnostic tasks, (1) melanoma vs. atypical nevus and (2) pulmonary edema vs. pneumonia, where visual features are highly confounded despite substantial differences in clinical management. We introduce a multi-agent framework based on contrastive adjudication. Experimental results show improved diagnostic performance (an 11-percentage-point gain in accuracy on dermoscopy data) and reduced unsupported claims on qualitative samples, although overall performance remains insufficient for clinical deployment. We acknowledge the inherent uncertainty in human annotations and the absence of clinical context, which further limit the translation to real-world settings. Within this controlled setting, this pilot study provides preliminary insights into zero-shot agent performance in visually confounded scenarios.

</details>


### [66] [UCM: Unifying Camera Control and Memory with Time-aware Positional Encoding Warping for World Models](https://arxiv.org/abs/2602.22960)
*Tianxing Xu,Zixuan Wang,Guangyuan Wang,Li Hu,Zhongyi Zhang,Peng Zhang,Bang Zhang,Song-Hai Zhang*

Main category: cs.CV

TL;DR: UCM提出一种统一长程记忆与精确相机控制的视频世界模型，通过时间感知的位置编码扭曲与高效双流扩散Transformer，在大规模单目视频上训练，实现高保真、可控且长期一致的生成，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型在两方面存在痛点：1) 长时间尺度下重复访问场景时难以保持内容一致；2) 难以根据用户输入实现精确相机控制。基于显式3D重建的方法在无界场景与细粒度结构上受限；直接依赖先前帧的方法缺少显式空间对应，导致一致性和可控性不足。

Method: 提出UCM框架：核心是时间感知的position encoding warping，将长期记忆与相机控制统一起来；设计高效的双流扩散Transformer以降低计算开销并保证高保真；提出可扩展的数据策划策略，用基于点云的渲染来模拟场景重访，构建超过50万条单目视频训练集。

Result: 在真实与合成基准上，UCM在长期场景一致性方面显著优于SOTA，同时在高保真视频生成中实现精确相机可控。

Conclusion: UCM有效解决了视频世界模型的长期一致性与相机控制难题，兼顾可扩展训练与生成质量，显示出在交互环境模拟中的强竞争力。

Abstract: World models based on video generation demonstrate remarkable potential for simulating interactive environments but face persistent difficulties in two key areas: maintaining long-term content consistency when scenes are revisited and enabling precise camera control from user-provided inputs. Existing methods based on explicit 3D reconstruction often compromise flexibility in unbounded scenarios and fine-grained structures. Alternative methods rely directly on previously generated frames without establishing explicit spatial correspondence, thereby constraining controllability and consistency. To address these limitations, we present UCM, a novel framework that unifies long-term memory and precise camera control via a time-aware positional encoding warping mechanism. To reduce computational overhead, we design an efficient dual-stream diffusion transformer for high-fidelity generation. Moreover, we introduce a scalable data curation strategy utilizing point-cloud-based rendering to simulate scene revisiting, facilitating training on over 500K monocular videos. Extensive experiments on real-world and synthetic benchmarks demonstrate that UCM significantly outperforms state-of-the-art methods in long-term scene consistency, while also achieving precise camera controllability in high-fidelity video generation.

</details>


### [67] [Small Object Detection Model with Spatial Laplacian Pyramid Attention and Multi-Scale Features Enhancement in Aerial Images](https://arxiv.org/abs/2602.23031)
*Zhangjian Ji,Huijia Yan,Shaotong Qiao,Kai Feng,Wei Wei*

Main category: cs.CV

TL;DR: 提出一种面向航拍图像小目标的检测方法：在ResNet-50各stage后加入空间拉普拉斯金字塔注意力（SLPA），在FPN侧连的C5分支加入多尺度特征增强模块（MSFEM），并在FPN上下层融合时用可变形卷积进行特征对齐；在VisDrone与DOTA上优于基线。


<details>
  <summary>Details</summary>
Motivation: 航拍图像中小目标尺寸小、密集且分布不均，高分辨率带来计算与特征稀释问题，传统FPN融合存在未对齐导致特征退化，需提升小目标表征、语义表达与融合对齐以提高检测精度与效率。

Method: - 主干网络：以ResNet-50为骨干。
- SLPA：在ResNet-50每个stage之后插入空间拉普拉斯金字塔注意力，利用拉普拉斯金字塔多尺度空间细节与边缘响应，突出关键局部区域，增强小目标显著性。
- MSFEM：在构建FPN时，于来自C5的侧连分支上引入多尺度特征增强（可能通过并行不同感受野/空洞率卷积、上下采样路径）以强化高层语义并补充细粒度信息。
- 对齐融合：在FPN自顶向下与横向融合处，使用可变形卷积进行特征对齐，缓解尺度变换与采样导致的misalignment，提升融合质量与小目标检测能力。

Result: 在VisDrone与DOTA两个基准数据集上进行广泛实验，相比原始算法（推测为FPN+ResNet-50等基线）取得明显性能提升，尤其在小目标检测上更优。

Conclusion: 通过SLPA增强小目标空间细节、MSFEM丰富多尺度语义，并以可变形卷积实现FPN对齐融合，整体提升航拍小目标检测效果；实验验证在标准数据集上优于基线。

Abstract: Detecting objects in aerial images confronts some significant challenges, including small size, dense and non-uniform distribution of objects over high-resolution images, which makes detection inefficient. Thus, in this paper, we proposed a small object detection algorithm based on a Spatial Laplacian Pyramid Attention and Multi-Scale Feature Enhancement in aerial images. Firstly, in order to improve the feature representation of ResNet-50 on small objects, we presented a novel Spatial Laplacian Pyramid Attention (SLPA) module, which is integrated after each stage of ResNet-50 to identify and emphasize important local regions. Secondly, to enhance the model's semantic understanding and features representation, we designed a Multi-Scale Feature Enhancement Module (MSFEM), which is incorporated into the lateral connections of C5 layer for building Feature Pyramid Network (FPN). Finally, the features representation quality of traditional feature pyramid network will be affected because the features are not aligned when the upper and lower layers are fused. In order to handle it, we utilized deformable convolutions to align the features in the fusion processing of the upper and lower levels of the Feature Pyramid Network, which can help enhance the model's ability to detect and recognize small objects. The extensive experimental results on two benchmark datasets: VisDrone and DOTA demonstrate that our improved model performs better for small object detection in aerial images compared to the original algorithm.

</details>


### [68] [PackUV: Packed Gaussian UV Maps for 4D Volumetric Video](https://arxiv.org/abs/2602.23040)
*Aashish Rai,Angela Xing,Anushka Agarwal,Xiaoyan Cong,Zekun Li,Tao Lu,Aayush Prakash,Srinath Sridhar*

Main category: cs.CV

TL;DR: PackUV 提出一种将4D高斯属性打包到多尺度UV图集中的表示，并配套PackUV-GS在UV域直接优化以保持时间一致性；结合流引导标注与关键帧策略，解决长序列、大运动与遮挡问题；UV图集可无损兼容标准视频编解码，实现高效流式传输；并发布超大数据集PackUV-2B；实验显示在渲染质量与可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 体积视频（4D）重建、存储与传输在长时序、时间一致性、大位移与遮挡下仍困难；现有高斯Splatting方法在长序列退化，且结果不易纳入传统视频编码管线，限制落地应用。

Method: 1）PackUV：将所有高斯参数（位置、尺度、方向、密度、颜色等）映射为结构化、多尺度UV图集，实现图像原生的紧凑存储；2）PackUV-GS：在UV域直接优化高斯参数，设计流引导的高斯标注与视频关键帧模块，自动区分动态/静态高斯、稳定静态区域、在大运动与遮挡下保持时序一致；3）以标准视频编解码（如FFV1）对UV图集进行无质量损失压缩与流式传输。

Result: - 首个与标准视频编解码完全兼容的统一体积视频表示（UV图集格式），可无损编码与高效流式传输；- 在包含>50机位、频繁遮挡与大运动的超大数据集上，渲染保真度超越基线，并可稳定处理长达30分钟序列且质量一致。

Conclusion: PackUV通过将4D高斯映射到UV图集并在UV域进行时序一致优化，实现了高质量、可扩展、与现有多媒体基础设施兼容的体积视频重建与传输方案；伴随PackUV-2B数据集，推动长时序体积视频的研究与应用落地。

Abstract: Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications.
  We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure.
  To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.

</details>


### [69] [D-FINE-seg: Object Detection and Instance Segmentation Framework with multi-backend deployment](https://arxiv.org/abs/2602.23043)
*Argo Saakyan,Dmitry Solntsev*

Main category: cs.CV

TL;DR: D-FINE-seg在D-FINE实时检测器基础上加入轻量级掩码头与分割感知训练（包含裁剪框BCE与Dice损失、辅助与去噪掩码监督、改进的匈牙利匹配代价），在TACO数据集上以统一TensorRT FP16端到端协议优于Ultralytics YOLOv8-seg的F1，同时保持接近的延迟，并提供跨ONNX/TensorRT/OpenVINO的一体化训练与推理开源框架。


<details>
  <summary>Details</summary>
Motivation: Transformer实时检测器表现出色，但基于Transformer的实时实例分割仍稀缺；需要在不牺牲延迟的前提下，实现更强的实例分割性能与可部署的一体化端到端流程。

Method: 以D-FINE为骨干，新增轻量级mask head；引入分割感知训练策略：框裁剪的BCE与Dice掩码损失、辅助与去噪掩码监督；调整匈牙利匹配代价以考虑分割质量；并构建跨ONNX、TensorRT、OpenVINO的训练导出与优化推理流水线。

Result: 在TACO数据集上，D-FINE-seg在统一TensorRT FP16端到端基准下F1优于Ultralytics YOLOv8-seg，同时延迟具有竞争力。

Conclusion: D-FINE-seg实现了基于Transformer的实时实例分割的有效方案，在保持低延迟的同时提升精度，并提供可复用的端到端开源部署框架（Apache-2.0）。

Abstract: Transformer-based real-time object detectors achieve strong accuracy-latency trade-offs, and D-FINE is among the top-performing recent architectures. However, real-time instance segmentation with transformers is still less common. We present D-FINE-seg, an instance segmentation extension of D-FINE that adds: a lightweight mask head, segmentation-aware training, including box cropped BCE and dice mask losses, auxiliary and denoising mask supervision, and adapted Hungarian matching cost. On the TACO dataset, D-FINE-seg improves F1-score over Ultralytics YOLO26 under a unified TensorRT FP16 end-to-end benchmarking protocol, while maintaining competitive latency. Second contribution is an end-to-end pipeline for training, exporting, and optimized inference across ONNX, TensorRT, OpenVINO for both object detection and instance segmentation tasks. This framework is released as open-source under the Apache-2.0 license. GitHub repository - https://github.com/ArgoHA/D-FINE-seg.

</details>


### [70] [GeoWorld: Geometric World Models](https://arxiv.org/abs/2602.23058)
*Zeyu Zhang,Danning Li,Ian Reid,Richard Hartley*

Main category: cs.CV

TL;DR: GeoWorld将能量式世界模型的潜在空间从欧氏映射到双曲流形，并结合几何强化学习进行能量优化，实现更稳健的多步视觉规划，在CrossTask与COIN上实现3–4步规划的SR小幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有能量式预测世界模型：1) 潜在表征多在欧氏空间，难以捕获状态间固有的层次/树状几何结构；2) 长时域滚动预测不稳，误差快速累积，导致规划退化。

Method: 提出GeoWorld：以Hyperbolic JEPA将欧氏潜在表示嵌入到双曲流形以保留层级与几何关系；并引入几何强化学习在双曲潜在空间中进行能量基优化与多步规划，稳定长地平线推理。

Result: 在CrossTask与COIN数据集上，相比SOTA V-JEPA 2，3步规划SR约提升3%，4步规划SR约提升2%，表现更稳健。

Conclusion: 通过双曲表示与几何RL的结合，GeoWorld在长时域多步视觉规划中提升了稳定性与性能，说明保留潜在空间的几何与层级结构对能量式世界建模有效。

Abstract: Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.

</details>


### [71] [Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception](https://arxiv.org/abs/2602.23069)
*Yiding Sun,Jihua Zhu,Haozhe Cheng,Chaoyi Lu,Zhichuan Yang,Lin Chen,Yaonan Wang*

Main category: cs.CV

TL;DR: 提出PointATA“先对齐再适配”的参数高效迁移范式，把3D点云预训练模型迁移到4D点云视频任务。先用最优传输度量并收敛3D-4D分布差异（对齐），再插入轻量时间适配器与空间上下文编码器（适配）以缓解过拟合、增强时序建模，在多项4D任务上以更少参数达到或超越全量微调。


<details>
  <summary>Details</summary>
Motivation: 4D点云视频对机器人理解至关重要，但4D数据稀缺，难以大规模自监督训练。直接迁移3D预训练到4D常遇到两大障碍：过拟合与模态鸿沟（3D静态 vs 4D时序）。需要一种既能缩小分布差异又能稳健引入时间建模、且参数高效的迁移方案。

Method: 提出Align-then-Adapt两阶段：Stage1“Align”——用最优传输(OT)量化并缩小3D与4D数据分布差距，训练point align embedder以缓解模态差异；Stage2“Adapt”——冻结3D主干，插入高效point-video adapter与spatial-context encoder增强时序与空间上下文建模，控制参数规模、抑制过拟合。

Result: 在多项任务上取得强性能且参数更省：3D动作识别97.21%准确率；4D动作分割提升+8.7%；4D语义分割84.06% mIoU（或准确率，摘要未明示），整体可匹配或超越强全量微调模型。

Conclusion: 通过“先对齐再适配”的参数高效策略，3D无时序先验的预训练模型也能有效理解4D动态点云视频，缓解模态差异与过拟合，在多任务上实现高精度与高参数效率。

Abstract: Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel "Align then Adapt" (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \% accuracy on 3D action recognition, $+8.7 \%$ on 4 D action segmentation, and 84.06\% on 4D semantic segmentation.

</details>


### [72] [Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy](https://arxiv.org/abs/2602.23088)
*Matthew Sutton,Katrin Amunts,Timo Dickscheid,Christian Schiffer*

Main category: cs.CV

TL;DR: 提出一种无需成对图文数据、通过标签媒介将显微图像与语言连接的方法，在人脑皮层细胞体染色切片的细胞结构学分析中实现自然语言描述，并取得高准确度与可拓展性。


<details>
  <summary>Details</summary>
Motivation: 许多研究与临床领域缺乏可用于训练视觉-语言耦合的配对图文数据，但又需要交互式、可代理的工作流以自然语言解释显微图像，尤其是在大脑细胞结构学分析中。

Method: 利用“标签作为桥”的弱监督：给定脑区标签，自动从相关文献挖掘该脑区的规范性细胞结构学描述，作为合成 caption；将现有的细胞结构学视觉基础模型（CytoNet）与大语言模型通过图像到文本目标进行耦合训练，从而让显微区域可被自然语言描述；并设计开放集识别，通过显式拒识未见脑区。

Result: 覆盖57个脑区，方法生成合理的脑区级描述；对在域图像补丁匹配参考标签的准确率为90.6%；在遮蔽脑区标签的8类识别测试中，模型仅基于生成描述即可以68.6%准确率恢复脑区；具备开放集拒识能力。

Conclusion: 弱标签媒介的配对足以把现有生物医学视觉基础模型连接到语言，为细粒度标注稀缺的领域提供了实用的自然语言集成方案，兼具可解释性、准确性与开放集稳健性。

Abstract: Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.

</details>


### [73] [Locally Adaptive Decay Surfaces for High-Speed Face and Landmark Detection with Event Cameras](https://arxiv.org/abs/2602.23101)
*Paul Kielty,Timothy Hanley,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出局部自适应衰减表面（LADS）来将事件相机的异步事件转为适合神经网络的稠密张量，通过按像素位置的局部动态自适应时间衰减，兼顾静止区域结构保真与快速运动的清晰边缘，在人脸检测与人脸关键点任务上显著优于传统全局固定衰减/直方图表示，且在高频（240 Hz）下维持甚至超越以往30 Hz结果，同时允许使用更轻量网络实现实时。


<details>
  <summary>Details</summary>
Motivation: 现有事件表示（直方图、全局时间表面）使用固定时间参数，导致在不同运动状态下存在保结构与保锐度的两难：静止期易丢细节，快速运动时易模糊，限制了事件相机在人脸分析等任务的准确性与实时性。

Method: 提出LADS：在每个像素（或局部邻域）根据局部信号动态自适应调整时间衰减率，给出三种调制策略——基于事件率、LoG（拉普拉斯-高斯）响应、以及高频谱能量——以在低活动区减慢衰减保结构，在高活动区加快衰减抑制拖影/模糊，从而生成更信息有效的稠密张量输入神经网络。

Result: 在公开数据上，LADS在30 Hz下人脸检测mAP50与关键点误差均优于非自适应基线；在240 Hz下缓解常见的性能下降，达到2.44%归一化平均误差与0.966 mAP50；高频结果甚至超过此前工作在30 Hz的报告，树立新的事件人脸分析基准，并使更轻量网络仍可实时运行。

Conclusion: 上下文感知（空间局部自适应）的时间积分对神经形态视觉至关重要。LADS通过在表示阶段保护空间结构与边缘，提升事件相机在人脸检测/关键点定位的准确性与鲁棒性，并推动面向实时高频人机交互的轻量级系统。

Abstract: Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion. We introduce Locally Adaptive Decay Surfaces (LADS), a family of event representations in which the temporal decay at each location is modulated according to local signal dynamics. Three strategies are explored, based on event rate, Laplacian-of-Gaussian response, and high-frequency spectral energy. These adaptive schemes preserve detail in quiescent regions while reducing blur in regions of dense activity. Extensive experiments on the public data show that LADS consistently improves both face detection and facial landmark accuracy compared to standard non-adaptive representations. At 30 Hz, LADS achieves higher detection accuracy and lower landmark error than either baseline, and at 240 Hz it mitigates the accuracy decline typically observed at higher frequencies, sustaining 2.44 % normalized mean error for landmarks and 0.966 mAP50 in face detection. These high-frequency results even surpass the accuracy reported in prior works operating at 30 Hz, setting new benchmarks for event-based face analysis. Moreover, by preserving spatial structure at the representation stage, LADS supports the use of much lighter network architectures while still retaining real-time performance. These results highlight the importance of context-aware temporal integration for neuromorphic vision and point toward real-time, high-frequency human-computer interaction systems that exploit the unique advantages of event cameras.

</details>


### [74] [FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time](https://arxiv.org/abs/2602.23115)
*David Dirnfeld,Fabien Delattre,Pedro Miraldo,Erik Learned-Miller*

Main category: cs.CV

TL;DR: 论文提出一种基于单位球面S²的广义霍夫变换来稳健估计单目视频下的相机前进方向（heading），在噪声与外点较高时兼顾精度与效率，并在SLAM初始化中降低RMSE。


<details>
  <summary>Details</summary>
Motivation: 现有在已知旋转前提下恢复相机航向的方法在噪声/外点低时表现良好，但噪声或外点增多会显著降准或计算代价高。需要一种既稳健又高效的航向估计方法，适用于动态场景和测量误差。

Method: 1) 从两帧提取匹配；2) 每对对应关系在S²上生成与之相容的一条大圆（所有可能的航向集合）；3) 用Fibonacci格点离散化单位球作为投票桶中心；4) 每条大圆对一段方向范围投票，使未受噪声/动态物体影响的特征在正确方向上累积一致票；5) 取累积最高的方向作为航向估计；并在SLAM初始化中用该航向校正位姿。

Result: 在三个数据集上，方法位于精度-效率的帕累托前沿；在SLAM实验中，通过在位姿初始化阶段校正航向，可显著降低轨迹RMSE。

Conclusion: 基于S²广义霍夫投票的航向估计对噪声与外点更稳健，计算高效，可作为VO/SLAM/SfM在已知旋转条件下的实用组件，并能提升SLAM初始化精度。

Abstract: Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera's heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.

</details>


### [75] [No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors](https://arxiv.org/abs/2602.23141)
*Tao Liu,Gang Wan,Kan Ren,Shibo Wen*

Main category: cs.CV

TL;DR: 提出一种无监督在线视频稳像框架，三阶段经典管线+多线程缓冲，解决端到端学习的数据稀缺、可控性差与资源受限效率问题；并发布多模态UAV-Test数据集，在在线稳像上优于SOTA，接近离线方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习稳像依赖成对稳定/不稳定数据，数据难获取；端到端方法可控性差、资源占用高，且基准多聚焦手持可见光前视，难以覆盖无人机夜间遥感等场景。

Method: 以经典稳像三阶段（运动估计-路径平滑-帧重映射）为骨架，采用无监督学习/优化方案，不需配对数据；引入多线程缓冲队列以在线处理、提升吞吐与稳定性；并构建多模态UAV-Test无人机航拍数据集用于评测。

Result: 在现有在线稳像基准与新UAV-Test上，定量指标与主观视觉质量均优于当前在线SOTA；整体性能与离线稳像方法相当。

Conclusion: 该无监督、可控且高效的在线稳像框架在多种场景（含UAV夜间多模态）均表现优异，缓解数据依赖与硬件限制；新数据集为更广泛稳像评测提供基础。

Abstract: We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.

</details>


### [76] [Efficient Encoder-Free Fourier-based 3D Large Multimodal Model](https://arxiv.org/abs/2602.23153)
*Guofeng Mei,Wei Lin,Luigi Riz,Yujiao Wu,Yiming Wang,Fabio Poiesi*

Main category: cs.CV

TL;DR: 提出Fase3D：首个无需重型视觉编码器、以傅里叶为核心的3D场景多模态大模型，靠点云序列化+FFT近似自注意力，实现高效且具置换不变。


<details>
  <summary>Details</summary>
Motivation: 现有3D LMM依赖预训练几何编码器，计算与参数开销大；2D领域已有去编码器趋势，但3D点云无序且规模大，难以直接套用，需一种既高效又能保持全局上下文的3D数据tokenization方案。

Method: 设计无编码器架构Fase3D：1) 以结构化superpoints紧凑表示大场景；2) 通过空间填充曲线对点云进行序列化，随后用FFT近似自注意力，实现高效全局上下文建模，并进行基于图的token合并；3) 在LLM中加入Fourier-augmented LoRA，使全局频域交互以极低成本注入。

Result: 在计算与参数大幅降低的同时，性能与依赖编码器的3D LMM相当，展现出良好的可扩展性与效率。

Conclusion: 通过FFT与序列化策略替代繁重的几何编码器，Fase3D在保证精度的前提下显著提升3D LMM的效率，验证了无编码器、频域增强在3D多模态中的可行性与潜力。

Abstract: Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.

</details>


### [77] [DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation](https://arxiv.org/abs/2602.23165)
*Yichen Peng,Jyun-Ting Song,Siyeol Jung,Ruofan Liu,Haiyang Liu,Xuangeng Chu,Ruicong Liu,Erwin Wu,Hideki Koike,Kris Kitani*

Main category: cs.CV

TL;DR: 提出DyaDiT：一种利用双人对话音频与社交上下文生成更自然对话手势的多模态扩散Transformer，客观指标与用户偏好均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有手势生成多将单路音频映射到单人动作，忽视社交情境与双人互动动态，导致生成的对话手势缺乏互相呼应与社会性自然度。

Method: 基于多模态扩散Transformer：输入为双人（dyadic）音频及可选的社交上下文token；融合双方语音信息以建模互动动态；引入动作字典（motion dictionary）编码先验；可选接入对话伙伴的手势以提升响应性；在Seamless Interaction Dataset上训练；用标准动作生成指标与用户研究评估。

Result: 在标准客观指标上优于现有方法，用户研究中显著被偏好，显示生成动作更稳健、社会性更强。

Conclusion: 融合双人语音与社交先验的扩散Transformer能生成更符合情境与互动的手势，优于单人音频到动作的方法；代码与模型将开源以促进复现与应用。

Abstract: Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.

</details>


### [78] [AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios](https://arxiv.org/abs/2602.23166)
*Zhaochen Su,Jincheng Gao,Hangyu Guo,Zhenhua Liu,Lueyang Zhang,Xinyu Geng,Shijue Huang,Peng Xia,Guanyu Jiang,Cheng Wang,Yue Zhang,Yi R. Fung,Junxian He*

Main category: cs.CV

TL;DR: AgentVista是一个评测通用多模态智能体长链、多工具、跨模态推理能力的新基准，覆盖25个子领域与7大类场景，包含真实细致的视觉任务与自然的混合工具使用；当前SOTA模型在此上表现很低，最佳仅27.3%准确率，揭示长程多模态工具使用的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准聚焦单轮视觉推理或单一工具技能，无法反映现实任务中的视觉细节、跨资料整合与长程、多步骤工具调用需求；需要一个更贴近真实应用、能系统检验多模态智能体在复杂工作流中的能力的评测框架。

Method: 构建AgentVista基准：覆盖7大类别、25个子领域的真实且细节丰富的视觉情境；任务要求跨模态的长程工具交互，包括网页搜索、图片搜索、页面导航、以及用于图像处理与通用编程的代码执行等；对多种当前最先进模型进行全面评测。

Result: 在AgentVista上的系统性评估显示当前模型存在明显能力缺口：即使是带工具的Gemini-3-Pro，整体准确率也仅27.3%；困难实例需要超过25轮的工具调用，暴露长程多模态工具使用与推理的脆弱性。

Conclusion: AgentVista为评测与驱动通用多模态智能体在真实且极具挑战的任务上的进步提供了标准化平台，预期将加速更强、更可靠的多模态代理的研发。

Abstract: Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.

</details>


### [79] [Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration](https://arxiv.org/abs/2602.23169)
*Xiaole Tang,Xiaoyi He,Jiayi Xu,Xiang Gu,Jian Sun*

Main category: cs.CV

TL;DR: 提出BaryIR，通过在Wasserstein重心（WB）空间对齐多源退化特征，并以正交残差子空间保留退化特异信息，提升全能图像复原在未知退化上的泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有“一体化”图像复原模型对分布外退化（未知类型/强度/组合）泛化差，容易过拟合训练退化分布。作者认为不同退化是从同一退化无关的潜在分布出发产生的偏移，因此若能恢复这一共享分布，就可在多退化间实现稳健泛化。

Method: - 构建BaryIR表征学习框架：将多源退化特征分布在Wasserstein Barycenter空间对齐，使该重心分布刻画退化无关的共享内容。
- 通过最小化各退化分布到WB的平均Wasserstein距离来学习重心嵌入。
- 设计残差子空间：与WB嵌入正交，并在不同退化间进行对比学习，专门承载退化特异知识。
- 显式解耦两正交空间：WB空间（共享不变内容）+ 残差子空间（自适应退化特征），以支持自适应复原并降低对训练退化的过拟合。

Result: 在多项基准上与SOTA一体化方法竞争；对未见退化（类型与强度）有更好泛化；即使只用少量退化类型训练，也能在真实混合退化数据上保持鲁棒、学到更具泛化性的特征。

Conclusion: 对齐到Wasserstein重心以学习退化无关共享表征、并用正交残差子空间保留退化特异信息，可显著提升一体化图像复原在分布外退化与真实场景中的泛化与鲁棒性。

Abstract: Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.

</details>


### [80] [Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking](https://arxiv.org/abs/2602.23172)
*Maximilian Luz,Rohit Mohan,Thomas Nürnberg,Yakov Miron,Daniele Cattaneo,Abhinav Valada*

Main category: cs.CV

TL;DR: LaGS提出以潜在高斯泼绘将多视角时空信息高效汇聚到体素网格，实现端到端4D全景占据跟踪，并在Occ3D-nuScenes与Waymo上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么只做基于框的粗跟踪、要么给出细致的3D占据却缺少显式时序关联，难以满足动态场景中机器人对连续、精细且可跟踪环境建模的需求。

Method: 构建相机端到端框架：先将多视角观测融合为稀疏点心的3D潜在高斯（作为场景隐式表示），再将其特征泼绘到3D体素网格，由基于掩码的分割头解码得到全景占据；同时进行实例/类别级的时序跟踪。核心是高效的潜在高斯泼绘以聚合多视图信息。

Result: 在Occ3D的nuScenes与Waymo数据集上进行评测，4D全景占据跟踪指标达到最新最优（SOTA）。

Conclusion: 潜在高斯泼绘能高效融合多视图、提升时空全景占据与跟踪的一体化性能，为动态环境机器人感知提供更全面可靠的方案；代码已开源。

Abstract: Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a holistic direction. Our approach incorporates camera-based end-to-end tracking with mask-based multi-view panoptic occupancy prediction, and addresses the key challenge of efficiently aggregating multi-view information into 3D voxel grids via a novel latent Gaussian splatting approach. Specifically, we first fuse observations into 3D Gaussians that serve as a sparse point-centric latent representation of the 3D scene, and then splat the aggregated features onto a 3D voxel grid that is decoded by a mask-based segmentation head. We evaluate LaGS on the Occ3D nuScenes and Waymo datasets, achieving state-of-the-art performance for 4D panoptic occupancy tracking. We make our code available at https://lags.cs.uni-freiburg.de/.

</details>


### [81] [Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms](https://arxiv.org/abs/2602.23177)
*Bin Zeng,Johannes Künzel,Anna Hilsmann,Peter Eisert*

Main category: cs.CV

TL;DR: 提出一种面向行进列车车载单目相机的、实时且物理约束的跟踪-计数框架；融合YOLOv11m检测、EfficientNet-B0外观编码与DeepSORT，并以基于针孔几何的3D物理约束卡尔曼滤波(Phys-3D)建模运动，结合“虚拟计数带+持久化”缓解遮挡导致的计数脆弱；在自建MOT-RPCH基准上将计数误差降至2.97%。


<details>
  <summary>Details</summary>
Motivation: 站台拥挤度的实时精确计数对安全与运能管理至关重要；但列车进站时相机在运动、视角剧变与密集遮挡使传统假设静态相机或弱物理先验的跟踪-检测方法失效，导致不稳定计数。

Method: 构建统一的检测-外观-3D运动推理实时管线：1) 迁移学习的YOLOv11m进行头部检测；2) EfficientNet-B0提取外观特征并在DeepSORT中做数据关联；3) 物理约束3D卡尔曼滤波(Phys-3D)将针孔成像几何纳入状态与观测模型，限制为物理可行的3D运动；4) 设计带持久化的虚拟计数带，跨越短时丢失/遮挡维持稳定计数。

Result: 在MOT-RailwayPlatformCrowdHead数据集上实现实时运行，并将人群计数误差降至2.97%，在有相机运动与严重遮挡条件下仍保持鲁棒。

Conclusion: 将第一性原理几何与运动先验注入跟踪框架，可在车载动态视角下实现可靠人群计数，服务于列车调度与站台安全管理。

Abstract: Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.

</details>


### [82] [Uni-Animator: Towards Unified Visual Colorization](https://arxiv.org/abs/2602.23191)
*Xinyuan Chen,Yao Xu,Shaowen Wang,Pengjie Song,Bowen Deng*

Main category: cs.CV

TL;DR: Uni-Animator提出基于Diffusion Transformer的统一素描上色框架，兼顾图像与视频，通过参考增强、物理细节强化与动态RoPE编码，实现精确颜色迁移、高频细节保真与时序一致性，在两任务上达到了与专用方法相当的表现。


<details>
  <summary>Details</summary>
Motivation: 现有素描上色方法难以统一图像与视频场景：多/单参考颜色迁移不精准；高频物理细节（纹理）易丢失；大运动视频存在时序不一致与运动伪影。需要一种统一方法同时解决颜色对齐、细节保真与时空一致性。

Method: 1) 视觉参考增强：引入实例级patch嵌入以精准对齐并融合参考颜色信息，实现更稳健的单/多参考颜色迁移；2) 物理细节强化：利用“物理特征”表征并保留高频纹理与物理细节；3) 基于素描的动态RoPE编码：自适应建模运动感知的时空依赖，缓解大运动下的时序不一致与伪影；整体构建在DiT扩散变换器框架上，实现图像与视频的统一建模。

Result: 广泛实验显示：在图像与视频素描上色任务上均取得有竞争力表现，颜色对齐更精准，细节更丰富，且视频时序一致性更强；在统一范式下达到或匹配任务特定方法的效果。

Conclusion: Uni-Animator证明了在单一DiT框架下可统一处理图像与视频素描上色，通过参考增强、细节强化与动态时空编码兼顾颜色精度、细节保真与时序稳定，具备跨域统一能力。

Abstract: We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.

</details>


### [83] [FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification](https://arxiv.org/abs/2602.23192)
*Thomas Woergaard,Raghavendra Selvan*

Main category: cs.CV

TL;DR: 提出FairQuant：在比特预算下进行面向公平性的混合精度量化，4–6位平均精度即可接近8位准确率，同时提升最差群体表现。


<details>
  <summary>Details</summary>
Motivation: 现有量化（QAT/PTQ）主要关注效率与总体精度权衡，忽略算法公平性；在医疗影像分类中，不同人群（如肤色分组）可能受影响更大，需要在压缩同时显式控制公平性与比特资源分配。

Method: 提出FairQuant框架：1) 基于群体的敏感性/重要性分析，识别对不同群体影响的模块；2) 在显式比特预算下进行混合精度分配（按层/单元分配位宽）；3) 可学习的位宽感知量化（BAQ）同时优化权重与每单元位宽；4) 在训练目标中加入码率与公平性正则，联合优化准确率、公平性与压缩率。评测于ResNet18/50、DeiT-Tiny、TinyViT与Fitzpatrick17k、ISIC2019。

Result: 在平均4–6位量化下，可恢复大部分统一8位模型的准确率；在相同预算下，相比统一4位与统一8位基线，提升最差群体（worst-group）性能；总体公平性指标与共享预算基线相当或更优。

Conclusion: 面向公平的混合精度量化在严格比特预算下可同时保留精度并改进群体公平性。FairQuant通过群体感知重要性、预算化分配与可学习BAQ实现对准确率-公平性-压缩的三者平衡，适用于多种CNN/ViT与皮肤病数据集。

Abstract: Compressing neural networks by quantizing model parameters offers useful trade-off between performance and efficiency. Methods like quantization-aware training and post-training quantization strive to maintain the downstream performance of compressed models compared to the full precision models. However, these techniques do not explicitly consider the impact on algorithmic fairness. In this work, we study fairness-aware mixed-precision quantization schemes for medical image classification under explicit bit budgets. We introduce FairQuant, a framework that combines group-aware importance analysis, budgeted mixed-precision allocation, and a learnable Bit-Aware Quantization (BAQ) mode that jointly optimizes weights and per-unit bit allocations under bitrate and fairness regularization. We evaluate the method on Fitzpatrick17k and ISIC2019 across ResNet18/50, DeiT-Tiny, and TinyViT. Results show that FairQuant configurations with average precision near 4-6 bits recover much of the Uniform 8-bit accuracy while improving worst-group performance relative to Uniform 4- and 8-bit baselines, with comparable fairness metrics under shared budgets.

</details>


### [84] [ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation](https://arxiv.org/abs/2602.23203)
*Junhu Fu,Shuyu Liang,Wutong Li,Chen Ma,Peng Huang,Kehao Wang,Ke Chen,Shengli Lin,Pinghong Zhou,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: ColoDiff 是一个面向结肠镜视频的扩散式生成框架，兼顾时间一致性与临床属性可控性，并通过非马尔可夫采样大幅加速生成；在多数据集与多下游任务上验证，生成平滑、动态丰富，可用于缓解临床数据稀缺。


<details>
  <summary>Details</summary>
Motivation: 结肠镜视频对肠道疾病诊断至关重要，但真实数据昂贵稀缺、病变外观多样、肠道结构不规则、成像模态多变，现有生成模型在时间一致性与精细临床属性控制方面不足，难以为下游临床任务提供高质量、可控的合成数据。

Method: 提出扩散模型 ColoDiff：1) TimeStream 模块通过跨帧标记化解耦时序依赖，增强对不规则肠道运动与相机运动的动态建模；2) Content-Aware 模块引入噪声注入嵌入与可学习原型，实现对疾病类型、模态、肠道清洁度等临床属性的精细控制；3) 采用非马尔可夫采样策略，将采样步数减少90%以上以实现近实时生成。

Result: 在三个公共数据集与一个医院自有数据库上进行评测，除常规生成质量指标外，还在疾病诊断、模态判别、肠道准备评分、病灶分割等下游任务上验证。结果显示，ColoDiff 生成的视频过渡平滑、动态丰富，并能有效支持下游任务。

Conclusion: ColoDiff 能生成时间一致、内容可控的结肠镜视频，显著加速生成过程，并展示以合成视频补充真实数据、缓解临床数据稀缺的潜力。

Abstract: Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.

</details>


### [85] [Motion-aware Event Suppression for Event Cameras](https://arxiv.org/abs/2602.23204)
*Roberto Pellerito,Nico Messikommer,Giovanni Cioffi,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出首个“运动感知事件抑制”框架，实时过滤由自运动与独立运动物体触发的事件；在分割IMOs并预测其未来运动的同时进行前瞻性抑制，SOTA 精度与速度兼得，并提升下游ViT与事件里程计表现。


<details>
  <summary>Details</summary>
Motivation: 事件相机在动态场景中会被自运动与独立运动物体触发的大量事件淹没，影响静态场景理解与下游任务（如VO/SLAM、识别）。需要一种能在产生前就抑制不利动态事件、并保持实时性的机制。

Method: 提出Motion-aware Event Suppression框架：将当前事件流中IMOs进行联合分割并预测其未来运动，由此对即将发生的动态事件进行前瞻性抑制；采用轻量级架构，实时运行（173Hz，<1GB显存）。

Result: 在EVIMO基准上，分割精度较前SOTA提升67%，推理速率提升53%；在下游任务中，通过token pruning使Vision Transformer推理加速83%；在事件视觉里程计上将ATE降低13%。

Conclusion: 该框架在保证极高实时性的同时显著提升IMO分割与下游任务性能，证明了面向未来的事件抑制策略的有效性与实用价值。

Abstract: In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the challenging EVIMO benchmark by 67\% in segmentation accuracy while operating at a 53\% higher inference rate. Moreover, we demonstrate significant benefits for downstream applications: our method accelerates Vision Transformer inference by 83\% via token pruning and improves event-based visual odometry accuracy, reducing Absolute Trajectory Error (ATE) by 13\%.

</details>


### [86] [EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents](https://arxiv.org/abs/2602.23205)
*Wenjia Wang,Liang Pan,Huaijin Pi,Yuke Lou,Xuqian Ren,Yifan Wu,Zhouyingcheng Liao,Lei Yang,Rishabh Dabral,Christian Theobalt,Taku Komura*

Main category: cs.CV

TL;DR: 提出EmbodMocap：用两部可移动iPhone的便携低成本双目RGB‑D采集方案，将人和场景统一重建到度量尺度世界坐标，实现场景一致的人体动作捕捉，并显著优于单目/单机方法；数据驱动三类任务：单目人-景重建、基于物理的角色综合动画、以及仿人机器人控制（经仿真到现实）。


<details>
  <summary>Details</summary>
Motivation: 现有人体动作与场景联合数据采集依赖昂贵棚拍与可穿戴设备，难以在真实环境大规模获取带度量尺度、场景一致的人类行为数据；单目/单机方法存在深度与尺度歧义，限制了对具身智能中感知-理解-行动的训练。

Method: 使用两台移动iPhone同步获取双视角RGB‑D序列，提出联合标定与配准，将两路数据对齐到统一的度量世界坐标；进行人体与场景的联合重建，实现无静态相机/标记的场景一致捕捉；与光学动作捕捉真值比较评估。

Result: 双视角设置有效消除深度与尺度歧义，在人体与场景的对齐和重建精度上优于单iPhone或单目模型，并与光学动捕真值接近；基于该数据成功推动三项任务：1) 微调前馈模型实现单目度量尺度的人-景重建；2) 物理角色动画中扩展人-物交互技能与场景感知跟踪；3) 通过仿真到现实的强化学习训练仿人机器人复现视频中的人类动作。

Conclusion: EmbodMocap以低成本、便携的双iPhone采集与联合标定重建，实现日常环境下度量尺度、场景一致的人体与场景捕捉，显著优于单目方案，并为单目重建、物理动画与机器人控制等具身AI任务提供高质量数据与性能提升。

Abstract: Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.

</details>


### [87] [Through BrokenEyes: How Eye Disorders Impact Face Detection?](https://arxiv.org/abs/2602.23212)
*Prottay Kumar Adhikary*

Main category: cs.CV

TL;DR: 该论文构建“BrokenEyes”计算框架，模拟5种常见视力障碍，并量化其对深度网络特征表示的扰动；发现白内障与青光眼造成的特征失真最显著。


<details>
  <summary>Details</summary>
Motivation: 现实中视觉障碍广泛存在，但我们缺少系统方法去模拟这些障碍并评估它们对机器视觉（类神经特征表示）的影响，从而难以理解感知退化如何影响表征学习与下游任务鲁棒性。

Method: 提出BrokenEyes系统，模拟五类眼疾（AMD、白内障、青光眼、屈光不正、糖网病）的成像退化；在正常与疾病条件下，使用人类与非人类数据训练/测试深度模型；通过特征图分析与度量（激活能量、余弦相似度）比较表示差异，定位受到干扰的网络层与通道。

Result: 在疾病条件下，模型特征出现系统性扭曲；其中白内障与青光眼导致的特征破坏最强，表现为激活能量下降、特征方向变化（余弦相似度显著降低）；这些模式与临床/神经加工已知困难相一致。

Conclusion: 视觉输入退化会显著改变深度模型的内部表征，不同病种影响差异化；定量指标可用于评估失真严重度并为模型与辅助技术的鲁棒性改进提供依据。

Abstract: Vision disorders significantly impact millions of lives, altering how visual information is processed and perceived. In this work, a computational framework was developed using the BrokenEyes system to simulate five common eye disorders: Age-related macular degeneration, cataract, glaucoma, refractive errors, and diabetic retinopathy and analyze their effects on neural-like feature representations in deep learning models. Leveraging a combination of human and non-human datasets, models trained under normal and disorder-specific conditions revealed critical disruptions in feature maps, particularly for cataract and glaucoma, which align with known neural processing challenges in these conditions. Evaluation metrics such as activation energy and cosine similarity quantified the severity of these distortions, providing insights into the interplay between degraded visual inputs and learned representations.

</details>


### [88] [Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction](https://arxiv.org/abs/2602.23214)
*Chenhe Du,Xuanyu Tian,Qing Wu,Muyu Liu,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 提出Dual-Coupled PnP Diffusion和频域Spectral Homogenization以解决PnP扩散先验在强退化下的稳态偏差与幻觉问题，保证对数据一致性收敛并与扩散先验统计假设匹配，CT/MRI中实现SOTA且更快收敛。


<details>
  <summary>Details</summary>
Motivation: 现有PnP扩散（HQS/PG等）是“无记忆”更新，仅依赖瞬时梯度，缺乏历史积分反馈，导致在强噪声/欠采样下对物理测量不严格一致，出现不可消除的稳态偏差；引入经典对偶变量可消偏差，但与扩散先验AWGN假设不匹配会诱发结构化幻觉。

Method: 1) Dual-Coupled PnP Diffusion：在PnP框架中恢复并耦合对偶变量，提供积分型反馈，几何上保证轨迹渐近收敛至精确数据流形（严格数据一致性）。2) Spectral Homogenization（SH）：对累积的对偶残差做频域调制，将其“去色化”为与扩散先验匹配的伪AWGN输入，使优化轨迹与去噪器统计流形对齐。

Result: 在CT与MRI重建上，相比主流PnP解算器，达到更高保真度，解决偏差—幻觉权衡，并显著加速收敛；在强退化场景下保持严格数据一致性且无明显幻觉伪影。

Conclusion: 通过对偶耦合实现无偏的数据一致性收敛，通过频谱均质化消除与扩散先验统计失配，二者协同在医学成像逆问题中带来SOTA性能与更快收敛，兼顾物理一致性与先验有效性。

Abstract: Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.

</details>


### [89] [Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks](https://arxiv.org/abs/2602.23217)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.CV

TL;DR: 提出多维任务学习（MTL）框架，基于广义爱因斯坦MLP（GE-MLP），直接在张量上通过爱因斯坦乘积计算，统一表述分类/分割/检测等任务并扩展到更广的任务空间。


<details>
  <summary>Details</summary>
Motivation: 现有视觉任务常以矩阵/向量参数和展平输入来建模，导致结构信息破坏与任务表达受限；需要一种能原生处理多维结构、并在不丢失信息的前提下控制维度保留/收缩的统一理论框架。

Method: 构建GE-MLP：以张量参数与爱因斯坦乘积为核心，不依赖展平操作；形式化定义“任务空间”，用维度配置（哪些维度被保留、哪些被收缩）来刻画任务；通过数学推导展示不同视觉任务在该空间中的位置与等价性。

Result: 证明分类、分割、检测等是MTL的特例；严格证明MTL的任务空间严格大于传统矩阵化方法所能原生表达的空间，能够无损处理如时空与跨模态预测等在传统方法中需破坏性展平的情形。

Conclusion: GE-MLP提供基于张量代数的统一视角与工具，避免展平带来的信息损失，指导和扩展视觉任务的设计与比较，支持更丰富的多维任务配置。

Abstract: This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.

</details>


### [90] [UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception](https://arxiv.org/abs/2602.23224)
*Mohammad Mahdavian,Gordon Tan,Binbin Xu,Yuan Ren,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: UniScale 是一个统一且具尺度感知能力的多视角3D重建模型，单次前向即可从多张图像联合估计相机内外参、尺度不变深度/点云以及场景的米制尺度，并能按需融合几何先验；在机器人场景中具有强泛化与稳健性。


<details>
  <summary>Details</summary>
Motivation: 机器人视觉导航需要从原始图像可靠获取场景结构与真实尺度，以服务定位、规划与交互；现有方法要么依赖已知相机参数/位姿、要么缺乏米制尺度与跨环境泛化，且常需复杂训练与耦合模块，难以在资源受限的机器人部署。

Method: 提出统一的前向网络：结合全局上下文推理与“相机感知”的特征表达，从多视图同时回归相机内参与外参、尺度不变深度与点图(Point Map)，并估计场景米制尺度；框架模块化、语义感知，可按需插入几何先验（已知内参、已知位姿等）以提升性能；无需从零训练，复用无几何编码的预训练世界先验。

Result: 在多项基准上取得强泛化与稳定表现，能在不同环境下恢复米制尺度；在提供相机内参与/或位姿先验时进一步提升精度与鲁棒性。

Conclusion: UniScale 将尺度恢复与多视角重建统一于单模型，兼容可选几何先验，且易部署于资源受限的机器人系统，为具米制感知的3D重建与导航提供通用方案。

Abstract: We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.

</details>


### [91] [MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction](https://arxiv.org/abs/2602.23228)
*Yizhi Li,Xiaohan Chen,Miao Jiang,Wentao Tang,Gaoang Wang*

Main category: cs.CV

TL;DR: 提出MovieTeller：一个免训练、工具增强的渐进式抽象框架，用于长片电影/剧集摘要，借助人脸识别作为事实锚点并分阶段汇总以绕过VLM上下文限制，显著提升人物一致性、事实准确性与叙事连贯性。


<details>
  <summary>Details</summary>
Motivation: 通用VLM在长时视频摘要中常出现人物ID不一致、事实错误与叙事破碎；对长片进行直接端到端总结受上下文长度限制且微调代价高。

Method: 使用“工具增强+渐进式抽象”。1) 工具：外接人脸识别模型产出角色身份与边界框，形成可验证的Factual Groundings，并注入VLM提示以约束生成；2) 渐进式：将整部电影拆分多阶段/多片段进行场景级描述，再逐级汇总成剧情梗概；3) 训练自由：以即插即用方式调用现成模型，无需微调。

Result: 在实验中，相比端到端基线，MovieTeller在事实准确度、角色一致性与整体叙事连贯性上取得显著提升。

Conclusion: 通过工具增强的人脸ID锚定与分阶段摘要可有效缓解VLM长上下文与一致性问题，实现对长片更可靠的自动梗概生成，且无需昂贵的模型训练/微调。

Abstract: With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external "tool" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM's reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines.

</details>


### [92] [Large Multimodal Models as General In-Context Classifiers](https://arxiv.org/abs/2602.23229)
*Marco Garosi,Matteo Farina,Alessandro Conti,Massimiliano Mancini,Elisa Ricci*

Main category: cs.CV

TL;DR: 论文比较对比式VLM（如CLIP）与生成式LMM在分类任务中的适用性：LMM零样本不如CLIP，但借助少量上下文示例可追平/超越；在开放世界分类中提出无需训练的CIRCLE，通过为上下文样本赋伪标签并迭代自我修正，显著提升鲁棒性并超过VLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有共识偏向用CLIP类模型做分类，认为LMM更适合复杂推理；但该观点忽视了LMM的“上下文学习”能力，尤其在开放世界设定下，生成式LMM可能更合适，却受不完美上下文的干扰，缺乏鲁棒方法。

Method: 1) 系统基准：在多数据集上评测SOTA LMM，用闭集与开集分类设定，比较零样本与少样本（in-context）表现；与CLIP及其cache-based适配器作为“类in-context”对照。2) 提出CIRCLE：训练免方法，为上下文示例分配伪标签，并利用上下文本身迭代细化伪标签，提升开放世界分类鲁棒性。

Result: - 闭集：LMM零样本落后于CLIP，但加入少量上下文示例后可匹敌或超越带cache适配器的CLIP。- 开放世界：原生LMM对不完美上下文敏感；引入CIRCLE后，LMM在多项实验中建立强鲁棒基线并超过VLM对手。

Conclusion: LMM具备被低估的分类潜力：借助上下文学习，在闭集与开集均可成为统一、灵活的分类器。CIRCLE提供简单有效、免训练的改进途径，凸显以LMM替代专用对比式模型的可行性。

Abstract: Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and find that, although their zero-shot performance is lower than CLIP's, LMMs with a few in-context examples can match or even surpass contrastive VLMs with cache-based adapters, their "in-context" equivalent. We extend this analysis to the open-world setting, where the generative nature of LMMs makes them more suitable for the task. In this challenging scenario, LMMs struggle whenever provided with imperfect context information. To address this issue, we propose CIRCLE, a simple training-free method that assigns pseudo-labels to in-context examples, iteratively refining them with the available context itself. Through extensive experiments, we show that CIRCLE establishes a robust baseline for open-world classification, surpassing VLM counterparts and highlighting the potential of LMMs to serve as unified classifiers, and a flexible alternative to specialized models.

</details>


### [93] [Skarimva: Skeleton-based Action Recognition is a Multi-view Application](https://arxiv.org/abs/2602.23231)
*Daniel Bermuth,Alexander Poeppel,Wolfgang Reif*

Main category: cs.CV

TL;DR: 多视角三角化可显著提升骨架数据精度，从而大幅提高现有人体动作识别模型的性能，说明输入数据质量是当前瓶颈。建议将多摄像头作为标准配置。


<details>
  <summary>Details</summary>
Motivation: 尽管算法层面研究活跃，但对输入骨架数据质量关注不足；作者想验证更精准的3D骨架是否能提升主流模型表现，并评估多相机在实际应用中的性价比。

Method: 采用多摄像头获取同步视频，通过几何三角化生成更准确的3D骨架，将其输入到当前最先进的骨架动作识别模型中，与单视角/低质骨架作为输入的基线进行对比评测。

Result: 使用多视角三角化得到的高精度3D骨架显著提升了SOTA动作识别模型的准确率，实验表明模型性能受限于输入骨架质量而非模型容量。

Conclusion: 数据质量（尤其是骨架精度）是限制骨架动作识别性能的关键因素；多相机带来的成本相对于性能收益非常划算，建议未来研究将多视角设置视为标准方案。

Abstract: Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself. This work demonstrates that by making use of multiple camera views to triangulate more accurate 3D~skeletons, the performance of state-of-the-art action recognition models can be improved significantly. This suggests that the quality of the input data is currently a limiting factor for the performance of these models. Based on these results, it is argued that the cost-benefit ratio of using multiple cameras is very favorable in most practical use-cases, therefore future research in skeleton-based action recognition should consider multi-view applications as the standard setup.

</details>


### [94] [Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents](https://arxiv.org/abs/2602.23235)
*Zhou Xu,Bowen Zhou,Qi Wang,Shuwen Feng,Jingyu Xiao*

Main category: cs.CV

TL;DR: GUIPruner是一个无需训练的纯视觉GUI智能体压缩框架，通过时间自适应分辨率与分层结构感知剪枝，在高压缩下保持空间拓扑与注意力匹配，显著降低FLOPs与编码时延，同时维持>94%的性能，实现实时高精度导航。


<details>
  <summary>Details</summary>
Motivation: 纯视觉GUI智能体需要处理高分辨率截图与长历史轨迹，存在大量时空冗余。现有压缩方法与智能体注意模式不匹配：时间上对历史统一编码违背“记忆衰退”；空间上无结构的剪枝破坏网格与布局，导致坐标对齐错误与空间幻觉。

Method: 提出GUIPruner（免训练），包含两大组件：1）时间自适应分辨率（TAR）：依据历史重要性进行衰减式重采样/缩放，去除时序冗余；2）分层结构感知剪枝（SSP）：优先保留可交互前景与语义锚点，同时保护全局布局与网格完整性，避免空间拓扑破坏。

Result: 在多种基准上取得SOTA；在Qwen2-VL-2B上，FLOPs降低3.4倍、视觉编码时延加速3.3倍，同时保留原始性能的94%以上，避免大模型在高压缩下的性能崩塌。

Conclusion: 与智能体注意模式对齐且维护空间拓扑的训练免压缩策略可在不牺牲准确度的前提下显著提升纯视觉GUI代理的效率，实现低资源的实时高精度导航。

Abstract: Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's "fading memory" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.

</details>


### [95] [Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving](https://arxiv.org/abs/2602.23259)
*Jiangxin Sun,Feng Xue,Teng Long,Chang Liu,Jian-Fang Hu,Wei-Shi Zheng,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出RaWMPC：不依赖专家示范的端到端自动驾驶框架，通过世界模型预测和显式风险评估选择低风险动作，并利用风险感知交互和自评蒸馏，在分布内外均优于SOTA且更可解释。


<details>
  <summary>Details</summary>
Motivation: IL在E2E自动驾驶中依赖专家行为最小化偏差，遇到长尾/未见场景泛化差、易做出不安全决策。作者探问：无专家监督能否做出可靠决策？希望以鲁棒控制克服“只像专家开”的局限。

Method: 1) 构建风险感知世界模型，输入环境与候选动作，预测多步后果；2) 显式风险评估，从预测后果中计算风险并选低风险动作；3) 风险感知交互：在训练中有意暴露模型于危险/灾难性行为，使其学会预测并规避；4) 自评蒸馏：将已训练世界模型的风险规避能力蒸馏到生成式动作提议网络，用于测试时生成低风险候选动作，全程无需专家示范。

Result: 在多种数据集与场景中（含OOD长尾），RaWMPC在安全性、成功率、鲁棒性等指标上优于现有方法，并提供更好的决策可解释性。

Conclusion: 通过风险感知的世界模型预测与MPC式低风险选择、加上自评蒸馏，RaWMPC在不依赖专家示范的前提下实现更强的泛化与安全决策，为E2E自动驾驶提供了鲁棒且可解释的替代范式。

Abstract: With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of "only driving like the expert" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.

</details>


### [96] [Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling](https://arxiv.org/abs/2602.23262)
*Jasmine Bayrooti,Weiwei Kong,Natalia Ponomareva,Carlos Esteves,Ameesh Makadia,Amanda Prorok*

Main category: cs.CV

TL;DR: 提出一种光谱差分隐私框架：低频（小波域）用DP训练自回归分词器生成粗略结构，高频细节用公共超分模型上采样，从而在保证隐私的同时提升图像质量。


<details>
  <summary>Details</summary>
Motivation: DP在敏感图像生成中很重要，但DP-SGD对所有参数一视同仁加噪，导致图像质量尤其高频纹理劣化。观察到隐私敏感信息多在低频（形状、人脸轮廓），高频纹理更通用公共。

Method: 两阶段：1）在敏感数据的小波低分辨率系数上，对自回归光谱图像分词器进行差分隐私微调；2）用公共预训练的超分辨模型进行高分辨率上采样。将隐私预算集中于全局结构，细节通过DP后处理性质由公共模型补足。

Result: 在MS-COCO与MM-CelebA-HQ上，相比主流DP图像生成框架，本方法生成的图像质量与风格捕获更好，实现更优隐私-效用权衡。

Conclusion: 按频谱解耦隐私敏感度、低频用DP建模、高频用公共上采样可显著缓解DP训练带来的画质损失，是DP图像生成的有效新范式。

Abstract: Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.

</details>


### [97] [LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction](https://arxiv.org/abs/2602.23290)
*Zhengyang Wei,Renzhi Jing,Yiyi He,Jenny Suckale*

Main category: cs.CV

TL;DR: 提出LineGraph2Road：在稀疏全局欧式图上进行边二分类，通过线图+Graph Transformer预测道路连通性，含立交分层头与耦合NMS，在City-scale/SpaceNet/Global-scale上以TOPO-F1与APLS达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有道路提取常分为关键点检测与连通性预测，但难以捕获长距离依赖与复杂拓扑（如立交、多层交叉），以及端点融合对同构链接表示弱，导致拓扑断裂与错误连接。

Method: 1) 从分割掩膜提取关键点并构建稀疏全局欧式图：节点为关键点，边为阈值距离内潜在道路段；2) 将该图转换为线图，使“边”变为“节点”，在其上用Graph Transformer进行连通性二分类，获得更丰富的链路表示与全局关系推理；3) 设计立交（overpass/underpass）判别头解决多层交叉；4) 提出耦合NMS以在去冗余时保留关键连接。

Result: 在City-scale、SpaceNet、Global-scale三基准上，连通性与拓扑指标优异，TOPO-F1与APLS达SOTA，同时能捕捉细粒度视觉细节，适于实用部署。

Conclusion: 将连通性预测表述为线图上的边二分类并用Graph Transformer进行全局关系推理，有效提升道路拓扑恢复与多层交叉处理，结合耦合NMS实现更可靠网络；方法在多数据集上验证有效并具实际应用潜力。

Abstract: The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available.

</details>


### [98] [PGVMS: A Prompt-Guided Unified Framework for Virtual Multiplex IHC Staining with Pathological Semantic Learning](https://arxiv.org/abs/2602.23292)
*Fuqiang Chen,Ranran Zhang,Wanming Hu,Deboch Eyob Abera,Yue Peng,Boyun Zheng,Yiwen Sun,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 提出PGVMS：用仅单通道IHC训练数据，实现针对H&E的虚拟多重IHC染色，解决语义指导不足、IHC分布不一致与跨染色空间错位三难题。


<details>
  <summary>Details</summary>
Motivation: 小活检样本组织量不足限制真实多重IHC；现有虚拟多染色方法在语义指导、免疫化学分布一致性和跨模态配准上存在缺陷，影响临床可用性与定量可靠性。

Method: PGVMS框架，基于仅uniplex数据训练。三项关键设计：1) 自适应提示引导：利用病理视觉语言模型，动态生成/调整多染色提示以提升语义指导；2) PALS（protein-aware learning strategy）：直接量化并约束蛋白表达分布，保持蛋白表达图样的准确性；3) PCLS（prototype-consistent learning strategy）：通过跨图像语义原型交互，校正不同染色间的空间错位。

Result: 在虚拟多重IHC生成任务中，PGVMS在语义一致性、蛋白分布保真度与空间对齐方面优于现有方法（摘要未给出具体指标，但宣称显著改进）。

Conclusion: 通过引入提示引导、蛋白分布感知与原型一致性学习，PGVMS在仅用单通道IHC训练的条件下，能更可靠地从H&E生成多种IHC映射，为小样本病理中全面分子表型分析提供可行路径。

Abstract: Immunohistochemical (IHC) staining enables precise molecular profiling of protein expression, with over 200 clinically available antibody-based tests in modern pathology. However, comprehensive IHC analysis is frequently limited by insufficient tissue quantities in small biopsies. Therefore, virtual multiplex staining emerges as an innovative solution to digitally transform H&E images into multiple IHC representations, yet current methods still face three critical challenges: (1) inadequate semantic guidance for multi-staining, (2) inconsistent distribution of immunochemistry staining, and (3) spatial misalignment across different stain modalities. To overcome these limitations, we present a prompt-guided framework for virtual multiplex IHC staining using only uniplex training data (PGVMS). Our framework introduces three key innovations corresponding to each challenge: First, an adaptive prompt guidance mechanism employing a pathological visual language model dynamically adjusts staining prompts to resolve semantic guidance limitations (Challenge 1). Second, our protein-aware learning strategy (PALS) maintains precise protein expression patterns by direct quantification and constraint of protein distributions (Challenge 2). Third, the prototype-consistent learning strategy (PCLS) establishes cross-image semantic interaction to correct spatial misalignments (Challenge 3).

</details>


### [99] [Towards Long-Form Spatio-Temporal Video Grounding](https://arxiv.org/abs/2602.23294)
*Xin Gu,Bing Fan,Jiali Yao,Zhipeng Zhang,Yan Huang,Cheng Han,Heng Fan,Libo Zhang*

Main category: cs.CV

TL;DR: 提出一种面向长视频的自回归Transformer（ART-STVG），按帧串流处理并结合记忆选择与级联时空解码，在扩展的长视频数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时空视频定位研究多针对不到一分钟的短视频，无法有效应对真实场景中分钟到小时级长视频；长视频包含更长时间跨度与大量无关信息，使一次性处理全部帧的传统方法低效且易受干扰。

Method: 将长视频视为流式输入，采用自回归Transformer顺序处理帧；在解码器中引入空间与时间记忆库以建模时空上下文，并设计记忆选择策略过滤与当前帧无关的记忆；采用级联式时空定位结构，先进行空间解码再将细粒度空间线索传递给时间解码以提升长视频中的时间定位。

Result: 在新扩展的LF-STVG数据集上，相比最先进方法取得显著提升；在传统短视频STVG基准上也保持有竞争力的性能。

Conclusion: 流式自回归处理结合记忆选择与级联时空解码有效解决长视频STVG难题，兼顾效率与精度，并具备对短视频任务的良好泛化。

Abstract: In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.

</details>


### [100] [ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2602.23295)
*Ayush Roy,Wei-Yang Alex Lee,Rudrasis Chakraborty,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 提出ManifoldGD：一种无需再训练的扩散式数据蒸馏方法，用流形一致性引导在每个去噪步约束生成轨迹，使合成数据更具代表性、多样性与保真度，并在FID、嵌入距离与分类准确率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集训练代价高且含冗余概念；现有基于扩散的免训练蒸馏多为无引导或简单朝IPC质心的模式引导，无法充分捕获语义结构与类内变化，导致合成数据的多样性与保真度受限。

Method: 1) 使用预训练VAE对真实数据提取潜特征，进行层次性分裂聚类，得到多尺度IPC（实例原型）核集，既覆盖粗语义模态又保留细粒度类内变化；2) 在每个扩散去噪步，基于局部IPC邻域构建该步的潜空间流形与其切空间；3) 计算与语义模式相关的对齐向量，并将其投影到局部切空间，强制生成轨迹沿流形一致方向演化，确保语义一致且贴合真实潜流形；4) 全流程无需额外训练。

Result: 在多项基准上，相较于现有免训练与需训练的蒸馏基线，FID更低、合成与真实嵌入的l2距离更小、用合成数据训练的分类准确率更高，表现稳定且一致。

Conclusion: ManifoldGD首个显式几何感知的免训练数据蒸馏框架，通过流形一致性引导提升代表性、多样性与图像质量，无需再训练即可取得全面优于现有方法的效果。

Abstract: In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.

</details>


### [101] [PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM](https://arxiv.org/abs/2602.23297)
*Yiqing Wang,Chunming He,Ming-Chen Lu,Mercy Pawar,Leslie Niziol,Maria Woodward,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出PRIMA框架，将临床风险—疾病知识融入图像—元数据对齐，通过RAG精炼BERT、与DINOv3双编码器预训练及四重损失实现多粒度软标签对齐，并用Qwen-3融合特征，显著提升医疗影像分类的准确性与鲁棒性，且无需海量数据与算力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态诊断方法把临床元数据当作孤立标签，忽略临床描述中富含的语义与先验，导致图像与文本之间语义鸿沟大、对模糊关联处理差、对小样本/有限算力环境不友好。

Method: 1) 通过RAG构建风险—疾病相关性专家语料，微调/精炼Clinical ModernBERT，将诊断先验注入文本编码器；2) 采用DINOv3(图像)与精炼BERT(文本)的双编码器预训练；3) 设计四种互补损失实现多粒度语义对齐，并以软标签刻画临床相关性的模糊性；4) 训练后用Qwen-3对齐并融合图像与文本特征用于疾病分类；5) 进行对比与消融实验评估鲁棒性与SOTA表现。

Result: 在多项实验中，PRIMA在准确率/鲁棒性方面显著优于SOTA基线，能更好地协调像素级特征与抽象临床知识，同时在数据量与算力受限条件下仍保持优势。

Conclusion: 将领域知识显式注入并在预训练阶段实现多粒度对齐，可有效缩小图文模态鸿沟，提升医疗诊断模型的精度与稳健性；PRIMA为低资源场景提供可扩展路径，代码将在录用后公开。

Abstract: Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.

</details>


### [102] [ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding](https://arxiv.org/abs/2602.23306)
*Yiran Guan,Sifan Tu,Dingkang Liang,Linghao Zhu,Jianzhong Ju,Zhenbo Luo,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 提出ThinkOmni：无需额外训练与数据，通过让大型推理模型在解码中指导OLLM，并用逐步对比缩放自适应融合感知与推理信号，在六个多模态推理基准上显著提升（MathVista 70.2、MMAU 75.5）。


<details>
  <summary>Details</summary>
Motivation: 现有OLLM虽能处理多模态输入，但缺乏类似LRM的复杂推理能力；直接强化训练面临高质数据稀缺、任务适配困难与算力昂贵等瓶颈，因此需要一种不依赖额外训练与数据、可泛化迁移LRM推理能力到多模态场景的方法。

Method: 提出ThinkOmni框架，核心包括：1) LRM-as-a-Guide：使用现成的大型推理模型在生成过程中对OLLM进行引导，提升其推理链质量；2) Stepwise Contrastive Scaling：在逐步解码中对比与调节来自感知（OLLM）与推理（LRM）的信号强度，实现自适应、无需手工超参的平衡融合，从而把文本推理能力迁移到全模态。

Result: 在六个多模态推理基准上稳定增益，主结果达MathVista 70.2、MMAU 75.5，相比原始OLLM显著提升，表明方法在多任务、多数据源下具有鲁棒与泛化性。

Conclusion: ThinkOmni以零训练、零数据的方式，将文本LRM的推理力迁移至多模态推理，提供了灵活可扩展的方案与对感知-推理融合的新见解，为提升OLLM推理能力提供了通用路径。

Abstract: Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.

</details>


### [103] [Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?](https://arxiv.org/abs/2602.23339)
*Tilemachos Aravanis,Vladan Stojnić,Bill Psomas,Nikos Komodakis,Giorgos Tolias*

Main category: cs.CV

TL;DR: 提出在开放词汇分割中引入“少样本+检索增强测试时自适应器”，通过融合文本提示与像素标注支持集的视觉特征，在每幅图像上学习轻量分类器；以学习式、逐查询融合替代手工后期融合，支持持续扩展支持集与个性化细粒度分割，显著缩小零样本与全监督分割性能差距，同时保持开放词汇能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的开放词汇分割仅用图像级监督且语言存在语义歧义，导致像素级预测弱于全监督方法。需要一种既保留开放词汇泛化、又能利用少量像素标注以提升精度的方案，并能处理细粒度与个性化类别。

Method: 引入少样本设定：提供文本提示外，再给出带像素标注的支持集图像；在测试阶段使用检索增强的自适应器，为每张待测图像学习一个轻量级的、按查询（像素/区域）进行的文本-视觉融合分类器。与以往后期、手工融合不同，采用可学习的逐查询融合机制；支持持续扩展的支持集以适配新类别与个性化需求。

Result: 在多项实验中，该方法在保持开放词汇能力的同时，显著缩小零样本与全监督分割的性能差距，尤其在细粒度与个性化分割场景中表现突出。

Conclusion: 通过少样本支持和检索增强的测试时适配、以及学习式逐查询融合，可有效缓解VLM监督粗糙与语言歧义带来的限制，提升开放词汇分割性能并保持类无限扩展能力。

Abstract: Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.

</details>


### [104] [Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training](https://arxiv.org/abs/2602.23357)
*Aheli Saha,René Schuster,Didier Stricker*

Main category: cs.CV

TL;DR: 研究分析事件相机内在参数对基于事件数据目标检测模型性能的影响，并据此提升模型的跨传感器鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 事件相机输出具有异步、低延迟和高动态范围等优势，但数据形态新颖，现有数据多样性不足，关于描述其信号的参数影响也缺乏系统研究，限制了下游任务（如目标检测）的应用与泛化。

Method: 系统地改变/建模事件相机的内在参数（如阈值、噪声特性、时间分辨率、极性响应、对比度阈值等），并在这些设定下训练与评测目标检测模型；分析参数变化对性能的敏感性与交互影响；据此设计训练与数据处理策略（如参数随机化、域随机化或传感器仿真）以提升模型对不同传感器的适应性。

Result: 实验证明模型性能对若干关键内参显著敏感；通过在训练中引入面向内参的多样化/随机化与相机无关的数据增强，模型在不同事件相机与配置上的检测精度与稳健性明显提升。

Conclusion: 理解与利用事件相机内参对目标检测至关重要；通过针对内参的分析与训练策略，可获得对传感器差异鲁棒的模型，实现更强的跨设备泛化能力。

Abstract: Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.

</details>


### [105] [SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation](https://arxiv.org/abs/2602.23359)
*Vaibhav Agrawal,Rishubh Parihar,Pradhaan Bhat,Ravi Kiran Sarvadevabhatla,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 提出SeeThrough3D：在3D布局条件生成中显式建模遮挡，利用半透明3D盒子渲染和视觉token调控预训练扩散/flow文生图模型，实现多物体精准遮挡与相机控制。


<details>
  <summary>Details</summary>
Motivation: 现有遵循3D布局的生成方法虽能生成逼真场景，但难以精确处理物体间遮挡，导致部分可见物体的深度与尺度不一致、属性混淆；缺乏显式相机控制与对遮挡区域的表示。

Method: 1) 提出遮挡感知3D场景表示（OSCR）：将对象表示为半透明3D包围盒，在虚拟环境中从给定相机视角渲染；透明度编码被遮挡区域。2) 从渲染得到的视图提取视觉token，作为对预训练基于flow的文生图模型的条件。3) 采用masked self-attention，将每个3D框与其文本描述绑定，避免多物体属性混淆。4) 构建包含强遮挡的多物体合成数据集进行训练。

Result: 模型在未见类别上也能泛化，能在严格的3D布局与相机控制下，生成具有真实遮挡关系、深度一致与尺度合理的多物体图像；优于现有方法在遮挡精度与多对象一致性上的表现。

Conclusion: 显式遮挡建模与OSCR表示可有效提升3D布局条件生成的遮挡真实性与可控性；结合视觉token与掩码自注意力可缓解属性混淆并提升泛化性，实现精确相机与布局控制。

Abstract: We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.

</details>


### [106] [VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale](https://arxiv.org/abs/2602.23361)
*Sven Elflein,Ruilong Li,Sérgio Agostinho,Zan Gojcic,Laura Leal-Taixé,Qunjie Zhou,Aljosa Osep*

Main category: cs.CV

TL;DR: 提出VGG-T^3：一种将离线前馈3D重建的二次复杂度瓶颈压缩为线性可扩展的方法，通过测试时训练把可变长度KV表示蒸馏为固定规模MLP，实现快速、可扩展且保持全局聚合能力的重建与定位。


<details>
  <summary>Details</summary>
Motivation: 现有离线前馈3D重建方法对输入图像数目的计算与内存开销呈二次增长，限制了大规模场景和大量视角下的应用；而在线线性方法虽可扩展，却常牺牲全局场景聚合与精度。需要一种既线性扩展又保留全局信息的方案。

Method: 关键洞见：二次复杂度源自场景几何以可变长度Key-Value注意力缓存表示。方法：在测试时训练阶段，将该可变长度KV空间通过蒸馏压缩为固定大小的MLP（VGG-T^3），从而将复杂度与内存改为与视角数线性相关；同时保留全局场景聚合能力。

Result: 在1k张图像的重建任务上，54秒完成，较依赖softmax注意力的基线快11.6倍；点云/点图（point map）重建误差显著低于其他线性时间方法；还能用未见图像查询已学习的场景表示以实现可视定位。

Conclusion: 通过将可变KV表示蒸馏为固定MLP，VGG-T^3在保持全局聚合与高精度的同时，实现线性可扩展的3D重建，并具备视觉定位能力，适合大规模多视角场景。

Abstract: We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.

</details>


### [107] [MediX-R1: Open Ended Medical Reinforcement Learning](https://arxiv.org/abs/2602.23363)
*Sahal Shaji Mullappilly,Mohammed Irfan Kurpath,Omair Mohamed,Mohamed Zidan,Fahad Khan,Salman Khan,Rao Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: MediX-R1 提出一种面向医学多模态大模型的开放式强化学习框架，通过多种奖励信号与统一的LLM评测，实现对自由文本与图像+文本任务的稳定训练与更可靠的临床推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLM多依赖选择题或可验证奖励，难以支持开放式、临床可解释的自由回答；传统字符串匹配评测脆弱，无法稳健衡量语义正确性与推理质量。

Method: 以视觉-语言基座为起点，采用Group-Based RL微调，并设计复合奖励：1) 基于LLM裁判的语义准确性（严格YES/NO）；2) 医学嵌入的语义相似度奖励，覆盖同义与术语变体；3) 轻量格式与模态奖励，促进可解释推理与模态识别。评测端采用参考答案驱动的LLM-as-judge统一框架，覆盖纯文本与图文任务，关注语义正确性、推理与情境对齐。

Result: 在仅约5.1万条指令数据下，于医学文本LLM与图文VLM基准上超过强劲开源基线，尤其在开放式临床任务上取得显著提升。

Conclusion: 多信号奖励的开放式RL结合LLM评测为提升医学多模态模型的可靠临床推理提供了实用路径；模型、数据与代码已开源。

Abstract: We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com

</details>
