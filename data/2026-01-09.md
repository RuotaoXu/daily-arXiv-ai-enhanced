<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 79]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: 提出一种将扩散模型与层级细粒度专家标准对齐的方法：先用SFT注入领域知识，再用扩展自DPO的复杂偏好优化（CPO）同时提升正面属性概率、抑制负面属性概率，显著提高绘画生成质量与专家一致性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型对齐多依赖标量奖励或二元偏好，难以表达真实的人类专家评估，其往往是层级化、细粒度和多维正负属性并存；因此需要能刻画复杂标准并据此训练的对齐框架。

Method: 1) 与领域专家共建层级细粒度评价体系，将图像质量分解为树状的多级正负属性；2) 通过监督微调训练一个“辅助扩散模型”以注入该领域知识；3) 提出复杂偏好优化（CPO），在目标扩散模型上扩展DPO：以辅助模型为参考，联合最大化正面属性生成概率并最小化负面属性生成概率，实现对非二元、层级标准的对齐；4) 在绘画生成领域实现，并使用按该标准标注的细粒度属性数据进行训练与评测。

Result: 在绘画生成任务上，CPO相较基线显著提升生成质量与与专家标准的一致性；实验广泛验证了方法在细粒度标准上的对齐效果。

Conclusion: 将层级化细粒度标准引入扩散模型后训练，并用CPO实现非二元偏好对齐，可明显提高复杂专家标准下的对齐与图像质量，为更精细的后训练对齐提供新方向。

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: 提出一种在RGB空间利用五进组合（每通道5种强度，共125种组合）将文本嵌入单像素的方法，单像素对应一个字符，几乎不引入可感知失真，较LSB/PVD/变换/深度学习方法更高效、计算更低。


<details>
  <summary>Details</summary>
Motivation: 现有图像文本隐写多依赖LSB/MSB翻转、PVD、变换域或深度学习等，常需跨多像素或多步骤编码，易引入噪声与可检测失真，且深度模型计算开销大、确定性强易被对抗分析发现。需要一种在空间域中更高容量、更低失真、低计算复杂度的方法。

Method: 在RGB三通道中为每通道设定5个受控强度等级，形成5×5×5=125个离散像素强度组合；将这125个组合映射到字符表（大小写字母、数字、空白和常见符号）。编码：选取最接近目标组合的像素或调整像素到对应组合，实现“一像素一符号”。解码：读取像素三通道各自落入的强度等级，反向查表得到字符。对嵌入前后图像进行MSE、MAE、SNR、PSNR、SSIM、直方图与热力图对比评估。

Result: 在测试中，嵌入后图像在MSE/MAE低、SNR/PSNR高、SSIM接近原图，直方图与热力图差异不显著，说明失真轻微；容量效率显著提升，每像素可编码一个字符；相较LSB/变换/学习方法，计算开销更低、编码解码更简洁。

Conclusion: 基于RGB五进组合的单像素符号映射可在保持视觉质量的同时实现高效文本嵌入，较传统与学习式方法具备更高的嵌入效率与更低计算成本，适用于低失真、低复杂度的图像隐写应用。

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 论文提出一种后训练方法，使统一的多模态生成模型在一次推理中自动从文本推理过渡到图像合成，无需显式切换；通过奖励加权和针对性数据构造，在多个T2I基准上提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态生成架构常需要先生成推理文本再手动切换到图像生成，导致跨模态耦合差、无法自动化多模态生成。作者希望实现真正统一的端到端文本-图像联合生成并探究这种训练与数据策略对T2I性能的影响。

Method: 进行离线的奖励加权后训练：使用模型自生成的合成数据，同时对文本与图像两个模态的质量进行奖励加权优化；比较不同后训练数据策略，尤其是针对模型弱点的定向数据与通用图文语料或基准对齐数据的效果差异；评估联合文本-图像生成过程中各模态的重要性。

Result: 采用上述奖励加权的联合后训练与定向数据策略，在四个不同的T2I基准上取得了一致改进，显示联合奖励与数据设计的有效性。

Conclusion: 通过离线奖励加权与精心设计的定向后训练数据，可实现无需显式模态切换的统一文本-图像生成，并显著提升T2I性能；同时强调两种模态在后训练中的共同重要性。

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: ReHyAt提出混合注意力的递归机制，将Softmax注意力的质量与线性注意力的效率结合，实现分块递归推理与常量显存；通过从软最大模型蒸馏，训练成本降至约160 GPU小时，同时保持SOTA视频质量与线性扩展性。


<details>
  <summary>Details</summary>
Motivation: Transformer式视频扩散模型在长序列上因二次复杂度的自注意而难以扩展；现有纯线性注意（如SANA Video）质量受限，且缺乏从高质量软最大模型迁移的高效管道，需要一种兼顾质量与效率、可用于长时长与端侧生成的机制。

Method: 提出ReHyAt：将Softmax注意力与线性注意力混合，并重构为按块递归的推理，使注意力开销与内存使用随序列长度线性扩展且常量内存；设计轻量蒸馏与微调管线，从现有双向Softmax模型蒸馏知识到ReHyAt，大幅降低训练成本。

Result: 在VBench与VBench-2.0以及人工偏好评测中达到或接近SOTA视频质量；注意力复杂度由二次降为线性，实现长时长与设备端生成的可扩展性；相较从头训练，蒸馏使总训练成本降至约160 GPU小时。

Conclusion: ReHyAt通过混合注意力与递归分块设计，在保持高保真视频生成的同时实现线性复杂度与常量内存，并提供通用的轻量蒸馏/微调食谱，为未来软最大双向视频扩散模型的高效落地与长序列生成解锁可扩展性。

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [5] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: 提出一种用于3D Gaussian Splatting的渐进式压缩编解码器，用RVQ替代标量量化，并使用多分辨率哈希网格引导的自回归熵模型高效编码索引，实现更优率失真与逐层传输。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting在中大型场景下存储开销大，不利于云端与流式传输。已有渐进式压缩多用掩码与标量量化结合空间上下文建模，但标量量化难以充分利用高维特征间相关性，限制率失真表现。

Method: 以残差向量量化（RVQ）替代标量量化对高维原语特征进行分层（粗到细）编码；设计由多分辨率哈希网格引导的自回归熵模型，按顺序预测每个RVQ码本索引的条件概率，从而对各层索引进行高效熵编码，支持渐进式（coarse/refinement）传输。

Result: 在给定抽象中未给出具体数值，但声称能以更高效率压缩RVQ索引，实现较低比特率下保持高保真重建，相比传统方法获得更优率失真。

Conclusion: 通过RVQ与哈希网格引导的自回归熵模型，可更好利用特征相关性，提升3DGS渐进式压缩的率失真效率，适用于云端与流式场景的高效传输。

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [6] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: 对比自建CNN与预训练模型（ResNet‑18、VGG‑16）在五个孟加拉国图像分类数据集上的表现。结果：微调式迁移学习普遍最优，部分数据集提升3%–76%，ResNet‑18微调在Road Damage BD达100%准确。自建CNN更小更快，但在复杂/小数据任务上不如预训练模型微调。


<details>
  <summary>Details</summary>
Motivation: 实践界常面临在算力受限与数据规模不一条件下，如何在自建模型、特征提取或迁移学习间做选择。需要系统量化不同策略在多样数据集上的效果与资源权衡，为方法选型提供经验指南。

Method: 选取五个来自孟加拉国场景的图像分类数据集；比较三类方案：1) 从零训练的自建CNN；2) 预训练模型做固定特征提取+线性/浅层分类器；3) 预训练模型全网络或部分层微调的迁移学习。以ResNet‑18与VGG‑16为代表，评估准确率、参数规模与训练效率。

Result: 微调迁移学习在全部数据集上领先，较自建与特征提取提升3%–76%；在Road Damage BD上，ResNet‑18微调达到100%准确。自建CNN参数约3.4M，显著小于预训练模型（11–134M），在简单任务上训练更高效。

Conclusion: 在复杂任务或训练样本有限时，应优先采用预训练模型并进行微调以获得最佳性能；若资源受限且任务简单，自建轻量CNN可平衡精度与效率。研究给出基于数据特性、算力与性能需求的实用选型建议。

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>


### [7] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: 提出PackCache：一种无需训练的KV缓存管理方法，通过条件锚定、跨帧衰减及空间保持位置编码三机制，动态压缩统一自回归多模态（尤其视频）生成的KV缓存，在48帧生成上整体加速1.7–2.2倍，最后四帧在A40/H200上分别达2.6×/3.7×。


<details>
  <summary>Details</summary>
Motivation: 统一自回归多模态/视频模型以共享token空间做序列建模，依赖KV-cache将注意力复杂度从O(T^2)降至O(T)。但KV-cache随生成长度线性膨胀，成为推理速度与可生成时长的主要瓶颈。观察到：文本/条件图像token长期高关注度（语义锚），而对过往帧的注意随时间距离自然衰减，提示可按时空重要性进行有损压缩。

Method: 提出PackCache，训练免改：1) 条件锚定：优先保留文本与条件图像等语义参考的KV条目；2) 跨帧衰减建模：依据时间距离为历史帧分配缓存预算，距离越远越强压缩/丢弃；3) 空间保持位置编码：在删除缓存时调整/保留空间位置信息，维持3D结构一致性。整体为动态KV压缩与重排策略。

Result: 在48帧长序列的视频生成中，端到端加速1.7–2.2×；对最靠后的四帧（最受KV增长影响的昂贵段），在A40与H200上分别实现2.6×与3.7×加速。

Conclusion: KV缓存具有可利用的时空稀疏性。PackCache无需再训练即可有效压缩KV-cache、维持语义与空间一致性，并显著提升长序列视频生成效率，启示未来统一自回归模型可通过结构化缓存管理延长可生成长度并降低成本。

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

</details>


### [8] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 论文利用EMOCA提取的3D表情与姿态系数并结合时序Transformer与跨模态注意力，实现高精度（AUROC约92%）的压力识别，证明了时序建模与3D几何特征的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有压力识别多依赖AU（面部动作单元）或生理信号，忽视了可与表情解耦的3D面部几何；压力主观且面部可自我控制，需寻找更稳健、可泛化的视觉表征并验证其与生理指标的对应关系。

Method: 在分心驾驶情境中，用EMOCA提取3D表情与姿态系数；通过配对假设检验比较基线与施压阶段，筛选出对压力敏感的系数；提出基于Transformer的时序建模框架，评估单模态、早期融合与跨模态注意力的融合策略；将EMOCA与生理（以及EMOCA+眼动）进行跨模态注意力融合并训练压力分类器。

Result: 56个系数中有41个在基线与压力阶段呈一致、特定相位的响应，且与生理标记可比；跨模态注意力融合EMOCA+生理达到最佳（AUROC 92%，Acc 86.7%），EMOCA+凝视也接近（AUROC 91.8%）。

Conclusion: 解耦的3D面部几何在压力识别中具有显著价值；结合时序Transformer与跨模态注意力能显著提升性能；视觉几何与生理/凝视信息互补，适合构建稳健的驾驶场景压力识别系统。

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

</details>


### [9] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: 用少量配对样本，通过在FLUX.1 Kontext上插入LoRA对流匹配生成模型进行微调，实现RGB→IR与RGB→SAR跨光谱翻译；用合成目标模态训练检测器，并用LPIPS作为下游mAP的可靠代理，显著提升IR/SAR检测效果。


<details>
  <summary>Details</summary>
Motivation: 现实中许多安全关键场景依赖IR/SAR等不可见模态，但主流视觉基础模型主要在RGB上训练，跨模态标注稀缺昂贵。希望用极少配对数据把已有RGB基础模型“改装”为跨光谱译码器，生成与RGB像素对齐的IR/SAR，从而复用现有标注并提升下游检测。

Method: 以FLUX.1 Kontext为基底，插入LoRA模块进行少样本（每域约100对配对图像）微调，设置两种任务：KAIST上RGB→IR，M4-SAR上RGB→SAR。系统扫描一组LoRA超参，评估在50对留出样本上的LPIPS，并据此选择最佳适配器。用译出的IR/SAR合成数据训练YOLOv11n与DETR等检测器；同时尝试将外部RGB数据（LLVIP、FLIR ADAS）经最佳LoRA生成IR来扩充训练集。

Result: 微调后的模型能将RGB翻译成与原图像素对齐的IR/SAR，使得现有RGB框标注可直接用于目标模态。实验发现：在IR与SAR两域，以及DETR在KAIST IR上，较低的LPIPS稳定对应更高的mAP。采用LPIPS挑选的最佳LoRA后，用外部RGB合成的IR可提升KAIST行人检测；在M4-SAR上，合成SAR与少量真实SAR结合，显著提升基础设施检测。

Conclusion: 只需极少配对样本，对流匹配基础模型进行LoRA微调即可获得高质量跨光谱翻译，并能有效提升IR/SAR下游检测。LPIPS可作为强有力的效率代理指标。该策略为不可见模态提供“基础模型式”支持具有前景。

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

</details>


### [10] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: 比较自建CNN与VGG-16、ResNet-50、MobileNet在同一图像分类任务中的表现：预训练模型在少样本下更准、更快收敛；自建CNN参数更少、计算更省，性能具竞争力。


<details>
  <summary>Details</summary>
Motivation: 实务中常需在“从零设计CNN”与“采用成熟预训练架构”之间权衡。作者希望系统评估两者在同一设置下的性能、效率与复杂度权衡，为架构选择提供实证依据。

Method: 构建一个从零开始训练的自定义CNN；同时对VGG-16、ResNet-50、MobileNet进行迁移学习。在相同实验条件下训练与评估，度量指标包括Accuracy、Precision、Recall、F1，并比较收敛速度、参数规模与计算复杂度。

Result: 预训练架构整体优于自建CNN，尤其在训练数据有限时，准确率更高、收敛更快。自建CNN以显著更少的参数与更低的计算开销取得接近的、具有竞争力的结果。

Conclusion: 存在性能—复杂度—效率的权衡：数据少或追求快速高精度时，优先选预训练模型；在资源受限或需轻量部署时，自建精简CNN可达可用性能。该比较为图像分类任务的架构选择提供实践指引。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

</details>


### [11] [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](https://arxiv.org/abs/2601.04404)
*Jusheng Zhang,Yijia Fan,Zimo Wen,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: Tri-MARF是一个三模态、多智能体协同框架，用于大规模3D目标标注。它融合多视角2D图像、文本与3D点云，通过生成-聚合-门控三个代理协作，提高语义与几何对齐与标注吞吐。在多个数据集上显著超越现有方法，并在单卡A100上达到每小时约1.2万对象的处理速度。


<details>
  <summary>Details</summary>
Motivation: 传统单模型方法难以同时处理3D标注中的空间复杂性、遮挡和视角不一致问题。需要结合多模态信息并以协同方式整合，既保证语义表达，又对齐3D几何，同时兼顾规模化效率。

Method: 提出Tri-MARF多智能体框架：1) 视觉-语言代理：从多视角2D图像生成描述；2) 信息聚合代理：从多候选描述中选择最优；3) 门控代理：将文本语义与3D点云几何对齐，进行精炼描述。整体整合2D/文本/3D三模态以提升标注质量与一致性。

Result: 在Objaverse-LVIS、Objaverse-XL和ABO等数据集上取得SOTA：CLIPScore 88.7；ViLT-R@5检索准确率45.2%与43.8%；吞吐率单卡A100达每小时约12,000对象。

Conclusion: 三模态输入与多代理协同机制显著提升3D对象标注的语义准确性、几何对齐与处理效率，为大规模3D标注提供可扩展、效果优于现有方法的方案。

Abstract: Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU

</details>


### [12] [From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery](https://arxiv.org/abs/2601.04405)
*Yike Zhang,Eduardo Davalos,Dingjie Su,Ange Lou,Jack Noble*

Main category: cs.CV

TL;DR: 作者提出一种结合自监督与弱监督的框架，直接从术前CT预测耳蜗植入手术中的乳突切除区域，在无人工标注的条件下达到Dice 0.72，优于现有方法，并为从术前CT重建术后三维表面奠定基础。


<details>
  <summary>Details</summary>
Motivation: 乳突切除形状对耳蜗植入手术入路与安全性至关重要，但获取精确标注极其困难，限制了深度学习研究与临床应用；需要能在缺乏人工标注的条件下，从术前影像准确预测切除区域的方法，以改进术前规划并降低风险。

Method: 提出混合自监督与弱监督学习框架：在无标注CT上进行自监督表征学习以获取解剖先验，再用弱监督信号（无需人工像素级标签）进行训练；引入基于三维t分布的损失以提高对复杂、边界不明确区域的鲁棒性，并直接从完整术前CT预测乳突切除体积与形状。

Result: 在预测复杂且无明显边界的乳突切除形状任务上取得平均Dice 0.72，超过现有最先进方法；能有效从术前CT生成与术后相符的切除区域预测。

Conclusion: 首次将自监督与弱监督结合用于乳突切除形状预测，提供稳健高效的无标注解决方案，支持CI术前规划，并为从术前CT直接构建术后三维表面提供基础。

Abstract: Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.

</details>


### [13] [CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction](https://arxiv.org/abs/2601.04428)
*Donghang Lyu,Marius Staring,Hildo Lamb,Mariya Doneva*

Main category: cs.CV

TL;DR: 提出CRUNet-MR-Univ这一用于心脏MRI重建的统一基础模型，结合时空相关与提示先验，在多样化扫描设置下优于基线并具更强泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在CMR重建上虽优于传统方法且能处理高加速率，但在不同对比度、采样、厂商、解剖与疾病类型的分布变动下易退化；现有模型多只适配单一或狭窄场景，临床可用性受限，亟需统一且可泛化的模型。

Method: 构建CRUNet-MR-Univ基础模型，显式利用时空相关性，并引入基于prompt的先验以适应不同CMR扫描多样性；统一框架应对全谱系的对比度、采样模式与设备差异。

Result: 在广泛设置下，该方法相较多种基线均取得一致更优表现，显示其有效性。

Conclusion: CRUNet-MR-Univ能在多样CMR场景中保持强泛化与重建质量，具备面向真实临床应用的潜力。

Abstract: In recent years, deep learning has attracted increasing attention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clinical applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or narrow subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that leverages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently outperforms baseline methods across a wide range of settings, highlighting its effectiveness and promise.

</details>


### [14] [Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization](https://arxiv.org/abs/2601.04442)
*Xingjian Diao,Zheyuan Liu,Chunhui Zhang,Weiyi Wu,Keyi Kong,Lin Shi,Kaize Ding,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: 提出GPRO元推理控制器，在生成过程中动态在快路径、感知重审路径与推理自省路径之间切换，以减少过度思考并修复视觉感知失败；基于约79万条失败归因数据与多目标强化学习训练，在五个基准上同时提升准确率与效率，响应更短。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM依赖链式思维带来强推理，但常因过度思考导致冗长、低效，且关键瓶颈在于视觉感知失败被忽略；稳定推理需要可靠的低层视觉落地，许多错误源自感知而非推理不足。

Method: 提出GPRO元控制器：逐步生成时在三条路径间门控路由——轻量快路径、慢感知路径（重审图像）、慢推理路径（内部自反思）。构建约79万样本的失败归因监督，利用教师模型区分感知幻觉与推理错误；再以多目标强化学习，在准确率与算力成本间权衡，学习不确定性下的最优路由策略。

Result: 在五个基准上，相比近期慢思方法，GPRO取得更高准确率与更短回应，整体推理效率显著提升。

Conclusion: 通过把感知与推理解耦并进行门控路由，GPRO在不牺牲甚至提升准确率的同时减少冗长推理，实现更高效、更稳健的LVLM推理。

Abstract: Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

</details>


### [15] [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
*Zhexiao Xiong,Xin Ye,Burhan Yaman,Sheng Cheng,Yiren Lu,Jingru Luo,Nathan Jacobs,Liu Ren*

Main category: cs.CV

TL;DR: UniDrive-WM提出一个统一的基于视觉-语言模型的世界模型，把场景理解、轨迹规划与受轨迹条件的未来图像生成整合于单一架构，并在Bench2Drive上显著提升规划与安全指标。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶多将感知、预测、规划割裂处理，且VLM用于规划的工作缺少与生成式世界建模的紧密耦合，难以相互提供监督与迭代改进。作者希望通过统一模型让未来预测为规划与理解提供额外监督，从而提升驾驶性能与安全性。

Method: 构建UniDrive-WM：一个VLM驱动的统一模型。其规划器先预测未来轨迹，然后以该轨迹作为条件驱动图像生成器，生成未来帧；生成结果反过来作为辅助监督信号增强场景理解并迭代优化轨迹。系统同时研究未来图像预测的离散与连续表示，并分析其对下游驾驶的影响。

Result: 在Bench2Drive上，方法生成高保真未来图像，并相较此前最佳方法，L2轨迹误差降低5.9%，碰撞率降低9.2%。

Conclusion: 将VLM的推理、规划与生成式世界建模紧密耦合，能提升未来预测质量并显著改进自动驾驶的规划与安全表现；统一架构优于模块化管线。

Abstract: World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .

</details>


### [16] [Vision-Language Agents for Interactive Forest Change Analysis](https://arxiv.org/abs/2601.04497)
*James Brock,Ce Zhang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 提出一个由LLM驱动、结合MCI视觉-语言骨干的森林变化一体化分析系统，并发布Forest-Change数据集；在两个数据集上实现较高的变化分割与语义描述性能，展示交互式RSICI的潜力。


<details>
  <summary>Details</summary>
Motivation: 遥感森林监测面临两大难题：像素级精确的变化检测与对复杂森林动态的可解释语义描述。尽管LLM用于交互分析的应用增多，将LLM与VLM结合用于遥感变化解读（RSICI）仍研究不足。

Method: 以多级变化解读（MCI）视觉-语言模型为骨干，引入LLM作为编排/代理，支持多RSICI任务的自然语言查询与协同；同时构建Forest-Change数据集，包含双时相高分影像、像素级变化掩膜与多粒度语义变化描述（人标+规则生成）。

Result: 在Forest-Change上mIoU 67.10%、BLEU-4 40.17；在LEVIR-MCI-Trees上mIoU 88.13%、BLEU-4 34.41，表明在变化检测与语义生成两方面均取得强性能。

Conclusion: LLM驱动的RSICI系统可提升森林变化分析的可及性、可解释性与效率；所提出的数据与代码开源，为该方向研究与应用提供基础。

Abstract: Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.

</details>


### [17] [TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression](https://arxiv.org/abs/2601.04519)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: 提出TokenSeg：基于稀疏token与边界感知的高效3D医学分割框架，在保持SOTA精度（Dice 94.49%、IoU 89.61%）的同时，将显存与时延分别降低64%与68%。


<details>
  <summary>Details</summary>
Motivation: 3D体数据分割计算量随体素立方增长，且在组织同质区域存在大量冗余计算；需要一种既捕获全局解剖语义又兼顾精细边界、同时大幅降低计算/显存成本的方法。

Method: 1) 多尺度层级编码器：在四个分辨率层级提取约400个候选token，兼顾全局上下文与细粒度边界特征；2) 边界感知tokenizer：VQ-VAE量化+重要性评分，筛选约100个显著token（>60%位于肿瘤边界附近）；3) 稀疏到致密解码器：通过token回投、渐进上采样与跳跃连接重建全分辨率分割掩膜。

Result: 在包含960例的3D乳腺DCE-MRI上，取得Dice 94.49%、IoU 89.61%，显存占用降64%，推理时延降68%；在MSD心脏与脑MRI基准上同样达到最优或近最优，体现良好的跨解剖泛化。

Conclusion: 解剖先验与边界感知驱动的稀疏token表示可在显著降低计算资源的同时，实现精确、稳健的3D医学影像分割，具备通用性与效率优势。

Abstract: Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.

</details>


### [18] [FaceRefiner: High-Fidelity Facial Texture Refinement with Differentiable Rendering-based Style Transfer](https://arxiv.org/abs/2601.04520)
*Chengyang Li,Baoping Cheng,Yao Cheng,Haocheng Zhang,Renshuai Liu,Yinglin Zheng,Jing Liao,Xuan Cheng*

Main category: cs.CV

TL;DR: 提出FaceRefiner：将3D可见区域采样得到的真实人脸纹理作为“风格”，将现有单图纹理生成的UV作为“内容”，通过结合可微渲染的多层次风格迁移来细化人脸全纹理，显著提升细节、结构与身份保真；在Multi-PIE、CelebA、FFHQ上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度网络的单图人脸纹理生成常在训练数据或2D生成器的隐空间内生成UV纹理，导致泛化受限；对野外图像时面部细节、结构与身份与输入不一致。需要一种能从输入图像中保留语义与细节并提升真实感的纹理细化方法。

Method: 提出FaceRefiner：1) 将从输入图像通过3D重建/可见面采样得到的纹理视为风格图（photo-realistic style），将任意现有纹理生成器输出的UV视为内容图；2) 设计融合可微渲染的风格迁移，除高/中层特征外，通过在可见区域施加像素级（低层）一致性约束，将可见面的真实细节与颜色分布迁移到内容UV；3) 多级信息传递确保保留输入的语义结构与身份特征；可作为通用后处理细化器。

Result: 在Multi-PIE、CelebA、FFHQ上实验显示：与SOTA相比，FaceRefiner提升纹理质量与身份保持（定量与可视化）；在野外场景下泛化更好，生成的全UV更贴合输入图像细节与结构。

Conclusion: 结合可微渲染的多层次风格迁移能有效弥补既有单图纹理生成的域偏与细节缺失问题；FaceRefiner作为可插拔细化模块可提升真实感与身份一致性，并在多数据集上优于现有方法。

Abstract: Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.

</details>


### [19] [All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction](https://arxiv.org/abs/2601.04567)
*Ziyou Jiang,Mingyang Li,Junjie Wang,Yuekai Huang,Jie Huang,Zhiyuan Chang,Zhaoyang Li,Qing Wang*

Main category: cs.CV

TL;DR: 提出RepMD，通过“设计概念再现”检测随时间与类型漂移的有害迷因；构建并复原“设计概念图”(DCG)，用以指导多模态大模型判别，达81.1%准确率，对跨类型与时间迁移鲁降较小，并提高人工发现效率。


<details>
  <summary>Details</summary>
Motivation: 网络有害迷因不断演化、跨类型迁移，导致基于表面特征或静态分类器的检测失效。作者观察到不同迷因背后存在不变的“攻击/设计”原则，若能抽象并复用这些原则，将有助于解释与检测新型有害迷因。

Method: 1) 借鉴攻击树提出“设计概念图”(DCG)，刻画生成有害迷因的分步设计过程；2) 从历史迷因中通过“设计步骤复原+图剪枝”自动导出DCG；3) 使用DCG引导多模态大语言模型(MLLM)进行检测与判别。

Result: 在基准评测中，RepMD取得81.1%最高准确率；在类型漂移与时间演化场景下准确率仅有小幅下降。人工评测显示，借助RepMD进行人工发现平均每张迷因需约15–30秒。

Conclusion: 抽象并复用跨迷因不变的设计概念能够有效提升对演化型有害迷因的检测与解释能力。DCG结合MLLM在准确性与泛化上具备优势，并能提升人工审核效率。

Abstract: Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.

</details>


### [20] [3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks](https://arxiv.org/abs/2601.04588)
*Yusri Al-Sanaani,Rebecca Thornhill,Sreeraman Rajan*

Main category: cs.CV

TL;DR: 论文提出用条件生成模型合成3D LGE MRI来扩充数据，以提升左心房壁/腔分割。SPADE-LDM生成的图像最逼真（FID=4.063），用于数据增强时，3D U-Net对LA腔的Dice由0.908升至0.936（p<0.05）。


<details>
  <summary>Details</summary>
Motivation: LGE MRI中LA壁与内膜分割对量化房颤纤维化至关重要，但训练数据稀缺且心房解剖复杂，限制了监督分割模型的性能。需要一种能在结构一致性下扩充训练分布的方案。

Method: 构建基于语义标签（专家解剖标注+无监督组织聚类的复合标签图）的条件3D图像合成流程，比较三种3D条件生成器：Pix2Pix GAN、SPADE-GAN、SPADE-LDM（基于SPADE的潜空间扩散）。评估合成图像逼真度（FID）及其对下游3D U-Net左心房分割的增益。

Result: SPADE-LDM在结构与纹理上最逼真：FID=4.063，优于SPADE-GAN（7.652）与Pix2Pix（40.821）。将其合成数据用于增强训练后，LA腔Dice从0.908提升到0.936，提升具有统计学显著性（p<0.05）。

Conclusion: 标签条件的3D合成可有效缓解LGE数据稀缺，显著提升左心房分割表现。扩散式生成（SPADE-LDM）优于GAN基线，显示在欠代表心脏结构的建模中具备实际价值。

Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.

</details>


### [21] [MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing](https://arxiv.org/abs/2601.04589)
*Zihao Lin,Wanrong Zhu,Jiuxiang Gu,Jihyung Kil,Christopher Tensmeyer,Lin Zhang,Shilong Liu,Ruiyi Zhang,Lifu Huang,Vlad I. Morariu,Tong Sun*

Main category: cs.CV

TL;DR: 提出MiLDEAgent与MiLDEBench/MiLDEEval，解决多层设计文档（装饰、文本、图像）基于自然语言的层感知编辑难题；在20K+样本基准上显著优于开源方法，接近闭源表现。


<details>
  <summary>Details</summary>
Motivation: 现实设计文档是多层构成，编辑需识别相关层并协调修改；现有工作多为单层图像编辑或多层生成，不具备“改什么、在哪里改”的细粒度推理与层级理解，缺乏相应评测体系。

Method: 提出MiLDEAgent：将经强化学习训练的多模态推理器用于逐层理解与定位，再调用图像编辑器执行目标化修改；同时构建MiLDEBench（20K+人机协作的设计文档+多样指令）与MiLDEEval（从指令遵循、版式一致性、美学、文本渲染四维评估）。

Result: 在14个开源与2个闭源模型上实验：开源方法常无法完成多层编辑，闭源方法有格式违规；MiLDEAgent展现更强层感知推理与精准编辑，显著超越开源基线，并与闭源模型性能相当。

Conclusion: 多层文档编辑需要显式层级推理与任务特化评测。MiLDEAgent提供首个强基线并验证该范式有效，MiLDEBench/Eval为该领域建立系统化基准与评价协议。

Abstract: Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.

</details>


### [22] [Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems](https://arxiv.org/abs/2601.04605)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.CV

TL;DR: 论文探讨AI驱动的人本网络物理系统在运行中因人与系统交互导致的“不确定工况”，提出评估与保障安全/安全性的框架，并以1型糖尿病闭环控糖中“未报餐”检测为案例展示个性化图像方法。


<details>
  <summary>Details</summary>
Motivation: AI已广泛嵌入医疗监护、自动驾驶等人本CPS中，但实际运行会遇到偏离预设协议的异常情境（尤其源自人机交互），可能触发安全与安全性风险，缺乏系统化的识别与缓解评估方法。

Method: 1) 系统性梳理运行偏差类型，定义“未知/不确定工况”。2) 构建一个用于比较不同安全与安全性策略的评估框架（覆盖检测、缓解、恢复等环节）。3) 以闭环血糖控制为示例，提出个性化、基于图像的未报餐检测新技术，用于识别餐食未告知导致的控制失配。

Result: 给出可操作的评估框架，并展示在1型糖尿病闭环场景下，个性化图像方法能够检测未报餐事件（摘要未给出量化指标）。

Conclusion: AI驱动的人本CPS需要面向“未知工况”的运行保障。所提框架可指导策略选择与部署；案例表明个性化图像检测在闭环控糖中具有可行性，用于提升安全与安全性。

Abstract: In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements. 
  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

</details>


### [23] [HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation](https://arxiv.org/abs/2601.04607)
*Xiaoyu Liu,Siwen Wei,Linhao Qu,Mingyuan Pan,Chengsheng Zhang,Yonghong Shi,Zhijian Song*

Main category: cs.CV

TL;DR: 提出HUR-MACL：以高不确定区域为导向，协同Vision Mamba与可变形CNN分工强化头颈部小而形态复杂器官分割，结合异构特征蒸馏损失，在两公共与一私有数据集达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有头颈部放疗OAR分割中，小体积、形态复杂器官常被深度模型漏检或边界不准。混合架构虽有改进，但多为简单特征拼接，未有效利用各子模型互补性，导致功能重叠与精度受限。

Method: 1) 用CNN估计像素级不确定性，自适应定位高不确定区域；2) 针对这些区域，联合引入Vision Mamba（全球/长程依赖建模）与Deformable CNN（形变与局部几何适配）细化分割；3) 设计异构特征蒸馏损失，在高不确定区域对两种架构进行跨模态协同与一致性约束；4) 面向多器官分割的端到端训练。

Result: 在两套公共数据集与一套私有数据集上实现SOTA，显著提升小器官与复杂边界的分割精度（定量指标未给出）。

Conclusion: 围绕不确定性驱动的区域化协同，发挥Mamba的全局建模与Deformable CNN的局部对齐优势，并以异构蒸馏促进互补学习，可有效提升头颈部多器官分割性能，尤其是困难器官。

Abstract: Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.

</details>


### [24] [HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment](https://arxiv.org/abs/2601.04614)
*Wenzhi Chen,Bo Hu,Leida Li,Lihuo He,Wen Lu,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出HyperAlign：基于双曲蕴含几何的自适应文本-图像对齐评估框架，将CLIP欧氏特征映射到双曲空间，通过动态监督的蕴含建模与自适应调制回归器，校准余弦相似度，显著提升单库与跨库评测性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要在欧氏空间用距离/相似度衡量图文一致性，忽视语义蕴含的层级与偏序结构，且对不同样本缺乏自适应能力，导致对齐评估不精准、泛化性不足。

Method: 1) 用CLIP提取欧氏特征并进行双曲映射；2) 设计动态监督的“蕴含建模”机制：将离散的蕴含/矛盾等逻辑关系转化为在双曲空间中的连续几何监督，学习层级化语义结构；3) 提出自适应调制回归器：基于双曲几何特征为每个样本生成调制参数，对欧氏余弦相似度进行自适应校准，输出最终对齐分数。

Result: 在单数据集评测和跨数据集泛化任务上取得高度竞争的表现，优于或匹敌现有方法，表明双曲几何与蕴含建模能有效提升图文对齐评估的准确性与鲁棒性。

Conclusion: 利用双曲蕴含几何与样本级自适应调制可更有效地建模文本-图像语义对齐，克服欧氏度量的局限，并在泛化场景中具备优势。

Abstract: With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

</details>


### [25] [Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2601.04672)
*Wentao Zhang,Lifei Wang,Lina Lu,MingKun Xu,Shangyang Li,Yanchao Yang,Tao Fang*

Main category: cs.CV

TL;DR: 提出Agri-R1：用自动合成推理数据与GRPO训练的农业多模态推理模型，在少量样本下显著提升病害识别、知识问答与跨域泛化，3B规模可媲美7B–13B基线。


<details>
  <summary>Details</summary>
Motivation: 现有农业VLM微调依赖大量标注、可解释性差、泛化弱；已有“推理增强”方法又依赖昂贵专家标注，且难以覆盖农业开放式、多样化问答需求。

Method: 1) 自动生成高质量推理数据：通过视觉-语言合成与LLM过滤，仅用19%原始样本获得结构化推理轨迹；2) 用GRPO训练，设计新奖励：结合领域词表与模糊匹配，同时评估答案正确性与开放式语言表达的灵活性；3) 进行系统评测与消融。

Result: 在CDDMBench上，3B模型达到与7B–13B基线相当或更优：病害识别准确率相对提升+23.2%，农业知识QA提升+33.3%，相较标准微调的跨域泛化提升+26.10点。

Conclusion: 结构化推理数据与GRPO驱动的探索形成协同效应，显著增强鲁棒性与泛化，且收益随问题复杂度增加而放大；证明小参数模型在农业领域可通过推理增强与定制奖励达到大模型水平。

Abstract: Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.

</details>


### [26] [DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation](https://arxiv.org/abs/2601.04676)
*Qiu Guan,Zhiqiang Yang,Dezhang Ye,Yang Chen,Xinli Xu,Ying Tang*

Main category: cs.CV

TL;DR: 提出DB-MSMUNet用于胰腺及病灶CT分割，通过多尺度状态空间与可变形卷积建模全局-局部信息，并采用边界与区域双解码器加多尺度监督，在三套数据上取得SOTA级Dice（89.47/87.59/89.02%）。


<details>
  <summary>Details</summary>
Motivation: 胰腺解剖边界模糊、与周围组织对比度低、形状不规则且病灶小，现有方法难以兼顾全局上下文、局部形变与精细边界，导致分割不稳定与病灶漏检。

Method: 构建双分支U形编码-解码网络。编码器为多尺度Mamba模块（MSMM），融合可变形卷积与多尺度状态空间模型，增强全局建模与局部形变适配。解码端双分支：边界解码器含边缘增强路径EEP以显式捕获边界；区域解码器采用多层解码器MLD利用多尺度深语义特征重建细节与小病灶。两分支多尺度辅监督（ADS）提供更精确梯度与判别特征。

Result: 在NIH、MSD及临床胰腺肿瘤数据集上分别达Dice 89.47%、87.59%、89.02%，在分割精度、边缘保持与跨数据集鲁棒性方面优于大多数SOTA方法。

Conclusion: DB-MSMUNet通过MSMM与双解码器+多尺度辅监督有效缓解胰腺分割中的低对比与边界模糊问题，兼顾全局与局部细节，在多数据集上具备良好泛化与实用价值。

Abstract: Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.

</details>


### [27] [HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution](https://arxiv.org/abs/2601.04682)
*Yang Zou,Xingyue Zhu,Kaiqi Han,Jun Ma,Xingyuan Li,Zhiying Jiang,Jinyuan Liu*

Main category: cs.CV

TL;DR: 提出HATIR：在扩散采样路径中注入“热感知”形变先验，联合建模红外视频湍流退化与分辨率损失，实现红外视频超分辨；并构建首个湍流红外VSR数据集FLIR‑IVSR。


<details>
  <summary>Details</summary>
Motivation: 红外视频在恶劣环境下有重要价值，但受大气湍流与压缩退化影响严重；现有VSR要么忽视红外与可见光模态差异，要么无法处理湍流形变。将湍流消除与VSR串联会带来误差传播，缺乏联合建模方法。

Method: 基于扩散模型的HATIR：1) 在逆扩散过程中引入“热感知”形变先验；2) 相位子引导流估计器（Phasor-Guided Flow Estimator），利用“热活跃区域在时间上具有一致相位子响应”的物理规律，估计可靠的湍流感知光流以指导逆扩散；3) 湍流感知解码器（Turbulence-Aware Decoder），通过湍流门控与结构感知注意力，抑制不稳定时序线索并增强边缘特征聚合；4) 构建FLIR‑IVSR数据集（FLIR T1050sc，1024×768，640个多样场景，含不同相机/目标运动），提供成对LR‑HR序列。

Result: 在红外视频超分辨与湍流恢复任务上取得更好的结构还原与细节保真（相较级联TM+VSR与现有VSR方法），在新建的FLIR‑IVSR上实现SOTA表现。

Conclusion: 联合建模湍流退化与超分辨的热感知扩散框架有效，物理启发的相位子流与湍流感知解码器提升了结构与稳定性；数据集为后续红外VSR研究提供基准与推动。

Abstract: Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR

</details>


### [28] [WebCryptoAgent: Agentic Crypto Trading with Web Informatics](https://arxiv.org/abs/2601.04687)
*Ali Kurban,Wei Luo,Liangyu Zuo,Zeyu Zhang,Renda Han,Zhaolu Kang,Hao Tang*

Main category: cs.CV

TL;DR: 提出WebCryptoAgent：多模态网页与市场信号融合的加密货币交易智能体，采用证据汇总与置信校准推理，并用解耦的实时风险控制架构实现亚秒级冲击防御，实证显示稳定性与尾部风险管理优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前加密交易需在高波动、短周期内整合异构信息（网页内容、社交情绪、OHLCV微结构）。现有系统难以同时稳健融合噪声多源证据并在亚秒级价格冲击下快速响应；慢速、臃肿的推理流水线既放大伪相关，也无法进行及时风险防御。

Method: 1) 代理式分解：为不同模态（网页、社媒、结构化行情）设计专属子代理，分别抽取与评估证据；2) 统一证据文档：将多代理输出整合为可解释的证据汇编，进行置信度校准推理，生成交易决策；3) 解耦控制：将“小时级战略推理”与“秒级实时风险模型”分离，后者独立监测冲击并快速触发保护（如减仓/停牌），不阻塞交易循环。

Result: 在真实加密市场数据上的大量实验表明：相较基线，WebCryptoAgent交易更稳定、减少伪信号驱动的交易活动、并显著改善尾部风险处理能力（抗冲击与回撤控制更佳）。

Conclusion: 通过多模态代理+证据整合的可解释推理，以及与交易环路解耦的实时风险模块，系统在稳健性与极端行情防御上优于现有方法；该框架适合高波动、信息异构且需要短时决策的场景。

Abstract: Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.

</details>


### [29] [Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models](https://arxiv.org/abs/2601.04706)
*Yanbing Zeng,Jia Wang,Hanghang Ma,Junqiang Wu,Jie Zhu,Xiaoming Wei,Jie Hu*

Main category: cs.CV

TL;DR: 提出Forge-and-Quench统一框架，通过理解模型提升生成图像的真实感与细节：先由MLLM生成增强指令，再经Bridge Adapter得到“Bridge Feature”，共同指导T2I生成，跨模型迁移高效、训练开销低，显著提升保真与细节，同时保持跟指令与知识应用。


<details>
  <summary>Details</summary>
Motivation: 现有“理解—生成”一体化研究多着眼于把推理与世界知识注入生成，但较少关注让理解直接提升图像保真度与细节。作者希望把多模态理解的表征优势转化为对图像生成过程的直接、可控指导。

Method: 提出Forge-and-Quench两阶段流程：1) Forge（锻造）：MLLM对整段对话与文本指令进行推理，输出增强版文本指令；2) Quench（淬火）：通过新设计的Bridge Adapter将增强指令映射为虚拟视觉表征“Bridge Feature”，并将其与增强指令一起作为引导信号注入T2I主干网络，细化与约束生成。作者系统研究了Bridge Feature形态与Adapter结构，确保与不同MLLM/T2I兼容。

Result: 在多种MLLM与T2I组合上验证，显著提升生成图像的保真度与细节，同时保持良好的指令遵循能力并增强世界知识应用；框架具有高可扩展性与灵活迁移性，训练开销显著降低且不削弱原MLLM的理解能力。

Conclusion: 理解与生成可在统一框架中相互促进：以MLLM增强指令并经Bridge Feature将理解转化为对生成的视觉引导，可在不牺牲理解能力的前提下显著提升图像质量与细节，且具备跨模型、低开销的实用价值。

Abstract: Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.

</details>


### [30] [On the Holistic Approach for Detecting Human Image Forgery](https://arxiv.org/abs/2601.04715)
*Xiao Guo,Jie Zhu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出HuForDet，一个面向人像图像篡改的统一检测框架，双分支结合脸部伪造特征（RGB+频域+自适应LoG）与全身语义一致性（MLLM+置信加权），并构建涵盖人脸与全身的HuFor数据集，实验达SOTA与更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法割裂：要么只检测人脸要么只检测全身合成，难以在不同人像伪造类型间泛化。AIGC推动的深伪从脸到全身快速演进，统一、鲁棒的全谱系人像伪造检测迫切需要。

Method: 提出HuForDet双分支架构：1）人脸伪造分支：异构专家在RGB与频域联合学习，并引入自适应LoG模块，感知从细粒度融合边界到粗尺度纹理异常。2）上下文伪造分支：利用多模态大模型（MLLM）对整身图像进行语义一致性分析，并设计置信度估计，在特征融合时动态加权其贡献。另构建HuFor数据集，统一既有人脸伪造数据并新增全身合成数据。

Result: 在多种人像伪造场景和数据集上达到SOTA，且在跨域、跨类型的伪造检测中展现更强鲁棒性。

Conclusion: 统一的人像伪造检测可通过脸部微观伪迹与全身语义一致性的结合取得更优性能；HuForDet与HuFor数据集为覆盖从人脸到全身的检测提供了有效基线与资源。

Abstract: The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.

</details>


### [31] [Training a Custom CNN on Five Heterogeneous Image Datasets](https://arxiv.org/abs/2601.04727)
*Anika Tabassum,Tasnuva Mahazabin Tuba,Nafisa Naznin*

Main category: cs.CV

TL;DR: 评估自定义轻量CNN与ResNet-18、VGG-16在五个异构视觉数据集上的表现，比较从零训练与迁移学习对收敛与泛化的影响，给出在数据受限场景下的实用建议。


<details>
  <summary>Details</summary>
Motivation: 真实世界农业与城市场景存在光照、分辨率、环境复杂度和类别不均衡等差异，传统手工特征难以适配；需评估在资源受限条件下，何种CNN策略更稳健高效。

Method: 构建一个任务定制的轻量CNN，并与ResNet-18、VGG-16对比；分别进行从零训练与迁移学习；统一的预处理、数据增强与受控实验，考察架构复杂度、深度与预训练对不同规模与难度数据集的收敛、泛化和性能影响。

Result: 定制轻量CNN在多任务上取得有竞争力的表现；在数据受限或难度较高的数据集上，迁移学习和更深层架构（如ResNet-18、VGG-16）带来显著优势；不同任务对模型复杂度的敏感度不同。

Conclusion: 在资源有限的实际部署中，优先考虑轻量定制CNN以兼顾效率与精度；当数据量不足或任务复杂时，采用预训练的深层架构可显著提升性能。研究为多域视觉分类任务的模型选择与部署提供了实用指南。

Abstract: Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.

</details>


### [32] [AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection](https://arxiv.org/abs/2601.04734)
*Yunqing Hu,Zheming Yang,Chang Zhao,Qi Guo,Meng Gao,Pengcheng Li,Wen Ji*

Main category: cs.CV

TL;DR: AIVD框架：边缘轻量检测+云端MLLM协同，精确定位与高质量语义生成统一；通过视觉-语义协同增强微调提升鲁棒性；提出异构资源感知动态调度，兼顾吞吐与时延；实验显示在降低资源消耗的同时提升分类与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽擅长语义与推理，但在精确目标定位不足，且在边云协同中受资源限制、网络波动与边缘裁剪噪声影响，导致鲁棒性与时延/吞吐受损。

Method: 提出AIVD：1) 边缘侧部署轻量化检测器进行目标框裁剪与初筛；2) 云端MLLM负责精细语义理解与生成；3) 设计视觉-语义协同数据增强与高效微调策略，提升对裁剪框噪声与场景变化的鲁棒性；4) 提出异构资源感知的动态调度算法，依据设备与网络状态在边云间自适应分配任务，优化吞吐与时延。

Result: 在多场景实验中，相较基线显著降低计算与带宽资源消耗，同时提升MLLM分类准确率与语义生成一致性/质量；所提调度策略在异构与动态条件下实现更高吞吐与更低时延。

Conclusion: AIVD可在边云协同中统一实现精确定位与高质量语义生成，具备对噪声与场景变化的鲁棒性，并通过资源感知调度在多样环境下达到更优的性能与效率。

Abstract: Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.

</details>


### [33] [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752)
*Masatomo Yoshida,Haruto Namura,Nicola Adami,Masahiro Okuda*

Main category: cs.CV

TL;DR: 提出一种基于骨架化(skeletonization)的对抗攻击，聚焦含文本（尤其数学公式）的图像，以更高效扰动并揭示大模型视觉理解与推理的脆弱性；并在ChatGPT上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型在含文本/公式图像上的鲁棒性与推理可靠性不足，但常规对抗搜索空间大、效率低、难以定位关键视觉要素。作者希望通过压缩搜索空间、突出结构性信息来更有针对性地评测与攻破模型视觉能力。

Method: 将图像进行骨架化以提取关键笔画/结构，基于此进行对抗扰动，专门针对含文本与公式（需LaTeX转换、结构复杂）的图像；同时从字符级与语义级评估原始与对抗输出差异，分析模型视觉解释与推理过程中的薄弱环节。

Result: 骨架化驱动的对抗方法在含文本/公式图像上能高效诱发模型输出的字符与语义偏差；在ChatGPT的实测中显示明显的攻击成功率与可操作性，表明该方法在真实应用场景具有威胁。

Conclusion: 通过结构化（骨架化）约束有效缩小对抗搜索空间，可系统揭示并放大基础模型在文本/公式视觉理解与推理方面的脆弱性；该方法对实际系统（如ChatGPT）有现实影响，提示需要更强的鲁棒性防护与面向结构信息的训练/校验策略。

Abstract: This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.

</details>


### [34] [ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting](https://arxiv.org/abs/2601.04754)
*Yen-Jen Chiou,Wei-Tse Cheng,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: ProFuse：一种结合3D高斯溅射的高效、上下文感知开放词汇3D场景理解框架，通过预配准与跨视角上下文提案，实现无需渲染监督微调、低开销、约5分钟/场景的快速语义绑定，同时保持几何重建质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的开放词汇3D场景理解往往需要渲染监督微调、额外优化或依赖预训练的3DGS模型，效率低、跨视角语义不一致、掩膜内聚性差。需要一种在不牺牲几何重建与效率的前提下，提升跨视一致性和语义融合质量的方法。

Method: 1) 引入稠密对应引导的预配准：在无预训练3DGS的情况下，用跨视角稠密匹配初始化高斯，获得更准确几何。2) 跨视聚类生成3D Context Proposals：将多视角的语义/特征聚成提案，并为每个提案聚合得到全局特征。3) 直接配准阶段语言特征融合：把提案全局特征融合到对应高斯上，保证每个primitive在跨视角的语义一致与掩膜内聚。4) 由于先验关联已建立，语义融合不再需要超出标准重建的额外优化，同时避免密化带来的开销。

Result: 在开放词汇3DGS理解任务上取得强劲性能；语义附着仅需约5分钟/场景，速度较SOTA快2倍，同时保持/提升跨视一致性、掩膜内聚和几何精度，无需渲染监督微调与额外优化。

Conclusion: 通过预配准与上下文提案驱动的特征融合，ProFuse以最小开销实现高效且一致的开放词汇3D语义理解，兼顾速度（≈5分钟/场景、较SOTA快2倍）与几何质量，并简化训练流程（无渲染监督微调、无密化）。

Abstract: We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.

</details>


### [35] [Segmentation-Driven Monocular Shape from Polarization based on Physical Model](https://arxiv.org/abs/2601.04776)
*Jinyu Zhang,Xu Ma,Weili Chen,Gonzalo R. Arce*

Main category: cs.CV

TL;DR: 提出一种分割驱动的单目偏振形状恢复框架，通过自适应区域生长分割为局部凸区域，并结合多尺度凸性先验融合，有效消除方位角二义性，显著提升法向/几何重建精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 单目SfP虽紧凑且鲁棒，但受偏振物理分析固有的方位角二义性影响，易导致法向与形状恢复不稳定、细节丢失与伪影。需要一种既能抑制全局歧义、又能保留表面连续性与细节的重建策略。

Method: 1) 将全局形状恢复重构为在自适应分割的局部凸子区域上的一组局部重建；2) 提出偏振辅助的自适应区域生长(PARG)分割，用偏振线索将全局凸性假设解耦为多个局部凸区域以抑制方位角歧义并保持连续性；3) 设计多尺度融合凸性先验(MFCP)约束，跨尺度融合局部凸性与一致性，增强细节与结构恢复。

Result: 在合成与真实数据集上，相比现有基于物理的单目SfP方法，显著提升方位角消歧准确率与几何保真度，实验广泛且稳定。

Conclusion: 通过“分而治之”的局部凸性建模与多尺度凸性先验融合，可有效缓解偏振方位角二义性，提升单目SfP的稳定性与细节重建，证明了分割驱动的SfP框架的有效性。

Abstract: Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

</details>


### [36] [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/abs/2601.04777)
*Shurong Zheng,Yousong Zhu,Hongyin Zhao,Fan Yang,Yufei Zhan,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出GeM-VG，一种面向广义多图视觉指代的多模态大模型；构建MG-Data-240K数据集并提出结合CoT与直接回答的混合强化微调策略（R1式+规则奖励）；在多图与单图定位和多图理解上均显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有多图视觉指代研究开始起步，但多局限于单目标定位、任务类型有限，缺乏统一建模与数据支持，难以覆盖不同跨图线索与推理强度的场景。

Method: 1) 任务体系：按跨图线索与推理需求系统化划分多图指代任务；2) 数据：构建MG-Data-240K，扩展目标数量与图间关系多样性；3) 模型：提出GeM-VG；4) 训练：混合强化微调，将链式思考(CoT)与直接回答结合，采用类R1算法，并用规则化奖励设计引导，提升感知与推理鲁棒性。

Result: 在多图基准上超越SOTA：MIG-Bench +2.0%，MC-Bench +9.7%；单图指代ODINW相对基模提升9.1%；同时保持一般多图理解能力强。

Conclusion: 统一的任务建模+更丰富的数据+混合强化微调，使GeM-VG在广义多图指代上取得全面领先，并兼顾单图定位与多图理解的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.

</details>


### [37] [CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models](https://arxiv.org/abs/2601.04778)
*Tobia Poppi,Burak Uzkent,Amanmeet Garg,Lucas Porto,Garin Kessler,Yezhou Yang,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara,Florian Schiffers*

Main category: cs.CV

TL;DR: 提出可扩展的反事实视频生成框架与偏好优化方法，以减少视频-语言模型在动作与时序推理上的幻觉，并在合成偏好数据上微调后取得显著提升。


<details>
  <summary>Details</summary>
Motivation: VLM 在多模态理解上强，但对动作与时间顺序常产生幻觉，根源在于过度依赖语言先验而非细粒度视觉动态。现有方法（文本过滤、随机视频扰动）无法从根本上解决。

Method: 1) 反事实视频生成：使用多模态大模型提出动作与编辑指导，结合扩散式图像/视频生成，合成仅在动作或时序结构上不同、场景保持一致的“语义硬负例”。据此构建约 2.6 万对偏好样本数据集 CounterVid，聚焦动作识别与时序推理。2) MixDPO：统一的直接偏好优化，将文本与视觉偏好联合用于微调。以此在 Qwen2.5-VL 上进行微调。

Result: 使用 MixDPO 在 CounterVid 上微调 Qwen2.5-VL 后，整体性能稳定提升，特别是在时间顺序任务上表现显著；并能有效迁移到标准视频幻觉基准。

Conclusion: 通过可扩展的反事实视频合成与联合偏好优化，可系统性减轻 VLM 的动作与时序幻觉，并提升对细粒度视觉动态的依赖；代码和模型将开源。

Abstract: Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.

</details>


### [38] [Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices](https://arxiv.org/abs/2601.04779)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 论文提出并验证：在常规成像设备的特定拍摄设置下，离焦点扩散函数可近似为高斯模型，从而使基于离焦的深度估计具备解析、实时且高精度的解法。实验（1–100 m、焦深±10%）表明相对/绝对离焦均可用同一高斯模型描述，MAE<1%。


<details>
  <summary>Details</summary>
Motivation: 单幅或双幅图像的基于离焦深度估计长期受制于病态问题：空间变离焦与固有模糊难区分；缺乏统一模型也阻碍实时应用。若能找到既适用于绝对离焦（单幅）又适用于相对离焦（双幅）的简单解析模型，可把问题从病态转为可解并提升效率与鲁棒性。

Method: 从几何光学出发，结合衍射极限光学下的离焦像差理论，推导常规相机在何种拍摄设定（同视点、不同对焦、光圈等）下，离焦算子可由高斯核近似；分析高斯近似与真实离焦点扩散函数（PSF）的拟合误差；给出相对模糊（双幅）与绝对模糊（单幅）的统一高斯表述，并在1–100 m、焦深变化≤±10%条件下评估误差（MAE）。

Result: 在典型拍摄范围（聚焦距离1–100 m、在焦点处最大深度变化10%）内，高斯模型对离焦PSF的拟合误差很小，最大MAE<1%。该模型同时适用于单幅的绝对离焦与双幅的相对离焦，满足实时应用的计算效率与解析解需求。

Conclusion: 在常规成像设备与合理参数设置下，离焦算子可安全地采用高斯模型，既统一了单/双幅的离焦建模，也将基于离焦的深度估计从病态转为可解问题，且具备实时性与高精度（MAE<1%）。

Abstract: Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\!M\!A\!E)$ of less than $1\%$, underscoring the model's accuracy and reliability.

</details>


### [39] [SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning](https://arxiv.org/abs/2601.04785)
*Xihe Qiu,Yang Dai,Xiaoyu Tan,Sijia Li,Fenghao Sun,Lu Gan,Liang Liu*

Main category: cs.CV

TL;DR: 提出一种改进版 Pix2Pix，用 SEResNet 与 U-Net++ 增强生成器，并用简化 PatchGAN 判别器，在<500张少样本 MRI 任务下仍能保持结构一致性与高画质，具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: MRI 成像信息丰富但采集慢、成本高、分辨率受限；现有 Pix2Pix 应用于医学图像翻译但潜力未被充分挖掘，需要在少样本情境下进一步提升生成质量与结构保真。

Method: 将 SEResNet 融入生成器以通过通道注意力强化关键特征表征；采用 U-Net++ 进行密集跳连与多尺度特征融合；配合简化版 PatchGAN 判别器以稳定训练并改善局部解剖真实性；在多种 MRI 同模态翻译任务与少于500张图像条件下进行实验。

Result: 在少样本(<500)条件下，方法在多种 MRI 同模态翻译任务上取得一致的结构保真与更高图像质量，表现出较强的泛化能力。

Conclusion: 该方法为 Pix2Pix 在医学图像翻译中的有效扩展，可在数据有限的临床场景中提升生成质量与结构一致性，具有应用潜力。

Abstract: Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.

</details>


### [40] [Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers](https://arxiv.org/abs/2601.04791)
*Lee Hyoseok,Sohwi Lim,Eunju Cha,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 提出一种纠正模块MCLC，用于稳定LDM零样本逆问题求解，通过与测量一致的Langevin更新缩小求解器与真实逆扩散动力学的差距，减少伪影并提升一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜空间扩散(LDM)的零样本逆问题求解器常不稳定，出现斑块/气泡式伪影与质量下降。作者将不稳定性归因于求解器与真实逆扩散动力学不匹配，需一种普适且稳健的纠正机制。

Method: 提出Measurement-Consistent Langevin Corrector（MCLC）：在每步或若干步采样后，进行与测量一致的Langevin校正更新，将样本向满足观测约束的分布推进；该校正不依赖“线性流形”假设，直接在潜空间执行，理论上缩小动力学差距。模块可即插即用地配合多种现有LDM逆求解器。

Result: 在多种图像复原任务（如去噪、去模糊、超分辨、去遮挡/去雨等）上，与现有LDM逆求解器结合显著提升稳定性和图像质量，明显减少blob伪影；对不同求解器均兼容有效。提供对伪影成因的实证分析。

Conclusion: 通过将测量一致的Langevin校正融入潜扩散逆求解，MCLC有效弥合动力学差距，带来更稳健的零样本逆问题求解，为更通用可靠的扩散先验求解器迈出关键一步。

Abstract: With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.

</details>


### [41] [PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference](https://arxiv.org/abs/2601.04792)
*Denis Korzhenkov,Adil Karjauv,Animesh Karnewar,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 提出一种将预训练扩散视频模型低成本微调为金字塔式（多分辨率、多阶段）模型的管线，并比较不同步数蒸馏策略，在不降质的前提下显著加速多步去噪推理。


<details>
  <summary>Details</summary>
Motivation: 现有金字塔扩散/视频模型通过低分辨率处理高噪声、高清分辨率处理低噪声，能降推理成本，但开源视频模型多从零训练，画质不及SOTA。需要一种将成熟高质量扩散模型迁移为金字塔结构、兼顾速度与质量的方法。

Method: 以预训练扩散视频模型为基底，采用低成本微调将其分解为多分辨率多阶段（pyramidal）正反扩散流程：低分辨率阶段处理高噪声，逐级向高分辨率 refinement。并系统研究与比较金字塔架构中的步数蒸馏（step distillation）策略，用更少的去噪步数近似多步过程，从而进一步加速。

Result: 在不降低视频生成质量的情况下，将预训练模型成功转换为金字塔模型；多种步数蒸馏策略对推理效率带来显著提升。作者提供了可视化与结果页面（链接所示），显示与现有开源金字塔视频模型相比具备更好的视觉可信度与效率。

Conclusion: 通过对预训练扩散视频模型进行金字塔化微调并配合合适的步数蒸馏，可在保持输出质量的同时显著降低推理成本，为高效视频扩散生成提供可行路径。

Abstract: Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.

</details>


### [42] [Detector-Augmented SAMURAI for Long-Duration Drone Tracking](https://arxiv.org/abs/2601.04798)
*Tamara R. Lenhard,Andreas Weinmann,Hichem Snoussi,Tobias Koch*

Main category: cs.CV

TL;DR: 论文系统评估基础模型SAMURAI在城市监控场景下对无人机的长期跟踪能力，并提出结合检测器的增强版以提升鲁棒性，在长序列与出入场情形下显著优于零样本基线。


<details>
  <summary>Details</summary>
Motivation: 无人机在监控中的威胁上升，需要长期、稳健的跟踪。基于检测的方案帧级精度高但易因漏检导致时间不一致。RGB无人机跟踪研究稀缺且依赖传统运动模型；而通用基础模型（如SAMURAI）在类无关跟踪上表现强，但其在无人机场景的适用性未被验证。

Method: 对SAMURAI在城市监控无人机跟踪任务上进行首个系统性评估；并提出“检测器增强的SAMURAI”，利用外部检测器提示以缓解对初始框与序列长度的敏感性，将检测线索与SAMURAI的跟踪机制融合，形成更稳健的长期跟踪框架。

Result: 增强版在复杂城市环境与长时序（尤其目标离场-再入场）下显著提升：相较SAMURAI零样本，跨数据集/指标均有稳定增益，成功率最高提升+0.393，漏检率（FNR）最大降低-0.475。

Conclusion: 基础模型在无人机长期跟踪上具潜力，但原生SAMURAI对初始化与长序列敏感。引入检测器线索可显著提升鲁棒性与一致性，特别适用于城市监控中长时无人机出入场等挑战场景。

Abstract: Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.

</details>


### [43] [Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents](https://arxiv.org/abs/2601.04800)
*Bapu D. Chendage,Rajivkumar S. Mente*

Main category: cs.CV

TL;DR: 论文提出一种针对古代手稿/铭文图像的增强流程，通过二值化与互补预处理去污渍、提高清晰度，并在石刻、金属板与文档三类数据上验证；分类器精度有所提升，表明可读性得到改善。


<details>
  <summary>Details</summary>
Motivation: 古代铭文图像常受老化、环境影响导致背景噪声重、对比度低、前景与背景相似，造成难以辨读。为提高可读性，需要有效的图像增强与去噪方法。

Method: 构建基于图像二值化的增强框架，并配合互补预处理（如去污渍、对比度增强、前景提取等）以突出文字、抑制背景；随后以KNN与SVM分类准确率作为客观指标评估增强效果，分别在石刻、金属板、历史文档三种脚本类型上测试。

Result: 在增强后，KNN分类准确率：石刻55.7%、金属板62.0%、文档65.6%；SVM分类准确率：石刻53.2%、金属板59.5%、文档67.8%。结果显示增强对不同材质的古代马拉地语铭文图像均有正向作用，尤其在文档类上提升更明显。

Conclusion: 所提二值化+互补预处理的增强方法能提升古代铭文图像的可读性，并通过下游分类精度验证其有效性；对不同载体均适用，但性能仍受材质与退化程度影响，未来可结合更先进的学习式增强与自适应分割以进一步提升效果。

Abstract: Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.

</details>


### [44] [SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2601.04824)
*Oriol Rabasseda,Zenjie Li,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: 提出SOVABench，一个以车辆相关动作为核心、面向监控视频检索的真实世界基准，并给出跨动作与时序方向理解两种评测协议；同时提出一个无需训练、用MLLM描述生成可解释嵌入的框架，在SOVABench与若干空间/计数任务上表现强。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索基准多关注场景相似度，缺乏对监控场景中动作判别与时序方向理解的评测，导致在事件识别与复现行为分析上的需求无法有效衡量。

Method: 1) 构建SOVABench数据集与两种评测协议：inter-pair（跨动作区分）与intra-pair（同对象不同时序方向理解）；2) 设计一个训练免调的框架：利用MLLM的视觉推理与指令跟随能力，生成图像/视频的自然语言描述，并从描述中提取可解释的语义嵌入以用于检索。

Result: SOVABench对当前SOTA视觉与多模态模型构成挑战；所提训练免调的MLLM描述-嵌入框架在SOVABench上取得强性能，并在空间关系与计数类基准（对比式VLM常失败处）也表现优异。

Conclusion: 监控检索需要动作级、方向敏感的评测。SOVABench填补该空白；借助MLLM生成的可解释语义嵌入可在无需训练的条件下显著提升此类任务表现，并具有跨任务泛化潜力。

Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.

</details>


### [45] [Character Detection using YOLO for Writer Identification in multiple Medieval books](https://arxiv.org/abs/2601.04834)
*Alessandra Scotto di Freca,Tiziana D Alessandro,Francesco Fontanella,Filippo Sarria,Claudio De Stefano*

Main category: cs.CV

TL;DR: 该论文将抄写员识别从“模板匹配+CNN”管线升级为单一的YOLOv5检测-分类框架，围绕中世纪手稿中“a”字形的自动定位与归属，显著提高了可检出的字母数量与最终识别准确度，并利用置信度实现可拒识的稳健推断。


<details>
  <summary>Details</summary>
Motivation: 古文字学需要对手稿断代并理解书写演化，关键在于区分不同抄写员。既有数字方法仍不稳定：模板匹配依赖阈值、召回有限；多阶段管线复杂、泛化差。作者欲在相同任务框架中，以更鲁棒的检测器替代传统模板匹配，并改进后续抄写员归属的准确性与可用性（含拒识）。

Method: 延续仅基于单个高度区分性的字形（字母“a”）的思路，用YOLOv5进行目标检测直接在页面上定位所有“a”，并以其置信度作为质量信号；随后进行第二阶段的抄写员分类（文中暗示为基于检测到的实例进行归属），整体以YOLO替代了此前的“模板匹配+CNN分类”方案。

Result: 与此前方法相比，YOLO能检测到更多“a”的实例，提升了第二阶段分类样本量与覆盖度，进而带来更高的抄写员识别准确率；同时，检测置信度可用作阈值以拒绝不可靠样本，在未见手稿上也能保持可靠识别。

Conclusion: YOLOv5在中世纪手稿中目标字形的提取上优于模板匹配，带来端到端识别性能与稳健性的提升；置信度驱动的拒识机制使该方法具备更强的实际应用潜力，支持在新手稿上的可靠抄写员识别。

Abstract: Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter "a", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character "a" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.

</details>


### [46] [DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation](https://arxiv.org/abs/2601.04860)
*Ayush Pande*

Main category: cs.CV

TL;DR: DivAS 提出一个无需优化、交互式的 NeRF 分割框架：用 SAM 点提示生成多视角2D掩码，结合 NeRF 深度先验精炼，然后通过自定义CUDA核在约200ms内把多视图掩码聚合到统一3D体素网格，实现实时反馈，速度与精度优于或匹敌需训练的方法。


<details>
  <summary>Details</summary>
Motivation: 现有 NeRF 分割多依赖逐场景优化/训练，导致慢、打破2D大模型的零样本优势，且交互体验差。需要一个既能利用2D基础模型、又不需 per-scene 训练、可实时交互的分割方案。

Method: 1) GUI 交互：用户点提示→SAM 生成2D掩码；2) 使用从NeRF推导的深度先验对2D掩码做几何精炼（改善前景/背景分离与边界）；3) 设计自定义CUDA核，将多视角精炼掩码聚合为统一3D体素网格，约200ms完成，提供实时可视化；全流程无需针对场景的优化/训练。

Result: 在 Mip-NeRF 360° 和 LLFF 数据集上，分割质量可与优化式方法相当；端到端速度快2–2.5倍；若不计用户提示时间，整体可快至数量级级别（最高约10倍）。

Conclusion: DivAS 证明了利用2D基础模型与NeRF深度先验、配合高效并行体素聚合，可在无需 per-scene 优化的情况下实现高质量、可实时交互的NeRF分割。

Abstract: Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.

</details>


### [47] [Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform](https://arxiv.org/abs/2601.04891)
*Suyash Mishra,Qiang Li,Srikanth Patil,Satyanarayan Pati,Baddu Narendra*

Main category: cs.CV

TL;DR: 论文面向工业场景的长视频多模态理解，提出可扩展框架并系统评测40+ VLM，在GPU/时延/成本约束下揭示效率与效果的关键权衡与失败模式，给出可操作建议。


<details>
  <summary>Details</summary>
Motivation: 现有VLM评测多基于短视频且默认资源充裕，不符合制药等工业真实环境下需在有限GPU与低时延下处理海量、长时视频与多格式文档/音频的需求，亟需系统性架构与实证分析指导落地。

Method: 构建一套工业级GenAI流水线与架构，处理20万+PDF、2.5万+多格式视频及多语音频；在Video-MME与MMBench及自有25,326长视频数据集上，对40+开源/闭源VLM进行对比评测；从多模态输入、注意力机制（含SDPA）、时间推理、视频切分策略等维度开展系统消融与效率分析。

Result: 在普通GPU上，采用SDPA注意力带来3–8倍效率提升；多模态融合对8/12类任务有显著增益，尤其对长度敏感任务；然而当前VLM在时间对齐与关键帧检测上存在明显瓶颈，影响长视频推理的稳定性与准确性。

Conclusion: 不提出新模型，而是刻画在真实部署约束下VLM长视频推理的实践边界与权衡，指出多模态益处、注意力优化的效率红利及时间建模/切分的痛点，并为研究者与工程师提供可落地的系统设计与优化建议。

Abstract: Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.

</details>


### [48] [Rotation-Robust Regression with Convolutional Model Trees](https://arxiv.org/abs/2601.04899)
*Hongyi Li,William Ward Armstrong,Jun Xu*

Main category: cs.CV

TL;DR: 论文提出在图像输入上实现旋转鲁棒学习的方法，基于可几何变换的卷积模型树（CMT），通过三种几何感知偏置与部署时离散旋转搜索提升鲁棒性；在MNIST的受控回归与识别任务上验证其有效性与局限。


<details>
  <summary>Details</summary>
Motivation: 传统树模型与部分深度模型对图像旋转等几何变换较敏感；需要能在不重新训练或微调参数的情况下，利用模型结构与几何先验提升在任意朝向上的稳健性，并评估基于置信度的部署时姿态选择是否可靠。

Method: 采用可在图像网格上结构化并在部署时进行几何变换的卷积模型树（CMT）。在受控MNIST旋转不变回归设定中引入三种对分裂方向的几何归纳偏置：卷积平滑、倾斜主导约束、基于重要性的剪枝；并提出部署时的离散取向搜索，通过最大化森林层面的置信度代理来选择旋转角度且不更新参数。

Result: 三种偏置均不同程度提升了模型在平面旋转下的鲁棒性。部署时的取向搜索在大角度严重旋转下显著提升性能，但当样本接近标准朝向且“置信度-正确性”错配时会导致性能下降。在MNIST一对其余的回归式识别任务中复现了相同趋势。

Conclusion: CMT结合几何感知偏置与部署期取向搜索能提升旋转鲁棒性，但基于置信度的选择并非始终可靠，尤其在接近标准朝向时可能有害；指出了需要更可靠的正确性代理或自适应策略，以在模型树集成中更稳健地进行取向选择。

Abstract: We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.

</details>


### [49] [Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics](https://arxiv.org/abs/2601.04946)
*Subhadeep Roy,Gagan Bhatia,Steffen Eger*

Main category: cs.CV

TL;DR: 论文提出并量化“原型性偏置”：多模态自动评估指标在语义正确与社会/视觉原型冲突时，倾向选择更原型但语义不符的图像。作者构建对比基准ProtoBias，并提出更稳健的评测指标ProtoScore（7B）。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像评估大量依赖自动指标（CLIPScore、PickScore、VQA-based、LLM-as-Judge），但不清楚它们是否真正对齐文本语义，还是偏好训练数据中学到的“原型”与社会刻板印象。需要一个可控、可定向检测该失效模式的基准，并寻求更稳健的指标。

Method: 1) 构建ProtoBias对比基准：覆盖动物、物体、人口学三类；为每个文本对，配对两张图：a) 语义正确但非原型；b) 语义有细微错误但更原型的“对抗”图。通过方向性比较测试指标是否追随文本或原型。2) 系统评测主流指标（CLIPScore、PickScore、VQA-based、LLM-as-Judge）。3) 人类评测作对照。4) 训练并提出ProtoScore：一个7B参数的评估模型，旨在降低误排，兼顾效率与稳健性。

Result: 主流指标在ProtoBias上经常将原型但语义不符的图像打高分，表现出显著原型性偏置；LLM评审在社会属性相关样例上鲁棒性不均。人类评估稳定偏向语义正确，且置信边际更大。ProtoScore显著降低失误与误排序，鲁棒性接近更大闭源评审，同时推理速度远快于GPT-5级别大模型。

Conclusion: 多模态评估存在系统性原型性偏置，现用自动指标不足以可靠反映语义正确性。ProtoBias提供了检测与对比的基准；ProtoScore在效率与鲁棒性上取得良好折中，能更好抑制该偏置，建议未来评测/过滤流程采用并在更广领域验证。

Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.

</details>


### [50] [TEA: Temporal Adaptive Satellite Image Semantic Segmentation](https://arxiv.org/abs/2601.04956)
*Juyuan Kang,Hao Zhu,Yan Zhu,Wei Zhang,Jianing Chen,Tianxiang Xiao,Yike Ma,Hao Jiang,Feng Dai*

Main category: cs.CV

TL;DR: 提出TEA：一种面向卫星影像时间序列（SITS）的时域自适应分割框架，通过教师-学生蒸馏与重建辅助任务，在不同时间长度输入下显著提升地块分割泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有SITS分割方法多在固定序列长度上训练与评估，忽视了真实场景中时间长度变化（缺帧、不同覆盖周期、传感器差异）导致的泛化退化，实际应用（作物地块映射）中表现显著变差，亟需能跨时间长度稳健工作的模型。

Method: 提出TEA，核心是教师-学生框架：教师以全时序输入学习“全局时序知识”；学生接受可变时长输入。通过三种通道进行知识蒸馏与对齐：（1）中间特征嵌入对齐，（2）原型（类中心）对齐，（3）软标签蒸馏。为避免学生遗忘且提升教师质量，动态聚合学生更新教师。另引入“全序列重建”辅助任务：学生在短序列输入下重建完整时序表征，提升跨时长表征一致性。

Result: 在常用基准上，针对不同时间长度输入均获得显著提升（相较现有方法在变长情形下明显优于基线），展示了强泛化与稳健性。

Conclusion: 通过时域自适应的教师-学生蒸馏与全序列重建，TEA有效缓解SITS分割在变长时序下的性能崩塌问题，为作物地块分割等实际应用提供更鲁棒的方案；代码将开源。

Abstract: Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.

</details>


### [51] [SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://arxiv.org/abs/2601.04968)
*Maximilian Pittner,Joel Janai,Mario Faigle,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: SparseLaneSTP提出稀疏Transformer结合车道几何先验与时序信息，配合连续车道表示与时间正则，并发布精确一致的3D车道数据集，在多个基准和新数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 密集BEV特征到3D的变换易失真，导致与真实路面错配；稀疏方法虽更优但忽略车道先验；现有方法未利用历史观测以在能见度差时消歧。

Method: 构建稀疏车道Transformer，设计车道特定的时空注意力，提出适配稀疏结构的连续车道表示与时间正则；同时用简单有效的自动标注策略构建精确一致的3D车道数据集。

Result: 在现有3D车道检测基准和新数据集上，在检测率与误差各项指标上均达到了SOTA表现，实验验证了各组件的有效性。

Conclusion: 融合车道几何先验与时序约束的稀疏Transformer可显著提升3D车道检测的鲁棒性与精度；高质量自动标注数据集进一步推动性能提升与评测一致性。

Abstract: 3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.

</details>


### [52] [OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction](https://arxiv.org/abs/2601.04984)
*Minseong Kweon,Jinsun Park*

Main category: cs.CV

TL;DR: OceanSplat 通过三视一致性、合成极线深度先验与深度感知不透明度调节，解决水下多视退化引发的不一致与漂浮伪影，使3D Gaussians更准确表示水下几何并显著提升重建与修复效果。


<details>
  <summary>Details</summary>
Motivation: 水下成像受散射与吸收影响，导致多视图不一致、深度/几何模糊与漂浮伪影，现有3D高斯/NeRF方法难以分离介质与物体几何，重建与修复质量受限。

Method: 1) 三目一致性：对每个输入视角合成水平与垂直平移的“平移相机”，渲染并通过逆向扭曲对齐，强制跨视一致。2) 合成极线深度先验：利用这些平移视角做三角化，得到自监督深度正则，约束3D高斯的空间优化。3) 深度感知alpha调整：在训练早期依据高斯z分量与视向调制其不透明度，抑制由介质散射诱发的中等层伪原语生成。整体使高斯与介质解耦，保持水下场景结构。

Result: 在真实与模拟水下数据上，相比现有3DGS/NeRF类方法，显著降低漂浮伪影，几何更准确，重建与图像恢复指标全面领先（文中称“substantially outperforms”）。

Conclusion: 通过几何一致性约束与深度先验结合深度感知不透明度控制，OceanSplat有效解耦散射介质与物体几何，显著提升水下场景的3D重建与视觉恢复表现。

Abstract: We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.

</details>


### [53] [Higher-Order Adversarial Patches for Real-Time Object Detectors](https://arxiv.org/abs/2601.04991)
*Jens Bayer,Stefan Becker,David Münch,Michael Arens,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 研究通过“猫鼠游戏”式的循环对抗（生成更高阶对抗补丁并用对抗训练加固检测器）评估其对YOLOv10的影响。发现更高阶补丁泛化更强，单纯对抗训练不足以有效防御。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本与对抗训练常陷入你追我赶：攻击进化、模型加固、再被攻破。需要系统检验更高阶（多轮交替优化/迭代）对抗补丁对目标检测器的破坏性与泛化性，以及常规对抗训练的防御上限。

Method: 以YOLOv10为代表目标检测器，采用规避式对抗补丁攻击。通过多轮循环：生成更高阶对抗补丁→对检测器进行对抗训练加固→再生成新一轮补丁，逐步提升攻击阶次，评估在同域与跨模型/跨设置上的效果。

Result: 更高阶对抗补丁不只对其训练目标模型有效，对未见配置也有更强的迁移与泛化，胜过低阶补丁。仅依赖对抗训练无法充分抵御这类高阶补丁，仍留显著脆弱性。

Conclusion: 高阶对抗补丁在目标检测场景中具更强泛化与威胁；传统仅靠对抗训练的加固不足，需探索更丰富的防御（如多样化训练、检测与拒绝机制、稳健架构与正则化）以缓解“猫鼠游戏”式循环。

Abstract: Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder

</details>


### [54] [Patch-based Representation and Learning for Efficient Deformation Modeling](https://arxiv.org/abs/2601.05035)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: 提出PolyFit：基于局部贴片喷气（jet）函数拟合的可学习曲面表示，用少量系数高效变形，应用于SfT与服装披挂，兼顾准确度与速度。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格逐顶点优化的曲面变形与拟合在速度与泛化上受限；需要一种紧凑、可监督学习、可跨不同曲面类型泛化、并能在下游任务中高效优化的表示。

Method: 将曲面分解为局部贴片，在每个贴片上拟合低阶jet函数，形成紧凑多项式系数表示（PolyFit）。通过监督学习从解析函数与真实数据联合训练；推理/测试时仅优化少量jet系数以实现曲面变形。展示两个管线：1）SfT中采用test-time optimization基于PolyFit调整系数；2）服装披挂中训练自监督、与网格和服装类型无关的模型，可跨分辨率与类别泛化。

Result: 在SfT任务上，以显著快于离线物理求解器的速度达到有竞争力的精度，并在适度额外运行时间下超过近期物理引导神经模拟器的精度；在服装披挂上，实现跨分辨率与服装类型的泛化，推理速度较强基线提升至多一个数量级。

Conclusion: PolyFit提供了一种紧凑、可学习、可泛化的曲面表示，通过优化少量jet系数即可高效变形，在SfT与服装披挂等任务中兼顾速度与精度，优于或匹配现有方法并具较强实用性。

Abstract: In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

</details>


### [55] [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/abs/2601.05059)
*Suyash Mishra,Qiang Li,Srikanth Patil,Anubhav Girdhar*

Main category: cs.CV

TL;DR: 提出一个面向医药领域的视频到短片生成框架，结合音频/视觉语言模型，通过可复现的剪接算法、角色化个性化提示与性价比管线，实现高效、低成本且更高质量的精彩片段提取，并在公开与私有数据集上验证速度、成本与质量优势。


<details>
  <summary>Details</summary>
Motivation: 医药行业的多模态内容（长视频/音频、文本、图像等）依赖人工标注，存在一致性差、效率低与质量波动问题；尤其长访谈与研讨视频难以高效利用。需要一种自动、可扩展且合规友好的方法，将长视频转为可用的高质量片段。

Method: 提出“视频到视频片段生成”框架：1) Cut & Merge算法（带淡入淡出、时间戳规范化）保证转场平滑与视听对齐；2) 角色定义与提示注入的个性化机制，面向市场、培训与法规等不同场景；3) 平衡ALM/VLM工作负载的端到端成本优化管线。采用ALM+VLM协同，生成并筛选高亮片段。

Result: 在Video-MME基准（900样本）与16,159条、覆盖14个疾病领域的自有医药视频上，获得3–4倍速度提升、4倍成本降低；在片段质量上与SOTA（如Gemini 2.5 Pro）竞争。报告的片段连贯性得分0.348、信息量得分0.721，优于VLM基线。

Conclusion: 该框架以透明、可定制、支持合规为目标，实现医药领域长视频的高效抽取式摘要与精彩片段生成，在效率与成本上显著受益，同时提升连贯性与信息量，适合营销、培训、监管等应用落地。

Abstract: Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.

</details>


### [56] [Driving on Registers](https://arxiv.org/abs/2601.05083)
*Ellington Kirby,Alexandre Boulch,Yihong Xu,Yuan Yin,Gilles Puy,Éloi Zablocki,Andrei Bursuc,Spyros Gidaris,Renaud Marlet,Florent Bartoccioni,Anh-Quan Cao,Nermin Samet,Tuan-Hung VU,Matthieu Cord*

Main category: cs.CV

TL;DR: DrivoR是一种简洁高效的端到端自动驾驶纯Transformer架构，利用ViT与相机感知的寄存器token压缩多相机特征，并通过双解码器生成与打分轨迹，在多项基准上达SOTA或同等表现。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶常面临多摄像头视觉特征冗余、计算开销大、以及缺乏可解释与可控行为生成的问题；需要一种既高效又能保持精度、并能在推理时进行行为调控的架构。

Method: 在预训练ViT上引入相机感知的register tokens，将多摄像头特征压缩为紧凑场景表征；使用两个轻量级Transformer解码器：一者生成候选轨迹；另一者学习模仿“oracle”进行可解释子分数（安全、舒适、效率等）评分，从而在推理时进行行为条件化。

Result: 在NAVSIM-v1、NAVSIM-v2及高真实感闭环HUGSIM基准上，DrivoR优于或匹配当代强基线；在保持或提升精度的同时显著降低下游计算。

Conclusion: 纯Transformer结合有针对性的token压缩即可实现准确、高效且可自适应的端到端驾驶；该设计简洁，并支持通过可解释子分数进行行为调节，代码与模型将开放。

Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.

</details>


### [57] [UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition](https://arxiv.org/abs/2601.05105)
*Filippo Ghilotti,Samuel Brucker,Nahku Saidy,Matteo Matteucci,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 提出一种无需人工标注、利用时间一致性将文本与2D视觉基础模型线索提升到3D的LiDAR伪标注方法，联合生成3D语义、3D框和加密点云，并能检测动态物体，跨数据集泛化良好并显著提升远距深度预测。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的LiDAR数据蕴含丰富3D几何，但缺乏人工标注使其难以利用，标注成本成为研究瓶颈；希望用无监督方式从未标注日志中挖掘几何与语义信息。

Method: 构建时间累计的LiDAR地图学习强几何先验，利用多模态（文本与2D视觉基础模型）线索，通过时间—几何一致性将其提升并融合到3D。提出迭代更新规则，联合约束几何—语义一致性，利用不一致检测动态物体；同时生成3D语义标签、3D检测框和密集LiDAR扫描。

Result: 在三个数据集上展现出强泛化，优于需要额外监督的现有语义分割与目标检测伪标注方法；使用少量几何一致、加密的LiDAR即可在80–150m和150–250m距离范围分别将深度预测MAE提升51.5%与22.0%。

Conclusion: 无需人工标注即可从未标注LiDAR中获得高质量3D语义与检测标注并加密点云，联合一致性带来稳健泛化与动态物体识别，并切实改进远距离深度估计。

Abstract: Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.

</details>


### [58] [From Rays to Projections: Better Inputs for Feed-Forward View Synthesis](https://arxiv.org/abs/2601.05116)
*Zirui Wu,Zeren Jiang,Martin R. Oswald,Jie Song*

Main category: cs.CV

TL;DR: 提出用投影条件替代射线（Plücker）编码，配合掩码自编码预训练，使前馈新视角合成更稳健、更一致，并在基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有前馈视角合成常用Plücker射线编码，受任意世界坐标规范影响，对小相机变换敏感，导致跨视角几何一致性差。需要一种对坐标规范稳定、对小扰动鲁棒的条件方式。

Method: 1) 项目式条件（projective conditioning）：不用原始相机参数/射线图，而是为目标视角构造一个稳定的2D“投影线索”（projective cue），将问题转化为目标视角的图像到图像翻译；2) 针对该线索设计的掩码自编码预训练，可用大规模未标定数据进行预训练；3) 在前馈网络中以该投影线索作为条件进行新视角预测。

Result: 相比基于射线条件的基线，在作者的视图一致性基准上有更强的跨视图一致性与更高保真度；在标准新视角合成基准上达到SOTA质量。

Conclusion: 通过稳定的2D投影条件与相匹配的MAE预训练，可显著提升前馈新视角合成的稳健性与一致性，并改善总体视觉质量。

Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.

</details>


### [59] [Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing](https://arxiv.org/abs/2601.05124)
*Runze He,Yiji Cheng,Tiankai Hang,Zhimin Li,Yu Xu,Zijin Yin,Shiyi Zhang,Wenxun Dai,Penghui Du,Ao Ma,Chunyu Wang,Qinglin Lu,Jizhong Han,Jiao Dai*

Main category: cs.CV

TL;DR: 提出Re-Align框架，通过结构化推理引导对齐与强化学习，提升“上下文内”图像生成与编辑的意图理解与执行一致性。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型虽然在理解上强，但其理解能力难以迁移到图像生成与编辑，尤其在交错图文提示中难以准确分离语义目标与参考图像关联，导致执行偏差。

Method: 1) 提出IC-CoT（In-Context Chain-of-Thought）结构化推理，将“语义目标指导”与“参考图像关联”解耦，生成清晰的文本目标并减少多参考图混淆；2) 设计Re-Align对齐框架：以IC-CoT为核心，将结构化推理文本与生成图像之间的对齐度作为代理奖励，进行强化学习训练，提升生成阶段对理解的遵循。

Result: 在等规模与相近资源设置下，Re-Align在上下文内图像生成与编辑两类任务上均优于竞争方法，表现出更好的意图对齐与参考图关联能力。

Conclusion: 将结构化推理显式引入生成过程并通过代理奖励进行RL对齐，可有效弥合理解—生成鸿沟，提升ICGE任务的忠实度与鲁棒性。

Abstract: In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

</details>


### [60] [VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding](https://arxiv.org/abs/2601.05125)
*Ignacio de Rodrigo,Alvaro J. Lopez-Lopez,Jaime Boal*

Main category: cs.CV

TL;DR: VERSE是一种通过探索视觉嵌入空间来分析并改进用于视觉丰富文档理解（VRDU）的视觉-语言模型的方法。它可视化潜在表示、定位易错簇并用合成数据针对性再训练，从而提升F1且不损伤泛化；并使本地模型（Donut、Idefics2）在优化后可匹敌/超越GPT-4、Pixtral。


<details>
  <summary>Details</summary>
Motivation: VRDU任务中，VLM常存在不可解释的错误与数据分布偏移问题；缺乏系统化手段来评估可行性、定位失效模式，并以低成本数据来修复这些模式。需要一种能从表示层面诊断模型并指导数据合成与再训练的流程。

Method: 提出VERSE框架：1) 从模型提取视觉嵌入并可视化/聚类，评估可分性与可行性；2) 识别错误高发的嵌入簇，分析其视觉特征；3) 基于这些特征生成针对性的合成样本（使用MERIT合成数据）；4) 以含这些特征的样本进行再训练/微调；5) 在真实集（MERIT Secret）上评估泛化。对Donut、Idefics2等本地模型应用该流程，并与SaaS（GPT-4、Pixtral）对比。

Result: VERSE揭示出与错误相关的视觉特征与簇；在这些簇上加入有针对性的合成数据再训练后，F1显著提升，同时对其他分布的泛化未下降。在MERIT→MERIT Secret迁移上证明有效。本地模型经VERSE优化后达到或超过GPT-4、Pixtral表现。

Conclusion: 通过探索和利用视觉嵌入空间，VERSE提供了一条可解释、可操作的路径来诊断与修复VRDU模型失效模式。面向错误簇的合成数据增强能稳定提升性能且保持泛化，使得本地开源模型在成本与隐私优势下获得与SaaS相当甚至更优的效果。

Abstract: This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.

</details>


### [61] [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138)
*Sixiao Zheng,Minghao Yin,Wenbo Hu,Xiaoyu Li,Ying Shan,Yanwei Fu*

Main category: cs.CV

TL;DR: VerseCrafter 是一个4D感知的视频世界模型，通过统一的4D几何控制同时精确操控相机与多物体运动，并借助自动数据引擎从野外视频提取4D标注来训练，实现高保真、视角一致的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成/世界模型大多在2D图像平面上建模，难以统一、精确地控制相机与多物体的三维/时变运动；同时缺乏大规模带显式4D标注的数据来支持这类建模与控制。

Method: 提出4D Geometric Control 表达：静态背景点云 + 每个物体的3D高斯轨迹（同时编码路径与随时间变化的概率占据）。将该4D控制渲染为条件信号，驱动预训练视频扩散模型生成视频，保证对指定动态的严格遵循与跨视角一致性。并构建自动数据引擎，从野外视频中自动估计提取上述4D控制，以形成大规模多样数据用于训练。

Result: 在提供4D控制的前提下，可生成高保真且多视角一致的视频，能精确复现/操控相机与多物体的复杂动态；显示出较以往2D约束方法更强的可控性与一致性。（摘要未列具体指标，重点在质的提升与可控性。）

Conclusion: 将视频世界建模从2D投影提升到统一的4D几何状态，通过3D高斯轨迹与点云实现类别无关、灵活的运动控制，并借助自动数据引擎解决4D标注稀缺，实现了可控、逼真的视频生成。

Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.

</details>


### [62] [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/abs/2601.05143)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Rakibul Islam,Md. Siam Ansary*

Main category: cs.CV

TL;DR: 提出一种轻量级视觉-语言框架，用于叶片图像的作物与病害识别，在保证准确性的同时显著减少参数，并在分类与生成任务上优于大型基线。


<details>
  <summary>Details</summary>
Motivation: 作物病害VQA需要既精准的视觉理解又可靠的语言生成，现有大模型参数庞大、计算昂贵且跨模态对齐不足，缺乏针对农业场景的任务特化预训练与可解释性评估。

Method: 采用Swin Transformer作为视觉编码器，配合序列到序列的语言解码器；设计两阶段训练以增强视觉表征与跨模态对齐；在大规模作物病害数据集上进行分类与自然语言生成评测；引入Grad-CAM与token级归因进行可解释性分析。

Result: 在作物与病害识别上取得高准确率；在BLEU、ROUGE、BERTScore等生成指标上表现强劲；在显著减少参数的同时，整体性能优于大型视觉-语言基线；在多样化用户查询下表现稳健。

Conclusion: 面向任务的视觉预训练与轻量级跨模态建模可有效提升农业病害VQA的准确性与效率，并具备良好可解释性与泛化能力。

Abstract: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.

</details>


### [63] [Atlas 2 -- Foundation models for clinical deployment](https://arxiv.org/abs/2601.05148)
*Maximilian Alber,Timo Milbich,Alexandra Carpen-Amarie,Stephan Tietz,Jonas Dippel,Lukas Muttenthaler,Beatriz Perez Cancer,Alessandro Benetti,Panos Korfiatis,Elias Eulig,Jérôme Lüscher,Jiasen Wu,Sayed Abid Hashimi,Gabriel Dernbach,Simon Schallenberg,Neelay Shah,Moritz Krügener,Aniruddh Jammoria,Jake Matras,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan*

Main category: cs.CV

TL;DR: 论文提出Atlas 2/2-B/2-S三款病理学视觉基础模型，在80个公共基准上实现预测性能、稳健性与资源效率的SOTA，同时基于目前规模最大的病理学数据集（550万张全切片图像，来自三家机构）训练。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型虽推动领域进步，但在性能、稳健性和计算成本之间存在权衡，限制了临床部署；需要一种同时提升精度、鲁棒性并降低资源需求的解决方案。

Method: 构建三种规模/配置的病理视觉基础模型（Atlas 2、Atlas 2-B、Atlas 2-S），使用来自三家医疗机构的超大规模WSI数据集（550万张）进行预训练；在80个公共基准上进行系统评测，覆盖预测准确性、鲁棒性和资源效率。

Result: 三种模型在广泛基准中达成或刷新SOTA，在预测性能、对分布偏移的稳健性以及计算/资源效率方面均优于现有方法。

Conclusion: Atlas 2系列通过大规模多机构数据和架构设计，实现对性能-稳健性-效率三者的兼顾，为病理AI的临床可部署性迈出关键一步。

Abstract: Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.

</details>


### [64] [Multi-Scale Local Speculative Decoding for Image Generation](https://arxiv.org/abs/2601.05149)
*Elia Peruzzo,Guillaume Sautière,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 提出MuLo-SD：通过多尺度起草与空间感知验证，加上局部拒绝-重采样机制，加速AR图像生成，在保持质量与对齐的同时实现最高约1.7×提速，优于EAGLE-2与LANTERN。


<details>
  <summary>Details</summary>
Motivation: AR图像生成逐token顺序解码导致时延大；现有推测解码在图像任务中存在token级歧义且缺乏空间意识，易引入错误与冗余回退，限制加速效果。

Method: 使用低分辨率drafter生成候选图像token，并配合可学习上采样器进行多分辨率起草；高分辨率target模型并行验证。核心是局部拒绝与重采样：一旦检测到错误，不进行整行/整体回退，而是只在空间邻域内重采样，并支持邻域自适应扩展；同时探索概率池化策略及上采样设计以稳健连接多尺度候选与验证。

Result: 在MS-COCO 5k val上，基于GenEval、DPG-Bench、FID/HPSv2评测，达到最高约1.7×加速，在不显著牺牲语义对齐和感知质量的前提下，超越EAGLE-2与LANTERN等强基线。消融显示上采样器设计、概率池化与局部拒绝-重采样（含邻域扩张）对性能提升关键。

Conclusion: MuLo-SD通过多尺度起草+空间感知验证+局部拒绝重采样，实现更高效的AR图像推测解码，在效率-保真度间取得更优权衡，树立图像合成推测解码新SOTA。

Abstract: Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.

</details>


### [65] [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/abs/2601.05159)
*Shuliang Liu,Songbo Yang,Dong Fang,Sihang Jia,Yuqi Tang,Lingfeng Su,Ruoshui Peng,Yibo Yan,Xin Zou,Xuming Hu*

Main category: cs.CV

TL;DR: 提出VLI，一个免训练、推理阶段的自省式框架，通过诊断并因果地引导视觉证据，降低多模态大模型的物体幻觉并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型易出现物体幻觉，根源在于缺乏内省：模型倾向依赖语言先验而非具体视觉证据。现有对比解码只在输出层面抑制，未修复内部语义错位；潜变量操控多用静态向量，难以适配实例，缺乏可解释性与精细度。

Method: 提出Vision-Language Introspection (VLI)，无需再训练。1) 归属性内省：以概率冲突检测识别潜在幻觉，并定位引发结论的因果视觉锚点。2) 可解释双因果引导：在推理过程中动态调节，突出有效视觉证据、抑制背景噪声，并通过自适应校准削弱过度自信，实现对语言先验与视觉证据的双向因果控制。

Result: 在MMHal-Bench上将物体幻觉率降低12.67%；在POPE上准确率提升5.8%，并在先进模型上达到SOTA。

Conclusion: VLI通过实例化、可解释的因果引导与内省式校准，有效缓解多模态物体幻觉问题，优于对比解码和静态潜变量操控方法，且无需额外训练。

Abstract: Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

</details>


### [66] [CoV: Chain-of-View Prompting for Spatial Reasoning](https://arxiv.org/abs/2601.05172)
*Haoyu Zhao,Akide Liu,Zeyu Zhang,Weijie Wang,Feng Chen,Ruihan Zhu,Gholamreza Haffari,Bohan Zhuang*

Main category: cs.CV

TL;DR: 提出一种无需训练的测试时推理框架Chain-of-View（CoV），把VLM转为主动视角推理者，通过先选关键视角再迭代微调视角，从3D环境中逐步收集答题所需信息，显著提升EQA表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在EQA中只能处理固定有限视角，难以在推理时主动获取被遮挡或分散在多视点的关键信息，限制了复杂空间推理能力。需要一种在测试时即可驱动视角选择与探索的通用方法。

Method: 提出CoV：1）粗阶段View Selection代理从候选帧中过滤冗余，挑选与问题对齐的锚点视角；2）细阶段在推理与离散相机动作之间交替，进行视角微调与新观测收集，直至上下文充足或达到步数预算。方法训练无关、可接入任意VLM。

Result: 在OpenEQA上对四个主流VLM平均提升+11.56%（LLM-Match），其中Qwen3-VL-Flash最高+13.62%；增加最小动作预算进一步平均+2.51%，在Gemini-2.5-Flash上达+3.73%。在ScanQA与SQA3D上也取得强性能（ScanQA：116 CIDEr / 31.9 EM@1；SQA3D：51.1 EM@1）。

Conclusion: 将与问题对齐的视角选择同开放式视角搜索相结合，是提升3D EQA空间推理的有效、与模型无关且无需额外训练的策略，并具备测试时可扩展性。

Abstract: Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.

</details>


### [67] [VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice](https://arxiv.org/abs/2601.05175)
*Shuming Liu,Mingchen Zhuge,Changsheng Zhao,Jun Chen,Lemeng Wu,Zechun Liu,Chenchen Zhu,Zhipeng Cai,Chong Zhou,Haozhe Liu,Ernie Chang,Saksham Suri,Hongyu Xu,Qi Qian,Wei Wen,Balakrishnan Varadarajan,Zhuang Liu,Hu Xu,Florian Bordes,Raghuraman Krishnamoorthi,Bernard Ghanem,Vikas Chandra,Yunyang Xiong*

Main category: cs.CV

TL;DR: 提出VideoAuto-R1：视频理解中“必要时才推理”。训练时先答再想再复核，推理受奖励监督；推理时依据首答置信度决定是否触发思考。相比全程CoT，在多基准上达SOTA且效率提升，平均输出长度降约3.3倍。


<details>
  <summary>Details</summary>
Motivation: 质疑多模态视频任务中CoT的必要性：在RL训练的视频模型里，直接作答常能匹配甚至超越CoT，而后者成本高。希望获得在性能与效率间更优的折中，并理解何时需要显式语言推理。

Method: 提出“Think Once, Answer Twice”框架：1) 训练：模型先给初始答案；2) 再执行显式推理；3) 产出复核答案；两次答案均通过可验证奖励监督。推理阶段的启用由策略学习。2) 推理/推断：依据初始答案的置信度自适应决定是否进入思考模式（reason-when-necessary）。

Result: 在视频问答与定位等基准上达SOTA，同时显著提效，平均输出token从约149降至44（~3.3×缩短）。思考模式在偏感知任务上触发率低、在重推理任务上触发率高。

Conclusion: 显式语言推理并非总是必要；通过自适应触发推理，可在保持甚至提升精度的同时大幅降低计算与延迟。VideoAuto-R1体现了对“何时需要思考”的有效建模。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.

</details>


### [68] [Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable](https://arxiv.org/abs/2601.05191)
*Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: AgentCompress通过任务难度预测与动态压缩/路由，将不同复杂度的研究代理任务分配给不同精度/压缩程度的LLM变体，在不显著牺牲成功率的前提下大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 大模型在自主研究工作流（文献综述、假设生成等）中的计算成本高昂，单次会话花费可达百美元以上，限制了学术实验室的可用性。观察到任务难度差异巨大（如写新假设 vs. 参考文献格式化），不应一刀切使用全精度模型。

Method: 构建一个小型神经网络对每个任务在输入开头几个词的基础上进行难度估计，并在亚毫秒内将任务路由到合适的压缩模型变体（不同压缩/精度级别）。在四个科学领域、500个研究工作流上进行评测。

Result: 在保持原始成功率的96.2%的同时，将计算成本降低了68.3%，路由决策延迟低于1毫秒。

Conclusion: 难度感知的动态模型压缩与路由能够显著降低研究代理的运行成本而仅带来有限性能损失，为预算受限的实验室提供可行的高效替代方案。

Abstract: When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines

</details>


### [69] [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/abs/2601.05201)
*William Rudman,Michal Golovanevsky,Dana Arad,Yonatan Belinkov,Ritambhara Singh,Carsten Eickhoff,Kyle Mahowald*

Main category: cs.CV

TL;DR: 论文研究大型视觉-语言模型在计数任务中因迎合文本提示而产生幻觉的问题，并通过机制分析找到少量注意力头，移除后可显著降低提示诱导的幻觉。


<details>
  <summary>Details</summary>
Motivation: VLM 常“听话不看图”，当文本提示与图像证据矛盾时易产生幻觉，尤其在对象数量增加时更严重。作者希望在可控的物体计数场景中系统化刻画这种失败模式，并理解其内部机制以寻找无需再训练的缓解方法。

Method: 在“提示高估物体数”的受控计数任务下评估3个VLM；随着对象数上升观察从纠正到顺从提示的转变。进行机制可解释性分析，定位介导“提示拷贝”的少量注意力头（PIH-heads）；对这些头进行消融（ablation），比较消融前后幻觉率与对视觉证据的依从性变化，并跨模型比较这些头的作用差异。

Result: 发现一小组注意力头主导提示诱导幻觉；对其消融可在无额外训练下将PIH至少降低40%，并提高模型依据图像纠正提示的能力。不同模型中这些头实现“提示拷贝”的方式存在差异。

Conclusion: 提示诱导幻觉源于少数特定注意力头的机制性作用；有针对性的头级消融可显著缓解而无需再训练。不同VLM内部实现存在模型特异性差异，为理解与干预VLM幻觉提供了可行路径。

Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.

</details>


### [70] [MoE3D: A Mixture-of-Experts Module for 3D Reconstruction](https://arxiv.org/abs/2601.05208)
*Zichen Wang,Ang Cao,Liam J. Wang,Jeong Joon Park*

Main category: cs.CV

TL;DR: MoE3D 是一个可插拔的“专家混合”模块，为现有前馈式三维重建模型生成多候选深度并自适应融合，以在几乎不增加计算的情况下显著改进深度边界与飞点问题。


<details>
  <summary>Details</summary>
Motivation: 现有前馈3D重建（如单/多视深度预测）常出现边界模糊与飞点伪影，影响几何质量与后续应用；需在不大幅增加计算成本或重训的前提下，提升重建精度与稳定性。

Method: 在预训练3D重建主干（如 VGGT）之上插入 MoE3D：1）并行预测多张候选深度图（多个专家）；2）学习像素级动态权重进行加权融合；3）以端到端方式训练，使不同专家专注于不同结构/不确定性区域，从而锐化边界并抑制伪影。

Result: 集成到现有主干后，MoE3D 生成的深度边界更清晰、飞点显著减少，整体重建质量明显提升，同时额外计算开销很小。

Conclusion: MoE3D 作为轻量可插拔的专家混合深度融合模块，可普适增强现有前馈3D重建模型的几何质量，尤其在边界与伪影控制方面有效。

Abstract: MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.

</details>


### [71] [FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching](https://arxiv.org/abs/2601.05212)
*Danilo Danese,Angela Lombardi,Matteo Attimonelli,Giuseppe Fasano,Tommaso Di Noia*

Main category: cs.CV

TL;DR: 提出FlowLet：在可逆3D小波域进行flow matching的年龄条件生成模型，快速生成高保真3D脑MRI，缓解潜空间扩散的伪影与慢推理；用于数据增强可提升脑龄预测，特别是稀缺年龄段。


<details>
  <summary>Details</summary>
Motivation: BAP需要大规模、年龄分布均衡的3D MRI数据，但现有数据集人口学偏斜、采集昂贵且受伦理限制；主流潜空间扩散在体数据上推理慢、潜压缩引入伪影、且少做年龄条件控制，影响BAP公平性和泛化。

Method: 提出FlowLet：将3D MRI通过可逆3D小波变换到多尺度、信息保真的可逆域，在该域内采用flow matching进行条件生成；以年龄作为条件，实现少步采样的高效3D体数据生成；避免潜空间重建误差并降低内存/计算需求；并进行区域级别（解剖结构）一致性评估与用于训练BAP模型的数据增强。

Result: FlowLet以很少的采样步数生成高保真3D MRI体数据；用于增强训练的BAP模型在年龄稀缺群体上性能提升；区域分析显示解剖结构得到良好保留。

Conclusion: 在可逆小波域进行条件flow生成可兼顾效率与保真，缓解数据偏斜并提升BAP在弱代表年龄段的表现，显示出在医学影像数据增强中的实践潜力。

Abstract: Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

</details>


### [72] [ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos](https://arxiv.org/abs/2601.05237)
*Rustin Soraki,Homanga Bharadhwaj,Ali Farhadi,Roozbeh Mottaghi*

Main category: cs.CV

TL;DR: ObjectForesight 提出一个以对象为中心的3D动态模型，从短时第一视角视频直接预测刚体的未来6-DoF姿态与轨迹；借助自动化3D标注管线构建200万+片段数据集，显著提升准确性、几何一致性与泛化。


<details>
  <summary>Details</summary>
Motivation: 人类能通过被动观察轻松预测物体交互后的运动与状态变化，但现有模型多在像素/潜变量层面建模，缺乏显式3D与对象层级的物理约束，难以获得可解释、几何一致且具可泛化的动态预测能力。

Method: 提出ObjectForesight：以对象为中心的3D世界表示与动力学预测框架。模型输入短时第一视角视频，输出多个刚体对象的未来6-DoF姿态序列与轨迹。训练上，整合最新的分割、网格重建、3D位姿估计，自动构建带伪真值的200万+短视频3D轨迹数据集，用于监督或自监督式学习，强调几何约束与时间连贯性以捕捉可供性与物理一致性。

Result: 在大量实验中，相比传统像素/潜变量世界模型，ObjectForesight在预测准确度、几何一致性与跨未见物体/场景的泛化上显著提升，能产生更符合物理和可供性的未来轨迹与姿态。

Conclusion: 对象级、显式3D表示的动力学建模可从纯观察中学习到物理一致、可泛化的未来运动预测。ObjectForesight提供了可扩展的数据与方法框架，为具物理常识的视觉预测与交互规划奠定基础。

Abstract: Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io

</details>


### [73] [Plenoptic Video Generation](https://arxiv.org/abs/2601.05239)
*Xiao Fu,Shitao Tang,Min Shi,Xian Liu,Jinwei Gu,Ming-Yu Liu,Dahua Lin,Chen-Hsuan Lin*

Main category: cs.CV

TL;DR: PlenopticDreamer提出一种在多视角下保持时空一致性的相机可控生成视频重渲染框架，通过同步生成幻觉以维持跨时间与视角的记忆，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有相机可控生成视频重渲染在单视角有效，但在多视角下易出现跨视角/时序不一致，尤其在需要“幻觉”补全缺失区域时，生成模型的随机性导致记忆断裂与漂移，需要一种机制来对齐并稳定这些幻觉。

Method: 训练一个“多入单出”的视频条件自回归模型；利用“相机引导的视频检索”从以往生成结果中自适应挑选关键视频作为条件输入；在训练中采用渐进式上下文扩展以稳定收敛；使用自条件化以抑制长程误差累积导致的退化；加入长视频条件机制以支持更长时序生成与控制。

Result: 在Basic与Agibot基准上取得SOTA的重渲染效果，表现为更好的多视角同步、一致的时空连贯性、更高的视觉保真与精确相机控制，并支持多样视角转换（如第三人称↔第三人称、头戴视角→机械臂夹爪视角）。

Conclusion: 通过同步与记忆化的条件设计，PlenopticDreamer有效缓解多视角生成中的随机性与漂移问题，实现稳定的相机可控视频重渲染，并可扩展到长视频与多样视角转换场景。

Abstract: Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/

</details>


### [74] [RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation](https://arxiv.org/abs/2601.05241)
*Boyang Wang,Haoran Zhang,Shujie Zhang,Jinkun Hao,Mingda Jia,Qi Lv,Yucheng Mao,Zhaoyang Lyu,Jia Zeng,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 提出以视觉身份提示（用示例图像而非仅文本）引导扩散模型生成多视角、时序一致的场景来增强机器人操作数据，并构建可扩展的视觉身份库；在多种策略模型与仿真/真实机器人上带来稳定性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有真实世界操作数据难以在多环境大规模获取；基于文本提示的图像扩增方法无法可靠指定场景且缺乏多视角与时间一致性，难以满足先进策略模型对输入的需求。

Method: 引入“视觉身份提示”（visual identity prompting），以示例图像作为条件输入，显式约束扩散模型生成的背景、台面物体与布局；并搭建从大规模机器人数据集中自动整理可扩展的视觉身份池与数据增强流水线，生成多视角、时序一致的观察序列；将增强后的数据用于训练VLA与视觉运动控制策略。

Result: 使用该增强数据训练的策略在仿真与真实机器人实验中均获得一致的性能提升，相比仅文本提示或未增强的数据更有效。

Conclusion: 视觉身份提示与可扩展视觉身份库能提高扩散式数据增强对机器人操作的适配性，解决多视角、时序一致与场景可控性问题，从而稳步提升下游策略在多环境中的表现。

Abstract: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

</details>


### [75] [GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation](https://arxiv.org/abs/2601.05244)
*Henghui Ding,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出GREx框架（GRES/GREC/GREG）与大规模数据集gRefCOCO，支持单/多/零目标的指代表达任务；并给出关系建模基线ReLA，在GRES/GREC上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统REx（RES/REC/REG）仅处理“一句对应一个目标”，不覆盖多目标和无目标情形，限制真实应用与模型泛化评估；缺乏相应数据集与统一基准，难以系统比较现有方法在更一般场景下的表现。

Method: 1) 定义广义任务GREx：允许表达指向任意数量的目标（包括0、1、多）。2) 构建gRefCOCO数据集，包含多/零/单目标标注，且与传统REx后向兼容。3) 提出ReLA基线：自适应将图像划分为含“子实例线索”的区域；显式建模区域-区域与区域-语言依赖关系，以处理复杂关系表达。

Result: 在GRES与GREC两项任务上，ReLA取得SOTA表现；基于gRefCOCO的广泛实验揭示现有REx方法在GREx任务上的性能差距。

Conclusion: GREx任务设定与gRefCOCO数据集为广义指代表达研究提供统一基准；ReLA验证了显式区域与语言关系建模对多/零目标理解的有效性，推动REx从单目标走向更通用的实际场景。

Abstract: Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.

</details>


### [76] [Pixel-Perfect Visual Geometry Estimation](https://arxiv.org/abs/2601.05246)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Sida Peng,Hangjun Ye,Xin Yang*

Main category: cs.CV

TL;DR: 提出像素空间生成模型的单/视频深度估计PPD/PPVD，通过语义提示与级联DiT实现无飞点、高细节的点云，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有几何基础模型存在飞点(flying pixels)与细节损失，影响机器人与AR的几何精度。需要一种既保留全局语义又恢复精细结构、并能在图像与视频中高效推理的生成式方案。

Method: 以像素空间扩散Transformer(DiT)为核心：1) 语义提示DiT：引入来自视觉基础模型的语义表征作为扩散条件，兼顾全局语义与细节；2) 级联DiT：逐级扩大图像token数量，先粗后细以提升效率与精度；扩展到视频PPVD时，提出语义一致DiT，从多视角几何基础模型抽取时序一致的语义，并在DiT内进行参考引导的token传播，维持时序连贯且节省算力/显存。

Result: 在单目与视频深度估计上取得当前生成式模型的最佳指标，生成的点云显著更干净、几乎无飞点，细节恢复优于其他方法。

Conclusion: 像素空间生成式几何建模结合语义提示与级联结构，可高效产生无飞点、高细节的单目/视频深度与点云；时序一致语义与token传播进一步保证视频连贯性，整体刷新SOTA。

Abstract: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

</details>


### [77] [RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes](https://arxiv.org/abs/2601.05249)
*Yuan-Kang Lee,Kuan-Lin Chen,Chia-Che Chang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出RL-AWB：把夜间统计白平衡与深度强化学习结合，动态调参实现夜景色彩恒常；并发布多传感器夜间数据集，跨设备与光照下泛化更好。


<details>
  <summary>Details</summary>
Motivation: 夜间成像噪声高、光源复杂，传统AWB与白天训练的深度方法泛化差，缺少跨传感器夜间数据，急需兼具鲁棒性与可迁移性的方案。

Method: 两阶段：1) 统计式夜景AWB：显著灰像素检测与新型照明估计结合，得到可解释的基础解。2) 强化学习控制器：以统计算法为内核，学习专家调参策略，针对每张图自适应调节参数；同时构建多传感器夜间数据集用于训练/评估。

Result: 在低照和正常光照图像上均取得更优或可比的误差与视觉质量；表现出跨传感器的强泛化能力，优于仅基于统计或端到端深度的方法。

Conclusion: 统计先验与RL策略互补可提升夜间色彩恒常与跨设备泛化；所建数据集推动跨传感器夜景AWB研究，方法可作为实际AWB调优框架。

Abstract: Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/

</details>


### [78] [QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer](https://arxiv.org/abs/2601.05250)
*Daniele Lizzio Bosco,Shuteng Wang,Giuseppe Serra,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 提出QNeRF：将量子参数化电路融入NeRF以做新视角合成，在中等分辨率训练时以更少参数达到或优于经典NeRF性能。


<details>
  <summary>Details</summary>
Motivation: NeRF在新视角合成上表现强，但模型大、训练耗时；QVF在表示连续信号时显示出压缩与加速潜力。作者希望利用量子叠加与纠缠提升表示效率，降低参数规模与复杂度，同时保持或提升渲染质量。

Method: 构建混合量子-经典框架QNeRF，用参数化量子电路(PQC)编码空间与视角相关信息。提出两种结构：1) Full QNeRF：最大化利用量子幅度提升表达力；2) Dual-Branch QNeRF：将空间与视角的量子态制备分支化，引入任务先验以降低量子态制备复杂度，提升可扩展性与潜在硬件可实现性。与经典NeRF在相同训练设置和中等分辨率图像上比较。

Result: 在中等分辨率数据上，QNeRF以不足经典NeRF一半的参数量达到或超越基线NeRF的重建质量与新视角合成表现。

Conclusion: 量子机器学习可作为连续信号表示的竞争性替代方案，尤其在计算机视觉的中层任务（如从2D观测学习3D表示）中，QNeRF展现了参数更少、性能可比或更优的潜力；Dual-Branch结构具备更好可扩展性与硬件兼容前景。

Abstract: Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.

</details>


### [79] [Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video](https://arxiv.org/abs/2601.05251)
*Zeren Jiang,Chuanxia Zheng,Iro Laina,Diane Larlus,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Mesh4D是一种单目视频到4D网格（时变3D形状）重建的前馈模型，通过一次性预测完整动画的变形场，重建准确的几何与运动，并在基准上优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目动态重建方法往往需逐帧优化、依赖骨骼或多视角，难以高效稳定地从单目视频恢复完整3D形状与时序变形；需要一个既能编码全局时空一致性的紧凑表示，又能在推理时无需骨骼监督的一次性动画重建方法。

Method: 1) 设计一个由骨骼结构引导训练的自编码器，学习将整段动画压缩到紧凑潜空间的表示；2) 编码器采用时空注意力获取稳定的全局变形表示；3) 在该潜空间上训练条件潜扩散模型，条件为输入视频与首帧重建网格，一次性预测完整序列的变形场，从而得到4D网格。

Result: 在重建与新视角合成基准上，Mesh4D在3D形状精度与变形恢复上优于现有方法，表现出更稳定、准确的动画重建能力。

Conclusion: 以骨骼先验引导的紧凑潜空间结合时空注意与潜扩散，使得单目视频下可一次性重建完整4D网格；方法在准确性与鲁棒性上领先，且推理无需骨骼信息。

Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.

</details>
