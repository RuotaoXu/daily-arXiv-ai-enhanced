<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 173]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements](https://arxiv.org/abs/2601.00812)
*Takashi Ushio,Kazuhiro Onishi,Hideyoshi Yanagisawa*

Main category: cs.CV

TL;DR: 该研究提出一种仅基于广告视频场景级“表达元素”来估计情绪（愉悦、惊奇、习惯化）的可解释方法，不依赖生理信号或主观评分；以自由能框架为理论，将KLD、贝叶斯惊奇和不确定度对应到情绪成分，在1059段15秒食品广告上验证，并在多超参与跨类型广告上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 广告观看过程中的情绪会影响注意、记忆与购买意图，但现有方法常依赖昂贵或不可扩展的外部数据（生理/主观评分）。需要一种仅用视频本身即可解释与量化情绪动态的通用方法，并与认知理论（自由能原理）衔接。

Method: 以自由能（FE）原理为总体框架：用KLD表示预测误差（对应愉悦的变化）、用贝叶斯惊奇（BS）表示信念更新（惊奇）、用不确定度（UN）表示先验模糊。仅从广告视频的场景级表达特征（元素类型、空间布局、数量与变异度等）计算上述量，得到时间序列情绪指标；在1059个15秒食品广告上实验，并进行九组超参鲁棒性检验与六类日本广告（3体裁×2时长）的泛化测试。

Result: 实验证明：KLD与品牌呈现相关的“愉悦”相符；BS捕捉由信息复杂度引发的惊奇；UN反映由元素类型/空间布置不确定性、以及元素数量与变异度驱动的惊奇。识别出三种特征情绪轨迹：不确定刺激、持续高情绪、瞬时峰值后衰减。上述趋势在多超参与跨类型/时长的测试中稳定。

Conclusion: 基于表达元素、依托FE原理的可解释情绪估计可在无需外部标注的条件下稳定工作，能揭示广告中的关键情绪机制；未来可扩展更多表达元素并结合主观评分进行验证，为更具吸引力的广告创作提供技术支持。

Abstract: Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified "pleasantness," "surprise," and "habituation" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected "pleasantness" associated with brand presentation, BS has captured "surprise" arising from informational complexity, and UN has reflected "surprise" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.

</details>


### [2] [Can Generative Models Actually Forge Realistic Identity Documents?](https://arxiv.org/abs/2601.00829)
*Alexander Vinogradov*

Main category: cs.CV

TL;DR: 研究评估开源扩散式生成模型能否伪造可通过人类或自动审核的身份证件。结果显示：可仿外观但难以达到结构与取证层面的真实性，当前风险被高估。


<details>
  <summary>Details</summary>
Motivation: 生成图像逼真度提升引发对证件伪造滥用的担忧，需要客观评估开源、公众可用的扩散模型在身份证件伪造上的真实能力与风险。

Method: 选取多种公开模型家族（如Stable Diffusion、Qwen、Flux、Nano-Banana等），分别在文生图与图生图两种流程下生成证件图像；从人类可感知的外观与取证学（结构一致性、微观特征、安保元素等）角度评估其能否绕过人工或自动验证。

Result: 当代模型能较好模拟证件表层美学风格，但在版式结构、版纹/微缩文字、序列号一致性、光变/水印/UV元素、材质/压印、字体细节与防伪逻辑等方面失真，难以通过严格的人审或取证级系统。

Conclusion: 短期内利用公开扩散模型实现取证级身份证件深伪的可行性有限，风险可能被夸大；建议ML研究者与证据鉴定专家合作，进行更现实的威胁建模与防御设计。

Abstract: Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.

</details>


### [3] [Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs](https://arxiv.org/abs/2601.00837)
*Agniv Roy Choudhury*

Main category: cs.CV

TL;DR: 研究比较自建CNN与迁移学习（ResNet50、DenseNet121、EfficientNet-B0）在儿科胸片肺炎检测的表现，最佳为微调ResNet50（Acc 99.43%、F1 99.61%、AUC 99.93%），Grad-CAM显示关注肺部相关区域。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎致死率高而放射科医师资源有限且诊断一致性不足，需要自动化、准确且可解释的胸片筛查方法。

Method: 使用5216张儿科胸片，按8/1/1划分训练/验证/测试；训练7个模型，包含从零训练的自建CNN与三种预训练架构（ResNet50、DenseNet121、EfficientNet-B0）在“冻结骨干”和“全量微调”两种策略；以准确率、F1、AUC评估，并用Grad-CAM做可解释性分析。

Result: 微调ResNet50表现最佳：准确率99.43%、F1 99.61%、AUC 99.93%，仅3例误判；整体上微调比冻结骨干平均高5.5个百分点；Grad-CAM热力图聚焦临床相关肺区。

Conclusion: 迁移学习加微调显著优于从零训练的CNN，达近乎完美的儿科肺炎检测性能；有望在资源有限场景用于筛查，但需在多中心与成人数据上进一步验证与泛化评估。

Abstract: Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.
  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.
  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.
  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\% accuracy, 99.61\% F1-score, and 99.93\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.
  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.
  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.

</details>


### [4] [Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS](https://arxiv.org/abs/2601.00839)
*Zahid Ullah,Muhammad Hilal,Eunsoo Lee,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: 论文在CAMUS超声心动图数据集上，对U‑Net、Attention U‑Net、TransUNet在统一预处理与评估下做了“同台竞技”，并验证保持原始动态范围、SSL预训练与GPT辅助伪标注能显著提升分割稳健性与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有综述多讨论心脏影像与深度学习进展，但缺少将综述与可复现实验基准相结合的工作；数据预处理不一致常导致结论不可比，且如何利用SSL与大模型辅助标注仍缺乏系统评估。

Method: 在CAMUS数据集上建立标准化基准：统一训练划分、损失与评估；比较三种网络（U‑Net、Attention U‑Net、TransUNet）；考察多种数据路线（原生NIfTI、16位PNG导出、GPT辅助多边形伪标注、数千未标注序列上的自监督预训练）；并给出强约束的预处理策略（保持强度/分辨率/对齐一致）。

Result: 在相同训练与评估下：U‑Net基于原生NIfTI可达平均Dice 94%，PNG‑16位流程约91%；Attention U‑Net在小体积/低对比区域略优并减少边界泄漏；TransUNet在困难帧泛化最强，尤其配合SSL初始化；伪标注经置信过滤扩大训练集并增强鲁棒性。

Conclusion: 贡献包括：1) 在标准化CAMUS预处理与评估下，对三种主流架构给出可复现、可比的基准；2) 提供超声数据准备的实操要点（保持强度保真、分辨率一致与空间对齐）；3) 展望可扩展的自监督与基于GPT的多模态标注流水线，用于快速标注、质控与定向数据策展。

Abstract: Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.

</details>


### [5] [Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge](https://arxiv.org/abs/2601.00854)
*Igor Lodin,Sergii Filatov,Vira Filatova,Dmytro Filatov*

Main category: cs.CV

TL;DR: 提出一种面向边缘设备的视觉态势感知方法：在稳定的基准坐标系中维护两张“潜在语义画布”，分别累积静态语义与快速更新的动态语义；通过运动门控，仅在有新运动信息时触发昂贵的全景分割推理，并用稳像/运动补偿保持语义记忆一致。原型在480p视频上相比逐帧分割将分割调用减少>30倍、端到端平均时延降低>20%，同时保持连贯的静/动态叠加。


<details>
  <summary>Details</summary>
Motivation: 边缘设备算力受限，逐帧运行昂贵的全景分割会造成高延迟和能耗；但视频场景中大量内容在短时内保持稳定，可以被复用。因此需要一种能够在不牺牲语义一致性的前提下，显著减少语义推理频率并维持场景连续性的机制。

Method: 构建两个在视频稳像后的基准坐标系中持久存在的“潜在语义画布”：1) 静态层，缓慢累积并维护持续不变的背景/静态语义；2) 动态层，快速更新以捕捉运动对象。使用Mask2Former作为昂贵的全景分割器，采用运动门控策略：仅当运动检测表明出现新信息时才触发分割；其余帧通过稳像与运动补偿在基准坐标系中传播已存在的语义。这样在时间上复用语义结果，减少冗余推理。

Result: 在预录制的480p视频上，原型系统相较于朴素的逐帧分割：分割调用次数减少超过30倍；端到端平均处理时间降低超过20%；同时获得连贯的一致的静/动态语义叠加可视化。

Conclusion: 通过将语义记忆持久化到稳像坐标系的静/动态潜在画布，并用运动门控触发昂贵推理，能在资源受限边缘端显著减少分割调用与延迟，同时保持语义覆盖的时空一致性。这为低算力设备上的视频语义感知提供了高效且实用的范式。

Abstract: We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.

</details>


### [6] [VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.00879)
*Zahid Ullah,Jihie Kim*

Main category: cs.CV

TL;DR: 提出VLOrdinalFormer：结合ViT-L/16、CORAL序数回归与CLIP语义对齐，用于自动KOA的KL分级，显著提升对KL1/2的区分，达到SOTA性能并具可解释性。


<details>
  <summary>Details</summary>
Motivation: KL分级对临床决策关键，但早期阶段（KL1与KL2）影像差异细微，放射科医师之间一致性差。需要一个既能利用临床语义又能处理序数关系的模型，提高早期分级准确性与稳定性，并具备可解释性以便临床采用。

Method: 构建VLOrdinalFormer：以ViT-L/16为视觉主干；采用CORAL进行序数回归学习KL等级；引入CLIP驱动的图文语义对齐模块，将“关节间隙变窄、骨赘、软骨下硬化”等临床概念文本与影像特征对齐；训练上使用分层五折交叉验证、类别感知重加权突出中间等级、测试时增强与全局阈值优化以提升鲁棒性并减轻过拟合；可解释性通过Grad-CAM与CLIP相似度图验证关注区域。

Result: 在公开OAI kneeKL224数据集上，宏平均F1与总体准确率优于CNN与ViT基线，尤其对KL1与KL2显著提升，同时对轻度或重度病例的准确性不下降；解释性分析显示模型关注临床相关解剖区域。

Conclusion: 视觉-语言对齐的序数Transformer能可靠、可解释地进行KOA KL分级，特别改善早期分级，有潜力用于常规放射学实践中的疾病分级与进展评估。

Abstract: Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we propose VLOrdinalFormer, a vision language guided ordinal learning framework for fully automated KOA grading from knee radiographs. The proposed method combines a ViT L16 backbone with CORAL based ordinal regression and a Contrastive Language Image Pretraining (CLIP) driven semantic alignment module, allowing the model to incorporate clinically meaningful textual concepts related to joint space narrowing, osteophyte formation, and subchondral sclerosis. To improve robustness and mitigate overfitting, we employ stratified five fold cross validation, class aware re weighting to emphasize challenging intermediate grades, and test time augmentation with global threshold optimization. Experiments conducted on the publicly available OAI kneeKL224 dataset demonstrate that VLOrdinalFormer achieves state of the art performance, outperforming CNN and ViT baselines in terms of macro F1 score and overall accuracy. Notably, the proposed framework yields substantial performance gains for KL1 and KL2 without compromising classification accuracy for mild or severe cases. In addition, interpretability analyses using Grad CAM and CLIP similarity maps confirm that the model consistently attends to clinically relevant anatomical regions. These results highlight the potential of vision language aligned ordinal transformers as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice.

</details>


### [7] [VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition](https://arxiv.org/abs/2601.00887)
*Hongbo Jin,Kuanwei Lin,Wenhao Zhang,Yichen Jin,Ge Li*

Main category: cs.CV

TL;DR: VideoCuRL提出一个双轴难度课程学习框架，用训练免代理将视频样本映射到“感知负载×推理深度”的2D网格，并以对角波前策略进行RL训练，同时通过动态稀疏KL与结构化复习稳定训练，最终在视频感知与推理基准上优于强RL基线且无推断开销。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs的RL微调多用随机或基于单一标量难度的课程，但单标量无法区分“视觉时序感知负载”和“认知推理深度”这两类正交难点，导致训练调度与稳定性欠佳，且生成式难度评估带来高昂推断成本。

Method: 1) 将难度分解为两轴：视觉复杂度（用光流与关键帧熵的训练免代理）与认知复杂度（用校准惊讶度Calibrated Surprisal）。2) 将数据映射到2D课程网格，并用“能力感知的对角波前”调度，从基础对齐到高阶推理逐步训练。3) 提出动态稀疏KL，抑制奖励崩塌；引入结构化复习，缓解灾难性遗忘。4) 全流程避免生成式课程的推断开销。

Result: 在VSI-Bench推理任务上提升+2.5，在VideoMME感知任务上提升+2.9，相比强RL基线更优，并保持训练稳定与可扩展性。

Conclusion: 按感知与推理双轴构建课程并配合稳定化策略，可在不增加推断成本的情况下显著提升VideoLLMs的RL后训练效果，提供可扩展、稳健的视频课程RL方案。

Abstract: Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.

</details>


### [8] [Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study](https://arxiv.org/abs/2601.00888)
*Happy Gery Pangestu,Andi Prademon Yunus,Siti Khomsah*

Main category: cs.CV

TL;DR: 比较五类CNN骨干在印尼蜡染（batik）风格迁移中的性能：结构保持差异不显著；ResNet类在相似感知质量下收敛更快、算力需求更低；VGG风格更浓郁但代价高；Inception居中但更噪。结论：以效率与结构保真为导向，优先选ResNet。


<details>
  <summary>Details</summary>
Motivation: 现有NST多依赖VGG以获得强风格表达，但在资源受限环境（工业部署、移动端）下计算与内存开销过高，限制了蜡染数字化保护与规模化生成的实际应用。因此需要系统比较不同骨干网络在风格表达、结构保真与效率之间的权衡。

Method: 基于五类主流CNN骨干（VGG16/19、Inception V3、ResNet50/101），进行245次受控实验；采用定量指标（SSIM、LPIPS、FLOPs、收敛速度）、定性评估（纹理、几何稳定性、笔触保留）与统计分析（ANOVA检验SSIM）综合比较。

Result: ANOVA显示不同骨干的SSIM无显著差异（p=0.83），结构保真相当；ResNet较VGG收敛约快5–6倍，LPIPS相近（0.53），计算开销大幅下降（约0.63 vs 10.12 GFLOPs，>16倍减少）。定性上：VGG纹理更致密、笔触更“绘画化”；ResNet几何更稳定、保留canting笔触但风格更温和；Inception居中但更噪。

Conclusion: NST骨干选择应从“风格强度最大化”转向“效率与结构保真”优先。ResNet系作为蜡染生成的实用基座更适合规模化与工业化部署。

Abstract: Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.

</details>


### [9] [CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis](https://arxiv.org/abs/2601.00897)
*Sai Teja Erukude,Jane Mascarenhas,Lior Shamir*

Main category: cs.CV

TL;DR: CornViT 提出三阶段CvT-13级联模型，实现玉米单籽粒纯度、形态与胚位朝向的自动判定，在三个公开基准上取得约91–94%的测试准确率，并配套发布数据集与可部署的Flask网页应用。


<details>
  <summary>Details</summary>
Motivation: 玉米籽粒分级对种子认证、定向播种与育种至关重要，但仍主要依赖人工目检，效率低、主观性强、难以规模化与标准化，亟需准确、可部署的自动化方案。

Method: 构建三阶段串联的CvT-13分类框架：Stage1判别纯籽/杂质；Stage2对纯籽分为扁平/圆形；Stage3对纯且扁平的籽粒判别胚朝上/下。使用384×384 RGB图像，基于ImageNet-22k预训练的CvT-13仅微调分类头。作者从公开数据集中重标注与筛选，分别形成三套基准数据（7265/3859/1960张）。并开发Flask网页端进行分阶段推理与可解释展示。

Result: 在相同训练条件下，CvT-13测试准确率：纯度93.76%、形态94.11%、胚向91.12%；对比ResNet-50仅76.56–81.02%，DenseNet-121为86.56–89.38%，显示卷积增强自注意力在该任务上的优越性。

Conclusion: 三阶段CornViT结合精心构建的数据集与可部署Web应用，为种子质量流程中的玉米籽粒自动分级提供高精度、可落地的解决方案，并以公开源码与数据促进复现与应用。

Abstract: Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.

</details>


### [10] [Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems](https://arxiv.org/abs/2601.00905)
*Eliot Park,Abhi Kumar,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 研究评估前沿视觉-语言模型（GPT-4o、GPT-4o-mini、Claude 3.5）在日常物品“是否可回收、应投何种垃圾桶、是否能放入桶口”上的判断能力，并在多情境（地区规则、污染/破损、多材料）下对其表现进行系统比较。结论：模型具备更强情境理解，但仍存在边界与不足，需持续优化以助力公众回收与可持续发展。


<details>
  <summary>Details</summary>
Motivation: 公众普遍认可高效回收的重要性，但准确判断物品可回收性及正确投放对普通人仍然困难；现有自动化方法受限。新一代视觉-语言模型具备更强的上下文理解和多模态推理能力，可能在实际回收决策中提供帮助，因此需要系统评估其有效性和局限。

Method: 构建经筛选的数据集，包含常见弃置物的图像；让多种VLM（GPT-4o、GPT-4o-mini、Claude 3.5）预测对应回收桶类别，并判断物体是否能物理放入给定桶口。设计三类挑战情境：(i) 不同地区回收指南差异；(ii) 受污染或结构破损的物品；(iii) 多材料构成的复合物。比较各模型在基础与挑战任务上的表现。

Result: 这些模型在将物品与合适回收桶匹配以及结合情境信息方面优于以往迭代，展现出显著进步；在位置依赖规则切换、污染/破损识别、多材料权衡与尺寸/适配判断上仍有错误和不稳定情形。

Conclusion: 具有情境感知能力的VLM能显著提升回收决策的准确性与实用性，但仍需进一步优化（尤其在地区规则适配、污染判定、物理可行性与多材料处理）以更好地支持公众实践并促进环境可持续。

Abstract: While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.

</details>


### [11] [Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting](https://arxiv.org/abs/2601.00913)
*Subhankar Mishra*

Main category: cs.CV

TL;DR: Clean-GS 利用稀疏语义分割掩码对 3D Gaussian Splatting 模型进行多阶段清理（白名单投影筛选、深度缓冲与颜色验证、邻域离群点移除），去除背景杂点与漂浮高斯，在基本不损失渲染质量的前提下实现 60–80% 压缩，适用于网络与 AR/VR 部署。


<details>
  <summary>Details</summary>
Motivation: 3DGS 重建虽高质，但会产生大量漂浮高斯与背景杂点，既遮挡目标又显著增大模型体积，不利于带宽受限场景（如网页、移动端、AR/VR）部署。现有剪枝依赖全局重要性指标，难以针对性去除非目标高斯。

Method: 提出 Clean-GS：利用极少量（约 1% 视角、≥3 张）的语义分割掩码引入目标先验，采用三阶段流程：1）白名单筛选：将高斯投影到掩码区域，仅保留命中目标的高斯；2）基于深度缓冲的颜色验证：结合深度一致性与颜色相似性剔除误保留；3）邻域统计的离群点去除：利用邻域密度/相似度清除残余孤立或不一致高斯。

Result: 在 Tanks and Temples 数据上实现 60–80% 模型压缩（如从 125MB 降至 47MB），同时保持目标渲染质量；在复杂户外场景中可有效分离纪念碑/物体与背景。

Conclusion: Clean-GS 以稀疏语义信息驱动、分阶段的几何-外观一致性筛选，可在不牺牲可视质量的情况下显著压缩 3DGS，提升其在网页与 AR/VR 等带宽受限应用中的可部署性，并优于依赖全局重要性指标的常规剪枝方案。

Abstract: 3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs

</details>


### [12] [Four-Stage Alzheimer's Disease Classification from MRI Using Topological Feature Extraction, Feature Selection, and Ensemble Learning](https://arxiv.org/abs/2601.00918)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出TDA‑Alz：以拓扑数据分析提取MRI拓扑描述符，配合特征选择与集成学习，实现阿尔茨海默病四级分期分类，在OASIS‑1上达98.19%准确率、99.75%AUC，无需数据增强或预训练，具可解释性与高效性。


<details>
  <summary>Details</summary>
Motivation: 在MRI分级任务中，样本规模有限、深度模型依赖大量数据与算力且可解释性不足，影响临床落地。需要一种在小数据、低计算成本场景下仍具鲁棒性与可解释性的分类框架。

Method: 利用TDA从脑MRI中提取刻画结构模式的拓扑描述符（如持久同调等），经特征选择保留判别性最强的特征，再用集成学习进行四分类（非痴呆、极轻度、轻度、中度）。不使用数据增强、预训练或大型卷积网络。

Result: 在OASIS‑1数据集上获得98.19%准确率与99.75%AUC，优于或匹配同数据集上报道的深度学习方法，同时推理与训练计算开销较低。

Conclusion: TDA‑Alz在小样本条件下提供轻量、可解释且高性能的AD分期方案，适合临床决策支持；拓扑特征与集成学习的组合为替代深度网络提供了有效路径。

Abstract: Accurate and efficient classification of Alzheimer's disease (AD) severity from brain magnetic resonance imaging (MRI) remains a critical challenge, particularly when limited data and model interpretability are of concern. In this work, we propose TDA-Alz, a novel framework for four-stage Alzheimer's disease severity classification (non-demented, moderate dementia, mild, and very mild) using topological data analysis (TDA) and ensemble learning. Instead of relying on deep convolutional architectures or extensive data augmentation, our approach extracts topological descriptors that capture intrinsic structural patterns of brain MRI, followed by feature selection to retain the most discriminative topological features. These features are then classified using an ensemble learning strategy to achieve robust multiclass discrimination.
  Experiments conducted on the OASIS-1 MRI dataset demonstrate that the proposed method achieves an accuracy of 98.19% and an AUC of 99.75%, outperforming or matching state-of-the-art deep learning--based methods reported on OASIS and OASIS-derived datasets. Notably, the proposed framework does not require data augmentation, pretrained networks, or large-scale computational resources, making it computationally efficient and fast compared to deep neural network approaches. Furthermore, the use of topological descriptors provides greater interpretability, as the extracted features are directly linked to the underlying structural characteristics of brain MRI rather than opaque latent representations. These results indicate that TDA-Alz offers a powerful, lightweight, and interpretable alternative to deep learning models for MRI-based Alzheimer's disease severity classification, with strong potential for real-world clinical decision-support systems.

</details>


### [13] [Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis](https://arxiv.org/abs/2601.00925)
*I-Hsien Ting,Yi-Jun Tseng,Yu-Sheng Lin*

Main category: cs.CV

TL;DR: 研究提出一种无需造影剂的CT图像肺栓塞自动分类方法，基于3D卷积神经网络，在非增强CT上达到约85%准确率与0.84 AUC，显示可行性。


<details>
  <summary>Details</summary>
Motivation: 对PE的及时诊断能显著降低死亡率；现有依赖造影CTPA的深度学习方法存在肾损伤风险与时间延迟，尤其对合并慢性肾病或急性PE患者不利，因此需要在非造影CT上实现自动诊断。

Method: 构建并训练一个3D卷积神经网络，对无造影剂的CT体数据进行端到端分类以判别是否存在肺动脉栓塞。未提供数据规模、预处理、分割或训练细节。

Result: 模型在非增强CT上的分类性能为准确率85%、AUC 0.84，显示对PE检测具有较好区分能力。

Conclusion: 在非增强CT上使用3D CNN进行PE自动分类是可行的，可为不能使用造影剂或需快速决策的患者提供辅助；仍需更大规模、多中心验证与临床流程评估以确认鲁棒性与泛化能力。

Abstract: Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.
  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.

</details>


### [14] [Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store](https://arxiv.org/abs/2601.00928)
*Luis Yoichi Morales,Francesco Zanlungo,David M. Woollard*

Main category: cs.CV

TL;DR: 研究提出一种从零售店内顾客轨迹中自动识别“货架访问”（浏览行为）的算法，经两套独立数据（8138与15129条轨迹、不同门店）标注校准，并在同店/跨店留出集上验证具备泛化识别浏览行为能力，进一步分析浏览模式与购买行为关系，并探讨其在零售规划与人机交互中的应用。


<details>
  <summary>Details</summary>
Motivation: 零售场景将机器人部署为面向客户的服务者面临理解顾客意图的挑战。若能自动识别顾客在货架前的浏览行为，就能为自主决策（如引导、补货、陈列优化）提供关键语义信号。

Method: 基于顶视摄像头与机器视觉3D跟踪获取顾客轨迹；提出“货架访问”计算算法，从连续轨迹中检测顾客在货架区域的停留/交互片段。采用两个相互独立、不同门店的数据集进行人工标注与模型校准，并在同域与跨域留出集上评估。随后用校准模型对大规模轨迹挖掘浏览模式，并关联交易数据分析浏览—购买关系。

Result: 算法在同店与跨店测试中均能较好识别浏览活动，表明具备跨环境泛化能力；在大规模数据上得出有意义的浏览模式统计，并发现浏览行为与实际购买存在相关性。

Conclusion: 基于轨迹的货架浏览识别为理解顾客意图提供了可迁移的基础能力，可支持零售规划（如陈列与运营优化）与人机交互（机器人何时何地介入）的应用前景。

Abstract: Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.

</details>


### [15] [ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery](https://arxiv.org/abs/2601.00939)
*Feng Luo,Hongbo Pan,Xiang Yang,Baoyu Jiang,Fengqing Liu,Tao Huang*

Main category: cs.CV

TL;DR: 提出ShadowGS：在多时相卫星影像中基于3D Gaussian Splatting，通过物理遥感成像方程与高效ray marching显式建模几何一致的阴影，解耦光照与表观属性，并引入阴影一致性约束与先验，提高稀视角下的重建与新视图质量。


<details>
  <summary>Details</summary>
Motivation: 多时相卫星图像因太阳高度与方位变化导致阴影时变、位置/长度不一致，传统3DGS或NeRF在此场景下易将阴影当作表观纹理而非可预测的几何投影，造成几何-辐射耦合、重建失真与新视图伪影，尤其在稀视角与不同传感器（RGB/全色融合）条件下更明显。需要一种既高效又物理一致的方式来建模阴影与光照，以提升几何精度与跨时相鲁棒性。

Method: 在3DGS框架中引入遥感物理渲染方程，显式分解直射光、天空光/环境光与物体本征反射；结合高效ray marching在高斯体素间传播可见性与遮挡，几何一致地产生阴影；设计阴影一致性约束，将多时相阴影与场景几何联动，抑制把阴影当作纹理的错误拟合；提出阴影图先验，在稀视角/弱监督时提供阴影可见性引导；整体保持分钟级训练效率，适配RGB、融合（pansharpened）与稀视角输入。

Result: 相较现有SOTA，ShadowGS在阴影解耦准确度、3D重建几何精度与新视图合成质量上全面提升；在多数据设置（RGB、全色融合、稀视角）下鲁棒；训练仅需数分钟即可取得优于基线的结果。

Conclusion: 物理驱动的阴影建模与3DGS结合可在多时相卫星影像中实现几何一致的阴影生成与光照-表观解耦，通过阴影一致性约束与阴影图先验显著增强几何重建与渲染质量，并在效率与泛化上兼顾，适合实际遥感应用。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.

</details>


### [16] [Learning to Segment Liquids in Real-world Images](https://arxiv.org/abs/2601.00940)
*Jonas Li,Michelle Li,Luke Liu,Heng Fan*

Main category: cs.CV

TL;DR: 提出液体语义分割的新数据集与模型：构建含14类、5000张真实图像的LQDS，并提出带边界-主干跨注意力的LQDM，在LQDS测试集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实世界中水、酒、药液等液体无处不在，机器人安全交互与规避需感知液体。但液体外观多样、形状可变，且常透明或高反射，容易与背景/环境混淆，现有研究与数据匮乏，导致分割困难。

Method: 1) 数据集：采集并精标5000张真实场景图像，定义14种液体类别，构成大规模液体分割数据集LQDS。2) 模型：提出LQDM，包含主分割分支与专门的边界分支；通过两分支间的跨注意力机制，将边界信息注入主分支以强化预测边缘与细节，从而提升液体区域辨识。3) 评估：与多种SOTA分割方法对比实验。

Result: 在LQDS测试集上，LQDM的分割性能全面优于现有最先进方法，建立强基线；实验表明跨注意力与边界分支设计能显著提升液体分割精度。

Conclusion: 构建了首个大规模、多类别的液体分割数据集LQDS，并提出跨注意力边界增强的LQDM，有效缓解液体透明/反射与形变带来的分割难题，为机器人等场景中的液体感知提供强基线与新方法。

Abstract: Different types of liquids such as water, wine and medicine appear in all aspects of daily life. However, limited attention has been given to the task, hindering the ability of robots to avoid or interact with liquids safely. The segmentation of liquids is difficult because liquids come in diverse appearances and shapes; moreover, they can be both transparent or reflective, taking on arbitrary objects and scenes from the background or surroundings. To take on this challenge, we construct a large-scale dataset of liquids named LQDS consisting of 5000 real-world images annotated into 14 distinct classes, and design a novel liquid detection model named LQDM, which leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions. Extensive experiments demonstrate the effectiveness of LQDM on the test set of LQDS, outperforming state-of-the-art methods and establishing a strong baseline for the semantic segmentation of liquids.

</details>


### [17] [PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education](https://arxiv.org/abs/2601.00943)
*Megha Mariam K. M,Aditya Arun,Zakaria Laskar,C. V. Jawahar*

Main category: cs.CV

TL;DR: 提出首个面向物理教育的文本生成视频(T2V)解释性视频基准，用以评估模型在视觉上讲解物理概念的准确性；结果显示当前模型画面流畅但概念正确率不足，尤其在电磁与热学上薄弱。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型可生成高质量视频，但缺乏系统性评价其在教育场景、尤其是物理概念讲解中的有效性与对课程大纲的一致性；需要一个细粒度、可量化的基准来衡量“视觉质量—概念正确性”的差距。

Method: 构建物理教育基准：将核心物理概念拆解为细粒度教学要点；为每个要点设计用于视觉解释的精心提示词；用T2V模型生成视频并评估其对教学要点的概念准确性与视觉质量；覆盖力学、流体、光学、电磁与热学等领域；公开代码与数据。

Result: 当前T2V视频在视觉一致性、运动平滑与闪烁控制方面表现良好；在概念准确性上不稳定。力学、流体、光学相对可行；电磁与热学因抽象交互难以可视化而表现较差，暴露了视觉质量与概念正确性之间的鸿沟。

Conclusion: 该基准为评测与推动教育向的T2V提供了标准化平台，呼吁社区聚焦提升概念正确性与课程对齐能力，尤其在抽象领域；公开资源将促进研发更可靠、可规模化的物理教育视频生成系统。

Abstract: Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.

</details>


### [18] [Deep Clustering with Associative Memories](https://arxiv.org/abs/2601.00963)
*Bishwajit Saha,Dmitry Krotov,Mohammed J. Zaki,Parikshit Ram*

Main category: cs.CV

TL;DR: 提出DCAM：基于能量的关联记忆损失，将表示学习与聚类在同一目标中紧密耦合，在多种网络与模态上提升聚类质量。


<details>
  <summary>Details</summary>
Motivation: 深度聚类常需把可微的表示学习与离散的聚类通过近似/正则硬拼在一条可微链路中，二者割裂，影响性能与稳健性。需要一种能在端到端训练中自然兼容聚类离散性的机制。

Method: 引入能量式动态（Associative Memories）构造新的损失函数，将聚类作为能量极小化过程嵌入网络训练，以单一目标联合优化表征与簇分配；方法适配多种架构（卷积、残差、全连接）与模态（图像、文本）。

Result: 在多数据模态与网络架构上，DCAM相较基线取得更高的聚类质量指标（未给出具体数值）。

Conclusion: 通过能量驱动的关联记忆损失，DCAM更紧密地联结表示学习与聚类，避免以往不一致的近似，带来稳定且更优的聚类表现。

Abstract: Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).

</details>


### [19] [A Deep Learning Approach for Automated Skin Lesion Diagnosis with Explainable AI](https://arxiv.org/abs/2601.00964)
*Md. Maksudul Haque,Rahnuma Akter,A S M Ahsanul Sarkar Akib,Abdul Hasib*

Main category: cs.CV

TL;DR: 提出一种用于HAM10000多类皮肤病变分类的深度学习系统，集成数据均衡、强数据增强、带通道注意力的EfficientNetV2-L与三阶段渐进学习，并用Grad-CAM与显著性图解释模型；在七类上取得91.15%准确率、85.45%宏F1、99.33%微AUC，尤其对黑色素瘤与痣表现突出。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌常见且致命，临床需要快速、准确且可解释的自动化诊断方法；现有方法在类间不平衡、泛化与可解释性上存在不足。

Method: - 使用HAM10000数据集
- 采用高质量数据重采样/均衡策略与大规模数据增强
- 以EfficientNetV2-L为骨干并融合通道注意力机制
- 三阶段渐进式学习（可能包括分辨率/难度/冻结层逐步解冻）
- 使用XAI（Grad-CAM、Saliency）生成可视化解释

Result: 在七类病变上总体准确率91.15%，宏平均F1为85.45%，微平均AUC为99.33%；黑色素瘤与黑素细胞痣类别表现尤佳。

Conclusion: 该混合架构与训练策略有效提升多类皮肤病变分类性能，并通过XAI提高诊断透明性与临床可信度，表明其在临床辅助诊断中具备潜在应用价值。

Abstract: Skin cancer is also one of the most common and dangerous types of cancer in the world that requires timely and precise diagnosis. In this paper, a deep-learning architecture of the multi-class skin lesion classification on the HAM10000 dataset will be described. The system suggested combines high-quality data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Moreover, we also use explainable AI (XAI) techniques such as Grad-CAM and saliency maps to come up with intelligible visual representations of model predictions. Our strategy is with a total accuracy of 91.15 per cent, macro F1 of 85.45\% and micro-average AUC of 99.33\%. The model has shown high performance in all the seven lesion classes with specific high performance of melanoma and melanocytic nevi. In addition to enhancing diagnostic transparency, XAI also helps to find out the visual characteristics that cause the classifications, which enhances clinical trustworthiness.

</details>


### [20] [Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss](https://arxiv.org/abs/2601.00988)
*Lin Xi,Yingliang Ma,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 提出FSVOS：用方向驱动的局部匹配采样替代im2col/特制CUDA算子，实现高效、可移植的局部搜索；结合监督的时空对比学习与新X射线血管造影多目标数据集MOSXAV，跨CADICA、XACV、MOSXAV在已见/未见类上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频目标分割中的局部匹配常依赖卷积或特制CUDA内核（变形卷积、邻域注意力等），在效率、灵活性与跨设备可移植性上受限；同时医学X光血管造影缺少高质量、多目标分割数据集，且跨帧特征一致性不足。

Method: 1) 提出方向式局部采样：非参数化、可动态变化的采样区域，通过方向引导限制到最相关邻域，避免im2col类实现与硬件专用内核；2) 监督的时空对比学习，强化跨帧特征一致性；3) 构建MOSXAV数据集，提供精细人工标注的多目标分割真值。

Result: 在CADICA、XACV、MOSXAV三数据集上，FSVOS在分割精度与泛化（已见/未见类别）方面均超越现有SOTA。

Conclusion: 方向式非参数局部采样+时空对比学习提供了高效、可移植且对多样空间结构自适应的FSVOS框架，并在临床应用场景具备更强的实用潜力；MOSXAV填补了X射线血管造影多目标分割基准的空缺。

Abstract: We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.

</details>


### [21] [UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data](https://arxiv.org/abs/2601.00991)
*Joshua Kawaguchi,Saad Manzur,Emily Gao Wang,Maitreyi Sinha,Bryan Vela,Yunxi Wang,Brandon Vela,Wayne B. Hayes*

Main category: cs.CV

TL;DR: 提出UnrealPose-Gen管线与UnrealPose-1M数据集，面向高质量三维人体姿态数据的生成与评测，涵盖标注齐全、多视角、多动作，并给出在多任务上的实证验证。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的高质量、标注准确的3D人体姿态数据获取昂贵且受限于棚内环境；野外场景数据缺乏可靠的地面真值，限制了训练与评测。需要一种可扩展、逼真且标注完备的合成数据途径来弥补缺口。

Method: 基于Unreal Engine 5 和 Movie Render Queue 构建UnrealPose-Gen离线渲染管线，生成包含3D关节（世界/相机坐标）、2D投影与COCO关键点（含遮挡/可见性标记）、行人框、以及相机内外参的高保真帧；据此合成UnrealPose-1M约百万帧数据，包含8个序列：5个“连贯”脚本化序列（5个场景、约40个动作、5名受试者）与3个随机化序列（3个场景、约100个动作、5名受试者），并使用多样相机轨迹覆盖广视角。

Result: 发布UnrealPose-1M与生成管线；以四项任务进行真实性—泛化检验：图像到3D姿态、2D关键点检测、2D到3D提升、行人检测/分割，报告了合成到真实的效果，证明数据在多任务中具有实用价值。

Conclusion: UnrealPose-Gen提供可复用的高保真合成数据生成能力，UnrealPose-1M为研究者提供大规模、标注完备的人体姿态数据源；尽管规模受限，但开放数据与管线可支持社区进一步扩展与研究。

Abstract: Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted "coherent" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.

</details>


### [22] [WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift](https://arxiv.org/abs/2601.00993)
*Julian D. Santamaria,Claudia Isaza,Jhony H. Giraldo*

Main category: cs.CV

TL;DR: 提出WildIng，通过融合文本描述与图像特征来提升相机陷阱野生动物识别在跨地域（地理域迁移）场景下的泛化；相较基础模型在域外显著掉点（如CLIP适配器非洲→美洲84.77%→16.17%），WildIng在跨域上可为BioCLIP等带来约30%精度提升，并在美洲与非洲数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习/基础模型在同域表现强，但对地理分布偏移（背景、光照、场景差异）敏感，域外性能剧烈下降；原因在于过度依赖图像表征而忽视能跨域稳定的语义信息。

Method: 构建WildIng：将文本描述（物种外观等稳定语义）与图像特征融合，形成对地理域偏移更不敏感的联合表征（基于视觉-语言范式，如在CLIP/BioCLIP框架上集成文本侧先验/描述；具体实现细节未详述，但核心是多模态对齐与适配）。

Result: 在跨域（非洲训练→美洲测试或反向）的设置下，WildIng显著提升基础模型性能；示例：CLIP+adapter在同域可达84.77%，跨域仅16.17%；采用WildIng思路，对BioCLIP等在地理域迁移下带来约30%的准确率提升。

Conclusion: 引入文本描述以构建域不变语义，有效缓解相机陷阱图像在跨地域迁移中的性能崩塌；WildIng在美洲与非洲数据集上验证有效并开源，表明多模态语义融合是改进野生动物识别跨域泛化的可行方向。

Abstract: Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.

</details>


### [23] [DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models](https://arxiv.org/abs/2601.00998)
*Yue Zhou,Jue Chen,Zilun Zhang,Penghui Huang,Ran Ding,Zhentao Zou,PengFei Gao,Yuchen Wei,Ke Li,Xue Yang,Xue Jiang,Hongxin Yang,Jonathan Li*

Main category: cs.CV

TL;DR: 提出DVGBench无人机遥感隐式视觉指代基准，并基于此构建引入I2E-CoT与强化学习的DroneVG-R1，显著提升隐式指代的定位能力，并揭示主流LVLM在隐式推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有遥感VG数据集多依赖显式线索（相对位置、大小、颜色），难以覆盖需要场景知识的隐式指代，导致LVLM在无人机场景中推理受限，缺乏系统性评测与针对性方法。

Method: 1) 构建DVGBench：涵盖交通、灾害、安全、体育、社会活动、生产活动六大无人机场景，每个目标同时提供显式与隐式查询；2) 设计DroneVG-R1：在LVLM中引入“隐式到显式”的链式思维（I2E-CoT），结合强化学习，使模型先将隐式线索转化为可操作的显式描述，再进行定位；3) 系统评测主流模型在显式/隐式VG上的表现。

Result: DVGBench揭示主流LVLM在隐式VG上显著劣于显式VG，存在明显推理短板；DroneVG-R1在隐式指代任务上取得领先表现，降低了隐式到定位的难度。

Conclusion: 隐式指代需要场景专知与分步推理。通过I2E-CoT与RL可有效将隐式线索外化为显式约束，从而提升无人机遥感VG能力。DVGBench为该方向提供了标准化评测与数据基础。

Abstract: Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench

</details>


### [24] [Lightweight Channel Attention for Efficient CNNs](https://arxiv.org/abs/2601.01002)
*Prem Babu Kanaparthi,Tulasi Venkata Sri Varshini Padamata*

Main category: cs.CV

TL;DR: 对比SE、ECA与新提出的LCA通道注意力，在ResNet-18与MobileNetV2上（CIFAR-10）做实证；LCA用自适应1D分组卷积，参数省、速度好、精度接近/优于现有方法，并给出FLOPs/参数/延迟等全面基准。


<details>
  <summary>Details</summary>
Motivation: 通道注意力能以很小代价提升CNN性能，但不同设计在“精度—效率”权衡上的差异缺少系统量化与实证指导，尤其是在资源受限部署场景。

Method: 在ResNet-18与MobileNetV2上嵌入三种通道注意力：SE、ECA与提出的LCA。LCA采用自适应的一维卷积并使用分组操作以降低参数与计算；在CIFAR-10上统一训练与评测，并报告精度、参数量、FLOPs及GPU推理延迟。

Result: LCA在ResNet-18上达94.68%准确率、在MobileNetV2上达93.10%，参数效率与ECA相当，推理延迟保持有利；提供了包含FLOPs、参数量与GPU延迟的全面对比。

Conclusion: LCA在不增加显著开销的情况下实现与ECA相当的参数效率和竞争性精度，适合资源受限环境中的注意力增强CNN部署；文中基准为模型选择提供实践参考。

Abstract: Attention mechanisms have become integral to modern convolutional neural networks (CNNs), delivering notable performance improvements with minimal computational overhead. However, the efficiency accuracy trade off of different channel attention designs remains underexplored. This work presents an empirical study comparing Squeeze and Excitation (SE), Efficient Channel Attention (ECA), and a proposed Lite Channel Attention (LCA) module across ResNet 18 and MobileNetV2 architectures on CIFAR 10. LCA employs adaptive one dimensional convolutions with grouped operations to reduce parameter usage while preserving effective attention behavior. Experimental results show that LCA achieves competitive accuracy, reaching 94.68 percent on ResNet 18 and 93.10 percent on MobileNetV2, while matching ECA in parameter efficiency and maintaining favorable inference latency. Comprehensive benchmarks including FLOPs, parameter counts, and GPU latency measurements are provided, offering practical insights for deploying attention enhanced CNNs in resource constrained environments.

</details>


### [25] [Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2601.01022)
*Shiao Wang,Xiao Wang,Haonan Zhao,Jiarui Xu,Bo Jiang,Lin Zhu,Xin Zhao,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出一种在频域早期融合且运动引导稀疏化的RGB-Event目标跟踪框架：在FFT域用幅度/相位注意选择性注入事件的高频信息，并用运动引导的空间稀疏化筛除低信息区域，只将稀疏且目标相关的特征送入骨干，兼顾精度与效率，在FE108、FELT、COESOT上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-Event跟踪多为特征级后期融合，未充分利用事件相机的高动态范围与对运动敏感的优势；对低信息区域一视同仁导致骨干网络冗余计算，限制了效率与鲁棒性。

Method: 1) 频域早期融合：对RGB与事件数据做FFT，解耦幅度与相位；通过幅度与相位注意，将事件模态的高频信息有选择地融合到RGB表示中，以增强边缘/纹理与运动相关细节，同时减少骨干计算。2) 运动引导空间稀疏化：利用事件对运动的敏感性，建立目标运动线索与空间概率分布的关系，过滤低信息区域，保留目标相关区域形成稀疏特征集。3) 仅将稀疏的目标相关特征送入骨干学习，最终由跟踪头预测目标位置。

Result: 在FE108、FELT、COESOT三大RGB-Event跟踪基准上取得高性能与高效率（文中称显著优于对比方法且计算量明显下降）。

Conclusion: 频域的幅度/相位选择性交融与运动引导稀疏化能更好发挥事件相机优势，同时降低冗余计算，实现精确且高效的RGB-Event跟踪；代码将于GitHub开源。

Abstract: Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking

</details>


### [26] [ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval](https://arxiv.org/abs/2601.01024)
*Tien-Huy Nguyen,Huu-Loc Tran,Thanh Duc Ngo*

Main category: cs.CV

TL;DR: 提出ITSELF：一种利用模型自注意力进行隐式局部对齐的VLM框架，用GRAB将注意力转为高显著token库，并配合MARS与ATS实现稳健且非冗余的选择与逐步收缩的训练调度，在TBPS上达SOTA与强泛化。


<details>
  <summary>Details</summary>
Motivation: TBPS需要细粒度图文对齐以区分个体，现有局部对齐方法易走捷径、受伪相关影响且外加先验可能破坏模态内结构。作者观察到编码器注意力在早期训练已能定位空间证据，遂利用其引导学习以缓解误对齐与先验引入的问题。

Method: 提出ITSELF框架：1) GRAB将模型自注意力转化为“Attentive Bank”高显著token集合，并在其上施加局部目标以学习细粒度对应，无需额外监督；2) MARS跨层聚合注意力并进行考虑多样性的top-k选择，提升选择的稳定性与非冗余性；3) ATS在训练中自适应地从粗到细调度token保留预算，先保留上下文、后聚焦判别细节。

Result: 在三个主流TBPS数据集上取得SOTA表现，并表现出强跨数据集泛化能力，验证了方法的有效性与鲁棒性。

Conclusion: 利用模型自身注意力进行隐式局部对齐可在无需外部先验监督的前提下提升TBPS的细粒度图文匹配与泛化能力；GRAB+MARS+ATS的组合实现了稳定、去冗余且逐步聚焦的训练过程。

Abstract: Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself

</details>


### [27] [Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation](https://arxiv.org/abs/2601.01026)
*Douglas Costa Braga,Daniel Oliveira Dantas*

Main category: cs.CV

TL;DR: 提出一套可复现实验与工程规范的深度学习流水线，用注意力增强的EfficientNetV2-B3对ALL白血病细胞进行自动分类，在C-NMC 2019上以更少参数取得SOTA级精度并给出可解释性热力图。


<details>
  <summary>Details</summary>
Motivation: 显微镜下ALL诊断依赖经验、存在观察者间差异且耗时；现有方法在数据不均衡、评估不严谨与可复现性不足方面存在缺陷，需一种兼顾准确性、稳健性、效率与可解释性的方案以利临床落地。

Method: 构建注意力卷积网络：以EfficientNetV2-B3为骨干，加入Squeeze-and-Excitation注意力；采用全面数据增强、focal loss缓解类不平衡；患者级划分保证独立性；进行100次蒙特卡洛重复实验检验稳健性；与基线（含VGG16）对比并提供注意力可视化。

Result: 在C-NMC 2019（12,528图像/62名患者）上，测试集F1=97.89%、Accuracy=97.89%；相较现有方法最高提升4.67%；参数量15.2M，仅为VGG16的11%（少89%）；统计检验p<0.001显示显著优于基线。

Conclusion: 现代注意力增强架构在保持计算高效的同时可显著提升ALL细胞分类性能，并通过可视化提供可解释性；该可复现流水线具备临床部署潜力。

Abstract: We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.

</details>


### [28] [Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising](https://arxiv.org/abs/2601.01036)
*Kiet Dang Vu,Trung Thai Tran,Kien Nguyen Do Trung,Duc Dung Nguyen*

Main category: cs.CV

TL;DR: 提出Mono3DV：在DETR范式下，将3D信息纳入匹配并通过去噪与变分去噪稳定训练，实现KITTI单目3D检测SOTA。


<details>
  <summary>Details</summary>
Motivation: DETR类方法常用2D属性做匈牙利匹配，单目3D估计病态导致若纳入3D会不稳定，结果是高质量3D预测可能被2D准则错配与抑制，性能受限。

Method: 1) 3D-Aware Bipartite Matching：在匹配代价中显式加入3D几何（如深度/尺寸/姿态/3D IoU等），缓解2D-3D错位。
2) 3D-DeNoising：训练期对查询/目标注入受控3D扰动，作为去噪任务稳住整合3D属性时的训练。
3) Variational Query DeNoising：为解决传统去噪梯度消失，引入变分式查询噪声/先验与采样机制，提升梯度信号与收敛效果。

Result: 在未使用外部数据的前提下，于KITTI单目3D目标检测上达到SOTA（具体数值未给出）。

Conclusion: 把3D几何纳入DETR匹配并通过（变分）去噪稳定训练，可显著提升单目3D检测效果；方法简单通用、无需外部数据。

Abstract: While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.

</details>


### [29] [Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking](https://arxiv.org/abs/2601.01041)
*Xiang Zhang,Wenliang Weng,Daoyong Fu,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出MASM方法：将预训练权重分解为语义主子空间与多个人工痕迹子空间，通过选择性层掩码和正交/谱一致性约束，实现对多样伪造伪迹的解耦建模与稳健泛化。


<details>
  <summary>Details</summary>
Motivation: 跨数据集与真实场景中，深度伪造的伪迹分布多样，现有方法全局更新或额外监督导致语义结构被破坏、过拟合某一伪迹，泛化差。需要在保持通用语义的同时，分别建模多种伪迹。

Method: 对预训练模型权重做SVD，划分为稳定的语义主子空间与可学习的多个人工伪迹子空间；引入选择性层掩码，根据各伪迹子空间学习状态自适应控制层更新；对多个伪迹子空间施加正交约束与谱一致性约束，鼓励互补、多样且保持整体谱结构稳定。

Result: 在跨数据集设置下提升鲁棒性与泛化能力，能抑制对单一伪迹的过拟合，并保持语义表示稳定。（摘要未给出具体数字，但强调显著提升。）

Conclusion: 通过子空间解耦与自适应更新，MASM在不破坏通用语义的前提下有效覆盖多类伪迹，改进跨域深度伪造检测性能；方法具有可扩展性与对真实复杂场景的适应性。

Abstract: Deepfake detection still faces significant challenges in cross-dataset and real-world complex scenarios. The root cause lies in the high diversity of artifact distributions introduced by different forgery methods, while pretrained models tend to disrupt their original general semantic structures when adapting to new artifacts. Existing approaches usually rely on indiscriminate global parameter updates or introduce additional supervision signals, making it difficult to effectively model diverse forgery artifacts while preserving semantic stability. To address these issues, this paper proposes a deepfake detection method based on Multi-Artifact Subspaces and selective layer masks (MASM), which explicitly decouples semantic representations from artifact representations and constrains the fitting strength of artifact subspaces, thereby improving generalization robustness in cross-dataset scenarios. Specifically, MASM applies singular value decomposition to model weights, partitioning pretrained weights into a stable semantic principal subspace and multiple learnable artifact subspaces. This design enables decoupled modeling of different forgery artifact patterns while preserving the general semantic subspace. On this basis, a selective layer mask strategy is introduced to adaptively regulate the update behavior of corresponding network layers according to the learning state of each artifact subspace, suppressing overfitting to any single forgery characteristic. Furthermore, orthogonality constraints and spectral consistency constraints are imposed to jointly regularize multiple artifact subspaces, guiding them to learn complementary and diverse artifact representations while maintaining a stable overall spectral structure.

</details>


### [30] [Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data](https://arxiv.org/abs/2601.01044)
*Jin Wang,Angelo De Castro,Yuxi Zhang,Lucas Basolli Borsatto,Yuechen Guo,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 研究比较在奶牛体重预测中，跨场景迁移学习与不同数据模态（深度图像 vs 点云）的效果，发现从大场转移到小场显著提升小样本预测，且两种模态无一致优劣。


<details>
  <summary>Details</summary>
Motivation: 养牛场需要非侵入、可扩展的自动化监测以进行管理与健康评估。现有体重预测多用ImageNet/COCO预训练迁移，但在畜牧场景中其有效性与最佳微调策略尚不清楚，且深度图像与点云两种三维信息表征缺乏直接对比，尤其在跨牧场、数据受限的小场景下。

Method: 采集三个规模（大/中/小）牧场的俯视深度图与点云（大1201头、中215头、小58头）。评估四个深度学习模型：深度图像用ConvNeXt、MobileViT；点云用PointNet、DGCNN。在三种实验设计下比较：单源学习、联合学习、以及从大场到小场的迁移学习；衡量小场体重预测表现。

Result: 迁移学习在四个模型上均显著提升小场体重预测，相比单源学习更优，且效果与联合学习相当或更佳；预训练表征能跨不同成像条件与牛群差异良好泛化。深度图像与点云模型之间未观察到稳定的性能差异。

Conclusion: 对于因隐私、物流或政策限制而难以共享原始数据的小型牧场，仅共享预训练权重的迁移学习是一条有效路径；两种三维模态在整体性能上可替代选择，重点在于获得跨场泛化的预训练模型而非特定模态。

Abstract: Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.

</details>


### [31] [EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos](https://arxiv.org/abs/2601.01050)
*Hongming Fu,Wenjia Wang,Xiaozhen Qiao,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: EgoGrasp提出从头戴单目动态视频重建世界坐标系下的手-物体交互，结合多阶段预处理、解耦扩散先验与测试时多目标优化，显著提升在强相机运动与遮挡条件下的稳定性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有HOI方法多停留在单帧或相机坐标，难以捕捉时间一致性与全局轨迹；少数做世界坐标的工作忽视物体姿态与交互约束，且在剧烈相机运动与遮挡下表现不佳。需要一种能在自然场景下稳健重建世界坐标手-物体交互的方案，以支撑行为理解与VR/具身智能应用。

Method: 提出EgoGrasp：1) 多阶段鲁棒预处理，利用最新空间智能模型获取稳定的时空线索；2) 基于解耦扩散模型的全身HOI先验，模板无关、可扩展到多物体；3) 测试时多目标优化，联合约束手与物体的时序一致性、几何与交互物理，抵御强相机运动与遮挡。

Result: 在W-HOI重建任务上达到SOTA，较现有方法在世界坐标的手与物体轨迹、姿态和交互一致性上明显提升，尤其在剧烈相机运动与频繁遮挡的真实佩戴视频中表现优异。

Conclusion: EgoGrasp实现首个面向野外头戴单目视频的世界坐标手-物体交互重建，通过解耦扩散先验与测试时优化在复杂场景中取得稳健高精度结果，为人类行为理解与VR/具身智能提供关键能力。

Abstract: We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.

</details>


### [32] [Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance](https://arxiv.org/abs/2601.01056)
*Ifeanyi Ezuma,Ugochukwu Ugwu*

Main category: cs.CV

TL;DR: 在LC25000病理图像数据集上，对比微调的InceptionResNet‑v2作为端到端分类器与其深度特征驱动的传统ML/DL模型；深度特征方案整体更优，最高AUC达99.99%、准确率99.84%，且在噪声下更鲁棒（尤其GBM、KNN），HOG+深度特征在洁净数据上有益但对噪声敏感。


<details>
  <summary>Details</summary>
Motivation: 数字病理推动自动化图像分析在临床的重要性提升，需要评估主流深度网络与基于深度特征的传统机器学习在多类别病理分类任务上的准确性与鲁棒性，以指导实际部署。

Method: 以LC25000五分类病理切片数据集为基准：微调InceptionResNet‑v2用于端到端分类；并提取其深度特征训练多种模型（如神经网络、GBM、KNN等）；同时考察HOG与深度特征的融合；在不同信噪比（SNR）条件下评估鲁棒性，比较准确率与AUC。

Result: 微调InceptionResNet‑v2端到端准确率96.01%，平均AUC 96.8%；使用其深度特征训练的模型整体优于端到端网络，其中神经网络获得AUC 99.99%、准确率99.84%；在降低SNR的噪声条件下，基于深度特征的模型更稳健，GBM与KNN表现尤佳；HOG+深度特征在无噪时提升有限，在噪声下效果变差。

Conclusion: 在LC25000上，深度特征+传统/浅层模型可超越直接微调的大型CNN，并在噪声扰动下更鲁棒；部署时可优先考虑以强表征的深度特征为基础的下游分类器，谨慎使用HOG融合于噪声场景。

Abstract: The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\% and an average AUC of 96.8\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\% and accuracy of 99.84\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.

</details>


### [33] [Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers](https://arxiv.org/abs/2601.01064)
*Jianan Li,Wangcai Zhao,Tingfa Xu*

Main category: cs.CV

TL;DR: 提出LSST：以分而治之思路分别建模光谱与空间，结合SSTB（分组光谱自注意力+Spectrum Shuffle）与LSCB（深度可分离卷积），并引入动态加权的Focal Spectrum Loss，在更少FLOPs/参数下实现更优HSI压缩感知重建。


<details>
  <summary>Details</summary>
Motivation: HSI含有丰富光谱信息，但从压缩感知测量高效重建难：光谱与空间相关性不同且跨波段非局部关系复杂，现有方法要么计算/参数昂贵，要么未充分利用光谱结构，需一种轻量且能分别高效刻画光谱与空间关系的架构。

Method: 分而治之的网络LSST：1) 光谱支路SSTB：采用Grouped Spectral Self-attention以分组方式捕获局部与非局部光谱关系，并通过Spectrum Shuffle在组间重排增强跨组交互；2) 空间支路LSCB：使用深度可分离卷积与特定顺序设计强化空间特征提取、降计算量；3) 损失Focal Spectrum Loss：训练中对光谱上困难/复杂波段自适应赋予更高权重，动态调整以提升整体重建质量。

Result: 在多组实验中，LSST以更少的FLOPs与参数实现更优重建性能（定量指标优于对比方法），显示出高效与有效性。

Conclusion: 将光谱与空间建模解耦并采用轻量注意力/卷积结合，配合光谱焦点损失，可在HSI压缩感知重建中取得SOTA性能与更低计算成本；代码已开源以便复现与扩展。

Abstract: Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.

</details>


### [34] [A UAV-Based Multispectral and RGB Dataset for Multi-Stage Paddy Crop Monitoring in Indian Agricultural Fields](https://arxiv.org/abs/2601.01084)
*Adari Rama Sukanya,Puvvula Roopesh Naga Sri Sai,Kota Moses,Rimalapudi Sarvendranath*

Main category: cs.CV

TL;DR: 该论文构建并公开了一个覆盖水稻从育秧到收获阶段的超大规模UAV RGB+多光谱数据集（42,430张，415GB，1 cm/像素），附带完整元数据并经Pix4D验证，可用于病害检测、定量植被指数与产量估计等研究。


<details>
  <summary>Details</summary>
Motivation: 现有水稻遥感数据集分辨率、时相覆盖与元数据完整性不足，难以支撑针对印度本地品种与种植管理的精准农业应用（如定点喷洒、病虫害监测、产量评估）。

Method: 在印度安得拉邦Vijayawada区域5英亩稻田，按SOP与检查清单进行可重复的多时相航拍；使用20MP RGB相机与5MP四波段多光谱相机（R、G、Red-edge、NIR）采集；记录GPS、飞行高度与环境信息；用Pix4D Fields生成正射镶嵌与植被指数（NDVI、NDRE）图以进行数据有效性验证；整理并发布至IEEE DataPort。

Result: 获得42,430张原始图像（415GB），空间分辨率约1 cm/像素，覆盖水稻全生育期且含丰富元数据；成功生成高质量正射与NDVI/NDRE产品，验证数据可用性。

Conclusion: 该数据集在时空分辨率、光谱丰富度与元数据完备性方面具有稀缺性，可直接支持精准农业应用（定向喷洒、病害分析、产量估计）与相关算法研发；为印度稻作场景提供基准数据资源。

Abstract: We present a large-scale unmanned aerial vehicle (UAV)-based RGB and multispectral image dataset collected over paddy fields in the Vijayawada region, Andhra Pradesh, India, covering nursery to harvesting stages. We used a 20-megapixel RGB camera and a 5-megapixel four-band multispectral camera capturing red, green, red-edge, and near-infrared bands. Standardised operating procedure (SOP) and checklists were developed to ensure repeatable data acquisition. Our dataset comprises of 42,430 raw images (415 GB) captured over 5 acres with 1 cm/pixel ground sampling distance (GSD) with associated metadata such as GPS coordinates, flight altitude, and environmental conditions. Captured images were validated using Pix4D Fields to generate orthomosaic maps and vegetation index maps, such as normalised difference vegetation index (NDVI) and normalised difference red-edge (NDRE) index. Our dataset is one of the few datasets that provide high-resolution images with rich metadata that cover all growth stages of Indian paddy crops. The dataset is available on IEEE DataPort with DOI, . It can support studies on targeted spraying, disease analysis, and yield estimation.

</details>


### [35] [Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models](https://arxiv.org/abs/2601.01085)
*Jiayi Xu,Zhang Zhang,Yuanrui Zhang,Ruitao Chen,Yixian Xu,Tianyu He,Di He*

Main category: cs.CV

TL;DR: Luminark：无需训练、具概率认证的通用视觉生成模型水印。以补丁亮度阈值形成二值模式，检测用统计检验控假阳；通过“水印引导”作为通用指导机制即插即用，跨扩散/自回归/混合模型，保持画质并具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方案常需训练、依赖特定模型或缺乏可证伪阳控制与跨范式通用性。作者希望提供一种对任意生成模型友好的、无需再训练、并能在统计意义上认证检测可靠性的水印方法，同时尽量不牺牲图像质量和对常见变换的鲁棒性。

Method: 提出基于补丁级亮度统计的水印定义：服务方预设一个二值图案和对应的每补丁亮度阈值。生成或注入时，利用通用的“指导(guidance)”机制设计“水印引导”，使生成图像在不显著降质下满足这些阈值。检测时，对图像各补丁亮度与阈值比较得到二值串，并与目标图案匹配。通过简单统计分析给出可控的假阳率上界，实现概率认证。该机制可即插即用地应用于扩散、自回归、混合等框架。

Result: 在九个代表性模型上实证，Luminark在不显著影响视觉质量的情况下，获得高检测准确率；对常见图像变换具有强鲁棒性；并展现跨扩散、自回归与混合范式的一致有效性。

Conclusion: Luminark实现了无需训练、具概率认证与跨模型通用的水印方案。其基于补丁亮度阈值与指导机制的设计，在保持图像质量的同时提供可控假阳率与稳健检测，适合作为通用生成模型水印解决方案。

Abstract: In this paper, we introduce \emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.

</details>


### [36] [600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script](https://arxiv.org/abs/2601.01088)
*Haq Nawaz Malik*

Main category: cs.CV

TL;DR: 600K-KS-OCR 是面向克什米尔语（改良波斯-阿拉伯字母）的60万级合成词级图像数据集，含标准标注与多框架兼容格式，用于训练/评估OCR，带多字体、增强与背景多样化，CC-BY-4.0 开源。


<details>
  <summary>Details</summary>
Motivation: 克什米尔语属于资源稀缺、濒危的达尔德语支语言，现有OCR训练数据匮乏，严重制约模型训练、评测与应用落地，需要一个规模大、格式友好、贴近真实场景的公开数据集填补空白。

Method: 合成生成约602k词级图像（256x64），采用三种传统克什米尔字体；进行丰富的数据增强（模拟文档退化）与多样背景纹理以提升鲁棒性；提供与CRNN、TrOCR及通用ML流程兼容的转写标注；数据按十个压缩包分发，总体约10.6GB。

Result: 得到覆盖多字体与退化场景的高质量大规模OCR训练/评测数据，具备多框架即插即用的标注格式，便于快速实验复现与基准对比。

Conclusion: 该数据集为低资源语言OCR提供了关键基础设施，预计可显著提升克什米尔语OCR模型的性能与泛化，CC-BY-4.0 许可促进社区共享与后续研究扩展。

Abstract: This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.

</details>


### [37] [NarrativeTrack: Evaluating Video Language Models Beyond the Frame](https://arxiv.org/abs/2601.01095)
*Hyeonjeong Ha,Jinjin Ge,Bo Feng,Kaixin Ma,Gargi Chakraborty*

Main category: cs.CV

TL;DR: 提出NarrativeTrack基准，用CRP框架评估MLLM对视频叙事中实体的持续性、变化与歧义的理解，发现感知扎根与时间推理存在权衡，当前模型难以跨时间连续追踪实体且易幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准多聚焦静态图像、短片段或场景级语义，缺乏对“谁在何时何地做什么”的持续叙事理解评测；需要能检验实体在时序与情境变化下的连续表征与推理能力。

Method: 构建NarrativeTrack：将视频分解为实体，使用全自动实体中心管线提取具时间对齐的实体表征；提出组合推理进程（CRP）框架，沿三维度（实体存在、实体变化、实体歧义）逐步提高难度，从时间持续性到情境演化再到细粒度感知推理，对MLLM进行系统评估。

Result: 对最先进MLLM评估显示：模型在视觉转场与时间动态中难以稳健跟踪实体，情境变换时常出现身份幻觉；通用开源MLLM感知扎根强但时间一致性弱；视频专用MLLM能利用时间上下文但易对实体情境产生幻觉，揭示感知扎根与时间推理的权衡。

Conclusion: 叙事理解需要融合强感知扎根与时间推理；NarrativeTrack首次提供系统化、实体中心且时间对齐的评测框架，可用于诊断并推动MLLM在时序叙事理解上的进展。

Abstract: Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.

</details>


### [38] [Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks](https://arxiv.org/abs/2601.01099)
*Mahmudul Hasan,Mabsur Fatin Bin Hossain*

Main category: cs.CV

TL;DR: 比较自定义CNN与预训练/迁移学习模型在5个真实图像数据集（含二分类、细粒度多分类与目标检测）上的表现，揭示更深网络利于细粒度任务，轻量预训练模型适合简单二分类，并将自定义架构拓展到检测未授权三轮车以示适应性，为按任务复杂度与资源约束选型提供指南。


<details>
  <summary>Details</summary>
Motivation: 实际应用中任务类型多样、数据与算力受限，研究者需在自定义网络与预训练/迁移学习方案间做权衡。本研究动机是系统量化架构因素（深度、残差连接、特征提取策略）对分类与定位性能的影响，给出不同任务场景下的选择依据。

Method: 构建一个自定义CNN并与多种常用预训练与迁移学习CNN在五个真实数据集上对比；数据集覆盖二分类、细粒度多分类、以及将自定义架构扩展到目标检测场景。分析网络深度、残差连接、特征提取方式等对性能的影响，评估分类准确率与定位/检测效果。

Result: 更深的CNN在细粒度多分类数据集上带来显著性能提升；对简单的二分类任务，轻量级的预训练与迁移学习模型依然高效且性价比高；自定义架构可成功扩展至目标检测，用于在真实交通场景中识别未授权机动三轮车，表现出良好适应性。

Conclusion: 根据任务复杂度和资源约束选择网络：细粒度复杂任务倾向于更深/更复杂的架构；简单二分类优先考虑轻量预训练或迁移学习模型；定制架构具有可扩展性，可迁移到检测任务。研究为模型选型提供了实用指导。

Abstract: This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.

</details>


### [39] [Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization](https://arxiv.org/abs/2601.01103)
*Abhinav Attri,Rajeev Ranjan Dwivedi,Samiran Das,Vinod Kumar Kurmi*

Main category: cs.CV

TL;DR: HAQAGen 是一种用于近红外（NIR）到可见光（RGB）上色的统一生成模型，兼顾全局色彩真实与局部结构细节，并通过自适应分辨率推理实现高分辨率无损扩展，较SOTA在多数据集与多指标上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有 NIR->RGB 翻译常在两难间取舍：要么追求鲜艳自然的颜色但牺牲纹理与结构，要么保持结构却颜色失真；同时许多方法难以在高分辨率下稳定泛化。作者希望构建一个分辨率无关、兼顾全局色彩统计与局部色度一致、且能保留细节纹理的统一模型。

Method: 提出 HAQAGen，核心包含：1) 复合损失：可微直方图匹配以对齐全局色彩统计；感知质量度量与特征相似度以保持纹理与结构；2) 通过 SPADE 注入局部色调-饱和度先验，稳定色彩重建；3) 在 Mamba 主干中加入纹理感知监督强化细节；4) 自适应分辨率推理引擎，支持高分辨率翻译而不降质。

Result: 在 FANVID、OMSIV、VCIP2020、RGB2NIR 等数据集与多种评测指标下，优于多种 SOTA 基线。生成结果具备更锐利纹理和更自然色彩，感知指标显著提升。

Conclusion: HAQAGen 同时保证全局色彩统计与局部色度一致性，并能无缝扩展到原生分辨率而不牺牲纹理与泛化性，是跨多种成像场景的可扩展、有效的 NIR->RGB 翻译方案。

Abstract: We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature based similarity to preserve texture information, (ii) local hue-saturation priors injected via Spatially Adaptive Denormalization (SPADE) to stabilize chromatic reconstruction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consistency, while scaling to native resolutions without compromising texture fidelity or generalization. Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR using different evaluation metrics demonstrate consistent improvements over state-of-the-art baseline methods. HAQAGen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios. Project Page: https://rajeev-dw9.github.io/HAQAGen/

</details>


### [40] [Cross-Layer Attentive Feature Upsampling for Low-latency Semantic Segmentation](https://arxiv.org/abs/2601.01167)
*Tianheng Cheng,Xinggang Wang,Junchao Liao,Wenyu Liu*

Main category: cs.CV

TL;DR: 提出Guided Attentive Interpolation（GAI），通过引入空间与语义关系的注意力引导上采样，生成细粒度且富语义的高分辨率特征，实现低延迟语义分割，在Cityscapes与CamVid上取得新的SOTA权衡（78.8 mIoU@22.3 FPS；80.6 mIoU@64.5 FPS）。


<details>
  <summary>Details</summary>
Motivation: 传统双线性等坐标引导的低分辨率特征插值会造成高分辨率特征粗糙、存在特征错位且缺乏上下文；而直接在高分辨率上丰富语义又计算开销大，难以满足低时延推理需求。需要一种既对齐、富语义又高效的上采样机制。

Method: 提出GAI：从不同分辨率特征中自适应地学习像素间空间与语义关系，使用注意力引导将低分辨率语义特征投射到高分辨率，进行细粒度插值与融合；方法可作为通用模块嵌入任意CNN语义分割网络，构成GAIN。

Result: 在NVIDIA 1080Ti上，Cityscapes达78.8 mIoU与22.3 FPS，CamVid达80.6 mIoU与64.5 FPS，相比同类低延迟方法性能更优，刷新低时延语义分割SOTA。

Conclusion: GAI通过显式建模空间/语义关系实现对齐且富语义的高分辨率特征，上采样更精细、计算更高效，可无缝集成到现有网络，兼顾精度与速度，适合实时/近实时场景。

Abstract: Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of lowlatency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg.

</details>


### [41] [CardioMOD-Net: A Modal Decomposition-Neural Network Framework for Diagnosis and Prognosis of HFpEF from Echocardiography Cine Loops](https://arxiv.org/abs/2601.01176)
*Andrés Bell-Navas,Jesús Garicano-Mena,Antonella Ausiello,Soledad Le Clainche,María Villalba-Orero,Enrique Lara-Pezzi*

Main category: cs.CV

TL;DR: 提出CardioMOD-Net：基于标准小鼠超声心动图cine的统一AI框架，实现多分类HFpEF表型诊断与连续化发病时间预测。方法以HODMD提取时序特征，构建共享潜在表征，分别驱动ViT分类器与回归器。四类总体诊断准确率65%（各类>50%），误判多见于早期OB/SAH与对照重叠；HFpEF起病时间RMSE为21.72周，OB与SAH预测更准，群体分布与真实吻合。框架在小样本下可实现联合诊断-预后建模。


<details>
  <summary>Details</summary>
Motivation: HFpEF由多种共病驱动、早期潜伏期长，临床与前临床中早期诊断与预后困难。现有超声AI多为二分类检测，缺乏对具体共病表型的区分与对疾病进展时间的连续预测。需要一个能在标准cine数据上同时完成多分类表型识别与发病时间估计的统一框架，以支持前临床研究与转化。

Method: 收集四组小鼠（对照、糖代谢异常/高血糖、肥胖、系统性动脉高压）的二维胸骨旁长轴cine视频。用高阶动态模态分解（HODMD）从cine中提取时序动力学特征，形成共享潜在表示。基于该表示训练两个Vision Transformer：分类器用于四类诊断；回归器用于预测HFpEF起病年龄（时间到HFpEF）。

Result: 多分类总体准确率65%，四类均>50%；主要误分源于早期OB/SAH与对照的表型重叠。时间到HFpEF回归RMSE为21.72周，其中OB与SAH组预测最准确；各组预测的起病时间分布与真实分布高度匹配。

Conclusion: 单一标准cine即可在小样本条件下实现HFpEF多表型诊断与连续化起病时间预测。CardioMOD-Net为前临床HFpEF研究提供了集成诊断-预后的方法学基础，并为未来向临床转化与个体化监测提供了可能。

Abstract: Introduction: Heart failure with preserved ejection fraction (HFpEF) arises from diverse comorbidities and progresses through prolonged subclinical stages, making early diagnosis and prognosis difficult. Current echocardiography-based Artificial Intelligence (AI) models focus primarily on binary HFpEF detection in humans and do not provide comorbidity-specific phenotyping or temporal estimates of disease progression towards decompensation. We aimed to develop a unified AI framework, CardioMOD-Net, to perform multiclass diagnosis and continuous prediction of HFpEF onset directly from standard echocardiography cine loops in preclinical models.
  Methods: Mouse echocardiography videos from four groups were used: control (CTL), hyperglycaemic (HG), obesity (OB), and systemic arterial hypertension (SAH). Two-dimensional parasternal long-axis cine loops were decomposed using Higher Order Dynamic Mode Decomposition (HODMD) to extract temporal features for downstream analysis. A shared latent representation supported Vision Transformers, one for a classifier for diagnosis and another for a regression module for predicting the age at HFpEF onset.
  Results: Overall diagnostic accuracy across the four groups was 65%, with all classes exceeding 50% accuracy. Misclassifications primarily reflected early-stage overlap between OB or SAH and CTL. The prognostic module achieved a root-mean-square error of 21.72 weeks for time-to-HFpEF prediction, with OB and SAH showing the most accurate estimates. Predicted HFpEF onset closely matched true distributions in all groups.
  Discussion: This unified framework demonstrates that multiclass phenotyping and continuous HFpEF onset prediction can be obtained from a single cine loop, even under small-data conditions. The approach offers a foundation for integrating diagnostic and prognostic modelling in preclinical HFpEF research.

</details>


### [42] [GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation](https://arxiv.org/abs/2601.01181)
*Chenglizhao Chen,Shaojiang Yuan,Xiaoxue Lu,Mengke Song,Jia Song,Zhenyu Wu,Wenfeng Song,Shuai Li*

Main category: cs.CV

TL;DR: 提出GenCAMO体系：用生成模型合成高保真伪装场景的密集标注数据，构建多模态大规模数据集GenCAMO-DB，并显著提升RGB-D伪装检测与开放词汇伪装分割等CDP任务表现。


<details>
  <summary>Details</summary>
Motivation: CDP任务（如RGB-D伪装检测、开放词汇伪装分割）需要大量高质量、密集标注的数据，但真实伪装数据难采集且标注昂贵，现有数据规模与质量不足，限制模型在复杂伪装场景中的理解与推理能力。

Method: 1) 构建GenCAMO-DB：包含大规模、多模态密集标注（深度图、场景图、属性描述、文本提示）。2) 设计GenCAMO生成框架：面向环境、无需先验掩码，生成高保真伪装图像及一致的密集标注。3) 以合成数据作为训练补充，为CDP模型提供细粒度表征、先验与辅助推理信息，并在多模态设置下训练/评估。

Result: 在多种模态与任务上（RGB-D伪装检测、开放词汇伪装分割等），使用GenCAMO生成的数据显著提升了密集预测性能，相较仅用真实数据的基线有一致增益。

Conclusion: 利用环境感知、无掩码的生成模型可有效合成高质量伪装密集数据；配套的GenCAMO-DB为多模态CDP提供数据基础，实验验证其能显著增强复杂伪装场景下的密集预测能力。

Abstract: Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.

</details>


### [43] [Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors](https://arxiv.org/abs/2601.01192)
*Hao Lu,Xuhui Zhu,Wenjing Zhang,Yanan Li,Xiang Bai*

Main category: cs.CV

TL;DR: 该文提出用于视频个体计数（VIC）的新基线OMAN++，结合“群体社会性”与“时空位移”先验，通过一对多匹配与位移先验注入，显著提升在高拥挤场景的计数与对应性能，并发布武汉地铁拥挤数据集WuhanMetroCrowd。


<details>
  <summary>Details</summary>
Motivation: 现有VIC需在跨帧识别同一行人（对应问题），但在地铁通勤等高拥挤、强遮挡、外观变化大、速度变化快的场景下易失效；现有数据集与方法对拥挤动态流刻画不足，亟需更能反映真实人群流动的数据与更鲁棒的匹配策略。

Method: 1) 构建WuhanMetroCrowd数据集：涵盖稀疏到密集、短到长、慢到快、前后视角变化、轻到重遮挡等特性。2) 提出两大先验：a) 社会群组先验——行人倾向以群体移动；b) 时空位移先验——个体在短时内位移受限，不能“瞬移”。3) 设计OMAN++：a) 隐式上下文生成器与一对多（O2M）匹配器，放宽传统一对一（O2O）匹配以更好处理成团与遮挡；b) 位移先验注入器，约束匹配搜索空间并用于特征提取与训练中的正则/引导，增强稳健性。

Result: 在SenseCrowd、CroHD、MovingDroneCrowd上超过当前SOTA；在新数据集WuhanMetroCrowd上相较基线将误差降低38.12%，显示对拥挤场景的显著优势。

Conclusion: 通过把VIC重构为结合群体社会性与位移约束的对应问题，采用O2M匹配与位移先验注入，OMAN++在复杂拥挤场景实现了更稳健的个体计数与跨帧关联；同时新数据集为拥挤人群流研究提供了有力基准与资源。

Abstract: Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.

</details>


### [44] [MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity](https://arxiv.org/abs/2601.01200)
*Zhang Chen,Shuai Wan,Yuezhe Zhang,Siyu Ren,Fuzheng Yang,Junhui Hou*

Main category: cs.CV

TL;DR: 提出MS-ISSM：用RBF把点云局部特征连续化，以系数比对替代点到点匹配；并设计ResGrouped-MLP网络，将多尺度差异映射为主观质量分数，在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 点云无规则、无结构导致PCQA中难以建立可靠的感知特征对应，传统点到点/邻域匹配易引入误差，限制了质量评估的准确性与泛化。

Method: 1) 多尺度隐式结构相似度MS-ISSM：用径向基函数对局部（亮度、色度、几何）特征进行连续表示，将失真度量转化为隐式函数（RBF）系数的比较，避免不稳定的点级匹配；2) ResGrouped-MLP：分组编码保持luma/chroma/geometry物理语义，结合残差块与通道注意力，在高/中/低多尺度上自适应聚焦显著失真特征，并将多尺度差异映射到感知分数。

Result: 在多个基准数据集上，所提指标在相关性、鲁棒性与泛化性能上均优于当前SOTA方法。

Conclusion: 用RBF的隐式连续表征替代显式点匹配可显著降低错配带来的评估误差；结合分组残差与注意力的多尺度网络能更好对齐人类感知，提升PCQA的可靠性与泛化。

Abstract: The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.

</details>


### [45] [RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models](https://arxiv.org/abs/2601.01202)
*Jiazhu Dai,Huihui Jiang*

Main category: cs.CV

TL;DR: 提出RefSR-Adv：一种仅扰动参考图像即可显著降低参考超分辨(RefSR)输出质量的对抗攻击；在多架构与多数据集上有效，并揭示模型对参考特征的过度依赖是关键安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有RefSR研究多聚焦后门攻击与性能提升，较少系统性探讨针对RefSR的对抗攻击脆弱性；需要评估RefSR在仅参考图被扰动时的鲁棒性与安全风险。

Method: 设计RefSR-Adv：固定低分辨输入，仅对参考图施加对抗扰动，通过最大化干净输出与对抗输出之间差异的目标函数驱动；在CNN、Transformer、Mamba等RefSR框架上实施，并在CUFED5、WR-SR、DRefSR等数据集评测。

Result: 在多种网络与数据集上显著降低SR指标，产生明显伪影与纹理退化；攻击效果与低分辨输入和参考图的相似度正相关。

Conclusion: RefSR对参考特征高度敏感且存在安全漏洞；应重视RefSR鲁棒性，考虑降低对参考特征的过度依赖并引入防御机制。

Abstract: Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.

</details>


### [46] [XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression](https://arxiv.org/abs/2601.01204)
*Zunhai Su,Weihao Ye,Hansen Feng,Keyu Fan,Jing Zhang,Dahai Yu,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 提出XStreamVGGT：通过对KV缓存进行联合剪枝与量化，在几乎不损精度的前提下，将内存降4.42×、推理加速5.48×的流式3D几何重建方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的流式3D视觉模型（如StreamVGGT）依赖帧级因果注意力，随帧数增长KV缓存线性累积，导致内存和时延不断攀升，限制了大规模与在线应用的可扩展性。

Method: 在无需重新训练的前提下，对注意力的KV缓存进行系统性压缩：1）多视角冗余KV剪枝——通过高效的token重要性评估选择性保留关键KV，实现固定内存预算；2）KV量化——利用KV张量的分布特性进行低比特量化，进一步压缩存储与带宽开销。二者联合，形成调参-free的推理时KV管理策略。

Result: 在大量评测中，性能几乎不下降，同时内存占用降低约4.42×、推理速度提升约5.48×，在流式重建场景中表现稳定且高效。

Conclusion: XStreamVGGT在保持精度的同时大幅降低流式Transformer的KV缓存开销，解决了随时间增长的内存与延迟问题，为可扩展、实用的流式3D应用提供了高效方案；代码已开源。

Abstract: Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\times$ and accelerating inference by 5.48$\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.

</details>


### [47] [Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission](https://arxiv.org/abs/2601.01210)
*Kazuhiko Murasaki,Shunsuke Konagai,Masakatsu Aoki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: 提出一种面向沉浸式远程临场的高速度LiDAR稀疏点云致密化方法：融合多路LiDAR与高分辨率彩色图像，使用基于联合双边滤波思想的CNN，实现全高清、实时（30fps）深度补全，几何准确且无多视不一致与重影，比近期训练式方法快15倍以上。


<details>
  <summary>Details</summary>
Motivation: 沉浸式远程临场需要低时延的空间传输：既要密集捕获动态三维场景，又要实时处理。现有LiDAR虽可实时获取3D，但点云稀疏，直接用于重建会导致细节不足或需离线重建，无法满足低延迟。需要一种能边采集边致密化、保持实时性的深度补全方案。

Method: 融合多路LiDAR输入与高分辨率RGB图像，采用以联合双边滤波为核心思想的卷积神经网络架构，实现引导式深度补全；网络在彩色图像引导下对稀疏深度进行边缘保真的空间传播，生成全高清密集深度；整体强调低延迟实现与高吞吐。

Result: 在全高清分辨率下实现实时30fps的致密深度输出；相较近期基于训练的深度补全方法，速度提升超过15倍；生成的致密点云在几何上准确，且无多视不一致或重影伪影。

Conclusion: 所提方法满足沉浸式远程临场的低时延需求，在保证几何精度与视觉一致性的同时实现全高清实时深度补全，显著优于现有训练式方法的速度与部署实用性。

Abstract: To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.

</details>


### [48] [Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation](https://arxiv.org/abs/2601.01213)
*Riccardo Gelato,Carlo Sgaravatti,Jakob Grahn,Giacomo Boracchi,Filippo Maria Bianchi*

Main category: cs.CV

TL;DR: 将自然图像上的SAM迁移到Sentinel‑1 SAR，用适配器、多编码器、提示工程与高效训练策略，提升雪崩分割并加速标注。


<details>
  <summary>Details</summary>
Motivation: 雪崩分割与制图对风险预报与减灾关键，但SAR图像标注依赖专家、成本高且耗时。需要一种能降低标注负担、提高效率的模型与工具。

Method: 以SAM为基础，面向SAR的特性进行定制：1) 领域适配器缓解自然图像与SAR的域差异；2) 多编码器以处理多通道SAR输入，突破SAM仅RGB限制；3) 提示工程提升对小且低对比度雪崩的定位鲁棒性；4) 训练算法限制编码器训练时间与范围，降低计算开销。并将模型集成到交互式标注工具中。

Result: 实验表明，所提出的定制SAM在SAR雪崩分割任务上能更鲁棒地响应不精确提示，并显著加速SAR图像的标注过程。

Conclusion: 通过域适配、多通道输入处理、提示优化与高效训练，对SAM进行轻量级定制，可在保持性能的同时降低计算成本，实用地加速雪崩标注工作流程。

Abstract: Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.

</details>


### [49] [UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass](https://arxiv.org/abs/2601.01222)
*Mengfei Li,Peng Li,Zheng Zhang,Jiahao Lu,Chengfeng Zhao,Wei Xue,Qifeng Liu,Sida Peng,Wenxiao Zhang,Wenhan Luo,Yuan Liu,Yike Guo*

Main category: cs.CV

TL;DR: UniSH提出统一的前馈框架，实现度量尺度的三维场景与人体联合重建，通过蒸馏高频细节与两阶段监督，在单次前向中恢复场景几何、人点云、相机与一致的SMPL体，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 真实标注数据稀缺，训练依赖合成数据导致显著的仿真到真实域间隙，表现为泛化差、人体几何细节差、与野外视频对齐不佳，需要能有效利用无标注真实数据并统一场景与HMR先验的训练与推理框架。

Method: 统一的前馈模型，融合场景重建与人体重建（HMR）先验。核心包含：1）鲁棒蒸馏策略：从专家深度模型蒸馏高频人体表面细节以提升人体几何质量；2）两阶段监督：先用合成数据学习粗定位，再用真实数据微调，通过直接优化SMPL网格与人体点云的几何对应关系，获得度量尺度与对齐能力。

Result: 模型在单次前向同时恢复高保真场景几何、人体点云、相机参数与一致的度量尺度SMPL体；在人-场景重建上达SOTA，并在人体全局运动估计上与优化式与仅HMR方法相比具有强竞争力。

Conclusion: 通过蒸馏与两阶段监督有效缩小仿真到真实域间隙，UniSH实现高保真、度量尺度的一体化人-场景重建与运动估计，并在多任务上取得领先或可比表现。

Abstract: We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/

</details>


### [50] [Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment](https://arxiv.org/abs/2601.01224)
*Bac Nguyen,Yuhta Takida,Naoki Murata,Chieh-Hsin Lai,Toshimitsu Uesaka,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出CODA，在预训练扩散+Slot Attention框架中引入寄存器槽与对比对齐损失，显著缓解槽纠缠并加强槽-图像对应，在合成与真实数据集上提升对象发现、属性预测与组合生成效果，且开销小、可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的SA虽能进行对象中心学习，但存在槽纠缠、槽与图像内容对齐弱的问题，限制了对象发现质量与下游能力。需要一种高效、可扩展的方法增强槽-输入的互信息与表示纯度。

Method: 在SA+预训练扩散模型框架上做两点改动：(1) 引入“寄存器槽”（register slots）吸收残余注意力，减少对象槽间干扰；(2) 设计对比式对齐损失，显式鼓励每个对象槽与对应图像区域对齐。整体目标可视为最大化槽与输入之间互信息的可处理近似。

Result: 在MOVi-C/E、VOC、COCO等数据集上，相比强基线，CODA在对象发现（如COCO上FG-ARI提升6.1%）、属性预测及组合式图像生成上均有提升；寄存器槽带来可忽略的计算与内存开销，方法高效可扩展。

Conclusion: CODA通过寄存器槽与对比对齐显著增强槽表示质量和槽-图像对应，在复杂真实场景中实现稳健的对象中心学习，具备实用潜力。

Abstract: Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.

</details>


### [51] [HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training](https://arxiv.org/abs/2601.01228)
*Markus Haltmeier,Lukas Neumann,Nadja Gruber,Johannes Schwab,Gyeongha Hwang*

Main category: cs.CV

TL;DR: 提出HyDRA：一种仅用测量数据训练DEQ的重建框架，通过测量一致性+自适应去噪正则与数据驱动早停，在稀疏视角CT上实现有竞争力的重建质量与快速推理。


<details>
  <summary>Details</summary>
Motivation: 传统图像重建A x = y病态且难以获得大规模标注对(x,y)，DEQ虽强但依赖监督样本。许多实际场景只有测量y可用，亟需无GT、仅测量的高效训练方法。

Method: 构建Hybrid Denoising Regularization Adaptation：在DEQ固定点训练中引入两部分损失——(1)测量一致性项确保重建投影符合观测y；(2)自适应去噪正则作为先验引导，强度可调并随训练自适应；同时提供数据驱动的早停准则以避免过拟合或漂移。

Result: 在稀疏视角CT任务上，HyDRA重建质量与现有方法具有竞争力，并在推理阶段保持快速（DEQ式一次前向求解）。

Conclusion: 无需成对监督数据即可训练DEQ进行高质量重建，测量一致性+自适应去噪正则与早停的结合在实际仅测量场景中有效且高效。

Abstract: Solving image reconstruction problems of the form \(\mathbf{A} \mathbf{x} = \mathbf{y}\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \((\mathbf{x},\mathbf{y})\). In many practical settings, only measurements \(\mathbf{y}\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.

</details>


### [52] [RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection](https://arxiv.org/abs/2601.01240)
*Ziqian Guan,Xieyi Fu,Yuting Wang,Haowen Xiao,Jiarui Zhu,Yingying Zhu,Yongtao Liu,Lin Gu*

Main category: cs.CV

TL;DR: 提出RFAssigner，一种基于高斯感受野距离的标签分配策略，先用点先验选初始正样本，再用GRF相似度自适应补充正样本，缓解小目标正样本不足，提升多尺度检测性能，在多个数据集上用FCOS-R50达到各尺度SOTA且无需额外模块。


<details>
  <summary>Details</summary>
Motivation: 现有密集检测器的标签分配多为为每个样本给正/负权重并在训练中自适应优化，但对小目标往往分到的正样本不足，造成尺度不平衡、影响多尺度学习与总体精度。需要一种能在不同目标尺度上更公平、稳定分配正样本的策略。

Method: 提出RFAssigner：1) 以点基先验（point-based prior）先选出一批初始正样本；2) 定义高斯感受野（Gaussian Receptive Field, GRF）距离度量候选位置与GT对象GRF的相似性；3) 基于该距离从未分配候选中自适应补充正样本，从而在不同尺度上获得更均衡的正样本集合；4) 无需额外模块或启发式规则，可无缝集成到FCOS等密集检测器中。

Result: 在三个具有不同目标尺度分布的数据集上做了全面实验，验证了方法的有效性与泛化性。单个FCOS-ResNet-50结合RFAssigner在各尺度（小/中/大）均达到SOTA，稳定优于现有分配策略。

Conclusion: 通过引入GRF距离并自适应补充正样本，RFAssigner缓解了小目标正样本不足与尺度不平衡问题，显著增强了密集检测器的多尺度学习能力，以简洁方式在多数据集与模型上取得一致领先，无需额外模块或启发式。

Abstract: Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.

</details>


### [53] [MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance](https://arxiv.org/abs/2601.01260)
*Hamad Khan,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出MambaFormer混合MoE框架，在医疗问答中按token动态路由到Transformer专家(ET5)或状态空间模型专家(EMamba)，在DentalQA与PubMedQA上以极低时延取得SOTA级效果。


<details>
  <summary>Details</summary>
Motivation: 临床场景需要既高准确度又低时延/低算力的LLM部署，传统Transformer在长序列推理成本高，线性时间模型虽快但在复杂推理上受限；缺乏能按输入特性自适应选择计算路径的高效框架。

Method: 构建LLM驱动的MambaFormer混合MoE：轻量路由器基于token嵌入的上下文复杂度、归一化序列长度与领域特征进行动态路由；短且复杂查询→Transformer专家ET5，长高吞吐序列→状态空间模型专家EMamba。两位专家在维度、嵌入、序长与任务头上定制，并用迁移学习在新构建的DentalQA数据集上微调。提出效用引导的多目标损失，联合优化路由决策、参数、专家利用率与计算成本，通过自适应调节token级专家激活逼近Pareto最优的时延-准确率权衡。

Result: 在DentalQA与PubMedQA上进行留出验证，对比多种SOTA方法，MambaFormer取得BERTScore=0.9180，推理时延0.077秒，相比T5-Large实现24.4倍加速。

Conclusion: MambaFormer通过混合MoE与智能路由，在保证精度的同时显著降低延迟与算力开销，为资源受限的临床部署提供可扩展方案。

Abstract: The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.

</details>


### [54] [AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures](https://arxiv.org/abs/2601.01281)
*Sifatullah Sheikh Urmi,Kirtonia Nuzath Tabassum Arthi,Md Al-Imran*

Main category: cs.CV

TL;DR: 评估4个AI模型（3个CNN与1个ViT）进行深伪检测；数据预处理与增强提升性能；VFDNET结合MobileNetV3表现最佳，准确且高效。


<details>
  <summary>Details</summary>
Motivation: 深度伪造广泛传播威胁数字真实性与信任，需要可靠且高效的自动检测方法来应对不同场景与数据分布。

Method: 在大型人脸图像数据集上，比较三种卷积神经网络与一种视觉Transformer；采用数据预处理与数据增强以提升泛化；重点评估VFDNET与MobileNetV3的组合表现。

Result: 所有模型在预处理与增强后性能提升；VFDNET结合MobileNetV3取得最高准确率与较高效率。

Conclusion: AI模型可实现可靠的深伪检测，其中VFDNET+MobileNetV3在准确性与效率上表现突出，适合实际部署。

Abstract: The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.

</details>


### [55] [S2M-Net: Spectral-Spatial Mixing for Medical Image Segmentation with Morphology-Aware Adaptive Loss](https://arxiv.org/abs/2601.01285)
*Md. Sanaullah Chowdhury Lameya Sabrin*

Main category: cs.CV

TL;DR: S2M-Net用频域稀疏先验+形态自适应损失，在保持全局上下文的同时避免Transformer二次复杂度，实现小参数、快且稳的医学分割SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学分割需要同时满足：边界级局部精度、全局解剖一致性和高效推理（数据/算力受限）。CNN局部强但全局弱；ViT全局强但O(n^2)昂贵且小数据易过拟合，因此需要一种既具全局感受野又具计算与数据效率的架构与训练目标。

Method: 提出S2M-Net（4.7M参数），两大核心：1) Spectral-Selective Token Mixer（SSTM）：利用医学图像频谱能量集中性，采用截断2D FFT+可学习频率滤波+内容门控空间投影，实现O(HW log HW)的全局建模，替代自注意力；2) Morphology-Aware Adaptive Segmentation Loss（MASL）：自动解析目标结构（紧致度、管状性、不规则度、尺度），用受约束的可学习权重自适应调配五种互补损失，免去逐数据集调参。

Result: 在16个数据集、8种模态上达SOTA：息肉Dice 96.12%，手术器械83.77%（较前作+17.85%），脑肿瘤80.90%；相对专门化基线提升3–18%，参数量比Transformer方法少3.5–6倍。

Conclusion: 通过频域选择性全局混合与形态自适应损失，S2M-Net在不牺牲效率的前提下兼顾局部精度与全局一致性，缓解小样本过拟合，具备广泛医学影像分割适用性。

Abstract: Medical image segmentation requires balancing local precision for boundary-critical clinical applications, global context for anatomical coherence, and computational efficiency for deployment on limited data and hardware a trilemma that existing architectures fail to resolve. Although convolutional networks provide local precision at $\mathcal{O}(n)$ cost but limited receptive fields, vision transformers achieve global context through $\mathcal{O}(n^2)$ self-attention at prohibitive computational expense, causing overfitting on small clinical datasets. We propose S2M-Net, a 4.7M-parameter architecture that achieves $\mathcal{O}(HW \log HW)$ global context through two synergistic innovations: (i) Spectral-Selective Token Mixer (SSTM), which exploits the spectral concentration of medical images via truncated 2D FFT with learnable frequency filtering and content-gated spatial projection, avoiding quadratic attention cost while maintaining global receptive fields; and (ii) Morphology-Aware Adaptive Segmentation Loss (MASL), which automatically analyzes structure characteristics (compactness, tubularity, irregularity, scale) to modulate five complementary loss components through constrained learnable weights, eliminating manual per-dataset tuning. Comprehensive evaluation in 16 medical imaging datasets that span 8 modalities demonstrates state-of-the-art performance: 96.12\% Dice on polyp segmentation, 83.77\% on surgical instruments (+17.85\% over the prior art) and 80.90\% on brain tumors, with consistent 3-18\% improvements over specialized baselines while using 3.5--6$\times$ fewer parameters than transformer-based methods.

</details>


### [56] [VReID-XFD: Video-based Person Re-identification at Extreme Far Distance Challenge Results](https://arxiv.org/abs/2601.01312)
*Kailash A. Hambarde,Hugo Proença,Md Rashidunnabi,Pranita Samale,Qiwei Yang,Pingping Zhang,Zijing Gong,Yuhao Wang,Xi Zhang,Ruoshui Qu,Qiaoyun He,Yuhang Zhang,Thi Ngoc Ha Nguyen,Tien-Dung Mai,Cheng-Jun Kang,Yu-Fan Lin,Jin-Hui Jiang,Chih-Chung Hsu,Tamás Endrei,György Cserey,Ashwat Rajbhandari*

Main category: cs.CV

TL;DR: 提出VReID-XFD极远距离航拍-地面跨视角行人重识别视频基准与挑战，量化显示高度/距离上升和俯视（nadir）视角会显著降低性能，当前最佳方法在航到地仅mAP 43.93%。


<details>
  <summary>Details</summary>
Motivation: 现有ReID假设依赖清晰外观与稳定视角，在极远距离的航拍-地面场景中因分辨率极低、视角极端、运动不稳定、着装变化等而失效；缺乏针对该新操作范式的大规模视频基准与系统性评测。

Method: 构建VReID-XFD：从DetReIDX派生，含371身份、11288轨迹、1175万帧，覆盖高度5.8–120m、视角30–90度、水平距离至120m；提供丰富物理元数据与严格身份拆分，支持空-空、空-地、地-空评测；举办VReID-XFD-25挑战并对参赛系统做系统分析。

Result: 社区基准与挑战吸引10支队伍数百次提交；实验显示随高度/距离单调降效，nadir视角普遍更差，性能峰值与鲁棒性存在权衡；最佳SAS-PReID在空-地设定的mAP仅43.93%。

Conclusion: 极远距离跨视角ReID极具挑战，现有最优方法仍远未达可用水平；VReID-XFD为研究提供数据、标注与评测协议，促进面向高空与极端视角的鲁棒ReID方法研究。

Abstract: Person re-identification (ReID) across aerial and ground views at extreme far distances introduces a distinct operating regime where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation jointly undermine the appearance-based assumptions of existing ReID systems. To study this regime, we introduce VReID-XFD, a video-based benchmark and community challenge for extreme far-distance (XFD) aerial-to-ground person re-identification. VReID-XFD is derived from the DetReIDX dataset and comprises 371 identities, 11,288 tracklets, and 11.75 million frames, captured across altitudes from 5.8 m to 120 m, viewing angles from oblique (30 degrees) to nadir (90 degrees), and horizontal distances up to 120 m. The benchmark supports aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation under strict identity-disjoint splits, with rich physical metadata. The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Systematic analysis reveals monotonic performance degradation with altitude and distance, a universal disadvantage of nadir views, and a trade-off between peak performance and robustness. Even the best-performing SAS-PReID method achieves only 43.93 percent mAP in the aerial-to-ground setting. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/ .

</details>


### [57] [LinMU: Multimodal Understanding Made Linear](https://arxiv.org/abs/2601.01322)
*Hongjie Wang,Niraj K. Jha*

Main category: cs.CV

TL;DR: LinMU 用线性复杂度取代自注意力，实现在高分辨率图像与长视频上的高效多模态理解，同时保持与全局注意力 VLM 相当的性能。核心是将所有自注意力层替换为双分支 M-MATE（全局的状态空间模型 Flex-MA + 局部的 Swin 窗注意），并通过三阶段蒸馏从教师 VLM 迁移能力。在多项基准上匹配教师性能，TTFT 最多提升2.7倍、吞吐最高9倍。


<details>
  <summary>Details</summary>
Motivation: 现有 VLM 依赖二次复杂度的自注意力，导致在边缘设备部署困难、对高分辨率/长时长输入的推理代价高昂。亟需在不牺牲性能的前提下降低复杂度，使长上下文多模态推理可行。

Method: 提出 LinMU：将每层自注意力替换为 M-MATE 双分支模块——(1) Flex-MA：双向状态空间模型捕获全局上下文；(2) Local-Swin：Swin 风格局部窗口注意捕捉邻域相关性。给出三阶段蒸馏流程将预训练 VLM 迁移到 LinMU：(i) 用教师的自注意力权重初始化两分支，先仅训练 Flex-MA；(ii) 解冻并联合微调 Local-Swin 与 Flex-MA；(iii) 逐步解冻剩余模块并用 LoRA 微调，同时对齐教师隐藏态与token级logits。

Result: 在 MMMU、TextVQA、LongVideoBench、Video-MME 等基准上，LinMU 与教师模型性能匹配；在分钟级长视频上，TTFT 提升至多2.7×，token 吞吐提升至多9.0×。消融表明三阶段蒸馏与双分支设计均不可或缺。

Conclusion: 无需二次复杂度注意力也可实现最先进多模态推理。LinMU 为高分辨率与长视频场景中的长上下文 VLM 提供了高效可扩展的路径。

Abstract: Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.

</details>


### [58] [Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning](https://arxiv.org/abs/2601.01339)
*Weihang You,Hanqi Jiang,Yi Pan,Junhao Chen,Tianming Liu,Fei Dou*

Main category: cs.CV

TL;DR: 提出NeuroAlign：一种仿生两阶段框架，将fMRI与视频进行细粒度对齐，通过时序对比学习获取全局语义并用增强向量量化做细节匹配，配合动态多模态融合；在跨模态检索上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有将神经解码简化为生成或简单相关的做法难以反映视觉分层与时间加工，且神经数据与视觉输入存在模态鸿沟，导致对视觉认知机制的解析受限。

Method: 仿照人类视觉系统的分层通路，构建两阶段框架：1) Neural-Temporal Contrastive Learning（NTCL），在fMRI与视频间进行双向预测与时序建模以获得全局语义；2) 基于增强向量量化的细粒度模式匹配。引入DynaSyncMM-EMA，实现动态多模态融合与自适应加权。

Result: 在跨模态检索任务上显著优于现有方法，表明能更好地对齐fMRI与视频表征。

Conclusion: NeuroAlign建立了更符合生物学的细粒度fMRI-视频对齐范式，同时提升检索性能并为理解视觉认知机制提供新路径。

Abstract: Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.

</details>


### [59] [Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding](https://arxiv.org/abs/2601.01352)
*Yixuan Lai,He Wang,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: 提出一种以身份为条件的扩散-Transformer视频生成方法，利用短参考视频而非单张图像，显著提升在大姿态与丰富表情下的身份保真、动作自然与提示一致性。


<details>
  <summary>Details</summary>
Motivation: 单图像条件的视频生成忽略时间特征，导致姿态锁定、形变不自然与身份平均化，难以在保持身份的同时实现自然动作与提示忠实。

Method: 在扩散-Transformer视频生成框架中引入“短参考视频”条件：通过Sinkhorn路由的编码器从参考片段中提取紧凑的身份token，这些token捕获主体特有的动态（如笑容形成方式），并与预训练主干兼容，仅作为轻量级条件注入模型。

Result: 在多样主体与文本提示下，相较单图像条件基线，方法在大视角变化与夸张表情时显著提升身份保持，同时维持提示忠实与视觉真实感。

Conclusion: 利用短视频中的动态特征作为身份条件能缓解身份保持与动作自然性的拉扯，在不显著增大模型复杂度的情况下稳定提升视频生成质量。

Abstract: Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and "average" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.

</details>


### [60] [Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance](https://arxiv.org/abs/2601.01356)
*Dang H. Pham,Tu N. Nguyen,Hoa N. Nguyen*

Main category: cs.CV

TL;DR: 论文提出三种用于行人重识别（ReID）的方法，分别覆盖有监督、无监督域自适应与完全无监督场景，并在多个数据集上显著提升mAP与Rank-1，缓解外观变化、域偏移与标注不足问题。


<details>
  <summary>Details</summary>
Motivation: 现实监控中跨摄像机匹配身份困难，受外观变化、域差异和标注匮乏制约。现有方法在特征判别性、跨域泛化和伪标签噪声处理上存在不足，需要统一而有效的训练策略与模型设计来提升鲁棒性与可迁移性。

Method: 1) 有监督：SCM-ReID将监督式对比学习与混合损失（分类、center、triplet、centroid-triplet）联合优化，以增强类内紧致与类间分离。2) UDA：IQAGA与DAPRH通过GAN图像增广、域不变映射与伪标签精炼组合，缓解源-目标域分布差异并提升跨域泛化。3) 全无监督：ViTC-UReID采用Vision Transformer特征编码与摄像头感知的代理学习，结合全局/局部注意与相机身份约束，稳定无标注训练。

Result: 在Market-1501、CUHK03、DukeMTMC-reID、MSMT17等基准上取得显著提升：有监督方法达SOTA；UDA场景在困难迁移上mAP与Rank-1最高提升约12%；全无监督方案在大规模数据上优于既有方法。

Conclusion: 综合策略在特征学习、域自适应与噪声标签处理方面均取得进展，为实际监控部署提供更鲁棒的ReID解决方案，并在多基准上验证了有效性。

Abstract: Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.

</details>


### [61] [Garment Inertial Denoiser (GID): Endowing Accurate Motion Capture via Loose IMU Denoiser](https://arxiv.org/abs/2601.01360)
*Jiawei Fang,Ruonan Zheng,Xiaoxia Gao,Shifan Jiang,Anjun Chen,Qi Ye,Shihui Guo*

Main category: cs.CV

TL;DR: 提出GID，一个用于宽松服装IMU的轻量级Transformer去噪与姿态恢复模块，分三阶段处理并在新数据集GarMoCap上验证，可即插即用提升现有惯性MoCap精度与实时性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴惯性MoCap便携、无遮挡且保护隐私，但要求IMU紧贴身体，日常不舒适。将IMU嵌入宽松服装更实用，却因传感器相对身体位移产生结构化、位置相关噪声，传统惯性算法失效，亟需能纠正宽松穿戴干扰的通用方法与数据。

Method: 提出GID（Garment Inertial Denoiser）：带位置感知专家结构的轻量Transformer。总体骨干为共享时空建模，外加每个IMU的专家头捕捉局部服装动力学，并通过轻量融合模块实现跨部位一致性。将宽松MoCap分解为三阶段：(i) 位置特定去噪；(ii) 自适应跨部位/跨穿戴融合；(iii) 通用姿态预测。依赖有限的宽松-紧身配对IMU数据进行稳定训练。构建GarMoCap数据集，涵盖多用户、多动作、多服装。

Result: 在单用户训练设定下实现实时、准确去噪，能泛化到未见用户、动作与服装类型；作为即插即用模块接入现有惯性MoCap方法，可稳定提升其性能，优于现有SOTA。

Conclusion: GID以位置感知专家+跨部位融合的归纳偏置，有效处理宽松服装导致的IMU位移噪声，实现实用的宽松穿戴惯性MoCap；GarMoCap为研究提供数据支撑，方法具备实时性、泛化性与可集成性。

Abstract: Wearable inertial motion capture (MoCap) provides a portable, occlusion-free, and privacy-preserving alternative to camera-based systems, but its accuracy depends on tightly attached sensors - an intrusive and uncomfortable requirement for daily use. Embedding IMUs into loose-fitting garments is a desirable alternative, yet sensor-body displacement introduces severe, structured, and location-dependent corruption that breaks standard inertial pipelines. We propose GID (Garment Inertial Denoiser), a lightweight, plug-and-play Transformer that factorizes loose-wear MoCap into three stages: (i) location-specific denoising, (ii) adaptive cross-wear fusion, and (iii) general pose prediction. GID uses a location-aware expert architecture, where a shared spatio-temporal backbone models global motion while per-IMU expert heads specialize in local garment dynamics, and a lightweight fusion module ensures cross-part consistency. This inductive bias enables stable training and effective learning from limited paired loose-tight IMU data. We also introduce GarMoCap, a combined public and newly collected dataset covering diverse users, motions, and garments. Experiments show that GID enables accurate, real-time denoising from single-user training and generalizes across unseen users, motions, and garment types, consistently improving state-of-the-art inertial MoCap methods when used as a drop-in module.

</details>


### [62] [Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography](https://arxiv.org/abs/2601.01364)
*Mostofa Rafid Uddin,Mahek Vora,Qifeng Wu,Muyuan Chen,Min Xu*

Main category: cs.CV

TL;DR: 提出一种在cryo-ET中将SE(3)变换与形态内容解耦的深度表征学习框架，并通过多选学习模块在强噪声下学习形态模板，优于EM类方法且能发现稀有新形态。


<details>
  <summary>Details</summary>
Motivation: 现有基于EM的形态估计在噪声高、异质性强的cryo-ET中易忽略低频/稀有形态，且需繁琐调参；同时需要同时解决形态模板与SE(3)位姿估计的耦合逆问题。

Method: 设计一个解耦表示学习框架：在表示空间中将SE(3)变换与形态内容分离；核心引入“多选择（multi-choice）学习”模块以在强噪声条件下实现稳定解耦与聚类；利用学得的形态内容生成模板形态，用于下游分析。

Result: 在模拟和真实cryo-ET数据上，相比既有方法获得更清晰、稳健的模板重建，能更好地检出并表征稀有或此前未识别的宏分子形态。

Conclusion: 解耦SE(3)与形态内容的深度框架能有效解决cryo-ET形态估计的耦合与噪声挑战，减少调参依赖，并提升对稀有形态的发现能力。

Abstract: Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.

</details>


### [63] [ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking](https://arxiv.org/abs/2601.01386)
*Xiaobao Wei,Zhangjie Ye,Yuxiang Gu,Zunjie Zhu,Yunfei Guo,Yingying Shen,Shan Zhao,Ming Lu,Haiyang Sun,Bing Wang,Guang Chen,Rongfeng Lu,Hangjun Ye*

Main category: cs.CV

TL;DR: 提出ParkRecon3D基准与ParkGaussian框架，用3D Gaussian Splatting进行停车场景重建，并通过“车位感知”的重建策略提升与车位检测的一致性，实验达SOTA并有利于下游感知。


<details>
  <summary>Details</summary>
Motivation: 自动代客泊车在拥挤、无GPS的停车场景中面临独特挑战。现有工作多停留在2D车位感知、建图与定位，忽视3D重建；而仅提升重建视觉质量未必能帮助停车，因为关键入口在车位感知模块，亟需一个既服务重建又服务车位检测的方案与数据基准。

Method: 1) 构建ParkRecon3D：四路环视鱼眼相机、多传感器标定与密集车位标注的数据集，用于停车场景重建评测。2) 提出ParkGaussian：将3D Gaussian Splatting用于停车场景重建；3) 设计“车位感知/slot-aware”重建策略，借助现有车位感知结果，对车位区域进行加权/引导以提升这些关键区域的合成质量与几何对齐。

Result: 在ParkRecon3D上，ParkGaussian获得最先进的重建质量，同时在与下游车位检测的一致性上更好（即在重建后进行感知时性能更稳健）。

Conclusion: 首个面向停车场景重建的基准与方法，证明了3DGS结合车位感知引导能同时提升重建质量与下游一致性，为自动泊车提供更实用的三维表示。

Abstract: Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian

</details>


### [64] [Evaluation of Convolutional Neural Network For Image Classification with Agricultural and Urban Datasets](https://arxiv.org/abs/2601.01393)
*Shamik Shafkat Avro,Nazira Jesmin Lina,Shahanaz Sharmin*

Main category: cs.CV

TL;DR: 定制CNN（CustomCNN）结合残差、SE注意力、渐进通道扩展与Kaiming初始化，在多领域图像分类（安防与农业）上取得与主流CNN相当且更高效的表现，强调架构设计对实用场景的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究网络架构设计（残差、注意力、通道设计与初始化等）对多领域（智慧城市与农业）图像分类性能与训练效率的影响，并验证能否以更轻量的自定义CNN在多数据集上达到主流模型的竞争力。

Method: 设计一个包含残差连接、Squeeze-and-Excitation注意力、渐进式通道扩展以及Kaiming初始化的CustomCNN；在五个公开数据集上进行训练与测试（未授权车辆、行人道占用、道路损伤/井盖多边形标注、MangoImageBD、PaddyVarietyBD）；与流行CNN架构进行对比评测，关注准确性与计算效率。

Result: CustomCNN在五个数据集上取得与主流CNN竞争的性能，同时保持较低的计算开销与更快训练速度，显示良好的跨领域适应性。

Conclusion: 精心的架构选择（残差+SE注意力+通道扩展+合适初始化）能在实际多领域图像任务中实现准确与效率的平衡，对智慧城市与农业影像应用具有实际价值。

Abstract: This paper presents the development and evaluation of a custom Convolutional Neural Network (CustomCNN) created to study how architectural design choices affect multi-domain image classification tasks. The network uses residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization to improve its ability to represent data and speed up training. The model is trained and tested on five publicly available datasets: unauthorized vehicle detection, footpath encroachment detection, polygon-annotated road damage and manhole detection, MangoImageBD and PaddyVarietyBD. A comparison with popular CNN architectures shows that the CustomCNN delivers competitive performance while remaining efficient in computation. The results underscore the importance of thoughtful architectural design for real-world Smart City and agricultural imaging applications.

</details>


### [65] [SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution](https://arxiv.org/abs/2601.01406)
*Habiba Kausar,Saeed Anwar,Omar Jamal Hammad,Abdul Bais*

Main category: cs.CV

TL;DR: 提出SwinIFS：结合人脸关键点热图与Swin Transformer的层级注意力，实现身份保持的人脸超分，尤其在8×放大下仍具备锐利与结构一致性，并兼顾效能。


<details>
  <summary>Details</summary>
Motivation: 传统人脸超分在低分辨率、极端放大时易丢失细粒度结构与身份特征，导致感知质量差与结构失真；需要一种能引入结构先验并兼顾全局上下文与局部几何的方法。

Method: 将密集高斯关键点热图与LR人脸共同作为输入，引导网络自初始阶段关注语义关键区域；采用精简版Swin Transformer骨干，利用层级自注意力捕获长程依赖与局部几何；在CelebA上进行训练与评估，关注感知质量、锐度与身份保持，并测试至8×放大。

Result: 在CelebA上取得优于现有方法的感知质量与身份保持，重建更锐利且结构一致；在8×放大情形依然能恢复有意义结构；在重建精度与计算效率之间取得良好平衡。

Conclusion: 引入关键点热图的结构先验并结合Swin Transformer的层级注意力，可在中高倍率下实现身份保持的人脸超分，兼顾质量与效率，适用于增强、监控与数字修复等应用；代码与模型已开源。

Abstract: Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.

</details>


### [66] [Mask-Guided Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01408)
*Gong Gao,Zekai Wang,Jian Zhao,Ziqi Xie,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: 提出MGMTN，通过自适应掩码和组-全局特征融合提升人脸属性识别，减少全局冗余并显著提高精度。


<details>
  <summary>Details</summary>
Motivation: 传统多任务FAR常基于整幅特征图进行提取与分类，过度依赖全局区域导致冗余特征与负迁移，难以精确定位与判别细粒度属性（如眼、口相关属性）。

Method: 构建Mask-Guided Multi-Task Network（MGMTN）。1）Adaptive Mask Learning（AML）：借助预训练关键点标注模型与全卷积网络，定位关键面部部位（眼、口等），生成分组掩码以选定有意义的特征区域，抑制无关全局信息。2）Group-Global Feature Fusion（G2FF）：将由分组区域提取的局部特征与全局特征融合，用于多属性联合学习与分类。

Result: 在两个具有挑战性的人脸属性识别数据集上，MGMTN显著优于对比方法，验证其在精度与鲁棒性方面的提升。

Conclusion: 关注关键部位的自适应掩码与组-全局融合可有效缓解全局特征冗余与负迁移问题，提升多任务FAR的精确度；所提MGMTN是一种高效、可泛化的FAR框架。

Abstract: Face Attribute Recognition (FAR) plays a crucial role in applications such as person re-identification, face retrieval, and face editing. Conventional multi-task attribute recognition methods often process the entire feature map for feature extraction and attribute classification, which can produce redundant features due to reliance on global regions. To address these challenges, we propose a novel approach emphasizing the selection of specific feature regions for efficient feature learning. We introduce the Mask-Guided Multi-Task Network (MGMTN), which integrates Adaptive Mask Learning (AML) and Group-Global Feature Fusion (G2FF) to address the aforementioned limitations. Leveraging a pre-trained keypoint annotation model and a fully convolutional network, AML accurately localizes critical facial parts (e.g., eye and mouth groups) and generates group masks that delineate meaningful feature regions, thereby mitigating negative transfer from global region usage. Furthermore, G2FF combines group and global features to enhance FAR learning, enabling more precise attribute identification. Extensive experiments on two challenging facial attribute recognition datasets demonstrate the effectiveness of MGMTN in improving FAR performance.

</details>


### [67] [AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval](https://arxiv.org/abs/2601.01416)
*Yue Zhou,Ran Ding,Xue Yang,Xue Jiang,Xingzhao Liu*

Main category: cs.CV

TL;DR: 论文提出AirSpatial数据集与AirSpatialBot，聚焦无人机车辆图像的空间理解，包含206K+指令与两项新任务（空间定位与空间问答），并引入3D边界框。采用两阶段训练（图像理解预训练+空间理解微调），显著提升VLM在空间任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感VLM在空间关系、定位与几何理解上薄弱，限制了实际应用（如车辆属性识别、检索与任务执行）。需要面向无人机场景的数据与训练策略来补齐空间理解短板，并系统评估现有VLM的空间能力。

Method: 1) 构建AirSpatial数据集：206K+指令，涵盖车辆细粒度属性、空间关系；提出两项新任务：Spatial Grounding与Spatial QA；提供3D边界框（3DBB）。2) 两阶段训练：先进行图像理解预训练以继承通用视觉-语言能力，再进行空间理解微调以学习空间定位与推理。3) 基于训练后的空间感知VLM构建AirSpatialBot，通过任务规划、图像与空间理解结合，实现属性识别、检索与执行。

Result: 实验显示所提方法在空间相关任务上优于现有VLM，揭示了现有模型的空间局限；AirSpatialBot在细粒度车辆属性识别与检索方面表现有效。

Conclusion: 面向无人机车辆影像的空间理解可通过专门数据与两阶段训练显著提升；AirSpatial与AirSpatialBot为评测与应用提供了基准和工具，代码与数据将开源。

Abstract: Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot

</details>


### [68] [DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer](https://arxiv.org/abs/2601.01425)
*Xu Guo,Fulong Ye,Xinghui Li,Pengqi Tu,Pengze Zhang,Qichao Sun,Songtao Zhao,Xiangwang Hou,Qian He*

Main category: cs.CV

TL;DR: 提出DreamID-V：基于扩散Transformer的视频换脸框架，通过新数据管线、模态感知条件、合成到真实课程学习与身份一致性强化等策略，实现更强身份保持、属性保真与时序一致；并发布视频基准IDBench-V。


<details>
  <summary>Details</summary>
Motivation: 现有视频换脸难以同时兼顾身份相似度、属性（姿态/表情/光照/背景/动态）保真与时序一致性，且视频标注/基准匮乏。图像换脸在单帧上表现优，但迁移到视频存在缺口，需要系统性框架与数据/训练策略填补。

Method: 1) SyncID-Pipe数据管线：预训练“身份锚定视频合成器”，结合图像换脸模型构造双向ID四元组，获得显式配对监督；2) DreamID-V：首个基于扩散Transformer的VFS框架，引入模态感知条件模块，区分性注入多模态条件（如身份、姿态、表情、时序线索等）；3) 训练策略：合成到真实课程机制逐步提升真实感；身份一致性强化学习策略优化身份相似度与稳定性；4) 基准：提出多场景综合基准IDBench-V。

Result: 在广泛实验与多任务设置中，DreamID-V在身份保持、视觉质量与时序一致性上优于SOTA，并能无缝适配多种换相关任务。

Conclusion: 通过数据管线、扩散Transformer与针对性训练策略的协同，DreamID-V将图像换脸优势迁移至视频领域，显著提升视频换脸的身份一致性与属性保真，并为社区提供新基准。

Abstract: Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.

</details>


### [69] [EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views](https://arxiv.org/abs/2601.01431)
*Weiqi Yu,Yiyang Yao,Lin He,Jianming Lv*

Main category: cs.CV

TL;DR: EdgeNeRF 通过边缘引导的正则化，在稀疏视角下显著减少NeRF几何伪影，同时保留清晰边界细节。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF在稀疏多视角时易出现几何伪影。现有做法用全局深度正则化抑制伪影，但会磨平几何边界的高频细节，导致边缘模糊。需要一种既能稳定几何又能保留边界的策略。

Method: 利用“深度与法线的突变对应图像边缘”的先验：1) 从输入图像提取边缘；2) 在非边缘区域施加深度与法线一致性正则，强化几何连续性；3) 在边缘处减少/取消这类约束以保留高频变化；4) 模块化的边缘引导深度正则可无缝插入其他NeRF方法。

Result: 在LLFF与DTU数据集上取得更优重建质量，明显保留锐利几何边界并抑制伪影；即插即用地集成到其他方法也能显著提升效果，训练时间几乎不增加。

Conclusion: 边缘引导的区域化正则化兼顾了稀疏视角下的几何稳定性与边界细节，优于全局深度正则；该模块具备通用性与低开销，实用价值高。

Abstract: Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts. Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details. To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm. Our method leverages the prior that abrupt changes in depth and normals generate edges. Specifically, we first extract edges from input images, then apply depth and normal regularization constraints to non-edge regions, enhancing geometric consistency while preserving high-frequency details at boundaries. Experiments on LLFF and DTU datasets demonstrate EdgeNeRF's superior performance, particularly in retaining sharp geometric boundaries and suppressing artifacts. Additionally, the proposed edge-guided depth regularization module can be seamlessly integrated into other methods in a plug-and-play manner, significantly improving their performance without substantially increasing training time. Code is available at https://github.com/skyhigh404/edgenerf.

</details>


### [70] [In defense of the two-stage framework for open-set domain adaptive semantic segmentation](https://arxiv.org/abs/2601.01439)
*Wenqi Ren,Weijie Wang,Meng Zheng,Ziyan Wu,Yang Tang,Zhun Zhong,Nicu Sebe*

Main category: cs.CV

TL;DR: 论文提出SATS：先分离再自适应的训练策略，用于开放集域自适应语义分割。先区分已知/未知，再进行“未知感知”的域适应，并引入“困难未知探索”增强。相比SOTA，GTA5→Cityscapes的H-Score提升+3.85%，SYNTHIA→Cityscapes提升+18.64%。


<details>
  <summary>Details</summary>
Motivation: 现有OSDA-SS方法在单一阶段同时做已知类域适应与未知识别，受已知/未知标注分布不均影响：已知类产生负迁移，未知类欠拟合，导致对真正未知目标识别不佳。需要一种能缓解类别不平衡与任务干扰的训练范式。

Method: 提出SATS两阶段策略：1) 分离阶段：进行已知/未知分离，获取更准确、对齐更好的未知样本集合；2) 适应阶段：执行“未知感知”的域适应，使模型在已知与未知上均衡学习判别特征。同时设计“困难未知探索”的数据增强，使模型暴露于更具挑战性的未知样本，提升对目标域未知的覆盖与鲁棒性。

Result: 在公开OSDA-SS基准上评测：GTA5→Cityscapes的H-Score提升+3.85%，SYNTHIA→Cityscapes提升+18.64%，均超过先前SOTA。

Conclusion: 将已知/未知分离与域适应解耦，避免任务相互干扰，配合困难未知探索，能更好学习已知与未知的判别特征，显著提升OSDA-SS性能。

Abstract: Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.

</details>


### [71] [PartImageNet++ Dataset: Enhancing Visual Models with High-Quality Part Annotations](https://arxiv.org/abs/2601.01454)
*Xiao Li,Zilong Liu,Yining Liu,Zhuhong Li,Na Dong,Sitian Qin,Xiaolin Hu*

Main category: cs.CV

TL;DR: 提出PartImageNet++数据集（覆盖ImageNet-1K所有1000类、每类100张、共10万图像的细粒度部件标注），并基于其训练部件分割与多尺度部件监督分类模型MPM，在ImageNet及多下游任务上取得提升。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺少覆盖全面、质量高的部件级标注，限制了利用部件信息进行鲁棒识别与迁移到下游任务（如分割、少样本学习）的研究与性能。

Method: 1) 构建PIN++：为ImageNet-1K每类挑选100张图像并提供细粒度部件标注；2) 以PIN++训练部件分割网络，并对未标注图像生成伪部件标签；3) 提出MPM，在常规模型上加入多尺度辅助旁路层，联合使用伪部件标签与真实部件标注进行监督；4) 在PIN++上系统评测（部件/目标分割、少样本学习等），探索部件标注的多种用法。

Result: 在ImageNet-1K上，MPM提升了基于部件的鲁棒目标识别性能；在PIN++的多项下游任务上建立了强基线并显示出部件监督带来的显著收益。

Conclusion: 全面的部件标注与多尺度部件监督可显著增强识别与多任务表现；PIN++为研究部件级学习提供了大规模基准与代码，表明部件信息在改进模型鲁棒性与泛化方面具有重要价值。

Abstract: To address the scarcity of high-quality part annotations in existing datasets, we introduce PartImageNet++ (PIN++), a dataset that provides detailed part annotations for all categories in ImageNet-1K. With 100 annotated images per category, totaling 100K images, PIN++ represents the most comprehensive dataset covering a diverse range of object categories. Leveraging PIN++, we propose a Multi-scale Part-supervised recognition Model (MPM) for robust classification on ImageNet-1K. We first trained a part segmentation network using PIN++ and used it to generate pseudo part labels for the remaining unannotated images. MPM then integrated a conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and the original part annotations. Furthermore, we conducted extensive experiments on PIN++, including part segmentation, object segmentation, and few-shot learning, exploring various ways to leverage part annotations in downstream tasks. Experimental results demonstrated that our approach not only enhanced part-based models for robust object recognition but also established strong baselines for multiple downstream tasks, highlighting the potential of part annotations in improving model performance. The dataset and the code are available at https://github.com/LixiaoTHU/PartImageNetPP.

</details>


### [72] [Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration](https://arxiv.org/abs/2601.01456)
*Wentao Bian,Fenglei Xu*

Main category: cs.CV

TL;DR: 提出DA-FSS用于多模态小样本点云语义分割，通过解耦语义与几何专家并仲裁融合，缓解“先融合再细化”的可塑性-稳定性困境与CLIP类间混淆，实现更优泛化与边界/完整性/纹理表现，优于MM-FSS。


<details>
  <summary>Details</summary>
Motivation: 现有“Fuse-then-Refine”范式在小样本多模态点云分割中易陷入可塑性-稳定性两难：新类适应与已学稳固难以兼顾；同时CLIP文本引导存在类间混淆导致语义失明，影响多模态协同。

Method: 提出DA-FSS：使用与MM-FSS相同骨干与预训练文本编码器生成文本嵌入；并行专家精炼模块分别建模几何与语义相关性；堆叠仲裁模块（SAM）执行卷积式多模态融合并对各通道相关性做仲裁；解耦对齐模块（DAM）在两专家间传递知识、约束梯度，避免混淆传播，使几何专家保持可塑性、语义专家保持稳定性。

Result: 在S3DIS与ScanNet上显著优于MM-FSS；几何边界、对象完整性与纹理区分能力均有提升。

Conclusion: 解耦“几何-语义”双专家并通过仲裁与对齐实现稳定-可塑的协同学习，可缓解CLIP语义混淆与Fuse-then-Refine的内在矛盾，提升多模态小样本点云分割的泛化与质量。

Abstract: In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in "Fuse-then-Refine" paradigms: the "Plasticity-Stability Dilemma." In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.

</details>


### [73] [Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation](https://arxiv.org/abs/2601.01457)
*Mingxing Zhan,Li Zhang,Beibei Wang,Yingjie Wang,Zenglin Shi*

Main category: cs.CV

TL;DR: 论文提出在冻结相对深度骨干网络和CLIP文本编码器的前提下，仅通过图像特定的逆深度仿射变换来恢复单目“米制”深度；利用文本生成不确定性包络约束校准参数，再用冻结的多尺度视觉特征在包络内选择具体校准；以闭式最小二乘“oracle”提供每张图像的监督。方法在NYUv2、KITTI上提升域内精度，并在SUN-RGBD、DDAD零样本迁移中优于强文本基线，显示更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 相对深度模型跨域迁移性强，但单目米制深度本质上存在全局尺度不可辨识，且对域偏移敏感；仅依赖文本提示的尺度往往粗糙且受表述与漏检影响，点估计不稳。动机是：在不微调庞大骨干的情况下，利用轻量校准层和多模态线索稳定、可迁移地恢复米制尺度。

Method: - 冻结相对深度骨干与CLIP文本编码器。
- 在逆深度空间用图像特定的仿射变换（两个参数）把相对深度校准为米制深度，仅训练轻量“校准头”。
- 文本（图像字幕）用于预测一个不确定性包络（参数可行域），而非单点尺度，缓解文本噪声与遗漏。
- 用冻结的多尺度视觉特征在该包络内选择最终的图像特定校准参数。
- 训练时引入闭式最小二乘oracle（在逆深度域）为每张图像提供监督信号，指导包络学习与选择器学习。

Result: 在NYUv2、KITTI上域内精度提升；在SUN-RGBD、DDAD的零样本迁移上，相较于强语言仅基线表现更稳健、误差更低，显示对域偏移更不敏感。

Conclusion: 借助“语言给出不确定性约束、视觉在约束内做实例化选择”的两阶段校准，在冻结骨干的前提下即可将相对深度有效转为米制深度；该策略兼顾精度与泛化，减少对大规模微调的依赖，并提升跨域鲁棒性。

Abstract: Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.

</details>


### [74] [Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network](https://arxiv.org/abs/2601.01460)
*Mohd Usama,Belal Ahmad,Christer Gronlund,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: 提出一种GAN为核心的超声图像跨设备/参数域自适应方法，通过图像到图像翻译改变纹理并去除混响噪声，同时保持解剖内容不变；在两套颈动脉超声数据、三域上验证，较无适配与CycleGAN更优，直方图相关性提高、Bhattacharyya距离降低。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度学习常假设训练与测试同分布，但不同设备/设置导致纹理与混响噪声差异，跨域性能下降；为避免对每个设备重训的高成本，需要一种无需标注或重训即可在推理时适配域差异的方法。

Method: 将域适配表述为图像到图像翻译：以GAN框架学习从源域到目标域的映射，显式修改纹理模式并抑制/去除超声混响噪声，同时通过约束保持解剖内容不变。与CycleGAN进行对照实验；在三域的两套颈动脉超声数据上进行无监督翻译与评估。

Result: 模型成功将源域图像纹理转换为目标域风格并去除混响噪声。在两数据集上，直方图相关性由无适配的约0.916(0.062)与0.890(0.077)提升至0.960(0.019)与0.920(0.043)；Bhattacharyya距离由0.090(0.070)与0.121(0.095)降低至0.040(0.020)与0.085(0.048)。优于CycleGAN（摘要未给出具体数值但称对照不及本方法）。

Conclusion: 所提GAN式域适配能在不改变图像内容的前提下，统一不同设备/设置下的超声图像纹理并减噪，显著提升跨域一致性，减少为每个设备重训的需求；具备推广到其他超声域迁移场景的潜力。

Abstract: Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.

</details>


### [75] [Robust Ship Detection and Tracking Using Modified ViBe and Backwash Cancellation Algorithm](https://arxiv.org/abs/2601.01481)
*Mohammad Hassan Saghafi,Seyed Majid Noorhosseini,Seyed Abolfazl Seyed Javadein,Hadi Khalili*

Main category: cs.CV

TL;DR: 提出一种鲁棒的实时海岸视频船舶检测与跟踪方法：改进ViBe实现移动目标检测，并结合几何与亮度畸变抑制尾流，实现在波浪与光照变化下的稳定检测与实时跟踪。


<details>
  <summary>Details</summary>
Motivation: 海岸场景不可预测、波浪与光照变化剧烈，传统方法易漏检或被海浪/尾流干扰。需要一种能快速更新背景、抗干扰、低漏检的船舶检测与跟踪方案。

Method: 1) 提出改进的ViBe背景建模/前景检测：降低目标丢失概率，快速背景更新，增强对海浪与光照变化的鲁棒性；2) 利用船舶几何先验与亮度畸变（brightness distortion）构建尾流（backwash）消除算法；3) 在此基础上进行目标跟踪，实现实时处理。

Result: 实验结果显示：在真实海岸视频中对船舶与尾流的区分效果好，漏检率下降，检测与跟踪精度提升，并满足实时性要求。

Conclusion: 改进ViBe结合几何与亮度畸变的尾流抑制策略，可在复杂海岸场景中实现鲁棒、精准、实时的船舶检测与跟踪。

Abstract: In this paper, we propose a robust real time detection and tracking method for detecting ships in a coastal video sequences. Since coastal scenarios are unpredictable and scenes have dynamic properties it is essential to apply detection methods that are robust to these conditions. This paper presents modified ViBe for moving object detection which detects ships and backwash. In the modified ViBe the probability of losing ships is decreased in comparison with the original ViBe. It is robust to natural sea waves and variation of lights and is capable of quickly updating the background. Based on geometrical properties of ship and some concepts such as brightness distortion, a new method for backwash cancellation is proposed. Experimental results demonstrate that the proposed strategy and methods have outstanding performance in ship detection and tracking. These results also illustrate real time and precise performance of the proposed strategy.

</details>


### [76] [Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization](https://arxiv.org/abs/2601.01483)
*Xinyu Qiu,Heng Jia,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Yi Yang,Linchao Zhu*

Main category: cs.CV

TL;DR: 提出ADPO：在单一策略内同时学习生成与自验证，显著提升验证AUC并降低推理时延，同时保持或提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 并行测试时扩展通常需要独立的生成与验证模型，训练与推理成本高，且协同效果有限。需要一种在单策略内统一学习生成与自验证的方法，降低成本并提升验证可靠性与任务表现。

Method: 提出优势解耦偏好优化（ADPO）。核心包括：1）偏好验证奖励：以正负样本的均值验证分数为阈，若预测正确性与答案正确性一致则给予正反馈，从而校准验证器；2）优势解耦优化：分别为生成与验证计算优势，利用token mask隔离梯度，将掩蔽后的GRPO目标组合，避免相互干扰，在保持生成质量的同时校准验证分数。整个过程在单一策略内联合学习生成与自验证。

Result: 相较基线，验证AUC最高提升+34.1%，推理时间降低53.5%；任务性能上：MathVista/MMMU准确率分别+2.8%/+1.4%，ReasonSeg cIoU +1.9，AndroidControl/GUI Odyssey步骤成功率+1.7%/+1.0%。

Conclusion: ADPO在单策略框架下实现生成与验证的协同强化学习，通过偏好验证奖励与优势解耦优化兼顾质量与校准，大幅提升验证能力并降低推理成本，同时在多项任务上带来稳健增益。

Abstract: Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.

</details>


### [77] [Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease](https://arxiv.org/abs/2601.01485)
*Zobia Batool,Diala Lteif,Vijaya B. Kolachalama,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 提出一种Extended MixStyle (EM) 方法，通过在特征空间混合更高阶统计量（偏度、峰度）来模拟域偏移，从而提升阿尔茨海默病sMRI分类在单源域到未见域的泛化；在NACC训练、三外部队列测试，平均宏F1较SOTA SDG提高约2.4个百分点。


<details>
  <summary>Details</summary>
Motivation: AD真实应用面临跨中心设备、协议与人群差异导致的域偏移，单源域泛化能力不足；现有注意力/融合方法虽强化表征，但对SDG的系统性探索不足。需要无需多源数据也能获得域不变表征的方法。

Method: 在深度模型的特征空间扩展MixStyle：除均值/方差外引入偏度与峰度的混合与扰动，合成更丰富的分布变化以正则化网络；在NACC单源训练，将NC与MCI/AD作为分类任务；在三个未见外部队列上评估。

Result: 在三未见队列（总n=3,126）上，EM较当前SDG基线平均宏F1提升约+2.4个百分点，体现更强的跨域鲁棒性。

Conclusion: 通过混合高阶矩的EM能更好模拟现实异质性，提升单源训练下的跨域AD检测稳定性与可靠性；对临床落地具有潜力，代码将开源。

Abstract: Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.

</details>


### [78] [DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion](https://arxiv.org/abs/2601.01487)
*Ziyue Zhang,Luxi Lin,Xiaolin Hu,Chao Chang,HuaiXi Wang,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出DeepInv：一种自监督的扩散反演方法，通过无标注自监督目标与数据增强生成高质量伪噪声，配合迭代与多尺度训练，学习可参数化的逐步反演求解器，实现快速准确的图像→噪声映射，性能与速度显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型编辑依赖将真实图像映射回扩散过程中的噪声（反演）；但缺乏可行监督信号，现有多为近似推断，牺牲性能或效率。需要一种无需真实噪声标注、可训练且高效准确的反演方案。

Method: 1) 自监督目标：无需真实噪声标签，通过设计的训练损失在真实图像上生成高质量伪噪声；2) 数据增强策略：提升伪噪声质量与鲁棒性；3) 迭代+多尺度训练机制：训练一个参数化反演求解器，逐步预测各扩散步的噪声，实现稳定、快速的图像到噪声映射。

Result: 在COCO上显著超越现有方法：相较EasyInv提升SSIM约+40.435%，相较ReNoise推理速度约+9887.5%。总体表现与速度均大幅领先，支持高效可控图像编辑。

Conclusion: DeepInv首次提出可训练的逐步反演求解器，以自监督与增强生成伪噪声并配合迭代多尺度训练，实现快速准确的扩散反演；为社区提供可扩展的反演框架与实践启示，代码与模型将开源。

Abstract: Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.

</details>


### [79] [DiffKD-DCIS: Predicting Upgrade of Ductal Carcinoma In Situ with Diffusion Augmentation and Knowledge Distillation](https://arxiv.org/abs/2601.01507)
*Tao Li,Qing Li,Na Li,Hui Xie*

Main category: cs.CV

TL;DR: 提出DiffKD-DCIS：用条件扩散生成超声增广数据，教师-学生蒸馏训练紧凑学生模型，实现DCIS是否升级为IDC的更稳健预测；在多中心数据上学生模型更小更快、外部测试优于部分对照，接近资深放射科医师、优于初级。


<details>
  <summary>Details</summary>
Motivation: DCIS是否会升级为IDC直接影响手术策略，但超声样本少、深度模型泛化差，难以在外部中心稳定表现；需要既能扩充分布又能训练轻量且泛化好的模型，便于临床部署。

Method: 三阶段框架：1) 条件扩散模型在多模态条件（如病灶/临床信息等）约束下生成高保真超声影像做数据增广；2) 深层教师网络在原始+合成数据上学习并提取鲁棒特征；3) 通过知识蒸馏，将教师的表征/软标签传给紧凑学生网络，以兼顾精度与效率。

Result: 在1435例多中心数据上，合成图像质量良好；学生模型参数更少、推理更快；在外部测试集上优于仅用扩散或仅用蒸馏等部分组合；其准确性与资深放射科医生相当，显著优于初级医生。

Conclusion: 结合条件扩散增广与教师-学生蒸馏的DiffKD-DCIS可在有限超声数据情境下提升DCIS升级预测的泛化与效率，具备临床应用潜力。

Abstract: Accurately predicting the upgrade of ductal carcinoma in situ (DCIS) to invasive ductal carcinoma (IDC) is crucial for surgical planning. However, traditional deep learning methods face challenges due to limited ultrasound data and poor generalization ability. This study proposes the DiffKD-DCIS framework, integrating conditional diffusion modeling with teacher-student knowledge distillation.
  The framework operates in three stages: First, a conditional diffusion model generates high-fidelity ultrasound images using multimodal conditions for data augmentation. Then, a deep teacher network extracts robust features from both original and synthetic data. Finally, a compact student network learns from the teacher via knowledge distillation, balancing generalization and computational efficiency.
  Evaluated on a multi-center dataset of 1,435 cases, the synthetic images were of good quality. The student network had fewer parameters and faster inference. On external test sets, it outperformed partial combinations, and its accuracy was comparable to senior radiologists and superior to junior ones, showing significant clinical potential.

</details>


### [80] [A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI](https://arxiv.org/abs/2601.01512)
*Wenhui Chu,Aobo Jin,Hardik A. Gohel*

Main category: cs.CV

TL;DR: 提出GBU-Net：在U-Net中引入Group+Batch Normalization并加强上下采样路径的上下文建模，用于短轴cine MRI左心室精确分割；在SunnyBrook等数据上Dice最高达97%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN/标准U-Net在心脏MRI左心室分割中对跨尺度上下文捕获不足、边界细节恢复有限，导致Dice与边界距离指标受限。需要一种既保留细节又强化上下文理解、并在小样本医疗数据上稳定训练的网络。

Method: 设计GBU-Net：以U-Net为骨干，采用group-batch normalization策略；下采样通路强化特征抽取与多尺度上下文融合，上采样通路增强细节恢复；引入针对医学影像的上下文建模改进；与基线方法比较，评价指标为Dice与mean perpendicular distance；并使用模型集成提升稳健性。数据集含45位患者共805张短轴cine MRI左心室图像，SunnyBrook作为测试集之一。

Result: GBU-Net在左心室分割上显著优于传统方法，Dice与平均垂直距离均取得改进；模型集成在SunnyBrook测试集达到97% Dice。

Conclusion: GBU-Net通过更强的上下文理解与稳定归一化策略，提升了cine MRI左心室分割的精度与鲁棒性，适用于外科机器人与医学分析等临床场景。

Abstract: This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.

</details>


### [81] [FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01513)
*Gen Li,Peiyu Liu*

Main category: cs.CV

TL;DR: 提出VideoSpeculateRAG：用“轻量草稿模型+重型验证模型”的推测解码，并配合相似度过滤纠正实体错配，实现~2倍加速且准确率不降。


<details>
  <summary>Details</summary>
Motivation: VLM在视觉推理强，但难以利用外部知识；现有RAG效率低且质量受实体检索误差影响，需要既快又准的多模态知识增强方案。

Method: 1) 推测式解码：轻量草稿VLM并行生成多候选答案，重型VLM负责验证与精炼，减少总推理时间。2) 实体对齐过滤：对检索到的知识项按与视频/问题中实体的相似度进行筛选，过滤掉实体识别错误导致的干扰文档。3) 将上述两点集成到多模态RAG流程中。

Result: 在多模态知识密集任务上，相比标准RAG实现约2倍推理加速，同时准确率持平或更高。

Conclusion: 结合推测式解码与相似度驱动的实体过滤，可显著提升VLM-RAG在复杂多模态任务中的效率与可靠性，展示了该范式的潜力。

Abstract: Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.

</details>


### [82] [BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding](https://arxiv.org/abs/2601.01526)
*Hongbing Li,Linhui Xiao,Zihan Zhao,Qi Shen,Yixiang Huang,Bo Xiao,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出BARE：一个面向单塔架构的偏置感知与推理增强视觉指代定位框架，通过保持模态特异性并强化指代语义，减轻多模态干扰并提升理解，达成SOTA且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有单塔式视觉指代迁移方法存在两大问题：多模态表征过度纠缠导致欺骗性模态偏置；语义推理不足限制对指代表达的理解。需要一种既能抑制偏置又能增强指代推理的框架。

Method: 提出BARE框架，核心是保留模态特异表征并显式构建指代语义。包含三大模块：（i）语言显著性调制器，用于突出文本中的关键指代线索；（ii）视觉偏置校正，缓解由视觉端先验或伪相关造成的偏置；（iii）指代关系增强，建模并强化实体与关系的组合语义。整体在单塔模型内实现偏置感知与推理增强。

Result: 在五个基准数据集上达到最新最优效果，同时推理/训练计算效率优于现有方法。

Conclusion: 通过在单塔VG中显式分离模态、校正视觉偏置、增强指代关系，BARE有效缓解多模态干扰与推理不足问题，兼顾性能与效率；代码已开源。

Abstract: Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.

</details>


### [83] [DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving](https://arxiv.org/abs/2601.01528)
*Yang Zhou,Hao Shao,Letian Wang,Zhuofan Zong,Hongsheng Li,Steven L. Waslander*

Main category: cs.CV

TL;DR: 提出DrivingGen，一个面向自动驾驶生成式世界模型的综合基准，涵盖多源多样场景与新指标，用于评估视觉真实感、轨迹可行性、时序一致性与可控性，揭示通用视频模型与驾驶特定模型之间的取舍。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成/世界模型在自动驾驶中的评估缺乏统一、严格且与安全相关的基准：通用视频指标忽略安全关键视觉因素；对轨迹物理可行性缺乏量化；时序与多智能体一致性未重视；对自车（ego）条件的可控性评估缺失；同时公开数据集覆盖条件不够多样，难以支撑真实部署需求。

Method: 构建DrivingGen基准：1) 数据层面，从专业驾驶数据与互联网视频中精选覆盖多天气、昼夜、地域、复杂机动的多样场景；2) 指标层面，设计一组联合评测指标，分别衡量视觉真实感、轨迹物理与可行性、时序连贯与多主体一致性、以及相对自车条件的可控性；3) 用该基准系统评测14个SOTA模型（通用视频模型与驾驶特定模型）。

Result: 实验显示明确权衡：通用视频生成模型视觉质量更佳但常违背物理与运动规律；驾驶特定模型能更好地刻画运动与交互，但视觉质量偏弱。新基准能区分模型在不同维度的长短板。

Conclusion: DrivingGen为生成式驾驶世界模型提供统一、全面的评测框架，可推动更可靠、可控、可部署的模型发展，并支持可扩展仿真、规划与数据驱动决策。

Abstract: Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.

</details>


### [84] [Improving Flexible Image Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.01535)
*Zixuan Fu,Lanqing Guo,Chong Wang,Binbin Song,Ding Liu,Bihan Wen*

Main category: cs.CV

TL;DR: 提出ReToK，一种可变长度图像分词器，通过冗余填充与分层语义正则，避免信息过度集中在前端token，提升AR图像生成；在ImageNet 256上优于现有灵活与定长分词器。


<details>
  <summary>Details</summary>
Motivation: 现有可变长度图像分词器常用“尾部截断”训练，导致图像信息过度集中于前几个token，序列变长时AR生成效率与质量受限，需要一种能充分利用所有token的方案。

Method: 1) 冗余Token填充：在训练中更频繁激活尾部token，缓解信息集中在前部；2) 分层语义正则：用预训练视觉基础模型对前部token的解码特征进行对齐，并沿序列逐渐减弱正则强度，使尾部专注于细粒度低层细节重建。

Result: 在ImageNet 256×256上，ReToK在图像生成质量上优于现有灵活与定长分词器；实现更好的AR潜空间建模与生成性能。

Conclusion: 通过冗余填充与分层语义正则化，ReToK缓解尾截断带来的信息偏置，充分利用全序列token，显著提升可变长度图像分词与下游生成效果。

Abstract: Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \textbf{ReToK}, a flexible tokenizer with \underline{Re}dundant \underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}

</details>


### [85] [FAR-AMTN: Attention Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01537)
*Gong Gao,Zekai Wang,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: 提出FAR-AMTN：通过组特异注意力与跨组融合提升人脸属性多任务识别的泛化，参数更少、精度更高。


<details>
  <summary>Details</summary>
Motivation: 传统多任务网络常用“低层共享+高层分支”的架构，随着任务数增加参数呈指数增长；高层特征彼此独立，难以建模属性间的语义关系，导致泛化受限。需要一种既能减少参数又能促进跨任务/跨属性交互的方案。

Method: 1) 设计FAR-AMTN框架。2) Weight-Shared Group-Specific Attention（WSGSA）：按属性组建模注意力，注意力权重在组间共享以降复杂度，同时保留组内特异性以提升表征。3) Cross-Group Feature Fusion（CGFF）：在不同属性组之间进行特征交互与融合，显式挖掘跨组语义关系。4) Dynamic Weighting Strategy（DWS）：训练中自适应调整各任务损失权重，使多任务同步收敛。

Result: 在CelebA与LFWA数据集上，相比现有模型，以显著更少的参数实现更高的识别准确率。

Conclusion: 通过共享权重的组特异注意力、跨组特征融合与动态损失加权，FAR-AMTN在减少模型规模的同时有效提升FAR多任务学习的泛化能力与精度。

Abstract: To enhance the generalization performance of Multi-Task Networks (MTN) in Face Attribute Recognition (FAR), it is crucial to share relevant information across multiple related prediction tasks effectively. Traditional MTN methods create shared low-level modules and distinct high-level modules, causing an exponential increase in model parameters with the addition of tasks. This approach also limits feature interaction at the high level, hindering the exploration of semantic relations among attributes, thereby affecting generalization negatively. In response, this study introduces FAR-AMTN, a novel Attention Multi-Task Network for FAR. It incorporates a Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to minimize complexity while improving group feature representation. Furthermore, a Cross-Group Feature Fusion (CGFF) module is utilized to foster interactions between attribute groups, enhancing feature learning. A Dynamic Weighting Strategy (DWS) is also introduced for synchronized task convergence. Experiments on the CelebA and LFWA datasets demonstrate that the proposed FAR-AMTN demonstrates superior accuracy with significantly fewer parameters compared to existing models.

</details>


### [86] [EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding](https://arxiv.org/abs/2601.01547)
*Tianjun Gu,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: 提出“目的-空间智能（TSI）”范式与EscherVerse基准/数据/模型套件，用于评测与推动模型对物理动态与人类意图的联合推理。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理多聚焦物体交互的物理规律，忽视驱动这些变化的人类意图；现实场景开放且动态，缺乏能系统评测“意图驱动推理”的资源与基准。

Method: 提出TSI，统一两类推理：物理-动态推理与意图驱动推理；构建EscherVerse：包含开放世界基准Escher-Bench、来源于真实视频的数据集Escher-35k，以及对应模型Escher系列；设计新型数据筛选/标注流水线，用视频任务显式考察物体永恒性、状态转移、轨迹预测，并将事件与潜在人类目标关联。

Result: 形成大规模现实视频数据与评测套件，首次系统化评测“意图驱动推理”；实证细节未给出，但声称能推动模型在动态、人本场景中的物理与意图联合推理能力评估。

Conclusion: TSI与EscherVerse为从被动场景描述迈向目的导向的世界理解提供基础设施，促进在开放、动态、人类中心情境下的空间智能研究。

Abstract: The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.

</details>


### [87] [Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation](https://arxiv.org/abs/2601.01593)
*Haonan Cai,Yuxuan Luo,Zhouhui Lian*

Main category: cs.CV

TL;DR: 提出GAR-Font：一种多模态自回归少样本字体生成框架，借助全局感知分词、语言-风格适配器与后处理精修，在结构保持与风格一致性上优于现有方法，并能利用文本风格指引生成更高质量字体。


<details>
  <summary>Details</summary>
Motivation: 现有少样本字体生成难以同时保持字形结构与整体风格一致性；AR模型在该任务中受限于局部补丁分词，缺乏全局依赖；主流方法仅依赖图像参考，忽视语言在表达风格意图中的作用。

Method: 1) 全局感知tokenizer：在分词阶段编码局部结构与全局风格依赖，克服传统patch级分词的局限。2) 多模态风格编码器：结合视觉参考与文本描述，通过轻量级语言-风格适配器实现可控风格，无需重型多模态预训练。3) 自回归生成+后期精修：AR生成初稿后，通过后处理提升结构保真与风格一致。

Result: 在广泛实验中，相比现有FFG方法，GAR-Font在全局风格忠实度、结构一致性与文本引导质量方面取得更优表现。

Conclusion: 融合全局感知建模与多模态风格控制的自回归框架能显著提升少样本字体生成的质量与可控性；文本风格提示是有效补充，配合后期精修可进一步保证结构与风格的统一。

Abstract: Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.

</details>


### [88] [Guiding Token-Sparse Diffusion Models](https://arxiv.org/abs/2601.01608)
*Felix Krause,Stefan Andreas Baumann,Johannes Schusterbauer,Olga Grebenkova,Ming Gui,Vincent Tao Hu,Björn Ommer*

Main category: cs.CV

TL;DR: 提出稀疏引导（Sparse Guidance, SG）以解决稀疏训练扩散模型在推理时对无分类器引导（CFG）响应不足的问题；通过token级稀疏作为引导信号，兼顾质量、方差与算力效率，在ImageNet-256上以更少FLOPs达更好FID，并在大规模文生图模型中提升构图与人偏好同时提高吞吐。


<details>
  <summary>Details</summary>
Motivation: 稀疏训练能显著降低扩散模型的训练成本，但会削弱模型在推理阶段对CFG的响应，导致生成质量下降与方差不足。需要一种既与稀疏训练兼容又能在推理时有效引导的机制，以恢复或提升生成质量并降低推理算力。

Method: 引入Sparse Guidance：将传统CFG中的条件dropout替换为token级稀疏信号，保留条件预测的高方差特性；在训练与推理均可利用token级稀疏（训练时用稀疏子集学习，推理时以稀疏引导计算），从而在不增加或减少FLOPs的同时提升有效引导强度与样本多样性。

Result: 在ImageNet-256基准上，以25%更少FLOPs达到1.58 FID；在与基线质量匹配时可节省最多58% FLOPs。对一个25亿参数的文生图扩散模型，采用训练期稀疏与推理期SG，提升构图质量和人类偏好评分，同时提高吞吐。

Conclusion: SG以token级稀疏替代CFG的条件dropout作为引导信号，缓解稀疏训练模型在推理中的引导不足问题，实现更高质量与多样性的生成，并显著降低推理计算成本，适用于大规模文生图场景。

Abstract: Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.

</details>


### [89] [CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment](https://arxiv.org/abs/2601.01613)
*Kazi Ramisa Rifa,Jie Zhang,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出CAP-IQA：将文本先验与实例级上下文提示结合，并用因果去偏，在腹部CT图像上评估诊断可见性、解剖清晰度与噪声感知。采用CNN视觉编码器与领域文本编码器的上下文感知融合，LDCTIQA 2023基准上相关性总分2.8590，超越榜首4.24%；在9.15万张儿科CT上验证泛化。


<details>
  <summary>Details</summary>
Motivation: 现有CT图像质量评估中基于提示的先验嵌入较少，且理想化文本先验易在现实退化（噪声、运动、机型差异）下引入偏差，难以准确反映图像的客观质量与诊断可用性。

Method: 提出CAP-IQA：1) 文本层面放射学风格先验提示；2) 实例级上下文提示捕获图像特异退化；3) 因果去偏，将理想化知识与事实退化因素分离；4) CNN视觉编码器+领域文本编码器，进行语义-感知对齐与上下文感知融合；5) 仅编码器的简化架构以提升可解释性与特征对齐。评估指标覆盖诊断可见性、解剖清晰度、噪声感知。

Result: 在LDCTIQA 2023基准上，PLCC+SROCC+KROCC总分2.8590，较排行榜第一2.7427提升4.24%；消融实验证明提示引导融合与简化编码器设计均提升性能与可解释性；在9.15万张儿科CT自有数据上展现良好泛化与感知保真评估能力。

Conclusion: 上下文感知的提示引导与因果去偏能有效缓解文本先验偏差，增强语义-感知对齐，显著提升CT IQA的相关性与可解释性，并具备跨人群与数据集的泛化能力。

Abstract: Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.

</details>


### [90] [An Empirical Study of Monocular Human Body Measurement Under Weak Calibration](https://arxiv.org/abs/2601.01639)
*Gaurav Sekar*

Main category: cs.CV

TL;DR: 本文对三种“弱校准”单目RGB人体测量策略进行实证比较：基于关键点的几何、姿态驱动回归、以及以物标定的轮廓法；重点不在SOTA精度，而在不同校准假设对测量稳定性、鲁棒性与失效模式的影响与取舍。结论：用户校准投入与围度等量的稳定性存在明确权衡，为消费级设备上的轻量级测量系统提供设计参考。


<details>
  <summary>Details</summary>
Motivation: 单目RGB缺乏显式深度，且存在尺度与视角敏感性，导致在消费级场景下做人体围度/尺寸估计困难。现有工作多追求精度或特定方法，缺少在统一条件下对不同弱校准策略的系统性对比与设计启示。

Method: 在半受控条件、消费级相机下，分别实现并评测三类策略：1) 关键点几何：由2D人体关键点配合假设比例/人体先验推算尺寸；2) 姿态驱动回归：利用估计的人体姿态或特征，通过学习型回归直接预测尺寸；3) 以物标定的轮廓：在画面中放置已知尺度的常见物体进行尺度恢复，再基于人体分割/轮廓计算围度与长度。比较不同校准假设下的测量行为、稳健性与失效模式。

Result: 三策略在稳定性与用户负担之间形成梯度：以物标定的轮廓在有外部参照时围度最稳定、对体型变化鲁棒，但需用户准备与摆放参照物；关键点几何对姿态与关键点定位误差敏感，特别在非标准体型与遮挡下波动较大；姿态回归在平均误差上可观，但对训练分布外体型与视角存在域偏移，易产生系统性偏差。整体显示校准投入越高，围度等量的方差越低。

Conclusion: 单目人体测量无法回避尺度/视角不确定性，弱校准策略需在用户交互成本与测量稳定性间权衡。对消费级应用，推荐在轻交互场景采用最小物标定（如标准A4、手机等）结合稳健分割；在零交互场景，需结合不确定性估计与失效检测以缓解域偏移与姿态/关键点误差。本研究提供面向落地系统的经验法则与设计参考。

Abstract: Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.

</details>


### [91] [Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows](https://arxiv.org/abs/2601.01660)
*Aymen Mir,Riza Alp Guler,Jian Wang,Gerard Pons-Moll,Bing Zhou*

Main category: cs.CV

TL;DR: 提出Deep Gaussian Shadow Maps（DGSM），在3D Gaussian Splatting(3DGS)表示中实现一致的体积阴影与照明，用于动态头像与场景/插入物体交互，实时渲染无须网格化。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在动态人物与场景交互时缺乏一致的阴影与环境光照，传统网格/离线方法难以实时且与体渲染一致；需要一种能在3DGS体表示内原生处理投/受阴影与重光照的方案。

Method: 1) 提出DGSM：基于深度阴影映射思想，利用3DGS在光线方向的闭式光能累积与透射计算，按光源为中心预计算径向同心壳层的透射率，并存入八面体贴图图集；GPU实时采样以对受影响的高斯体进行衰减，产生一致投/受阴影。2) 动态头像重光照：用SH基的HDRI探针近似局部环境光，执行快速逐高斯辐射传输，无需显式BRDF或离线优化。3) 全流程在体积3DGS域内，无需网格化。

Result: 在AvatarX、ActorsHQ头像与ScanNet++、DL3DV、SuperSplat等场景中实现环境一致的阴影与重光照；支持单/多头像与插入物体交互，实时渲染表现出连贯阴影与光照一致性。

Conclusion: DGSM结合SH重光照在3DGS体域内实现实时、网格无关的阴影与光照一致性，为动态头像与场景/插入物体交互提供高质量、可扩展的体积渲染解决方案。

Abstract: We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.

</details>


### [92] [LabelAny3D: Label Any Object 3D in the Wild](https://arxiv.org/abs/2601.01676)
*Jin Yao,Radowan Mahmud Redoy,Sebastian Elbaum,Matthew B. Dwyer,Zezhou Cheng*

Main category: cs.CV

TL;DR: 提出LabelAny3D，通过“分析-合成”重建整体3D场景，从2D图像自动生成高质量3D框框，并据此构建开放词汇单目3D检测基准COCO3D；在多基准上提升单目3D检测并优于既有自动标注方法。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测在真实开放环境中表现受限，原因是缺乏大规模、开放词汇、贴近野外分布的3D数据集以及昂贵困难的3D标注。需要一种可扩展、低人工成本、可泛化到多类别与复杂场景的标注方案。

Method: 提出LabelAny3D，一种analysis-by-synthesis流程：从单幅2D图像重建完整3D场景（利用基础模型能力），进而合成或优化出与图像一致的3D几何与位姿，自动产生高质量3D包围盒；据此将MS-COCO转化为COCO3D，覆盖现有3D数据集中缺失的大量类别，实现开放词汇单目3D检测标注。

Result: 自动生成的标注在多个基准上显著提升单目3D检测性能，并在标注质量上超过以往自动标注方法；表明该流程在多场景、多类别上有效。

Conclusion: 以基础模型驱动的analysis-by-synthesis标注可规模化获取高质量3D框，从而推动开放世界的单目3D识别与检测研究；COCO3D为开放词汇3D检测提供了新的标准基准。

Abstract: Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.

</details>


### [93] [Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada](https://arxiv.org/abs/2601.01677)
*Zhengsen Xu,Lanying Wang,Sibo Cheng,Xue Rui,Kyle Gao,Yimin Zhu,Mabel Heffring,Zack Dewis,Saeid Taleghanidoozdoozan,Megan Greenwood,Motasem Alkayid,Quinn Ledingham,Hongjie He,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出一个可信的数据驱动野火风险预测框架，在加拿大西部2023–2024极端火季上，以低计算成本显著优于现有时序方法（F1=0.90、PR-AUC=0.98），并显式量化不确定性与提供可解释的过程级洞察。


<details>
  <summary>Details</summary>
Motivation: 西加拿大野火加剧带来巨大损失；野火点燃与蔓延具随机性，燃料—气象—气候—地形—人类活动的非线性交互使纯数据驱动模型可靠性与可解释性受限，需要既准确又可信、可解释并能量化不确定性的预测方法。

Method: 构建长序列、多尺度时间建模框架，融合异质驱动因子（温度、湿度/燃料含水量、气象、气候变率、地形、人类活动等），在模型中显式估计预测不确定性，并以SHAP实现过程层面的可解释；在西加拿大2023与2024火季数据上评估，与现有时间序列方法对比。

Result: 在2023与2024创纪录火季上取得F1=0.90、PR-AUC=0.98，计算成本低；不确定性感知分析揭示空间与季节性的置信结构：在决策边界与模糊情形下不确定性增加；SHAP显示温度相关因子主导两年总体风险，而2024中湿度约束更强，解释了空间与土地覆被差异；2023呈更广泛的高温干燥主控。

Conclusion: 该框架在保持高精度与低成本的同时，提供不确定性量化与机制可解释性，适用于复杂驱动下的野火风险预测；开源数据与代码可复现与拓展（https://github.com/SynUW/mmFire）。

Abstract: In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.

</details>


### [94] [Evaluating Deep Learning-Based Face Recognition for Infants and Toddlers: Impact of Age Across Developmental Stages](https://arxiv.org/abs/2601.01680)
*Afzal Hossain,Mst Rumana Sumi,Stephanie Schuckers*

Main category: cs.CV

TL;DR: 评估FaceNet/ArcFace/MagFace/CosFace在0–3岁纵向数据上的识别与验证，幼龄段表现差、随年龄提升；时间间隔越短越准；用DANN缓解嵌入漂移，TAR提升约12%，为智慧城市中儿童生物识别提供依据。


<details>
  <summary>Details</summary>
Motivation: 婴幼儿面部随发育快速变化、类别间相似度高且数据稀缺，现有成人/青少年模型泛化不足；需要量化不同年龄与时间间隔下识别可靠性，并探索提升跨时间稳定性的手段以满足医疗、安防与数字身份等场景。

Method: 构建0–3岁、为期24个月、7次会话的纵向儿童人脸数据集；评测FaceNet、ArcFace、MagFace、CosFace在不同年龄段与不同时间间隔下的TAR@FAR；分析嵌入随时间漂移；引入域对抗神经网络（DANN）以学习时间不变特征并比较性能。

Result: 0–6月龄TAR@0.1% FAR仅30.7%，2.5–3岁提升至64.7%；验证间隔越短准确率越高，表明嵌入随时间漂移显著；采用DANN后TAR提升超过12%，特征更稳定、可泛化。

Conclusion: 标准深度人脸模型在婴幼儿上随年龄与时间间隔表现显著波动，早期月龄尤其困难；通过对抗域适配可缓解时间漂移，但仍不足以满足严格应用。需在隐私保护、时变稳健特征学习与数据采集规范上持续研究，以支撑智慧城市中儿童身份验证的可靠部署。

Abstract: Face recognition for infants and toddlers presents unique challenges due to rapid facial morphology changes, high inter-class similarity, and limited dataset availability. This study evaluates the performance of four deep learning-based face recognition models FaceNet, ArcFace, MagFace, and CosFace on a newly developed longitudinal dataset collected over a 24 month period in seven sessions involving children aged 0 to 3 years. Our analysis examines recognition accuracy across developmental stages, showing that the True Accept Rate (TAR) is only 30.7% at 0.1% False Accept Rate (FAR) for infants aged 0 to 6 months, due to unstable facial features. Performance improves significantly in older children, reaching 64.7% TAR at 0.1% FAR in the 2.5 to 3 year age group. We also evaluate verification performance over different time intervals, revealing that shorter time gaps result in higher accuracy due to reduced embedding drift. To mitigate this drift, we apply a Domain Adversarial Neural Network (DANN) approach that improves TAR by over 12%, yielding features that are more temporally stable and generalizable. These findings are critical for building biometric systems that function reliably over time in smart city applications such as public healthcare, child safety, and digital identity services. The challenges observed in early age groups highlight the importance of future research on privacy preserving biometric authentication systems that can address temporal variability, particularly in secure and regulated urban environments where child verification is essential.

</details>


### [95] [FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation](https://arxiv.org/abs/2601.01687)
*Abdur R. Fayjie,Pankhi Kashyap,Jutika Borah,Patrick Vandewalle*

Main category: cs.CV

TL;DR: FALCON提出跨域少样本3D医学分割：用2D切片处理3D体数据，先在自然图像做元学习，再通过对抗与边界感知微调到医学域；推理时利用支持集线索进行任务自适应，达到更精准边界、更低计算与标注成本。


<details>
  <summary>Details</summary>
Motivation: 3D医学分割受限于3D标注稀缺、个体差异、隐私限制与计算开销大，现有AI难以临床落地；需要能在少标注、低算力且具跨域泛化与患者特异适应性的方案。

Method: 提出FALCON框架：1) 以2D切片方式处理3D体数据；2) 在自然图像上进行元训练，学习可迁移的分割先验；3) 通过对抗式微调与边界感知学习迁移到医学域；4) 任务感知推理，基于支持集线索在切片间自适应以应对患者特异解剖差异。

Result: 在四个基准上稳定获得最低的Hausdorff距离（更优边界精度），同时Dice与SOTA相当；无需数据增强、标注更少且计算负担更低。

Conclusion: FALCON在少标注与低算力条件下实现高精度3D医学分割，尤其提升边界质量，并具备跨域迁移与患者特异适配能力，显示出更强的实际临床应用潜力。

Abstract: Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.

</details>


### [96] [Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data](https://arxiv.org/abs/2601.01689)
*Afzal Hossain,Stephanie Schuckers*

Main category: cs.CV

TL;DR: 研究评估在儿童纵向人脸识别中，合成人脸数据是否能作为“纵向稳定器”，以降低随时间推移的模板漂移与验证错误。结果显示：在YFA数据集上，用StyleGAN2-ADA生成并过滤的合成数据与真实数据共同微调MagFace，可在6–36个月间隔显著降低错误率，优于仅预训练或仅真实数据微调。


<details>
  <summary>Details</summary>
Motivation: 儿童面部随时间快速且非线性变化，导致模板漂移与验证性能随时间劣化；现有模型在跨月/年匹配上鲁棒性不足。作者希望检验：受控、去泄露的合成人脸能否提升时间鲁棒性，从而增强身份持久性、降低长期验证错误。

Method: 在YFA数据集上采用身份互斥协议。比较三种设置：(i) 直接使用预训练MagFace嵌入；(ii) 仅用真实儿童人脸对MagFace进行微调；(iii) 用真实+合成（StyleGAN2-ADA生成、并经过滤以减轻身份泄露与伪影）的数据微调MagFace。评估不同注册-验证时间间隔（6–36个月）的性能。

Result: 相较于预训练基线与仅真实数据微调，加入合成数据的微调在全部或大多数时间间隔显著降低验证错误率，表现出更强的纵向鲁棒性与身份持久性。

Conclusion: 经风险感知的合成数据增强（含生成后筛选）可作为有效的纵向稳定器，提升儿童人脸识别在长期间隔下的可靠性；提示在儿少人脸识别中，合成数据与真实数据的混合训练具有实际价值。

Abstract: Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.

</details>


### [97] [Learnability-Driven Submodular Optimization for Active Roadside 3D Detection](https://arxiv.org/abs/2601.01695)
*Ruiyu Mao,Baoming Zhang,Nicholas Ruozzi,Yunhui Guo*

Main category: cs.CV

TL;DR: 提出一种面向路侧单目3D目标检测的“可学性驱动”主动学习框架LH3D，在仅标注25%数据下接近全量性能，并显著优于基于不确定性的选择策略。


<details>
  <summary>Details</summary>
Motivation: 实际部署常因硬件/隐私只能获取路侧数据，缺少车载视角使许多远距、模糊、遮挡目标在单视角下本质含糊，连专家也难以可靠标注，导致标注成本高且存在“可学性”瓶颈。现有主动学习多依赖预测不确定性，无法区分本质不可判的样本，造成标注浪费。

Method: 面向路侧单目3D检测，设计以“可学性”为核心的主动学习框架：在样本选择时同时考虑信息量、可可靠标注性与覆盖性，显式抑制“先天含糊”的样本并保证场景多样性；将该策略应用到DAIR‑V2X‑I路侧数据增量标注与训练流程中。

Result: 在DAIR‑V2X‑I数据集上，仅用25%标注预算即可达到相对于全量标注的性能：车辆86.06%、行人67.32%、骑行者78.67%；显著优于基于不确定度的基线方法。

Conclusion: 路侧3D感知中，关键在于选择“可学且可可靠标注”的样本，单纯的不确定性并不等价于可学性。LH3D通过可学性驱动的主动学习有效减少无效标注并保持高性能。

Abstract: Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.

</details>


### [98] [Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems](https://arxiv.org/abs/2601.01696)
*Yian Liu,Xiong Wang,Ping Xu,Lei Zhu,Ming Yan,Linyun Xue*

Main category: cs.CV

TL;DR: 提出一个用于嵌入式端实时车道线检测的协方差分布优化（CDO）模块，在不增加计算量的情况下，通过对齐特征分布与标签分布提升精度，跨三类方法、六个模型与三大数据集验证，精度提升约0.01%–1.5%，易集成、对功耗友好。


<details>
  <summary>Details</summary>
Motivation: 嵌入式场景下车道线在RGB图像中信号稀疏、细微，且受算力与功耗限制；现有分割/锚点/曲线类方法缺乏通用、可移植且面向低功耗的优化手段。

Method: 设计CDO模块：在训练时利用模型现有参数与特征，度量并对齐车道相关特征与标注之间的协方差/分布差异，使特征分布更贴近标签分布，从而在不改动网络结构与推理复杂度的前提下提升判别性与稳健性。模块可无缝插入多种架构，作为正则或分布对齐损失使用。

Result: 在三类方法（分割、锚点、曲线）共六个模型（含两个实时优化模型与四个SOTA）上，于CULane、TuSimple、LLAMAS进行评测，带来0.01%–1.5%的精度提升；不增加推理计算，保持实时性与低功耗。

Conclusion: CDO是一种通用、低开销、易集成的分布对齐优化模块，能在嵌入式实时车道线检测中提升精度与能效，无需结构改动，适合在现有系统中持续训练与部署。

Abstract: Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.

</details>


### [99] [FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing](https://arxiv.org/abs/2601.01720)
*Xijie Huang,Chengming Xu,Donghao Luo,Xiaobin Hu,Peng Tang,Xu Peng,Jiangning Zhang,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 论文提出了一个真正“免引导”的首帧传播(FFP)视频编辑框架，依托新建的大规模高质量数据集FFP-300K，并在模型中引入AST-RoPE与自蒸馏的身份传播正则，以同时保持首帧外观与源视频运动，在EditVerseBench上显著超越学术与商用基线。


<details>
  <summary>Details</summary>
Motivation: 现有FFP方法在推理时需依赖笨重的运行时引导，其根因是训练数据集太短、分辨率低、任务多样性不足，导致模型难以学习稳健的时序先验与长期一致性。

Method: 1) 数据：构建FFP-300K，包含30万对720p、81帧的视频对，采用双轨管线覆盖多样的局部与全局编辑。2) 架构：提出自适应时空旋转位置编码(AST-RoPE)，动态重映射位置编码，以解耦外观(首帧)与运动(源视频)。3) 目标：引入自蒸馏的身份传播任务作为强正则，提升长期时序稳定，抑制语义漂移，实现无引导FFP。

Result: 在EditVerseBench上，方法较学术与商用模型取得约+0.2 PickScore与+0.3 VLM分的提升，展示更好的外观保持、运动保真与长期稳定性。

Conclusion: 补齐数据短板并在架构与训练目标上协同设计，可实现真正免引导的FFP视频编辑；FFP-300K与AST-RoPE+自蒸馏正则是关键，带来显著的主客观指标提升。

Abstract: First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.

</details>


### [100] [Point-SRA: Self-Representation Alignment for 3D Representation Learning](https://arxiv.org/abs/2601.01746)
*Lintong Wei,Jian Lu,Haozhe Cheng,Jihua Zhu,Kaibing Zhang*

Main category: cs.CV

TL;DR: 提出Point-SRA：通过多掩码比例MAE、MeanFlow Transformer概率重建与双重自蒸馏对齐，学习互补几何与语义表示，并在微调阶段用流模型条件化，显著提升分类、分割与检测性能。


<details>
  <summary>Details</summary>
Motivation: 固定掩码比的3D MAE忽视不同层级表示的互补性与点云固有几何结构；点对点重建假设与点云多样性冲突，限制表示能力。

Method: 1) 多掩码比MAE以捕获互补几何/语义信息；2) 引入MeanFlow Transformer（MFT），利用跨模态条件嵌入进行多样化概率重建；3) 发现MFT不同时间步表示互补，提出在MAE层面与MFT层面的双重自表示对齐（Dual Self-Representation Alignment）；4) 设计Flow-Conditioned Fine-Tuning，在下游微调时利用MeanFlow学到的分布信息。

Result: 在ScanObjectNN上较Point-MAE提升5.37%；颅内动脉/动脉瘤分割mIoU分别为96.07%/86.87%；3D检测AP@50达47.3%，较MaskPoint提升5.12%。

Conclusion: 通过自蒸馏对齐与流式概率重建结合多掩码策略，Point-SRA能学习更全面、互补的3D表示，显著优于现有MAE类方法并在多任务上泛化良好。

Abstract: Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.

</details>


### [101] [MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement](https://arxiv.org/abs/2601.01749)
*Lei Zhu,Lijian Lin,Ye Zhu,Jiahao Wu,Xuehan Hou,Yu Li,Yunfei Liu,Jie Chen*

Main category: cs.CV

TL;DR: 提出MANGO两阶段框架与MANGO-Dialog数据集，实现多说话人、可交互的音频驱动3D对话头像，利用图像级监督与交替训练替代伪3D标签，显著提升双人对话中说-听状态的自然过渡、精细表情与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多针对单说话人，缺少双向“听-说”互动；对话头像常依赖噪声较大的伪3D标注，无法刻画细粒度面部动态，导致口型-表情-头部运动不自然、状态切换不连贯。

Method: 提出两阶段MANGO框架：1) 阶段一：扩散式Transformer结合双音频交互模块，从多说话人音频建模自然3D运动；2) 阶段二：利用快速3D高斯渲染器生成高保真图像，以2D光度级监督对3D运动进行交替训练，缓解伪3D标签噪声并贴合真实对话行为；同时构建MANGO-Dialog数据集（50+小时、500+身份、2D-3D对齐）。

Result: 在双人3D对话运动建模上取得高精度与高真实感，较现有方法在自然度、对齐度与可控性上显著提升。

Conclusion: 纯图像级监督配合交替训练可有效替代伪3D标注，实现更自然的说-听状态过渡；MANGO与MANGO-Dialog为高保真、可控的音频驱动对话头像提供了新范式。

Abstract: Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.

</details>


### [102] [CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology](https://arxiv.org/abs/2601.01769)
*Hao Lu,Ziniu Qian,Yifu Li,Yang Zhou,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: 提出基于临床报告模板的管线，构建标准化病理要素，衍生出用于视觉-语言对齐的CTIS-Align与VQA基准CTIS-Bench，并提出双流滑片级问答模型CTIS-QA，在多项数据集与任务上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 病理报告信息零散且缺乏标准化，限制了从真实临床数据中构建高质量的视觉-语言训练数据与可靠评测基准；现有WSI-VQA常包含开放性或需要外部知识的题目，难以反映真实诊断流程。

Method: 与病理专家协作并遵循CAP规范，设计临床病理报告模板（CPRT），按模板从病理报告中系统抽取要素；基于抽取要素构建两套资源：CTIS-Align（804张WSI的约8万滑片-描述对）用于视觉-语言对齐训练，CTIS-Bench（977张WSI、14,879个QA）作为临床闭集、强调视觉证据的VQA基准；同时提出CTIS-QA双流模型：一条流通过聚类特征汇聚获取全局上下文，另一条流用注意力引导的patch感知模块聚焦显著局部区域。

Result: 在WSI-VQA、CTIS-Bench及滑片级诊断任务上，CTIS-QA在多项指标上持续优于现有SOTA；所建资源支持更可靠的训练与评测，验证了模板化抽取与双流建模的有效性。

Conclusion: 模板化的临床病理信息抽取能高质构建训练与评测数据；结合全局-局部感知的CTIS-QA更契合病理诊断过程，显著提升WSI问答与诊断表现；代码与数据已开源，具备复现与扩展价值。

Abstract: In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.

</details>


### [103] [Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery](https://arxiv.org/abs/2601.01781)
*Lakshay Sharma,Alex Marin*

Main category: cs.CV

TL;DR: 提出一种自监督预训练任务“子图重叠预测”，在遥感语义分割中用更少预训练数据即可达到更快收敛与更高/相当mIoU。


<details>
  <summary>Details</summary>
Motivation: 现有自监督方法普遍依赖海量预训练数据，成本高，且在遥感分割场景中数据获取与标注昂贵，需一种数据效率更高的SSL任务以提升小样本与低标注条件下的表现与收敛速度。

Method: 从原始图像中裁剪一个子图（sub-image），模型被训练去预测该子图在原图中的空间位置语义掩码（即子图与原图的重叠区域/位置）。将此作为预训练目标，再将预训练权重迁移到下游语义分割任务。实验覆盖多种网络架构与多个数据集。

Result: 与无预训练或其他SSL相比，该方法在下游分割任务上实现显著更快的收敛，并在mIoU上持平或更优；当标注数据减少时优势更明显；同时所需预训练数据量显著更少也能匹配/超越其他SSL。

Conclusion: 子图重叠预测是一种数据高效的自监督预训练任务，能在遥感语义分割中以更少预训练数据获得更快收敛和更优/相当性能，具有良好的跨架构与跨数据集泛化潜力。

Abstract: Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.

</details>


### [104] [DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization](https://arxiv.org/abs/2601.01784)
*Boyang Zhao,Xin Liao,Jiaxin Chen,Xiaoshuai Wu,Yufeng Wu*

Main category: cs.CV

TL;DR: 提出DDNet，通过双流图学习与特征解耦实现视频时间篡改精准定位，显著提升AP@0.95与跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视频只在少量片段被AIGC篡改会导致整段级检测失效，需精确到时间轴定位；现有方法局限于局部视角，难以捕获全局异常。

Method: 设计双流框架DDNet：1) Temporal Distance Stream建模相邻帧局部伪迹与时间距离关系；2) Semantic Content Stream构建长程语义连接捕获全局线索；并提出TDA（Trace Disentanglement and Adaptation）解耦并适配通用伪造指纹；CLFE（Cross-Level Feature Embedding）深度融合多层级特征以构建稳健表示，整体以图学习协调两流信息，进行时间篡改定位。

Result: 在ForgeryNet与TVIL基准上，相比SOTA在AP@0.95提升约9%，并在跨域场景表现更稳健。

Conclusion: 双流图学习结合解耦与跨层特征嵌入能同时抓住局部伪迹与全局异常，提升时间篡改定位精度与泛化能力。

Abstract: The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \emph{local view}, failing to capture global anomalies. To address this, we propose a \underline{d}ual-stream graph learning and \underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \emph{Temporal Distance Stream} for local artifacts and a \emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\% in AP@0.95, with significant improvements in cross-domain robustness.

</details>


### [105] [VerLM: Explaining Face Verification Using Natural Language](https://arxiv.org/abs/2601.01798)
*Syed Abdul Hannan,Hazim Bukhari,Thomas Cantalapiedra,Eman Ansar,Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.CV

TL;DR: 提出一种用于人脸核验的视觉-语言模型（VLM），在判断两张脸是否同一人时，同时给出简洁与详尽两种可解释说明；跨模态迁移自音频差异化方法并适配视觉，性能与可解释性均优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有人脸核验虽准确但缺乏决策透明度，难以解释为何判同/不同人，影响信任、审计与合规。作者希望在保证精度的同时提供可理解的、可审计的决策依据。

Method: 构建面向人脸核验的VLM：融合先进视觉特征提取与语言推理模块；设计双风格解释训练目标（简洁摘要式+详尽差异式）；将一套用于音频区分的最先进方法进行跨模态适配到图像；训练时联合优化核验准确度与解释质量。

Result: 在与多种基线和现有模型对比中，实现更高的核验准确率，并能生成清晰、条理的决策说明；跨模态迁移带来显著精度与可解释性提升。

Conclusion: VLM在面部核验场景中可同时提升准确性与可解释性，促进更透明、可靠的核验系统；双风格解释和跨模态迁移是关键。

Abstract: Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.

</details>


### [106] [Causality-Aware Temporal Projection for Video Understanding in Video-LLMs](https://arxiv.org/abs/2601.01804)
*Zhengjian Kang,Qi Chen,Rui Liu,Kangtong Mo,Xingyu Zhang,Xiaoyu Deng,Ye Zhang*

Main category: cs.CV

TL;DR: V-CORE通过显式时间顺序约束提升Video-LLM的时序与因果推理，采用LSA压缩空间冗余与CATP实现单向时序汇聚，在低资源(4-bit QLoRA+冻结LLM)下训练高效，并在NExT-QA等基准上取得显著提升，尤其在时序/因果子任务。


<details>
  <summary>Details</summary>
Motivation: 现有许多参数高效的Video-LLM使用双向投影器处理帧间交互，导致后帧信息影响前帧表示，时间顺序被模糊，缺乏尊重视频因果与方向性的结构化机制，限制了需要严格时序一致性的任务表现。

Method: 提出V-CORE框架，包含：1) 可学习空间聚合(LSA)：自适应选择并聚合显著空间token以去冗余、保留帧内细节；2) 因果感知时间投影器(CATP)：通过分块因果注意力与末端动态汇总token(因果“汇”/“sink”)，强制单向信息流，保证严格的时间顺序聚合。结合4-bit QLoRA并冻结LLM骨干，实现单卡高效训练。

Result: 在NExT-QA上达61.2%准确率；在MSVD-QA、MSRVTT-QA、TGIF-QA上具竞争力；在时序与因果推理子类分别提升+3.5%、+5.2%，显示显式时间约束的有效性。

Conclusion: 显式的时间顺序与因果结构约束对视频理解至关重要。V-CORE以LSA减少空间冗余、以CATP实现单向时序聚合，在保持参数与算力友好的同时，显著提升了涉及时序/因果推理的任务表现。

Abstract: Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.

</details>


### [107] [Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification](https://arxiv.org/abs/2601.01807)
*Ubaidullah,Muhammad Abid Hussain,Mohsin Raza Jafri,Rozi Khan,Moid Sandhu,Abd Ullah Khan,Hyundong Shin*

Main category: cs.CV

TL;DR: 提出LUMPNet：结合YOLOv11目标检测、EfficientNet分类与自适应混合优化器，用于早期识别牛结节性皮肤病（LSD），在公开数据上获得约99%训练、98%验证准确率，优于对比方法。


<details>
  <summary>Details</summary>
Motivation: LSD传播快、危害畜牧业与粮食安全，早期、精准识别至关重要。现有方法在早期病灶定位与稳健训练方面仍有限，需要端到端、高效且稳定的检测与分类框架。

Method: 构建混合深度学习框架LUMPNet：1) 使用YOLOv11在牛体图像中检测并定位皮肤结节/病灶；2) 将检测到的区域或裁剪图输入EfficientNet（带复合缩放）进行二分类（LSD/健康）；3) 设计并应用“自适应混合优化器”以稳定并加速YOLOv11与EfficientNet的联合训练；4) 在公开数据集、不同病程阶段进行评测，并以优化的EfficientNet-B0+AdamW作为对照。

Result: LUMPNet训练准确率99%、验证准确率98%，整体性能优于现有方案与对照模型（EfficientNet-B0+AdamW）。

Conclusion: LUMPNet能在早期基于图像有效检测与分类LSD病灶，性能优于对比方法；自适应混合优化器有助于训练稳定与收敛加速，具备在畜牧业早期预警中的应用潜力。

Abstract: Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.

</details>


### [108] [Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning](https://arxiv.org/abs/2601.01818)
*Sungjune Park,Hongda Mao,Qingshuang Chen,Yong Man Ro,Yelin Kim*

Main category: cs.CV

TL;DR: 提出一种语言引导的场景上下文感知学习框架，用于第一视角视觉注意力预测：用语言描述引导的视频上下文编码器，配合“聚焦目标兴趣点+抑制无关干扰”的双重训练目标，在Ego4D与AEA上达成SOTA且更稳健。


<details>
  <summary>Details</summary>
Motivation: 第一视角视频场景动态复杂、目标模糊，传统方法难以稳定预测佩戴者的注意力。心理与视觉证据表明场景语义上下文能调制人类注意分配，因此引入语言描述以高层语义对视频进行约束，有望缓解歧义并提升鲁棒性。

Method: 1) 语言引导的“context perceiver”（上下文感知器）：以场景文字描述为条件对自我中心视频进行摘要，生成上下文感知的表示；2) 两个训练目标：a) 促进模型聚焦目标兴趣点（POI）区域；b) 抑制与第一视角注意无关的干扰区域，从而提升判别性与鲁棒性。

Result: 在Ego4D与Aria Everyday Activities数据集上进行大量实验，取得当前最优（SOTA）性能，并在多样、动态的一人称场景中表现出更强鲁棒性。

Conclusion: 语言引导的场景上下文建模能够有效缓解第一视角注意预测中的歧义与噪声，通过聚焦-抑制的双目标训练提升准确性与稳健性，验证了跨模态上下文对人类注意建模的价值。

Abstract: As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.

</details>


### [109] [RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images](https://arxiv.org/abs/2601.01835)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出一种用于天花猴(Mpox)皮损分类的定制残差SwinTransformerV2（RSwinV2），通过层级Transformer、移位窗口注意力与逆残差块融合，提升全局-局部特征联结与梯度稳定性，在Kaggle数据集上优于CNN与原Swin。


<details>
  <summary>Details</summary>
Motivation: Mpox皮损与水痘、麻疹、牛痘等外观相似，易误判；传统CNN或基础Swin在全局-局部模式耦合与梯度稳定方面不足，需要更高效、鲁棒的计算机辅助诊断方法提升区分度与临床可用性。

Method: 在SwinTransformerV2框架上定制层级结构：依据输入维度与目标输出调整嵌入与层深；将图像划分为非重叠patch，采用移位窗口与窗口内注意力连接跨窗信息；加入patch与位置嵌入以强化全局关联的多头注意力；引入逆残差块(IRB)与卷积跳连，缓解梯度消失并融合局部卷积先验与全局注意力。

Result: 在Kaggle公开数据集测试中，RSwinV2取得Accuracy 96.21、F1 95.62，超过标准CNN与SwinTransformer基线。

Conclusion: RSwinV2通过移位窗口注意力+位置/patch嵌入+IRB的混合设计，有效联通全局与局部模式、稳定训练并提升Mpox与相似疾病的可分性，展现为实用的计算机辅助皮损诊断工具。

Abstract: In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.

</details>


### [110] [ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting](https://arxiv.org/abs/2601.01847)
*Chuhang Ma,Shuai Tan,Ye Pan,Jiaolong Yang,Xin Tong*

Main category: cs.CV

TL;DR: 提出ESGaussianFace：基于3D Gaussian Splatting的情感与风格可控的语音驱动说话人头像生成，兼顾效率、质量与3D一致性，超越SOTA于口型准确度、表情变化与风格表现力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注中性情感的视频生成，难以高效地产生同时具备情感表达与风格特征且保持3D一致性的高质量说话人头像。为解决情感特征融合不足、口型与表情细节失真、风格迁移受限和训练不稳定的问题。

Method: 1) 使用3D Gaussian Splatting重建与渲染，保证高效与3D一致性；2) 情感-音频引导的空间注意力，将情感特征与音频内容融合，提升多情感状态下的面部细节重建；3) 两个3D Gaussian形变预测器，分别受情感与风格特征驱动，对Gaussian点进行几何/外观形变；4) 多阶段训练策略，分步学习口型、情感变化与风格特征。

Result: 生成的情感化、风格化说话人头像在效率、画质与3D一致性方面表现优异；在定量与定性实验中，相比SOTA在口型同步准确性、表情多样性与风格表达力上更优。

Conclusion: ESGaussianFace通过情感-音频注意力、双形变预测器与多阶段训练，在3D Gaussian框架下高效实现情感与风格可控的语音驱动面部动画，并显著优于现有方法。

Abstract: Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.

</details>


### [111] [GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection](https://arxiv.org/abs/2601.01856)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: 提出GCR：在持续类目扩展、类目未知的工业异常检测中，用共享冻结的patch嵌入空间做几何一致路由，先路由到合适专家再在该头内做原型式评分，显著提升路由稳定性、接近零遗忘，同时保持检测/定位表现。


<details>
  <summary>Details</summary>
Motivation: 现实部署中常需任务无关地处理不断增加的类别，测试时类目未知。性能常被“专家选择/路由”主导，但不同头的异常分数不可比（尺度/尾部分布差异大），导致错误路由与性能崩塌。需要一种不依赖端到端再训练、能稳定跨头决策的路由机制。

Method: 提出几何一致路由（GCR）：冻结大模型的patch-embedding空间；为每个类别维护原型库；对测试图像按patch到各类别原型的最近原型距离累积进行最小化以决定路由；随后仅在被选专家内用标准原型式异常评分生成异常图。通过将跨头决策与头内评分解耦，避免跨头分数可比性问题。

Result: 在MVTec AD与VisA上，GCR显著提升路由稳定性，缓解持续学习中的性能崩塌；在检测与定位上保持有竞争力，同时达到近乎零遗忘。

Conclusion: 持续异常检测中的许多失败源于跨头路由的决策不稳定而非表示遗忘。GCR以轻量、无需端到端训练的几何一致路由，实现稳定的任务无关推理与强健性能。

Abstract: Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.
  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.
  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR

</details>


### [112] [RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations](https://arxiv.org/abs/2601.01865)
*Wenlong Yang,Canran Jin,Weihang Yuan,Chao Wang,Lifeng Sun*

Main category: cs.CV

TL;DR: RRNet提出一种轻量、可配置的实时视频曝光/补光框架，通过少量虚拟光源参数+深度感知渲染实现局部重光照，在无像素对齐数据下保持高效与人脸身份一致性，并在多任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实时场景（如视频会议、移动端拍照）中存在光照不均、低照和眩光等问题；现有算法要么画质好但慢，要么快但难以精细控制曝光与局部补光，且需要昂贵的成对/对齐数据。需要一种可解释、可控、低成本训练且能在高分辨率实时运行的方法。

Method: 提出RRNet：1) 以少量虚拟点光源参数化场景光照（位置/强度/颜色等），以对象/人脸为中心的“对象感知”表述； 2) 深度感知渲染模块用估计的深度与虚拟光源进行局部重光照，无需像素对齐训练； 3) 轻量编码器+小型预测头回归光源参数； 4) 通过生成式AI数据管线合成多样光照训练数据，降低数据成本； 5) 结构具有可解释的光照控制接口。

Result: 在低照增强、局部光照调节、眩光去除等任务上，RRNet在视觉质量与效率上优于现有方法，支持高分辨率实时运行与人脸身份保持；展示了更好的曝光控制与本地化补光效果。

Conclusion: RRNet通过“虚拟光源参数化+深度渲染”的可解释框架，实现了质量-速度兼优的实时视频重光照；配合生成式数据合成，具有低数据成本与强泛化，适合视频会议、AR人像增强、移动摄影等实际应用。

Abstract: With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.

</details>


### [113] [Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion](https://arxiv.org/abs/2601.01870)
*Wenyu Shao,Hongbo Liu,Yunchuan Ma,Ruili Wang*

Main category: cs.CV

TL;DR: EGMT提出以实体为核心的多任务学习框架，用实体级文本监督与跨模态交互提升红外-可见图像融合的语义密度与细节保真，性能优于SOTA，并发布实体标注数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用句子级文本，冗余造成语义噪声，且难以深挖文本深层语义，导致融合图像的语义一致性与目标/纹理保留不足。

Method: 1) 从由大规模视觉-语言模型生成的图像描述中抽取实体级文本，去噪保留关键信息；2) 构建并行多任务架构：主任务为图像融合，辅以以实体为伪标签的多标签分类，提供语义监督；3) 设计实体引导的跨模态交互模块，细粒度对齐与融合视觉与实体级文本特征，建模跨模态与跨视觉依赖；并发布四个公开数据集的实体标注版本（TNO、RoadScene、M3FD、MSRS）。

Result: 在突出显著目标、纹理细节与语义一致性方面显著优于SOTA，实验全面验证有效性。

Conclusion: 实体级文本引导与多任务学习可有效提升红外-可见融合的语义密度和视觉质量；所提框架通用且可推广，配套数据与代码将公开。

Abstract: Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.

</details>


### [114] [CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving](https://arxiv.org/abs/2601.01874)
*Shuhang Chen,Yunqiu Xu,Junjie Xie,Aojun Lu,Tao Feng,Zeying Huang,Ning Zhang,Yi Sun,Yi Yang,Hangjie Yuan*

Main category: cs.CV

TL;DR: CogFlow提出一个认知启发的三阶段框架（感知→内化→推理）来提升多模态大模型在视觉数学题上的表现；通过视觉奖励提升感知、内化奖励保证视觉知识被忠实吸收、视觉门控策略优化约束推理扎根视觉；并发布包含12万+感知-推理对齐标注的MathCog数据集，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视觉数学推理上受限于视觉感知瓶颈，既有工作多强调“看准”和“读懂”，但忽视“把看见的内容如何可靠地用到后续推理中”。作者认为缺乏对感知信息的忠实整合和在推理中的可追溯利用，是性能与鲁棒性不足的关键。

Method: 提出CogFlow三阶段框架：1) 感知阶段：设计“协同视觉奖励”（Synergistic Visual Rewards）在参数空间与语义空间联合提升对符号与图形的抽取；2) 内化阶段：引入“知识内化奖励”模型，评估并强化已抽取视觉线索被正确编码并可供推理使用；3) 推理阶段：提出“视觉门控策略优化”（Visual-Gated Policy Optimization），在生成推理链时显式约束其依赖并调用已内化的视觉知识，抑制看似连贯但与视觉不一致的捷径推理；同时构建包含12万+对齐标注的MathCog数据集辅助训练。

Result: 在常用视觉数学推理基准上进行全面实验与消融，CogFlow在准确率与稳健性上均显著超越现有方法；数据与奖励机制共同带来对符号识别、图表理解与步骤推理的一致提升。

Conclusion: 仅提升“看”的能力不足以解决视觉数学推理问题；通过感知→内化→推理的层级流显式建模，并用奖励与策略优化将视觉知识贯穿其间，可显著增强多模态模型的视觉扎根推理能力；MathCog为该范式提供了高质量训练支撑。

Abstract: Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\Rightarrow$internalization$\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.

</details>


### [115] [Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems](https://arxiv.org/abs/2601.01891)
*Niloufar Alipour Talemi,Julia Boone,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 该综述指出：遥感分析正从静态深度模型转向具备规划与工具编排能力的“代理式AI”，提出统一分类（单代理副驾与多代理系统）、架构要素（规划、RAG、记忆）、新评测范式（从像素精度到轨迹/推理正确性），并梳理局限（对齐/落地、安全、编排），给出发展路线图。


<details>
  <summary>Details</summary>
Motivation: 现有视觉大模型与多模态大语言模型虽提升表征，但缺乏面向复杂地学工作流的顺序规划、主动工具调用与协作能力；遥感界缺少系统性综述来界定“代理式AI”在该领域的框架、评测与挑战。

Method: 文献综述与框架构建：提出统一分类法区分单代理“副驾”与多代理系统；剖析关键架构组件（规划机制、检索增强生成、短长时记忆）；梳理并对比新兴基准，强调轨迹感知的推理评测；系统性讨论落地/对齐、安全与编排方面的瓶颈。

Result: 形成首个面向遥感领域的代理式AI全景综述与分类体系；总结通用架构模式与关键能力缺口；汇总并重构评测指标从像素级准确度转向过程与决策正确性；明确当前技术在落地对齐、安全与工具编排方面的不足。

Conclusion: 遥感从“静态感知”迈向“自主智能”，需围绕规划、RAG与记忆等模块化能力构建单/多代理系统，并以轨迹与过程导向的基准进行评估；未来应聚焦更强的场景落地与地理对齐、可验证的安全机制和可靠的工具/多代理编排，以实现稳健的自主地理智能。

Abstract: The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.

</details>


### [116] [Forget Less by Learning from Parents Through Hierarchical Relationships](https://arxiv.org/abs/2601.01892)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: FLLP在双曲（洛伦兹）空间引入“父子”概念关系，把旧概念当作新概念的指导，从而在自定义扩散模型的连续个性化学习中减少灾难性遗忘并提升泛化，在多数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: CDMs在逐步学习新概念时易遗忘旧概念。现有方法多做“去干扰”，忽视不同概念间可能的正向迁移与结构关系，需要一种能利用概念层级关系、在连续学习中保留并互促知识的机制。

Method: 将概念表征嵌入洛伦兹（双曲）流形以刻画树状层级，显式定义父子（parent-child）概念关系：已学“父”概念作为对“子”新概念适配的指导与约束，实现跨概念的正向交互与知识蒸馏式适配，从而在个性化扩散模型的增量训练中减少遗忘并支持持续集成。

Result: 在三个公开数据集与一个合成基准上，FLLP在鲁棒性与泛化上均优于对比方法，表现出更低遗忘与更好新概念适配，结果稳定一致。

Conclusion: 利用双曲空间中的层级父子关系可在CDMs的连续个性化中兼顾保留与学习，缓解遗忘并提升泛化；证明了正向跨概念交互的价值。

Abstract: Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.

</details>


### [117] [Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection](https://arxiv.org/abs/2601.01908)
*Jingjing Wang,Qianglin Liu,Zhuo Xiao,Xinning Yao,Bo Liu,Lu Li,Lijuan Niu,Fugen Zhou*

Main category: cs.CV

TL;DR: 提出Nodule-DETR用于超声甲状腺结节检测，通过频域通道注意、分层特征融合与多尺度可变形注意显著提升mAP（+0.149），在真实临床数据上达SOTA，具潜在临床应用价值。


<details>
  <summary>Details</summary>
Motivation: 超声是甲状腺结节筛查与诊断的首选，但受低对比度、边界模糊、小而形态不规则结节影响，现有检测方法（含常规DETR/卷积型检测器）鲁棒性与精度不足，亟需一种能增强低对比特征并有效融合多尺度信息的检测框架。

Method: 在DETR框架上提出三项改进：1）MSFCA：在频域进行多谱段通道注意，突出低对比结节相关频带特征；2）HFF：分层特征融合，实现高低层语义/细节的高效多尺度集成；3）MSDA：多尺度可变形注意，适配小目标与不规则形状的采样与聚合。整体在真实临床超声数据训练评估。

Result: 在临床真实世界数据集上，相比基线模型，mAP@0.5:0.95提升0.149，达到当前SOTA水平。

Conclusion: Nodule-DETR在甲状腺超声结节检测上显著优于基线，三项模块协同提升对低对比、小目标与不规则结节的检测，具备临床落地潜力；代码已开源，便于复现与推广。

Abstract: Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.

</details>


### [118] [Learning Action Hierarchies via Hybrid Geometric Diffusion](https://arxiv.org/abs/2601.01914)
*Arjun Ramesh Kaushik,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: 提出HybridTAS：在扩散模型的去噪过程中混合欧氏与双曲几何，利用动作层级结构，以粗到细地完成时间动作分割，并在GTEA、50Salads、Breakfast上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有时间动作分割多用迭代式细化，但未显式建模人类动作的层级结构；双曲空间擅长表示树状/层级关系，若将其引入扩散去噪，可更好地从高层到细粒度指导标签生成。

Method: 设计HybridTAS框架：在扩散模型的时序标签生成中，引入混合几何表示。高扩散步（噪声大）在双曲空间中受高层抽象类别（树根）约束，低扩散步（噪声小）在欧氏/双曲组合下由细粒度类别（叶子）精修；实现从粗到细的标签去噪，引导时序分割。

Result: 在GTEA、50Salads、Breakfast三个基准上取得最先进性能（具体指标未给出），显示双曲引导的去噪对时序动作分割有效。

Conclusion: 利用双曲几何刻画动作层级并与扩散去噪结合，可显著提升时间动作分割；粗到细的混合几何引导是关键。

Abstract: Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.

</details>


### [119] [TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing](https://arxiv.org/abs/2601.01915)
*Yujie Hu,Zecheng Tang,Xu Jiang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: TalkPhoto 是一个无需训练的对话式图像编辑框架，通过提示工程驱动开源LLM分解用户需求并分层调用现有编辑工具，实现精准、灵活、可扩展的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑多用MLLM并依赖大规模多指令数据集训练，代价高且效果不稳。需要一种无需额外训练、可准确理解指令并灵活组合多种编辑方法的方案。

Method: 设计特殊提示模板引导开源LLM对用户意图进行层级化分析与规划，然后以“插件化/可插拔”的方式有序调用已有先进图像编辑工具；框架支持高效调用、可扩展至复杂和未见过的任务；全程训练免疫（training-free）。

Result: 在多类图像编辑任务上，TalkPhoto实现更准确的方法调用、更少token消耗，并取得更高的编辑质量与稳定性（据实验评估）。

Conclusion: 通过对话式规划与分层调用现有方法，TalkPhoto在无需再训练的前提下实现高质量、可控、可扩展的图像编辑，优于传统需多指令训练的MLLM方案。

Abstract: Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.

</details>


### [120] [AR-MOT: Autoregressive Multi-object Tracking](https://arxiv.org/abs/2601.01925)
*Lianjie Jia,Yuhan Wu,Binghao Ran,Yifan Wang,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: AR-MOT将多目标跟踪重构为LLM中的自回归序列生成任务，用对象token、区域对齐与时序记忆实现无任务头的可扩展跟踪，在MOT17与DanceTrack上达到接近SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有MOT方法多依赖固定输出头与定制管线，难以跨任务与模态扩展，也不易适配指令化或更复杂的跟踪需求；需要一种统一、可扩展且对多模态友好的范式。

Method: 将MOT表述为LLM的序列生成：通过对象Tokenizer（基于预训练检测器）产出区域级视觉token；提出区域感知对齐（RAA）缓解全局与局部特征错配；提出时序记忆融合（TMF）缓存历史对象token以实现长时跟踪；以输出序列格式约定结构化结果，无需任务特定输出头。

Result: 在MOT17与DanceTrack上进行大量实验，性能与当代SOTA相当，验证了方法可行性与竞争力。

Conclusion: 自回归LLM范式使MOT具备更强的通用性与可扩展性，通过灵活的输出序列即可支持新模态或指令，无需更改架构；为更通用、灵活的MOT系统奠定基础。

Abstract: As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.

</details>


### [121] [MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering](https://arxiv.org/abs/2601.01926)
*Zhifei Li,Yiran Wang,Chenyi Xiong,Yujing Xia,Xiaoju Hou,Yue Zhao,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.CV

TL;DR: 提出MacVQA：结合自适应记忆分配与全局噪声过滤的持续学习VQA框架，在10个连续任务上优于现有方法（标准任务平均准确率43.38%、遗忘2.32%；新组合任务准确率42.53%、遗忘3.60%）。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习VQA方法在知识保持、对新任务的适应以及鲁棒表示之间难以权衡，且常受噪声干扰与记忆利用低效的限制，需要一种能够在连续任务中兼顾表示质量、记忆有效性与组合泛化的方案。

Method: MacVQA包含两大核心机制：1) 全局噪声过滤与多模态融合，联合视觉与问题信息并在融合时抑制噪声，获得更稳健表示；2) 基于原型的自适应记忆分配，通过原型表示评估与选择特征，动态优化存储质量与容量利用，以在任务序列中实现有效的知识保留与更新。

Result: 在10个连续VQA任务上超过基线：标准任务平均准确率43.38%，平均遗忘2.32%；新组合任务平均准确率42.53%，平均遗忘3.60%。

Conclusion: 通过原型记忆分配与全局噪声过滤，MacVQA在持续VQA中实现了知识获取与保持的平衡，并提升了组合泛化与鲁棒性，显著优于现有方法。

Abstract: Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.

</details>


### [122] [Face Normal Estimation from Rags to Riches](https://arxiv.org/abs/2601.01950)
*Meng Wang,Wenjing Dai,Jiawan Zhang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 提出一种粗到细的人脸法线估计框架：先用小数据训练得到“粗法线”作为示例，再用带自注意力的精炼网络联合输入图像与示例生成高质量法线，从而显著降低对大规模成对数据与算力的依赖，并在精度与训练成本上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有人脸法线估计方法对大规模成对训练数据依赖强，获取昂贵且限制了泛化与落地。作者希望在训练数据与计算资源受限的情况下仍获得高质量法线估计。

Method: 两阶段粗到细架构：1) 以小规模数据训练一个“整洁”的基础模型，得到作为指导的粗法线（exemplars）；2) 设计带自注意力的精炼网络，联合输入人脸图像与对应粗法线，利用长程依赖抑制局部伪影，输出细粒度高质量法线。逻辑功能拆分减少对大规模配对数据的需求。

Result: 在多项实验与消融中，该方法在估计质量与训练成本上均优于现有先进方法；能够缓解小数据情境下的性能下降。

Conclusion: 粗到细并引入自注意力的设计有效降低数据与算力需求，同时提升法线估计质量；方法通用且代码开源，便于复现与扩展。

Abstract: Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.

</details>


### [123] [MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization](https://arxiv.org/abs/2601.01955)
*Zhexin Zhang,Yifeng Zhu,Yangyang Xu,Long Chen,Yong Du,Shengfeng He,Jun Yu*

Main category: cs.CV

TL;DR: 提出MotionAdapter，在DiT式文本到视频模型中实现鲁棒、语义对齐的运动迁移：通过跨帧注意力提炼运动场，并用DINO引导自适应定制，最终在去噪中引导合成，兼顾参考运动与目标外观。


<details>
  <summary>Details</summary>
Motivation: 尽管DiT类T2V在画质与时序一致性上进步显著，但跨视频迁移复杂运动仍困难：需将运动从外观中解耦，并让参考运动能自适应地匹配目标内容语义。

Method: 1) 在3D全注意力模块中分析跨帧注意力，提取“注意力驱动的运动场”以显式分离运动；2) 基于DINO的内容对应关系，对运动场进行重排与细化，实现语义对齐的运动定制；3) 将定制后的运动场作为引导信号，融入DiT去噪过程，使生成视频继承参考运动、保持目标外观与语义。

Result: 在定性与定量评测上均优于SOTA；能够稳健地支持复杂运动迁移，并自然支持如缩放等运动编辑任务。

Conclusion: 显式运动-外观解耦与DINO引导的自适应定制，使得在DiT型T2V中可实现鲁棒、语义对齐的运动迁移与编辑，兼顾参考运动与目标内容保真。

Abstract: Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \romannumeral1) explicit disentanglement of motion from appearance and \romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.

</details>


### [124] [AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing](https://arxiv.org/abs/2601.01957)
*Tianbo Wang,Yuqing Ma,Kewei Liao,Zhange Zhang,Simin Li,Jinyang Guo,Xianglong Liu*

Main category: cs.CV

TL;DR: 提出AFTER方法，通过事实增强的激活引导与查询自适应偏移，显式对齐视觉-文本语义，降低LVLM对象幻觉，在多基准上显著优于基线（AMBER上最高降16.3%）。


<details>
  <summary>Details</summary>
Motivation: LVLM在跨模态任务中常因语言偏置产生物体幻觉（类别/属性/关系），削弱可信度。现有激活编辑虽高效但缺少由事实文本语义提供的有效引导，难以显式缓解语言偏置。

Method: AFTER包含两部分：1) FAS（Factual-Augmented Activation Steering）：利用事实增强的文本语义构造通用的激活引导向量，显式建模精确的视-文关联以矫正偏置激活；2) QAO（Query-Adaptive Offset Optimization）：基于查询感知的偏移估计器，在通用引导向量上学习样本/查询特定的编辑偏移，提高编辑的多样性与粒度。整体通过在推理时对中间激活进行自适应引导与微调。

Result: 在标准幻觉基准与三种主流LVLM上进行广泛实验，AFTER持续降低类别/属性/关系幻觉；在AMBER基准相较基线最高降低16.3%的幻觉率。

Conclusion: 事实引导的自适应激活编辑能显式对齐视觉-文本语义、有效抑制语言偏置引发的幻觉；AFTER在不同模型与基准上表现稳健，且无需大规模再训练，具备实用性与可复现性。

Abstract: Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.

</details>


### [125] [Forget Less by Learning Together through Concept Consolidation](https://arxiv.org/abs/2601.01963)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: 提出FL2T框架，实现自定义扩散模型的并行、次序无关的概念学习，通过集合不变的概念间学习模块与代理引导，显著缓解灾难性遗忘并提升旧概念保留与新概念融合，在三数据集十任务上平均CLIP对齐提升≥2%。


<details>
  <summary>Details</summary>
Motivation: 现有自定义扩散模型在持续引入新概念时易灾难性遗忘，且多在固定顺序的序列学习设置下忽视概念间交互，限制了知识迁移与保留能力。

Method: 提出Forget Less by Learning Together (FL2T)：(1) 并行、顺序无关的概念学习；(2) 集合不变的跨概念学习模块；(3) 使用代理（proxies）在不同概念间引导特征选择与知识共享，形成“概念间指导”，以保留旧概念同时高效吸收新概念。

Result: 在三个数据集、十个增量概念学习任务上，显著提升概念保留与抗遗忘性能，平均CLIP Image Alignment分数至少提高2%。

Conclusion: 跨概念的集合不变学习与代理引导能有效催化知识迁移，支持并行与次序无关的个性化扩散模型增量学习，显著缓解灾难性遗忘。

Abstract: Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.

</details>


### [126] [Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation](https://arxiv.org/abs/2601.01984)
*Weijian Ma,Shizhao Sun,Tianyu Yu,Ruiyu Wang,Tat-Seng Chua,Jiang Bian*

Main category: cs.CV

TL;DR: 提出在VLM中引入“对象中心蓝图（JSON式）”的两阶段框架：先生成对象位置/尺寸/属性的结构化蓝图，再基于蓝图推理；配合监督迹、RL奖励与反捷径增强，显著提升空间推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么反复查看局部补丁，细粒度感知强但全局空间意识弱；要么仅标注孤立坐标，可定位却难刻画整体组织关系。缺乏能同时捕获对象间空间结构与可解释推理的机制。

Method: 构建“蓝图→推理→答案”的流程：1) 以JSON风格蓝图记录相关对象的位置信息（位置、大小、属性）。2) 监督微调：使用嵌入蓝图的推理轨迹，引导模型学习基础空间推理。3) 强化学习：设计蓝图感知奖励，鼓励蓝图包含合适数量对象并使最终答案与蓝图的因果链一致。4) 反捷径数据增强：对图像与问题施加针对性扰动，抑制对表面视觉/语言线索的依赖。

Result: 在多项空间推理基准上，方法稳定优于通用VLM与专门空间推理模型，显示更强的全局空间理解与鲁棒性。

Conclusion: 对象中心蓝图作为中间结构化表示，结合监督、RL与反捷径训练，可提升VLM的空间推理准确性与可解释性；为从感知到空间语义理解提供有效范式。

Abstract: Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.

</details>


### [127] [VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis](https://arxiv.org/abs/2601.01989)
*Aly R. Elkammar,Karim M. Gamaleldin,Catherine M. Elias*

Main category: cs.CV

TL;DR: 提出基于（视频）Transformer的多模态行人意图预测模型，在JAAD数据集上取得SOTA的准确率、AUC与F1，并通过消融实验分析设计选择带来的收益。


<details>
  <summary>Details</summary>
Motivation: L3→L4自动驾驶需要更可靠的行人过街意图预测，以提升安全和决策能力；现有方法对多元素、多模态信息与时空建模仍不足。

Method: 构建不同规模的Transformer/Video Vision Transformer模型，融合多种数据模态（视频、可能的上下文/轨迹/行为线索），进行时空特征建模与分类；并通过一系列消融实验评估各设计组件的贡献。

Result: 在JAAD行人行为数据集上达到并超过现有SOTA，在Accuracy、AUC、F1等指标上取得领先。

Conclusion: 多模态、基于Transformer的视频时空建模对行人意图预测有效；模型规模与架构选择显著影响性能，消融验证其设计优势，为更高阶自动驾驶中的安全感知提供支持。

Abstract: Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.

</details>


### [128] [API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning](https://arxiv.org/abs/2601.01992)
*Chen Zhu,Huiwen Zhang,Yujie Li,Mu He,Xiaotian Qiao*

Main category: cs.CV

TL;DR: 提出一个面向真实场景的图像去雾框架API，由自动雾化生成(AHG)与密度感知去雾(DHR)两部分组成，并配合多负样本对比去雾损失(MNCD)。AHG合成多样真实雾图扩充训练；DHR按补丁自适应地关注不同雾密度区域；MNCD在空域与频域引入多负样本缓解细节歧义。实验在多真实基准上达SOTA，具备强泛化与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 学习式去雾在真实复杂雾分布下易退化，主要因真实数据稀缺、雾密度分布复杂，导致泛化不足与细节模糊，需要同时解决数据与建模两侧的问题。

Method: 1) AHG：混合数据增强策略，自动生成真实感、多样性的雾化图像，作为额外高质量训练数据；2) DHR：密度感知的自适应补丁重要性机制，对不同雾密度区域赋予不同关注与处理；3) MNCD损失：在空间与频率域引入多负样本的对比学习，约束去雾结果，缓解细节歧义。

Result: 在多个人工与真实世界去雾基准上取得SOTA，量化指标与主观视觉质量均显著提升，并在多样雾分布下表现出强鲁棒泛化能力。

Conclusion: 通过结合真实感数据生成、密度自适应建模与多负样本对比损失，API框架有效提升真实世界去雾的性能与泛化，适合复杂多变的雾密度场景。

Abstract: Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.

</details>


### [129] [Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors](https://arxiv.org/abs/2601.01998)
*Chen Zhu,Huiwen Zhang,Mu He,Yujie Li,Xiaotian Qiao*

Main category: cs.CV

TL;DR: 提出一个互相强化雾霾与低照先验的一体化夜间去霾与增亮框架，结合图像/补丁/像素级专家与频域路由，实现逐级还原结构与细节，在夜间去霾上取得SOTA，并可泛化到日间去霾与低照增强。


<details>
  <summary>Details</summary>
Motivation: 夜间含雾图像同时受低照与雾霾等多重退化影响，现有方法多仅处理单一退化，忽视两者相互作用，导致可见度提升有限。观察到低照与雾霾先验间有共享/互补知识，可相互加强以获得更好可视化。

Method: 构建一个互补学习框架：通过图像级、块级、像素级三类专家在视觉域与频率域协同工作，逐步恢复全局结构、区域模式与细粒度细节；引入频率感知路由器，自适应分配各专家贡献，实现稳健复原与动态选择。

Result: 在夜间去霾基准上取得显著优于现有方法的定量与定性表现；同时在日间去霾与低照增强任务上显示良好泛化能力。

Conclusion: 通过互相强化低照与雾霾先验，并采用多粒度专家与频率感知路由的渐进式重建策略，可更有效地提升夜间含雾图像的可见度，方法稳健且具跨任务泛化潜力。

Abstract: Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.

</details>


### [130] [Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach](https://arxiv.org/abs/2601.02016)
*Matthias Bartolo,Dylan Seychell,Gabriel Hili,Matthew Montebello,Carl James Debono,Saviour Formosa,Konstantinos Makantasis*

Main category: cs.CV

TL;DR: 提出在目标检测中引入LUPI（仅训练期可用的特权信息）范式，通过师生框架把掩膜、显著图、深度等信息注入学生模型，训练后推理不增加复杂度；在多种检测器与数据集上显著提升，尤其对中大目标效果更好，并通过消融发现中等权重的教师指导最优。


<details>
  <summary>Details</summary>
Motivation: 目标检测在真实与资源受限场景中需要更高精度且不增加推理开销；训练阶段常能获取更细粒度的辅助信息（如标注掩膜、深度、显著性），但推理阶段不可用。动机是利用LUPI在训练时吸收这些信息，以提升学生模型的泛化与精度，同时保持推理成本不变。

Method: 提出模型无关的LUPI注入策略：构建教师-学生架构，教师接收特权信息（掩膜、显著图、深度等）与常规输入，学生仅用常规输入；通过蒸馏/一致性损失将教师的中间/输出表征与学生对齐；在5个SOTA检测器与多个数据集上训练；进行消融以研究教师引导权重的影响。

Result: 在UAV垃圾检测与Pascal VOC 2012等基准上，LUPI训练的学生稳定优于各自基线，检测精度显著提升，推理阶段无额外复杂度或模型增大；增益对中/大型目标尤为明显；消融显示教师指导的中等权重取得最佳平衡。

Conclusion: LUPI是一种有效、实用且通用的目标检测增强策略：利用训练期的特权信息通过师生蒸馏提升精度与泛化，同时保持部署成本不变，适合资源受限与真实应用场景。

Abstract: This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.

</details>


### [131] [Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement](https://arxiv.org/abs/2601.02018)
*Guangqian Guo,Aixi Ren,Yong Guo,Xuehui Yu,Jiacheng Tian,Wenli Li,Yaoxing Wang,Shan Gao*

Main category: cs.CV

TL;DR: 提出GleSAM++：在SAM/SAM2前加入生成式潜空间增强模块，并配合FDA、CRE与DAE，使其在低质/退化图像上显著提升分割鲁棒性，同时不损清晰图像泛化，并对未见退化同样有效。


<details>
  <summary>Details</summary>
Motivation: SAM零样本分割强，但在严重退化（噪声、压缩、模糊等）低质图像上性能显著下滑，限制真实场景应用。需要一种能够跨图像质量稳健泛化的增强方案，且与预训练扩散模型、分割框架兼容高效。

Method: 在SAM/SAM2前加入Generative Latent space Enhancement（GLE），用预训练扩散模型在特征潜空间进行重建增强；提出两项对齐策略：1) Feature Distribution Alignment（FDA）用于对齐扩散模型与分割特征分布；2) Channel Replication and Expansion（CRE）用于通道维度适配与容量扩展。为解决退化程度无显式指导的问题，引入Degradation-aware Adaptive Enhancement（DAE）：先预测退化等级，再进行退化感知的重建，从而分解学习难度。整体仅引入少量可学习参数，可高效训练并可直接套用到预训练SAM/SAM2。

Result: 在多种复杂退化场景下显著提升分割鲁棒性；在清晰图像上保持或不降泛化；对未见退化也有良好表现，展现方法与数据集的通用性。

Conclusion: GleSAM++通过潜空间生成式增强与退化自适应机制，有效弥合SAM在低质图像上的性能缺口，兼顾效率与兼容性，并在已知与未知退化上均表现稳健，适合实际复杂环境部署。

Abstract: Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.

</details>


### [132] [Adapting Depth Anything to Adverse Imaging Conditions with Events](https://arxiv.org/abs/2601.02020)
*Shihan Peng,Yuyang Xiong,Hanyu Zhou,Zhiwei Shi,Haoyue Liu,Gang Chen,Luxin Yan,Yi Chang*

Main category: cs.CV

TL;DR: 提出ADAE框架，将事件相机与Depth Anything进行熵感知空间融合与运动引导的时间校正，在强光/弱光与运动模糊等退化场景中显著提升深度估计鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度基础模型在理想条件下表现优异，但在极端光照、运动模糊等退化情况下，帧相机的视觉信号被破坏，导致空间与时间特征区分度下降；现有事件-帧融合方法多需从零训练且数据域专一，无法继承基础模型的开放世界知识与泛化能力。

Method: 构建事件引导的时空融合框架ADAE以增强Depth Anything：1) 熵感知空间融合——利用信息熵指示光照退化程度，自适应融合帧特征与事件特征；2) 运动引导时间校正——使用事件流提供的运动线索，对因模糊而含糊的区域特征进行时间维度上的再校准。两模块在统一框架下协同工作。

Result: 在多种退化成像条件（极端照明、运动模糊等）下进行广泛实验，ADAE较基线与现有方法表现更优，验证了所提框架的有效性与鲁棒性（代码将开源）。

Conclusion: 通过事件驱动的空间与时间互补融合，ADAE显著增强了Depth Anything在恶劣成像条件下的深度估计能力，兼顾基础模型的泛化与对退化的适应性。

Abstract: Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.

</details>


### [133] [Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding](https://arxiv.org/abs/2601.02029)
*Toshihiko Nishimura,Hirofumi Abe,Kazuhiko Murasaki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: 提出一种无需3D标注或RGB配对的训练数据即可进行大规模点云3D语义分割的方法：将点云以虚拟相机投影到2D，用基于文本提示的基金模型做2D分割，多视角加权投票回融到3D；精度优于现有免训练方法，接近监督方法，并支持开放词汇查询。


<details>
  <summary>Details</summary>
Motivation: 3D点云语义分割通常依赖大量昂贵的3D标注或与图像配对的数据，且受限于封闭类别集；需要一种成本更低、可扩展到开放词汇的方案。

Method: 1) 用虚拟相机从多视角将3D点云投影到2D图像；2) 利用带自然语言提示的2D基础模型（如开放词汇分割/视觉-语言模型）进行2D语义分割；3) 将多视角预测通过加权投票聚合回3D点层以得到最终标签。

Result: 在无需3D训练的前提下，整体性能优于现有训练自由（training-free）方法，并与有监督方法在精度上相当；还实现开放词汇识别。

Conclusion: 通过“3D→2D投影 + 文本引导2D分割 + 多视角加权投票回融3D”的流程，可在无3D标注与无图像配对的条件下实现高质量3D语义分割，并突破封闭类别限制。

Abstract: This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.

</details>


### [134] [AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off](https://arxiv.org/abs/2601.02038)
*Yihan Zhu,Mengying Ge*

Main category: cs.CV

TL;DR: AlignVTOFF 提出一个并行U-Net框架，通过参考U-Net与纹理-空间特征对齐模块（TSFA）协同，显式对齐纹理与几何，缓解扩散去噪过程中的高频细节丢失，在多种设置下优于现有方法，生成更具结构真实性与高频细节的平铺服装图像。


<details>
  <summary>Details</summary>
Motivation: 现有VTOFF方法为追求快速特征提取而使用轻量模块，导致在复杂几何形变与高频纹理条件下难以保持结构化图案与细节，出现纹理衰减与几何失真，亟需在扩散生成中有效注入并对齐参考服装的几何与纹理特征。

Method: 提出AlignVTOFF：1）参考U-Net进行多尺度特征抽取，强化几何保真与结构模式建模；2）纹理-空间特征对齐（TSFA）模块将参考服装特征注入冻结的去噪U-Net，采用“可训练的交叉注意力 + 冻结的自注意力”的混合注意力设计，显式对齐纹理与空间线索，减少去噪中的高频信息丢失。

Result: 在多种实验设置与基线对比中，AlignVTOFF在结构真实感与高频细节保真方面持续优于现有SOTA，生成平铺服装图像的纹理和几何质量均显著提升。

Conclusion: 并行U-Net结合参考特征注入与混合注意力的对齐策略，有效解决VTOFF中的纹理衰减与几何失真问题，可作为高保真平铺服装生成的强基线。

Abstract: Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.

</details>


### [135] [Agentic Retoucher for Text-To-Image Generation](https://arxiv.org/abs/2601.02046)
*Shaocheng Shen,Jianfeng Liang. Chunlei Cai,Cong Geng,Huiyu Duan,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出Agentic Retoucher：以感知-推理-行动的人类式决策环路，自动定位并修复T2I扩散模型生成中的细粒度失真；并构建GenBlemish-27K数据集；在多项指标与人偏好上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型虽具高写实度，但常出现四肢、面部、文本等局部瑕疵。现有修复方法要么代价高（反复再生成），要么依赖空间落地能力弱的VLM，易产生语义漂移与不稳定局部编辑。因此需要一种具备精确定位、可靠推理与可控局部修复的统一框架。

Method: 提出层级化的决策驱动框架Agentic Retoucher：1) 感知代理：在文本-图像一致性线索下学习上下文显著性，进行细粒度失真定位；2) 推理代理：通过渐进式偏好对齐，进行符合人类直觉的因果/诊断式推理；3) 行动代理：依据用户偏好自适应规划局部inpainting修复，执行可控编辑。三者构成闭环的自纠正决策流程。并构建GenBlemish-27K数据集（6K图像、27K标注、12类伪影）用于监督与评测。

Result: 在感知质量、失真定位精度与人类偏好一致性等方面，Agentic Retoucher较当前SOTA方法取得一致领先；在多数据集与设置上均验证其有效性与稳健性。

Conclusion: 将感知证据、语言推理与可控修复整合为自纠正决策闭环，可有效减少T2I生成中的细粒度失真。GenBlemish-27K促进精细监督与量化评估。该范式为更可靠的T2I后处理与自我修复生成提供了新方向。

Abstract: Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.

</details>


### [136] [PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction](https://arxiv.org/abs/2601.02088)
*Jiahao Bao,Huazhen Liu,Yu Zhuang,Leran Tao,Xinyu Xu,Yongtao Shi,Mengjia Cheng,Yiming Wang,Congshuang Ku,Ting Zeng,Yilang Du,Siyi Chen,Shunyao Shen,Suncheng Xiang,Hongbo Yu*

Main category: cs.CV

TL;DR: 提出PhysSFI-Net，一种物理先验+几何深度学习框架，用于正颌术后面部软组织变形预测，精度优于现有方法（如ACMT-Net）。


<details>
  <summary>Details</summary>
Motivation: 传统生物力学仿真计算代价高、几何深度学习可解释性不足，临床需要既高精度又高可解释、可高分辨重建的术后面形预测模型以辅助术前方案制定。

Method: 构建物理信息指导的几何深度学习框架PhysSFI-Net：1）分层图模块，结合颅颌骨与手术方案编码器及注意力机制，捕获骨-面相互作用特征；2）基于LSTM的序列预测器，逐步模拟软组织增量形变；3）受生物力学启发的高分辨率面部表面重建模块。以135例正畸-正颌联合治疗患者为数据，采用点云形状误差（Hausdorff）、表面偏差、以及颅颌标志点欧氏距离评估。

Result: 在验证集中取得：点云形状误差1.070±0.088 mm，表面偏差1.296±0.349 mm，标志点定位误差2.445±1.326 mm；与SOTA的ACMT-Net相比在预测精度上更优。

Conclusion: PhysSFI-Net实现了可解释、高清的术后面形预测，精度领先，具备用于正颌术前规划与仿真的临床应用潜力。

Abstract: Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.

</details>


### [137] [MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation](https://arxiv.org/abs/2601.02091)
*Zhehuan Cao,Fiseha Berhanu Tesema,Ping Fu,Jianfeng Ren,Ahmed Nasr*

Main category: cs.CV

TL;DR: 提出首个大规模“仅光学影像”的冰碛垄（moraine）分割数据集与轻量化模型MCD‑Net，实现在高海拔冰川区的可复现基准与可部署基线。


<details>
  <summary>Details</summary>
Motivation: 冰碛垄分割对重建冰川演化与评估气候驱动地貌变化至关重要，但光学影像对比度弱、且高分辨率DEM稀缺，使自动化制图困难；缺乏公开、规模化、专门面向冰碛垄的仅光学数据集与基线方法。

Method: 构建包含3340幅来自谷歌地球的高分辨率手工标注图像（覆盖中国四川、云南冰川区）的光学冰碛垄分割数据集；提出轻量级MCD‑Net：MobileNetV2编码器 + CBAM注意力模块 + DeepLabV3+解码器；与更深骨干（ResNet152、Xception）进行基准对比。

Result: 在仅光学输入下，MCD‑Net获得62.3% mIoU与72.8% Dice，计算成本较深层骨干降低60%以上；展示在冰碛体主体分割上可靠，但脊线因亚像素宽度与光谱歧义仍受限。

Conclusion: 仅依赖光学影像即可实现可靠的冰碛体分割；公开数据与代码（GitHub: Lyra-alpha/MCD-Net）为冰碛垄特定分割建立了可复现基准，并提供可部署的高海拔冰川监测轻量化基线。

Abstract: Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\% mean Intersection over Union (mIoU) and 72.8\% Dice coefficient while reducing computational cost by more than 60\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.

</details>


### [138] [InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting](https://arxiv.org/abs/2601.02098)
*Jinlong Fan,Shanshan Zhao,Liang Zheng,Jing Zhang,Yuxiang Yang,Mingming Gong*

Main category: cs.CV

TL;DR: 提出InpaintHuman：从被遮挡的单目视频重建完整、可动画的人体头像，结合UV多尺度表示与身份保持扩散修补，在合成与真实数据上显著提升几何与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 单目视频因遮挡与视角局限导致人体观测不完整，现有基于3D Gaussian Splatting的人体方法易出现几何破损、纹理缺失与时间不稳定；SDS优化还会带来身份漂移。需要一种既能补全遮挡区域又能保持身份与可动画性的重建方法。

Method: 1) 多尺度UV参数化表示：在人体模板UV域建立层级式粗到细特征插值，将视频帧投影到UV空间，稳健融合可见与不可见区域以恢复细节；与3D高斯表示结合，提升几何与纹理一致性。2) 身份保持的扩散修补：引入文本反演(TI)获得主体特定token，结合语义条件指导做时序一致的图像/特征级inpainting；采用像素级监督代替SDS，避免身份漂移并增强细节还原。3) 结果可驱动/可动画化：在重建后绑定到可变形人体骨架，实现跨姿态与视角渲染。

Result: 在PeopleSnapshot、ZJU-MoCap(合成)与OcMotion(真实遮挡)上取得与SOTA竞争甚至更优的表现，在重建完整性、几何细节、纹理一致性与时间稳定性上持续提升，尤其在重遮挡场景中显著减少破损与闪烁。

Conclusion: UV域多尺度表示与身份保持扩散修补的结合，有效解决单目重遮挡下的人体重建难题，产生高保真、完整且可动画的头像；直接像素级监督保证身份一致性，并在多数据集上验证了鲁棒性与泛化能力。

Abstract: Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.

</details>


### [139] [360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images](https://arxiv.org/abs/2601.02102)
*Jiaqi Yao,Zhongmiao Yan,Jingyi Xu,Songpengcheng Xia,Yan Xiang,Ling Pei*

Main category: cs.CV

TL;DR: 提出一种面向360°图像的前馈式3D高斯泼洒（3DGS）重建框架，通过几何正则提升几何一致性，同时保持高渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有MVS在稀疏视角/低纹理区域易失败；NeRF类需逐场景优化、难实时；显式3DGS虽高效但现有前馈方法偏重视觉质量，几何一致性不足，影响表面重建与空间感知可靠性。

Method: 基于3DGS的前馈管线，输入360°图像直接生成高斯原语；引入Depth-Normal几何正则，将渲染深度的梯度与法线信息耦合，用于共同监督高斯的旋转、尺度与位置，从而约束点云与隐式表面。

Result: 在保持高质量渲染的同时，显著提升几何一致性与表面/点云精度；实验表明该方法较现有前馈3DGS方案在几何指标上明显领先。

Conclusion: 所提框架为360°场景的实时/高效3D重建提供有效方案，兼顾视觉质量与几何可靠性，适用于空间感知任务（AR、机器人、数字孪生等）。

Abstract: 3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.

</details>


### [140] [HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures](https://arxiv.org/abs/2601.02103)
*Yating Wang,Yuan Sun,Xuan Wang,Ran Yi,Boyao Zhou,Yipengjing Sun,Hongyu Liu,Yinuo Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: HeadLighter在3D高斯点渲染的人头生成中，采用监督式、物理可解释的外观-光照解耦，实现实时高保真生成并支持显式重光照与视角编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D人头生成虽具实时、写实与多视角一致性，但外观与照明深度耦合导致无法可控重光照；弱监督解耦方法依赖强假设，难以处理复杂光照。需要一种更物理、可监督的解耦框架。

Method: 提出HeadLighter：1) 双分支结构，一支学习与光照无关的人头外观属性，另一支学习物理渲染成分（如光照、法线等）；2) 渐进式解耦训练，逐步将外观先验注入生成架构；3) 使用光舞台多视角受控光照数据进行监督；4) 蒸馏策略生成高质量法线以提升真实感渲染；5) 基于3D Gaussian Splatting保持实时性。

Result: 在保持高质量生成与实时渲染的同时，模型实现显式光照与视角编辑；实验显示在复杂光照下的解耦与可控性优于现有方法。

Conclusion: HeadLighter实现了物理可信的外观-光照分解，克服既有弱监督方法对光照复杂度的限制，提供可控重光照与多视角编辑能力；代码与数据集将开源。

Abstract: Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.

</details>


### [141] [MagicFight: Personalized Martial Arts Combat Video Generation](https://arxiv.org/abs/2601.02107)
*Jiancheng Huang,Mingfu Yan,Songyan Chen,Yi Huang,Shifeng Chen*

Main category: cs.CV

TL;DR: 提出MagicFight任务与方法：个性化双人武术对打视频生成；利用Unity合成数据集，解决身份混淆、肢体异常与动作不匹配，生成高保真、连贯的两人互动视频。


<details>
  <summary>Details</summary>
Motivation: 当前个性化视频多聚焦单人，难以刻画双人互动（尤其武术对打）的细节与复杂性；现有单人舞蹈生成模型在两人场景中出现身份混淆、肢体异常、动作不一致等问题，缺少相应数据集与基准。

Method: 提出新任务“个性化武术对打视频生成”。构建Unity物理引擎驱动的合成数据集，包含多样3D角色、武术动作与场景。基于现有文本/视频生成与个性化策略进行改造与精炼，使模型能在两人互动中保持身份一致、动作连贯与时空一致性，输出高保真视频。

Result: MagicFight能生成高质量的双人对打视频：人物身份可分辨、四肢结构合理、动作衔接顺滑，较单人迁移基线显著减少身份混淆与动作错位问题。

Conclusion: 首次系统化提出双人武术对打的个性化视频生成任务，给出MagicFight方法与Unity合成数据集（KungFu-Fiesta），为交互式视频内容生成提供基础并指引后续研究。

Abstract: Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.
  Website: https://MingfuYAN.github.io/MagicFight/
  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta

</details>


### [142] [Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model](https://arxiv.org/abs/2601.02112)
*Utkarsh Singh,Absaar Ali,Adarsh Roy*

Main category: cs.CV

TL;DR: 提出一种轻量级替代模型，通过顺序切片处理3D车辆点云，快速高精度预测空气阻力系数Cd（R^2>0.9528，MAE≈6.046e−3，单样本推理≈0.025s）。


<details>
  <summary>Details</summary>
Motivation: CFD和风洞在早期设计阶段成本高、迭代慢；现有ML替代模型要么复杂、可解释性差、要么对复杂几何精度不足。需要一种既高效又具可解释性的几何到Cd映射方法。

Method: 将车辆3D点云沿来流方向分解为有序2D横截面切片；每片经轻量PointNet2D编码得到嵌入，再用双向LSTM建模沿程几何演化，输出Cd预测。在DrivAerNet++上训练评估。

Result: 在DrivAerNet++数据集上取得R^2>0.9528、MAE≈6.046×10^-3；消费者级GPU上单样本推理约0.025秒。

Conclusion: 该顺序切片+BiLSTM的轻量模型能快速、准确且具一定可解释性地预测Cd，为汽车早期气动设计提供敏捷反馈，提升设计探索效率。

Abstract: The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.

</details>


### [143] [Remote Sensing Change Detection via Weak Temporal Supervision](https://arxiv.org/abs/2601.02126)
*Xavier Bou,Elliot Vincent,Gabriele Facciolo,Rafael Grompone von Gioi,Jean-Michel Morel,Thibaud Ehret*

Main category: cs.CV

TL;DR: 提出一种弱时间监督策略：用单时相数据的多时相观测扩展数据集，在“同地大多不变、异地配对代表变化”的假设下训练，并通过对象感知的变化图和迭代细化降噪；在扩展的FLAIR与IAILD上实现强零样本与小样本表现，并在法国大范围展示可扩展性。


<details>
  <summary>Details</summary>
Motivation: 像素级变化标注昂贵，现有用合成或伪造变化对的数据泛化差、跨域弱。需要无需新增标注即可利用现有单时相数据与额外时相观测，提升语义变化检测的可用性、泛化与规模化。

Method: 1) 数据构建：把已有单时相遥感数据集加入不同时间点的同地观测，默认真实同地双时相对“多数不变”；再用异地图像配对构造“变化”样本。2) 弱标签降噪：采用对象感知（基于分割对象/实例）的变化图生成，减少像素噪声。3) 迭代细化：用模型预测反哺弱标签，多轮更新以抑制噪声与偏差。4) 训练：以弱时间监督学习一个变化检测模型。5) 评估：在扩展的FLAIR与IAILD上进行零样本与低标注实验，并做大范围法国区域推理展示可扩展性。

Result: 在扩展版FLAIR与IAILD上取得强零样本（无需目标域标注）与小样本（少量标注）性能，跨多个基准表现优异；并能在法国大范围场景稳定运行，显示良好可扩展性与鲁棒性。

Conclusion: 无需新增像素级标注即可通过弱时间监督有效训练语义变化检测模型；对象感知与迭代细化能缓解弱标签噪声；相较依赖合成数据的方法，本方法泛化更强、可扩展性更好，适合大范围应用。

Abstract: Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.

</details>


### [144] [Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery](https://arxiv.org/abs/2601.02139)
*Chenyang Lai,Shuaiyu Chen,Tianjin Huang,Siyang Song,Guangliang Cheng,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出OSCD任务与TAHI框架，用双时相变化检测替代单幅分割，在缺少真实“事前”SAR时用时序感知修复生成伪事前图像，构建数据集并基准评测，显著降低误报并提升检测准确性。


<details>
  <summary>Details</summary>
Motivation: 单幅SAR分割难以区分油膜与生物膜、低风区等相似现象，易高误报、泛化差；实际应用常缺少配准的事前影像，限制了时序方法的发展，亟需兼顾可用性与可靠性的方案。

Method: 提出“油污变化检测（OSCD）”双时相任务；设计TAHI（时序感知混合修复）框架，用后时相SAR生成油污清除的“伪事前”图像。TAHI包含：1）高保真混合修复，实现油污区域的无油重建；2）时序真实感增强，保证辐射特性与海况一致性。基于TAHI构建OSCD数据集，并对多种SOTA变化检测模型进行基准测试。

Result: 在OSCD设定下，与传统单幅分割相比，显著降低假阳性，提高整体检测精度；在数据稀缺条件下仍具更好泛化能力。

Conclusion: 利用时序信息与合成“事前”影像的变化检测范式比静态分割更可靠、可扩展，适合真实场景中的大范围海上溢油监测。

Abstract: Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.

</details>


### [145] [Efficient Unrolled Networks for Large-Scale 3D Inverse Problems](https://arxiv.org/abs/2601.02141)
*Romain Vo,Julián Tachella*

Main category: cs.CV

TL;DR: 提出一种域划分与正规算子近似，使深度解卷积/深度展开类网络在超大规模成像（如3D）中也能把正向算子嵌入架构，并在单GPU上训练与推理，达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 深度学习在成像逆问题中表现突出，但最强方法依赖将物理正向算子嵌入网络（如深度展开）。面对3D等大规模问题，正向算子是全局/高维的，内存开销巨大，导致无法用常规打补丁（patching）策略训练，限制了这类物理一致网络的适用性。

Method: 提出两点：1) 域划分（domain partitioning），将大体积问题分解为可管理的子域，使算子作用局部化并可流水/并行处理；2) 正规算子（normal operator）近似，以近似的A^T A替代精确全局算子，降低内存与算力；据此构建可端到端训练的重建网络（深度展开/算子嵌入），在不牺牲精度的前提下，用单GPU完成训练与推理。

Result: 在3D X射线锥束CT与3D多线圈加速MRI上实现新的SOTA重建质量；同时显著降低内存需求，使单张GPU即可完成端到端训练和推理。

Conclusion: 通过域划分与正规算子近似，可在超大规模三维成像中把物理正向算子高效融入深度网络，兼顾物理一致性与可扩展性，实证优于现有方法且资源要求更低。

Abstract: Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.

</details>


### [146] [BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models](https://arxiv.org/abs/2601.02147)
*Sunny Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: 提出BiPrompt，在测试时同时在视觉与文本两侧去偏，降低对伪相关的依赖，无需再训练或域监督，显著提升平均与最差组准确率。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP等视觉语言模型虽具零样本泛化，但易受跨模态伪相关影响；多数去偏方法仅处理单一模态，导致在分布偏移下鲁棒性不稳定与覆盖不全。

Method: 提出双侧提示优化BiPrompt用于测试时自适应：1) 视觉侧：结构化注意力引导擦除，抑制背景激活，并对因果与伪区域施加正交预测一致性约束；2) 文本侧：平衡提示归一化，学习型重中心化，使类别嵌入对齐到各向同性语义空间；两者共同最小化伪线索与预测的条件互信息，引导因果、域不变推理；无需再训练或域监督。

Result: 在真实与合成偏置基准上，较现有测试时去偏方法在平均准确率与最差组准确率上均取得稳定提升。

Conclusion: BiPrompt作为轻量的测试时双模态去偏框架，有效缓解跨模态伪相关，促进因果且可信的视觉语言适配。

Abstract: Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.

</details>


### [147] [Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32](https://arxiv.org/abs/2601.02177)
*Oliver Custance,Saad Khan,Simon Parkinson*

Main category: cs.CV

TL;DR: 作者用廉价ESP32 WiFi传感器系统性评测多种信号分离方法用于多人的步态识别，发现准确率均低（约45–56%），差异不显著，指出硬件信号质量是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 单人基于WiFi CSI的步态识别已很准，但多人识别仍少见且常依赖昂贵、改固件的复杂系统。关键未解问题：多人表现差是算法瓶颈还是硬件根限？

Method: 在商品级ESP32传感器上搭建低成本实验平台，覆盖1–10人的7种场景；系统评估6种异质信号分离方法（FastICA、SOBI、PCA、NMF、小波、张量分解）。提出诊断性指标：受试者内变异度、受试者间可区分度、随人数增长的性能衰减率；统计检验比较方法差异。

Result: 所有方法在多人场景准确率均低且相近，约45–56%，标准差3.74%，差异统计上不显著（p>0.05）；最佳也仅NMF达56%。观测到高受试者内变异、低受试者间可分、人数增加导致严重性能衰减。

Conclusion: 在不改固件的ESP32商品硬件上，多人步态分离与识别受限于信号质量而非主要由算法导致。用现成廉价设备难以实现可靠的多人识别，需要更高质量硬件、阵列设计或额外传感模态。

Abstract: WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\%, $σ$=3.74\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.

</details>


### [148] [QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition](https://arxiv.org/abs/2601.02189)
*Cheng Ying Wu,Yen Jui Chang*

Main category: cs.CV

TL;DR: 提出轻量级量子启发式交互分类器（QuIC），在不增加维度和训练不稳定的情况下，用二阶特征交互提升浅层网络在细粒度分类上的性能。


<details>
  <summary>Details</summary>
Motivation: FGVC需要捕捉细微差异，但资源受限设备无法承受深模型计算；浅网络配合GAP只用一阶统计，区分不了相似子类；既有双线性池化虽能建模二阶关系，却维度爆炸且训练不稳，亟需兼顾效率与效果的新头部模块。

Method: 将通道视为量子态，通过可学习的“可观测算符”建模二阶协方差，实现二阶特征交互；模块为即插即用的轻量级分类头，可与浅骨干端到端单阶段稳定训练，避免双线性特征维度膨胀。

Result: 在CUB-200-2011等FGVC任务中显著提升浅骨干表现：VGG16 Top-1提升近20%，在ResNet18上优于SE-Block；t-SNE与定性分析显示更紧致的类内聚类与对细粒度判别区域的关注。

Conclusion: QuIC在不显著增加计算成本的前提下有效补足浅层网络的高阶特征建模能力，提供稳定高效的二阶交互机制，适合资源受限边缘设备上部署的FGVC应用。

Abstract: Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.

</details>


### [149] [Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models](https://arxiv.org/abs/2601.02198)
*Alexander Möllers,Julius Hense,Florian Schulz,Timo Milbich,Maximilian Alber,Lukas Ruff*

Main category: cs.CV

TL;DR: 论文研究了病理基础模型在不同放大倍数下的表现与训练时放大倍数采样策略的影响，提出将“放大倍数采样”建模为多源领域自适应问题，指出常用离散均匀采样会在中间放大倍数退化，提出连续放大倍数采样与优化采样分布，在两个新基准（TCGA-MS、BRACS-MS）上验证，在中间倍数显著提升（最高+4pp BCA），并发现放大倍数是模型性能差异的主因。


<details>
  <summary>Details</summary>
Motivation: 实际诊断中病理学家会在低倍看结构、在高倍看细节，但现有病理基础模型跨放大倍数的鲁棒性与训练采样策略的作用尚不清楚；社区常用的离散倍数采样（0.25/0.5/1.0/2.0 mpp）可能导致对未覆盖的中间倍数泛化差。作者希望建立理论框架解释取舍，并设计更好的采样策略，使模型在各倍数上稳定发挥。

Method: 1) 将放大倍数视作多源领域（不同mpp）并建立简单理论框架，刻画不同采样分布对表示质量的权衡；2) 证明与实证离散均匀采样在中间倍数会出现性能洼地；3) 提出连续放大倍数采样，消除倍数空隙，同时推导可优化的采样分布以在全尺度上最优；4) 构建两个多倍数评测基准（TCGA-MS、BRACS-MS）与合适指标，系统评估基础模型与采样策略。

Result: 连续采样在中间放大倍数相较离散采样显著提升，平衡分类准确率最高提升约4个百分点；在标准倍数上不降；进一步用理论导出的最优分布还能带来额外收益。评测还表明：不同基础模型间的性能差异主要由放大倍数驱动。

Conclusion: 训练中采用连续放大倍数采样并配合优化采样分布，可在不牺牲标准倍数性能的前提下，显著提升中间倍数与整体跨倍数鲁棒性。放大倍数是病理基础模型性能变化的关键因素；应面向全倍数一致性的未来模型与基准。

Abstract: In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.

</details>


### [150] [Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules](https://arxiv.org/abs/2601.02203)
*Oliver Custance,Saad Khan,Simon Parkinson,Quan Z. Sheng*

Main category: cs.CV

TL;DR: 提出一种两阶段WiFi CSI无设备人群计数框架：自监督对比学习预训练的CSI-ResNet-A + 轻量Adapter微调，随后用有状态计数器生成稳定占用人数；在跨域泛化与样本稀缺下取得SOTA和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: WiFi CSI人群计数具有隐私友好和物联网应用价值，但现实部署受“域迁移”困扰：在一个场景训练的模型难以泛化到新环境。需要能在少样本甚至无标注下跨域泛化且高效适配的方法。

Method: 提出CSI-ResNet-A为核心的两阶段框架：1) 自监督对比学习预训练以学习域不变表示；2) 通过轻量Adapter模块进行高效微调（少量参数）；随后将得到的事件序列输入有状态的计数机（stateful counting machine）以输出平滑稳定的占用估计。并引入Generalisation Index（GI）指标度量鲁棒泛化。

Result: 在WiFlow数据集的10-shot场景中，无监督方法达到MAE=0.44，优于监督基线；在GI指标上接近满分，显示出强泛化；在公开WiAR基准上达到98.8%准确率；消融显示Adapter微调以仅2.8%的参数训练达到98.84%准确率，接近全量微调的99.67%。

Conclusion: 该框架在跨域、少样本条件下实现稳定且精确的人群计数，具备可扩展与实用性；Adapter微调在几乎不牺牲性能的情况下显著降低训练成本，为真实IoT部署提供可行方案。

Abstract: Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\% of a full fine-tune (98.84\% vs. 99.67\%) while training 97.2\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.

</details>


### [151] [NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation](https://arxiv.org/abs/2601.02204)
*Huichao Zhang,Liao Qu,Yiheng Liu,Hang Chen,Yangyang Song,Yongsheng Dong,Shikun Sun,Xian Li,Xu Wang,Yi Jiang,Hu Ye,Bo Chen,Yiming Gao,Peng Liu,Akide Liu,Zhipeng Yang,Qili Deng,Linjie Xing,Jiyang Liu,Zhao Wang,Yang Zhou,Mingcong Liu,Yi Zhang,Qian He,Xiwei Hu,Zhongqi Qi,Jie Shao,Zhiye Fu,Shuai Wang,Fangmin Chen,Xuezhi Chai,Zhihua Wu,Yitong Wang,Zehuan Yuan,Daniel K. Du,Xinglong Wu*

Main category: cs.CV

TL;DR: NextFlow 是一个统一的自回归解码器Transformer，在6万亿图文离散token上训练，采用文本“下一词”与视觉“下一尺度”联合预测，实现高效多模态理解与生成（含编辑、交错内容与视频），能在约5秒生成1024×1024图像，性能达SOTA并可匹敌扩散模型。


<details>
  <summary>Details</summary>
Motivation: 传统统一AR模型将图像按栅格扫描逐token生成，效率低且难与文本的严格序列性兼容；图像具有分层/多尺度结构，与文本的线性序列本质不同。需要一种既保留文本自回归优势，又能高效捕捉图像层次结构并稳定训练的统一方法。

Method: - 统一的解码器式自回归架构，使用统一的离散视觉表示，与文本token交错训练（6T图文token）
- 文本采用标准下一token预测；图像采用“下一尺度（next-scale）”预测，按多尺度金字塔逐级生成，而非逐像素/栅格扫描
- 针对多尺度生成不稳定性给出稳健训练配方
- 引入前缀微调（prefix-tuning）结合强化学习以进一步优化
- 原生支持图像编辑、交错图文与视频生成

Result: 在统一模型中取得SOTA；图像质量可与专门的扩散基线媲美；1024×1024图像约5秒生成，远快于传统AR光栅扫描方法；多模态理解与生成能力全面激活。

Conclusion: 通过将文本的下一token预测与视觉的下一尺度预测在同一AR框架中统一，NextFlow在效率、质量与多模态通用性上取得兼顾，成为统一多模态模型的有力方案，并展示了相对扩散方法的速度优势与竞争性视觉质量。

Abstract: We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.

</details>


### [152] [Seeing the Unseen: Zooming in the Dark with Event Cameras](https://arxiv.org/abs/2601.02206)
*Dachun Kai,Zeyu Xiao,Huyue Zhu,Jiaxiao Wang,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出RetinexEVSR：首个事件驱动的低光视频超分框架，结合Retinex先验与双向跨模态融合，显著提升细节与效率，在多数据集达SOTA，SDSD上PSNR提升最高2.95 dB且推理耗时降65%。


<details>
  <summary>Details</summary>
Motivation: 低光+低分辨率视频因对比度低与高频信息匮乏，现有LVSR方法难以恢复细节，直接融合退化RGB与噪声事件往往引入伪影与信息冗余，需一种能在低光下可靠利用事件高对比优势并引入物理先验的方案。

Method: 构建事件驱动LVSR框架RetinexEVSR：1) Retinex分解获取照度与反射；2) 设计照度引导的事件增强模块，用由Retinex照度图引导逐级净化与强化事件特征，抑制低光伪影并保留高对比细节；3) 提出事件引导的反射增强模块，利用增强后的事件特征通过多尺度融合动态恢复反射细节；4) 采用双向跨模态融合，在事件与RGB间相互提取有用线索而非直接拼接。

Result: 在三套数据集上达到SOTA；在SDSD基准上，相比以往事件方法PSNR最高提升2.95 dB，运行时间降低65%。

Conclusion: 通过将Retinex先验与事件高对比信息结合，并以双向跨模态融合有选择地整合两域线索，RetinexEVSR在低光视频超分中兼顾细节恢复与效率，显著优于现有方法，并提供可复现代码。

Abstract: This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.

</details>


### [153] [Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion](https://arxiv.org/abs/2601.02211)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: 论文系统剖析基于MMDiT的扩散 Transformer（如FLUX、Qwen-Image）在文生图与编辑中的内部机制，发现不同块对语义与细节的分工及其与文本条件的交互规律，据此提出免训练的文本对齐、精确编辑与加速策略，并在SD3.5上显著提升对齐指标且不降质。


<details>
  <summary>Details</summary>
Motivation: 虽有针对位置编码、注意力等局部组件的分析，但缺乏对MMDiT各层/各块及其与文本条件交互如何共同塑造生成过程的系统性理解。这限制了模型可解释性与实用改进（如更强对齐、编辑可控与推理效率）。

Method: 构建一套“块级干预”分析流水线：在特定块对文本隐藏状态进行移除（remove）、禁用（disable）或增强（enhance），观察生成结果变化以归纳功能分工与依赖关系；据此设计无需再训练的策略，包括在关键块选择性增强文本条件以提升语义对齐、在编辑时对相关块进行精确控制，以及基于对块重要性的认识进行推理加速。

Result: 发现：1）早期块承载语义组织，后期块负责细节渲染；2）移除个别块对结果破坏小于全局禁用文本条件；3）在特定块增强文本条件可显著改善语义属性。实验显示在SD3.5上T2I-Combench++从56.92%升至63.00%，GenEval从66.42%升至71.63%，同时保持合成质量；方法适用于文生图、图像编辑与推理加速，并优于多种基线。

Conclusion: 对MMDiT的层级职能与文本交互给出系统证据，证明针对性块级干预能在免训练前提下提升文本对齐、实现更精确编辑并加速推理；该理解为后续模型设计与实用优化提供了可操作的路径。

Abstract: Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.

</details>


### [154] [Prior-Guided DETR for Ultrasound Nodule Detection](https://arxiv.org/abs/2601.02212)
*Jingjing Wang,Zhuo Xiao,Xinning Yao,Bo Liu,Lijuan Niu,Xiangzhi Bai,Fugen Zhou*

Main category: cs.CV

TL;DR: 提出一种先验引导的DETR用于超声结节检测，通过在骨干、特征融合和编码器交互阶段注入几何与结构先验，显著提升对不规则、边界模糊、多尺度且含散斑噪声结节的检测性能。


<details>
  <summary>Details</summary>
Motivation: 超声结节（甲状腺、乳腺）早诊依赖准确检测，但实际存在形状不规整、边界不清、多尺度变化大以及散斑噪声强等难点，传统检测器或纯数据驱动方法难以稳定提取有效特征并保持结构一致性。

Method: 构建先验引导的DETR框架：1) 在CNN骨干嵌入带先验正则的空间自适应可变形FFN（SDFPR），以几何先验约束可变形采样，稳定不规则与模糊目标的特征提取；2) 设计多尺度时空-频域特征混合器（MSFFM），空间域突出轮廓连续与边界线索，频域建模全局形态并抑制散斑；3) 引入致密特征交互（DFI），在所有编码层传播并利用先验调制特征，为解码器查询细化提供一致的几何与结构指导。

Result: 在两个临床甲状腺数据集（Thyroid I、II）与两个公共基准（TN3K、BUSI）上，对比18种检测方法取得更高精度，尤其在形态复杂结节上优势明显；代码开源于GitHub链接。

Conclusion: 多阶段引入几何与结构先验可有效补足纯数据驱动DETR在超声场景的不足，提升对不规则、边界不清和多尺度结节的鲁棒检测，具有临床应用潜力。

Abstract: Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.

</details>


### [155] [FMVP: Masked Flow Matching for Adversarial Video Purification](https://arxiv.org/abs/2601.02228)
*Duoxun Tang,Xueyi Zhang,Chak Hin Wang,Xi Xiao,Dasen Dai,Xinhang Jiang,Wentao Shi,Rui Li,Qing Li*

Main category: cs.CV

TL;DR: 提出FMVP：基于条件流匹配与掩膜修复的对抗视频净化方法，通过物理“打散”对抗结构与频域门控损失分离语义与噪声，在UCF-101/HMDB-51上显著优于现有方法，并具备零样本检测与抗自适应攻击能力。


<details>
  <summary>Details</summary>
Motivation: 扩散式净化采样低效、轨迹弯曲，直接回归易破坏细节且难以去除细微对抗扰动。需要一种既能破坏全局对抗结构、又能忠实重建视频动态的更稳定高效框架。

Method: 1) 通过掩膜策略物理“打散”全局对抗结构，将净化转化为带条件的修复问题；2) 使用Conditional Flow Matching进行视频动态重建，避免扩散采样低效与轨迹问题；3) 设计Frequency-Gated Loss，抑制高频对抗残差、保留低频语义；4) 提供Attack-Aware与Generalist两种训练范式，分别面向已知与未知威胁。

Result: 在UCF-101与HMDB-51上，FMVP对PGD鲁棒准确率>87%，对CW>89%，优于DiffPure、DP、TS、FlowPure；对自适应攻击DiffHammer更稳健；作为零样本检测器，PGD检测98%，对高度不可感知的CW检测79%。

Conclusion: FMVP通过流匹配+修复和频域门控，有效分离语义与对抗噪声，提升视频识别鲁棒性与检测能力，相比扩散与传统防御方法更高效、效果更强，适用于已知与未知攻击场景。

Abstract: Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.

</details>


### [156] [VIBE: Visual Instruction Based Editor](https://arxiv.org/abs/2601.02242)
*Grigorii Alekseenko,Aleksandr Gordeev,Irina Tolstykh,Bulat Suleimanov,Vladimir Dokholyan,Georgii Fedorov,Sergey Yakubson,Aleksandra Tsybina,Mikhail Chernyshov,Maksim Kuprashevich*

Main category: cs.CV

TL;DR: 提出一种紧凑高吞吐的指令驱动图像编辑管线：用2B参数的Qwen3-VL做编辑控制，1.6B参数的Sana1.5扩散模型生成；在保证源图一致性的同时实现高质量编辑，性能匹配或超越更大模型，H100上BF16约4秒可生成2K分辨率，显存24GB可运行。


<details>
  <summary>Details</summary>
Motivation: 现有开源指令编辑方法能达到真实世界质量的很少；主流扩散骨干通常体量大（6B–20B），推理昂贵，不利于部署与研究；需要一种低成本、保持源图一致性且覆盖主流编辑类型的方案。

Method: 以小模型组合实现管线：使用2B Qwen3-VL负责理解指令与编辑控制，1.6B Sana1.5扩散模型负责图像生成；在架构、数据处理、训练配置与评估上做出针对低成本推理与严格源一致性的设计权衡。

Result: 在ImgEdit与GEdit基准上，性能达到或超过更大、更贵的基线；在需强一致性的编辑（属性调整、物体移除、背景编辑、定向替换）上表现尤佳；2K分辨率约4秒生成，24GB显存可运行，无需额外推理优化或蒸馏。

Conclusion: 小参数多模态控制+轻量扩散的紧凑管线可在低成本下实现高质量指令编辑，并在保持源图一致性方面具备优势，提供了相较大型模型更高的性价比与部署可行性。

Abstract: Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.

</details>


### [157] [A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets](https://arxiv.org/abs/2601.02246)
*Annoor Sharara Akhand*

Main category: cs.CV

TL;DR: 比较三种CNN实践范式：自建小模型、预训练特征抽取、迁移学习（微调）。在五个真实图像分类数据集上，迁移学习准确率与宏F1最佳；自建模型在算力受限时性价比高。


<details>
  <summary>Details</summary>
Motivation: 在实际视觉任务中，如何在模型性能与资源约束间权衡并选择：从零训练的小型模型、将大模型作固定特征器，或对预训练骨干进行（部分/全部）微调，缺乏系统、可控的对比。该工作旨在提供跨多数据集的实证证据。

Method: 在五个真实世界图像分类数据集上，对三种范式进行受控实验：①从头训练紧凑型自定义CNN；②使用大型预训练CNN作固定特征提取器；③对预训练骨干进行部分或全量微调。评估指标包括准确率、宏F1，以及效率指标（每轮训练时间、参数量）。

Result: 迁移学习在五个数据集上稳定取得最强预测表现（准确率与宏F1均优）。自建小型CNN在效率上占优，训练更快、参数更少，在资源受限情境下达到较好性能。固定特征抽取介于两者之间。

Conclusion: 对于真实世界图像分类，优先采用迁移学习可获得最佳效果；在算力/内存受限时，自建紧凑CNN提供良好的效率—准确率权衡。

Abstract: Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.

</details>


### [158] [SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection](https://arxiv.org/abs/2601.02249)
*Xiantai Xiang,Guangyao Zhou,Zixiao Wen,Wenshuai Li,Ben Niu,Feng Wang,Lijia Huang,Qiantong Wang,Yuhan Liu,Zongxu Pan,Yuxin Hu*

Main category: cs.CV

TL;DR: 提出SLGNet：在冻结ViT基础上，以结构先验与语言引导调制实现高效RGB-IR多模态目标检测，兼顾结构一致性与环境感知，在多数据集达SOTA且参数开销低。


<details>
  <summary>Details</summary>
Motivation: 现有基于适配器的方法虽可高效迁移RGB预训练模型，但牺牲跨模态结构一致性，尤其在高对比/夜间等域差大场景丢失关键结构；同时静态融合缺乏对环境变化的感知，限制复杂动态场景下的检测性能。

Method: 在冻结的ViT基础上，引入两大模块：1) 结构感知适配器（Structure-Aware Adapter），从RGB与IR提取分层结构表征，动态注入ViT以弥补ViT对结构建模的不足与跨模态结构退化；2) 语言引导调制（Language-Guided Modulation），利用VLM生成的结构化字幕对视觉特征进行动态重标定，提升环境感知与自适应融合能力。整体为参数高效的适配-调制框架。

Result: 在LLVIP、FLIR、KAIST、DroneVehicle上均达SOTA；LLVIP上mAP=66.1，较全量微调可减少约87%的可训练参数。

Conclusion: SLGNet通过层级结构先验与语言引导的环境自适应调制，在冻结ViT上实现高效且稳健的RGB-IR多模态检测，兼顾性能与参数效率，适用于复杂多变天气与光照场景。

Abstract: Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.

</details>


### [159] [VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation](https://arxiv.org/abs/2601.02256)
*Shikun Sun,Liao Qu,Huichao Zhang,Yiheng Liu,Yangyang Song,Xian Li,Xu Wang,Yi Jiang,Daniel K. Du,Xinglong Wu,Jia Jia*

Main category: cs.CV

TL;DR: 提出一种改进GRPO的框架，解决VAR在异步结构下用RL训练的策略冲突，通过中间奖励、时间步重加权和基于ReFL的掩码传播三招，显著提升样本质量与对齐稳定性。


<details>
  <summary>Details</summary>
Motivation: VAR在生成过程中输入结构随时间异构，导致策略在不同步的状态空间中产生冲突。此问题在RL优化时尤为严重，带来训练不稳定和对齐欠佳。现有GRPO未显式处理这类异步冲突，需要新的机制稳定训练并精细归因。

Method: 在GRPO上加入三组件：1) 稳定化的中间奖励，引导早期生成阶段；2) 动态时间步重加权，进行更精确的信用分配；3) 受ReFL启发的掩码传播算法，在时空维度上隔离优化影响，避免互相干扰。

Result: 相较原始GRPO基线，在样本质量与目标对齐上取得显著提升；对VAR模型实现更稳健、有效的RL优化。

Conclusion: 通过显式管理异步策略冲突，所提出的GRPO增强框架能稳定并提高VAR模型的RL训练效果，提供了一条在异构生成流程中实现高质量与高对齐度的可行路径。

Abstract: Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.

</details>


### [160] [DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies](https://arxiv.org/abs/2601.02267)
*Renke Wang,Zhenyu Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: 提出DiffProxy：利用扩散生成先验，生成多视角一致的人体代理以改进从多视图图像到人体网格的恢复；纯合成数据训练，在多项真实基准上SOTA并具零样本强泛化。


<details>
  <summary>Details</summary>
Motivation: 现实多视图人体数据的标注存在噪声会偏置训练；而合成数据虽精确但存在域差。需要一种方法既能利用合成精标注，又能在真实场景上泛化。

Method: 构建DiffProxy框架，以扩散模型作为生成先验，生成与像素对齐、跨视角一致的人体代理；引入多条件生成机制确保多视图一致性；加入可用灵活视觉提示的手部细化模块提升局部细节；在测试时采用不确定性感知的优化/缩放策略，提高在遮挡和部分视野等困难情况下的鲁棒性；整体在合成数据上训练，用这些代理推动网格恢复。

Result: 在五个真实世界基准上取得SOTA，展示出在遮挡与部分视角等困难场景中的强零样本泛化能力。

Conclusion: 扩散先验驱动的多视图一致人体代理能有效弥合合成训练与真实泛化的鸿沟，使网格恢复受益于合成精标注与扩散生成优势，并在实际基准上验证其有效性。

Abstract: Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html

</details>


### [161] [TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation](https://arxiv.org/abs/2601.02273)
*Salim Khazem*

Main category: cs.CV

TL;DR: 提出TopoLoRA-SAM：在冻结SAM的ViT编码器上用LoRA与轻量卷积适配器，并可选用clDice进行拓扑监督，实现对细薄结构和噪声模态的二分类语义分割的高效适配，训练仅约5.2%参数且在多数据集上取得最佳Dice。


<details>
  <summary>Details</summary>
Motivation: 基础分割模型（如SAM）具备零样本泛化，但迁移到特定领域任务（尤其细小结构如视网膜血管与噪声强的SAR）依然困难；全量微调成本高且易遗忘。因此需要参数高效、对拓扑敏感、适合细结构的适配策略。

Method: 在冻结的SAM ViT编码器中注入LoRA以进行低秩参数高效适配；加入轻量空间卷积适配器以补充局部空间细节；可选地采用可微clDice进行拓扑感知监督，鼓励保持连通性等拓扑结构。整体用于二分类语义分割。

Result: 在DRIVE、STARE、CHASE_DB1（视网膜血管）、Kvasir-SEG（息肉）、SL-SSDD（SAR海陆）五个基准上与U-Net、DeepLabV3+、SegFormer、Mask2Former对比，TopoLoRA-SAM在视网膜平均Dice和总体平均Dice上均最佳；仅训练约5.2%（≈4.9M）参数；在困难的CHASE_DB1上显著提升精度与鲁棒性。

Conclusion: 拓扑感知的参数高效适配（LoRA+轻量卷积+clDice）能够在保持低训练开销的同时，在细薄与噪声场景中达到或超过全量微调的专用模型表现。

Abstract: Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \textbf{5.2\%} of model parameters ($\sim$4.9M). On the challenging CHASE\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git

</details>


### [162] [InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams](https://arxiv.org/abs/2601.02281)
*Shuai Yuan,Yantai Yang,Xiaotian Yang,Xupeng Zhang,Zhonghao Zhao,Lingming Zhang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出InfiniteVGGT：一种带滚动记忆（有界、可自适应、持续表达的KV缓存）的因果视觉几何Transformer，实现真正的无限时域流式3D几何估计，并发布超长序列评测集Long3D（约1万帧）。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉几何方法在“可扩展性 vs. 长期稳定性”间难以兼顾：离线批处理模型（如VGGT）性能强但不适合在线；流式架构要么不能处理无限时域，要么长序列发生灾难性漂移。缺乏超长、连续基准也阻碍了严格验证。

Method: 设计InfiniteVGGT：在Transformer中引入有界但自适应、可持续表达的KV缓存作为滚动记忆；提出无需训练、与注意力实现无关的剪枝策略，智能丢弃过时信息，实现随帧推进的“滚动”；与FlashAttention兼容，支持高效流式推理。

Result: 在无限时域流式设定下，相比现有流式方法显著提升长期稳定性，且无需牺牲可扩展性；配套提出Long3D基准（约1万帧连续序列）以首次对连续3D几何估计进行严格评测。

Conclusion: InfiniteVGGT打破长期困境，实现无限时域流式3D几何理解的稳定与高效统一；Long3D为未来长期3D几何研究提供权威评测平台。

Abstract: The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT

</details>


### [163] [Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2601.02289)
*Tom Burgert,Leonard Hackel,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 提出GeoRank：一种在对比自监督学习中直接优化球面距离以编码地理关系的正则化方法，适用于多光谱遥感；在多种算法与设置上提升或匹配集成地理元数据的方法，并系统研究了增强、数据规模/图像尺寸、时间视角等适配因素。


<details>
  <summary>Details</summary>
Motivation: 多光谱遥感图像存在显著的地理与时间变化，给传统CV中的自监督学习带来挑战；现有做法虽尝试融入地理元数据，但仍不足以充分刻画地理关系并稳定泛化。

Method: 提出GeoRank，将地理信息通过优化样本间的球面距离（地理球面度量）直接注入对比学习的表示空间，作为正则项，可无缝集成到BYOL、DINO等对比SSL框架；同时对数据增强策略、数据规模与图像大小、以及多时相视角的任务依赖性进行系统实证。

Result: GeoRank在整合地理元数据的基线之上实现一致提升或持平，在多种对比SSL算法中表现稳健；实验显示针对多光谱RS的特定增强、适当的数据规模与图像尺寸选择，以及根据下游任务选择时间视角均显著影响性能。

Conclusion: 通过直接优化球面距离将地理关系嵌入表征空间，GeoRank为多光谱遥感的对比自监督提供通用、有效的正则化；配套的系统研究为该领域SSL实践提供了可操作的指导。

Abstract: Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.

</details>


### [164] [SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting](https://arxiv.org/abs/2601.02299)
*Sara Inácio,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 提出SortWaste真实世界垃圾分拣目标检测数据集与ClutterScore杂乱度指标，并在不同“硬度”水平上基准测试SOTA检测器；结果显示在高杂乱场景性能显著下降，表明需要更具挑战性的数据与方法。


<details>
  <summary>Details</summary>
Motivation: 现实垃圾流具有高度多样性、遮挡与视觉复杂度，人工分拣低效且有健康风险；现有自动化方法受限于缺乏真实世界、密集标注的数据集与统一难度衡量标准，难以有效泛化与评估。

Method: 1) 采集并密集标注来自回收分拣线的真实图像，构建SortWaste数据集；2) 设计ClutterScore，通过对象数量、类别与尺寸熵、空间重叠等代理变量综合衡量场景“硬度”；3) 对多种SOTA目标检测器进行系统基准评测，并按ClutterScore分层分析性能。

Result: 在塑料类别检测任务上达到约59.7% mAP；但随着场景杂乱度提高，所有模型性能显著下滑，表明当前检测器对高遮挡/高重叠/多类混杂情形鲁棒性不足。

Conclusion: SortWaste与ClutterScore为垃圾分拣检测提供了标准化数据与难度度量；基准结果暴露出高杂乱场景下的明显性能瓶颈，呼吁更具挑战的数据与针对复杂场景的鲁棒检测方法与训练策略。

Abstract: The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.

</details>


### [165] [360DVO: Deep Visual Odometry for Monocular 360-Degree Camera](https://arxiv.org/abs/2601.02309)
*Xiaopeng Guo,Yinzhe Xu,Huajian Huang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 提出360DVO：首个基于深度学习的单目全向视觉里程计，利用失真感知的球面特征与可微分的全向BA，实现在剧烈运动与光照变化下显著更稳、更准。


<details>
  <summary>Details</summary>
Motivation: 传统全向VO依赖手工特征或光度目标，遇到视角畸变、剧烈运动、光照变化时鲁棒性不足；需要一种能适应360°成像畸变并端到端优化位姿的学习方法与评测基准。

Method: 1) 设计DAS-Feat：在球面域自适应学习抗畸变的稀疏特征补丁；2) 通过这些稀疏特征构建约束，提出ODBA（全向可微分束调整）模块进行位姿优化；3) 构建真实场景OVO基准并在公开合成数据集上评测。

Result: 在新建真实基准及TartanAir V2、360VO数据集上，相比SOTA（360VO、OpenVSLAM），鲁棒性提升约50%，精度提升约37.5%。

Conclusion: 深度学习与球面畸变建模结合可显著提升单目全向VO的鲁棒性与精度；360DVO提供了可微端到端框架与评测基准，为360°视觉里程计奠定新的基线。

Abstract: Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage

</details>


### [166] [Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping](https://arxiv.org/abs/2601.02315)
*Saurabh Kaushik,Lalit Maurya,Beth Tellman*

Main category: cs.CV

TL;DR: 提出Prithvi-CAFE：在Prithvi地理基础模型的编码器上并行加入带注意力的CNN分支与多尺度融合，通过适配器高效微调，在两套洪涝制图数据集上取得SOTA，显著优于U‑Net与多种GFM。


<details>
  <summary>Details</summary>
Motivation: 现有地理基础模型（GFMs）在多任务上表现良好，但在洪涝制图（如Sen1Flood11）中无法超过简单的U‑Net，主要因难以捕捉关键的局部细节。需要一种既保留长程依赖又强化局部纹理/边界信息的架构。

Method: 提出Prithvi‑CAFE（Complementary Adaptive Fusion Encoder）：以Prithvi预训练编码器为主干，通过适配器进行快速高效微调；并联一个带残差的CNN分支，嵌入卷积注意力模块（CAM）；在多尺度、多层级对Transformer与CNN特征进行自适应融合，以结合全局上下文与局部细节。

Result: 在Sen1Flood11测试集IoU 83.41，超过原Prithvi 82.50、TerraMind 82.90、DOFA 81.54、spectralGPT 81.02；在保留测试站点IoU 81.37，显著优于U‑Net 70.57与Prithvi 72.42。于FloodPlanet数据集IoU 64.70，优于U‑Net 60.14、Terramind 62.33、DOFA 59.15、Prithvi 2.0 61.91。

Conclusion: 将GFM的全局表征与CNN的局部建模通过自适应多尺度融合相结合，可在多通道/多模态卫星数据的洪涝分割中显著提升性能。方法简单高效、易于微调，具备推广到强调局部细节的其他遥感分割任务的潜力。

Abstract: Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}

</details>


### [167] [Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching](https://arxiv.org/abs/2601.02318)
*Roja Sahoo,Anoop Namboodiri*

Main category: cs.CV

TL;DR: 提出Fusion2Print(F2P)：将同一手指的闪光/非闪光无接触指纹成对采集与融合，显著提升脊线清晰度与跨域识别性能（AUC 0.999、EER 1.12%）。


<details>
  <summary>Details</summary>
Motivation: 无接触指纹更卫生便捷，但易受光照、皮下色差与镜面反射影响，导致脊线对比度低、噪声高。单一拍摄模式存在权衡：闪光保细节但噪声大；非闪光噪声小但对比度差。需要一种在采集与算法端联合优化、既保留脊线信息又抑制噪声、并能与接触式模板互通的方案。

Method: 1) 采集：构建成对闪光-非闪光(FNF)无接触指纹数据库；人工做差分以分离保脊线的成分。2) 融合：设计轻量注意力融合网络，按通道强调信息、抑制噪声，融合两模态。3) 增强：用U-Net模块进一步增强与加权，输出单通道最优灰度图。4) 表征：训练具跨域兼容性的深度嵌入模型，将无接触与接触式指纹映射到统一嵌入空间，实现验证。

Result: 在识别上优于单次采集基线（如Verifinger、DeepPrint），报告AUC=0.999、EER=1.12%，同时可见的脊线清晰度提升。

Conclusion: 通过成对采集+注意力融合+U-Net增强+跨域嵌入，F2P在保留细节与抑制噪声间取得更佳平衡，实现与接触式指纹的统一表示，并在验证性能上显著超过现有方法。

Abstract: Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).

</details>


### [168] [BEDS: Bayesian Emergent Dissipative Structures](https://arxiv.org/abs/2601.02329)
*Laurent Caraffa*

Main category: cs.CV

TL;DR: BEDS 提出一个把非平衡热力学、贝叶斯推断、信息几何与机器学习统一起来的框架：学习=通过熵外排把通量转化为结构；在形式上将热力学演化与贝叶斯更新同构化，并给出分层“后验结晶为下一层先验”的涌现机制；声称在极简公理下 e/π/φ 是贝叶斯固定点；提出哥德尔不完备性与热力学约束的类比；并以一种点对点网络架构作验证，据称在能效上较现有共识提升 10^6 倍，同时实现连续学习。


<details>
  <summary>Details</summary>
Motivation: 解释跨物理—生物—计算系统中“学习”的普适机理，寻求把热力学的耗散结构与统计学习/推断统一起来，并为可持续 AI 提供理论与工程路径。

Method: - 从普里高津耗散结构出发，建立热力学过程与贝叶斯更新的形式同构与信息几何表述。
- 证明或论证在最小公理下 e、π、φ 作为贝叶斯更新的固定点。
- 提出一个将不完备性与热力学资源/耗散限制对应的猜想。
- 设计并实现符合 BEDS 原则的 P2P 网络与学习型共识协议，报告能效对比。

Result: 理论上：给出同构框架与固定点常数的推导与猜想；提出分层涌现中的“后验→先验”递归机制。工程上：实现一个 BEDS 风格的 P2P 原型，声称相较既有分布式共识能效提升六个数量级，且可持续在线学习。

Conclusion: 学习可被视为通过熵外排将通量凝结为结构的耗散过程；贝叶斯更新与热力学演化在形式上同构，由此涌现分层学习；某些数学常数与形式系统病理可能源于这一普适结构；据称该框架还能指导高能效、可持续的分布式智能系统设计。

Abstract: We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.
  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.
  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.

</details>


### [169] [Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding](https://arxiv.org/abs/2601.02339)
*Jingming He,Chongyi Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出一个联合增强的3D语义高斯(3DGS)建模框架：用各向异性Chebyshev形状描述子+语义/形状驱动的自适应高斯与SH分配+跨场景知识迁移，实现更准分割与高质量渲染且高帧率。


<details>
  <summary>Details</summary>
Motivation: 现有将语义特征融入3DGS的方法：1) 语义与渲染分支割裂，仅用2D监督，忽视3D几何，导致对纹理弱/细微区域不稳；2) 自适应策略只看渲染梯度，难以在无纹理或细节处有效配置高斯资源；3) 新场景往往需从头学习形状先验，收敛慢、泛化弱。

Method: - 引入各向异性3D Gaussian Chebyshev描述子，结合拉普拉斯-贝尔特拉米算子编码精细3D形状，缓解对噪声2D监督的依赖；- 以局部语义与形状信号共同驱动自适应：动态调整高斯分配与球谐系数(SH)，在重要/复杂区域投入更多资源，提高渲染效率；- 设计跨场景知识迁移模块，持续累积与更新形状模式，在新场景中复用以加速收敛并提升稳健性。

Result: 在多数据集上，相比基线同时提升语义分割精度与渲染质量，并保持高渲染帧率。

Conclusion: 联合利用语义与渲染并显式建模3D形状可提升3DGS的语义与视觉表现；语义/形状驱动的自适应资源分配与跨场景形状迁移带来更快、更稳和更高效的训练与推理。

Abstract: Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.

</details>


### [170] [Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices](https://arxiv.org/abs/2601.02353)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.CV

TL;DR: 将剪枝与小样本学习结合，通过提出疾病感知通道重要性评分（DACIS）并嵌入“三段式PMP（先剪枝-再元学习-再剪枝）”流程，在PlantVillage与PlantDoc上以较小精度损失实现显著压缩与在树莓派上的实时推理。


<details>
  <summary>Details</summary>
Motivation: 边远地区农户缺乏实验室与高算力设备；现有深度模型虽准确但对边缘设备过于庞大；获取大量标注病叶图像成本高且耗时，亟需能在低资源设备上、用少量样本仍能有效诊断的方案。

Method: 提出DACIS（Disease-Aware Channel Importance Scoring）用于评估网络中对疾病区分最关键的通道；将其集成至PMP流水线：1）Prune：依据DACIS进行结构化剪枝以去除冗余；2）Meta-Learn：采用小样本/元学习在少量样本上适配；3）Prune：在适配后再次精修剪枝以进一步压缩。在PlantVillage与PlantDoc上验证，并在树莓派4上部署测试FPS。

Result: 模型尺寸减少约78%，保持原始精度的92.3%；在树莓派4上达到约7 FPS，实现接近实时；在两个公开数据集上均验证有效。

Conclusion: 疾病感知通道评分结合PMP流程，可在低算力设备上以少量样本实现高效病害识别，兼顾模型压缩与泛化，适用于小农户现场快速诊断的实际场景。

Abstract: Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\% while maintaining 92.3\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.

</details>


### [171] [Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes](https://arxiv.org/abs/2601.02356)
*Jing Tan,Zhaoyang Zhang,Yantao Shen,Jiarui Cai,Shuo Yang,Jiajun Wu,Wei Xia,Zhuowen Tu,Stefano Soatto*

Main category: cs.CV

TL;DR: Talk2Move 是一个结合强化学习与扩散模型的文本指令驱动场景内目标空间变换框架，能在无需成对监督的情况下，实现目标级的平移、旋转与缩放，优于现有文本编辑方法的空间精度与场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本编辑多擅长外观/风格调整，但难以实现目标级几何变换，原因包括：缺乏成对监督数据、像素级优化难以探索离散/连续的几何动作空间、以及奖励信号与语言语义对齐不足。因此需要一种能高效探索几何动作、与语言描述对齐、且可解释的空间奖励机制。

Method: 提出 Talk2Move：基于扩散模型生成多样轨迹，并用 GRPO（Group Relative Policy Optimization）进行几何动作策略学习；通过轻量文本变体与输入图像生成多样 rollout，摆脱昂贵配对数据；设计对象中心的空间奖励（位移、旋转、缩放）直接度量变换行为，并加入语言引导的空间奖励实现语义对齐；使用离策略步骤评估与主动步骤采样，聚焦信息量大的阶段以提升学习效率与稳定性。

Result: 在精心构建的基准上，Talk2Move 能实现精确、一致、语义忠实的目标空间变换，在空间精度与场景一致性方面均超过现有文本引导编辑方法。

Conclusion: 强化学习与扩散的结合、对象中心的空间奖励设计，以及高效的离策略/主动采样训练策略，使 Talk2Move 在无需成对监督的前提下，实现可解释且高质量的文本指令空间操作，推进了多模态编辑从外观到几何层面的能力。

Abstract: We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.

</details>


### [172] [VINO: A Unified Visual Generator with Interleaved OmniModal Context](https://arxiv.org/abs/2601.02358)
*Junyi Chen,Tong He,Zhoujie Fu,Pengfei Wan,Kun Gai,Weicai Ye*

Main category: cs.CV

TL;DR: VINO 是一个统一的视觉生成与编辑模型，用同一扩散骨干处理图像与视频的生成/编辑，依赖文本、图像和视频的交错条件编码，实现多参考对齐、长指令跟随与身份一致性；通过多阶段训练将视频生成基座扩展为多任务统一生成器，跨多基准取得高质量与可控编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法常为不同任务/模态设计专用模型或独立模块，带来工程复杂、泛化差与难以保持身份与属性一致的问题。需要一种能在单一架构下统一图像与视频的生成与编辑，并可靠遵循复杂指令与多参考约束的通用方案。

Method: 提出 VINO：将视觉语言模型（VLM）与多模态扩散 Transformer（MMDiT）耦合，把文本、图像、视频编码为交错的条件 token，用以引导扩散过程；不依赖模态特定组件。训练上采用多阶段流程，从视频生成基座逐步扩展为支持图像与视频输入/输出的统一多任务生成器，支持多参考定位、长文本指令与身份保持。

Result: 在多种生成与编辑基准上，VINO 展现出更强的画质、指令遵循度、参考与属性保持能力，并能更可控地进行多身份编辑。

Conclusion: 交错的“in-context”多模态条件与统一扩散骨干，为可扩展的通用视觉生成提供了可行路径；VINO 证明了在单模型内统一图像/视频生成与编辑的有效性，并提升了可控性与一致性。

Abstract: We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.

</details>


### [173] [ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors](https://arxiv.org/abs/2601.02359)
*Kaede Shiohara,Toshihiko Yamasaki,Vladislav Golyanik*

Main category: cs.CV

TL;DR: ExposeAnyone：一种基于扩散模型、完全自监督的“人-特定”人脸伪造检测方法。通过个性化适配受试者并利用扩散重建误差作为身份距离，实现对未知操纵、跨数据集、以及Sora2生成视频的鲁棒检测，在四个数据集平均AUC上超越SOTA 4.22个百分点，并对模糊与压缩等扰动具备稳健性。


<details>
  <summary>Details</summary>
Motivation: 监督式深伪检测依赖已知或伪造样本，易对特定伪造模式过拟合，泛化到未知操纵效果差；现有自监督方法难以仅凭自监督学到足够判别性的特征。需要一种无需伪造样本训练、却能泛化到未知操纵并保持鲁棒性的检测框架。

Method: 提出ExposeAnyone：使用从音频生成表情序列的扩散模型作为自监督骨干。先用少量参考集对特定“人”进行个性化（personalization），再将待检视频输入，计算其与该个性化主体之间的扩散重建误差，作为身份距离与一致性度量，从而进行面向“关注对象”的伪造检测。无需使用任何真实深伪或伪造数据进行监督训练。

Result: 在DF-TIMIT、DFDCP、KoDF、IDForge四数据集上，平均AUC较前SOTA提高4.22个百分点；对Sora2生成视频检测效果显著优于既有方法；在模糊、压缩等常见破坏条件下保持高鲁棒性。

Conclusion: 通过个性化扩散重建误差来度量身份一致性，实现了无需伪造监督的未知操纵检测与强泛化。该框架在多数据集、合成来源和破坏条件下均表现出色，显示其在真实场景人脸伪造取证中的应用潜力。

Abstract: Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.

</details>
