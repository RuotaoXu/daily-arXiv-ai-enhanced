<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dynamic Mask-Based Backdoor Attack Against Vision AI Models: A Case Study on Mushroom Detection](https://arxiv.org/abs/2601.18845)
*Zeineb Dridi,Jihen Bennaceur,Amine Ben Hassouna*

Main category: cs.CV

TL;DR: 提出一种针对目标检测模型的动态掩码后门攻击：用SAM生成掩码在中毒数据上动态植入触发器，使YOLOv7在干净数据上保持高精度、在带触发样本上高攻击成功率，优于静态图案方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击多用于分类、采用静态触发器，易被检测或在检测任务上效果受限；实际外包训练流程存在数据投毒风险，需展示在关键场景（如蘑菇检测）中的现实威胁。

Method: 利用SAM为每张图像生成分割掩码，在这些区域内随机/自适应放置动态触发器；通过数据集投毒将触发器嵌入训练样本，训练YOLOv7目标检测器；评估在干净数据上的精度与在触发样本上的攻击成功率，并与静态触发器方法对比。

Result: 在蘑菇检测数据集上，模型对干净数据维持高准确度（未显著下降），对投毒触发样本达到高攻击成功率；与传统静态/一致图案的后门注入方法相比，性能更优且更隐蔽。

Conclusion: 动态掩码驱动的后门对目标检测模型构成严重安全威胁，能在保持模型表观性能的同时大幅提高攻击效果，凸显对鲁棒防御与供应链安全的迫切需求。

Abstract: Deep learning has revolutionized numerous tasks within the computer vision field, including image classification, image segmentation, and object detection. However, the increasing deployment of deep learning models has exposed them to various adversarial attacks, including backdoor attacks. This paper presents a novel dynamic mask-based backdoor attack method, specifically designed for object detection models. We exploit a dataset poisoning technique to embed a malicious trigger, rendering any models trained on this compromised dataset vulnerable to our backdoor attack. We particularly focus on a mushroom detection dataset to demonstrate the practical risks posed by such attacks on critical real-life domains. Our work also emphasizes the importance of creating a detailed backdoor attack scenario to illustrate the significant risks associated with the outsourcing practice. Our approach leverages SAM, a recent and powerful image segmentation AI model, to create masks for dynamic trigger placement, introducing a new and stealthy attack method. Through extensive experimentation, we show that our sophisticated attack scenario maintains high accuracy on clean data with the YOLOv7 object detection model while achieving high attack success rates on poisoned samples. Our approach surpasses traditional methods for backdoor injection, which are based on static and consistent patterns. Our findings underscore the urgent need for robust countermeasures to protect deep learning models from these evolving adversarial threats.

</details>


### [2] [Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding](https://arxiv.org/abs/2601.18849)
*Yuhui Zhang,Hui Yu,Wei Liang,Sunjie Zhang*

Main category: cs.CV

TL;DR: 提出一种用于说话人像的动态NeRF改进方法，通过眨眼嵌入与哈希网格地标编码，配合动态地标Transformer与音频残差，实现更准确高效的口型与面部动态重建，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态NeRF在说话人像上尽管速度与质量进步显著，但口腔区域（口型/口腔内部）运动细节难以准确高效捕获，造成逼真度不足与时序不稳定，因此需要一种能更好约束与驱动口部动态的表示和条件化机制。

Method: 1) 设计眨眼嵌入与哈希网格地标编码，将人脸关键点以高效多尺度表示输入神经场；2) 构建Dynamic Landmark Transformer，将面部特征作为条件，音频特征以残差方式融合，学习音频到面部动态的映射；3) 使用NeRF建模整张面部的辐射场，实现统一可微渲染。

Result: 在实验评估中，该方法在口型还原精度、整体面部逼真度与渲染一致性方面优于现有方法；展现出更高的口部运动保真与效率（摘要未给出具体指标与数据）。

Conclusion: 通过结合地标编码与Transformer式的条件/残差融合，将音频与人脸动态紧密对齐，显著提升说话人像的口部和整体面部重建质量；方法自动化、适用于动态NeRF框架，并在实验中显示优越性能。

Abstract: Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.

</details>


### [3] [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/abs/2601.18851)
*Wei Liang,Hui Yu,Derui Ding,Rachael E. Jack,Philippe G. Schyns*

Main category: cs.CV

TL;DR: 提出一种仅用自拍视频的详细头部化身复演方法：3DMM参数驱动 + StyleGAN生成器，混合损失兼顾前景重建与对抗训练以恢复高频细节，在自/跨复演中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3DMM重建虽高保真但难以实时覆盖整头与背景；GAN方法可高质量复演但细粒度细节（皱纹、发丝）欠缺；多数方法需大量数据，难以用单个自拍视频完成高质量化身复演。

Method: 将3DMM的可控性与StyleGAN的生成能力结合：以3DMM提供几何与表情/姿态驱动，设计面向前景的重建与对抗式头像生成的混合损失，强化高频细节恢复；在自拍视频上训练/微调，支持自复演与跨复演。

Result: 在自复演与跨复演的定性和定量评估中，相比现有方法，生成的头部化身具有更丰富细腻的纹理与细节，整体重建质量更优。

Conclusion: 融合3DMM与StyleGAN并采用混合损失，可在仅有自拍视频的设定下实现高保真、细节丰富的头部化身复演，兼顾可控性与细节表现，优于当下方法。

Abstract: Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.

</details>


### [4] [Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)](https://arxiv.org/abs/2601.18891)
*Ghazaleh Serati,Samuel Foucher,Jerome Theau*

Main category: cs.CV

TL;DR: 提出一种针对北极驯鹿航拍影像的弱监督补丁级预训练策略，提升 HerdNet 在多场景、类不平衡与小目标条件下的检测与计数性能。


<details>
  <summary>Details</summary>
Motivation: 北极驯鹿数量近几十年下降，亟需可扩展、准确的自动检测以支撑保护决策；但航拍影像存在背景异质性强、空背景占比高、小/遮挡目标与密度尺度变化大，人工解读费时且易错。

Method: 基于检测网络架构进行弱监督的补丁级预训练：使用来自阿拉斯加五个鹿群的数据，仅用“空/非空”粗标注训练补丁分类模型，获得可迁移的早期表征；随后将该预训练权重用于完整目标检测与计数，与传统 ImageNet 初始化对比。

Result: 补丁级预训练在2017多鹿群与2019独立测试上补丁分类F1分别达93.7%/92.6%；迁移到检测任务后，相比 ImageNet 初始化，正样本补丁F1提升至92.6%/93.5%（对比89.3%/88.6%），整幅图计数F1达95.5%/93.3%（对比91.5%/90.4%）。

Conclusion: 在标注有限时，先用粗粒度“空/非空”标签进行补丁级预训练，可提供优于通用权重的初始化，显著提升驯鹿自动检测与计数的稳健性；仍存在动物相似背景导致的误报与低密度遮挡导致的漏检。

Abstract: Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.

</details>


### [5] [RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection](https://arxiv.org/abs/2601.18900)
*Haim Zisman,Uri Shaham*

Main category: cs.CV

TL;DR: 提出一个统计学基础且免训练的AI假图像检测框架：对多种训练无关统计量计算p值并用经典统计集成，输出可与真实图像总体对齐、可解释的概率分数，提升跨分布鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有检测器虽有效，但缺乏形式化可解释性，常隐含关于“伪造”模式的假设，遇到分布转移可能失效。需要一个能在不断演化的生成模型下仍稳健、且分数含义清晰（相对真实分布）的检测方法。

Method: 收集多种现有、训练无关的图像统计量；针对每个统计量相对于“真实图像总体”计算p值；通过经典统计集成（如Fisher/Stouffer等）将多p值聚合，得到与统一真实分布对齐的综合度量，作为假图像检测的可解释概率评分。整个流程无需额外训练，框架通用且可插拔。

Result: 该框架能稳健地整合不同检测信号，给出与真实分布一致性的显著性评分，在多种数据分布和不断演化的生成模型下表现出更强的鲁棒性（摘要暗示优于单一或依赖训练的检测器）。

Conclusion: 以统计检验为核心、p值集成为手段的免训练通用框架，可提供相对于真实图像总体可解释的概率分数，并在多样、变化的场景中实现更稳健的假图像检测。

Abstract: As generative models continue to evolve, detecting AI-generated images remains a critical challenge. While effective detection methods exist, they often lack formal interpretability and may rely on implicit assumptions about fake content, potentially limiting robustness to distributional shifts. In this work, we introduce a rigorous, statistically grounded framework for fake image detection that focuses on producing a probability score interpretable with respect to the real-image population. Our method leverages the strengths of multiple existing detectors by combining training-free statistics. We compute p-values over a range of test statistics and aggregate them using classical statistical ensembling to assess alignment with the unified real-image distribution. This framework is generic, flexible, and training-free, making it well-suited for robust fake image detection across diverse and evolving settings.

</details>


### [6] [On the Role of Depth in Surgical Vision Foundation Models: An Empirical Study of RGB-D Pre-training](https://arxiv.org/abs/2601.18929)
*John J. Han,Adam Schmidt,Muhammad Abdullah Jamal,Chinedu Nwoye,Anita Rau,Jie Ying Wu,Omid Mohareri*

Main category: cs.CV

TL;DR: 对比8种ViT系VFM在外科场景中使用RGB与RGB-D预训练的效果：显式几何建模（如MultiMAE）在检测/分割/深度/位姿上全面优于仅RGB；仅在预训练阶段引入深度即可带来显著增益与数据效率提升，推理无需改动。


<details>
  <summary>Details</summary>
Motivation: 现有外科视觉大多基于单模态RGB预训练，忽视手术场景复杂3D几何；虽通用CV已有多模态/几何感知框架，但在手术领域引入深度信息的收益尚不清楚，需系统量化。

Method: 构建含140万机器人外科图像及由现成网络生成的深度图的数据集；选取8个基于ViT的VFM，覆盖不同预训练域、目标与输入模态（RGB vs. RGB-D/几何标记）；在冻结与端到端两种协议下，于8个外科数据集上评估四类任务：目标检测、分割、深度估计、位姿估计；重点考察显式几何分词与仅在预训练阶段使用深度的策略。

Result: 显式几何分词模型（如MultiMAE）在所有任务上稳定超过单模态基线；几何感知预训练显著提升数据效率：只用25%标注微调即可超越用全部数据训练的RGB-only模型；在推理端无需输入深度或改动架构，收益仍然保留。

Conclusion: 在外科视觉中，多模态（特别是显式几何/深度）预训练显著提升跨任务性能与数据效率；将深度仅用于预训练即可无缝部署，表明多模态预训练是构建更强外科视觉系统的可行路径。

Abstract: Vision foundation models (VFMs) have emerged as powerful tools for surgical scene understanding. However, current approaches predominantly rely on unimodal RGB pre-training, overlooking the complex 3D geometry inherent to surgical environments. Although several architectures support multimodal or geometry-aware inputs in general computer vision, the benefits of incorporating depth information in surgical settings remain underexplored. We conduct a large-scale empirical study comparing eight ViT-based VFMs that differ in pre-training domain, learning objective, and input modality (RGB vs. RGB-D). For pre-training, we use a curated dataset of 1.4 million robotic surgical images paired with depth maps generated from an off-the-shelf network. We evaluate these models under both frozen-backbone and end-to-end fine-tuning protocols across eight surgical datasets spanning object detection, segmentation, depth estimation, and pose estimation. Our experiments yield several consistent findings. Models incorporating explicit geometric tokenization, such as MultiMAE, substantially outperform unimodal baselines across all tasks. Notably, geometric-aware pre-training enables remarkable data efficiency: models fine-tuned on just 25% of labeled data consistently surpass RGB-only models trained on the full dataset. Importantly, these gains require no architectural or runtime changes at inference; depth is used only during pre-training, making adoption straightforward. These findings suggest that multimodal pre-training offers a viable path towards building more capable surgical vision systems.

</details>


### [7] [Smart Split-Federated Learning over Noisy Channels for Embryo Image Segmentation](https://arxiv.org/abs/2601.18948)
*Zahra Hafezi Kafshgari,Ivan V. Bajic,Parvaneh Saeedi*

Main category: cs.CV

TL;DR: 提出在SplitFed学习中针对通信信道噪声的“智能平均”策略，能在保持模型精度的同时容忍比常规平均高两个数量级的噪声（在胚胎图像分割任务上验证）。


<details>
  <summary>Details</summary>
Motivation: SplitFed学习降低了客户端算力需求，但其训练依赖在客户端与服务器之间传输特征、梯度和模型更新；实际通信信道存在噪声，可能破坏收敛与性能，需提升对噪声的鲁棒性。

Method: 在SplitFed框架中设计一种“智能平均”（smart averaging）聚合策略，替代传统的简单平均以抑制由信道噪声引入的偏差/方差。方法细节未在摘要中给出，推测是对上传/下发更新进行加权或去噪处理，以减小噪声累积效应。

Result: 在胚胎图像分割模型上实验，智能平均在信道噪声强度提高两个数量级时，仍能维持最终模型精度；而常规平均在该噪声水平下性能退化。

Conclusion: 智能平均显著增强了SplitFed学习对通信噪声的鲁棒性，可在低质量或高噪声网络环境中保持训练效果，相比传统平均具有明显优势。

Abstract: Split-Federated (SplitFed) learning is an extension of federated learning that places minimal requirements on the clients computing infrastructure, since only a small portion of the overall model is deployed on the clients hardware. In SplitFed learning, feature values, gradient updates, and model updates are transferred across communication channels. In this paper, we study the effects of noise in the communication channels on the learning process and the quality of the final model. We propose a smart averaging strategy for SplitFed learning with the goal of improving resilience against channel noise. Experiments on a segmentation model for embryo images shows that the proposed smart averaging strategy is able to tolerate two orders of magnitude stronger noise in the communication channels compared to conventional averaging, while still maintaining the accuracy of the final model.

</details>


### [8] [Pay Attention to Where You Look](https://arxiv.org/abs/2601.18970)
*Alex Beriand,JhihYang Wu,Daniel Brignac,Natnael Daba,Abhijit Mahalanobis*

Main category: cs.CV

TL;DR: 提出一种用于少视角新视角合成（NVS）的相机加权机制，通过根据与目标视角的相关性自适应地为源视角分配权重，从而提升合成质量与真实感。既提供基于几何的确定性加权，也提供基于交叉注意力的可学习加权，并可作为可插拔模块整合并进一步训练以改善性能。


<details>
  <summary>Details</summary>
Motivation: 现有少视角NVS方法通常默认所有输入视角对目标视角同等重要，这在视角覆盖不充分、基线差异大或存在遮挡时会导致错误聚合与伪影，限制了准确性与真实感。需要一种能按与目标相机位姿相关性动态分配贡献度的机制。

Method: 提出两类相机加权策略：1) 确定性加权：依据源-目标相机的欧氏距离、视线夹角等几何指标计算权重；2) 基于交叉注意力的学习式加权：以源视角特征与目标视角查询进行交叉注意力，学习到反映相关性的权重。该机制作为可插拔组件嵌入现有NVS管线，并允许在任务数据上端到端微调以进一步校准权重分配。

Result: 在实验证明下，引入自适应相机加权后，合成精度和视觉真实感均有提升，较假设等权的基线更好地处理不同视角覆盖与遮挡情况。

Conclusion: 自适应视角加权是提升少视角NVS质量的有效方向，可广泛集成到多种NVS算法中；几何先验与学习式权重互补，联合使用与进一步训练可持续改进合成表现。

Abstract: Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.
  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.

</details>


### [9] [FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction](https://arxiv.org/abs/2601.18993)
*Wei Cao,Hao Zhang,Fengrui Tian,Yulun Wu,Yingying Li,Shenlong Wang,Ning Yu,Yaoyao Liu*

Main category: cs.CV

TL;DR: FreeOrbit4D提出一种训练免框架，通过构建“几何完备”的4D代理来作为视频扩散生成的结构支撑，从而在大角度相机重定向中保持几何与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 单目视频只覆盖动态场景的狭窄时空视锥，导致在大角度视角变化下缺乏视觉锚点，出现几何歧义与时间不一致；现有扩散方法在远离原相机轨迹时容易崩溃，需要一种能从不完整观测中恢复完整且一致的4D表示来作为可靠的几何约束。

Method: 训练免两阶段：1) 通过将单目视频反投影到统一全局空间，分离静态背景与几何不完备的前景点云；2) 使用对象中心的多视角扩散模型合成多视图并在“规范（canonical）对象空间”重建几何完备的前景点云；随后基于像素同步的稠密3D–3D对应，将规范前景点云对齐回全局场景空间，得到几何完备的4D代理；最后把该代理投影到目标相机视角，为条件视频扩散模型提供几何脚手架，指导生成。

Result: 在大角度相机轨迹下，生成结果更忠实、几何更稳定、时间一致性更好；还能实现编辑传播与4D数据合成等下游应用潜力。

Conclusion: 几何完备的4D代理作为结构化先验能有效缓解单目重定向的几何歧义，训练免流程在大角度视角下显著提升扩散生成的稳健性与保真度，并为可编辑与可重用的4D表示提供基础。

Abstract: Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.

</details>


### [10] [Anatomically-aware conformal prediction for medical image segmentation with random walks](https://arxiv.org/abs/2601.18997)
*Mélanie Gaillochet,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: 提出RW-CP：在医学影像分割中结合随机游走的保形预测，保证统计覆盖率同时提升空间连贯与解剖一致性。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在分割任务中忽视解剖与空间上下文，产生碎片化、过分割的集合，临床可用性差；需要既具严格误差保证又具解剖连贯性的预测不确定性方法。

Method: 在任意分割模型之上：用预训练视觉基础模型特征构建像素/超像素的k近邻图；在非一致性分数上进行随机游走扩散以正则化不确定性，使预测集合对校准参数λ不敏感；最终生成空间连贯的保形预测集合，保持边缘覆盖率。

Result: 在多模态公共数据集上，相比标准CP基线，在允许错误率α=0.1下，分割质量最高提升35.4%，同时维持严格的边际覆盖。

Conclusion: RW-CP在不改变底层分割模型的前提下，将空间连贯性注入CP，稳定边界、降低对λ的敏感性，在保证统计有效性的同时显著提升解剖合理性与实用性。

Abstract: The reliable deployment of deep learning in medical imaging requires uncertainty quantification that provides rigorous error guarantees while remaining anatomically meaningful. Conformal prediction (CP) is a powerful distribution-free framework for constructing statistically valid prediction intervals. However, standard applications in segmentation often ignore anatomical context, resulting in fragmented, spatially incoherent, and over-segmented prediction sets that limit clinical utility. To bridge this gap, this paper proposes Random-Walk Conformal Prediction (RW-CP), a model-agnostic framework which can be added on top of any segmentation method. RW-CP enforces spatial coherence to generate anatomically valid sets. Our method constructs a k-nearest neighbour graph from pre-trained vision foundation model features and applies a random walk to diffuse uncertainty. The random walk diffusion regularizes the non-conformity scores, making the prediction sets less sensitive to the conformal calibration parameter $λ$, ensuring more stable and continuous anatomical boundaries. RW-CP maintains rigorous marginal coverage while significantly improving segmentation quality. Evaluations on multi-modal public datasets show improvements of up to $35.4\%$ compared to standard CP baselines, given an allowable error rate of $α=0.1$.

</details>


### [11] [Non-Invasive 3D Wound Measurement with RGB-D Imaging](https://arxiv.org/abs/2601.19014)
*Lena Harkämper,Leo Lebrat,David Ahmedt-Aristizabal,Olivier Salvado,Mattias Heinrich,Rodrigo Santa Cruz*

Main category: cs.CV

TL;DR: 提出一种基于RGB-D的快速非侵入式3D创面测量算法，结合RGB-D里程计与B样条曲面重建，实现高精度创面网格与自动指标计算；在仿真硅胶创面上达到亚毫米精度，复测一致性好，优于现有方法且可实时部署。


<details>
  <summary>Details</summary>
Motivation: 临床与远程医疗需要准确、高效、可重复的创面尺寸与形态量化，但现有手工或2D方法误差大、效率低，且3D重建方法在速度、鲁棒性与易用性上不足。

Method: 使用RGB-D里程计获取多视角序列并配准，采用B样条曲面重建生成细致的3D创面网格；在网格上自动计算创面周长、表面积、长宽等临床指标。以高分辨率真值扫描为基准评估精度，并与先进的面向对象的RGB-D重建方法对比，同时测试重复采集的稳定性与运行时长。

Result: 在逼真硅胶创面体模上实现亚毫米级3D重建精度；自动提取的测量在重复采集中方差低，与人工评估高度一致；总体性能优于对比的SOTA对象中心RGB-D方法，并保持接近实时的运行时间。

Conclusion: 该RGB-D+B样条管线可在临床和远程场景中提供快速、准确、稳定的自动化创面评估，具备实时部署潜力，优于现有方法。

Abstract: Chronic wound monitoring and management require accurate and efficient wound measurement methods. This paper presents a fast, non-invasive 3D wound measurement algorithm based on RGB-D imaging. The method combines RGB-D odometry with B-spline surface reconstruction to generate detailed 3D wound meshes, enabling automatic computation of clinically relevant wound measurements such as perimeter, surface area, and dimensions. We evaluated our system on realistic silicone wound phantoms and measured sub-millimetre 3D reconstruction accuracy compared with high-resolution ground-truth scans. The extracted measurements demonstrated low variability across repeated captures and strong agreement with manual assessments. The proposed pipeline also outperformed a state-of-the-art object-centric RGB-D reconstruction method while maintaining runtimes suitable for real-time clinical deployment. Our approach offers a promising tool for automated wound assessment in both clinical and remote healthcare settings.

</details>


### [12] [NC-Reg : Neural Cortical Maps for Rigid Registration](https://arxiv.org/abs/2601.19042)
*Ines Vati,Pierrick Bourgeat,Rodrigo Santa Cruz,Vincent Dore,Olivier Salvado,Clinton Fookes,Léo Lebrat*

Main category: cs.CV

TL;DR: 提出“神经皮层图”（Neural Cortical Maps, NCM）作为连续、紧凑的皮层特征表示，能从任意大小网格学习并在任意分辨率输出，优化效率高，在刚性配准任务中实现亚度量精度并大幅提速。


<details>
  <summary>Details</summary>
Motivation: 传统皮层特征多依赖离散网格/网格插值（如重心插值），在跨个体、跨分辨率应用中存在内存占用大、插值与优化低效、分辨率受限等问题。需要一种与网格无关、可连续采样、优化友好的表征来改进皮层表面配准等下游任务。

Method: 1) 提出连续神经表示NCM：以球面为域，学习皮层特征的神经网络函数，可从任意尺寸三角网格监督学习，推理时可在任意分辨率采样；2) 在球面上进行高效优化，比较与经典重心插值的迭代代价；3) 作为示例任务，提出NC-Reg：利用NCM提供的特征场，结合梯度下降与模拟退火进行刚性配准；4) 通过消融与被试对模板实验评估精度与稳健性。

Result: - 在相同迭代数下，相比重心插值，运行时间最高快至30倍；- 在刚性配准中达到“亚度量”角度误差，距离全局最优<1°；- 消融显示神经表示与退火策略对稳健收敛有关键作用。

Conclusion: 神经皮层图为皮层特征提供了与网格无关、可连续采样的紧凑表示，能显著提升球面优化与配准效率。NC-Reg验证其在刚性预对齐中的准确与稳健，具备临床前置对齐的实用潜力。

Abstract: We introduce neural cortical maps, a continuous and compact neural representation for cortical feature maps, as an alternative to traditional discrete structures such as grids and meshes. It can learn from meshes of arbitrary size and provide learnt features at any resolution. Neural cortical maps enable efficient optimization on the sphere and achieve runtimes up to 30 times faster than classic barycentric interpolation (for the same number of iterations). As a proof of concept, we investigate rigid registration of cortical surfaces and propose NC-Reg, a novel iterative algorithm that involves the use of neural cortical feature maps, gradient descent optimization and a simulated annealing strategy. Through ablation studies and subject-to-template experiments, our method demonstrates sub-degree accuracy ($<1^\circ$ from the global optimum), and serves as a promising robust pre-alignment strategy, which is critical in clinical settings.

</details>


### [13] [NuiWorld: Exploring a Scalable Framework for End-to-End Controllable World Generation](https://arxiv.org/abs/2601.19048)
*Han-Hung Lee,Cheng-Yu Yang,Yu-Lun Liu,Angel X. Chang*

Main category: cs.CV

TL;DR: NuiWorld提出一种面向大规模场景的可控、高效世界生成框架：用少量图像经生成式自举扩充数据，学习端到端模型；以可扩展场景块+向量集合压缩表示，保持大场景几何保真并降低训练/推理成本；通过“伪素描”标签实现可控生成并可泛化到未见素描。


<details>
  <summary>Details</summary>
Motivation: 现有世界/场景生成在三方面受限：1) 端到端模型缺数据导致难以训练；2) 以对象为中心的固定分辨率表示在大场景上失真、扩展性差；3) 训练免方法虽灵活但推理慢、算力昂贵。需要一种既可控又可扩展、且高效的生成方案。

Method: 提出NuiWorld：1) 生成式自举数据管线——从少量输入图像出发，结合最新3D重建与可扩展场景生成技术，合成不同规模与布局的场景，形成足量训练数据；2) 可控性——引入“伪素描”标签作为条件，训练后可对未见素描具备一定泛化；3) 表示——将场景划分为可变数量的scene chunks，并压缩为扁平的向量集合表示，大幅缩短大场景的token长度；4) 基于该表示训练端到端生成模型。

Result: 在多种场景规模下保持一致的几何保真度，同时相较固定分辨率或训练免方法显著提升训练与推理效率；在素描条件下实现可控生成，并对未见素描具备一定泛化能力；能从少量图像出发构建足量训练数据。

Conclusion: NuiWorld通过生成式自举和向量集合场景表示解决了数据稀缺、可扩展性与效率瓶颈，实现可控、可扩展的大规模世界生成，并在几何保真与计算效率间取得良好平衡。

Abstract: World generation is a fundamental capability for applications like video games, simulation, and robotics. However, existing approaches face three main obstacles: controllability, scalability, and efficiency. End-to-end scene generation models have been limited by data scarcity. While object-centric generation approaches rely on fixed resolution representations, degrading fidelity for larger scenes. Training-free approaches, while flexible, are often slow and computationally expensive at inference time. We present NuiWorld, a framework that attempts to address these challenges. To overcome data scarcity, we propose a generative bootstrapping strategy that starts from a few input images. Leveraging recent 3D reconstruction and expandable scene generation techniques, we synthesize scenes of varying sizes and layouts, producing enough data to train an end-to-end model. Furthermore, our framework enables controllability through pseudo sketch labels, and demonstrates a degree of generalization to previously unseen sketches. Our approach represents scenes as a collection of variable scene chunks, which are compressed into a flattened vector-set representation. This significantly reduces the token length for large scenes, enabling consistent geometric fidelity across scenes sizes while improving training and inference efficiency.

</details>


### [14] [Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models](https://arxiv.org/abs/2601.19060)
*Jeonghwan Kim,Renjie Tao,Sanat Sharma,Jiaqi Wang,Kai Sun,Zhaojiang Lin,Seungwhan Moon,Lambert Mathias,Anuj Kumar,Heng Ji,Xin Luna Dong*

Main category: cs.CV

TL;DR: PixSearch 是一个端到端的“可检索+可分割”的多模态大模型：在VQA中，它通过在编码阶段发出<Search>信号，自动决定何时检索、用哪种模态（文本/整图/区域），并用像素级掩码作为视觉查询，从而提升事实一致性与泛化，在CRAG-MM上相对整图检索提高约19.7%准确率，同时保持VQA与纯文本QA的推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有MM-RAG能引入外部知识但缺少内部策略：无法精细决定何时检索、检索什么内容、以及如何从图像中提取合适的区域；且依赖检测/分割/描述等外部模块导致误差累积与耦合度高。目标是将区域级感知与检索增强推理统一到单一LMM中，自动化检索时机与查询构造，提升事实对齐和泛化。

Method: 提出PixSearch：在编码阶段模型可生成<Search>标记触发检索，同时选择查询模态（文本/整图/图像区域）。通过直接生成像素级分割掩码作为视觉查询，省去独立的检测/分割/描述器。训练采用两阶段SFT并在训练过程中交错检索监督，学习检索时机和查询选择，同时保持分割能力。推理时在egocentric与实体中心VQA任务中进行检索增强回答。

Result: 在多组VQA基准上提升事实一致性与泛化；相较“整图检索”在CRAG-MM上取得约19.7%的相对准确率提升；同时在各类VQA与纯文本QA任务上保持有竞争力的推理效果。

Conclusion: 将区域级分割与检索策略学习合为一体的LMM能有效改进VQA中的知识检索与事实对齐；像素级掩码作为查询显著优于整图检索，且端到端训练减少模块化流水线的误差与开销，兼顾性能与通用性。

Abstract: Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits <search> tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.

</details>


### [15] [m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning](https://arxiv.org/abs/2601.19099)
*Yosub Shin,Michael Buriek,Igor Molybog*

Main category: cs.CV

TL;DR: 提出m2sv基准，用地图与街景对齐推断相机朝向；现有VLM在该任务仅65.2%准确，远低于人类95%，揭示跨视角空间推理薄弱。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在多模态基准上表现强，但在需要将抽象的正北朝上俯视图与第一人称街景对齐的空间推理上易崩。缺乏系统、可扩展、可控歧义的数据来评测并推动这类跨视角几何推理能力。

Method: 构建m2sv基准：给定同一路口的北向俯视地图与街景图，要求模型预测摄像机朝向。发布两个数据集：m2sv-20k（地理多样、歧义受控）与m2sv-sft-11k（含结构化推理轨迹用于监督微调）。进行SFT与RL适配，跨基准评测，并用结构信号与人工标注分析难度与失败模式。

Result: 最强评测VLM在m2sv上仅65.2%准确，人类基线为95%。SFT与RL带来稳定增益，但跨基准迁移有限。系统分析揭示任务难度来源与模型在几何对齐、证据汇聚、推理一致性上的不足。

Conclusion: 当前VLM缺乏稳健的跨视角空间推理能力。m2sv提供了可扩展评测与训练资源；尽管微调与RL有帮助，但仍存在显著差距，需面向几何对齐与一致性推理的未来研究。

Abstract: Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.

</details>


### [16] [Glance and Focus Reinforcement for Pan-cancer Screening](https://arxiv.org/abs/2601.19103)
*Linshan Wu,Jiaxin Zhuang,Hao Chen*

Main category: cs.CV

TL;DR: 提出GF-Screen：以“扫视-聚焦”强化学习框架在大规模CT中进行泛癌筛查，通过先定位可疑子体素再精细分割，显著缓解前景-背景极端不平衡与冗余关注问题，并在多数据集与挑战赛中大幅领先。


<details>
  <summary>Details</summary>
Motivation: 现有AI难以在大体积CT中定位多类型微小病灶，前景极少导致模型难聚焦病变，冗余处理正常区域降低效率且增加假阳性；临床放射科医师常先迅速扫视后再聚焦可疑区域，启发新的算法范式。

Method: 提出GF-Screen双阶段框架：Glance模型从整卷CT裁剪子体积并选择疑似含病灶的群组输入Focus模型；Focus执行精细分割，其分割质量通过强化学习回馈给Glance。为解决非可导选择操作，采用基于分割结果的奖励信号，并提出“群组相对学习”（group relative learning），在每组子体积内比较与筛选高优势预测，抑制低优势预测，以提升效率并降低假阳性。首次将前沿RL技巧有效扩展到泛癌筛查场景。

Result: 在覆盖9种病灶类型的16个内部与7个外部数据集上验证，有显著效果提升；在MICCAI FLARE25泛癌挑战公开验证榜单登顶，相比FLARE24冠军方案提升+25.6% DSC与+28.2% NSD。

Conclusion: “扫视-聚焦+RL回馈”的策略有效缓解CT泛癌筛查中的极端不平衡与误检问题，兼顾效率与准确性，并在多数据集与权威挑战中取得领先，展示了将RL用于医学成像筛查的可行性与优势。

Abstract: Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).

</details>


### [17] [Reg-TTR, Test-Time Refinement for Fast, Robust and Accurate Image Registration](https://arxiv.org/abs/2601.19114)
*Lin Chen,Yue He,Fengting Zhang,Yaonan Wang,Fengming Lin,Xiang Chen,Min Liu*

Main category: cs.CV

TL;DR: 提出Reg-TTR，一种测试时优化框架，在推理阶段微调预训练配准模型预测，仅增加约21%推理时间（0.56s），即可显著提升配准精度，达SOTA，同时接近深度学习方法的速度。


<details>
  <summary>Details</summary>
Motivation: 传统配准稳健但迭代慢；深度学习快但对域移敏感；新兴“配准基础模型”兼具一定稳健与速度，却难以达到特定数据集上精调专用模型的最高精度。需要一种在不显著牺牲速度的情况下，缩小基础模型与专用SOTA之间精度差距的方法。

Method: 提出Reg-TTR（test-time refinement）：在推理阶段对预训练（深度或基础）配准模型的输出进行小步迭代细化，将传统配准的优化思想与深度模型的初值相结合，实现轻量化的在线自适应调整，提升跨域鲁棒性与精度，计算开销小。

Result: 在两个不同任务上评测，Reg-TTR在仅增加约21%推理时间的情况下（0.56秒），显著提升注册精度并达到SOTA，同时保持接近纯深度方法的速度。

Conclusion: Reg-TTR作为通用测试时优化框架，可与各类预训练/基础配准模型结合，有效缩小与专用SOTA方法的性能差距，兼顾速度与鲁棒性；代码将在论文接收后开源。

Abstract: Traditional image registration methods are robust but slow due to their iterative nature. While deep learning has accelerated inference, it often struggles with domain shifts. Emerging registration foundation models offer a balance of speed and robustness, yet typically cannot match the peak accuracy of specialized models trained on specific datasets. To mitigate this limitation, we propose Reg-TTR, a test-time refinement framework that synergizes the complementary strengths of both deep learning and conventional registration techniques. By refining the predictions of pre-trained models at inference, our method delivers significantly improved registration accuracy at a modest computational cost, requiring only 21% additional inference time (0.56s). We evaluate Reg-TTR on two distinct tasks and show that it achieves state-of-the-art (SOTA) performance while maintaining inference speeds close to previous deep learning methods. As foundation models continue to emerge, our framework offers an efficient strategy to narrow the performance gap between registration foundation models and SOTA methods trained on specialized datasets. The source code will be publicly available following the acceptance of this work.

</details>


### [18] [FBSDiff++: Improved Frequency Band Substitution of Diffusion Features for Efficient and Highly Controllable Text-Driven Image-to-Image Translation](https://arxiv.org/abs/2601.19115)
*Xiang Gao,Yunpeng Jia*

Main category: cs.CV

TL;DR: FBSDiff/FBSDiff++提出以频域特征替换实现免训练、可控的文本驱动图像到图像翻译，并显著加速推理、支持任意分辨率与局部编辑，综合性能优于同类方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型强大，但将其扩展到文本驱动的I2I需要在保持源图像关键信息的同时实现可控编辑，且最好无需再训练、具备高效率与多样化控制。空间域方法在可控性、效率和对不同分辨率的适配上存在限制。

Method: 从频域视角，对扩散过程中的潜在特征进行动态频带替换：逐步用源图像的低/中/高频成分替换生成过程的对应频带，实现外观（低频）、布局（中频）、轮廓（高频）引导。通过调节替换带宽连续控制相关强度。FBSDiff++在此基础上：1）重构与精简推理架构，实现约8.9倍加速；2）改进频带替换模块，支持任意分辨率与纵横比输入；3）扩展功能以支持局部编辑与特定风格内容生成，核心方法仅需轻量改动。

Result: 在大量定性与定量实验中，FBSDiff++在视觉质量、效率（显著加速）、多样性与可控性方面均优于相关先进方法；可稳定实现外观/布局/轮廓引导与连续强度控制，并支持任意分辨率及局部编辑、风格化生成。

Conclusion: 频域的动态频带替换为将现成T2I扩散模型无训练地适配到I2I提供了高效、通用且可控的路径。FBSDiff++进一步在速度、适配性与功能上完善该框架，展示出优于现有方法的综合表现与实用潜力。

Abstract: With large-scale text-to-image (T2I) diffusion models achieving significant advancements in open-domain image creation, increasing attention has been focused on their natural extension to the realm of text-driven image-to-image (I2I) translation, where a source image acts as visual guidance to the generated image in addition to the textual guidance provided by the text prompt. We propose FBSDiff, a novel framework adapting off-the-shelf T2I diffusion model into the I2I paradigm from a fresh frequency-domain perspective. Through dynamic frequency band substitution of diffusion features, FBSDiff realizes versatile and highly controllable text-driven I2I in a plug-and-play manner (without need for model training, fine-tuning, or online optimization), allowing appearance-guided, layout-guided, and contour-guided I2I translation by progressively substituting low-frequency band, mid-frequency band, and high-frequency band of latent diffusion features, respectively. In addition, FBSDiff flexibly enables continuous control over I2I correlation intensity simply by tuning the bandwidth of the substituted frequency band. To further promote image translation efficiency, flexibility, and functionality, we propose FBSDiff++ which improves upon FBSDiff mainly in three aspects: (1) accelerate inference speed by a large margin (8.9$\times$ speedup in inference) with refined model architecture; (2) improve the Frequency Band Substitution module to allow for input source images of arbitrary resolution and aspect ratio; (3) extend model functionality to enable localized image manipulation and style-specific content creation with only subtle adjustments to the core method. Extensive qualitative and quantitative experiments verify superiority of FBSDiff++ in I2I translation visual quality, efficiency, versatility, and controllability compared to related advanced approaches.

</details>


### [19] [Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection](https://arxiv.org/abs/2601.19127)
*Zhilong Zhang,Lei Zhang,Qing He,Shuyin Xia,Guoyin Wang,Fuxiang Huang*

Main category: cs.CV

TL;DR: 论文提出GB-DAL，通过细粒度“域”划分与模拟非因果扰动，缓解开集/跨域目标检测中的非因果因素干扰，显著提升未知域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于域对抗学习（DAL）的域泛化方法只追求域不变表征，却忽视由数据偏差引入的隐式非因果因素；且“一个数据集一个域”的稀疏域标注使判别器只能捕捉显性因素，难以覆盖隐式偏差，导致开世界检测在新域失效。

Method: 提出GB-DAL框架：1) PGBS模块以原型为中心进行“颗粒球”式划分，把有限数据集划成更密集、细粒度的伪域，以暴露更多潜在非因果因素；2) SNF模块借鉴对抗扰动思想，生成模拟非因果因素的数据增强，以降低其隐蔽性并辅助对抗训练；整体在域判别器对齐下学习更稳健表征。

Result: 在多项基准数据集与开放世界/跨域检测场景中，GB-DAL优于现有DG与DAL方法，表现为更高的检测精度与更强的新域泛化（文中称“显著提升”）。

Conclusion: 细粒度域构造与显式模拟非因果因素能弥补传统DAL对隐式偏差的忽略，GB-DAL在开放世界目标检测中取得更好的跨域泛化，可作为处理非因果干扰的有效范式。

Abstract: Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.

</details>


### [20] [Resolving Primitive-Sharing Ambiguity in Long-Tailed Industrial Point Cloud Segmentation via Spatial Context Constraints](https://arxiv.org/abs/2601.19128)
*Chao Yin,Qing Han,Zhiwei Hou,Yue Liu,Anjin Dai,Hongda Hu,Ji Yang,Wei Yao*

Main category: cs.CV

TL;DR: 他们针对工业点云中“尾部类（如减速器、阀门）被当作管道等头部类误分”的难题，提出在CB Loss上加入两种空间上下文约束（Boundary-CB与Density-CB），在不改网络结构的情况下显著提升尾部类分割，同时不牺牲头部类性能。


<details>
  <summary>Details</summary>
Motivation: 工业数字孪生需要可靠识别安全关键部件，但训练数据极度类别不均衡（约215:1），且尾部类与头部类（如管道）在局部几何上同为圆柱体，产生“统计失衡+几何歧义”的双重危机。现有基于频次重加权的方法只能缓解样本不均衡，无法区分局部几何相似的类别。

Method: 在Class-Balanced Loss框架上，加入两个与架构无关、可即插即用的损失约束：1) Boundary-CB：基于熵的边界约束，强化预测不确定/边界区域的一致性，突出易混淆部位；2) Density-CB：基于点密度的约束，补偿扫描密度差异带来的不稳定性。无需改动网络，仅替换损失函数。

Result: 在Industrial3D数据集（6.1亿点，水处理设施）上：总体mIoU=55.74%；尾部类相对提升21.7%（29.59% vs. 24.32%）；头部类准确率保持在88.14%。对“原语共享”类显著改善：减速器IoU从0%到21.12%；阀门相对提升24.3%。

Conclusion: 通过引入空间上下文一致性的损失约束，能在极端类别失衡且几何相似的工业点云中缓解几何歧义，实现尾部类识别显著提升且不牺牲头部类，为数字孪生关键部件的自动化知识提取提供可靠性保障。

Abstract: Industrial point cloud segmentation for Digital Twin construction faces a persistent challenge: safety-critical components such as reducers and valves are systematically misclassified. These failures stem from two compounding factors: such components are rare in training data, yet they share identical local geometry with dominant structures like pipes. This work identifies a dual crisis unique to industrial 3D data extreme class imbalance 215:1 ratio compounded by geometric ambiguity where most tail classes share cylindrical primitives with head classes. Existing frequency-based re-weighting methods address statistical imbalance but cannot resolve geometric ambiguity. We propose spatial context constraints that leverage neighborhood prediction consistency to disambiguate locally similar structures. Our approach extends the Class-Balanced (CB) Loss framework with two architecture-agnostic mechanisms: (1) Boundary-CB, an entropy-based constraint that emphasizes ambiguous boundaries, and (2) Density-CB, a density-based constraint that compensates for scan-dependent variations. Both integrate as plug-and-play modules without network modifications, requiring only loss function replacement. On the Industrial3D dataset (610M points from water treatment facilities), our method achieves 55.74% mIoU with 21.7% relative improvement on tail-class performance (29.59% vs. 24.32% baseline) while preserving head-class accuracy (88.14%). Components with primitive-sharing ambiguity show dramatic gains: reducer improves from 0% to 21.12% IoU; valve improves by 24.3% relative. This resolves geometric ambiguity without the typical head-tail trade-off, enabling reliable identification of safety-critical components for automated knowledge extraction in Digital Twin applications.

</details>


### [21] [CLIP-Guided Unsupervised Semantic-Aware Exposure Correction](https://arxiv.org/abs/2601.19129)
*Puzhen Wu,Han Weng,Quan Zheng,Yi Zhan,Hewei Wang,Yiming Li,Jiahui Han,Rui Xu*

Main category: cs.CV

TL;DR: 提出一种无监督的语义感知曝光校正网络，通过融合FastSAM语义信息与多尺度残差Spatial Mamba组恢复细节与曝光，并用CLIP引导的伪真值与语义-提示一致性损失实现无监督训练，数值与视觉上均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有曝光校正易出现颜色偏移与细节丢失，关键原因在于缺乏对象级语义信息以及真实场景无标注、人工标注代价高。

Method: 1) 语义自适应融合模块：将预训练FastSAM提取的语义嵌入自适应地融合到共享图像特征空间；2) 多尺度残差Spatial Mamba组：利用序列建模能力在多尺度上恢复细节、调整曝光；3) CLIP引导伪真值生成器：微调CLIP以自动识别曝光类型并生成校正指导作为伪标签；4) 语义提示一致性损失：结合FastSAM与CLIP先验，约束语义一致与图像-文本对齐，实现无监督训练。

Result: 在真实曝光图像上进行大量实验，方法在客观指标与主观视觉质量上均超过现有无监督SOTA。

Conclusion: 引入对象级语义与跨模态先验可在无监督框架下显著提升曝光校正的准确性与稳定性，减少人工标注需求，并在实际场景中取得领先表现。

Abstract: Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually.

</details>


### [22] [QA-ReID: Quality-Aware Query-Adaptive Convolution Leveraging Fused Global and Structural Cues for Clothes-Changing ReID](https://arxiv.org/abs/2601.19133)
*Yuxiang Wang,Kunming Jiang,Tianxiang Zhang,Ke Tian,Gaozhe Jiang*

Main category: cs.CV

TL;DR: 提出QA-ReID，通过双分支与质量感知匹配，应对换衣行人重识别的大外观变化，融合RGB与人体解析结构特征，在多个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 换衣场景下行人外观变化大，传统仅依赖外观特征的ReID易失效；需要引入对衣物变化不敏感的结构线索，并在匹配时抑制噪声、提升鲁棒性。

Method: 1) 质量感知双分支：RGB外观分支+基于人体解析的结构分支，建模全局外观与衣物无关结构；2) 多模态注意力自适应融合异构特征；3) 匹配阶段提出QAConv-QA：在查询自适应卷积中加入像素级重要性加权与双向一致性约束，提升对衣物变化的鲁棒性。

Result: 在PRCC、LTCC、VC-Clothes等基准上取得SOTA；在跨衣服场景显著优于现有方法。

Conclusion: 融合外观与结构并在匹配中引入质量感知与一致性约束，可有效缓解换衣带来的外观差异，显著提升CC-ReID性能。

Abstract: Unlike conventional person re-identification (ReID), clothes-changing ReID (CC-ReID) presents severe challenges due to substantial appearance variations introduced by clothing changes. In this work, we propose the Quality-Aware Dual-Branch Matching (QA-ReID), which jointly leverages RGB-based features and parsing-based representations to model both global appearance and clothing-invariant structural cues. These heterogeneous features are adaptively fused through a multi-modal attention module. At the matching stage, we further design the Quality-Aware Query Adaptive Convolution (QAConv-QA), which incorporates pixel-level importance weighting and bidirectional consistency constraints to enhance robustness against clothing variations. Extensive experiments demonstrate that QA-ReID achieves state-of-the-art performance on multiple benchmarks, including PRCC, LTCC, and VC-Clothes, and significantly outperforms existing approaches under cross-clothing scenarios.

</details>


### [23] [TFFM: Topology-Aware Feature Fusion Module via Latent Graph Reasoning for Retinal Vessel Segmentation](https://arxiv.org/abs/2601.19136)
*Iftekhar Ahmed,Shakib Absar,Aftar Ahmad Sami,Shadman Sakib,Debojyoti Biswas,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 提出一种拓扑感知的视网膜动静脉分割框架，通过图注意力与拓扑损失保持血管连通性，在Fundus-AVSeg上达SOTA（Dice 90.97%、95HD 3.50px），将碎裂减少约38%，生成适于临床图分析的连通血管树。


<details>
  <summary>Details</summary>
Motivation: 传统卷积网络虽有高像素级精度，但常出现血管断裂与不连通，破坏基于图的临床分析与生物标志量化。需要一个能捕获全局结构依赖并显式约束拓扑连通性的方案。

Method: 提出拓扑感知框架：在编码器语义特征上引入Topological Feature Fusion Module（TFFM），将局部特征映射到潜在图空间，并用Graph Attention Network（GAT）建模全局结构依赖；训练时采用混合损失，Tversky loss处理类别不平衡，soft clDice显式惩罚拓扑断裂。

Result: 在Fundus-AVSeg数据集上取得SOTA：Combined Dice 90.97%，95% Hausdorff Distance 3.50像素；相较基线血管碎裂率减少约38%，生成更连通的血管树。

Conclusion: 融合图注意的拓扑特征与拓扑约束损失能显著提升血管分割的连通性与临床可用性；方法开源，适合自动生物标志量化与下游图分析。

Abstract: Precise segmentation of retinal arteries and veins carries the diagnosis of systemic cardiovascular conditions. However, standard convolutional architectures often yield topologically disjointed segmentations, characterized by gaps and discontinuities that render reliable graph-based clinical analysis impossible despite high pixel-level accuracy. To address this, we introduce a topology-aware framework engineered to maintain vascular connectivity. Our architecture fuses a Topological Feature Fusion Module (TFFM) that maps local feature representations into a latent graph space, deploying Graph Attention Networks to capture global structural dependencies often missed by fixed receptive fields. Furthermore, we drive the learning process with a hybrid objective function, coupling Tversky loss for class imbalance with soft clDice loss to explicitly penalize topological disconnects. Evaluation on the Fundus-AVSeg dataset reveals state-of-the-art performance, achieving a combined Dice score of 90.97% and a 95% Hausdorff Distance of 3.50 pixels. Notably, our method decreases vessel fragmentation by approximately 38% relative to baselines, yielding topologically coherent vascular trees viable for automated biomarker quantification. We open-source our code at https://tffm-module.github.io/.

</details>


### [24] [GTFMN: Guided Texture and Feature Modulation Network for Low-Light Image Enhancement and Super-Resolution](https://arxiv.org/abs/2601.19157)
*Yongsong Huang,Tzu-Hsuan Peng,Tomo Miyazaki,Xiaofeng Liu,Chun-Ting Chou,Ai-Chun Pang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出GTFMN，把低光超分任务分解为“光照估计+纹理恢复”，用预测的空间变换光照图指导特征调制，实现暗区强化、亮区保细节，SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 低光图像同时存在分辨率低与照明不足的耦合退化，传统SR或增强方法难以兼顾，需一种能按空间光照自适应恢复纹理的框架。

Method: 构建双流网络：Illumination Stream预测空间可变的光照图；Texture Stream进行纹理重建。提出Illumination Guided Modulation（IGM）模块，将光照图作为显式引导，对纹理流特征进行动态、空间自适应调制，在暗区加大增强、在良好曝光区抑制过度处理。

Result: 在OmniNormal5与OmniNormal15数据集上，GTFMN在定量指标与视觉质量上均优于对比方法，达最佳表现。

Conclusion: 通过显式光照估计并用于特征调制，可有效解耦低光与超分问题，实现空间自适应恢复；GTFMN验证了该思路的有效性并取得SOTA表现。

Abstract: Low-light image super-resolution (LLSR) is a challenging task due to the coupled degradation of low resolution and poor illumination. To address this, we propose the Guided Texture and Feature Modulation Network (GTFMN), a novel framework that decouples the LLSR task into two sub-problems: illumination estimation and texture restoration. First, our network employs a dedicated Illumination Stream whose purpose is to predict a spatially varying illumination map that accurately captures lighting distribution. Further, this map is utilized as an explicit guide within our novel Illumination Guided Modulation Block (IGM Block) to dynamically modulate features in the Texture Stream. This mechanism achieves spatially adaptive restoration, enabling the network to intensify enhancement in poorly lit regions while preserving details in well-exposed areas. Extensive experiments demonstrate that GTFMN achieves the best performance among competing methods on the OmniNormal5 and OmniNormal15 datasets, outperforming them in both quantitative metrics and visual quality.

</details>


### [25] [SNR-Edit: Structure-Aware Noise Rectification for Inversion-Free Flow-Based Editing](https://arxiv.org/abs/2601.19180)
*Lifan Jiang,Boxi Wu,Yuhang Pei,Tianrun Wu,Yongyuan Chen,Yan Zhao,Shiyu Yu,Deng Cai*

Main category: cs.CV

TL;DR: SNR-Edit是一种无需反演、无需训练的图像编辑框架，通过自适应噪声控制校正潜空间轨迹，减少由固定高斯噪声引起的轨迹漂移，从而在保持结构的同时提升编辑质量，并且仅带来约1秒的额外开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于流模型的无反演编辑通常用固定高斯噪声启动源轨迹，这会造成轨迹动力学偏差，带来结构破坏或画质下降；希望在不做反演、不微调模型的前提下，稳住潜在轨迹并忠实保留结构。

Method: 提出SNR-Edit：通过结构感知的噪声矫正，将分割约束注入初始噪声，使随机分量锚定在真实图像的隐式反演位置；以自适应信噪比（SNR）控制进行潜在轨迹校正，实现更平滑的源—目标迁移轨迹。整个流程训练自由、对底模无修改，适用于SD3与FLUX。

Result: 在PIE-Bench与SNR-Bench上，SNR-Edit在像素级指标与基于VLM的评分均取得更优或有竞争力的表现；在单图像上仅增加约1秒的推理时间开销，同时显著减少结构漂移与质量损失。

Conclusion: 通过对初始噪声进行结构感知与SNR自适应控制，SNR-Edit可在不反演、不微调的设置下有效校正潜在轨迹，实现高保真结构保持与高质量编辑，且具有轻量级、通用性的优点。

Abstract: Inversion-free image editing using flow-based generative models challenges the prevailing inversion-based pipelines. However, existing approaches rely on fixed Gaussian noise to construct the source trajectory, leading to biased trajectory dynamics and causing structural degradation or quality loss. To address this, we introduce SNR-Edit, a training-free framework achieving faithful Latent Trajectory Correction via adaptive noise control. Mechanistically, SNR-Edit uses structure-aware noise rectification to inject segmentation constraints into the initial noise, anchoring the stochastic component of the source trajectory to the real image's implicit inversion position and reducing trajectory drift during source--target transport. This lightweight modification yields smoother latent trajectories and ensures high-fidelity structural preservation without requiring model tuning or inversion. Across SD3 and FLUX, evaluations on PIE-Bench and SNR-Bench show that SNR-Edit delivers performance on pixel-level metrics and VLM-based scoring, while adding only about 1s overhead per image.

</details>


### [26] [Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP](https://arxiv.org/abs/2601.19210)
*Sen Nie,Jie Zhang,Zhuo Wang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出一种高效测试时防御方法CSR，通过频域对比学习自适应纠正对抗扰动，使VLM在AutoAttack下平均提升18.1%鲁棒性且开销小。


<details>
  <summary>Details</summary>
Motivation: VLM具备零样本泛化但对对抗样本脆弱；现有测试时防御对强攻击鲁棒性不足、推理延迟高且任务特定。作者观察到对抗样本在逐步频率衰减下呈现显著特征不一致，并将其归因于模型的谱偏置，动机是利用这一性质进行高效、通用的鲁棒化。

Method: 提出Contrastive Spectral Rectification (CSR)：在推理时为每个输入优化一个小的“校正扰动”，通过谱引导的对比目标，使扰动后的输入与其在频率衰减轨迹上的“自然”表征对齐，从而回到自然流形。核心包括：1) 频域渐进衰减生成参考视图；2) 光谱引导的对比损失约束一致性；3) 轻量自适应迭代以学习校正扰动；适用于CLIP等VLM并可迁移到多任务。

Result: 在16个分类基准上，对强AutoAttack较SOTA平均提升18.1%鲁棒准确率，且推理开销适中；方法对多种视觉任务均表现良好。

Conclusion: 对抗样本在频域衰减下的特征不一致源于VLM的谱偏置。利用谱引导的对比一致性进行输入自适应校正，可在小开销下显著提升VLM鲁棒性，并具备广泛任务适用性。

Abstract: Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model's inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.

</details>


### [27] [UniPCB: A Unified Vision-Language Benchmark for Open-Ended PCB Quality Inspection](https://arxiv.org/abs/2601.19222)
*Fuxiang Sun,Xi Jiang,Jiansheng Wu,Haigang Zhang,Feng Zheng,Jinfeng Yang*

Main category: cs.CV

TL;DR: 提出UniPCB统一视语基准与PCB-GPT模型，系统整合多源PCB数据并以渐进式课程学习训练，显著提升细粒度缺陷定位等任务表现，刷新领域基线并开放数据与模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在通用质检上有潜力，但在PCB等高复杂场景中表现不佳；缺少高质量、统一的视语基准导致无法客观评估与推动进展，原因包括数据稀缺、来源分散与标准不一致。

Method: 构建UniPCB：从多异构来源经统一清洗、标注与标准化，覆盖三个已注释场景，形成开放式问答式评测基准；同时通过该管线生成新的指令数据，并以模仿人类专家学习过程的“渐进式课程”训练专用MLLM——PCB-GPT。

Result: 在UniPCB评测上，通用MLLM在领域特定任务上失利；PCB-GPT成为新基线，尤其在细粒度缺陷定位上较最强对手性能提升超过一倍，并在定位与分析能力上具显著优势。

Conclusion: UniPCB与PCB-GPT填补PCB质检视语评测与模型空白，建立强基线并将数据、基准与模型开源，期望推动后续研究。

Abstract: Multimodal Large Language Models (MLLMs) show promise for general industrial quality inspection, but fall short in complex scenarios, such as Printed Circuit Board (PCB) inspection. PCB inspection poses unique challenges due to densely packed components, complex wiring structures, and subtle defect patterns that require specialized domain expertise. However, a high-quality, unified vision-language benchmark for quantitatively evaluating MLLMs across PCB inspection tasks remains absent, stemming not only from limited data availability but also from fragmented datasets and inconsistent standardization. To fill this gap, we propose UniPCB, the first unified vision-language benchmark for open-ended PCB quality inspection. UniPCB is built via a systematic pipeline that curates and standardizes data from disparate sources across three annotated scenarios. Furthermore, we introduce PCB-GPT, an MLLM trained on a new instruction dataset generated by this pipeline, utilizing a novel progressive curriculum that mimics the learning process of human experts. Evaluations on the UniPCB benchmark show that while existing MLLMs falter on domain-specific tasks, PCB-GPT establishes a new baseline. Notably, it more than doubles the performance on fine-grained defect localization compared to the strongest competitors, with significant advantages in localization and analysis. We will release the instruction data, benchmark, and model to facilitate future research.

</details>


### [28] [Towards Pixel-Level VLM Perception via Simple Points Prediction](https://arxiv.org/abs/2601.19228)
*Tianhui Song,Haoyu Lu,Hao Yang,Lin Sui,Haoning Wu,Zaida Zhou,Zhiqi Huang,Yiping Bao,Y. Charles,Xinyu Zhou,Limin Wang*

Main category: cs.CV

TL;DR: SimpleSeg将分割任务转化为点序列（文本坐标）生成，无需专用架构；通过监督微调后再用基于IoU奖励的强化学习精修点序列，在多基准上达到或超越复杂特定方案。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型难以原生进行像素级分割，通常依赖额外头部/掩码解码器等复杂组件；作者想验证通用MLLM是否可在不改结构的前提下，通过纯语言空间输出实现精细空间理解。

Method: 把语义/实例分割重述为序列生成：直接生成描边点的文本坐标；提出两阶段训练管线：1) 监督微调(SF)学习基本点序列生成；2) 强化学习(RL)以IoU为奖励，优化点序列使其贴合真值轮廓；不引入专门视觉分割架构。

Result: 在多项分割基准上表现与复杂、任务特定方法相当或更优，证明标准MLLM具备强的低层感知潜力；点序列经RL优化后与真实轮廓高度匹配。

Conclusion: 精确空间理解可从简单的点预测中涌现，无需辅助组件；提出统一而简洁的路径为更通用且能力更强的VLM铺路。

Abstract: We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/

</details>


### [29] [VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics](https://arxiv.org/abs/2601.19236)
*Zhiyu Yin,Zhipeng Liu,Kehai Chen,Lemao Liu,Jin Liu,Hong-Dong Li,Yang Xiang,Min Zhang*

Main category: cs.CV

TL;DR: 提出“视频连接”任务：在给定起始与结束视频片段之间生成平滑过渡内容；并发布首个针对该任务的评测基准VC-Bench（含数据集与三类指标），揭示现有视频生成模型在起终一致性与过渡平滑性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现实应用（视频编辑、Vlog等）需要把分离的片段自然衔接，但现有视频生成多依赖文本/图像条件，且缺乏标准化评测，限制了该方向的发展与可比性。

Method: 1) 定义“视频连接”任务；2) 构建VC-Bench基准：收集1,579个高质量视频，涵盖15大类与72子类；3) 设计三维度评价体系：VQS（视频质量）、SECS（起终一致性）、TSS（转场平滑度）；4) 用多款SOTA视频生成模型在VC-Bench上评测。

Result: 在VC-Bench上，现有模型总体质量尚可，但在起终一致性与转场平滑方面表现不佳，导致整体连贯性与流畅度偏低。

Conclusion: VC-Bench为视频连接任务提供了首个系统化数据与指标框架，可作为社区共用基准，推动提升模型在连续性与过渡生成方面的研究；代码与数据已公开。

Abstract: While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.

</details>


### [30] [TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment](https://arxiv.org/abs/2601.19247)
*Jiarun Liu,Qifeng Chen,Yiru Zhao,Minghua Liu,Baorui Ma,Sheng Yang*

Main category: cs.CV

TL;DR: 提出TIGaussian：利用3D Gaussian Splatting与多分支tokenizer、跨模态对齐策略，提升图文-3D任务的对齐与泛化，在多数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型已能对齐图像与文本，但将3D模态（点云、3D高斯）纳入预训练可支持3D检索、零样本分类、场景识别等。然而3D特征提取困难且跨模态鸿沟大，需要新的表示与对齐方法。

Method: 1) 多分支3DGS tokenizer：将3DGS的固有结构属性解耦为紧凑潜在表示，提升泛化的特征抽取。2) 双向跨模态对齐：a) 图像-3D：基于扩散先验的多视角特征融合，缓解视角歧义；b) 文本-3D：自适应投影模块将3D特征映射到文本嵌入空间，增强文本-3D对齐。

Result: 在多种数据集与任务（跨模态检索、零样本分类、场景识别等）上取得最新SOTA表现。

Conclusion: 通过3DGS特性驱动的表示学习与双向跨模态对齐，TIGaussian有效缩小图像/文本与3D之间的模态差距，提供强泛化的3D预训练框架。

Abstract: While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.

</details>


### [31] [Handcrafted Feature Fusion for Reliable Detection of AI-Generated Images](https://arxiv.org/abs/2601.19262)
*Syed Mehedi Hasan Nirob,Moqsadur Rahman,Shamim Ehsan,Summit Haque*

Main category: cs.CV

TL;DR: 论文系统评测多种手工特征与多类传统/集成分类器在CIFAKE真伪图像检测上的表现；混合手工特征+LightGBM最优（PR-AUC≈0.988，ROC-AUC≈0.988，F1≈0.945，Brier≈0.041），证明手工特征结合集成学习在可解释、轻量场景依然有效。


<details>
  <summary>Details</summary>
Motivation: 生成模型产出逼真合成图像引发信任与鉴伪难题。深度方法虽主导，但手工特征具可解释、计算高效、可迁移优势；缺少系统性基准来比较其在现代数据集上的效用。

Method: 在CIFAKE数据集上，构建三种特征配置：基线（如单一或简单手工特征）、高级（更复杂统计/频域/纹理特征）、混合（多种手工特征融合：像素、颜色直方图、DCT、HOG、LBP、GLCM、小波）。使用5万训练、1万测试，评测7类分类器（Logistic到LightGBM/XGBoost/CatBoost），以PR-AUC、ROC-AUC、F1、Brier等度量，并比较校准与判别能力。

Result: LightGBM在混合特征下取得最优：PR-AUC 0.9879、ROC-AUC 0.9878、F1 0.9447、Brier 0.0414；随特征由基线→高级→混合，性能单调提升；与简单描述子相比，校准与判别显著改进。

Conclusion: 精心工程的多样手工特征结合梯度提升等集成模型可在合成图像检测中达到高准确与良好校准，适用于需可解释与资源受限的场景；手工特征仍具现实价值。

Abstract: The rapid progress of generative models has enabled the creation of highly realistic synthetic images, raising concerns about authenticity and trust in digital media. Detecting such fake content reliably is an urgent challenge. While deep learning approaches dominate current literature, handcrafted features remain attractive for their interpretability, efficiency, and generalizability. In this paper, we conduct a systematic evaluation of handcrafted descriptors, including raw pixels, color histograms, Discrete Cosine Transform (DCT), Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gray-Level Co-occurrence Matrix (GLCM), and wavelet features, on the CIFAKE dataset of real versus synthetic images. Using 50,000 training and 10,000 test samples, we benchmark seven classifiers ranging from Logistic Regression to advanced gradient-boosted ensembles (LightGBM, XGBoost, CatBoost). Results demonstrate that LightGBM consistently outperforms alternatives, achieving PR-AUC 0.9879, ROC-AUC 0.9878, F1 0.9447, and a Brier score of 0.0414 with mixed features, representing strong gains in calibration and discrimination over simpler descriptors. Across three configurations (baseline, advanced, mixed), performance improves monotonically, confirming that combining diverse handcrafted features yields substantial benefit. These findings highlight the continued relevance of carefully engineered features and ensemble learning for detecting synthetic images, particularly in contexts where interpretability and computational efficiency are critical.

</details>


### [32] [A Multi-View Consistency Framework with Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2601.19266)
*Yuting Hong,Li Dong,Xiaojie Qiu,Hui Xiao,Baochen Yao,Siming Zheng,Chengbin Peng*

Main category: cs.CV

TL;DR: 提出一种用于半监督领域自适应（SSDA）的多视角一致性框架，通过强增强的两种视角训练、类偏差校正、伪负标签以及跨域亲和力学习，缓解目标域少标注引起的类间相似与偏置问题，并在DomainNet与Office-Home上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SSDA在目标域标注稀缺时，类在特征空间的内在相似会导致模型在平衡数据上仍出现偏置预测，影响泛化与对齐效果；需要一种能利用少量标注和大量未标注数据、同时降低类偏置并加强跨域同类对齐的方法。

Method: 1) 多视角一致性：对强增强样本构建两种训练视角。2) 去偏策略：依据当前模型对各类的预测性能，调整类级预测概率（校准/重标定），缓解偏置。3) 伪负标签：从模型预测中挖掘“非该类”的置信信息，作为约束以提高判别性。4) 跨域亲和力学习：学习/构建跨域同类样本间的亲和关系，使源/目标同类特征对齐。整体在SSDA设定下联合优化。

Result: 在DomainNet与Office-Home两个标准数据集上取得比现有方法更好的性能，验证了稳健的跨域对齐与偏置缓解能力。

Conclusion: 结合多视角一致性、类去偏与伪负标签，以及跨域亲和力对齐，可在目标域少标注条件下显著提升SSDA性能；该方向对工业界具有价值，因其能提升模型适应性、降低标注成本并提高整体效果。

Abstract: Semi-Supervised Domain Adaptation (SSDA) leverages knowledge from a fully labeled source domain to classify data in a partially labeled target domain. Due to the limited number of labeled samples in the target domain, there can be intrinsic similarity of classes in the feature space, which may result in biased predictions, even when the model is trained on a balanced dataset. To overcome this limitation, we introduce a multi-view consistency framework, which includes two views for training strongly augmented data. One is a debiasing strategy for correcting class-wise prediction probabilities according to the prediction performance of the model. The other involves leveraging pseudo-negative labels derived from the model predictions. Furthermore, we introduce a cross-domain affinity learning aimed at aligning features of the same class across different domains, thereby enhancing overall performance. Experimental results demonstrate that our method outperforms the competing methods on two standard domain adaptation datasets, DomainNet and Office-Home. Combining unsupervised domain adaptation and semi-supervised learning offers indispensable contributions to the industrial sector by enhancing model adaptability, reducing annotation costs, and improving performance.

</details>


### [33] [ProMist-5K: A Comprehensive Dataset for Digital Emulation of Cinematic Pro-Mist Filter Effects](https://arxiv.org/abs/2601.19295)
*Yingtie Lei,Zimeng Li,Chi-Man Pun,Wangyu Wu,Junke Yang,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出ProMist-5K：面向电影Pro-Mist滤镜风格复现的5K级成对数据集，采用物理启发的线性空间管线，覆盖两种滤镜密度与两种焦距，共2万高分图像对，强调真实的高光晕染与扩散，可稳健支持多种图像翻译训练并再现微妙到强烈的电影化效果。


<details>
  <summary>Details</summary>
Motivation: 实拍Pro-Mist滤镜产生的柔化、降对比与高光晕染具有复杂的光扩散特性，现有数字方法难以逼真复现；通用风格数据集缺少对真实、可控的光学扩散与晕光的聚焦支持，因此需要一个物理扎实且一致可控的目标域数据集，填补数字灵活性与传统镜头美学之间的鸿沟。

Method: 构建基于物理启发的场景参照（scene-referred）线性空间生成管线：使用多层模糊与精心加权来模拟强度与扩散范围随亮度变化的光学扩散；采集并合成20,000对高分辨率图像，覆盖两档Pro-Mist密度（1/2、1/8）与两种焦距（20mm、50mm），形成四种配置；确保目标域一致、可控，以适配多种图像翻译与学习范式。

Result: 在多种训练设定与模型上验证，数据集能稳定驱动从细微到强烈的电影化外观学习，良好复现真实的高光扩散与晕光效果，表现出跨设定的鲁棒性与可迁移性。

Conclusion: ProMist-5K提供了一个实用且物理扎实的电影风格图像变换资源，能作为一致可控的目标域，帮助模型逼真模拟Pro-Mist滤镜带来的光学晕染与对比变化，缩小数字后期与传统镜头美学之间的差距；数据集已公开发布。

Abstract: Pro-Mist filters are widely used in cinematography for their ability to create soft halation, lower contrast, and produce a distinctive, atmospheric style. These effects are difficult to reproduce digitally due to the complex behavior of light diffusion. We present ProMist-5K, a dataset designed to support cinematic style emulation. It is built using a physically inspired pipeline in a scene-referred linear space and includes 20,000 high-resolution image pairs across four configurations, covering two filter densities (1/2 and 1/8) and two focal lengths (20mm and 50mm). Unlike general style datasets, ProMist-5K focuses on realistic glow and highlight diffusion effects. Multiple blur layers and carefully tuned weighting are used to model the varying intensity and spread of optical diffusion. The dataset provides a consistent and controllable target domain that supports various image translation models and learning paradigms. Experiments show that the dataset works well across different training settings and helps capture both subtle and strong cinematic appearances. ProMist-5K offers a practical and physically grounded resource for film-inspired image transformation, bridging the gap between digital flexibility and traditional lens aesthetics. The dataset is available at https://www.kaggle.com/datasets/yingtielei/promist5k.

</details>


### [34] [Beyond Shadows: A Large-Scale Benchmark and Multi-Stage Framework for High-Fidelity Facial Shadow Removal](https://arxiv.org/abs/2601.19309)
*Tailong Luo,Jiesong Bai,Jinyang Huang,Junyu Xia,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出ASFW真实配对人脸阴影数据集与FSE方法，显著提升真实场景人脸去阴影效果并树立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有去阴影方法在复杂光照下难以兼顾去除阴影与纹理保真，且缺乏真实世界成对（有阴影/无阴影）数据用于训练与评测，导致模型在现实场景泛化不足。

Method: 1) 构建ASFW：以专业Photoshop流程从真实人脸图像生成1,081对阴影/无阴影配对，提供逼真的阴影多样性与精确的无阴影真值；2) 提出Face Shadow Eraser（FSE）作为基线/示范模型，在ASFW上训练以验证数据集有效性；3) 以多种深度模型在ASFW上训练与评测，与现有数据/方法对比。

Result: 在ASFW上训练的深度模型在真实场景人脸去阴影任务中取得更好的去阴影与纹理保真表现；FSE展示了数据集的有效性；实验结果刷新了该任务的性能水平。

Conclusion: ASFW填补了真实配对人脸去阴影数据的空缺，缩小合成与真实域差距；结合FSE证明其对模型训练与泛化的显著助益，为人脸去阴影建立了新的数据与方法基准。

Abstract: Facial shadows often degrade image quality and the performance of vision algorithms. Existing methods struggle to remove shadows while preserving texture, especially under complex lighting conditions, and they lack real-world paired datasets for training. We present the Augmented Shadow Face in the Wild (ASFW) dataset, the first large-scale real-world dataset for facial shadow removal, containing 1,081 paired shadow and shadow-free images created via a professional Photoshop workflow. ASFW offers photorealistic shadow variations and accurate ground truths, bridging the gap between synthetic and real domains. Deep models trained on ASFW demonstrate improved shadow removal in real-world conditions. We also introduce the Face Shadow Eraser (FSE) method to showcase the effectiveness of the dataset. Experiments demonstrate that ASFW enhances the performance of facial shadow removal models, setting new standards for this task.

</details>


### [35] [Instance-Guided Radar Depth Estimation for 3D Object Detection](https://arxiv.org/abs/2601.19314)
*Chen-Chou Lo,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 提出InstaRadar与将预训练RCDPT整合进BEVDepth，利用实例分割引导的雷达扩密与显式深度监督，显著提升单目3D检测与深度估计，但仍弱于直接BEV特征级融合；未来将扩展为点云式表示并加入雷达时序分支。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测受深度歧义和复杂场景鲁棒性不足影响；雷达在弱光/恶劣天气下鲁棒但稀疏、分辨率低，难以直接用于检测。需要更有效的雷达-相机融合与深度估计/预处理策略以提升3D感知。

Method: 1) InstaRadar：使用预训练实例分割掩膜对雷达回波进行实例级引导扩密与语义对齐，形成结构化、语义一致的雷达引导深度特征；2) 将预训练的RCDPT深度预测模块替换BEVDepth原深度分支，并以InstaRadar增强输入提供显式深度监督，实现端到端的单目3D检测提升。

Result: InstaRadar在雷达引导的深度估计上达成SOTA；将RCDPT集成到BEVDepth并结合InstaRadar后，3D检测指标在各项上持续超过BEVDepth基线。

Conclusion: 实例分割引导的雷达扩密与显式深度监督有效提升单目3D检测，但因雷达仅作引导而非独立BEV分支，整体仍落后于直接BEV特征级的雷达-相机融合。未来将把InstaRadar扩展为点云式表示，并加入带时序信息的专用雷达分支以增强BEV融合。

Abstract: Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.

</details>


### [36] [Innovator-VL: A Multimodal Large Language Model for Scientific Discovery](https://arxiv.org/abs/2601.19325)
*Zichen Wen,Boxue Yang,Shuang Chen,Yaojie Zhang,Yuhang Han,Junlong Ke,Cong Wang,Yicheng Fu,Jiawang Zhao,Jiangchao Yao,Xi Fang,Zhen Wang,Henxing Cai,Lin Yao,Zhifeng Gao,Yanhui Hong,Nang Yuan,Yixuan Li,Guojiang Zhao,Haoyi Tao,Nan Wang,Han Lyu,Guolin Ke,Ning Liao,Xiaoxing Wang,Kai Chen,Zhiyu Li,Feiyu Xiong,Sihan Hu,Kun Chen,Yanfeng Wang,Weinan E,Linfeng Zhang,Linfeng Zhang*

Main category: cs.CV

TL;DR: Innovator-VL 是一个面向科学场景的多模态大模型，强调透明可复现的训练流程、数据高效性与广泛泛化能力，在不依赖大规模领域预训练的情况下，用不到500万高质量样本在科学与通用视觉任务上取得有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 当前科学多模态模型多依赖海量领域预训练与不透明管线，难以复现与扩展；作者希望证明通过“原则化的数据与训练设计”可在更少数据下实现强科学推理，并兼顾通用多模态能力，同时为社区提供可扩展、可重复的实践范式。

Method: 1) 提供端到端透明训练管线：数据收集/清洗/预处理、SFT、RL与评测，并给出优化细节；2) 精选少于500万条高质量、任务对齐的数据，避免大规模无差别预训练；3) 在统一模型中进行科学对齐与通用任务训练，确保跨领域泛化与推理能力。

Result: 用小于500万条精心策划的数据，在科学任务、通用视觉与多模态推理基准上取得与现有方法有竞争力的成绩，展示出显著的数据效率与强泛化。

Conclusion: 原则化的数据选择与透明训练策略可以在无需大规模数据的前提下构建高性能、可复现的科学多模态模型，并不牺牲通用视觉与推理能力，为后续研究提供实用基础。

Abstract: We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.

</details>


### [37] [Pareto-Guided Optimization for Uncertainty-Aware Medical Image Segmentation](https://arxiv.org/abs/2601.19365)
*Jinming Zhang,Xi Yang,Youpeng Yang,Haosen Shi,Yuyao Yan,Qiufeng Wang,Guangliang Cheng,Kaizhu Huang*

Main category: cs.CV

TL;DR: 论文提出一种针对医学图像分割中区域不确定性（边界高、内部低）的课程式训练与损失设计，先学确定区再逐步纳入不确定边界，并配合Pareto一致的自适应损失与模糊标注，稳定优化、降低梯度方差，在脑转移瘤与非转移瘤数据上全面优于传统硬标签方法。


<details>
  <summary>Details</summary>
Motivation: 分割任务中边界像素比内部像素更不确定，常规“像素等权”训练在早期预测不可靠时会导致梯度噪声大、优化不稳，难以达到在不同区域权衡良好的（近）帕累托最优解。需要一种能按区域不确定性循序渐进学习并在损失层面协调不同区域权衡的策略。

Method: 1) 区域课程学习：按不确定性从低到高（先内部、后边界）逐步纳入训练，降低早期梯度方差。2) Pareto一致损失：自适应重塑损失地形，约束内部与边界区域的收敛动态，使两者的优化权衡满足帕累托一致性，引导至Pareto近似解。3) 模糊标注：非边界区保持二值高置信；边界区采用平滑过渡的模糊标签，缓解边界歧义、稳定梯度并扩大损失表面的平坦区域。

Result: 在脑转移瘤与非转移瘤分割任务、多个配置与所有肿瘤子区域上，所提方法均较传统硬标签/等权训练取得一致性提升。

Conclusion: 区域课程学习+Pareto一致损失+边界模糊标注可有效处理分割中的非均匀不确定性，稳定训练、降低梯度方差并更好权衡内部与边界性能，从而在多数据集上取得全面领先。

Abstract: Uncertainty in medical image segmentation is inherently non-uniform, with boundary regions exhibiting substantially higher ambiguity than interior areas. Conventional training treats all pixels equally, leading to unstable optimization during early epochs when predictions are unreliable. We argue that this instability hinders convergence toward Pareto-optimal solutions and propose a region-wise curriculum strategy that prioritizes learning from certain regions and gradually incorporates uncertain ones, reducing gradient variance. Methodologically, we introduce a Pareto-consistent loss that balances trade-offs between regional uncertainties by adaptively reshaping the loss landscape and constraining convergence dynamics between interior and boundary regions; this guides the model toward Pareto-approximate solutions. To address boundary ambiguity, we further develop a fuzzy labeling mechanism that maintains binary confidence in non-boundary areas while enabling smooth transitions near boundaries, stabilizing gradients, and expanding flat regions in the loss surface. Experiments on brain metastasis and non-metastatic tumor segmentation show consistent improvements across multiple configurations, with our method outperforming traditional crisp-set approaches in all tumor subregions.

</details>


### [38] [Establishing dermatopathology encyclopedia DermpathNet with Artificial Intelligence-Based Workflow](https://arxiv.org/abs/2601.19378)
*Ziyang Xu,Mingquan Lin,Yiliang Zhou,Zihan Xu,Seth J. Orlow,Zihan Xu,Shane A. Meehan,Alexandra Flamm,Ata S. Moshiri,Yifan Peng*

Main category: cs.CV

TL;DR: 提出DermpathNet：通过混合流程从PMC自动化筛选与分类皮肤病理图像，构建经专家审核的开放数据集，并验证其优于单一方法。


<details>
  <summary>Details</summary>
Motivation: 临床与培训中缺乏高质量、开放获取的皮肤病理图像数据集，不利于学习、交叉检索与机器学习研究。

Method: 建立半自动化管线：用关键词从PMC抓取图像；采用深度学习模型进行图像模态/类别判别，并结合图注文本解析进行混合分类；在手工标注集上评估；由认证皮肤病理学家复核并整理诊断标签。

Result: 在651张人工标注图像上，深度学习法F-score为89.6%，关键词法为61.0%，混合法为90.4%；最终获取7,772+张、涵盖166种诊断的、完整标注图像；实测现有OpenAI图像算法在该任务上表现不足。

Conclusion: DermpathNet提供了规模大、经专家审核的开放皮肤病理图像资源；混合式深度学习+文本方法在图像筛选与分类上效果最好，能支持教育与研究，但通用图像模型仍需改进以胜任皮肤病理分析。

Abstract: Accessing high-quality, open-access dermatopathology image datasets for learning and cross-referencing is a common challenge for clinicians and dermatopathology trainees. To establish a comprehensive open-access dermatopathology dataset for educational, cross-referencing, and machine-learning purposes, we employed a hybrid workflow to curate and categorize images from the PubMed Central (PMC) repository. We used specific keywords to extract relevant images, and classified them using a novel hybrid method that combined deep learning-based image modality classification with figure caption analyses. Validation on 651 manually annotated images demonstrated the robustness of our workflow, with an F-score of 89.6\% for the deep learning approach, 61.0\% for the keyword-based retrieval method, and 90.4\% for the hybrid approach. We retrieved over 7,772 images across 166 diagnoses and released this fully annotated dataset, reviewed by board-certified dermatopathologists. Using our dataset as a challenging task, we found the current image analysis algorithm from OpenAI inadequate for analyzing dermatopathology images. In conclusion, we have developed a large, peer-reviewed, open-access dermatopathology image dataset, DermpathNet, which features a semi-automated curation workflow.

</details>


### [39] [Tri-Reader: An Open-Access, Multi-Stage AI Pipeline for First-Pass Lung Nodule Annotation in Screening CT](https://arxiv.org/abs/2601.19380)
*Fakrul Islam Tushar,Joseph Y. Lo*

Main category: cs.CV

TL;DR: Tri-Reader 是一个基于公开数据与开源模型的肺部影像三阶段流水线：肺分割→结节检测→恶性分类，强调高敏感度并降低标注负担，并在多内外部数据集上与专家与基准对照验证泛化与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有结节检测/分类流程往往分散、对高敏感度与标注效率难以兼顾，且泛化性在跨机构数据上不稳定。需要一个可复现、免费、端到端的系统，在保证高召回的同时减少候选数量，便于临床与标注工作流。

Method: 整合多个开源、基于公共数据训练的模型，构建三阶段管线：1) 肺野分割以限定搜索空间；2) 结节检测生成候选并控制候选量；3) 恶性风险分类筛选与排序。以“高敏感度优先”的阈值与策略设计，并在多个内部与外部数据集上，与专家标注和官方基准进行比较评估。

Result: Tri-Reader 在多数据集上实现较高敏感度，同时显著降低给标注者的候选负担（具体指标未在摘要中给出），并在跨机构评估中显示良好的一致性与泛化能力。

Conclusion: 一个免费可用、可泛化的三阶段胸部CT结节分析流水线能够在保持高敏感度的同时降低标注成本，并在多中心数据上保持稳定表现，适合用于研究与临床前验证。

Abstract: Using multiple open-access models trained on public datasets, we developed Tri-Reader, a comprehensive, freely available pipeline that integrates lung segmentation, nodule detection, and malignancy classification into a unified tri-stage workflow. The pipeline is designed to prioritize sensitivity while reducing the candidate burden for annotators. To ensure accuracy and generalizability across diverse practices, we evaluated Tri-Reader on multiple internal and external datasets as compared with expert annotations and dataset-provided reference standards.

</details>


### [40] [Unveiling Perceptual Artifacts: A Fine-Grained Benchmark for Interpretable AI-Generated Image Detection](https://arxiv.org/abs/2601.19430)
*Yao Xiao,Weiyan Chen,Jiahao Chen,Zijie Cao,Weijian Deng,Binbin Yang,Ziyi Dong,Xiangyang Ji,Wei Ke,Pengxu Wei,Liang Lin*

Main category: cs.CV

TL;DR: 提出X-AIGD细粒度基准，为AI生成图像检测提供像素级、类别化的伪迹标注，揭示现有检测器对伪迹依赖弱、可解释性不足；通过将注意力与伪迹区域对齐，可提升可解释性与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有AIGI检测多为二分类，缺乏可解释证据；基准数据集虽广但伪迹类型覆盖不足、缺少局部化标注，难以评估与提升模型的可解释性与因果证据。

Method: 构建X-AIGD数据集：对感知伪迹进行像素级、类别化标注，涵盖低级失真、高级语义与认知层面的反事实线索；基于该数据开展系统评测与对比实验，并探索通过显式将模型注意力对齐到伪迹区域来提升检测器。

Result: 实证表明：(1) 现有检测器即使面对基础失真，也几乎不依赖可感知伪迹；(2) 检测器可被训练识别特定伪迹，但决策仍大量依赖不可解释特征；(3) 将注意力与伪迹区域对齐能显著提升可解释性与跨域泛化。

Conclusion: 细粒度、像素级伪迹标注可作为评估与提升AIGI检测可解释性的关键工具；对齐注意力到伪迹区域是一条有效路径，可在保持性能的同时增强可解释与泛化能力。

Abstract: Current AI-Generated Image (AIGI) detection approaches predominantly rely on binary classification to distinguish real from synthetic images, often lacking interpretable or convincing evidence to substantiate their decisions. This limitation stems from existing AIGI detection benchmarks, which, despite featuring a broad collection of synthetic images, remain restricted in their coverage of artifact diversity and lack detailed, localized annotations. To bridge this gap, we introduce a fine-grained benchmark towards eXplainable AI-Generated image Detection, named X-AIGD, which provides pixel-level, categorized annotations of perceptual artifacts, spanning low-level distortions, high-level semantics, and cognitive-level counterfactuals. These comprehensive annotations facilitate fine-grained interpretability evaluation and deeper insight into model decision-making processes. Our extensive investigation using X-AIGD provides several key insights: (1) Existing AIGI detectors demonstrate negligible reliance on perceptual artifacts, even at the most basic distortion level. (2) While AIGI detectors can be trained to identify specific artifacts, they still substantially base their judgment on uninterpretable features. (3) Explicitly aligning model attention with artifact regions can increase the interpretability and generalization of detectors. The data and code are available at: https://github.com/Coxy7/X-AIGD.

</details>


### [41] [RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming](https://arxiv.org/abs/2601.19433)
*Jisheng Chu,Wenrui Li,Rui Zhao,Wangmeng Zuo,Shifeng Chen,Xiaopeng Fan*

Main category: cs.CV

TL;DR: RoamScene3D提出从文本生成沉浸式3D场景的新框架，通过语义推理与运动自适应补洞提升一致性与写实度。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D扩散先验的方法对空间关系“失明”，依赖预设相机轨迹，无法理解场景语义布局与对象关系，难以自适应地探索被遮挡区域；同时2D图像域修复难以处理相机运动导致的空洞与视差问题。

Method: 1) 使用VLM从文本/图像提取对象与关系，构建场景图；2) 基于场景图进行语义驱动的相机感知与自适应漫游轨迹规划，关注显著对象边界与关系；3) 设计Motion-Injected Inpainting，在含真实相机轨迹的合成全景数据上微调，使修复模型对相机运动敏感，缓解2D先验的静态局限；4) 结合语义推理与几何约束，生成一致、写实的多视角内容。

Result: 在大量实验中，相比SOTA方法，RoamScene3D在场景一致性、写实度和对遮挡区域的合理推断方面显著提升；在动态相机条件下的补洞质量更好。

Conclusion: 将语义关系建模与空间生成耦合，并引入运动注入的修复模块，可以显著改进文本到3D场景生成的一致性与写实度；语义驱动的自适应相机规划与运动自适应修复是关键。

Abstract: Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.

</details>


### [42] [DSTCS: Dual-Student Teacher Framework with Segment Anything Model for Semi-Supervised Pubic Symphysis Fetal Head Segmentation](https://arxiv.org/abs/2601.19446)
*Yalin Luo,Shun Long,Huijin Wang,Jieyun Bai*

Main category: cs.CV

TL;DR: 提出DSTCS：结合CNN与SAM的双学生-教师框架，用协同学习、边界增强增广和新损函数，显著提升PSFH超声分割，在MICCAI 2023/2024基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: PSFH在产时监护中用于评估产程与并发症，超声分割受类别不平衡、边界模糊、噪声和标注稀缺所困，现有多依赖CNN/Transformer，尚未充分利用更强的分割先验（如SAM）。

Method: 设计Dual-Student and Teacher combining CNN and SAM（DSTCS）架构：以CNN与SAM为双学生分支，引入师生与协同学习机制促进互补特征与伪标签互蒸馏；提出针对边界的专门数据增强策略；引入新型损失函数以缓解类不平衡与边界不确定；整体端到端训练。

Result: 在MICCAI 2023与2024 PSFH分割基准上进行大量实验，表现出更强鲁棒性并显著超越现有方法（定量指标未给出）。

Conclusion: DSTCS在PSFH超声分割中提供更可靠的临床工具，证明CNN与SAM协同的师生框架、边界增强增广与新损失对提升精度与鲁棒性有效。

Abstract: Segmentation of the pubic symphysis and fetal head (PSFH) is a critical procedure in intrapartum monitoring and is essential for evaluating labor progression and identifying potential delivery complications. However, achieving accurate segmentation remains a significant challenge due to class imbalance, ambiguous boundaries, and noise interference in ultrasound images, compounded by the scarcity of high-quality annotated data. Current research on PSFH segmentation predominantly relies on CNN and Transformer architectures, leaving the potential of more powerful models underexplored. In this work, we propose a Dual-Student and Teacher framework combining CNN and SAM (DSTCS), which integrates the Segment Anything Model (SAM) into a dual student-teacher architecture. A cooperative learning mechanism between the CNN and SAM branches significantly improves segmentation accuracy. The proposed scheme also incorporates a specialized data augmentation strategy optimized for boundary processing and a novel loss function. Extensive experiments on the MICCAI 2023 and 2024 PSFH segmentation benchmarks demonstrate that our method exhibits superior robustness and significantly outperforms existing techniques, providing a reliable segmentation tool for clinical practice.

</details>


### [43] [Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods](https://arxiv.org/abs/2601.19461)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 本文针对林业场景无人机任务对深度估计跨域泛化的需求，首次对8种零样本立体匹配方法在四大标准基准与新提出的树枝数据集上进行系统评测，发现不同场景下方法表现存在显著模式差异，并确定DEFOM在植被场景中的基准地位。


<details>
  <summary>Details</summary>
Motivation: 现有立体深度估计评测多聚焦城市与室内，缺乏对高植被密度（林业）场景的系统性评估；无人机林业作业需要在复杂纹理、重复结构、遮挡与细枝条等挑战下具备强跨域泛化能力。

Method: 零样本设置：统一使用各方法官方在Scene Flow上训练的预训练权重；选择8种范式覆盖迭代细化、基础模型、扩散式、3D CNN等；评测基准包括ETH3D、KITTI 2012/2015、Middlebury，以及新采集的5313对、1920×1080分辨率的Canterbury Tree Branches数据集；报告像素误差并进行定性排序与跨基准排名统计。

Result: 呈现明显场景依赖性：基础模型在结构化场景更强（BridgeDepth在ETH3D达0.23 px，DEFOM在Middlebury 4.65 px）；迭代类跨基准波动大（IGEV++：ETH3D 0.36 px但Middlebury 6.77 px；IGEV：ETH3D 0.33 px但Middlebury 4.99 px）。在树枝数据集的定性评测中，DEFOM表现最稳定，跨基准平均名次1.75，常居第1-2。

Conclusion: 不同方法对场景结构的敏感性显著，基础模型在规则结构中占优，而在植被等非结构化细节丰富场景中，DEFOM展现最佳跨域一致性。作者建议将DEFOM预测作为未来植被深度估计基准的伪真值。

Abstract: Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments. We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms. All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \times 1080$). Results reveal scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury), while iterative methods show variable cross-benchmark performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury; IGEV: 0.33 px on ETH3D but 4.99 px on Middlebury). Qualitative evaluation on the Tree Branches dataset establishes DEFOM as the gold-standard baseline for vegetation depth estimation, with superior cross-domain consistency (consistently ranking 1st-2nd across benchmarks, average rank 1.75). DEFOM predictions will serve as pseudo-ground-truth for future benchmarking.

</details>


### [44] [Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes](https://arxiv.org/abs/2601.19484)
*Yin Wang,Zhiying Leng,Haitian Liu,Frederick W. B. Li,Mu Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 提出Dyn-HSI：首个面向动态场景的人-场景交互生成认知架构，含视觉(动态导航)、记忆(分层经验记忆)与控制(HSI扩散模型)三组件；并构建动态基准Dyn-Scenes，实验显示在静态与动态场景均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有HSI方法将场景视为静态，难以反映真实世界中不断变化的环境，导致导航、交互与动作生成在动态情境下失效或泛化差。受世界模型启发，需要一个能感知变化、利用过往经验并进行高保真控制的整体架构。

Method: 构建三模块认知架构Dyn-HSI：1) 视觉：动态场景感知导航，持续感知环境变化并自适应预测下一导航路点；2) 记忆：分层经验记忆，训练期累积并更新经验，推理时利用先验做情境感知动作预热（priming）以提升质量与泛化；3) 控制：人-场景交互扩散模型，结合多模态条件生成高保真交互动作。并将静态HSI数据扩展为动态基准Dyn-Scenes用于评测。

Result: 在新构建的Dyn-Scenes以及静态数据集上进行定量与定性评测，Dyn-HSI在动态与静态设置均持续优于现有方法，生成更高质量的人-场景交互动作。

Conclusion: 动态场景感知的认知式HSI框架可显著提升在变化环境中的动作生成质量与泛化；分层记忆与多模态扩散控制协同发挥作用。Dyn-Scenes为该方向提供了评测基准。

Abstract: Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.

</details>


### [45] [Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation](https://arxiv.org/abs/2601.19488)
*Yizhao Han,Tianxing Shi,Zhao Wang,Zifan Xu,Zhiyuan Pu,Mingxiao Li,Qian Zhang,Wei Yin,Xiao-Xiao Long*

Main category: cs.CV

TL;DR: 提出一种用于视频自回归解码的采样策略ENkG：依据每个token预测分布的熵自适应调整候选集规模，在低熵区域缩小k、在高熵区域增大k，以同时抑制冗余噪声并缓解误差累积，训练无关、几乎零开销，较静态top-k/top-p显著提升感知质量与结构稳定性。


<details>
  <summary>Details</summary>
Motivation: 静态top-k/top-p在语言中有效，但视频token语义密度低、时空冗余高，固定候选规模要么在低不确定区域引入无谓随机性、要么在高不确定区域被早期错误束缚，导致长时程误差积累与质量劣化，需要一种能随不确定性自适应的采样机制。

Method: 以每个token预测分布的熵衡量不确定性，动态设定对应的候选集大小k：低熵（确定性强、如静态背景）使用更小k以保持结构；高熵（不确定性高、如前景运动物体）使用更大k以提高探索与纠错。该策略与模型无关、无需训练改动，计算开销可忽略。

Result: 在视频生成实验中，相比静态top-k/top-p，ENkG在感知质量与结构稳定性上取得一致提升，尤其在长时程生成中更少错误累积与结构漂移。

Conclusion: 依据熵的自适应采样能更匹配视频token的不确定性分布，兼顾稳定性与多样性，是一种简单、通用且高效的AR视频解码改进策略。

Abstract: Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.

</details>


### [46] [Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction](https://arxiv.org/abs/2601.19489)
*Ziyu Zhang,Tianle Liu,Diantao Tu,Shuhan Shen*

Main category: cs.CV

TL;DR: 提出一个在一分钟内收敛的3D高斯泼洒(3DGS)重建流水线，通过分两阶段针对有噪SLAM位姿与高精COLMAP位姿分别优化，结合高效并行反向优化、紧凑前向渲染、锚点式Neural-Gaussian、位姿全局细化、多视一致性引导的高斯分裂与深度监督等技巧，最终在赛题中以PSNR 28.43夺冠。


<details>
  <summary>Details</summary>
Motivation: 3DGS方法虽快但在严格时间预算（≤1分钟）下难以在不同位姿质量（SLAM噪声 vs. COLMAP高精）场景中稳定获得高保真重建；需要一个既稳健又高效的统一方案，适配挑战赛两轮不同输入。

Method: 两阶段流水线：
- 第一轮（SLAM位姿，噪声）：采用反向逐高斯并行优化与紧凑前向splat（基于Taming-GS、Speedy-splat）、负载均衡tiling；引入锚点式Neural-Gaussian以减少可学习参数、加速收敛；利用单目深度与部分前馈3DGS模型做初始化；加入全局位姿细化模块以纠正SLAM轨迹噪声。
- 第二轮（COLMAP位姿，高精）：关闭位姿细化；从Neural-Gaussian切回标准3DGS以消除MLP推理开销；加入受Fast-GS启发的多视一致性引导高斯分裂；引入深度估计器监督渲染深度。整体在一分钟预算内完成优化与渲染。

Result: 在SIGGRAPH Asia 3DGS Fast Reconstruction Challenge中取得PSNR 28.43的最高成绩，综合排名第一，展示了在极端时间预算下的高保真重建能力。

Conclusion: 通过针对位姿质量差异的阶段化设计与一系列高效工程与算法改进（并行优化、紧凑渲染、Neural-Gaussian/标准3DGS切换、多视一致性分裂、深度监督与位姿细化），方法在一分钟内实现稳健且高质量的3DGS重建，适用于不同位姿来源的实际场景。

Abstract: We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition.

</details>


### [47] [Cortex-Grounded Diffusion Models for Brain Image Generation](https://arxiv.org/abs/2601.19498)
*Fabian Bongratz,Yitong Li,Sama Elbaroudy,Christian Wachinger*

Main category: cs.CV

TL;DR: Cor2Vox提出以皮层形态为连续先验的3D脑MRI合成框架，用高分辨率皮层表面驱动Brownian bridge扩散实现拓扑一致、可控的影像生成，并配套基于3.3万UKB的皮层形状统计模型；在质量指标、表面重建、全脑分割上优于基线，支持解剖一致合成、灰质萎缩进程模拟与跨域和谐化，且对形态与疾病变异具鲁棒性、无需再训练。


<details>
  <summary>Details</summary>
Motivation: 真实神经影像数据存在稀有表型稀缺、不同扫描域转移、纵向覆盖不足等限制；现有生成模型多用标签/文本等弱条件，缺乏精细的解剖约束，易生成生物学不合理的结果，因此需要一个与皮层解剖强绑定、可精细控制的生成方法。

Method: 提出Cor2Vox：以高分辨率皮层表面作为连续结构先验，进行3D形到像的Brownian bridge扩散；构建来自3.3万UK Biobank扫描的皮层形态大型统计形状模型以生成新颖而真实的脑形状；在扩散过程中保证拓扑一致性与对底层解剖的精细控制。

Result: 在传统图像质量指标、先进的皮层表面重建和全脑分割质量上优于多种基线；能在三个应用场景中稳定工作：（i）解剖一致的合成，（ii）渐进性灰质萎缩模拟，（iii）将院内FTD数据与公共数据和谐化；在亚体素层面保留皮层细节，对皮层几何与疾病表型变化具有显著鲁棒性且无需再训练。

Conclusion: 以皮层结构为先验的形到像扩散结合大规模统计形状模型，可生成解剖可信、可控且具跨域鲁棒性的脑MRI，实现稀有表型扩充、病程模拟与数据和谐化，对临床与研究场景具有广泛潜力。

Abstract: Synthetic neuroimaging data can mitigate critical limitations of real-world datasets, including the scarcity of rare phenotypes, domain shifts across scanners, and insufficient longitudinal coverage. However, existing generative models largely rely on weak conditioning signals, such as labels or text, which lack anatomical grounding and often produce biologically implausible outputs. To this end, we introduce Cor2Vox, a cortex-grounded generative framework for brain magnetic resonance image (MRI) synthesis that ties image generation to continuous structural priors of the cerebral cortex. It leverages high-resolution cortical surfaces to guide a 3D shape-to-image Brownian bridge diffusion process, enabling topologically faithful synthesis and precise control over underlying anatomies. To support the generation of new, realistic brain shapes, we developed a large-scale statistical shape model of cortical morphology derived from over 33,000 UK Biobank scans. We validated the fidelity of Cor2Vox based on traditional image quality metrics, advanced cortical surface reconstruction, and whole-brain segmentation quality, outperforming many baseline methods. Across three applications, namely (i) anatomically consistent synthesis, (ii) simulation of progressive gray matter atrophy, and (iii) harmonization of in-house frontotemporal dementia scans with public datasets, Cor2Vox preserved fine-grained cortical morphology at the sub-voxel level, exhibiting remarkable robustness to variations in cortical geometry and disease phenotype without retraining.

</details>


### [48] [Bridging Information Asymmetry: A Hierarchical Framework for Deterministic Blind Face Restoration](https://arxiv.org/abs/2601.19506)
*Zhengjian Yao,Jiakui Hu,Kaiwen Li,Hangzhou He,Xinliang Zhang,Shuang Zeng,Lei Zhu,Yanye Lu*

Main category: cs.CV

TL;DR: Pref-Restore提出一种用于盲脸修复的分层框架，通过“增强输入信息密度”和“裁剪输出分布”来缓解低质输入与高质输出的信息不对称，实现偏好对齐与确定性修复，达到SOTA并降低解空间熵。


<details>
  <summary>Details</summary>
Motivation: 盲脸修复面临严重的病态性：低质输入信息稀缺，导致到高质输出的一对多映射与幻觉伪影。现有生成方法虽能合成纹理，但受信息不对称影响，稳定性与可控性不足，需要一种能引入高层语义约束并抑制随机性的机制。

Method: 提出Pref-Restore分层框架：1) 增强输入密度：使用自回归整合器将文本指令重构为致密潜在查询，为退化信号注入稳定的高层语义约束；2) 裁剪输出分布：在扩散修复循环中引入on-policy强化学习，将人类偏好转化为可微约束，对随机偏离进行显式惩罚，收紧后验分布以获得高保真结果。

Result: 在合成与真实数据基准上达成SOTA表现；偏好对齐策略显著降低了解的熵（不确定性），带来更可靠、确定性的修复结果。

Conclusion: 通过结合离散语义逻辑与连续纹理生成，并以偏好对齐的RL约束扩散过程，Pref-Restore有效缓解信息不对称问题，实现确定性、高保真的盲脸修复，为可靠的盲修复提供了可行路径。

Abstract: Blind face restoration remains a persistent challenge due to the inherent ill-posedness of reconstructing holistic structures from severely constrained observations. Current generative approaches, while capable of synthesizing realistic textures, often suffer from information asymmetry -- the intrinsic disparity between the information-sparse low quality inputs and the information-dense high quality outputs. This imbalance leads to a one-to-many mapping, where insufficient constraints result in stochastic uncertainty and hallucinatory artifacts. To bridge this gap, we present \textbf{Pref-Restore}, a hierarchical framework that integrates discrete semantic logic with continuous texture generation to achieve deterministic, preference-aligned restoration. Our methodology fundamentally addresses this information disparity through two complementary strategies: (1) Augmenting Input Density: We employ an auto-regressive integrator to reformulate textual instructions into dense latent queries, injecting high-level semantic stability to constrain the degraded signals; (2) Pruning Output Distribution: We pioneer the integration of on-policy reinforcement learning directly into the diffusion restoration loop. By transforming human preferences into differentiable constraints, we explicitly penalize stochastic deviations, thereby sharpening the posterior distribution toward the desired high-fidelity outcomes. Extensive experiments demonstrate that Pref-Restore achieves state-of-the-art performance across synthetic and real-world benchmarks. Furthermore, empirical analysis confirms that our preference-aligned strategy significantly reduces solution entropy, establishing a robust pathway toward reliable and deterministic blind restoration.

</details>


### [49] [Mocap Anywhere: Towards Pairwise-Distance based Motion Capture in the Wild (for the Wild)](https://arxiv.org/abs/2601.19519)
*Ofir Abramovich,Ariel Shamir,Andreas Aristidou*

Main category: cs.CV

TL;DR: 提出基于稀疏UWB节点间成对距离(PWD)的无相机实时动捕系统Wild-Poser，直接从噪声距离预测3D关节位置，实现对人/动物的形状无关、室外可用、低成本动捕。


<details>
  <summary>Details</summary>
Motivation: 现有光学/IMU动捕受环境(光照、磁干扰)、布置成本、形体依赖和室外鲁棒性限制。需要一种无需外部相机、少传感器、对形体不敏感、可在非受控场景下稳定工作的动捕方案。

Method: 使用UWB节点间飞行时间测距获得稀疏PWD，设计紧凑实时Transformer(WiP)直接从可能噪声/缺失的距离预测3D关节位置；随后可用学习方法重建关节旋转。模型不做个体形体拟合，训练以增强对不同体型与物种的泛化。

Result: 在实时条件下，WiP达到低关节位置误差，能在野外场景对人类与动物进行准确3D动作重建；对噪声/损坏距离具有鲁棒性并跨主体泛化。

Conclusion: 基于UWB PWD与Transformer的方案实现可扩展、低成本、通用、形状无关且室外鲁棒的动捕，展示在真实世界中替代传统光学/惯性系统的潜力。

Abstract: We introduce a novel motion capture system that reconstructs full-body 3D motion using only sparse pairwise distance (PWD) measurements from body-mounted(UWB) sensors. Using time-of-flight ranging between wireless nodes, our method eliminates the need for external cameras, enabling robust operation in uncontrolled and outdoor environments. Unlike traditional optical or inertial systems, our approach is shape-invariant and resilient to environmental constraints such as lighting and magnetic interference. At the core of our system is Wild-Poser (WiP for short), a compact, real-time Transformer-based architecture that directly predicts 3D joint positions from noisy or corrupted PWD measurements, which can later be used for joint rotation reconstruction via learned methods. WiP generalizes across subjects of varying morphologies, including non-human species, without requiring individual body measurements or shape fitting. Operating in real time, WiP achieves low joint position error and demonstrates accurate 3D motion reconstruction for both human and animal subjects in-the-wild. Our empirical analysis highlights its potential for scalable, low-cost, and general purpose motion capture in real-world settings.

</details>


### [50] [A Non-Invasive 3D Gait Analysis Framework for Quantifying Psychomotor Retardation in Major Depressive Disorder](https://arxiv.org/abs/2601.19526)
*Fouad Boutaleb,Emery Pierson,Mohamed Daoudi,Clémence Nineuil,Ali Amad,Fabien D'Hondt*

Main category: cs.CV

TL;DR: 用单目RGB视频重建临床可解释的3D步态动力学，并通过稳健学习在小数据上识别抑郁相关运动特征，检测PMR达83.3%准确，抑郁严重度解释度R^2=0.64。


<details>
  <summary>Details</summary>
Motivation: 现有MDD评估主观性强，尤其PMR评价；3D动作捕捉客观但昂贵难以临床推广。需要一种基于常规视频的、可解释且可量化的运动表征以实现客观监测。

Method: 提出从单目RGB视频到3D步态运动学的计算框架：使用重力视角坐标系和基于改造TUG闭环路径的轨迹校正以缓解单目深度误差；从单摄像头提取297个明确的步态生物力学生物标志物。为小样本问题设计稳定性驱动的机器学习流程，筛选稳健运动特征并防止过拟合；在CALYPSO数据集上验证。

Result: 在检测PMR上达到83.3%准确率；对总体抑郁严重度的方差解释度R^2=0.64。识别出踝部推进力降低与骨盆活动受限与抑郁运动表型显著相关。

Conclusion: 身体运动可作为认知/情绪状态的可靠代理。该透明、可扩展的单摄像头步态分析工具为标准临床环境中的抑郁客观监测提供了可行途径。

Abstract: Predicting the status of Major Depressive Disorder (MDD) from objective, non-invasive methods is an active research field. Yet, extracting automatically objective, interpretable features for a detailed analysis of the patient state remains largely unexplored.
  Among MDD's symptoms, Psychomotor retardation (PMR) is a core item, yet its clinical assessment remains largely subjective. While 3D motion capture offers an objective alternative, its reliance on specialized hardware often precludes routine clinical use. In this paper, we propose a non-invasive computational framework that transforms monocular RGB video into clinically relevant 3D gait kinematics. Our pipeline uses Gravity-View Coordinates along with a novel trajectory-correction algorithm that leverages the closed-loop topology of our adapted Timed Up and Go (TUG) protocol to mitigate monocular depth errors. This novel pipeline enables the extraction of 297 explicit gait biomechanical biomarkers from a single camera capture.
  To address the challenges of small clinical datasets, we introduce a stability-based machine learning framework that identifies robust motor signatures while preventing overfitting. Validated on the CALYPSO dataset, our method achieves an 83.3% accuracy in detecting PMR and explains 64% of the variance in overall depression severity (R^2=0.64). Notably, our study reveals a strong link between reduced ankle propulsion and restricted pelvic mobility to the depressive motor phenotype. These results demonstrate that physical movement serves as a robust proxy for the cognitive state, offering a transparent and scalable tool for the objective monitoring of depression in standard clinical environments.

</details>


### [51] [The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments](https://arxiv.org/abs/2601.19557)
*Riccardo Giubilato,Marcus Gerhard Müller,Marco Sewtz,Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel*

Main category: cs.CV

TL;DR: S3LI Vulcano 是在意大利西西里火山岛采集的多模态（视觉+激光雷达）数据集，用于SLAM与地点识别研究，并配套开源工具生成真值位姿与标注样本。


<details>
  <summary>Details</summary>
Motivation: 现有用于多模态SLAM与地点识别的数据在场景多样性（地质纹理、复杂地形、自然环境）与高质量真值支持方面不足，限制了算法在恶劣自然环境中的泛化与基准评测。

Method: 在Vulcano火山岛的多样地貌（玄武岩、富铁岩、古熔岩通道地貌、干燥植被与水域）中采集多段序列，提供视觉与LiDAR传感器数据；同时发布配套开源工具链，用于生成位姿真值和制作用于地点识别的标注样本。

Result: 形成覆盖多环境、纹理与地形变化的多段多模态序列数据集，并公开下载链接与代码工具，便于研究者复现与基准化评测。

Conclusion: S3LI Vulcano 数据集与工具箱为多模态SLAM与地点识别提供了新的标准化基准与数据来源，促进在自然复杂地形条件下的算法开发与比较。

Abstract: We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities. Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy. The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water. The data (rmc.dlr.de/s3li_dataset) is accompanied by an open source toolkit (github.com/DLR-RM/s3li-toolkit) providing tools for generating ground truth poses as well as preparation of labelled samples for place recognition tasks.

</details>


### [52] [MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation](https://arxiv.org/abs/2601.19577)
*Ronglai Zuo,Rolandos Alexandros Potamias,Qi Sun,Evangelos Ververas,Jiankang Deng,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 提出MaDiS：用于手语生成的掩码扩散式语言模型，具备双向依赖建模与并行多token生成；配合三层跨模态预训练、时间检查点解掩策略与部件混合嵌入，显著提升精度并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型用于手语生成存在单向上下文与逐token推理慢的问题；现有表示难以同时对齐文本、潜在表示与3D物理空间，且训练与解码效率受限。

Method: 1) 掩码扩散式语言模型：在掩码-去噪框架中实现双向上下文与并行多token采样；2) 三层跨模态预训练：在token级、潜变量级与3D物理空间级联合目标上学习，使表征更具语义与物理对齐；3) 时间检查点的解掩策略：通过在时间维度设置检查点，显著缩小解掩顺序的组合空间；4) 部件混合嵌入：以可学习门控与优化码本融合不同肢体/部位的手语token信息。

Result: 在CSL-Daily、Phoenix-2014T、How2Sign上，DTW误差与新提出的SiBLEU、SiCLIP等指标均优于现有方法；推理延迟降低约30%。

Conclusion: MaDiS通过双向并行生成、跨模态预训练、有效解掩与部件级融合，在手语生成的准确性与效率上同时取得显著提升，具有实用价值。

Abstract: Sign language generation (SLG) aims to translate written texts into expressive sign motions, bridging communication barriers for the Deaf and Hard-of-Hearing communities. Recent studies formulate SLG within the language modeling framework using autoregressive language models, which suffer from unidirectional context modeling and slow token-by-token inference. To address these limitations, we present MaDiS, a masked-diffusion-based language model for SLG that captures bidirectional dependencies and supports efficient parallel multi-token generation. We further introduce a tri-level cross-modal pretraining scheme that jointly learns from token-, latent-, and 3D physical-space objectives, leading to richer and more grounded sign representations. To accelerate model convergence in the fine-tuning stage, we design a novel unmasking strategy with temporal checkpoints, reducing the combinatorial complexity of unmasking orders by over $10^{41}$ times. In addition, a mixture-of-parts embedding layer is developed to effectively fuse information stored in different part-wise sign tokens through learnable gates and well-optimized codebooks. Extensive experiments on CSL-Daily, Phoenix-2014T, and How2Sign demonstrate that MaDiS achieves superior performance across multiple metrics, including DTW error and two newly introduced metrics, SiBLEU and SiCLIP, while reducing inference latency by nearly 30%. Code and models will be released on our project page.

</details>


### [53] [QuaMo: Quaternion Motions for Vision-based 3D Human Kinematics Capture](https://arxiv.org/abs/2601.19580)
*Cuong Le,Pavlo Melnyk,Urs Waldmann,Mårten Wadenbäck,Bastian Wandt*

Main category: cs.CV

TL;DR: QuaMo提出用四元数微分方程(QDE)与状态空间模型进行视频中的3D人体运动捕捉，避免欧拉角不连续与抖动；结合带自适应加速增强的meta-PD控制器，在线估计连续、物理合理的运动，并在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统基于姿态逐帧估计忽视时间一致性，导致抖动和不合理动作；现有运动学方法多用欧拉角，存在万向节锁与角度跳变，在线场景无法依赖后处理平滑，亟需一种本质连续、稳定的姿态表示与动态建模方法。

Method: - 采用四元数作为姿态状态，构建状态空间模型；以四元数微分方程(QDE)描述角速度到姿态的演化，并显式在单位球约束下数值求解以保持归一化。
- 通过meta-PD控制器估计角加速度，并提出“加速度增强”机制，在快速切换姿态时自适应调节控制信号，提高跟踪响应与稳定性。
- 在线估计：利用QDE + PD控制得到平滑且连续的姿态轨迹，避免欧拉角不连续与漂移。

Result: 在Human3.6M、Fit3D、SportsPose、AIST上优于可比SOTA；重建结果连续无不连续跳变，抖动与非物理现象显著减少；在在线设置下仍保持稳定准确。

Conclusion: 以四元数和QDE为核心、结合自适应加速度增强的PD控制与单位球约束求解，可在无后处理情况下实现连续、稳定、精确的3D人体运动学捕捉；方法通用、实时性友好，并在多数据集上证明其有效性。

Abstract: Vision-based 3D human motion capture from videos remains a challenge in computer vision. Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion. The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead. A major drawback in current kinematics approaches is their reliance on Euler angles. Despite their simplicity, Euler angles suffer from discontinuity that leads to unstable motion reconstructions, especially in online settings where trajectory refinement is unavailable. Contrarily, quaternions have no discontinuity and can produce continuous transitions between poses. In this paper, we propose QuaMo, a novel Quaternion Motions method using quaternion differential equations (QDE) for human kinematics capture. We utilize the state-space model, an effective system for describing real-time kinematics estimations, with quaternion state and the QDE describing quaternion velocity. The corresponding angular acceleration is computed from a meta-PD controller with a novel acceleration enhancement that adaptively regulates the control signals as the human quickly changes to a new pose. Unlike previous work, our QDE is solved under the quaternion unit-sphere constraint that results in more accurate estimations. Experimental results show that our novel formulation of the QDE with acceleration enhancement accurately estimates 3D human kinematics with no discontinuity and minimal implausibilities. QuaMo outperforms comparable state-of-the-art methods on multiple datasets, namely Human3.6M, Fit3D, SportsPose and AIST. The code is available at https://github.com/cuongle1206/QuaMo

</details>


### [54] [ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.19582)
*Yujin Wang,Yutong Zheng,Wenxian Fan,Tianyi Wang,Hongqing Chu,Daxin Tian,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: 提出ScenePilot-Bench，一个用于自动驾驶场景下评估视觉-语言模型（VLM）的第一人称大规模基准，覆盖理解、感知、规划与GPT-Score四大评测轴，并配套安全与跨区域泛化指标；实测多种代表性VLM以揭示性能边界与差距。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在安全关键的自动驾驶中缺乏系统性、规模化、贴近真实驾驶需求的评测；需要统一数据与指标，检验从场景理解到规划决策的端到端多能力，以及模型在不同地区分布转移下的泛化与安全鲁棒性。

Method: 构建基于ScenePilot-4K的基准：包含3847小时驾驶视频及多粒度标注（场景描述、风险评估、关键体参与者、Ego轨迹、相机参数）；设计四轴评测套件（场景理解、空间感知、运动规划、GPT-Score），引入安全感知指标与跨区域泛化设置；对多种代表性VLM进行基准测试与对比分析。

Result: 给出各代表性VLM在四轴任务上的实证结果，刻画其当前能力上限与短板，尤其在面向驾驶推理与安全相关任务上仍存在显著差距与不稳定性；跨区域泛化表现揭示数据分布转移带来的性能退化。

Conclusion: ScenePilot-Bench提供了覆盖理解—感知—规划的综合评测框架与安全/泛化维度，使研究者能够系统衡量并推进VLM在自动驾驶等安全关键应用中的能力；当前模型仍需在驾驶导向推理与安全鲁棒性上持续改进。

Abstract: In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.

</details>


### [55] [Localized Latent Editing for Dose-Response Modeling in Botulinum Toxin Injection Planning](https://arxiv.org/abs/2601.19593)
*Estèphe Arnaud,Mohamed Daoudi,Pierre Guerreschi*

Main category: cs.CV

TL;DR: 提出一种基于StyleGAN2的局部潜空间编辑与剂量-反应建模框架，用于模拟肉毒毒素注射在特定面部区域的效果，辅助制定个体化注射方案。


<details>
  <summary>Details</summary>
Motivation: 临床上Botox用于面部不对称与美学年轻化已成金标准，但剂量选择多依赖经验，可能导致疗效不稳或副作用。缺乏能在个体层面预测“单位剂量—形态变化”的可解释工具。

Method: 1) 在StyleGAN2潜空间中提出“区域特异的潜轴发现”（Region-Specific Latent Axis Discovery），学习各面部肌群的局部放松轨迹，实现对指定区域的可控编辑且避免全局漂移；2) 将这些局部潜轨迹与注射单位数建立对应关系，学习剂量—反应模型；3) 在含46名患者、360张图像的临床数据集上比较直接指标回归与基于生成模拟两种策略；4) 设计“人机协同”流程，让临床医生交互式微调模拟结果。

Result: 在留出测试集上，生成式框架与几何不对称结构指标呈中到强的相关，证明模型能正确捕捉形态变化方向；但由于生物学差异，绝对数值预测精度受限。生成式方法在可解释性与区域控制上优于纯指标回归。

Conclusion: 局部潜空间编辑结合剂量—反应建模可作为Botox注射规划的可视化与半定量工具；通过人机协同可弥补个体差异与模型误差，提升临床可用性。

Abstract: Botulinum toxin (Botox) injections are the gold standard for managing facial asymmetry and aesthetic rejuvenation, yet determining the optimal dosage remains largely intuitive, often leading to suboptimal outcomes. We propose a localized latent editing framework that simulates Botulinum Toxin injection effects for injection planning through dose-response modeling. Our key contribution is a Region-Specific Latent Axis Discovery method that learns localized muscle relaxation trajectories in StyleGAN2's latent space, enabling precise control over specific facial regions without global side effects. By correlating these localized latent trajectories with injected toxin units, we learn a predictive dose-response model. We rigorously compare two approaches: direct metric regression versus image-based generative simulation on a clinical dataset of N=360 images from 46 patients. On a hold-out test set, our framework demonstrates moderate-to-strong structural correlations for geometric asymmetry metrics, confirming that the generative model correctly captures the direction of morphological changes. While biological variability limits absolute precision, we introduce a hybrid "Human-in-the-Loop" workflow where clinicians interactively refine simulations, bridging the gap between pathological reconstruction and cosmetic planning.

</details>


### [56] [GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining](https://arxiv.org/abs/2601.19606)
*Shentong Mo,Zehua Chen,Jun Zhu*

Main category: cs.CV

TL;DR: 提出GMS-CAVP，通过多尺度对齐与扩散式预训练改进视频-音频联合表征与生成，在VGGSound、AudioSet、Panda70M上优于以往检索与生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有V-A方法（如CAVP）虽能用对比学习对齐语义与时序，但未充分建模视频与音频在空间与时间上的致密、多尺度对应关系，导致跨模态检索与生成效果受限。

Method: 构建GMS-CAVP，包含两部分：1) 多尺度对比学习：在细到粗的时间与空间粒度上进行语义/时序对齐，学习层级化的联合嵌入；2) 多尺度时空扩散式生成目标：在预训练中引入扩散模型，实现跨模态翻译与合成（视频↔音频），将判别式（对比）与生成式（扩散）统一训练，强化跨模态理解与可控生成能力。

Result: 在VGGSound、AudioSet、Panda70M上，GMS-CAVP在跨模态检索与生成质量方面均明显优于现有方法（含CAVP），体现更强的多尺度对齐与合成能力。

Conclusion: 多尺度对齐结合扩散式生成的统一框架能更充分建模视频-音频的致密多尺度对应关系，提升跨模态理解与高保真生成，为V-A检索与合成提供新范式。

Abstract: Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.

</details>


### [57] [The role of self-supervised pretraining in differentially private medical image analysis](https://arxiv.org/abs/2601.19618)
*Soroosh Tayebi Arasteh,Mina Farajiamiri,Mahshad Lotfinia,Behrus Hinrichs-Puladi,Jonas Bienzeisler,Mohamed Alhaskir,Mirabela Rusu,Christiane Kuhl,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 本文在大规模胸片分类基准上系统评估差分隐私训练下的初始化策略，发现自监督DINOv3优于ImageNet监督初始化，但仍不及领域内监督预训练；初始化对效用、公平性和泛化至关重要。


<details>
  <summary>Details</summary>
Motivation: 差分隐私能保护医疗影像数据，但显著降低诊断性能；近期发现初始化影响DP性能，但在“全模型DP”与现代自监督学习结合下的作用尚不清楚。作者欲量化不同初始化在DP医疗影像中的效用、公平性与泛化影响。

Method: 以>80万张胸片的ConvNeXt为骨干，使用DP-SGD在多种实际隐私预算下训练；比较三种初始化：1) 非领域监督（ImageNet），2) 非领域自监督（DINOv3），3) 领域监督（MIMIC-CXR）。在五个外部数据集上做跨机构评测，并分析人口公平性、跨数据集泛化、以及在隐私约束下对数据规模与模型容量的鲁棒性。

Result: 在DP训练中，DINOv3初始化稳定优于ImageNet监督初始化，但仍落后于MIMIC-CXR领域监督预训练；领域监督最接近非隐私上限。初始化显著影响人口群体间差异、公平性指标、跨数据集表现，以及在不同数据量与模型大小下的稳健性。

Conclusion: 在差分隐私医疗影像中，初始化是决定效用、公平性与泛化的核心因素；领域监督预训练目前最有效，自监督虽有益但不足以取代领域监督。

Abstract: Differential privacy (DP) provides formal protection for sensitive data but typically incurs substantial losses in diagnostic performance. Model initialization has emerged as a critical factor in mitigating this degradation, yet the role of modern self-supervised learning under full-model DP remains poorly understood. Here, we present a large-scale evaluation of initialization strategies for differentially private medical image analysis, using chest radiograph classification as a representative benchmark with more than 800,000 images. Using state-of-the-art ConvNeXt models trained with DP-SGD across realistic privacy regimes, we compare non-domain-specific supervised ImageNet initialization, non-domain-specific self-supervised DINOv3 initialization, and domain-specific supervised pretraining on MIMIC-CXR, the largest publicly available chest radiograph dataset. Evaluations are conducted across five external datasets spanning diverse institutions and acquisition settings. We show that DINOv3 initialization consistently improves diagnostic utility relative to ImageNet initialization under DP, but remains inferior to domain-specific supervised pretraining, which achieves performance closest to non-private baselines. We further demonstrate that initialization choice strongly influences demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity under privacy constraints. The results establish initialization strategy as a central determinant of utility, fairness, and generalization in differentially private medical imaging.

</details>


### [58] [Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework](https://arxiv.org/abs/2601.19640)
*Hao Chang,Zhihui Wang,Lingxiang Wu,Peijin Wang,Wenhui Diao,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出GovLA-10K基准与GovLA-Reasoner框架，面向城市治理的低空多模态感知与推理，聚焦“管理可行动”目标与建议，通过特征适配器联合视觉检测与LLM，无需任务特定微调即显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有低空视觉多聚焦于对象级检测与松耦合视觉-语言流程，难以满足城市治理中对“管理导向的异常理解”和可执行建议的需求；缺少以治理任务为核心的多模态数据与统一推理框架。

Method: 1) 构建GovLA-10K：围绕与治理直接相关的功能性显著目标进行标注，并附带可执行的管理建议；2) 设计GovLA-Reasoner：引入高效特征适配器，隐式协调视觉检测器与LLM之间的判别式表征共享，实现细粒度视觉定位与高层语境推理的协同；3) 在无需对单独组件进行任务特定微调的前提下进行统一推理。

Result: 在广泛实验中，GovLA-Reasoner在治理感知与推理任务上显著优于基线方法，同时保持零/少微调设置，对各子任务的性能均有提升。

Conclusion: 以治理为中心的基准与统一推理框架为低空城市治理场景中的多模态理解提供了新视角与基础，证明了通过特征适配实现检测器与LLM协同的有效性，并为未来管理感知型低空视觉-语言系统研究奠定了方向。

Abstract: Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.

</details>


### [59] [KeepLoRA: Continual Learning with Residual Gradient Adaptation](https://arxiv.org/abs/2601.19659)
*Mao-Lin Luo,Zi-Hao Zhou,Yi-Lin Zhang,Yuanyu Wan,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: 提出KeepLoRA：在视觉-语言预训练模型的连续学习中，将新任务更新限制在“残差子空间”，以同时保持预训练通识、历史任务知识与新任务可塑性，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 连续学习需兼顾三目标：不遗忘预训练通识、保留多任务序列知识、对新任务保持可塑性。现有方法难以在不干扰旧能力的前提下高效注入新知识。作者通过参数空间分析发现不同知识在不同子空间编码，为设计约束提供依据。

Method: 1) 分析参数/特征空间：通识主要在“主子空间”（principal subspace），任务特定知识在“残差子空间”。2) 基于LoRA的适配：仅允许LoRA参数在残差子空间更新。3) 梯度投影：将新任务梯度投影到与预训练主子空间以及历史任务主方向都正交的子空间，避免干扰通识与旧任务。4) 提供理论与实证支持。

Result: 在多组连续学习场景下达到SOTA，显示能同时减少灾难性遗忘、保持历史任务性能，并提升对新任务的学习效率与稳定性；理论分析证明上述子空间约束能降低干扰项。

Conclusion: 通过子空间分解与正交投影，KeepLoRA在不牺牲可塑性的前提下更好地保持预训练与历史任务知识，是简单有效的连续学习方案，具备实际可复现实现（代码开源）。

Abstract: Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.

</details>


### [60] [A new Image Similarity Metric for a Perceptual and Transparent Geometric and Chromatic Assessment](https://arxiv.org/abs/2601.19680)
*Antonio Di Marino,Vincenzo Bevilacqua,Emanuel Di Nardo,Angelo Ciaramella,Ivanoe De Falco,Giovanna Sannino*

Main category: cs.CV

TL;DR: 提出一种新的感知图像相似度度量，由纹理与色度两部分组成；在BAPPS数据集上优于现有方法，尤其对形状失真更敏感，并能给出可视化解释。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA图像相似度指标并非真正的感知指标，遇到纹理与形状失真时评估困难；深度黑盒方法虽准确但缺乏可解释性，仅给出分数不解释差异。

Method: 构建双项度量：1) 纹理项使用Earth Mover’s Distance衡量两图纹理分布差异；2) 色度项在Oklab感知色彩空间衡量色彩差异；并生成对应的可视化解释。

Result: 在Berkeley-Adobe Perceptual Patch Similarity（BAPPS）这一包含复杂形状与颜色失真的数据集上，新度量整体优于SOTA，尤其在存在形状失真时表现更好，体现更强的感知一致性。

Conclusion: 所提度量兼顾准确性与可解释性：既提升对形状/纹理/色彩失真的感知一致性，又通过可视化解释使相似度评分透明、可追溯。

Abstract: In the literature, several studies have shown that state-of-the-art image similarity metrics are not perceptual metrics; moreover, they have difficulty evaluating images, especially when texture distortion is also present. In this work, we propose a new perceptual metric composed of two terms. The first term evaluates the dissimilarity between the textures of two images using Earth Mover's Distance. The second term evaluates the chromatic dissimilarity between two images in the Oklab perceptual color space. We evaluated the performance of our metric on a non-traditional dataset, called Berkeley-Adobe Perceptual Patch Similarity, which contains a wide range of complex distortions in shapes and colors. We have shown that our metric outperforms the state of the art, especially when images contain shape distortions, confirming also its greater perceptiveness. Furthermore, although deep black-box metrics could be very accurate, they only provide similarity scores between two images, without explaining their main differences and similarities. Our metric, on the other hand, provides visual explanations to support the calculated score, making the similarity assessment transparent and justified.

</details>


### [61] [SharpNet: Enhancing MLPs to Represent Functions with Controlled Non-differentiability](https://arxiv.org/abs/2601.19683)
*Hanting Niu,Junkai Deng,Fei Hou,Wencheng Wang,Ying He*

Main category: cs.CV

TL;DR: 提出SharpNet：在MLP中引入由“跳跃Neumann边界条件”的Poisson方程求解得到的辅助特征函数，通过可微的局部积分评估，使网络能学习具有用户指定C0尖锐特征的函数；在保持非特征区域平滑的同时准确恢复边/角。


<details>
  <summary>Details</summary>
Motivation: 标准MLP输出全局平滑，难以表达连续但不可微（存在梯度跃迁/尖锐特征）的目标函数与几何；现有方法常需事后处理或牺牲可控性与可微性。

Method: 设计SharpNet架构：在MLP外增广一个辅助特征函数ϕ，该函数为满足跳跃Neumann边界条件的Poisson方程解；通过高效的局部积分形式数值评估，且对特征位置可微，从而可与MLP参数一起端到端联合优化；显式控制C0连续性，在特征处保持C0、其余区域平滑。

Result: 在2D函数拟合与3D CAD重建上，相比多种SOTA基线，SharpNet更准确地恢复尖锐边与角，避免梯度被抹平；在定性与定量评估中均显示优势。

Conclusion: 将可微的PDE驱动特征编码注入MLP，可精确控制C0连续性并端到端学习尖锐几何/函数特征，优于传统平滑MLP与基线方法。

Abstract: Multi-layer perceptrons (MLPs) are a standard tool for learning and function approximation, but they inherently yield outputs that are globally smooth. As a result, they struggle to represent functions that are continuous yet deliberately non-differentiable (i.e., with prescribed $C^0$ sharp features) without relying on ad hoc post-processing. We present SharpNet, a modified MLP architecture capable of encoding functions with user-defined sharp features by enriching the network with an auxiliary feature function, which is defined as the solution to a Poisson equation with jump Neumann boundary conditions. It is evaluated via an efficient local integral that is fully differentiable with respect to the feature locations, enabling our method to jointly optimize both the feature locations and the MLP parameters to recover the target functions/models. The $C^0$-continuity of SharpNet is precisely controllable, ensuring $C^0$-continuity at the feature locations and smoothness elsewhere. We validate SharpNet on 2D problems and 3D CAD model reconstruction, and compare it against several state-of-the-art baselines. In both types of tasks, SharpNet accurately recovers sharp edges and corners while maintaining smooth behavior away from those features, whereas existing methods tend to smooth out gradient discontinuities. Both qualitative and quantitative evaluations highlight the benefits of our approach.

</details>


### [62] [Video-KTR: Reinforcing Video Reasoning via Key Token Attribution](https://arxiv.org/abs/2601.19686)
*Ziyue Wang,Sheng Jin,Zhongrong Zuo,Jiawei Wu,Han Qiu,Qi She,Hao Zhang,Xudong Jiang*

Main category: cs.CV

TL;DR: Video-KTR 通过“按需挑选关键token”的强化学习来提升视频推理：用视觉归因、时间归因和不确定性三种信号定位关键token，只对这些token进行强化更新，兼顾精度与可解释性，并在多项基准上达到SOTA或强竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理RL多用粗粒度的序列级奖励或单一因子筛token，忽视视觉输入、时间动态与语言输出之间的细粒度关联，导致准确率与可解释性受限。

Method: 提出Video-KTR：在LLM视频推理中进行模态感知的token级策略塑形。三种归因信号联合选取关键token——(1) 视觉感知：反事实遮挡评估对视觉证据的依赖；(2) 时间敏感：帧洗牌评估对时序的敏感性；(3) 高熵：以预测不确定性识别需要重点学习的token。仅对这些token施加强化更新，过滤低价值token，实现选择性RL。

Result: 在5个基准上取得SOTA或强竞争力：例如Video-Holmes达42.7%，超过GPT-4o；在推理与通用视频理解任务上均有稳定增益。消融显示三种归因信号互补，且token级更新稳健。

Conclusion: Video-KTR是一个简单、可即插即用的RL扩展，通过模态敏感的token级强化学习提升视频推理的准确性与可解释性。源码与模型开源。

Abstract: Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.

</details>


### [63] [DSVM-UNet : Enhancing VM-UNet with Dual Self-distillation for Medical Image Segmentation](https://arxiv.org/abs/2601.19690)
*Renrong Shao,Dongyang Li,Dong Xia,Lin Shao,Jiangdong Lu,Fen Zheng,Lulu Zhang*

Main category: cs.CV

TL;DR: 提出DSVM-UNet：在VM-UNet基础上引入双重自蒸馏（全局与局部特征对齐），以无新增复杂结构的方式提升医学图像分割性能，并在ISIC2017/2018与Synapse上达SOTA且高效。


<details>
  <summary>Details</summary>
Motivation: 现有Vision Mamba与基于UNet的VM-UNet在医学分割中多依赖复杂结构改造来加强语义感知，但这增加了模型复杂度与计算成本。作者希望在不增加繁琐结构的前提下，利用训练策略提升长程依赖建模与多尺度语义对齐能力，从而兼顾精度与效率。

Method: 在VM-UNet框架上引入“双重自蒸馏”：1) 全局层面，将深层/教师特征与浅层/学生特征进行对齐，强化长程与高语义信息的传递；2) 局部层面，在编码器/解码器或多尺度分辨率间进行细粒度特征对齐，提升边界与细节。整体为训练策略，不引入复杂结构，保持线性时序复杂度与推理效率。

Result: 在ISIC2017、ISIC2018皮肤病变与Synapse器官分割基准上取得SOTA性能，同时维持或提升计算效率（未给出具体指标，但强调高效）。代码开源。

Conclusion: 无需复杂架构增改，通过全局与局部双重自蒸馏即可显著提升VM-UNet的分割性能与效率，证明训练策略对Vision Mamba-based分割模型的有效性与普适性。

Abstract: Vision Mamba models have been extensively researched in various fields, which address the limitations of previous models by effectively managing long-range dependencies with a linear-time overhead. Several prospective studies have further designed Vision Mamba based on UNet(VM-UNet) for medical image segmentation. These approaches primarily focus on optimizing architectural designs by creating more complex structures to enhance the model's ability to perceive semantic features. In this paper, we propose a simple yet effective approach to improve the model by Dual Self-distillation for VM-UNet (DSVM-UNet) without any complex architectural designs. To achieve this goal, we develop double self-distillation methods to align the features at both the global and local levels. Extensive experiments conducted on the ISIC2017, ISIC2018, and Synapse benchmarks demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. Code is available at https://github.com/RoryShao/DSVM-UNet.git.

</details>


### [64] [Self-Supervised Weight Templates for Scalable Vision Model Initialization](https://arxiv.org/abs/2601.19694)
*Yucheng Xie,Fu Feng,Ruixiao Shi,Jing Wang,Yong Rui,Xin Geng*

Main category: cs.CV

TL;DR: 提出SWEET：基于Tucker分解的“共享权重模板+尺寸特定缩放器”的自监督预训练，以便对不同深宽度的视觉模型进行可伸缩初始化；并引入宽度向随机缩放以提升跨宽度泛化，在多任务上达SOTA初始化表现。


<details>
  <summary>Details</summary>
Motivation: 传统预训练通常固定模型规模，实际部署需要不同大小架构（不同深度/宽度），导致迁移与适配效率低、性能不稳。需要一种能一次预训练、面向多种尺寸架构灵活初始化的方法。

Method: 1) 采用Tucker分解学习共享权重模板（shared weight template）与尺寸特定的权重缩放器（weight scalers），实现模块化参数表示；2) 目标模型通过“组合+重加权”从模板生成，并用轻量缩放器在少量数据上快速学习；3) 提出宽度向随机缩放（width-wise stochastic scaling）对模板在与宽度相关的维度进行正则，强化宽度不变表征、提升跨宽度泛化；4) 自监督框架下进行约束式预训练。

Result: 在分类、检测、分割、生成等多种视觉任务上，作为不同尺寸模型的初始化方案取得了优于现有方法的性能（SOTA），并展现出高效少数据适配能力。

Conclusion: SWEET通过共享模板+可调缩放器的Tucker因子化和宽度向随机缩放，实现对不同深宽度架构的统一、可伸缩初始化，提升跨宽度泛化与下游任务性能，是更适合实际多规模部署的预训练范式。

Abstract: The increasing scale and complexity of modern model parameters underscore the importance of pre-trained models. However, deployment often demands architectures of varying sizes, exposing limitations of conventional pre-training and fine-tuning. To address this, we propose SWEET, a self-supervised framework that performs constraint-based pre-training to enable scalable initialization in vision tasks. Instead of pre-training a fixed-size model, we learn a shared weight template and size-specific weight scalers under Tucker-based factorization, which promotes modularity and supports flexible adaptation to architectures with varying depths and widths. Target models are subsequently initialized by composing and reweighting the template through lightweight weight scalers, whose parameters can be efficiently learned from minimal training data. To further enhance flexibility in width expansion, we introduce width-wise stochastic scaling, which regularizes the template along width-related dimensions and encourages robust, width-invariant representations for improved cross-width generalization. Extensive experiments on \textsc{classification}, \textsc{detection}, \textsc{segmentation} and \textsc{generation} tasks demonstrate the state-of-the-art performance of SWEET for initializing variable-sized vision models.

</details>


### [65] [DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization](https://arxiv.org/abs/2601.19717)
*Yitong Yang,Xuexin Liu,Yinglin Wang,Jing Wang,Hao Dou,Changshuo Wang,Shuting He*

Main category: cs.CV

TL;DR: 提出DiffStyle3D：在扩散模型的潜变量空间对3D 高斯泼溅进行风格迁移，通过注意力对齐与几何引导保证跨视角一致与稳定训练，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VGG/CLIP方法难以在模型内部刻画多视图一致性；扩散法虽可建模一致性，但依赖去噪方向导致训练不稳定。需要一种既能保持多视角一致、又稳定高效的3D风格迁移方案。

Method: 提出DiffStyle3D（面向3DGS）：1) 在扩散模型的潜变量空间直接优化；2) 注意力感知损失：在自注意力空间对齐风格特征，并用内容特征对齐保持原始几何与语义；3) 几何引导的多视图一致性：将几何信息融入自注意力以建立跨视角对应；4) 基于几何的掩膜，抑制视角重叠区的冗余优化，提升一致性与稳定性。

Result: 在广泛实验中，相较SOTA方法获得更高的风格化质量与真实感，并展现更好的多视角一致性与训练稳定性。

Conclusion: 在潜变量空间结合注意力对齐与几何引导，可在3DGS中实现稳定且一致的3D风格迁移；DiffStyle3D在质量与真实感上优于现有方法。

Abstract: 3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.

</details>


### [66] [WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration](https://arxiv.org/abs/2601.19753)
*Xinrui Zhang,Yufeng Wang,Shuangkang Fang,Zesheng Wang,Dacheng Qi,Wenrui Ding*

Main category: cs.CV

TL;DR: WaterClear-GS将水下光学的局部衰减与散射直接嵌入3D高斯点，实现无需介质网络的实时水下NeRF替代，在NVS与UIR上兼顾高质量与高速度。


<details>
  <summary>Details</summary>
Motivation: NeRF类方法渲染慢且颜色恢复欠佳；3DGS虽快但无法表达体散射与波长相关衰减等复杂水下成像物理，导致几何与外观重建受限。需要一种既快又物理一致的水下3D重建与外观还原方法。

Method: 提出纯3DGS框架WaterClear-GS：在高斯原语中显式建模局部衰减与散射；采用双分支优化，一支保证水下光度一致性，另一支自然回归无水外观。引入深度引导的几何正则与感知驱动图像损失，并结合曝光约束、空间自适应正则、以及物理引导的光谱正则，确保局部3D一致性与自然视觉感受，同时无需辅助介质网络。

Result: 在标准基准与新采集数据集上，WaterClear-GS在新视角合成与水下图像复原两任务上取得优异指标，同时保持实时渲染速度。

Conclusion: 将水下物理显式融入3DGS并配合多重正则与双分支优化，可在保证实时性的同时显著提升水下3D重建与外观还原质量。

Abstract: Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.

</details>


### [67] [PaW-ViT: A Patch-based Warping Vision Transformer for Robust Ear Verification](https://arxiv.org/abs/2601.19771)
*Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 提出PaW-ViT：在ViT前对耳部图像进行基于解剖知识的补丁扭曲与归一化，使token边界与耳部特征边界对齐，从而提升鲁棒性与识别性能。


<details>
  <summary>Details</summary>
Motivation: 矩形token会包含目标外部背景信息，且ViT对位置敏感；耳生物特征存在形状、尺寸、姿态差异，导致传统矩形划分与耳部自然曲率和边界不匹配，影响表示一致性与识别效果。

Method: 设计预处理流程PaW-ViT：检测耳部关键特征与边界，依据耳的自然曲率与解剖结构进行图像扭曲/归一化，使patch/token边界与耳部解剖边界对齐，再将归一化后的图像输入标准ViT（适配ViT-T/S/B/L）。

Result: 在多种ViT规模上实验验证，PaW-ViT带来更高的鲁棒性与更一致的token表示，对形状、尺寸、姿态变化具有“合理的”对齐鲁棒性，并提升视觉识别性能（摘要未给具体数值）。

Conclusion: 通过解剖感知的patch对齐与空间归一化，缓解了耳部形态变化与Transformer位置敏感之间的不匹配，为耳生物识别与身份认证提供了可行方向。

Abstract: The rectangular tokens common to vision transformer methods for visual recognition can strongly affect performance of these methods due to incorporation of information outside the objects to be recognized. This paper introduces PaW-ViT, Patch-based Warping Vision Transformer, a preprocessing approach rooted in anatomical knowledge that normalizes ear images to enhance the efficacy of ViT. By accurately aligning token boundaries to detected ear feature boundaries, PaW-ViT obtains greater robustness to shape, size, and pose variation. By aligning feature boundaries to natural ear curvature, it produces more consistent token representations for various morphologies. Experiments confirm the effectiveness of PaW-ViT on various ViT models (ViT-T, ViT-S, ViT-B, ViT-L) and yield reasonable alignment robustness to variation in shape, size, and pose. Our work aims to solve the disconnect between ear biometric morphological variation and transformer architecture positional sensitivity, presenting a possible avenue for authentication schemes.

</details>


### [68] [GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance](https://arxiv.org/abs/2601.19785)
*Haozhi Zhu,Miaomiao Zhao,Dingyao Liu,Runze Tian,Yan Zhang,Jie Guo,Fenggen Yu*

Main category: cs.CV

TL;DR: GeoDiff3D提出一种无需严格多视一致、以粗几何为锚点并结合几何约束2D扩散参考的自监督3D场景生成框架，兼顾结构一致性与高频细节，训练高效、对标注依赖低，较现有方法在复杂场景中更稳健与高质。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成/重建方法要么依赖大规模有标注数据，要么结构建模薄弱，导致复杂场景中出现结构伪影、几何不一致和高频细节缺失；行业对快速迭代、高保真和门槛低的3D内容需求强烈，需要一种更高效、鲁棒、弱监督的方案。

Method: 1) 用粗几何（如体素/网格）作为结构锚点；2) 训练一个受几何约束的2D扩散模型生成纹理丰富的参考图像，即使这些参考不严格多视一致；3) 提出体素对齐的3D特征聚合，将多参考的2D特征对齐到3D体素空间；4) 设计双重自监督（如基于重投影/重渲染的一致性与几何先验约束）以维持场景连贯与细节；5) 整体在自监督/弱监督设定下低算力训练，实现快速高质量生成。

Result: 在多类复杂与具有挑战性的场景上，相比现有间接2D→3D与直接3D生成基线，GeoDiff3D在结构一致性、高频细节、泛化能力和鲁棒性上均显著提升，同时训练成本更低、推理更快。

Conclusion: GeoDiff3D提供了一条实用的自监督3D场景生成路线：以粗几何稳结构，以几何约束扩散供纹理参考，并通过体素对齐聚合与双自监督保持全局连贯与细节，减少对标注和算力依赖，适合高效可及的3D内容生产。

Abstract: 3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.

</details>


### [69] [Diffusion for De-Occlusion: Accessory-Aware Diffusion Inpainting for Robust Ear Biometric Recognition](https://arxiv.org/abs/2601.19795)
*Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 研究评估将扩散式耳部修复（inpainting）作为预处理，缓解耳饰/耳机等遮挡对基于Transformer的耳部识别性能的影响；结果表明该方法在多数据集与不同ViT配置下均能提升识别表现。


<details>
  <summary>Details</summary>
Motivation: 耳饰、耳机等在非受控拍摄中常造成耳部局部遮挡，显著伤害耳生物识别的稳定性与鲁棒性。现有识别模型（尤其Transformer）对局部结构破坏较敏感，亟需一种在不改变主识别框架的前提下，恢复关键耳部解剖结构、提升匹配可靠性的预处理方案。

Method: 提出并评估一种基于扩散模型的耳部图像修复流程：输入耳图与自动生成的配饰遮挡掩膜，扩散模型合成缺失像素，重建解剖合理、几何连续的耳部区域（如耳轮、对耳轮、耳甲与耳垂）。随后将修复后的图像输入多种视觉Transformer（不同补丁尺寸）进行识别评测，并在多个基准数据集上对比有/无预处理的表现。

Result: 在多种ViT架构与patch大小、多个基准数据集上，扩散式修复作为预处理能够缓解配饰遮挡所致的性能下降，整体提升识别准确率（或相关识别指标）。

Conclusion: 扩散模型的耳部inpainting可作为通用、有效的预处理模块，重建关键耳部结构、改善Transformer耳识别在遮挡场景下的鲁棒性与总体性能。

Abstract: Ear occlusions (arising from the presence of ear accessories such as earrings and earphones) can negatively impact performance in ear-based biometric recognition systems, especially in unconstrained imaging circumstances. In this study, we assess the effectiveness of a diffusion-based ear inpainting technique as a pre-processing aid to mitigate the issues of ear accessory occlusions in transformer-based ear recognition systems. Given an input ear image and an automatically derived accessory mask, the inpainting model reconstructs clean and anatomically plausible ear regions by synthesizing missing pixels while preserving local geometric coherence along key ear structures, including the helix, antihelix, concha, and lobule. We evaluate the effectiveness of this pre-processing aid in transformer-based recognition systems for several vision transformer models and different patch sizes for a range of benchmark datasets. Experiments show that diffusion-based inpainting can be a useful pre-processing aid to alleviate ear accessory occlusions to improve overall recognition performance.

</details>


### [70] [Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision](https://arxiv.org/abs/2601.19798)
*Zhixiang Wei,Yi Li,Zhehan Kan,Xinghua Jiang,Zuwei Long,Shifeng Liu,Hongze Shen,Wei Liu,Xiaoyu Tan,Haojia Lin,Yubo Zhu,Qianyu Li,Di Yin,Haoyu Cao,Weibo Gu,Xin Li,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Mingkong Tang,Shuangyin Liu,Lexiang Tang,Haodong Lin,Junru Lu,Jiarui Qin,Lingfeng Qiao,Ruizhi Qiao,Bo Ke,Jianfeng He,Ke Li,Yangning Li,Yunhang Shen,Mengdan Zhang,Peixian Chen,Kun Yin,Bing Liu,Yunfei Wu,Huang Chen,Zhongpeng Cai,Xiaotian Li*

Main category: cs.CV

TL;DR: Youtu-VL提出将视觉信息从“输入条件”变为“预测目标”，用统一自回归监督同时预测视觉token与文本，从而保留更细粒度的视觉细节，并在通用多模态与视觉中心任务上取得有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM训练偏向文本主导：视觉仅作为条件，不作为监督目标，导致细粒度视觉信息在建模中被弱化，产生粗粒度理解与推理能力不足的问题。

Method: 提出Vision-Language Unified Autoregressive Supervision (VLUAS)：将视觉token并入生成序列，与文本一同接受自回归监督，实现“vision-as-target”。在同一框架下扩展到视觉中心任务，使标准VLM无需任务特定模块即可处理检测/分割等。核心是统一的序列化表示与自回归训练目标，对视觉与语言同时预测。

Result: 在广泛实验中，Youtu-VL在通用多模态任务（如VQA、描述等）与视觉中心任务上均达到具有竞争力或领先的结果，显示出更好的细粒度视觉保持能力与任务泛化。

Conclusion: 将视觉信号纳入自回归预测目标能缓解文本偏置，提升细粒度视觉理解，并以统一框架支持多类任务，为构建更通用的视觉智能体奠定基础。

Abstract: Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.

</details>


### [71] [Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering](https://arxiv.org/abs/2601.19821)
*Kun Li,Michael Ying Yang,Sami Sebastian Brandt*

Main category: cs.CV

TL;DR: 提出QSTar与QCR两大模块，通过问题引导在空间-时间-频率三域交互来强化音视问答，多项基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AVQA多偏重视觉，音频仅作补充，且问题文本往往只在最后融合，导致跨模态对齐与关键证据提取不足。需要一种能在特征学习早期利用问题线索，并充分挖掘音频频域特性的机制。

Method: 1) QSTar：以问题为引导，在空间、时间与频率三域进行多层交互与选择性聚合，突出与问题相关的视听要素，尤其利用音频频域特征以区分声源与事件模式。2) QCR：受提示式学习启发，构造问题上下文指引模块，动态调节注意力，使模型聚焦语义相关的音频与视觉区域/片段。整体在多基准上以预训练特征为底座，端到端训练。

Result: 在多个AVQA基准上，相较于音频QA、视觉QA、视频QA及现有AVQA方法取得显著提升（未给出具体数字），验证了在三域交互与问题引导下的有效性。

Conclusion: 问题引导的空间-时间-频率交互与上下文推理能显著增强视听理解，克服以往对音频与问题信息利用不足的问题；方法通用，可作为AVQA的新框架。

Abstract: Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.

</details>


### [72] [HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation](https://arxiv.org/abs/2601.19849)
*Haya Alyoussef,Ahmad Bdeir,Diego Coello de Portugal Mecke,Tom Hanika,Niels Landwehr,Lars Schmidt-Thieme*

Main category: cs.CV

TL;DR: 提出HexFormer：在注意力中用指数映射聚合的双曲几何版ViT/混合模型，跨数据集优于欧氏基线与既有双曲ViT，且训练梯度更稳定、对warmup不敏感。


<details>
  <summary>Details</summary>
Motivation: 多模态数据常具层级与关系结构，欧氏空间难以有效建模；双曲几何天然契合此类结构，但如何将其有效融入视觉Transformer并保持训练稳定与精度仍未解决。

Method: 设计两种模型：纯双曲ViT（HexFormer）与双曲编码器+欧氏线性分类头的混合版（HexFormer-Hybrid）。核心是在注意力聚合中以指数映射聚合（非简单质心平均），以获得更精确、稳定的表示；并分析训练中的梯度稳定性与对warmup的敏感性。

Result: 在多个数据集上，两种模型均优于欧氏基线和先前双曲ViT，混合版总体最好。实验显示双曲模型具有更稳定梯度、对warmup策略更不敏感。

Conclusion: 将双曲几何引入ViT可提升精度与训练稳定性；简单机制如指数映射聚合即可带来显著实际收益，混合架构尤为有效。

Abstract: Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.

</details>


### [73] [EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning](https://arxiv.org/abs/2601.19850)
*Binzhu Xie,Shi Qiu,Sicheng Zhang,Yinqiao Wang,Hao Xu,Muzammal Naseer,Chi-Wing Fu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出EgoHandICL：面向第一人称视角的3D手部重建的ICL框架，通过示例检索、ICL专用tokenizer与MAE架构，在ARCTIC与EgoExo4D上超越SOTA并具备更强泛化与下游交互推理增益。


<details>
  <summary>Details</summary>
Motivation: 现有方法面对第一人称（自我视角）中手部重建的深度歧义、自遮挡及手物交互复杂性，依赖扩大数据或辅助线索，但对未知场景泛化差。作者希望用ICL增强语义对齐与视觉一致性，提升在困难条件下的鲁棒性与泛化。

Method: 1) 用VLM引导的互补示例检索，构建与当前输入相关的多模态上下文；2) 设计面向ICL的多模态tokenizer，将图像与文本/上下文有效编码；3) 采用MAE式架构，结合手部先验的几何与感知损失进行训练；并将重建手作为视觉提示用于下游HOI推理。

Result: 在ARCTIC与EgoExo4D基准上稳定超越SOTA；在真实场景中表现出良好泛化；将重建的手作为视觉提示可提升EgoVLM在手物交互推理上的表现。

Conclusion: ICL范式可显著提升自我视角3D手部重建的语义一致性和鲁棒性；结合VLM引导的示例检索、ICL tokenizer与MAE训练目标带来全面性能提升，并对下游交互理解有正向作用。

Abstract: Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL

</details>


### [74] [SONIC: Spectral Oriented Neural Invariant Convolutions](https://arxiv.org/abs/2601.19884)
*Gijs Joppe Moens,Regina Beets-Tan,Eduardo H. P. Pooch*

Main category: cs.CV

TL;DR: 提出SONIC：一种连续频域、方向选择的卷积参数化，实现全局感受野、跨分辨率自适应；在多任务上以更少参数匹配/超越CNN与ViT并具更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CNN受限于局部卷积核与深度才能覆盖长程依赖；ViT虽具全局连接但缺乏空间归纳偏置、依赖位置编码且受初始patch大小限制。需要既具结构化归纳偏置又能全局建模的表示。

Method: 以连续谱域参数化卷积算子：用少量共享、方向选择的组件在完整频域上定义平滑响应（SONIC），形成全局感受野，并随分辨率自然适配；即“谱导向的神经不变卷积”。

Result: 在合成基准、大规模图像分类与3D医学数据上，SONIC对几何变换、噪声、分辨率变化更鲁棒，且以数量级更少的参数匹配或超越CNN、注意力和既有谱方法。

Conclusion: 连续、具方向意识的频域参数化为传统空间/谱算子的可扩展替代方案，兼具全局性与结构化偏置，带来性能与参数效率优势。

Abstract: Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.

</details>


### [75] [VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction](https://arxiv.org/abs/2601.19887)
*Dominic Maggio,Luca Carlone*

Main category: cs.CV

TL;DR: VGGT-SLAM 2.0 是一个实时、仅 RGB 输入、前馈式的 SLAM 系统，相比前代在去除高维漂移与平面退化、图优化设计、无监督相机内参歧义处理、以及基于模型注意力的免训练图像检索验证和闭环方面显著改进；在多种场景与TUM基准上取得更高精度（相对 VGGT-SLAM 降低约23%位姿误差），并能在 Jetson Thor 上实时运行与扩展到开放集目标检测。


<details>
  <summary>Details</summary>
Motivation: 解决 VGGT-SLAM 的两个核心问题：1) 由于子图增量对齐导致的15自由度高维漂移和在平面场景中的退化；2) VGGT 在未知相机内参下的重建歧义。此外，提升闭环鲁棒性与召回率、减少误匹配，并验证系统在真实机器人和多样场景下的实用性。

Method: - 设计新的因子图（factor graph）以从系统中去除15-DoF 漂移与平面退化，同时处理未知内参导致的重建歧义；
- 分析 VGGT 的注意力层，发现其中一层可直接用作图像检索的匹配验证，不需额外训练，用于拒绝误匹配并完成更多闭环；
- 在地面机器人（Jetson Thor）上实现在线实时运行，并展示可无缝适配开放集目标检测。

Result: - 在室内公寓、办公室、4200 平方英尺谷仓等多样环境中稳定运行；
- 于 TUM 数据集达到最高精度，相较 VGGT-SLAM 位姿误差降低约23%；
- 实时性能验证：在 Jetson Thor 上在线运行；
- 展示了开放集目标检测适配的可行性。

Conclusion: 通过重新设计因子图和利用模型内部注意力进行免训练的检索验证，VGGT-SLAM 2.0 在鲁棒性、闭环能力与精度上显著超越前代，并具备良好的实时性与可扩展性；代码将于发表时开源。

Abstract: We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.

</details>


### [76] [DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding](https://arxiv.org/abs/2601.19898)
*Shubham Patle,Sara Ghaboura,Hania Tariq,Mohammad Usman Khan,Omkar Thawakar,Rao Muhammad Anwer,Salman Khan*

Main category: cs.CV

TL;DR: 提出DuwatBench：一个面向阿拉伯书法的多模态评测基准，含1272样本/约1475词，覆盖6种书体并带句级标注，显示主流多模态模型在书法化文本上显著失效。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在一般阿拉伯文本上已进步，但对艺术化、风格化的阿拉伯书法识别与对齐能力研究不足；现有系统难以处理复杂笔画、致密连写与风格变体，缺乏标准化评测数据。

Method: 构建并整理DuwatBench数据集：六类经典与现代阿拉伯书法风格，共1272样本、约1475唯一词，并提供句级检测标注；用该基准系统评测13个领先的阿拉伯及多语言多模态模型，比较其在干净文本与书法化样本上的表现，检验视觉-文本对齐与识别鲁棒性。

Result: 模型在清洁文本上表现良好，但在书法变体、艺术畸变、复杂连笔与精确视觉-文本对齐方面显著退化，普遍难以可靠识别与定位书法文本。

Conclusion: DuwatBench揭示当前多模态模型对阿拉伯书法处理的显著短板，并提供公开数据与评测工具以推动文化语境下的多模态研究与阿拉伯语言公平纳入。数据集与评测套件已在Hugging Face与GitHub公开。

Abstract: Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and showed that while they perform well on clean text, they struggle with calligraphic variation, artistic distortions, and precise visual-text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of the Arabic language and visual heritage in AI systems, and support continued progress in this area. Our dataset (https://huggingface.co/datasets/MBZUAI/DuwatBench) and evaluation suit (https://github.com/mbzuai-oryx/DuwatBench) are publicly available.

</details>
