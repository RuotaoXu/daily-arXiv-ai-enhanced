<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 204]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出一种在智能农业中对抗光度扰动的可解释VLA模型，基于OpenVLA-OFT，内置Evidence-3检测与解释模块，在对抗条件下显著降低动作预测误差（当前动作L1降21.7%，下一步动作L1降18.4%）。


<details>
  <summary>Details</summary>
Motivation: 智能农业常用RGB感知+机械臂控制，但易受色相、光照、噪声等光度扰动与对抗攻击影响，导致感知失真、动作决策错误。现有方法缺乏在对抗环境中的鲁棒性与对原因-后果的可解释性。

Method: 在OpenVLA-OFT框架上引入可解释的Vision-Language-Action模型：集成Evidence-3模块以检测输入中的光度扰动（色相、亮度、噪声等），并以自然语言解释其成因与对动作选择的影响；在对抗情景下训练/评估以提升鲁棒性与可解释性，度量采用当前与未来动作的L1误差。

Result: 相较基线，当前动作L1误差降低21.7%，下一步动作L1误差降低18.4%，显示在对抗光度扰动下的动作预测精度与可解释性提升。

Conclusion: 结合扰动检测与语言解释的对抗鲁棒VLA可有效缓解智能农业系统在光度对抗下的失效问题，并在动作预测上取得显著改进。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [2] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: 提出Temporal-Anchor3DLane，在单目3D车道线检测中通过改进损失、多帧时序特征融合（轻量LSTM）与时序一致性训练，大幅缓解回归异常、几何监督弱、损失权衡难与时序不稳问题；在OpenLane上F1提升+6.2，并获得更平滑的时序轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D车道检测（如Anchor3DLane）虽强但仍受深度歧义、遮挡与跨帧不稳影响，且对异常回归敏感、全局曲线几何监督弱、损失项难权衡、对时序连续性利用不足，需要在不增加传感器或大幅扩模的前提下提升鲁棒性与时序平滑度。

Method: 在Anchor3DLane基础上三方面改进：1) 多任务损失改良：回归用Balanced L1与Chamfer点集距离，结合基于不确定性的损失加权；分类与可见性采用Focal与Dice成分，提升难例与类不平衡表现。2) 轻量Temporal LSTM Fusion：按锚点聚合跨帧特征，以LSTM替代更重的Transformer式时序融合。3) ESCOP风格训练细化：将曲线级监督与时序一致性耦合，促进几何与时间维度的联合优化。

Result: 在OpenLane基准上，F1提升+6.2，并呈现更平滑的时序车道轨迹，显示在不增加传感器与模型规模下即能显著增强稳健性。

Conclusion: 小规模架构与损失设计的针对性改进即可显著提升单目3D车道检测的鲁棒性与时序稳定性；LSTM式轻量时序融合与不确定性加权、多几何监督是关键。

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [3] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 在资源受限的提格雷地区，为仙人掌果病害诊断构建离线优先系统：比较轻量CNN、EfficientNet-Lite1与MobileViT-XS；MobileViT-XS在准确率上最佳，轻量CNN在部署效率上最优；实现本地化离线推理，提升粮食安全诊断可及性。


<details>
  <summary>Details</summary>
Motivation: 当地超过八成人口依赖农业，但基础设施中断导致难以及时获得专家病害诊断。需要在低算力、离线与多语言环境中，为本土作物（仙人掌果）提供可靠的自动识别模型与应用。

Method: 新构建3,587张田间图像数据集，分三类核心症状；在离线边缘部署约束下，基准测试三类移动高效模型：自研轻量CNN、EfficientNet-Lite1、以及CNN-Transformer混合的MobileViT-XS；在系统中虽然有马铃薯、苹果、玉米模块，但本研究单独评估仙人掌果，以分析注意力敏感性与归纳偏置迁移；在ARM兼容设备上以Flutter应用实现本地化（提格利尼亚语与阿姆哈拉语）离线推理。

Result: 存在明显的精度-资源帕累托权衡：EfficientNet-Lite1测试准确率90.7%；轻量CNN测试准确率89.5%，但推理仅42 ms、模型4.8 MB，部署画像最佳；MobileViT-XS平均交叉验证准确率97.3%，显示MHSA的全局推理优于仅依赖局部纹理的CNN来区分虫害簇与二维真菌斑。

Conclusion: 在后冲突、低资源边缘环境下，兼顾精度与部署成本的模型选择至关重要。MobileViT-XS提供最高精度，轻量CNN最适于超低算力部署；多语言、离线Flutter应用在Cortex-A53设备上可行，提升本地农民病害诊断的包容性与可达性。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [4] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 他们提出用于全球小麦全语义分割竞赛的方案：以SegFormer(MiT-B4)为核心，结合两阶段混合训练、强数据增强与迭代式师生自训练循环，逐步提升精度并更充分利用数据，在开发与测试阶段均取得有竞争力表现。


<details>
  <summary>Details</summary>
Motivation: 竞赛场景下标注成本高、域间差异大且训练数据有限，单纯监督学习难以充分利用未标注或弱标注数据；需要一种能够高效利用数据、提升泛化与精度的训练框架。

Method: 构建系统化自训练框架：以SegFormer(MiT-B4)为主干；采用两阶段混合训练策略（先/后不同监督或伪标注权重与策略组合）；配合大规模/多样的数据增强；通过迭代的教师-学生循环产生并精炼伪标签，使学生模型逐步提升，进而更新教师模型。

Result: 在竞赛的开发集与测试集上均获得具有竞争力的分割性能（具体指标未给出，但相较基线有显著提升）。

Conclusion: 自训练+两阶段混合训练+强数据增强，与SegFormer(MiT-B4)结合能有效提高小麦语义分割的精度与数据利用效率，在实际竞赛数据上验证了有效性。

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [5] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: 论文比较了零样本的SAM3与经微调的YOLO11（nano/medium/large）在MinneApple密集果园数据集上的实例分割表现。YOLO在IoU=0.15下F1为68.9/72.2/71.9%，SAM3为59.8%；但跨IoU阈值YOLO性能急剧下降（48–50点），SAM3仅降约4点，显示SAM3掩膜边界更稳定而YOLO检测更完整。作者强调评价时IoU选择会显著影响结论（差距可被夸大至30%），并给出开源代码与评测流程。


<details>
  <summary>Details</summary>
Motivation: 实例分割领域出现了两类范式：经任务微调的专用模型与可零样本的通用基础模型。实际场景（高密度、遮挡）下两者谁更优缺乏系统对比；同时，评价指标（尤其IoU阈值）对结论的敏感性尚未被充分量化。

Method: 在MinneApple（670张、28,179个苹果实例，高密度且遮挡）上，对比零样本SAM3与微调后的YOLO11三种规模。系统遍历不同IoU阈值，报告F1等指标，并分析跨阈值的性能衰减以衡量边界稳定性。提供统一开源评测管线。

Result: 在被认为“合适”的IoU=0.15下，YOLO11 nano/medium/large的F1为68.9/72.2/71.9%，SAM3为59.8%。但随IoU提升，YOLO的F1下降48–50个百分点，SAM3仅约4点；不同IoU选择可使模型间差距被夸大至30%。

Conclusion: YOLO11（微调）在检测完整性与召回上占优，适合需要高找全率的密集实例场景；SAM3（零样本）在掩膜边界精度与跨阈值稳定性上更强，更具泛化与鲁棒性。评测应明确并合理设定IoU阈值；选择模型应依据应用对召回与边界精度的权衡。开源库与流程可复现实验并指导方法选择。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [6] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: 提出 mmWeaver：用隐式神经表示与超网络，在环境与动作条件下生成逼真 mmWave 复杂 I/Q 信号；较仿真更快、更小且助力下游任务。


<details>
  <summary>Details</summary>
Motivation: mmWave 雷达在行为识别、姿态估计等任务上需要大量多样且与环境相关的信号数据，但实际采集昂贵、仿真计算量大且难以覆盖多场景；同时 mmWave 信号复杂、稀疏、高维，传统显式建模与渲染效率低，难以保留相位等关键信息。需要一种既逼真又高效、可适配不同环境与人体动作的信号生成与增强方法。

Method: 将复杂 mmWave I/Q 信号视为连续函数，用隐式神经表示（INR）建模并实现多分辨率重建；通过超网络根据环境上下文（由RGB‑D提取）与人体动作特征（由MotionGPT文本到姿态生成）动态生成INR参数，实现条件化、可泛化的信号合成；以语义与几何先验为条件，生成保持相位信息的多尺度信号，并实现最高49×压缩。

Result: 在合成质量上达成 complex SSIM 0.88、PSNR 35 dB，优于现有方法；下游任务中，活动识别准确率提升至多7%，人体姿态估计误差降低至多15%；运行速度较物理仿真快6–35倍，并显著压缩存储（至多49×）。

Conclusion: mmWeaver 能以环境与动作条件高效生成逼真 mmWave 复杂信号，兼顾相位与多分辨率特性，既提升数据集多样性与现实性，也实证改进下游感知任务表现，较传统仿真具速度与压缩优势，适合作为数据增强与快速场景适配工具。

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [7] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: 提出“Hot Hém”GeoAI工作流，用街景+遥感+机器学习在胡志明市估算并上图步行热暴露，并把热度集成到路径规划。


<details>
  <summary>Details</summary>
Motivation: 密集热带城市行人面临高温健康风险，而常规路径算法忽视街区尺度的热环境差异，需要一种可在城市路网层面量化并用于导航的热暴露估计方法。

Method: 构建空间数据科学流水线：收集Google Street View图像并做语义分割，结合遥感数据；在若干行政坊(phường)用GSV训练集训练两套XGBoost模型来预测地表温度(LST)；将模型以拼贴方式部署到通过OSMnx提取的全市步行网络节点上，得到每节点热度并用于热感知路径规划。

Result: 模型可在城市尺度对步行网络节点赋予LST估计，实现热负荷可视化与热感知路由；表明以街景语义与遥感特征能较好解释与预测微尺度热差异。

Conclusion: Hot Hém为识别与理解城市走廊热不均提供基础设施级工具，可支持发现异常高温路段与制定降温干预与更安全的行走路线。

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [8] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 论文公开了利用无人机采集的城市异质性、面域交通条件下的微观车辆轨迹数据（MVT），30 fps，含位置、速度、纵/横向加速度与车辆类型，覆盖印度首都辖区6处路段，并经多种方法验证，可用于建模、仿真与安全/行为研究。


<details>
  <summary>Details</summary>
Motivation: 传统路侧视频在混合密集交通中受遮挡、视角受限与不规则机动等问题影响，难以获取高质量轨迹数据；需要能在复杂城市环境中可靠捕捉时空动力学的开放数据来支持模型开发与验证。

Method: 使用无人机自顶向下拍摄异质性、面域交通，借助Data from Sky平台提取轨迹数据；以人工计数、区间平均速度与探测车轨迹进行一致性验证；形成包含时标位置、速度、纵横向加速度、车辆分类的30 fps数据集，并进行探索性行为分析。

Result: 构建了来自印度国家首都区6个中段位置、不同交通组成与密度的开放MVT数据集；验证显示与人工与外部测度一致；探索性结果揭示车道保持偏好、速度分布与横向机动等关键行为特征。

Conclusion: 公开的数据集为全球研究者在面域混合交通条件下开展仿真建模、安全评估与行为研究提供了高分辨率、经验证的经验基础，有助于开发与校验更贴近复杂城市交通的模型。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [9] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 提出RIO-VQA任务与RIO-Bench基准，评测并提升LVLM在需要“读或忽略”文本的选择性场景中的稳健性，发现现有方法在抗排版攻击与读文能力间失衡，并给出数据驱动的自适应防御。


<details>
  <summary>Details</summary>
Motivation: LVLM易受排版攻击：图中误导性文字会压过视觉理解。现有评测/防御多聚焦物体识别并鼓励忽略文字，但真实场景常需联合理解物体与文字（如识别行人且读取交通标志）。需要一种能在语境下决定何时读文字、何时忽略的框架与评测。

Method: 1) 定义Read-or-Ignore VQA（RIO-VQA）任务：模型需基于上下文选择性使用图中文本；2) 构建RIO-Bench：为每张真实图像提供同场景反事实样本，仅改变文字内容与问题类型，形成成对“应读/应忽略”设置与标准化评测协议；3) 基于该基准系统评测强LVLM与现有防御；4) 设计利用RIO-Bench的数据驱动防御，学习自适应的选择性读文策略，区别于以往非自适应、倾向忽略文本的防御。

Result: 在RIO-Bench上，强LVLM与现有防御都无法同时兼顾抗排版攻击的稳健性与读文能力，表现出明显权衡失衡。基准促使训练出能根据情境自适应读/忽略文本的模型，优于过去的静态忽略式防御。

Conclusion: 现有评测范围与真实需求存在根本错位。RIO-VQA与RIO-Bench为选择性读文提供了任务与标准化评测，并启用自适应防御路径，指向更可靠的LVLM。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [10] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: CLARGA 是一种可扩展、通用的多模态融合架构，基于图注意力在样本级构建模态间交互，并结合监督与对比学习目标，实现高效、鲁棒的多模态表示学习，在多领域数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法常受限于特定模态/模态数量、融合复杂度高、对缺失模态与噪声不鲁棒，难以在多任务与多领域通用。作者希望提出一种可插拔、对模态数目与类型友好、在样本级自适应建模模态关系、且训练上增强跨模态一致性的通用融合框架。

Method: 提出 CLARGA：在每个样本上对各模态表示构建注意力加权图，通过多头图注意力网络进行消息传递与融合；图构建具子二次复杂度，随模态数增长更高效；引入可学习掩码以适配缺失模态；训练使用监督任务损失与 InfoNCE 对比损失的混合目标，提高跨模态一致性与抗噪性。

Result: 在跨金融、人机交互、多媒体分类、情感计算等 7 个数据集上，CLARGA 在多模态表示学习任务中持续优于基线、SOTA 与消融版本；额外实验显示其对缺失输入更鲁棒，并能在小众任务上表现出色。

Conclusion: CLARGA 是一种易于集成的通用多模态融合模块，能在不同任务与模态设置下，实现高效、鲁棒且自适应的表示学习，具备良好的可扩展性和实际应用潜力。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [11] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 研究利用智能手机被动记录的“自然互动”视频中笑容强度，作为主观幸福感的客观行为指标，发现其与国家幸福调查和日重构法结果高度相关，并与日常体力活动与光照水平正相关，显示被动感知可用于大规模研究情感动态。


<details>
  <summary>Details</summary>
Motivation: 传统幸福感测量依赖自报告，易受回忆偏差和负担影响，难以捕捉日常生活中的真实情感波动；需要一种可扩展、生态效度高、客观的行为指标。

Method: 对233名受试者一周内被动采集的405,448段智能手机视频进行分析，利用深度学习量化笑容强度；检验笑容在昼夜与一周周期的模式，并与国家幸福调查和日重构法进行相关；将每日平均笑容强度与体力活动、光照与手机使用进行关联分析（回归，报告Beta与95%CI）。

Result: 笑容强度呈现清晰的昼夜节律与周内模式；周内模式与国家幸福调查相关r=0.92，昼夜节律与日重构法相关r=0.80；更高每日平均笑容强度与更多体力活动（Beta=0.043, 95%CI [0.001, 0.085]）与更高光照（Beta=0.038, 95%CI [0.013, 0.063]）显著相关；与手机使用无显著关系。

Conclusion: 自然交互中的笑容可作为积极情感的可扩展客观行为指标；被动智能手机感知方法具备较高生态效度，能用于群体尺度上研究情感行为动态。

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [12] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 提出MPath：用视觉前缀提示将WSI特征接入BioBART，冻结语言模型，实现轻量级病理报告生成，在RED 2025挑战中获第4名。


<details>
  <summary>Details</summary>
Motivation: 从整张数字病理切片直接生成临床一致的报告很难：形态多样性大、叙述结构复杂，且端到端视觉-语言预训练代价高、数据匮乏与不稳定。需要一种数据高效、可扩展且可解释的方法。

Method: 构建轻量多模态框架MPath：利用WSI基础模型特征（CONCH+Titan）作为视觉嵌入，通过学习到的“视觉前缀”提示机制注入到预训练生物医学语言模型BioBART中。采用紧凑投影模块对齐模态，冻结语言骨干以提升稳定性和样本效率，无需大规模端到端联合预训练。

Result: 在RED 2025 Grand Challenge数据集上开发与评测，尽管提交次数受限，MPath在测试第二阶段排名第4，表现具有竞争力。

Conclusion: 基于提示的多模态条件化是一种可扩展、稳健、可解释的病理报告生成策略；通过复用WSI与语言基础模型并采用视觉前缀，可在有限资源下取得强性能。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [13] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: FloraForge提出一个由LLM协助的流程，用自然语言迭代细化可参数化的3D植物模型脚本，生成连续的B样条层级表示，并能无缝转成网格用于可视化与定量分析，降低植物科学家的几何建模门槛。


<details>
  <summary>Details</summary>
Motivation: 现有学习式重建依赖大量物种特定数据、可编辑性差；程序化建模虽可控但需深厚几何/规则编写经验，不利于植物科学家进行精确、可分析的3D建模。

Method: 构建LLM协作的“Plant Refinements (PR)”工作流：用自然语言迭代修改Python脚本，脚本生成带植物学约束的层级B样条曲面（显式控制点与参数化形变函数）。以“Plant Descriptor (PD)”人类可读文件驱动；可高精度镶嵌为多边形网格；产生两类输出（可视化网格与带参数元数据的网格）。并在玉米、大豆、绿豆上通过手动PD微调拟合到点云数据。

Result: 在多个作物（玉米、大豆、绿豆）上展示：能将程序化模型拟合实测点云，输出既适合渲染又携带参数用于定量分析的网格；实现模板创建、连续表示与直接参数控制的结合。

Conclusion: 该框架以LLM辅助实现可编辑、可参数控制且数学上连续的植物几何建模，兼顾表型分析与渲染，显著降低几何建模门槛并保持严谨性，促进植物科学应用。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [14] [TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder](https://arxiv.org/abs/2512.11926)
*Qinghao Meng,Chenming Wu,Liangjun Zhang,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出联合点云补全与检测框架，通过跨网络Transformer上采样模块TransBridge和动态-静态重建DSRecon，为补全网络生成致密监督并将隐式补全特征注入检测网络，在不增加推理成本的前提下显著提升稀疏远距区域的3D检测mAP（+0.7~1.5，最高+5.78）。


<details>
  <summary>Details</summary>
Motivation: 远距区域LiDAR点稀疏导致3D检测漏检和定位不准。现有“增密/补全→检测”方案常增加计算/时延或与检测特征耦合不足，难在端到端框架中兼顾精度与成本。需要一种在保持开销不变的同时提升稀疏区域检测特征的联合方案。

Method: 1) 联合补全-检测端到端框架。2) TransBridge：Transformer式上采样块，融合检测与补全网络的多尺度特征，建立通道与空间关系，将补全的高分辨率隐式特征传递给检测分支。3) DSRecon：动态-静态重建策略，为补全网络构造致密点云监督（结合场景静态先验与动态对象重建）。4) 利用Transformer建模通道与空间关联，获得高分辨率补全特征图，并与检测特征对齐融合。5) 在nuScenes与Waymo上与多种单/双阶段检测器兼容集成。

Result: 在nuScenes与Waymo上，框架对多种方法的一致增益：端到端3D检测mAP提升约0.7–1.5；在两阶段检测框架上最高提升5.78点，显示良好泛化与可插拔性。

Conclusion: 通过TransBridge将补全隐式信息桥接到检测，并以DSRecon提供致密监督，可在不增加推理成本下有效改善稀疏远距区域的3D检测性能；方法通用、可与现有检测器集成并带来稳定提升。

Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.

</details>


### [15] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: 作者提出用扩散模型MONET从明场图像虚拟生成Cell Painting通道，解决传统方法耗时且需固定导致无法观察动态的问题；在大规模数据下效果随规模提升，并用一致性架构生成无法实采的时间序列，还表现出一定的跨细胞系/成像协议迁移与“情境内学习”能力。虚拟染色作为实体染色的补充工具，助力新型生物学工作流。


<details>
  <summary>Details</summary>
Motivation: 物理Cell Painting需要复杂湿实验流程、人工成本高且需化学固定，无法研究细胞动态；研究者希望在不牺牲可解释形态对比度的前提下，利用现成明场成像，通过学习映射来获得等价的多通道形态信息，并实现时间分辨分析与更高通量的筛选。

Method: 构建大规模配对数据集，训练扩散模型MONET以从明场输入预测Cell Painting多通道图。采用一致性（consistency）架构以支持从单帧或短序列生成时间推演（time-lapse）视频，尽管缺乏真实的染色视频监督。评估随模型/数据规模扩展的性能提升，并测试在分布外细胞系与成像协议上的迁移，通过提示式条件和上下文示例实现一定程度的“in-context”适配。

Result: 1) 从明场到虚拟Cell Painting重建质量随模型与数据规模显著提升；2) 一致性架构可生成连贯的时间序列，虽无真实染色视频训练数据；3) 在OOD细胞系与不同成像设置上保持部分泛化，表现出基于上下文的快速适配能力。

Conclusion: MONET可作为虚拟细胞染色工具，缓解实验成本与静态限制，扩展到时间维度并具备一定跨域鲁棒性。其目标是补充而非取代物理Cell Painting，为高通量筛选与活细胞动态研究提供新工作流。

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [16] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: 提出结合情境化Peano扫描(CPS)与证据型隐马尔可夫链(HEMC)的模型HEMC-CPS，用于无监督图像分割；在MPM准则与SEM估计下，较HMC/HMC-CPS取得更优分割，且具备推广至复杂与更广泛空间数据的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统将二维像素经Peano扫描转为一维序列后，用隐马尔可夫链(HMC)做无监督分割，效率高、能与隐马尔可夫场(HMF)媲美。近期的情境化PS（CPS）与证据型HMC（HEMC）分别在捕捉空间上下文与处理不确定性方面带来改进，但尚未被联合。需要一个同时利用上下文与证据理论的不确定性表达的模型，以提升分割质量并保持计算效率。

Method: 将CPS生成的一维上下文序列与HEMC框架融合，提出HEMC-CPS；在贝叶斯框架下采用最大后验模态(MPM)作为分割准则，并用随机期望最大化(SEM)进行无监督参数估计；与HMC-CPS进行比较，在合成与真实图像上评估。

Result: HEMC-CPS在分割精度与稳健性方面优于基于HMC与HMC-CPS的方案；在保持较快计算的同时，更好地处理噪声与不确定性；实验覆盖合成与真实数据均验证有效。

Conclusion: 将上下文敏感的扫描(CPS)与证据型HMC相结合，可在无监督贝叶斯分割中带来一致改进；方法易扩展到三维、多源多分辨率等更复杂图像与更广泛的空间相关数据，不局限于图像分割。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [17] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: 提出DynaPURLS框架，通过多尺度、层次化文本描述与自适应骨架分区的细粒度视觉表征进行对齐，并在推理时动态细化语义以解决零样本骨架动作识别的域移问题，达成新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-SAR多将骨架特征与静态、类别级语义对齐，粒度粗、难以覆盖局部肢体动态，且在见类到未见类迁移时存在显著域移，导致细粒度视觉知识无法有效迁移，需要构建更稳健的视觉-语义对应并在推理阶段适配。

Method: 1) 利用大语言模型生成层次化文本描述，覆盖全局动作与局部关节/部位动态；2) 自适应分区模块按语义对骨架关节进行分组，提取细粒度视觉特征；3) 动态细化模块在推理时以轻量可学习投影将文本特征适配到输入视觉流；4) 以置信度感知、类别均衡的记忆库稳定自训练过程，抑制噪声伪标签的误差传播。

Result: 在NTU RGB+D 60/120与PKU-MMD三大数据集上，显著优于既有方法并刷新SOTA指标。

Conclusion: 多尺度、层次化的视觉-语义对齐结合推理期动态细化与稳定的记忆机制，可有效缓解见/未见域移，提升零样本骨架动作识别的泛化能力。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [18] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

TL;DR: 论文评估在半导体晶圆缺陷分类中，DeiT在小样本与类别不平衡条件下优于多种CNN基线（最高Acc 90.83%、F1 90.78%、收敛更快、少数类更稳健），对预测性维护有帮助。


<details>
  <summary>Details</summary>
Motivation: 半导体制造的预测性维护需要可靠、敏感的缺陷检测。然而实际生产中的数据往往稀缺且类别分布不均，传统CNN在此情形下性能下滑。作者动机是验证数据高效的Transformer（DeiT）在受限数据情境下是否能更好地识别晶圆图缺陷，从而支撑维护决策与成本降低。

Method: 将DeiT应用于晶圆图（wafer map）缺陷分类任务，并与多种CNN（VGG-19、SqueezeNet、Xception、Hybrid）进行对比。评估指标包括分类准确率、F1-score、收敛速度以及对少数类的鲁棒性。实验在数据量有限且类别不平衡的设置下进行，以检验模型的数据效率与泛化能力。

Result: DeiT取得最高准确率90.83%，优于VGG-19(65%)、SqueezeNet(82%)、Xception(66%)、Hybrid(67%)；F1-score达90.78%；训练收敛更快；对少数类缺陷检测更稳健。

Conclusion: 在数据受限与不平衡的半导体晶圆缺陷分类任务中，基于Transformer的DeiT优于传统CNN，具有更高精度与鲁棒性，可用于增强预测性维护策略。

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [19] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: CARI4D 提出一种从单目RGB视频恢复度量尺度、时空一致的人-物体4D交互的类别无关方法，结合多模型预测假设选择、可微渲染对比优化与物理接触约束，显著优于现有方法并具零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 单目RGB下的人-物体交互重建受未知对象与人体信息、深度歧义、遮挡与复杂运动影响，难以获得三维与时间上的一致结果。以往方法依赖真实对象模板或限定对象类别，限制了实用性与泛化。

Method: 1) 提出姿态假设选择算法，鲁棒融合多种基础模型的独立预测；2) 采用“渲染-对比”的联合优化框架，在学习到的可微渲染器下，对人体与物体进行空间、时间及像素对齐的联合细化；3) 基于接触推理与物理约束进行进一步精化，确保交互与动力学合理；4) 全流程实现单目输入下的度量尺度重建与类别无关建模。

Result: 在分布内数据集上重建误差降低38%，在未见类别/数据上降低36%，展示出强泛化，支持对野外互联网视频的零样本应用。

Conclusion: CARI4D实现从单目视频恢复类别无关、度量尺度、时空一致的人-物体4D交互，并在精度与泛化上显著超越既有方法，为人理解、游戏与机器人学习等场景提供实用方案；代码与模型将开源。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [20] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 提出V-REX评测套件，用链式问题(CoQ)刻画并量化多步视觉探索推理的“规划+跟随”能力，覆盖多域场景，揭示VLM在多步推理中的差距与改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多在基准中回答单轮、明确目标的问题，但在真实应用中的开放式、多轮视觉探索与推理表现欠佳；同时，中间探索路径难以评估，缺乏细粒度、可量化的评价框架。

Method: 构建V-REX：1) 设计多域、原生需要多步视觉探索的任务；2) 将任务形式化为“问题链”(CoQ)，并将每一步的问题与答案限制在有限选项，以便客观打分；3) 将能力分解为两项：规划(选择合适的问题链)与跟随(按链逐步回答)；4) 制定评测协议，对SOTA专有和开源VLM进行系统测评。

Result: 在多款SOTA模型上观测到一致的扩展趋势（随规模能力提升），但规划与跟随能力存在显著差异，整体多步探索推理仍有明显提升空间；V-REX实现对中间步骤的可靠、细粒度量化分析。

Conclusion: V-REX为多步视觉探索推理提供了标准化基准与评测协议，通过CoQ分解与有限选项机制，既可客观衡量中间过程，又能区分规划与执行能力，为改进VLM多步视觉推理提供方向。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [21] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: 提出本地优先的神经-符号框架“Semantic-Drive”，用开集目标检测+推理型VLM实现自动驾驶长尾语义数据挖掘，在nuScenes上显著优于CLIP并降低风险评估误差，且可在消费级GPU离线运行。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶训练受长尾稀有事件数据匮乏所限；现有以元数据检索或云端VLM为主的方法要么不精确，要么成本高且有隐私问题。需要一种隐私友好、可本地运行、对稀有安全关键事件具有高召回的语义检索/挖掘方法。

Method: 采用两阶段神经-符号流水线：1) 符号落地（Symbolic Grounding）：实时开词表检测器YOLOE锚定关键实体与交互；2) 认知分析（Cognitive Analysis）：本地推理型VLM进行取证式场景理解与语义判定。为抑制幻觉，引入“System 2”对齐策略：多模型“Judge-Scout”一致性机制在推理时进行交叉验证与校正。

Result: 在nuScenes上、针对WOD-E2E分类体系评测，Semantic-Drive召回率0.966（CLIP为0.475），相较单模型将风险评估误差降低40%。全流程在消费级硬件（RTX 3090）离线运行。

Conclusion: Semantic-Drive以本地、隐私友好方式显著提升长尾事件语义挖掘能力，通过神经-符号解耦与一致性推理降低幻觉与风险误差，较现有基线有明显优势并具备实际部署可行性。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [22] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

TL;DR: 提出用于毫米波雷达点云的离散动态图神经网络（DDGNN）与“星形图”表示，实现稀疏、变尺寸点云的人体动作识别，在真实数据上达94.27%准确率，接近视觉骨架的97.25%，且可在树莓派4上高效推理。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达点云稀疏、帧间点数可变，直接套用密集视觉点云的预处理并不适配，导致空间-时序特征提取不足。亟需一种能处理稀疏、变尺寸数据并有效捕捉人类运动空间-时序关系的方法。

Method: 以每帧人为添加一个静态中心点，并将其与同帧及相邻帧的雷达点构建“星形图”，用离散动态图神经网络（DDGNN）在可变规模的图上学习高维相对关系与时序特征；无需重采样或帧聚合。并进行不同DDGNN结构的消融实验与在树莓派4上的推理评估。

Result: 在真实HAR数据集上，方法优于多种基线；总体分类准确率94.27%，接近基于视觉骨架的97.25%。在树莓派4上实现有效推理；同时超越三种近年的雷达特定方法。

Conclusion: 星形图+DDGNN能稳健处理毫米波雷达稀疏、变尺寸点云并提取空间-时序特征，达到接近视觉方法的性能且具边缘设备可用性。消融研究验证了模型设计的合理性。

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [23] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: 该研究在多源卫星数据上用联邦学习做舰船检测，较各自本地训练显著提升精度，接近集中式全量训练，同时强调通信轮数与本地轮次等配置对精度与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 商业卫星影像与舰船标注常涉隐私与敏感信息，难以集中汇聚；同时每个机构数据量有限、分布异构，导致单独训练精度不足。需要一种既保护数据隐私又能整合多方知识以提升检测性能的方法。

Method: 以YOLOv8为基础检测器，将不同卫星数据集作为不同客户端，比较四种联邦优化算法（FedAvg、FedProx、FedOpt、FedMedian）与“本地独训”基线，并与“集中式全量训练”上限作对照。系统分析通信轮数与本地训练epoch等超参对性能与计算的影响。

Result: 所有联邦模型在小数据本地基线之上取得显著精度增益，整体性能接近集中式全量训练；不同联邦算法与超参设置对结果有明显影响。

Conclusion: 联邦学习能在不共享原始数据的前提下有效提升跨卫星数据的舰船检测精度，接近集中式效果；应根据场景合理选择联邦算法与通信/本地训练配置以在精度与效率间取舍。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [24] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 提出一套面向应急场景的烧毁范围（BA）高效分割流程：在SPOT‑6/7高分影像上比较U‑Net与SegFormer，辅以土地覆盖多任务学习与测试时增强（TTA），并结合混合精度等推理优化，兼顾精度与时效。


<details>
  <summary>Details</summary>
Motivation: 现有BA制图多关注事后精度，忽略灾后应急中“时间—资源”受限的可用性。SPOT‑6/7具有超高分与按需获取优势，但在有限训练数据与计算资源下如何快速、稳健地分割BA仍未系统评估。

Method: 构建监督语义分割流程：选用U‑Net与SegFormer在SPOT‑6/7上训练与测试；评价指标为Dice、IoU与推理时延；引入土地覆盖作为辅助任务提升鲁棒性；在推理端采用TTA提升精度，并用混合精度等优化抵消时延开销。

Result: U‑Net与SegFormer在小样本下精度相当；SegFormer资源占用更高，不利于紧急应用；加入土地覆盖多任务后模型更稳健且不增加推理时间；TTA能提升分割精度，但增加推理时延，可通过混合精度等优化部分抵消。

Conclusion: 在应急制图中，轻量的U‑Net更具实用性；多任务学习可“免费”提升鲁棒性；TTA需与推理优化协同使用以平衡精度与时效。整体流程兼顾性能与效率，适合灾后快速BA划定。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [25] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: CreativeVR提出一个面向AI生成与真实视频严重结构伪影的扩散先验引导视频修复框架，含单一“精度旋钮”、时序一致的降质模块与AIGC54评测基准，在强伪影场景达SOTA并在标准VSR上具竞争力，且具实用推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有T2V扩散模型在细粒度结构与时间一致性上脆弱，常出现面部/手部变形、背景扭曲与不稳定运动。传统VR/VSR主要针对合成模糊/降采样，反而固化AIGC伪影；基于扩散先验的修复多聚焦光度噪声，缺少感知-保真权衡的可控性。需要一种既能处理严重结构/时序伪影、又能在标准退化下保持保真的统一方案。

Method: 提出CreativeVR：1) 扩散先验驱动的修复框架，采用deep-adapter并暴露单一“precision knob”控制对输入跟随强度，在保真与创意修复（结构/运动矫正）间平滑权衡；2) 训练期引入“时序一致的降质模块”，用精心设计的变换生成逼真的结构性失败并保持时间一致性；3) 面向评测，构建AIGC54基准，涵盖FIQA、人脸语义与感知指标及多维评分。

Result: 在含严重结构/时序伪影的视频上达SOTA；在标准视频修复基准上表现有竞争力；推理速度实用：单张80GB A100在720p约13 FPS。

Conclusion: CreativeVR通过可控的扩散先验与时序一致降质训练，有效修复AIGC与真实视频中的结构与时间伪影，兼顾保真与感知质量，并提供强泛化与实用吞吐，AIGC54为该领域提供了适配的评测标准。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [26] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出BAgger：一种利用模型自身rollout构建“反向聚合”纠正轨迹的自监督训练方案，缓解自回归视频模型的曝光偏差，提升长时序稳定性与一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型在逐帧预测中因训练-推理分布不匹配（训练用真实上下文、推理用自生成帧）而产生误差累积与质量漂移，现有缓解方法多依赖短步蒸馏或分布匹配损失，可能压制质量/多样性且计算昂贵，需要一种既不依赖大教师也能保持多样性与稳定性的训练方法。

Method: 提出Backwards Aggregation（BAgger）：从模型自身长rollout中构造“纠错轨迹”，以标准score/flow matching目标进行训练，无需长链BPTT或大型教师。具体做法是用模型生成序列、从末端向前聚合误差信号，形成能把模型从偏离态拉回到正确轨迹的监督，适配于因果扩散Transformer。

Result: 在文本生成视频、视频续写、多提示生成等任务上，BAgger带来更稳定的长时运动、更好的视觉一致性，显著减少质量漂移，并保持质量与多样性。

Conclusion: BAgger通过利用模型自身rollout构建自监督纠错信号，在不增加教师或复杂损失的情况下缓解曝光偏差，提升自回归视频建模的长时稳定性与一致性，可作为因果扩散Transformer等模型的通用训练增强策略。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [27] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: 提出RePack，将高维VFM特征压缩为低维、解码器友好的表示，为DiT加速收敛并提升重建质量，避免信息过载；在DiT-XL/2上64个epoch达成FID 3.66，较SOTA快35%。


<details>
  <summary>Details</summary>
Motivation: 直接把高维VFM（如DINOv3）特征注入LDM/DiT虽带来更强语义，但会因特征维度过高造成信息过载与解码器负担，甚至特征尺寸超过原图解码需求，影响训练稳定与效率。需要一种既保留核心语义又避免高维副作用的表征注入方式。

Method: 提出RePack（Representation Packing）：将VFM嵌入通过投影到低维流形实现紧凑重参数化，过滤非语义噪声，得到更“解码器友好”的特征，再注入DiT解码过程。核心是低维流形投影/压缩以保留结构语义并减轻信息冗余。

Result: 在图像重建任务上，RePack显著加速DiT收敛并优于直接注入原始VFM特征的方法；以DiT-XL/2为例，仅64个epoch达到FID 3.66，比现有SOTA收敛速度快约35%。

Conclusion: 通过将VFM高维表征压缩为低维、保语义的表示，RePack有效避免信息过载，提升DiT的训练效率与生成质量，证明了抽取核心语义并绕开高维副作用的可行性。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [28] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: 论文提出VEGAS：在推理时将视觉编码器的注意力图注入到LVLM语言模型中层，以抑制与图像不一致的幻觉，显著提升多基准上的防幻觉效果。


<details>
  <summary>Details</summary>
Motivation: LVLM常产生语言流畅但与图像事实不一致的“幻觉”。已有方法缓解有限，关键未明：何种视觉注意形式在解码中最能压制幻觉？

Method: 1) 经验观察：当最终视觉注意未聚焦关键目标时更易幻觉；视觉编码器自身注意更集中、相关性更高。2) 冲突分析：在语言模型中层出现视觉-文本冲突峰值。3) 方案VEGAS：在推理时，将视觉编码器的注意力图注入语言模型中层，对未对准关键目标的token进行自适应引导，强化对关键图像区域的关注，无需再训练。

Result: 在多个基准上持续获得SOTA的幻觉抑制表现，显著降低与图像证据不一致的输出，同时保持或提升整体任务表现。

Conclusion: 视觉编码器的注意力图是抑制LVLM幻觉的有效信号；在LM中层进行注意力注入与自适应引导可在无需训练的前提下显著减少幻觉。

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [29] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 提出SPDMark，一种在生成过程中通过选择性参数位移实现视频扩散模型水印的方法，利用LoRA构造层级基向量位移，将密钥索引的组合嵌入视频；训练时联合优化消息恢复、感知相似度与时序一致性损失；提取端用哈希生成帧级消息并通过最大二分匹配恢复被篡改的帧顺序，实现在保持不可感知的同时，具备鲁棒且高效的水印检测与追踪。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成兴起，急需既不可感知又鲁棒、且计算高效的水印技术以用于来源追踪与篡改检测；现有事后或生成中水印方法难以三者兼得。

Method: 在视频扩散模型内进行“选择性参数位移”（Selective Parameter Displacement）：- 将水印表示为各层的基向量位移的加性组合，并由水印密钥索引该组合；- 通过LoRA实现参数高效的位移；- 训练时联合学习基位移与水印提取器，优化消息恢复、感知相似度、时序一致性三类损失；- 使用加密哈希从基密钥导出帧级水印消息；- 提取阶段对可能被时序篡改的视频，用最大二分匹配恢复帧顺序并解码。

Result: 在文本转视频与图像转视频模型上，生成的视频水印不可感知、恢复准确率高；对多种常见视频修改（包括时序篡改）表现出强鲁棒性，同时保持计算效率。

Conclusion: SPDMark通过LoRA驱动的层级参数位移，在保证视觉质量的同时实现了高准确、鲁棒且高效的生成中视频水印，并能定位与纠正时序篡改，为生成视频溯源提供实用方案。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [30] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 论文研究了在传统光学显微与数字在线全息显微（DIHM）两种成像下的花粉自动识别基线与改进方法。光学图像下检测mAP50达91.3%、分类97%；DIHM图像因散斑噪声与孪生像伪影严重，仅8.15%与50%。通过扩展DIHM的目标框与GAN合成数据增强（WGAN-SN，FID=58.246；实:合=1:1.5混合），检测提升至15.4%，缩小了与光学成像的性能差距。


<details>
  <summary>Details</summary>
Motivation: DIHM未重建全息图像与明场外观差异巨大，受散斑与孪生像影响，现有模型在DIHM上表现显著退化；需要建立可复现的跨模态基线，并探索数据与模型层面的方法以提升DIHM中的自动花粉检测与分类，推动兽医影像的自动化流程落地。

Method: 构建光学与DIHM双模态、自动标注且仿射对准的数据集；以YOLOv8s做目标检测、MobileNetV3-L做分类，分别在光学与DIHM上训练评估；在DIHM端尝试以光学对齐框扩展DIHM中的目标框；训练带谱归一化的Wasserstein GAN（WGAN-SN）生成DIHM合成图像，混合真实与合成样本（1:1.5）进行数据增强训练。

Result: 光学图像：检测mAP50=91.3%，分类准确率=97%。DIHM图像：检测mAP50=8.15%，分类=50%。使用框扩展：DIHM检测mAP50=13.3%，分类=54%。WGAN-SN生成图像FID=58.246；以实:合=1:1.5混合训练后，DIHM检测提高到15.4%。

Conclusion: 未重建DIHM图像上自动花粉识别仍然远落后于光学成像；简单的框扩展与GAN数据增强能带来小幅提升，证明合成数据有助于缩小跨模态差距，但距离实用仍有明显距离。未来需从成像物理先验、重建/去伪影、域自适应与更强的检测架构等方面持续改进。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [31] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: 提出EchoGround-MIMIC测量约束的超声心动图多模态数据集与EchoVLM模型，通过引入视图感知与否定感知的对比学习目标，在多项临床相关任务上达成SOTA并体现良好可迁移性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图解读多步骤、多模态且耗时，需要视图识别、定量测量、定性判断与指南推理。现有VLM缺乏大规模、临床对齐的图文数据与基于测量的推理能力，限制了在超声领域的应用。

Method: 构建EchoGround-MIMIC：包含19,065对图文、标准化视图、结构化测量、基于测量的描述与指南派生疾病标签。提出EchoVLM并设计两种预训练目标：1) 视图知悉的对比损失，编码超声成像的视图依赖结构；2) 否定感知的对比损失，区分临床关键的阴性与阳性发现。随后在多任务上评测，包括多模态疾病分类、图文检索、视图分类、心腔分割与解剖标志点检测。

Result: 在36项任务中达到SOTA：零样本疾病分类AUC 86.5%，视图分类准确率95.1%；展现出可迁移的视觉表征，支持端到端的超声心动图解读。

Conclusion: 临床测量对齐的多模态预训练显著提升超声心动图任务性能与可迁移性；EchoVLM可作为超声心动图基础模型。将开放数据集与整理代码，促进可复现与后续研究。

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [32] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 论文提出一种面向3D CT数据的“基于补丁（patch）的持久同调（PH）”构建方法，相比传统3D立方复形过滤在分类性能与运行时间上均更优，并发布了Python包Patch-TDA。


<details>
  <summary>Details</summary>
Motivation: 现有在3D CT上的PH多采用立方复形过滤，但在高分辨率体数据上计算开销大、且并非总能带来最佳判别性能；作者希望在不牺牲拓扑判别力的前提下，提升效率与下游ML性能。

Method: 将体数据划分为局部体素补丁，对每个补丁独立构建PH（持久同调）并提取拓扑特征（如连通分量、环、空腔等）的持久条形码/图，再进行汇聚（如统计量或嵌入）形成图像级特征；在多数据集、多参数设置下与传统3D立方复形算法系统对比。

Result: 在多个3D CT数据集上，补丁式TDA在分类准确率、AUC、敏感度、特异度、F1上分别平均提升10.38%、6.94%、2.06%、11.58%、8.51%，同时时间效率更高。

Conclusion: 补丁式PH能在不牺牲拓扑信息的情况下显著提升体数据的TDA特征提取效率与下游分类性能，是传统立方复形方法的有效替代；并提供可用的Python实现Patch-TDA以促进应用。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [33] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 他们构建了目前最大规模的灾后道路评估与对齐基准数据集（CRASAR-U-DRIODs），并在10次联邦灾害的sUAS图像上训练了18个基线模型；同时揭示道路中心线空间错位会显著降低模型表现并影响应急决策。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集要么规模小、要么分辨率低，难以检测应急管理关心的细粒度道路受损现象；既有ML方案缺乏实际应急场景的运营级验证。此外，实务中存在显著的道路线要素与影像/现实道路错位问题，影响模型评估与部署。

Method: 标注了657.25公里道路并采用10类标签体系；在2024年飓风Debby与Helene的应急响应中训练并部署18个基线模型；同时对既有道路线进行9,184次空间对齐调整，系统评估错位对模型性能与标注准确性的影响。

Result: 在真实世界存在道路线错位时，18个基线模型的平均宏观IoU下降5.596%；若不考虑空间对齐，约8%（11公里）的不良路况会被误标，且约9%（59公里）的道路线偏离真实道路。

Conclusion: 高分辨率、大规模、经实战部署验证的数据与模型对于灾后道路评估至关重要；空间对齐是影响性能与决策准确性的关键环节，ML/CV/机器人社区需系统性纳入对齐与配准问题以提升灾时决策支持效能。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [34] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 论文提出一种深度学习融合方法，将多源遥感与物理模型数据结合，生成格陵兰东部Helheim冰川2017–2023年每日100米分辨率的地表融水格网图；相比仅用RCM或PMW的方法，精度显著提升（约95%准确率），并发布对齐数据集MeltwaterBench与代码。


<details>
  <summary>Details</summary>
Motivation: 现有格陵兰地表融水制图在时间与空间分辨率之间存在权衡，且融化过程难观测、机理未完全理解；需要一种既高时频又高空分辨率的产品，以更好表征和研究融水分布与极端融化事件。

Method: 以SAR推导的融水作为“近似真值”，将区域气候模式（RCM）输出进行时空下采样/上采样融合，结合合成孔径雷达（SAR）、被动微波（PMW）与数字高程模型（DEM），利用深度学习（评估UNet与DeepLabv3+）实现至每日100 m的时空下尺度化；并与非深度学习基线（仅RCM、仅PMW、以及基于SAR的滑动窗口格网化）对比。

Result: 在Helheim冰川2017–2023数据上，深度学习融合多源数据达到约95%准确率，较仅RCM（83%）与仅PMW（72%）提升>10个百分点；SAR滑动窗口格网化无需深度学习也达90%但低估极端融化事件；发布对齐数据集MeltwaterBench与代码，用于进一步方法对比。

Conclusion: 多源融合深度学习可显著提升高时空分辨率融水制图的准确性，为理解冰盖融化过程与极端事件提供更可靠的数据支撑；公开基准与代码将促进更复杂下尺度方法的研究与比较。

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [35] [Open Horizons: Evaluating Deep Models in the Wild](https://arxiv.org/abs/2512.12146)
*Ayush Vaibhav Bhatti,Deniz Karakay,Debottama Das,Nilotpal Rajbongshi,Yuito Sugimoto*

Main category: cs.CV

TL;DR: 在 CIFAR-10 上统一评估开集识别与小样本增量学习：CLIP+Energy 在开集分离最稳健；FSCIL 中 ConCM 在10-shot下达84.7%且最干净混淆矩阵，方法在≥5-shot后性能趋于饱和。


<details>
  <summary>Details</summary>
Motivation: 实际开放世界中既要识别已知类也要在遇到未知类时保持可靠，并在新类到来时以少量样本进行增量适应，然而不同骨干与后处理策略对未知检测与遗忘抑制的影响缺乏系统比较。

Method: 在 CIFAR-10 上：1) 开集识别：冻结三种预训练视觉编码器（ResNet-50、ConvNeXt-Tiny、CLIP ViT-B/16），线性探针分类头，并用 MSP、Energy、Mahalanobis、kNN 四种打分进行未知检测；评估 AUROC/AUPR/FPR@95/OSCR。2) 小样本类增量：部分冻结 ResNet-50，比较改进版 SPPR、OrCo、ConCM 于 1/5/10-shot 情形，并观察混淆矩阵与灾难性遗忘。

Result: 开集：CLIP 一致取得已知/未知最强可分性，Energy 在不同骨干上最稳定。增量：ConCM 在10-shot下达84.7%且混淆最少；所有方法在>5-shot 后收益趋缓（饱和）。

Conclusion: 骨干与后验打分机制显著影响未知检测；原型/聚类式方法在增量适应中更能缓解灾难性遗忘。实践上，推荐使用 CLIP 编码器配合 Energy 评分做开集检测，增量阶段采用基于原型的 ConCM，并预期在5-shot后提升有限。

Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.

</details>


### [36] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: 用日常环境音辅助相机相对位姿估计：将DOA声源方向谱与双耳嵌入融入视觉模型，在两大数据集上相对强视觉基线稳定提升，尤其在模糊/遮挡等退化视觉条件下更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 纯视觉位姿估计在运动模糊、遮挡、低光等退化情况下易失效。现实视频常伴随“偶发的、被动采集”的环境音，这些声音蕴含方位与空间结构线索，可能补充视觉不足。

Method: 提出一个简洁的音视频融合框架：从音频提取声源到达方向（DOA）谱与“拟双耳化”嵌入，将其作为附加特征输入并与最先进的视觉相对位姿估计网络融合（似采用特征级融合/条件化模块），无需主动发声，仅用被动环境声音。

Result: 在两个大规模数据集上，相比强视觉基线取得一致性能提升；当视觉信息被人为腐化（如模糊、遮挡）时鲁棒性显著提高。

Conclusion: 首次在真实世界视频中成功利用被动环境音提升相机相对位姿估计；日常音频是解决经典空间感知问题的一个意想不到而有前景的信号。

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [37] [SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation](https://arxiv.org/abs/2512.12193)
*Xuancheng Xu,Yaning Li,Sisi You,Bing-Kun Bao*

Main category: cs.CV

TL;DR: SMRABooth提出以对象级主体与运动表示为核心的可控文本到视频定制方法，通过自监督主体编码器与光流编码器提供对齐信号，并用稀疏LoRA解耦主体与运动，兼顾外观保真与时序一致运动。


<details>
  <summary>Details</summary>
Motivation: 现有定制视频生成难以同时保证主体外观高度一致与运动模式时序一致，关键在于缺乏对象级（object-level）的主体与运动指导信号，且LoRA微调中主体与运动耦合互相干扰。

Method: 三阶段：1) 用自监督编码器提取主体对象级表示，对齐模型以提升整体结构和高层语义一致性；2) 用光流编码器提取与外观无关的对象级运动轨迹表示，提供时序一致的运动指导；3) 设计主体-运动关联解耦策略，在空间位置与时间上稀疏注入多组LoRA，减少主体LoRA与运动LoRA的相互干扰。整体在LoRA微调中对齐上述表示。

Result: 大量实验表明，本方法在主体与运动的定制上优于现有方法，能更好保持主体外观一致与运动模式一致，实现更有效的可控文本到视频生成。

Conclusion: 对象级表示与稀疏LoRA解耦是同时提升主体保真与运动一致性的关键；SMRABooth在定制视频生成中取得显著优势，验证了该框架的有效性。

Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.

</details>


### [38] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 提出一种适用于微型无人机编队在野火环境、带宽受限下的轻量级火线周界跟踪方法，融合热红外与RGB边缘信息，采用规则级合并与RDP简化，配合周期信标与惯性反馈以抗GPS退化，在SoC上实现<50ms闭环；仿真表明路径更短、边界抖动更小且覆盖率不降，能在标准微型平台以10–15 m/s运行，能低通信快速部署用于应急侦察。


<details>
  <summary>Details</summary>
Motivation: 野火态势感知需要小型无人机在带宽受限、GPS可靠性差的恶劣环境中快速、稳定地跟踪火线边界；现有纯边缘或高算力方法在抗噪、实时性和能耗上不足，难以在低成本SoC与小电池平台上部署。

Method: - 热红外帧：自适应阈值+形态学得到粗热区掩膜。
- RGB帧：梯度滤波提供边缘线索并抑制纹理伪检。
- 规则级合并：选择边界候选并用Ramer–Douglas–Peucker算法简化轮廓。
- 控制与稳态：周期性信标与惯性反馈环在GPS退化时维持轨迹稳定。
- 计算优化：限制每帧像素操作、预计算梯度表，使SoC上闭环延迟<50 ms。

Result: 小规模仿真对比纯边缘跟踪基线：平均路径长度更短、边界抖动更小，同时通过交并度量保持环境覆盖；功耗与算力占用表明在标准微型平台可实现10–15 m/s前向飞行。

Conclusion: 该轻量化、多模态融合与计算受限优化的周界跟踪方案在低通信和GPS不稳条件下表现稳健，适合微型无人机编队的应急野火侦察快速部署。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [39] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: 该论文发布了一个涵盖2021–2025年、来自英国布里斯托尔22个固定视角摄像头的路灯图像大规模长期数据集（52.6万张），并配套自监督CNN-VAE框架与两类漂移度量，用于研究视觉漂移、异常检测与MLOps策略。


<details>
  <summary>Details</summary>
Motivation: 现实城市视觉系统在长期部署中会遭遇光照、季节、天气与设备变化引发的分布漂移与性能退化。缺乏带时间跨度、真实世界、可复现实验的数据集与统一度量，限制了对长期稳定性、漂移感知学习和部署策略的系统研究。

Method: 构建并公开数据集：22个固定角度摄像头，2021–2025按小时采集，附时间戳、GPS与设备ID，图像与结构化元数据分别以JPEG/CSV发布。提出自监督基线：按摄像头与昼/夜分别训练CNN-VAE。定义两种逐样本漂移指标：1) 相对质心漂移（相对某基准季度的潜空间偏移）；2) 相对重建误差（归一化的图像域退化）。

Result: 得到52.6万+张多时段、多天气/光照条件的图像及配套元数据；提供可复现实验基线与漂移度量，在长期、细粒度层面可用于评估模型稳定性、异常检测与漂移监控。

Conclusion: 该数据集与自监督框架为研究长期部署视觉系统提供了现实、可扩展的基准，支持漂移感知学习与MLOps实践，并可用于路灯监测、天气推断与城市场景理解等下游任务。数据已通过两个DOI公开发布。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [40] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 提出ALERT真实驾驶IR-UWB分心行为数据集与输入尺寸无关的ViT（ISA-ViT），通过保留雷达多域特征并融合频域与距离域，实现对驾驶员分心行为的高精度识别，较现有ViT方案提升22.68%。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶是交通致死的重要原因。IR-UWB雷达具备抗干扰、低功耗、保护隐私等优势，适合驾驶员活动识别。但现实中缺少大规模、真实场景的UWB分心驾驶数据集；同时，传统ViT要求固定输入尺寸，难以直接适配维度非标准的UWB雷达数据，简单缩放会破坏多普勒与相位等关键信息。

Method: 1) 构建ALERT数据集：在真实驾驶环境采集7类分心行为，共10,220条UWB样本。2) 提出ISA-ViT：通过输入尺寸无关的框架将UWB数据重整至ViT可接收的形状，同时保持多普勒与相位特征；通过可调patch配置和利用预训练位置嵌入（PEVs）适配不同输入；3) 设计域融合策略：融合距离域与频域信息，增强分类判别力。

Result: 在全面实验中，ISA-ViT相较既有基于ViT的UWB-DAR方法，分类准确率提升22.68%。

Conclusion: 公开ALERT数据集与ISA-ViT方法，为雷达驱动的分心驾驶识别提供可扩展、鲁棒的基线与策略，推动实际部署。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [41] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

TL;DR: 提出面向ASD儿童在社交机器人（NAO）呼名情境下的情绪识别深度学习管线；用ResNet-50 CNN + 三层GCN融合视觉与几何特征，采用DeepFace与FER的加权软标签，KL散度优化融合嵌入，基于约5万帧、15名ASD儿童的数据，在印度首次构建大规模真实世界数据与方法，能捕捉微表情，服务临床与治疗型HRI。


<details>
  <summary>Details</summary>
Motivation: ASD儿童在社会互动中的情绪反应难以客观识别，传统方法对微妙表情与神经多样性特征不敏感；HRI领域缺乏针对ASD、尤其在印度语境下的真实世界大规模数据与专门方法。

Method: 在NAO呼名实验中采集视频，提取约5万张面部帧；利用MediaPipe FaceMesh获得几何关键点；构建ResNet-50微调的CNN提取外观特征，三层GCN处理关键点图；两套现成模型（DeepFace与FER）对七类情绪输出概率，进行加权集成生成软标签；将CNN与GCN嵌入融合，并以KL散度优化/对齐，进行最终分类。

Result: 管线在建模ASD儿童细微情感反应上表现稳健，能有效捕捉微情绪线索；在该特定任务与数据上取得良好分类性能（摘要未给出具体数值）。

Conclusion: 该工作提供了印度首个面向ASD情绪分析的真实世界大规模数据集与端到端管线，弥补自闭症特定HRI研究空白；方法对临床评估与个性化辅助技术开发具有潜在价值。

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [42] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 提出CineLOG数据集与分阶段生成管线，实现对电影属性（机位/镜头运动与类型）更精细可控的视频合成，并通过轨迹引导过渡模块生成连贯多镜头序列，较现有端到端T2V在遵循摄像与剧本指令上显著更好。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的视频生成对细粒度可控性不足，尤其难以精确控制电影摄影要素（如镜头运动轨迹、类型风格）；公开数据集存在不平衡、噪声标注或仿真到真实域差距，制约训练与评测。

Method: 1) 构建CineLOG：5000段高质量、未剪切视频，提供详细场景描述、基于标准电影摄影分类法的显式摄像指令、以及影片类型标签，覆盖17类镜头运动与15种类型并保持均衡；2) 设计分解式T2V管线，将复杂生成任务拆为四个更成熟的子阶段；3) 提出Trajectory Guided Transition Module，实现在多镜头之间基于轨迹的时空平滑插值，保证连贯性。

Result: 在人类评测中，该管线在遵循特定摄像与剧本指令方面显著优于SOTA端到端T2V，同时维持专业级画质。

Conclusion: 通过均衡高质的数据与可解释、可控的分阶段生成，再辅以轨迹引导的过渡，本工作显著提升电影化视频合成的可控性与连贯性，并提供可复现实验资源（代码与数据已开源）。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [43] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 提出评估与改进“视觉-语言推理链”视觉忠实性的无训练框架，并通过自反生成修复不忠实的感知步骤，在不损伤最终答案准确率的前提下降低不忠实感知比率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM常生成显式思维链以增强能力与可解释性，但存在两类新失败模式：1) 通过与图像不一致的中间感知步骤得到正确答案；2) 感知与推理都忠实但最终预测仍错误。仅评估最终答案无法区分并诊断这些问题，需要一个关注“视觉忠实性”的维度。

Method: 1) 将思维链自动拆分为“感知步骤”(与图像内容绑定)与“推理步骤”(逻辑操作)。2) 使用现成VLM作为裁判，对每个感知步骤进行逐步的视觉忠实性判定，无需训练与参考答案；并用人工元评估验证裁判可靠性。3) 基于该指标，提出轻量级自反程序：检测不忠实的感知步骤并在局部重新生成，迭代修复而不改动其余链条。

Result: 在多种经过推理训练的VLM与以感知为主的基准上，方法显著降低不忠实感知率（Unfaithful Perception Rate, UPR），同时维持最终答案准确率，证明了对多模态推理可靠性的提升。

Conclusion: 视觉忠实性应作为独立评估维度；利用无训练、参考无关的逐步判定与自反式局部重生成，可有效修复VLM推理链中的感知偏差，在不牺牲答案准确性的情况下提高多模态推理的可信度。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [44] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: 提出一种在零样本细粒度识别中进行属性解缠结的框架ACR，通过两类“专家混合”模块在表征学习阶段就将颜色、形状、纹理等属性分离，从而实现更稳健的泛化与SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 零样本细粒度分类需要分辨微小差异，但现有方法把不同属性压缩到单一嵌入，产生属性缠结与互扰；事后处理已难以纠正已混合的表示，因此需要在表示学习阶段直接实现属性解缠结。

Method: 提出Attribute-Centric Representations (ACR)。在Transformer中加入两级专家混合：1) MoPE（Mixture of Patch Experts）采用双层路由，对图像patch进行条件分派，使同一属性族由专门专家处理；2) MoAE（Mixture of Attribute Experts）作为头部，将专家精炼后的特征投影为稀疏、部位感知的属性图，并用于零样本分类。

Result: 在CUB、AwA2、SUN三个零样本学习基准上取得一致的SOTA结果。

Conclusion: 在训练阶段实施属性解缠结可有效缓解属性缠结问题，提升零样本细粒度识别的可解释性与性能；ACR通过MoPE与MoAE协同实现对属性的专门化建模并带来SOTA表现。

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [45] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: 该论文提出并发布面向“专业级图像生成”的评测基准ProImage-Bench，用细粒度评分标准评测生物示意图、工程/专利图与一般科学图，揭示现有文本到图像模型在科学精确性上的显著不足，并展示用评测rubric作为可扩展监督信号可显著提升生成与编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型更擅长视觉上“像”的图，而在专业领域需要信息密度高、科学精准、可检验的插图（如教材/专利/科研图）。缺乏系统化、量化且可行动的评测工具，阻碍了对这类能力的衡量与改进。

Method: 1) 构建ProImage-Bench：收集654张来自教材与技术报告的真实专业图；基于原文上下文与参考图，用多模态大模型自动生成分层rubric，将正确性分解为6,076个准则与44,131个二值检查；2) 设计自动化LMM裁判与惩罚聚合方案，形成可解释的准则分数与rubric准确率；3) 在基准上对多种代表性文生图模型测试；4) 将失败检查回馈给图像编辑模型进行迭代修正，实现“评测即监督”。

Result: 最强基础模型在开域表现强，但在该基准上仅达0.791的rubric准确率与0.553的准则分数，显示细粒度科学忠实度存在明显差距。利用失败检查驱动的迭代编辑，可将强生成器从0.653/0.388提升到0.865/0.697（rubric准确率/准则分数）。

Conclusion: ProImage-Bench提供了严格、可解释、可扩展的专业图生成诊断工具，并可直接作为监督信号用于闭环改进，从而推动规范、可检验的科学插图生成。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [46] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 对71例1–9月龄婴儿脑MRI，系统比较SynthSeg与SamSeg分割精度及其对体积与分形维度（FD）估计的影响。SynthSeg整体更优，体积更接近人工标注；SamSeg显著高估脑室与全脑体积。分割精度随月龄提升。分割误差会显著影响FD，且体积偏差与FD偏差正相关。结论：SynthSeg当前更可靠，但对小效应的体积/FD差异需谨慎解读。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI因髓鞘化进行中、组织对比度低而自动分割困难，而分割精度直接影响结构体积与复杂度（FD）等关键发育指标，需要定量评估不同工具在婴幼儿期的数据表现及其对下游指标的偏倚。

Method: 使用BOB数据集（71次扫描，1–9月龄），以专家标注为参考，评测SynthSeg与SamSeg在多指标上的分割性能（Dice、IoU、95% Hausdorff、NMI），并比较由两种分割产生的体积与FD，与人工基准做一致性分析（含Bland–Altman）。考察年龄对分割精度的影响，并分析体积与FD偏差的相关性。

Result: SynthSeg在所有质量指标上优于SamSeg（主要脑区平均Dice>0.8），体积估计与人工参考接近（均值+4%，范围-28%到+71%）；SamSeg系统性高估脑室与全脑体积（均值+76%，范围-12%到+190%）。分割精度随年龄增加而提升。与专家分割相比，SynthSeg的FD在多个区域存在显著差异；Bland–Altman显示分割导致的FD变异度超过多数发育队列报道的组间差异。体积与FD偏差在多个结构上正相关。

Conclusion: 在儿科MRI上，SynthSeg较为可靠，能提供更接近人工标注的体积与FD。然而，分割相关的不确定性会放大或掩盖小幅形态学差异，尤其是FD，因此在解释细小体积/FD效应时应谨慎，必要时应进行方法学敏感性分析与年龄分层。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [47] [Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder](https://arxiv.org/abs/2512.12229)
*Tianyu Zhang,Dong Liu,Chang Wen Chen*

Main category: cs.CV

TL;DR: 提出AEIC：用浅/中等编码器+单步扩散解码，实现超低码率(<0.05 bpp)高感知重建与高编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有超低码率方法依赖大型预训练编码器（VAE/Tokenizer），编码端计算重，难在弱端设备部署；需要在极低码率下同时兼顾编码简洁与解码质量。

Method: 构建非对称压缩框架AEIC：发送端采用中等或浅层编码器进行轻量特征提取与量化；接收端使用单步扩散解码器进行高保真、高真实感重建。为提升浅层编码器效果，提出双端特征蒸馏，从中等编码器AEIC向浅层变体传递知识。

Result: 在<0.05 bpp下，AEIC在率-失真-感知三者权衡上优于现有方法；编码端在1080P图像上达到35.8 FPS，解码速度与现有方法相当。

Conclusion: 通过非对称设计与单步扩散解码+双端蒸馏，AEIC在极低码率下兼顾高感知质量与高编码效率，适合带宽受限、端侧算力有限场景。

Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.

</details>


### [48] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: 他们让MLLM不再输出文本时间戳，而是对每帧输出“0/1”字符序列，并把这些字符的概率当作分割掩码，从而端到端训练并直接得到时刻与显著性分数。用极少帧采样也达到或超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本式时间戳生成无法为帧级预测提供直接梯度，导致训练信号弱；RL弥补但复杂且不稳定。希望在保持LLM推理优势的同时，获得可微、稳定的帧级学习信号，并提升时刻/高光检测效率。

Method: 将固定数量的视频帧与提示输入MLLM，强制其按帧输出连续的“0/1”字符序列：1表示前景/高光，0表示背景。把每个字符的输出logit视为前景/背景概率，联合优化：分割类损失（例如逐帧二元交叉熵、连续性/平滑正则等）+常规因果语言建模损失。推理时用束搜索生成0/1序列（得到时刻区间）并读取对应logits作为显著性分数。仅采样约25帧。

Result: 在QVHighlights上高光检测取得56.74 HIT@1；在时刻检索上MAP 35.28，超过基线。训练过程中，当LM损失进入平台期时，分割损失仍提供稳定补充信号。

Conclusion: 把LLM输出从文本时间戳改为逐帧0/1“分割”使模型同时利用语言能力与可微帧级监督，训练更稳定、推理更直接且高效，在低帧采样下仍能取得强性能并优于基线。

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [49] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: 提出MetaTPT：在测试时通过元学习到的自监督辅助任务来动态学习增强并联合提示微调，从而更稳健地适应域移。


<details>
  <summary>Details</summary>
Motivation: 现有VLM如CLIP零样本能力强但对域移敏感。TPT用固定增强来做一致性驱动的测试时提示微调，遇到复杂域移和多样目标域时增强表达力不足、易失效。需要一种能为每个样本自适应生成信息量大的视图、并能直接提升测试时适配效果的方法。

Method: 提出Meta Test-Time Prompt Tuning（MetaTPT）。核心是双环优化：内环通过自监督学习一个辅助任务，为每个样本动态学习参数化增强（生成多视图），以捕获目标域关键特征；外环在这些视图之间施加一致性损失，对VLM的文本提示进行测试时微调。增强学习与提示调优紧密耦合，共同优化以产生更有辨识度的视图并提升鲁棒性。

Result: 在域泛化与跨数据集基准上广泛实验，MetaTPT优于现有方法（含TPT），取得SOTA表现，显示在不同域移场景下更强的适配能力与稳定性。

Conclusion: 通过元学习到的自监督辅助任务动态生成增强，并与提示微调协同优化，MetaTPT显著提升VLM在测试时的域移适应与泛化能力。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [50] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: 提出一种用于表情识别的持续学习混合框架，融合深度卷积特征与FACS动作单元，用BGMM进行概率建模，无需重训即可逐步从基础到复合表情，提升准确率与抗遗忘。


<details>
  <summary>Details</summary>
Motivation: 情绪交互需求增长，表情具有动态性与文化差异，传统模型在持续学习中易灾难性遗忘，需要能逐步学习新表情且保留旧知识、同时轻量可部署的方法。

Method: 构建混合表示：深度卷积特征 + FACS动作单元（AUs）；使用贝叶斯高斯混合模型（BGMM）对联合特征进行概率判别，实现无需反向重训的增量更新；在CFEE数据集上先学习基本表情，再逐步加入复合表情进行评估。

Result: 相较基线，模型在增量任务上取得更高识别准确率，更强知识保持与更低遗忘率；在从基础到复合表情的阶段性学习中性能稳定。

Conclusion: 该轻量概率式持续学习框架有效缓解灾难性遗忘，并提升复合表情识别能力，适用于教育、医疗与自适应界面等情感智能应用场景。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [51] [Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection](https://arxiv.org/abs/2512.12281)
*Jiahao Zhao*

Main category: cs.CV

TL;DR: Cognitive-YOLO提出用LLM直接“合成”检测网络：先提取数据集元特征，再由LLM结合RAG生成结构化架构描述，最后编译成可部署模型；在五个数据集上比强基线更优，且参数效率高。


<details>
  <summary>Details</summary>
Motivation: 手工设计检测网络耗时、NAS计算昂贵，而现有LLM方法多在搜索环中做迭代调参，缺乏基于数据整体理解的直接架构生成能力。作者希望以数据驱动的认知式推理替代代价高的搜索。

Method: 三阶段流程：1) 数据分析模块提取关键元特征（目标尺度分布、场景密度等）；2) 通过RAG检索SOTA组件，LLM基于元特征进行链式推理并以NADL（神经架构描述语言）生成完整架构；3) 编译器将NADL实例化为可训练/部署的检测模型。包含消融实验分析LLM推理与RAG组件的贡献。

Result: 在五个不同检测数据集上自动生成的架构优于强基线，达到有竞争力的精度，同时以更少参数获得更佳性能/参数比。

Conclusion: 数据驱动的LLM推理是性能提升的主因，理解数据“第一性原理”比简单堆叠SOTA组件更关键；Cognitive‑YOLO提供了一条低计算成本的高性能架构合成路径。

Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.

</details>


### [52] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 提出RealDrag基准与四个新指标，弥补拖拽式图像编辑评测缺乏标准与真值的问题，并对17个SOTA模型进行系统评估，给出可复现基线与权衡分析。


<details>
  <summary>Details</summary>
Motivation: 当前拖拽式（point-based）图像编辑模型评测缺乏统一基准、协议与含真值目标图像的数据集，导致方法间客观对比困难，研究进展难以量化。

Method: 构建RealDrag基准：收集400+来自多样视频源的人类标注样本，提供源/目标图像、句柄/目标点、可编辑区域掩膜，以及图像与编辑动作的描述性文本；提出四个任务专用指标——SeD（语义/像素匹配距离）、OMPS（掩膜外区域保持）、IPPS（掩膜内未编辑贴片保持）与DiS（方向相似性）。据此对17个SOTA模型进行大规模系统评测。

Result: 实验证明不同方法间存在明确的性能权衡；基准与工具链实现了稳健、可复现的比较，揭示各方法在像素匹配、区域保持与语义对齐上的差异。

Conclusion: RealDrag与所提指标为拖拽式图像编辑提供首个含真值的标准化评测框架，确立可复现实验基线，可作为后续研究与方法改进的统一参照；数据集与评测工具将公开。

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [53] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: 提出GrowTAS与GrowTAS+：从小到大逐步训练Transformer超网，减轻权重共享干扰，提升大/小子网性能；在ImageNet与多项迁移任务上优于现有TAS方法。


<details>
  <summary>Details</summary>
Motivation: TAS依赖超网权重共享，导致不同规模子网相互干扰，尤其小子网性能受损。观察到“先训练好小子网可作为训练大子网的良好基础”，因此希望通过训练流程设计降低干扰、稳定搜索与训练。

Method: 提出渐进式超网训练框架GrowTAS：先仅训练较小子网，再逐步纳入更大子网参与训练，控制搜索空间与参数共享范围，减少干扰并稳定优化；进一步提出GrowTAS+：在后期仅微调部分关键权重（子集权重微调），重点增强大子网表现。

Result: 在ImageNet分类及CIFAR-10/100、Flowers、CARS、INAT-19等迁移任务上进行大量实验，GrowTAS/GrowTAS+整体优于现有TAS基线，在效率与精度上均取得改进。

Conclusion: 通过从小到大的渐进式训练与选择性微调，能有效缓解超网权重共享带来的子网干扰，稳定TAS训练并提升最终ViT结构的性能与泛化。

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [54] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 论文提出“Intention-Drive”基准，用于评估自动驾驶从理解高层人类意图到执行安全精准动作的能力，并引入数据集与以意图成功率ISR为核心的评价协议；基线模型在该基准上表现明显不足。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶多为执行低层指令（如转向）而非理解抽象意图，缺少标准化基准来衡量“从人类意图到行为”的能力，阻碍了真正智能自治的发展。

Method: 构建Intention-Drive基准：1）收集复杂场景并配套自然语言意图的数据集；2）设计以Intent Success Rate（ISR）为核心的评估协议，强调语义层面的目标达成而非仅几何误差；3）在该基准上对一系列基线模型进行系统评测。

Result: 基线模型在Intention-Drive上表现显著不佳，难以实现对复杂场景与高层意图的充分理解与执行。

Conclusion: 需要面向高层意图理解的模型与训练策略；Intention-Drive为研究社区提供了首个系统化的数据与评测框架，可推动从“命令执行者”到“意图实现者”的转变。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [55] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: 提出OMUDA：通过分层遮蔽（上下文、特征、类别）缓解UDA中的域间语义分割偏移，在SYNTHIA→Cityscapes与GTA5→Cityscapes上可无缝集成并带来约7%的SOTA提升。


<details>
  <summary>Details</summary>
Motivation: 现有UDA分割方法受三类问题制约：跨域上下文歧义、特征表示不一致、按类别的伪标签噪声高，导致模型难以泛化到目标域。需要一个统一框架在不同表示层面同时缩小域差。

Method: 提出Omni-level Masking（OMUDA）的层级遮蔽框架，包含三种策略：1) Context-Aware Masking（CAM）自适应区分前景/背景，平衡全局上下文与局部细节；2) Feature Distillation Masking（FDM）利用预训练教师进行知识蒸馏，增强鲁棒与一致的特征学习；3) Class Decoupling Masking（CDM）显式建模类别不确定性，对噪声伪标签进行类别级解耦与遮蔽。三者在上下文、表示与类别层面共同减小域移。

Result: 在多个跨域语义分割基准上验证有效；在SYNTHIA→Cityscapes与GTA5→Cityscapes任务中，可与现有UDA方法无缝结合，平均带来约7%的性能提升，达到SOTA。

Conclusion: 分层遮蔽是一种统一且通用的UDA范式，可同时缓解上下文、特征及类别层面的域偏移，提升目标域泛化并可模块化集成到现有框架中。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [56] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: 提出MRD方法，用可微分的物理渲染搜索与模型激活等效的3D场景（模型“同形异体”），以诊断视觉模型对几何与材质等物理参数的敏感性与不变性。实验显示在保持高激活相似度下可得到外观多样的重建，揭示模型对形状/材质的表征特点。


<details>
  <summary>Details</summary>
Motivation: 深度视觉模型虽在2D任务上表现出色，但其是否、以及如何隐式表征3D物理世界仍不清楚。现有分析多基于像素或非物理约束，难以将模型行为与真实可解释的物理因素对应起来。需要一种在物理可解释空间中探针模型表征的方法。

Method: 构建MRD：将目标图像送入模型，使用可微分的物理渲染器参数化3D场景（几何、材质、光照等），通过梯度优化寻找能诱发与目标图像相同模型激活的场景参数，即模型的3D“变形同现”(metamers)。可在控制某些物理变量不变的条件下，探索模型对其他变量的敏感性。

Result: 在多种视觉模型上，优化得到的场景与目标在模型激活空间高度相似，但视觉外观可能差异显著。按需分别评估几何与BRDF材质的可恢复性与不变性，结果揭示不同模型对物理属性的敏感度差异。

Conclusion: MRD以物理可解释的场景参数为基底来分析模型表征，能系统检验模型对形状、材质、光照等的敏感性/不变性，为理解计算机视觉与人类视觉提供新工具。

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [57] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 提出WeDetect家族：以检索为核心的非融合开放词汇检测，实现实时、SOTA性能，并统一检测、通用候选生成、历史对象检索与指代表达理解，且高效推理。


<details>
  <summary>Details</summary>
Motivation: 开放词汇检测常需跨模态融合层，推理慢、扩展性差；而将识别视作“区域-文本”检索的非融合范式潜力未被充分挖掘。作者希望在效率、通用性与能力边界上系统证明非融合方案的竞争力，并拓展到候选生成、对象检索与复杂指代理解。

Method: 提出双塔非融合架构WeDetect，在精心构建的数据与全量训练下优化检索能力；基于其冻结检测器，仅微调“objectness提示”得到WeDetect-Uni，输出类特异的proposal嵌入以支持历史对象检索；进一步用LMM构建WeDetect-Ref，将复杂自然语句映射为对proposal的单步分类，摒弃自回归的下一词预测，实现高效REC。

Result: WeDetect在实时速度下超越融合式SOTA，作为开放词汇基础模型表现突出；WeDetect-Uni无需解冻主干即可生成高质量、可检索的类特异proposal，并实现对历史数据的对象检索；WeDetect-Ref在REC上通过一次前向实现高效精确分类。整体在15个基准上达成SOTA并保持高推理效率。

Conclusion: 非融合的检索范式在开放词汇检测中不仅可达SOTA，还能统一检测、proposal生成、历史检索与REC，兼具效率与多样性；通过冻结主模型与轻量提示微调，实现快速适配与应用扩展。

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [58] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: UniCoDe 是一种在扩散模型对齐中统一“采样选择”和“梯度引导”的推理期、免训练算法，通过在采样过程中注入局部梯度信号来提高对齐奖励与效率之间的权衡，在多任务上与SOTA相当。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型对齐主要有两类：1) 采样型（多候选、按奖励筛选），但效率低；2) 梯度引导型（可微奖励直接引导），但可能偏离无条件先验、易不稳。缺乏能兼顾效率、对齐程度与分布保真度的通用方法。

Method: 提出 UniCoDe：在扩散采样过程中无训练地融合局部梯度信号与候选评估。具体地，一边进行扩散步进，一边利用可微奖励的局部梯度对更新方向进行微调，同时保留采样与筛选的机制，从而在探索（采样）与利用（梯度）间平衡，减少纯采样法的低效并控制偏离无条件先验。

Result: 在多项下游任务上，UniCoDe 的奖励对齐度与样本质量达到或接近当前最优基线，同时在同等或更少的采样成本下取得更高的效率与更好的对齐-发散权衡。

Conclusion: 将采样法与梯度引导法统一可在无需额外训练的前提下提升扩散模型对齐效率与效果；UniCoDe 在多任务上验证了其通用性与竞争力。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [59] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: 提出Daylily-Leaf病斑级数据集与TCLeaf-Net检测器，针对田间复杂背景、下采样信息损失与尺度/特征漂移，通过TCM、RSFRS、DFPN三模块实现，在自建数据集田间划分上mAP@50达78.2%，较基线提升5.4个百分点且更省算力，并在多数据集上具备较强泛化。


<details>
  <summary>Details</summary>
Motivation: 真实田间环境中背景杂乱、域移与病斑级标注数据稀缺，导致现有病害检测鲁棒性差、泛化差、计算负担大；需构建高质量、跨场景数据并设计更适配田间应用的高效检测框架。

Method: 1) 数据：发布Daylily-Leaf，包含1746张RGB图与7839个病斑，含理想与田间场景的成对样本。2) 模型：提出TCLeaf-Net（Transformer+卷积混合）。- TCM：融合全局上下文与局部卷积，抑制非叶片区域干扰。- RSFRS：结合双线性重采样与卷积，在下采样时回忆并保留细粒度空间信息。- DFPN：可变形对齐+多感受野特征金字塔，加强多尺度融合与尺度鲁棒性。3) 评价：与基线、YOLO、RT-DETR等比较，并跨数据集验证。

Result: 在Daylily-Leaf田间划分上，mAP@50提升5.4个百分点至78.2%，同时推理计算量降低7.5 GFLOPs、显存占用降8.7%；在精度与召回上优于近期YOLO与RT-DETR系列；在PlantDoc、Tomato-Leaf、Rice-Leaf上获得优良结果，表现出较强鲁棒性与可迁移性。

Conclusion: 结合新数据集与混合检测框架，TCLeaf-Net有效缓解背景干扰、下采样信息损失与尺度/特征偏移问题，在真实性能与效率上优于主流方法，并能泛化到多种作物病害检测场景，适用于真实田间应用。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [60] [VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding](https://arxiv.org/abs/2512.12360)
*Yufei Yin,Qianke Meng,Minghao Chen,Jiajun Ding,Zhenwei Shao,Zhou Yu*

Main category: cs.CV

TL;DR: 提出VideoARM：一种面向长视频理解的“代理式推理+层级记忆”方案，在不做耗Token的静态预处理下，通过自适应的观察-思考-行动-记忆循环与分层多模态记忆，达到更少Token、优于SOTA（DVD）的效果。


<details>
  <summary>Details</summary>
Motivation: 长视频具有长时间跨度与密集多模态线索，现有方法依赖手工设计的推理流程或高成本的预处理，导致灵活性差、Token消耗大，限制了多模态大模型的自主推理效率与性能。

Method: 提出VideoARM框架：以控制器为核心，执行“观察-思考-行动-记忆”的连续自适应循环，按粗到细调用工具解析视频；并行构建与更新分层多模态记忆，持续汇聚多层级线索，为控制器提供精确上下文以支持决策，从而减少冗余处理与Token占用。

Result: 在主流长视频基准上，VideoARM在准确率等指标上优于当前SOTA方法DVD，同时显著降低Token消耗。

Conclusion: 动态的代理式推理与层级记忆结合，替代静态、全面的预处理，可在长视频理解中实现更高效、更强性能的推理；证明了自适应工具调用与分层记忆对长序列多模态任务的有效性。

Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.

</details>


### [61] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: 提出STAGE：以故事板为锚的多镜头视频生成流程，通过结构化起止帧预测与记忆/编码/训练策略，提升叙事控制与跨镜头一致性，并发布大规模ConStoryBoard数据集。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频模型虽能合成高保真单段视频，但难以在多镜头场景中保持人物/场景一致与符合电影语言的叙事连贯；端到端方法代价高，关键帧方法虽高效可控，却易丢失跨镜头一致性与电影化转场。

Method: 1) 将任务重构为“结构化故事板”生成：提出STEP2，为每个镜头预测起止帧对而非稀疏关键帧。2) 多镜头记忆包（multi-shot memory pack）维护长程实体一致性。3) 双重编码策略（dual-encoding）确保镜头内连贯。4) 两阶段训练（two-stage training）学习电影化的镜头间转场。5) 构建ConStoryBoard数据集，含电影片段与关于故事进程、电影属性和人类偏好的细粒度标注。

Result: 在多镜头叙事控制与跨镜头一致性指标上取得优于现有方法的性能；实验显示生成视频更好地遵循结构化故事板并呈现更自然的镜头间过渡。

Conclusion: 以故事板为锚的工作流与结构化起止帧对预测，配合记忆、编码与训练策略，可显著提升多镜头视频生成的叙事可控性与一致性；所提数据集为该方向研究提供了坚实基础。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [62] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: V-Warper是一种无需视频再训练的个性化视频生成框架，通过“图像端轻量适配 + 推理时细粒度外观注入”两阶段，在保持文案与运动一致的同时显著提升跨帧外观一致性与身份忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法依赖重视频微调或大数据集，计算成本高、难扩展，且仍难以维持精细的跨帧外观一致性。需要一种既高效又能提升细节身份保真的方案。

Method: 提出训练免（training-free）、用于Transformer式视频扩散模型的两阶段粗到细框架：1) 粗外观适配：仅用少量参考图像，通过图像端LoRA和主体嵌入（subject embedding）适配，编码全局主体身份；2) 推理时细外观注入：从去RoPE的中层query-key特征计算语义对应关系，用这些对应关系引导将富含外观信息的value表示“变形/扭曲-对齐”到生成过程中的语义一致区域，并结合掩膜保证空间可靠性。

Result: 无需大规模视频微调即可显著提升外观保真度与身份一致性，同时保持文本对齐与运动动态；效率更高、资源需求更低。

Conclusion: V-Warper通过训练免的粗到细外观对齐策略，有效解决视频个性化中外观一致性与计算成本的权衡问题，提供可扩展、实用的高保真个性化视频生成方案。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [63] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: M4Human 提供目前最大规模的多模态（mmWave雷达＋RGB＋深度）人体网格重建基准，含高质量MoCap网格与轨迹标注，用于推进基于雷达的人体建模与多模态融合研究。


<details>
  <summary>Details</summary>
Motivation: 现有HMR大多依赖可见光RGB，容易受遮挡、光照与隐私限制；已有雷达数据集规模小、标注稀疏、动作简单，难以支撑复杂场景的人体网格重建研究。

Method: 构建M4Human数据集：收集高分辨率mmWave雷达数据，提供原始雷达张量（RT）与处理后的雷达点云（RPC），同步RGB与深度；用高质量MoCap生成3D网格与全局轨迹标注；涵盖20名受试者、50类多样动作（原地、坐姿原地、自由空间体育/康复）。在RT、RPC及与RGB-D融合上建立基线与评测基准。

Result: 得到661K帧（比此前最大数据集大约9倍）的多模态数据与配套基线结果；实验显示该数据集对雷达驱动的人体建模价值显著，但在快速、无约束运动下仍存在困难。

Conclusion: M4Human填补了雷达HMR在规模、标注质量与动作多样性上的空缺，为单/多模态（雷达与RGB-D融合）人体网格重建提供标准评测平台；然而快速复杂动作仍是待攻克难点。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [64] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: SR-DiT 将多种提速与稳训技术系统化整合，在小模型上实现与大模型相当的扩散生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer的提效方法（如token路由、结构改进、训练技巧）多被单独研究，缺乏体系化组合与相互作用分析；同时希望在小参数量与更少训练步数下达到高质量生成表现，降低计算门槛。

Method: 以“表示对齐”为基础，统一集成三类技术：1) token routing（例如对重要token优先计算/跳跃不重要token）；2) 架构级改进（DiT结构细化与高效块设计）；3) 训练策略修改（稳定/加速收敛的损失、正则与调度等）。进行系统性消融，评估各技术的独立与组合效果，并记录协同与不兼容性。

Result: 在ImageNet-256上，140M参数模型、40万迭代、无CFG条件下达到FID 3.49、KDD 0.319，接近/匹敌以往需685M参数且更长训练的结果；为同规模模型的最新SOTA。

Conclusion: 多种提效与稳训技术存在明显协同效应；经过系统整合与筛选，可显著降低训练成本而保持高质量生成。SR-DiT提供一个计算亲民、可复现的基线以推动后续研究。

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [65] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: ArtGen是一种条件扩散框架，可从单视图图像或文本在任意部件状态下生成具有精确几何与连贯运动学的可动三维对象，通过跨状态采样与推理模块解耦形状与关节动态，并在PartNet-Mobility上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多以单视图闭合状态为条件，几何与关节运动学纠缠，导致生成的关节结构含糊或不真实；需要能在多部件状态下同时保证几何精度与全局运动一致性的生成模型。

Method: 提出ArtGen：1) 条件扩散框架，输入为单视图图像或文本；2) 跨状态蒙特卡洛采样，显式施加全局运动学一致性，降低结构-运动纠缠；3) 链式思维推理模块，推断结构先验（部件语义、关节类型、连通性），引导稀疏专家的扩散Transformer处理多样关节交互；4) 具局部-全局注意力的组合式3D-VAE潜空间，兼顾细粒度几何与部件级全局关系。

Result: 在PartNet-Mobility基准上，几何精度与运动学一致性等指标均显著优于现有方法，实现SOTA；可在任意部件状态条件下稳定生成。

Conclusion: 通过跨状态一致性约束、结构先验推理和专门化扩散架构，ArtGen有效解耦几何与运动学，能从单视图或文本生成具真实关节与高质量几何的可动3D资产，优于当前方法。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [66] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

TL;DR: 提出一种基于图注意力网络（GAT）的点云补全方法，面向旋转式LiDAR因硬件老化或环境因素导致的垂直束丢失，仅用当前帧点云在图结构上回归缺失z高度，KITTI上达成约11.7 cm RMSE与88%≤10 cm精度，但推理约14.7 s/帧。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的旋转式LiDAR在老化、灰尘、雪雾、强反射等情况下会出现整条垂直通道掉线，造成点云出现整片垂直缺口，严重影响3D感知。现有方法多依赖相机/时序或规则插值，鲁棒性与部署成本受限，亟需一种仅依赖当前帧几何的鲁棒重建方案。

Method: 将每帧点云构建为无结构空间图：点为节点，边连接近邻并保留原始束索引顺序。使用多层图注意力网络在局部几何邻域上自适应学习注意力权重，直接对掉线位置回归缺失的z（高度）值。训练与评估在KITTI原始序列上通过模拟通道掉线进行；考察不同近邻规模k的稳定性。

Result: 在1065段KITTI原始序列的模拟垂直通道丢失上，平均高度RMSE为11.67 cm，87.98%的重建点误差≤10 cm；不同k下重建质量稳定；单GPU推理约14.65 s/帧。

Conclusion: 纯基于点云几何的GAT模型无需相机或时序即可有效恢复现实退化条件下的垂直束缺失，但当前推理速度较慢，表明仍需在效率上优化以满足在线应用。

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [67] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: 提出并评测首个越南语信息图VQA基准ViInfographicVQA，含6,747张信息图与20,409问答，涵盖单图与多图推理；现有多模态模型在多图跨图整合与非片段型推理上错误最多，暴露低资源场景下版面与跨图推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 信息图融合文本、图表、图标与设计布局，需OCR、版面理解、数值与语义推理的综合能力。现有VQA多聚焦自然场景文本或实物图像，越南语与跨图推理评测缺失，亟需系统基准揭示模型短板。

Method: 构建越南语信息图VQA数据集：收集6,747张真实信息图（经济、医疗、教育等领域），标注并人工核验20,409问答；设计两种评测：单图任务与多图任务（跨多张语义相关信息图综合证据）；基准评测多种最新视觉-语言模型，分析错误类型。

Result: 各模型在单图任务上表现尚可，但在多图任务上性能显著下降；主要错误集中在跨图证据整合与非span式（非直接可抄录）推理；显示不同模型间存在较大差距。

Conclusion: ViInfographicVQA为越南语信息图VQA提供首个系统基准，揭示当前多模态模型在低资源环境下对版面与跨图推理的不足，呼吁发展布局感知与跨图推理方法，并为后续研究提供参照。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [68] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: 提出BokehDepth：将散景合成与深度预测解耦，利用无监督的离焦线索提升散景质量与单目度量深度精度。


<details>
  <summary>Details</summary>
Motivation: 现有散景渲染依赖噪声深度图导致伪影；单目度量深度在弱纹理、远景、几何歧义区域表现差，恰是离焦最有信息处。需要一种既能稳定合成高质量散景，又能把离焦作为几何线索增强深度估计的方法。

Method: 两阶段框架：Stage-1 用物理引导、可控的散景生成器（基于强大的预训练图像编辑骨干）从单张清晰图像生成“与深度无关”的、具有标定散景强度的散景堆栈；Stage-2 在现有单目深度编码器中插入轻量的“离焦感知聚合”模块，沿“离焦维度”融合特征，显式暴露稳定的与深度相关变化；下游解码器保持不变。

Result: 在多项具有挑战性的基准上：相较基于深度图的散景基线，视觉保真度更高；对强大的单目深度基础模型，度量精度与鲁棒性均持续提升，尤其在弱纹理、远景、歧义区域。

Conclusion: 将离焦作为无需标注的几何监督信号，通过解耦的两阶段设计，同时提升散景合成质量与单目度量深度性能，可作为现有深度框架的可插拔增强。

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [69] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: Endless World 提出一个可实时生成“无限时长”、3D一致的流式视频系统，结合自回归条件训练与全局3D感知注意力，在单GPU上实现长时稳定、几何一致的视频生成，并在视觉质量与空间一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成在长时序和流式场景中易出现漂移、破碎的3D结构与不连贯内容；同时推理成本高，难以实时与无限延展。作者旨在在单GPU上实现长时、稳定、3D一致且可无限延展的实时视频生成。

Method: 1) 条件自回归训练：将新生成帧与历史视频对齐，保留长程依赖并提高推理效率，用于无限延展与流式生成。2) 全局3D感知注意力：在时间轴上注入全局几何引导，保持连续的3D结构。3) 3D注入机制：在特征层面施加物理可行性与几何一致性约束，缓解长时与动态场景中的结构漂移。系统可在单GPU上实时推理，无需额外训练开销。

Result: 在多项实验中，模型可生成长且稳定、视觉连贯的视频；在视觉保真度与空间一致性指标上达到与现有方法相当或更优的表现；支持实时、无限时长输出。

Conclusion: Endless World 实现了实时、无限延展、3D一致的视频生成，解决了长时序下的几何漂移与不连贯问题，在效率与质量上兼顾，具有实际部署潜力。

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [70] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 提出Gaussian Photon Field（GPF），把光子映射重构为可复用的连续辐射度场，实现多视图高效渲染，保持光物理精度同时显著降算。


<details>
  <summary>Details</summary>
Motivation: 传统光子映射在多视图渲染时需对每个视点独立追踪光子并进行随机核估计，造成重复计算与低效；希望在不牺牲对焦散与镜面-漫反射等复杂全局光照精度的前提下，构建可跨视图复用、可微且高效的辐射评估方式。

Method: 将光子分布参数化为各向异性三维高斯基元（位置、旋转、尺度、光谱），形成可学习的Gaussian Photon Field。以首轮SPPM物理追踪的光子初始化，再用多视图最终辐射监督优化，将基于光子的传输“蒸馏”为连续场；训练后沿相机光线进行可微辐射求值，无需重复追踪/迭代。

Result: 在包含焦散与镜面-漫反射耦合等复杂光传输的场景中，GPF达到接近光子级的准确度，同时将计算成本降低数量级，相较传统多视图光子映射显著加速。

Conclusion: GPF把物理严谨的光子映射与神经场效率统一：用可学习的高斯场复用跨视图辐射，支持可微评估，显著提速且保持高保真，适合多视图真实感渲染。

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [71] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: PeRL-VL在RLVR基础上，将“感知”和“推理”解耦优化：用描述奖励提升图像信息抽取的真实充分性，再用文本推理SFT提升链路一致性，显著提高多模态长链推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于RLVR的多模态模型虽能用可验证信号强化最终答案，但仍常见两类错误：视觉细节抽取不准（缺失/幻觉）与推理链逻辑不一致，其根源在于监督仅作用于最终答案，无法约束中间感知与推理过程。

Method: 提出PeRL-VL：1) 感知侧：设计VLM驱动的“描述奖励”，对模型自生成的图像描述从“忠实度+充分性”打分，用于优化视觉信息抽取；2) 推理侧：引入文本-only的Reasoning SFT，在富含逻辑的CoT数据上微调，独立于视觉提升推理连贯与一致性；3) 在RLVR之上以解耦方式联合两者。

Result: 在多种多模态基准上，Qwen2.5-VL-7B的Pass@1由63.3%提升到68.8%，优于标准RLVR、纯文本推理SFT和直接用GPT-4o进行多模态蒸馏。

Conclusion: 对可验证奖励的末端监督进行解耦强化：分别优化感知与推理可显著缓解视觉幻觉与逻辑不一致，带来稳健的多模态长链推理提升。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [72] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: 提出AdaptiveDetector：YOLOv11探测器+VLM验证器，两阶段、带自适应阈值与成本敏感RL微调，在合成退化的内镜图像上实现显著提高召回且精度基本不降，减少漏检息肉风险。


<details>
  <summary>Details</summary>
Motivation: 临床内镜存在亮度变化、运动模糊、遮挡等不良成像条件，导致在干净数据上训练的息肉检测器在真实部署中性能下降；现有方法难以跨越实验室与临床之间的域间鸿沟，尤其在漏检方面临床成本极高。

Method: 两阶段“检测器-验证器”框架。1) 探测器：YOLOv11在VLM指导下对每帧自适应调整置信度阈值；2) 验证器：采用视觉-语言模型，使用GRPO进行微调，并设计非对称、成本敏感的奖励函数强力惩罚漏检；3) 构建合成测试平台：对干净数据集系统性施加临床常见退化（光照、模糊、遮挡等），用于零样本评估。

Result: 在合成退化的CVC-ClinicDB与Kvasir-SEG上零样本评测：相较单独YOLO，召回提升14–22个百分点；精度变化在-0.7到+1.7个百分点内，整体漏检显著减少。

Conclusion: 自适应阈值+成本敏感强化学习的两阶段检测-验证方案，在开放世界、恶劣成像条件下实现临床一致的息肉检测：明显减少假阴性，维持精度，潜在提升临床检出率与患者预后。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [73] [Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention](https://arxiv.org/abs/2512.12498)
*Tasweer Ahmad,Arindam Sikdar,Sandip Pradhan,Ardhendu Behera*

Main category: cs.CV

TL;DR: 提出一种基于补丁关系的缓存适配方法，通过图注意力精炼补丁表示，仅在训练期引入图模块，推理仍与标准缓存检索同价，显著提升少样本跨域分类表现。


<details>
  <summary>Details</summary>
Motivation: 现有如Tip-Adapter的缓存式适配在少样本与域移情景下受限：CLIP的全局、通用表征在低数据下不够判别，难以把“通才”适配为“专才”。需要在不增加推理成本的前提下，挖掘图像内部局部依赖，提升缓存键与目标类别的对齐度。

Method: 提出补丁驱动的关系精炼：将图像划分为补丁并构建补丁图，使用“关系门控图注意力网络”（边感知注意力）加强信息性补丁间交互，得到上下文增强的补丁嵌入；再用可学习的多聚合池化将其汇聚成紧凑、任务判别的表示，用于学习缓存适配器权重。训练期以此蒸馏关系结构到缓存中；推理期仅进行标准缓存查找。最终预测为缓存相似度与CLIP零样本logits的残差融合。

Result: 在11个基准上相较最先进的CLIP适配与缓存方法实现一致提升，并保持零样本级的高效推理；同时在新引入的“受伤/未受伤士兵”数据集上验证对战场伤情识别的实用价值。

Conclusion: 利用补丁级关系图在训练中增强缓存表示，可在不增加推理开销的情况下显著改进少样本跨域分类，并具备实际应急救援与战场分诊场景的应用潜力。

Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

</details>


### [74] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 利用视频基础模型进行时空数据增广，从单幅图像数据集中合成具有多视角与动态变化的视频片段，并用于训练，在低数据场景取得稳定提升。


<details>
  <summary>Details</summary>
Motivation: 传统增广多为几何或外观扰动，难以扩展真实的视角变化与时间动态；在无人机等标注稀缺场景，需更丰富的时空多样性以提升模型泛化。

Method: 使用现成的视频扩散（foundation）模型，从原始图像生成逼真的3D空间与时间变化的视频片段；将其作为补充训练数据，并给出三项实用流程：选择合适的时空生成配置、将原标注转移到合成帧、处理新暴露且无标注的遮挡解除区域。

Result: 在 COCO 子集与无人机数据集上，在低数据设定下持续带来性能提升，覆盖了传统与以往生成式增广方法难以触达的分布轴向。

Conclusion: 合理应用的时空增广能在数据稀缺条件下有效扩展数据分布与提升性能，并配套提供配置选择、标注迁移与遮挡解除处理的实操指南。

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [75] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D 提出一个文本驱动的 3D 动画框架，可从静态 3D 资产和文字生成高保真、幅度更大的运动场。核心是以“运动得分蒸馏（MSD）”替代传统 SDS，从视频扩散模型稳健蒸馏运动，同时配合时间/空间正则与运动细化模块，提升时空一致性与细节。


<details>
  <summary>Details</summary>
Motivation: 基于文本的视频扩散蒸馏到 3D 动画常出现两大问题：1) 使用标准 SDS 会导致运动幅度小或抖动明显；2) 外观易被破坏、时间/空间几何一致性差，且受视频模型固定分辨率限制，缺乏细节与高时间分辨率。作者希望在保持外观完整的前提下，显著增强运动强度与稳定性。

Method: 1) 提出 Motion Score Distillation (MSD)：以 LoRA 增强的视频扩散模型，将蒸馏的“源分布”从纯噪声改为静态源分布；结合基于反演的噪声估计，保证在引导运动时保留外观。2) 加入显式的时域与空域正则，抑制跨帧与空间的几何畸变，提升运动保真与稳定性。3) 运动细化模块：对时间分辨率进行上采样并增强细节，突破底层视频模型固定分辨率的限制。

Result: 在多种文本提示与静态 3D 资产上进行大量实验，相比 SOTA 基线生成更大幅度、更细节、且更稳定的运动，同时保持高视觉完整度。

Conclusion: Animus3D 通过 MSD、时空正则与运动细化，有效解决了基于 SDS 的运动弱与抖动问题，能从静态 3D 资产生成高保真、细节丰富且稳定的文本驱动 3D 动画。代码将开源。

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [76] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 该研究提出一种结合心肌先验、结构感知编码与3D小波-逆小波变换的冠状动脉CTA分割框架，在ImageCAS数据集上取得Dice 0.8082、Sensitivity 0.7946、Precision 0.8471、HD95 9.77 mm，优于多种主流方法。


<details>
  <summary>Details</summary>
Motivation: 冠脉CTA分割受小血管口径、复杂分叉、边界模糊与心肌干扰影响，现有方法在稳定性与细粒度结构保持方面不足，难以为定量分析与临床决策提供可靠支撑。

Method: 框架由三部分组成：1) 在编码阶段引入心肌解剖先验与残差注意力增强，强化对冠脉结构的表征；2) 采用三维小波/逆小波替代传统下/上采样，联合建模空间与频域信息，保持多尺度结构一致性；3) 解码端多尺度特征融合模块整合语义与几何信息。训练在ImageCAS上，使用3D重叠patch，数据按7:1:2划分为训练/验证/测试。

Result: 在测试集上取得Dice 0.8082、Sensitivity 0.7946、Precision 0.8471、HD95 9.77 mm，整体优于多种主流分割模型。消融实验验证各组件（心肌先验、残差注意力、小波上下采样、多尺度融合）的互补效益。

Conclusion: 融合解剖先验与频域建模的框架能在复杂几何条件下实现更稳定一致的冠脉分割，为后续冠脉结构定量分析提供可靠分割结果。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [77] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: 提出一种监督式对比学习的影片表征框架：把视频多帧聚合为单张“拼接图”，用CNN骨干做投影，对同类视频投影做拉近、其余拉远；通过不同时间采样产生多视图，减少过拟合，精度高且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有视频表征依赖视频Transformer或重数据增广，计算昂贵且容易过拟合；同时难以捕获跨时间的全局上下文。作者希望：1) 利用成熟的2D CNN预训练能力；2) 在不依赖复杂增广与庞大算力下获得强视频表示；3) 在监督与自监督场景都适用。

Method: 1) 帧到图像聚合：将同一视频的多帧按空间布局拼接成单张输入，使ResNet50等2D CNN可直接处理并感知时间全局。2) 多视图生成：对同一视频进行不同时间采样，形成自然多视图而非强数据增广。3) 监督式对比目标：比较样本间投影，标签相同为正对，其余为负；直接对成对投影进行对比学习。4) 训练后可用于分类、字幕等下游任务；也支持自监督设置。

Result: 在Penn Action与HMDB51上优于现有方法：Penn Action分类准确率76%（对比ViVIT的43%）；HMDB51为48%（对比ViVIT的37%）。同时计算资源更省。

Conclusion: 通过帧聚合+监督对比学习，在不依赖视频Transformer与重增广的前提下，获得具时间全局上下文的强视频表示，兼具高效与准确，适用于监督与自监督并可推广到分类、字幕等视频任务。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [78] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 提出一种面向在线视频理解的高效MLLM上下文精简方法：基于空间相邻最大相似度(MSSAVT)的token冗余度度量＋掩码式剪枝以避免相互依赖＋结合时间冗余剪枝，在多基准上以<1ms额外延迟实现最高约4%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解需处理大量帧，MLLM上下文长度极长，导致显存占用高、推理延迟大；现有方法难在保证关键信息的同时有效压缩视频token，尤其对空间与时间冗余的联合处理不足。

Method: 1) 提出MSSAVT冗余指标：综合token间相似度与其空间相邻关系，衡量空间冗余；2) 设计“掩码式剪枝”：仅对互不相邻的token执行剪枝，缓解剪枝与冗余评估的双向依赖；3) 融合已有的时间冗余剪枝方法，联合去除空间与时间冗余；4) 面向在线推理的高效实现，控制剪枝开销(<1ms)。

Result: 在多项在线视频与离线视频理解基准上，较基线显著提升精度（最高约+4%），同时几乎不增加推理延迟（剪枝开销<1ms），显存与计算负担随上下文缩短而降低。

Conclusion: 空间-时间联合冗余感知的token剪枝能在不牺牲、甚至提升MLLM视频理解精度的同时显著降低上下文长度与资源开销；方法简单、高效，适合在线场景，并具备可复现实现。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [79] [From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models](https://arxiv.org/abs/2512.12571)
*Boyeong Im,Wooseok Lee,Yoojin Kwon,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: 提出MVP：把测试时自适应从数字层面转到物理成像层面，用相机曝光三要素作为“物理提示”，通过多视角采集、选择与投票在不改模型不反传的前提下显著提升VLM在物理环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多在网络图片上训练/评测，实际物理场景中受光照、曝光等成像条件影响大；数字增强式TTA对这类域偏移适应有限。作者希望在“测量时”而非“后处理”阶段控制成像，以更好匹配源域并提升鲁棒性与校准性，且无需模型改动与梯度。

Method: 将ISO、快门、光圈视作可控“物理提示”。测试时：1) 采集场景的多组物理视图（不同曝光三角组合）；2) 用source-affinity分数从候选中选top-k设置；3) 对保留视图施加轻量数字增强；4) 以熵为准则过滤出不确定性最低的增强子集；5) 用零温度softmax（硬投票）聚合预测。整个流程前向推理、无梯度、无需改模型，且校准友好。

Result: 在ImageNet-ES与ImageNet-ES-Diverse上，MVP相较仅数字TTA在单次自动曝光输入上最高提升25.6个点；相较“传统传感器控制+TTA”的流水线再额外提升最高3.4个点。在压缩候选参数集合以降低采集时延时仍保持有效，显示出实用性。

Conclusion: 将测试时自适应前移到“测量时”，通过选取并组合真实物理视图可显著增强VLM在物理环境的稳健性，优于单纯后采集数字提示/增强，且无需模型修改与反向传播。

Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.

</details>


### [80] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: 提出StegaVAR：把动作“秘密视频”嵌入普通“封面视频”中，并直接在隐写域做动作识别，以同时保隐私与保持识别精度；并提出STeP与CroDA两种模块缓解隐写域分析困难，在多数据集与多隐写模型上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有视频隐私保护多依赖匿名化/模糊化，造成两大问题：1) 低隐蔽性——视频外观明显失真，易引起传输时的注意；2) 时空破坏——关键时空特征被削弱，导致动作识别性能下降。需要一种既隐蔽传输又保留完整时空信息、还能实现高精度动作识别的方法。

Method: 构建StegaVAR框架：将“动作秘密视频”隐写进“封面视频”，在隐写视频上直接进行动作识别。为克服隐写域分析难题，提出两项技术：1) Secret Spatio-Temporal Promotion (STeP)：训练中用秘密视频对隐写域的时空特征提取进行引导，以提升对动作相关模式的感知；2) Cross-Band Difference Attention (CroDA)：通过捕捉不同频带/通道的跨带语义差异，抑制封面视频带来的干扰。框架可与多种隐写生成模型搭配。

Result: 在主流VAR数据集上，StegaVAR同时提升了动作识别精度与隐私保护效果；在传输过程中外观自然、隐蔽性强，且隐写域的分析准确率优于现有匿名化方案。方法对多种隐写模型具有鲁棒与通用性。

Conclusion: 把动作识别迁移到隐写域是可行且有效的：既保持秘密视频完整的时空信息，又显著提升传输隐蔽性。通过STeP与CroDA，可在隐写域中抵抗封面干扰并获得强性能，为隐私友好的视频动作识别提供新方向。

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [81] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: 提出一种半自动机器视觉系统，用于线束装配检测，实测达到100%检测准确率并将检验时间缩短44%。


<details>
  <summary>Details</summary>
Motivation: 线束人工检验在EMS行业中耗时、易错、成本高，尤其在不同形态（线性/环形）线束、极性与颜色序列核查方面，迫切需要更可靠高效的自动化手段。

Method: 构建模块化机电平台，集成5台工业CMOS相机；采用HSV与RGB双域颜色值比对的颜色序列分类器；基于至少5个参考样本进行批次训练，生成并复用训练文件；系统包含可调光照、用户管理、会话数据存储与安全登录。

Result: 在GPV Lanka Pvt. Ltd.落地应用，对线束位置、连接器极性与颜色序列实现100%检测准确率；相较人工检验将检测时间降低44%。

Conclusion: 该半自动机器视觉方案在实际生产中表现稳定可靠，显著提升检测效率与准确度，适用于类似线束类型的可复用部署。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [82] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: 提出一个将视觉增强LLM与先进Transformer架构融合的生成框架，通过整流流(rectified flow)与双向分词整合文本/图像/视频，实现高分辨率图像合成与多模态理解；在基准上提高清晰度25%，计算开销降20%，具备良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散/多模态生成方法在高分辨率合成、跨模态统一表示与噪声鲁棒性方面存在效率与质量瓶颈，需要一种既能高效建模噪声-数据映射，又能统一处理文本、图像与视频的框架。

Method: 1) 采用rectified flow将噪声与数据通过近线性路径连接，提升采样效率与稳定性；2) 设计双向tokenization，将文本、图像、视频编码为可互操作的token并实现跨模态对齐；3) 融合时空特征并使用混合的文本-图像序列建模；4) 引入噪声感知的学习算法以缓解噪声分布失配；5) 在Transformer骨干上进行端到端优化。

Result: 在标准基准上，生成图像分辨率清晰度提升约25%，计算需求较扩散式方法降低约20%；合成质量与多模态一致性显著改善，并展现出良好的伸缩性与适应性。

Conclusion: 视觉中心的LLM与rectified flow及双向分词的结合可在高分辨率生成与多模态理解上带来性能/效率兼得的收益，具有在自动系统、创意生成与视频分析中的应用潜力。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [83] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 提出用视觉-语言模型（VLM）理解背景图内容，生成广告版式的文本“布局方案”，再转为HTML布局，相比基于显著性的方法更优。


<details>
  <summary>Details</summary>
Motivation: 传统广告版式多依赖显著性图，难以捕捉图像的细粒度构图与语义，导致文本/Logo放置与场景语义不一致、遮挡主体或视觉层级不佳。作者希望利用VLM的物体识别与关系理解能力，使版式决策显式对齐背景语义。

Method: 两阶段流水线：1）用VLM解析背景图的物体类别、位置与空间关系，产出文本化“placement plan”（建议文字与Logo的区域、优先级与约束）；2）将该计划渲染为最终版式，生成HTML代码实现元素摆放。并对比基线（显著性导向方法）进行量化与质化评估。

Result: 实验显示该方法在客观指标与主观评测上均优于现有方法，生成的广告版式更好地避开关键主体、尊重语义结构，整体质量显著提升。

Conclusion: 显式引入背景语义与构图理解的VLM驱动布局生成，可明显提升广告版式质量；文本化中间计划与HTML渲染的两段式框架有效且通用，可拓展到其他版式生成任务。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [84] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 提出一种几何感知、场景一致的图像生成方法：在保持参考场景几何与外观的同时，根据文本描述在指定空间关系处生成实体。通过新数据构建管线与几何引导注意力损失，较现有方法在场景对齐与文本一致性上更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在“保留原场景”与“严格遵循文本指令”之间兼顾：要么高度还原环境但对提示不敏感，要么强贴合提示却破坏场景结构。需要一种同时维护场景几何与文本所述空间关系的生成机制。

Method: 1) 场景一致的数据构建管线：自动生成多样、具几何约束的训练对（含参考场景、目标实体及其与场景的空间关系）。2) 几何引导的注意力损失：利用跨视角/跨视图线索，引导扩散模型的注意力在空间上对齐，从而正则化模型的空间推理，提升实体与场景几何一致性。

Result: 在自建的场景一致性基准上，方法在自动指标与人类偏好评测中均优于SOTA：更好的场景对齐（环境保持度高）、更强的文本-图像一致性（实体位置/关系正确），并能产出多样化、几何连贯的构图。

Conclusion: 通过数据与损失的两项关键设计，可在不牺牲场景保真的前提下提高对文本空间指令的响应，生成在几何上连贯且与提示一致的图像，优于现有方法。

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [85] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim 是一种无需训练的缓存加速框架，跨时间步、网络结构块与空间token三层统一复用特征，通过“双阈值”控制实现“先猛推、再打磨、触发重置”，在多任务上显著降低延迟、保持甚至提升质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但推理代价大；时间步、网络深度、序列长度使计算随规模线性或超线性增长。已有缓存复用在时间步可带来大加速但易积累误差损伤保真；在块/Token层面更安全但节省有限。需要一个能跨层级、自适应权衡速度与质量的统一机制。

Method: 提出 X-Slim：训练无关、基于缓存的统一框架。核心是“双阈值控制器”把缓存变为“推进-抛光-重置”的流程：1) 在时间步层面强力复用直至接近“预警线”；2) 触发轻量的块/Token级刷新以抛光残余误差；3) 一旦越过“临界线”则执行一次完整前向重推以重置误差。各层配备上下文感知指标来决定何时/何处缓存或刷新。

Result: 在多任务上推进速度-质量前沿：FLUX.1-dev 与 HunyuanVideo 分别最高提速约4.97x与3.52x，感知损失很小；在 DiT-XL/2 上达到3.13x加速，并相较已有方法 FID 提升2.42。

Conclusion: X-Slim 通过跨时间步、结构与空间的统一缓存策略与双阈值控制，有效平衡速度与保真，在各类扩散生成任务中实现显著加速且保持/提升质量。

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [86] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: Patchify 提出一种无需微调的补丁级图像检索框架，用局部补丁对齐全局查询，兼顾准确性、可扩展性与可解释性；并提出定位感知指标 LocScore 评估检索区域是否对齐目标；结合PQ实现大规模高效检索并强调信息性特征压缩的重要性。


<details>
  <summary>Details</summary>
Motivation: 实例级图像检索在目标尺度、位置、外观变化下仍需找到同一实例，传统全局描述符方法难以同时兼顾准确定位与可解释性，且在大规模场景下需要高效与可压缩的表示与评估手段。

Method: 1) Patchify：将库中每张图划分为少量结构化补丁，提取局部特征；用查询的全局描述符与这些局部特征进行相似度匹配，实现空间对齐的检索与定位。2) LocScore：提出定位感知指标，评估检索到的区域是否与目标对齐。3) 大规模检索：采用Product Quantization进行压缩与高效检索，并在压缩前后挑选更具信息性的特征以提升性能。4) 在多数据集、多骨干和区域选择策略上系统实验，并与重排序管线结合。

Result: Patchify在多个基准上优于纯全局方法，并可与SOTA重排序方法互补；LocScore作为诊断工具能够刻画空间正确性；在PQ压缩下，通过使用更具信息性的特征显著提升大规模检索性能。

Conclusion: 补丁级局部匹配结合全局查询可在不微调的前提下实现高精度、可解释且可扩展的实例检索；LocScore提供面向定位的评估维度；信息性特征+PQ使大规模检索更高效且性能更佳。

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [87] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出D3D-VLP：一个将规划-落地-导航-问答统一到单一3D多模态链式推理中的动态3D视觉语言规划模型，并用碎片化监督的协同学习在大规模部分标注数据上训练，刷新多项 embodied AI 基准并在真实机器人上验证。


<details>
  <summary>Details</summary>
Motivation: 端到端具身智能缺乏可解释性与显式3D推理；模块化系统又割裂组件、难以利用跨模块协同。需要一种既统一又可解释、可从不完整监督中学习且可与在线学习结合的方法。

Method: 1) 动态3D链式思维（3D CoT）：在单一3D-VLM中统一规划、视觉落地、导航与问答，通过链式推理显式建模3D语义与动作序列；2) 碎片化监督的协同学习（SLFS）：基于掩码自回归损失，在海量、异构且部分标注的数据上训练，让不同CoT阶段互相约束与隐式监督；3) 构建包含1,000万混合样本、覆盖5K真实扫描与20K合成场景的数据集，兼容RL/DAgger等在线学习。

Result: 在R2R-CE、REVERIE-CE、NavRAG-CE等视觉语言导航、HM3D-OVON目标导航、以及SG3D任务式序贯落地与导航等基准上取得SOTA，并在真实移动操作实验中验证有效性。

Conclusion: 统一的动态3D链式推理结合碎片化监督学习，打通规划-感知-语言的跨模块协同，能在大规模部分标注数据与真实场景中实现强泛化与可解释的具身智能。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [88] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: 提出DMLR：一种测试时动态多模态潜在推理框架，通过置信度引导的潜在策略梯度优化与动态视觉注入，在不显著增加计算的情况下提升MLLM的感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有将CoT扩展到视觉的做法依赖显式逐步推理、感知-推理交互不稳定且计算开销大；人类思维更像是感知与推理的交替与互相促动，启发设计能在潜在空间中动态交织两者的机制。

Method: 在测试时引入Dynamic Multimodal Latent Reasoning（DMLR）：1）置信度引导的潜在策略梯度优化，直接优化模型的潜在think tokens以强化深度推理；2）动态视觉注入策略，在每个latent think token处检索最相关视觉特征并维护/更新最佳视觉patch集合，将其注入到think tokens中，实现视觉-文本动态交替。

Result: 在7个多模态推理基准与多种模型架构上，DMLR显著提升推理与感知性能，同时保持高推理效率。

Conclusion: 通过在潜在空间对“思考”进行优化并动态注入相关视觉信息，可在不依赖显式逐步推理和高额开销的前提下，稳定而高效地提升MLLM的跨模态推理表现。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [89] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: DiG 通过“找不同”代理任务，教会多模态大模型精细感知与空间推理，显著提升多项视觉理解基准与下游指代任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在粗粒度任务上表现优异，但在细粒度视觉感知与精准空间推理上仍有短板；缺乏可扩展、可控、带强监督的训练信号来系统性提升这类能力。

Method: 提出 Differential Grounding（DiG）代理任务：输入两张相似图像，无先验差异数量的情况下识别并定位所有差异；构建自动化的3D渲染数据生成流水线，能生成高质量、可控差异的成对图像；为缓解差异信号稀疏与优化不稳，采用课程学习，从单一差异逐步增加到多差异；在通用 MLLM 上进行训练/微调，并评估零样本或微调后的迁移能力。

Result: 在多种细粒度视觉感知基准上显著优于现有方法；学到的精细感知能力可有效迁移到标准下游任务（如 RefCOCO/+/g）与通用多模态感知基准，取得一致提升。

Conclusion: 将“差异定位”作为可扩展且鲁棒的训练代理任务，可以系统性增强 MLLM 的细粒度视觉推理与空间理解；基于3D渲染的数据与课程学习是关键，使得能力可泛化到真实下游任务。

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [90] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: 提出CARe方法解决OCTA与广域彩色眼底(wfCFP)在跨模态配准中大视野差异的问题，通过先裁剪再配准，结合RANSAC与多项式双重拟合，实现稳健对齐，并在60对数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态眼底图像配准方法假设视野差异小，遇到OCTA(小FoV)与wfCFP(大FoV)时失败。需要一种能处理大FoV差异并复用现有方法的简洁流程。

Method: CARe包含两步：1) Crop：利用视网膜生理结构先验，从目标wfCFP中裁剪出与源OCTA视野大致匹配的子图，使问题转化为小视野差异配准，从而可复用既有算法。2) Alignment：提出“双重拟合”对齐模块，先用RANSAC进行鲁棒初始匹配/模型估计，再进行基于多项式的坐标拟合精细化空间变换。

Result: 在作者构建的包含60对OCTA-wfCFP的测试集上，实验显示该方法在大视野差异场景下可行且有效，较直接使用现有方法显著提升配准性能。

Conclusion: 通过先裁剪后对齐的简单流程，CARe有效应对大FoV差异的跨模态眼底配准问题，能将复杂问题化简并提升稳健性，可作为连接现有小差异方法与更广泛临床场景的实用方案。

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [91] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出CogDoc：模仿人类“粗到细”阅读的统一推理框架，先低分辨率快速定位信息，再高分辨率聚焦深推理；训练上直接用RL优于先SFT再RL，因可避免策略冲突；7B模型在多模态文档基准上达SOTA，超越更大闭源模型如GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 长文档/多模态文档推理存在“可扩展性vs细粒度保真”矛盾：要么能处理超长上下文但抓不住细节，要么能看清细节但难以扩展到长文档。需要一种既能高效全局浏览又能在关键部分深度推理的统一框架。

Method: 提出CogDoc两阶段思维：1) Fast Reading：低分辨率、快速全局扫描以定位关键信息区域，提升可扩展性；2) Focused Thinking：对候选区域使用高分辨率输入进行细粒度、多步推理。训练上系统性比较后训练策略，发现“直接RL”在统一思维框架上更优于“SFT初始化+RL”，并分析SFT引入的策略冲突问题。

Result: 在文档视觉推理基准上，带有7B参数的CogDoc达到了同规模SOTA，且在具有丰富视觉细节的困难基准上显著超过更大闭源模型（如GPT-4o）。

Conclusion: 粗到细的统一思维与直接RL训练能同时兼顾长上下文可扩展与细节保真，缓解传统范式的核心权衡，并在实际基准上验证其有效性。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [92] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

TL;DR: 提出SSMT-Net：利用半监督与多任务Transformer提升甲状结节超声分割，在TN3K与DDTI上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 甲状结节超声分割面临边界模糊、尺度变化、标注数据稀缺与上下文利用不足，现有深度模型泛化与利用腺体上下文能力有限。

Method: 两阶段框架：1) 无监督阶段利用未标注数据增强Transformer编码器特征提取；2) 监督阶段联合优化三任务——结节分割、甲状腺腺体分割与结节尺寸估计，融合局部与全局上下文信息。

Result: 在TN3K和DDTI数据集上取得优于现有方法的精度与鲁棒性（SOTA表现），跨病例泛化更好。

Conclusion: 半监督+多任务+Transformer能有效缓解标注稀缺与边界/尺度挑战，提升临床可用性；方法具备真实场景落地潜力。

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [93] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: InteracTalker提出统一的语音手势与物体交互生成框架，通过多阶段训练在共享嵌入空间与自适应融合策略上实现高逼真、可控的人体动作，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只处理语音驱动手势或物体交互之一，缺乏同时涵盖语音与物体条件的综合数据与模型，限制了真实交互场景的应用。

Method: 1) 构建带物体交互标注的数据集：在现有文本到动作数据上增强细粒度的人-物体交互标注；2) 多阶段训练学习统一的动作-语音-提示（prompt）嵌入空间；3) 引入广义运动自适应模块（Generalized Motion Adaptation Module），在各自条件上独立训练并在推理时动态组合；4) 针对异构条件不平衡，设计扩散采样过程中的自适应融合策略，动态重加权语音、物体/提示等条件信号。

Result: 在同一框架下统一了共语手势生成与物体交互合成两项先前分离的任务；在两类任务上均优于以手势为主的扩散基线，生成更逼真、具物体感知的全身动作，并提升了灵活性与可控性。

Conclusion: 通过统一嵌入学习、条件自适应模块与自适应融合策略，InteracTalker实现对语音与物体交互的协同建模，显著提升动作生成的真实性与可控性，为交互式数字体验提供更通用的方案。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [94] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出用于开放世界DeepFake归因的CAL框架，通过信心感知的一致性正则与非对称信心强化，缓解伪标签偏差与已知未知类别信心失衡，并配合动态原型剪枝自估未知类别数，在基准与扩展数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有OW-DFA方法：对未知伪造类型的伪标签易被高/低置信度偏斜误导，训练产生偏置；且通常假定未知类别数量已知，不符合现实，限制了泛化与扩展性。

Method: 提出Confidence-Aware Asymmetric Learning（CAL），含两核心：1) Confidence-Aware Consistency Regularization（CCR）：依据归一化置信度对样本损失进行动态缩放，训练重心由高置信度逐步过渡到低置信度样本，缓解伪标签偏差；2) Asymmetric Confidence Reinforcement（ACR）：对已知与未知类别分别进行置信度校准，仅在高置信度样本上选择性学习，并利用二者置信度差引导学习，实现非对称强化。二者互补形成正反馈。另引入Dynamic Prototype Pruning（DPP）：以由粗到细的方式自动估计并剪枝原型，从而推断未知伪造类型数量，去除先验假设。

Result: 在标准OW-DFA基准与包含更先进操控的扩展基准上，CAL在已知与未知伪造归因上均显著优于现有方法，取得新的SOTA表现。

Conclusion: 通过信心感知与非对称强化相结合，并辅以动态原型剪枝以自适应未知类别数量，CAL有效解决了OW-DFA中的置信度偏斜与未知类数先验假设问题，显著提升开放世界DeepFake归因能力与可扩展性。

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [95] [Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation](https://arxiv.org/abs/2512.12673)
*Yushun Tang,Ziqiong Liu,Jiyuan Jia,Yi Zhang,Zhihai He*

Main category: cs.CV

TL;DR: 提出PCSR方法，在Transformer的每层用条件化的尺度-平移因子对自注意力(Q/K/V)做线性重标定，配合在线学习的域分离与因子生成网络，实现在线测试时自适应，在ImageNet-C上分类精度最高提升3.9%。


<details>
  <summary>Details</summary>
Motivation: Transformer在新目标域推理时，自注意力的Query/Key/Value统计与源域差异显著，导致性能大幅下降；现有在线测试时自适应方法难以直接针对注意力内部表征漂移进行精细校准。

Method: 把从源域到目标域的在线自适应视作逐层的“渐进式域偏移分离”。在每个Transformer层，训练一个轻量的域分离网络(DSN)提取域偏移特征，并由因子生成网络(FGN)基于该特征预测自注意力的尺度(scale)与平移(shift)参数，通过局部线性变换对Q/K/VC做重标定。DSN与FGN在推理阶段按样本序列在线更新。

Result: 在多个基准上验证，PCSR显著提升在线测试时域自适应性能；在ImageNet-C上分类准确率最高提升3.9%。

Conclusion: 针对目标域导致的注意力表征漂移，PCSR以逐层条件化尺度-平移重标定实现高效在线适配，带来稳定且可观的性能增益。

Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.

</details>


### [96] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Scone提出一种统一的“理解-生成”框架，把多主体合成与主体区分同时解决，并发布了评测基准SconeEval；实验显示其在合成与区分两方面优于开源方法。


<details>
  <summary>Details</summary>
Motivation: 现有多主体图像生成虽能把多个主体放入同一画面，但缺乏“区分能力”，在输入包含多个候选主体时难以识别并生成正确目标，限制了复杂现实场景的适用性。

Method: 提出Scone框架：将系统分为“理解专家”和“生成专家”，前者作为语义桥梁，传递语义并指导生成保持主体身份、减少相互干扰。训练采用两阶段：先学合成能力，再通过语义对齐与基于注意力的掩蔽机制强化区分能力。同时构建SconeEval基准，覆盖多种情景，联合评估合成与区分。

Result: 在两个基准上，Scone在主体合成和主体区分任务上均优于现有开源模型。

Conclusion: 将合成与区分统一到一个理解-生成范式，并配合两阶段训练与注意力掩蔽，可显著提升多主体场景中的身份保持与干扰抑制；所提SconeEval为该任务提供系统化评测。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [97] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: β-CLIP提出多粒度、层次化的图文对齐框架，通过跨注意力为不同文本粒度（整句/子句/短语）动态汇聚图像patch，实现细粒度区域-文本对应；并以β-情境化对比损失在严格匹配与同图上下文宽松匹配间调节，显著提升密集对齐与细粒度检索的SOTA表现（无hard negatives）。


<details>
  <summary>Details</summary>
Motivation: CLIP虽在零样本检索表现强，但对细粒度、区域级对齐能力不足，即便在长描述上微调也不理想。需要一种能同时处理不同文本粒度并与对应视觉区域对齐的训练机制，兼顾查询特异性与图内上下文共享语义的挑战。

Method: 1) 多粒度文本条件对比学习：将文本分解为多层粒度（全句、子句、短语），对每层级使用跨注意力对图像patch进行动态加权汇聚，得到情境化视觉嵌入；2) β-情境化对比对齐损失（β-CAL）：通过β参数在“严格的查询-区域匹配”和“图内上下文放宽”之间可调，支持软的交叉熵和硬的二元交叉熵两种形式；3) 端到端训练，不依赖硬负样本。

Result: 在密集对齐与细粒度检索上显著提升：Urban1K数据集T2I与I2T的R@1分别达91.8%与92.3%；FG-OVD（Hard）R@1为30.9%，在不使用硬负样本的方法中达到SOTA。

Conclusion: β-CLIP通过跨注意力的多粒度视觉汇聚与可调β-CAL损失，实现文本多层级与图像区域的层次化对齐，构建了稳健、可适配的密集视觉-语言对应基线，并在多个基准上取得SOTA，无需硬负样本。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [98] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: 提出一种在视觉-语言模型中进行自适应token剪枝（ATP）的推理机制，在不改骨干的前提下降低约40% FLOPs、1.5×时延，准确率几乎不降。


<details>
  <summary>Details</summary>
Motivation: 现有VLM对所有视觉token一视同仁，计算开销大，不利于真实场景部署；需要一种能根据上下文动态挑选最有信息量token的方法，同时兼顾效率、准确性与鲁棒性。

Method: 在视觉-语言接口处引入轻量gate模块ATP：为每个视觉token计算混合重要性分数=ViT CLS注意力（模态内显著性）+ CLIP图文相似度（跨模态相关性），据此保留top-K给LLM处理；不改动主干，适配BLIP-2、LLaVA、Flamingo等；相较静态压缩，可按输入自适应剪枝。

Result: 在VQAv2、GQA、COCO等上，推理FLOPs降约40%，端到端延迟加速约1.5×，精度下降<1%；定性结果显示保留视觉对齐与可解释性；在扰动/腐化下更稳定，抑制虚假相关。

Conclusion: ATP在不牺牲准确性的前提下显著提升VLM推理效率，并增强鲁棒性与可解释性，表明资源受限推理与可靠性可兼得，适用于边缘多模态计算流程。

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [99] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

TL;DR: 从海量网络视频中提取人体动作，但帧内身体局部常缺失；作者提出基于“可信部件”的分部编码与掩码自回归生成框架，并发布约20万序列的新基准K700-M，在干净与噪声数据上均优于基线。


<details>
  <summary>Details</summary>
Motivation: 海量网络视频提供了可扩展的动作数据来源，但遮挡/出框导致身体部分不可见。如果丢弃含缺失的样本会损失规模与多样性，保留又会引入噪声、降低模型性能，亟需能在局部缺失和噪声下仍可学习与生成的鲁棒方法。

Method: 1) 将人体划分为5个部件，并自动检测每帧中“清晰可见”的部件为可信部件；2) 设计部件感知的变分自编码器（part-aware VAE），将可信部件编码为潜在token；3) 采用鲁棒的部件级掩码自回归生成模型，对被掩码的可信部件进行预测，同时忽略被判断为噪声/不可信的部件，从而以部分观测指导完整动作生成。

Result: 构建了K700-M基准（约20万真实视频动作序列）。实验显示在干净和含噪声数据集上，本方法在动作质量、语义一致性与多样性等指标均显著优于多种基线。

Conclusion: 在存在遮挡与缺失的现实视频条件下，基于可信部件的分部编码与掩码式自回归生成能够有效提升动作生成的鲁棒性与质量；K700-M为评测提供了更具挑战性的标准。

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [100] [Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images](https://arxiv.org/abs/2512.12718)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Changgyun Kim,Taemin Lee*

Main category: cs.CV

TL;DR: 提出一个整合四向深度图的3D人体姿态分析系统，重建人体模型并自动估计脊柱中心线，通过分层配准与自适应网格简化和LoD集成提升稳健性与精度，无需训练数据或复杂深度网络。


<details>
  <summary>Details</summary>
Motivation: 多视图重建成本高、流程复杂；单视图易受遮挡与视角限制，难以准确恢复内部结构（如脊柱中心线）。需要一种在成本与精度间折中、对噪声/遮挡更鲁棒、能准确估计脊柱参数的方法。

Method: - 采集四个方向的深度图并融合重建3D人体网格
- 分层（全局到细节）配准以提升在噪声和遮挡下的匹配鲁棒性
- 应用Adaptive Vertex Reduction，在保形的同时控制网格分辨率
- 采用Level of Detail（LoD）集成以同时提升脊柱角度估计的精度与稳定性
- 自动估计脊柱中心线，实现高精度3D脊柱配准

Result: 系统在无需训练数据或复杂神经网络的前提下，实现高精度的3D脊柱配准和角度估计；实验验证匹配质量得到提升（摘要未给出具体数值）。

Conclusion: 该方法在成本与流程上优于多图像方案，并克服单图像限制；通过分层配准、顶点自适应简化与LoD集成，实现鲁棒、稳定且精确的脊柱中心线与角度估计。

Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.

</details>


### [101] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: GenieDrive 提出先预测物理一致的4D占据，再以其为条件生成多视角驾驶视频，相比直接动作到视频的扩散方法，兼顾物理一致性、可控性和效率。


<details>
  <summary>Details</summary>
Motivation: 直接用单一扩散模型把控制/动作映射到视频，学习难、易产生物理不一致（碰撞/遮挡/几何失真），且对闭环评测和OOD合成不友好；需要一个能显式承载三维几何与动力学约束的物理感知世界模型。

Method: 1) 先生成包含高分辨率几何与动态的4D占据，作为物理先验；2) 设计占据VAE，将占据编码为潜在tri-plane表示，潜码仅为既往的58%；3) 提出Mutual Control Attention（MCA）精确建模控制对占据演化的影响，VAE与预测模块端到端联合训练；4) 在视频生成端引入Normalized Multi-View Attention，用4D占据指导多视角视频合成；

Result: 占据预测mIoU提升7.2%，推理41 FPS，仅3.47M参数；视频质量FVD降低20.7%，多视角一致性与可控性显著提升。

Conclusion: 以4D占据为中间物理表征，配合压缩高效的tri-plane潜空间与控制感知注意力，实现高效、可控、物理一致的多视角驾驶视频生成，优于直接动作到视频的扩散基线。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [102] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: FysicsWorld 提出一个首个覆盖图像/视频/音频/文本、支持双向输入输出的“全模态”统一基准，用于任意到任意的理解、生成与推理评测，并以CMCS策略构建融合依赖的跨模态数据；系统评测30+模型，揭示当前方法在全模态能力上的不足，提供统一基线与推进方向。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM与“全模态”体系快速发展，但评测仍分散：模态覆盖不全、仅支持文本输出、模态间互补性弱、缺少真正的任意到任意评估，难以系统衡量理解/生成/推理的跨模态能力。

Method: 构建FysicsWorld基准：涵盖图像、视频、音频、文本四模态，支持双向I/O与any-to-any任务；包含16类任务、3,268样本，源自40+高质量数据源，题型多样；提出并集成“跨模态互补性筛选”（CMCS）策略于数据构建流程，生成面向口语交互与融合依赖推理的全模态数据；将30+前沿模型（MLLM、单模态、统一理解-生成、全模态LLM）进行系统评测。

Result: 评测揭示不同模型在理解、生成、推理上的显著差距与短板，特别是在需要多模态互补与跨模态推理的任务上表现不稳或不足；FysicsWorld为各方向提供了可对比的统一结果与强基线。

Conclusion: FysicsWorld为下一代全模态架构提供统一、全面的评测平台与基线，通过CMCS与任意到任意设置推动对真实世界多模态理解、生成与推理能力的研究与提升。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [103] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: CoRe3D 提出一个将语义推理与空间推理紧密耦合的3D理解与生成统一框架，通过将3D潜空间分解为局部区域并结合语言链式思维，实现与文本高一致性的可组成、可程序化3D输出。


<details>
  <summary>Details</summary>
Motivation: 尽管在语言与视觉任务中显式推理已显著提升可靠性、可解释性与跨模态对齐，但在3D领域的推理式方法仍显不足，亟需能把高层语言意图直接落地到低层几何生成的统一框架。

Method: 提出空间落地的推理表示：将3D潜空间划分为局部区域，以可组合、程序化方式在几何上推理；同时将语义链式思维与结构化空间推理紧密耦合，使语言推理直接驱动3D内容形成与编辑。

Result: 生成的3D结果在局部一致性方面更强，并与语言描述更忠实对齐，展示出有效的三维理解与生成能力。

Conclusion: 显式、结构化的语义-空间联合推理是提升3D模型可靠性、可解释性与跨模态对齐的有效途径，CoRe3D验证了该思路在统一3D理解与生成中的可行性与优势。

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [104] [Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior](https://arxiv.org/abs/2512.12774)
*Hao Wang,Ashish Bastola,Chaoyi Zhou,Wenhui Zhu,Xiwen Chen,Xuanzhao Dong,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 提出Fast-2DGS：通过深度高斯先验与属性回归网络，实现单次前向即可高质量2D高斯Splatting重建，少量微调即可，显著降低计算开销并保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 生成模型推动下，对高保真图像的高效、可解释、可编辑表示需求增长。2DGS具备显式控制与实时渲染优势，但高质量重建常需耗时的事后优化；现有随机/启发式初始化对复杂度不敏感、收敛慢；引入可学习网络的方案又带来较高计算与架构复杂度。需要一种既高效又轻量、兼顾质量与速度的初始化与重建框架。

Method: 提出Fast-2DGS的解耦架构：1) 深度高斯先验（Deep Gaussian Prior，DGP）：条件网络，根据图像复杂度预测高斯原语的空间分布（位置/密度），提供强先验的初始化；2) 属性回归网络：预测高斯的稠密属性（如尺度、方向、颜色/不透明度等）。整体在单次前向完成高质量初始化，再进行极少量微调达到最终重建。

Result: 在实验中，Fast-2DGS单次前向即可获得接近最终质量的重建，仅需极少微调；与传统随机/启发式或复杂可学习初始化方法相比，显著降低计算成本与收敛时间（>10s问题被大幅缓解），同时保持或不降低视觉质量与实时渲染能力（>1000 FPS）。

Conclusion: Fast-2DGS通过深度先验与属性回归的轻量化、解耦设计，实现高质量、可解释、可编辑且高效的2D高斯表示，减少后续优化开销，使2DGS更接近工程化与工业部署。

Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.

</details>


### [105] [L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context](https://arxiv.org/abs/2512.12790)
*Tiange Zhang,Zhimeng Huang,Xiandong Meng,Kai Zhang,Zhipin Deng,Siwei Ma*

Main category: cs.CV

TL;DR: 提出L‑STEC：在神经视频压缩中引入长期时空上下文，结合LSTM扩展参考链与像素域扭曲的空间上下文融合，多尺度感受野网络提升细节保真，显著降码率并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有条件式神经视频编解码多仅依赖上一帧特征作时域预测：1) 短参考窗口忽视长程依赖与细纹理；2) 仅特征级传播在多帧累积误差，导致预测偏差与纹理流失。需要同时建模长期时序与高保真空间信息。

Method: 提出L‑STEC：① 用LSTM扩展参考链，显式捕获长期时序依赖；② 从像素域获取并光流扭曲的空间上下文，与时域特征在多感受野融合网络中进行时空融合，增强参考细节与稳健性。

Result: 在客观指标上显著优于主流方法：相较DCVC‑TCM，PSNR下节省码率37.01%，MS‑SSIM下节省31.65%；同时超越VTM‑17.0与DCVC‑FM，达成新的SOTA。

Conclusion: 联合长期时序建模与像素域空间上下文融合能有效缓解短窗与特征传播误差问题，提升细节保持与压缩效率；L‑STEC在标准与学习式方案上均展现领先性能。

Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.

</details>


### [106] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DrivePI 提出一个面向自动驾驶的空间感知4D多模态大模型（MLLM），在统一的视觉-语言-动作框架中，端到端并行完成3D占用、占用流预测与规划，同时支持语言指令。利用点云、多视角图像和语言训练，并构建文本-占用/流的QA数据引擎。以仅0.5B的Qwen2.5为骨干，在多个基准上超越7B级VLA与专用VA模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM虽强，但在自动驾驶中产生细粒度3D感知与预测（占用/占用流）并联动规划仍欠探索；VLA与专用VA模型割裂，难以统一端到端优化且难兼顾几何精度与视觉语义。

Method: 提出DrivePI：统一的Vision-Language-Action架构，融合点云、多视角图像与语言，学习空间感知4D表示；在同一模型中并行执行3D占用估计、占用流预测与规划（动作输出），并通过数据引擎自动构建文本-占用与文本-流的QA对以监督4D空间理解；以Qwen2.5-0.5B作为MLLM骨干进行端到端训练，同时兼容VA模式。

Result: 在nuScenes-QA上较OpenDriveVLA-7B提升2.5% mean accuracy；在nuScenes上碰撞率较ORION下降70%（0.37%→0.11%）；在OpenOcc上3D占用较FB-OCC提升10.3 RayIoU，占用流mAVE从0.591降至0.509；在nuScenes上规划L2误差较VAD降32%（0.72m→0.49m）。

Conclusion: DrivePI证明小参数量（0.5B）MLLM即可在统一框架下同时实现4D空间理解、3D占用/流预测与规划，达到或超过现有大型VLA与专用VA模型的表现，展示了将MLLM用于端到端自动驾驶的有效性与可行性。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [107] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

TL;DR: 提出一个对比分析（CA）框架，在两数据集上学习并分离共性与显著（仅属单一数据集）生成因素，适配GAN与扩散模型，通过新训练策略与损失实现清晰解耦并保持高质量生成，在人脸、动物、医疗影像上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成主要做条件编辑或可解耦表示，但依赖显式属性监督且难以从数据层面区分“跨数据集共享”与“数据集特有”因素。作者希望在仅有数据集标签的弱监督设定下，将两数据集的共通因素与各自显著因素分离，以实现更灵活的编辑与更稳健的生成。

Method: 提出对比分析（CA）框架：给定两数据集，仅用数据集指示作为监督，在生成模型（GAN或扩散）中构建潜变量拆分为“common”与“salient”子空间；设计配套的训练策略与损失，鼓励跨数据集对齐与共享的一致性（common）以及数据集特异性的分离（salient），同时维持生成质量。框架可无缝适配GAN和扩散模型。

Result: 在人脸、动物、医疗影像等多种数据集上实验，所提方法在共性/显著因素分离度与合成质量上均优于现有方法，展示更清晰的因素解耦与更高保真图像。

Conclusion: 仅凭数据集层级信号即可有效学习并分离共性与显著生成因素。所提CA框架通用于GAN与扩散模型，能在保证高质量生成的同时取得更优的因素分离，为弱监督的编辑与理解提供新路径。

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [108] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: Lemon 提出一个统一的 Transformer，把3D点云补丁与文本词元当作同一序列联合建模，通过结构化“patch化+标记化”、三阶段课程学习，实现早期空间-语言融合、去除专用编码器、提升参数效率与可扩展性，并在多种3D理解与推理任务上达成SOTA与良好缩放特性。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在3D上的难点：点云稀疏不规则；多模态体系往往依赖各模态专用编码器与对齐模块，结构割裂；训练不稳定、扩展性差。需要一个能稳定扩展、参数高效、支持3D空间-语言紧密融合的统一框架。

Method: 设计统一Transformer，将3D点云补丁tokens与语言tokens拼接为单序列，进行早期跨模态融合；提出保留空间上下文的结构化patch化与token化方案；采用三阶段课程：从物体级识别→叙述/描述→场景级空间推理，逐步构建能力；移除模态专用编码器与显式对齐模块以简化架构。

Result: 在对象识别、描述/字幕生成到三维场景空间推理等全面3D任务上取得新的SOTA；展现随模型规模与数据量增加的稳健缩放性质（性能持续提升）。

Conclusion: 统一的序列级跨模态Transformer为3D空间智能提供基础：参数更高效、训练更稳定、可扩展性更好，并能在真实应用中推动3D理解与推理的发展。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [109] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 研究针对CoCa类生成-对比多模态模型在极少样本图像分类中的适配，比较从免训练原型法到LoRA微调等策略，揭示增强与训练方法的关键权衡，并给出可复现实践建议。


<details>
  <summary>Details</summary>
Motivation: 现有少样本适配研究多集中在CLIP等双编码器，对CoCa这类带生成头且潜在空间不同的模型在参数高效微调场景下缺乏系统认识；同时，实际下游任务常数据稀缺，需要明确哪些策略最稳健、最有效。

Method: 以CoCa视觉骨干为对象，系统评测一系列适配策略：训练免原型/混合原型（hybrid prototyping）、线性探测、加入Supervised Contrastive（SupCon）的混合目标、以及LoRA等PEFT深层参数适配。分析不同shot数下的数据增强、正则化强度、LoRA秩、采样策略对性能与稳定性的影响。

Result: 发现“增强分歧”：强数据增强会降低低shot线性探测效果，但对LoRA微调的收敛与稳定至关重要；采用含SupCon的混合损失相较纯交叉熵在不同shot下持续提升表现。给出在极少样本条件下，对正则化规模、LoRA秩与数据采样的敏感性与推荐设置。

Conclusion: CoCa这类生成-对比基础模型可通过精心设计的PEFT与目标函数在少样本下有效适配；需在增强策略上区分线性探测与LoRA微调，并优先考虑加入SupCon的混合损失与合适的正则化/秩/采样配置，以获得稳定且高效的迁移性能。

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [110] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: 提出SAVEBench数据集与SAVE模型，实现端到端联合音视频编辑，能在保持同步与语义一致性的同时移除目标对象。


<details>
  <summary>Details</summary>
Motivation: 联合编辑音频与视频对可控内容创作很重要，但缺乏成对“编辑前后”的多模态数据，且跨模态异质性强、建模难，导致现有方法常为音频/视频各自独立编辑并后期对齐，效果与一致性不足。

Method: 构建SAVEBench：带文本与掩膜条件、以目标对象为锚点的成对音视频数据，用于源到目标的学习；提出SAVE：基于flow matching并融合Schrödinger Bridge，学习从源到目标的直接传输，端到端并行编辑音频与视频，过程内保持跨模态对齐。

Result: 在移除目标对象的任务上，SAVE可同时在音频与视觉中删除目标并保留其余内容；相较于将独立音频编辑器与视频编辑器的两两组合，SAVE表现出更强的时间同步性与音视频语义对应。

Conclusion: 数据与建模的结合（SAVEBench+SAVE）有效推动了联合音视频编辑：在一致性、同步与编辑精度上优于分离式管线，证明了源到目标直接传输的可行性与优势。

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [111] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: 提出一种端到端跨层级融合方法：把V2X/智能传感器输出的对象列表与原始相机图像，通过Transformer联合用于3D检测，并用可变形高斯掩码引导注意力；在无公开对象列表数据时，用带噪伪对象列表模拟训练；在nuScenes上显著优于视觉基线且对不同噪声与真实检测器具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实车辆传感融合中常获得的是经过处理的对象列表（来自智能传感器或V2X），缺少原始传感器数据，传统做法多在对象级后融合，难以充分利用原始视觉信息并存在误差累积。需一种能直接将高抽象对象信息与原始图像联合建模、提升3D检测的方案，同时解决缺少对象列表数据集的问题。

Method: - 端到端跨层级融合：将对象列表作为Transformer中的去噪查询，与可学习查询共同在特征聚合中传播。
- 在Transformer解码器中显式注入由对象列表位置与尺寸先验生成的“可变形高斯掩码”，引导注意力集中至目标区域并加速收敛。
- 伪对象列表合成：基于真值框，模拟状态噪声、漏检与误检以构建训练数据。
- 与纯视觉基线对比，并在不同噪声强度与真实检测器输出上评估泛化。

Result: 在nuScenes数据集上，相较纯视觉3D检测基线取得显著性能提升；训练更快收敛；对模拟的不同噪声水平及真实检测器对象列表均表现出较强鲁棒与泛化能力。

Conclusion: 跨层级融合（对象列表+图像）可有效增强3D目标检测，显式先验引导的Transformer（可变形高斯掩码+去噪查询）提升精度与收敛效率；合成伪对象列表为缺数据情形提供可行训练途径，并验证方法对噪声与实际场景的泛化。

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [112] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 提出一种将RAG思想用于交通标志零样本识别的框架：VLM生成描述→向量库检索候选→LLM推理细粒度判别；在303类俄亥俄MUTCD标志上取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习需要大规模标注且难覆盖海量标志类别，实际部署中长尾类与新类频出、全量标注不可行，因而需要无需任务特定训练、可扩展到未见类别的零样本方案。

Method: 1) 用视觉-语言模型从输入标志图像生成文本描述；2) 利用该描述在参考设计的向量数据库中检索少量最相关候选标志；3) 将检索到的候选及其信息交由大语言模型进行推理，完成细粒度最终识别；整个流程为RAG式管线，无需针对任务再训练。

Result: 在包含303种监管类标志的Ohio MUTCD数据上评估：理想参考图像上准确率95.58%，真实复杂路况数据上准确率82.45%，验证了方法的有效性。

Conclusion: RAG架构与VLM+LLM结合可在无需任务特定训练的条件下，实现可扩展、准确的道路标志零样本识别，对大规模智能交通系统具有可行性。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [113] [Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification](https://arxiv.org/abs/2512.12887)
*Han Liu,Bogdan Georgescu,Yanbo Zhang,Youngjin Yoo,Michael Baumgartner,Riqiang Gao,Jianing Wang,Gengyan Zhao,Eli Gibson,Dorin Comaniciu,Sasa Grbic*

Main category: cs.CV

TL;DR: 提出AnyMC3D：在冻结2D基础模型上以百万级参数轻量插件适配，实现高效通用的3D医学图像分类，并在12项任务与竞赛中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学分类研究存在三大问题：数据分布与规模依赖导致偏置（data‑regime bias）、对基础模型的适配方式低效或不充分、任务覆盖面窄。需要一种能在不同病种、解剖部位、模态下统一扩展且高效的方案。

Method: 将通用2D基础模型作为冻结骨干，引入每任务约百万参数的轻量插件进行适配；支持多视角输入（multi‑view）、引入像素级辅助监督以提升分类、并生成可解释热力图。统一在一个框架内对多任务进行训练/推理。

Result: 构建覆盖多病种、解剖部位、成像模态的12项3D分类基准，系统比较SOTA方法。得到三点发现：1）有效的适配策略是释放FM潜力的关键；2）通用FMs若适配得当可匹敌甚至媲美医学专用FMs；3）基于2D的方案在3D分类上普遍优于原生3D架构。方法在多任务上达SOTA，并夺得VLM3D挑战赛第一。

Conclusion: 一个在冻结2D FM上通过轻量插件扩展的统一框架，能够跨多任务实现SOTA的3D医学图像分类，减少为每个任务训练独立模型的需求，并提供可扩展、可解释且数据效率高的解决方案。

Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.

</details>


### [114] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

TL;DR: 提出“查询卷积（Qonvolutions）”，通过将低频信号与查询（如坐标）进行卷积，显著提升神经网络对高频信号的学习；在1D回归、2D超分、2D图像回归与新视角合成等任务上取得优异甚至SOTA表现（结合高斯喷溅）。


<details>
  <summary>Details</summary>
Motivation: 神经网络存在谱偏置与优化困难，难以学习高频信号；尽管傅里叶编码等方法有效，但在更复杂高频场景仍有改进空间。

Method: 在网络中引入“Qonvolution”：利用卷积的邻域特性，将低频特征与外部查询（如空间坐标）进行卷积，从而在特征空间中显式注入高频变化；作为一个简单可插拔的模块，可与现有表示学习/渲染框架（如高斯喷溅）组合。

Result: 在1D函数回归、2D超分辨率、2D图像回归及新视角合成等高频任务上，Qonvolution均带来显著性能提升；与高斯喷溅结合用于NVS，在真实复杂场景上达到SOTA图像质量，超过部分强大的辐射场模型。

Conclusion: 通过将查询与低频信号进行卷积，可有效缓解谱偏置与优化难题，提升对高频细节的拟合与重建能力；方法通用、简单易集成，并在多种任务中验证了实用价值与领先性能。

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [115] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出一种基于预测样本分配(PSA)的简洁SCOOD框架，通过能量分数的双阈值三元分配与概念对比学习，提升ID/OOD样本纯度与区分度，并配合重训练策略，在两大基准上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SCOOD多依赖聚类式ID过滤，将未选中的样本当作辅助OOD，导致训练中混入大量噪声样本，降低检测效果。需要一种能更干净地挑选ID/OOD并提升表征可分性的方案。

Method: 1) 预测能量分数驱动的双阈值三元分配：把未标注数据按高置信(ID)、低置信(OOD)与不确定(丢弃)进行划分，提高ID与OOD集合纯度；2) 概念对比表示学习损失：在表示空间扩大ID与OOD的间隔；3) 重训练策略：用筛选出的辅助ID/OOD样本重新训练以充分拟合。

Result: 在两个标准SCOOD基准上，方法在OOD检测指标上显著优于现有最先进方法，取得大幅领先。

Conclusion: 以PSA为核心的SCOOD框架能有效降低训练噪声、增强ID/OOD表示分离，并通过重训练进一步提升性能，体现出稳定且显著的SOTA水平。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [116] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出一种用于广义类别发现（GCD）的新方法，通过抑制伪标签噪声与提升未知类建模，实现多基准SOTA。核心包括损失锐度惩罚（LSP）与动态锚点选择（DAS）。


<details>
  <summary>Details</summary>
Motivation: DINO式伪标签在大模型偏好特定视觉模式时易受伪相关影响，导致无标签样本产生噪声伪标签；同时已知/未知类置信度不均与未知类表征学习缓慢，降低聚类性能。

Method: 1) LSP：以最坏情形损失锐度为目标，惩罚对小扰动敏感的参数，抑制对琐碎特征的编码，提升伪标签稳健性。2) DAS：在训练过程中基于KNN密度与类别概率选择代表性未知类样本为锚点，并赋予硬伪标签，缩小已知/未知置信度差距，加速未知类特征分布学习。

Result: 在多个GCD基准上达到SOTA，显著降低伪标签噪声并提升聚类/识别精度。

Conclusion: 通过联合LSP与DAS，方法有效缓解伪标签噪声与类别置信度偏差，快速塑造未知类分布，整体提升GCD性能。

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [117] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: MADTempo提出一个统一的时间检索与大规模视觉定位的视频检索框架：用事件级相似度聚合实现多事件时间推理，并用基于Google图片搜索的回退模块增强对OOD查询的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索难以同时建模跨事件的时间依赖，也难以处理包含未见或稀有视觉概念的查询，导致对复杂多事件叙事和长尾概念的检索效果不佳。

Method: 1) 时间检索：将视频切分为连续片段，计算片段级相似度并在时间序列上进行聚合（事件级连续性建模），以支持多事件查询的连贯匹配；2) 回退模块：当预训练视觉嵌入无法很好覆盖查询概念时，调用Google图片搜索扩展查询的视觉表征，通过外部网络图像进行特征增强，从而提升对分布外/长尾概念的鲁棒性。

Result: 两大组件结合后，系统能更好地进行时间推理与多事件检索，并在包含未见或稀有概念的查询上表现更稳健，提升对大规模视频库的检索效果。（摘要未给出具体数值）

Conclusion: 通过统一时间结构建模与网络级视觉扩展，MADTempo提高了复杂事件级别的语义感知与泛化能力，为大规模视频检索提供更自适应、鲁棒的方案。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [118] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 提出一个统一的多模态片段检索系统，融合强检索与重排序、时间一致性评分和代理驱动的查询分解，实现对含歧义查询的自适应多模态融合与时序连贯检索。


<details>
  <summary>Details</summary>
Motivation: 视频内容爆炸式增长，现有方法在三方面受限：固定权重融合在跨模态噪声与歧义查询下失效；时间建模难以既保持事件序列连贯又避免不现实的时间间隙；需要人工选择模态，影响可用性。

Method: 1) 级联双嵌入：用BEIT-3与SigLIP做广覆盖召回，再用BLIP-2重排序以平衡召回与精度。2) 时序感知评分：在束搜索中对大时间间隙施加指数衰减惩罚，构造连贯的事件序列而非孤立帧。3) Agent(如GPT-4o)引导的查询分解：自动解析歧义，将查询拆为视觉/OCR/ASR子查询，并进行自适应得分融合，免去手动选模态。

Result: 定性分析表明系统能有效处理歧义查询、检索时间上连贯的序列，并动态调整多模态融合策略，提升交互式片段检索能力。

Conclusion: 所提系统通过级联检索+重排序、时序惩罚与代理驱动的模态自适应融合，缓解跨模态噪声、时间不连贯与人工模态选择问题，推进多模态瞬时片段检索的实用性与鲁棒性。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [119] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 提出CAMA：一种内容自适应的视频压缩框架，通过更精确的运动对齐、多参考质量感知与训练/推理模块改进，在标准数据集上较基线DCVC-TCM获得约24.95% BD-rate (PSNR) 降低，并优于DCVC-DC与HM-16.25。


<details>
  <summary>Details</summary>
Motivation: 端到端视频压缩虽然统一优化，但缺乏对不同内容特性的自适应，导致在复杂运动、不同纹理/质量参考场景下性能受限，误差传播明显。作者动机是引入内容自适应的运动对齐与参考质量建模，提升补偿准确性并抑制误差累积。

Method: 1) 两阶段、光流引导的可变形（deformable）变形/重采样：先粗到细预测offset并用mask调制，实现更精确的特征对齐与运动补偿。2) 多参考质量感知策略：根据参考帧质量自适应调整失真权重，并在分层训练中使用以降低误差传播。3) 训练无关（training-free）下采样模块：依据运动幅度与分辨率对帧做下采样以获得更平滑的运动估计。整体形成内容自适应的运动对齐与编码策略。

Result: 在标准测试集上，CAMA相对基线DCVC-TCM在PSNR指标的BD-rate降低24.95%；同时超过复现实验的DCVC-DC与传统编解码器HM-16.25，显示显著压缩效率提升。

Conclusion: 内容自适应的运动对齐与质量感知设计能有效提升端到端视频压缩性能，降低误差传播并改进运动补偿精度；所提CAMA在多基准上取得显著优势，验证方法通用有效。

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [120] [UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction](https://arxiv.org/abs/2512.12941)
*Siyuan Yao,Dongxiu Liu,Taotao Li,Shengjie Li,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出UAGLNet，通过不确定性建模引导全局-局部特征协同与融合，实现更精确的建筑物遥感分割，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 建筑物在遥感图像中存在形态多样、尺度跨度大与边界模糊等问题。现有基于CNN或Transformer的多尺度特征金字塔存在层间语义鸿沟，且全局-局部信息融合不足，导致提取结果不准、边界含糊。作者旨在缩小特征鸿沟、强化全局-局部协同，并用不确定性建模缓解模糊区域的分类偏差。

Method: 1) 协同编码器：在不同stage混合使用CNN（捕获局部纹理与边缘）与Transformer（建模长程依赖与全局语义）。2) 协同交互块CIB：在中间层显式交互，缩小局部与全局特征语义差距。3) 全局-局部融合模块GLF：互补式融合两类表征，提升多尺度一致性与细节。4) 不确定性聚合解码器UAD：显式估计像素级不确定性，用其引导解码与特征重加权，缓解边界与小目标处的歧义，提升分割精度。

Result: 在多组遥感建筑分割基准上取得优于现有SOTA的方法的性能（文中称“Extensive experiments demonstrate superior performance”），并开源代码。

Conclusion: 通过不确定性引导的全局-局部协同建模与融合，UAGLNet有效缓解金字塔语义鸿沟和模糊区域歧义，显著提升建筑物提取精度，方法通用且可复用到其他遥感分割任务。

Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet

</details>


### [121] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: 提出SCAdapter：在CLIP图像空间解耦内容与风格，结合CSAdaIN、KVS Injection与一致性目标，实现更真实的照片级风格迁移；较现有扩散法更快（至少2×）且效果更佳。


<details>
  <summary>Details</summary>
Motivation: 扩散式风格迁移常出现“油画化”、细节丢失与内容风格干扰：内容图自带风格影响、参照风格图的内容特征泄漏，导致难以实现逼真的照片级迁移。

Method: 在CLIP图像空间分离内容与风格：1) 纯内容抽取与纯风格抽取；2) CSAdaIN实现可控、多风格精细混合；3) 通过KVS Injection在扩散网络的Key/Value特征层定点注入风格；4) 设计风格迁移一致性损失，保证扩散过程前后一致；取消DDIM反演与推理期优化以提速。

Result: 在传统与扩散基线中均显著优于SOTA，转移更真实、细节更完整；同时推理至少快2倍。

Conclusion: SCAdapter通过在CLIP空间解耦内容/风格并以CSAdaIN与KVS注入进行可控融合，实现高保真、效率高的照片级风格迁移，适合实用场景。

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [122] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: VLCache通过重用多模态输入产生的KV缓存与编码器缓存，形式化分析并抑制非前缀复用误差，结合层重要性自适应重算策略，在几乎不降精度的前提下仅计算2-5%令牌，实现1.2x-16x首token延迟加速，并已集成到SGLang用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在相同或相似输入重复出现时需重复计算，造成TTFT高、推理低效。现有基于启发式的缓存复用方法缺乏对误差累积的系统刻画与控制，且未充分考虑不同层对精度与效率的异质影响。需要一种能系统化控制复用误差、动态权衡重算与复用的框架，以在真实服务场景中提升吞吐与时延。

Method: 提出VLCache框架：同时复用历史多模态的KV缓存与编码器缓存；形式化定义非前缀缓存复用带来的累积误差，并给出最小化该误差的策略；分析各层对输出敏感度与重要性，设计动态、按层感知的重算策略，对关键层重算、非关键层复用，从而在精度与效率间自适应折中；将整套流水线集成进SGLang以便工程化应用。

Result: 在多组实验中，VLCache在保持与完全重算近似的准确率的同时，只需计算2-5%的token，带来1.2x-16x的TTFT加速；在工程实现上已成功并入SGLang并在实际部署中显著提升推理速度。

Conclusion: 可同时复用KV与编码器缓存并对非前缀复用误差进行形式化控制，再结合按层动态重算，可在基本不损失精度的情况下显著降低计算量与首token延迟；方法通用于实际系统并已在SGLang中验证其工程可行性与收益。

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [123] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 论文发现“先益后害”困境：汇集更多生成器数据训练通用AIGI检测器，初期提升，随后因冲突导致性能停滞甚至恶化。作者提出GAPL，通过生成器感知的原型学习与LoRA双阶段适配，缓解数据异质性与模型瓶颈，获得SOTA泛化检测。


<details>
  <summary>Details</summary>
Motivation: 通用AIGI检测常用多源数据以提升泛化，但随着生成器多样性增加，真实与伪造特征分布重叠加剧，固定预训练编码器难以适应复杂性，整体性能反而下降。需要一种能在多源异质条件下稳定学习、又能在不遗忘预训练知识的前提下增强判别力的方法。

Method: 提出GAPL（Generator-Aware Prototype Learning）：1）原型学习：学习一组紧凑的“伪造原型”，在统一、低方差特征空间中约束表示，缓解不同生成器导致的分布异质。2）两阶段训练+LoRA：在预训练编码器上以低秩适配进行渐进式训练，既提高可分性又保留预训练知识，构建更稳健的决策边界。

Result: 在大量GAN与扩散生成器上进行广泛实验，GAPL在跨生成器、跨域的检测任务中取得SOTA性能，相比基线具有更高准确率与更强泛化。

Conclusion: 单纯堆叠多源生成器数据会触发“先益后害”困境。通过生成器感知的原型约束与LoRA式两阶段适配，可显著缓解数据异质与模型固定瓶颈，建立稳健、可泛化的AIGI检测器；代码已开源。

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [124] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: 论文提出一种对CLIP进行对抗微调的新目标，使其在保持零样本能力的同时，既提升鲁棒性又恢复不确定性校准。核心做法是将CLIP输出重新参数化为狄利克雷分布的浓度参数，在对抗扰动下对整分布进行对齐而非仅匹配单一logit，从而纠正对抗扰动导致的过度自信。


<details>
  <summary>Details</summary>
Motivation: 现有对抗微调多以干净与对抗样本的logit匹配为主，忽略了不确定性校准，导致零样本泛化退化。更重要的是，在对抗场景中常出现“越难越自信”的反常现象：扰动降低准确率的同时压低模型不确定性，造成严重失配与过度自信，暴露出鲁棒性之外的可靠性缺口。

Method: 将CLIP的输出重参数化为狄利克雷分布的浓度参数，用这一统一表示同时编码类别间相对语义结构与预测置信度大小。设计对抗微调目标，使干净与对抗样本的狄利克雷分布在整体上对齐（而非仅对齐单个logit），以实现预测与不确定性的联合对齐与校准。

Result: 在多个零样本分类基准上，该方法在保持干净准确率的同时，显著恢复了不确定性校准，并取得与现有方法相当的对抗鲁棒性。

Conclusion: 面向CLIP的对抗微调应超越logit匹配，采用分布级（狄利克雷）对齐以共同考虑预测准确性与不确定性，从而修复对抗扰动造成的过度自信并提升整体可靠性。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [125] [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)
*Yifan Pu,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Fan Wang,Bohan Zhuang,Gao Huang*

Main category: cs.CV

TL;DR: 论文系统研究将扩散蒸馏用于开放域文本到图像（T2I）生成，基于强教师FLUX.1-lite，总结统一框架、关键难点与实践指南，并开源代码与学生模型，实现快速高保真、资源高效的T2I生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散蒸馏成功加速类别条件图像合成，但从离散类标签迁移到自由文本提示时，适用性、稳定性与质量尚不明确。业界缺少系统性比较、统一框架与可复现的实践范式，阻碍在真实T2I应用中部署高效生成器。

Method: 将多种最先进蒸馏方法在同一统一框架下适配到T2I场景，以FLUX.1-lite为教师，系统比较并分析从类条件到文本条件迁移的核心障碍；给出输入缩放、网络结构与超参的实践要点，并提供开源实现与预训练学生模型。

Result: 在开放域T2I上实现了快速、保持高保真且资源高效的蒸馏学生模型；明确了从类标签到自由文本导致的关键问题与解决策略；提供可复用的工程配置与经验。

Conclusion: 经统一框架与系统实验，扩散蒸馏在T2I任务中可行且高效；配套的指南与开源资源为实际落地提供坚实基础，促进快速、高质量、低资源成本的T2I生成部署。

Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.

</details>


### [126] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 提出一种基于光场图像与高斯splat统一表示的无模板6DoF目标跟踪方法，对复杂外观（如强反射）更稳健，并公布含精确姿态标注的数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 现有高性能跟踪器多依赖预采集的对象视图构建显式参考模型，只能处理已知对象，且对复杂外观（反射等）易退化；需要一种能泛化到未知物体、在复杂视觉行为下仍稳健的方案。

Method: 利用光场图像输入，通过视觉基础模型提取语义与几何特征，并转换为视角相关的Gaussian splats作为统一物体表示；该表示支持可微渲染与姿态优化，实现无需预训练特定对象模型的跟踪。

Result: 构建了包含强反射等挑战场景且具精确真值姿态的光场目标跟踪数据集；实验显示在这些困难案例中，该方法与最先进的基于模型的跟踪器性能相当。

Conclusion: 基于光场和高斯splat的统一表示可在无需对象先验的情况下实现对复杂外观物体的鲁棒6DoF跟踪，为机器人系统中的通用目标跟踪铺路。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [127] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: 提出TWLR：一个两阶段、弱监督且可解释的糖网病评估框架，结合视觉-语言知识注入与迭代病变回退（修复）机制，实现DR分级与病灶定位的准确、可解释、少标注学习。


<details>
  <summary>Details</summary>
Motivation: 医学影像（如眼底图）像素级标注昂贵，限制监督学习；深度模型虽有效但可解释性弱，阻碍临床落地。需要一种既节省标注成本又能提供病灶级可解释性的DR自动评估方法。

Method: 两阶段TWLR：1）视觉-语言阶段：引入眼科领域知识到文本嵌入，联合执行DR分级与病灶类别识别，将医学语义与图像特征对齐。2）严重度回归阶段：基于弱监督语义分割，迭代生成病灶显著图并进行逐步图像修复（inpainting），系统性移除病理特征，使图像严重度向健康方向回退；同时用这一过程进行严重度回归。

Result: 在FGADR、DDR及私有数据集上，TWLR在DR分类与病灶分割上达到了具竞争力的结果；无需像素级标注即可获得较准的病灶定位，并提供从病到健的可视化转变。

Conclusion: TWLR实现了面向DR的标注高效与可解释统一：通过知识增强的视觉-语言建模和迭代弱监督回退机制，同时提升分级准确性与分割质量，并给出直观的病灶到健康可视化，为临床可采纳性提供支持。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [128] [JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion](https://arxiv.org/abs/2512.13014)
*Haoyu Wang,Lei Zhang,Wenrui Liu,Dengyang Jiang,Wei Wei,Chen Ding*

Main category: cs.CV

TL;DR: 提出JoDiffusion：一个能仅凭文本同时生成图像与语义一致的像素级标注的扩散式数据集生成框架，配合掩码优化，生成的数据用于训练分割模型，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 像素级标注昂贵且缓慢；现有合成数据方案要么先生成图再做伪标注（引入语义不一致），要么用手工标注条件生成（难扩展）。需要一种既可扩展又保证图-标注一致性的生成方法。

Method: 在标准潜空间扩散模型上，新增独立的标注VAE，将标注掩码映射到与图像共享的潜空间；修改扩散模型以学习在文本条件下图像与其标注的联合分布，从而一次性生成成对的图与掩码；另外设计掩码优化策略以降低生成噪声/错误。

Result: 在Pascal VOC、COCO、ADE20K上，用该方法生成的合成标注数据训练的语义分割模型，显著优于现有生成与伪标注方案。

Conclusion: JoDiffusion能以文本驱动同时生成语义一致的图像与像素标注，具备高可扩展性；通过掩码优化进一步提升标注质量，实际提升分割性能。

Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.

</details>


### [129] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: 论文提出“下一场景预测”（NSP）任务，推动统一视频模型进行时间与因果推理；构建由Qwen-VL理解、LTX生成、潜在查询与连接器桥接的统一框架，并结合三阶段训练与因果一致性奖励的RL，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态/视频生成模型多集中于文本生成视频等常规任务，忽视了对未来事件的时间与因果推理能力；需要一个能从上下文预测合理未来的新任务与对应方法，来评估并提升模型的时序理解与因果推理。

Method: 提出NSP任务：给定前序视频/文本上下文，预测下一场景。方法上构建统一框架：用Qwen-VL进行语义与时序理解，用LTX进行视频合成；二者通过“潜在查询嵌入（latent query embedding）”与“连接器模块（connector）”衔接。训练采用三阶段：1) 文本到视频预训练；2) 监督微调；3) 基于GRPO的强化学习，引入“因果一致性奖励”以鼓励生成与上下文在因果与时间上连贯。

Result: 在作者新构建的大规模NSP数据集与基准上，所提模型取得SOTA性能，显示其能更好地根据先前上下文预测合理未来镜头，提升统一多模态系统的时序与因果推断能力。

Conclusion: NSP任务与统一框架有效激发并评估统一视频模型的时间/因果推理潜力；通过理解-生成解耦、潜在查询桥接及带因果奖励的RL训练，模型显著提升“接下来会发生什么”的预测能力，为通用多模态系统的前瞻性理解迈出一步。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [130] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 论文系统评估了室内FMCW MIMO雷达人数统计任务中的空间泛化方法，发现幅度加权（Sigmoid）最稳健，数据增强有小幅增益，而在大空间迁移时必须用迁移学习以显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 深度学习RF感知在实际部署时常遇到跨场景/空间布局变化导致性能骤降的问题，需要系统对比可提升空间泛化能力的技术，为稳健部署提供可行路径。

Method: 以室内人数统计为代表任务，基于FMCW MIMO雷达，系统比较：1) 幅度统计预处理（Sigmoid加权、阈值归零）；2) 频域滤波；3) 自编码器背景抑制；4) 多种数据增强；5) 迁移学习。跨两个不同布局环境采集数据，评估跨环境泛化的RMSE与MAE。

Result: Sigmoid幅度加权在跨环境下持续优胜：RMSE与MAE分别下降50.1%与55.2%相对基线。数据增强带来额外但有限收益，MAE最高再降8.8%。对于大空间位移，迁移学习效果显著：用540个目标域样本，RMSE与MAE分别降低82.1%与91.3%。

Conclusion: 将深度模型与幅度基预处理结合，可在中等空间变化下获得稳健泛化；当空间分布差异较大时，应配合少量目标域数据进行高效迁移学习以恢复高精度。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [131] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: 提出SneakPeek：面向教学视频的未来驱动式流式生成框架，以扩散+自回归方式从初始图像与结构化多提示生成多步骤、时序一致的演示视频。核心是因果预测适配、未来引导的自强制(KV双区缓存)与多提示条件控制，显著缓解时序漂移与曝光偏差，支持交互式、步骤级可控生成，在实验中实现更高的时序一致性与语义忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型难以在长序列、多步骤任务中同时保持时序一致性与可控性，导致操作演示类（instructional）视频出现漂移、失控与指令不完全遵循的问题。需要一种能在生成过程中利用未来结构与多步指令约束、并可在线交互更新的生成机制。

Method: 提出SneakPeek：扩散基础上的自回归流式生成。1) 预测性因果适配：训练一个因果模型进行下一帧与未来关键帧的预测，用于引导扩散去噪，提前对齐后续动作。2) 未来引导自强制：在推理时采用双区域KV缓存（区分历史与未来引导），缓解曝光偏差，并让已预测的未来关键帧对当前生成起到约束。3) 多提示条件：结构化、多步文本提示对每个步骤进行细粒度条件控制，支持在生成过程中更新后续提示。整体实现未来驱动的自回归扩散管线，支持流式输出。

Result: 在教学视频基准上，相比现有视频扩散/文本到视频方法，SneakPeek生成的视频在时序一致性、动作连贯性与任务遵循度上更优；能够精确跟随复杂的多步骤指令并减少时序漂移；展示交互式生成能力，即未来提示更新能即时影响后续片段。

Conclusion: 通过因果预测、未来引导的自强制与多提示条件化，SneakPeek实现了可控、时序稳定的流式教学视频生成，解决长序列曝光偏差与漂移问题，并支持交互式、步骤级控制，适用于内容创作与教育等场景。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [132] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Motus提出一个统一的潜在动作世界模型，通过MoT架构整合理解、视频生成与动作专家，并用UniDiffuser式调度器在多种建模模式间灵活切换；利用光流学习潜在动作，配合三阶段训练与六层数据金字塔，实现像素级“delta action”和大规模动作预训练，在仿真与真实机器人任务上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能方法在理解、世界建模与控制上割裂，难以统一多模态生成能力，也无法充分利用大规模异构数据。需要一个能统一各模块并从可共享的运动信息中学习的通用框架，以提升下游机器人任务性能。

Method: 提出Motus：1) 架构：Mixture-of-Transformer (MoT) 汇聚三个专家（理解、视频生成、动作）；2) 调度：借鉴UniDiffuser，实现世界模型、视觉-语言-动作、逆动力学、视频生成、视频-动作联合预测等多模式之间灵活切换；3) 动作表征：使用光流学习潜在动作（像素级“delta action”）；4) 训练与数据：三阶段训练流程与六层数据金字塔，支撑大规模动作预训练与统一建模。

Result: 在仿真任务上相较X-VLA提升约15%，相较Pi0.5提升约45%；在真实场景机器人任务上带来约11%到48%的提升，覆盖多种功能与先验的统一建模带来一致收益。

Conclusion: 统一的潜在动作世界模型与MoT+UniDiffuser设计可兼容多种任务形态，通过光流驱动的潜在动作与分阶段/分层数据训练实现大规模预训练，显著提升仿真与真实机器人表现，验证统一建模对下游任务的价值。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [133] [Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs](https://arxiv.org/abs/2512.13031)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 论文比较了规则方法、传统机器学习和深度学习在FMCW MIMO雷达感知中的表现：深度模型在训练环境最准确，但对新布局泛化差；规则方法不细致但更稳健；所有模型在二分类“有人/无人”上均稳健。揭示了输出粒度与空间泛化能力的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前FMCW MIMO雷达感知中，模型类型繁多，缺乏在相同数据与设置下对规则法、传统ML与深度学习的系统性对比，尤其是跨空间布局的泛化与输出粒度之间关系尚不清晰。

Method: 在两个室内场景（不同布局）上，评估五种方法：规则的连通域法；三种传统ML（KNN、RF、SVM）；一种CNN+LSTM深度模型。在训练布局内与新布局上比较多类别细粒度任务与二分类（有人/无人）的性能与稳定性。

Result: 在训练环境中，CNN+LSTM准确率最高，传统ML中等；在新布局上，所有学习型方法性能大幅下降，而规则法保持稳定。对二分类任务，所有模型跨布局均表现良好。

Conclusion: 高容量深度模型能在同域给出高精度细粒度输出，但对域移敏感；规则法缺乏细粒度能力却更抗域移。总体上存在输出粒度与空间泛化性能的权衡。

Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.

</details>


### [134] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: 提出Bi-Erasing：在扩散模型中同时进行有害概念抑制与安全概念增强的双向、图像引导式概念擦除框架，结合掩膜过滤与联合文本-图像表示，较现有方法更好平衡“擦除效果”与“生成质量”。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除多为单向策略（仅抑制有害概念或仅强化安全替代），导致要么擦除不彻底、要么生成质量和可用性下降，难以兼顾安全与保真。

Method: 基于文本-图像联合表示，设计两条解耦的图像分支：负向分支用于抑制目标有害语义，正向分支提供安全替代的视觉引导；二者联合优化，彼此补充。同时对两分支施加掩膜过滤，屏蔽与目标无关区域，减少干扰。整体在扩散模型微调阶段执行，追求抑制强度与可用性的折中。

Result: 在广泛实验中，相比基线方法（单向抑制或单向增强），Bi-Erasing在概念移除有效性与图像保真度之间取得更佳平衡，定量与定性指标均优。

Conclusion: 双向、图像引导与掩膜过滤的结合可显著提升概念擦除的安全-质量权衡；方法通用，适用于文本到图像扩散模型的安全化微调。

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [135] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: GTR-Turbo 用训练中产生的多重检查点进行“权重合并”，构造一个无需外部特权模型的免费教师，对多模态VLM智能体的多轮RL进行指导，从而在不依赖GPT/Gemini等教师的情况下提升精度10–30%，并将时间、算力成本分别降至原GTR的一半与六成。


<details>
  <summary>Details</summary>
Motivation: 多轮多模态RL存在稀疏奖励与长程信用分配难题；现有做法靠昂贵、不可复现实验的外部教师（如GPT/Gemini）提供步级反馈来加密奖励，代价高且依赖特权模型，且易出现训练不稳定与熵坍塌。

Method: 在RL训练过程中保存的多个模型检查点进行权重合并，得到一个“自蒸馏”的合并模型，作为免费教师；随后用该教师对正在训练的学生模型进行监督微调或软logit蒸馏，引导策略更新，避免外部查询；同时保持训练稳定并缓解熵坍塌。

Result: 在多种视觉代理任务上，相比基线模型准确率提升10–30%；相对原GTR墙钟训练时间减少约50%，算力成本降低约60%；性能与GTR相当而无需昂贵教师。

Conclusion: 通过将训练过程中的自身检查点融合为教师，GTR-Turbo在无需特权VLM的前提下实现高效、稳定的密集反馈与自蒸馏，显著提升性能并降低时间与计算成本，提升可复现性与实用性。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [136] [Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing](https://arxiv.org/abs/2512.13055)
*Jaeyoon Kim,Yoonki Cho,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: 提出一种用于资源受限设备的非对称VPR框架：离线用大模型提取图库特征，在线用轻量查询网络；通过地理记忆库替代昂贵的kNN兼容训练，并用隐式嵌入增广提升轻量网络的表征，达到更低计算成本且更高准确率。


<details>
  <summary>Details</summary>
Motivation: 基金模型（如DINOv2）在VPR上表现卓越，但计算开销大，难以在嵌入式/移动端实时部署。非对称检索（重图库、轻查询）是可行路径，但不同模型的特征空间不兼容，现有方法依赖昂贵的kNN对齐训练，限制了效率与可扩展性。

Method: 1) 非对称VPR：图库端使用高容量模型离线提取与存储特征；查询端使用轻量网络实时推理。2) 地理记忆库：利用VPR数据库固有的地理位置信息组织图库特征，替代kNN全量搜索，实现高效的跨模型兼容训练/匹配。3) 隐式嵌入增广：在不显式增大网络容量的前提下，对查询嵌入进行隐式变化建模，提升轻量网络对视角/光照等变化的鲁棒性。

Result: 在多组数据集上进行大量实验，方法显著降低训练与检索的计算成本，同时在准确率上优于现有非对称检索技术，刷新资源受限场景下的SOTA。

Conclusion: 以地理记忆库+隐式嵌入增广的非对称VPR为资源受限部署提供新范式：兼顾效率与精度，避免kNN带来的高昂代价，并在实际基准上验证了优越性。

Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR

</details>


### [137] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 提出在医学多模态VLM的持续学习中引入多模态多层次RAG与动态知识蒸馏，并构建更严格的MGTIL基准，显著提升跨模态迁移、细粒度保留与新任务适应能力，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学多模态VLM在持续学习面临两难：跨模态域差距大、细粒度模态内特征易遗忘/退化。现有方法很难同时兼顾知识迁移与精细特征保留，并缺少能同步考察这两点与实时学习的新基准。

Method: 1) 基于来自PubMed的1800万规模多模态医学检索库，引入多模态、多层级的RAG，为微调过程按需动态检索知识并提供实时指导；2) 提出动态知识蒸馏框架，按需求动态调整参数空间的重要性、蒸馏粒度（粗/细层级）、以及参考数据分布，以在跨域适配与细粒度保持之间权衡；3) 设计MGTIL基准，用于评测大域迁移、细粒度保留与实时新任务学习。

Result: 在MGTIL及多项评测上获得全指标SOTA，验证了方法在适应域迁移、保持细粒度特征与学习新复杂任务方面的有效性。

Conclusion: 多模态多层RAG与动态蒸馏的协同，可在医学VLM的持续学习中有效弥合跨模态域差距并保留细粒度信息；所提MGTIL为更严格和全面的评测基准，方法在实践中具有临床价值与可扩展性。

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [138] [Heart Disease Prediction using Case Based Reasoning (CBR)](https://arxiv.org/abs/2512.13078)
*Mohaiminul Islam Bhuiyan,Chan Hue Wah,Nur Shazwani Kamarudin,Nur Hafieza Ismail,Ahmad Fakhri Ab Nasir*

Main category: cs.CV

TL;DR: 论文比较了三种智能方法（模糊逻辑、神经网络、基于案例推理）在心脏病预测上的表现，最终选用CBR，并在预处理与数据分割后取得97.95%准确率；性别分布显示男性风险更高，相关研究指向吸烟与饮酒是重要因素。


<details>
  <summary>Details</summary>
Motivation: 传统依赖医生经验的诊断在精度与一致性上有限，需要借助智能系统提升心脏病预测的准确性与可解释性，并比较主流方法以选出效果更优的方案。

Method: 对心脏病数据集进行数据清洗与训练/测试划分；分别评估模糊逻辑、神经网络、与CBR的预测准确率；根据比较结果选定CBR进行最终预测与分析（含性别风险统计），并参考相关研究探讨吸烟、饮酒等因素。

Result: CBR在实验中达到97.95%准确率；样本中推断男性心脏病概率为57.76%，女性为42.24%。

Conclusion: 在该数据与设置下，CBR优于模糊逻辑与神经网络，适合作为心脏病预测方法；性别差异及生活方式因素（吸烟、饮酒）与心脏病风险相关。仍需注意数据集代表性、外部验证与可推广性。

Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.

</details>


### [139] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 提出一个可插拔的多样性正则项DiRe（由余弦相似度与欧氏距离构成），用于数据集凝练以减少冗余、提升多样性与泛化；在CIFAR-10到ImageNet-1K等基准上提升多种SOTA方法表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集凝练方法常生成高度冗余的合成样本，导致模型泛化受限、数据效率不佳；亟需系统性地提高合成数据的多样性并降低冗余。

Method: 设计多样性正则器DiRe：同时最小化样本间的余弦相似度（鼓励方向差异）并最大化欧氏距离（鼓励幅度/空间分离），作为通用正则项无缝集成至多种凝练算法的目标函数；在多基准上与现有方法联合训练评估。

Result: 在CIFAR-10到ImageNet-1K等数据集上，将DiRe加入多种SOTA凝练方法后，泛化性能与多样性指标均有一致提升，相比原方法显著减少合成数据冗余。

Conclusion: 多样性是高效数据凝练的关键；DiRe作为简单通用的正则项，能广泛提升凝练数据的有效性与多样性，并可无缝应用于现有方法。

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [140] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: 提出UniVCD：基于冻结的SAM2与CLIP，通过轻量特征对齐与后处理，实现无监督、开放词表的高分辨率变化检测，在多数据集上达SOTA级别F1/IoU。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测多依赖有监督训练，标注成本高、类目受限、跨场景泛化差；需要一种无需标注、可跨场景/几何、支持开放词表语义的通用方案。基础视觉模型（SAM2、CLIP）的兴起提供了可迁移的空间与语义先验，值得结合以降低监督需求。

Method: 冻结SAM2获取细粒度空间表示，冻结CLIP提供开放词表语义先验；设计轻量特征对齐模块融合二者，使语义与边界细节对齐以估计变化热图；无需成对标注或类别定义。再加一条简洁的后处理流水线（抑制噪声与伪变化，强化有清晰边界目标），最终输出二值/语义变化图。参数量小、训练需求低。

Result: 在多套公开的二值变化检测（BCD）与语义变化检测（SCD）基准上，UniVCD取得稳定强劲表现，在F1、IoU等关键指标上匹配或超越现有开放词表方法。

Conclusion: 冻结视觉基础模型结合轻量多模态对齐，可在无监督条件下实现有效的开放词表变化检测；该范式实用、泛化性强。代码与预训练模型将开源。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [141] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: 提出ADHint：在提示（hint）强化学习中显式建模“难度”，通过自适应提示比例、提示一致性梯度调制与选择性掩码、以及基于回滚难度的优势估计，实现在探索与模仿间更稳健的权衡，显著提升多模态与跨分布推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有将“提示前缀”引入后训练的RL方法能拓展知识与泛化，但常忽视样本/轨迹难度：提示比例调度与相对优势估计未区分难例，导致学习不稳定、过度模仿离策略提示，影响探索与真实能力提升。

Method: 1) 自适应提示（Adaptive Hint）与样本难度先验：用当前策略评估样本难度，据此为每个样本分配合适的提示比例，引导其rollout。2) 一致性驱动的梯度调制与提示保留的选择性掩码：在提示token内进行梯度幅度与方向调制，并对易破坏提示信息的更新进行掩码，避免偏置/破坏性更新。3) 基于回滚难度后验的优势估计：比较带/不带提示的rollout相对难度，分别估计优势，平衡探索与模仿带来的更新强度。

Result: 在多模态、不同模型规模与多领域基准上，ADHint在pass@1与avg@8上稳定优于现有方法，表现出更强的推理能力与分布外泛化。

Conclusion: 显式引入难度到提示比例调度与优势估计，并在提示区间进行细粒度梯度控制，可缓解过度模仿、提高稳定性与泛化；ADHint为结合SFT与RL的提示式后训练提供了更优的探索-模仿权衡方案。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [142] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: 提出UnCoL：一种不确定性感知的双教师半监督分割框架，将冻结的视觉基础模型的通用知识与可自适应教师的任务专长相结合，通过不确定性调控伪标签，显著提升2D/3D医学分割泛化并减少标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型虽具强泛化，但在标注稀缺、罕见病变等专业临床场景中易失配，通用先验难以满足任务特异需求。需要一种既能继承基础模型通识、又能学习细粒度任务表示，并在半监督设定下稳健训练的方法。

Method: UnCoL为双教师框架：1) 冻结的基础模型教师（视觉与语义蒸馏）迁移通用知识；2) 可逐步自适应的教师捕获任务特异、细粒度表示；3) 利用预测不确定性自适应调节伪标签学习，抑制不可靠监督，特别是模糊区域；适用于2D与3D数据。

Result: 在多种2D/3D医学图像分割基准上，UnCoL持续优于最先进半监督方法和各类基础模型基线；在显著减少标注的情况下接近全监督性能。

Conclusion: 通过不确定性调控的双教师协同学习，UnCoL在半监督医学分割中兼顾泛化与专门化，实现强鲁棒性与低标注成本，为临床特定任务提供更可靠的分割方案。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [143] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: 提出FID-Net，用可见光无人机图像精准检测虫害树，并结合空间统计（核密度、邻域评估、DBSCAN）开展虫情分析，性能优于YOLO基线。


<details>
  <summary>Details</summary>
Motivation: 传统林业虫害监测难以在大范围内实现精细化、实时的病株识别与扩散风险评估；现有检测方法对树木细微病征不敏感，且缺乏与空间模式分析的联动，难以支撑预警与精准管护。

Method: 在YOLOv8n上构建FID-Net：1) 设计轻量特征增强模块FEM，强化对病斑/黄化等细微线索的提取；2) 自适应多尺度特征融合AMFM，将RGB主干与FEM增强分支进行对齐融合；3) 引入高效通道注意ECA提升判别力。基于检测结果，构建虫情空间分析框架：核密度定位热点；邻域评估健康树受染风险；DBSCAN识别高密度健康树簇作为优先保护区。

Result: 在中国东天山32块林地的无人机可见光数据上，FID-Net取得P=86.10%，R=75.44%，mAP@0.5=82.29%，mAP@0.5:0.95=64.30%，整体优于主流YOLO模型；空间分析验证受害树呈显著聚集分布。

Conclusion: FID-Net能高精度区分健康/受害树，并与空间指标联动，为虫害智能监测、早期预警与精准治理提供可靠依据与可操作的区域优先级划分。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [144] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: DiffFusion提出基于扩散模型的多模态3D检测鲁棒框架：先用扩散恢复图像与点云，再通过双向自适应融合与BEV对齐缓解模态失配，在多数据集与零样本真实恶劣天气上达SOTA且不损干净数据表现。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气会引入图像噪声、能见度下降和LiDAR稀疏/遮挡等退化，并造成多模态（图像-点云）时空/几何不对齐，现有多模态3D检测在这种条件下鲁棒性差。需要一种既能恢复退化数据、又能稳健对齐与融合多模态信息的方法。

Method: 提出DiffFusion框架：1) Diffusion-IR：利用扩散模型对受天气影响的图像进行去噪与重建；2) PCR（Point Cloud Restoration）：借助图像目标线索补全/修复受损的点云；3) BAFAM（双向自适应融合与对齐模块）：在BEV空间进行动态跨模态特征融合，并进行双向BEV对齐以缓解模态错位，维持一致空间对应。整体端到端用于多模态3D检测。

Result: 在三个公开数据集上，在恶劣天气下取得SOTA鲁棒性，同时在晴天/干净数据上保持强性能；在真实世界DENSE数据集上实现零样本泛化验证有效性。

Conclusion: 基于扩散的恢复结合自适应跨模态双向对齐与融合，能显著提升恶劣天气下的多模态3D检测鲁棒性且不牺牲常规场景表现；方法具有良好泛化，代码将开源。

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [145] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

TL;DR: DePT3R提出一种无需相机位姿与时间顺序、在一次前向中同时完成动态场景稠密点跟踪与3D重建的框架，利用强大的时空特征骨干和密集预测头实现端到端多任务学习，在多基准上效果强且更省内存。


<details>
  <summary>Details</summary>
Motivation: 现有稠密3D点跟踪方法多依赖成对处理、已知位姿或帧序假设，限制灵活性与普适性；而无位姿大规模图像的高效3D重建已取得进展，提示可将动态场景理解统一化。

Method: 设计DePT3R：以强时空骨干提取深度特征，通过密集预测头回归像素级地图，实现点跟踪与3D重建的多任务联合学习；不依赖相机位姿或帧序，以单次前向从多张图像同时推理。

Result: 在多种动态场景基准上取得强性能，并较SOTA显著降低内存占用。

Conclusion: DePT3R在无需位姿与时间顺序的前提下，实现动态场景的高效稠密点跟踪与3D重建，兼具准确性与内存效率，具备更强适应性与实用性。

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [146] [LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping](https://arxiv.org/abs/2512.13130)
*Shanghua Liu,Majharulislam Babor,Christoph Verduyn,Breght Vandenberghe,Bruno Betoni Parodi,Cornelia Weltzien,Marina M. -C. Höhne*

Main category: cs.CV

TL;DR: 提出CanolaTrack数据集与LeafTrackNet方法，实现在真实条件下对油菜叶片的高精度多时序跟踪，较SOTA提升HOTA 9%。


<details>
  <summary>Details</summary>
Motivation: 单叶级高分辨率表型可揭示发育与胁迫响应，但复杂作物（如油菜）缺乏鲁棒的跨时间跟踪方法与大规模真实场景数据；通用MOT不适配动态生物体场景，现有植物跟踪受限于小物种或受控成像。

Method: 构建CanolaTrack基准：5,704张RGB、31,840标注叶实例、184株油菜早期生长阶段；提出LeafTrackNet框架：YOLOv10叶片检测器+MobileNetV3嵌入特征网络；推理时使用基于嵌入的记忆关联策略维持叶片身份。

Result: LeafTrackNet在CanolaTrack上优于植物专用与SOTA通用MOT基线，在HOTA指标上提升9%，实现更稳健的跨时序叶片身份保持。

Conclusion: CanolaTrack为农业作物叶片跟踪提供最大规模数据集与真实条件基准；LeafTrackNet树立叶级跟踪新标准并将推动植物表型研究；代码与数据集已开源。

Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.

</details>


### [147] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 提出一种“权重空间相关分析”，用来量化临床任务是否实际利用了嵌入中的特征，而非仅仅包含元数据；在合成与真实超声sPTB任务上验证其能识别并排除捷径学习。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度模型常无意中依赖扫描仪型号等元数据形成捷径学习；虽然这些信息常被编码进图像嵌入，但关键是模型是否在最终预测中真正使用了它。需要一种可解释、可量化的方法来评估“被编码的特征是否被利用”。

Method: 提出权重空间相关分析：训练一个主临床任务分类头与若干元数据/辅助任务分类头，计算它们权重向量在嵌入空间的对齐度（相关性/夹角），以此估计主任务对各类特征的利用程度。先通过人为注入偏置来验证方法能检测捷径；再应用于SA-SonoNet预测早产（sPTB），比较与临床相关（如出生体重）与与采集无关（如扫描仪）因素的相关性。

Result: 方法能在注入偏置时成功检测到捷径学习；在真实sPTB模型上，虽然嵌入含有大量元数据，主分类器权重与临床相关因素高度相关，而与扫描仪等采集因素解耦。

Conclusion: 该方法提供了一种验证模型可信度的工具：当不存在人为偏置时，临床模型会选择性利用与真实临床信号相关的特征，而非与采集相关的无关元数据。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [148] [StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion](https://arxiv.org/abs/2512.13147)
*Sangmin Hong,Suyoung Lee,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: StarryGazer提出一种无需真值深度的无监督深度补全框架：用预训练单目深度模型生成相对深度，合成稠密-稀疏对训练一个精炼网络，最终在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度补全依赖辅助数据（如多视角/光流），不贴近实际；直接将单目深度经仿射对齐与稀疏深度融合会因跨物体相对深度误差而高误差。需要一种既能利用大规模单目深度模型能力、又能用稀疏深度纠错的通用方法。

Method: 1) 用大规模预训练单目深度估计（MDE）生成相对深度图；2) 对MDE深度进行分割并随机分段缩放，构造合成稠密伪真值及对应稀疏深度对，形成训练数据；3) 训练一个精炼网络，输入为RGB、相对深度与稀疏深度，学习从合成对中纠偏与细化；4) 推理时无需真值，仅用单张RGB与单张稀疏深度得到稠密深度。

Result: 在多种数据集上，StarryGazer优于现有无监督方法以及对MDE简单变换的基线，显示更高精度与鲁棒性。

Conclusion: 利用大模型MDE提供的强先验，并通过稀疏深度与合成训练策略校正其系统性误差，可在无真值条件下实现通用、稳健的深度补全。

Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.

</details>


### [149] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出“内在图像融合”方法：结合单视图扩散先验与多视图稳健融合，在低维参数空间中重建一致的物理材质，并以逆路径追踪微调，取得比现有方法更清晰锐利的可重光照结果。


<details>
  <summary>Details</summary>
Motivation: 仅凭多视图进行材质分解高度欠定，传统分析-合成需大量噪声高、代价大的路径追踪；单视图方法虽有扩散模型先验，但跨视角不一致。需要把强单视图先验与多视图几何/光照一致性结合，降低优化难度与成本。

Method: 1) 使用扩散式材质估计器为每个视角生成多组候选内在分解；2) 用显式低维参数化函数拟合这些预测，降低维度与不一致；3) 设计稳健优化：基于置信度的软选择与软多视图内点集，融合跨视角最一致的候选到统一参数空间；4) 最后以逆路径追踪在低维空间中微调参数实现物理一致重建。

Result: 在合成与真实场景上，相比最先进方法，显著提升材质解耦质量，重建更干净、边缘更锐利，适合高质量重光照；定量定性均优。

Conclusion: 将单视图扩散先验与多视图稳健融合及低维参数化结合，可有效缓解欠定与噪声问题，减少路径追踪负担，得到一致且可重光照的高质量材质重建。

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [150] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: 提出CRAFTS：病理学专用文本到图像生成的基础模型，通过相关性调控的对齐机制与双阶段训练，生成生物学一致的多癌种病理图像，并在多项下游临床任务中提升性能，同时可与ControlNet结合实现组织结构可控合成，缓解数据匮乏与隐私问题。


<details>
  <summary>Details</summary>
Motivation: 临床级病理AI受限于高质量、多样化、带注释数据稀缺；现有生成模型在病理领域存在语义不稳定与形态幻觉，影响诊断可靠性，亟需一种能保持生物学准确性的文本到图像生成方法以扩充数据并支持临床任务。

Method: 提出CRAFTS（Correlation-Regulated Alignment Framework for Tissue Synthesis），在约280万图文对上采用双阶段训练；引入“相关性调控对齐机制”抑制语义漂移、保证生物学准确；生成覆盖30种癌症的病理图像；并与ControlNet耦合，以核分割掩码、荧光图像等作为条件，精确控制组织结构。

Result: CRAFTS生成的图像在客观指标与病理医生评估中质量可靠且多样；将CRAFTS合成数据用于增强数据集，可显著提升分类、跨模态检索、自监督学习、视觉问答等多项任务表现；可实现对组织架构的可控合成。

Conclusion: CRAFTS作为病理专用生成式基础模型，通过对齐机制与可控合成，缓解数据稀缺与隐私限制，提供持续、可注释的组织学数据源，为罕见与复杂癌症表型的稳健诊断工具开发铺路。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [151] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: 提出DFSS：一种针对语义分割的数据无关蒸馏框架，利用教师BN统计进行近似分布采样（ADS）和加权分布渐进蒸馏（WDPD），在无需真实数据情况下保持结构连续性并取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有DFKD方法多为分类设计，忽视语义分割中的空间连续性与结构一致性，直接迁移会导致显著性能下降；需要一种既不依赖原数据又能保留场景结构与上下文的蒸馏方法。

Method: 1) 近似分布采样（ADS）：从教师模型的BN统计恢复/约束特征与输入分布，选择更贴近原训练分布的合成或无标签数据，同时避免直接依赖可能偏差的教师预测。2) 加权分布渐进蒸馏（WDPD）：在训练早期更重视与原分布更接近、可靠性更高的样本，随后逐步纳入更困难样本，实现由易到难的动态权重与课程式学习。整体上显式建模分割任务的结构与上下文连续性，而非逐像素独立处理。

Result: 在标准基准上优于现有数据无关蒸馏方法，达成SOTA；在极少或不依赖辅助数据条件下也能获得更高的分割性能。

Conclusion: 通过利用教师BN统计进行数据选择与分布对齐，并结合渐进式加权蒸馏，DFSS有效保留语义分割的结构连续性，显著提升DFKD在分割任务中的适用性与效果。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [152] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: MMDrive提出融合占用图、激光点云与文本的多模态视觉-语言框架，利用文本引导的动态加权与跨模态抽象令牌，实现更强的3D场景理解与推理，在DriveLM与NuScenes-QA上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多依赖2D图像理解，难以获取3D空间感知与深层语义融合，导致在复杂自动驾驶场景中表现不佳。需要一种能综合3D几何与语义、多源传感的统一框架。

Method: 引入三模态输入：占用图（空间结构）、LiDAR点云（几何精细度）、文本场景描述（高层语义）。提出两大模块：1) 文本导向多模态调制器（TMM），依据问题中的语义线索自适应地为各模态分配权重，实现上下文敏感的特征融合；2) 跨模态抽象器（CMA），通过可学习抽象token生成紧凑的跨模态摘要，聚焦关键区域与核心语义，提升信息提炼与推理效率。

Result: 在DriveLM与NuScenes-QA基准上取得领先：DriveLM的BLEU-4=54.56、METEOR=41.78；NuScenes-QA准确率62.7%，显著优于现有VLM。

Conclusion: MMDrive打破图像单模态限制，实现对复杂驾驶环境的稳健多模态推理，为可解释的自动驾驶场景理解提供新基础。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [153] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: 提出CoRA混合融合架构，通过特征级融合与目标级校正实现高性能与鲁棒性兼得，在恶劣通信下仍显著提升AP且降低带宽。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知多采用中间融合以获得高性能与通信效率，但在通信不良（时延、丢包、异步、姿态误差）下会引入特征错位，性能显著下降，限制实际部署。需要一种既能保留中间融合性能与扩展性，又能在恶劣通信与位姿误差下保持鲁棒性的方案。

Method: 提出CoRA：混合式协同架构，解耦“性能”与“鲁棒性”。包含两条分支：1) 特征级融合分支：选择关键特征并高效融合，兼顾性能与可扩展性、降低通信；2) 目标级校正分支：利用语义相关性进行空间位移纠正，抵抗姿态误差与通信引起的错位。整体实现低通信量下的稳健协同。

Result: 在极端场景中，相比基线在AP@0.7上提升约19%，同时通信量减少超过5倍，优于SOTA的鲁棒性-性能权衡。

Conclusion: 中间融合与后期融合并非此消彼长，而是可互补。CoRA通过特征融合确保性能，通过目标级校正确保鲁棒性，在低通信下实现稳健协同感知，具备实际部署潜力。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [154] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: 提出POLAR数据集与POLARNet模型，实现基于单张人像的物理一致、可控的人脸重光照。


<details>
  <summary>Details</summary>
Motivation: 人脸重光照受限于缺乏大规模、物理一致的光照数据，使得现有方法难以准确建模方向敏感、可解释的光照效应并保持身份与几何。

Method: 1) 采集POLAR：200+受试者、156个单光源（OLAT）方向、多视角、多表情，物理标定的光照。2) 设计POLARNet：基于flow（可逆流）生成模型，从单张人像预测每个光源方向的OLAT响应；将光照建模为在光照状态间的连续、可解释变换，强调方向感知与身份保持。

Result: POLAR与POLARNet实现对细粒度、方向感知的光照效果合成，在无需依赖统计或背景上下文的情况下生成物理一致的重光照结果，支持可扩展与可控的重光照。

Conclusion: 数据（POLAR）与模型（POLARNet）共同构成统一的光照学习框架，将真实数据、生成合成与物理化重光照闭环耦合，形成可扩展、可复现的“鸡生蛋蛋生鸡”式循环，推进人像重光照的规模化与可控性。

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [155] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Ego-EXTRA 是一个用于专家-学员辅助的第一人称视频-语言数据集：50小时无脚本的学员执行流程任务视频，配套专家以自然语言指导，形成高质量双向对话与15k+视觉问答，用于评测多模态大模型；结果显示任务具有挑战性、现有模型在专家级辅助上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在真实流程任务中的专家级指导能力不足，缺乏带有高质量专家-学员互动与第一人称视角的视频-语言数据与基准，难以系统评估与提升模型的实用辅助能力。

Method: 采用“绿野仙踪”范式：专家佩戴设备仅基于学员的第一人称视角观察并扮演智能助手，按需回答或主动建议。采集50小时无脚本流程活动，记录并转写专家-学员双向对话，构建包含15k+高质量视觉问答对的基准，用于评测多模态大模型在专家级辅助情境中的表现。

Result: 数据集与基准公开；在多个多模态大模型上的评测显示：Ego-EXTRA任务难度高，现有模型在理解第一人称视频、上下文对话、以及提供专家级操作建议方面存在显著局限。

Conclusion: Ego-EXTRA为第一人称视频-语言助手提供了规模化、高质量的专家-学员互动数据和评测基准，揭示当前模型短板，为后续研究改进专家级辅助能力和对话式流程理解提供基础。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [156] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: STARCaster 是一个身份感知的时空视频扩散模型，在同一框架下同时支持语音驱动的人像动画与自由视角的说话人像合成；通过弱化参考图像依赖与在2D视频域中隐式建模3D一致性，提升运动多样性、视角一致性与身份保持。


<details>
  <summary>Details</summary>
Motivation: 现有2D语音到视频扩散强依赖参考图导致动作单一；3D感知方法多通过三平面生成器反演，重建不完美且易身份漂移。需要一种既能保留身份、又能在多视角与时间维度上保持一致，同时具备更高运动多样性的统一方法。

Method: 提出 STARCaster：1) 在预训练中采用更“软”的身份约束，降低对严格参考条件的依赖，实现ID-aware的运动建模；2) 在2D视频域中利用视频的多视角属性隐式建模3D感知；3) 采用组成式训练路径：先做身份感知的动作建模，再通过基于唇读的监督实现音视同步，最后用“时域到空域”的适配实现新视角动画；4) 数据稀缺下的解耦学习：将视角一致性训练与时间一致性训练分开；5) 自我强制（self-forcing）训练，使模型在训练时看到比推理更长的时间上下文，缓解自回归方法常见的过于静态问题。

Result: 在多项基准上优于现有方法，在不同任务（语音驱动动画与自由视角说话人像）和不同身份上具有更好的泛化，表现为更丰富的运动、更稳定的身份保持、更好音唇同步与视角一致性。

Conclusion: 通过弱化参考条件与在2D域中隐式注入3D意识，配合解耦与自我强制训练，STARCaster 在统一框架下实现高质量、可泛化的说话人像生成，解决了运动单一与身份漂移等痛点，并在多基准上取得领先性能。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [157] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: 提出VG-AVS任务：仅基于当前图像选择下一最信息视角，并通过SFT+强化学习微调VLM，实现更强VQA与跨场景泛化，并提升现有EQA系统表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多停留在静态图像的快照视觉，无法像具身智能体那样主动移动获取更有信息的视角；需要一种仅依赖当前视觉、无需场景记忆或外部知识的主动视角选择机制，以支撑具身VQA与探索。

Method: 1) 构建合成数据集：自动生成成对的查询-目标视图及问答提示；2) 设计VG-AVS任务：输入当前图像与问题，输出下一最信息化视角；3) 模型框架：在预训练VLM上先做监督微调（SFT）学习视角选择与回答，再进行基于强化学习的策略优化，以最大化问答表现；4) 将学得策略集成到现有基于场景探索的EQA系统。

Result: 模型在基于所选视角的问答上取得强结果，且在未见过的合成与真实场景上表现稳健泛化；当集成到现有EQA系统时，可显著提升下游问答准确率。

Conclusion: 只用当前图像即可学得有效的主动视角选择策略，能增强VLM在具身场景中的问答能力，并作为通用组件提升EQA系统性能，显示出良好的泛化潜力。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [158] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: 提出CogniEdit，将多模态推理与密集奖励优化结合，实现对扩散模型指令编辑的细粒度、轨迹级控制，达成SOTA的指令遵循、画质与可编辑性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑对颜色、位置、数量等精细属性的理解与控制不足；采用GRPO的对齐方法只在单步采样处优化，反馈稀疏，难以进行轨迹级别的控制与优化。

Method: 统一框架CogniEdit含三部分：1) 使用多模态大模型将复杂指令分解为可执行子指令；2) 动态Token焦点迁移机制，自适应强调细粒度属性；3) 基于Dense GRPO的优化，在连续去噪步骤间传播梯度，实现轨迹级监督与梯度流。

Result: 在多个基准上取得SOTA：在精细指令遵循、视觉质量与可编辑性保持之间实现更佳平衡，较现有方法显著提升。

Conclusion: 将多模态推理与密集奖励的轨迹级优化相结合，可显著增强扩散模型的细粒度指令编辑能力，同时不牺牲画质和原图可编辑性。

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [159] [Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?](https://arxiv.org/abs/2512.13281)
*Jiaqi Wang,Weijia Wu,Yi Zhan,Rui Zhao,Ming Hu,James Cheng,Wei Liu,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 提出Video Reality Test：基于ASMR场景、强调音画强耦合的生成视频真实感评测基准。采用“创作者-审稿人”对抗协议，用VLM做审稿人。实验显示：最强生成器能骗过大多数VLM（Gemini 2.5-Pro仅56%），人类专家达81.25%。音频有助判别，但水印等表层线索会误导模型，暴露VLM在感知保真与视听一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC视频检测基准多忽略音频、聚焦宽泛叙事且只做二分类，无法回答当代视频生成在强视听耦合、沉浸式场景下能否欺骗人类与VLM。亟需一个细粒度、音画联动、可对抗评测的标准化测试套件。

Method: 构建Video Reality Test：1) 从精挑细选的真实ASMR视频出发，覆盖细粒度动作-物体交互，具备对象、动作、背景多样性与强音画耦合；2) 设计“创作者（生成模型）-审稿人（VLM）”对抗评审协议，生成模型试图以伪乱真视频迷惑审稿人，VLM负责识别真伪；3) 比较仅视频与视频+音频设置，并考察水印等表层提示对判别的影响。

Result: 最强生成器Veo3.1-Fast可显著欺骗VLM；Gemini 2.5-Pro仅56%准确率（接近随机50%），明显落后于人类专家81.25%。加入音频能提升真伪判别，但VLM仍会被水印等非语义线索误导。

Conclusion: 在强音画耦合的ASMR场景下，当前视频生成逼真度已足以迷惑主流VLM，而人类仍明显更强。VLM在感知保真与视听一致性理解上存在短板；应发展更鲁棒的多模态鉴伪方法并抑制对表层线索的依赖。

Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.

</details>


### [160] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: 提出CausalCLIP，通过因果推断指导的特征去纠缠与筛选，提升生成图像检测在未知模型上的泛化，较SOTA提升约6.8%准确率、4.1% AP。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测器（含基于预训练视觉语言模型的方法）容易将任务相关取证线索与无关或伪相关模式混杂，导致在新型或分布移位的生成模型上泛化差。需要一种能显式分离因果线索并保留可迁移取证特征的方法。

Method: 构建生成过程的结构因果模型，设计CausalCLIP：在CLIP表征上进行因果/非因果特征的显式去纠缠；利用Gumbel-Softmax特征掩码进行可微选择与过滤；通过HSIC约束强化因果与非因果特征的统计独立；最终仅保留稳定的因果取证特征用于判别。

Result: 在未见过的生成模型与跨系列设置上，较现有SOTA实现+6.83%准确率、+4.06%平均精度的提升，显示更强的零样本/跨分布泛化能力。

Conclusion: 因果引导的表征去纠缠与有针对性的特征筛选能有效隔离稳定取证线索，显著提升对新型生成模型的检测泛化；CausalCLIP验证了结合因果推断与预训练多模态模型在取证任务中的优势。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [161] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: 提出LINA框架，通过因果干预引导扩散模型，提升物理一致性与分布外指令跟随，在因果生成与Winoground上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像/视频生成中强，但常出现物理不对齐（如不符合力学、遮挡、接触等）与分布外复杂指令跟随失败。作者认为根源是模型未学到正确的因果方向，且无法将因果因素解耦以进行新组合，需要可诊断与可干预的工具来定位问题并改进。

Method: 1) 构建Causal Scene Graph（CSG）和Physical Alignment Probe（PAP）数据集，对模型进行因果诊断与干预测试；2) 实证分析发现：a) 模型对提示未明确元素的多跳推理薄弱；b) 提示嵌入中纹理与物理表征可分离；c) 视觉因果结构主要在早期降噪步骤确立；3) 基于上述洞见提出LINA：a) 学习预测“提示特定”的干预；b) 在提示和视觉潜空间实施定向引导；c) 重分配并因果感知的降噪日程（在关键早期步强化因果结构）。

Result: LINA在图像与视频扩散模型上显著提升物理一致性与OOD指令跟随能力，在多项因果生成任务和Winoground数据集上达到SOTA表现。

Conclusion: 通过因果图谱与探针诊断揭示扩散模型的因果学习缺陷，并用LINA进行自适应因果干预与调度，可系统性提升物理对齐与复杂指令跟随，具有普适性与可扩展性。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [162] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: 提出“创意表格可视化”任务与评测集TableVisBench，并给出ShowTable管线（MLLM+扩散模型逐步自纠）显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 通用生成/统一模型在深度推理、规划、精确数据到视觉映射上表现不足，难以从表格数据高保真且美观地生成信息图。需要新任务与方法推动模型在多模态推理与可控生成方面的能力。

Method: 提出ShowTable：以MLLM为中枢负责视觉规划、错误判别与指令迭代，扩散模型按指令执行生成；采用渐进式自纠循环提升保真度与美学质量。为训练不同模块，构建三条自动化数据构建流水线。并发布包含800个实例、5个评估维度的基准TableVisBench。

Result: 在TableVisBench上，用不同底座实例化的ShowTable显著优于多种基线，展示出在多模态推理、图像生成与错误纠正方面的优势与稳定提升。

Conclusion: 通过将MLLM的规划/评估与扩散模型的执行相结合，并配合自纠与自动化数据构建，ShowTable有效解决表格到信息图的高保真美学可视化，推动该方向研究；TableVisBench为标准化评测提供基础。

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [163] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: 提出KlingAvatar 2.0：一种时空级联的头像视频生成框架，先生成低分辨率关键帧蓝图，再按首尾帧策略精炼为高分辨率、长时段时序一致的子片段，并引入多模态LLM“协同导演”和“负向导演”以提升指令对齐、身份与口型同步，多角色可控。实验显示在长时高分视频上效率、清晰度、身份保持与多模态对齐显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有头像视频生成在长时高分辨率场景下效率低、时序漂移、画质下降、指令跟随弱，难以实现稳定连贯的长视频与多模态对齐。

Method: 1) 时空级联：先生成低分辨率蓝图关键帧捕获全局语义与运动；再以“首-尾帧”策略精炼为高分辨率、时间一致的子片段，并保证子片段间平滑过渡，实现时空放大。2) 协同导演：由三个模态专长LLM专家组成，通过多轮对话进行模态优先级推理与用户意图解析，将输入转化为细化剧情。3) 负向导演：自动生成与优化负向提示词提升指令对齐与鲁棒性。4) 扩展到ID特定的多角色控制。

Result: 在大量实验中，相较以往方法，能够高效生成长时高分辨率头像视频，显著改善视觉清晰度、时序一致性、口唇牙的真实渲染与唇形同步、身份保持，以及对多模态指令的连贯遵循。

Conclusion: KlingAvatar 2.0通过时空级联和多模态LLM导演机制，有效缓解长视频生成中的漂移与画质衰减问题，实现更高效、更清晰且更一致的多模态对齐与多角色可控的视频生成。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [164] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 论文研究人脸检索中的“身份遗忘”：让被请求遗忘的人的特征在超球面上被打散，从而无法形成紧致聚类、难以被重新检索；同时保持其余身份的检索性能。提出一种简单有效的基于“分散”的遗忘方法，在VGGFace2与CelebA上优于现有近似类别遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 人脸检索依赖紧致判别簇，但这也带来隐私风险（未授权跟踪）。现有“机器遗忘”多针对分类/监督场景，对现代嵌入式人脸检索的适用性不足。需要一种能对指定身份“可检索性”进行撤销的机制，同时不破坏其余身份的检索能力。

Method: 系统性评估多种近似类别遗忘策略（随机标注、梯度上升、边界遗忘等）在面部检索中的表现；提出一种“基于分散”的遗忘：通过优化使被遗忘身份的嵌入在超球面上扩散、削弱聚类结构，同时约束保持其余身份的嵌入判别结构与检索性能。

Result: 在VGGFace2与CelebA基准上，所提方法在“遗忘效果”（目标身份不可检索/聚类破坏）方面优于对比方法，同时更好地保持其余身份的检索效用。

Conclusion: 在人脸检索中，直接将“遗忘”表述为在嵌入超球面上对目标身份进行分散是有效的；比现有近似类别遗忘方法在遗忘-效用权衡上更优，提供了面向隐私保护的实用方案。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [165] [Automated User Identification from Facial Thermograms with Siamese Networks](https://arxiv.org/abs/2512.13361)
*Elizaveta Prozorova,Anton Konev,Vladimir Faerman*

Main category: cs.CV

TL;DR: 论文评估热成像人脸生物识别：比较NIR/SWIR/MWIR/LWIR，给出相机关键指标（分辨率、热灵敏度、≥30 Hz帧率），采用孪生网络实现自动识别，在自建数据集上约80%准确率，并探讨可见光+红外的混合系统，结论为热成像对安防有潜力。


<details>
  <summary>Details</summary>
Motivation: 传统可见光人脸在光照、伪装、活体检测等条件下鲁棒性有限；红外不同波段对环境与伪装敏感性不同，需系统性比较并明确设备参数与算法路径，以评估热成像在人脸识别中的可行性与工程要求。

Method: （1）综述并对比NIR/SWIR/MWIR/LWIR在成像机制、环境适应性与隐私/抗伪造特性；（2）提出热相机配置要求：足够分辨率、较高热敏感度、≥30 Hz帧率；（3）采用孪生神经网络进行热脸特征提取与相似度度量；（4）在自建数据集上实验，并讨论与可见光融合的混合系统方案。

Result: 孪生网络在专有数据集上达到约80%识别准确率；不同红外波段各有优势与限制；混合可见光+红外有助于弥补单一模态不足。

Conclusion: 热成像具备在安防生物识别中应用的前景，但当前准确率受数据规模、传感器规格和算法限制；建议采用满足关键指标的热相机，并探索多模态融合以提升鲁棒性与可靠性。

Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.

</details>


### [166] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: 提出利用DINO自注意力的key特征+简单卷积解码器进行息肉分割，在多中心DG与极端单域ESDG协议下取得SOTA泛化性能，优于nnU-Net与UM-Net，并系统评测DINO演进对下游的影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习息肉分割方法在数据受限或跨域场景下泛化差，且多依赖复杂、任务特定的架构；需要一种更鲁棒、可泛化、且架构简单的方案。

Method: 使用DINO（自监督ViT）中的自注意力模块的key特征作为鲁棒表征，摒弃仅取最深层token的做法；配套一个简单的卷积解码器输出息肉掩码。基于多中心数据，在Domain Generalization与Extreme Single Domain Generalization两种严格协议下训练/评测；并对不同版本DINO架构演进进行系统化基准对比。

Result: 在DG与ESDG设置下达到SOTA，统计分析显示显著优于现有方法，尤其在数据稀缺与复杂场景中；在无需息肉特定设计的情况下超越nnU-Net与UM-Net。

Conclusion: 利用DINO自注意力key特征可显著提升息肉分割的跨域泛化与稳健性，简单解码器即可取得强性能；同时提供了DINO演进对下游分割表现的量化证据，证明该管线具有通用且高可迁移的潜力。

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [167] [Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs](https://arxiv.org/abs/2512.13392)
*Anran Qi,Changjian Li,Adrien Bousseau,Niloy J. Mitra*

Main category: cs.CV

TL;DR: 提出一种训练免的图像到视频方法，利用可编辑的代理动态图（PDG）分离“运动控制”和“外观合成”，在保持扩散先验冻结的同时，实现可预测的关节化运动与对最终帧遮挡消解区域的用户可控编辑。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频方法虽能生成合理运动，但难以在“新显露（遮挡消解）区域”中同时满足用户对内容的明确约束，并产生可预测的关节化运动。需要一种既能精确控制运动又能让用户指定新显露区域外观的流程，且最好无需微调。

Method: 引入轻量、可用户编辑的代理动态图（PDG）来确定性地近似驱动部件运动；保持扩散模型冻结，把其当作遵循给定运动场的“运动引导着色器”。流程：1) 用户对PDG进行松弛标注与重定姿；2) 由PDG计算稠密运动光流；3) 使用扩散模型依据该光流进行外观合成；4) 用户在最终帧的遮挡消解区域进行外观编辑；5) 借助PDG编码的可见性，在潜空间做复合以协调运动与用户意图。全流程无需训练/微调。

Result: 在多类对象（关节体、家具、车辆、可形变体）的图像转短视频任务上，相比现有方法展现出更强的关节控制力与对新显露区域的可控性，生成更可预测的运动与匹配用户指定外观的结果。

Conclusion: 将“松弛姿态/结构”形式的生成控制与“最终帧新显露区域外观”这种可预测控制结合，通过PDG+冻结扩散先验实现训练免、可编辑、可控的图像到视频新工作流；为用户提供对遮挡消解的直接掌控并显著优于现有方案。

Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

</details>


### [168] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 该工作提出用神经元胞自动机作为通用“细化器”，对分割掩膜做局部迭代修复，能自动消除碎片、断裂并恢复拓扑一致性，跨任务/模型有效；在血管和心肌数据上显著降低Betti错误并提升Dice等指标。


<details>
  <summary>Details</summary>
Motivation: 通用分割模型常产生拓扑错误（断裂、孤立碎片），现有修复方式依赖任务特定规则或专用架构，泛化差、维护重。作者希望用一种任务无关、可学习且只依赖局部信息的机制，自动从粗糙掩膜收敛到拓扑正确的目标形状。

Method: 将神经元胞自动机（NCA）重用于“精修器”（rNCA）：以图像与初始掩膜为输入，通过局部状态与邻域更新的迭代动力学演化掩膜。训练时以不完美掩膜与真值对监督，使NCA学到目标形状的结构规律；推理时对任意基础模型的粗掩膜迭代更新，逐步连接断点、剪除游离片段，直到收敛为稳定、拓扑一致的结果。

Result: 在碎裂的视网膜血管分割上，rNCA带来Dice/clDice提升约2–3%，Betti误差显著下降：β0减少60%、β1减少20%；在心肌分割上，零样本条件下修复61.5%的断裂案例，并将ASSD与HD分别降低19%与16%。

Conclusion: NCA可作为简单、通用且数据驱动的分割后处理细化器，仅用局部信息即可纠正多种拓扑错误，跨模型与任务有效，能稳定收敛到拓扑一致的掩膜，减少对手工规则或专用网络的依赖。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [169] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

TL;DR: 提出End2Reg，一个端到端联合分割与配准的RGB-D方法，在脊柱术中导航场景将TRE降至1.83mm、RMSE至3.95mm，减少对弱分割标签与手工步骤的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有术中导航依赖放射影像和骨锚标记，侵入性强、辐射高且影响流程；基于RGB-D的无标记配准虽有前景，但常需依赖质量不高的弱分割标签来提取解剖结构，误差会在配准过程中放大。需要一种能在不依赖弱标签和人工干预的前提下，直接优化与配准目标一致的特征/掩膜的方法。

Method: 提出End2Reg端到端框架，将分割与配准共同优化：网络学习到的分割掩膜并不受显式分割监督，而是通过配准目标（如点云/深度-几何对齐误差）间接引导；整个流程从RGB-D输入到位姿/变换参数输出，无需手工步骤或弱标签。包含消融研究验证联合优化的贡献。

Result: 在体外与在体基准上均达SOTA：中位目标配准误差(TRE)降低32%至1.83mm；均方根误差(RMSE)降低45%至3.95mm。消融显示端到端联合优化显著提升配准精度。

Conclusion: End2Reg实现了不依赖弱分割标签和人工干预的端到端RGB-D配准，向全自动、无标记的术中脊柱导航迈进一步；代码与交互可视化已开源。

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [170] [Computer vision training dataset generation for robotic environments using Gaussian splatting](https://arxiv.org/abs/2512.13411)
*Patryk Niżeniec,Marcin Iwanowski*

Main category: cs.CV

TL;DR: 提出一种利用3D高斯泼溅与游戏引擎物理仿真生成高保真、自动标注的大规模视觉数据集的管线；通过双通道渲染与阴影合成缩小合成-真实域间隙，并验证少量真实+大量合成的混合训练在检测与分割上最佳。


<details>
  <summary>Details</summary>
Motivation: 合成数据标注便宜但与真实分布存在域间隙；真实数据获取与逐像素标注代价高。需要一种既高保真又可自动生成标注的数据管线，以提升机器人视觉任务的泛化与精度。

Method: 1) 以3D Gaussian Splatting重建环境与物体，获得逼真外观；2) 将资产导入游戏引擎，用物理仿真产生自然布局；3) 提出双通道渲染：一条渲染3DGS“splat”的真实感影像，另一条由代理网格生成阴影贴图；4) 通过算法将阴影图与渲染结果合成，加入物理合理阴影与高光；5) 自动输出像素级分割掩码，适配YOLO等检测/分割训练格式；6) 用小量真实+大量合成进行混合训练。

Result: 生成的数据具有增强的光照与阴影真实感，自动得到像素级掩码。实验表明：单用合成或单用真实不如混合训练；“少量真实+大量合成”在目标检测与分割性能上最好。

Conclusion: 基于3DGS与双通道渲染的自动数据生成管线有效缩小域间隙，降低标注成本，并通过混合训练显著提升机器人视觉检测/分割的鲁棒性与准确度，是高效获取高质量训练数据的可行策略。

Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.

</details>


### [171] [USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition](https://arxiv.org/abs/2512.13415)
*Ahmed Abul Hasanaath,Hamzah Luqman*

Main category: cs.CV

TL;DR: 提出USTM：以Swin Transformer为空间骨干，配合带位置嵌入的轻量级时间适配器TAPE，统一建模手语视频的时空信息；在PHOENIX14/PHOENIX14T/CSL-Daily上取得SOTA或具竞争力表现，且仅用RGB、无需多流或多模态。


<details>
  <summary>Details</summary>
Motivation: 现有CSLR多用CNN+时序卷积/循环网络，难以捕获手部/面部细粒度线索且对长程时序依赖建模不足；多流或多模态方法复杂度高、依赖额外输入。需要一个统一、轻量而强大的时空编码器，用纯RGB也能高效学习细粒度空间与长短期时序信息。

Method: 提出Unified Spatio-Temporal Modeling (USTM)：1) 空间：Swin Transformer作为主干提取层级化、局部-全局结合的空间特征；2) 时间：在各层插入轻量级Temporal Adapter with Positional Embeddings (TAPE)，通过显式时间位置编码与小型适配器模块捕获短期与长期依赖；3) 端到端单流RGB训练与推理，无需关键点/光流/多模态。

Result: 在PHOENIX14、PHOENIX14T、CSL-Daily基准上，对比纯RGB与多模态/多流CSLR方法均达SOTA或具竞争力；证明USTM在准确率与效率间取得良好平衡。

Conclusion: 统一的时空编码器USTM结合Swin与TAPE，可在不依赖多流或辅助模态的情况下，精准建模细粒度空间线索与长短期时序关系，显著提升CSLR性能，展示出鲁棒性与实用性；代码开源以促进复现与扩展。

Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

</details>


### [172] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 提出MCT-UEG框架，通过元学习与平坦极小化训练/测试来生成对多任务均“不可利用”的个人图像；实验显示有效。


<details>
  <summary>Details</summary>
Motivation: 现有“不可利用样本”方法往往只对特定模型或任务有效，难以在真实世界多种视觉任务中普适地阻止对在线个人图像的未授权利用，限制了实用性。

Method: 构建Meta Cross-Task Unexploitable Example Generation（MCT-UEG）框架：以跨任务的元训练/元测试机制优化一个扰动生成器，使其学习在不同下游计算机视觉任务上均能诱导训练失败；关键是引入面向平坦极小值（flat minima）的优化策略，促使生成器参数落在对任务/模型分布更稳健的平坦盆地，从而提升跨任务不可利用性与泛化。

Result: 在多种真实世界视觉任务上进行广泛实验（未给出细节），结果表明所提框架生成的样本比现有方法更“不可利用”、更稳健，具有更好的跨任务泛化。

Conclusion: MCT-UEG通过平坦极小值导向的元学习，显著提升了不可利用样本在多任务场景下的普适性与实用性，能更有效保护个人图像免遭多种下游利用。

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [173] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: 提出RecTok，通过在flow matching中进行语义蒸馏与重建-对齐蒸馏，使高维视觉tokenizer在重建与生成上同时提升，并随维度增大持续受益，达到SOTA gFID-50K。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型中的视觉tokenizer受制于潜空间维度：低维利于生成质量与训练稳定，但牺牲语义与重建；高维虽具表达力却生成表现欠佳。尽管引入VFM可加速收敛与增强语义，现有高维tokenizer仍不如低维。作者希望打破“维度-质量”权衡，使高维tokenizer既语义丰富又具优良生成质量。

Method: 提出RecTok，核心是将“语义”从潜空间转向flow matching的前向流作为训练空间并进行蒸馏：1) Flow Semantic Distillation：把VFM（如视觉基础模型）的语义信息蒸馏到flow matching的前向流轨迹，使流场本身语义丰富；2) Reconstruction–Alignment Distillation：引入masked feature reconstruction损失，将重建特征与VFM表征对齐，增强语义一致性与判别性；整体在高维tokenizer上训练，使扩散Transformer在语义丰富的流空间学习。

Result: 在图像重建、生成质量与判别性能上优于现有方法；在有/无CFG两种设置下的gFID-50K均取得SOTA；并且随着潜在维度增加，性能持续改善，未出现高维退化。

Conclusion: 通过将语义注入flow matching前向流并辅以重建-对齐蒸馏，RecTok打破了高维tokenizer在扩散模型中的性能瓶颈，实现更好重建与生成，同时保留语义丰富的潜在结构，并展现随维度扩展的可扩放性。

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [174] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: 提出MineTheGap：用遗传算法自动挖掘能触发文本到图像模型偏见的提示词，并用新偏见分数量化严重度。


<details>
  <summary>Details</summary>
Motivation: TTI模型在提示含糊处会体现系统性偏见，影响社会公正与用户多样性体验。需要一种自动化方法不仅检测，还能主动发现并放大模型在含糊提示下的偏见案例。

Method: 构建一个由提示词组成的种群，使用遗传算法（选择、变异、交叉）迭代优化，使其最大化“偏见分数”。偏见分数通过比较两种分布获得：1）模型对给定提示生成图像的属性分布；2）LLM对该提示生成的语义变体文本所隐含的属性分布。评分越高表示图像分布与文本预期分布偏离越严重。方法在含已知偏见的数据集上进行验证。

Result: MineTheGap可以自动找到使TTI模型显著表现偏见的提示集，并按严重度排序；在带有已知偏见的数据上，偏见分数与真实偏见情况一致，优于仅对单一提示做静态检测。

Conclusion: 通过基于遗传搜索与新偏见评分的框架，MineTheGap能系统地挖掘并量化TTI模型的偏见，为审计、红队与改进生成多样性提供工具；代码与示例已开源可复现。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [175] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 提出一个轻量少样本植物叶病识别框架：MobileNetV2/V3特征提取+特征融合，Bi-LSTM注意力分类器；在PlantVillage与实地数据上小样本下取得接近SOTA且移动友好。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大规模标注与高算力，不适用于数据稀缺、资源受限的农业场景；需要在小样本、复杂背景下仍能准确诊断叶片病害，且可在移动端部署。

Method: 采用少样本学习框架：1) 使用经领域自适应的MobileNetV2与V3作为双路特征提取器；2) 进行特征融合以形成鲁棒表示；3) 以带注意力机制的Bi-LSTM分类器捕获序列依赖并聚焦关键特征；4) 在实验室与田间数据集（PlantVillage、Dhan Shomadhan）上按1–15-shot设置评估；5) 控制模型规模与GFLOPs以适配移动端。

Result: PlantVillage番茄叶病：1–15-shot均提升，15-shot达98.23±0.33%，接近基于Transductive LSTM+注意力的99.98%基准；在六类PlantVillage任务上以15-shot取得99.72%，超过此前96.0% SOTA。实地Dhan Shomadhan：15-shot达69.28±1.49%，对复杂背景具鲁棒性。模型约40 MB，推理约1.12 GFLOPs。

Conclusion: 该轻量少样本框架在数据稀缺与资源受限环境中实现高效且接近SOTA的病害识别，并具移动端可用性与可扩展性；在实验室与实地场景均表现稳健，适合作为移动化植物病害诊断的基础方案。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [176] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: IMILIA 提出一种端到端、可解释的多实例学习框架，用于在H&E数字切片上预测IBD的微观炎症并量化驱动证据的细胞/组织学特征，取得跨数据集稳定的高AUC（0.83/0.99/0.84）。


<details>
  <summary>Details</summary>
Motivation: IBD治疗目标正从临床/内镜缓解转向组织学缓解，需要可靠、自动、可解释的微观炎症评估方法，以辅助疾病活动度评估与疗效监测。

Method: 构建IMILIA：1) 以MIL为核心的炎症存在性预测模块；2) 可解释模块含两部分：HistoPLUS进行细胞实例检测、分割与多类分类（免疫细胞与上皮细胞等）；EpiSeg进行上皮组织分割。端到端在H&E全幅切片上训练与评估，并基于高分瓦片计算细胞密度等标志物。

Result: 在发现队列进行交叉验证AUC=0.83；在两个外部验证队列AUC分别为0.99与0.84。解释结果显示高分瓦片中淋巴细胞、浆细胞、中性粒细胞、嗜酸性粒细胞密度升高，低分瓦片以正常上皮细胞为主，且各数据集一致。

Conclusion: IMILIA能准确预测IBD组织学炎症并提供生物学一致的可解释证据，具有跨队列泛化性；公开代码和模型可在IBDColEpi上部分复现。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [177] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: 用扩散模型在测试时将目标域图像“逆映射”回训练用的源域，以提升在未知分布转移下的分割/检测/分类性能，无需大规模合成数据；在多数据集上显著提升。


<details>
  <summary>Details</summary>
Motivation: 生成式基础模型具备广泛视觉先验与多样化生成能力，但将其用于训练时的数据增强存在慢、贵、难以覆盖目标域多样性的局限。作者希望避免在训练阶段大量合成目标域数据，而是在测试阶段直接适配目标域到源域，从而提升域泛化且保持下游模型不变。

Method: 提出在测试阶段使用扩散模型对目标图像进行分布“回源”映射：给定源域文本/风格描述，构造扩散去噪（或引导）过程，将目标域外观转化为源域外观，同时保留语义结构。该流程作为可插拔预处理置于任何下游模型之前；并探索多种生成/下游模型组合与集成（ensemble）变体以增强鲁棒性。

Result: 在真实到真实的域泛化场景、未知目标分布下，方法在分割、检测、分类任务上均有稳定收益。报告的相对提升包括：BDD100K-Night 137%，ImageNet-R 68%，DarkZurich 62%。

Conclusion: 测试时的扩散映射能在不改动任务模型与无需大规模合成数据的前提下，有效缓解环境域偏移，跨多任务与多模型取得显著且一致的提升，并可通过集成进一步增强鲁棒性。

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [178] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出PoseAnything：首个可处理人类与非人类骨架的通用姿态引导视频生成框架；引入部件感知的时序一致性模块与“主体/相机运动解耦”的CFG指导策略，并构建5万对非人类姿态-视频数据集XPose；在有效性与泛化上显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有姿态引导视频生成方法只能接受人体姿态，导致对动物、卡通等非人类角色泛化差，且运动一致性与相机运动控制受限。需要一个能处理任意骨架、保持细粒度时序一致，并能独立控制相机运动的通用方法与数据支撑。

Method: 1) 提出PoseAnything框架：输入任意骨架序列（人/非人），进行姿态引导视频生成。2) 部件感知时序一致性模块（Part-aware Temporal Coherence）：将主体划分为多个部件，建立跨帧部件对应关系，对对应部件执行跨注意力以强化细粒度的部件级时序一致性。3) 主体与相机运动解耦的CFG（Decoupled CFG）：在正/负anchor中分别注入主体与相机运动控制信息，实现首次在姿态引导视频中独立控制相机运动。4) 构建XPose数据集：包含5万对非人类姿态-视频对，并提供自动化标注与筛选流程。

Result: 在广泛实验中，相比现有SOTA，在有效性（动作可控、视觉质量、一致性）与泛化能力（对非人类骨架与多样场景）上显著提升，支持任意骨架输入并能稳定控制相机运动。

Conclusion: PoseAnything将姿态引导视频生成从“仅人体”扩展到“任意骨架”，通过部件级时序一致性与主体/相机运动解耦的CFG实现高质量、可控且泛化良好的视频生成，并以XPose数据集提供了必要的数据基础。

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [179] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: 提出T3-Video：在不改动预训练全注意力Transformer主干的情况下，通过优化前向与注意力模式，实现原生4K视频生成10倍以上加速且质量提升。


<details>
  <summary>Details</summary>
Motivation: 原生4K视频生成受全注意力在时空分辨率上二次方增长的计算爆炸限制，现有方法难兼顾效率与质量，需要在不牺牲模型能力的前提下显著降本提效。

Method: 提出T3（Transform Trained Transformer）改造策略：保持预训练全注意力结构不变，通过前向逻辑重构实现注意力模式迁移。具体在T3-Video中引入多尺度权重共享的窗口注意力；结合分层blocking与保持轴一致性的全注意力设计，少量算力与数据即可把预训练模型的注意力从全局改造成高效模式。

Result: 在4K-VBench上，相比现有方法，T3-Video以超过10×的速度提升实现原生4K视频生成，同时VQA提升+4.29，VTC提升+0.08。

Conclusion: T3-Video在不修改架构的前提下，通过注意力模式转换显著降低4K视频生成计算开销并提升质量，为高分辨率视频生成提供高效通用的改造范式。

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [180] [Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation](https://arxiv.org/abs/2512.13495)
*Jiangning Zhang,Junwei Zhu,Zhenye Gan,Donghao Luo,Chuming Lin,Feifan Xu,Xu Peng,Jianlong Hu,Yuansen Liu,Yijia Hong,Weijian Cao,Han Feng,Xu Chen,Chencan Fu,Keke He,Xiaobin Hu,Chengjie Wang*

Main category: cs.CV

TL;DR: Soul 是一个多模态驱动的长期高保真数字人动画框架：从单张人像+文本+音频生成语义连贯视频，精准对口型、表情生动、身份稳定，并在速度与质量间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有音频/文本驱动的人像视频生成存在口型不准、身份漂移、长时一致性差、评测不统一与数据稀缺等问题。需要更大规模标注数据、标准基准与可高效推理的模型来提升实用性。

Method: 1) 构建大规模标注数据集 Soul-1M（含人像、半身、全身、多人物）与评测基准 Soul-Bench。2) 以 Wan2.2-5B 为骨干，加入音频注入层，配合多种训练策略与“阈值感知的码本替换”以提升长时一致性。3) 采用 step/CFG 蒸馏与轻量 VAE 优化推理，获得 11.4× 加速、几乎无质量损失。

Result: 在视频质量、文视频对齐、身份保持与唇同步上显著优于现有开源与商用模型；可稳定生成长视频，并在多场景下鲁棒。

Conclusion: Soul 通过大规模数据、专门的音频融合与一致性机制，以及蒸馏与轻量化推理，实现在真实应用（虚拟主播、影视制作等）中的高保真、长时稳定与高效生成。

Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

</details>


### [181] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro 是面向原生音视频联合生成的基础模型，采用双分支DiT与跨模态联合模块及多阶段数据管线，实现高质量、强同步的多语言对口型与叙事视频生成，并通过SFT、RLHF与推理加速在实用场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: 尽管视频生成进展迅速，但原生的音视频联合生成仍在同步精度、画质与可控性（如口型、镜头语言）方面存在不足，且推理成本与延迟较高，限制了专业内容创作的落地。

Method: 提出双分支Diffusion Transformer（音频分支与视频分支），在其中加入跨模态联合模块以对齐与融合特征；构建多阶段数据处理与训练管线；进行SFT以对高质量数据对齐风格与内容；使用RLHF配合多维奖励（同步、画质、叙事等）做偏好优化；引入推理加速框架实现10倍以上加速。

Result: 模型实现了卓越的音视频同步（多语种与方言口型精准）、更高的生成质量、动态电影级镜头控制与更强叙事连贯性；推理速度显著提升（>10X）。

Conclusion: Seedance 1.5 pro 作为原生音视频联合生成引擎，在同步、质量、可控性与效率上取得平衡，适用于专业内容创作，并已在火山引擎平台开放使用。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [182] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出TARA，一种无需视频数据即可将多模态大模型适配为时间感知的视频-文本嵌入，用于检索；在“手性动作”基准上优于现有方法，并在标准基准上表现强劲，具备否定理解与动词/副词理解的额外优势，零样本效果SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本检索模型对时间方向与顺序的敏感性不足，难以分辨时间相反的动作（如戴上/摘下），导致检索混淆；同时，构建时间感知模型常需大量视频数据，成本高、难泛化。作者希望：1）评估并提升模型的“时间意识”；2）以低成本方式（不依赖视频数据）将MLLM适配为时间感知嵌入；3）建立更严格的评测基准。

Method: 提出TARA（Time Aware Retrieval Adaptation）：一种简单高效的配方，将现成多模态大模型（MLLM）适配为时间感知的视频-文本嵌入模型且完全不使用视频数据。核心包括：通过文本侧构造与时间方向相关的对比/约束信号（如手性动作、否定样例、动词/副词敏感性）来调整嵌入空间，使其对动作发生顺序与方向更敏感；并在检索训练/适配中引入与时间相关的负样本（硬负样本）来强化区分能力。

Result: 在新提出的手性（时间相反）动作基准上，TARA优于所有现有视频-文本模型；在常规检索基准上亦取得强竞争力。额外发现：1）在NegBench上表现出“否定感知”能力；2）在视频中的动词与副词理解上达到SOTA。整体实现了强大、多才且时间敏感的零样本检索性能。

Conclusion: TARA以无需视频数据的适配方式，使MLLM获得时间感知的视频-文本嵌入能力，在手性与标准基准上均表现优异，并带来否定理解与细粒度语义（动词/副词）理解的提升，成为零样本视频检索中的SOTA与通用方案。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [183] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 提出Pancakes框架：在未知域单张生物医学图像上，自动产出多种合理分割协议的多标签分割图，并在跨图像保持语义一致；在7个留出数据集上优于现有基础模型。


<details>
  <summary>Details</summary>
Motivation: 同一医学影像在不同任务中需要按不同协议分割（组织类型、血管供血区、粗/细解剖、病灶等）。现有方法要么只支持单一协议，要么需大量人工提示，难以自动为新域图像生成多套合理且跨图像一致的分割。

Method: 提出Pancakes——一种新问题设定与框架：给定来自未见域的新图像，自动生成多种“可行分割协议”的多标签分割图；并引入机制在相关图像间保持语义一致性。将其与多种基础模型对比评估。

Result: 在7个留出数据集上，Pancakes能产生多种合理的全图分割，并且在语义一致性与质量上显著优于现有基础模型。

Conclusion: Pancakes实现了无需人工提示的多协议自动分割，跨图像保持一致，拓展了基础模型未覆盖的新任务设定，并在多数据集上展现显著优势。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [184] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

TL;DR: 提出首个面向双人协作动作的异常检测任务H2IAD，并给出IADNet模型，利用时间注意力共享与距离关系编码，结合正态化流进行评分，在多基准上优于单人AD方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测多聚焦单人行为，难以捕获双人互动中复杂、非对称的协作动态与社交线索，导致对互动异常（如冲突、异常配合）的检出不准，亟需面向人-人互动的新任务与模型。

Method: 定义H2IAD任务；提出IADNet：1) 时间注意力共享模块TASM，将两人的编码运动嵌入在时间维共享，使协作相关性同步、对齐互动节律；2) 距离关系编码模块DREM，编码两人间的空间/距离配置以体现社交线索；3) 采用normalizing flow对序列生成基于密度的异常分数。

Result: 在多个双人三维人体动作基准上，IADNet在H2IAD场景显著优于现有单人AD基线，取得最佳或有竞争力的性能。

Conclusion: 建构了人-人互动异常检测新范式，并通过IADNet有效建模时间协作与空间关系，验证了相较单人模型的优势；为后续互动理解与群体行为安全监测提供基础。

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


### [185] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: 提出MMhops基准与MMhops-R1框架，用于评测与提升多模态多跳推理；通过mRAG与强化学习实现动态规划与检索，显著优于强基线并具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多为单步推理，缺乏能驱动与评测多模态多跳推理的复杂基准，限制了模型应对真实复杂任务的能力。

Method: 1) 构建大型多模态多跳数据集MMhops，涵盖Bridging与Comparison两类任务，需要外部知识与跨模态信息整合。2) 提出MMhops-R1：多模态检索增强生成（mRAG）框架，使用强化学习训练模型自主规划推理路径、生成面向性的检索查询、并分层整合多源信息完成动态推理。

Result: 在MMhops上显著超越强基线；在需要固定跳数的任务上也有良好表现，显示动态规划方法的鲁棒性与泛化能力。

Conclusion: 发布了一个更具挑战性的多模态多跳推理基准与强力基线模型；动态规划与跨模态知识整合对复杂推理至关重要；代码、数据与权重将开放，促进后续研究。

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [186] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: LiMo 是一种基于扩散模型的时空光照估计方法，可同时实现高频细节复原与准确照度（HDRI）重建，并取得当前最优的空间控制与预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有光照估计方法往往在两难之间取舍：要么能给出物理上可用的照度/强度（HDR）但缺少高频细节与时空一致性，要么能合成逼真的外观却难以准确量化光照强度/方向与空间变化。尤其在动态场景与相机移动时，仅用深度或单帧条件难以进行稳定、可控的时空光照重建。

Method: 1) 在输入场景的目标3D位置上，合成一组具有不同曝光的镜面球与漫反射球作为光探针表征；2) 利用扩散先验，对大规模室内/室外数据及对应的时空光探针进行微调，学习从图像与几何条件到光探针外观的映射；3) 提出新的几何条件（不仅深度，还编码目标3D位置与场景的相对关系）以提升空间定向与控制；4) 将多曝光的漫反与镜面预测通过可微渲染融合为单一HDRI环境图，实现强度可量化的光照重建。

Result: 实验表明：仅用深度作为条件不足以实现准确的空间控制；加入相对几何条件后，时空一致性与定位精度明显提升。与现有方法相比，LiMo 在高频细节、照度准确性（HDR 强度）、以及跨时空位置的控制上均达到了SOTA。

Conclusion: 通过利用扩散模型先验、设计针对性的几何条件与多曝光镜/漫球探针表示，并以可微渲染将其汇聚为HDRI，LiMo 实现了兼顾真实性与物理准确性的时空光照估计，为动态场景的可控光照重建树立了新的基准。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [187] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

TL;DR: 提出DA-SSL适配器，将病理基础模型(PFM)的特征无监督对齐到TURBT域，在不微调PFM的前提下提升MIL在膀胱肿瘤预测中的表现，取得较强跨中心效果。


<details>
  <summary>Details</summary>
Motivation: PFM在少见癌种或含罕见伪影的标本上受域偏移影响，性能下降；TURBT切除标本常有碎片化组织与电灼伪影，且在公开PFM预训练集中代表性不足。临床上亟需利用TURBT形态学特征预测MIBC患者对NAC的疗效，以指导个体化治疗。

Method: 提出一种轻量的域自适应自监督适配器(DA-SSL)：不对PFM本体微调，而是在其特征之上训练一个自监督的对齐模块，使PFM提取的表示更贴合TURBT域；与MIL框架结合用于治疗反应预测。进行多中心五折交叉验证与外部测试，并采用多数投票评估。

Result: 五折交叉验证AUC为0.77±0.04；外部测试中总体准确率0.84，敏感度0.71，特异度0.91（多数投票）。

Conclusion: 在PFM+MIL管线中加入轻量自监督域适配可显著缓解域偏移，提升在TURBT等临床挑战性任务上的预测性能，具有实际可用性与跨中心泛化潜力；代码已开源。

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [188] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie 2 是一个分三阶段训练的自回归视频世界模型，强化可控性、长时视觉质量与时间一致性，并配套 LongVGenBench 基准，达成最长 5 分钟连续生成与 SOTA 表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成/世界模型在长时可控性、时间一致性与画质维护间难以兼顾；训练与推理分布不匹配导致长序列画质衰减；缺少统一、长时、高分辨率的评测基准。

Method: 提出 LongVie 2：端到端自回归框架，三阶段训练：1) 多模态引导，将稠密与稀疏控制信号融合，提供隐式世界级监督以提升可控性；2) 面向输入帧的退化感知训练，缩小训练-长时推理分布差，维持高视觉质量；3) 历史上下文引导，对齐相邻片段的上下文以保证时间一致性。同时构建 LongVGenBench：含 100 个一分钟高分辨率视频，覆盖真实与合成场景，用于评测长时可控性、时序一致性与画质。

Result: 在长程可控性、时序一致性与视觉保真度上达到 SOTA；支持最长 5 分钟的连续视频生成；在 LongVGenBench 上取得优异成绩。

Conclusion: 分阶段训练与多重引导机制有效解决长时视频世界建模中的可控性、画质与一致性三难题；配套基准推动统一视频世界模型评测，LongVie 2 向通用时空智能迈进一步。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [189] [DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis](https://arxiv.org/abs/2512.13608)
*Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge*

Main category: cs.CV

TL;DR: 提出并评测首个用于数字乳腺断层摄影（DBT）的基础模型DBT-DINO，基于大规模DBT切片自监督预训练，在密度分类与5年癌症风险预测上优于通用基线，但在一般性病灶检测上不及ImageNet预训练的DINOv2；对癌性病灶检测略占优。


<details>
  <summary>Details</summary>
Motivation: 三维医学影像领域缺乏专属的基础模型，DBT在乳腺筛查中广泛应用却尚无相应foundation model。需要检验面向DBT的领域自监督预训练是否能在多下游临床任务上带来性能提升。

Method: 使用DINOv2自监督框架，在27,990位患者的487,975个DBT体积（约2500万张2D切片）上进行预训练。下游任务包括：1）乳腺密度分类（5,000例）；2）5年乳腺癌发生风险预测（106,417例）；3）病灶检测（393个有标注体积）。与MetaAI DINOv2与DenseNet-121进行比较。

Result: 密度分类：DBT-DINO准确率0.79，显著优于DINOv2的0.73与DenseNet-121的0.74。5年风险预测：AUROC 0.78，较DINOv2的0.76，差异不显著。病灶检测：总体平均敏感度DINOv2=0.67优于DBT-DINO=0.62（差异不显著），但对癌性病灶DBT-DINO检出率78.8%略高于DINOv2的77.3%。

Conclusion: 构建了首个DBT基础模型并在多任务上验证。领域特定预训练对表征级任务（密度、风险）带来稳定收益，但在定位型任务（普适病灶检测）收益有限，甚至不及ImageNet预训练基线，提示检测类任务需要进一步的方法改进（如更强的检测头、3D时空建模或多尺度/弱监督策略）。

Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.

</details>


### [190] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出Do-Undo任务与基准，评测多模态模型对现实动作驱动的可逆物理变化的理解与生成能力；构建可逆动作视频数据集与一致性训练策略，发现现有模型在物理可逆性上表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言工作多停留在对象级编辑，缺少对“执行一个真实动作→产生物理变化→能否复原”的因果与可逆性推理与生成评测；为推动具身智能、机器人与物理感知生成模型的发展，需要一个直观、标准化的测试台。

Method: 1) 定义Do-Undo任务：给定场景与动作，预测动作后的场景变化，并能准确反向恢复；2) 策划大规模真实世界视频中的可逆动作数据集；3) 训练时引入一致性约束以强化动作落地与可逆对齐（action grounding consistency）。

Result: 实验表明现有多模态/生成模型在物理可逆性与因果一致性上表现欠佳，难以稳定模拟动作结果并复原。

Conclusion: Do-Undo作为直观基准可系统评估并推动多模态系统的物理推理、具身与机器人应用，以及物理感知生成建模的发展。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [191] [SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning](https://arxiv.org/abs/2512.13635)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Chongyu Qu,Juming Xiong,Siqi Lu,Zhengyi Lu,Yanfan Zhu,Marilyn Lionts,Yuechen Yang,Yalin Zheng,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出SCR2-ST框架：用单细胞先验指导空间转录组的主动采样与表达预测；在低预算下提升采样效率与预测精度，达SOTA。


<details>
  <summary>Details</summary>
Motivation: ST昂贵且传统固定网格采样在形态相似或信息贫乏区域产生冗余，导致有效数据稀缺，限制下游方法；相对地，单细胞测序丰富、成熟，可作为外部先验辅助高效数据获取与学习，需要一个能把单细胞知识注入ST采集与预测的统一方法。

Method: 提出SCR2-ST，由两部分组成：1) SCRL（单细胞引导的强化学习主动采样）：将单细胞基础模型嵌入与空间密度信息融合，构造生物学意义的奖励函数，在预算约束下选择信息量高的组织区域进行测序；2) SCR2Net（混合回归-检索预测网络）：在主动采样数据上训练，结合回归建模与检索增强推理；通过多数细胞类型过滤抑制噪声匹配，并把检索到的表达谱作为软标签进行辅助监督。

Result: 在三个公开ST数据集上评估，相比现有方法在采样效率与表达预测精度上均达SOTA，尤其在低预算情境下优势更明显。

Conclusion: 利用单细胞先验统一优化ST数据获取与建模，可在资源受限时显著提升信息获取效率与预测准确度；SCR2-ST为经济高效的空间转录组研究提供了可复用策略与开源实现。

Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST

</details>


### [192] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: 提出MindDrive：用一个LLM加两套LoRA分头负责“决策推理”和“动作映射”，把连续控制问题转化为离散语言决策的在线强化学习，从而提升探索效率与闭环驾驶表现（Bench2Drive：DS 78.04、SR 55.09）。


<details>
  <summary>Details</summary>
Motivation: 现有VLA多基于模仿学习，易受分布偏移与因果混淆影响；而直接在连续动作空间做在线RL探索效率低，限制了VLA在自动驾驶中的强化学习应用。

Method: 构建一个共享基座LLM，加载两套LoRA：Decision Expert进行场景理解与离散语言级驾驶决策；Action Expert将语言决策动态映射为可行轨迹。利用轨迹级奖励回传至语言推理空间，进行基于有限离散决策集合的在线RL更新，避免直接在连续动作上探索。

Result: 在Bench2Drive闭环评测中取得Driving Score 78.04、Success Rate 55.09，表现强劲。

Conclusion: 通过把连续控制问题语言化、离散化，MindDrive实现高效在线RL与更人性化、稳健的决策执行，首次验证了在线强化学习在自动驾驶VLA中的有效性。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [193] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 提出一个来自高质量动画电影的多模态4D新视角合成数据集，涵盖密集/稀疏/单目三类基准设置，适配动态场景训练与评测。


<details>
  <summary>Details</summary>
Motivation: 现有NVS/4D重建数据集在真实感、动态复杂性与多模态标注上不足，限制了方法在复杂光照、精细纹理和运动中的泛化与评估。

Method: 从高保真动画影片中导出多视角动态场景帧，提供RGB、深度、法向、分割、光流等多模态标注，并将数据组织为三种实验场景：密集多相机、稀疏相机与单目视频，以支持不同数据稀疏度下的训练与基准测试。

Result: 得到一个具有高视觉逼真度和精细细节的动态多模态数据集，覆盖多样场景与运动，能用于训练和客观比较前沿4D重建与新视角生成模型。

Conclusion: 该数据集以其视觉丰富度、精确标注与多样基准配置，为推动新视角合成与3D/4D视觉研究提供了独特且实用的资源。

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [194] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 提出Grab-3D：利用消失点等显式3D几何表征，检测扩散模型生成视频中的几何时序不一致；在静态场景合成数据上训练并评测，跨生成器泛化强。


<details>
  <summary>Details</summary>
Motivation: 扩散视频生成愈发逼真，现有检测器对3D几何模式（尤其时序一致性）利用不足，难以稳健泛化；需要一种能显式建模3D几何并随时间一致性的检测方法与可靠评测基准。

Method: 以消失点作为显式3D几何特征，构建静态场景的AI生成视频数据集，便于稳定抽取几何轨迹；提出Grab-3D几何感知Transformer：引入几何位置编码、时序-几何注意力、以及基于EMA的几何分类头，将3D几何信息注入时序建模用于真假判别。

Result: 在多种数据与生成器上，Grab-3D显著优于现有最先进方法，并在未见过的生成器上保持强健跨域泛化能力。

Conclusion: 显式3D几何（消失点）与时序一致性建模对AI生成视频检测有效；几何感知Transformer框架在鲁棒性与泛化上具优势，可为后续几何驱动的检测研究提供方向。

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [195] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

TL;DR: 提出AgentIAD，一个具备“放大观察”和“对比检索”工具的代理式IAD框架，通过多阶段检查与RL优化，在MMAD上达到了97.62%新SOTA，并输出可解释的检测过程。


<details>
  <summary>Details</summary>
Motivation: 工业异常往往细微且局部，且正常样本稀缺。单次推理的VLM易忽略小缺陷，也缺少与标准正常模式对比的显式机制，需要一种能分步观察、局部放大、并在不确定时依赖正常参考的方案。

Method: 设计AgentIAD代理：1) Perceptive Zoomer进行局部细粒度放大分析；2) Comparative Retriever在证据模糊时检索正常样本进行对比；3) 从MMAD构建感知与对比的结构化轨迹，先监督微调，再强化学习；4) 两部分奖励：感知奖励（分类准确、空间对齐、类型正确）与行为奖励（鼓励高效工具使用），引导逐步观察-放大-核验的行为策略。

Result: 在MMAD数据集上实现97.62%分类准确率，超过以往基于多模态大模型的方法，并产生透明、可解释的检测轨迹。

Conclusion: 多阶段、工具驱动的代理框架能有效弥补VLM对局部细微异常与参照不足的问题；通过SFT+RL与专门奖励设计，模型在IAD上取得SOTA并具备可解释性与高效行为策略。

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [196] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: 提出“交互智能”数字人范式，并给出端到端系统Mio（思考-说话-面部-身体-渲染五模块），在新基准上全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数字人多停留在表层模仿，缺乏一致的人格表达、对话自适应与持续进化能力，缺少统一架构与系统化评测。

Method: 构建Mio统一端到端框架，集成五个专门模块：Thinker（认知推理/规划）、Talker（人格一致的语言生成与多模态对齐）、Face Animator、Body Animator（实时表情与肢体驱动）、Renderer（多模态实时呈现）；并提出一个用于“交互智能”多维评测的新基准。

Result: 在所提出的新基准与多项实验中，Mio在各维度指标上均优于SOTA，展现更流畅、一致与自适应的交互表现。

Conclusion: 数字人从“表面拟态”迈向“智能交互”：统一认知—具身架构+标准化评测推动了可人格一致、可自适应与可进化的交互智能发展。

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [197] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA提出一个简洁的统一Transformer框架，用联合自注意力直接在视频与音频token间交互，并配合基于人脸关键点的口部区域损失，实现更好的唇形-语音同步与整体视听生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法多只能生成环境声，难以生成与唇形同步的人声；统一视听生成常依赖显式融合或对齐模块，导致结构复杂、破坏Transformer简洁性与可扩展性。

Method: 在每层Transformer中对视频与音频token执行联合自注意力（joint self-attention），无需额外对齐/融合模块即可实现跨模态交互；引入口部区域损失：通过人脸关键点检测定位口部区域，强调该区域的监督信号以提升唇形-语音对齐，同时保持架构简洁。

Result: 在多项基准上，JoVA在唇形同步准确度、语音质量以及整体视听生成保真度上优于或可比于当前统一与声驱动SOTA方法。

Conclusion: JoVA以简洁的联合自注意力与口部区域损失，实现高质量、同步精准的视听联合生成，兼顾性能与架构优雅性，具有通用与可扩展潜力。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [198] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Steer3D 提出一种前馈式方法，让图像到3D生成模型具备文本可控的编辑能力，数据由自动引擎规模化合成，并以流匹配训练+偏好优化(DPO)两阶段训练；相比现有方法，更忠实执行指令、保持原资产一致性且快2.4×–28.5×。


<details>
  <summary>Details</summary>
Motivation: 现有image-to-3D生成虽强，但缺乏易用的可编辑性，尤其是用自然语言对已生成3D资产进行细粒度、快速、可控的编辑，这限制了在设计、AR/VR和机器人中的实际落地。

Method: 受ControlNet启发，将“可控分支”适配到image-to-3D生成框架，实现一次前向推理即可按文本指令引导编辑；构建可扩展自动数据生成引擎；采用两阶段训练：1) 基于flow matching进行主干学习与对齐；2) 以Direct Preference Optimization对文本指令遵循度进行偏好优化。

Result: 在与同类方法对比中，Steer3D更好地遵循语言指令，同时更好地保留原始3D资产的一致性；推理速度显著提升，快2.4×–28.5×；仅用约10万规模数据即可为预训练的image-to-3D模型新增文本操控模态。

Conclusion: Steer3D验证了以较小数据量即可为既有image-to-3D生成器引入高效的文本可编辑能力，兼顾编辑忠实度、资产一致性与速度，为真实应用中的3D资产编辑提供可行路径。

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [199] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: LASER 是一种无需再训练的框架，把离线重建模型转为可流式处理：通过跨时间窗口对齐预测，并引入分层尺度对齐解决单目尺度歧义导致的层深错配问题，从而在低显存下实现高质量位姿与点图重建的实时化。


<details>
  <summary>Details</summary>
Motivation: 现有最强的前馈3D重建模型（如 VGGT、π^3）重建质量高，但注意力/特征交互的二次内存开销使其无法处理长时流式视频；而现有流式方案依赖学习型记忆或因果注意力，需大量再训练，且难以充分利用离线模型的几何先验。需要一种不再训练、能把离线模型能力迁移到流式场景的方法。

Method: 提出 LASER：训练自由，将离线模型窗口化并在相邻时间窗口间对齐预测。观察到仅用 Sim(3) 全局对齐会失败，因为单目尺度歧义使不同“景深层”的相对尺度在窗口间不一致，产生层深错配。为此，进行“分层尺度对齐”：把深度预测分割成若干离散层，计算每层的尺度因子，并在相邻窗口与时间戳间传播这些因子，实现逐层的尺度一致性，进而对齐相机位姿与点图。

Result: 在相机位姿估计与点地图重建上达成SOTA；在RTX A6000上仅用约6GB峰值显存即可14 FPS运行，支持公里级流式视频实际部署。

Conclusion: 通过分层尺度对齐，LASER 在不再训练的前提下将高质量离线重建模型转换为高效的流式系统，兼顾精度与资源效率，适用于长时、大规模视频重建场景。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [200] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: 他们把一个预训练的3D实例生成器“重编程”为场景级学习器，不靠数据集标注而用模型自身的空间先验做监督，得到一个可泛化的、前馈的交互式3D场景生成方法，能在新布局与新物体组合上进行合理的空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有交互式3D场景生成依赖有限的场景数据集与固定的“标准化空间”设定，导致对新布局、新组合泛化差；作者想利用大型预训练3D实例生成器中蕴含的可迁移空间知识，替代数据集受限的监督，提升泛化与推理能力。

Method: 对预训练的3D实例生成器进行“重编程”，让其在视图中心（view-centric）场景表示中充当场景级学习器：用实例模型本身提供的空间信号（几何一致性、支撑、邻近、对称等）作为监督，端到端前馈地学习物体间关系与布局，而不依赖传统的规范空间或大量场景标注。

Result: 即便训练场景由随机拼接的物体组成，模型仍能学会有效的空间关系（邻近、支撑、对称），并在未见过的布局与新物体组合上生成合理场景；定量与定性评估均显示该3D实例生成器隐式地具备空间学习与推理能力。

Conclusion: 预训练3D实例生成器可作为隐式空间先验与推理器，通过视图中心、前馈的重编程框架实现更强的场景生成泛化，提示迈向用于交互式3D理解与生成的“基础模型”。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [201] [Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684)
*Daniel Zoran,Nikhil Parthasarathy,Yi Yang,Drew A Hudson,Joao Carreira,Andrew Zisserman*

Main category: cs.CV

TL;DR: RVM是一种基于Transformer的循环式视频掩码自编码器，通过时间维度递归聚合密集图像特征，以简单像素重建目标进行预训练，兼具高效与通用，在视频与图像下游任务上均表现强劲，且在小模型下具显著参数效率和长时稳定传播能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频自监督方法多依赖重型时空注意力或蒸馏，参数/计算成本高且长时建模受限；同时很少兼顾视频级语义与密集几何理解。作者希望用更高效的循环结构，在不依赖复杂目标或蒸馏的情况下，学习可迁移的通用视频表征，并能线性成本处理长时序。

Method: 提出Recurrent Video Masked-Autoencoders：以Transformer为核心的循环网络，在时间上递归汇聚帧级密集特征；采用非对称掩码预测，仅以标准像素重建作为训练目标；通过循环机制实现特征随时间的稳定传播，计算/内存随序列长度线性增长。

Result: 在动作识别、点/目标跟踪等视频级任务上与VideoMAE、V-JEPA等SOTA相当；在测试几何与密集空间理解的任务上优于图像模型如DINOv2；在小模型条件下无需蒸馏即可达到高性能，相比竞品视频MAE实现最高约30倍参数效率；长时间跨度上保持稳定特征传播且计算线性扩展；可视化显示学到丰富的语义、结构和运动表征。

Conclusion: RVM以简单像素重建目标与循环Transformer实现高效而通用的视频表征学习，兼顾视频与图像类任务，具备显著的参数效率与长时序可扩展性，克服传统时空注意力架构在长时间建模与资源消耗方面的局限。

Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

</details>


### [202] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: 论文提出VTP，一种联合对比、自监督与重建损失的视觉分词器预训练框架，旨在获得更具高层语义的潜空间，从而显著改善生成模型的可扩展性与生成质量。实验证明：理解驱动生成，且VTP在算力、参数与数据扩大时能有效提升生成性能，优于传统仅重建的自动编码器。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器（如VAE）采用重建驱动的训练，潜空间偏向低层像素信息。结果是：更好的重建精度并不带来更高的生成质量，导致在分词器预训练上加大算力与数据投入回报很差（预训练可扩展性问题）。为解决这一根本缺陷，需要让潜空间浓缩高层语义以更好服务生成任务。

Method: 提出VTP框架：在视觉分词器预训练中联合优化三类目标——图文对比学习（促进对高层语义与跨模态对齐的表征）、自监督学习（丰富语义抽象与不变性）、重建损失（保留必要的细节与稳定训练）。通过大规模实验研究其可扩展性与对生成性能的影响。

Result: 大规模实验证明两点：1）“理解”能力（高层语义表征）是生成质量的关键驱动力；2）VTP具备更好的扩展曲线，随着算力、参数、数据增加，生成性能持续提升。具体指标：ImageNet零样本准确率78.2、rFID 0.36；相较先进蒸馏方法，生成训练收敛速度提升4.1倍；在不改动标准DiT训练设置下，仅增加VTP预训练FLOPs即可带来下游生成FID 65.8%的改进；而传统自动编码器在1/10 FLOPs处即早早停滞。

Conclusion: 仅靠重建的视觉分词器难以支撑高质量生成与良好扩展性。VTP通过融合对比、自监督与重建，实现具备高层语义的潜空间，显著提升生成质量与扩展效率，提供了将算力投入到分词器预训练即可带来下游生成显著收益的路径。模型与代码已开源。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [203] [LitePT: Lighter Yet Stronger Point Transformer](https://arxiv.org/abs/2512.13689)
*Yuanwen Yue,Damien Robert,Jianyuan Wang,Sunghwan Hong,Jan Dirk Wegner,Christian Rupprecht,Konrad Schindler*

Main category: cs.CV

TL;DR: 提出LitePT：前层用卷积提低层几何，后层用注意力捕捉高层语义，并引入训练免调的3D位置编码PointROPE；在保持或提升性能的同时，较Point Transformer V3参数降3.6倍、速度提升2倍、显存减半。


<details>
  <summary>Details</summary>
Motivation: 现有点云网络常混合卷积与注意力，但如何高效组合尚不明确；注意力在高分辨率早期层计算昂贵且收益有限，而在低分辨率深层更能建模全局语义，需系统化设计准则与更轻量的骨干。

Method: 实证分析不同计算模块在不同深度/分辨率下的作用：高分辨率早期层用卷积提取局部几何，深层低分辨率阶段切换为注意力；为弥补减少卷积带来的空间布局信息丢失，引入无需训练的3D位置编码PointROPE；据此构建新的轻量级主干LitePT。

Result: 在多任务与数据集上，LitePT以更少参数（3.6×更少）、更快推理（2×）、更低内存（2×）达到与Point Transformer V3相当或更优的精度。

Conclusion: 基于“前卷积、后注意力”的设计原则与PointROPE位置编码，可在保持或提升性能的同时显著降低3D点云模型的计算与内存成本，提供实用的轻量化骨干。

Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.

</details>


### [204] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: 提出DiffusionBrowser：在扩散去噪任意中间步生成高速可视化预览并进行交互控制的轻量解码框架。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型生成慢、结果不精确且过程不透明，用户在长时间等待中无法获知或干预中间状态；需要一种既能实时预览又能在中间过程施加控制的手段，并借此理解模型在时序上如何逐步构建场景与运动。

Method: 设计与底层生成器无关（model-agnostic）的轻量级解码器，连接到扩散流程中的任意时间步或Transformer块，输出多模态中间预览（RGB与场景内在属性，如深度/法线等）。解码器以>4×实时速度生成（4秒视频<1秒预览），并通过两种交互机制——随机性再注入（stochasticity reinjection）与模态引导（modal steering）——在中间噪声阶段引导采样轨迹。利用这些解码器对去噪过程进行系统探针分析。

Result: 实现了与最终视频外观和运动一致的中途预览；达成显著的速度优势（超实时）；可在中间步骤对生成进行有效控制（如改变运动/外观走向）；通过探测分析展示了场景、物体与细节在去噪中的逐步组装规律。

Conclusion: DiffusionBrowser让视频扩散的生成过程可视、可交互、可分析：在不中断主模型的情况下提供高速多模态预览与中程控制，并作为研究工具揭示黑箱去噪中信息逐层/逐步的形成机制。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>
