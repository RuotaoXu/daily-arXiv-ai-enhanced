<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 210]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Replication Study: Federated Text-Driven Prompt Generation for Vision-Language Models](https://arxiv.org/abs/2602.18439)
*Suraj Prasad,Anubha Pant*

Main category: cs.CV

TL;DR: 论文复现实验证实：在联邦学习中，基于文本驱动的提示生成（FedTPG）能在不共享私有数据的前提下，较静态提示学习更好地泛化到未见类别；复现结果与原文几乎一致。


<details>
  <summary>Details</summary>
Motivation: CLIP 等视觉-语言模型具备零样本能力，但在联邦学习场景下对未见类别的泛化受限；需要一种既保护数据隐私又能跨类别泛化的提示学习方法。

Method: 复现 FedTPG：使用文本驱动的提示生成网络，根据类别名称动态生成提示；在六个视觉数据集（Caltech101、Oxford Flowers、FGVC Aircraft、Oxford Pets、Food-101、DTD）上评估预训练模型；比较见类（base）与未见类（new）的精度，并与原论文报告对齐。

Result: 复现精度与原文相差≤0.2%；见类平均74.58%，未见类76.00%，未见类相对见类提升+1.43 个百分点，表明更强跨类泛化能力。

Conclusion: 文本驱动的提示生成优于静态提示学习，尤其在未见类别上的泛化；联邦训练的提示生成器在多域保持高性能且无需共享私有数据；FedTPG 方法具有鲁棒性与可复现性。

Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot capabilities, yet their adaptation to federated learning scenarios presents significant challenges, particularly regarding generalization to unseen classes. The original FedTPG paper \cite{Qiu2024} addresses this limitation by introducing a text driven prompt generation network that dynamically creates prompts conditioned on class names, enabling better cross-class generalization in federated settings. In this work, we present a faithful replication study of FedTPG, evaluating the pre-trained model on six diverse vision datasets: Caltech101, Oxford Flowers, FGVC Aircraft, Oxford Pets, Food-101, and DTD. Our evaluation achieves results within 0.2\% of the original paper's reported accuracies, with an average accuracy of 74.58\% on seen (base) classes and 76.00\% on unseen (new) classes, demonstrating a +1.43 percentage point improvement in generalization. These results validate the original paper's core claims: (1) text-driven prompt generation enables superior generalization to unseen classes compared to static prompt learning methods, and (2) federated training of prompt generators maintains high performance across diverse visual domains without sharing private data. Our successful replication confirms the robustness and reproducibility of the FedTPG approach.

</details>


### [2] [A Patient-Specific Digital Twin for Adaptive Radiotherapy of Non-Small Cell Lung Cancer](https://arxiv.org/abs/2602.18496)
*Anvi Sud,Jialu Huang,Gregory R. Hart,Keshav Saxena,John Kim,Lauren Tressel,Jun Deng*

Main category: cs.CV

TL;DR: 提出COMPASS：利用每分次（per‑fraction）PET/CT、剂量组学/影像组学与BED动力学，构建正常组织“数字孪生”时间序列模型，用于放疗中毒性的早期预警与自适应治疗决策。


<details>
  <summary>Details</summary>
Motivation: 现行放疗多采用静态、群体级NTCP模型，忽视了个体在疗程中持续产生的高频影像与剂量数据所反映的动态生物学轨迹，难以及时预警正常组织毒性并实施个体化自适应治疗。

Method: 构建COMPASS数字孪生架构：汇聚每分次的PET、CT、dosiomics、radiomics与累积BED动力学；以GRU自编码器学习器官特异的潜在时间轨迹，并用逻辑回归对潜变量进行分类，预测是否发生CTCAE≥1的毒性。数据来自8例BGRT治疗的NSCLC患者，共99个器官‑分次观测，覆盖脊髓、心脏、食管三类器官的24条轨迹。

Result: 尽管样本量小，但高时序分辨率使得模型捕获到个体化剂量‑反应动态；在临床毒性出现前的多个分次即出现风险评分上升的“早期预警窗口”；BED驱动的高维表征揭示出在毒性发生前的空间剂量纹理特征，而这些特征在传统体积基剂量学中被平均化而难以观察。

Conclusion: COMPASS作为放疗安全的个体化评估与自适应平台的概念验证，证明了通过持续更新的数字孪生追踪患者正常组织生物学响应，可实现毒性的早期预警并为自适应放疗提供依据。

Abstract: Radiotherapy continues to become more precise and data dense, with current treatment regimens generating high frequency imaging and dosimetry streams ideally suited for AI driven temporal modeling to characterize how normal tissues evolve with time. Each fraction in biologically guided radiotherapy(BGRT) treated non small cell lung cancer (NSCLC) patients records new metabolic, anatomical, and dose information. However, clinical decision making is largely informed by static, population based NTCP models which overlook the dynamic, unique biological trajectories encoded in sequential data. We developed COMPASS (Comprehensive Personalized Assessment System) for safe radiotherapy, functioning as a temporal digital twin architecture utilizing per fraction PET, CT, dosiomics, radiomics, and cumulative biologically equivalent dose (BED) kinetics to model normal tissue biology as a dynamic time series process. A GRU autoencoder was employed to learn organ specific latent trajectories, which were classified via logistic regression to predict eventual CTCAE grade 1 or higher toxicity. Eight NSCLC patients undergoing BGRT contributed to the 99 organ fraction observations covering 24 organ trajectories (spinal cord, heart, and esophagus). Despite the small cohort, intensive temporal phenotyping allowed for comprehensive analysis of individual dose response dynamics. Our findings revealed a viable AI driven early warning window, as increasing risk ratings occurred from several fractions before clinical toxicity. The dense BED driven representation revealed biologically relevant spatial dose texture characteristics that occur before toxicity and are averaged out with traditional volume based dosimetry. COMPASS establishes a proof of concept for AI enabled adaptive radiotherapy, where treatment is guided by a continually updated digital twin that tracks each patients evolving biological response.

</details>


### [3] [Scaling Ultrasound Volumetric Reconstruction via Mobile Augmented Reality](https://arxiv.org/abs/2602.18500)
*Kian Wei Ng,Yujia Gao,Deborah Khoo,Ying Zhen Tan,Chengzheng Mao,Haojie Cheng,Andrew Makmur,Kee Yuan Ngiam,Serene Goh,Eng Tat Khoo*

Main category: cs.CV

TL;DR: 提出MARVUS：基于移动端增强现实的超声容积测量系统，兼容常规2D超声，通过基础模型与AR引导实现低成本、可移植的近3D体积重建，显著提升体积估计准确性并降低操作者间差异。


<details>
  <summary>Details</summary>
Motivation: 2D超声因成本与安全性在乳腺与甲状腺成像中常用，但体积估计主观性强、操作者差异大；现有3D超声需专用探头或外部追踪，成本高、便携性差，限制临床普及。

Method: 构建名为MARVUS的系统：与常规超声设备互操作，利用基础模型实现跨专科泛化；用移动端AR进行探头与空间位姿感知与可视化，引导扫描并从2D序列重建容积；提供AR叠加的界面以辅助测量与交互。

Result: 在有经验临床医师对乳腺仿体的用户研究中，MARVUS相较基线显著提高体积估计准确性（平均差0.469 cm3）并降低操作者间变异（平均差0.417 cm3）；AR可视化提升客观表现指标与主观可用性评分。

Conclusion: MARVUS在不增加昂贵硬件的情况下，为基于超声的癌症筛查、诊断与治疗规划提供更准确、可重复、可扩展且成本敏感的容积评估方案。

Abstract: Accurate volumetric characterization of lesions is essential for oncologic diagnosis, risk stratification, and treatment planning. While imaging modalities such as Computed Tomography provide high-quality 3D data, 2D ultrasound (2D-US) remains the preferred first-line modality for breast and thyroid imaging due to cost, portability, and safety factors. However, volume estimates derived from 2D-US suffer from high inter-user variability even among experienced clinicians. Existing 3D ultrasound (3D-US) solutions use specialized probes or external tracking hardware, but such configurations increase costs and diminish portability, constraining widespread clinical use. To address these limitations, we present Mobile Augmented Reality Volumetric Ultrasound (MARVUS), a resource-efficient system designed to increase accessibility to accurate and reproducible volumetric assessment. MARVUS is interoperable with conventional ultrasound (US) systems, using a foundation model to enhance cross-specialty generalization while minimizing hardware requirements relative to current 3D-US solutions. In a user study involving experienced clinicians performing measurements on breast phantoms, MARVUS yielded a substantial improvement in volume estimation accuracy (mean difference: 0.469 cm3) with reduced inter-user variability (mean difference: 0.417 cm3). Additionally, we prove that augmented reality (AR) visualizations enhance objective performance metrics and clinician-reported usability. Collectively, our findings suggests that MARVUS can enhance US-based cancer screening, diagnostic workflows, and treatment planning in a scalable, cost-conscious, and resource-efficient manner. Usage video demonstration available (https://youtu.be/m4llYcZpqmM).

</details>


### [4] [Mitigating Shortcut Learning via Feature Disentanglement in Medical Imaging: A Benchmark Study](https://arxiv.org/abs/2602.18502)
*Sarah Müller,Philipp Berens*

Main category: cs.CV

TL;DR: 论文系统评估了特征解耦方法（对抗学习、基于依赖最小化的潜空间拆分）在医学影像中缓解“捷径学习”的效果，展示在强混杂相关下可提升泛化，最优策略是数据重平衡与模型解耦结合，且计算效率相近。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度模型常借由与任务非因果相关的捷径（混杂因素、伪相关）获得高分，但跨机构/人群/设备时失效，存在临床风险。需要能从表示中剥离混杂信息、保留任务因果信号的机制，并量化其在不同混杂强度与数据情境下的鲁棒性与代价。

Method: 在一个人工数据集与两个含自然/合成混杂的医学数据集上，比较多种特征解耦方法：1）对抗学习（使表示对混杂变量不可判别）；2）依赖最小化的潜空间拆分（显式分割任务相关与混杂相关子空间）。评估包括：分类性能、潜表示解耦质量（潜空间分析）、对不同混杂强度的鲁棒性，以及计算效率。

Result: 在训练数据存在强伪相关时，解耦方法能显著改善跨分布分类性能。潜空间分析揭示不同方法在表示质量上的差异，这些差异单靠分类指标无法体现。模型对捷径的依赖随训练混杂强度而变。将数据重平衡与模型解耦联合使用，优于仅重平衡：在保持相近计算开销下实现更强且更稳健的捷径缓解。

Conclusion: 单靠泛化精度不足以评估捷径风险；需结合潜空间解耦度量。面对混杂，最佳实践是数据层面的重平衡与模型层面的特征解耦协同应用，以在不增加明显计算成本的前提下获得更稳健的临床可迁移性。

Abstract: Although deep learning models in medical imaging often achieve excellent classification performance, they can rely on shortcut learning, exploiting spurious correlations or confounding factors that are not causally related to the target task. This poses risks in clinical settings, where models must generalize across institutions, populations, and acquisition conditions. Feature disentanglement is a promising approach to mitigate shortcut learning by separating task-relevant information from confounder-related features in latent representations. In this study, we systematically evaluated feature disentanglement methods for mitigating shortcuts in medical imaging, including adversarial learning and latent space splitting based on dependence minimization. We assessed classification performance and disentanglement quality using latent space analyses across one artificial and two medical datasets with natural and synthetic confounders. We also examined robustness under varying levels of confounding and compared computational efficiency across methods. We found that shortcut mitigation methods improved classification performance under strong spurious correlations during training. Latent space analyses revealed differences in representation quality not captured by classification metrics, highlighting the strengths and limitations of each method. Model reliance on shortcuts depended on the degree of confounding in the training data. The best-performing models combine data-centric rebalancing with model-centric disentanglement, achieving stronger and more robust shortcut mitigation than rebalancing alone while maintaining similar computational efficiency.

</details>


### [5] [A Computer Vision Framework for Multi-Class Detection and Tracking in Soccer Broadcast Footage](https://arxiv.org/abs/2602.18504)
*Daniel Tshiani*

Main category: cs.CV

TL;DR: 该论文利用单摄像头转播画面，通过YOLO检测与ByteTrack跟踪构建端到端管线，实现对球员、裁判、守门员及足球的检测与跟踪；在人/官员目标上取得高精度、高召回和较高mAP50，足球检测仍是主要瓶颈；结果表明可在无昂贵多摄/GPS设备下提取有用的球员级空间信息，促进低成本、可扩展的足球数据分析。


<details>
  <summary>Details</summary>
Motivation: 高成本的多相机或GPS系统使资源有限的球队难以获取细粒度比赛数据，造成竞争不平等；作者希望评估能否直接从常规单机位转播视频中提取可用的球员级时空数据，以降低门槛。

Method: 构建单摄像头计算机视觉端到端系统：采用YOLO作为目标检测器，结合ByteTrack实现多目标跟踪；对球员、裁判、守门员和球进行识别与跨帧关联；以精度、召回、mAP50等指标评估检测性能与跟踪效果。

Result: 对球员与裁判等人体目标的检测与跟踪表现优异，精度、召回和mAP50较高；足球目标的检测效果明显较弱，是性能瓶颈。

Conclusion: AI可从单一转播镜头中提取有意义的球员级空间信息，减少对专用硬件的依赖，使高校、青训与业余俱乐部也能采用可扩展的数据驱动分析；但需进一步提升球检测的鲁棒性与准确性。

Abstract: Clubs with access to expensive multi-camera setups or GPS tracking systems gain a competitive advantage through detailed data, whereas lower-budget teams are often unable to collect similar information. This paper examines whether such data can instead be extracted directly from standard broadcast footage using a single-camera computer vision pipeline. This project develops an end-to-end system that combines a YOLO object detector with the ByteTrack tracking algorithm to identify and track players, referees, goalkeepers, and the ball throughout a match. Experimental results show that the pipeline achieves high performance in detecting and tracking players and officials, with strong precision, recall, and mAP50 scores, while ball detection remains the primary challenge. Despite this limitation, our findings demonstrate that AI can extract meaningful player-level spatial information from a single broadcast camera. By reducing reliance on specialized hardware, the proposed approach enables colleges, academies, and amateur clubs to adopt scalable, data-driven analysis methods previously accessible only to professional teams, highlighting the potential for affordable computer vision-based soccer analytics.

</details>


### [6] [Suppression or Deletion: A Restoration-Based Representation-Level Analysis of Machine Unlearning](https://arxiv.org/abs/2602.18505)
*Yurim Jang,Jaeung Lee,Dohyun Kim,Jaemin Jo,Simon S. Woo*

Main category: cs.CV

TL;DR: 提出一种基于恢复的表征层面评估框架，利用稀疏自编码器提取类特异专家特征，并在推理时通过“引导/操控”来区分抑制与真正删除；对12种图像分类卸载方法评测发现大多仅在输出层抑制，表征中仍可恢复；甚至从预训练检查点重训也保留可恢复的语义特征。建议建立以表征验证为核心的新评估准则。


<details>
  <summary>Details</summary>
Motivation: 现有卸载（unlearning）评估多依赖输出行为，无法判断信息是被真正删除还是仅被抑制；在预训练模型广泛共享且涉及隐私/版权的背景下，仅抑制不足以满足合规与安全需求，需要能在表征层面验证“遗忘”。

Method: 提出“恢复驱动”的分析：1) 训练稀疏自编码器，在中间层学习可解释、类特异的专家特征；2) 对被宣称已忘记的类别，通过推理时的特征操控/引导（steering）来尝试恢复该类信号；3) 以恢复率定量区分抑制（高可恢复）与删除（低可恢复）。在图像分类上系统评测12种主流卸载方法，并含“从预训练点重训”的基线。

Result: 多数卸载方法在输出层面表现合格，但在中间表征中保留丰富语义，导致高恢复率，说明主要是“决策边界层面的抑制”；即使从预训练检查点重新训练，仍能高恢复，显示预训练继承的鲁棒语义未被移除。

Conclusion: 输出导向指标低估风险；表征层面的保留使敏感信息可被恢复，带来隐私与合规隐患。应采用优先检验表征删除的评估准则，并在预训练时代特别重视隐私关键应用中的表征验证。

Abstract: As pretrained models are increasingly shared on the web, ensuring that models can forget or delete sensitive, copyrighted, or private information upon request has become crucial. Machine unlearning has been proposed to address this challenge. However, current evaluations for unlearning methods rely on output-based metrics, which cannot verify whether information is completely deleted or merely suppressed at the representation level, where suppression is insufficient for true unlearning. To address this gap, we propose a novel restoration-based analysis framework that uses Sparse Autoencoders to identify class-specific expert features in intermediate layers and applies inference-time steering to quantitatively distinguish between suppression and deletion. Applying our framework to 12 major unlearning methods in image classification tasks, we find that most methods achieve high restoration rates of unlearned information, indicating that they only suppress information at the decision-boundary level, while preserving semantic features in intermediate representations. Notably, even retraining from pretrained checkpoints shows high restoration, revealing that robust semantic features inherited from pretraining are not removed by retraining. These results demonstrate that representation-level retention poses significant risks overlooked by output-based metrics, highlighting the need for new unlearning evaluation criteria. We propose new evaluation guidelines that prioritize representation-level verification, especially for privacy-critical applications in the era of pre-trained models.

</details>


### [7] [Depth from Defocus via Direct Optimization](https://arxiv.org/abs/2602.18509)
*Holly Jackson,Caleb Adams,Ignacio Lopez-Francos,Benjamin Recht*

Main category: cs.CV

TL;DR: 提出一种基于交替最小化的全局优化方法，从多张离焦图像中同时恢复深度图与全清晰图，比现有深度学习在高分辨率下更可行且效果有竞争力。


<details>
  <summary>Details</summary>
Motivation: 深度自离焦（DfD）有物理上合理的成像/模糊前向模型，但联合恢复深度与清晰图是高维、非凸且计算代价高的优化问题；现有学习方法在高分辨率与物理一致性上受限，因此需要一种可扩展、可并行、可全球求解的优化框架。

Method: 采用交替最小化：1) 固定深度图，关于全清晰图的子问题为线性且凸，可用凸优化高效求解；2) 固定全清晰图，各像素深度可独立通过并行网格搜索求解，实现“尴尬并行”；在两步之间迭代直至收敛。与基于深度学习的方法对比，并在合成与真实离焦数据上评估。

Result: 在基准数据集（含合成与真实离焦模糊）上，所提方法在较高分辨率场景下取得优于或可比于现有方法（尤其是深度学习方案）的深度重建精度与稳定性；优化过程可并行化、可扩展，展示了全局优化方案的可行性。

Conclusion: 交替最小化结合凸优化与并行网格搜索，可以有效解决高分辨率深度自离焦问题，提供了物理一致且可扩展的全局优化替代方案；代码已开源，便于复现与拓展。

Abstract: Though there exists a reasonable forward model for blur based on optical physics, recovering depth from a collection of defocused images remains a computationally challenging optimization problem. In this paper, we show that with contemporary optimization methods and reasonable computing resources, a global optimization approach to depth from defocus is feasible. Our approach rests on alternating minimization. When holding the depth map fixed, the forward model is linear with respect to the all-in-focus image. When holding the all-in-focus image fixed, the depth at each pixel can be computed independently, enabling embarrassingly parallel computation. We show that alternating between convex optimization and parallel grid search can effectively solve the depth-from-defocus problem at higher resolutions than current deep learning methods. We demonstrate our approach on benchmark datasets with synthetic and real defocus blur and show promising results compared to prior approaches. Our code is available at github.com/hollyjackson/dfd.

</details>


### [8] [Sketch2Feedback: Grammar-in-the-Loop Framework for Rubric-Aligned Feedback on Student STEM Diagrams](https://arxiv.org/abs/2602.18520)
*Aayam Bansal*

Main category: cs.CV

TL;DR: 论文提出Sketch2Feedback：一个将图像理解与规则引擎结合的“语法闭环”框架，为学生手绘图（受力图与电路图）提供与量表对齐的反馈，并显著抑制幻觉；在两个人工基准上与端到端多模态大模型比较，结果显示精度—幻觉存在权衡，框架与端到端方法具互补性。


<details>
  <summary>Details</summary>
Motivation: 课堂中对学生图示提供及时、依据评分量表的一致性反馈很难；端到端多模态大模型虽能看图说话，但幻觉高、难以信任。作者希望用可验证的符号约束把语言模型“关在轨道上”，只让其陈述已被规则引擎确认的违规点，从而兼顾可解释性与可靠性。

Method: 四阶段管线：1) 混合感知（传统视觉+检测器）提取图元；2) 将感知结果构造成符号图（对象与关系）；3) 以任务语法/评分规则进行约束检查，定位违反项；4) 受约束的VLM只口头化已验证的违规点并生成反馈。与多种基线比较（LLaVA-1.5-7B、Qwen2-VL-7B、vision-only、YOLOv8-nano、以及挑最优预测的集成oracle）。

Result: 在FBD-10与Circuit-10各500图像、每基准n=100测试样本下：Qwen2-VL-7B微平均F1最高（FBD 0.570；电路 0.528），但幻觉率极高（0.78，0.98）。集成oracle在FBD上F1=0.556且幻觉0.320，显示语法法与端到端互补。置信阈值τ=0.7可将电路幻觉从0.970降至0.880且F1不降。噪声增强显示领域鲁棒性差异：FBD稳健、电路明显退化。LLM-as-judge评估显示语法管线的电路反馈更可操作（4.85/5 vs 3.11/5）。

Conclusion: 语法闭环能把语言模型限制在可验证事实上，降低幻觉并提升可操作反馈；端到端LMM在原始F1上仍有优势，但与规则法互补。未来可沿更强感知、规则学习、自适应阈值与人机协同方向提升精度、鲁棒性与课堂可用性。

Abstract: Providing timely, rubric-aligned feedback on student-drawn diagrams is a persistent challenge in STEM education. While large multimodal models (LMMs) can jointly parse images and generate explanations, their tendency to hallucinate undermines trust in classroom deployments. We present Sketch2Feedback, a grammar-in-the-loop framework that decomposes the problem into four stages -- hybrid perception, symbolic graph construction, constraint checking, and constrained VLM feedback -- so that the language model verbalizes only violations verified by an upstream rule engine. We evaluate on two synthetic micro-benchmarks, FBD-10 (free-body diagrams) and Circuit-10 (circuit schematics), each with 500 images spanning standard and hard noise augmentation tiers, comparing our pipeline against end-to-end LMMs (LLaVA-1.5-7B, Qwen2-VL-7B), a vision-only detector, a YOLOv8-nano learned detector, and an ensemble oracle. On n=100 test samples per benchmark with 95% bootstrap CIs, results are mixed and instructive: Qwen2-VL-7B achieves the highest micro-F1 on both FBDs (0.570) and circuits (0.528), but with extreme hallucination rates (0.78, 0.98). An ensemble oracle that selects the best prediction per sample reaches F1=0.556 with hallucination 0.320 on FBDs, demonstrating exploitable complementarity between grammar and end-to-end approaches. Confidence thresholding at tau=0.7 reduces circuit hallucination from 0.970 to 0.880 with no F1 loss. Hard noise augmentation reveals domain-dependent robustness: FBD detection is resilient while circuit detection degrades sharply. An LLM-as-judge evaluation confirms that the grammar pipeline produces more actionable circuit feedback (4.85/5) than the end-to-end LMM (3.11/5). We release all code, datasets, and evaluation scripts.

</details>


### [9] [Do Generative Metrics Predict YOLO Performance? An Evaluation Across Models, Augmentation Ratios, and Dataset Complexity](https://arxiv.org/abs/2602.18525)
*Vasile Marian,Yong-Bin Kang,Alexander Buddery*

Main category: cs.CV

TL;DR: 论文系统评测用合成图像增强YOLOv11目标检测训练集，发现效益强依赖任务场景；并提出在训练前用多种特征与目标级分布度量评估合成数据，并通过控制增量比分析度量与mAP的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估生成数据质量的全局度量（如FID）常与下游检测mAP弱相关，导致难以在训练前判断合成数据是否有用。需要在不同检测难度场景下，系统检验“哪些生成方法+多少合成量”真正提升检测性能，并寻找更可靠的预训练数据集指标。

Method: 以三个单类检测任务为基准（交通标志稀疏/近饱和、Cityscapes行人密集遮挡、COCO盆栽多实例高变异），对6种GAN/扩散/混合生成器，在合成占实训集10%~150%多种配比下，训练YOLOv11（从零训练与COCO预训练两种初始化），在保留的真实测试集上以mAP@0.50:0.95评估。并在与训练集大小匹配的自助抽样协议下，计算：i) Inception-v3与DINOv2嵌入空间的全局特征度量；ii) 基于目标框统计的对象级分布距离。报告原始与“控制合成量后的残差”相关性，并进行多重检验校正。

Result: 合成增强在困难场景带来显著收益：行人最高+7.6%相对mAP，盆栽最高+30.6%；对交通标志和在预训练微调设置下提升有限。度量与性能的一致性强依赖任务场景；在控制合成量后，许多看似显著的原始相关性显著减弱。

Conclusion: 合成数据并非普适增益：收益取决于任务难度与初始化。常见全局生成度量不能稳定预测检测收益；应采用任务/对象感知的度量，并在固定合成比下比较其与性能的残差相关性，以避免被“数据量”混淆。

Abstract: Synthetic images are increasingly used to augment object-detection training sets, but reliably evaluating a synthetic dataset before training remains difficult: standard global generative metrics (e.g., FID) often do not predict downstream detection mAP. We present a controlled evaluation of synthetic augmentation for YOLOv11 across three single-class detection regimes -- Traffic Signs (sparse/near-saturated), Cityscapes Pedestrian (dense/occlusion-heavy), and COCO PottedPlant (multi-instance/high-variability). We benchmark six GAN-, diffusion-, and hybrid-based generators over augmentation ratios from 10% to 150% of the real training split, and train YOLOv11 both from scratch and with COCO-pretrained initialization, evaluating on held-out real test splits (mAP@0.50:0.95). For each dataset-generator-augmentation configuration, we compute pre-training dataset metrics under a matched-size bootstrap protocol, including (i) global feature-space metrics in both Inception-v3 and DINOv2 embeddings and (ii) object-centric distribution distances over bounding-box statistics. Synthetic augmentation yields substantial gains in the more challenging regimes (up to +7.6% and +30.6% relative mAP in Pedestrian and PottedPlant, respectively) but is marginal in Traffic Signs and under pretrained fine-tuning. To separate metric signal from augmentation quantity, we report both raw and augmentation-controlled (residualized) correlations with multiple-testing correction, showing that metric-performance alignment is strongly regime-dependent and that many apparent raw associations weaken after controlling for augmentation level.

</details>


### [10] [JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments](https://arxiv.org/abs/2602.18527)
*Zhan Liu,Changli Tang,Yuxin Wang,Zhiyuan Zhu,Youjun Chen,Yiwen Shao,Tianzi Wang,Lei Ke,Zengrui Jin,Chao Zhang*

Main category: cs.CV

TL;DR: JAEGER 将音视频大模型从2D扩展到3D，融合RGB-D与多通道一阶全向声，提出“Neural Intensity Vector”以稳健定位声源，并构建61k条SpatialSceneQA数据用于指令微调与评测，显著优于2D基线于空间感知与推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有AV-LLM仅用RGB视频与单声道音频，导致与真实3D世界的维度不匹配，难以进行可靠的声源定位与空间推理；需要一个能够显式建模3D几何与空间声学的框架。

Method: 提出JAEGER：1) 输入端融合RGB-D与多通道一阶Ambisonics；2) 设计Neural Intensity Vector，学习式空间音频表征，编码稳健的方向线索以改进到达方向估计（抗重叠声源与噪声）；3) 构建SpatialSceneQA，含61k条从仿真物理环境采集的指令微调样本；4) 在AV-LLM中实现联合空间落地（grounding）与推理。

Result: 在多种空间感知与空间推理基准上，JAEGER稳定地超过2D中心化基线，尤其在声源定位、方向估计与多源干扰条件下表现更优。

Conclusion: 显式的3D建模与多通道空间音频对推进物理环境中的AI至关重要；JAEGER与SpatialSceneQA为该方向提供可复现的模型与数据基础。

Abstract: Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.

</details>


### [11] [Image-Based Classification of Olive Varieties Native to Turkiye Using Multiple Deep Learning Architectures: Analysis of Performance, Complexity, and Generalization](https://arxiv.org/abs/2602.18530)
*Hatice Karatas,Irfan Atabas*

Main category: cs.CV

TL;DR: 该研究比较了10种深度学习架构在土耳其5种黑橄榄图像分类任务上的表现，EfficientNetV2-S准确率最高(95.8%)，而EfficientNetB0在精度与计算代价间最均衡；在小数据条件下，参数效率比单纯加深模型更关键。


<details>
  <summary>Details</summary>
Motivation: 本地栽培橄榄品种需要快速、自动、可靠的图像识别以服务育种、分选与供应链管理；现有方法对小样本、计算资源受限与实时性需求适配不足，亟需系统比较不同架构以寻找兼顾精度与效率的方案。

Method: 收集2500张5个品种的图像，采用迁移学习分别训练10种代表性CNN与ViT/Transformer架构（MobileNetV2、EfficientNet系列、ResNet、DenseNet、InceptionV3、ConvNeXt-Tiny、ViT-B16、Swin-T）。用多维指标评估：准确率、精确率、召回率、F1、MCC、Kappa、ROC-AUC、参数量、FLOPs、推理时延与泛化间隙，并比较精度-复杂度权衡。

Result: EfficientNetV2-S获得最高分类准确率95.8%；EfficientNetB0在精度与计算复杂度上性价比最佳；总体上参数高效的架构在小数据下优于仅靠更深的模型。

Conclusion: 在受限数据与资源情境下，选择参数与计算更高效的模型（如EfficientNet家族）优于追求更深/更大网络；应重视精度-复杂度权衡，并以多指标综合评估模型实用性。

Abstract: This study compares multiple deep learning architectures for the automated, image-based classification of five locally cultivated black table olive varieties in Turkey: Gemlik, Ayvalik, Uslu, Erkence, and Celebi. Using a dataset of 2500 images, ten architectures - MobileNetV2, EfficientNetB0, EfficientNetV2-S, ResNet50, ResNet101, DenseNet121, InceptionV3, ConvNeXt-Tiny, ViT-B16, and Swin-T - were trained using transfer learning. Model performance was evaluated using accuracy, precision, recall, F1-score, Matthews Correlation Coefficient (MCC), Cohen's Kappa, ROC-AUC, number of parameters, FLOPs, inference time, and generalization gap. EfficientNetV2-S achieved the highest classification accuracy (95.8%), while EfficientNetB0 provided the best trade-off between accuracy and computational complexity. Overall, the results indicate that under limited data conditions, parametric efficiency plays a more critical role than model depth alone.

</details>


### [12] [VLANeXt: Recipes for Building Strong VLA Models](https://arxiv.org/abs/2602.18532)
*Xiao-Ming Wu,Bin Fan,Kang Liao,Jian-Jian Jiang,Runze Yang,Yihang Luo,Zhonghua Wu,Wei-Shi Zheng,Chen Change Loy*

Main category: cs.CV

TL;DR: 提出统一框架系统梳理VLA设计空间，并给出实用配方，产出简单有效模型VLANeXt，在LIBERO及LIBERO-plus上SOTA并具备真实场景泛化；即将开源统一代码库以复现实验与扩展。


<details>
  <summary>Details</summary>
Motivation: 当前VLA研究碎片化：不同团队使用不一致的训练协议和评测设定，难以判断哪些设计真正关键。亟需统一框架与公平评测，澄清设计抉择并指导更强模型的构建。

Method: 以类似RT-2与OpenVLA的简洁基线为起点，建立统一训练与评测设置，从三大维度系统消融：基础组件（foundation models等）、感知要素（视觉/语言处理策略）、动作建模视角；归纳12条关键结论，形成实践配方，并据此构建简单高效的VLANeXt。

Result: VLANeXt在LIBERO与LIBERO-plus基准上超越既有SOTA，并在真实机器人实验中展现出良好泛化能力；提供统一、易用的代码库，便于复现与继续探索设计空间。

Conclusion: 统一框架与系统化消融能厘清VLA关键设计，按配方即可构建强大且简洁的模型。VLANeXt验证了该策略的有效性，并通过开源平台促进社区可复现与进一步创新。

Abstract: Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.

</details>


### [13] [Morphological Addressing of Identity Basins in Text-to-Image Diffusion Models](https://arxiv.org/abs/2602.18533)
*Andrew Fraser*

Main category: cs.CV

TL;DR: 论文发现：在文生图扩散模型中，“形态学压力”（由特征描述词或音象征子词构成）能在多层级上形成可导航的梯度，进而稳定地指向特定身份或全新视觉概念，无需显式姓名或训练泄漏。


<details>
  <summary>Details</summary>
Motivation: 理解与利用扩散模型潜空间中的“可导航结构”。作者猜测：由形态学特征（如发色、时代风格）和音象征（如 cr-, sn-, -oid 等）构成的描述，会对生成过程施加系统性偏压，形成可沿梯度搜索的“身份盆地”与概念轨迹，从而在不依赖实体姓名或数据泄漏的情况下收敛到稳定身份或新概念。

Method: 两项研究：Study 1 以 Stable Diffusion 1.5 为例，用身份形态学描述词（如“铂金金发”“美人痣”“50 年代魅力”）替代姓名；通过自蒸馏环路（用这些描述词生成合成图，再用其微调 LoRA），并用 ArcFace 相似度评估收敛；探索在“最大反向条件”下，原模型与加 LoRA 模型的失真差异。Study 2 以音象征（phonestheme）为切入，基于英语的声形素簇生成 200 个无意义词（如 sn-, cr-, -oid, -ax 组合），比较其与随机对照在视觉一致性（Purity@1）上的差异，并报告达到完美一致性的个别词。

Result: Study 1：自蒸馏 + LoRA 在无姓名/照片条件下，ArcFace 指标稳定收敛到特定身份；LoRA 在潜空间构成局部坐标系，向“反向”最大化时，基模出现“怪诞崩解”，而带 LoRA 的模型产生“恐怖谷”式、结构连贯但精确错误的输出。Study 2：含音象征的无意义词显著优于随机词（Purity@1=0.371 vs 0.209，p<1e-5，d=0.55）；三个词（snudgeoid、crashax、broomix）在零污染前提下达到完美一致性（Purity@1=1.0），各自诱发稳定且可辨的全新视觉概念。

Conclusion: 文生图模型的潜空间存在由形态结构诱发的可导航梯度：特征级描述可定位“身份盆地”，音系层面的子词结构可催生新概念并提升一致性。作者记录了身份盆地的相变、对 CFG 不变的身份稳定性，以及由次词级音象征触发的新视觉概念。

Abstract: We demonstrate that morphological pressure creates navigable gradients at multiple levels of the text-to-image generative pipeline. In Study~1, identity basins in Stable Diffusion 1.5 can be navigated using morphological descriptors -- constituent features like platinum blonde,'' beauty mark,'' and 1950s glamour'' -- without the target's name or photographs. A self-distillation loop (generating synthetic images from descriptor prompts, then training a LoRA on those outputs) achieves consistent convergence toward a specific identity as measured by ArcFace similarity. The trained LoRA creates a local coordinate system shaping not only the target identity but also its inverse: maximal away-conditioning produces eldritch'' structural breakdown in base SD1.5, while the LoRA-equipped model produces ``uncanny valley'' outputs -- coherent but precisely wrong. In Study~2, we extend this to prompt-level morphology. Drawing on phonestheme theory, we generate 200 novel nonsense words from English sound-symbolic clusters (e.g., \emph{cr-}, \emph{sn-}, \emph{-oid}, \emph{-ax}) and find that phonestheme-bearing candidates produce significantly more visually coherent outputs than random controls (mean Purity@1 = 0.371 vs.\ 0.209, p<0.00001p < 0.00001 p<0.00001, Cohen's d=0.55d = 0.55 d=0.55). Three candidates -- \emph{snudgeoid}, \emph{crashax}, and \emph{broomix} -- achieve perfect visual consistency (Purity@1 = 1.0) with zero training data contamination, each generating a distinct, coherent visual identity from phonesthetic structure alone. Together, these studies establish that morphological structure -- whether in feature descriptors or prompt-level phonological form -- creates systematic navigational gradients through diffusion model latent spaces. We document phase transitions in identity basins, CFG-invariant identity stability, and novel visual concepts emerging from sub-lexical sound patterns.

</details>


### [14] [Rodent-Bench](https://arxiv.org/abs/2602.18540)
*Thomas Heap,Laurence Aitchison,Emma Cahill,Adriana Casado Rodriguez*

Main category: cs.CV

TL;DR: Rodent-Bench是一套用于评估多模态大模型在啮齿动物行为视频标注任务上的基准。结果显示当前顶尖MLLM在长时序、细粒度行为分割与区分方面表现不足，暂不适合作为科研标注助手。


<details>
  <summary>Details</summary>
Motivation: 行为神经科学依赖耗时且主观性强的人工作标注。现有MLLM虽具视觉-语言能力，但其在长时序、细微行为识别与多范式数据上的真实效用不明，缺乏统一评测标准。需要一个覆盖多行为范式、长视频、并具备多维度评价指标的基准来系统检验与对比模型能力与局限。

Method: 构建Rodent-Bench，包含社会互动、梳理、搔抓、冻结等多种行为范式、时长10–35分钟的视频；提供两种版本以适配不同模型能力；制定标准化评测指标（逐秒准确率、宏平均F1、mAP、互信息、Matthews相关系数），并在该基准上评测Gemini-2.5-Pro、Gemini-2.5-Flash与Qwen-VL-Max。

Result: 整体性能不足，尤其在时间分割、长序列处理、与细微行为状态区分上表现不佳；少数数据集（如梳理检测）有中等改善，但仍未达到可作为可靠助手的水平。

Conclusion: 当前MLLM不适合直接用于科学级行为视频自动标注。Rodent-Bench揭示了关键瓶颈并为未来模型设计与训练（长时序建模、精细时域分割、微差异识别、鲁棒性与可扩展评测）提供方向，也为社区追踪进展奠定基准。

Abstract: We present Rodent-Bench, a novel benchmark designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to annotate rodent behaviour footage. We evaluate state-of-the-art MLLMs, including Gemini-2.5-Pro, Gemini-2.5-Flash and Qwen-VL-Max, using this benchmark and find that none of these models perform strongly enough to be used as an assistant for this task. Our benchmark encompasses diverse datasets spanning multiple behavioral paradigms including social interactions, grooming, scratching, and freezing behaviors, with videos ranging from 10 minutes to 35 minutes in length. We provide two benchmark versions to accommodate varying model capabilities and establish standardized evaluation metrics including second-wise accuracy, macro F1, mean average precision, mutual information, and Matthew's correlation coefficient. While some models show modest performance on certain datasets (notably grooming detection), overall results reveal significant challenges in temporal segmentation, handling extended video sequences, and distinguishing subtle behavioral states. Our analysis identifies key limitations in current MLLMs for scientific video annotation and provides insights for future model development. Rodent-Bench serves as a foundation for tracking progress toward reliable automated behavioral annotation in neuroscience research.

</details>


### [15] [BloomNet: Exploring Single vs. Multiple Object Annotation for Flower Recognition Using YOLO Variants](https://arxiv.org/abs/2602.18585)
*Safwat Nusrat,Prithwiraj Bhattacharjee*

Main category: cs.CV

TL;DR: 论文基于FloralSix花朵数据集，对YOLOv5/8/12多种小型到中型架构在两种标注范式（单图单框SISBB、单图多框SIMBB）下做系统基准：稀疏场景以YOLOv8m+SGD最佳，密集场景以YOLOv12n+SGD更稳健，SGD整体优于其他优化器。


<details>
  <summary>Details</summary>
Motivation: 自动化农业需要精准花朵定位与识别来支撑表型测量、坐果估计与产量监测；现有研究对不同标注密度、IoU阈值与模型规模在花朵检测中的影响缺乏系统评测。

Method: 构建含六类花、2816张高分辨率图像的FloralSix数据集，并分别以SISBB（稀疏/孤立）与SIMBB（密集/成簇）两套标注；在YOLOv5s、YOLOv8n/s/m、YOLOv12n等模型上，比较不同优化器（侧重SGD），以Precision、Recall、mAP@0.5与mAP@0.5:0.95为指标评估。

Result: SISBB：YOLOv8m(SGD)最好，P=0.956, R=0.951, mAP@0.5=0.978, mAP@0.5:0.95=0.865，适合稀疏花朵检测；SIMBB：YOLOv12n(SGD)更优，mAP@0.5=0.934, mAP@0.5:0.95=0.752，显示在密集多目标下的鲁棒性。总体上，召回导向模型在拥挤环境更优，精度导向模型在稀疏环境更优；SGD稳定领先。

Conclusion: 花朵检测性能强烈受标注密度、IoU阈值与模型尺寸交互影响；应按场景选择模型与优化器：稀疏用高精度中型模型（如YOLOv8m+SGD），密集用高召回轻量模型（如YOLOv12n+SGD）；该类密度敏感检测对无损作物分析、生长跟踪、机器人授粉与胁迫评估具应用潜力。

Abstract: Precise localization and recognition of flowers are crucial for advancing automated agriculture, particularly in plant phenotyping, crop estimation, and yield monitoring. This paper benchmarks several YOLO architectures such as YOLOv5s, YOLOv8n/s/m, and YOLOv12n for flower object detection under two annotation regimes: single-image single-bounding box (SISBB) and single-image multiple-bounding box (SIMBB). The FloralSix dataset, comprising 2,816 high-resolution photos of six different flower species, is also introduced. It is annotated for both dense (clustered) and sparse (isolated) scenarios. The models were evaluated using Precision, Recall, and Mean Average Precision (mAP) at IoU thresholds of 0.5 (mAP@0.5) and 0.5-0.95 (mAP@0.5:0.95). In SISBB, YOLOv8m (SGD) achieved the best results with Precision 0.956, Recall 0.951, mAP@0.5 0.978, and mAP@0.5:0.95 0.865, illustrating strong accuracy in detecting isolated flowers. With mAP@0.5 0.934 and mAP@0.5:0.95 0.752, YOLOv12n (SGD) outperformed the more complicated SIMBB scenario, proving robustness in dense, multi-object detection. Results show how annotation density, IoU thresholds, and model size interact: recall-optimized models perform better in crowded environments, whereas precision-oriented models perform best in sparse scenarios. In both cases, the Stochastic Gradient Descent (SGD) optimizer consistently performed better than alternatives. These density-sensitive sensors are helpful for non-destructive crop analysis, growth tracking, robotic pollination, and stress evaluation.

</details>


### [16] [Effect of Patch Size on Fine-Tuning Vision Transformers in Two-Dimensional and Three-Dimensional Medical Image Classification](https://arxiv.org/abs/2602.18614)
*Massoud Dehghan,Ramona Woitek,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 研究系统评估了ViT在医学影像中不同patch大小对分类性能的影响：更小patch（1/2/4）通常显著提升2D与3D任务的平衡准确率，但计算成本更高；将多种小patch模型做简单融合可进一步提升，尤其在2D上。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT架构改进众多，但初始设计中的patch大小选择在医学场景（含2D与3D模态）下缺乏系统性研究；医学任务常对细粒度结构敏感，需明确patch大小与性能/成本权衡。

Method: 在12个医学影像数据集（7个2D、5个3D）上，使用单GPU微调ViT，系统变化patch大小{1,2,4,7,14,28}，比较分类平衡准确率与计算代价；并将小patch（1/2/4）模型的预测做简单融合以评估集成效应。

Result: 较小patch显著优于大patch：2D上patch=2相较28最高提升平衡准确率12.78%；3D上patch=1相较14最高提升23.78%。小patch带来更高计算开销。将patch=1/2/4模型进行简单融合在多数数据集上进一步提升，2D收益尤为明显。

Conclusion: 在医学影像分类中，应优先考虑更小的patch以获取更高精度，并在资源允许时采用多patch集成以进一步提升；需在性能与计算成本之间权衡。实现已开源，便于复现与扩展。

Abstract: Vision Transformers (ViTs) and their variants have become state-of-the-art in many computer vision tasks and are widely used as backbones in large-scale vision and vision-language foundation models. While substantial research has focused on architectural improvements, the impact of patch size, a crucial initial design choice in ViTs, remains underexplored, particularly in medical domains where both two-dimensional (2D) and three-dimensional (3D) imaging modalities exist.
  In this study, using 12 medical imaging datasets from various imaging modalities (including seven 2D and five 3D datasets), we conduct a thorough evaluation of how different patch sizes affect ViT classification performance. Using a single graphical processing unit (GPU) and a range of patch sizes (1, 2, 4, 7, 14, 28), we fine-tune ViT models and observe consistent improvements in classification performance with smaller patch sizes (1, 2, and 4), which achieve the best results across nearly all datasets. More specifically, our results indicate improvements in balanced accuracy of up to 12.78% for 2D datasets (patch size 2 vs. 28) and up to 23.78% for 3D datasets (patch size 1 vs. 14), at the cost of increased computational expense. Moreover, by applying a straightforward ensemble strategy that fuses the predictions of the models trained with patch sizes 1, 2, and 4, we demonstrate a further boost in performance in most cases, especially for the 2D datasets. Our implementation is publicly available on GitHub: https://github.com/HealMaDe/MedViT

</details>


### [17] [Narrating For You: Prompt-guided Audio-visual Narrating Face Generation Employing Multi-entangled Latent Space](https://arxiv.org/abs/2602.18618)
*Aashish Chandra,Aashutosh A,Abhijit Das*

Main category: cs.CV

TL;DR: 提出一种从静态人脸图像、个体语音画像与目标文本联合生成“会说话/表情同步”的音视频的方法，通过多重纠缠（multi-entangled）潜空间联结语音与视频特征，实现逼真的声音与口型/面部动作合成。


<details>
  <summary>Details</summary>
Motivation: 现有“说话人脸”或“文本转语音+唇动驱动”方法往往难以同时保证：1) 个体说话人音色与说话风格一致性；2) 跨模态（文本→音频→视频）在时空对齐与人特异属性（长相、表情习惯）的一致性。作者希望用一个统一的潜空间来捕获并共享跨模态、跨时间的人特异与内容特征，从而提升音画一致性与真实感。

Method: - 输入：静态驱动图像、说话人声纹/声学画像（voice profile）、目标文本。
- 编码：分别对文本、图像、声纹进行编码，得到键值对与查询向量。
- 多重纠缠潜空间：将上述模态嵌入在一个共享的、可交互的潜空间中，通过键-值-查询机制建立语音与视频之间的时空与人特异特征的映射与耦合。
- 解码：将纠缠后的特征分别输入音频解码器与视频解码器，生成语音波形（或声学特征）与对齐的说话人脸视频。

Result: 在该设定下，模型能同时生成符合目标文本内容的语音与与之同步的口型/面部动作，并保持与提供的静态图像和语音画像在身份与音色上的一致性，整体真实感与时空对齐度更好。

Conclusion: 通过引入多模态多重纠缠潜空间并以键-值-查询机制联结文本、图像与声纹特征，可实现端到端的高保真、跨模态一致的说话人脸音视频联合生成。

Abstract: We present a novel approach for generating realistic speaking and talking faces by synthesizing a person's voice and facial movements from a static image, a voice profile, and a target text. The model encodes the prompt/driving text, the driving image, and the voice profile of an individual and then combines them to pass them to the multi-entangled latent space to foster key-value pairs and queries for the audio and video modality generation pipeline. The multi-entangled latent space is responsible for establishing the spatiotemporal person-specific features between the modalities. Further, entangled features are passed to the respective decoder of each modality for output audio and video generation.

</details>


### [18] [Deep LoRA-Unfolding Networks for Image Restoration](https://arxiv.org/abs/2602.18697)
*Xiangming Wang,Haijin Zeng,Benteng Sun,Jiezhang Cao,Kai Zhang,Qiangqiang Shen,Yongyong Chen*

Main category: cs.CV

TL;DR: 提出LoRun：在深度展开网络中，用单一预训练去噪器+分阶段LoRA适配器，动态匹配各阶段噪声水平，显著降参降内存且性能不降。


<details>
  <summary>Details</summary>
Motivation: 现有深度展开网络的PMM在各阶段结构与目标相同，无法针对不同噪声水平自适应；且重复堆叠造成参数冗余与高内存，限制大规模或资源受限部署。

Method: 以一个共享的预训练基座去噪器作为PMM核心，在每个展开阶段注入轻量级、阶段特定的LoRA适配器，按步骤噪声强度动态调节去噪；GDM保持优化迭代框架，PMM实现贝叶斯意义下的近端映射。通过解耦通用还原能力与任务/阶段适配，实现参数压缩（N阶段约可N倍降参）。

Result: 在三类图像复原任务（如光谱重建、压缩感知、超分辨）上，LoRun以显著更低参数与内存开销达到与现有DUN相当或更优的恢复性能。

Conclusion: 用LoRA对PMM做阶段化适配可在不复制完整网络的情况下精细控制去噪强度，缓解DUN参数与内存负担，同时保持甚至提升IR效果，适于大规模与资源受限场景。

Abstract: Deep unfolding networks (DUNs), combining conventional iterative optimization algorithms and deep neural networks into a multi-stage framework, have achieved remarkable accomplishments in Image Restoration (IR), such as spectral imaging reconstruction, compressive sensing and super-resolution.It unfolds the iterative optimization steps into a stack of sequentially linked blocks.Each block consists of a Gradient Descent Module (GDM) and a Proximal Mapping Module (PMM) which is equivalent to a denoiser from a Bayesian perspective, operating on Gaussian noise with a known level.However, existing DUNs suffer from two critical limitations: (i) their PMMs share identical architectures and denoising objectives across stages, ignoring the need for stage-specific adaptation to varying noise levels; and (ii) their chain of structurally repetitive blocks results in severe parameter redundancy and high memory consumption, hindering deployment in large-scale or resource-constrained scenarios.To address these challenges, we introduce generalized Deep Low-rank Adaptation (LoRA) Unfolding Networks for image restoration, named LoRun, harmonizing denoising objectives and adapting different denoising levels between stages with compressed memory usage for more efficient DUN.LoRun introduces a novel paradigm where a single pretrained base denoiser is shared across all stages, while lightweight, stage-specific LoRA adapters are injected into the PMMs to dynamically modulate denoising behavior according to the noise level at each unfolding step.This design decouples the core restoration capability from task-specific adaptation, enabling precise control over denoising intensity without duplicating full network parameters and achieving up to $N$ times parameter reduction for an $N$-stage DUN with on-par or better performance.Extensive experiments conducted on three IR tasks validate the efficiency of our method.

</details>


### [19] [Think with Grounding: Curriculum Reinforced Reasoning with Video Grounding for Long Video Understanding](https://arxiv.org/abs/2602.18702)
*Houlun Chen,Xin Wang,Guangyao Li,Yuwei Zhou,Yihan Chen,Jia Jia,Wenwu Zhu*

Main category: cs.CV

TL;DR: 提出Video-TwG：在长视频问答中引入“思考-并-落地(Think-with-Grounding)”范式，按需对相关片段进行定位与放大，通过两阶段强化学习课程与TwG-GRPO优化，实现更好推理与更少幻觉，SOTA于多基准。


<details>
  <summary>Details</summary>
Motivation: 长视频包含冗余与复杂多模态线索，现有方法在固定上下文下仅用文字推理，易忽略关键细节并引发幻觉；需要一种能在推理过程中主动选择何时、何处对视频进行落地对齐的机制。

Method: 提出Video-TwG框架：在交错的文本-视频推理中，模型可按需触发grounding并聚焦与问题相关的片段。训练采用两阶段强化课程：先在小型短视频GQA带标注数据学习TwG行为，再扩展到多领域通用QA数据以提升泛化。提出TwG-GRPO算法，包含细粒度grounding奖励、自确认伪奖励与精度门控机制；并构建TwG-51K数据集支持训练。整体端到端、无需复杂辅助手段或重标注推理轨迹。

Result: 在Video-MME、LongVideoBench、MLVU等基准上优于强LVU基线。消融显示两阶段课程是必要的，TwG-GRPO能更好利用无标注数据，提升grounding质量并减少冗余触发，同时不损失QA性能。

Conclusion: 按需grounding的Think-with-Grounding结合两阶段强化课程与TwG-GRPO，缓解长视频文本-only推理的幻觉与冗余上下文问题，提升长视频理解与问答的准确与效率。

Abstract: Long video understanding is challenging due to rich and complicated multimodal clues in long temporal range.Current methods adopt reasoning to improve the model's ability to analyze complex video clues in long videos via text-form reasoning.However,the existing literature suffers from the fact that the text-only reasoning under fixed video context may exacerbate hallucinations since detailed crucial clues are often ignored under limited video context length due to the temporal redundancy of long videos.To address this gap,we propose Video-TwG,a curriculum reinforced framework that employs a novel Think-with-Grounding paradigm,enabling video LLMs to actively decide when to perform on-demand grounding during interleaved text-video reasoning, selectively zooming into question-relevant clips only when necessary.Video-TwG can be trained end-to-end in a straightforward manner, without relying on complex auxiliary modules or heavily annotated reasoning tracesIn detail,we design a Two-stage Reinforced Curriculum Strategy, where the model first learns think-with-grounding behavior on a small short-video GQA dataset with grounding labels,and then scales to diverse general QA data with videos of diverse domains to encourage generalization. Further, to handle complex think-with-grounding reasoning for various kinds of data,we propose TwG-GRPO algorithm which features the fine-grained grounding reward, self-confirmed pseudo reward and accuracy-gated mechanism.Finally,we propose to construct a new TwG-51K dataset that facilitates training. Experiments on Video-MME, LongVideoBench, and MLVU show that Video-TwG consistently outperforms strong LVU baselines.Further ablation validates the necessity of our Two-stage Reinforced Curriculum Strategy and shows our TwG-GRPO better leverages diverse unlabeled data to improve grounding quality and reduce redundant groundings without sacrificing QA performance.

</details>


### [20] [IRIS-SLAM: Unified Geo-Instance Representations for Robust Semantic Localization and Mapping](https://arxiv.org/abs/2602.18709)
*Tingyang Xiao,Liu Liu,Wei Feng,Zhengyu Zou,Xiaolin Zhou,Wei Sui,Hao Li,Dingwen Zhang,Zhizhong Su*

Main category: cs.CV

TL;DR: IRIS-SLAM 通过将几何基础模型扩展为同时输出稠密几何与跨视角一致的实例嵌入，统一几何与语义表示，在数据关联与回环检测上实现语义协同，显著提升地图一致性与宽基线回环鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有几何型SLAM虽强于稠密建图，但缺乏深层语义与稳健回环；同时，现有语义映射多为解耦架构，数据关联脆弱，难以进行开放词汇的稳健语义建图与回环。需要一种统一几何-语义表征以提升关联与回环性能。

Method: 提出 IRIS-SLAM：在几何基础模型上引入实例扩展，使其联合预测稠密几何与跨视角一致的实例级嵌入；以此构建统一的几何-实例表示，设计语义协同的数据关联机制，并以实例为锚实现视角无关的回环检测；利用“语义锚”连接几何重建与开放词汇语义映射。

Result: 在实验中，IRIS-SLAM 相比最新方法在地图一致性与宽基线回环检测的可靠性方面显著领先。

Conclusion: 统一的几何-实例表示与实例引导的回环策略有效弥合几何重建与开放词汇语义映射的鸿沟，带来更稳健的数据关联与回环，从而整体提升SLAM性能。

Abstract: Geometry foundation models have significantly advanced dense geometric SLAM, yet existing systems often lack deep semantic understanding and robust loop closure capabilities. Meanwhile, contemporary semantic mapping approaches are frequently hindered by decoupled architectures and fragile data association. We propose IRIS-SLAM, a novel RGB semantic SLAM system that leverages unified geometric-instance representations derived from an instance-extended foundation model. By extending a geometry foundation model to concurrently predict dense geometry and cross-view consistent instance embeddings, we enable a semantic-synergized association mechanism and instance-guided loop closure detection. Our approach effectively utilizes viewpoint-agnostic semantic anchors to bridge the gap between geometric reconstruction and open-vocabulary mapping. Experimental results demonstrate that IRIS-SLAM significantly outperforms state-of-the-art methods, particularly in map consistency and wide-baseline loop closure reliability.

</details>


### [21] [HIME: Mitigating Object Hallucinations in LVLMs via Hallucination Insensitivity Model Editing](https://arxiv.org/abs/2602.18711)
*Ahmed Akl,Abdelwahed Khamis,Ali Cheraghian,Zhe Wang,Sara Khalifa,Kewen Wang*

Main category: cs.CV

TL;DR: 提出HIS指标与HIME分层编辑方法，按层定向抑制LVLM的物体幻觉，平均降低61.8%，且无需新增参数或推理开销。


<details>
  <summary>Details</summary>
Motivation: LVLM常出现物体幻觉（虚构不存在物体/错误属性），影响真实场景可靠性。全量微调成本高、落地难，训练-free的模型编辑是潜在可行方向，但盲目编辑会破坏预训练知识；需回答：在不同层应介入多少以压制幻觉同时保留知识。

Method: 对基于Qwen、LLaMA、Vicuna三类LLM骨干的LVLM解码器做层级脆弱性分析；提出层敏感度度量“Hallucination Insensitivity Score (HIS)”量化各层对幻觉的敏感度；据此提出“Hallucination Insensitivity Model Editing (HIME)”：一种按层自适应的权重编辑，选择性修改潜在特征，仅在高敏感层进行小幅编辑以抑制幻觉、尽量保持原有知识；不引入额外参数与推理时延。

Result: 在开放式生成基准上（CHAIR、MME、以及借助GPT-4V的评估），平均减少幻觉61.8%，同时不增加参数规模、推理时延或计算开销。

Conclusion: LVLM的幻觉具有明显的层级差异性。利用HIS进行有针对性的分层编辑（HIME）可在零训练成本下显著抑制幻觉且保全预训练知识，为训练-free可靠性提升提供了简单有效的路径。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal understanding capabilities, yet they remain prone to object hallucination, where models describe non-existent objects or attribute incorrect factual information, raising serious concerns for reliable real-world deployment. While fine-tuning is a commonly adopted mitigation strategy, its high computational cost and practical difficulty motivate the need for training-free alternatives, among which model editing has recently emerged as a promising direction. However, indiscriminate editing risks disrupting the rich implicit knowledge encoded in pre-trained LVLMs, leading to a fundamental question: how much intervention is necessary at each layer to suppress hallucinations while preserving pre-trained knowledge? To address this question, we present a systematic analysis of LVLM decoders built on three widely used large language model backbones-Qwen, LLaMA, and Vicuna-revealing clear layer-wise differences in susceptibility to object hallucination. Building on these insights, we introduce the Hallucination Insensitivity Score (HIS), a principled metric that quantifies each layer's sensitivity to hallucination and provides guidance for targeted intervention. Leveraging HIS, we propose Hallucination Insensitivity Model Editing (HIME), a simple yet effective layer-adaptive weight editing approach that selectively modifies latent features to suppress hallucinations while preserving pre-trained knowledge. Extensive experiments demonstrate that HIME reduces hallucinations by an average of 61.8% across open-ended generation benchmarks, including CHAIR, MME, and GPT-4V-aided evaluation, without introducing additional parameters, inference-time latency, or computational overhead.

</details>


### [22] [NeXt2Former-CD: Efficient Remote Sensing Change Detection with Modern Vision Architectures](https://arxiv.org/abs/2602.18717)
*Yufan Wang,Sokratis Makrogiannis,Chandra Kambhamettu*

Main category: cs.CV

TL;DR: 提出NeXt2Former-CD：用Siamese ConvNeXt（DINOv3初始化）+可变形注意力时序融合+Mask2Former解码器，在多数据集上优于SSM/Mamba基线，F1与IoU更高，推理时延可比SSM，适用于高分辨率变化检测。


<details>
  <summary>Details</summary>
Motivation: SSM在遥感变化检测因扩展性受关注，但存在对配准噪声、小尺度位移与语义歧义的鲁棒性与表达能力限制；作者探索现代卷积与注意力架构是否能作为具竞争力且实用的替代方案。

Method: 端到端NeXt2Former-CD：1) 双支路Siamese ConvNeXt编码器，使用DINOv3自监督权重以增强表征；2) 可变形注意力的时序融合模块，聚焦跨时相对应区域以缓解残余配准误差与小物体位移；3) Mask2Former解码器进行查询式掩码生成与语义分割式变化预测；整体设计针对双时相语义歧义与空间不对齐。

Result: 在LEVIR-CD、WHU-CD、CDD数据集上取得当前评测方法中的最优结果，相比近期Mamba/SSM基线在F1与IoU均有提升；尽管参数更多，但推理延迟与SSM方法相当。

Conclusion: 现代卷积+注意力（ConvNeXt+DINOv3+可变形注意力+Mask2Former）的组合能在遥感变化检测中超过SSM/Mamba基线，并在高分辨率场景保持可用的推理效率，是SSM之外的强有力替代。

Abstract: State Space Models (SSMs) have recently gained traction in remote sensing change detection (CD) for their favorable scaling properties. In this paper, we explore the potential of modern convolutional and attention-based architectures as a competitive alternative. We propose NeXt2Former-CD, an end-to-end framework that integrates a Siamese ConvNeXt encoder initialized with DINOv3 weights, a deformable attention-based temporal fusion module, and a Mask2Former decoder. This design is intended to better tolerate residual co-registration noise and small object-level spatial shifts, as well as semantic ambiguity in bi-temporal imagery. Experiments on LEVIR-CD, WHU-CD, and CDD datasets show that our method achieves the best results among the evaluated methods, improving over recent Mamba-based baselines in both F1 score and IoU. Furthermore, despite a larger parameter count, our model maintains inference latency comparable to SSM-based approaches, suggesting it is practical for high-resolution change detection tasks.

</details>


### [23] [Subtle Motion Blur Detection and Segmentation from Static Image Artworks](https://arxiv.org/abs/2602.18720)
*Ganesh Samarth,Sibendu Paul,Solale Tabarestani,Caren Chen*

Main category: cs.CV

TL;DR: 提出SMBlurDetect：一个用于从静态图像检测细微运动模糊的统一方案，包含高质量数据合成管线与端到端检测器，在跨数据集零样本上显著优于基线并支持像素级定位。


<details>
  <summary>Details</summary>
Motivation: 流媒体平台的缩略图和封面需高清晰度以提升点击率与信任度，但现有模糊检测聚焦于重度/合成模糊，缺少精细像素级标注；常用基准还存在“参照图仍残留模糊”的监督歧义，无法满足对“细微、局部”模糊的质量控制需求。

Method: 1) 数据集：从超高分辨率审美图像出发，基于SAM区域分割，对相机与物体运动进行可控仿真，配合alpha感知合成与均衡采样，生成细微、局部化的真实感运动模糊及精确掩膜。2) 模型：以ImageNet预训练编码器的U-Net为主干，采用混合“掩膜与图像中心”的训练策略，结合课程学习、难例挖掘、focal loss、模糊频域通道与分辨率自适应增强，实现从图像到像素多粒度检测与零样本泛化。

Result: 在零样本评测下，GoPro分类准确率89.68%（基线66.50%），CUHK分割mIoU 59.77%（基线9.00%），分割性能提升约6.6倍；定性结果显示对细微模糊区域定位准确，可支持自动过滤低质帧与ROI裁剪。

Conclusion: 结合高保真细微模糊合成与专门训练策略的端到端检测器，可在不依赖目标数据再训练的情况下实现强泛化与精准定位，适用于大规模内容质量控制与智能裁剪等实际生产场景。

Abstract: Streaming services serve hundreds of millions of viewers worldwide, where visual assets such as thumbnails, box art, and cover images are critical for engagement. Subtle motion blur remains a pervasive quality issue, reducing visual clarity and negatively affecting user trust and click-through rates. However, motion blur detection from static images is underexplored, as existing methods and datasets focus on severe blur and lack fine-grained pixel-level annotations needed for quality-critical applications. Benchmarks such as GOPRO and NFS are dominated by strong synthetic blur and often contain residual blur in their sharp references, leading to ambiguous supervision. We propose SMBlurDetect, a unified framework combining high-quality motion blur specific dataset generation with an end-to-end detector capable of zero-shot detection at multiple granularities. Our pipeline synthesizes realistic motion blur from super high resolution aesthetic images using controllable camera and object motion simulations over SAM segmented regions, enhanced with alpha-aware compositing and balanced sampling to generate subtle, spatially localized blur with precise ground truth masks. We train a U-Net based detector with ImageNet pretrained encoders using a hybrid mask and image centric strategy incorporating curriculum learning, hard negatives, focal loss, blur frequency channels, and resolution aware augmentation.Our method achieves strong zero-shot generalization, reaching 89.68% accuracy on GoPro (vs 66.50% baseline) and 59.77% Mean IoU on CUHK (vs 9.00% baseline), demonstrating 6.6x improvement in segmentation. Qualitative results show accurate localization of subtle blur artifacts, enabling automated filtering of low quality frames and precise region of interest extraction for intelligent cropping.

</details>


### [24] [WiCompass: Oracle-driven Data Scaling for mmWave Human Pose Estimation](https://arxiv.org/abs/2602.18726)
*Bo Liang,Chen Gong,Haobo Wang,Qirui Liu,Rungui Zhou,Fengzhi Shao,Yubo Wang,Wei Gao,Kaichen Zhou,Guolong Cui,Chenren Xu*

Main category: cs.CV

TL;DR: 提出WiCompass：一个覆盖感知的数据采集框架，用于提升毫米波人体姿态估计在分布移位下的鲁棒性；通过“姿态空间oracle”发现并优先采集欠覆盖动作，在相同预算下显著提升OOD精度并具备更优扩展性。


<details>
  <summary>Details</summary>
Motivation: 毫米波HPE具备隐私优势，但在跨场景/分布移位时泛化差。简单扩大数据量并不能带来稳健提升，瓶颈在于采集效率与动作覆盖度不足，因此需要一种能度量覆盖并引导采集的机制。

Method: 构建一个由大规模动作捕捉数据驱动的通用“姿态空间oracle”，用于量化数据集冗余与识别欠代表的动作；在其指导下，采用闭环策略优先采集信息量高、缺失的样本，实现覆盖感知的数据收集与迭代更新。

Result: 在相同采集预算下，相比传统随机或经验式采集策略，WiCompass在多项实验中持续提升OOD准确率，并展现更好的随数据规模扩展的性能曲线。

Conclusion: 鲁棒毫米波感知的关键不在于蛮力扩容，而在于覆盖感知的主动数据获取；WiCompass提供了一条可行路径，能够以更少成本获得更强的分布外泛化能力。

Abstract: Millimeter-wave Human Pose Estimation (mmWave HPE) promises privacy but suffers from poor generalization under distribution shifts. We demonstrate that brute-force data scaling is ineffective for out-of-distribution (OOD) robustness; efficiency and coverage are the true bottlenecks. To address this, we introduce WiCompass, a coverage-aware data-collection framework. WiCompass leverages large-scale motion-capture corpora to build a universal pose space ``oracle'' that quantifies dataset redundancy and identifies underrepresented motions. Guided by this oracle, WiCompass employs a closed-loop policy to prioritize collecting informative missing samples. Experiments show that WiCompass consistently improves OOD accuracy at matched budgets and exhibits superior scaling behavior compared to conventional collection strategies. By shifting focus from brute-force scaling to coverage-aware data acquisition, this work offers a practical path toward robust mmWave sensing.

</details>


### [25] [MiSCHiEF: A Benchmark in Minimal-Pairs of Safety and Culture for Holistic Evaluation of Fine-Grained Image-Caption Alignment](https://arxiv.org/abs/2602.18729)
*Sagarika Banerjee,Tangatar Madi,Advait Swaminathan,Nguyen Dao Minh Anh,Shivank Garg,Kevin Zhu,Vasu Sharma*

Main category: cs.CV

TL;DR: 提出MiSCHiEF基准（含安全MiS与文化MiC两部分），用成对、最小差异的图像与字幕测试VLM的细粒度跨模态对齐；多模型评测显示确认正确配对优于拒绝错误配对，给图选字幕优于给字幕选图，凸显当前VLM在细粒度语义-视觉对齐上的顽固挑战。


<details>
  <summary>Details</summary>
Motivation: 在安全风控与文化辨析等高敏感场景中，细微线索（视觉或语言）决定正确解读，轻微误判会带来现实风险；现有VLM可能在这种细粒度对齐上存在不足，缺少针对性评测基准。

Method: 构建MiSCHiEF，含两个对比式子数据集：MiS侧重安全情境（安全/不安全对），MiC侧重文化代理（两种文化语境对）。每个样本含两个最小差异的图像与两条最小差异的字幕，设计多种判别任务：确认正确配对、拒绝错误配对、给图选字幕与给字幕选图；对四个VLM进行系统评测。

Result: 各模型普遍更擅长确认正确图文配对而非拒绝错误配对；在给定图像从两条相似字幕中选正确字幕的任务上表现优于给定字幕从两张相似图像中选正确图像；整体准确率受细微跨模态差异影响显著。

Conclusion: 当前VLM在细粒度跨模态对齐与精确落地（grounding）方面仍存在系统性失配；在涉及微妙语义与视觉差异的高风险与文化敏感应用中，需要更强的对齐方法与更严格的评测基准来提升可靠性。

Abstract: Fine-grained image-caption alignment is crucial for vision-language models (VLMs), especially in socially critical contexts such as identifying real-world risk scenarios or distinguishing cultural proxies, where correct interpretation hinges on subtle visual or linguistic clues and where minor misinterpretations can lead to significant real-world consequences. We present MiSCHiEF, a set of two benchmarking datasets based on a contrastive pair design in the domains of safety (MiS) and culture (MiC), and evaluate four VLMs on tasks requiring fine-grained differentiation of paired images and captions. In both datasets, each sample contains two minimally differing captions and corresponding minimally differing images. In MiS, the image-caption pairs depict a safe and an unsafe scenario, while in MiC, they depict cultural proxies in two distinct cultural contexts. We find that models generally perform better at confirming the correct image-caption pair than rejecting incorrect ones. Additionally, models achieve higher accuracy when selecting the correct caption from two highly similar captions for a given image, compared to the converse task. The results, overall, highlight persistent modality misalignment challenges in current VLMs, underscoring the difficulty of precise cross-modal grounding required for applications with subtle semantic and visual distinctions.

</details>


### [26] [LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency](https://arxiv.org/abs/2602.18735)
*Weilong Yan,Haipeng Li,Hao Xu,Nianjin Ye,Yihao Ai,Shuaicheng Liu,Jingyu Hu*

Main category: cs.CV

TL;DR: LaS-Comp是一种零样本、类无关的3D形状补全方法，利用3D基础模型的生成先验，通过“显式替换+隐式细化”两阶段，在多种不完整观测下实现高保真、无缝补全，并在新提出的Omni-Comp基准上超过现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状补全方法通常依赖特定类别训练或受限于特定缺失模式，泛化性差；同时难以在保持已观测几何的前提下实现与生成区域的无缝衔接。随着3D基础模型提供强大的几何生成先验，亟需一种无需再训练、能适配多类部分观测的通用补全方案与更全面的评测基准。

Method: 提出LaS-Comp，训练自由、可对接多种3D基础模型。采用两阶段流程：1) 显式替换阶段，直接用生成先验对缺失区域进行替换/填充，同时严格保留已观测部分的几何以保证忠实性；2) 隐式细化阶段，在观测与合成边界处进行连续性和光滑性优化，提升整体一致性与无缝过渡。并构建Omni-Comp基准，融合真实与合成数据、覆盖多样且更具挑战的缺失模式。

Result: 在定量与定性评测中，LaS-Comp在Omni-Comp及其他数据上均优于此前SOTA，展示出更好的补全精度、边界无缝性与跨类别、跨缺失模式的鲁棒性。

Conclusion: 利用3D基础模型的几何先验并结合两阶段补全，能够在零样本、类无关设定下实现高质量3D形状补全；所提Omni-Comp基准促进更全面与真实的评测。方法通用、无需训练且可兼容不同3D基础模型。

Abstract: This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First, \ourname{} harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at \href{https://github.com/DavidYan2001/LaS-Comp}{LaS-Comp}.

</details>


### [27] [Synthesizing Multimodal Geometry Datasets from Scratch and Enabling Visual Alignment via Plotting Code](https://arxiv.org/abs/2602.18745)
*Haobo Lin,Tianyi Bai,Chen Chen,Jiajun Zhang,Bohan Zeng,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: 提出GeoCode：一种从零合成高复杂度多模态几何题目的数据与方法，并以“代码预测”作为视觉—符号对齐目标，从而显著提升几何推理基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉—语言模型在复杂几何构造与符号化推理上表现不佳，主要因训练数据稀缺与视觉—符号对齐弱。需要一种既高结构复杂度又可验证一致性的多模态几何数据与训练目标。

Method: 设计三阶段生成管线：1) 符号种子构造（抽象几何结构与推理链）；2) 落地实例化与自动验证（保证文本、结构、推理正确）；3) 基于代码渲染图形，确保图—文—推理一致。进一步利用渲染代码作为监督信号，引入“代码预测”任务以显式对齐视觉理解与符号结构。

Result: 得到复杂度与推理难度均显著高于现有基准的GeoCode数据集，并通过多阶段验证保证数学正确性。使用GeoCode训练的模型在多个几何基准上取得一致提升，验证数据与对齐策略有效。

Conclusion: 通过可验证的合成管线与代码预测对齐目标，GeoCode为多模态几何推理提供了高质量数据与有效训练信号，可系统提升模型在复杂几何任务上的表现；代码开源促进复现与扩展。

Abstract: Multimodal geometry reasoning requires models to jointly understand visual diagrams and perform structured symbolic inference, yet current vision--language models struggle with complex geometric constructions due to limited training data and weak visual--symbolic alignment. We propose a pipeline for synthesizing complex multimodal geometry problems from scratch and construct a dataset named \textbf{GeoCode}, which decouples problem generation into symbolic seed construction, grounded instantiation with verification, and code-based diagram rendering, ensuring consistency across structure, text, reasoning, and images. Leveraging the plotting code provided in GeoCode, we further introduce code prediction as an explicit alignment objective, transforming visual understanding into a supervised structured prediction task. GeoCode exhibits substantially higher structural complexity and reasoning difficulty than existing benchmarks, while maintaining mathematical correctness through multi-stage validation. Extensive experiments show that models trained on GeoCode achieve consistent improvements on multiple geometry benchmarks, demonstrating both the effectiveness of the dataset and the proposed alignment strategy. The code will be available at https://github.com/would1920/GeoCode.

</details>


### [28] [MIRROR: Multimodal Iterative Reasoning via Reflection on Visual Regions](https://arxiv.org/abs/2602.18746)
*Haoyu Zhang,Yuwei Wu,Pengxiang Li,Xintong Zhang,Zhi Gao,Rui Gao,Mingyang Gao,Che Sun,Yunde Jia*

Main category: cs.CV

TL;DR: 提出MIRROR框架：把“对可疑视觉区域的反思-验证-修订”纳入闭环迭代，以减少VLM的幻觉并提升多模态推理；并发布多轮监督数据集ReflectV支撑训练，实验证明在通用与推理基准上更准、更少幻觉。


<details>
  <summary>Details</summary>
Motivation: VLM在面对含糊/复杂图像时常给出貌似合理却不依图像的答案；即便使用“反思”提示，纠错仍易脱离视觉证据，导致幻觉与逻辑错误。需要一种将反思与图像证据真正耦合的方法。

Method: 提出MIRROR：一个迭代闭环，包括(1)草案回答；(2)批判性自检；(3)基于图像区域的证据核验（定位、检查、比对）；(4)依据已验证证据进行修订；循环直至答案与视觉证据一致。为训练该过程，构建ReflectV数据集，包含显式的反思触发、区域级验证动作与基于证据的答案修订，用于多轮监督。

Result: 在通用VLM基准和代表性多模态推理基准上，相比现有方法，MIRROR显著提升答案正确率并降低视觉幻觉发生率。

Conclusion: 将“反思”训练为面向证据、区域感知的验证过程（而非纯文本层面修订）能有效强化多模态推理的可靠性与扎根性；MIRROR与ReflectV验证了该思路的有效性。

Abstract: In the era of Vision-Language Models (VLMs), enhancing multimodal reasoning capabilities remains a critical challenge, particularly in handling ambiguous or complex visual inputs, where initial inferences often lead to hallucinations or logic errors. Existing VLMs often produce plausible yet ungrounded answers, and even when prompted to "reflect", their corrections may remain detached from the image evidence. To address this, we propose the MIRROR framework for Multimodal Iterative Reasoning via Reflection On visual Regions. By embedding visual reflection as a core mechanism, MIRROR is formulated as a closed-loop process comprising draft, critique, region-based verification, and revision, which are repeated until the output is visually grounded. To facilitate training of this model, we construct **ReflectV**, a visual reflective dataset for multi-turn supervision that explicitly contains reflection triggers, region-based verification actions, and answer revision grounded in visual evidence. Experiments on both general vision-language benchmarks and representative vision-language reasoning benchmarks show that MIRROR improves correctness and reduces visual hallucinations, demonstrating the value of training reflection as an evidence-seeking, region-aware verification process rather than a purely textual revision step.

</details>


### [29] [Benchmarking Computational Pathology Foundation Models For Semantic Segmentation](https://arxiv.org/abs/2602.18747)
*Lavish Ramchandani,Aashay Tinaikar,Dev Kumar Das,Rohit Garg,Tijo Thomas*

Main category: cs.CV

TL;DR: 评估10个基础模型在4个病理分割数据集上的表现，利用模型注意力图作为像素级特征并用XGBoost分类，无需微调即可快速、可解释地对像素级分割进行基准测试。CONCH总体最优，PathDino次之。将不同模型（CONCH、PathDino、CellViT）的特征拼接可显著提升性能，平均提升约7.95%。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP、DINO、CONCH等基础模型在多领域表现强大，但在病理图像像素级语义分割上的系统、独立评估很少。需要一种无需微调、可比较、可解释的方法来衡量这些模型在组织与细胞/核级任务上的泛化与实用价值。

Method: 提出一个稳健的基准流程：对四个病理数据集上的两类分割任务（组织区域与细胞/核）从基础模型提取注意力图作为像素特征，再用XGBoost进行像素分类，实现快速、模型无关、可解释的评估；同时研究不同模型特征的互补性，并测试特征拼接（集成）策略。

Result: CONCH在各数据集上总体最佳，PathDino紧随；来自不同病理训练队列的模型捕获互补的形态学表征；将CONCH、PathDino和CellViT的特征拼接在所有数据集上均优于单模型，平均提升7.95%。

Conclusion: 视觉-语言与视觉-only模型在病理分割中具有互补性。利用注意力特征+XGBoost可实现无需微调的快速评估。特征级集成优于单模型，表明基础模型组合能更好泛化于多样的病理分割任务。

Abstract: In recent years, foundation models such as CLIP, DINO,and CONCH have demonstrated remarkable domain generalization and unsupervised feature extraction capabilities across diverse imaging tasks. However, systematic and independent evaluations of these models for pixel-level semantic segmentation in histopathology remain scarce. In this study, we propose a robust benchmarking approach to asses 10 foundational models on four histopathological datasets covering both morphological tissue-region and cellular/nuclear segmentation tasks. Our method leverages attention maps of foundation models as pixel-wise features, which are then classified using a machine learning algorithm, XGBoost, enabling fast, interpretable, and model-agnostic evaluation without finetuning. We show that the vision language foundation model, CONCH performed the best across datasets when compared to vision-only foundation models, with PathDino as close second. Further analysis shows that models trained on distinct histopathology cohorts capture complementary morphological representations, and concatenating their features yields superior segmentation performance. Concatenating features from CONCH, PathDino and CellViT outperformed individual models across all the datasets by 7.95% (averaged across the datasets), suggesting that ensembles of foundation models can better generalize to diverse histopathological segmentation tasks.

</details>


### [30] [Optimizing ID Consistency in Multimodal Large Models: Facial Restoration via Alignment, Entanglement, and Disentanglement](https://arxiv.org/abs/2602.18752)
*Yuran Dong,Hang Dai,Mang Ye*

Main category: cs.CV

TL;DR: 提出EditedID框架，在不训练、可插拔的设置下，通过对扩散轨迹、采样器和注意力的系统分析，采用自适应混合、混合求解器和注意力门控三策略，在开放世界的人像编辑中同时保持原始人脸ID与编辑元素的一致性，达到SOTA并支持单/多人场景。


<details>
  <summary>Details</summary>
Motivation: 多模态编辑大模型在人像编辑时常出现人脸ID不一致。现有ID保持方法受跨源分布偏差与跨源特征污染影响，难以在保留原始身份的同时维持被编辑元素（如发型、饰品、风格）的知识产权/一致性。因此需要一种在不牺牲编辑效果的前提下稳健保持身份的通用方案。

Method: 提出EditedID的“对齐-解耦-再耦合”三步框架：1) 自适应混合策略，在扩散流程中动态对齐不同来源（原图与编辑条件）的潜变量表示；2) 混合求解器，分离并提取源特定的身份属性与细节特征，降低相互污染；3) 注意力门控，选择性地对视觉元素进行再耦合，使该保留与编辑目标在注意力层面可控地融合。该方案为训练-free、即插即用。

Result: 在广泛实验中，EditedID在原始人脸身份保持与被编辑元素一致性上均达到了最先进表现，适用于单人与多人肖像的开放域编辑场景。

Conclusion: EditedID通过对齐潜表示、解耦身份特征并门控再耦合，实现稳健的人脸ID保持与编辑一致性，建立了实用的人像编辑新基线，有助于多模态编辑大模型可靠落地；代码已开源。

Abstract: Multimodal editing large models have demonstrated powerful editing capabilities across diverse tasks. However, a persistent and long-standing limitation is the decline in facial identity (ID) consistency during realistic portrait editing. Due to the human eye's high sensitivity to facial features, such inconsistency significantly hinders the practical deployment of these models. Current facial ID preservation methods struggle to achieve consistent restoration of both facial identity and edited element IP due to Cross-source Distribution Bias and Cross-source Feature Contamination. To address these issues, we propose EditedID, an Alignment-Disentanglement-Entanglement framework for robust identity-specific facial restoration. By systematically analyzing diffusion trajectories, sampler behaviors, and attention properties, we introduce three key components: 1) Adaptive mixing strategy that aligns cross-source latent representations throughout the diffusion process. 2) Hybrid solver that disentangles source-specific identity attributes and details. 3) Attentional gating mechanism that selectively entangles visual elements. Extensive experiments show that EditedID achieves state-of-the-art performance in preserving original facial ID and edited element IP consistency. As a training-free and plug-and-play solution, it establishes a new benchmark for practical and reliable single/multi-person facial identity restoration in open-world settings, paving the way for the deployment of multimodal editing large models in real-person editing scenarios. The code is available at https://github.com/NDYBSNDY/EditedID.

</details>


### [31] [Driving with A Thousand Faces: A Benchmark for Closed-Loop Personalized End-to-End Autonomous Driving](https://arxiv.org/abs/2602.18757)
*Xiaoru Dong,Ruiqin Li,Xiao Han,Zhenxuan Wu,Jiamin Wang,Jian Chen,Qi Jiang,SM Yiu,Xinge Zhu,Yuexin Ma*

Main category: cs.CV

TL;DR: 提出Person2Drive：一个面向个性化端到端自动驾驶的完整平台与基准，涵盖数据采集、量化评估指标与可个性化的E2E算法，实现对个体驾驶风格的建模与快速适配，并在实验中验证可行性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有E2E自动驾驶多学“平均风格”，忽视驾驶者个体差异；现实中缺少带个体标注的数据、缺少量化个性风格的指标、也缺少能从用户轨迹中学习风格表示并用于控制的算法。

Method: 1) 开源灵活的数据采集系统：在逼真模拟场景中规模化采集多样且带个体属性的个性化驾驶数据；2) 基于风格向量的评估：用MMD与KL散度等统计距离度量个体驾驶行为差异；3) 个性化E2E框架：引入“风格奖励模型”，在保证安全的前提下高效适配E2E控制策略实现个体化驾驶。

Result: 实验显示该平台可进行细粒度风格分析、可复现实验评测，并能有效将E2E模型个性化，提升与个体风格一致性同时保持安全。

Conclusion: Person2Drive弥补个性化E2E-AD在数据、指标与方法层面的空白，提供可扩展数据、量化评估与可适配算法的统一方案；代码与数据将在录用后开源。

Abstract: Human driving behavior is inherently diverse, yet most end-to-end autonomous driving (E2E-AD) systems learn a single average driving style, neglecting individual differences. Achieving personalized E2E-AD faces challenges across three levels: limited real-world datasets with individual-level annotations, a lack of quantitative metrics for evaluating personal driving styles, and the absence of algorithms that can learn stylized representations from users' trajectories. To address these gaps, we propose Person2Drive, a comprehensive personalized E2E-AD platform and benchmark. It includes an open-source, flexible data collection system that simulates realistic scenarios to generate scalable and diverse personalized driving datasets; style vector-based evaluation metrics with Maximum Mean Discrepancy and KL divergence to comprehensively quantify individual driving behaviors; and a personalized E2E-AD framework with a style reward model that efficiently adapts E2E models for safe and individualized driving. Extensive experiments demonstrate that Person2Drive enables fine-grained analysis, reproducible evaluation, and effective personalization in end-to-end autonomous driving. Our dataset and code will be released after acceptance.

</details>


### [32] [TAG: Thinking with Action Unit Grounding for Facial Expression Recognition](https://arxiv.org/abs/2602.18763)
*Haobo Lin,Tianyi Bai,Jiajun Zhang,Xuanhao Chang,Sheng Lu,Fangming Gu,Zengjie Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出TAG：以面部动作单元（AU）为依据约束推理的VLM框架，用AU可视化证据支撑FER预测，并在多数据集上优于强基线且更忠实可解释。


<details>
  <summary>Details</summary>
Motivation: 现有用于表情识别的VLM虽然能生成自然语言解释，但推理常未与图像证据对齐，存在“无根据/幻觉”，跨数据集鲁棒性差；需要一种将推理与可验证的面部局部线索（如AUs）绑定的方法。

Method: 提出TAG（Thinking with Action Unit Grounding）：在多模态推理过程中强制中间步骤以AU相关面部区域为锚点；先用带AU标注的推理轨迹进行监督微调，再用强化学习引入AU感知奖励，使预测区域与外部AU检测器对齐；输出包含可视化AU证据的预测与解释。

Result: 在RAF-DB、FERPlus、AffectNet上，TAG较开源与闭源VLM基线均有一致性能提升，并显著提高视觉忠实度；消融与偏好实验显示AU奖励可稳定推理、缓解幻觉。

Conclusion: 以结构化、可落地的中间表示（AU grounding）约束VLM推理能提升FER的准确性与可解释性与鲁棒性；TAG验证了将可验证证据纳入训练与推理管线的价值。

Abstract: Facial Expression Recognition (FER) is a fine-grained visual understanding task where reliable predictions require reasoning over localized and meaningful facial cues. Recent vision--language models (VLMs) enable natural language explanations for FER, but their reasoning is often ungrounded, producing fluent yet unverifiable rationales that are weakly tied to visual evidence and prone to hallucination, leading to poor robustness across different datasets. We propose TAG (Thinking with Action Unit Grounding), a vision--language framework that explicitly constrains multimodal reasoning to be supported by facial Action Units (AUs). TAG requires intermediate reasoning steps to be grounded in AU-related facial regions, yielding predictions accompanied by verifiable visual evidence. The model is trained via supervised fine-tuning on AU-grounded reasoning traces followed by reinforcement learning with an AU-aware reward that aligns predicted regions with external AU detectors. Evaluated on RAF-DB, FERPlus, and AffectNet, TAG consistently outperforms strong open-source and closed-source VLM baselines while simultaneously improving visual faithfulness. Ablation and preference studies further show that AU-grounded rewards stabilize reasoning and mitigate hallucination, demonstrating the importance of structured grounded intermediate representations for trustworthy multimodal reasoning in FER. The code will be available at https://github.com/would1920/FER_TAG .

</details>


### [33] [A high-resolution nationwide urban village mapping product for 342 Chinese cities based on foundation models](https://arxiv.org/abs/2602.18765)
*Lubin Bai,Sheng Xiao,Ziyu Yin,Haoyu Wang,Siyang Wu,Xiuyuan Zhang,Shihong Du*

Main category: cs.CV

TL;DR: 提出GeoLink-UV：覆盖342座中国城市、清晰圈定城中村位置与边界的高分辨率全国数据集，基于多源遥感与地理矢量数据，并以基础模型驱动框架提升泛化与质量；经28城分层验证可靠，揭示城中村在全国平均占建成区约8%、于中南部集聚，呈低层高密但具区域差异性形态；数据开放可支撑SDG11相关研究与更新治理。


<details>
  <summary>Details</summary>
Motivation: 中国城中村异质性强、分布广，全国一致且可信的数据长期缺失，制约城市治理、更新与可持续评估，需要一种能在多样城市情境下稳定泛化、准确圈定城中村边界的制图方法与公开数据产品。

Method: 构建基础模型驱动的制图框架，融合多源地理数据（光学遥感影像与地理矢量数据），面向342城提取并边界化城中村；采用地理分层抽样的独立样本（28城）进行精度评估，以检验不同区域情境下的可靠性与科学性；开展基于建筑层面的形态分析与全国尺度的时空格局统计。

Result: 生成覆盖342城、清晰标注城中村位置与边界的GeoLink-UV数据集；评估显示在异质城市情境下依然可靠；揭示显著的区域差异：城中村面积平均占建成区约8%，在中部与华南显著集聚；建筑尺度表明全国一致的低层高密格局，同时呈现区域形态差异。

Conclusion: GeoLink-UV作为开放、系统验证的全国城中村数据底座，可支持城市研究、非正规住区监测与循证更新规划，直接服务于SDG11的大尺度评估；其基础模型框架缓解了跨区泛化难题，并为后续政策制定与学术研究提供可复用的数据与方法。

Abstract: Urban Villages (UVs) represent a distinctive form of high-density informal settlement embedded within China's rapidly urbanizing cities. Accurate identification of UVs is critical for urban governance, renewal, and sustainable development. But due to the pronounced heterogeneity and diversity of UVs across China's vast territory, a consistent and reliable nationwide dataset has been lacking. In this work, we present GeoLink-UV, a high-resolution nationwide UV mapping product that clearly delineates the locations and boundaries of UVs in 342 Chinese cities. The dataset is derived from multisource geospatial data, including optical remote sensing images and geo-vector data, and is generated through a foundation model-driven mapping framework designed to address the generalization issues and improve the product quality. A geographically stratified accuracy assessment based on independent samples from 28 cities confirms the reliability and scientific credibility of the nationwide dataset across heterogeneous urban contexts. Based on this nationwide product, we reveal substantial interregional disparities in UV prevalence and spatial configuration. On average, UV areas account for 8 % of built-up land, with marked clustering in central and south China. Building-level analysis further confirms a consistent low-rise, high-density development pattern of UVs nationwide, while highlighting regionally differentiated morphological characteristics. The GeoLink-UV dataset provides an open and systematically validated geospatial foundation for urban studies, informal settlement monitoring, and evidence-based urban renewal planning, and contributes directly to large-scale assessments aligned with Sustainable Development Goal 11. The GeoLink-UV dataset introduced in this article is freely available at https://doi.org/10.5281/zenodo.18688062.

</details>


### [34] [Initialization matters in few-shot adaptation of vision-language models for histopathological image classification](https://arxiv.org/abs/2602.18766)
*Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: 提出ZS-MIL：利用VLM文本编码的类别嵌入初始化MIL线性分类层，在少样本ETL场景下优于随机等常见初始化，并提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 在病理WSI分类中需用MIL。现有ETL少样本线性探测对分类器初始化极其敏感，随机初始化常低于VLM零样本表现，亟需更好的初始化策略以充分利用VLM的先验知识。

Method: 用VLM图像编码器提取patch特征并聚合为slide级表示；将VLM文本编码器生成的类嵌入用作MIL线性分类器权重的初始值，从而计算bag级概率；在少样本监督下进行微调，并与多种初始化方案比较。

Result: 在多个实验/数据集上，ZS-MIL在亚型预测的少样本ETL设置中，性能和方差均优于随机与其他常见初始化；相较零样本与线性探测，表现更稳健。

Conclusion: 以文本类嵌入初始化MIL分类层能缓解少样本ETL中初始化不稳定问题，提升WSI分类的精度与鲁棒性，是MIL与VLM结合的有效策略。

Abstract: Vision language models (VLM) pre-trained on datasets of histopathological image-caption pairs enabled zero-shot slide-level classification. The ability of VLM image encoders to extract discriminative features also opens the door for supervised fine-tuning for whole-slide image (WSI) classification, ideally using few labeled samples. Slide-level prediction frameworks require the incorporation of multiple instance learning (MIL) due to the gigapixel size of the WSI. Following patch-level feature extraction and aggregation, MIL frameworks rely on linear classifiers trained on top of the slide-level aggregated features. Classifier weight initialization has a large influence on Linear Probing performance in efficient transfer learning (ETL) approaches based on few-shot learning. In this work, we propose Zero-Shot Multiple-Instance Learning (ZS-MIL) to address the limitations of random classifier initialization that underperform zero-shot prediction in MIL problems. ZS-MIL uses the class-level embeddings of the VLM text encoder as the classification layer's starting point to compute each sample's bag-level probabilities. Through multiple experiments, we demonstrate the robustness of ZS-MIL compared to well-known weight initialization techniques both in terms of performance and variability in an ETL few-shot scenario for subtyping prediction.

</details>


### [35] [MaskDiME: Adaptive Masked Diffusion for Precise and Efficient Visual Counterfactual Explanations](https://arxiv.org/abs/2602.18792)
*Changlu Guo,Anders Nymark Christensen,Anders Bjorholm Dahl,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: 提出MaskDiME：一种局部采样的扩散式视觉反事实生成框架，兼顾语义一致与空间精确，训练免、推理比基线快30倍，在五个数据集上达SOTA或相当表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式反事实方法计算开销大、采样慢，且难以精确定位需修改的局部区域，导致反事实解释不高效、不精准。需要一种既能聚焦决策相关区域又保持图像保真与语义一致的高效方法。

Method: 提出MaskDiME，通过自适应定位与仅在决策相关局部区域进行扩散采样，统一“语义一致性”和“空间精度”。采用训练免的框架，结合掩码引导与局部化扩散步骤，在保持全局外观的同时最小化改变以翻转模型预测。

Result: 在五个跨视觉领域的基准上取得与SOTA相当或更优的性能，并在推理速度上较基线快30倍，同时生成的反事实区域更精准、图像保真度高。

Conclusion: MaskDiME为高效、实用且可泛化的视觉反事实解释提供了解决方案：以局部化、训练免的扩散生成方式，实现最小语义改动、精确空间定位与显著的推理加速。

Abstract: Visual counterfactual explanations aim to reveal the minimal semantic modifications that can alter a model's prediction, providing causal and interpretable insights into deep neural networks. However, existing diffusion-based counterfactual generation methods are often computationally expensive, slow to sample, and imprecise in localizing the modified regions. To address these limitations, we propose MaskDiME, a simple, fast, and effective diffusion framework that unifies semantic consistency and spatial precision through localized sampling. Our approach adaptively focuses on decision-relevant regions to achieve localized and semantically consistent counterfactual generation while preserving high image fidelity. Our training-free framework, MaskDiME, achieves over 30x faster inference than the baseline method and achieves comparable or state-of-the-art performance across five benchmark datasets spanning diverse visual domains, establishing a practical and generalizable solution for efficient counterfactual explanation.

</details>


### [36] [Rethinking Preference Alignment for Diffusion Models with Classifier-Free Guidance](https://arxiv.org/abs/2602.18799)
*Zhou Jiang,Yandong Wen,Zhen Liu*

Main category: cs.CV

TL;DR: 提出将偏好对齐视为无分类器引导（CFG），用经偏好微调的模型在采样时提供外部引导，无需重训基座；进一步以正/负两支路学习并在推理时相减形成“对比引导”，在SD1.5/SDXL与Pick-a-Pic v2、HPDv3上取得稳定提升。


<details>
  <summary>Details</summary>
Motivation: DPO等直接偏好优化虽简单有效，但在大规模微调时常出现泛化差距；需要一种不破坏基座、推理可控、能更好泛化的人类偏好对齐方式。

Method: 将偏好对齐重释为类似CFG的采样期引导：训练一个（或两个）偏好模型，不修改扩散基座。在推理时，将偏好模型的预测作为外部控制信号加入每步预测。为提升泛化与信号锐度，采用正/负两模块分别在正样本与负样本上训练；推理阶段计算正减负的“对比引导”向量，并按用户设定强度缩放后加到基座预测。

Result: 在Stable Diffusion 1.5与SDXL上，基于Pick-a-Pic v2与HPDv3基准，定量与定性评测均有一致增益；能提供更可控、更清晰的对齐信号。

Conclusion: 将偏好对齐转化为采样期的CFG式外部引导，可避免大规模微调带来的泛化问题；通过正负解耦训练与对比引导，在不重训基座的前提下实现更稳健、可控的对齐提升。

Abstract: Aligning large-scale text-to-image diffusion models with nuanced human preferences remains challenging. While direct preference optimization (DPO) is simple and effective, large-scale finetuning often shows a generalization gap. We take inspiration from test-time guidance and cast preference alignment as classifier-free guidance (CFG): a finetuned preference model acts as an external control signal during sampling. Building on this view, we propose a simple method that improves alignment without retraining the base model. To further enhance generalization, we decouple preference learning into two modules trained on positive and negative data, respectively, and form a \emph{contrastive guidance} vector at inference by subtracting their predictions (positive minus negative), scaled by a user-chosen strength and added to the base prediction at each step. This yields a sharper and controllable alignment signal. We evaluate on Stable Diffusion 1.5 and Stable Diffusion XL with Pick-a-Pic v2 and HPDv3, showing consistent quantitative and qualitative gains.

</details>


### [37] [Learning Multi-Modal Prototypes for Cross-Domain Few-Shot Object Detection](https://arxiv.org/abs/2602.18811)
*Wanqi Wang,Jingcai Guo,Yuxiang Cai,Zhi Chen*

Main category: cs.CV

TL;DR: 提出LMP：一个结合文本与视觉原型的双分支CD-FSOD检测器，通过多模态原型学习在少样本跨域检测中取得SOTA或竞争性mAP。


<details>
  <summary>Details</summary>
Motivation: 开放词汇VLM检测器跨域迁移强，但过度依赖文本提示，缺乏目标域特有的视觉细节，难以在少样本下实现精确定位；需要将域不变语义与域特定视觉线索融合。

Method: 构建双分支多模态原型学习框架LMP：1) 视觉原型构建模块从支持集RoI聚合类级视觉原型，并在查询图像中用抖动框动态生成“硬负例”原型以刻画干扰与相似背景；2) 将这些视觉原型注入检测流水线，结构与文本分支镜像，对齐训练；3) 并行的文本引导分支保留开放词汇语义；4) 两分支联合训练、推理时集成，结合语义抽象与域自适应细节。

Result: 在6个跨域基准和1/5/10-shot设置上达到SOTA或高度竞争的mAP。

Conclusion: 多模态原型（文本+视觉）与硬负例建模可显著提升CD-FSOD的跨域少样本检测与定位精度，兼顾开放词汇泛化与目标域细节。

Abstract: Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel classes in unseen target domains given only a few labeled examples. While open-vocabulary detectors built on vision-language models (VLMs) transfer well, they depend almost entirely on text prompts, which encode domain-invariant semantics but miss domain-specific visual information needed for precise localization under few-shot supervision. We propose a dual-branch detector that Learns Multi-modal Prototypes, dubbed LMP, by coupling textual guidance with visual exemplars drawn from the target domain. A Visual Prototype Construction module aggregates class-level prototypes from support RoIs and dynamically generates hard-negative prototypes in query images via jittered boxes, capturing distractors and visually similar backgrounds. In the visual-guided branch, we inject these prototypes into the detection pipeline with components mirrored from the text branch as the starting point for training, while a parallel text-guided branch preserves open-vocabulary semantics. The branches are trained jointly and ensembled at inference by combining semantic abstraction with domain-adaptive details. On six cross-domain benchmark datasets and standard 1/5/10-shot settings, our method achieves state-of-the-art or highly competitive mAP.

</details>


### [38] [HeRO: Hierarchical 3D Semantic Representation for Pose-aware Object Manipulation](https://arxiv.org/abs/2602.18817)
*Chongyang Xu,Shen Cheng,Haipeng Li,Haoqiang Fan,Ziliang Feng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: HeRO 提出一种将几何与语义耦合的分层语义场扩散策略，用于姿态感知机械臂操控，在多任务上刷新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 纯几何3D策略缺乏部件级语义，难以进行姿态感知操控（如区分鞋头与鞋跟），需要兼具精细判别与全局一致性的密集语义表示。

Method: 提出 HeRO：1) 语义提升（dense semantics lifting）融合 DINOv2 的判别性几何敏感特征与 Stable Diffusion 的全局一致对应，得到细粒度且空间一致的密集特征；2) 将特征处理并划分为全局场与若干局部场；3) 设计层级条件模块，以置换不变网络将全局与局部场注入到扩散式去噪器，避免对实例顺序的偏置，输出连贯的控制策略。

Result: 在多项姿态感知操控基准上达到新 SOTA：在 Place Dual Shoes 任务上成功率提升12.3%，在6个挑战任务上平均提升6.5%。

Conclusion: 通过分层语义场与扩散策略的耦合，HeRO 实现了兼具精细语义与全局一致性的控制表征，显著提升姿态感知操控性能，并具备良好泛化潜力；代码已开源。

Abstract: Imitation learning for robotic manipulation has progressed from 2D image policies to 3D representations that explicitly encode geometry. Yet purely geometric policies often lack explicit part-level semantics, which are critical for pose-aware manipulation (e.g., distinguishing a shoe's toe from heel). In this paper, we present HeRO, a diffusion-based policy that couples geometry and semantics via hierarchical semantic fields. HeRO employs dense semantics lifting to fuse discriminative, geometry-sensitive features from DINOv2 with the smooth, globally coherent correspondences from Stable Diffusion, yielding dense features that are both fine-grained and spatially consistent. These features are processed and partitioned to construct a global field and a set of local fields. A hierarchical conditioning module conditions the generative denoiser on global and local fields using permutation-invariant network architecture, thereby avoiding order-sensitive bias and producing a coherent control policy for pose-aware manipulation. In various tests, HeRO establishes a new state-of-the-art, improving success on Place Dual Shoes by 12.3% and averaging 6.5% gains across six challenging pose-aware tasks. Code is available at https://github.com/Chongyang-99/HeRO.

</details>


### [39] [Robust Self-Supervised Cross-Modal Super-Resolution against Real-World Misaligned Observations](https://arxiv.org/abs/2602.18822)
*Xiaoyu Dong,Jiahuan Li,Ziteng Cui,Naoto Yokoya*

Main category: cs.CV

TL;DR: 提出RobSelf：一种无需预训练数据、标注或预对齐的在线自监督跨模态超分方法，能在真实世界强错位场景中实现高分辨率高保真重建，并发布RealMisSR数据集。


<details>
  <summary>Details</summary>
Motivation: 现实跨模态超分常遇到少量样本、模态差异大且存在复杂空间错位；现有方法依赖配准、监督或大量训练数据，泛化与效率受限，需要一种在错位条件下仍能自适应、无需外部监督的SR方案。

Method: RobSelf包含两大核心：1）错位感知特征翻译器：将无监督的跨模态/跨分辨率对齐重构为“弱监督的错位感知翻译”子任务，生成与源内容对齐且具冗余的引导特征；2）内容感知参考过滤器：利用对齐后的引导特征，对源图像进行基于参考的判别式自增强，从而产出高分辨率、高保真SR结果。整体为在线优化、全自监督、无需预对齐。

Result: 在多种任务上达成SOTA且效率更优；并提供真实世界数据集RealMisSR以促进研究。

Conclusion: RobSelf在真实错位跨模态SR场景中，通过错位感知特征翻译与内容感知参考过滤，实现无需监督与预对齐的高效高质重建，并以RealMisSR推动该方向发展。

Abstract: We study cross-modal super-resolution (SR) on real-world misaligned data, where only a limited number of low-resolution (LR) source and high-resolution (HR) guide image pairs with complex spatial misalignments are available. To address this challenge, we propose RobSelf--a fully self-supervised model that is optimized online, requiring no training data, ground-truth supervision, or pre-alignment. RobSelf features two key techniques: a misalignment-aware feature translator and a content-aware reference filter. The translator reformulates unsupervised cross-modal and cross-resolution alignment as a weakly-supervised, misalignment-aware translation subtask, producing an aligned guide feature with inherent redundancy. Guided by this feature, the filter performs reference-based discriminative self-enhancement on the source, enabling SR predictions with high resolution and high fidelity. Across a variety of tasks, we demonstrate that RobSelf achieves state-of-the-art performance and superior efficiency. Additionally, we introduce a real-world dataset, RealMisSR, to advance research on this topic. Dataset and code: https://github.com/palmdong/RobSelf.

</details>


### [40] [Spatial-Temporal State Propagation Autoregressive Model for 4D Object Generation](https://arxiv.org/abs/2602.18830)
*Liying Yang,Jialun Liu,Jiakui Hu,Chenhao Guan,Haibin Huang,Fangqiu Yi,Chi Zhang,Yanyan Liang*

Main category: cs.CV

TL;DR: 4DSTAR提出一种自回归的时空状态传播框架，用离散token表示4D对象，通过动态维护并传播来自全部历史时间步的时空状态，指导下一组token生成，并配合4D VQ-VAE将token解码为时间一致的动态3D高斯，从而获得高质量、时空一致的4D生成，效果可与扩散模型竞争。


<details>
  <summary>Details</summary>
Motivation: 扩散式4D生成常出现时空不一致，核心原因是当前步无法充分利用所有历史步的生成信息。为解决长程依赖与条件利用不足，作者动机是设计能“显式记忆并传播”全历史时空状态的生成机制，同时用离散表示稳定建模4D结构。

Method: 1) STAR：将预测token按时间步分组；构建时空容器，动态汇聚并更新历史各组的有效时空状态，以此作为条件特征引导下一时间步token组的自回归生成，显式建模长程依赖与时空一致性。2) 4D VQ-VAE：将4D结构压缩到离散token空间，并把STAR预测的token解码为时间连贯的动态3D高斯表示。

Result: 实验证明：生成的4D对象具有更好的时空一致性，整体质量高，且在指标上与扩散模型具有竞争力。

Conclusion: 通过时空状态容器的动态传播与离散token建模，4DSTAR在4D对象生成中有效解决时空不一致与长程依赖问题，为非扩散范式的高质量4D生成提供了可行方案，并达到与主流扩散方法相当的性能。

Abstract: Generating high-quality 4D objects with spatial-temporal consistency is still formidable. Existing diffusion-based methods often struggle with spatial-temporal inconsistency, as they fail to leverage outputs from all previous timesteps to guide the generation at the current timestep. Therefore, we propose a Spatial-Temporal State Propagation AutoRegressive Model (4DSTAR), which generates 4D objects maintaining temporal-spatial consistency. 4DSTAR formulates the generation problem as the prediction of tokens that represent the 4D object. It consists of two key components: (1) The dynamic spatial-temporal state propagation autoregressive model (STAR) is proposed, which achieves spatial-temporal consistent generation. Unlike standard autoregressive models, STAR divides prediction tokens into groups based on timesteps. It models long-term dependencies by propagating spatial-temporal states from previous groups and utilizes these dependencies to guide generation at the next timestep. To this end, a spatial-temporal container is proposed, which dynamically updating the effective spatial-temporal state features from all historical groups, then updated features serve as conditional features to guide the prediction of the next token group. (2) The 4D VQ-VAE is proposed, which implicitly encodes the 4D structure into discrete space and decodes the discrete tokens predicted by STAR into temporally coherent dynamic 3D Gaussians. Experiments demonstrate that 4DSTAR generates spatial-temporal consistent 4D objects, and achieves performance competitive with diffusion models.

</details>


### [41] [IDperturb: Enhancing Variation in Synthetic Face Generation via Angular Perturbation](https://arxiv.org/abs/2602.18831)
*Fadi Boutros,Eduarda Caldeira,Tahar Chettaoui,Naser Damer*

Main category: cs.CV

TL;DR: 提出IDPERTURB：在单位超球面上对身份嵌入进行受限角度扰动，用预训练身份条件扩散模型生成更具类内多样性的合成人脸，从而提升人脸识别训练效果。


<details>
  <summary>Details</summary>
Motivation: 真实人脸数据受隐私与法规限制；现有身份条件扩散模型虽能生成逼真、身份一致的人脸，但类内变化不足，导致用其训练的FR模型泛化性不佳。

Method: 在不改动生成模型的前提下，对身份嵌入向量在单位超球面上施加小角度（受限角域）几何扰动，得到多样但相近的身份条件向量；用这些向量作为条件，驱动预训练扩散模型生成图像，兼顾外观多样性与身份一致性。

Result: 用IDPERTURB生成的数据训练FR模型，在多个FR基准上优于现有合成数据生成方法；表现提升归因于更丰富的类内变化且不损害身份一致性。

Conclusion: 简单、模型无关的采样策略可显著提升合成人脸的类内多样性，进而提升FR模型的鲁棒性与泛化；为在隐私受限场景下用合成数据替代真实数据提供了有效途径。

Abstract: Synthetic data has emerged as a practical alternative to authentic face datasets for training face recognition (FR) systems, especially as privacy and legal concerns increasingly restrict the use of real biometric data. Recent advances in identity-conditional diffusion models have enabled the generation of photorealistic and identity-consistent face images. However, many of these models suffer from limited intra-class variation, an essential property for training robust and generalizable FR models. In this work, we propose IDPERTURB, a simple yet effective geometric-driven sampling strategy to enhance diversity in synthetic face generation. IDPERTURB perturbs identity embeddings within a constrained angular region of the unit hyper-sphere, producing a diverse set of embeddings without modifying the underlying generative model. Each perturbed embedding serves as a conditioning vector for a pre-trained diffusion model, enabling the synthesis of visually varied yet identity-coherent face images suitable for training generalizable FR systems. Empirical results demonstrate that training FR on datasets generated using IDPERTURB yields improved performance across multiple FR benchmarks, compared to existing synthetic data generation approaches.

</details>


### [42] [CLAP Convolutional Lightweight Autoencoder for Plant Disease Classification](https://arxiv.org/abs/2602.18833)
*Asish Bera,Subhajit Roy,Sudiptendu Banerjee*

Main category: cs.CV

TL;DR: 提出一种轻量级卷积自编码器（CLAP）用于叶片图像的病害分类，在三套数据集上以约500万参数实现与主流方法相当或更优的准确率，训练约20ms/步、推理1ms/张。


<details>
  <summary>Details</summary>
Motivation: 现实田间环境下叶片图像存在光照、背景和细微病征差异，传统机器学习与重型预训练CNN在捕捉细粒度判别特征与计算成本间难以兼顾，需要一种既能提取判别特征又高效的模型。

Method: 设计使用可分离卷积的轻量级自编码器；在编码器中引入Sigmoid门控以增强判别性；通过解码器进一步细化特征，并将编码器与解码器的特征图融合后进行分类。无需重型预训练骨干，参数量约500万。

Result: 在Integrated Plant Disease、Groundnut、CCMT等公开数据集（含木薯、番茄、玉米、花生、葡萄等）上取得提升或具竞争力的准确率；训练时间约20ms/步，单张推理1ms。

Conclusion: CLAP在保证高准确率的同时显著降低计算成本，适合资源受限、田间实用场景的植物健康状态与病害分类。

Abstract: Convolutional neural networks have remarkably progressed the performance of distinguishing plant diseases, severity grading, and nutrition deficiency prediction using leaf images. However, these tasks become more challenging in a realistic in-situ field condition. Often, a traditional machine learning model may fail to capture and interpret discriminative characteristics of plant health, growth and diseases due to subtle variations within leaf subcategories. A few deep learning methods have used additional preprocessing stages or network modules to address the problem, whereas several other methods have utilized pre-trained backbone CNNs, most of which are computationally intensive. Therefore, to address the challenge, we propose a lightweight autoencoder using separable convolutional layers in its encoder decoder blocks. A sigmoid gating is applied for refining the prowess of the encoders feature discriminability, which is improved further by the decoder. Finally, the feature maps of the encoder decoder are combined for rich feature representation before classification. The proposed Convolutional Lightweight Autoencoder for Plant disease classification, called CLAP, has been experimented on three public plant datasets consisting of cassava, tomato, maize, groundnut, grapes, etc. for determining plant health conditions. The CLAP has attained improved or competitive accuracies on the Integrated Plant Disease, Groundnut, and CCMT datasets balancing a tradeoff between the performance, and little computational cost requiring 5 million parameters. The training time is 20 milliseconds and inference time is 1 ms per image.

</details>


### [43] [Detecting AI-Generated Forgeries via Iterative Manifold Deviation Amplification](https://arxiv.org/abs/2602.18842)
*Jiangling Zhang,Shuxuan Gao,Bofan Liu,Siqiang Feng,Jirui Huang,Yaxiong Chen,Ziyu Chen*

Main category: cs.CV

TL;DR: 提出IFA-Net，通过建模“真实”而非“伪造”，利用冻结的MAE作为通用真实先验，先粗分割再通过任务自适应先验注入引导MAE放大可疑区域重建失败，实现精细篡改定位；在扩散修补基准上显著优于现有方法并具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有篡改定位方法常依赖特定伪造模式，难以适应快速演化的新型编辑手段。作者认为所有篡改都偏离自然图像流形，若以“真实先验”替代“伪造特征”，可获得更强的泛化与鲁棒性。

Method: 1) 使用在真实图像上预训练且冻结的MAE作为通用真实先验；2) 双流分割网络(DSSN)融合原图与MAE重建残差进行粗定位；3) 任务自适应先验注入(TAPI)将粗预测转为引导提示，驱动MAE解码器在可疑区域放大重建失败；4) 形成两阶段闭环：粗分割→先验注入→细化分割。

Result: 在四个基于扩散的图像修补基准上，较第二优方法平均提升IoU 6.5%、F1 8.1%；并在传统篡改类型上展现出良好的泛化能力。

Conclusion: 通过以MAE的真实先验为核心并迭代放大可疑区域的重建失败，IFA-Net实现了对多种操纵的精细、泛化良好的像素级定位，优于现有特定伪造模式驱动的方法。

Abstract: The proliferation of highly realistic AI-generated images poses critical challenges for digital forensics, demanding precise pixel-level localization of manipulated regions. Existing methods predominantly learn discriminative patterns of specific forgeries and often struggle with novel manipulations as editing techniques continue to evolve. We propose the Iterative Forgery Amplifier Network (IFA-Net), which shifts from learning "what is fake" to modeling "what is real". Grounded in the principle that all manipulations deviate from the natural image manifold, IFA-Net leverages a frozen Masked Autoencoder (MAE) pretrained on real images as a universal realness prior. Our framework operates through a two-stage closed-loop process: an initial Dual-Stream Segmentation Network (DSSN) fuses the original image with MAE reconstruction residuals for coarse localization, followed by a Task-Adaptive Prior Injection (TAPI) module that converts this coarse prediction into guiding prompts to steer the MAE decoder and amplify reconstruction failures in suspicious regions for precise refinement. Extensive experiments on four diffusion-based inpainting benchmarks show that IFA-Net achieves an average improvement of 6.5% in IoU and 8.1% in F1-score over the second-best method, while demonstrating strong generalization to traditional manipulation types.

</details>


### [44] [Echoes of Ownership: Adversarial-Guided Dual Injection for Copyright Protection in MLLMs](https://arxiv.org/abs/2602.18845)
*Chengwei Xia,Fan Ma,Ruijie Quan,Yunqiu Xu,Kun Zhan,Yi Yang*

Main category: cs.CV

TL;DR: 提出一种为多模态大语言模型生成可验证“版权触发器”的框架：把图像当作可学习张量，用双重注入（文本一致性+CLIP语义对齐）进行对抗优化，使触发图在原模型的微调衍生体上触发归属文本，在非衍生模型上保持惰性；并加入一个抗触发的辅助模型训练以提高在重度微调场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: MLLMs快速落地后，模型版本归属与所有权争议频发，缺少可验证的版权水印/归属标记手段，尤其要能在模型被微调或域迁移后仍可追踪其谱系，同时避免在无关模型上误触发。

Method: - 将触发图像视为可学习张量进行对抗优化。
- 双重信息注入：
  1) 文本一致性注入：用辅助MLLM生成输出，与预设“所有权目标文本”做一致性损失，反向传播到图像，注入所有权语义；
  2) 语义级注入：最小化触发图像与目标文本的CLIP特征距离，实现跨模态对齐。
- 额外对抗阶段：基于原模型派生的辅助模型被训练为“抵抗”生成目标文本，以此提升触发器在重度微调衍生模型上的鲁棒性与专属性。

Result: 在多种微调方式与域偏移设置下，所生成的触发图像能在衍生模型上稳定诱发目标所有权文本，而在非衍生模型上保持低触发率，表现出较强的追踪能力与鲁棒性。

Conclusion: 双重注入+抗触发辅助模型的框架可有效为MLLMs嵌入可验证的所有权标记，支持在模型谱系追踪与知识产权保护场景中的实用化；在重度微调和域迁移下依然可靠。

Abstract: With the rapid deployment and widespread adoption of multimodal large language models (MLLMs), disputes regarding model version attribution and ownership have become increasingly frequent, raising significant concerns about intellectual property protection. In this paper, we propose a framework for generating copyright triggers for MLLMs, enabling model publishers to embed verifiable ownership information into the model. The goal is to construct trigger images that elicit ownership-related textual responses exclusively in fine-tuned derivatives of the original model, while remaining inert in other non-derivative models. Our method constructs a tracking trigger image by treating the image as a learnable tensor, performing adversarial optimization with dual-injection of ownership-relevant semantic information. The first injection is achieved by enforcing textual consistency between the output of an auxiliary MLLM and a predefined ownership-relevant target text; the consistency loss is backpropagated to inject this ownership-related information into the image. The second injection is performed at the semantic-level by minimizing the distance between the CLIP features of the image and those of the target text. Furthermore, we introduce an additional adversarial training stage involving the auxiliary model derived from the original model itself. This auxiliary model is specifically trained to resist generating ownership-relevant target text, thereby enhancing robustness in heavily fine-tuned derivative models. Extensive experiments demonstrate the effectiveness of our dual-injection approach in tracking model lineage under various fine-tuning and domain-shift scenarios.

</details>


### [45] [DUET-VLM: Dual stage Unified Efficient Token reduction for VLM Training and Inference](https://arxiv.org/abs/2602.18846)
*Aditya Kumar Singh,Hitesh Kandala,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CV

TL;DR: DUET-VLM 提出一个可插拔的双阶段视觉token压缩框架：先在视觉编码器端进行冗余感知压缩，再在语言骨干中进行分层、由文本显著性引导的token丢弃；在显著降低视觉token数量的同时几乎不损失精度，甚至在视频场景中超过基线。


<details>
  <summary>Details</summary>
Motivation: VLM 的视觉token密集，导致推理/训练计算昂贵。现有方法要么合并冗余token，要么在语言模型中逐层丢弃，但常以精度换速度，泛化与稳健性不足。需要一种在显著压缩的同时保留关键信息、并能端到端适配的方案。

Method: 提出 DUET-VLM 双压缩框架：
1) 视觉侧：对视觉编码器输出进行仅视觉的冗余感知压缩，生成信息保持型精简token；
2) 语言侧：在LLM内部分层执行、由文本显著性引导的视觉token逐步丢弃，优先保留与当前文本更相关的token；
两阶段协调的token管理支持更激进的压缩而维持关键语义；并支持端到端训练以适配压缩表征。

Result: 在 LLaVA-1.5-7B 上：在减少67%视觉token时保持>99%基线精度；在89%减少时仍>97%。若在训练中使用双阶段压缩：67%时达99.7%，89%时97.6%，均优于现有SOTA。在 Video-LLaVA-7B 中：在减少53.1% token时精度超过基线（>100%），在极端93.4%减少下仍保留97.6%。

Conclusion: DUET-VLM 能在显著降低视觉token的同时维持或提升多模态性能；端到端训练促使模型在相同计算预算下学到紧凑且语义丰富的表征，适用于图像与视频场景，优于现有视觉token压缩方法。

Abstract: Vision-language models (VLMs) have achieved remarkable multimodal understanding and reasoning capabilities, yet remain computationally expensive due to dense visual tokenization. Existing efficiency approaches either merge redundant visual tokens or drop them progressively in language backbone, often trading accuracy for speed. In this work, we propose DUET-VLM, a versatile plug-and-play dual compression framework that consists of (a) vision-only redundancy aware compression of vision encoder's output into information-preserving tokens, followed by (b) layer-wise, salient text-guided dropping of visual tokens within the language backbone to progressively prune less informative tokens. This coordinated token management enables aggressive compression while retaining critical semantics. On LLaVA-1.5-7B, our approach maintains over 99% of baseline accuracy with 67% fewer tokens, and still retains >97% even at 89% reduction. With this dual-stage compression during training, it achieves 99.7% accuracy at 67% and 97.6% at 89%, surpassing prior SoTA visual token reduction methods across multiple benchmarks. When integrated into Video-LLaVA-7B, it even surpasses the baseline -- achieving >100% accuracy with a substantial 53.1% token reduction and retaining 97.6% accuracy under an extreme 93.4% setting. These results highlight end-to-end training with DUET-VLM, enabling robust adaptation to reduced visual (image/video) input without sacrificing accuracy, producing compact yet semantically rich representations within the same computational budget. Our code is available at https://github.com/AMD-AGI/DUET-VLM.

</details>


### [46] [Open-Vocabulary Domain Generalization in Urban-Scene Segmentation](https://arxiv.org/abs/2602.18853)
*Dong Zhao,Qi Zang,Nan Pu,Wenjing Li,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出开放词汇域泛化语义分割(OVDG-SS)新设定与首个自动驾驶基准，并用S2-Corr状态空间机制校正VLM的文图相关，在合成到真实与真实到真实跨域上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统DG-SS仅支持固定已知类，无法应对开放世界；OV-SS虽能识别更多概念，但对域移位敏感，尤其在自动驾驶场景鲁棒性差。需要同时处理“看不见的域”和“看不见的类别”。

Method: 构建OVDG-SS基准，覆盖多种合成→真实与真实→真实设置；观察到域移位会扭曲预训练VLM的文图相关；提出S2-Corr：基于状态空间模型的相关性精炼模块，在分布变化下稳定并校正文本-图像相关，以提升跨域鲁棒性与效率。

Result: 在所建基准的广泛实验中，所提方法在跨域性能与效率上均优于现有开放词汇分割方法，表现出更稳定的文图相关与更好的泛化能力。

Conclusion: OVDG-SS填补了开放词汇与域泛化的结合空白；S2-Corr有效缓解域移导致的文图相关失真，提升在未见域与未见类下的分割鲁棒性，适合自动驾驶等复杂场景。

Abstract: Domain Generalization in Semantic Segmentation (DG-SS) aims to enable segmentation models to perform robustly in unseen environments. However, conventional DG-SS methods are restricted to a fixed set of known categories, limiting their applicability in open-world scenarios. Recent progress in Vision-Language Models (VLMs) has advanced Open-Vocabulary Semantic Segmentation (OV-SS) by enabling models to recognize a broader range of concepts. Yet, these models remain sensitive to domain shifts and struggle to maintain robustness when deployed in unseen environments, a challenge that is particularly severe in urban-driving scenarios. To bridge this gap, we introduce Open-Vocabulary Domain Generalization in Semantic Segmentation (OVDG-SS), a new setting that jointly addresses unseen domains and unseen categories. We introduce the first benchmark for OVDG-SS in autonomous driving, addressing a previously unexplored problem and covering both synthetic-to-real and real-to-real generalization across diverse unseen domains and unseen categories. In OVDG-SS, we observe that domain shifts often distort text-image correlations in pre-trained VLMs, which hinders the performance of OV-SS models. To tackle this challenge, we propose S2-Corr, a state-space-driven text-image correlation refinement mechanism that mitigates domain-induced distortions and produces more consistent text-image correlations under distribution changes. Extensive experiments on our constructed benchmark demonstrate that the proposed method achieves superior cross-domain performance and efficiency compared to existing OV-SS approaches.

</details>


### [47] [Joint Post-Training Quantization of Vision Transformers with Learned Prompt-Guided Data Generation](https://arxiv.org/abs/2602.18861)
*Shile Li,Markus Karmann,Onay Urfalioglu*

Main category: cs.CV

TL;DR: 提出一种端到端的ViT量化PTQ框架，联合优化全网络及层间依赖，无需标注数据，1小时在单GPU完成ViT-S，刷新W4A4/W3A3 SOTA，并在极低比特(W1.58A8)保持强精度；提出用SD Turbo和多模态提示学习的数据无关校准，效果与真实数据相当且优于简单文本提示。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ多为逐块/分层重建，难以捕捉全局依赖；ViT在极低比特下精度退化显著且数据依赖强，限制边缘部署与实际落地；缺少在无标注甚至无数据场景下的有效校准方法。

Method: 1) 端到端联合PTQ：对所有层和跨块依赖同时优化，采用无标注的校准样本，全局目标最小化输出失真；2) 高效性：算法可随样本数扩展，ViT-S单GPU约1小时完成；3) 极低比特量化方案：支持W4A4/W3A3及近1比特(W1.58A8)；4) 数据自由校准：用Stable Diffusion Turbo生成多样化的无标签图像，通过学习多模态(多模式)提示，联合约束提示嵌入多样性与生成图像特征多样性，实现无数据校准。

Result: 在ImageNet上达成W4A4与W3A3的SOTA；首次报告ViT/DeiT/Swin-T在极低比特(W1.58A8)的PTQ且保持强精度；数据无关校准与真实ImageNet校准性能相当，显著优于简单文本提示基线。

Conclusion: 端到端联合PTQ能够在无标注甚至无数据条件下稳定量化ViT家族至低比特，兼顾精度与效率，具备面向边缘设备部署的潜力；基于SD Turbo与多模式提示的生成式校准是可行且有效的替代方案。

Abstract: We present a framework for end-to-end joint quantization of Vision Transformers trained on ImageNet for the purpose of image classification. Unlike prior post-training or block-wise reconstruction methods, we jointly optimize over the entire set of all layers and inter-block dependencies without any labeled data, scaling effectively with the number of samples and completing in just one hour on a single GPU for ViT-small. We achieve state-of-the-art W4A4 and W3A3 accuracies on ImageNet and, to the best of our knowledge, the first PTQ results that maintain strong accuracy on ViT, DeiT, and Swin-T models under extremely low-bit settings (W1.58A8), demonstrating the potential for efficient edge deployment. Furthermore, we introduce a data-free calibration strategy that synthesizes diverse, label-free samples using Stable Diffusion Turbo guided by learned multi-mode prompts. By encouraging diversity in both the learned prompt embeddings and the generated image features, our data-free approach achieves performance on par with real-data ImageNet calibration and surpasses simple text-prompt baselines such as "a <adjective> photo of <adjective> <cls>".

</details>


### [48] [Similarity-as-Evidence: Calibrating Overconfident VLMs for Interpretable and Label-Efficient Medical Active Learning](https://arxiv.org/abs/2602.18867)
*Zhuofan Xie,Zishan Lin,Jinliang Lin,Jie Qi,Shaohua Hong,Shuo Li*

Main category: cs.CV

TL;DR: 提出“Similarity-as-Evidence (SaE)”框架，用证据论化的方式校准VLM中文本-图像相似度，显式建模不确定性（vacuity与dissonance），并据此设计双因子主动学习采样策略，在多医学数据集上以有限标注预算取得SOTA性能与更好校准。


<details>
  <summary>Details</summary>
Motivation: 医学影像主动学习常因冷启动而选样失效；VLM可零样本缓解冷启动，但其温度缩放softmax把相似度当作确定性分数，忽略不确定性，导致过度自信与次优样本选择，浪费标注预算。需要一种既能量化不确定性又能指导可解释采样的机制。

Method: 将VLM的文本-图像相似度向量视为“证据”，通过新增Similarity Evidence Head（SEH）映射为Dirichlet分布参数；基于Dirichlet不确定性分解，显式度量vacuity（证据缺乏）与dissonance（证据冲突），替代传统softmax置信度。提出双因子获取策略：初期优先高vacuity样本以覆盖少见模式，后期优先高dissonance样本以精炼决策边界；并提供临床可解释的选样理由。

Result: 在10个公开医学影像数据集、总标注预算20%下，宏平均准确率达82.57%，为SOTA；在代表性BTMRI数据集上，校准更佳，NLL为0.425。

Conclusion: 将相似度视作证据并用Dirichlet建模能缓解VLM过度自信、改进主动学习的样本选择效率。SaE的双因子策略兼顾覆盖与精炼，带来更高准确率与更好校准，并具备临床可解释性。

Abstract: Active Learning (AL) reduces annotation costs in medical imaging by selecting only the most informative samples for labeling, but suffers from cold-start when labeled data are scarce. Vision-Language Models (VLMs) address the cold-start problem via zero-shot predictions, yet their temperature-scaled softmax outputs treat text-image similarities as deterministic scores while ignoring inherent uncertainty, leading to overconfidence. This overconfidence misleads sample selection, wasting annotation budgets on uninformative cases. To overcome these limitations, the Similarity-as-Evidence (SaE) framework calibrates text-image similarities by introducing a Similarity Evidence Head (SEH), which reinterprets the similarity vector as evidence and parameterizes a Dirichlet distribution over labels. In contrast to a standard softmax that enforces confident predictions even under weak signals, the Dirichlet formulation explicitly quantifies lack of evidence (vacuity) and conflicting evidence (dissonance), thereby mitigating overconfidence caused by rigid softmax normalization. Building on this, SaE employs a dual-factor acquisition strategy: high-vacuity samples (e.g., rare diseases) are prioritized in early rounds to ensure coverage, while high-dissonance samples (e.g., ambiguous diagnoses) are prioritized later to refine boundaries, providing clinically interpretable selection rationales. Experiments on ten public medical imaging datasets with a 20% label budget show that SaE attains state-of-the-art macro-averaged accuracy of 82.57%. On the representative BTMRI dataset, SaE also achieves superior calibration, with a negative log-likelihood (NLL) of 0.425.

</details>


### [49] [Enhancing 3D LiDAR Segmentation by Shaping Dense and Accurate 2D Semantic Predictions](https://arxiv.org/abs/2602.18869)
*Xiaoyu Dong,Tiankui Xian,Wanshui Gan,Naoto Yokoya*

Main category: cs.CV

TL;DR: 提出MM2D3D，用相机图像辅助3D激光雷达点云语义分割；通过跨模态引导滤波与动态交叉伪监督，生成更致密、更准确的中间2D预测，从而提升最终3D分割精度，2D与3D均优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 将LiDAR点云与标签投影到2D后可简化为2D分割问题，但投影导致的稀疏性使中间2D语义预测稀疏且不准，限制3D精度。需要利用额外模态信息来弥补稀疏性、提升2D中间结果的密度与准确性，进而提高3D表现。

Method: 提出多模态分割模型MM2D3D：1) 跨模态引导滤波（cross-modal guided filtering），利用相机图像中致密的语义关系来约束稀疏的LiDAR/标签投影，引导2D中间语义预测更致密、边界更清晰；2) 动态交叉伪监督（dynamic cross pseudo supervision），以相机分支的致密语义预测为教师，动态生成伪标签，鼓励LiDAR投影的2D预测逼近其分布；整体通过2D改进反馈到3D以提升最终点云语义分割。

Result: 实验表明：采用上述技术后，中间2D预测更致密、精度更高，并显著提升最终3D分割精度；与现有方法相比，在2D与3D评测上均取得更优结果。

Conclusion: 利用相机图像辅助并通过跨模态约束与伪监督缓解投影稀疏性，可有效提升2D中间预测质量，进而提高3D点云语义分割性能；方法在2D与3D上均表现先进。

Abstract: Semantic segmentation of 3D LiDAR point clouds is important in urban remote sensing for understanding real-world street environments. This task, by projecting LiDAR point clouds and 3D semantic labels as sparse maps, can be reformulated as a 2D problem. However, the intrinsic sparsity of the projected LiDAR and label maps can result in sparse and inaccurate intermediate 2D semantic predictions, which in return limits the final 3D accuracy. To address this issue, we enhance this task by shaping dense and accurate 2D predictions. Specifically, we develop a multi-modal segmentation model, MM2D3D. By leveraging camera images as auxiliary data, we introduce cross-modal guided filtering to overcome label map sparsity by constraining intermediate 2D semantic predictions with dense semantic relations derived from the camera images; and we introduce dynamic cross pseudo supervision to overcome LiDAR map sparsity by encouraging the 2D predictions to emulate the dense distribution of the semantic predictions from the camera images. Experiments show that our techniques enable our model to achieve intermediate 2D semantic predictions with dense distribution and higher accuracy, which effectively enhances the final 3D accuracy. Comparisons with previous methods demonstrate our superior performance in both 2D and 3D spaces.

</details>


### [50] [BiMotion: B-spline Motion for Text-guided Dynamic 3D Character Generation](https://arxiv.org/abs/2602.18873)
*Miaowei Wang,Qingxuan Yan,Zhi Cao,Yayuan Li,Oisin Mac Aodha,Jason J. Corso,Amir Vaxman*

Main category: cs.CV

TL;DR: 提出BiMotion：用可微B样条连续表示替代离散逐帧表示，配合闭式解与拉普拉斯正则的B样条求解器，将变长动作压缩为定量控制点，并引入法向融合与对应感知、局部刚性损失；基于新数据集BIMO训练，生成更贴合文本、更高质量、且更快的3D动态角色动作。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动3D角色生成常因固定时长输入与离散逐帧表示，导致子动作贫乏、动作不连贯、难以捕获丰富语义与变长时序。需要一种既能表达连续运动细节、又不改动底层生成器能力的表示与训练方案。

Method: 以连续可微B样条曲线表示运动轨迹；提出带拉普拉斯正则的闭式B样条求解器，将变长序列压缩为固定数目的控制点；设计法向融合以保持输入形状贴合；在重建阶段加入对应感知损失与局部刚性损失提升还原质量；构建含高质量文本标注与变长3D动作的BIMO数据集；整体为前馈式框架BiMotion。

Result: 在多项评测上，BiMotion相较SOTA生成更具表现力、更高质量且与文本更一致，同时推理更快；可处理变长动作并保持连贯性。

Conclusion: 连续B样条表示与闭式压缩求解结合的前馈框架，在无需改动底层生成器的前提下，显著提升文本到3D动作的表达性、连贯性与效率；BIMO数据集支持训练与评测。

Abstract: Text-guided dynamic 3D character generation has advanced rapidly, yet producing high-quality motion that faithfully reflects rich textual descriptions remains challenging. Existing methods tend to generate limited sub-actions or incoherent motion due to fixed-length temporal inputs and discrete frame-wise representations that fail to capture rich motion semantics. We address these limitations by representing motion with continuous differentiable B-spline curves, enabling more effective motion generation without modifying the capabilities of the underlying generative model. Specifically, our closed-form, Laplacian-regularized B-spline solver efficiently compresses variable-length motion sequences into compact representations with a fixed number of control points. Further, we introduce a normal-fusion strategy for input shape adherence along with correspondence-aware and local-rigidity losses for motion-restoration quality. To train our model, we collate BIMO, a new dataset containing diverse variable-length 3D motion sequences with rich, high-quality text annotations. Extensive evaluations show that our feed-forward framework BiMotion generates more expressive, higher-quality, and better prompt-aligned motions than existing state-of-the-art methods, while also achieving faster generation. Our project page is at: https://wangmiaowei.github.io/BiMotion.github.io/.

</details>


### [51] [Structure-Level Disentangled Diffusion for Few-Shot Chinese Font Generation](https://arxiv.org/abs/2602.18874)
*Jie Li,Suorong Yang,Jian Zhao,Furao Shen*

Main category: cs.CV

TL;DR: 提出SLD-Font：把内容与风格分开到结构层面、用扩散模型双通道融合，并加入背景噪声去除与参数高效微调；在保内容的同时显著提升风格保真，并引入新指标评估内容质量。


<details>
  <summary>Details</summary>
Motivation: 少样本中文字体生成需要在极少参考图像下同时保证字符内容准确与风格忠实。现有方法多在特征层面解耦，生成器后续可能重新耦合，导致笔画结构被扭曲、风格劣化。需要一种更强的内容/风格解耦与稳健适配方案。

Method: - 提出结构级解耦扩散模型（SLD-Font）：
  1) 内容通道：将宋体（SimSun）内容模板与噪声潜变量拼接作为扩散输入，锁定字符结构；
  2) 风格通道：用CLIP从目标风格参考图提取风格特征，通过跨注意力注入到去噪网络；
  3) 像素域背景噪声移除模块，专注复杂笔画区域的背景清理；
  4) 基于可证明的解耦有效性分析，只微调风格相关模块（参数高效微调），提升风格适配、避免过拟合参考字符内容；
  5) 提出Grey与OCR两项指标评估内容质量。

Result: 与SOTA相比，SLD-Font在风格保真度上显著提升，同时内容准确度保持相当水平；新提出的Grey与OCR指标验证其内容质量。

Conclusion: 结构级双通道解耦与跨注意力结合扩散框架、配合像素级噪声去除与参数高效微调，可在少样本场景下显著提升中文字体风格迁移的保真度且不牺牲内容准确性；所提评测指标为内容评估提供了更可靠工具。

Abstract: Few-shot Chinese font generation aims to synthesize new characters in a target style using only a handful of reference images. Achieving accurate content rendering and faithful style transfer requires effective disentanglement between content and style. However, existing approaches achieve only feature-level disentanglement, allowing the generator to re-entangle these features, leading to content distortion and degraded style fidelity. We propose the Structure-Level Disentangled Diffusion Model (SLD-Font), which receives content and style information from two separate channels. SimSun-style images are used as content templates and concatenated with noisy latent features as the input. Style features extracted by a CLIP model from target-style images are integrated via cross-attention. Additionally, we train a Background Noise Removal module in the pixel space to remove background noise in complex stroke regions. Based on theoretical validation of disentanglement effectiveness, we introduce a parameter-efficient fine-tuning strategy that updates only the style-related modules. This allows the model to better adapt to new styles while avoiding overfitting to the reference images' content. We further introduce the Grey and OCR metrics to evaluate the content quality of generated characters. Experimental results show that SLD-Font achieves significantly higher style fidelity while maintaining comparable content accuracy to existing state-of-the-art methods.

</details>


### [52] [FOCA: Frequency-Oriented Cross-Domain Forgery Detection, Localization and Explanation via Multi-Modal Large Language Model](https://arxiv.org/abs/2602.18880)
*Zhou Liu,Tonghua Su,Hongshi Zhang,Fuxiang Yang,Donglin Di,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: 提出FOCA框架：用多模态大语言模型结合空间RGB与频域特征，通过跨注意力融合，实现更准的篡改检测与定位，并给出可解释的跨域说明；同时发布含像素级掩膜与双域标注的大规模数据集FSE-Set；实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测/定位方法过度依赖语义、忽视纹理与频域线索，且对细微低层篡改痕迹的可解释性不足；生成式伪造技术进步使媒体取证与信任面临挑战，亟需既精确又可解释的跨域方法与标准数据。

Method: 构建FOCA：在多模态LLM框架下，提取空间RGB域与频域的判别特征，通过跨注意力融合模块进行信息对齐与融合；输出包括篡改检测、像素级定位与跨域（空间/频域）可解释说明。并构建FSE-Set数据集，包含真实与伪造图像、像素级掩膜及空间/频域双域注释，用于训练与评测。

Result: 在多项基准与新建FSE-Set上，FOCA在检测准确率、定位性能与可解释性指标上均超过现有SOTA；能在空间与频域提供清晰、可人读的篡改依据。

Conclusion: 融合空间与频域特征并借助跨注意力与LLM的FOCA，实现高精度、强可解释的图像篡改检测与定位；FSE-Set为该方向提供了标准化评测与训练资源。

Abstract: Advances in image tampering techniques, particularly generative models, pose significant challenges to media verification, digital forensics, and public trust. Existing image forgery detection and localization (IFDL) methods suffer from two key limitations: over-reliance on semantic content while neglecting textural cues, and limited interpretability of subtle low-level tampering traces. To address these issues, we propose FOCA, a multimodal large language model-based framework that integrates discriminative features from both the RGB spatial and frequency domains via a cross-attention fusion module. This design enables accurate forgery detection and localization while providing explicit, human-interpretable cross-domain explanations. We further introduce FSE-Set, a large-scale dataset with diverse authentic and tampered images, pixel-level masks, and dual-domain annotations. Extensive experiments show that FOCA outperforms state-of-the-art methods in detection performance and interpretability across both spatial and frequency domains.

</details>


### [53] [SceneTok: A Compressed, Diffusable Token Space for 3D Scenes](https://arxiv.org/abs/2602.18882)
*Mohammad Asim,Christopher Wewer,Jan Eric Lenssen*

Main category: cs.CV

TL;DR: SceneTok 提出一种将多视角场景压缩为无结构、可扩散的少量置换不变token的新型分词器，借助轻量化rectified flow解码器实现高质量新视角渲染与快速场景生成，在1–3个数量级更强压缩下仍达SOTA重建质量，并具备鲁棒不确定性处理与偏离轨迹渲染能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景表示/生成多依赖体素、点云、三平面等网格化或视图对齐结构，存储与计算成本高、难以高效扩散建模；需要一种与空间网格解耦、可紧凑表示多视角信息、又能高质量渲染与快速生成的表示。

Method: 1) 多视角tokenizer：输入大量上下文视图，输出一小组置换不变的场景token（无空间栅格绑定、可扩散）。2) 轻量rectified flow解码器：从token条件生成并渲染任意新视角；能处理不确定性与偏离输入轨迹的相机路径。3) 端到端训练以实现高压缩与重建质量。

Result: 相较其他表示，压缩率提升1–3个数量级，同时达到SOTA级别的重建质量；支持从新轨迹渲染并在不确定场景下表现稳健；用于场景生成时约5秒即可生成，质量-速度权衡显著优于既有方法。

Conclusion: 将场景表示为少量无结构latent token是可行且高效的：既能实现更强压缩与高质量重建，又便于快速扩散式场景生成与灵活新视角渲染，显示出超越传统3D结构化表示的潜力。

Abstract: We present SceneTok, a novel tokenizer for encoding view sets of scenes into a compressed and diffusable set of unstructured tokens. Existing approaches for 3D scene representation and generation commonly use 3D data structures or view-aligned fields. In contrast, we introduce the first method that encodes scene information into a small set of permutation-invariant tokens that is disentangled from the spatial grid. The scene tokens are predicted by a multi-view tokenizer given many context views and rendered into novel views by employing a light-weight rectified flow decoder. We show that the compression is 1-3 orders of magnitude stronger than for other representations while still reaching state-of-the-art reconstruction quality. Further, our representation can be rendered from novel trajectories, including ones deviating from the input trajectory, and we show that the decoder gracefully handles uncertainty. Finally, the highly-compressed set of unstructured latent scene tokens enables simple and efficient scene generation in 5 seconds, achieving a much better quality-speed trade-off than previous paradigms.

</details>


### [54] [PhysConvex: Physics-Informed 3D Dynamic Convex Radiance Fields for Reconstruction and Simulation](https://arxiv.org/abs/2602.18886)
*Dan Wang,Xinrui Cui,Serge Belongie,Ravi Ramamoorthi*

Main category: cs.CV

TL;DR: PhysConvex提出一种物理约束的动态三维凸体辐射场，将渲染与物理模拟统一，用凸形元与连续介质力学建模可形变场，并以降阶动力学高效模拟复杂几何与异质材料，在视频监督下重建外观、几何与物理属性，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF/3DGS等神经表示虽在外观重建上强，但难以刻画复杂材料形变与动力学，导致视觉真实与物理一致性难兼得；需要一种既能高保真渲染又具物理一致性的动态三维表示与高效模拟框架。

Method: 1) 以物理可解释的凸体原语表示可形变辐射场，受连续介质力学约束；2) 提出边界驱动的动态凸表示，通过顶点与表面动力学刻画空间自适应、非均匀形变与可演化边界；3) 设计降阶凸体模拟：用神经skinning本征模态作为形状与材料相关的形变基，时间变化的低维自由度在牛顿动力学下推进，从而高效推进动态凸场；4) 凸体动力学提供紧致、无缝体积覆盖，兼顾几何效率与模拟保真。

Result: 在多组视频数据上，实现对几何、外观与物理属性的高保真重建；相较现有方法取得更高的重建质量与物理一致性。

Conclusion: PhysConvex将物理模拟与可微渲染统一到基于凸体的动态辐射场中，以降阶动力学高效处理复杂几何与异质材料，提供紧致无缝体积覆盖并显著提升重建与物理一致性，优于现有技术。

Abstract: Reconstructing and simulating dynamic 3D scenes with both visual realism and physical consistency remains a fundamental challenge. Existing neural representations, such as NeRFs and 3DGS, excel in appearance reconstruction but struggle to capture complex material deformation and dynamics. We propose PhysConvex, a Physics-informed 3D Dynamic Convex Radiance Field that unifies visual rendering and physical simulation. PhysConvex represents deformable radiance fields using physically grounded convex primitives governed by continuum mechanics. We introduce a boundary-driven dynamic convex representation that models deformation through vertex and surface dynamics, capturing spatially adaptive, non-uniform deformation, and evolving boundaries. To efficiently simulate complex geometries and heterogeneous materials, we further develop a reduced-order convex simulation that advects dynamic convex fields using neural skinning eigenmodes as shape- and material-aware deformation bases with time-varying reduced DOFs under Newtonian dynamics. Convex dynamics also offers compact, gap-free volumetric coverage, enhancing both geometric efficiency and simulation fidelity. Experiments demonstrate that PhysConvex achieves high-fidelity reconstruction of geometry, appearance, and physical properties from videos, outperforming existing methods.

</details>


### [55] [SafeDrive: Fine-Grained Safety Reasoning for End-to-End Driving in a Sparse World](https://arxiv.org/abs/2602.18887)
*Jungho Kim,Jiyong Oh,Seunghoon Yu,Hongjae Shin,Donghyuk Kwak,Jun Won Choi*

Main category: cs.CV

TL;DR: SafeDrive 是一种端到端自动驾驶规划框架，通过“轨迹条件化稀疏世界模型”进行可解释的安全推理，包含SWNet与FRNet两个互补网络，在开放/闭环基准上达SOTA并显著降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 端到端(E2E)自动驾驶具有统一建模与可扩展性，但在单一框架内保证安全仍是关键难题。现有方法难以显式、可解释地评估未来交互与风险，因此需要能对关键体素/体代理进行未来建模并进行细粒度安全评估的方案。

Method: 提出SafeDrive，由两部分组成：1) Sparse World Network (SWNet)：依据候选自车轨迹构建“轨迹条件化稀疏世界”，聚焦关键动态体与道路元素，模拟其未来行为，形成交互为中心的稀疏表示；2) Fine-grained Reasoning Network (FRNet)：在该表示上评估各代理的碰撞风险与对可行驶区域的时间一致性，逐时刻定位潜在安全关键事件。整体实现显式、可解释的安全推理用于规划。

Result: 在NAVSIM上取得PDMS 91.6、EPDMS 87.5，仅61/12146场景碰撞(0.5%)；在Bench2Drive上获得66.8% driving score，达到开环与闭环SOTA表现。

Conclusion: 通过“稀疏世界+细粒度推理”的双网络，SafeDrive在保持E2E统一性的同时实现可解释的安全评估，显著提升规划安全性与基准成绩，显示该范式在实际部署中的潜力。

Abstract: The end-to-end (E2E) paradigm, which maps sensor inputs directly to driving decisions, has recently attracted significant attention due to its unified modeling capability and scalability. However, ensuring safety in this unified framework remains one of the most critical challenges. In this work, we propose SafeDrive, an E2E planning framework designed to perform explicit and interpretable safety reasoning through a trajectory-conditioned Sparse World Model. SafeDrive comprises two complementary networks: the Sparse World Network (SWNet) and the Fine-grained Reasoning Network (FRNet). SWNet constructs trajectory-conditioned sparse worlds that simulate the future behaviors of critical dynamic agents and road entities, providing interaction-centric representations for downstream reasoning. FRNet then evaluates agent-specific collision risks and temporal adherence to drivable regions, enabling precise identification of safety-critical events across future timesteps. SafeDrive achieves state-of-the-art performance on both open-loop and closed-loop benchmarks. On NAVSIM, it records a PDMS of 91.6 and an EPDMS of 87.5, with only 61 collisions out of 12,146 scenarios (0.5%). On Bench2Drive, SafeDrive attains a 66.8% driving score.

</details>


### [56] [Beyond Stationarity: Rethinking Codebook Collapse in Vector Quantization](https://arxiv.org/abs/2602.18896)
*Hao Lu,Onur C. Koyun,Yongxin Guo,Zhengjie Zhu,Abbas Alili,Metin Nafi Gurcan*

Main category: cs.CV

TL;DR: 论文解释VQ代码本坍塌根因在于编码器更新的非平稳性，并提出NSVQ与TransVQ两法将编码器漂移传播到未被选中的码或整体自适应变换码本，从而几乎消除坍塌并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: VQ类生成模型（VQ-VAE、VQ-GAN、潜变量扩散）广泛使用，但训练中大量码向量长期未被选中导致码本坍塌，限制表示能力与重建质量。现有解释与缓解策略不足以系统解决该问题，需要从原理上找出根因并提出稳定、可扩展的方法。

Method: 1) 理论：将编码器参数随训练迭代发生“漂移”建模为非平稳过程，证明未被选中码因得不到更新而逐步失活。2) NSVQ：基于核的传播规则，把编码器漂移对选中码的影响扩散到未选中码，使其得到间接更新。3) TransVQ：引入轻量Transformer样式的全局映射，对整个码本进行自适应变换，同时保证收敛到k-means解的性质不被破坏。

Result: 在CelebA-HQ上，两方法均实现接近完全的码本利用率，并在重建质量上优于基线VQ变体。

Conclusion: 编码器非平稳性是码本坍塌的关键原因。通过将编码器漂移显式传播到未被选中码或全局自适应变换码本，可在不牺牲k-means收敛特性的前提下显著提升码本利用和重建质量，为VQ式生成模型提供更稳健的训练框架。

Abstract: Vector Quantization (VQ) underpins many modern generative frameworks such as VQ-VAE, VQ-GAN, and latent diffusion models. Yet, it suffers from the persistent problem of codebook collapse, where a large fraction of code vectors remains unused during training. This work provides a new theoretical explanation by identifying the nonstationary nature of encoder updates as the fundamental cause of this phenomenon. We show that as the encoder drifts, unselected code vectors fail to receive updates and gradually become inactive. To address this, we propose two new methods: Non-Stationary Vector Quantization (NSVQ), which propagates encoder drift to non-selected codes through a kernel-based rule, and Transformer-based Vector Quantization (TransVQ), which employs a lightweight mapping to adaptively transform the entire codebook while preserving convergence to the k-means solution. Experiments on the CelebA-HQ dataset demonstrate that both methods achieve near-complete codebook utilization and superior reconstruction quality compared to baseline VQ variants, providing a principled and scalable foundation for future VQ-based generative models. The code is available at: https://github.com/CAIR- LAB- WFUSM/NSVQ-TransVQ.git

</details>


### [57] [SCHEMA for Gemini 3 Pro Image: A Structured Methodology for Controlled AI Image Generation on Google's Native Multimodal Model](https://arxiv.org/abs/2602.18903)
*Luca Cazzaniga*

Main category: cs.CV

TL;DR: 论文提出SCHEMA：面向Google Gemini 3 Pro Image的结构化提示工程方法，基于850次经验证的API预测与约4800张生成图像，跨六大专业场景构建。方法包含三层级控制体系、7+5模块化标签、带分流规则的决策树、以及针对模型局限的系统性补救。实验显示在621条结构化提示中，强制要求合规率91%、禁令合规率94%，批量一致性显著提升；独立实践者验证(n=40)与信息设计专项验证均显示高首轮合规（>95%）。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程多为通用或模型无关的经验性建议，难以在专业生产中稳定复用与控制输出；作者希望为特定模型（Gemini 3 Pro Image）提供可工程化、可验证、可扩展的结构化方法，提高跨场景一致性与合规性。

Method: - 构建三层渐进控制体系：BASE（探索~5%控制）、MEDIO、中级、AVANZATO（指令式~95%控制）。- 设计7个核心与5个可选的模块化标签组件，形成结构化提示模板。- 制定带显式路由规则的决策树，必要时转用替代工具。- 系统记录模型限制并提供相应变通策略。- 基于621条结构化提示、850次API验证与约4800张图像进行评估；开展批量一致性对比、独立从业者验证(n=40)与信息设计专项验证（~300信息图，空间与版式控制）。

Result: - 强制要求（Mandatory）合规率91%，禁令（Prohibitions）合规率94%。- 结构化提示在批量生成一致性上优于非结构化对照。- 独立从业者验证支持方法有效性。- 信息设计任务中，首轮>95%满足空间与字体排版控制，且结果可公开核验。

Conclusion: 面向特定视觉生成模型的结构化、分层与模块化提示工程可显著提升指令遵循、风格与版面一致性，以及专业生产可复用性；SCHEMA为跨多行业的可操作框架，并通过公开验证与实证数据支撑其实用价值。

Abstract: This paper presents SCHEMA (Structured Components for Harmonized Engineered Modular Architecture), a structured prompt engineering methodology specifically developed for Google Gemini 3 Pro Image. Unlike generic prompt guidelines or model-agnostic tips, SCHEMA is an engineered framework built on systematic professional practice encompassing 850 verified API predictions within an estimated corpus of approximately 4,800 generated images, spanning six professional domains: real estate photography, commercial product photography, editorial content, storyboards, commercial campaigns, and information design. The methodology introduces a three-tier progressive system (BASE, MEDIO, AVANZATO) that scales practitioner control from exploratory (approximately 5%) to directive (approximately 95%), a modular label architecture with 7 core and 5 optional structured components, a decision tree with explicit routing rules to alternative tools, and systematically documented model limitations with corresponding workarounds. Key findings include an observed 91% Mandatory compliance rate and 94% Prohibitions compliance rate across 621 structured prompts, a comparative batch consistency test demonstrating substantially higher inter-generation coherence for structured prompts, independent practitioner validation (n=40), and a dedicated Information Design validation demonstrating >95% first-generation compliance for spatial and typographical control across approximately 300 publicly verifiable infographics. Previously published on Zenodo (doi:10.5281/zenodo.18721380).

</details>


### [58] [Marginalized Bundle Adjustment: Multi-View Camera Pose from Monocular Depth Estimates](https://arxiv.org/abs/2602.18906)
*Shengjie Zhu,Ahmed Abdelkader,Mark J. Matthews,Xiaoming Liu,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 提出“边缘化捆绑调整（MBA）”以将单目深度估计（MDE）的稠密但高方差深度引入SfM，利用RANSAC风格的边缘化策略降低误差方差，在小到大规模多视图与重定位中达成SOTA或竞品表现。


<details>
  <summary>Details</summary>
Motivation: MDE能从单张图像预测稠密深度，但误差方差高且与经典SfM基于三角化的稀疏点云不匹配；如何将MDE有效整合进SfM、并在不同规模场景中稳健发挥，是当前缺口。

Method: 受现代RANSAC启发，提出Marginalized Bundle Adjustment：在BA中对高方差的MDE深度信息进行边缘化/加权处理，利用其稠密性统计降低噪声影响，从而同时优化相机位姿与场景，缓解MDE误差传播。

Result: 用MDE深度图即可在SfM与相机重定位任务上取得SOTA或具竞争力的精度与稳健性；在从少帧到上千张图的大规模多视图系统上均表现稳定、鲁棒。

Conclusion: MDE虽然噪声较大，但只要通过MBA正确建模与利用其稠密性，足以支撑高质量SfM与重定位，展现MDE在多视图3D视觉中的重要潜力。

Abstract: Structure-from-Motion (SfM) is a fundamental 3D vision task for recovering camera parameters and scene geometry from multi-view images. While recent deep learning advances enable accurate Monocular Depth Estimation (MDE) from single images without depending on camera motion, integrating MDE into SfM remains a challenge. Unlike conventional triangulated sparse point clouds, MDE produces dense depth maps with significantly higher error variance. Inspired by modern RANSAC estimators, we propose Marginalized Bundle Adjustment (MBA) to mitigate MDE error variance leveraging its density. With MBA, we show that MDE depth maps are sufficiently accurate to yield SoTA or competitive results in SfM and camera relocalization tasks. Through extensive evaluations, we demonstrate consistently robust performance across varying scales, ranging from few-frame setups to large multi-view systems with thousands of images. Our method highlights the significant potential of MDE in multi-view 3D vision.

</details>


### [59] [CRAFT-LoRA: Content-Style Personalization via Rank-Constrained Adaptation and Training-Free Fusion](https://arxiv.org/abs/2602.18936)
*Yu Li,Yujun Cai,Chi Zhang*

Main category: cs.CV

TL;DR: 提出CRAFT-LoRA，通过结构化解耦与无训练推理指导，实现个性化文生图中内容与风格的更好平衡、可控组合与稳定生成。


<details>
  <summary>Details</summary>
Motivation: LoRA能高效个性化并通过组合权重实现精细控制，但现有方法存在内容与风格纠缠、缺乏对元素影响的可控指引、以及权重融合不稳定常需额外训练的问题。

Method: CRAFT-LoRA包含三部分：1) 排秩约束的主干微调，在骨干中注入低秩投影残差，鼓励学习解耦的内容/风格子空间；2) 基于提示的专家编码器，含专门分支，通过选择性适配器聚合实现语义扩展与精确控制；3) 训练-free、随时间步变化的CFG方案，在扩散步中策略性调整噪声预测以提升稳定性。

Result: 显著提升内容-风格解耦，支持灵活的LoRA模块组合语义控制，并在无需额外再训练的情况下实现高保真生成与更稳定的合成过程。

Conclusion: 通过结构解耦的训练与时间步敏感的无训练指导，CRAFT-LoRA在个性化图像生成中兼顾保真度、风格一致性与可控性，解决了以往LoRA组合的纠缠与不稳定难题。

Abstract: Personalized image generation requires effectively balancing content fidelity with stylistic consistency when synthesizing images based on text and reference examples. Low-Rank Adaptation (LoRA) offers an efficient personalization approach, with potential for precise control through combining LoRA weights on different concepts. However, existing combination techniques face persistent challenges: entanglement between content and style representations, insufficient guidance for controlling elements' influence, and unstable weight fusion that often require additional training. We address these limitations through CRAFT-LoRA, with complementary components: (1) rank-constrained backbone fine-tuning that injects low-rank projection residuals to encourage learning decoupled content and style subspaces; (2) a prompt-guided approach featuring an expert encoder with specialized branches that enables semantic extension and precise control through selective adapter aggregation; and (3) a training-free, timestep-dependent classifier-free guidance scheme that enhances generation stability by strategically adjusting noise predictions across diffusion steps. Our method significantly improves content-style disentanglement, enables flexible semantic control over LoRA module combinations, and achieves high-fidelity generation without additional retraining overhead.

</details>


### [60] [Global Commander and Local Operative: A Dual-Agent Framework for Scene Navigation](https://arxiv.org/abs/2602.18941)
*Kaiming Jin,Yuefan Wu,Shengqiong Wu,Bobo Li,Shuicheng Yan,Tat-Seng Chua*

Main category: cs.CV

TL;DR: DACo提出一种“规划-落地解耦”的视觉语言导航框架：由全局指挥者做高层规划、局部执行者做感知与动作，用动态子目标与自适应重规划提升长程稳定性；在R2R/REVERIE/R4R零样本上较最佳基线分别提升4.9/6.5/5.4个百分点，并适配闭源与开源大模型。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体方法成本高、协调复杂；单智能体既要全局规划又要局部感知，易在长程任务中出现推理退化与指令漂移。需要一种既降低认知负载又保持长程鲁棒性的范式。

Method: 提出DACo解耦式架构：全局指挥者（Global Commander）负责高层策略与子目标规划；本地执行者（Local Operative）负责自我中心感知与精细执行。配合动态子目标规划与自适应重规划机制，形成结构化且可恢复的导航流程；支持不同模型骨干（GPT-4o、Qwen-VL等）。

Result: 在R2R、REVERIE、R4R零样本设定下，相比最佳基线分别提升4.9%、6.5%、5.4%绝对值；在闭源与开源视觉语言模型上均具备良好泛化。

Conclusion: 解耦全局推理与局部落地能显著缓解单体过载与长程不稳定，DACo提供了可扩展、稳健的长程视觉语言导航新范式。

Abstract: Vision-and-Language Scene navigation is a fundamental capability for embodied human-AI collaboration, requiring agents to follow natural language instructions to execute coherent action sequences in complex environments. Existing approaches either rely on multiple agents, incurring high coordination and resource costs, or adopt a single-agent paradigm, which overloads the agent with both global planning and local perception, often leading to degraded reasoning and instruction drift in long-horizon settings. To address these issues, we introduce DACo, a planning-grounding decoupled architecture that disentangles global deliberation from local grounding. Concretely, it employs a Global Commander for high-level strategic planning and a Local Operative for egocentric observing and fine-grained execution. By disentangling global reasoning from local action, DACo alleviates cognitive overload and improves long-horizon stability. The framework further integrates dynamic subgoal planning and adaptive replanning to enable structured and resilient navigation. Extensive evaluations on R2R, REVERIE, and R4R demonstrate that DACo achieves 4.9%, 6.5%, 5.4% absolute improvements over the best-performing baselines in zero-shot settings, and generalizes effectively across both closed-source (e.g., GPT-4o) and open-source (e.g., Qwen-VL Series) backbones. DACo provides a principled and extensible paradigm for robust long-horizon navigation. Project page: https://github.com/ChocoWu/DACo

</details>


### [61] [YOLOv10-Based Multi-Task Framework for Hand Localization and Laterality Classification in Surgical Videos](https://arxiv.org/abs/2602.18959)
*Kedi Sun,Le Zhang*

Main category: cs.CV

TL;DR: 基于YOLOv10的多任务模型在创伤外科术中视频中实时定位双手并判别左右手，mAP[0.5:0.95]=0.33，左右手分类准确率约67%/71%，具备实时性但前景-背景区分仍具挑战，奠定后续手-器械交互分析基础。


<details>
  <summary>Details</summary>
Motivation: 创伤手术场景需要对术者/助手的手进行高频、低延迟的感知，以辅助快速精准的术中决策与后续人机协作；现有方法在复杂光照、运动模糊及视角变化下鲁棒性与时延难以兼顾，且鲜有同时进行手定位与左右手判别的方案。

Method: 以YOLOv10为主干，构建多任务检测框架：在手部边界框检测的同时加入左右手判别分支；利用Trauma THOMPSON Challenge 2025 Task 2带标注的第一视角手术视频训练；通过大规模数据增强（应对运动模糊、光照变化与多样外观）提升鲁棒性；保持实时推理以满足术中部署。

Result: 在挑战数据上，左右手分类准确率分别为左手67%、右手71%；整体检测性能mAP[0.5:0.95]为0.33；能够实时推理，但将手与背景区分仍存在困难。

Conclusion: 所提框架可在创伤外科复杂术中场景中实现实时手部检测与左右手判别，尽管检测精度仍有限，但已具备临床实时应用潜力，并为进一步的手-器械交互分析与更高级的术中感知系统奠定基础。

Abstract: Real-time hand tracking in trauma surgery is essential for supporting rapid and precise intraoperative decisions. We propose a YOLOv10-based framework that simultaneously localizes hands and classifies their laterality (left or right) in complex surgical scenes. The model is trained on the Trauma THOMPSON Challenge 2025 Task 2 dataset, consisting of first-person surgical videos with annotated hand bounding boxes. Extensive data augmentation and a multi-task detection design improve robustness against motion blur, lighting variations, and diverse hand appearances. Evaluation demonstrates accurate left-hand (67\%) and right-hand (71\%) classification, while distinguishing hands from the background remains challenging. The model achieves an $mAP_{[0.5:0.95]}$ of 0.33 and maintains real-time inference, highlighting its potential for intraoperative deployment. This work establishes a foundation for advanced hand-instrument interaction analysis in emergency surgical procedures.

</details>


### [62] [Depth-Enhanced YOLO-SAM2 Detection for Reliable Ballast Insufficiency Identification](https://arxiv.org/abs/2602.18961)
*Shiyu Liu,Dylan Lester,Husnu Narman,Ammar Alzarrad,Pingping Zhu*

Main category: cs.CV

TL;DR: 提出一种结合深度信息的YOLO‑SAM2框架，用于铁路道砟不足检测；通过对RealSense深度进行校正并配合分割与几何分析，显著提高召回率与F1，适用于安全关键与视觉模糊场景。


<details>
  <summary>Details</summary>
Motivation: 仅用RGB的YOLOv8尽管定位可靠，但在“道砟不足”这一安全关键缺陷上召回很低（0.49），模型倾向过判“充足”，导致漏检；为提升安全与鲁棒性，需要利用深度几何信息并纠正传感器失真。

Method: 在YOLOv8检测基础上，引入深度增强：1) 枕木对齐的深度校正流程（多项式建模+RANSAC+时间平滑）以补偿RealSense空间畸变；2) 采用SAM2生成ROI并细化掩膜；3) 提取枕木与道砟剖面进行几何判别；4) 比较AABB与RBB采样策略及不同几何阈值。

Result: 在实地采集的俯视RGB‑D数据上，深度增强方案将召回从0.49提升至最高0.80，F1从0.66提升到>0.80，且在视觉模糊/安全关键情况下表现更稳健。

Conclusion: 将深度校正与YOLO‑SAM2联合，可更可靠地自动检测道砟不足；几何信息弥补RGB偏差，显著提升召回与整体性能，适用于安全要求高的轨道巡检。

Abstract: This paper presents a depth-enhanced YOLO-SAM2 framework for detecting ballast insufficiency in railway tracks using RGB-D data. Although YOLOv8 provides reliable localization, the RGB-only model shows limited safety performance, achieving high precision (0.99) but low recall (0.49) due to insufficient ballast, as it tends to over-predict the sufficient class. To improve reliability, we incorporate depth-based geometric analysis enabled by a sleeper-aligned depth-correction pipeline that compensates for RealSense spatial distortion using polynomial modeling, RANSAC, and temporal smoothing. SAM2 segmentation further refines region-of-interest masks, enabling accurate extraction of sleeper and ballast profiles for geometric classification.
  Experiments on field-collected top-down RGB-D data show that depth-enhanced configurations substantially improve the detection of insufficient ballast. Depending on bounding-box sampling (AABB or RBB) and geometric criteria, recall increases from 0.49 to as high as 0.80, and F1-score improves from 0.66 to over 0.80. These results demonstrate that integrating depth correction with YOLO-SAM2 yields a more robust and reliable approach for automated railway ballast inspection, particularly in visually ambiguous or safety-critical scenarios.

</details>


### [63] [Face Presentation Attack Detection via Content-Adaptive Spatial Operators](https://arxiv.org/abs/2602.18965)
*Shujaat Khan*

Main category: cs.CV

TL;DR: 提出CASO-PAD：在MobileNetV3中引入内容自适应空间算子（involution），实现仅RGB、单帧的人脸攻击检测，在多数据集上以极低开销取得SOTA级准确性与鲁棒性，适合移动端部署。


<details>
  <summary>Details</summary>
Motivation: 现有FacePAD常依赖多模态/时序或高算力模型，但移动端实用性差；传统卷积空间共享限制了对局部伪造线索（打印纹理、反射、边缘瑕疵等）的选择性建模，难以在轻量条件下兼顾精度与效率。

Method: 在MobileNetV3骨干中插入内容自适应空间算子（involution）：由输入动态生成位置特异、通道共享的核，以增强空间选择性、降低参数与计算量；采用适度分组共享，并将该算子放置于网络靠近head位置；端到端二分类（BCE）训练，单帧RGB输入，无辅助传感器与时序堆叠。

Result: 在Replay-Attack/Replay-Mobile/ROSE-Youtu/OULU-NPU上分别取得Accuracy 100/100/98.9/99.7%、AUC 1.00/1.00/0.9995/0.9999、HTER 0.00/0.00/0.82/0.44%；在SiW-Mv2 Protocol-1上Accuracy 95.45%、HTER 3.11%、EER 3.13%，显示在大规模多样化攻击下的鲁棒性。消融表明：将自适应算子放在网络head附近、采用中等分组共享在精度-效率上最优。

Conclusion: CASO-PAD通过轻量级involution增强MobileNetV3，对局部伪造线索更敏感；在多基准上达高精度与低误差，计算和参数量小，适合移动端实时部署，为无辅助传感器、单帧RGB的FacePAD提供实用方案。

Abstract: Face presentation attack detection (FacePAD) is critical for securing facial authentication against print, replay, and mask-based spoofing. This paper proposes CASO-PAD, an RGB-only, single-frame model that enhances MobileNetV3 with content-adaptive spatial operators (involution) to better capture localized spoof cues. Unlike spatially shared convolution kernels, the proposed operator generates location-specific, channel-shared kernels conditioned on the input, improving spatial selectivity with minimal overhead. CASO-PAD remains lightweight (3.6M parameters; 0.64 GFLOPs at $256\times256$) and is trained end-to-end using a standard binary cross-entropy objective. Extensive experiments on Replay-Attack, Replay-Mobile, ROSE-Youtu, and OULU-NPU demonstrate strong performance, achieving 100/100/98.9/99.7\% test accuracy, AUC of 1.00/1.00/0.9995/0.9999, and HTER of 0.00/0.00/0.82/0.44\%, respectively. On the large-scale SiW-Mv2 Protocol-1 benchmark, CASO-PAD further attains 95.45\% accuracy with 3.11\% HTER and 3.13\% EER, indicating improved robustness under diverse real-world attacks. Ablation studies show that placing the adaptive operator near the network head and using moderate group sharing yields the best accuracy--efficiency balance. Overall, CASO-PAD provides a practical pathway for robust, on-device FacePAD with mobile-class compute and without auxiliary sensors or temporal stacks.

</details>


### [64] [Frame2Freq: Spectral Adapters for Fine-Grained Video Understanding](https://arxiv.org/abs/2602.18977)
*Thinesh Thiyakesan Ponbagavathi,Constantin Seibold,Alina Roitberg*

Main category: cs.CV

TL;DR: 提出Frame2Freq频率感知适配器，在图像预训练VFM向视频迁移时进行时域FFT并学习频段嵌入，以覆盖多时间尺度动态，从而显著提升细粒度动作识别，超过现有PEFT并在多数数据集上超越全参微调。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频的适配器多在单一时间尺度工作，容易偏向静态线索与极快闪烁，忽略中速运动，难以处理如开瓶/关瓶这类细粒度时序差异。需要能覆盖多时间尺度的动态建模方法。

Method: 设计Frame2Freq：沿时间轴对特征做FFT，分频带学习可训练的频段特定嵌入/门控，突出判别性频率范围；作为轻量PEFT适配器插入到图像预训练的视觉基础模型中，实现频率感知的图像到视频迁移。

Result: 在5个细粒度活动识别数据集上，Frame2Freq全面优于既有PEFT方法，并在其中4个数据集上超过全参微调的性能。

Conclusion: 频域分析（FFT与频带选择）在图像到视频迁移中有效捕捉多时间尺度动态，可显著提升细粒度动作识别，是建模视频时序的有力工具。

Abstract: Adapting image-pretrained backbones to video typically relies on time-domain adapters tuned to a single temporal scale. Our experiments show that these modules pick up static image cues and very fast flicker changes, while overlooking medium-speed motion. Capturing dynamics across multiple time-scales is, however, crucial for fine-grained temporal analysis (i.e., opening vs. closing bottle).
  To address this, we introduce Frame2Freq -- a family of frequency-aware adapters that perform spectral encoding during image-to-video adaptation of pretrained Vision Foundation Models (VFMs), improving fine-grained action recognition. Frame2Freq uses Fast Fourier Transform (FFT) along time and learns frequency-band specific embeddings that adaptively highlight the most discriminative frequency ranges. Across five fine-grained activity recognition datasets, Frame2Freq outperforms prior PEFT methods and even surpasses fully fine-tuned models on four of them. These results provide encouraging evidence that frequency analysis methods are a powerful tool for modeling temporal dynamics in image-to-video transfer. Code is available at https://github.com/th-nesh/Frame2Freq.

</details>


### [65] [IDSelect: A RL-Based Cost-Aware Selection Agent for Video-based Multi-Modal Person Recognition](https://arxiv.org/abs/2602.18990)
*Yuyang Ji,Yixuan Shen,Kien Nguyen,Lifeng Zhou,Feng Liu*

Main category: cs.CV

TL;DR: 提出IDSelect：一个基于强化学习、按序列选择每模态单一预训练模型的成本感知选择器，实现视频行人识别在精度与计算之间的自适应权衡，并在CCVID/MEVID上以大幅降算力的同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频行人识别常用人脸、身体、步态多模态重型集成，但对所有输入一刀切，导致在简单样本上浪费计算、在复杂样本上组合不最优，难以兼顾效率与精度。

Method: 训练一个轻量级代理，采用actor-critic强化学习并引入预算/成本感知优化与熵正则。每个视频序列、每个模态从一组预训练模型中选择一个，推理时按模态选择概率最高的模型并计算相似度，随后进行多模态融合得到最终识别分数。奖励函数平衡识别准确率与计算成本，熵正则防止策略过早收敛。

Result: 在CCVID上，达到95.9% Rank-1，同时较强基线减少92.4%计算并提升1.8%精度；在MEVID上减少41.3%计算并保持具有竞争力的性能。

Conclusion: 输入自适应的跨模态模型选择可以在不牺牲、甚至提升识别精度的同时显著节省算力；IDSelect为多模态视频识别提供了一种有效的精度-效率权衡范式。

Abstract: Video-based person recognition achieves robust identification by integrating face, body, and gait. However, current systems waste computational resources by processing all modalities with fixed heavyweight ensembles regardless of input complexity. To address these limitations, we propose IDSelect, a reinforcement learning-based cost-aware selector that chooses one pre-trained model per modality per-sequence to optimize the accuracy-efficiency trade-off. Our key insight is that an input-conditioned selector can discover complementary model choices that surpass fixed ensembles while using substantially fewer resources. IDSelect trains a lightweight agent end-to-end using actor-critic reinforcement learning with budget-aware optimization. The reward balances recognition accuracy with computational cost, while entropy regularization prevents premature convergence. At inference, the policy selects the most probable model per modality and fuses modality-specific similarities for the final score. Extensive experiments on challenging video-based datasets demonstrate IDSelect's superior efficiency: on CCVID, it achieves 95.9% Rank-1 accuracy with 92.4% less computation than strong baselines while improving accuracy by 1.8%; on MEVID, it reduces computation by 41.3% while maintaining competitive performance.

</details>


### [66] [SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models](https://arxiv.org/abs/2602.18993)
*Jiwoo Chung,Sangeek Hyun,MinKyu Lee,Byeongju Han,Geonho Cha,Dongyoon Wee,Youngjun Hong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出SeaCache：基于频谱演化感知的缓存策略，在不训练的情况下用频谱对齐的特征决定何时复用扩散模型步的中间结果，实现更快采样且保持质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推理慢主要因逐步去噪。现有加速通过相邻步特征距离判定可复用性，但使用原始特征会把内容与噪声混在一起，忽视扩散过程“先低频结构、后高频细节”的频谱演化规律，导致缓存决策不稳与质量损失。

Method: 提出Spectral-Evolution-Aware（SEA）滤波器，从理论与实证分析出发，构造能保留与内容相关、抑制噪声成分的频谱对齐表示；用SEA滤波后的输入特征评估相邻步的冗余度，得到可按内容动态调整、并符合扩散频谱先验的缓存复用调度（SeaCache），无需额外训练，可插拔应用于多种视觉生成扩散模型。

Result: 在多种视觉生成模型和既有基线下进行大量实验，SeaCache在相同或更低延迟下达到更高生成质量，或在同质量下显著降低采样步延迟，达成当前最优的时延-质量折中。

Conclusion: 基于频谱演化的特征对齐能更可靠地区分内容与噪声，从而指导缓存复用；SeaCache作为训练无关、内容自适应的缓存调度，在加速扩散采样的同时保持高图像质量，并优于现有缓存策略。

Abstract: Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.

</details>


### [67] [Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction](https://arxiv.org/abs/2602.18996)
*Shannan Yan,Leqi Zheng,Keyu Lv,Jingchen Ni,Hongyang Wei,Jiajun Zhang,Guangting Wang,Jing Lyu,Chun Yuan,Fengyun Rao*

Main category: cs.CV

TL;DR: 提出一种基于条件二值分割与循环一致性的对象级跨视角视频对应方法，实现无需标注的自监督训练与推理阶段TTT，并在Ego-Exo4D与HANDAL-X上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 跨视角（尤其是主观-客观、客观-主观）视频中同一对象的对应极具挑战，现有方法常依赖标注或缺乏视角不变的稳健表示，因此需要一种无需标注、能学习视角不变特征并支持推理时自适应的框架。

Method: 以条件二值分割为核心：将源视角中的对象查询掩码编码为潜在表示，用于引导目标视频中对应对象的定位；设计循环一致性目标：把在目标视角预测的掩码投影回源视角以重构原始查询掩码；该双向约束作为强自监督信号，并支持测试时训练（TTT）在推理阶段自适应优化。

Result: 在Ego-Exo4D与HANDAL-X基准上取得最新SOTA，实验证明所提出的优化目标与TTT策略有效提升跨视角对象对应性能。

Conclusion: 基于条件分割与循环一致性的简单框架可在无标注下学习视角不变表示，并在推理时通过TTT进一步提升泛化，显著推进主客观跨视角对象对应任务的表现。

Abstract: We study the task of establishing object-level visual correspondence across different viewpoints in videos, focusing on the challenging egocentric-to-exocentric and exocentric-to-egocentric scenarios. We propose a simple yet effective framework based on conditional binary segmentation, where an object query mask is encoded into a latent representation to guide the localization of the corresponding object in a target video. To encourage robust, view-invariant representations, we introduce a cycle-consistency training objective: the predicted mask in the target view is projected back to the source view to reconstruct the original query mask. This bidirectional constraint provides a strong self-supervisory signal without requiring ground-truth annotations and enables test-time training (TTT) at inference. Experiments on the Ego-Exo4D and HANDAL-X benchmarks demonstrate the effectiveness of our optimization objective and TTT strategy, achieving state-of-the-art performance. The code is available at https://github.com/shannany0606/CCMP.

</details>


### [68] [A Benchmark and Knowledge-Grounded Framework for Advanced Multimodal Personalization Study](https://arxiv.org/abs/2602.19001)
*Xia Hu,Honglei Zhuang,Brian Potetz,Alireza Fathi,Bo Hu,Babak Samari,Howard Zhou*

Main category: cs.CV

TL;DR: 提出Life-Bench与LifeGraph，填补个性化多模态评测空白，显示现有模型在关系/时间/聚合推理上有显著短板，结构化知识有助改进但难题仍在。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在推理与个性化场景中潜力巨大，但缺乏合适的基准导致研究受阻，需要能覆盖多模态、个体长时历史与复杂推理的系统性评测工具。

Method: 构建Life-Bench：基于模拟用户数字足迹的合成多模态基准，涵盖从人格理解到对历史数据进行复杂推理的多类任务；提出LifeGraph：将个人上下文组织为知识图谱以支持结构化检索与推理的端到端框架；在基准上系统评测现有方法与所提框架。

Result: 在Life-Bench上，现有方法在复杂个性化任务上显著失分，尤其在关系、时间与聚合推理方面存在大性能缺口；LifeGraph通过结构化知识部分缩小差距，性能优于现有方法。

Conclusion: 高级个性化推理仍是开放挑战。基于知识图谱的结构化检索与推理是有前景的方向，但仍需进一步研究以弥补在关系、时间与聚合推理上的不足。

Abstract: The powerful reasoning of modern Vision Language Models open a new frontier for advanced personalization study. However, progress in this area is critically hampered by the lack of suitable benchmarks. To address this gap, we introduce Life-Bench, a comprehensive, synthetically generated multimodal benchmark built on simulated user digital footprints. Life-Bench features over questions evaluating a wide spectrum of capabilities, from persona understanding to complex reasoning over historical data. These capabilities expand far beyond prior benchmarks, reflecting the critical demands essential for real-world applications. Furthermore, we propose LifeGraph, an end-to-end framework that organizes personal context into a knowledge graph to facilitate structured retrieval and reasoning. Our experiments on Life-Bench reveal that existing methods falter significantly on complex personalized tasks, exposing a large performance headroom, especially in relational, temporal and aggregative reasoning. While LifeGraph closes this gap by leveraging structured knowledge and demonstrates a promising direction, these advanced personalization tasks remain a critical open challenge, motivating new research in this area.

</details>


### [69] [MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment](https://arxiv.org/abs/2602.19004)
*Duc Duy Nguyen,Tat-Jun Chin,Minh Hoai*

Main category: cs.CV

TL;DR: MoBind提出分层对比学习，将IMU与视频骨架2D姿态对齐，支持跨模态检索、时间同步、主体/部位定位与动作识别，并在多数据集上领先。


<details>
  <summary>Details</summary>
Motivation: IMU与视频信息互补，但存在背景干扰、多IMU传感器结构化关系难建模，以及需要亚秒级精细时间对齐的问题。现有方法难以同时兼顾语义一致性与细粒度时序对应。

Method: 用骨架序列替代原始像素，与IMU进行表示对齐；将全身动作分解为局部身体部位轨迹，并与对应IMU成对对齐，实现语义绑定；采用分层对比学习：先进行token级时间段对齐，再融合局部（部位）与全局（全身）对齐，获得细粒度到粗粒度的一致表示。

Result: 在mRi、TotalCapture、EgoHumans三数据集、四项任务（跨模态检索、时间同步、主体与部位定位、动作识别）上均优于强基线，表现出稳健的细粒度时间对齐与跨模态语义一致性。

Conclusion: 通过分层对比和局部-全局融合，MoBind实现IMU与2D姿态的高质量对齐与表示学习，能够同时兼顾亚秒级时序精度与语义一致，具有通用性与鲁棒性。

Abstract: We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind.

</details>


### [70] [GUIDE-US: Grade-Informed Unpaired Distillation of Encoder Knowledge from Histopathology to Micro-UltraSound](https://arxiv.org/abs/2602.19005)
*Emma Willis,Tarek Elghareb,Paul F. R. Wilson,Minh Nguyen Nhat To,Mohammad Mahdi Abootorabi,Amoon Jamzad,Brian Wodlinger,Parvin Mousavi,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 提出一种无需配对与配准的、基于病理大模型表征蒸馏的micro-US前列腺癌分级方法；在60%特异度下提升csPCa敏感度3.5%、总体敏感度1.2%，仅靠影像实现更可靠早期分层。


<details>
  <summary>Details</summary>
Motivation: micro-超声能用于无创分级与靶向活检，但受限于分辨率难以捕捉组织微结构，现有模型对侵袭性区域识别不足。需引入病理领域丰富的微结构知识来增强影像模型，同时避免昂贵的影像-病理配准与配对。

Method: 提出“非配对病理知识蒸馏”：用已预训练的病理基础模型作为教师，学习其按ISUP分级条件化的嵌入分布；训练仅用未配对的micro-US与病理切片，无需患者级配对或注册；推理阶段仅输入micro-US，输出分级/风险。

Result: 相较SOTA，在60%特异度下：对临床显著PCa（csPCa）敏感度+3.5%；总体敏感度+1.2%。

Conclusion: 该策略让仅凭影像即可更早、更稳健地进行前列腺癌风险分层，提升临床可行性；代码将公开。

Abstract: Purpose: Non-invasive grading of prostate cancer (PCa) from micro-ultrasound (micro-US) could expedite triage and guide biopsies toward the most aggressive regions, yet current models struggle to infer tissue micro-structure at coarse imaging resolutions.
  Methods: We introduce an unpaired histopathology knowledge-distillation strategy that trains a micro-US encoder to emulate the embedding distribution of a pretrained histopathology foundation model, conditioned on International Society of Urological Pathology (ISUP) grades. Training requires no patient-level pairing or image registration, and histopathology inputs are not used at inference.
  Results: Compared to the current state of the art, our approach increases sensitivity to clinically significant PCa (csPCa) at 60% specificity by 3.5% and improves overall sensitivity at 60% specificity by 1.2%.
  Conclusion: By enabling earlier and more dependable cancer risk stratification solely from imaging, our method advances clinical feasibility. Source code will be publicly released upon publication.

</details>


### [71] [TokenTrace: Multi-Concept Attribution through Watermarked Token Recovery](https://arxiv.org/abs/2602.19019)
*Li Zhang,Shruti Agarwal,John Collomosse,Pengtao Xie,Vishal Asnani*

Main category: cs.CV

TL;DR: 提出TokenTrace，一种主动水印框架，通过同时扰动文本嵌入与初始噪声在扩散生成中嵌入“语义令牌”签名，并以查询式检索模块在单张图像中分离验证多概念（对象/风格）归因，较现有方法在单/多概念任务上更鲁棒、更高准确且画质无显著损失。


<details>
  <summary>Details</summary>
Motivation: 生成式AI易无归属地复刻艺术风格与概念，现有水印在多概念合成图像中难以解耦并分别归因，且易受常见变换破坏，亟需能对多概念稳健归因的水印方案。

Method: 在扩散流程中“主动”嵌入：同时对文本提示的嵌入与初始潜空间噪声施加细微、带密钥的扰动，形成可检索的语义签名；检索阶段设计查询式TokenTrace模块，输入生成图像与文本查询（指明对象/风格等概念），实现对单图多概念的独立验证与 disentangle。

Result: 在对象与风格的单概念、以及对象+风格等多概念归因任务上达到SOTA，显著优于基线；对常见图像变换保持鲁棒，同时维持高视觉质量。

Conclusion: TokenTrace能在语义层面嵌入与按查询解耦检索多概念水印，为生成式内容的精细化IP归因提供实用方案，兼顾准确性、鲁棒性与画质。

Abstract: Generative AI models pose a significant challenge to intellectual property (IP), as they can replicate unique artistic styles and concepts without attribution. While watermarking offers a potential solution, existing methods often fail in complex scenarios where multiple concepts (e.g., an object and an artistic style) are composed within a single image. These methods struggle to disentangle and attribute each concept individually. In this work, we introduce TokenTrace, a novel proactive watermarking framework for robust, multi-concept attribution. Our method embeds secret signatures into the semantic domain by simultaneously perturbing the text prompt embedding and the initial latent noise that guide the diffusion model's generation process. For retrieval, we propose a query-based TokenTrace module that takes the generated image and a textual query specifying which concepts need to be retrieved (e.g., a specific object or style) as inputs. This query-based mechanism allows the module to disentangle and independently verify the presence of multiple concepts from a single generated image. Extensive experiments show that our method achieves state-of-the-art performance on both single-concept (object and style) and multi-concept attribution tasks, significantly outperforming existing baselines while maintaining high visual quality and robustness to common transformations.

</details>


### [72] [An interpretable framework using foundation models for fish sex identification](https://arxiv.org/abs/2602.19022)
*Zheng Miao,Tien-Chieh Hung*

Main category: cs.CV

TL;DR: 提出FishProtoNet：一种非侵入、可解释的计算机视觉框架，用于加州濒危物种三角洲胡瓜鱼全生命周期的性别识别；通过基础模型提取ROI并用原型网络分类，在早期产卵与产卵后阶段达74.40%/81.16%准确率，但亚成体阶段仍困难。


<details>
  <summary>Details</summary>
Motivation: 水产养殖与物种保护需要准确判别鱼类性别，但现有方法多为侵入式或易致应激、死亡，特别威胁濒危鱼类；需要一种稳健、非侵入且可解释的计算机视觉方法覆盖全生命周期。

Method: 构建FishProtoNet三模块：1) 利用视觉基础模型从原始图像中提取鱼体ROI以削弱背景噪声；2) 从ROI提取特征；3) 采用可解释的原型网络进行性别判别，通过原型表征提供可视化与可解释性。

Result: 在三角洲胡瓜鱼的早期产卵与产卵后阶段，分别获得74.40%与81.16%的准确率，F1分别为74.27%与79.43%。亚成体阶段因形态差异不明显，现有视觉方法表现欠佳。

Conclusion: FishProtoNet实现了对濒危鱼类的非侵入、可解释性别识别，并在特定成熟阶段表现良好；结合基础模型与原型网络提升了稳健性与可解释性，但亚成体阶段仍是难点，提示未来需更丰富特征或多模态手段。

Abstract: Accurate sex identification in fish is vital for optimizing breeding and management strategies in aquaculture, particularly for species at the risk of extinction. However, most existing methods are invasive or stressful and may cause additional mortality, posing severe risks to threatened or endangered fish populations. To address these challenges, we propose FishProtoNet, a robust, non-invasive computer vision-based framework for sex identification of delta smelt (Hypomesus transpacificus), an endangered fish species native to California, across its full life cycle. Unlike the traditional deep learning methods, FishProtoNet provides interpretability through learned prototype representations while improving robustness by leveraging foundation models to reduce the influence of background noise. Specifically, the FishProtoNet framework consists of three key components: fish regions of interest (ROIs) extraction using visual foundation model, feature extraction from fish ROIs and fish sex identification based on an interpretable prototype network. FishProtoNet demonstrates strong performance in delta smelt sex identification during early spawning and post-spawning stages, achieving the accuracies of 74.40% and 81.16% and corresponding F1 scores of 74.27% and 79.43% respectively. In contrast, delta smelt sex identification at the subadult stage remains challenging for current computer vision methods, likely due to less pronounced morphological differences in immature fish. The source code of FishProtoNet is publicly available at: https://github.com/zhengmiao1/Fish_sex_identification

</details>


### [73] [Towards Calibrating Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2602.19024)
*Ashshak Sharifdeen,Fahad Shamshad,Muhammad Akhtar Munir,Abhishek Basu,Mohamed Insaf Ismithdeen,Jeyapriyan Jeyamohan,Chathurika Sewwandi Silva,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出一种用于CLIP等视觉-语言模型提示调优的校准框架，在不改动权重的前提下显著降低ECE并保持嵌入空间几何与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提示调优虽然高效，但常出现置信度校准差、预测不确定性不可靠的问题；直接强校准可能破坏预训练嵌入几何，削弱跨域/新类泛化，需兼顾可靠性与几何保持。

Method: 在标准交叉熵上加入两类正则：1) 均值-方差边距惩罚，最大化类间logit边距均值并最小化其离散度，抑制过/欠置信的尖峰；2) 文本矩匹配损失，对齐调优后文本嵌入与冻结CLIP文本嵌入的一阶与二阶矩，维持语义分散性与几何结构。适用于多种提示调优方法。

Result: 在7种提示调优方法、11个数据集上，较多种主流校准方法显著降低ECE，且对基类与新类均有效，显示更可靠的不确定性与保持的泛化性能。

Conclusion: 通过边距稳定与矩匹配正则，可在不破坏CLIP嵌入几何的前提下提升提示调优的置信度校准与可靠性，具有通用性并在多数据集上验证有效。

Abstract: Prompt tuning of large-scale vision-language models such as CLIP enables efficient task adaptation without updating model weights. However, it often leads to poor confidence calibration and unreliable predictive uncertainty. We address this problem by proposing a calibration framework that enhances predictive reliability while preserving the geometry of the pretrained CLIP embedding space, which is required for robust generalization. Our approach extends the standard cross-entropy loss with two complementary regularizers: (1) a mean-variance margin penalty that stabilizes inter-class logit margins by maximizing their average while minimizing dispersion, mitigating underconfidence and overconfidence spikes; and (2) a text moment-matching loss that aligns the first and second moments of tuned text embeddings with their frozen CLIP counterparts, preserving semantic dispersion crucial for generalization. Through extensive experiments across 7 prompt-tuning methods and 11 diverse datasets, we demonstrate that our approach significantly reduces the Expected Calibration Error (ECE) compared to competitive calibration techniques on both base and novel classes

</details>


### [74] [OpenVO: Open-World Visual Odometry with Temporal Dynamics Awareness](https://arxiv.org/abs/2602.19035)
*Phuc D. A. Nguyen,Anh N. Nhu,Ming C. Lin*

Main category: cs.CV

TL;DR: OpenVO是一个面向开放世界、具时间感知的单目VO框架，可在观测频率变化与未标定相机下估计具备真实尺度的自运动，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VO通常在固定采样频率训练，忽视时间动态；且依赖已知内参。部署到不同频率或未标定相机时精度显著下降，限制了从行车记录仪等真实场景中提取轨迹与下游应用。

Method: 在两帧位姿回归框架中显式编码时间动态信息；结合来自基础模型的3D几何先验；可处理单目行车记录仪视频、变动观测率与未标定相机，输出具真实尺度的位姿。

Result: 在KITTI、nuScenes、Argoverse 2上较SOTA提升>20%；在变观测率设定下，对所有指标误差降低46%-92%，表现更稳健。

Conclusion: OpenVO在非固定频率与未标定条件下实现稳健、实尺度的单目VO，具备良好泛化能力，适用于真实场景3D重建与多种下游应用（如从行车记录仪中构建轨迹数据集）。

Abstract: We introduce OpenVO, a novel framework for Open-world Visual Odometry (VO) with temporal awareness under limited input conditions. OpenVO effectively estimates real-world-scale ego-motion from monocular dashcam footage with varying observation rates and uncalibrated cameras, enabling robust trajectory dataset construction from rare driving events recorded in dashcam. Existing VO methods are trained on fixed observation frequency (e.g., 10Hz or 12Hz), completely overlooking temporal dynamics information. Many prior methods also require calibrated cameras with known intrinsic parameters. Consequently, their performance degrades when (1) deployed under unseen observation frequencies or (2) applied to uncalibrated cameras. These significantly limit their generalizability to many downstream tasks, such as extracting trajectories from dashcam footage. To address these challenges, OpenVO (1) explicitly encodes temporal dynamics information within a two-frame pose regression framework and (2) leverages 3D geometric priors derived from foundation models. We validate our method on three major autonomous-driving benchmarks - KITTI, nuScenes, and Argoverse 2 - achieving more than 20 performance improvement over state-of-the-art approaches. Under varying observation rate settings, our method is significantly more robust, achieving 46%-92% lower errors across all metrics. These results demonstrate the versatility of OpenVO for real-world 3D reconstruction and diverse downstream applications.

</details>


### [75] [TeFlow: Enabling Multi-frame Supervision for Self-Supervised Feed-forward Scene Flow Estimation](https://arxiv.org/abs/2602.19053)
*Qingwen Zhang,Chenhan Jiang,Xiaomeng Zhu,Yunqi Miao,Yushan Zhang,Olov Andersson,Patric Jensfelt*

Main category: cs.CV

TL;DR: TeFlow 通过时间集成挖掘跨多帧一致的运动监督，解决自监督前馈场景流在遮挡与两帧对应不稳定下的失效问题，在 Argoverse2 与 nuScenes 上相对提升可达33%，速度较优化法快约150倍。


<details>
  <summary>Details</summary>
Motivation: 两帧点对应提供的自监督信号在遮挡与快速运动下不可靠，导致前馈场景流估计不稳。多帧可提供更稳定的时序线索，但天真地扩展两帧目标会因跨帧对应突变而产生不一致监督，需要一种能从多帧中提炼时序一致监督的机制。

Method: 提出 TeFlow：构建跨多帧的候选运动池，利用时间集成策略从中聚合最具时序一致性的运动线索，形成可靠的监督信号，从而对前馈网络进行自监督训练，实现多帧监督而不依赖显式稳定对应。

Result: 在 Argoverse 2 与 nuScenes 上，相对现有自监督前馈方法实现最高达33%的性能提升；与主流优化式方法精度相当，但推理速度快约150倍。代码与模型权重已开源。

Conclusion: 通过时间一致性挖掘的多帧自监督可显著提升前馈场景流的准确性与鲁棒性，在保持实时性的同时接近优化法性能，为无监督/自监督动态感知提供有效范式。

Abstract: Self-supervised feed-forward methods for scene flow estimation offer real-time efficiency, but their supervision from two-frame point correspondences is unreliable and often breaks down under occlusions. Multi-frame supervision has the potential to provide more stable guidance by incorporating motion cues from past frames, yet naive extensions of two-frame objectives are ineffective because point correspondences vary abruptly across frames, producing inconsistent signals. In the paper, we present TeFlow, enabling multi-frame supervision for feed-forward models by mining temporally consistent supervision. TeFlow introduces a temporal ensembling strategy that forms reliable supervisory signals by aggregating the most temporally consistent motion cues from a candidate pool built across multiple frames. Extensive evaluations demonstrate that TeFlow establishes a new state-of-the-art for self-supervised feed-forward methods, achieving performance gains of up to 33\% on the challenging Argoverse 2 and nuScenes datasets. Our method performs on par with leading optimization-based methods, yet speeds up 150 times. The code is open-sourced at https://github.com/KTH-RPL/OpenSceneFlow along with trained model weights.

</details>


### [76] [Direction-aware 3D Large Multimodal Models](https://arxiv.org/abs/2602.19063)
*Quan Liu,Weihao Xuan,Junjue Wang,Naoto Yokoya,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 提出面向方向感知的3D大模型范式：恢复并对齐自我位姿(ego pose)，将点云按位姿变换以支持方向性问答与空间推理；在多种3D LMM上显著提升指标。


<details>
  <summary>Details</summary>
Motivation: 现有点云基准含有方向性问题但缺少对应ego位姿，导致3D LMM方向性问答与空间推理欠准、设定不严谨；文本注入或特征编码式的位姿利用方式也不足以从根本上解决对齐问题。

Method: 1) PoseRecover：从RGB-D视频外参中自动恢复与问题匹配的ego位姿，基于目标-视锥体相交与Z-buffer可见性检查筛选；2) PoseAlign：不再仅在文本或投影特征层编码位姿，而是直接将点云根据识别的ego位姿进行几何对齐与变换；3) 在多种3D LMM骨干上仅通过指令微调进行集成。

Result: 在LL3DA、LL3DA-SONATA、Chat-Scene、3D-LLAVA等骨干上取得一致收益：ScanRefer mIoU提升30.0%，Scan2Cap以LLM评审的准确率提升11.7%。

Conclusion: 补充并对齐ego位姿是构建方向感知3D LMM的关键。所提PoseRecover+PoseAlign范式通用、简洁、训练高效，仅需指令微调即可建立强基线并显著提升方向性问答与空间推理表现。

Abstract: 3D large multimodal models (3D LMMs) rely heavily on ego poses for enabling directional question-answering and spatial reasoning. However, most existing point cloud benchmarks contain rich directional queries but lack the corresponding ego poses, making them inherently ill-posed in 3D large multimodal modelling. In this work, we redefine a new and rigorous paradigm that enables direction-aware 3D LMMs by identifying and supplementing ego poses into point cloud benchmarks and transforming the corresponding point cloud data according to the identified ego poses. We enable direction-aware 3D LMMs with two novel designs. The first is PoseRecover, a fully automatic pose recovery pipeline that matches questions with ego poses from RGB-D video extrinsics via object-frustum intersection and visibility check with Z-buffers. The second is PoseAlign that transforms the point cloud data to be aligned with the identified ego poses instead of either injecting ego poses into textual prompts or introducing pose-encoded features in the projection layers. Extensive experiments show that our designs yield consistent improvements across multiple 3D LMM backbones such as LL3DA, LL3DA-SONATA, Chat-Scene, and 3D-LLAVA, improving ScanRefer mIoU by 30.0% and Scan2Cap LLM-as-judge accuracy by 11.7%. In addition, our approach is simple, generic, and training-efficient, requiring only instruction tuning while establishing a strong baseline for direction-aware 3D-LMMs.

</details>


### [77] [L3DR: 3D-aware LiDAR Diffusion and Rectification](https://arxiv.org/abs/2602.19064)
*Quan Liu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 论文提出L3DR：在Range-view LiDAR扩散生成后，通过3D残差回归与鲁棒损失在三维空间纠正RV伪影，显著提升几何真实性，并在多数据集上达SOTA且通用于多种LiDAR扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于RV的LiDAR扩散虽具2D外观逼真度，但忽视三维几何真实性，常出现深度渗漏、波纹等伪影；需要一种能在三维层面校正局部几何并提升边界锐利度的方法。

Method: 1) 理论与实证分析说明3D模型优于2D模型在边界锐度与真实性上的先天优势；2) 设计3D残差回归网络，对每个点预测三维偏移量以纠正RV伪影、恢复局部几何；3) 提出Welsch鲁棒损失，强调局部几何、抑制异常区域影响；4) 将该校正模块无缝叠加到不同LiDAR扩散模型上，计算开销小。

Result: 在KITTI、KITTI360、nuScenes、Waymo等基准上取得SOTA的生成质量与更优几何真实性，边界更锐利、伪影显著减少；同时具备良好通用性与轻量的额外开销。

Conclusion: 通过在三维空间进行残差回归与鲁棒优化，L3DR有效消除RV伪影并恢复真实几何，实现稳定一致的几何高保真LiDAR生成，适配多种扩散框架且高效。

Abstract: Range-view (RV) based LiDAR diffusion has recently made huge strides towards 2D photo-realism. However, it neglects 3D geometry realism and often generates various RV artifacts such as depth bleeding and wavy surfaces. We design L3DR, a 3D-aware LiDAR Diffusion and Rectification framework that can regress and cancel RV artifacts in 3D space and restore local geometry accurately. Our theoretical and empirical analysis reveals that 3D models are inherently superior to 2D models in generating sharp and authentic boundaries. Leveraging such analysis, we design a 3D residual regression network that rectifies RV artifacts and achieves superb geometry realism by predicting point-level offsets in 3D space. On top of that, we design a Welsch Loss that helps focus on local geometry and ignore anomalous regions effectively. Extensive experiments over multiple benchmarks including KITTI, KITTI360, nuScenes and Waymo show that the proposed L3DR achieves state-of-the-art generation and superior geometry-realism consistently. In addition, L3DR is generally applicable to different LiDAR diffusion models with little computational overhead.

</details>


### [78] [ChordEdit: One-Step Low-Energy Transport for Image Editing](https://arxiv.org/abs/2602.19083)
*Liangsi Lu,Xuhang Chen,Minzhe Guo,Shichu Li,Jingchao Wang,Yang Shi*

Main category: cs.CV

TL;DR: 提出ChordEdit：面向一步T2I模型的训练/逆推/模型无关文本引导编辑方法，通过将编辑建模为源/目标文本分布间的传输并用动态最优传输得到低能量控制场，实现单步稳定、高保真、实时编辑。


<details>
  <summary>Details</summary>
Motivation: 一步T2I合成速度快，但现有训练自由的编辑器无法在单步推理中稳定工作，导致目标对象变形和未编辑区域一致性差。根因是对结构化场进行天真的向量运算会产生高能量、轨迹紊乱。需要一种既不训练也不反演、仍能单步稳定编辑的方法。

Method: 将文本引导编辑重述为源文本与目标文本诱导的分布间的运输问题；基于动态最优传输推导低能量控制策略，得到平滑、降方差的编辑矢量场（editing field）。该场可在一次大步长积分中稳定穿越，从而实现单步编辑；方法模型无关、训练自由、无需反演。

Result: 理论上给出低能量控制的推导与稳定性依据，并在实验中验证：在一步T2I模型上实现快速、轻量、精确的编辑，显著减少对象畸变并保持未编辑区域一致性，支持真正的实时编辑。

Conclusion: ChordEdit通过动态最优传输构造低能量编辑场，解决一步T2I编辑中的不稳定与失真问题，在无需训练与反演的前提下，实现高保真、单步、实时的文本引导图像编辑。

Abstract: The advent of one-step text-to-image (T2I) models offers unprecedented synthesis speed. However, their application to text-guided image editing remains severely hampered, as forcing existing training-free editors into a single inference step fails. This failure manifests as severe object distortion and a critical loss of consistency in non-edited regions, resulting from the high-energy, erratic trajectories produced by naive vector arithmetic on the models' structured fields. To address this problem, we introduce ChordEdit, a model agnostic, training-free, and inversion-free method that facilitates high-fidelity one-step editing. We recast editing as a transport problem between the source and target distributions defined by the source and target text prompts. Leveraging dynamic optimal transport theory, we derive a principled, low-energy control strategy. This strategy yields a smoothed, variance-reduced editing field that is inherently stable, facilitating the field to be traversed in a single, large integration step. A theoretically grounded and experimentally validated approach allows ChordEdit to deliver fast, lightweight and precise edits, finally achieving true real-time editing on these challenging models.

</details>


### [79] [Restoration-Guided Kuzushiji Character Recognition Framework under Seal Interference](https://arxiv.org/abs/2602.19086)
*Rui-Yang Ju,Kohei Yamashita,Hirotaka Kameko,Shinsuke Mori*

Main category: cs.CV

TL;DR: 提出RG-KCR三阶段框架，通过检测-修复-分类流水线缓解印章覆盖带来的楷书（崩字/くずし字）识别退化，显著提升在受干扰文档上的识别精度；代码已开源。


<details>
  <summary>Details</summary>
Motivation: 传统日本古文书常含印章覆盖，导致现有Kuzushiji识别在真实场景下精度下降；需要能在印章干扰下稳健工作的识别方案与评测数据。

Method: 设计三阶段流水线：Stage1 用目标检测（最佳为YOLOv12-medium）定位字符；Stage2 对受印章干扰的字符区域进行图像复原（以PSNR/SSIM定量评估）；Stage3 使用基于ViT的分类器Metom进行字符分类。构建独立数据集用于检测与分类评测，并做消融实验验证Stage2的贡献。

Result: 在检测测试集上，YOLOv12-medium达到Precision 98.0%、Recall 93.3%；复原阶段在PSNR与SSIM上取得可观提升；加入Stage2后，分类Top-1准确率由93.45%提升至95.33%。

Conclusion: 面向带印章干扰的古籍文档，RG-KCR的“检测-复原-分类”策略有效缓解重叠印章对识别的影响，并显著提升端到端字符识别性能，为真实历史文献处理提供更稳健的解决方案。

Abstract: Kuzushiji was one of the most popular writing styles in pre-modern Japan and was widely used in both personal letters and official documents. However, due to its highly cursive forms and extensive glyph variations, most modern Japanese readers cannot directly interpret Kuzushiji characters. Therefore, recent research has focused on developing automated Kuzushiji character recognition methods, which have achieved satisfactory performance on relatively clean Kuzushiji document images. However, existing methods struggle to maintain recognition accuracy under seal interference (e.g., when seals overlap characters), despite the frequent occurrence of seals in pre-modern Japanese documents. To address this challenge, we propose a three-stage restoration-guided Kuzushiji character recognition (RG-KCR) framework specifically designed to mitigate seal interference. We construct datasets for evaluating Kuzushiji character detection (Stage 1) and classification (Stage 3). Experimental results show that the YOLOv12-medium model achieves a precision of 98.0% and a recall of 93.3% on the constructed test set. We quantitatively evaluate the restoration performance of Stage 2 using PSNR and SSIM. In addition, we conduct an ablation study to demonstrate that Stage 2 improves the Top-1 accuracy of Metom, a Vision Transformer (ViT)-based Kuzushiji classifier employed in Stage 3, from 93.45% to 95.33%. The implementation code of this work is available at https://ruiyangju.github.io/RG-KCR.

</details>


### [80] [Ani3DHuman: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling](https://arxiv.org/abs/2602.19089)
*Qi Sun,Can Wang,Jiaxiang Shang,Yingchun Liu,Jing Liao*

Main category: cs.CV

TL;DR: Ani3DHuman结合运动学与视频扩散先验，通过分层运动表征分离刚体与残余非刚体运动，并用自引导随机采样在扩散恢复阶段同时保证真实感与身份一致性，进而优化非刚体场，生成更逼真的3D人物动画。


<details>
  <summary>Details</summary>
Motivation: 现有3D人物动画要么缺乏衣物等非刚体动态（纯运动学），要么虽有非刚体但画质差、身份漂移（视频扩散）。需要一种既保留身份与稳定几何，又引入高保真非刚体细节的方案。

Method: 1) 提出分层运动表示：刚体运动与残余非刚体运动解耦。2) 用运动学方法生成刚体驱动与粗渲染，作为视频扩散模型的条件。3) 将扩散模型用于“恢复”残余非刚体细节，但因条件渲染分布外，标准确定性ODE采样失败。4) 设计自引导随机采样：在随机采样带来真实感的同时，引入自引导项约束身份一致性。5) 利用恢复得到的高质视频反向监督，优化残余非刚体运动场。

Result: 在大量实验中，方法生成的3D人物动画具备高写实度、保身份、含非刚体细节，量化与主观评测均优于现有方法；提供开源代码。

Conclusion: 通过分层表示与自引导随机采样，将运动学的稳定性与扩散先验的细节能力成功融合，可稳定生成写实且身份一致的3D人物动画，并在基准上取得领先表现。

Abstract: Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non-rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present Ani3DHuman, a framework that marries kinematics-based animation with video diffusion priors. We first introduce a layered motion representation that disentangles rigid motion from residual non-rigid motion. Rigid motion is generated by a kinematic method, which then produces a coarse rendering to guide the video diffusion model in generating video sequences that restore the residual non-rigid motion. However, this restoration task, based on diffusion sampling, is highly challenging, as the initial renderings are out-of-distribution, causing standard deterministic ODE samplers to fail. Therefore, we propose a novel self-guided stochastic sampling method, which effectively addresses the out-of-distribution problem by combining stochastic sampling (for photorealistic quality) with self-guidance (for identity fidelity). These restored videos provide high-quality supervision, enabling the optimization of the residual non-rigid motion field. Extensive experiments demonstrate that \MethodName can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman.

</details>


### [81] [CREM: Compression-Driven Representation Enhancement for Multimodal Retrieval and Comprehension](https://arxiv.org/abs/2602.19091)
*Lihao Liu,Yan Wang,Biao Yang,Da Li,Jiangxia Cao,Yuxiao Luo,Xiang Chen,Xiangyu Wu,Wei Yuan,Fan Yang,Guiguang Ding,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: 提出CREM框架，通过“压缩驱动”的提示与训练，将生成式与检索式目标统一，在不牺牲生成能力的前提下显著提升MLLM检索表现。


<details>
  <summary>Details</summary>
Motivation: MLLM在理解与生成任务上强，但直接用于基于向量的检索表现欠佳；现有对比学习微调虽能提升检索，却常丧失生成能力。作者认为生成与检索共享核心机制（跨模态对齐与上下文理解），希望在统一范式下兼顾两者。

Method: 提出CREM：1) 压缩式提示设计，引入可学习的“合唱（chorus）”token聚合图文语义；2) 压缩驱动的训练，将对比与生成目标通过“compression-aware attention”融合，促使模型在保持生成能力的同时学到判别性表征；整体以统一框架提升多模态表示质量并兼容生成推理。

Result: 在MMEB检索基准上达到SOTA；同时在多项理解/生成类基准上维持强生成性能，显示压缩驱动的生成监督可进一步提升表示质量。

Conclusion: 通过压缩驱动的设计与训练，可在不牺牲生成能力的情况下增强MLLM的跨模态表征，兼顾检索与生成；生成监督在该范式下对表示学习具有正向增益。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable success in comprehension tasks such as visual description and visual question answering. However, their direct application to embedding-based tasks like retrieval remains challenging due to the discrepancy between output formats and optimization objectives. Previous approaches often employ contrastive fine-tuning to adapt MLLMs for retrieval, but at the cost of losing their generative capabilities. We argue that both generative and embedding tasks fundamentally rely on shared cognitive mechanisms, specifically cross-modal representation alignment and contextual comprehension. To this end, we propose CREM (Compression-driven Representation Enhanced Model), with a unified framework that enhances multimodal representations for retrieval while preserving generative ability. Specifically, we introduce a compression-based prompt design with learnable chorus tokens to aggregate multimodal semantics and a compression-driven training strategy that integrates contrastive and generative objectives through compression-aware attention. Extensive experiments demonstrate that CREM achieves state-of-the-art retrieval performance on MMEB while maintaining strong generative performance on multiple comprehension benchmarks. Our findings highlight that generative supervision can further improve the representational quality of MLLMs under the proposed compression-driven paradigm.

</details>


### [82] [Universal 3D Shape Matching via Coarse-to-Fine Language Guidance](https://arxiv.org/abs/2602.19112)
*Qinfeng Xiao,Guofeng Mei,Bo Yang,Liying Zhang,Jian Zhang,Kit-lun Yick*

Main category: cs.CV

TL;DR: UniMatch提出一个语义感知、粗到细的通用形状致密对应方法，利用类无关3D分割+MLLM命名+VLM文本嵌入实现粗粒度语义部件匹配，再用基于排序的对比学习细化为点级对应，适用于强非等距与跨类别对象，并在多种挑战场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统形状对应方法多依赖近等距假设和同质类别（如仅人体），难以处理跨类别、强非等距形状；缺乏对普适对象的语义一致性构建手段。需要一种不受类别限制、能从语义层面对齐并细化到致密对应的框架。

Method: 两阶段框架。粗阶段：进行类无关3D语义分割得到不重叠部件；调用多模态大语言模型给部件命名；用预训练视觉-语言模型提取文本嵌入并基于名称将语义部件跨形状匹配。细阶段：以粗语义对应为引导，设计基于排序的对比学习方案，学习点级致密对应，从而在强非等距情况下仍可对齐。无需预定义部件集合。

Result: 在多种具有挑战性的跨类别、强非等距数据集与场景中，UniMatch在构建致密对应的准确性上持续优于现有竞争方法。

Conclusion: 结合类无关分割、语言引导与排序式对比学习，可将粗语义线索有效提升为细致密对应，实现对任意类别、非等距形状的通用匹配，且无需预定义部件；方法鲁棒且效果领先。

Abstract: Establishing dense correspondences between shapes is a crucial task in computer vision and graphics, while prior approaches depend on near-isometric assumptions and homogeneous subject types (i.e., only operate for human shapes). However, building semantic correspondences for cross-category objects remains challenging and has received relatively little attention. To achieve this, we propose UniMatch, a semantic-aware, coarse-to-fine framework for constructing dense semantic correspondences between strongly non-isometric shapes without restricting object categories. The key insight is to lift "coarse" semantic cues into "fine" correspondence, which is achieved through two stages. In the "coarse" stage, we perform class-agnostic 3D segmentation to obtain non-overlapping semantic parts and prompt multimodal large language models (MLLMs) to identify part names. Then, we employ pretrained vision language models (VLMs) to extract text embeddings, enabling the construction of matched semantic parts. In the "fine" stage, we leverage these coarse correspondences to guide the learning of dense correspondences through a dedicated rank-based contrastive scheme. Thanks to class-agnostic segmentation, language guiding, and rank-based contrastive learning, our method is versatile for universal object categories and requires no predefined part proposals, enabling universal matching for inter-class and non-isometric shapes. Extensive experiments demonstrate UniMatch consistently outperforms competing methods in various challenging scenarios.

</details>


### [83] [Keep it SymPL: Symbolic Projective Layout for Allocentric Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2602.19117)
*Jaeyun Jang,Seunghui Shin,Taeho Park,Hyoseok Hwang*

Main category: cs.CV

TL;DR: 提出SymPL框架，将物体中心（allocentric）视角下的空间推理转化为VLM擅长的符号化布局表示，显著提升视角感知空间推理性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在自我中心（egocentric）场景表现良好，但在物体中心（allocentric）视角下推理显著退化，特别是在需要从物体视角理解空间关系、易受视觉错觉与多视角变化影响的问题上，缺乏有效方法。

Method: 提出Symbolic Projective Layout（SymPL）：以投影、抽象、二分与定位四个核心要素，将allocentric问题重构为结构化的符号-布局表示，使VLM能以其擅长的形式处理空间关系；将自然语言问题映射为符号化布局约束，并据此进行推理与回答。

Result: 在广泛实验中，SymPL在allocentric与egocentric任务上均带来显著性能提高；在视觉错觉与多视图条件下表现出更强鲁棒性；消融研究表明四个组件均对性能提升至关重要。

Conclusion: SymPL提供了一种有效且有原则的方法，将复杂的视角感知空间推理转化为符号布局问题，从而系统性改善VLM在不同视角与复杂条件下的推理能力。

Abstract: Perspective-aware spatial reasoning involves understanding spatial relationships from specific viewpoints-either egocentric (observer-centered) or allocentric (object-centered). While vision-language models (VLMs) perform well in egocentric settings, their performance deteriorates when reasoning from allocentric viewpoints, where spatial relations must be inferred from the perspective of objects within the scene. In this study, we address this underexplored challenge by introducing Symbolic Projective Layout (SymPL), a framework that reformulates allocentric reasoning into symbolic-layout forms that VLMs inherently handle well. By leveraging four key factors-projection, abstraction, bipartition, and localization-SymPL converts allocentric questions into structured symbolic-layout representations. Extensive experiments demonstrate that this reformulation substantially improves performance in both allocentric and egocentric tasks, enhances robustness under visual illusions and multi-view scenarios, and that each component contributes critically to these gains. These results show that SymPL provides an effective and principled approach for addressing complex perspective-aware spatial reasoning.

</details>


### [84] [StreetTree: A Large-Scale Global Benchmark for Fine-Grained Tree Species Classification](https://arxiv.org/abs/2602.19123)
*Jiapeng Li,Yingjing Huang,Fan Zhang,Yu liu*

Main category: cs.CV

TL;DR: 提出StreetTree：首个面向街道树精细分类的大规模基准，含1200万+图像、8300+物种、覆盖133国，揭示现有视觉模型在真实城市环境下的瓶颈并给出基线。


<details>
  <summary>Details</summary>
Motivation: 街道树精细分类对城市规划、街景管理与生态服务评估重要，但缺乏大规模、地理多样、公开可用且面向街道树的专用基准数据集，限制了方法发展与可比性。

Method: 构建StreetTree数据集：从五大洲133国街景采集1200万+图像，涵盖8300+常见街道树物种；引入专家核验观测数据；设计层级分类体系（目-科-属-种）；在多种视觉模型上进行广泛实验，建立基线并分析挑战。

Result: 实验显示：在高相似物种、长尾分布、季节性导致的类内差异、光照/遮挡/视角多变等真实复杂条件下，预训练视觉模型表现受限；提供强基线与诊断。

Conclusion: StreetTree为街道树精细识别提供关键资源，支持层级分类与表示学习研究，有望推动计算机视觉与城市科学交叉领域的新进展与实际管理应用。

Abstract: The fine-grained classification of street trees is a crucial task for urban planning, streetscape management, and the assessment of urban ecosystem services. However, progress in this field has been significantly hindered by the lack of large-scale, geographically diverse, and publicly available benchmark datasets specifically designed for street trees. To address this critical gap, we introduce StreetTree, the world's first large-scale benchmark dataset dedicated to fine-grained street tree classification. The dataset contains over 12 million images covering more than 8,300 common street tree species, collected from urban streetscapes across 133 countries spanning five continents, and supplemented with expert-verified observational data. StreetTree poses substantial challenges for pretrained vision models under complex urban environments: high inter-species visual similarity, long-tailed natural distributions, significant intra-class variations caused by seasonal changes, and diverse imaging conditions such as lighting, occlusions from buildings, and varying camera angles. In addition, we provide a hierarchical taxonomy (order-family-genus-species) to support research in hierarchical classification and representation learning. Through extensive experiments with various visual models, we establish strong baselines and reveal the limitations of existing methods in handling such real-world complexities. We believe that StreetTree will serve as a key resource for the refined management and research of urban street trees, while also driving new advancements at the intersection of computer vision and urban science.

</details>


### [85] [Mapping Networks](https://arxiv.org/abs/2602.19134)
*Lord Sen,Shyamapada Mukherjee*

Main category: cs.CV

TL;DR: 提出“映射网络”，用一个可训练的低维潜变量替代高维权重，通过映射定理与映射损失保证从潜空间到权重空间的可行映射。在图像分类、深伪检测等任务上，以约99.5%（约500×）的可训练参数减少，仍能达到与原网络相当或更优的性能，并显著缓解过拟合。


<details>
  <summary>Details</summary>
Motivation: 大模型参数规模不断增长导致训练成本高、易过拟合、资源受限。作者基于“已训练权重位于光滑、低维流形”的假设，寻求以低维表示替换高维权重，从而降低训练与存储开销并提升泛化。

Method: - 设计映射网络：以低维潜向量z作为可训练主体；通过一个专门的映射函数F(z)输出目标网络的权重W。
- 提出“映射定理”并配套“映射损失”，理论与实践上保证潜空间到权重空间存在有效映射并被学习到。
- 在多任务（视觉与序列）中用F(z)生成/调整基模型权重，并仅训练z与映射网络参数。

Result: 在图像分类、深伪检测等复杂任务上，使用映射网络能将可训练参数减少约99.5%（约500×），同时性能与原目标网络相当或更优，并表现出更强的抗过拟合能力。

Conclusion: 高维权重可由低维潜空间通过可学习映射有效表示。映射网络在显著减少可训练参数的同时维持或提升性能，提供了一条缓解大模型训练成本与过拟合的通用途径。

Abstract: The escalating parameter counts in modern deep learning models pose a fundamental challenge to efficient training and resolution of overfitting. We address this by introducing the \emph{Mapping Networks} which replace the high dimensional weight space by a compact, trainable latent vector based on the hypothesis that the trained parameters of large networks reside on smooth, low-dimensional manifolds. Henceforth, the Mapping Theorem enforced by a dedicated Mapping Loss, shows the existence of a mapping from this latent space to the target weight space both theoretically and in practice. Mapping Networks significantly reduce overfitting and achieve comparable to better performance than target network across complex vision and sequence tasks, including Image Classification, Deepfake Detection etc, with $\mathbf{99.5\%}$, i.e., around $500\times$ reduction in trainable parameters.

</details>


### [86] [CaReFlow: Cyclic Adaptive Rectified Flow for Multimodal Fusion](https://arxiv.org/abs/2602.19140)
*Sijie Mai,Shiqin Han*

Main category: cs.CV

TL;DR: 提出一种基于整流流(rectified flow)的模态分布映射方法，通过一对多映射让源模态样本感知目标模态的全局分布，并配合自适应宽松对齐与循环整流流，显著缩小模态鸿沟并提升多模态情感计算性能。


<details>
  <summary>Details</summary>
Motivation: 多模态融合常受“模态鸿沟”限制。现有通过扩散或对抗学习的方法多为一对一对齐，无法让源模态样本接触到目标模态的全局分布，且配对样本不足时易过拟合/对齐不稳。作者希望实现更全局、鲁棒的跨模态分布映射。

Method: 将整流流扩展到模态分布映射：1) 一对多映射：每个源模态点沿直线路径对齐到目标模态的整体分布，从局部配对转为全局统计对齐；2) 自适应宽松对齐：同一样本的跨模态对强约束，不同样本/不同类别对弱约束，缓解一对多方向歧义并提升映射精度；3) 循环整流流：引入可逆回译，保证映射后特征可回到原模态，防止信息丢失并保留模态特异性；随后进行简单融合。

Result: 在多项多模态情感计算任务上取得具有竞争力的结果；可视化显示模态间距离显著缩小，分布更为对齐。

Conclusion: 利用整流流的一对多全局映射、配合自适应对齐与循环一致性，可有效缓解模态鸿沟，提升多模态表示与下游情感任务性能，即使采用简单融合策略亦能取得优异表现。

Abstract: Modality gap significantly restricts the effectiveness of multimodal fusion. Previous methods often use techniques such as diffusion models and adversarial learning to reduce the modality gap, but they typically focus on one-to-one alignment without exposing the data points of the source modality to the global distribution information of the target modality. To this end, leveraging the characteristic of rectified flow that can map one distribution to another via a straight trajectory, we extend rectified flow for modality distribution mapping. Specifically, we leverage the `one-to-many mapping' strategy in rectified flow that allows each data point of the source modality to observe the overall target distribution. This also alleviates the issue of insufficient paired data within each sample, enabling a more robust distribution transformation. Moreover, to achieve more accurate distribution mapping and address the ambiguous flow directions in one-to-many mapping, we design `adaptive relaxed alignment', enforcing stricter alignment for modality pairs belonging to the same sample, while applying relaxed mapping for pairs not belonging to the same sample or category. Additionally, to prevent information loss during distribution mapping, we introduce `cyclic rectified flow' to ensure the transferred features can be translated back to the original features, allowing multimodal representations to learn sufficient modality-specific information. After distribution alignment, our approach achieves very competitive results on multiple tasks of multimodal affective computing even with a simple fusion method, and visualizations verify that it can effectively reduce the modality gap.

</details>


### [87] [VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval](https://arxiv.org/abs/2602.19146)
*Diogo Glória-Silva,David Semedo,João Maglhães*

Main category: cs.CV

TL;DR: VIGiA 是一个用于多模态对话的模型，专注在复杂、多步骤的教学视频行动计划上的理解与推理；它通过多模态计划推理与基于计划的检索，实现与视觉输入、任务计划和用户交互对齐的指导对话，并在新数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作多为纯文本指导或将视觉与语言割裂处理，难以在对话中进行基于视频与计划的多步推理与对齐。因此需要一个能在对话过程中兼顾视觉证据、任务计划与用户意图的模型，实现更准确的、具备计划意识的指导。

Method: 提出 VIGiA，包含两大能力：(1) 多模态计划推理：将单/多模态查询与当前任务计划对齐，生成准确回应；(2) 基于计划的检索：从文本或视觉表示中检索相关步骤。模型在包含烹饪与 DIY 计划、且视频对话对齐的新数据集上训练与评测。

Result: 在“会话式计划指导”设定下，VIGiA 在所有任务上优于 SOTA；在“计划感知 VQA”任务上准确率超过 90%。

Conclusion: 融合计划推理与检索的多模态对话框架能有效提升教学视频中的计划感知理解与指导能力，优于现有模型，并验证了在复杂多步任务中的实用性。

Abstract: We introduce VIGiA, a novel multimodal dialogue model designed to understand and reason over complex, multi-step instructional video action plans. Unlike prior work which focuses mainly on text-only guidance, or treats vision and language in isolation, VIGiA supports grounded, plan-aware dialogue that requires reasoning over visual inputs, instructional plans, and interleaved user interactions. To this end, VIGiA incorporates two key capabilities: (1) multimodal plan reasoning, enabling the model to align uni- and multimodal queries with the current task plan and respond accurately; and (2) plan-based retrieval, allowing it to retrieve relevant plan steps in either textual or visual representations. Experiments were done on a novel dataset with rich Instructional Video Dialogues aligned with Cooking and DIY plans. Our evaluation shows that VIGiA outperforms existing state-of-the-art models on all tasks in a conversational plan guidance setting, reaching over 90\% accuracy on plan-aware VQA.

</details>


### [88] [Artefact-Aware Fungal Detection in Dermatophytosis: A Real-Time Transformer-Based Approach for KOH Microscopy](https://arxiv.org/abs/2602.19156)
*Rana Gursoy,Abdurrahim Yilmaz,Baris Kizilyaprak,Esmahan Caglar,Burak Temelkuran,Huseyin Uvet,Ayse Esra Koku Aksu,Gulsum Gencoglan*

Main category: cs.CV

TL;DR: 提出一种基于RT-DETR的Transformer检测框架，在高分辨率KOH显微图中精确定位皮癣真菌结构；在独立测试集上实现高召回与高AP，图像级诊断灵敏度达100%，显示其作为自动筛查工具的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统KOH显微镜检查易受伪影、角蛋白清除不均及观察者差异影响，导致真菌菌丝识别不准确；需要一个能在复杂背景下稳健定位真菌结构、并减少人工变异的自动化方法。

Method: 构建基于RT-DETR的Transformer目标检测模型，采用多类别标注将真菌元素与干扰伪影显式区分；使用保持形态的增强策略以保护细细菌丝结构；在2540张常规显微图上训练，并在独立测试集评估，输出目标级与图像级诊断。

Result: 目标级：召回0.9737，精度0.8043，AP@0.50为93.56%；图像级：灵敏度100%，准确率98.8%，未漏诊任何阳性；可在伪影丰富、低对比区域稳健定位菌丝。

Conclusion: 该AI系统可作为高度可靠的自动化筛查工具，有效连接图像级分析与临床决策，在皮肤真菌病学中具备临床转化潜力。

Abstract: Dermatophytosis is commonly assessed using potassium hydroxide (KOH) microscopy, yet accurate recognition of fungal hyphae is hindered by artefacts, heterogeneous keratin clearance, and notable inter-observer variability. This study presents a transformer-based detection framework using the RT-DETR model architecture to achieve precise, query-driven localization of fungal structures in high-resolution KOH images. A dataset of 2,540 routinely acquired microscopy images was manually annotated using a multi-class strategy to explicitly distinguish fungal elements from confounding artefacts. The model was trained with morphology-preserving augmentations to maintain the structural integrity of thin hyphae. Evaluation on an independent test set demonstrated robust object-level performance, with a recall of 0.9737, precision of 0.8043, and an AP@0.50 of 93.56%. When aggregated for image-level diagnosis, the model achieved 100% sensitivity and 98.8% accuracy, correctly identifying all positive cases without missing a single diagnosis. Qualitative outputs confirmed the robust localization of low-contrast hyphae even in artefact-rich fields. These results highlight that an artificial intelligence (AI) system can serve as a highly reliable, automated screening tool, effectively bridging the gap between image-level analysis and clinical decision-making in dermatomycology.

</details>


### [89] [Flash-VAED: Plug-and-Play VAE Decoders for Efficient Video Generation](https://arxiv.org/abs/2602.19161)
*Lunjie Zhu,Yushi Huang,Xingtong Ge,Yufei Xue,Zhening Liu,Yumeng Zhang,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: 提出Flash-VAED：通过通用VAE解码器加速框架，在保持潜在分布对齐的前提下，将视频生成端到端延迟显著降低；在Wan与LTX-Video上约6倍解码加速、重建保持至96.9%，VBench-2.0端到端提速最高36%。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer推理效率提升后，视频生成的延迟瓶颈转移到VAE解码器（尤其含因果3D卷积），解码成本高且通道冗余严重；需要在不破坏潜在分布与重建质量的情况下通用且高效地加速。

Method: 提出通用的VAE解码器加速框架：1) 基于独立性感知的通道剪枝，针对通道冗余而设计，并保持与原始潜在分布的对齐；2) 分阶段主导算子优化，重点优化代价高的因果3D卷积；在此基础上构建Flash-VAED系列；并提出三阶段动态蒸馏，将原VAE解码器能力有效迁移至加速模型。

Result: 在Wan与LTX-Video的VAE解码器上，较基线在质量与速度上均有优势：推理加速约6×，重建性能保持至96.9%；在VBench-2.0端到端视频生成中，加速最高36%，质量下降可忽略。

Conclusion: Flash-VAED在保持潜在分布对齐和重建质量的同时显著加速VAE解码推理，并带来端到端视频生成延迟的可观降低，具备通用性与实用价值。

Abstract: Latent diffusion models have enabled high-quality video synthesis, yet their inference remains costly and time-consuming. As diffusion transformers become increasingly efficient, the latency bottleneck inevitably shifts to VAE decoders. To reduce their latency while maintaining quality, we propose a universal acceleration framework for VAE decoders that preserves full alignment with the original latent distribution. Specifically, we propose (1) an independence-aware channel pruning method to effectively mitigate severe channel redundancy, and (2) a stage-wise dominant operator optimization strategy to address the high inference cost of the widely used causal 3D convolutions in VAE decoders. Based on these innovations, we construct a Flash-VAED family. Moreover, we design a three-phase dynamic distillation framework that efficiently transfers the capabilities of the original VAE decoder to Flash-VAED. Extensive experiments on Wan and LTX-Video VAE decoders demonstrate that our method outperforms baselines in both quality and speed, achieving approximately a 6$\times$ speedup while maintaining the reconstruction performance up to 96.9%. Notably, Flash-VAED accelerates the end-to-end generation pipeline by up to 36% with negligible quality drops on VBench-2.0.

</details>


### [90] [JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation](https://arxiv.org/abs/2602.19163)
*Kai Liu,Yanhao Zheng,Kai Wang,Shengqiong Wu,Rongjunchen Zhang,Jiebo Luo,Dimitrios Hatzinakos,Ziwei Liu,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: JavisDiT++提出统一的音视频生成框架，通过MS‑MoE、TA‑RoPE与AV‑DPO显著提升质量、时序同步与人偏好对齐，在有限公开数据上达SOTA，并开源全套资源。


<details>
  <summary>Details</summary>
Motivation: 开源JAVG方法在画面/声音质量、跨模态时序同步和与人类偏好的一致性方面落后于商业模型（如Veo3）；需要一个参数高效、可统一优化的框架缩小差距。

Method: 1) 模态特定的专家混合（MS‑MoE）：在共享骨干中引入按模态路由的专家，既强化单模态生成也促进跨模态交互；2) 时序对齐的位置编码（TA‑RoPE）：对音频与视频token进行帧级同步位置映射，实现显式时序对齐；3) 音视频直接偏好优化（AV‑DPO）：基于人类偏好信号在质量、一致性、同步性维度进行对齐训练；4) 以Wan2.1‑1.3B‑T2V为基座，约100万条公开数据训练与系统性消融。

Result: 在多项定性与定量评测中取得SOTA，显著优于先前开源方法；在有限数据规模下依然实现高质量、强同步与更好的人偏好一致性。

Conclusion: JavisDiT++以统一建模与优化框架有效提升JAVG的质量与同步，并通过MS‑MoE、TA‑RoPE和AV‑DPO形成可泛化的模块化方案；完整代码、模型与数据集已开源，便于复现与扩展。

Abstract: AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.

</details>


### [91] [BriMA: Bridged Modality Adaptation for Multi-Modal Continual Action Quality Assessment](https://arxiv.org/abs/2602.19170)
*Kanglei Zhou,Chang Li,Qingyi Pan,Liyuan Wang*

Main category: cs.CV

TL;DR: 提出BriMA，用于在模态缺失与分布漂移下的多模态持续动作质量评估，通过记忆引导的模态重建与模态感知回放，提升鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现实部署中多模态AQA常遇到传感器故障、标注缺口导致的模态间歇缺失与不平衡；现有持续学习AQA假设模态始终完备稳定，限制实用性。

Method: BriMA包含两部分：1) 记忆引导的桥接插补模块，联合任务无关与任务相关表征来重建缺失模态；2) 模态感知回放机制，依据模态失真与分布漂移度量，优先重放更具信息量的样本，以缓解遗忘并适应非平稳模态。

Result: 在RG、Fis-V、FS1000三数据集、不同模态缺失情景下，相关性提升约6–8%，误差降低约12–15%，稳定优于现有方法。

Conclusion: BriMA有效应对非平稳模态缺失的多模态持续AQA，提升鲁棒性与泛化，向可在真实环境中部署的AQA系统迈进一步。

Abstract: Action Quality Assessment (AQA) aims to score how well an action is performed and is widely used in sports analysis, rehabilitation assessment, and human skill evaluation. Multi-modal AQA has recently achieved strong progress by leveraging complementary visual and kinematic cues, yet real-world deployments often suffer from non-stationary modality imbalance, where certain modalities become missing or intermittently available due to sensor failures or annotation gaps. Existing continual AQA methods overlook this issue and assume that all modalities remain complete and stable throughout training, which restricts their practicality. To address this challenge, we introduce Bridged Modality Adaptation (BriMA), an innovative approach to multi-modal continual AQA under modality-missing conditions. BriMA consists of a memory-guided bridging imputation module that reconstructs missing modalities using both task-agnostic and task-specific representations, and a modality-aware replay mechanism that prioritizes informative samples based on modality distortion and distribution drift. Experiments on three representative multi-modal AQA datasets (RG, Fis-V, and FS1000) show that BriMA consistently improves performance under different modality-missing conditions, achieving 6--8\% higher correlation and 12--15\% lower error on average. These results demonstrate a step toward robust multi-modal AQA systems under real-world deployment constraints.

</details>


### [92] [EMAD: Evidence-Centric Grounded Multimodal Diagnosis for Alzheimer's Disease](https://arxiv.org/abs/2602.19178)
*Qiuhui Chen,Xuancheng Yao,Zhenglei Zhou,Xinyue Hu,Yi Hong*

Main category: cs.CV

TL;DR: 论文提出EMAD框架，用分层句子-证据-解剖(SEA)定位机制，将阿尔茨海默病诊断报告中的每条陈述与多模态证据及3D MRI解剖部位显式绑定；结合GTX-Distill蒸馏以降低标注成本，并用可执行规则的GRPO强化微调保证临床一致性与推理-诊断连贯性；在AD-MultiSense上取得SOTA准确率与更透明的、解剖一致的报告。


<details>
  <summary>Details</summary>
Motivation: 医疗影像AI常是“黑箱”，缺乏与临床指南对齐和可验证证据支撑；在AD场景中，诊断应同时依据解剖学与临床线索，因此需要一个既能生成结构化报告又能显式溯源到证据与解剖的可信VLM。

Method: 1) EMAD框架：多模态视觉-语言模型，生成结构化AD诊断报告。2) SEA分层定位：a) 句子→证据短语对齐；b) 证据→3D MRI解剖结构定位。3) GTX-Distill：以有限监督训练的教师模型向学生迁移“定位/溯源”能力，学生在模型自生成报告上学习以减少密集标注。4) Executable-Rule GRPO：引入可执行临床规则作为可验证奖励，进行强化微调，约束临床一致性、流程合规与推理-诊断一致性。5) 在AD-MultiSense数据集上评估，与现有方法比较。

Result: 在AD-MultiSense上达到SOTA诊断准确率；所生成报告在透明度与解剖一致性上优于现有方法；提供可复现实验与将开源代码及grounding标注。

Conclusion: 通过SEA定位、蒸馏与基于规则的强化微调，EMAD将诊断陈述与多模态证据和解剖位置显式绑定，实现更可信、可审计的AD报告生成；方法减少标注开销并提升临床一致性，适合推动医疗VLM的可解释与可信发展。

Abstract: Deep learning models for medical image analysis often act as black boxes, seldom aligning with clinical guidelines or explicitly linking decisions to supporting evidence. This is especially critical in Alzheimer's disease (AD), where predictions should be grounded in both anatomical and clinical findings. We present EMAD, a vision-language framework that generates structured AD diagnostic reports in which each claim is explicitly grounded in multimodal evidence. EMAD uses a hierarchical Sentence-Evidence-Anatomy (SEA) grounding mechanism: (i) sentence-to-evidence grounding links generated sentences to clinical evidence phrases, and (ii) evidence-to-anatomy grounding localizes corresponding structures on 3D brain MRI. To reduce dense annotation requirements, we propose GTX-Distill, which transfers grounding behavior from a teacher trained with limited supervision to a student operating on model-generated reports. We further introduce Executable-Rule GRPO, a reinforcement fine-tuning scheme with verifiable rewards that enforces clinical consistency, protocol adherence, and reasoning-diagnosis coherence. On the AD-MultiSense dataset, EMAD achieves state-of-the-art diagnostic accuracy and produces more transparent, anatomically faithful reports than existing methods. We will release code and grounding annotations to support future research in trustworthy medical vision-language models.

</details>


### [93] [VLM-Guided Group Preference Alignment for Diffusion-based Human Mesh Recovery](https://arxiv.org/abs/2602.19180)
*Wenhao Shen,Hao Wang,Wanqi Yin,Fayao Liu,Xulei Yang,Chao Liang,Zhongang Cai,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出一个带双记忆与自反机制的“批判代理”给单图HMR生成结果打质量分，并用组偏好对齐微调扩散式HMR，使其既多样又更物理可行且与图像一致，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 单张图像HMR存在多解性；扩散方法虽能生成多假设，但常因遮挡/杂乱场景而牺牲精度，产生不物理或与输入不一致的结果，需要一个能判别、引导模型生成更合理网格的机制。

Method: 1) 设计带双记忆与自反的HMR批判代理，输出对每个预测网格的上下文感知质量分，覆盖3D结构、物理可行性与与图像对齐度；2) 以这些评分构建“组内偏好”数据集；3) 基于该数据提出组偏好对齐框架，对扩散式HMR模型进行微调，将偏好信号注入模型生成过程。

Result: 在多组实验中，方法在物理可行性与图像一致性上显著提升，并在标准基准上优于SOTA扩散式与非扩散式HMR方法。

Conclusion: 通过引入批判代理生成细粒度偏好信号，并用组偏好对齐微调扩散HMR，可在保持多样性的同时提升准确性与鲁棒性，适用于遮挡与野外复杂场景。

Abstract: Human mesh recovery (HMR) from a single RGB image is inherently ambiguous, as multiple 3D poses can correspond to the same 2D observation. Recent diffusion-based methods tackle this by generating various hypotheses, but often sacrifice accuracy. They yield predictions that are either physically implausible or drift from the input image, especially under occlusion or in cluttered, in-the-wild scenes. To address this, we introduce a dual-memory augmented HMR critique agent with self-reflection to produce context-aware quality scores for predicted meshes. These scores distill fine-grained cues about 3D human motion structure, physical feasibility, and alignment with the input image. We use these scores to build a group-wise HMR preference dataset. Leveraging this dataset, we propose a group preference alignment framework for finetuning diffusion-based HMR models. This process injects the rich preference signals into the model, guiding it to generate more physically plausible and image-consistent human meshes. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches.

</details>


### [94] [PositionOCR: Augmenting Positional Awareness in Multi-Modal Models via Hybrid Specialist Integration](https://arxiv.org/abs/2602.19188)
*Chen Duan,Zhentao Guo,Pei Fu,Zining Wang,Kai Zhou,Pengfei Yan*

Main category: cs.CV

TL;DR: 提出PositionOCR：一种参数高效的混合MLLM，将文本检测/识别专长与LLM语义推理结合，在文本定位与文本识别/指向任务上优于传统MLLM。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM依赖LLM解码器，擅长语言但缺乏精确位置推理，难以胜任文本指向、文本定位等；且参数庞大、训练成本高。相反，文本识别专用模型在坐标预测上强但缺语义推理。需要兼具两者优点的体系。

Method: 设计PositionOCR：将文本定位/识别专用模型（负责精确坐标与视觉特征）与LLM（负责上下文与语义推理）无缝集成；通过参数高效的混合架构（总可训练参数131M）实现多模态输入处理，并将定位信息与语言推理耦合用于VQA、文本指向与文本检测。

Result: 在多模态任务上，尤其是文本定位（text grounding）与文本识别/指向（text spotting）方面，性能稳定优于传统MLLM；以较少参数与训练资源实现更强定位精度与整体VQA表现。

Conclusion: 将专用OCR定位能力与LLM语义能力融合可得到位置准确、参数高效的MLLM。PositionOCR验证了该范式的有效性，并在多项OCR中心任务上取得SOTA或超越通用MLLM的结果。

Abstract: In recent years, Multi-modal Large Language Models (MLLMs) have achieved strong performance in OCR-centric Visual Question Answering (VQA) tasks, illustrating their capability to process heterogeneous data and exhibit adaptability across varied contexts. However, these MLLMs rely on a Large Language Model (LLM) as the decoder, which is primarily designed for linguistic processing, and thus inherently lacks the positional reasoning required for precise visual tasks, such as text spotting and text grounding. Additionally, the extensive parameters of MLLMs necessitate substantial computational resources and large-scale data for effective training. Conversely, text spotting specialists achieve state-of-the-art coordinate predictions but lack semantic reasoning capabilities. This dichotomy motivates our key research question: Can we synergize the efficiency of specialists with the contextual power of LLMs to create a positionally-accurate MLLM? To overcome these challenges, we introduce PositionOCR, a parameter-efficient hybrid architecture that seamlessly integrates a text spotting model's positional strengths with an LLM's contextual reasoning. Comprising 131M trainable parameters, this framework demonstrates outstanding multi-modal processing capabilities, particularly excelling in tasks such as text grounding and text spotting, consistently surpassing traditional MLLMs.

</details>


### [95] [FUSAR-GPT : A Spatiotemporal Feature-Embedded and Two-Stage Decoupled Visual Language Model for SAR Imagery](https://arxiv.org/abs/2602.19190)
*Xiaokun Zhang,Yi Yang,Ziqi Ye,Baiyun,Xiaorong Guo,Qingchen Fang,Ruyi Zhang,Xinpeng Zhou,Haipeng Wang*

Main category: cs.CV

TL;DR: 本文提出面向SAR的VLM——FUSAR-GPT，并发布首个SAR图文-AlphaEarth特征三联体数据集；通过地理空间基线“世界知识”先验、时空锚点特征注入与两阶段SFT解耦训练，在多项遥感视听语言基准上取得SOTA，较主流基线提升超12%。


<details>
  <summary>Details</summary>
Motivation: 通用VLM在RGB上表现强，但直接迁移至SAR受限：成像机理复杂、散射敏感、优质文本语料稀缺，导致语义理解与跨模态对齐不佳。亟需面向SAR的专用模型与数据，以提升全时全候智能解译能力。

Method: 1) 构建SAR图文-AlphaEarth特征三联体数据集，为跨模态对齐引入丰富地理与时序先验；2) 设计FUSAR-GPT：在视觉骨干中通过“时空锚点”嵌入多源遥感时序特征，动态补偿SAR目标稀疏表达；3) 引入地理空间基线模型作为“世界知识”先验，提供地理与语义背景；4) 提出两阶段SFT：先知识注入再任务执行，解耦大模型的知识学习与能力对齐。

Result: 在多项典型遥感视觉-语言基准上取得SOTA，整体较主流基线提升>12%，证明时空特征注入与解耦SFT的有效性。

Conclusion: 通过数据、先验与训练范式的协同，FUSAR-GPT显著提升SAR多模态理解能力，为全时全候遥感解译提供可行路径与新基线。

Abstract: Research on the intelligent interpretation of all-weather, all-time Synthetic Aperture Radar (SAR) is crucial for advancing remote sensing applications. In recent years, although Visual Language Models (VLMs) have demonstrated strong open-world understanding capabilities on RGB images, their performance is severely limited when directly applied to the SAR field due to the complexity of the imaging mechanism, sensitivity to scattering features, and the scarcity of high-quality text corpora. To systematically address this issue, we constructed the inaugural SAR Image-Text-AlphaEarth feature triplet dataset and developed FUSAR-GPT, a VLM specifically for SAR. FUSAR-GPT innovatively introduces a geospatial baseline model as a 'world knowledge' prior and embeds multi-source remote-sensing temporal features into the model's visual backbone via 'spatiotemporal anchors', enabling dynamic compensation for the sparse representation of targets in SAR images. Furthermore, we designed a two-stage SFT strategy to decouple the knowledge injection and task execution of large models. The spatiotemporal feature embedding and the two-stage decoupling paradigm enable FUSAR-GPT to achieve state-of-the-art performance across several typical remote sensing visual-language benchmark tests, significantly outperforming mainstream baseline models by over 12%.

</details>


### [96] [Prompt Tuning for CLIP on the Pretrained Manifold](https://arxiv.org/abs/2602.19198)
*Xi Yang,Yuanrong Xu,Weigang Zhang,Guangming Lu,David Zhang,Jie Wen*

Main category: cs.CV

TL;DR: 提出ManiPT，在预训练流形上进行提示调优，通过余弦一致性和结构偏置缓解小样本过拟合并提升跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 传统提示调优在监督有限时会扭曲预训练表示，使下游特征偏离预训练流形，导致泛化下降。需要一种方法在保留预训练几何结构的同时进行参数高效的适配。

Method: 在文本与图像两个模态引入余弦一致性约束，使学习到的表示保持在预训练表示的几何邻域内；同时加入结构偏置（incremental corrections），沿可迁移方向逐步校正，减少对捷径特征的依赖。给出理论分析说明该设计在小数据下能缓解过拟合。

Result: 在四类设置（未见类泛化、少样本分类、跨数据集迁移、域泛化）上，平均性能优于多种基线方法。

Conclusion: 在有限监督下，提示调优的过拟合部分源于偏离预训练流形；ManiPT通过流形内调优与结构偏置有效抑制这种漂移，提升多场景泛化。

Abstract: Prompt tuning introduces learnable prompt vectors that adapt pretrained vision-language models to downstream tasks in a parameter-efficient manner. However, under limited supervision, prompt tuning alters pretrained representations and drives downstream features away from the pretrained manifold toward directions that are unfavorable for transfer. This drift degrades generalization. To address this limitation, we propose ManiPT, a framework that performs prompt tuning on the pretrained manifold. ManiPT introduces cosine consistency constraints in both the text and image modalities to confine the learned representations within the pretrained geometric neighborhood. Furthermore, we introduce a structural bias that enforces incremental corrections, guiding the adaptation along transferable directions to mitigate reliance on shortcut learning. From a theoretical perspective, ManiPT alleviates overfitting tendencies under limited data. Our experiments cover four downstream settings: unseen-class generalization, few-shot classification, cross-dataset transfer, and domain generalization. Across these settings, ManiPT achieves higher average performance than baseline methods. Notably, ManiPT provides an explicit perspective on how prompt tuning overfits under limited supervision.

</details>


### [97] [UniE2F: A Unified Diffusion Framework for Event-to-Frame Reconstruction with Video Foundation Models](https://arxiv.org/abs/2602.19202)
*Gang Xu,Zhiyu Zhu,Junhui Hou*

Main category: cs.CV

TL;DR: 利用预训练视频扩散模型的生成先验，从稀疏事件相机数据重建高保真视频帧，并在零样本设定下统一支持重建、插帧与预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机仅记录亮度变化而非绝对强度，导致静态纹理与空间细节缺失，限制了在真实场景中直接获得高质量帧的能力；需要一种能补足纹理与结构细节的重建方法。

Method: 以预训练视频扩散模型为核心：1) 基线——将事件流直接作为条件输入扩散模型进行视频合成；2) 基于事件与视频帧的物理关联，引入事件驱动的帧间残差引导（inter-frame residual guidance），强化还原的准确性；3) 通过调制反向扩散采样过程，在零样本下扩展到视频插帧与未来帧预测，形成统一的事件到帧（UniE2F）框架。

Result: 在真实与合成数据集上，方法在定量与定性评价上均显著优于以往事件到图像/视频重建方法；附带视频演示，代码将开源（GitHub: CS-GangXu/UniE2F）。

Conclusion: 预训练视频扩散模型的生成先验结合事件残差引导，能够从稀疏事件流重建高保真视频，并在零样本下统一支持插帧与预测，显著提升事件相机数据的可用性与重建质量。

Abstract: Event cameras excel at high-speed, low-power, and high-dynamic-range scene perception. However, as they fundamentally record only relative intensity changes rather than absolute intensity, the resulting data streams suffer from a significant loss of spatial information and static texture details. In this paper, we address this limitation by leveraging the generative prior of a pre-trained video diffusion model to reconstruct high-fidelity video frames from sparse event data. Specifically, we first establish a baseline model by directly applying event data as a condition to synthesize videos. Then, based on the physical correlation between the event stream and video frames, we further introduce the event-based inter-frame residual guidance to enhance the accuracy of video frame reconstruction. Furthermore, we extend our method to video frame interpolation and prediction in a zero-shot manner by modulating the reverse diffusion sampling process, thereby creating a unified event-to-frame reconstruction framework. Experimental results on real-world and synthetic datasets demonstrate that our method significantly outperforms previous approaches both quantitatively and qualitatively. We also refer the reviewers to the video demo contained in the supplementary material for video results. The code will be publicly available at https://github.com/CS-GangXu/UniE2F.

</details>


### [98] [GS-CLIP: Zero-shot 3D Anomaly Detection by Geometry-Aware Prompt and Synergistic View Representation Learning](https://arxiv.org/abs/2602.19206)
*Zehao Deng,An Liu,Yan Wang*

Main category: cs.CV

TL;DR: 提出GS-CLIP用于零样本3D异常检测，通过几何先验提示和多视图协同学习，在无目标数据训练下提升几何异常识别，四个大规模数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有将3D点云投影为2D再用CLIP的方法会丢失几何细节且仅依赖单一2D模态，导致对多样化异常类型的检测不足；需要能保留/注入3D几何信息并融合多视图模态的方案。

Method: 两阶段框架：阶段一生成含3D几何先验的文本提示（由几何缺陷蒸馏模块GDDM提炼全局形状与局部缺陷信息）；阶段二采用协同视图表征学习并行处理渲染图与深度图，通过协同精炼模块SRM融合两路特征，发挥互补性。

Result: 在四个大规模公开数据集上检测性能优于现有方法（SOTA），实现更强的几何异常识别与零样本泛化；代码开源。

Conclusion: 将3D几何先验注入文本提示并协同融合渲染与深度视图，可缓解投影信息丢失与单模态限制，显著提升零样本3D异常检测效果。

Abstract: Zero-shot 3D Anomaly Detection is an emerging task that aims to detect anomalies in a target dataset without any target training data, which is particularly important in scenarios constrained by sample scarcity and data privacy concerns. While current methods adapt CLIP by projecting 3D point clouds into 2D representations, they face challenges. The projection inherently loses some geometric details, and the reliance on a single 2D modality provides an incomplete visual understanding, limiting their ability to detect diverse anomaly types. To address these limitations, we propose the Geometry-Aware Prompt and Synergistic View Representation Learning (GS-CLIP) framework, which enables the model to identify geometric anomalies through a two-stage learning process. In stage 1, we dynamically generate text prompts embedded with 3D geometric priors. These prompts contain global shape context and local defect information distilled by our Geometric Defect Distillation Module (GDDM). In stage 2, we introduce Synergistic View Representation Learning architecture that processes rendered and depth images in parallel. A Synergistic Refinement Module (SRM) subsequently fuses the features of both streams, capitalizing on their complementary strengths. Comprehensive experimental results on four large-scale public datasets show that GS-CLIP achieves superior performance in detection. Code can be available at https://github.com/zhushengxinyue/GS-CLIP.

</details>


### [99] [SegMoTE: Token-Level Mixture of Experts for Medical Image Segmentation](https://arxiv.org/abs/2602.19213)
*Yujie Lu,Jingwen Li,Sibo Ju,Yanzhou Su,he yao,Yisong Liu,Min Zhu,Junlong Cheng*

Main category: cs.CV

TL;DR: 提出SegMoTE：在保持SAM交互接口与零样本能力的前提下，用少量可学习参数实现对不同医学成像模态与解剖任务的自适应，并通过渐进式提示标记化实现全自动分割；用<1%数据量在多模态、多器官任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 通用交互分割模型（如SAM）迁移到医学影像时受限于：1）缺乏针对模态/解剖差异的自适应机制，OOD场景泛化差；2）现有医学适配多在大而杂的集合上无选择微调，监督噪声大、成本高、易负迁移。

Method: 构建高效自适应框架SegMoTE：在不改变SAM推理效率与提示接口的前提下，引入极少量可学习参数以动态适配不同模态与任务；提出“渐进式提示标记化（progressive prompt tokenization）”，由稀疏交互提示逐步扩展到密集提示，实现全自动分割并降低对像素级标注的依赖；在精选小规模MedSeg-HQ上训练。

Result: 在多种医学成像模态（如CT/MRI/超声等）与多解剖任务上达到SOTA；在极低标注成本（训练数据量<现有大规模数据集的1%）下，保持高精度与稳健泛化。

Conclusion: SegMoTE首次在极低标注成本下，实现对通用分割模型向医学领域的高效、鲁棒、可扩展适配，保留零样本与交互优势并实现自动化，推动基础视觉模型在临床落地。

Abstract: Medical image segmentation is vital for clinical diagnosis and quantitative analysis, yet remains challenging due to the heterogeneity of imaging modalities and the high cost of pixel-level annotations. Although general interactive segmentation models like SAM have achieved remarkable progress, their transfer to medical imaging still faces two key bottlenecks: (i) the lack of adaptive mechanisms for modality- and anatomy-specific tasks, which limits generalization in out-of-distribution medical scenarios; and (ii) current medical adaptation methods fine-tune on large, heterogeneous datasets without selection, leading to noisy supervision, higher cost, and negative transfer. To address these issues, we propose SegMoTE, an efficient and adaptive framework for medical image segmentation. SegMoTE preserves SAM's original prompt interface, efficient inference, and zero-shot generalization while introducing only a small number of learnable parameters to dynamically adapt across modalities and tasks. In addition, we design a progressive prompt tokenization mechanism that enables fully automatic segmentation, significantly reducing annotation dependence. Trained on MedSeg-HQ, a curated dataset less than 1% of existing large-scale datasets, SegMoTE achieves SOTA performance across diverse imaging modalities and anatomical tasks. It represents the first efficient, robust, and scalable adaptation of general segmentation models to the medical domain under extremely low annotation cost, advancing the practical deployment of foundation vision models in clinical applications.

</details>


### [100] [Questions beyond Pixels: Integrating Commonsense Knowledge in Visual Question Generation for Remote Sensing](https://arxiv.org/abs/2602.19217)
*Siran Li,Li Mi,Javiera Castillo-Navarro,Devis Tuia*

Main category: cs.CV

TL;DR: 论文提出KRSVQG模型，通过引入外部知识三元组与图像字幕中介表征，生成更丰富且贴合遥感图像与常识的提问；并在两个新构建数据集上验证，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像自动提问多为模板化、表面化，难以支持真实场景中的问答/对话系统。需要把图像内容与人类常识、领域知识结合，生成更有深度与多样性的提问，提升语义检索与交互质量。

Method: 1) 知识引入：从外部知识库检索与图像相关的知识三元组，扩展问题语义空间。2) 表达中介：先做图像字幕生成，将问题与具体图像内容对齐。3) 视觉-语言预训练+微调：用VL预训练权重，在小样本条件下适配任务。4) 数据集构建：制作两个知识感知的遥感VQG数据集（NWPU-300、TextRS-300）。

Result: 在自动评价指标与人工评测上均优于现有VQG方法，生成的问题更丰富、多样且与图像和领域知识更一致，证明方法有效。

Conclusion: 引入外部知识并以字幕为锚的KRSVQG能突破像素层理解，生成具备常识与领域知识的高质量问题，推动知识增强的视觉-语言系统发展，尤其适用于低数据场景。

Abstract: With the rapid development of remote sensing image archives, asking questions about images has become an effective way of gathering specific information or performing semantic image retrieval. However, current automatically generated questions tend to be simplistic and template-based, which hinders the deployment of question answering or visual dialogue systems for real-world applications. To enrich and diversify the questions with both image content and commonsense knowledge, we propose a Knowledge-aware Remote Sensing Visual Question Generation model (KRSVQG). The proposed model incorporates related knowledge triplets from external knowledge sources to broaden the question content, while employing image captioning as an intermediary representation to ground questions to the corresponding images. Moreover, KRSVQG utilizes a vision-language pre-training and fine-tuning strategy, enabling the model's adaptation to low data regimes. To evaluate the proposed KRSVQG model, we construct two knowledge-aware remote sensing visual question generation datasets: the NWPU-300 dataset and the TextRS-300 dataset. Evaluations, including metrics and human assessment, demonstrate that KRSVQG outperforms existing methods and leads to rich questions, grounded in both image and domain knowledge. As a key practice in vision-language research, knowledge-aware visual question generation advances the understanding of image content beyond pixels, facilitating the development of knowledge-enriched vision-language systems with vision-grounded human commonsense.

</details>


### [101] [Controlled Face Manipulation and Synthesis for Data Augmentation](https://arxiv.org/abs/2602.19219)
*Joris Kirchner,Amogh Gudi,Marian Bittner,Chirag Raman*

Main category: cs.CV

TL;DR: 作者提出一种在预训练人脸生成器的语义潜空间中进行可控AU（面部动作单元）编辑的方法，用于缓解表情识别中的标注稀缺与类别不均，利用依赖感知条件与正交投影降低属性纠缠，并通过中和步骤实现绝对强度编辑；生成数据用于平衡与多样化训练，显著提升AU检测精度、减少共激活捷径，优于其它数据高效策略且编辑更强、伪影更少、身份保持更好。


<details>
  <summary>Details</summary>
Motivation: 表情分析任务面临：1) 标注昂贵导致样本稀缺与类不均衡；2) AU之间存在共激活引发语义纠缠；3) 现有可控编辑常产生伪影且牵连非目标属性，影响数据增广的有效性与公平性。需要一种能精准、可控且低伪影的AU编辑来合成高质量、平衡且多样的数据，以减少对大量标注的依赖。

Method: 在预训练Diffusion Autoencoder的语义潜空间中进行编辑：
- 使用轻量线性模型学习语义方向；
- 依赖感知条件（考虑AU共激活分布）以减弱目标外属性的耦合；
- 正交投影去除干扰属性方向（如眼镜等）；
- 表情中和步骤，支持绝对AU强度的编辑；
- 用编辑过的标注人脸平衡AU分布，并通过可控合成扩展身份与人群多样性。

Result: 将生成数据用于AU检测器训练后：总体准确率提升，预测更解耦、对共激活捷径依赖减少；在数据效率上优于替代策略，学习曲线显示接近显著增加标注数据才能达到的改进；与过往编辑方法相比，所生成图像伪影更少、编辑幅度更大且身份保持更佳。

Conclusion: 在语义潜空间进行依赖感知与正交化的AU可控编辑，能有效缓解稀缺与不均衡，通过高质量数据增广显著提升AU检测性能与可解释性，是比现有数据高效方法更有效且更实用的路线。

Abstract: Deep learning vision models excel with abundant supervision, but many applications face label scarcity and class imbalance. Controllable image editing can augment scarce labeled data, yet edits often introduce artifacts and entangle non-target attributes. We study this in facial expression analysis, targeting Action Unit (AU) manipulation where annotation is costly and AU co-activation drives entanglement. We present a facial manipulation method that operates in the semantic latent space of a pre-trained face generator (Diffusion Autoencoder). Using lightweight linear models, we reduce entanglement of semantic features via (i) dependency-aware conditioning that accounts for AU co-activation, and (ii) orthogonal projection that removes nuisance attribute directions (e.g., glasses), together with an expression neutralization step to enable absolute AU edit. We use these edits to balance AU occurrence by editing labeled faces and to diversify identities/demographics via controlled synthesis. Augmenting AU detector training with the generated data improves accuracy and yields more disentangled predictions with fewer co-activation shortcuts, outperforming alternative data-efficient training strategies and suggesting improvements similar to what would require substantially more labeled data in our learning-curve analysis. Compared to prior methods, our edits are stronger, produce fewer artifacts, and preserve identity better.

</details>


### [102] [Knowledge-aware Visual Question Generation for Remote Sensing Images](https://arxiv.org/abs/2602.19224)
*Siran Li,Li Mi,Javiera Castillo-Navarro,Devis Tuia*

Main category: cs.CV

TL;DR: 提出KRSVQG模型：结合外部知识三元组与图像字幕，提高遥感图像问题生成的多样性与语义深度；在NWPU-300与TextRS-300上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成的图像问题多为模板化、浅层，难以支撑真实场景中的问答与视觉对话；遥感场景复杂且依赖领域知识，需引入外部知识以生成更丰富、具备上下文的提问。

Method: 提出知识感知的遥感视觉问题生成模型KRSVQG：输入为图像与与之相关的外部知识三元组；通过图像字幕（caption）作为中间表征以强化图像对齐与语义落地；利用知识与视觉融合生成问题，使问题同时基于图像内容与领域知识。

Result: 在作者人工标注的NWPU-300与TextRS-300两个数据集上评测，KRSVQG在多项指标上优于已有方法，生成的问题更具知识性与情境相关性。

Conclusion: 引入外部知识并通过字幕中介可显著提升遥感图像问题生成的质量与多样性；KRSVQG能够产生既与图像对齐又蕴含领域知识的问题，推动遥感领域问答/对话系统的发展。

Abstract: With the rapid development of remote sensing image archives, asking questions about images has become an effective way of gathering specific information or performing image retrieval. However, automatically generated image-based questions tend to be simplistic and template-based, which hinders the real deployment of question answering or visual dialogue systems. To enrich and diversify the questions, we propose a knowledge-aware remote sensing visual question generation model, KRSVQG, that incorporates external knowledge related to the image content to improve the quality and contextual understanding of the generated questions. The model takes an image and a related knowledge triplet from external knowledge sources as inputs and leverages image captioning as an intermediary representation to enhance the image grounding of the generated questions. To assess the performance of KRSVQG, we utilized two datasets that we manually annotated: NWPU-300 and TextRS-300. Results on these two datasets demonstrate that KRSVQG outperforms existing methods and leads to knowledge-enriched questions, grounded in both image and domain knowledge.

</details>


### [103] [No Need For Real Anomaly: MLLM Empowered Zero-Shot Video Anomaly Detection](https://arxiv.org/abs/2602.19248)
*Zunkai Dai,Ke Li,Jiajia Liu,Jie Yang,Yuanyuan Qiao*

Main category: cs.CV

TL;DR: 提出LAVIDA：零样本视频异常检测框架，利用伪异常暴露采样+MLLM语义增强+基于反向注意的token压缩，无需VAD数据训练，在四个基准上达成SOTA（帧级与像素级）。


<details>
  <summary>Details</summary>
Motivation: 现有VAD在开放世界下表现欠佳，原因在于：异常样本稀缺、数据多样性不足、对上下文相关异常语义理解不足，且异常时空信号稀疏、计算成本高。

Method: i) 端到端零样本框架LAVIDA；ii) 异常暴露采样器：将分割到的目标转化为伪异常，扩展类别覆盖并提升对未见异常的适应性；结合多模态大语言模型（MLLM）增强语义理解；iii) 基于反向注意的token压缩，针对异常时空稀缺性进行关键token保留与压缩以降算；训练仅使用伪异常、无真实VAD数据。

Result: 在4个VAD基准数据集上，零样本设定下于帧级和像素级检测均取得SOTA性能。

Conclusion: 通过伪异常暴露+MLLM语义增强+反向注意压缩，LAVIDA在无需VAD训练数据的前提下有效提升开放世界VAD性能并降低计算开销。

Abstract: The collection and detection of video anomaly data has long been a challenging problem due to its rare occurrence and spatio-temporal scarcity. Existing video anomaly detection (VAD) methods under perform in open-world scenarios. Key contributing factors include limited dataset diversity, and inadequate understanding of context-dependent anomalous semantics. To address these issues, i) we propose LAVIDA, an end-to-end zero-shot video anomaly detection framework. ii) LAVIDA employs an Anomaly Exposure Sampler that transforms segmented objects into pseudo-anomalies to enhance model adaptability to unseen anomaly categories. It further integrates a Multimodal Large Language Model (MLLM) to bolster semantic comprehension capabilities. Additionally, iii) we design a token compression approach based on reverse attention to handle the spatio-temporal scarcity of anomalous patterns and decrease computational cost. The training process is conducted solely on pseudo anomalies without any VAD data. Evaluations across four benchmark VAD datasets demonstrate that LAVIDA achieves SOTA performance in both frame-level and pixel-level anomaly detection under the zero-shot setting. Our code is available in https://github.com/VitaminCreed/LAVIDA.

</details>


### [104] [RegionRoute: Regional Style Transfer with Diffusion Model](https://arxiv.org/abs/2602.19254)
*Bowen Chen,Jake Zuena,Alan C. Bovik,Divya Kothandaraman*

Main category: cs.CV

TL;DR: 提出一种注意力监督的扩散风格迁移框架，通过将风格token的注意力与目标物体掩码对齐，实现无需掩码的局部风格编辑，辅以Focus与Cover损失及LoRA-MoE以高效多风格适配，并引入区域风格编辑评分，实验显示在区域准确性与保真度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型将风格视为全局属性、缺乏空间锚定，导致风格难以精准限定在特定物体/区域；现有方法依赖手工掩码或多阶段后处理，产生边界伪影、泛化性差，缺乏真正的局部风格迁移能力。

Method: 提出注意力监督扩散：训练时将风格token的注意力得分与目标对象掩码对齐；设计两种互补目标：基于KL散度的Focus loss促进注意力在目标内聚焦，基于BCE的Cover loss促进目标区域的稠密覆盖；采用模块化LoRA-MoE结构以参数高效地扩展到多风格；评估上提出Regional Style Editing Score，综合区域内CLIP相似度（风格匹配）与未编辑区域的掩码LPIPS/像素一致性（身份保持）。

Result: 在推理时无需掩码即可实现单对象局部风格迁移，生成结果区域定位准确、边界干净、整体视觉连贯；在新指标及既有编辑基准上，较现有扩散编辑方法在区域匹配与身份保持方面取得更优表现。

Conclusion: 通过注意力与对象掩码对齐的监督训练，可为扩散模型引入显式空间风格归因，实现真正的掩码-free局部风格编辑；配合LoRA-MoE实现高效多风格扩展，新提出的评测指标客观量化区域风格匹配与保真度，方法在定量与定性上均优于现有方案。

Abstract: Precise spatial control in diffusion-based style transfer remains challenging. This challenge arises because diffusion models treat style as a global feature and lack explicit spatial grounding of style representations, making it difficult to restrict style application to specific objects or regions. To our knowledge, existing diffusion models are unable to perform true localized style transfer, typically relying on handcrafted masks or multi-stage post-processing that introduce boundary artifacts and limit generalization. To address this, we propose an attention-supervised diffusion framework that explicitly teaches the model where to apply a given style by aligning the attention scores of style tokens with object masks during training. Two complementary objectives, a Focus loss based on KL divergence and a Cover loss using binary cross-entropy, jointly encourage accurate localization and dense coverage. A modular LoRA-MoE design further enables efficient and scalable multi-style adaptation. To evaluate localized stylization, we introduce the Regional Style Editing Score, which measures Regional Style Matching through CLIP-based similarity within the target region and Identity Preservation via masked LPIPS and pixel-level consistency on unedited areas. Experiments show that our method achieves mask-free, single-object style transfer at inference, producing regionally accurate and visually coherent results that outperform existing diffusion-based editing approaches.

</details>


### [105] [DD-CAM: Minimal Sufficient Explanations for Vision Models Using Delta Debugging](https://arxiv.org/abs/2602.19274)
*Krishna Khadka,Yu Lei,Raghu N. Kacker,D. Richard Kuhn*

Main category: cs.CV

TL;DR: 提出DD-CAM：在视觉模型中用无梯度方法找出最小且足以保持决策的单元子集，并生成更精确的显著图。


<details>
  <summary>Details</summary>
Motivation: 现有基于CAM的解释方法往往聚合大量单元，产生杂乱的显著图，缺乏最小且决策保持的解释；需要一种能识别最小充分单元集合、提高解释忠实度与定位精度的方法。

Method: 引入无梯度框架DD-CAM：通过隔离能保持预测的最小1-极小单元子集（去掉任一单元即改变预测）。借鉴软件调试中的delta debugging进行系统化缩减搜索；根据分类头中单元交互性配置搜索策略：若无交互则单元级测试，存在交互则组合级测试；最终以该子集激活生成最小、预测保持的显著图。

Result: 实验表明，相较SOTA基于CAM的方法，DD-CAM生成的解释更忠实，并在目标定位等指标上取得更高精度。

Conclusion: DD-CAM能高效找到最小充分决策单元并生成紧凑、预测保持的显著图，提升解释的可用性与定位准确性，对不同交互结构的模型具有适配性。

Abstract: We introduce a gradient-free framework for identifying minimal, sufficient, and decision-preserving explanations in vision models by isolating the smallest subset of representational units whose joint activation preserves predictions. Unlike existing approaches that aggregate all units, often leading to cluttered saliency maps, our approach, DD-CAM, identifies a 1-minimal subset whose joint activation suffices to preserve the prediction (i.e., removing any unit from the subset alters the prediction). To efficiently isolate minimal sufficient subsets, we adapt delta debugging, a systematic reduction strategy from software debugging, and configure its search strategy based on unit interactions in the classifier head: testing individual units for models with non-interacting units and testing unit combinations for models in which unit interactions exist. We then generate minimal, prediction-preserving saliency maps that highlight only the most essential features. Our experimental evaluation demonstrates that our approach can produce more faithful explanations and achieve higher localization accuracy than the state-of-the-art CAM-based approaches.

</details>


### [106] [A Two-Stage Detection-Tracking Framework for Stable Apple Quality Inspection in Dense Conveyor-Belt Environments](https://arxiv.org/abs/2602.19278)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 提出一个用于传送带多苹果质量检测的两阶段“检测-跟踪”框架：YOLOv8定位+ByteTrack跟踪保持ID，ResNet18对裁剪苹果进行缺陷分类，并在轨迹层面聚合以提高时序一致性；给出视频级工业评价指标，实验证明比逐帧推理更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有工业水果系统多在图像层面评估检测/分类，缺乏在视频流中保证时序稳定性的机制，导致预测抖动、无法满足连续生产线对稳定性的需求。

Method: 1) 用在果园数据上训练的YOLOv8进行苹果检测；2) 采用ByteTrack进行多目标跟踪，维持跨帧身份一致；3) 对跟踪到的苹果裁剪区域用微调的ResNet18进行健康/缺陷二分类；4) 在轨迹层面进行结果聚合（如投票/平滑）以抑制帧间振荡；5) 定义视频级指标：轨迹级缺陷比例与时序一致性，用于评估鲁棒性。

Result: 相较仅做逐帧推理的基线，引入跟踪与轨迹级聚合后，系统在视频中的稳定性显著提升（更高的时序一致性、更低的预测摆动），更贴近工业应用需求。

Conclusion: 在流水线环境下，检测+跟踪+轨迹级聚合是实现稳定多苹果质量检测的关键；应采用视频级指标来评估系统的实际可用性。

Abstract: Industrial fruit inspection systems must operate reliably under dense multi-object interactions and continuous motion, yet most existing works evaluate detection or classification at the image level without ensuring temporal stability in video streams. We present a two-stage detection-tracking framework for stable multi-apple quality inspection in conveyor-belt environments. An orchard-trained YOLOv8 model performs apple localization, followed by ByteTrack multi-object tracking to maintain persistent identities. A ResNet18 defect classifier, fine-tuned on a healthy-defective fruit dataset, is applied to cropped apple regions. Track-level aggregation is introduced to enforce temporal consistency and reduce prediction oscillation across frames. We define video-level industrial metrics such as track-level defect ratio and temporal consistency to evaluate system robustness under realistic processing conditions. Results demonstrate improved stability compared to frame-wise inference, suggesting that integrating tracking is essential for practical automated fruit grading systems.

</details>


### [107] [MRI Contrast Enhancement Kinetics World Model](https://arxiv.org/abs/2602.19285)
*Jindi Kong,Yuting He,Cong Xia,Rongjun Ge,Shuo Li*

Main category: cs.CV

TL;DR: 论文提出MRI CEKWorld与时空一致性学习（STCL），通过患者模板的潜在对齐和潜在差分的平滑约束，解决低时间分辨率MRI对比增强动力学建模中的内容失真与时间不连续问题，在两数据集上获得更真实内容与连续动力学。


<details>
  <summary>Details</summary>
Motivation: 临床MRI对比增强采集信息效率低、时间采样稀疏且固定，训练世界模型时缺乏连续时序监督与缺失时刻的数据，直接生成建模易出现内容对齐失败与时间不连续，影响对比剂动力学的可靠模拟与临床可用性。

Method: 提出MRI CEKWorld世界模型并引入STCL框架，包括：1）潜在对齐学习（LAL）：基于“同一患者解剖结构在增强过程中保持一致”的空间规律，构建患者级模板并在潜在空间对内容进行对齐与约束，减少过拟合无关特征导致的失真；2）潜在差分学习（LDL）：基于“动力学随时间平滑变化”的时间规律，在未观测时间段进行插值扩展，并对插值序列的潜在差分施加平滑约束，学习连续的动力学规律。

Result: 在两个数据集上进行广泛实验，模型在内容真实性与动力学连续性上优于对比方法，展示更稳定的时空一致性与更逼真的对比增强曲线。

Conclusion: 通过将患者级空间对齐与时间平滑差分约束纳入世界模型训练，MRI CEKWorld在稀疏时间采样条件下仍能生成连续且真实的对比增强动力学，为更高效、低风险的无对比剂临床成像提供可行路径。

Abstract: Clinical MRI contrast acquisition suffers from inefficient information yield, which presents as a mismatch between the risky and costly acquisition protocol and the fixed and sparse acquisition sequence. Applying world models to simulate the contrast enhancement kinetics in the human body enables continuous contrast-free dynamics. However, the low temporal resolution in MRI acquisition restricts the training of world models, leading to a sparsely sampled dataset. Directly training a generative model to capture the kinetics leads to two limitations: (a) Due to the absence of data on missing time, the model tends to overfit to irrelevant features, leading to content distortion. (b) Due to the lack of continuous temporal supervision, the model fails to learn the continuous kinetics law over time, causing temporal discontinuities. For the first time, we propose MRI Contrast Enhancement Kinetics World model (MRI CEKWorld) with SpatioTemporal Consistency Learning (STCL). For (a), guided by the spatial law that patient-level structures remain consistent during enhancement, we propose Latent Alignment Learning (LAL) that constructs a patient-specific template to constrain contents to align with this template. For (b), guided by the temporal law that the kinetics follow a consistent smooth trend, we propose Latent Difference Learning (LDL) which extends the unobserved intervals by interpolation and constrains smooth variations in the latent space among interpolated sequences. Extensive experiments on two datasets show our MRI CEKWorld achieves better realistic contents and kinetics. Codes will be available at https://github.com/DD0922/MRI-Contrast-Enhancement-Kinetics-World-Model.

</details>


### [108] [IPv2: An Improved Image Purification Strategy for Real-World Ultra-Low-Dose Lung CT Denoising](https://arxiv.org/abs/2602.19314)
*Guoliang Gong,Man Yu*

Main category: cs.CV

TL;DR: 提出图像净化策略IPv2，针对超低剂量CT与常规剂量CT配准失配带来的降噪结构丢失问题，新增“去背景-加噪-去噪”三模块，覆盖背景与肺实质的降噪训练与更合理的测试标注，跨多种主流模型显著提升背景抑制与肺实质恢复。


<details>
  <summary>Details</summary>
Motivation: 原始图像净化策略虽能通过中间分布对齐解剖结构、提升结构保真，但仅抑制胸壁与骨噪声、忽略背景噪声，且缺乏针对肺实质的专门降噪机制，导致对超低剂量CT的全面降噪与客观评测不足。

Method: 系统性重构净化流程为IPv2：1) Remove Background：在训练数据构造中显式去除背景干扰并构建对齐的中间分布；2) Add noise：向处理后的图像注入受控噪声以模拟真实低剂量特性；3) Remove noise：引导模型在背景与肺实质区域学习去噪；并在测试阶段通过精细化标签构造，提供更合理的评测协议。

Result: 在作者此前建立的真实患者2%剂量肺部CT数据集上，IPv2在多种主流降噪网络上均带来稳定增益，表现为更强的背景噪声抑制与更好的肺实质结构恢复。

Conclusion: IPv2弥补了原策略对背景与肺实质降噪不足的缺陷，通过三模块训练与精化测试标注，实现更全面、结构保真的超低剂量CT降噪，并具备良好通用性。

Abstract: The image purification strategy constructs an intermediate distribution with aligned anatomical structures, which effectively corrects the spatial misalignment between real-world ultra-low-dose CT and normal-dose CT images and significantly enhances the structural preservation ability of denoising models. However, this strategy exhibits two inherent limitations. First, it suppresses noise only in the chest wall and bone regions while leaving the image background untreated. Second, it lacks a dedicated mechanism for denoising the lung parenchyma. To address these issues, we systematically redesign the original image purification strategy and propose an improved version termed IPv2. The proposed strategy introduces three core modules, namely Remove Background, Add noise, and Remove noise. These modules endow the model with denoising capability in both background and lung tissue regions during training data construction and provide a more reasonable evaluation protocol through refined label construction at the testing stage. Extensive experiments on our previously established real-world patient lung CT dataset acquired at 2% radiation dose demonstrate that IPv2 consistently improves background suppression and lung parenchyma restoration across multiple mainstream denoising models. The code is publicly available at https://github.com/MonkeyDadLufy/Image-Purification-Strategy-v2.

</details>


### [109] [Pay Attention to CTC: Fast and Robust Pseudo-Labelling for Unified Speech Recognition](https://arxiv.org/abs/2602.19316)
*Alexandros Haliassos,Rodrigo Mira,Stavros Petridis*

Main category: cs.CV

TL;DR: 提出USR 2.0：用CTC驱动的教师强制与混合采样替代自回归伪标，单次前向同时生成CTC与注意力目标，训练时间减半并提升跨分布鲁棒性，在LRS3/LRS2/WildVSR达SOTA。


<details>
  <summary>Details</summary>
Motivation: 原始USR依赖自回归伪标与解耦的CTC/注意力监督：计算开销大，需要束搜索；且两分支误差相互放大，遇到长序列、噪声、未见域时易退化。需要一种高效、稳健、统一的伪标方法以提升OOD鲁棒与训练效率。

Method: 用CTC贪心解码产生伪标，并在同一前向中把这些伪标作为decoder输入（CTC-driven teacher forcing），从而同时得到与CTC等长的注意力目标，无需束搜索。为缓解decoder仅依赖CTC输入的曝光偏差，提出混合采样，将CTC伪标与模型自身输出/真值混合喂入。训练时联合预测CTC与注意力，提高鲁棒性（CTC）与表达性（注意力）。

Result: 相较USR：训练时间约减半；对长序列、噪声、跨域更稳健；在LRS3、LRS2、WildVSR上取得新的SOTA，超过先前USR与各模态自监督基线。

Conclusion: CTC驱动的教师强制把高效的CTC与 expressive 的注意力有机耦合，避免昂贵束搜索并减少自强化错误；配合混合采样缓解曝光偏差，实现更快训练与更强OOD泛化，形成更实用的统一视听识别框架。

Abstract: Unified Speech Recognition (USR) has emerged as a semi-supervised framework for training a single model for audio, visual, and audiovisual speech recognition, achieving state-of-the-art results on in-distribution benchmarks. However, its reliance on autoregressive pseudo-labelling makes training expensive, while its decoupled supervision of CTC and attention branches increases susceptibility to self-reinforcing errors, particularly under distribution shifts involving longer sequences, noise, or unseen domains. We propose CTC-driven teacher forcing, where greedily decoded CTC pseudo-labels are fed into the decoder to generate attention targets in a single forward pass. Although these can be globally incoherent, in the pseudo-labelling setting they enable efficient and effective knowledge transfer. Because CTC and CTC-driven attention pseudo-labels have the same length, the decoder can predict both simultaneously, benefiting from the robustness of CTC and the expressiveness of attention without costly beam search. We further propose mixed sampling to mitigate the exposure bias of the decoder relying solely on CTC inputs. The resulting method, USR 2.0, halves training time, improves robustness to out-of-distribution inputs, and achieves state-of-the-art results on LRS3, LRS2, and WildVSR, surpassing USR and modality-specific self-supervised baselines.

</details>


### [110] [US-JEPA: A Joint Embedding Predictive Architecture for Medical Ultrasound](https://arxiv.org/abs/2602.19322)
*Ashwath Radhachandran,Vedrana Ivezić,Shreeram Athreya,Ronit Anilkumar,Corey W. Arnold,William Speier*

Main category: cs.CV

TL;DR: 提出US-JEPA：在超声图像上用静态领域教师的掩码潜表示预测，自监督学习无需EMA在线教师，提升多任务线性探针表现并具稳定高效表征。


<details>
  <summary>Details</summary>
Motivation: 超声成像噪声高、散斑随机，像素级重建目标不稳；现有JEPA虽规避像素重建，但依赖EMA在线教师，计算昂贵且对超参敏感。需要一种既适配超声噪声特性，又训练稳定、高效的自监督方案。

Method: 提出US-JEPA，采用SALT目标：使用冻结的、领域特定的静态教师网络生成稳定的潜表征作为预测目标；学生网络对被遮挡区域的潜表示进行预测学习，从而与教师解耦并在其语义先验上拓展。并在UltraBench（多器官、多病变公共基准）上系统比较现有超声基础模型与通用视觉模型。

Result: 在UltraBench的多样分类任务上线性探针评测中，US-JEPA达到与最优域内/通用基础模型相当或更优的性能；训练稳定、效率更高。

Conclusion: 掩码潜表示预测结合静态领域教师为超声表示学习提供了稳定高效的路径，减轻了EMA教师的脆弱性与开销，并在多任务上取得强泛化表现。

Abstract: Ultrasound (US) imaging poses unique challenges for representation learning due to its inherently noisy acquisition process. The low signal-to-noise ratio and stochastic speckle patterns hinder standard self-supervised learning methods relying on a pixel-level reconstruction objective. Joint-Embedding Predictive Architectures (JEPAs) address this drawback by predicting masked latent representations rather than raw pixels. However, standard approaches depend on hyperparameter-brittle and computationally expensive online teachers updated via exponential moving average. We propose US-JEPA, a self-supervised framework that adopts the Static-teacher Asymmetric Latent Training (SALT) objective. By using a frozen, domain-specific teacher to provide stable latent targets, US-JEPA decouples student-teacher optimization and pushes the student to expand upon the semantic priors of the teacher. In addition, we provide the first rigorous comparison of all publicly available state-of-the-art ultrasound foundation models on UltraBench, a public dataset benchmark spanning multiple organs and pathological conditions. Under linear probing for diverse classification tasks, US-JEPA achieves performance competitive with or superior to domain-specific and universal vision foundation model baselines. Our results demonstrate that masked latent prediction provides a stable and efficient path toward robust ultrasound representations.

</details>


### [111] [DefenseSplat: Enhancing the Robustness of 3D Gaussian Splatting via Frequency-Aware Filtering](https://arxiv.org/abs/2602.19323)
*Yiran Qiao,Yiren Lu,Yunlai Zhou,Rui Yang,Linlin Hou,Yu Yin,Jing Ma*

Main category: cs.CV

TL;DR: 论文针对3D Gaussian Splatting在输入视图遭受对抗扰动时的脆弱性，提出基于小波频域分析的频率感知防御：抑制高频噪声、保留低频内容，从而在无需干净监督的条件下显著提升鲁棒性并兼顾干净数据性能与训练/渲染效率。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽可实时高保真重建，但对抗样本会显著降低渲染质量、拖慢训练/推理并增加显存/存储，甚至导致拒绝服务。现有工作忽视这种安全风险，缺乏无需干净监督、可通用、训练友好的防御方法。

Method: 先用小波变换剖析对抗扰动在高低频上的差异性；据此设计频率感知重建：对训练视图进行高频噪声滤除（保留低频结构/颜色），再用该“净化”视图训练3DGS，从而在不改变核心管线的前提下抑制对抗伪影与资源开销。

Result: 在多基准和广泛攻击强度下，所提方法在无需干净GT的条件下，显著提升对抗鲁棒性；同时对干净数据训练影响很小，在质量、时间与内存占用上取得更优的鲁攻权衡。

Conclusion: 从频域角度揭示并应对3DGS的安全脆弱性，提出简单高效、可落地的预处理式防御，提升鲁棒与安全的3D重建，为后续鲁棒3DGS研究奠定基础。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for real-time and high-fidelity 3D reconstruction from posed images. However, recent studies reveal its vulnerability to adversarial corruptions in input views, where imperceptible yet consistent perturbations can drastically degrade rendering quality, increase training and rendering time, and inflate memory usage, even leading to server denial-of-service. In our work, to mitigate this issue, we begin by analyzing the distinct behaviors of adversarial perturbations in the low- and high-frequency components of input images using wavelet transforms. Based on this observation, we design a simple yet effective frequency-aware defense strategy that reconstructs training views by filtering high-frequency noise while preserving low-frequency content. This approach effectively suppresses adversarial artifacts while maintaining the authenticity of the original scene. Notably, it does not significantly impair training on clean data, achieving a desirable trade-off between robustness and performance on clean inputs. Through extensive experiments under a wide range of attack intensities on multiple benchmarks, we demonstrate that our method substantially enhances the robustness of 3DGS without access to clean ground-truth supervision. By highlighting and addressing the overlooked vulnerabilities of 3D Gaussian Splatting, our work paves the way for more robust and secure 3D reconstructions.

</details>


### [112] [RetinaVision: XAI-Driven Augmented Regulation for Precise Retinal Disease Classification using deep learning framework](https://arxiv.org/abs/2602.19324)
*Mohammad Tahmid Noor,Shayan Abrar,Jannatul Adan Mahi,Md Parvez Mia,Asaduzzaman Hridoy,Samanta Ghosh*

Main category: cs.CV

TL;DR: 提出用OCT图像进行视网膜疾病八分类的深度学习方法；在C8数据集上，Xception达95.25%准确率、InceptionV3达94.82%，并结合CutMix/MixUp增强与GradCAM/LIME解释，且开发了名为RetinaVision的web应用。


<details>
  <summary>Details</summary>
Motivation: 视网膜疾病早期、准确分类有助于预防视力丧失与临床决策；现有方法需要提升在多类别OCT任务上的准确性与可解释性，并推动从研究到临床应用的落地。

Method: 使用Retinal OCT Image Classification - C8数据集（24,000张、8类），将图像缩放至224×224；比较Xception与InceptionV3两种CNN；采用CutMix与MixUp进行数据增强；利用GradCAM与LIME进行可解释性评估；将模型集成进名为RetinaVision的Web应用进行实际场景演示。

Result: Xception模型在测试中获得95.25%准确率，InceptionV3为94.82%；数据增强与解释方法辅助提升泛化和可解释性。

Conclusion: 深度学习可有效完成OCT视网膜疾病分类；Xception在该设置下表现最佳；将高准确率与可解释性结合对于推进临床应用重要，并已通过Web应用展示其可行性。

Abstract: Early and accurate classification of retinal diseases is critical to counter vision loss and for guiding clinical management of retinal diseases. In this study, we proposed a deep learning method for retinal disease classification utilizing optical coherence tomography (OCT) images from the Retinal OCT Image Classification - C8 dataset (comprising 24,000 labeled images spanning eight conditions). Images were resized to 224x224 px and tested on convolutional neural network (CNN) architectures: Xception and InceptionV3. Data augmentation techniques (CutMix, MixUp) were employed to enhance model generalization. Additionally, we applied GradCAM and LIME for interpretability evaluation. We implemented this in a real-world scenario via our web application named RetinaVision. This study found that Xception was the most accurate network (95.25%), followed closely by InceptionV3 (94.82%). These results suggest that deep learning methods allow effective OCT retinal disease classification and highlight the importance of implementing accuracy and interpretability for clinical applications.

</details>


### [113] [MultiDiffSense: Diffusion-Based Multi-Modal Visuo-Tactile Image Generation Conditioned on Object Shape and Contact Pose](https://arxiv.org/abs/2602.19348)
*Sirine Bhouri,Lan Wei,Jian-Qing Zheng,Dandan Zhang*

Main category: cs.CV

TL;DR: MultiDiffSense 提出一个统一扩散模型，可在单一架构下合成多种视觉型触觉传感器图像，并通过深度图与结构化提示实现可控、物理一致的多模态生成，显著提升合成质量并减少真实数据需求。


<details>
  <summary>Details</summary>
Motivation: 获取对齐的视觉-触觉数据昂贵且缓慢，现有合成多为单模态，难以支持跨模态学习与可控生成，因此需要一种能在多传感器、可控姿态下高质量生成触觉图像的方法以缓解数据瓶颈。

Method: 提出统一扩散模型MultiDiffSense：在单架构内针对ViTac、TacTip、ViTacTip三类视觉触觉传感器进行合成；采用双重条件输入——（1）由CAD模型生成且姿态对齐的深度图；（2）编码传感器类型与4-DoF接触姿态的结构化提示，实现可控且物理一致的生成。与Pix2Pix cGAN进行对比评测。

Result: 在8个物体（5已见、3新）与未见姿态上，相比Pix2Pix基线，SSIM分别提升：ViTac +36.3%，ViTacTip +134.6%，TacTip +64.7%。在下游3-DoF位姿估计任务中，用50%合成+50%真实即可将所需真实数据减半且保持可比性能。

Conclusion: MultiDiffSense 有效缓解触觉数据采集瓶颈，实现可扩展、可控的多模态数据生成，并对机器人触觉场景中的感知与位姿估计等任务带来实际收益。

Abstract: Acquiring aligned visuo-tactile datasets is slow and costly, requiring specialised hardware and large-scale data collection. Synthetic generation is promising, but prior methods are typically single-modality, limiting cross-modal learning. We present MultiDiffSense, a unified diffusion model that synthesises images for multiple vision-based tactile sensors (ViTac, TacTip, ViTacTip) within a single architecture. Our approach uses dual conditioning on CAD-derived, pose-aligned depth maps and structured prompts that encode sensor type and 4-DoF contact pose, enabling controllable, physically consistent multi-modal synthesis. Evaluating on 8 objects (5 seen, 3 novel) and unseen poses, MultiDiffSense outperforms a Pix2Pix cGAN baseline in SSIM by +36.3% (ViTac), +134.6% (ViTacTip), and +64.7% (TacTip). For downstream 3-DoF pose estimation, mixing 50% synthetic with 50% real halves the required real data while maintaining competitive performance. MultiDiffSense alleviates the data-collection bottleneck in tactile sensing and enables scalable, controllable multi-modal dataset generation for robotic applications.

</details>


### [114] [UP-Fuse: Uncertainty-guided LiDAR-Camera Fusion for 3D Panoptic Segmentation](https://arxiv.org/abs/2602.19349)
*Rohit Mohan,Florian Drews,Yakov Miron,Daniele Cattaneo,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出UP-Fuse：在2D Range-View进行不确定性感知的多模态融合，以在相机退化/失效、标定漂移下仍稳健地做3D全景分割。


<details>
  <summary>Details</summary>
Motivation: LiDAR-相机融合可提升3D全景分割，但在恶劣环境中相机退化或失效会使系统不可靠；此外存在相机-激光雷达失配与2D投影产生的空间歧义问题，需要一种对不确定性敏感、能在退化下仍稳健的融合方法。

Method: 1) 将原始LiDAR投影到Range-View并用LiDAR编码器提特征；2) 相机特征抽取并投影到同一Range-View空间；3) 通过不确定性引导的融合模块，根据在多种视觉退化下学习到的表示分歧来预测不确定性图，并用其动态调制跨模态交互，仅让可靠的视觉线索影响融合表示；4) 采用混合2D-3D Transformer解码器，缓解2D投影的空间歧义，直接预测3D全景分割掩码。

Result: 在Panoptic nuScenes、SemanticKITTI以及作者提出的Panoptic Waymo基准上进行大量实验，UP-Fuse在严重视觉腐蚀或外参失配条件下仍保持强劲性能，较现有方法展现更高稳健性与有效性。

Conclusion: 不确定性感知的Range-View跨模态融合结合混合2D-3D解码可在相机退化/失效、标定漂移等不利条件下实现稳健3D全景分割，适用于安全关键的机器人感知。

Abstract: LiDAR-camera fusion enhances 3D panoptic segmentation by leveraging camera images to complement sparse LiDAR scans, but it also introduces a critical failure mode. Under adverse conditions, degradation or failure of the camera sensor can significantly compromise the reliability of the perception system. To address this problem, we introduce UP-Fuse, a novel uncertainty-aware fusion framework in the 2D range-view that remains robust under camera sensor degradation, calibration drift, and sensor failure. Raw LiDAR data is first projected into the range-view and encoded by a LiDAR encoder, while camera features are simultaneously extracted and projected into the same shared space. At its core, UP-Fuse employs an uncertainty-guided fusion module that dynamically modulates cross-modal interaction using predicted uncertainty maps. These maps are learned by quantifying representational divergence under diverse visual degradations, ensuring that only reliable visual cues influence the fused representation. The fused range-view features are decoded by a novel hybrid 2D-3D transformer that mitigates spatial ambiguities inherent to the 2D projection and directly predicts 3D panoptic segmentation masks. Extensive experiments on Panoptic nuScenes, SemanticKITTI, and our introduced Panoptic Waymo benchmark demonstrate the efficacy and robustness of UP-Fuse, which maintains strong performance even under severe visual corruption or misalignment, making it well suited for robotic perception in safety-critical settings.

</details>


### [115] [PoseCraft: Tokenized 3D Body Landmark and Camera Conditioning for Photorealistic Human Image Synthesis](https://arxiv.org/abs/2602.19350)
*Zhilin Guo,Jing Yang,Kyle Fogarty,Jingyi Wan,Boqiao Zhang,Tianhao Wu,Weihao Xia,Chenliang Zhou,Sakar Khattar,Fangcheng Zhong,Cristina Nader Vasconcelos,Cengiz Oztireli*

Main category: cs.CV

TL;DR: PoseCraft提出用离散3D标记（稀疏3D关键点+相机外参）作为扩散模型条件，通过跨注意力驱动生成，实现具备显式3D姿态与相机控制的高保真头像合成，并配套GenHumanRF大规模数据生成流程。结果在感知质量上优于扩散范式方法、与最新体渲染SOTA相当或更好，且更好保留织物与头发细节。


<details>
  <summary>Details</summary>
Motivation: 现有管线要么依赖蒙皮绑定与模板拟合（手工繁琐、泛化差），要么是神经体积需要针对每个新姿态重新优化并受制于二维重投影歧义。需要一种既能保持3D语义、可控姿态与视角、又能生成照片级细节的通用生成框架。

Method: 1) 设计PoseCraft：将稀疏3D地标与相机外参量化为离散条件token，通过跨注意力注入扩散模型，而不仅依赖栅格化几何的2D控制图；避免大姿态/大视角变化时2D重投影歧义。2) 构建GenHumanRF数据流程：从体积重建中渲染多样监督，支撑大规模训练与评测。

Result: 在大规模实验中，相比以扩散为中心的基线显著提升感知质量；与最先进体渲染方法在指标上相当或更优，同时在身份保持与外观细节（织物、头发）上更出色。

Conclusion: 基于3D离散token的扩散条件化是实现显式姿态与相机可控的人体照片级合成的有效途径；无需逐姿态再优化，兼具质量与细节保真，并为VR、远程临场与娱乐等应用提供更实用的生成方案。

Abstract: Digitizing humans and synthesizing photorealistic avatars with explicit 3D pose and camera controls are central to VR, telepresence, and entertainment. Existing skinning-based workflows require laborious manual rigging or template-based fittings, while neural volumetric methods rely on canonical templates and re-optimization for each unseen pose. We present PoseCraft, a diffusion framework built around tokenized 3D interface: instead of relying only on rasterized geometry as 2D control images, we encode sparse 3D landmarks and camera extrinsics as discrete conditioning tokens and inject them into diffusion via cross-attention. Our approach preserves 3D semantics by avoiding 2D re-projection ambiguity under large pose and viewpoint changes, and produces photorealistic imagery that faithfully captures identity and appearance. To train and evaluate at scale, we also implement GenHumanRF, a data generation workflow that renders diverse supervision from volumetric reconstructions. Our experiments show that PoseCraft achieves significant perceptual quality improvement over diffusion-centric methods, and attains better or comparable metrics to latest volumetric rendering SOTA while better preserving fabric and hair details.

</details>


### [116] [MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations](https://arxiv.org/abs/2602.19357)
*Nilay Yilmaz,Maitreya Patel,Naga Sai Abhiram Kusumba,Yixuan He,Yezhou Yang*

Main category: cs.CV

TL;DR: MentalBlackboard评测VLM在“纸张折叠与打孔”中的空间可视化：整体准确率低，对对称与旋转最薄弱；规划更难，最佳仅10%；在不需可视化的迁移任务可高达71.6%，但文本预测仅25%。


<details>
  <summary>Details</summary>
Motivation: 检验最先进视觉-语言模型是否具备人类式空间可视化能力（想象、变换、操控空间特征），并量化其在对称、旋转、展开等核心认知过程上的表现与缺陷。

Method: 构建MentalBlackboard基准，含两类任务：1) 预测（给定折叠与打孔过程，预测展开后的孔洞分布），2) 规划（设计折叠与打孔步骤以达成目标图样）；并含一般化任务（不需可视化，仅在空间数据表征间迁移）。对多种SOTA VLM进行评测与对比。

Result: 预测：模型即便能正确还原展开步骤，也常在应用对称变换上失败；旋转显著削弱情境物理理解。规划：分析对称关系与多阶段对称序列执行受限，Claude Opus 4.1在规划任务最高但仅10%准确。一般化任务中o3达71.6%，但文本式预测仅25%。

Conclusion: 当前VLM缺乏稳健的空间可视化与对称/旋转推理与多步规划能力；在不需显式可视化的表征迁移上可达较高表现。研究应强化几何群等变性、物理一致性与分步可视化记忆，以提升对称与旋转推理及多阶段计划执行。

Abstract: Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. To explore whether state-of-the-art Vision-Language Models (VLMs) exhibit this ability, we develop MentalBlackboard, an open-ended spatial visualization benchmark for Paper Folding and Hole Punching tests within two core tasks: prediction and planning. Our prediction experiments reveal that models struggle with applying symmetrical transformations, even when they predict the sequence of unfolding steps correctly. Also, rotations introduce a significant challenge to the physical situational awareness for models. The planning task reveals limitations of models in analyzing symmetrical relationships and in implementing the multi-stage symmetry process, with Claude Opus 4.1 achieving the highest planning score at an accuracy of 10\%. The top-performing model, o3, attains a peak performance of 71.6\% on the generalization task, which does not require spatial visualization but transfers spatial data; however, it achieves only 25\% accuracy on text-based prediction tasks.

</details>


### [117] [Referring Layer Decomposition](https://arxiv.org/abs/2602.19358)
*Fangyi Chen,Yaojie Shen,Lu Xu,Ye Yuan,Shu Zhang,Yulei Niu,Longyin Wen*

Main category: cs.CV

TL;DR: 论文提出“指代式图层分解（RLD）”任务：给定单张RGB图像与灵活的用户提示，预测完整的对象级RGBA图层，实现可控编辑与组合生成；并发布大规模数据集RefLade与基线模型RefLayer，提供自动化、偏好对齐的评测协议，取得高保真、强语义一致与良好零样本泛化的结果。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑多以整图为单位，难以独立操作场景中具体对象与效果；而分层表示更直观可控，但缺乏标准化任务定义、规模数据与稳健评测。作者动机是把对象感知、可控编辑与组合生成统一到一个可训练、可评测的框架中。

Method: 1) 定义RLD任务：在点/框/掩码、自然语言或其组合提示条件下，从单张RGB预测多个对象/环境/效果的完整RGBA图层。2) 构建RefLade数据集：通过可扩展数据引擎合成或挖掘1.11M图像-图层-提示三元组，并人工精选10万高保真图层。3) 设计人类偏好对齐的自动评测协议，关注感知质量与语义一致。4) 提出RefLayer基线模型，面向提示条件的分层分解，追求简单架构但高保真与对齐。

Result: RefLayer在RefLade上训练，能稳定高质量地将图像分解为对象级RGBA层，与提示高度对齐；评测协议支持可靠比较；实验显示良好的零样本泛化与在编辑/合成任务中的实用性与鲁棒性。

Conclusion: RLD被确立为可基准化的新任务；配套的RefLade数据与评测为社区提供训练与比较的基础；RefLayer证明了在简单方法下即可达到高视觉质量与语义对齐，推动对象感知的可控图像分层与编辑研究。

Abstract: Precise, object-aware control over visual content is essential for advanced image editing and compositional generation. Yet, most existing approaches operate on entire images holistically, limiting the ability to isolate and manipulate individual scene elements. In contrast, layered representations, where scenes are explicitly separated into objects, environmental context, and visual effects, provide a more intuitive and structured framework for interpreting and editing visual content. To bridge this gap and enable both compositional understanding and controllable editing, we introduce the Referring Layer Decomposition (RLD) task, which predicts complete RGBA layers from a single RGB image, conditioned on flexible user prompts, such as spatial inputs (e.g., points, boxes, masks), natural language descriptions, or combinations thereof. At the core is the RefLade, a large-scale dataset comprising 1.11M image-layer-prompt triplets produced by our scalable data engine, along with 100K manually curated, high-fidelity layers. Coupled with a perceptually grounded, human-preference-aligned automatic evaluation protocol, RefLade establishes RLD as a well-defined and benchmarkable research task. Building on this foundation, we present RefLayer, a simple baseline designed for prompt-conditioned layer decomposition, achieving high visual fidelity and semantic alignment. Extensive experiments show our approach enables effective training, reliable evaluation, and high-quality image decomposition, while exhibiting strong zero-shot generalization capabilities.

</details>


### [118] [Detector-in-the-Loop Tracking: Active Memory Rectification for Stable Glottic Opening Localization](https://arxiv.org/abs/2602.19380)
*Huayu Wang,Bahaa Alattar,Cheng-Yen Yang,Hsiang-Wei Huang,Jung Heon Kim,Linda Shapiro,Nathan White,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 论文提出一种闭环记忆校正(CL-MC)框架，将单帧检测器纳入循环监督基础追踪器(SAM2)，在喉镜视频中显著降低漂移与漏检，实现SOTA时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 单帧检测器缺少时序上下文，基础模型追踪器易因记忆漂移而累计误差；喉镜视频存在快速形变、遮挡与歧义，特别在急诊插管场景中需要鲁棒、具时序意识且能抑制渐进式漂移的方案。

Method: 提出CL-MC：以检测器作为“监督者”，对SAM2进行闭环控制。通过与置信度对齐的状态决策和主动记忆整顿：当检测高置信度时触发“语义重置”，用检测结果覆盖被污染的追踪记忆；低置信度则保留/平滑更新，形成训练免疫(无训练)的记忆校正机制，从而抑制漂移累积。

Result: 在急诊插管视频上，CL-MC优于SAM2变体与开环方法，显著降低漂移与漏检率，达成SOTA临床视频追踪性能。

Conclusion: 闭环的记忆校正是临床视频稳健追踪的关键组成，可在无需重新训练的前提下提升基础追踪器的时序可靠性。

Abstract: Temporal stability in glottic opening localization remains challenging due to the complementary weaknesses of single-frame detectors and foundation-model trackers: the former lacks temporal context, while the latter suffers from memory drift. Specifically, in video laryngoscopy, rapid tissue deformation, occlusions, and visual ambiguities in emergency settings require a robust, temporally aware solution that can prevent progressive tracking errors. We propose Closed-Loop Memory Correction (CL-MC), a detector-in-the-loop framework that supervises Segment Anything Model 2(SAM2) through confidence-aligned state decisions and active memory rectification. High-confidence detections trigger semantic resets that overwrite corrupted tracker memory, effectively mitigating drift accumulation with a training-free foundation tracker in complex endoscopic scenes. On emergency intubation videos, CL-MC achieves state-of-the-art performance, significantly reducing drift and missing rate compared with the SAM2 variants and open loop based methods. Our results establish memory correction as a crucial component for reliable clinical video tracking. Our code will be available in https://github.com/huayuww/CL-MR.

</details>


### [119] [Adaptive Data Augmentation with Multi-armed Bandit: Sample-Efficient Embedding Calibration for Implicit Pattern Recognition](https://arxiv.org/abs/2602.19385)
*Minxue Tang,Yangyang Yu,Aolin Ding,Maziyar Baran Pouyan,Taha Belkhouja Yujia Bao*

Main category: cs.CV

TL;DR: ADAMAB提出在冻结嵌入模型上训练轻量校准器，并用MAB自适应数据增广与改进UCB来稳定少样本训练，实现多模态长尾隐式模式识别的大幅提升（≤5样本/类可得最高≈40%准确率增益）。


<details>
  <summary>Details</summary>
Motivation: 长尾与隐式模式识别对LLM/VLM等基础模型仍困难；微调需要大量数据与高算力，不现实。需要一种在数据稀缺与资源受限下仍能有效提升识别能力的方法。

Method: 在固定嵌入模型之上训练“嵌入器无关”的轻量级校准器（不访问底层参数）；提出基于多臂老虎机（MAB）的自适应数据增广策略，采用改进的UCB以在少样本下选择最有效的增广操作，减少梯度漂移，并给出收敛性保证；在多模态场景中进行评测。

Result: 在多模态实验中，相较于直接用预训练嵌入或常规方法，ADAMAB在极少样本（每类<5）条件下可带来最高约40%的准确率提升。

Conclusion: 冻结大模型嵌入配合轻量校准与MAB驱动的自适应增广，能在计算与数据受限下高效提升隐式与长尾模式识别性能，并具有理论收敛保证。

Abstract: Recognizing implicit visual and textual patterns is essential in many real-world applications of modern AI. However, tackling long-tail pattern recognition tasks remains challenging for current pre-trained foundation models such as LLMs and VLMs. While finetuning pre-trained models can improve accuracy in recognizing implicit patterns, it is usually infeasible due to a lack of training data and high computational overhead. In this paper, we propose ADAMAB, an efficient embedding calibration framework for few-shot pattern recognition. To maximally reduce the computational costs, ADAMAB trains embedder-agnostic light-weight calibrators on top of fixed embedding models without accessing their parameters. To mitigate the need for large-scale training data, we introduce an adaptive data augmentation strategy based on the Multi-Armed Bandit (MAB) mechanism. With a modified upper confidence bound algorithm, ADAMAB diminishes the gradient shifting and offers theoretically guaranteed convergence in few-shot training. Our multi-modal experiments justify the superior performance of ADAMAB, with up to 40% accuracy improvement when training with less than 5 initial data samples of each class.

</details>


### [120] [Redefining the Down-Sampling Scheme of U-Net for Precision Biomedical Image Segmentation](https://arxiv.org/abs/2602.19412)
*Mingjie Li,Yizheng Chen,Md Tauhidul Islam,Lei Xing*

Main category: cs.CV

TL;DR: 提出“阶梯池化”以减缓下采样速率、降低信息丢失，从而提升U‑Net在2D/3D生物医学分割中的长程信息建模与重建能力，Dice平均提升约3.8%，并用传递熵定量评估选优下采样路径与信息保留。


<details>
  <summary>Details</summary>
Motivation: 传统U‑Net为提高计算效率，常用步长/池化产生1/4面积降采样，导致长程依赖建模不足与细节重建受损；BIS任务对细粒度结构与全局上下文均敏感，亟需在算力可控下减少下采样带来的信息丢失。

Method: 提出“Stair Pooling”：将每次2D池化的降维幅度由1/4改为1/2，通过一系列小而窄、方向各异的池化操作串联并级联特征，节制性地下采样；方案可扩展到3D池化以进一步保留体素信息。辅以传递熵用于选择最优下采样路径，并量化信息损失变化。

Result: 在三个生物医学分割基准上，对2D与3D U‑Net均带来平均约+3.8%的Dice提升；传递熵分析显示信息损失显著降低，验证路径选择合理性与方法有效性。

Conclusion: 通过更细粒度、方向多样的阶梯式池化，U‑Net在保持可控计算成本的同时更好保留与传递特征信息，增强长程信息捕获与上采样重建效果，进而稳定提升BIS分割精度；方法简单通用，可拓展至3D场景并由信息论指标指导配置。

Abstract: U-Net architectures have been instrumental in advancing biomedical image segmentation (BIS) but often struggle with capturing long-range information. One reason is the conventional down-sampling techniques that prioritize computational efficiency at the expense of information retention. This paper introduces a simple but effective strategy, we call it Stair Pooling, which moderates the pace of down-sampling and reduces information loss by leveraging a sequence of concatenated small and narrow pooling operations in varied orientations. Specifically, our method modifies the reduction in dimensionality within each 2D pooling step from $\frac{1}{4}$ to $\frac{1}{2}$. This approach can also be adapted for 3D pooling to preserve even more information. Such preservation aids the U-Net in more effectively reconstructing spatial details during the up-sampling phase, thereby enhancing its ability to capture long-range information and improving segmentation accuracy. Extensive experiments on three BIS benchmarks demonstrate that the proposed Stair Pooling can increase both 2D and 3D U-Net performance by an average of 3.8\% in Dice scores. Moreover, we leverage the transfer entropy to select the optimal down-sampling paths and quantitatively show how the proposed Stair Pooling reduces the information loss.

</details>


### [121] [PA-Attack: Guiding Gray-Box Attacks on LVLM Vision Encoders with Prototypes and Attention](https://arxiv.org/abs/2602.19418)
*Hefei Mei,Zirui Wang,Chang Xu,Jianyuan Guo,Minjing Dong*

Main category: cs.CV

TL;DR: 提出PA-Attack：利用共享视觉编码器的“灰盒”信息，通过原型锚定与注意力增强，实现对LVLMs跨任务、跨模型的高效对抗攻击，平均SRR达75.1%。


<details>
  <summary>Details</summary>
Motivation: 现有白盒攻击难以跨任务泛化，黑盒攻击依赖代价高的迁移；而LVLM普遍共享的视觉编码器提供稳定的灰盒支点，可望提升攻击通用性与效率。

Method: 1) 原型锚定指导：以与目标属性普遍不相似的通用原型作为攻击方向，缓解属性受限与任务泛化差的问题；2) 两阶段注意力增强：i) 利用token级注意力分数聚焦关键视觉token的扰动；ii) 在对抗过程中自适应重标定注意力权重以跟踪注意力的动态变化。整体在共享视觉编码器上开展，利用其跨模型可迁移性。

Result: 在多种下游任务与多种LVLM架构上，PA-Attack实现平均75.1%的分数下降率（SRR），表现出强攻击效果、效率与任务泛化能力。

Conclusion: 以视觉编码器为灰盒支点、结合原型锚定与自适应注意力的PA-Attack能在多任务多模型间稳定迁移并高效破坏LVLM性能，优于以往泛化性与成本受限的攻击方法。

Abstract: Large Vision-Language Models (LVLMs) are foundational to modern multimodal applications, yet their susceptibility to adversarial attacks remains a critical concern. Prior white-box attacks rarely generalize across tasks, and black-box methods depend on expensive transfer, which limits efficiency. The vision encoder, standardized and often shared across LVLMs, provides a stable gray-box pivot with strong cross-model transfer. Building on this premise, we introduce PA-Attack (Prototype-Anchored Attentive Attack). PA-Attack begins with a prototype-anchored guidance that provides a stable attack direction towards a general and dissimilar prototype, tackling the attribute-restricted issue and limited task generalization of vanilla attacks. Building on this, we propose a two-stage attention enhancement mechanism: (i) leverage token-level attention scores to concentrate perturbations on critical visual tokens, and (ii) adaptively recalibrate attention weights to track the evolving attention during the adversarial process. Extensive experiments across diverse downstream tasks and LVLM architectures show that PA-Attack achieves an average 75.1% score reduction rate (SRR), demonstrating strong attack effectiveness, efficiency, and task generalization in LVLMs. Code is available at https://github.com/hefeimei06/PA-Attack.

</details>


### [122] [Prefer-DAS: Learning from Local Preferences and Sparse Prompts for Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2602.19423)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出Prefer-DAS：面向电镜跨域分割的可提示多任务模型，结合自训练、提示对比学习与局部/稀疏偏好优化（LPO/SLPO/UPO），可在有点提示、部分提示或无提示下进行自动与交互式分割，在四项任务上优于SAM类与主流UDA/弱监督方法，接近或超越全监督。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应在EM跨域分割上常出现性能有限与偏置，实际应用需要更低标注成本且可利用目标域的少量弱标注与人类偏好反馈，支持交互式与自动模式并具备强泛化。

Method: 1) 设定：利用目标域稀疏点与局部人类偏好作为弱标签；2) 模型：Prefer-DAS为可提示多任务框架，融合自训练与提示引导的对比学习；3) 交互性：训练/推理均可使用全/部分/无点提示；4) 偏好对齐：提出局部直接偏好优化LPO与稀疏版SLPO，实现空间可变或稀疏反馈的对齐；5) 缺失反馈时提出UPO，用自学习偏好进行无监督对齐；6) 同时支持弱监督与无监督DAS。

Result: 在四个具有挑战性的DAS任务上，自动与交互式两种模式均超过SAM类、主流无监督与弱监督DAS方法；泛化与灵活性强，性能接近甚至超过全监督模型。

Conclusion: 通过将可提示学习与局部/稀疏偏好优化结合，Prefer-DAS在电镜跨域分割中以极低标注成本实现高性能、强泛化与交互灵活性，为实际应用提供更可行的DAS方案。

Abstract: Domain adaptive segmentation (DAS) is a promising paradigm for delineating intracellular structures from various large-scale electron microscopy (EM) without incurring extensive annotated data in each domain. However, the prevalent unsupervised domain adaptation (UDA) strategies often demonstrate limited and biased performance, which hinders their practical applications. In this study, we explore sparse points and local human preferences as weak labels in the target domain, thereby presenting a more realistic yet annotation-efficient setting. Specifically, we develop Prefer-DAS, which pioneers sparse promptable learning and local preference alignment. The Prefer-DAS is a promptable multitask model that integrates self-training and prompt-guided contrastive learning. Unlike SAM-like methods, the Prefer-DAS allows for the use of full, partial, and even no point prompts during both training and inference stages and thus enables interactive segmentation. Instead of using image-level human preference alignment for segmentation, we introduce Local direct Preference Optimization (LPO) and sparse LPO (SLPO), plug-and-play solutions for alignment with spatially varying human feedback or sparse feedback. To address potential missing feedback, we also introduce Unsupervised Preference Optimization (UPO), which leverages self-learned preferences. As a result, the Prefer-DAS model can effectively perform both weakly-supervised and unsupervised DAS, depending on the availability of points and human preferences. Comprehensive experiments on four challenging DAS tasks demonstrate that our model outperforms SAM-like methods as well as unsupervised and weakly-supervised DAS methods in both automatic and interactive segmentation modes, highlighting strong generalizability and flexibility. Additionally, the performance of our model is very close to or even exceeds that of supervised models.

</details>


### [123] [Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images](https://arxiv.org/abs/2602.19424)
*Yuxuan Yang,Zhonghao Yan,Yi Zhang,Bo Yun,Muxi Diao,Guowei Zhao,Kongming Liang,Wenbin Li,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出Hepato-LLaVA多模态大模型，用稀疏拓扑打包注意力在WSI上高效聚合多尺度病理证据，并发布33K病理VQA数据集，在HCC诊断与描述上SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有HCC病理分析方法受限于固定分辨率与低效特征聚合：要么信息丢失，要么冗余过高，且缺少多尺度、临床语义丰富的数据支撑多模态推理。

Method: 1) 设计面向肝细胞癌病理的多模态LLM——Hepato-LLaVA；2) 提出Sparse Topo-Pack Attention：显式建模二维组织拓扑，将局部诊断证据稀疏汇聚为语义摘要token，同时保留全局上下文；3) 构建HepatoPathoVQA数据集：包含3.3万分层问答，由临床病理专家验证，用于训练/评测多尺度病理理解与推理。

Result: 在HCC诊断与图像描述任务上取得SOTA，显著优于现有方法；方法与代码开源（给出项目页链接）。

Conclusion: 通过拓扑感知的稀疏注意力与临床落地的数据集，Hepato-LLaVA在WSI上实现高效且细粒度的多模态病理理解，缓解信息丢失与冗余问题，为临床级HCC诊断与报告生成提供更强基线。

Abstract: Hepatocellular Carcinoma diagnosis relies heavily on the interpretation of gigapixel Whole Slide Images. However, current computational approaches are constrained by fixed-resolution processing mechanisms and inefficient feature aggregation, which inevitably lead to either severe information loss or high feature redundancy. To address these challenges, we propose Hepato-LLaVA, a specialized Multi-modal Large Language Model designed for fine-grained hepatocellular pathology analysis. We introduce a novel Sparse Topo-Pack Attention mechanism that explicitly models 2D tissue topology. This mechanism effectively aggregates local diagnostic evidence into semantic summary tokens while preserving global context. Furthermore, to overcome the lack of multi-scale data, we present HepatoPathoVQA, a clinically grounded dataset comprising 33K hierarchically structured question-answer pairs validated by expert pathologists. Our experiments demonstrate that Hepato-LLaVA achieves state-of-the-art performance on HCC diagnosis and captioning tasks, significantly outperforming existing methods. Our code and implementation details are available at https://pris-cv.github.io/Hepto-LLaVA/.

</details>


### [124] [TherA: Thermal-Aware Visual-Language Prompting for Controllable RGB-to-Thermal Infrared Translation](https://arxiv.org/abs/2602.19430)
*Dong-Guw Lee,Tai Hyoung Rhee,Hyunsoo Jang,Young-Sik Shin,Ukcheol Shin,Ayoung Kim*

Main category: cs.CV

TL;DR: 提出TherA：结合VLM条件嵌入与潜变量扩散，实现可控、物理合理的RGB→TIR翻译，跨场景/物体/材料/热状态生成多样、可信的热图像，零样本指标平均提升达33%。


<details>
  <summary>Details</summary>
Motivation: TIR感知在低光、恶劣环境下具优势，但TIR大规模标注昂贵稀缺。现有RGB→TIR方法依赖RGB先验，忽视热物理，导致不合理热分布，需要一种兼顾可控性与物理一致性的合成框架以缓解数据瓶颈。

Method: 构建TherA框架：以TherA-VLM从单张RGB与用户给定的条件对（时间、天气、物体状态等）生成“热感知”嵌入，编码场景、物体、材料与热辐射上下文；再以潜变量扩散模型在该嵌入条件下进行图像翻译，提供细粒度可控生成，支持场景与对象层面的热分布调节与多样性。

Result: 相较基线，达到SOTA的RGB→TIR翻译质量；在零样本翻译设定下，跨全部指标的平均提升最高达33%。

Conclusion: TherA能在保持热物理合理性的同时提供细粒度可控与多样化的TIR合成，有望缓解TIR数据匮乏并提升TIR感知任务性能。

Abstract: Despite the inherent advantages of thermal infrared(TIR) imaging, large-scale data collection and annotation remain a major bottleneck for TIR-based perception. A practical alternative is to synthesize pseudo TIR data via image translation; however, most RGB-to-TIR approaches heavily rely on RGB-centric priors that overlook thermal physics, yielding implausible heat distributions. In this paper, we introduce TherA, a controllable RGB-to-TIR translation framework that produces diverse and thermally plausible images at both scene and object level. TherA couples TherA-VLM with a latent-diffusion-based translator. Given a single RGB image and a user-prompted condition pair, TherA-VLM yields a thermal-aware embedding that encodes scene, object, material, and heat-emission context reflecting the input scene-condition pair. Conditioning the diffusion model on this embedding enables realistic TIR synthesis and fine-grained control across time of day, weather, and object state. Compared to other baselines, TherA achieves state-of-the-art translation performance, demonstrating improved zero-shot translation performance up to 33% increase averaged across all metrics.

</details>


### [125] [CountEx: Fine-Grained Counting via Exemplars and Exclusion](https://arxiv.org/abs/2602.19432)
*Yifeng Huang,Gia Khanh Nguyen,Minh Hoai*

Main category: cs.CV

TL;DR: 提出CountEx，一个可判别的视觉计数框架，通过同时“包含+排除”的提示来避免对相似干扰物的误计数；并发布细粒度计数基准CoCount，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的计数方法只能用“包含”提示指定要数的目标，在拥挤或类别相近的场景中容易把相似干扰物也计入，导致歧义与过计数，缺乏显式的排除机制。

Method: 引入多模态提示（文本+可选视觉示例）表达“包含与排除”意图；核心为判别式查询细化（Discriminative Query Refinement, DQR）模块：先找出包含与排除线索的共享视觉特征，再分离排除特有模式，最后进行选择性抑制以细化计数查询，从而在特征空间区分易混类别。

Result: 在新建的CoCount基准（1780个视频、10086帧、97对类别）上，CountEx在已知与新类别计数任务上显著超越现有最先进方法，显示更强的抗干扰与泛化能力。

Conclusion: 显式建模“包含+排除”的判别式计数范式有效缓解相似干扰带来的过计数问题；DQR提供可扩展的细粒度区分能力；配套的CoCount基准促进系统性评测与后续研究。

Abstract: This paper presents CountEx, a discriminative visual counting framework designed to address a key limitation of existing prompt-based methods: the inability to explicitly exclude visually similar distractors. While current approaches allow users to specify what to count via inclusion prompts, they often struggle in cluttered scenes with confusable object categories, leading to ambiguity and overcounting. CountEx enables users to express both inclusion and exclusion intent, specifying what to count and what to ignore, through multimodal prompts including natural language descriptions and optional visual exemplars. At the core of CountEx is a novel Discriminative Query Refinement module, which jointly reasons over inclusion and exclusion cues by first identifying shared visual features, then isolating exclusion-specific patterns, and finally applying selective suppression to refine the counting query. To support systematic evaluation of fine-grained counting methods, we introduce CoCount, a benchmark comprising 1,780 videos and 10,086 annotated frames across 97 category pairs. Experiments show that CountEx achieves substantial improvements over state-of-the-art methods for counting objects from both known and novel categories. The data and code are available at https://github.com/bbvisual/CountEx.

</details>


### [126] [FinSight-Net:A Physics-Aware Decoupled Network with Frequency-Domain Compensation for Underwater Fish Detection in Smart Aquaculture](https://arxiv.org/abs/2602.19437)
*Jinsong Yang,Zeyuan Hu,Yichen Li,Hong Yu*

Main category: cs.CV

TL;DR: FinSight-Net是一种面向复杂养殖水域的高效、物理感知的鱼类检测框架，通过频段/尺度解耦与高效特征聚合，在高浑浊与模糊情况下实现更稳健的目标定位与识别，达到SOTA并兼顾轻量与实时性。


<details>
  <summary>Details</summary>
Motivation: 现有水下鱼类检测多依赖更深/更重的网络与注意力模块，忽视水下成像物理（波长吸收、浑浊散射、回散射噪声）带来的对比度下降与细节模糊，从而限制了在实际复杂水域中的可靠检测与部署效率。

Method: 提出FinSight-Net，包括：1）MS-DDSP瓶颈：多尺度解耦双流处理，使用异构卷积分支针对频段特异的信息丢失；通过尺度感知与通道加权路径抑制回散射伪影并补偿生物学细节。2）EPA-FPN高效路径聚合：建立长距跳连，删减冗余融合路径，回填深层衰减的高频空间信息，提升非刚体鱼目标在强模糊/高浑浊下的可检测性。

Result: 在DeepFish、AquaFishSet与自建UW-BlurredFish上取得SOTA；其中UW-BlurredFish上mAP达92.8%，较YOLOv11s提升4.8个百分点，同时参数减少29.0%，兼顾精度与模型轻量。

Conclusion: 结合物理感知的结构设计与高效特征聚合，FinSight-Net在恶劣水下环境中实现高精度、低开销的实时鱼类检测，适用于智慧渔业的自动化监测场景。

Abstract: Underwater fish detection (UFD) is a core capability for smart aquaculture and marine ecological monitoring. While recent detectors improve accuracy by stacking feature extractors or introducing heavy attention modules, they often incur substantial computational overhead and, more importantly, neglect the physics that fundamentally limits UFD: wavelength-dependent absorption and turbidity-induced scattering significantly degrade contrast, blur fine structures, and introduce backscattering noise, leading to unreliable localization and recognition. To address these challenges, we propose FinSight-Net, an efficient and physics-aware detection framework tailored for complex aquaculture environments. FinSight-Net introduces a Multi-Scale Decoupled Dual-Stream Processing (MS-DDSP) bottleneck that explicitly targets frequency-specific information loss via heterogeneous convolutional branches, suppressing backscattering artifacts while compensating distorted biological cues through scale-aware and channel-weighted pathways. We further design an Efficient Path Aggregation FPN (EPA-FPN) as a detail-filling mechanism: it restores high-frequency spatial information typically attenuated in deep layers by establishing long-range skip connections and pruning redundant fusion routes, enabling robust detection of non-rigid fish targets under severe blur and turbidity. Extensive experiments on DeepFish, AquaFishSet, and our challenging UW-BlurredFish benchmark demonstrate that FinSight-Net achieves state-of-the-art performance. In particular, on UW-BlurredFish, FinSight-Net reaches 92.8% mAP, outperforming YOLOv11s by 4.8% while reducing parameters by 29.0%, providing a strong and lightweight solution for real-time automated monitoring in smart aquaculture.

</details>


### [127] [UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment](https://arxiv.org/abs/2602.19442)
*Yecheng Zhang,Rong Zhao,Zhizhou Sha,Yong Li,Lei Wang,Ce Hou,Wen Ji,Hao Huang,Yunshan Wan,Jian Yu,Junhao Xia,Yuru Zhang,Chunlei Shi*

Main category: cs.CV

TL;DR: 提出一种无需训练的后处理概念瓶颈流程，使VLM在主观感知任务上与人类偏好对齐，通过少量标注挖掘维度、多智能体结构化打分、以及几何校准，实现可解释、零参数更新的高精度对齐，并在城市感知任务上显著超越有监督与未经校准基线。


<details>
  <summary>Details</summary>
Motivation: 在特定领域、尤其主观感知类任务中，使VLM输出符合人类偏好通常依赖微调或RL，代价高且需标注数据与GPU计算。作者观察到VLM善于提取概念但决策校准差，因而希望用训练外的方法将其“概念力”转化为与人类一致的评分，同时保持可解释性和零权重改动。

Method: 提出三阶段、端到端维度优化闭环的训练免费后处理流水线：1) 概念挖掘：从少量人工标注中挖掘可解释的评价维度；2) 多智能体结构化打分：构建观察者-辩手-法官链条，利用冻结的VLM提取稳健连续概念分数；3) 几何校准：在混合视觉-语义流形上用局部加权岭回归，将概念分数对齐到人类评分。整个流程不更新模型权重但联合优化维度选择。

Result: 在城市感知任务（UrbanAlign）上，Place Pulse 2.0六类指标达到72.2%准确率（κ=0.45），较最佳有监督基线提升+15.1个百分点，较未经校准的VLM评分提升+16.3个百分点，并提供维度级可解释性。

Conclusion: 无需微调或RL即可通过概念瓶颈与几何校准将VLM对齐到人类偏好，兼具性能与可解释性，显示训练外后处理在主观感知任务中的有效性与实用价值。

Abstract: Aligning vision-language model (VLM) outputs with human preferences in domain-specific tasks typically requires fine-tuning or reinforcement learning, both of which demand labelled data and GPU compute. We show that for subjective perception tasks, this alignment can be achieved without any model training: VLMs are already strong concept extractors but poor decision calibrators, and the gap can be closed externally. We propose a training-free post-hoc concept-bottleneck pipeline consisting of three tightly coupled stages: concept mining, multi-agent structured scoring, and geometric calibration, unified by an end-to-end dimension optimization loop. Interpretable evaluation dimensions are mined from a handful of human annotations; an Observer-Debater-Judge chain extracts robust continuous concept scores from a frozen VLM; and locally-weighted ridge regression on a hybrid visual-semantic manifold calibrates these scores against human ratings. Applied to urban perception as UrbanAlign, the framework achieves 72.2% accuracy ($κ=0.45$) on Place Pulse 2.0 across six categories, outperforming the best supervised baseline by +15.1 pp and uncalibrated VLM scoring by +16.3 pp, with full dimension-level interpretability and zero model-weight modification.

</details>


### [128] [Decoupling Vision and Language: Codebook Anchored Visual Adaptation](https://arxiv.org/abs/2602.19449)
*Jason Wu,Tianchen Zhao,Chang Liu,Jiarui Cai,Zheng Zhang,Zhuowei Li,Aaditya Singh,Xiang Xu,Mani Srivastava,Jonathan Wu*

Main category: cs.CV

TL;DR: 提出CRAFT方法：用离散码本调控微调视觉编码器，使其在特定领域适配且与LLM解耦，不改其他模块；在10个基准上平均提升13.51%，并保持语言能力，优于连续特征调优方法。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在医学诊断、细粒度分类等领域表现不足，因视觉编码器表征误差会传递到语言模型；当前适配通常在编码器与LLM间的连续特征接口上做投影器/高效参数调优，导致两者耦合、且更换编码器需重新对齐，成本高、泛化差。

Method: 引入Codebook Regulated Fine-Tuning（CRAFT）：为视觉编码器引入共享的离散码本，将图像表征量化到稳定的离散token空间；仅微调编码器使其输出对齐该码本，不修改LLM或投影层。这样的解耦使经适配的编码器可无缝接入任意共享同一码本的LLM。

Result: 在10个特定领域基准（如VQARAD、PlantVillage等）上平均提升13.51%；同时保持LLM语言能力；在与只对连续特征进行调优的同类方法相比取得更优性能。

Conclusion: 通过离散码本锚定视觉表示，CRAFT实现了对视觉编码器的轻量且可迁移的领域适配，解耦了编码器与LLM，既提升特定领域性能又维持语言能力，并具备跨LLM复用性。

Abstract: Large Vision-Language Models (LVLMs) use their vision encoders to translate images into representations for downstream reasoning, but the encoders often underperform in domain-specific visual tasks such as medical image diagnosis or fine-grained classification, where representation errors can cascade through the language model, leading to incorrect responses. Existing adaptation methods modify the continuous feature interface between encoder and language model through projector tuning or other parameter-efficient updates, which still couples the two components and requires re-alignment whenever the encoder changes. We introduce CRAFT (Codebook RegulAted Fine-Tuning), a lightweight method that fine-tunes the encoder using a discrete codebook that anchors visual representations to a stable token space, achieving domain adaptation without modifying other parts of the model. This decoupled design allows the adapted encoder to seamlessly boost the performance of LVLMs with different language architectures, as long as they share the same codebook. Empirically, CRAFT achieves an average gain of 13.51% across 10 domain-specific benchmarks such as VQARAD and PlantVillage, while preserving the LLM's linguistic capabilities and outperforming peer methods that operate on continuous tokens.

</details>


### [129] [HD-TTA: Hypothesis-Driven Test-Time Adaptation for Safer Brain Tumor Segmentation](https://arxiv.org/abs/2602.19454)
*Kartik Jhawar,Lipo Wang*

Main category: cs.CV

TL;DR: 提出一种安全导向的测试时自适应方法（HD-TTA），通过在推理时生成并选择“压缩 vs 膨胀”的几何假设，结合门卫预筛与表征引导选择器，降低负迁移与外溢风险，在跨域脑肿瘤分割上显著降低HD95并提升Precision，同时保持Dice不降。


<details>
  <summary>Details</summary>
Motivation: 传统TTA将推理视为统一优化，对所有样本盲目适配，易在医疗分割等高风险场景中导致：肿瘤分割蔓延至健康组织、原本正确的预测被破坏（负迁移）。临床更关心安全性（边界外溢、极端误差）而非仅Dice。因此需要一种可选择、可控、以安全为先的自适应框架。

Method: 将TTA重构为动态决策：1）为每个样本生成两类直观几何假设——压缩（去伪影、收紧噪声）与膨胀（在不确定时安全补全欠分）；2）表征引导的选择器依据纹理一致性等内在特征，自动选择更安全的假设输出；3）门卫（Gatekeeper）在高置信样本上跳过适配以避免负迁移。该流程用于跨域二分类脑肿瘤分割（成人BraTS训练，迁移到儿科与脑膜瘤）。

Result: 在安全性指标上优于多种SOTA基线：HD95降低约6.4mm，Precision提升超过4%，同时Dice保持可比，显示在安全受限场景（易外溢、极端误差）下的稳健性。

Conclusion: 通过显式构造与选择几何假设，能够在保持精度的同时改善临床安全相关指标，是实现安全临床部署的可行路径；预筛门卫与表征引导选择器有效减少负迁移。代码将于录用后开源。

Abstract: Standard Test-Time Adaptation (TTA) methods typically treat inference as a blind optimization task, applying generic objectives to all or filtered test samples. In safety-critical medical segmentation, this lack of selectivity often causes the tumor mask to spill into healthy brain tissue or degrades predictions that were already correct. We propose Hypothesis-Driven TTA, a novel framework that reformulates adaptation as a dynamic decision process. Rather than forcing a single optimization trajectory, our method generates intuitive competing geometric hypotheses: compaction (is the prediction noisy? trim artifacts) versus inflation (is the valid tumor under-segmented? safely inflate to recover). It then employs a representation-guided selector to autonomously identify the safest outcome based on intrinsic texture consistency. Additionally, a pre-screening Gatekeeper prevents negative transfer by skipping adaptation on confident cases. We validate this proof-of-concept on a cross-domain binary brain tumor segmentation task, applying a source model trained on adult BraTS gliomas to unseen pediatric and more challenging meningioma target domains. HD-TTA improves safety-oriented outcomes (Hausdorff Distance (HD95) and Precision) over several state-of-the-art representative baselines in the challenging safety regime, reducing the HD95 by approximately 6.4 mm and improving Precision by over 4%, while maintaining comparable Dice scores. These results demonstrate that resolving the safety-adaptation trade-off via explicit hypothesis selection is a viable, robust path for safe clinical model deployment. Code will be made publicly available upon acceptance.

</details>


### [130] [Laplacian Multi-scale Flow Matching for Generative Modeling](https://arxiv.org/abs/2602.19461)
*Zelin Zhao,Petr Molodyk,Haotian Xue,Yongxin Chen*

Main category: cs.CV

TL;DR: LapFlow：用拉普拉斯金字塔将图像分解为多尺度残差，并用并行的混合Transformer与因果注意力同时生成各尺度表示，无需级联“重噪”桥接；在CelebA-HQ与ImageNet上以更少GFLOPs与更快采样获得更高质量，并可扩展至1024×1024。


<details>
  <summary>Details</summary>
Motivation: 现有flow matching生成多依赖单尺度或级联多尺度，需逐级桥接/去噪，带来计算开销与延迟，限制高分辨率扩展与采样速度。作者希望通过并行多尺度表示与更高效的体系结构提升质量与速度并改善可扩展性。

Method: 将图像分解为拉普拉斯金字塔残差；为每个尺度配备子Transformer，并以混合（Mixture-of-Transformers, MoT）方式并行处理；采用因果注意力以匹配流动方向与依赖；在训练与采样中并行生成所有尺度的残差，而非级联地在尺度间重噪/桥接；整体以flow matching目标进行优化。

Result: 在CelebA-HQ与ImageNet上，相比单尺度与既有多尺度flow matching基线，LapFlow获得更优样本质量，同时减少GFLOPs并缩短推理时间；在高分辨率（至1024×1024）生成中维持较低计算开销。

Conclusion: 并行的拉普拉斯多尺度表征与MoT架构能有效提升flow matching的生成质量与效率，避免级联重噪带来的代价，并可平滑扩展至高分辨率。

Abstract: In this paper, we present Laplacian multiscale flow matching (LapFlow), a novel framework that enhances flow matching by leveraging multi-scale representations for image generative modeling. Our approach decomposes images into Laplacian pyramid residuals and processes different scales in parallel through a mixture-of-transformers (MoT) architecture with causal attention mechanisms. Unlike previous cascaded approaches that require explicit renoising between scales, our model generates multi-scale representations in parallel, eliminating the need for bridging processes. The proposed multi-scale architecture not only improves generation quality but also accelerates the sampling process and promotes scaling flow matching methods. Through extensive experimentation on CelebA-HQ and ImageNet, we demonstrate that our method achieves superior sample quality with fewer GFLOPs and faster inference compared to single-scale and multi-scale flow matching baselines. The proposed model scales effectively to high-resolution generation (up to 1024$\times$1024) while maintaining lower computational overhead.

</details>


### [131] [Physics-informed Active Polarimetric 3D Imaging for Specular Surfaces](https://arxiv.org/abs/2602.19470)
*Jiazhang Wang,Hyelim Yang,Tianyi Wang,Florian Willomitzer*

Main category: cs.CV

TL;DR: 提出一种结合偏振与结构光的物理先验单帧方法，利用双编码器互调网络直接预测镜面物体表面法向，实现快速、鲁棒、精确的3D成像。


<details>
  <summary>Details</summary>
Motivation: 现有镜面表面3D成像在动态/在线场景中难以兼顾速度与精度：多帧偏折测量精度高但不适合单帧；傅里叶单帧法在高频几何或大曲率下性能下降；偏振单帧法鲁棒但受正交投影假设限制，精度受限。需要一种既单帧又高精度、能处理复杂几何的方案。

Method: 提出物理驱动的深度学习框架：将偏振提供的表面取向先验与结构光编码的几何信息结合；设计双编码器并行提取两类线索，通过互相特征调制来解析二者非线性耦合，端到端直接回归表面法向，实现单帧推理。

Result: 在单帧条件下实现对复杂镜面表面的准确、鲁棒、快速法向估计；在高空间频率结构和大曲率场景下优于传统傅里叶单帧及受限的偏振方法；具备实用推理速度。

Conclusion: 融合偏振先验与结构光信息并以物理约束指导的双编码器网络，可在单帧内稳健、准确地重建镜面表面法向，推动复杂镜面物体在动态/在线场景中的实用3D成像。

Abstract: 3D imaging of specular surfaces remains challenging in real-world scenarios, such as in-line inspection or hand-held scanning, requiring fast and accurate measurement of complex geometries. Optical metrology techniques such as deflectometry achieve high accuracy but typically rely on multi-shot acquisition, making them unsuitable for dynamic environments. Fourier-based single-shot approaches alleviate this constraint, yet their performance deteriorates when measuring surfaces with high spatial frequency structure or large curvature. Alternatively, polarimetric 3D imaging in computer vision operates in a single-shot fashion and exhibits robustness to geometric complexity. However, its accuracy is fundamentally limited by the orthographic imaging assumption. In this paper, we propose a physics-informed deep learning framework for single-shot 3D imaging of complex specular surfaces. Polarization cues provide orientation priors that assist in interpreting geometric information encoded by structured illumination. These complementary cues are processed through a dual-encoder architecture with mutual feature modulation, allowing the network to resolve their nonlinear coupling and directly infer surface normals. The proposed method achieves accurate and robust normal estimation in single-shot with fast inference, enabling practical 3D imaging of complex specular surfaces.

</details>


### [132] [Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model](https://arxiv.org/abs/2602.19471)
*Zheang Huai,Hui Tang,Hualiang Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出FRLA方法用于无源域自适应的眼底图像诊断，结合遗忘抵抗与病灶感知机制，利用ViL模型的细粒度知识与目标模型高置信预测，显著优于ViL与现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统SFDA在域偏移下易出错；借助ViL模型虽可改进，但会遗忘目标模型原有优势类别预测，且未充分利用ViL的细粒度病灶知识。

Method: 设计两大模块：1）遗忘抵抗适配模块，显式保护目标模型的高置信预测，缓解类级性能下降；2）病灶感知适配模块，从ViL模型获取patch级预测/定位信息，引导目标模型关注病灶区域并吸收细粒度知识。联合优化实现源不可得条件下的自适应。

Result: 在眼底图像诊断任务上进行大量实验，方法显著超越仅用ViL的性能，并在多个基准上持续优于现有最先进SFDA方法。

Conclusion: 通过保留目标模型可靠知识并注入ViL的细粒度病灶信息，FRLA有效缓解适配过程中的遗忘问题并提升跨域诊断性能，具备通用性与实用价值。

Abstract: Source-free domain adaptation (SFDA) aims to adapt a model trained in the source domain to perform well in the target domain, with only unlabeled target domain data and the source model. Taking into account that conventional SFDA methods are inevitably error-prone under domain shift, recently greater attention has been directed to SFDA assisted with off-the-shelf foundation models, e.g., vision-language (ViL) models. However, existing works of leveraging ViL models for SFDA confront two issues: (i) Although mutual information is exploited to consider the joint distribution between the predictions of ViL model and the target model, we argue that the forgetting of some superior predictions of the target model still occurs, as indicated by the decline of the accuracies of certain classes during adaptation; (ii) Prior research disregards the rich, fine-grained knowledge embedded in the ViL model, which offers detailed grounding for fundus image diagnosis. In this paper, we introduce a novel forgetting-resistant and lesion-aware (FRLA) method for SFDA of fundus image diagnosis with ViL model. Specifically, a forgetting-resistant adaptation module explicitly preserves the confident predictions of the target model, and a lesion-aware adaptation module yields patch-wise predictions from ViL model and employs them to help the target model be aware of the lesion areas and leverage the ViL model's fine-grained knowledge. Extensive experiments show that our method not only significantly outperforms the vision-language model, but also achieves consistent improvements over the state-of-the-art methods. Our code will be released.

</details>


### [133] [Exploiting Label-Independent Regularization from Spatial Dependencies for Whole Slide Image Analysis](https://arxiv.org/abs/2602.19487)
*Weiyi Wu,Xinwen Xu,Chongyang Gao,Xingjian Diao,Siting Li,Jiang Gui*

Main category: cs.CV

TL;DR: 提出一种空间正则化的多实例学习（MIL）框架，通过利用切片图像中patch间的空间关系，缓解仅有包级标注、监督稀疏导致的训练不稳定与次优解问题，并在多公共数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: WSI分析面临两大难题：数据体量巨大、精细标注稀缺。现有MIL以少量包级标签监督大量patch特征，难以稳定识别判别性patch，优化易不稳、效果受限。作者希望引入与标签无关且广泛可得的空间结构先验，增强训练信号与鲁棒性。

Method: 提出空间正则化MIL：在共享表示空间中，联合优化两类目标—(1) 由特征诱导的空间重构（利用patch之间的固有空间关系进行重建/一致性约束）作为无标签正则；(2) 标签引导的分类损失。通过强制内在结构模式与监督信号一致，实现表示学习与判别学习的协同。

Result: 在多个公开数据集上显著优于现有SOTA方法，表现出更高的分类准确性与稳定性（文中未给出具体数值）。

Conclusion: 利用空间关系作为标签无关的正则信号，可有效缓解MIL监督稀疏与不稳定优化问题；联合空间重构与分类的共享表示学习，为WSI MIL提供了更稳健且性能更优的方向。

Abstract: Whole slide images, with their gigapixel-scale panoramas of tissue samples, are pivotal for precise disease diagnosis. However, their analysis is hindered by immense data size and scarce annotations. Existing MIL methods face challenges due to the fundamental imbalance where a single bag-level label must guide the learning of numerous patch-level features. This sparse supervision makes it difficult to reliably identify discriminative patches during training, leading to unstable optimization and suboptimal solutions. We propose a spatially regularized MIL framework that leverages inherent spatial relationships among patch features as label-independent regularization signals. Our approach learns a shared representation space by jointly optimizing feature-induced spatial reconstruction and label-guided classification objectives, enforcing consistency between intrinsic structural patterns and supervisory signals. Experimental results on multiple public datasets demonstrate significant improvements over state-of-the-art methods, offering a promising direction.

</details>


### [134] [MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models](https://arxiv.org/abs/2602.19497)
*Mingrui Wu,Hang Liu,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出MICON-Bench基准、MLLM驱动的按检查点评测框架和无需训练的动态注意力再平衡（DAR），系统评测并提升多图上下文生成中的跨图合成、语境推理与身份一致性；实验证明基准揭示挑战，DAR提升一致性与减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有UMM虽能一定程度上跨多图推理，但主流基准与方法侧重文本生成或单图编辑，缺少对多图上下文生成（跨图合成、身份保持、语义一致）的系统评测与改进手段，评测还缺乏自动化且贴近语义-视觉一致性的验证机制。

Method: 1) 构建MICON-Bench，涵盖六类任务，评估跨图组合、上下文推理、身份保持等；2) 提出MLLM驱动的Evaluation-by-Checkpoint：在生成流程关键点用多模态大模型作为校验器，自动核验语义与视觉一致性；3) 提出DAR：推理时的训练-free可插拔机制，动态调整注意力权重，抑制无关信息、增强跨图一致性与降低幻觉。

Result: 在多种SOTA开源模型上进行大量实验：MICON-Bench能严格暴露多图推理与一致性方面的不足；应用DAR可显著提升生成质量、跨图连贯性与降低幻觉，优于未使用DAR的基线。

Conclusion: MICON-Bench为多图上下文生成提供系统化评测标准；MLLM校验框架实现自动一致性评估；DAR在无需额外训练的前提下有效提升多图生成的连贯性与可靠性。

Abstract: Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks. In this work, we introduce \textbf{MICON-Bench}, a comprehensive benchmark covering six tasks that evaluate cross-image composition, contextual reasoning, and identity preservation. We further propose an MLLM-driven Evaluation-by-Checkpoint framework for automatic verification of semantic and visual consistency, where multimodal large language model (MLLM) serves as a verifier. Additionally, we present \textbf{Dynamic Attention Rebalancing (DAR)}, a training-free, plug-and-play mechanism that dynamically adjusts attention during inference to enhance coherence and reduce hallucinations. Extensive experiments on various state-of-the-art open-source models demonstrate both the rigor of MICON-Bench in exposing multi-image reasoning challenges and the efficacy of DAR in improving generation quality and cross-image coherence. Github: https://github.com/Angusliuuu/MICON-Bench.

</details>


### [135] [A Text-Guided Vision Model for Enhanced Recognition of Small Instances](https://arxiv.org/abs/2602.19503)
*Hyun-Ki Jung*

Main category: cs.CV

TL;DR: 提出一种改进版的文本引导目标检测模型（基于YOLO-World/YOLOv8骨干），面向无人机小目标与特定目标检索，兼顾精度与轻量化。


<details>
  <summary>Details</summary>
Motivation: 无人机场景中目标小、密集且类别多样，用户希望通过文本提示精确检索特定目标；现有模型对小目标与精确边界表征不足，且在边缘端部署受限于计算与参数规模。

Method: 在YOLO-World框架上将YOLOv8骨干的C2f层替换为C3k2层，以强化局部特征与边界表征，尤其利于小目标；对并行处理进行优化以提升吞吐与效率，整体设计更轻量。

Result: 在VisDrone上，相较原YOLO-World：Precision 40.6%→41.6%，Recall 30.8%→31.0%，F1 35.0%→35.5%，mAP@0.5 30.4%→30.7%；参数量4.0M→3.8M，FLOPs 15.7G→15.2G。

Conclusion: 改进的文本引导检测器在无人机小目标检测中实现了更高精度与更低计算开销，具备实用性与部署友好性，可用于精确目标检索场景。

Abstract: As drone-based object detection technology continues to evolve, the demand is shifting from merely detecting objects to enabling users to accurately identify specific targets. For example, users can input particular targets as prompts to precisely detect desired objects. To address this need, an efficient text-guided object detection model has been developed to enhance the detection of small objects. Specifically, an improved version of the existing YOLO-World model is introduced. The proposed method replaces the C2f layer in the YOLOv8 backbone with a C3k2 layer, enabling more precise representation of local features, particularly for small objects or those with clearly defined boundaries. Additionally, the proposed architecture improves processing speed and efficiency through parallel processing optimization, while also contributing to a more lightweight model design. Comparative experiments on the VisDrone dataset show that the proposed model outperforms the original YOLO-World model, with precision increasing from 40.6% to 41.6%, recall from 30.8% to 31%, F1 score from 35% to 35.5%, and mAP@0.5 from 30.4% to 30.7%, confirming its enhanced accuracy. Furthermore, the model demonstrates superior lightweight performance, with the parameter count reduced from 4 million to 3.8 million and FLOPs decreasing from 15.7 billion to 15.2 billion. These results indicate that the proposed approach provides a practical and effective solution for precise object detection in drone-based applications.

</details>


### [136] [Test-Time Computing for Referring Multimodal Large Language Models](https://arxiv.org/abs/2602.19505)
*Mingrui Wu,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Zhiyuan Liu,Liujuan Cao,Ming-Ming Cheng,Rongrong Ji*

Main category: cs.CV

TL;DR: ControlMLLM++ 在不微调/重训练 MLLM 的前提下，于测试时注入可学习的视觉提示，通过优化潜在视觉token修饰器与能量函数，引导跨模态注意力聚焦到用户指定区域，实现细粒度区域级视觉推理，并提出稳定优化（Optim++）与提示去偏（PromptDebias），适配多种视觉提示形式，具备良好的泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型对区域级精细推理能力不足，且常依赖代价高昂的再训练/微调；同时注意力往往受语言提示偏置影响，难以稳定聚焦用户关心的图像区域。作者希望在冻结模型权重的前提下，通过测试时适配，直接操控模型的跨模态注意力，实现可控、稳健、可解释的区域级推理。

Method: 提出在推理阶段为冻结的 MLLM 注入可学习的视觉提示（latents/visual token modifier），并以任务特定能量函数为目标进行优化，使跨模态注意力对齐到用户指定区域；利用跨模态注意力天然编码文本-视觉对应的性质进行引导。为提高稳定性与减轻语言提示偏置，提出改进优化策略 Optim++ 与提示去偏机制 PromptDebias。方法支持多种视觉提示（框、掩码、涂鸦、点）。

Result: 在多种需要区域控制与细粒度推理的设置中，方法表现出色，具备较强的域外泛化与良好的可解释性，相比基线更稳健，能有效将注意力引导至用户指定区域。

Conclusion: 无需训练或微调即可对冻结 MLLM 实现可控的区域级视觉推理；通过优化可学习视觉提示并结合稳定优化与去偏策略，获得强泛化与可解释性，适用于多种视觉提示类型。

Abstract: We propose ControlMLLM++, a novel test-time adaptation framework that injects learnable visual prompts into frozen multimodal large language models (MLLMs) to enable fine-grained region-based visual reasoning without any model retraining or fine-tuning. Leveraging the insight that cross-modal attention maps intrinsically encode semantic correspondences between textual tokens and visual regions, ControlMLLM++ optimizes a latent visual token modifier during inference via a task-specific energy function to steer model attention towards user-specified areas. To enhance optimization stability and mitigate language prompt biases, ControlMLLM++ incorporates an improved optimization strategy (Optim++) and a prompt debiasing mechanism (PromptDebias). Supporting diverse visual prompt types including bounding boxes, masks, scribbles, and points, our method demonstrates strong out-of-domain generalization and interpretability. The code is available at https://github.com/mrwu-mac/ControlMLLM.

</details>


### [137] [Relational Feature Caching for Accelerating Diffusion Transformers](https://arxiv.org/abs/2602.19506)
*Byunggwan Son,Jeimin Jeon,Jeongwoo Choi,Bumsub Ham*

Main category: cs.CV

TL;DR: 提出RFC框架，通过利用输入-输出关系改进扩散Transformer中特征缓存，降低预测误差并提升速度与质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于时间外推的特征缓存方法仅依赖历史步长进行预测，易因输出特征变化幅度不规律而产生较大误差，导致性能下降；同时观察到模块输入与输出强相关，激发利用这种关系改进预测的动机。

Method: 在DiT推理中引入RFC框架：1) 关系特征估计（RFE）根据当前输入估计输出特征变化幅度，从而校正基于缓存的预测；2) 关系缓存调度（RCS）用输入估计当前步的预测误差，若误差预计较大则触发完整计算，否则复用缓存与外推结果。

Result: 在多种DiT模型和设置上，RFC相较以往仅用时间外推的缓存方法取得显著更优的质量-速度权衡，误差更小、加速更稳健。

Conclusion: 利用输入-输出关系进行特征变化估计与误差感知调度，可显著提升扩散Transformer的特征缓存效果；RFC在广泛实验中优于现有方法，并提供通用的加速策略。

Abstract: Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC

</details>


### [138] [OSInsert: Towards High-authenticity and High-fidelity Image Composition](https://arxiv.org/abs/2602.19523)
*Jingyuan Wang,Li Niu*

Main category: cs.CV

TL;DR: 提出一个两阶段生成式图像合成框架：先用高真实性方法调整前景形状/姿态以契合背景，再以高保真方法在该形状条件下重建细节；在 MureCOM 验证有效，并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有方法在“真实性（姿态/视角与背景一致）”与“保真度（前景细节保留）”间难以兼得，往往偏重其一，导致要么合成不自然，要么细节流失。需要一个兼顾两者的策略。

Method: 采用两阶段流水线：阶段一使用高真实性方法生成与背景协调的前景形状/姿态，得到合理的条件（如形状、视图、掩码或引导）；阶段二以该条件驱动高保真模型，对前景进行细节重建与无缝融合，从而同时满足结构匹配与细节保留。

Result: 在 MureCOM 数据集上实验显示该两阶段策略有效，能够提升合成图的真实性与细节保留指标（文中未给出具体数值，但强调验证通过）；提供代码与模型以复现。

Conclusion: 通过将“形状/姿态对齐”和“细节重建”解耦为两个阶段，可在生成式图像合成中同时达到高真实性与高保真；方法已在公开数据集验证并开源，具有实用价值。

Abstract: Generative image composition aims to regenerate the given foreground object in the background image to produce a realistic composite image. Some high-authenticity methods can adjust foreground pose/view to be compatible with background, while some high-fidelity methods can preserve the foreground details accurately. However, existing methods can hardly achieve both goals at the same time. In this work, we propose a two-stage strategy to achieve both goals. In the first stage, we use high-authenticity method to generate reasonable foreground shape, serving as the condition of high-fidelity method in the second stage. The experiments on MureCOM dataset verify the effectiveness of our two-stage strategy. The code and model have been released at https://github.com/bcmi/OSInsert-Image-Composition.

</details>


### [139] [ORION: ORthonormal Text Encoding for Universal VLM AdaptatION](https://arxiv.org/abs/2602.19530)
*Omprakash Chakraborty,Jose Dolz,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 提出ORION，一种仅用类别名微调VLM文本编码器的框架，通过低秩适配和正交-保真联合损失，使类文本原型更正交且贴近初始原型，在多基准与多骨干、零/少样本与测试时自适应场景下均显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 零样本分类常依赖冻结文本编码器与手工提示生成的类原型，但这些嵌入往往相关性强、可分性弱，限制任务判别力。需要一种方法在不依赖额外标注的前提下，改良类文本原型的几何结构以提升泛化。

Method: 在预训练VLM上，仅用类名对文本编码器进行低秩适配微调。提出由两部分组成的新损失：(1) 类间正交项，促使任务内各类别文本表示两两正交，(2) 保真项，惩罚偏离初始CLIP类原型的程度。并给出正交项的概率解释，将其与最大似然估计通过Huygens定理联系。生成的精炼文本嵌入可替换标准CLIP原型，并作为即插即用模块集成到多种方法与设置中。

Result: 在11个基准与3个大规模VLM骨干上，ORION稳定且显著优于使用标准CLIP原型的基线；作为插件用于零样本、少样本和测试时自适应方法时，均带来一致提升。

Conclusion: 优化类文本原型的几何结构（促进正交、保持保真）能在不需额外数据的前提下显著增强VLM的判别力；ORION提供了简单高效、可插拔的文本侧微调方案，并有坚实的概率学解释。

Abstract: Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability. We introduce ORION, a text encoder fine tuning framework that improves pretrained VLMs using only class names. Our method optimizes, via low rank adaptation, a novel loss integrating two terms, one promoting pairwise orthogonality between the textual representations of the classes of a given task and the other penalizing deviations from the initial class prototypes. Furthermore, we provide a probabilistic interpretation of our orthogonality penalty, connecting it to the general maximum likelihood estimation (MLE) principle via Huygens theorem. We report extensive experiments on 11 benchmarks and three large VLM backbones, showing that the refined textual embeddings yield powerful replacements for the standard CLIP prototypes. Added as plug and play module on top of various state of the art methods, and across different prediction settings (zero shot, few shot and test time adaptation), ORION improves the performance consistently and significantly.

</details>


### [140] [Fore-Mamba3D: Mamba-based Foreground-Enhanced Encoding for 3D Object Detection](https://arxiv.org/abs/2602.19536)
*Zhiwei Ning,Xuanang Gao,Jiaxi Cao,Runze Yang,Huiying Xu,Xinzhong Zhu,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: 提出Fore-Mamba3D：在3D目标检测中对仅前景体素进行线性建模，结合区域到全局滑窗与语义-空间融合模块，缓解前景仅编码导致的响应衰减与上下文缺失，取得多基准优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Mamba的线性建模骨干对整段非空体素序列做双向编码，其中包含大量背景冗余，既低效又可能干扰检测。直接只编码前景虽能去噪，但会因线性自回归的距离/因果依赖导致响应衰减与上下文不足，从而性能下降。

Method: 1) 前景体素采样：依据预测分数选取前景体素，进行前景优先的序列构建；2) 区域到全局滑动窗口（RGSW）：先在局部区域内交互，再通过滑动机制将信息逐步传播至全局，缓解跨实例交互的响应衰减；3) 语义辅助与状态空间融合（SASFMamba）：在Mamba中融合语义与几何线索，增强上下文与形状感知；整体上弱化线性自回归的距离与因果约束，实现高效的前景仅编码。

Result: 在多个3D检测基准上取得优于现有方法的性能（文中未给出具体数值），证明前景增强与改进的线性建模有效。

Conclusion: 通过Fore-Mamba3D实现“只编码前景+RGSW+SASFMamba”，在保持线性模型效率的同时弥补上下文与长程依赖不足，减少背景干扰并提升3D检测精度。

Abstract: Linear modeling methods like Mamba have been merged as the effective backbone for the 3D object detection task. However, previous Mamba-based methods utilize the bidirectional encoding for the whole non-empty voxel sequence, which contains abundant useless background information in the scenes. Though directly encoding foreground voxels appears to be a plausible solution, it tends to degrade detection performance. We attribute this to the response attenuation and restricted context representation in the linear modeling for fore-only sequences. To address this problem, we propose a novel backbone, termed Fore-Mamba3D, to focus on the foreground enhancement by modifying Mamba-based encoder. The foreground voxels are first sampled according to the predicted scores. Considering the response attenuation existing in the interaction of foreground voxels across different instances, we design a regional-to-global slide window (RGSW) to propagate the information from regional split to the entire sequence. Furthermore, a semantic-assisted and state spatial fusion module (SASFMamba) is proposed to enrich contextual representation by enhancing semantic and geometric awareness within the Mamba model. Our method emphasizes foreground-only encoding and alleviates the distance-based and causal dependencies in the linear autoregression model. The superior performance across various benchmarks demonstrates the effectiveness of Fore-Mamba3D in the 3D object detection task.

</details>


### [141] [Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems](https://arxiv.org/abs/2602.19539)
*Xingyu Shen,Tommy Duong,Xiaodong An,Zengqi Zhao,Zebang Hu,Haoyu Hu,Ziyou Wang,Finn Guo,Simiao Ren*

Main category: cs.CV

TL;DR: 论文评估了化妆与发型等家庭可及“物理”改动对AI年龄估计器的影响，提出ACR指标，并在329张10–21岁人脸上用VLM编辑合成胡须、白发、化妆和皱纹，测8个模型。合成胡须即可触发28–69%未成年→成年翻转；四攻组合平均+7.7岁，最高83% ACR；VLM型总体略低于专用模型，但区间重叠且未做统计检验。结论：现有在线年龄核验存在显著脆弱性，需将对抗鲁棒性纳入选型标准。


<details>
  <summary>Details</summary>
Motivation: 线上内容与服务常以年龄门禁合规，但真实环境下用户可通过简单外观改动规避。既往缺少系统化量化：哪些日常可行的外观变化会使未成年人被判定为成年人、规模效应如何、不同模型家族差异怎样，以及如何在不同样本年龄分布下公平比较。

Method: - 构造数据：收集329张10–21岁人脸。
- 攻击生成：用VLM图像编辑器合成四类“可家庭实现”的外观变换：胡须、灰发、化妆、皱纹；亦考虑组合攻击。
- 模型评测：8个年龄估计模型（5个专用模型与3个VLM）。
- 指标：提出Attack Conversion Rate（ACR），度量基线判为未成年且经攻击后翻为成年的比例，避免受数据中未成年的占比影响；并报告预测年龄偏移。

Result: - 单一胡须攻击即可在全部8个模型上实现28–69% ACR。
- 四攻组合使平均预测年龄上升7.7岁；ACR最高达83%。
- VLM模型在全攻下ACR约59–71%，专用模型约63–83%，范围重叠且差异未做统计显著性检验。

Conclusion: 简单、低成本的外观改动即可大幅提升未成年被判为成年的概率，当前年龄核验流程存在关键安全缺口。应在模型选型和部署中强制开展对抗鲁棒性评估，并针对可行物理/合成伪装设计缓解措施（如多模态信号、活体与一致性检测、攻击感知训练、阈值与不确定性校准等）。

Abstract: Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano). We introduce the Attack Conversion Rate (ACR), defined as the fraction of images predicted as minor at baseline that flip to adult after attack, a population-agnostic metric that does not depend on the ratio of minors to adults in the test set. Our results reveal that a synthetic beard alone achieves 28 to 69 percent ACR across all eight models; combining all four attacks shifts predicted age by +7.7 years on average across all 329 subjects and reaches up to 83 percent ACR; and vision-language models exhibit lower ACR (59 to 71 percent) than specialized models (63 to 83 percent) under the full attack, although the ACR ranges overlap and the difference is not statistically tested. These findings highlight a critical vulnerability in deployed age-verification pipelines and call for adversarial robustness evaluation as a mandatory criterion for model selection.

</details>


### [142] [A Green Learning Approach to LDCT Image Restoration](https://arxiv.org/abs/2602.19540)
*Wei Wang,Yixing Wu,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: 提出一种“绿色学习”(GL)方法用于医学图像恢复（以低剂量CT为例），在更小模型与更低推理复杂度下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: LDCT图像因成像剂量低而含有噪声与伪影，影响后续诊断分析；现有深度学习方法虽有效但往往黑箱、参数多、资源消耗大，亟需一种可解释、计算与内存效率高且性能优异的替代方案。

Method: 采用Green Learning(GL)范式构建图像恢复模型，强调数学可解释性与资源效率；在LDCT去噪/去伪影任务上设计轻量级恢复流程，并以较小模型规模与较低推理复杂度实现重建。

Result: 实验表明，所提GL方法在LDCT恢复上达到或超过当前最先进（SOTA）性能，同时显著降低模型大小与推理开销。

Conclusion: GL为医学图像恢复提供了一种兼具高性能与高效率、且具数学透明性的可行替代方案，适合作为LDCT等场景的预处理步骤。

Abstract: This work proposes a green learning (GL) approach to restore medical images. Without loss of generality, we use low-dose computed tomography (LDCT) images as examples. LDCT images are susceptible to noise and artifacts, where the imaging process introduces distortion. LDCT image restoration is an important preprocessing step for further medical analysis. Deep learning (DL) methods have been developed to solve this problem. We examine an alternative solution using the Green Learning (GL) methodology. The new restoration method is characterized by mathematical transparency, computational and memory efficiency, and high performance. Experiments show that our GL method offers state-of-the-art restoration performance at a smaller model size and with lower inference complexity.

</details>


### [143] [Vinedresser3D: Agentic Text-guided 3D Editing](https://arxiv.org/abs/2602.19542)
*Yankuan Chi,Xiang Li,Zixuan Huang,James M. Rehg*

Main category: cs.CV

TL;DR: Vinedresser3D 是一个基于代理的文本引导3D编辑框架，在原生3D生成模型的潜空间中进行无掩码、高保真编辑，自动定位编辑区域并保持未编辑内容与3D一致性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导3D编辑方法难以同时：1) 理解复杂自然语言指令；2) 在3D中自动定位需编辑的区域；3) 在编辑时保持未修改内容与跨视角一致性。需要一个能综合语言-视觉理解、自动分解与定位编辑，并在3D潜空间稳定执行编辑的统一方案。

Method: 提出代理式流水线：1) 用多模态大语言模型分析输入3D资产与编辑指令，产出原始资产的丰富描述、编辑区域与编辑类型（加/改/删），并分解为结构级与外观级文本指导；2) 代理选择信息量大的视角，调用图像编辑模型生成视觉引导；3) 在原生3D生成模型的潜空间中，采用基于反演的rectified-flow修补（inpainting）管线与交替采样模块，结合视觉与文本引导执行编辑，确保提示对齐、3D一致性与未编辑区域保真。

Result: 在多样化3D编辑任务上，相比以往基线在自动评价指标与人类偏好评测中均取得更好表现，达成更精确、连贯、无需掩码的3D编辑效果。

Conclusion: 通过在3D潜空间内配合代理式理解与分解、视角选择与视觉引导，以及反演式rectified-flow修补策略，Vinedresser3D 实现高质量文本引导3D编辑，兼顾编辑定位准确性、3D一致性与内容保真，优于现有方法。

Abstract: Text-guided 3D editing aims to modify existing 3D assets using natural-language instructions. Current methods struggle to jointly understand complex prompts, automatically localize edits in 3D, and preserve unedited content. We introduce Vinedresser3D, an agentic framework for high-quality text-guided 3D editing that operates directly in the latent space of a native 3D generative model. Given a 3D asset and an editing prompt, Vinedresser3D uses a multimodal large language model to infer rich descriptions of the original asset, identify the edit region and edit type (addition, modification, deletion), and generate decomposed structural and appearance-level text guidance. The agent then selects an informative view and applies an image editing model to obtain visual guidance. Finally, an inversion-based rectified-flow inpainting pipeline with an interleaved sampling module performs editing in the 3D latent space, enforcing prompt alignment while maintaining 3D coherence and unedited regions. Experiments on diverse 3D edits demonstrate that Vinedresser3D outperforms prior baselines in both automatic metrics and human preference studies, while enabling precise, coherent, and mask-free 3D editing.

</details>


### [144] [DICArt: Advancing Category-level Articulated Object Pose Estimation in Discrete State-Spaces](https://arxiv.org/abs/2602.19565)
*Li Zhang,Mingyu Mei,Ailing Wang,Xianhui Meng,Yan Zhong,Xinyuan Song,Liu Liu,Rujing Wang,Zaixing He,Cewu Lu*

Main category: cs.CV

TL;DR: 提出DICArt：将关节物体位姿估计从连续回归改为条件离散扩散，结合动态“去噪/重置”流选择与分层运动学耦合，提升在合成与真实数据上的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 连续空间回归易受巨大复杂搜索空间影响，并且难以显式纳入物体的内在运动学约束，导致不稳定或不物理的位姿估计。需要一种既能高效搜索又能尊重结构先验的新范式。

Method: 把位姿表示离散化为token，并以条件离散扩散建模：从噪声token逐步反向去噪到GT位姿。引入可学习的flow decider，对每个token动态判定“继续去噪”或“重置”，在真实分布与噪声分布间自适应平衡；再以分层运动学耦合，自上而下逐部件估计位姿，显式满足关节结构。

Result: 在合成与真实数据集上验证，较现有方法取得更高精度与更强鲁棒性（摘要未给具体数值），表明离散生成与结构先验的结合有效。

Conclusion: 将位姿估计表述为条件离散扩散并融入分层运动学先验，可在复杂环境下实现更可靠的类别级6D位姿估计，提供了一条有前景的新范式。

Abstract: Articulated object pose estimation is a core task in embodied AI. Existing methods typically regress poses in a continuous space, but often struggle with 1) navigating a large, complex search space and 2) failing to incorporate intrinsic kinematic constraints. In this work, we introduce DICArt (DIsCrete Diffusion for Articulation Pose Estimation), a novel framework that formulates pose estimation as a conditional discrete diffusion process. Instead of operating in a continuous domain, DICArt progressively denoises a noisy pose representation through a learned reverse diffusion procedure to recover the GT pose. To improve modeling fidelity, we propose a flexible flow decider that dynamically determines whether each token should be denoised or reset, effectively balancing the real and noise distributions during diffusion. Additionally, we incorporate a hierarchical kinematic coupling strategy, estimating the pose of each rigid part hierarchically to respect the object's kinematic structure. We validate DICArt on both synthetic and real-world datasets. Experimental results demonstrate its superior performance and robustness. By integrating discrete generative modeling with structural priors, DICArt offers a new paradigm for reliable category-level 6D pose estimation in complex environments.

</details>


### [145] [VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense](https://arxiv.org/abs/2602.19570)
*Nadav Kadvil,Ayellet Tal*

Main category: cs.CV

TL;DR: 提出一种针对对抗性图像攻击的高效、免训练防御：用内容保持的图像变换+文本嵌入一致性检测，并在必要时调用LLM整合多次回答，既提升鲁棒性又控制开销。


<details>
  <summary>Details</summary>
Motivation: LVLM 在面对细微扰动的对抗图像时会被引导产生看似合理但错误的回答，现有防御往往代价高、需再训练或泛化差，因此需要一种通用、高效、免训练且对干净样本低开销的防御方法。

Method: 提出两阶段检测与“智能体式”多响应整合：1) 低成本的内容保持变换（如裁剪、缩放、色彩轻调）后检查模型输出一致性，快速筛除大多数干净样本；2) 对可疑样本在文本嵌入空间评估回答差异；仅当仍存在显著分歧时，调用强LLM进行冲突解析与多回答整合，利用回答间的相似点和差异点进行巩固。

Result: 在对抗样本存在的情况下达到SOTA准确率；对干净图像多数可跳过昂贵步骤，整体推理开销很小；即使面对大量对抗样本，额外开销仍保持在较低水平。

Conclusion: 通过变换一致性检测、嵌入差异评估与按需LLM整合的三级流程，可在无需训练的前提下显著提升LVLM对对抗图像的鲁棒性，并兼顾准确性与效率。

Abstract: Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior. A key component of our approach is a two-stage detection mechanism that quickly filters out the majority of clean inputs. We first assess image consistency under content-preserving transformations at negligible computational cost. For more challenging cases, we examine discrepancies in a text-embedding space. Only when necessary do we invoke a powerful LLM to resolve attack-induced divergences. A key idea is to consolidate multiple responses, leveraging both their similarities and their differences. We show that our method achieves state-of-the-art accuracy while maintaining notable efficiency: most clean images skip costly processing, and even in the presence of numerous adversarial examples, the overhead remains minimal.

</details>


### [146] [HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies](https://arxiv.org/abs/2602.19571)
*Chang Liu,Yunfan Ye,Qingyang Zhou,Xichen Tan,Mengxuan Luo,Zhenyu Qiu,Wei Peng,Zhiping Cai*

Main category: cs.CV

TL;DR: HOCA-Bench 是一个用于评测视频多模态大模型物理常识与预测建模能力的新基准，区分“本体异常”和“因果异常”，并展示当前模型在因果物理推理上存在显著性能短板。


<details>
  <summary>Details</summary>
Motivation: 尽管 Video-LLMs 在语义理解上进步显著，但在可预测的世界建模与物理一致性上表现不足。作者希望构建一个系统化基准，明确区分对象自洽性与交互物理规律两类异常，从而量化并诊断模型在物理常识与因果推理方面的缺陷。

Method: 提出 HOCA-Bench，从黑格尔哲学视角将异常分为两类：本体异常（对象违背其定义/持续性，如形状突变）与因果异常（交互违背物理关系，如重力/摩擦）。利用最先进生成式视频模型作为对抗式模拟器生成/扩充数据，构建含 1,439 段视频与 3,470 组问答对的测试集；在 17 个 Video-LLMs 上进行系统评测，并比较常规与 System-2 “思考”模式的表现。

Result: 模型在静态本体异常检测上相对较好，但在因果机制（如重力、摩擦）相关任务上显著掉队，因果任务性能下降超过 20%。尽管引入 System-2 思考模式能一定程度提升推理，但仍无法弥补与本体类任务之间的差距。

Conclusion: 当前 Video-LLMs 更擅长识别视觉模式而非应用基本物理定律。HOCA-Bench 揭示了模型在因果物理推理上的系统性短板，提示未来需要在具身物理建模、动态交互理解与可微物理/显式物理先验融合方面进行架构与训练范式改进。

Abstract: Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 "Thinking" modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.

</details>


### [147] [ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization](https://arxiv.org/abs/2602.19575)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: ConceptPrism 通过在同一集合内比较多张参考图像，自动将共享概念与图像特有残差分离；联合优化一个“目标概念”token与图像级残差tokens，以重建损失保真、排除损失去除共享概念，从而在无需人工标注的情况下提升个性化文生图的概念保真与文本对齐折中。


<details>
  <summary>Details</summary>
Motivation: 个性化文生图常把参考图像里的无关信息一并学进去，导致“概念保真 vs. 文本对齐”的拉扯。现有方法多依赖人工提示（语言线索、分割掩码等），通用性差且难完整刻画目标概念，亟需一种自动、弱监督的解缠方案。

Method: 提出 ConceptPrism：给定多张同一概念的参考图像，联合学习一个共享的目标概念token和每张图像的残差tokens。优化包含两部分：1) 重建损失，保证从概念token+对应残差可重建参考图像（保证保真）；2) 新的排除损失，强制残差tokens不携带跨图共享的概念信息，从而把共享语义挤压进目标概念token，实现自动解缠。推理时，用目标token驱动文生图以达成更好对齐，同时可按需叠加残差以调节个性化细节。

Result: 在大量实验中，相较现有解缠方法，ConceptPrism 显著缓解概念缠结，在保持或提升概念保真的同时，明显增强文本对齐与可控性，取得更优的折中。

Conclusion: 自动化、无显式人工标注的概念与残差分离是可行的。通过重建与排除的互补目标，ConceptPrism 能学得“纯净”概念表示，进而提升个性化文生图的对齐与保真表现。

Abstract: Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.

</details>


### [148] [Learning Mutual View Information Graph for Adaptive Adversarial Collaborative Perception](https://arxiv.org/abs/2602.19596)
*Yihang Tao,Senkang Hu,Haonan An,Zhengru Fang,Hangcheng Cao,Yuguang Fang*

Main category: cs.CV

TL;DR: 提出MVIG攻击：利用互视信息图与时序图学习自适应地定位与时机化篡改，在多种防御下显著降低协同感知的防御成功率与检测率。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知通过多车共享特征提高安全，但对特征层对抗伪造脆弱；阈值一致性防御易被更复杂策略绕过，尤其在有系统的时间/区域优化与由共享信息泄露脆弱性时。需要能跨多种防御配置自适应并泛化的攻击框架，以揭示并量化CP系统安全缺口。

Method: 构建统一的互视信息图（MVIG）表示不同车端与防御系统的协作与置信交互；结合时序图学习生成动态“伪造风险图”；通过熵感知的脆弱性搜索优化攻击的位置、时机与持续性；形成可适配多种防御配置的对抗样本生成与投放流程，实现特征层伪造物体。

Result: 在OPV2V与Adv-OPV2V上，相比SOTA防御，防御成功率最多下降62%；对持续性攻击的检测率降低47%；推理速度约29.9 FPS，显示高效且普适的攻效。

Conclusion: MVIG攻击能利用共享协作数据中的隐式置信泄露，跨防御配置自适应地实施高效对抗，显著削弱现有阈值共识防御，暴露CP系统在时序与多视融合维度的关键安全漏洞。

Abstract: Collaborative perception (CP) enables data sharing among connected and autonomous vehicles (CAVs) to enhance driving safety. However, CP systems are vulnerable to adversarial attacks where malicious agents forge false objects via feature-level perturbations. Current defensive systems use threshold-based consensus verification by comparing collaborative and ego detection results. Yet, these defenses remain vulnerable to more sophisticated attack strategies that could exploit two critical weaknesses: (i) lack of robustness against attacks with systematic timing and target region optimization, and (ii) inadvertent disclosure of vulnerability knowledge through implicit confidence information in shared collaboration data. In this paper, we propose MVIG attack, a novel adaptive adversarial CP framework learning to capture vulnerability knowledge disclosed by different defensive CP systems from a unified mutual view information graph (MVIG) representation. Our approach combines MVIG representation with temporal graph learning to generate evolving fabrication risk maps and employs entropy-aware vulnerability search to optimize attack location, timing and persistence, enabling adaptive attacks with generalizability across various defensive configurations. Extensive evaluations on OPV2V and Adv-OPV2V datasets demonstrate that MVIG attack reduces defense success rates by up to 62\% against state-of-the-art defenses while achieving 47\% lower detection for persistent attacks at 29.9 FPS, exposing critical security gaps in CP systems. Code will be released at https://github.com/yihangtao/MVIG.git

</details>


### [149] [CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning](https://arxiv.org/abs/2602.19605)
*Chunlei Meng,Guanhong Huang,Rong Fu,Runmin Jian,Zhongxue Gan,Chun Ouyang*

Main category: cs.CV

TL;DR: 提出CLCR框架，通过跨层级的共享/私有解耦与受控交互，缓解多模态异步与语义错配问题，提升泛化与表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法常把所有模态压到同一潜空间，忽视不同模态在语义层级（浅/中/深）上的异步与多粒度特性，导致语义错位与误差传播，削弱表示质量。

Method: 1) 语义层级编码器：将各模态特征组织为浅/中/深三层，并在层级间对齐以建立交互基线。2) IntraCED：在每层内将特征分解为共享与私有子空间，并通过可学习的token预算限制跨模态注意力仅在共享子空间内进行，防止私有信息泄漏。3) InterCAD：利用学习到的锚点跨层同步语义尺度，选择性融合共享表示，并对私有线索加门控，形成紧凑的任务表示。4) 正则化：加入分离共享/私有与减少跨层干扰的正则项。5) 在6个基准上验证，涵盖情感识别、事件定位、情绪/情感分析与动作识别。

Result: 在六个跨任务基准上取得强性能并展现良好泛化能力，相比常规单潜空间融合方法更稳健，减少错配与误差扩散。

Conclusion: 显式的跨层级协同表示与共享/私有受控交互能有效对齐多模态多粒度语义，抑制信息泄漏与干扰，提升多任务多数据集上的表现与泛化。

Abstract: Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.

</details>


### [150] [Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning](https://arxiv.org/abs/2602.19608)
*Girmaw Abebe Tadesse,Titien Bartette,Andrew Hassanali,Allen Kim,Jonathan Chemla,Andrew Zolli,Yves Ubelmann,Caleb Robinson,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CV

TL;DR: 利用PlanetScope月度影像与阿富汗1943处遗址数据集，对比端到端CNN与传统ML特征法，发现以ImageNet预训练+空间掩膜的CNN在掠夺检测上达最佳F1=0.926，显著优于最佳传统方案F1=0.710；掩膜与预训练关键，而地理大模型嵌入并未超越手工特征，表明掠夺信号高度局地化。


<details>
  <summary>Details</summary>
Motivation: 考古遗址被盗挖对文化遗产破坏严重，而成千上万偏远遗址的持续人工监测成本高、难度大，亟需可扩展的自动化遥感方法来及时、可靠地识别被盗挖遗址。

Method: 构建2016–2023年PlanetScope月度马赛克（4.7m/像素）与阿富汗1943处遗址（898被掠夺、1045保存）及遗址轮廓掩膜的数据集；比较两类方法：(i) 端到端CNN对原始RGB补丁进行分类，并结合空间掩膜；(ii) 传统机器学习，基于手工光谱/纹理特征与遥感基础模型（如SatCLIP等）嵌入，使用随机森林并做时间均值聚合；并开展消融实验评估ImageNet预训练与空间掩膜等要素。

Result: 最佳方案为ImageNet预训练的CNN结合空间掩膜，F1=0.926；最强传统ML为SatCLIP-V+位置信息嵌入+随机森林+时间均值聚合，F1=0.710。消融显示：ImageNet预训练和空间掩膜均显著提升性能；地理基础模型嵌入与手工特征表现相当，未显著优于后者。

Conclusion: 可扩展的卫星影像管线能高效检测被盗挖遗址，端到端且利用通用预训练与空间掩膜的CNN显著领先传统ML；基础模型嵌入未带来预期增益，暗示盗挖迹象高度局部化。代码与数据仓库已公开，便于复现与扩展。

Abstract: Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.

</details>


### [151] [RAID: Retrieval-Augmented Anomaly Detection](https://arxiv.org/abs/2602.19611)
*Mingxiu Cai,Zhe Zhang,Gaochang Wu,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: RAID 将“检索增强生成”思想引入无监督异常检测，用分层检索+匹配代价体+引导式MoE抑噪，生成细粒度异常图并取得多基准SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有UAD依赖重建或模板检索，但测试图与正常模板的匹配会因类内变化、对应不准与模板有限而产生噪声，削弱检测与定位精度；需要一种能直接利用检索样本并在生成异常图时抑制匹配噪声的框架。

Method: 提出RAID：将RAG范式用于UAD。1) 构建分层向量库，按类别/语义/实例三级检索正常样本；2) 以检索样本与输入之间的相关性构造匹配代价体；3) 设计由检索样本引导的MoE网络，自适应选择专家以抑制匹配噪声并输出细粒度异常图；4) 形成由粗到细的处理流水线。

Result: 在MVTec、VisA、MPDD、BTAD上于full-shot、few-shot、多数据集等设定均达SOTA，显著优于重建与直接检索方法，提供更精细的异常定位。

Conclusion: 通过检索增强与MoE抑噪，RAID有效缓解模板匹配噪声，统一提升UAD检测与定位性能，并具备跨数据集与低样本场景的鲁棒性。

Abstract: Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.

</details>


### [152] [Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness](https://arxiv.org/abs/2602.19615)
*Xin Hu,Haomiao Ni,Yunbei Zhang,Jihun Hamm,Zechen Li,Zhengming Ding*

Main category: cs.CV

TL;DR: 提出一个即插即用模块，无需微调VLM，通过学习稀有类别的多模态类嵌入，精炼视觉token并为文本提示注入对象线索，从而显著提升对稀有物体的识别与推理。


<details>
  <summary>Details</summary>
Motivation: VLM在通用理解上强，但因预训练数据中稀有物体样本稀缺，导致对象中心的稀有类推理薄弱。现有方法需额外数据检索或更强视觉编码器，微调代价高，且未充分利用原始训练数据。

Method: 1) 利用视觉基础模型先验与同义词扩展的文本描述，学习稀有对象的多模态类嵌入；2) 以轻量注意力增强模块，用这些嵌入去细化VLM内部的视觉token，提升细粒度对象细节；3) 将学得的嵌入作为对象感知“探测器”，生成信息性提示（hints），注入到文本prompt，引导模型关注相关图像区域。无需对VLM进行微调，模块可插拔。

Result: 在两个基准上，多个预训练VLM在稀有对象识别与推理任务上取得一致且显著提升；消融与可视化分析显示该方法能更好地聚焦并推理稀有对象。

Conclusion: 通过多模态类嵌入与轻量增强及提示注入的组合，在不微调VLM的前提下，有效缓解稀有对象样本稀缺带来的推理不足，为低成本增强VLM稀有类能力提供了通用方案。

Abstract: Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.

</details>


### [153] [PedaCo-Gen: Scaffolding Pedagogical Agency in Human-AI Collaborative Video Authoring](https://arxiv.org/abs/2602.19623)
*Injun Baek,Yearim Kim,Nojun Kwak*

Main category: cs.CV

TL;DR: 论文提出PedaCo-Gen：一种以教学法为导向的人机协同文本生成视频系统，通过在生成前加入“中间表示（IR）”阶段，让教师先与AI审阅者互动迭代脚本与画面蓝图，从而生成更符合多媒体学习认知理论（CTML）的教学视频。23位教学专家实验表明：相较基线，视频质量与CTML契合度显著提升，用户感知指导有效、效率高。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型多追求视觉逼真，忽视教学有效性；教学视频需要遵循CTML等原则并让教师保有教学主导权。缺少将人类教学设计专业性与生成式AI能力融合的有效流程与工具。

Method: 提出PedaCo-Gen流程：摒弃“一步到位”的视频生成，在正式生成前引入IR阶段，产出可编辑的“视频蓝图”（脚本+视觉描述）；提供AI审阅者基于CTML原则对蓝图进行逐点反馈与调整，支持教育者交互改进；与基线方法对比的用户研究（N=23教育专家）评估视频质量、CTML符合度、使用体验。

Result: 与基线相比，PedaCo-Gen在多主题与多条CTML原则上显著提升视频质量；参与者将AI指导视为“元认知支架”，报告较高生产效率（平均4.26）与指导有效性（平均4.04）。

Conclusion: 以教学法为核心的人机共创流程能显著提升教学视频质量，兼顾生成式能力与人类专业判断；强调通过IR与AI审阅实现“夺回教学主导权”。该范式为未来AI作者工具提供方向：用原则化共创来统一生成力与专业知识。

Abstract: While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer's Cognitive Theory of Multimedia Learning (CTML). Moving away from traditional "one-shot" generation, PedaCo-Gen introduces an Intermediate Representation (IR) phase, enabling educators to interactively review and refine video blueprints-comprising scripts and visual descriptions-with an AI reviewer. Our study with 23 education experts demonstrates that PedaCo-Gen significantly enhances video quality across various topics and CTML principles compared to baselines. Participants perceived the AI-driven guidance not merely as a set of instructions but as a metacognitive scaffold that augmented their instructional design expertise, reporting high production efficiency (M=4.26) and guide validity (M=4.04). These findings highlight the importance of reclaiming pedagogical agency through principled co-creation, providing a foundation for future AI authoring tools that harmonize generative power with human professional expertise.

</details>


### [154] [Accurate Planar Tracking With Robust Re-Detection](https://arxiv.org/abs/2602.19624)
*Jonas Serych,Jiri Matas*

Main category: cs.CV

TL;DR: 提出两种新的平面跟踪器：SAM-H 与 WOFTSAM，结合SAM2的长期分割跟踪与8自由度单应变换估计，在POT-210与PlanarTrack上创SOTA，并提供更精确的PlanarTrack初始标注。


<details>
  <summary>Details</summary>
Motivation: 现有平面跟踪方法在目标外观剧变、遮挡和丢失后重识别方面鲁棒性不足；同时，对高精度基准（如p@5）评测受初始姿态标注误差影响。

Method: 1) SAM-H：利用SAM 2提供的分割掩膜及其轮廓来稳健估计8 DoF单应变换，实现对外观变化的鲁棒平面位姿跟踪；2) WOFTSAM：在现有SOTA平面跟踪器WOFT上集成SAM-H的丢失重检能力，提升长期稳定性；3) 提供更准确的PlanarTrack初始姿态再标注以改善高精度评测。

Result: 在POT-210和PlanarTrack基准上达成新的SOTA。在PlanarTrack上，p@15指标较第二名分别提升+12.4和+15.2个百分点；再标注使高精度p@5评测更可靠。

Conclusion: 结合分割驱动的长期跟踪与单应估计可显著提升平面跟踪鲁棒性与精度；通过为现有方法提供重检能力与改进标注，可在多基准上取得大幅领先并促进公平评测。

Abstract: We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes. WOFTSAM significantly improves the current state-of-the-art planar tracker WOFT by exploiting lost target re-detection provided by SAM-H. The proposed methods are evaluated on POT-210 and PlanarTrack tracking benchmarks, setting the new state-of-the-art performance on both. On the latter, they outperform the second best by a large margin, +12.4 and +15.2pp on the p@15 metric. We also present improved ground-truth annotations of initial PlanarTrack poses, enabling more accurate benchmarking in the high-precision p@5 metric. The code and the re-annotations are available at https://github.com/serycjon/WOFTSAM

</details>


### [155] [Localized Concept Erasure in Text-to-Image Diffusion Models via High-Level Representation Misdirection](https://arxiv.org/abs/2602.19631)
*Uichan Lee,Jeonghyeon Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: 提出HiRM方法，在文本编码器早期层通过高层表示“误导”来精确擦除目标概念，降低对无关概念的伤害，并与去噪器擦除方法互补。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型可被滥用以生成有害、隐私或受版权保护的内容。现有概念擦除多在U-Net去噪器上微调，但因因果追踪表明视觉属性多存于文本编码器早期自注意力层，提示新的擦除位置与策略。

Method: 仅更新文本编码器中承载视觉属性因果状态的早期层，同时在高层语义空间对目标概念的表示进行“误导”，将其对齐到指定向量（随机方向或语义方向如上位类）。这种“高层表示误导”（HiRM）解耦了位置（低层更新）与语义目标（高层约束），实现精确擦除。

Result: 在UnlearnCanvas与NSFW等基准上，对对象、风格、裸露等多类目标实现强擦除且对无关概念影响最小；保留生成质量与实用性、训练成本低；可零/低成本迁移到如Flux等SOTA架构；与基于去噪器的擦除方法结合有协同增益。

Conclusion: 在文本编码器侧进行高层表示误导并只更新早期层，是比仅微调去噪器更稳健的概念擦除途径；HiRM实现更精确的目标抑制、较小的副作用与良好的可迁移性，并可与现有方法互补。

Abstract: Recent advances in text-to-image (T2I) diffusion models have seen rapid and widespread adoption. However, their powerful generative capabilities raise concerns about potential misuse for synthesizing harmful, private, or copyrighted content. To mitigate such risks, concept erasure techniques have emerged as a promising solution. Prior works have primarily focused on fine-tuning the denoising component (e.g., the U-Net backbone). However, recent causal tracing studies suggest that visual attribute information is localized in the early self-attention layers of the text encoder, indicating a potential alternative for concept erasing. Building on this insight, we conduct preliminary experiments and find that directly fine-tuning early layers can suppress target concepts but often degrades the generation quality of non-target concepts. To overcome this limitation, we propose High-Level Representation Misdirection (HiRM), which misdirects high-level semantic representations of target concepts in the text encoder toward designated vectors such as random directions or semantically defined directions (e.g., supercategories), while updating only early layers that contain causal states of visual attributes. Our decoupling strategy enables precise concept removal with minimal impact on unrelated concepts, as demonstrated by strong results on UnlearnCanvas and NSFW benchmarks across diverse targets (e.g., objects, styles, nudity). HiRM also preserves generative utility at low training cost, transfers to state-of-the-art architectures such as Flux without additional training, and shows synergistic effects with denoiser-based concept erasing methods.

</details>


### [156] [Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation](https://arxiv.org/abs/2602.19668)
*He Zhu,Ren Togo,Takahiro Ogawa,Kenji Hirata,Minghui Tang,Takaaki Yoshimura,Hiroyuki Sugimori,Noriko Nishioka,Yukie Shimizu,Kohsuke Kudo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出FedTAR：在联邦学习中显式建模时间演化与个体差异，用人口统计学驱动的LoRA个性化与时间感知聚合（MAML优化的时间权重）来改进纵向医疗报告生成；在J-MID与MIMIC-CXR上全面提升准确性、时序一致性与跨站泛化。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习多假设客户端分布静态，忽视患者随访多次检查的时间漂移与个体异质性，导致优化不稳与报告生成效果差；需要既隐私友好又能刻画纵向动态的方法。

Method: 提出联邦时间自适应(FTA)设定，并在此基础上给出FedTAR框架：1) 用人口统计学嵌入生成轻量LoRA适配器实现个性化；2) 进行时间残差聚合，将不同时访次的更新按元学习得到的时间策略加权；3) 时间策略通过一阶MAML训练；实现同时兼顾个体化与时间变化的全局-本地协同。

Result: 在J-MID（百万级检查）与MIMIC-CXR数据集上，相比基线在语言准确度、时间一致性与跨站泛化上均取得稳定提升。

Conclusion: FedTAR在隐私保护前提下有效建模联邦纵向动态，缓解非平稳分布与个体异质性带来的优化难题，提供了鲁棒的纵向医疗报告生成范式。

Abstract: Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation.
  We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML.
  Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.

</details>


### [157] [TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures](https://arxiv.org/abs/2602.19679)
*Hyeongjin Nam,Daniel Sungho Jung,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: TeHOR提出从单张图像联合重建3D人-物体，突破仅依赖接触与局部几何的限制，通过文本语义与外观线索对齐，实现包含非接触交互的语义一致且视觉可信的重建，并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖物理接触与局部几何，难以表达凝视、指向等非接触交互，且忽视人/物体外观带来的全局上下文，导致重建语义不足与视觉不合理。

Method: 构建TeHOR框架：1) 引入人-物交互的文本描述，与3D重建进行语义对齐，扩展到非接触交互推理；2) 将人和物体的外观线索纳入对齐过程，结合全局上下文以约束几何与外观的一致性。

Result: 在包含多种交互（含非接触）的评测中，TeHOR生成更准确且语义一致的3D人-物重建，整体性能达到或超过最新方法（SOTA）。

Conclusion: 通过将文本交互语义与外观上下文融入联合重建，TeHOR有效补足接触与局部几何的局限，实现更全面、合理的3D人-物重建，具备通用性与实用潜力。

Abstract: Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.

</details>


### [158] [BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU](https://arxiv.org/abs/2602.19697)
*Soumya Mazumdar,Vineet Kumar Rakesh,Tapas Samanta*

Main category: cs.CV

TL;DR: BayesFusion-SDF 提出一种在CPU上运行的概率化SDF体素融合方法，用稀疏高斯随机场建模距离并显式给出后验与不确定性，在精度上优于传统TSDF，同时支持基于不确定性的表面提取与下一视角规划，作为轻量、可解释的神经隐式替代。


<details>
  <summary>Details</summary>
Motivation: 现有TSDF等体素融合方法依赖启发式加权，无法系统地表征不确定性；神经隐式重建虽精度高但训练/优化耗GPU且难以解释与用于后续决策。需要一种既高效、可在CPU上运行、又能给出概率不确定性的重建框架。

Method: 两阶段：1) 先用粗TSDF重建确定自适应窄带体素域；2) 在窄带内将深度观测融入异方差贝叶斯模型，把SDF视为稀疏高斯随机场，通过稀疏线性代数与预条件共轭梯度求解后验均值；用随机化对角估计快速近似后验方差；基于后验可进行表面提取与不确定性感知的NBV规划。

Result: 在受控消融场景与CO3D物体序列上，较TSDF基线获得更高几何精度；同时产生有用的不确定性估计，可用于主动感知（下一视角规划）。

Conclusion: BayesFusion-SDF在保持CPU友好与确定性求解的同时，实现了显式概率建模与不确定性量化，提供了清晰、可解释且可操作的传统TSDF替代方案，并避免了神经方法的GPU开销与黑箱性。

Abstract: Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however, they depend on heuristic weighting and fail to transparently convey uncertainty in a systematic way. Recent neural implicit methods, on the other hand, get very high fidelity but usually need a lot of GPU power for optimization and aren't very easy to understand for making decisions later on. This work presents BayesFusion-SDF, a CPU-centric probabilistic signed distance fusion framework that conceptualizes geometry as a sparse Gaussian random field with a defined posterior distribution over voxel distances. First, a rough TSDF reconstruction is used to create an adaptive narrow-band domain. Then, depth observations are combined using a heteroscedastic Bayesian formulation that is solved using sparse linear algebra and preconditioned conjugate gradients. Randomized diagonal estimators are a quick way to get an idea of posterior uncertainty. This makes it possible to extract surfaces and plan the next best view while taking into account uncertainty. Tests on a controlled ablation scene and a CO3D object sequence show that the new method is more accurate geometrically than TSDF baselines and gives useful estimates of uncertainty for active sensing. The proposed formulation provides a clear and easy-to-use alternative to GPU-heavy neural reconstruction methods while still being able to be understood in a probabilistic way and acting in a predictable way. GitHub: https://mazumdarsoumya.github.io/BayesFusionSDF

</details>


### [159] [HDR Reconstruction Boosting with Training-Free and Exposure-Consistent Diffusion](https://arxiv.org/abs/2602.19706)
*Yo-Tin Lin,Su-Kai Chen,Hou-Ning Hu,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 论文提出一个无需训练的方法，用扩散式修复提升现有单张LDR到HDR重建在过曝区域的效果，通过文本引导扩散与SDEdit细化，在多曝光LDR间保持一致并迭代补偿亮度，实现更自然细节与更好客观指标。


<details>
  <summary>Details</summary>
Motivation: 单张LDR转HDR时，过曝区域信息完全丢失，传统直接或间接方法难以恢复细节；现有扩散/重建方法多需大量训练且与HDR流程耦合度高，缺乏一种可直接增强现有管线、在多曝光一致性与亮度连贯性上表现稳定的方案。

Method: 在不额外训练的前提下，将文本引导扩散模型用于过曝区的内容生成，并用SDEdit进行结构与风格细化；与现有间接/直接HDR方法迭代耦合，通过“迭代补偿机制”在多曝光图像间校正与传播亮度，使修复内容在不同曝光下保持一致和物理合理；方法可插拔，作为后处理或中间步骤增强原有HDR重建。

Result: 在标准HDR数据集和真实场景中，主客观指标显著提升；在极端过曝情况下能生成自然、连贯的细节，同时保留原有HDR管线优势（例如结构保真、色调映射稳定性）。

Conclusion: 扩散式修复结合SDEdit与迭代亮度补偿，为单张LDR到HDR过曝细节恢复提供了训练无关、可即插即用的增强路径，既提升感知质量也改进量化指标，并与现有HDR方法兼容。

Abstract: Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting. Our method combines text-guided diffusion models with SDEdit refinement to generate plausible content in over-exposed areas while maintaining consistency across multi-exposure LDR images. Unlike previous approaches requiring extensive training, our method seamlessly integrates with existing HDR reconstruction techniques through an iterative compensation mechanism that ensures luminance coherence across multiple exposures. We demonstrate significant improvements in both perceptual quality and quantitative metrics on standard HDR datasets and in-the-wild captures. Results show that our method effectively recovers natural details in challenging scenarios while preserving the advantages of existing HDR reconstruction pipelines. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting

</details>


### [160] [ChimeraLoRA: Multi-Head LoRA-Guided Synthetic Datasets](https://arxiv.org/abs/2602.19708)
*Hoyoung Kim,Minwoo Jang,Jabin Koo,Sangdoo Yun,Jungseul Ok*

Main category: cs.CV

TL;DR: 提出将扩散模型的LoRA分解为共享（类先验）与个体（图像细节）两部分，并在训练时用框约束进行“语义增强”，生成时以Dirichlet混合各个个体LoRA，从而在小样本下同时提升多样性与细节，改进下游分类表现。


<details>
  <summary>Details</summary>
Motivation: 小样本/尾类数据稀缺导致模型偏置与不可靠。现有做法用扩散模型+LoRA在少量真实样本上微调以合成数据：单图LoRA细节足但多样性差；类级LoRA多样但忽略细节。需要一种既保留类先验的多样性，又能刻画细粒度细节的方法。

Method: 将适配器拆分为：1) 类共享LoRA A，编码类先验；2) 每张图像对应的LoRA集合 B，捕获图像特有细节。为使A学习到一致的类语义，在训练时保留类别的边界框（box）实现“语义增强/对齐”。在生成阶段，将A与若干个体LoRA按Dirichlet分布采样的系数进行线性组合，实现细节与多样性的权衡与融合。

Result: 在多种数据集上，合成图像兼具更高的多样性与细节保真，且分布更贴近少样本真实数据；用于数据增广后，可显著提升下游分类准确率，相比只用单图或类级LoRA的基线更优。

Conclusion: 分解式LoRA+语义增强+Dirichlet混合是一种有效的小样本合成策略，可在隐私受限/细粒度等稀缺场景中生成既多样又细致的图像，并带来稳健的分类性能提升。

Abstract: Beyond general recognition tasks, specialized domains including privacy-constrained medical applications and fine-grained settings often encounter data scarcity, especially for tail classes. To obtain less biased and more reliable models under such scarcity, practitioners leverage diffusion models to supplement underrepresented regions of real data. Specifically, recent studies fine-tune pretrained diffusion models with LoRA on few-shot real sets to synthesize additional images. While an image-wise LoRA trained on a single image captures fine-grained details yet offers limited diversity, a class-wise LoRA trained over all shots produces diverse images as it encodes class priors yet tends to overlook fine details. To combine both benefits, we separate the adapter into a class-shared LoRA~$A$ for class priors and per-image LoRAs~$\mathcal{B}$ for image-specific characteristics. To expose coherent class semantics in the shared LoRA~$A$, we propose a semantic boosting by preserving class bounding boxes during training. For generation, we compose $A$ with a mixture of $\mathcal{B}$ using coefficients drawn from a Dirichlet distribution. Across diverse datasets, our synthesized images are both diverse and detail-rich while closely aligning with the few-shot real distribution, yielding robust gains in downstream classification accuracy.

</details>


### [161] [Universal Pose Pretraining for Generalizable Vision-Language-Action Policies](https://arxiv.org/abs/2602.19710)
*Haitao Lin,Hanyang Yu,Jingshun Huang,He Zhang,Yonggen Ling,Ping Tan,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: Pose-VLA 通过将空间感知与动作对齐解耦，先学统一相机坐标下的3D姿态先验，再高效对齐到具体机器人的动作空间，用离散姿态token把多源3D数据与机器人示教轨迹贯通，最终在RoboTwin 2.0与LIBERO上达SOTA并以少量示教实现强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VLA把高层视觉语义与稀疏、具身相关的动作监督纠缠在一起，导致特征塌缩、训练低效；且多依赖为VQA优化的VLM骨干，善于语义识别却难以捕捉决定动作差异的细微3D状态变化，造成感知-动作错配。

Method: 提出Pose-VLA的解耦范式：两阶段训练。预训练阶段在统一的相机中心坐标系中学习通用3D空间先验，引入离散姿态token作为通用表示，先以姿态进行基础空间对齐，再以轨迹监督做运动对齐；后训练阶段在具体机器人动作空间内进行高效具身对齐，将从多样3D数据学到的空间落地到机器人示教的几何级轨迹。

Result: 在RoboTwin 2.0上平均成功率79.5%达SOTA，在LIBERO上96.0%具竞争力；真实机器人实验在每任务仅100条示教下，对多样物体展现稳健泛化与高成功率。

Conclusion: 解耦空间理解与动作映射、以离散姿态token统一多源3D与示教轨迹，可缓解特征塌缩并提升训练效率与跨具身泛化，验证了先学空间、后对齐动作的预训练范式有效。

Abstract: Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.
  To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.
  Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.

</details>


### [162] [Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision](https://arxiv.org/abs/2602.19715)
*Kartik Kuckreja,Parul Gupta,Muhammad Haris Khan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 提出DeepfakeJudge：一个用于深偽检测“推理忠实度”监督与评估的框架，含新型OOD伪造基准、人标可视推理子集和无需显式真值理据的评估模型；通过生成器-评估器自举扩展人类反馈，支持点对/成对评估，在元评测上显著超越更大基线，并与人类评价高度一致，证明可扩展、可解释的深偽推理评测可行。


<details>
  <summary>Details</summary>
Motivation: 现有深偽检测多产出文本解释，但常与图像证据脱节；主流评测只看分类准确率，忽视“推理是否与视觉证据一致”的忠实度，导致信任与可解释性不足。需要一个可扩展框架衡量并提升推理忠实度。

Method: 构建DeepfakeJudge框架：1) OOD基准，涵盖最新生成与编辑伪造；2) 带人类标注的可视推理子集；3) 一组专门评估解释合理性的模型，在无显式真值理据下评估。通过生成器-评估器自举，将有限人类反馈扩展为结构化推理监督；支持点对与成对评估；在元评测集上优化Judge。

Result: 在提出的元评测基准上，推理自举的Judge达96.2%准确率，优于体量大30倍的基线；在人标元评测子集上与人类评分高度相关，成对一致率达98.9%；用户研究中，受试者在忠实、落地与有用性上有70%偏好本框架生成的解释。

Conclusion: 推理忠实度是深偽检测可量化维度；DeepfakeJudge提供可扩展的人机协同监督与高一致性评估，能够生成更忠实、可解释的推理；开源数据集、模型与代码有望促进社区在可解释深偽检测上的发展。

Abstract: Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framework for scalable reasoning supervision and evaluation, that integrates an out-of-distribution benchmark containing recent generative and editing forgeries, a human-annotated subset with visual reasoning labels, and a suite of evaluation models, that specialize in evaluating reasoning rationales without the need for explicit ground truth reasoning rationales. The Judge is optimized through a bootstrapped generator-evaluator process that scales human feedback into structured reasoning supervision and supports both pointwise and pairwise evaluation. On the proposed meta-evaluation benchmark, our reasoning-bootstrapped model achieves an accuracy of 96.2\%, outperforming \texttt{30x} larger baselines. The reasoning judge attains very high correlation with human ratings and 98.9\% percent pairwise agreement on the human-annotated meta-evaluation subset. These results establish reasoning fidelity as a quantifiable dimension of deepfake detection and demonstrate scalable supervision for interpretable deepfake reasoning. Our user study shows that participants preferred the reasonings generated by our framework 70\% of the time, in terms of faithfulness, groundedness, and usefulness, compared to those produced by other models and datasets. All of our datasets, models, and codebase are \href{https://github.com/KjAeRsTuIsK/DeepfakeJudge}{open-sourced}.

</details>


### [163] [Generative 6D Pose Estimation via Conditional Flow Matching](https://arxiv.org/abs/2602.19719)
*Amir Hamza,Davide Boscaini,Weihang Li,Benjamin Busam,Fabio Poiesi*

Main category: cs.CV

TL;DR: 提出Flose：把6D位姿估计表述为R^3中的条件流匹配与去噪生成过程，结合外观语义与几何，引入RANSAC注册，BOP五数据集平均AR提升+4.5。


<details>
  <summary>Details</summary>
Motivation: 直接回归SE(3)受对称性困扰，基于局部特征匹配在缺乏显著特征时失效；现有条件流方法仅用几何指导，无法消解对称带来的多解。

Method: 将6D位姿估计重构为在R^3上的条件流匹配（CFM）/去噪生成：以局部几何与外观语义特征联合作为条件，逐步去噪生成物体关键点/姿态；为鲁棒性加入RANSAC式注册以剔除离群点并求解最终姿态。

Result: 在BOP基准的5个数据集上验证，取得平均召回（AR）+4.5的提升，相较于既有方法更优。

Conclusion: 融合语义外观与几何的CFM去噪生成框架可缓解对称性与特征不足难题，配合RANSAC提升鲁棒性，在标准基准上取得显著增益。

Abstract: Existing methods for instance-level 6D pose estimation typically rely on neural networks that either directly regress the pose in $\mathrm{SE}(3)$ or estimate it indirectly via local feature matching. The former struggle with object symmetries, while the latter fail in the absence of distinctive local features. To overcome these limitations, we propose a novel formulation of 6D pose estimation as a conditional flow matching problem in $\mathbb{R}^3$. We introduce Flose, a generative method that infers object poses via a denoising process conditioned on local features. While prior approaches based on conditional flow matching perform denoising solely based on geometric guidance, Flose integrates appearance-based semantic features to mitigate ambiguities caused by object symmetries. We further incorporate RANSAC-based registration to handle outliers. We validate Flose on five datasets from the established BOP benchmark. Flose outperforms prior methods with an average improvement of +4.5 Average Recall. Project Website : https://tev-fbk.github.io/Flose/

</details>


### [164] [Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets](https://arxiv.org/abs/2602.19723)
*Yue Zhang,Zhizheng Zhuo,Siyao Xu,Shan Lv,Zhaoxi Liu,Jun Qiu,Qiuli Wang,Yaou Liu,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出PMM-Synth，一个能在多数据集上泛化的个性化MRI模态合成框架，解决不同数据集间分布差异、模态缺失与训练批次不一致问题，并在四个临床数据集上一致优于SOTA（PSNR/SSIM、结构与病灶保真度、下游分割/报告）。


<details>
  <summary>Details</summary>
Motivation: 统一的MRI模态合成模型虽可处理多输入输出，但通常只在单一数据集训练/评估，跨数据集泛化差、难以临床部署。现实中模态缺失普遍、采集受限（时间、运动、耐受性），迫切需要能在异构数据集上稳健泛化、支持多任务的合成方法。

Method: 提出PMM-Synth并在多多模态MRI数据集上联合训练（模态覆盖、疾病类型、强度分布各异）。三项关键设计：1）个性化特征调制（PFM）：依据数据集ID动态调制特征，缓解分布偏移；2）模态一致性批次调度（MCBS）：在样本模态不一致下实现稳定高效的批训练；3）选择性监督损失：当部分真值模态缺失时仅对可用模态监督，避免噪声梯度。模型支持一对一与多对一合成。

Result: 在四个临床多模态MRI数据集上，PMM-Synth在PSNR与SSIM上均超过现有SOTA；质性结果显示更好的解剖与病灶细节保持；在下游肿瘤分割与放射学报告任务中带来更可靠的诊断支持，尤其在真实世界模态缺失场景。

Conclusion: 通过个性化特征调制、批次调度与选择性监督，PMM-Synth在跨数据集的MRI模态合成中实现稳健泛化与更高质量重建，具备临床部署潜力并能提升下游任务表现。

Abstract: Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.

</details>


### [165] [VGGT-MPR: VGGT-Enhanced Multimodal Place Recognition in Autonomous Driving Environments](https://arxiv.org/abs/2602.19735)
*Jingyi Xu,Zhangshuo Qi,Zhongmiao Yan,Xuyu Gao,Qianyun Jiao,Songpengcheng Xia,Xieyuanli Chen,Ling Pei*

Main category: cs.CV

TL;DR: 提出VGGT-MPR：用VGGT作为统一几何引擎的多模态车载地点识别框架，实现快速全局检索并配合免训练重排序，在多数据集上达SOTA且对环境变化与视角变化鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有多模态（相机+激光雷达）地点识别多依赖手工设计的融合策略与大参数量主干网络，需要代价高昂的再训练，且跨环境/视角泛化不足。需要一种可泛化、训练负担低、能同时提升全局检索与精排鲁棒性的统一方法。

Method: 以VGGT作为统一几何引擎：1）全局检索阶段：利用VGGT在深度与点云几何监督下提取几何丰富的视觉特征；用预测深度将稀疏LiDAR点云致密化，增强结构表达；将多模态特征融合生成全局描述子用于快速检索。2）重排序阶段：设计免训练的重排序机制，利用VGGT的跨视角关键点跟踪能力；结合掩码引导的关键点提取与置信度感知的匹配打分，对初始候选进行精排，无需额外参数优化。

Result: 在大规模自动驾驶基准和自采数据上取得SOTA，表现出对剧烈环境变化、视角变化与遮挡的强鲁棒性；全局检索与重排序两阶段均带来性能提升。

Conclusion: VGGT-MPR以统一几何引擎实现高效、鲁棒的多模态地点识别，减少训练与调参负担，通过免训练重排序进一步提高检索精度，适合实际自动驾驶场景；代码与数据将开源。

Abstract: In autonomous driving, robust place recognition is critical for global localization and loop closure detection. While inter-modality fusion of camera and LiDAR data in multimodal place recognition (MPR) has shown promise in overcoming the limitations of unimodal counterparts, existing MPR methods basically attend to hand-crafted fusion strategies and heavily parameterized backbones that require costly retraining. To address this, we propose VGGT-MPR, a multimodal place recognition framework that adopts the Visual Geometry Grounded Transformer (VGGT) as a unified geometric engine for both global retrieval and re-ranking. In the global retrieval stage, VGGT extracts geometrically-rich visual embeddings through prior depth-aware and point map supervision, and densifies sparse LiDAR point clouds with predicted depth maps to improve structural representation. This enhances the discriminative ability of fused multimodal features and produces global descriptors for fast retrieval. Beyond global retrieval, we design a training-free re-ranking mechanism that exploits VGGT's cross-view keypoint-tracking capability. By combining mask-guided keypoint extraction with confidence-aware correspondence scoring, our proposed re-ranking mechanism effectively refines retrieval results without additional parameter optimization. Extensive experiments on large-scale autonomous driving benchmarks and our self-collected data demonstrate that VGGT-MPR achieves state-of-the-art performance, exhibiting strong robustness to severe environmental changes, viewpoint shifts, and occlusions. Our code and data will be made publicly available.

</details>


### [166] [InfScene-SR: Spatially Continuous Inference for Arbitrary-Size Image Super-Resolution](https://arxiv.org/abs/2602.19736)
*Shoukun Sun,Zhe Wang,Xiang Que,Jiyin Zhang,Xiaogang Ma*

Main category: cs.CV

TL;DR: 提出InfScene-SR：在不重新训练的前提下，将扩散模型用于任意大场景的连续超分辨，避免块状拼接缝与纹理不一致。


<details>
  <summary>Details</summary>
Motivation: 标准扩散SR（如SR3）因显存限制只能在固定尺寸块上训练/推理，直接分块推理会产生边界缝与跨块纹理不一致，难以处理任意大图像与大场景。

Method: 在扩散迭代细化过程中引入“引导+方差校正”的融合机制：对重叠块的中间状态进行引导式合成，并校正噪声方差以一致化统计量，实现空间连续、可扩展的推理；无需重新训练即可对大幅面图像进行无缝SR。

Result: 在遥感数据上验证，InfScene-SR可重建细节、提升感知质量，并有效消除块边界伪影；进一步提升下游语义分割等任务表现。

Conclusion: InfScene-SR实现了扩散式SR在大场景上的无缝扩展，兼顾高感知质量与跨块一致性，对实际遥感等大图应用具有实用价值。

Abstract: Image Super-Resolution (SR) aims to recover high-resolution (HR) details from low-resolution (LR) inputs, a task where Denoising Diffusion Probabilistic Models (DDPMs) have recently shown superior performance compared to Generative Adversarial Networks (GANs) based approaches. However, standard diffusion-based SR models, such as SR3, are typically trained on fixed-size patches and struggle to scale to arbitrary-sized images due to memory constraints. Applying these models via independent patch processing leads to visible seams and inconsistent textures across boundaries. In this paper, we propose InfScene-SR, a framework enabling spatially continuous super-resolution for large, arbitrary scenes. We adapt the iterative refinement process of diffusion models with a novel guided and variance-corrected fusion mechanism, allowing for the seamless generation of large-scale high-resolution imagery without retraining. We validate our approach on remote sensing datasets, demonstrating that InfScene-SR not only reconstructs fine details with high perceptual quality but also eliminates boundary artifacts, benefiting downstream tasks such as semantic segmentation.

</details>


### [167] [RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing](https://arxiv.org/abs/2602.19753)
*Kaifa Yang,Qi Yang,Yiling Xu,Zhu Li*

Main category: cs.CV

TL;DR: RAP 提出一种无需渲染的快速前馈方法，用高斯自身属性与邻域统计，通过小型 MLP 预测每个 3DGS 原语的重要性，解决现有基于视图渲染方法慢、依赖可见性与光栅化器、可扩展性差的问题，并在剪枝、压缩与传输中通用泛化良好。


<details>
  <summary>Details</summary>
Motivation: 3DGS 在重建中会产生大量原语，但其对重建质量的贡献差异很大；需要准确评估重要性以去冗余、便于压缩与高效传输。现有方法依赖多视角渲染贡献分析，敏感于视角采样、需要专用可微光栅化器，计算随视角线性增长，难以作为可插拔模块且可扩展性差。

Method: 提出 RAP：基于原语的内在高斯属性（如位置、尺度、各向异性、颜色/不透明度等）与局部邻域统计，构建紧凑 MLP，前馈预测每个原语的重要性分数，无需渲染或可见性计算。训练时联合三种损失：渲染损失（对齐最终重建质量）、剪枝感知损失（鼓励可移除性识别）与重要性分布正则（稳定分布、避免塌陷）。在少量场景上训练后，可零/少样本泛化到未见数据。

Result: 在未见场景上 RAP 能快速准确预测原语重要性，支持在重建、压缩、传输管线中实现高效剪枝与资源分配；相比渲染式方法显著加速、减少对视图与光栅化器依赖，并具备良好可移植性。

Conclusion: 利用属性驱动的渲染无关重要性预测，可作为 3DGS 的通用可插拔组件，兼顾速度、精度与泛化，实用地支撑重建去冗余、压缩与传输任务。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.

</details>


### [168] [Multimodal Dataset Distillation Made Simple by Prototype-Guided Data Synthesis](https://arxiv.org/abs/2602.19756)
*Junhyeok Choi,Sangwoo Mo,Minwoo Chae*

Main category: cs.CV

TL;DR: 提出一种无需训练的多模态数据蒸馏框架：用CLIP提取图文对齐嵌入、聚类得到原型，并用unCLIP解码器合成图像；在极小数据预算下优于优化式蒸馏与子集筛选，跨架构泛化达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多模态任务依赖海量图文数据，训练成本高；现有筛选/剪枝在极小子集下崩溃；优化式蒸馏需全数据参与、像素与文本联合优化，依赖特定架构、泛化差。需要一种无需大规模训练、可跨架构泛化、在小预算下仍有效的蒸馏方法。

Method: 学习无关（learning-free）的蒸馏流程：1）用预训练CLIP抽取图像-文本对齐嵌入；2）在嵌入空间获取原型（如聚类/原型选择），形成紧凑代表集；3）通过unCLIP解码器从原型嵌入合成图像，与对应文本一起构建小而强的蒸馏数据集；无需端到端优化与大规模训练，天然与下游架构解耦。

Result: 在多项视觉-语言任务与极小数据配额下，所提方法稳定优于基于优化的多模态蒸馏与子集选择（过滤/剪枝）；在跨架构迁移训练中取得最新最优表现。

Conclusion: 利用CLIP对齐嵌入与unCLIP合成，可在不进行大规模训练的前提下实现高效、可扩展的多模态数据蒸馏，并显著提升跨架构泛化与小样本性能。

Abstract: Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient. Prior efforts in dataset filtering and pruning attempt to mitigate this issue, but still require relatively large subsets to maintain performance and fail under very small subsets. Dataset distillation offers a promising alternative, yet existing multimodal dataset distillation methods require full-dataset training and joint optimization of image pixels and text features, making them architecture-dependent and limiting cross-architecture generalization. To overcome this, we propose a learning-free dataset distillation framework that eliminates the need for large-scale training and optimization while enhancing generalization across architectures. Our method uses CLIP to extract aligned image-text embeddings, obtains prototypes, and employs an unCLIP decoder to synthesize images, enabling efficient and scalable multimodal dataset distillation. Extensive experiments demonstrate that our approach consistently outperforms optimization-based dataset distillation and subset selection methods, achieving state-of-the-art cross-architecture generalization.

</details>


### [169] [Training Deep Stereo Matching Networks on Tree Branch Imagery: A Benchmark Study for Real-Time UAV Forestry Applications](https://arxiv.org/abs/2602.19763)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 该论文在树枝场景中系统评测10种深度立体匹配网络，基于DEFOM-Stereo生成的视差作为监督，在真实树枝数据集上比较感知与结构指标，并在Jetson Orin Super上做实时性测试，给出质量-速度权衡与分辨率选型建议。


<details>
  <summary>Details</summary>
Motivation: 无人机自主修枝需要在中近距离下高精度、低延迟的深度估计；树木/植被纹理稀疏、重复图样多、遮挡严重，传统或通用深度网络在此域表现不稳。此前工作表明DEFOM-Stereo对植被视差较可靠，但缺乏在真实树枝图像上对主流深度网络的系统训练与评测。

Method: 构建并使用Canterbury Tree Branches数据集（ZED Mini，1080P与720P，共5,313对）；以DEFOM-Stereo生成的视差作为训练目标，对10个立体匹配网络（覆盖逐步细化、3D卷积、边缘感知注意力、轻量化等范式）进行训练与测试。评估采用感知指标（SSIM、LPIPS、ViTScore）与结构性指标（SIFT/ORB匹配）；在NVIDIA Jetson Orin Super（16GB，机载独立供电）上测量推理速度，并比较720P与1080P时延。

Result: BANet-3D在总体感知质量最佳（SSIM=0.883, LPIPS=0.157）；RAFT-Stereo在场景级理解（ViTScore=0.799）最优。AnyNet在1080P下达到6.99 FPS，是唯一接近实时的方案；BANet-2D以1.21 FPS在质量与速度间达到较佳平衡。给出不同分辨率下的处理时间对比。

Conclusion: 在真实树枝场景中，不同立体网络呈现明显的质量-速度权衡：AnyNet适合近实时应用但质量一般，BANet-3D质量领先，BANet-2D提供较好的综合权衡，RAFT-Stereo更擅长全局语义。针对林业无人机，分辨率选择需结合机载算力与任务精度，1080P虽更准但延迟高，720P可提升帧率。

Abstract: Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances. Building on our earlier work that identified DEFOM-Stereo as the best reference disparity generator for vegetation scenes, we present the first study to train and test ten deep stereo matching networks on real tree branch images. We use the Canterbury Tree Branches dataset -- 5,313 stereo pairs from a ZED Mini camera at 1080P and 720P -- with DEFOM-generated disparity maps as training targets. The ten methods cover step-by-step refinement, 3D convolution, edge-aware attention, and lightweight designs. Using perceptual metrics (SSIM, LPIPS, ViTScore) and structural metrics (SIFT/ORB feature matching), we find that BANet-3D produces the best overall quality (SSIM = 0.883, LPIPS = 0.157), while RAFT-Stereo scores highest on scene-level understanding (ViTScore = 0.799). Testing on an NVIDIA Jetson Orin Super (16 GB, independently powered) mounted on our drone shows that AnyNet reaches 6.99 FPS at 1080P -- the only near-real-time option -- while BANet-2D gives the best quality-speed balance at 1.21 FPS. We also compare 720P and 1080P processing times to guide resolution choices for forestry drone systems.

</details>


### [170] [One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image](https://arxiv.org/abs/2602.19766)
*Pengfei Wang,Liyi Chen,Zhiyuan Ma,Yanjun Guo,Guowen Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: One2Scene将单张图生成可探索3D场景的问题拆解为三步：单图生成全景锚视图→基于可泛化高斯点云的多视几何脚手架重建→在脚手架条件下的高保真新视图合成，实现大范围相机运动下稳定、几何一致的沉浸式探索。


<details>
  <summary>Details</summary>
Motivation: 单张图生成可自由探索的3D场景极其欠定，现有方法在离开原视角后易产生几何畸变与噪声，难以保持跨视角一致性与真实感，需要一种既能引入强几何先验又能高效重建的框架。

Method: 三阶段：1) 从输入单图用全景生成器得到全景锚视图；2) 将全景投影为多稀疏锚视角，用可泛化前馈式Gaussian Splatting网络进行多视立体匹配与重建，配合双向特征融合以强化跨视角一致性，得到显式3D几何脚手架；3) 以该脚手架为强先验，驱动新视图生成器在任意相机位姿下合成写实且几何准确的图像。

Result: 在全景深度估计、前馈式360°重建和可探索3D场景生成三项任务上显著优于SOTA；在大幅相机运动下保持稳定、低畸变与高一致性。

Conclusion: 通过“单图→全景锚→几何脚手架→条件新视图合成”的解耦式流程，One2Scene有效引入多视几何先验与3D一致性，克服单图到可探索3D的畸变与噪声问题，实现稳健、高保真、可沉浸探索的场景生成。

Abstract: Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360° reconstruction, and explorable 3D scene generation. Code and models will be released.

</details>


### [171] [TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding](https://arxiv.org/abs/2602.19768)
*Fan Yang,Shurong Zheng,Hongyin Zhao,Yufei Zhan,Xin Li,Yousong Zhu,Chaoyang Zhao Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出TraceVision：将轨迹感知的空间理解融入LVLM，以实现“看哪里、说什么、指哪”的统一。通过TVP模块双向融合视觉与轨迹、几何化简提取语义关键点、三阶段训练，并扩展到分割与视频理解，配合新RILN数据集，实验在多项任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM多停留在全局理解，难以像人类那样以注意力轨迹关联文本与局部区域，缺乏可解释的区域-描述对应与交互式空间推理能力。

Method: 1) 统一端到端框架TraceVision；2) 轨迹感知视觉感知（TVP）模块，实现视觉特征与轨迹信息的双向融合；3) 几何简化从原始轨迹中提取语义关键点；4) 三阶段训练：用轨迹引导描述生成与区域定位；5) 扩展到轨迹引导分割与视频理解（跨帧跟踪与时序注意分析）；6) 构建RILN数据集以增强逻辑推理与可解释性。

Result: 在轨迹引导描述、文本引导轨迹预测、图像理解与分割等任务上取得SOTA；支持跨帧跟踪与时序注意分析，显示出更强的空间交互与可解释性。

Conclusion: 引入轨迹感知的空间对齐与推理，使LVLM具备更直观的人机交互与可解释视觉理解能力；TraceVision与RILN为可解释、可交互视觉语言研究奠定基础。

Abstract: Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.

</details>


### [172] [Efficient endometrial carcinoma screening via cross-modal synthesis and gradient distillation](https://arxiv.org/abs/2602.19822)
*Dongjing Shan,Yamei Luo,Jiqing Xuan,Lu Huang,Jin Li,Mengchu Yang,Zeyu Chen,Fajin Lv,Yong Tang,Chunxiang Zhang*

Main category: cs.CV

TL;DR: 提出一个两阶段、低算力可用的深度学习框架，用跨模态合成缓解病理稀缺，并用梯度蒸馏的轻量筛查网络提高检测效率与准确性；在7951例多中心数据上，以极低算力达成99.5%敏感度、97.2%特异度、AUC 0.987，优于专家超声医师。


<details>
  <summary>Details</summary>
Motivation: 基层初筛主要依赖经阴道超声，但对肌层浸润判别受限于组织对比度低、操作者依赖强和阳性样本稀缺；现有AI在类别极不平衡、征象细微且算力受限时效果不佳，需要一种既能扩充有效数据、又能在低计算预算下高效筛查的方法。

Method: 两阶段框架：1) 结构引导的跨模态生成网络，用未配准的MRI生成多样且高保真、保解剖结点（如解剖交界区）的超声图，缓解正例稀缺并增强对关键结构的对齐；2) 轻量化筛查网络结合梯度蒸馏，从高容量教师模型转移判别知识，动态引导稀疏注意力聚焦任务关键区域，在极低GFLOPs下保持高性能。

Result: 在多中心7951名受试者数据上评估，取得敏感度99.5%、特异度97.2%、AUC 0.987，计算开销仅0.289 GFLOPs，显著优于专家超声医师的平均诊断准确度。

Conclusion: 跨模态合成增强与知识驱动的高效建模可在资源受限的初级医疗中实现专家级、实时的子宫内膜癌肌层浸润筛查，兼顾数据稀缺与算力限制的双重瓶颈。

Abstract: Early detection of myometrial invasion is critical for the staging and life-saving management of endometrial carcinoma (EC), a prevalent global malignancy. Transvaginal ultrasound serves as the primary, accessible screening modality in resource-constrained primary care settings; however, its diagnostic reliability is severely hindered by low tissue contrast, high operator dependence, and a pronounced scarcity of positive pathological samples. Existing artificial intelligence solutions struggle to overcome this severe class imbalance and the subtle imaging features of invasion, particularly under the strict computational limits of primary care clinics. Here we present an automated, highly efficient two-stage deep learning framework that resolves both data and computational bottlenecks in EC screening. To mitigate pathological data scarcity, we develop a structure-guided cross-modal generation network that synthesizes diverse, high-fidelity ultrasound images from unpaired magnetic resonance imaging (MRI) data, strictly preserving clinically essential anatomical junctions. Furthermore, we introduce a lightweight screening network utilizing gradient distillation, which transfers discriminative knowledge from a high-capacity teacher model to dynamically guide sparse attention towards task-critical regions. Evaluated on a large, multicenter cohort of 7,951 participants, our model achieves a sensitivity of 99.5\%, a specificity of 97.2\%, and an area under the curve of 0.987 at a minimal computational cost (0.289 GFLOPs), substantially outperforming the average diagnostic accuracy of expert sonographers. Our approach demonstrates that combining cross-modal synthetic augmentation with knowledge-driven efficient modeling can democratize expert-level, real-time cancer screening for resource-constrained primary care settings.

</details>


### [173] [Open-vocabulary 3D scene perception in industrial environments](https://arxiv.org/abs/2602.19823)
*Keno Moenck,Adrian Philip Florea,Julian Koch,Thorsten Schüppstuhl*

Main category: cs.CV

TL;DR: 提出一个免训练的开放词汇3D感知流程，以语义特征合并超点生成实例掩码，并结合工业领域自适应的VLFM（IndustrialCLIP）在工业车间场景实现开放词汇查询与分割。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇方法多依赖在非工业数据上预训练的与类别无关分割模型，难以泛化到工业环境，导致对常见工业物体的感知表现不佳。

Method: 不再使用预训练的实例提议模型；先对点云/场景得到预计算超点及其语义特征，然后根据语义相似性合并超点以生成掩码；再利用域自适应的视觉-语言基础模型IndustrialCLIP执行开放词汇查询与匹配，实现3D目标的分割与标注。

Result: 在具有代表性的3D工业车间场景上进行定性评估，展示了对工业物体成功的开放词汇分割效果。

Conclusion: 免训练的超点合并策略结合工业域VLFMs可在工业场景中实现有效的开放词汇3D感知，克服通用家居数据上预训练模型的泛化不足。

Abstract: Autonomous vision applications in production, intralogistics, or manufacturing environments require perception capabilities beyond a small, fixed set of classes. Recent open-vocabulary methods, leveraging 2D Vision-Language Foundation Models (VLFMs), target this task but often rely on class-agnostic segmentation models pre-trained on non-industrial datasets (e.g., household scenes). In this work, we first demonstrate that such models fail to generalize, performing poorly on common industrial objects. Therefore, we propose a training-free, open-vocabulary 3D perception pipeline that overcomes this limitation. Instead of using a pre-trained model to generate instance proposals, our method simply generates masks by merging pre-computed superpoints based on their semantic features. Following, we evaluate the domain-adapted VLFM "IndustrialCLIP" on a representative 3D industrial workshop scene for open-vocabulary querying. Our qualitative results demonstrate successful segmentation of industrial objects.

</details>


### [174] [TextShield-R1: Reinforced Reasoning for Tampered Text Detection](https://arxiv.org/abs/2602.19828)
*Chenfan Qu,Yiwu Zhong,Jian Liu,Xuekang Zhu,Bohan Yu,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出TextShield-R1：首个基于强化学习的多模态大语言模型，用于篡改文本检测与可解释推理，并配套TFR基准；通过持续取证预训练、GRPO细化奖励、以及OCR纠正提升定位，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在文本篡改检测上存在三大痛点：难以捕捉微观伪造痕迹；篡改文本区域定位不准；依赖昂贵且稀缺的解释型标注。缺少跨语言、跨手法、跨风格的系统评测数据集。

Method: 1) Forensic Continual Pre-training：以“易到难”课程学习将自然图像取证与OCR的大规模低成本数据注入MLLM，提升微痕迹与文本理解能力。2) Fine-tuning阶段采用Group Relative Policy Optimization（GRPO），设计新型奖励以弱化标注依赖并增强推理。3) 推理时引入OCR Rectification：利用模型自身强文本识别能力对定位结果进行校正。4) 构建TFR基准：含4.5万+真/伪图，覆盖16种语言、10类篡改技术与多领域，并提供推理式标注。

Result: 在TFR及其他设置下进行大量实验，TextShield-R1在可解释的文本篡改检测与定位上取得显著领先；跨风格、跨方法、跨语言评测中表现稳健；注释开销降低且推理质量提升。

Conclusion: 强化学习驱动的多模态训练与推理校正相结合，可有效缓解微观伪造难检、定位不准与标注依赖高的问题；TFR为全面评测提供了新标准，TextShield-R1推进了可解释文本篡改检测的最新进展。

Abstract: The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection.

</details>


### [175] [M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting](https://arxiv.org/abs/2602.19832)
*Penghui Niu,Taotao Cai,Suqi Zhang,Junhua Gu,Ping Zhang,Qiqi Liu,Jianxin Li*

Main category: cs.CV

TL;DR: 提出M3S-Net多模态多尺度融合网络，利用部分卷积提取薄云边界、FFT将气象时序转频域、跨模态Mamba以动态C矩阵交换实现视觉-时间耦合，在10分钟超短期光伏预测上较SOTA降MAE 6.2%。


<details>
  <summary>Details</summary>
Motivation: 光伏出力受云影导致的强间歇与高频波动影响，给高渗透率电网稳定性带来挑战。现有多模态方法多做浅层拼接与二值云分割，不能刻画薄云等精细光学特征，也难以建模视觉与气象间复杂时空耦合。需要一种能细粒度感知、多尺度表征、深层跨模态交互且计算高效的方法。

Method: 1) 多尺度部分通道选择网络：以Partial Convolution显式隔离薄云边界等精细特征，突破二值掩膜精度上限；2) 多尺度“序列到图像”分析网络：将气象时序经FFT得到时频图，分解不同时间尺度的周期模式；3) 跨模态Mamba交互模块：引入动态C矩阵交换，在视觉与时间流间交换状态空间参数，使一方状态演化受另一方上下文调制，以线性复杂度实现深层结构耦合；整体形成M3S-Net进行10分钟超短期PV功率预测。

Result: 在新构建的细粒度PV功率数据集上，M3S-Net相对现有最优基线在10分钟预测MAE降低6.2%。

Conclusion: 细粒度多尺度特征提取与跨模态状态空间深度耦合可显著提升超短期光伏预测精度；M3S-Net在效率与效果上优于浅层拼接范式，数据与代码将开源（链接提供）。

Abstract: The inherent intermittency and high-frequency variability of solar irradiance, particularly during rapid cloud advection, present significant stability challenges to high-penetration photovoltaic grids. Although multimodal forecasting has emerged as a viable mitigation strategy, existing architectures predominantly rely on shallow feature concatenation and binary cloud segmentation, thereby failing to capture the fine-grained optical features of clouds and the complex spatiotemporal coupling between visual and meteorological modalities. To bridge this gap, this paper proposes M3S-Net, a novel multimodal feature fusion network based on multi-scale data for ultra-short-term PV power forecasting. First, a multi-scale partial channel selection network leverages partial convolutions to explicitly isolate the boundary features of optically thin clouds, effectively transcending the precision limitations of coarse-grained binary masking. Second, a multi-scale sequence to image analysis network employs Fast Fourier Transform (FFT)-based time-frequency representation to disentangle the complex periodicity of meteorological data across varying time horizons. Crucially, the model incorporates a cross-modal Mamba interaction module featuring a novel dynamic C-matrix swapping mechanism. By exchanging state-space parameters between visual and temporal streams, this design conditions the state evolution of one modality on the context of the other, enabling deep structural coupling with linear computational complexity, thus overcoming the limitations of shallow concatenation. Experimental validation on the newly constructed fine-grained PV power dataset demonstrates that M3S-Net achieves a mean absolute error reduction of 6.2% in 10-minute forecasts compared to state-of-the-art baselines. The dataset and source code will be available at https://github.com/she1110/FGPD.

</details>


### [176] [DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation](https://arxiv.org/abs/2602.19848)
*Francisco Filho,Kelvin Cunha,Fábio Papais,Emanoel dos Santos,Rodrigo Mota,Thales Bezerra,Erico Medeiros,Paulo Borba,Tsang Ing Ren*

Main category: cs.CV

TL;DR: 用类条件扩散模型合成皮肤病变图像，并以MAE自监督预训练巨型ViT，再通过知识蒸馏将表示迁移到轻量级ViT，实现在类别极不均衡数据上的更优分类与移动端高效部署。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变数据集中恶性样本稀缺导致深度模型在训练中形成偏置决策边界，影响临床可靠性；同时临床应用需要能在移动端高效运行的轻量模型。

Method: 1) 利用类条件扩散模型生成多类别（特别是恶性）合成皮肤图像以缓解类别不平衡；2) 在合成数据上进行MAE自监督预训练，学习鲁棒、领域相关的表征，使用大型ViT作为教师；3) 通过知识蒸馏将教师模型的表征/软标签迁移到小型ViT学生模型，满足移动端部署。

Result: 将MAE在合成数据上的预训练与蒸馏结合，显著提升皮肤病变分类性能，同时学生模型实现高效的设备端推理。

Conclusion: 合成数据驱动的自监督预训练与蒸馏能够缓解类别不均衡并兼顾精度与效率，为真实临床场景中的移动端皮肤病变分类提供可行方案。

Abstract: Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.

</details>


### [177] [Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions](https://arxiv.org/abs/2602.19857)
*Rodrigo Mota,Kelvin Cunha,Emanoel dos Santos,Fábio Papais,Francisco Filho,Thales Bezerra,Erico Medeiros,Paulo Borba,Tsang Ing Ren*

Main category: cs.CV

TL;DR: 提出一种基于“视觉元域”的适配策略，把大规模皮镜数据的表示迁移到临床图像域，显著提升皮肤病变分类在跨域部署时的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在皮肤科图像上容易因采集差异（设备、光照、放大方式）与域特异视觉特征而性能下降，临床部署面临域移位与视觉伪影带来的误判风险，因此需要一种能跨皮镜与临床图像域泛化的训练与适配方法。

Method: 系统分析视觉伪影与域移位对皮肤病变分类的影响；提出“视觉元域”思想，将大规模皮镜数据学习到的通用视觉表示，通过域感知的适配训练迁移到临床图像域，实现跨域表示对齐与鲁棒特征提取；在多个皮肤科数据集上进行跨域训练与评估。

Result: 在多组皮肤科数据集与跨域设置（皮镜→临床）中，方法带来一致的分类性能提升，显著缩小皮镜与临床图像之间的性能差距，表现出更强的泛化与鲁棒性。

Conclusion: 域感知训练与基于视觉元域的适配可有效缓解获取差异与域特性导致的性能退化，是将皮肤病变分类模型可靠部署到临床环境的关键方向。

Abstract: Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems.

</details>


### [178] [Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation](https://arxiv.org/abs/2602.19863)
*Filip Wolf,Blaž Rolih,Luka Čehovin Zajc*

Main category: cs.CV

TL;DR: 提出一种用于多光谱遥感的双教师对比蒸馏框架，结合多光谱教师与光学视觉基础模型教师，使学生模型在多模态间对齐表征；在多项基准上的分割、变化检测、分类均达SOTA并保持对纯光学输入的性能。


<details>
  <summary>Details</summary>
Motivation: EO领域传感器与模态多样，单一通用模型难以覆盖；现有以掩码重建为主的预训练偏重局部、缺乏对全局语义结构的控制，且跨模态知识迁移不足。需要一种既能对齐跨模态语义又具可扩展性的预训练范式。

Method: 设计双教师对比自蒸馏：以多光谱教师提供谱域先验，光学VFM教师提供对比式全局语义结构监督；学生模型通过对比蒸馏同时对齐两种教师表征，实现跨模态一致的特征空间。训练范式贴近现代光学VFM的对比自蒸馏而非MIM。

Result: 在多样的光学与多光谱基准上取得SOTA：语义分割平均+3.64个百分点，变化检测+1.2，分类+1.31；在适配多光谱的同时不牺牲纯光学输入性能。

Conclusion: 对比蒸馏是跨异构EO数据可扩展表征学习的有效途径。双教师设计实现了多模态一致性与性能提升，为多EOFMs共存场景下的知识迁移提供了可行方案；代码即将开源。

Abstract: Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Code: Coming soon.

</details>


### [179] [ApET: Approximation-Error Guided Token Compression for Efficient VLMs](https://arxiv.org/abs/2602.19870)
*Qiankun Ma,Ziyao Zhang,Haofei Wang,Jie Chen,Zhen Song,Hairong Zheng*

Main category: cs.CV

TL;DR: ApET提出一种不依赖注意力的视觉token压缩方法，通过线性基重构并用近似误差筛除冗余token，在多种VLM上以约89%压缩率仍保持或提升任务性能，并兼容FlashAttention以提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM含大量冗余视觉token，导致推理开销高。主流做法依赖[CLS]或跨模态注意力来评估重要性，但会引入位置偏置，且与高效注意力内核（如FlashAttention）不兼容，限制实际加速部署。因此需要一种与注意力解耦、信息保持最大化的压缩方法。

Method: 提出ApET：从信息论视角进行注意力无关的token压缩。步骤：1) 用少量“基”token对原视觉token做线性近似重构；2) 计算各token的近似误差，将误差小（信息贡献低）的token判为冗余并丢弃；3) 以此在不依赖任何注意力信号的情况下完成压缩；因与注意力无关，可无缝结合FlashAttention。

Result: 在多种VLM与基准上：图像理解任务在压缩88.9%视觉token的同时保留95.2%原始性能；视频理解任务在压缩87.5%时达成100.4%相对性能。注意力无关设计使其与FlashAttention兼容，进一步加速推理。

Conclusion: ApET以近似误差指导的无注意力压缩，在大幅减少视觉token的同时维持甚至提升VLM性能，并解决了与高效注意力内核不兼容的问题，提升实际部署可行性；代码已开源。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.

</details>


### [180] [GOAL: Geometrically Optimal Alignment for Continual Generalized Category Discovery](https://arxiv.org/abs/2602.19872)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Shaokun Wang,Qiang Wang,Yihong Gong*

Main category: cs.CV

TL;DR: 提出GOAL框架：用固定ETF分类器维持一致几何结构，结合有监督与置信度引导对齐，实现持续泛化类别发现中的稳定新类融入与旧类保持；在四个基准上优于Happy，遗忘降16.1%，新类发现提升3.2%。


<details>
  <summary>Details</summary>
Motivation: C-GCD需在时间序列数据中从无标注样本中发现新类别，同时不忘已知类别。现有方法动态更新分类器权重，导致遗忘和特征对齐不一致，影响长期稳定性与性能。因此需要一种可在持续学习过程中保持一致几何结构、稳定对齐、降低遗忘并提升新类发现质量的方法。

Method: 引入固定的Equiangular Tight Frame (ETF) 分类器，作为整个学习过程中的不变几何骨架。对有标注样本进行有监督对齐（supervised alignment），对无标注的新类样本基于置信度进行引导对齐（confidence-guided alignment），使新类融入时不破坏旧类表征。整体形成统一的对齐与分类范式，避免动态权重更新造成的漂移。

Result: 在四个标准数据集基准上，GOAL相较于SOTA方法Happy表现更佳：遗忘降低16.1%，新类发现性能提升3.2%，体现更稳定的长期持续发现能力。

Conclusion: 固定ETF分类器提供一致的几何结构，配合有监督和置信度引导的对齐策略，可在C-GCD中显著缓解遗忘并提升新类发现效果，为长周期持续发现提供强有力的解决方案。

Abstract: Continual Generalized Category Discovery (C-GCD) requires identifying novel classes from unlabeled data while retaining knowledge of known classes over time. Existing methods typically update classifier weights dynamically, resulting in forgetting and inconsistent feature alignment. We propose GOAL, a unified framework that introduces a fixed Equiangular Tight Frame (ETF) classifier to impose a consistent geometric structure throughout learning. GOAL conducts supervised alignment for labeled samples and confidence-guided alignment for novel samples, enabling stable integration of new classes without disrupting old ones. Experiments on four benchmarks show that GOAL outperforms the prior method Happy, reducing forgetting by 16.1% and boosting novel class discovery by 3.2%, establishing a strong solution for long-horizon continual discovery.

</details>


### [181] [BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations](https://arxiv.org/abs/2602.19874)
*Lucas Martini,Alexander Lappe,Anna Bognár,Rufin Vogels,Martin A. Giese*

Main category: cs.CV

TL;DR: 提出BigMaQ：首个将动态3D姿态-形状表征纳入非人灵长类行为识别学习任务的大规模数据集与基准；利用个体化纹理化网格头像提升姿态重建精度，并显著提升动作识别mAP。


<details>
  <summary>Details</summary>
Motivation: 动物动态与社会行为的自动识别对行为学、生态、医学、神经科学至关重要；但现有方法多停留在视频特征与稀疏关键点层面，缺少高精度3D姿态与形状融合，尤其非人灵长类的网格跟踪研究滞后，难以捕捉丰富动作动力学。

Method: 构建BigMaQ数据集：>750个恒河猴交互场景，提供细致3D姿态-形状注释。基于高质量猕猴模板网格，为个体自适应生成具纹理的专属avatar（主体特异网格），扩展表面跟踪方法以得到更准确的姿态描述。由此派生BigMaQ500基准，将表面姿态向量与多只个体的单帧配对；与主流图像/视频编码器结合，比较含/不含姿态描述的动作识别性能。

Result: 主体特异网格带来比以往表面跟踪SOTA更高的姿态精度；在BigMaQ500上，加入3D表面姿态向量后，动作识别mAP显著提升（对比仅用图像/视频特征）。

Conclusion: BigMaQ首次把动态3D姿态-形状显式融入动物动作识别学习流程，提供丰富的非人灵长类外观、姿态与社交互动数据资源，推动精准行为理解；代码与数据已公开。

Abstract: The recognition of dynamic and social behavior in animals is fundamental for advancing ethology, ecology, medicine and neuroscience. Recent progress in deep learning has enabled automated behavior recognition from video, yet an accurate reconstruction of the three-dimensional (3D) pose and shape has not been integrated into this process. Especially for non-human primates, mesh-based tracking efforts lag behind those for other species, leaving pose descriptions restricted to sparse keypoints that are unable to fully capture the richness of action dynamics. To address this gap, we introduce the $\textbf{Big Ma}$ca$\textbf{Q}$ue 3D Motion and Animation Dataset ($\texttt{BigMaQ}$), a large-scale dataset comprising more than 750 scenes of interacting rhesus macaques with detailed 3D pose descriptions. Extending previous surface-based animal tracking methods, we construct subject-specific textured avatars by adapting a high-quality macaque template mesh to individual monkeys. This allows us to provide pose descriptions that are more accurate than previous state-of-the-art surface-based animal tracking methods. From the original dataset, we derive BigMaQ500, an action recognition benchmark that links surface-based pose vectors to single frames across multiple individual monkeys. By pairing features extracted from established image and video encoders with and without our pose descriptors, we demonstrate substantial improvements in mean average precision (mAP) when pose information is included. With these contributions, $\texttt{BigMaQ}$ establishes the first dataset that both integrates dynamic 3D pose-shape representations into the learning task of animal action recognition and provides a rich resource to advance the study of visual appearance, posture, and social interaction in non-human primates. The code and data are publicly available at https://martinivis.github.io/BigMaQ/ .

</details>


### [182] [Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations](https://arxiv.org/abs/2602.19881)
*Blaž Rolih,Matic Fučka,Filip Wolf,Luka Čehovin Zajc*

Main category: cs.CV

TL;DR: 提出MaSoN：在潜特征空间按数据统计动态合成变化的无监督遥感变化检测框架，摆脱手工假设与像素级伪造，跨类型与模态泛化强，五大基准SOTA，平均F1提升14.1个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有UCD方法依赖冻结大模型或像素空间的合成变化，需预设变化类型的先验（规则、外部数据或生成器），导致对罕见/复杂变化泛化差、实用性有限，且难扩展到新模态。

Method: 提出MaSoN（Make Some Noise）：在训练期间于潜特征空间直接合成“变化”，并用目标数据的特征统计自适应估计噪声/扰动分布，实现多样但与域对齐的变化；端到端训练，无需标注；框架可无缝扩展到SAR等新模态。

Result: 在五个基准上达到SOTA，平均F1比分别方法提升14.1个百分点，展示出对多种变化类型与模态的强泛化能力。

Conclusion: 在潜空间、基于数据统计的变化合成可显著提升UCD的泛化与性能，减少对预设先验的依赖，并具备跨模态适用性。

Abstract: Unsupervised change detection (UCD) in remote sensing aims to localise semantic changes between two images of the same region without relying on labelled data during training. Most recent approaches rely either on frozen foundation models in a training-free manner or on training with synthetic changes generated in pixel space. Both strategies inherently rely on predefined assumptions about change types, typically introduced through handcrafted rules, external datasets, or auxiliary generative models. Due to these assumptions, such methods fail to generalise beyond a few change types, limiting their real-world usage, especially in rare or complex scenarios. To address this, we propose MaSoN (Make Some Noise), an end-to-end UCD framework that synthesises diverse changes directly in the latent feature space during training. It generates changes that are dynamically estimated using feature statistics of target data, enabling diverse yet data-driven variation aligned with the target domain. It also easily extends to new modalities, such as SAR. MaSoN generalises strongly across diverse change types and achieves state-of-the-art performance on five benchmarks, improving the average F1 score by 14.1 percentage points. Project page: https://blaz-r.github.io/mason_ucd

</details>


### [183] [Monocular Mesh Recovery and Body Measurement of Female Saanen Goats](https://arxiv.org/abs/2602.19896)
*Bo Jin,Shichao Zhao,Jin Lyu,Bin Zhang,Tao Yu,Liang An,Yebin Liu,Meili Wang*

Main category: cs.CV

TL;DR: 提出FemaleSaanenGoat多视角RGBD数据集与SaanenGoat参数化三维形体模型，实现雌性萨能奶山羊高精度三维重建与关键体尺自动测量，显著提升畜牧精细化管理中的规模化三维视觉应用精度。


<details>
  <summary>Details</summary>
Motivation: 萨能奶山羊泌乳性能与体尺密切相关，但真实、面向山羊的三维数据与重建方法缺失，导致单个个体和群体层面的体尺评估与产奶潜力预测受限。

Method: 采集55只6–18月龄雌性萨能羊的八视角同步RGBD视频；用多视角DynamicFusion将含噪的非刚性点云序列融合为高保真三维扫描；据此构建包含41骨骼关节并强化乳房表征的SaanenGoat参数化模型，基于48只个体建立形状空间；利用该模型从单视角RGBD实现高精度三维重建，并自动测量六项体尺指标。

Result: 在三维重建精度与体尺测量误差上均优于现有方法，实现对体长、体高、胸宽、胸围、髋宽、髋高的自动、准确估计。

Conclusion: 面向雌性萨能羊的专用数据集与参数模型为精细化养殖中的大规模三维视觉提供了新范式，可支持更可靠的产奶潜力评估与管理决策。

Abstract: The lactation performance of Saanen dairy goats, renowned for their high milk yield, is intrinsically linked to their body size, making accurate 3D body measurement essential for assessing milk production potential, yet existing reconstruction methods lack goat-specific authentic 3D data. To address this limitation, we establish the FemaleSaanenGoat dataset containing synchronized eight-view RGBD videos of 55 female Saanen goats (6-18 months). Using multi-view DynamicFusion, we fuse noisy, non-rigid point cloud sequences into high-fidelity 3D scans, overcoming challenges from irregular surfaces and rapid movement. Based on these scans, we develop SaanenGoat, a parametric 3D shape model specifically designed for female Saanen goats. This model features a refined template with 41 skeletal joints and enhanced udder representation, registered with our scan data. A comprehensive shape space constructed from 48 goats enables precise representation of diverse individual variations. With the help of SaanenGoat model, we get high-precision 3D reconstruction from single-view RGBD input, and achieve automated measurement of six critical body dimensions: body length, height, chest width, chest girth, hip width, and hip height. Experimental results demonstrate the superior accuracy of our method in both 3D reconstruction and body measurement, presenting a novel paradigm for large-scale 3D vision applications in precision livestock farming.

</details>


### [184] [ExpPortrait: Expressive Portrait Generation via Personalized Representation](https://arxiv.org/abs/2602.19900)
*Junyi Wang,Yudong Guo,Boyang Guo,Shengming Yang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出一种高保真个性化头部表示和表达迁移模块，用作条件信号训练DiT生成器，实现更好身份保持、表情准确与时序稳定的电影感人像视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有用于人像生成的中间表示（2D关键点、参数模型）稀疏或低秩，难以解耦身份与表情，无法表达个性化细节，导致身份漂移、表情不准确、时序不稳，制约高质量人像视频生成。

Method: 1) 设计高保真的个性化头部表示，将静态的主体特定全局几何与动态的表情相关细节显式分离与建模；2) 引入表达迁移模块，实现不同身份之间的头姿与表情细节的个性化迁移；3) 以该表达力强的头部模型作为条件，训练基于DiT的扩散生成器以合成细节丰富的人像视频。

Result: 在自重演与跨重演实验中，相较以往方法，显著提升身份保持、表情精度与时间稳定性，尤其对复杂运动的细粒度细节捕捉更优。

Conclusion: 通过更强表达力与更好解耦的个性化头部表示，并结合表达迁移与DiT条件扩散生成，可生成更真实、稳定且可控的电影感人像视频，优于现有基线。

Abstract: While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.

</details>


### [185] [Gradient based Severity Labeling for Biomarker Classification in OCT](https://arxiv.org/abs/2602.19907)
*Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib,Stephanie Trejo Corona,Charles Wykoff*

Main category: cs.CV

TL;DR: 提出一种针对医学图像的对比学习样本选择策略：用疾病严重程度相似性替代常规数据增强来构造正负对。基于异常检测梯度响应为未标注OCT生成严重度标签，并在监督式对比学习中使用，最终在糖网生物标志物分类上较自监督基线提升约6%。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习依赖强数据增强来定义正负样本，但医学图像中的细小病灶对增强极其敏感，易被扭曲，导致表示学习偏差。医学任务更关注疾病进展相关结构，相似严重程度的样本更可能共享关键生物标志物，因此需要一种与病理进展一致的样本选择机制。

Method: 1) 使用异常检测算法，对未标注OCT的梯度响应进行分析；2) 将梯度响应归纳为反映疾病严重度的伪标签；3) 基于这些严重度标签构造监督式对比学习（同严重度为正对，不同严重度为负对），训练表征；4) 将学得表征用于糖尿病视网膜病变（DR）关键生物标志物分类。

Result: 与自监督对比学习等基线相比，在DR关键生物标志物分类任务上，所提方法的准确率最高提升约6%。

Conclusion: 基于疾病严重度相似性的样本选择能替代风险较高的强增强，提供更符合医学语义的对比学习信号；通过异常检测梯度生成的严重度伪标签可有效监督表征学习，从而提升下游生物标志物分类性能。

Abstract: In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these samples are more likely to have similar structures related to the progression of a disease. To enable this, we introduce a method that generates disease severity labels for unlabeled OCT scans on the basis of gradient responses from an anomaly detection algorithm. These labels are used to train a supervised contrastive learning setup to improve biomarker classification accuracy by as much as 6% above self-supervised baselines for key indicators of Diabetic Retinopathy.

</details>


### [186] [Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery](https://arxiv.org/abs/2602.19910)
*Wei He,Xianghan Meng,Zhiyuan Huang,Xianbiao Qi,Rong Xiao,Chun-Guang Li*

Main category: cs.CV

TL;DR: 提出SSR^2-GCD：一种用于广义类别发现（GCD）的多模态表示学习框架，通过“半监督率约简”强化模态内对齐，并利用视觉-语言模型的提示对齐来促进跨模态知识迁移，在通用与细粒度数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法多依赖多模态（如图像-文本）对齐以处理部分标注的已知类与未知类识别，但忽视了模态内结构的合理对齐，导致表示分布结构欠佳，限制了开放集识别与聚类质量。

Method: 提出半监督率约简（Semi-Supervised Rate Reduction, SSR^2）作为目标，显式塑造表示的结构属性：在每个模态内通过最小化类内速率、最大化类间速率实现更紧致与可分的簇；同时结合多模态对齐以获得跨模态一致性。进一步利用视觉-语言模型（VLM）的跨模态对齐能力，整合多种提示模板作为“提示候选”，以增强知识迁移与伪标签质量。

Result: 在通用与细粒度GCD基准上进行广泛实验，方法在已知类识别与未知类发现（如聚类/开放集指标）上均优于当前SOTA，显示更强的结构化表示与泛化能力。

Conclusion: 合理的模态内对齐对GCD至关重要；通过SSR^2实现的结构化多模态学习与VLM提示协同，可在半监督与开放集情形下显著提升已知/未知类别的发现与识别性能。

Abstract: Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.

</details>


### [187] [Augmented Radiance Field: A General Framework for Enhanced Gaussian Splatting](https://arxiv.org/abs/2602.19916)
*Yixin Yang,Bojian Wu,Yang Zhou,Hui Huang*

Main category: cs.CV

TL;DR: 提出增强高斯核，利用视角相关不透明度显式建模镜面反射，并结合误差驱动补偿策略，在已有3DGS场景上自适应插入与优化这些核，最终获得更高质量、参数更高效的辐射场；性能优于SOTA NeRF且具实时渲染优势。


<details>
  <summary>Details</summary>
Motivation: 传统3D Gaussian Splatting用球谐编码颜色，难以有效分离漫反射与镜面反射，故对复杂高光/反射场景拟合不足；需要在保持3DGS实时性的同时，更准确表达视角相关的镜面成分并提升质量与参数效率。

Method: 1) 从2D高斯初始化开始；2) 设计“增强高斯核”，通过视角相关的不透明度显式编码镜面效应（视点依赖项）；3) 误差驱动的补偿策略：在现有3DGS场景中检测误差区域，自适应插入并优化增强高斯核；4) 共同优化得到增强的辐射场。

Result: 在多组实验中，相比SOTA NeRF方法，渲染质量与速度更优，同时以更少参数达到更高或相当的指标；对复杂反射更准确，保留3DGS实时特性。

Conclusion: 通过将镜面项以视角相关不透明度形式纳入高斯核，并用误差驱动补偿自适应增广场景，可在不牺牲实时性的前提下显著提升复杂反射下的渲染质量与参数效率。

Abstract: Due to the real-time rendering performance, 3D Gaussian Splatting (3DGS) has emerged as the leading method for radiance field reconstruction. However, its reliance on spherical harmonics for color encoding inherently limits its ability to separate diffuse and specular components, making it challenging to accurately represent complex reflections. To address this, we propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity. Meanwhile, we introduce an error-driven compensation strategy to improve rendering quality in existing 3DGS scenes. Our method begins with 2D Gaussian initialization and then adaptively inserts and optimizes enhanced Gaussian kernels, ultimately producing an augmented radiance field. Experiments demonstrate that our method not only surpasses state-of-the-art NeRF methods in rendering performance but also achieves greater parameter efficiency. Project page at: https://xiaoxinyyx.github.io/augs.

</details>


### [188] [Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation](https://arxiv.org/abs/2602.19937)
*Yifei Shi,Boyan Wan,Xin Xu,Kai Xu*

Main category: cs.CV

TL;DR: 提出一种结合SO(3)等变卷积隐式网络与正激励点采样（PIPS）的姿态估计方法，在高遮挡、未知姿态、几何新颖与强噪声条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经隐式场可在任意分辨率下表示形状，并已被用于重建、视图合成与姿态估计。其通过学习相机空间与物体规范空间的稠密对应，即便对未观测区域也可推断，从而提升在遮挡与新形状情形下的姿态估计。然而，对未观测区域缺乏直接监督信号，导致高度依赖模型泛化并产生高不确定性；在整个相机空间进行稠密采样会引入大量不准估计，影响训练与性能。

Method: 1) 设计SO(3)-等变卷积隐式网络，在任意查询位置估计点级属性并保证对三维旋转的等变性，从而稳健处理姿态变化；2) 提出正激励点采样（PIPS），根据输入动态选择采样位置，集中于更有信息/更可靠的区域，减少无效或误导性样本，提升精度与训练效率。

Result: 在三个姿态估计数据集上超越SOTA，尤其在未见姿态、高遮挡、几何新颖与强噪声等困难场景中取得显著改进。

Conclusion: 通过SO(3)等变隐式网络与自适应采样（PIPS）的结合，缓解未观测区域预测不确定性、提升点级估计与训练效率，从而在多数据集上实现更强的三维物体姿态估计性能。

Abstract: Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space-including unobserved regions in camera space-significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling (PIPS) strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The PIPS strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. Our method outperforms the state-of-the-art on three pose estimation datasets. Notably, it demonstrates significant improvements in challenging scenarios, such as objects captured with unseen pose, high occlusion, novel geometry, and severe noise.

</details>


### [189] [Discover, Segment, and Select: A Progressive Mechanism for Zero-shot Camouflaged Object Segmentation](https://arxiv.org/abs/2602.19944)
*Yilong Yang,Jianxin Tian,Shengchuan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出一种用于零样本伪装目标分割的渐进式DSS(Discover–Segment–Select)框架：先用特征一致的候选生成，再用SAM细化，最后用MLLM语义评分选择最佳掩码，无需训练即达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有零样本COS多采用“先发现再分割”的两阶段：MLLM给出视觉提示，SAM分割。但仅靠MLLM发现伪装目标易出现定位不准、误检与漏检，特别是多实例场景。需要一种能在无监督条件下提高定位与分割可靠性的机制。

Method: 提出DSS三步走：1) FOD模块利用视觉特征生成多样、特征一致的对象候选；2) 分割模块用SAM对各候选进行精细分割得到多掩码；3) SMS模块用MLLM进行语义驱动的掩码评估与选择，挑出最优掩码。全流程无需训练或监督。

Result: 在多套伪装目标分割基准上达到最新SOTA，尤其在多实例场景表现显著优于以往零样本方法。

Conclusion: 通过“先发现-再分割-再选择”的渐进式无训练框架，融合视觉特征与语义评估，显著缓解MLLM单独发现的定位误差与误检漏检问题，提升零样本伪装分割的鲁棒性与精度。

Abstract: Current zero-shot Camouflaged Object Segmentation methods typically employ a two-stage pipeline (discover-then-segment): using MLLMs to obtain visual prompts, followed by SAM segmentation. However, relying solely on MLLMs for camouflaged object discovery often leads to inaccurate localization, false positives, and missed detections. To address these issues, we propose the \textbf{D}iscover-\textbf{S}egment-\textbf{S}elect (\textbf{DSS}) mechanism, a progressive framework designed to refine segmentation step by step. The proposed method contains a Feature-coherent Object Discovery (FOD) module that leverages visual features to generate diverse object proposals, a segmentation module that refines these proposals through SAM segmentation, and a Semantic-driven Mask Selection (SMS) module that employs MLLMs to evaluate and select the optimal segmentation mask from multiple candidates. Without requiring any training or supervision, DSS achieves state-of-the-art performance on multiple COS benchmarks, especially in multiple-instance scenes.

</details>


### [190] [When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators](https://arxiv.org/abs/2602.19946)
*Krzysztof Adamkiewicz,Brian Moser,Stanislav Frolov,Tobias Christian Nauen,Federico Raue,Andreas Dengel*

Main category: cs.CV

TL;DR: 论文发现：尽管近年T2I扩散模型更逼真、跟随提示更好，但用它们合成的训练数据训练分类器，在真实测试集上的准确率反而逐代下降。原因在于分布收缩至审美中心、类别对齐变差与多样性不足，说明“生成逼真度提升=数据真实度提升”的假设并不成立。


<details>
  <summary>Details</summary>
Motivation: 检验一个在视觉领域被广泛默认的假设：生成模型逼真度与提示遵从性的进步，能否自动转化为更好的“合成训练数据”，从而替代或补充真实数据以提升下游分类性能。

Method: - 选取2022–2025间多代最先进T2I扩散模型；
- 为多类别任务生成大规模合成数据集（受控提示）；
- 仅用合成数据训练标准图像分类器；
- 在真实测试集上评估；
- 分析数据分布：多样性、审美偏置、标签-图像对齐度等。

Result: 尽管视觉保真与提示遵循显著提升，但用更新T2I模型生成的数据训练的分类器，在真实测试集的准确率持续下降。伴随现象：分布向“美学中心”塌缩，多样性降低，标签-图像语义对齐变弱。

Conclusion: 生成逼真并不等于数据可用性与真实性的提升。现代T2I模型作为训练数据生成器存在系统性局限，需要重新审视与改进（提升多样性、减少审美偏置、强化语义对齐），否则难以可靠替代真实数据。

Abstract: Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.

</details>


### [191] [RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection](https://arxiv.org/abs/2602.19974)
*Tianyu Wang,Zhiyuan Ma,Qian Wang,Xinyi Zhang,Xinwei Long,Bowen Zhou*

Main category: cs.CV

TL;DR: 提出RL-RIG，一个“生成-反思-编辑”的强化学习框架，通过VLM驱动的反思与编辑，提升文生图的细粒度空间推理与结构一致性；在LAION-SG上以Scene Graph IoU与VLM评审衡量，较SOTA开源模型提升最高11%。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型在空间推理上薄弱：难以从文本中准确捕捉细粒度空间关系，生成的场景常缺乏结构完整性。作者希望通过引入可反思与可编辑的闭环过程，用强化学习直接优化空间一致性而非仅追求视觉质量。

Method: 提出RL-RIG框架，含四个组件：Diffuser（初始生成）、Checker（评估空间一致性与质量）、Actor（VLM生成编辑指令/编辑提示，触发链式推理）、Inverse Diffuser（根据编辑提示对图像进行定向编辑）。采用“Generate-Reflect-Edit”范式；设计Reflection-GRPO训练：分别训练VLM Actor生成更优编辑提示与图像编辑器在给定提示下提升图像质量。评估使用Scene Graph IoU和VLM-as-a-Judge以度量空间准确性与一致性。

Result: 在LAION-SG数据集上，相比开源SOTA基线，在可控且精确的空间推理指标上最高提升11%，并在结构一致性上显著优于仅追求视觉效果的方法。

Conclusion: 通过反思驱动的强化学习与可编辑生成闭环，RL-RIG有效缓解文生图的空间推理困境，能更好地遵循文本中的细粒度空间关系并生成结构合理的图像；评测框架也更关注空间一致性，验证了方法的有效性。

Abstract: Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture comprises four primary components: Diffuser, Checker, Actor, and Inverse Diffuser, following a Generate-Reflect-Edit paradigm to spark the Chain of Thought reasoning ability in image generation for addressing the dilemma. To equip the model with better intuition over generation trajectories, we further develop Reflection-GRPO to train the VLM Actor for edit prompts and the Image Editor for better image quality under a given prompt, respectively. Unlike traditional approaches that solely produce visually stunning yet structurally unreasonable content, our evaluation metrics prioritize spatial accuracy, utilizing Scene Graph IoU and employing a VLM-as-a-Judge strategy to assess the spatial consistency of generated images on LAION-SG dataset. Experimental results show that RL-RIG outperforms existing state-of-the-art open-source models by up to 11% in terms of controllable and precise spatial reasoning in image generation.

</details>


### [192] [RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather](https://arxiv.org/abs/2602.19994)
*Christof Leitgeb,Thomas Puchleitner,Max Peter Ronecker,Daniel Watzenig*

Main category: cs.CV

TL;DR: 提出RADE-Net与3D投影法，将4D雷达RADE张量高效压缩至投影表示（单帧数据减少约91.9%），在K-Radar上显著提升检测效果，并在恶劣天气下优于多种方法。


<details>
  <summary>Details</summary>
Motivation: 汽车感知需在恶劣天气下可靠工作。相机/激光雷达在雨雾雪中性能下降；原始全雷达张量数据量大且数据集稀缺，现有多用稀疏点云/2D投影导致信息损失。深度学习可从低层雷达数据提取更丰富特征，但需要兼顾效率与信息保留。

Method: 1) 对快速傅里叶变换后的4D RADE（距离-方位-多普勒-仰角）张量进行3D投影，保留关键多普勒与仰角信息，同时显著压缩数据量；2) 设计轻量化RADE-Net骨干，结合空间与通道注意力，融合低/高层特征；3) 解耦检测头：在距离-方位域直接预测目标中心点，在笛卡尔坐标系回归旋转3D框。

Result: 在K-Radar大规模数据集、含多类道路参与者与多种天气条件下评测：较其基线提升16.7%，较当前雷达-only模型提升6.5%；在恶劣天气场景中超过多种激光雷达方法。

Conclusion: 所提3D投影与RADE-Net在显著降低数据规模与模型复杂度的同时，提升训练/推理速度和3D检测精度，特别在恶劣天气中具备鲁棒优势，并验证了利用低层雷达张量信息的可行性与优越性。

Abstract: Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net.

</details>


### [193] [Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation](https://arxiv.org/abs/2602.20008)
*Louis Fabrice Tshimanga,Andrea Zanola,Federico Del Pup,Manfredo Atzori*

Main category: cs.CV

TL;DR: 提出Token-UNet：在UNet中引入TokenLearner/TokenFuser以低成本利用Transformer的全局建模，用更少token替代密集自注意力，显著降内存与推理时间，并在3D医学图像分割上略超SwinUNETR。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像分割需要全局依赖，但Transformer在3D场景下token数随分辨率立方增长，注意力代价二次方上升，导致内存与算力瓶颈；现有(Swin)UNETR虽融合Transformer，但硬件成本高，难以在受限资源中训练/部署。

Method: 保留UNet样式的卷积编码器以高效提取局部特征；在3D特征图上引入TokenLearner，从局部与全局结构自适应汇聚为固定数量的“精炼token”，再用TokenFuser将Transformer输出与多尺度特征融合解码，实现少token的全局交互与可解释注意图。

Result: 在与SwinUNETR对比中：最重模型的内存占用降至33%、推理时间至10%、参数量至35%，同时Dice由86.75%±0.19%提升到87.21%±0.35%，并产生自然可解释的注意力图。

Conclusion: 通过以卷积保留高效局部建模、以TokenLearner/TokenFuser稀疏化全局交互，Token-UNet在受限算力下实现更高效、略更优的3D分割；为低资源环境中的训练、微调与迁移学习提供可行路径，利于更广泛的医学影像研究与应用。

Abstract: We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.
  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.
  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.
  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.
  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.
  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\%, 10\%, and 35\% of the SwinUNETR values, with better average performance (86.75\% $\pm 0.19\%$ Dice score for SwinUNETR vs our 87.21\% $\pm 0.35\%$).
  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.

</details>


### [194] [Descriptor: Dataset of Parasitoid Wasps and Associated Hymenoptera (DAPWH)](https://arxiv.org/abs/2602.20028)
*Joao Manoel Herrera Pinheiro,Gabriela Do Nascimento Herrera,Luciana Bueno Dos Reis Fernandes,Alvaro Doria Dos Santos,Ricardo V. Godoy,Eduardo A. B. Almeida,Helena Carolina Onody,Marcelo Andrade Da Costa Vieira,Angelica Maria Penteado-Dias,Marcelo Becker*

Main category: cs.CV

TL;DR: 该论文提出并发布了一个面向寄生蜂（Ichneumonoidea）的高质量图像数据集，含多类标注（全虫体、翅脉与比例尺），以支持自动化物种/科级识别与监测模型的开发。


<details>
  <summary>Details</summary>
Motivation: 寄生蜂群体物种极其多样、形态隐蔽、未描述物种众多，传统形态学鉴定难度大，缺乏稳健的数字资源严重限制了生物多样性监测与农业害虫管理中的自动识别与应用。

Method: 构建并整理一个经过人工筛选与标注的图像数据集：共3,556张高分辨率图像，聚焦新热带区的姬小蜂科（Ichneumonidae）与姬蜂科（Braconidae），并加入多种相关膜翅目科作负样本/域扩展；其中1,739张以COCO格式提供多类别边界框，覆盖全虫体、翅脉与比例尺。

Result: 获得兼具广度与细粒度结构特征标注的数据资源，可直接用于训练与评测计算机视觉模型；跨科扩展样本（如Halictidae、Vespidae等）有助于提升模型稳健性与泛化能力。

Conclusion: 该数据集为寄生蜂自动化识别提供基准与起点，能促进在生物多样性监测与农业应用中的可靠家族/科级识别模型开发，并为后续细粒度到物种级的扩展打下基础。

Abstract: Accurate taxonomic identification is the cornerstone of biodiversity monitoring and agricultural management, particularly for the hyper-diverse superfamily Ichneumonoidea. Comprising the families Ichneumonidae and Braconidae, these parasitoid wasps are ecologically critical for regulating insect populations, yet they remain one of the most taxonomically challenging groups due to their cryptic morphology and vast number of undescribed species. To address the scarcity of robust digital resources for these key groups, we present a curated image dataset designed to advance automated identification systems. The dataset contains 3,556 high-resolution images, primarily focused on Neotropical Ichneumonidae and Braconidae, while also including supplementary families such as Andrenidae, Apidae, Bethylidae, Chrysididae, Colletidae, Halictidae, Megachilidae, Pompilidae, and Vespidae to improve model robustness. Crucially, a subset of 1,739 images is annotated in COCO format, featuring multi-class bounding boxes for the full insect body, wing venation, and scale bars. This resource provides a foundation for developing computer vision models capable of identifying these families.

</details>


### [195] [Closing the gap in multimodal medical representation alignment](https://arxiv.org/abs/2602.20046)
*Eleonora Grassucci,Giordano Cicchetti,Danilo Comminiello*

Main category: cs.CV

TL;DR: 论文指出CLIP式对比学习在多模态对齐中易产生“模态鸿沟”，导致潜表空间稀疏与碎片化；作者在医疗影像-临床文本上证实该问题并提出模态无关框架以弥合鸿沟，显著提升跨模态检索与影像描述。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP对比损失虽促成跨模态对齐，但会无意拉开不同模态之间的全局分布，损害真正的语义一致性；该问题在通用图文上部分缓解，但在更复杂的医疗多模态（放射影像与临床文本）中未被系统研究与解决。

Method: 系统分析医疗场景中的模态鸿沟，并提出一个“模态无关”的对齐框架：在不依赖具体模态假设的前提下，通过对比学习改造与额外正则/约束（如跨模态-同语义聚合、模态不变性约束、分布对齐或中心化/去偏移等）使相同语义的表示更紧密、不同语义更可分，同时抑制模态特有偏移造成的分裂。

Result: 在放射影像与临床文本任务上，框架缩小模态鸿沟并改进潜表空间的连续性与对齐度；下游跨模态检索与医学影像自动描述（captioning）性能均明显优于CLIP式基线。

Conclusion: 医疗多模态中同样存在CLIP引发的模态鸿沟；采用模态无关的对齐与正则可有效闭合鸿沟，带来更稳健的语义对齐与更强的跨模态下游性能。

Abstract: In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.

</details>


### [196] [SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency](https://arxiv.org/abs/2602.20051)
*Yeonsung Kim,Junggeun Do,Seunguk Do,Sangmin Kim,Jaesik Park,Jay-Yoon Lee*

Main category: cs.CV

TL;DR: 提出SEAL-pose：用可学习的“损失网络”来评估姿态结构合理性，从数据中学习关节依赖，作为训练信号提升3D人体姿态估计的准确性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 传统监督损失将每个关节独立对待，难以捕捉关节间复杂的局部与全局相关性；以人工先验或规则约束增强结构一致性的做法需要手工设计、且常不可微，不便端到端训练。

Method: 设计一个基于关节图的可学习损失网络（loss-net），作为“结构可行性评估器”，与姿态网络（pose-net）协同训练。loss-net不依赖手工先验，而是从数据中学习复杂结构依赖，并为pose-net提供可微的结构一致性训练信号；在多个骨干网络和数据集上进行通用适配与评估。

Result: 在三个3D HPE基准、八种骨干上，SEAL-pose在所有设置下均降低逐关节误差并提升姿态可合理性；同时超越带显式结构约束的模型，尽管自身未施加显式约束；并在跨数据集与真实场景中展示稳健性。

Conclusion: 通过数据驱动的可学习损失网络替代手工结构先验，可端到端学习关节结构依赖，全面提升3D人体姿态估计的精度与结构一致性，并具有良好的泛化能力。

Abstract: 3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.

</details>


### [197] [Decoupling Defense Strategies for Robust Image Watermarking](https://arxiv.org/abs/2602.20053)
*Jiahui Chen,Zehang Deng,Zeyu Zhang,Chaoyang Li,Lianchen Jia,Lifeng Sun*

Main category: cs.CV

TL;DR: 提出AdvMark：两阶段微调的深度水印防御框架，分离对抗与退化再生鲁棒性训练，兼顾图像质量与多类攻击下的鲁棒性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水印对常规失真具鲁棒，但对高级对抗与再生成攻击易失效。传统“编码器-噪声层-解码器”联合训练的防御存在两难：解码器对抗训练牺牲无攻击（clean）精度；同时对多类高级攻击联合训练导致鲁棒性受限。需要一种既不损伤干净性能、又能分别针对不同攻击机理优化的策略。

Method: AdvMark两阶段解耦微调：
- 阶段1（对抗鲁棒）：定制化对抗训练，主要微调编码器，解码器仅在条件触发时更新；目标是把含水印图像映射到“不可攻击区域”，而非改变解码判别边界，从而保持clean精度。
- 阶段2（失真/再生鲁棒）：以直接图像优化应对失真与再生成攻击；引入有理论保证的受约束图像损失，在保持与原始封面图与阶段1输出接近之间取得平衡，以保留阶段1获得的对抗鲁棒性；并加入质量感知的early-stop以保证视觉质量下界。

Result: 在广泛实验中，AdvMark在图像质量与综合鲁棒性上领先：相较基线，对失真、再生成、对抗攻击的准确率分别最高提升约29%、33%、46%。

Conclusion: 将对抗防御与重建/失真鲁棒性解耦，通过“编码器优先的对抗微调+受约束的图像级优化”可在不牺牲clean精度的前提下，显著提升多种攻击下的水印鲁棒性与图像质量。

Abstract: Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\%, 33\% and 46\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.

</details>


### [198] [MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.20060)
*Junli Wang,Xueyi Liu,Yinan Zheng,Zebing Xing,Pengfei Li,Guang Li,Kun Ma,Guang Chen,Hangjun Ye,Zhongpu Xia,Long Chen,Qichao Zhang*

Main category: cs.CV

TL;DR: MeanFuser用高斯混合噪声与MeanFlow Identity替代离散锚点和常规流匹配ODE，结合轻量自适应重建模块，在端到端自动驾驶中实现连续轨迹采样、免ODE数值误差与高效推理，并在NAVSIM闭环上取得强绩效与高效率，无需PDM Score监督。


<details>
  <summary>Details</summary>
Motivation: 锚点引导的生成式轨迹规划虽能刻画驾驶不确定性，但依赖离散锚点词表：词表过小覆盖不足、过大带来效率与鲁棒性问题，存在内在权衡；同时流匹配需数值ODE求解带来误差与推理开销。作者欲在保持/提升性能下去除锚点依赖并显著提速与增强鲁棒性。

Method: 1) 以高斯混合噪声(GMN)作为生成引导，形成连续轨迹空间表示，替代离散锚点；2) 将MeanFlow Identity引入端到端规划，学习GMN与真实轨迹分布之间的平均速度场，避免瞬时速度场与ODE求解，从而消除数值误差并加速推理；3) 设计轻量级自适应重建模块(ARM)，通过注意力在候选采样中自适应选择或在“均不满意”时重构新轨迹。

Result: 在NAVSIM闭环基准上，无需PDM Score监督即可获得“突出/卓越”的规划与驾驶表现，并显著提升推理效率；表现稳定、鲁棒。

Conclusion: MeanFuser通过连续化噪声引导、免ODE的MeanFlow以及自适应重建，突破锚点词表带来的覆盖-效率权衡，在端到端自动驾驶中同时实现高性能与高效率，具备实用鲁棒性。

Abstract: Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.

</details>


### [199] [HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images](https://arxiv.org/abs/2602.20066)
*Kundan Thota,Xuanhao Mu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.CV

TL;DR: 提出HeatPrompt：用零样本视觉语言模型从卫星图像与基础GIS、建筑特征中抽取语义属性，并用MLP回归估算建筑年度供热需求；相较基线R²提升93.7%，MAE降30%，为数据匮乏地区供热规划提供轻量支撑。


<details>
  <summary>Details</summary>
Motivation: 城市供热脱碳需要精细的热需求地图，但多数城市缺乏建筑级数据。作者希望在缺乏详尽台账的情况下，利用现成的遥感与少量元数据推断热负荷。

Method: 设计HeatPrompt框架：以领域化提示让预训练大规模视觉语言模型从RGB卫星图像抽取与热负荷相关的语义特征（如屋顶年龄、建筑密度等），结合基础GIS与建筑层面特征生成文本描述；再用这些caption作为输入特征训练MLP回归器，预测年度热需求。并进行定量评估（R²、MAE）及定性分析（高影响token与高需求区对齐）。

Result: 与基线相比，R²提高93.7%，MAE降低30%。定性结果显示由模型识别的高重要性词语与高热需求区域空间一致。

Conclusion: 零样本VLM语义特征与简单回归器即可显著提升热需求估算精度，为缺数据地区的供热规划提供低资源、可扩展的工具。

Abstract: Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.

</details>


### [200] [The Invisible Gorilla Effect in Out-of-distribution Detection](https://arxiv.org/abs/2602.20068)
*Harry Anthony,Ziyun Liang,Hermione Warr,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 论文揭示在图像分类的OOD检测中存在“隐形大猩猩效应”：当难检伪影的视觉属性（如颜色）与模型关注的ROI相似时，检测更容易；不相似时性能显著下降。基于3个公开数据集标注颜色伪影并构造换色反事实，评测7个基准下40种方法，发现多数方法在“伪影≠ROI”时性能大幅下滑，并以Mahalanobis为例展现31.5% AUROC差异。


<details>
  <summary>Details</summary>
Motivation: 尽管OOD检测被广泛用于识别训练分布外输入，但不同伪影类型（如标注墨迹颜色）会导致检测性能差异，背后成因未被系统解释。作者怀疑模型学到的ROI特征会与伪影的视觉相似性互动，从而系统性偏置OOD检测。

Method: 1) 在3个皮肤病变等公开数据集上对11,355张图像的伪影按颜色细标注；2) 生成颜色互换的反事实样本以排除数据集偏置；3) 在7个基准上系统评测40种主流OOD方法（含Mahalanobis等）在“伪影颜色与ROI相似vs不相似”条件下的差异；4) 以具体分类器（红色ROI）做案例分析。

Result: 观测到稳定的“隐形大猩猩效应”：当伪影颜色与ROI相似时，近OOD更易被检测；不相似时大幅退化。以皮肤病变分类为例，Mahalanobis分数检测红色墨迹的AUROC比黑色墨迹高31.5%。跨40方法与7基准，绝大多数方法在“伪影≠ROI”条件下显著掉点；反事实实验支持该效应非数据集混杂所致。

Conclusion: OOD检测存在被忽视的系统性偏差：与模型ROI相似的伪影更容易被标为OOD，反之则更难。应在基准与方法设计中显式考虑ROI-伪影相似性，开发对伪影属性变化更鲁棒的检测器；提供的代码与标注可支持后续研究复现与改进。

Abstract: Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.

</details>


### [201] [SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis](https://arxiv.org/abs/2602.20079)
*Xinya Chen,Christopher Wewer,Jiahao Xie,Xinting Hu,Jan Eric Lenssen*

Main category: cs.CV

TL;DR: SemanticNVS 是一种相机条件的多视角扩散式新视角合成模型，通过引入预训练的语义特征提取器，在长距离视角变化下显著提升生成质量与跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有 NVS 方法在接近输入视角时表现良好，但在大幅相机位移时会出现语义不合理与几何畸变，显示模型对条件与中间生成内容“理解不足”。作者希望用更强的场景语义作为条件，缓解远视角退化问题。

Method: 构建相机条件的多视角扩散模型，并融合预训练语义特征作为条件。提出两种策略：（1）将源视图的语义特征进行几何变换/投影得到“warped”语义特征以对齐目标视角；（2）在扩散去噪的每一步交替执行“理解（语义编码）”与“生成”，即在迭代中显式注入/更新语义约束，以强化跨步一致性与全局理解。

Result: 在多个数据集上，相较 SOTA 方法，定性与定量均有明显提升，FID 降低约 4.69%—15.26%，在远距离视角生成的语义合理性与结构一致性更好。

Conclusion: 将预训练语义特征整合进多视角扩散式 NVS 能有效缓解长距离视角退化问题；两种策略均能提升质量与一致性，验证了“更强语义理解”对 NVS 的价值。

Abstract: We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.

</details>


### [202] [Do Large Language Models Understand Data Visualization Principles?](https://arxiv.org/abs/2602.20084)
*Martin Sinnona,Valentin Bonas,Viviana Siless,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: 论文系统评测LLMs与VLMs能否直接依据可视化原则进行“检查与修复”，构建带硬性真值的基准（ASP生成约2k带违规标注的Vega-Lite规格+300余真实图表），比较其与符号求解器。结果：模型有潜力但仍落后于符号方法，且更擅长修复而非检测。


<details>
  <summary>Details</summary>
Motivation: 现有可视化原则源自设计与知觉研究，通常以约束/逻辑规则在工具中实现，但将自然语言原则形式化为符号规范需要专家技能、成本高。希望利用LLM/VLM直接从自然语言与图表/规格推理原则、执行验证与编辑，避免显式规则编写，并用系统性评测量化其可行性与局限。

Method: - 将一组可视化原则表述为自然语言；
- 用Answer Set Programming生成带“硬验证”真值的受控数据集：约2,000个Vega-Lite规格，并对每个注入明确的原则违规；另收集300+真实Vega-Lite图表；
- 设定两类任务：检测（判断是否违反何种原则）与修复（修改规格以消除违规）；
- 评测多种LLM与VLM在文本与图像/规格输入下的表现，并与基于符号规则的求解器作对比分析。

Result: - LLM/VLM能作为灵活的可视化设计验证与编辑器，表现出一定泛化与可用性；
- 在细粒度、依赖视觉知觉细节的规则上，性能显著落后于符号求解器；
- 出现“非对称性”：前沿模型在修复违规方面往往优于在稳定、准确地检测违规。

Conclusion: LLM/VLM为无须显式规则的原则检查与修复提供了可行路径，但仍难以替代符号方法在微妙知觉规则上的精确性。未来可探索混合范式：用模型进行候选修复/解释、由符号求解器做最终验证，或以ASP生成数据对模型进行对齐与训练，以缩小检测与感知细节上的差距。

Abstract: Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.

</details>


### [203] [StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues](https://arxiv.org/abs/2602.20089)
*Zanxi Ruan,Qiuyu Kong,Songqun Gao,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: StructXLIP通过引入“结构中心”的跨模态对齐，将图像边缘图与过滤后的结构文本对齐，并加入局部区域与彩色图像的约束，在微调阶段提升跨模态检索与对齐鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP式方法在长而细节丰富的描述上易受非结构性噪声干扰，导致视觉-语言对齐不稳定；而早期视觉研究表明边缘/结构是稳健线索，因此希望显式抽取并对齐跨模态结构表征以提升检索性能与泛化。

Method: 提出StructXLIP：1) 以Canny等算法提取图像边缘图作为视觉结构代理；2) 将对应文本过滤为突出结构信息的“结构中心”描述；3) 在标准对齐损失上增添三类结构损失：(i) 边缘图-结构文本全局对齐；(ii) 局部边缘区域与文本块的局部匹配；(iii) 边缘图与彩色图对齐，防止表示漂移。并从信息论角度在标准CLIP最大化I(V,T)的基础上，额外最大化结构层面的互信息I(V_s,T_s)。

Result: 在通用与专业域的跨模态检索任务上优于现有方法；对不同方法可即插即用地带来性能提升。代码与模型已开源。

Conclusion: 显式建模与对齐跨模态“结构”可在长文本场景下提供更稳健、语义更稳定的最优点，提升视觉-语言对齐与检索表现；该范式通用、可与未来方法无缝结合。

Abstract: Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them "structure-centric". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.

</details>


### [204] [Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine](https://arxiv.org/abs/2602.20100)
*Soumick Chatterjee*

Main category: cs.CV

TL;DR: 论文综述“无标注学习”在生物医学中的崛起：利用未标注的大规模影像与基因数据，通过无监督与自监督方法挖掘表型、连接形态与遗传、并无偏发现异常，部分任务已达到或超过监督学习表现。


<details>
  <summary>Details</summary>
Motivation: 临床AI长期受限于昂贵稀缺的专家标注，难以充分利用生物银行规模数据。需要能够直接从数据内在结构中学习的范式，降低对标注依赖、提升可扩展性与发现新知识的能力。

Method: 综述与综合：系统回顾无监督与自监督学习在多模态生物医学数据（MRI像素、体素、基因序列token、组织学切片）上的方法与应用，展示如何通过表征学习、对比学习、掩码重建、聚类/表型提取、异常检测等框架，发掘可遗传心脏特征、预测空间基因表达、及无偏病灶检测。

Result: 证据显示：1）从影像表征中提取的心脏结构与功能表型可证明其遗传性；2）在组织学上以SSL/无监督模型预测空间转录表达取得高准确度；3）异常检测与病理筛查在多任务上达到或超越监督模型性能。

Conclusion: “无标签学习”已成为生物医学AI的新范式：能规模化利用未标注数据、减少人为偏倚、发现新表型并链接形态与遗传。前景广阔，但仍需在临床泛化、可解释性、数据与伦理治理上继续完善。

Abstract: The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in "learning without labels," highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.

</details>


### [205] [Benchmarking Unlearning for Vision Transformers](https://arxiv.org/abs/2602.20114)
*Kairan Zhao,Iurie Luca,Peter Triantafillou*

Main category: cs.CV

TL;DR: 本文首次针对视觉Transformer（ViT、Swin-T）开展系统化机器反学习（MU）基准评测，跨模型容量、数据规模与复杂度、单次/持续卸载协议，统一度量遗忘质量与保留/测试精度，给出现有算法在VT场景的参考基线。


<details>
  <summary>Details</summary>
Motivation: MU被视为构建安全、公平AI的关键能力，但视觉领域MU研究几乎只聚焦CNN；同时VT在视觉任务中崛起，却缺乏针对VT的MU基准，导致算法在VT上的有效性与可比性不明。

Method: 构建基准套件：选择两类VT家族（ViT与Swin-T）及多种容量；覆盖多数据集以考察规模与复杂度影响；纳入代表不同范式的MU算法，特别聚焦“利用训练记忆”范式；设计单次与持续卸载协议；对比VT与CNN的记忆行为并评估多种记忆代理指标；采用统一指标，衡量两种互补的遗忘质量以及对未见测试集与保留数据的准确率。

Result: 给出系统化对比：揭示VT相较CNN的记忆特征与不同记忆代理对MU性能的影响；量化各MU算法在不同VT与设置下的遗忘质量与精度权衡；形成可复现实验与公平比较的结果集，建立首个VT场景MU性能参考基线。

Conclusion: 该基准为未来在视觉Transformer上的MU研究提供统一、可复现、全面的比较框架，并首次明确现有MU算法在VT中的适用性与局限，推动方法改进与新算法设计。

Abstract: Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.

</details>


### [206] [Do Large Language Models Understand Data Visualization Rules?](https://arxiv.org/abs/2602.20137)
*Martin Sinnona,Valentin Bonas,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 论文评估大模型能否直接根据可视化规则检测图表违规。用从Draco约束与ASP推导的硬验证真值，构建含显式违规标注的2,000个Vega-Lite规格数据集，比较准确率与格式遵循度。前沿模型对常见违规检测较准、格式遵循高，但在细微感知规则与从ASP直接表述时性能显著下降；将符号约束转写为自然语言能大幅提升小模型表现。


<details>
  <summary>Details</summary>
Motivation: 符号约束系统（如Draco）能精确检查可视化规则，但维护成本高且需专家知识；已有工作表明LLM能生成图或识别误导图，但不清楚其是否能直接依据规则进行约束性验证。需要系统性基准来量化LLM在可视化规则校验中的潜力与边界。

Method: 将Draco部分约束转写为自然语言，并用Answer Set Programming生成带有明确规则违规标注的受控数据集（2,000个Vega-Lite规范）。以“违规检测准确性（如F1）”与“提示/输出格式遵循度”两维评估多种LLM；同时比较从技术性ASP表述与自然语言表述输入下的性能差异。

Result: - 格式遵循度：Gemma 3 4B/27B达100%，GPT-oss 20B为98%。- 常见规则违规检测可靠（F1最高约0.82）。- 对细微的感知类规则，多类F1低于0.15。- 直接基于ASP技术表述时性能下滑。- 将约束转写为自然语言可使小模型性能最高提升约150%。

Conclusion: LLM可作为灵活的“以语言驱动”的可视化规则验证器，在通用与显性违规上效果可观，但相较符号求解器在细粒度感知规则上仍显不足。混合策略（规则自然语言化、与符号约束协同）可能是务实路径；未来需改进对感知规则的理解与对技术表述的鲁棒性。

Abstract: Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.

</details>


### [207] [Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning](https://arxiv.org/abs/2602.20157)
*Zhongxiao Cong,Qitao Zhao,Minsik Jeon,Shubham Tulsiani*

Main category: cs.CV

TL;DR: Flow3r用密集2D光流对应作为监督，替代昂贵的3D几何与位姿标注，从无标注单目视频可扩展训练，并在多基准上达SOTA，尤其在动态场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D/4D重建依赖密集几何与位姿监督，获取成本高且在真实动态场景稀缺；需要一种能利用海量无标注视频的监督替代信号以学习几何与相机运动，并能适配动态场景。

Method: 提出Flow3r：引入“分解式”光流预测，将两帧间光流的预测因子化为：使用一帧的几何潜变量和另一帧的位姿潜变量来预测对应光流。以密集2D对应作为训练监督，集成到现有视觉几何架构中，从无标注单目视频端到端学习几何与相机运动；在约80万条无标注视频上训练，并扩展到动态场景。

Result: 受控实验表明分解式光流优于其他设计，且性能随无标注数据规模稳定提升。将该机制集成到多种架构后，在横跨静态与动态场景的8个基准上达成SOTA，尤其在缺标注的野外动态视频上取得最大增益。

Conclusion: 密集2D对应作为监督、结合分解式光流预测，可在无标注单目视频上有效学习几何与位姿，实现可扩展的3D/4D重建，并在多基准上刷新性能，特别提升动态场景表现。

Abstract: Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.

</details>


### [208] [A Very Big Video Reasoning Suite](https://arxiv.org/abs/2602.20159)
*Maijunxian Wang,Ruisi Wang,Juyi Lin,Ran Ji,Thaddäus Wiedemer,Qingying Gao,Dezhi Luo,Yaoyao Qian,Lianyu Huang,Zelong Hong,Jiahui Ge,Qianli Ma,Hang He,Yifan Zhou,Lingzi Guo,Lantao Mei,Jiachen Li,Hanwen Xing,Tianqi Zhao,Fengyuan Yu,Weihang Xiao,Yizheng Jiao,Jianheng Hou,Danyang Zhang,Pengcheng Xu,Boyang Zhong,Zehong Zhao,Gaoyun Fang,John Kitaoka,Yile Xu,Hua Xu,Kenton Blacutt,Tin Nguyen,Siyuan Song,Haoran Sun,Shaoyue Wen,Linyang He,Runming Wang,Yanzhi Wang,Mengyue Yang,Ziqiao Ma,Raphaël Millière,Freda Shi,Nuno Vasconcelos,Daniel Khashabi,Alan Yuille,Yilun Du,Ziming Liu,Bo Li,Dahua Lin,Ziwei Liu,Vikash Kumar,Yijiang Li,Lei Yang,Zhongang Cai,Hokin Deng*

Main category: cs.CV

TL;DR: 论文提出VBVR超大规模视频推理数据集与评测基准，涵盖200类推理任务与逾百万视频，配合可验证、可复现实的规则化评分器，用于系统研究视频推理的扩展性，并观察到对未见任务的早期涌现泛化迹象。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型更多追求视觉质量，推理能力研究不足；缺乏大规模、系统化的视频推理训练与评测数据，限制了对视频推理与规模化效应的系统研究。

Method: 1) 构建VBVR数据集：依据系统化分类法策划200类视频推理任务，规模达百万级视频片段；2) 提出VBVR-Bench评测框架：采用可验证、与人类一致的规则化评分器，减少对模型打分器的依赖，提高可解释性与可复现性；3) 基于VBVR开展大规模扩展性实验，评估不同模型在多种推理类型与未见任务上的表现。

Result: 在VBVR上进行的扩展性研究显示，随着规模提升，模型在多类视频推理任务上性能稳步提升，并出现对未见推理任务的早期涌现式泛化迹象；评测框架实现了稳定、可验证、可解释的诊断。

Conclusion: VBVR提供了前所未有规模与覆盖度的视频推理训练与评测资源，为通用化视频推理研究奠定基础，并通过公开数据、工具与模型促进可复现研究。

Abstract: Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .

</details>


### [209] [tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction](https://arxiv.org/abs/2602.20160)
*Chen Wang,Hao Tan,Wang Yifan,Zhiqin Chen,Yuheng Liu,Kalyan Sunkavalli,Sai Bi,Lingjie Liu,Yiwei Hu*

Main category: cs.CV

TL;DR: 提出tttLRM：在大规模3D重建中引入测试时训练(TTT)层，实现长上下文、自回归重建，计算复杂度线性；将多视图压缩为潜空间“快权重”，可解码为高斯球(Gaussian Splat)等显式表示；可在线渐进重建；用NVS预训练迁移到显式3D，带来更好质量与更快收敛；在物体与场景上优于SOTA的前馈3D高斯重建。


<details>
  <summary>Details</summary>
Motivation: 现有大模型式3D重建受限于计算与上下文长度，难以在多视图/流式场景中高效融合长期信息并产出可用于下游的显式3D表示。需要一种既能线性扩展、多帧自回归，又能快速从多视图中构建紧凑隐式表征并解码为显式格式的方法。

Method: 在模型中插入TTT层作为“快权重”存储器：将多张图像的观察通过在线/自回归方式编码到TTT层，实现长上下文线性复杂度聚合，形成潜在隐式3D表示；随后以解码器将该表示转为多种显式3D形式（尤其是Gaussian Splats）。提供在线学习变体以支持流式输入的渐进重建与细化。以新视角合成任务进行预训练，使能力迁移到显式3D建模。

Result: 在物体与场景基准上，前馈式3D Gaussian重建的质量与速度均优于当前SOTA；预训练带来更快收敛与更高重建质量；模型可在流式输入下持续改进重建。

Conclusion: 通过TTT层将长上下文多视图高效压缩为可解码的潜在3D表征，tttLRM实现了可扩展、在线、性能领先的显式3D重建，为从NVS到显式3D的迁移提供有效路径。

Abstract: We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.

</details>


### [210] [Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device](https://arxiv.org/abs/2602.20161)
*Abdelrahman Shaker,Ahmed Heakl,Jaseel Muhammad,Ritesh Thawkar,Omkar Thawakar,Senmao Li,Hisham Cholakkal,Ian Reid,Eric P. Xing,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: Mobile-O 是一款可在移动端实时运行的统一多模态（理解+生成）模型，通过轻量的MCP模块高效融合视觉-语言与扩散生成，仅用数百万样本与“四元组”后训练即可达到与更大模型相当甚至更优的性能，并在iPhone上实现约3秒/512×512的生成。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型参数大、训练和推理成本高，不适合边缘设备；需要一种在移动端即可同时完成视觉理解与图像生成的实用方案，降低数据与计算开销，实现端侧实时性与隐私友好。

Method: 提出Mobile Conditioning Projector（MCP）：用深度可分离卷积与分层对齐将视觉-语言特征高效注入扩散生成器，实现低开销跨模态条件控制；训练采用少量数据并进行“四元组”（生成提示、图像、问题、答案）后训练，联合提升理解与生成；整体为紧凑的视觉-语言-扩散统一架构，可在手机上高效运行。

Result: 在生成上，GenEval得分74%，较Show-O与JanusFlow分别高5%与11%，推理速度快6×与11×；在理解任务上，平均超过二者15.3%与5.1%；在iPhone上对512×512图像约3秒/张，满足实时交互需求。

Conclusion: Mobile-O以轻量MCP和“四元组”后训练实现端侧统一多模态理解与生成的实用突破，在有限数据与算力下仍具竞争力，首次在移动端提供接近实时的统一多模态框架，并开放代码与模型，促进无云依赖的在端智能研究。

Abstract: Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/

</details>
