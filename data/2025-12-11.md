<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 113]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn,Yura Choi,Hyeonbeom Choi,Seongwon Cho,San Kim,Jonghyun Choi*

Main category: cs.CV

TL;DR: 论文发现许多视频多模态大模型在帧顺序被打乱时仍能在现有基准上取得高分，说明其并未真正理解事件时序。据此提出专测时序理解的基准VECTOR，并以多事件指令微调结合思维链推理（MECOT）提升时序感知，显著优于以往方法，同时也提升了传统基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准可能被“常识先验+语境模式匹配”投机解题，难以区分模型是否真正依赖时间顺序信息。需要一个能明确评测多事件时序理解的新基准，并探索训练与推理策略来增强模型对事件顺序的感知。

Method: 1) 构建VECTOR基准：面向多事件视频，明确评测事件的先后次序识别能力。2) 诊断实验：将视频帧打乱，观察主流VLMM在现有基准上的鲁棒高分现象。3) 提出MECOT：a) 多事件、逐事件的细粒度描述用于指令微调；b) 推理时加入思维链提示，鼓励按事件顺序逐步推理。

Result: 在VECTOR上，多种VLMM表现不佳，表明其时序理解薄弱；采用MECOT后，在VECTOR上显著提升并超过现有方法，同时在传统视频基准上也有增益。

Conclusion: 现有VLMM常倚赖先验而非真实时序建模。VECTOR有效暴露了时序理解缺陷；MECOT通过多事件指令微调与思维链推理强化了时序意识，提升了新老基准的整体表现。

Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.

</details>


### [2] [Training Multi-Image Vision Agents via End2End Reinforcement Learning](https://arxiv.org/abs/2512.08980)
*Chengqi Dong,Chuhuai Yue,Hang He,Rongge Mao,Fenghe Tang,S Kevin Zhou,Zekun Xu,Xiaohan Wang,Jiajun Chai,Wei Lin,Guojun Yin*

Main category: cs.CV

TL;DR: IMAgent 是一个面向多图像复杂任务的开源视觉智能体，采用端到端强化学习训练，结合多代理生成的数据集与视觉反思/确认工具，在不依赖昂贵监督微调的情况下实现稳定工具使用，在多图像QA上显著提升，同时保持单图任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有仿O3“带图思考”的开源方法多仅支持单图输入，难以应对真实场景中的多图问答；此外，随着推理深度增加，VLM容易忽视视觉证据，导致工具使用与视觉对齐不足。

Method: 1) 多代理系统自动生成高难度、视觉信息丰富的多图QA数据，并人工校验得到MIFG-QA（约1万样本）；2) 设计两类工具：视觉反思与视觉确认，引导模型在推理过程中主动回看图像；3) 提出“动作-轨迹”双层掩码策略，使得纯RL训练即可学到稳定的工具使用策略，无需SFT数据。

Result: IMAgent在保持现有单图基准性能的同时，在新提出的多图数据集上取得大幅提升；实验与消融显示视觉反思/确认工具与双层掩码对稳定工具使用与视觉对齐至关重要。

Conclusion: 通过面向多图任务的端到端RL、专用视觉工具与数据构建方案，IMAgent能有效激活VLM的工具使用能力，缓解深推理时的“忽视视觉”问题，为多图视觉推理与智能体研究提供可复现、可扩展的路径。

Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.

</details>


### [3] [Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding](https://arxiv.org/abs/2512.08981)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出UTIE，通过将他群体的文本人口特征注入人脸嵌入，诱导人口属性模糊化，从而减轻FR中的群体偏见，并在RFW与BFW上以CLIP/OpenCLIP/SigLIP验证，降低偏差同时不降甚至提升准确率。


<details>
  <summary>Details</summary>
Motivation: FR在多元城市和智慧城市应用中存在显著的人口统计偏见，原因是身份特征与人口属性在嵌入空间纠缠，导致群体间验证性能不公。需要方法在不牺牲准确率的前提下解纠缠并提升公平性。

Method: 利用视觉-语言模型的零样本与跨模态对齐能力，将每个群体的人脸嵌入与来自其他群体的文本人口特征进行融合，促使嵌入在人口属性上更中性、强调身份相关信息；在CLIP、OpenCLIP、SigLIP框架下实现并评估。

Result: 在RFW与BFW偏差评测基准上，UTIE稳定降低偏见指标，同时保持或在若干场景提升人脸验证准确率。

Conclusion: 通过跨模态文本特征注入实现的人口属性模糊化，可在不损失甚至提升识别性能的同时显著减轻FR偏见；VLM作为公平性增强器在FR中具有广泛适用性。

Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.

</details>


### [4] [Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement](https://arxiv.org/abs/2512.08982)
*Jian Xu,Wei Chen,Shigui Li,Delu Zeng,John Paisley,Qibin Zhao*

Main category: cs.CV

TL;DR: 提出Consist-Retinex，将一致性模型扩展到Retinex低照度增强，实现单步推理并达SOTA，训练成本仅为基线的1/8。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在低照增强上效果好但采样步数多，难以部署；一致性模型虽可单步生成，但主要用于无条件生成，尚未解决有条件增强的需求与训练动力学差异。

Method: 基于Retinex的条件增强框架，引入两项创新：(1) 双目标一致性损失：在随机时间采样下，将时间一致性与与真值对齐结合，覆盖全噪声谱域以稳定收敛；(2) 自适应重噪强调采样：在训练中优先大型噪声区域，强化从退化输入到增强输出的一步映射能力。

Result: 在VE-LOL-L数据集上，单步采样即达SOTA：PSNR 25.51（对比Diff-Retinex++的23.41）、FID 44.73（优于49.59）；且相较1000步的Diff-Retinex训练预算仅需1/8。

Conclusion: 一致性建模可有效扩展到条件低照增强。通过双目标损失与强调大噪声的采样策略，实现稳定的一步增强，兼顾精度与效率。

Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.

</details>


### [5] [HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification](https://arxiv.org/abs/2512.08983)
*Maoyu Wang,Yao Lu,Bo Zhou,Zhuangzhi Chen,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: 提出HSCP分层谱聚类剪枝框架，在RFFI任务上同时进行层与通道剪枝，实现极致压缩、加速且提准；在ResNet18上参数与FLOPs均降约85%，精度反升1.49%，并在低信噪比下保持鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有无人机低空安防中，传统识别难以在复杂电磁环境下稳定、实时地提取射频指纹；深度学习虽提升准确率，但模型大、算力需求高，不利于边缘端部署。通用剪枝方法（权重/通道/层）难以同时兼顾压缩率、硬件加速友好与精度保持，因此需要新的联合剪枝策略。

Method: 提出HSCP：1) 使用基于CKA相似度度量的谱聚类先在“层”维度聚类，识别冗余层并删除；2) 以同样策略在“通道”维度做细粒度剪枝；3) 采用噪声鲁棒微调策略，提升在低SNR环境下的泛化与稳健性。整体实现层-通道的分层联合剪枝与后续微调。

Result: 在UAV-M100基准上，HSCP优于现有通道/层剪枝方法：对ResNet18参数减少86.39%、FLOPs减少84.44%，相较未剪枝基线准确率提升1.49%，并在低信噪比下保持更强鲁棒性。

Conclusion: 分层谱聚类+CKA指导的联合层/通道剪枝能在RFFI任务上实现高压缩、高加速且精度不降反升，并具备噪声鲁棒性，适合资源受限边缘设备部署。

Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.

</details>


### [6] [RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition](https://arxiv.org/abs/2512.08984)
*Nirhoshan Sivaroopan,Hansi Karunarathna,Chamara Madarasingha,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.CV

TL;DR: 提出RAG-HAR：一种无需训练、基于LLM的检索增强人类活动识别框架，利用轻量统计特征+向量库检索+提示优化与活动描述生成，实现跨6个基准的SOTA并能识别未见活动。


<details>
  <summary>Details</summary>
Motivation: 现有HAR深度学习方法依赖大量标注、特定数据集训练与高算力，泛化差、部署成本高；需要一种无需训练、可跨数据集泛化、能处理未见类且资源友好的方案。

Method: 1) 从传感数据提取轻量统计描述符；2) 在向量数据库中检索语义相似样本作为上下文；3) 利用LLM在检索证据基础上进行活动识别；4) 通过提示优化与LLM生成的“活动描述符”构建更丰富的向量库以提升检索相关性与准确性；整体为训练免疫（无模型训练/微调）。

Result: 在六个多样HAR基准上达到SOTA性能；在无需训练/微调条件下仍显著优于现有方法，并能对多种未见活动进行识别与有意义的标注。

Conclusion: RAG-HAR证明了检索增强+LLM可在无训练条件下实现强大的HAR性能与泛化，降低数据与算力需求，并扩展到未见活动识别，具备实际可用性与鲁棒性。

Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.

</details>


### [7] [An Efficient Test-Time Scaling Approach for Image Generation](https://arxiv.org/abs/2512.08985)
*Vignesh Sundaresha,Akash Haridas,Vikram Appia,Lav Varshney*

Main category: cs.CV

TL;DR: 提出Verifier-Threshold方法，在扩散/流模型的测试时搜索噪声样本时自适应分配推理计算，达到相同性能下较SOTA快2-4倍。


<details>
  <summary>Details</summary>
Motivation: 图像生成中，增加测试时计算（如搜索噪声、推理推敲）可显著提升质量，但现有方法对各去噪步的计算分配依赖贪心策略，效率低、分配不均。需要一种更优的、自动化的计算预算重分配方法。

Method: 引入Verifier-Threshold机制：在扩散/流模型的多步去噪过程中，结合一个验证器（评估候选样本或中间状态的质量/进展）和阈值策略，动态决定在哪些步和哪些噪声样本上投入更多搜索与采样计算，从而在固定总体预算下自适应重分配测试时计算。与以往固定或贪心分配不同，该方法全局考虑预算并按验证信号触发追加计算或早停。

Result: 在GenEval基准上，在保持与现有最先进方法相同的性能指标下，推理计算时间减少2-4倍。

Conclusion: 通过Verifier-Threshold的自适应预算重分配，测试时计算被更有效地用在关键去噪步骤与更有潜力的候选上，实现显著效率提升且不损失生成质量。

Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.

</details>


### [8] [Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy](https://arxiv.org/abs/2512.08986)
*Anca Mihai,Adrian Groza*

Main category: cs.CV

TL;DR: 该论文提出一套用于糖尿病视网膜病变数据质量控制的流程：先用可解释特征分类器过滤低质眼底图，再对图像增强并在深度学习辅助下标注，最后以新公式计算多标注者一致性决定标注可用性。


<details>
  <summary>Details</summary>
Motivation: DR 诊断依赖高质量、可靠标注的数据，但眼底图像易受采集质量与解剖复杂性影响，人工标注主观且易错，直接用于训练会降低AI性能与可解释性，故需要完整的质控机制保障训练与评估数据标准化。

Method: 三阶段框架：1）质量筛选：构建可解释的基于特征的分类器，特征来自传统图像处理与对比学习表征，剔除不合格图像；2）图像增强与辅助标注：对保留图像进行增强，并借助深度学习辅助标注，提高标注效率与一致性；3）一致性评估：基于推导公式计算多标注者一致性，用于判定标注是否可用于训练与评测。

Result: 框架可在数据流入训练前过滤低质样本，并在标注阶段通过DL辅助与一致性度量提高标注可靠性；整体提升用于DR分类/分级的训练数据质量与可用性。

Conclusion: 通过端到端质控（质量过滤—增强与辅助标注—一致性判定），可构建高标准DR数据集，降低人工误差对AI模型的影响，为临床可用的DR自动分析提供更稳健的数据基础。

Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.

</details>


### [9] [3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization](https://arxiv.org/abs/2512.08987)
*Yuze Hao,Linchao Zhu,Yi Yang*

Main category: cs.CV

TL;DR: 提出3DID框架：用连续潜空间统一物理-几何表征，并通过两阶段物理感知优化（梯度引导扩散探索+目标驱动且保持拓扑的细化）直接在3D体素/隐式空间设计，从零生成高保真3D形状，优于现有2D投影或微调3D方法。


<details>
  <summary>Details</summary>
Motivation: 3D逆向设计空间巨大，传统网格穷举不可行；现有深度学习方法多依赖2D投影或对现有3D形状微调，牺牲体积细节、限制探索，难以“从零开始”的真3D设计。

Method: 1) 学习统一的物理-几何连续潜表示，将形状与物理场嵌入同一潜空间；2) 物理感知优化两阶段：阶段一用梯度引导的扩散采样在潜流形进行全局探索；阶段二进行面向目标、保持拓扑的一致性细化，将候选进一步雕刻至目标。

Result: 在生成质量与设计多样性上均优于以2D投影或微调为主的现有方法，能产生高保真3D几何；实验表明更好的解质量与更广的设计空间覆盖。

Conclusion: 通过连续潜空间与物理感知两阶段优化，3DID实现真正的三维从零逆向设计，兼顾全局探索与局部精修，提升解的质量与多样性，突破现有方法对体积细节与设计自由度的限制。

Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.

</details>


### [10] [Enhancing Knowledge Transfer in Hyperspectral Image Classification via Cross-scene Knowledge Integration](https://arxiv.org/abs/2512.08989)
*Lu Huo,Wenjian Huang,Jianguo Zhang,Min Xu,Haimin Zhang*

Main category: cs.CV

TL;DR: 提出CKI框架以在完全异构跨场景HSI分类中实现知识迁移：通过ASC做光谱对齐、CKSP（含SSM）解决语义不匹配、CII整合目标私有补充信息；在多种跨场景设置上取得SOTA且稳定。


<details>
  <summary>Details</summary>
Motivation: 跨域HSI分类常受两大难题制约：不同传感器引起的光谱差异与异构场景造成的语义不一致。现有方法多假设同质域或仅有重叠类别的异构情形；当标签空间不重叠时，又依赖完整的源域覆盖，忽视目标域的私有信息，导致迁移受限。

Method: 提出Cross-scene Knowledge Integration（CKI）框架，显式纳入目标私有知识，包含三部分：1) ASC（Alignment of Spectral Characteristics）：采用与域无关的投影减少光谱差异；2) CKSP（Cross-scene Knowledge Sharing Preference）：通过源相似度机制SSM来度量并选择可共享的源知识，缓解语义错配；3) CII（Complementary Information Integration）：在迁移过程中挖掘并整合目标域的特定补充线索，最大化利用目标私有信息。

Result: 在多种跨场景HSI场景与设置上，CKI取得了当前最优（SOTA）的分类性能，并展现出强鲁棒性与稳定性。

Conclusion: 在完全异构（包含非重叠标签空间）条件下，通过光谱对齐、语义筛选与目标补充信息整合，CKI有效推进HSI跨场景知识迁移，克服传统方法对类别重叠与源域完全覆盖的依赖。

Abstract: Knowledge transfer has strong potential to improve hyperspectral image (HSI) classification, yet two inherent challenges fundamentally restrict effective cross-domain transfer: spectral variations caused by different sensors and semantic inconsistencies across heterogeneous scenes. Existing methods are limited by transfer settings that assume homogeneous domains or heterogeneous scenarios with only co-occurring categories. When label spaces do not overlap, they further rely on complete source-domain coverage and therefore overlook critical target-private information. To overcome these limitations and enable knowledge transfer in fully heterogeneous settings, we propose Cross-scene Knowledge Integration (CKI), a framework that explicitly incorporates target-private knowledge during transfer. CKI includes: (1) Alignment of Spectral Characteristics (ASC) to reduce spectral discrepancies through domain-agnostic projection; (2) Cross-scene Knowledge Sharing Preference (CKSP), which resolves semantic mismatch via a Source Similarity Mechanism (SSM); and (3) Complementary Information Integration (CII) to maximize the use of target-specific complementary cues. Extensive experiments verify that CKI achieves state-of-the-art performance with strong stability across diverse cross-scene HSI scenarios.

</details>


### [11] [Deterministic World Models for Verification of Closed-loop Vision-based Systems](https://arxiv.org/abs/2512.08991)
*Yuang Geng,Zhuoyang Zhou,Zhongzheng Zhang,Siyuan Pan,Hoang-Dung Tran,Ivan Ruchkin*

Main category: cs.CV

TL;DR: 提出确定性世界模型（DWM）替代含随机潜变量的生成相机模型，用于视觉闭环控制系统的可验证性；结合StarV可达域分析与保序（保覆盖）置信界，获得更紧的可达集与更强验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有以生成模型充当相机的验证流程依赖随机潜变量，导致输入不确定性被过度放大，带来过宽可达集与保守验证结论；高维图像和难以刻画的视觉环境使闭环验证本已困难，需要一种既可生成图像又便于确定性边界传播的替代。

Method: 提出确定性世界模型DWM：直接由系统状态生成图像，去除潜变量；训练时使用双目标损失=像素重建损失+控制差异损失（保持与真实系统的控制行为一致）。将DWM嵌入基于Star集合的可达性分析（StarV）管线中，实现端到端确定性边界传播；并引入保序/一致性推断（conformal prediction）以给出世界模型与真实系统轨迹偏差的统计置信界。

Result: 在标准基准上，相比含潜变量的生成模型基线，DWM产生显著更紧的可达集与更好的验证通过率/性能；验证结果更少过近似，精度与有效性提升。

Conclusion: 去除潜变量并用DWM进行确定性映射使视觉闭环系统验证更精确；结合StarV和保守统计界限，可在实践中提供既严谨又不保守的可达性与安全验证。

Abstract: Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.

</details>


### [12] [Demo: Generative AI helps Radiotherapy Planning with User Preference](https://arxiv.org/abs/2512.08996)
*Riqiang Gao,Simon Arberet,Martin Kraus,Han Liu,Wilko FAR Verbakel,Dorin Comaniciu,Florin-Cristian Ghesu,Ali Kamen*

Main category: cs.CV

TL;DR: 提出一种基于用户偏好“风味”的生成式模型，直接预测3D放疗剂量分布，不依赖参考计划，能在一定场景下优于RapidPlan，提升适应性与计划质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D剂量预测多以既有参考计划为真值训练，易受机构和个人规划风格偏置影响，限制跨机构泛化与个体化需求。希望提供可定制、可迁移、能体现临床权衡偏好的方法。

Method: 构建可条件生成的模型，以用户定义的偏好向量（对OAR与PTV权衡的“风味”）作为条件输入，预测完整三维剂量分布；模型可与临床TPS无缝集成，支持交互式调整偏好以生成不同方案；与Varian RapidPlan进行对比评估。

Result: 在多种评估场景中，模型展现出较高的自适应性与计划质量；在部分任务上超过RapidPlan。

Conclusion: 通过以偏好为条件的生成式剂量预测，减少了对参考计划风格的依赖，实现个性化权衡与高效规划，具有临床集成潜力并在部分指标上优于现有商用系统。

Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.

</details>


### [13] [Diffusion Model Regularized Implicit Neural Representation for CT Metal Artifact Reduction](https://arxiv.org/abs/2512.08999)
*Jie Wen,Chenhe Du,Xiao Wang,Yuyao Zhang*

Main category: cs.CV

TL;DR: 提出一种将扩散模型先验与隐式神经表示相结合的CT金属伪影消除方法，兼顾物理一致性与数据保真，在模拟与临床数据上表现稳健且具泛化能力。


<details>
  <summary>Details</summary>
Motivation: 监督式MAR依赖有限的成对金属/无金属数据，导致在已知数据上也不稳定且临床适用性受限；无监督方法又存在两点不足：未有效融入CT几何与重建物理确保数据保真；依赖启发式正则项，难以充分表达丰富先验。

Method: 构建“扩散模型正则化的隐式神经表示”框架：用隐式神经表示（INR）建模图像/体素，并显式嵌入CT物理几何与投影一致性以保证数据保真；引入预训练扩散模型作为强先验，对INR解空间进行正则化，引导去伪影重建。

Result: 在模拟与真实临床数据上，与现有监督/无监督方法相比取得更佳的伪影抑制和结构保真，表现出更好的稳健性与泛化能力。

Conclusion: 将物理一致性（INR+投影约束）与学习先验（扩散模型）融合能有效解决金属伪影问题，具有临床应用潜力。

Abstract: Computed tomography (CT) images are often severely corrupted by artifacts in the presence of metals. Existing supervised metal artifact reduction (MAR) approaches suffer from performance instability on known data due to their reliance on limited paired metal-clean data, which limits their clinical applicability. Moreover, existing unsupervised methods face two main challenges: 1) the CT physical geometry is not effectively incorporated into the MAR process to ensure data fidelity; 2) traditional heuristics regularization terms cannot fully capture the abundant prior knowledge available. To overcome these shortcomings, we propose diffusion model regularized implicit neural representation framework for MAR. The implicit neural representation integrates physical constraints and imposes data fidelity, while the pre-trained diffusion model provides prior knowledge to regularize the solution. Experimental results on both simulated and clinical data demonstrate the effectiveness and generalization ability of our method, highlighting its potential to be applied to clinical settings.

</details>


### [14] [A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography](https://arxiv.org/abs/2512.09001)
*Yuehua Hu,Jiyeong Kong,Dong-yeol Shin,Jaekyun Kim,Kyung-Tae Kang*

Main category: cs.CV

TL;DR: 提出一种物理约束的合成缺陷数据生成管线，用DMD光刻制样并以缺陷/无缺陷对照生成像素级标注，构建包含四类缺陷的大规模数据集；基于该数据集，Mask R-CNN在AP@0.5上显著优于Faster R-CNN。


<details>
  <summary>Details</summary>
Motivation: 半导体微纳制造中的缺陷检测AI受限于现实厂内缺陷数据稀缺且难以开放共享，导致缺乏高质量、物理可信且像素级标注的数据集，从而制约了算法的训练与评估。

Method: 1) 从设计版图出发，采用受物理约束、可控的数学形态学操作（腐蚀/膨胀）在ab initio层面合成缺陷版图；2) 利用DMD高保真光刻将合成缺陷与无缺陷版图制成实际样品；3) 采集两者的光学显微图；4) 将缺陷样本与其无缺陷参考图配准对比，自动获得一致的像素级缺陷分割标注；5) 构建包含四类缺陷（桥连、毛刺、收缩/夹点、污染）的数据集。

Result: 得到3,530张显微图、13,365个实例、四类像素级分割标注；在该数据集上，Mask R-CNN的AP@0.5分别为桥0.980、毛刺0.965、夹点0.971、污染显著提升，较Faster R-CNN平均提升约34%，污染类提升约42%。

Conclusion: 通过物理约束的缺陷合成与DMD制样、对照成像实现稳健的像素级标注数据生成流程，显著提高分割检测模型性能，证明该方法可行并有望支持半导体制造中的AI量测/检测应用。

Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.

</details>


### [15] [A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques](https://arxiv.org/abs/2512.09005)
*Lownish Rai Sookha,Nikhil Pakhale,Mudasir Ganaie,Abhinav Dhall*

Main category: cs.CV

TL;DR: 该综述聚焦“从语音/语境/视觉线索生成面部与全身动作”，系统梳理表示、生成方法、数据集与评测，并指出在二人互动场景中提升逼真度、一致性与表现力的未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态与生成建模进步显著，但要生成既表现丰富又语义连贯的面身动作仍困难：口头/非口头线索相互作用复杂，且个体差异（人格、风格）显著，现有工作多只关注脸或身，缺少统一综述与系统化框架。

Method: 进行文献调研与分类综述：覆盖核心概念与任务定义、运动表示（面部与身体）、生成范式（如条件扩散、GAN、自回归、跨模态对齐）、训练与条件信号（语音、对话上下文、视觉线索）、以及数据集与评测指标；并对二人互动（dyadic）场景进行专章讨论。

Result: 给出对现有方法的系统梳理与比较，汇总数据资源与指标，提炼当前技术瓶颈（真实感、时序一致、跨模态对齐、个性化）与实践难点，并提供在线资源目录。

Conclusion: 该工作是首个同时覆盖面部与身体运动生成的全面综述，强调未来需在个性化建模、跨模态一致性、情感/语用对齐、长时程互动与更可靠的评测上发力，以提升虚拟形象在对话场景中的真实度与表现力。

Abstract: Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.

</details>


### [16] [Towards Lossless Ultimate Vision Token Compression for VLMs](https://arxiv.org/abs/2512.09010)
*Dehua Zheng,Mouxiao Huang,Borui Jiang,Hailin Hu,Xinghao Chen*

Main category: cs.CV

TL;DR: 提出LUVC框架，通过在视觉编码器侧的正交迭代合并与在LLM侧的频谱低通剪枝，逐层无损压缩视觉token，直至最终层完全消除，实现约2倍推理加速且精度几乎不降，并可训练免部署到多种VLM。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像/视频在VLM中产生大量冗余视觉token，导致计算开销与时延大。现有基于注意力/相似度的压缩存在位置偏置、类别不平衡、浅层跨模态弱而泛化差等问题，且难与现代高效注意力实现兼容。

Method: 1) 在视觉编码器端：提出沿空间轴正交的迭代token合并方案，将冗余视觉token在编码早期就合并以减少全链路计算；2) 在LLM端：引入无注意力/相似度的频谱低通滤波器作为谱剪枝单元，渐进式地剪除冗余视觉token，完全兼容FlashAttention；3) 组合成LUVC框架，使视觉token在层间逐步压缩并在最终层被消除，视觉信息融入多模态查询。方法训练免调，可直接部署。

Result: 在多种VLM上实现约2倍语言模型侧推理加速，且精度下降可忽略；方法无需额外训练即可即插即用。

Conclusion: LUVC通过编码端合并与LLM端频域剪枝的协同，使视觉token逐层无损压缩并最终消除，兼顾速度与精度，具备通用性与工程兼容性（如FlashAttention），为高分辨率多模态推理提供高效方案。

Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.

</details>


### [17] [An Approach for Detection of Entities in Dynamic Media Contents](https://arxiv.org/abs/2512.09011)
*Nzakiese Mbongo,Ngombo Armando*

Main category: cs.CV

TL;DR: 提出一种用深度学习在视频序列中搜索与检测特定人物的方法，利用监督学习的神经网络从简单特征出发实现人物定位，据称优于现有方法，并具安防应用价值（以安哥拉公共安保为例）。


<details>
  <summary>Details</summary>
Motivation: 在多目标复杂视频场景中，如何可靠地从海量图像/视频数据中检索并定位特定人物，对安防、失踪人口寻找等具有重要意义；作者希望利用深度学习改进传统方法的准确性与效率。

Method: 采用人工神经网络的深度学习框架进行监督学习，对目标人物的“简单特征”进行建模与分类，从视频帧中检测并定位该人物；与现有计算机视觉方法比较，强调网络结构化与监督学习流程。

Result: 实验结果表明，相比“state of the art”，所提方法在效率与检测成功率上取得若干“成功”，能在私有或公共图像库及视频序列中有效定位被检索的个人。

Conclusion: 该分类器能在实务中支持基于目标人员数据库与公共监控视频的身份检索，特别是在安哥拉国家安保系统（如CISP）中具有潜在应用价值。

Abstract: The notion of learning underlies almost every evolution of Intelligent Agents. In this paper, we present an approach for searching and detecting a given entity in a video sequence. Specifically, we study how the deep learning technique by artificial neuralnetworks allows us to detect a character in a video sequence. The technique of detecting a character in a video is a complex field of study, considering the multitude of objects present in the data under analysis. From the results obtained, we highlight the following, compared to state of the art: In our approach, within the field of Computer Vision, the structuring of supervised learning algorithms allowed us to achieve several successes from simple characteristics of the target character. Our results demonstrate that is new approach allows us to locate, in an efficient way, wanted individuals from a private or public image base. For the case of Angola, the classifier we propose opens the possibility of reinforcing the national security system based on the database of target individuals (disappeared, criminals, etc.) and the video sequences of the Integrated Public Security Centre (CISP).

</details>


### [18] [Learning to Remove Lens Flare in Event Camera](https://arxiv.org/abs/2512.09016)
*Haiqian Han,Lingdong Kong,Jianing Li,Ao Liang,Chengtao Zhu,Jiacheng Lyu,Lai Xing Ng,Xiangyang Ji,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: 提出E-Deflare：首个系统化去除事件相机镜头炫光的框架，含物理前向模型、模拟与真实配对基准、以及SOTA网络E-DeflareNet，验证在还原与下游任务中的显著提升。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽具高时域分辨率与高动态范围，但易受镜头炫光影响；在事件流中，炫光呈现复杂的时空畸变，长期被忽视，严重降低视觉感知与下游任务性能。

Method: 1) 推导基于物理的非线性抑制前向模型，刻画炫光对事件生成机制的影响；2) 构建基准E-Deflare Benchmark：包含大规模模拟训练集E-Flare-2.7K与首个真实配对测试集E-Flare-R（通过新型光学系统采集）；3) 设计E-DeflareNet以学习去炫光恢复，利用基准进行训练与评测。

Result: E-DeflareNet在去炫光恢复上达到SOTA；在多项下游任务中显著提升性能；基准和代码数据公开，可复现。

Conclusion: 物理建模+基准数据+专用网络可有效去除事件相机炫光，显著改善事件数据质量与下游应用表现，填补该方向的系统性研究空白。

Abstract: Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.

</details>


### [19] [ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors](https://arxiv.org/abs/2512.09056)
*Liming Kuang,Yordanka Velikova,Mahdi Saleh,Jan-Nico Zaech,Danda Pani Paudel,Benjamin Busam*

Main category: cs.CV

TL;DR: 提出ConceptPose：基于VLM、训练与模型均免的开词汇物体相对位姿估计，利用概念向量3D地图与3D-3D对应实现6DoF估计，零样本设定下显著超现有方法（ADD(-S)提升>62%）。


<details>
  <summary>Details</summary>
Motivation: 传统位姿估计依赖大量、特定数据集训练，泛化差；而大规模视觉语言模型展现出强零样本能力。动机是将VLM的开放词汇语义与几何配准结合，实现无需对象或数据集训练的通用位姿估计。

Method: 1) 用VLM对图像生成显著图并提取语义“概念向量”；2) 将多视角/场景点投到3D，形成每个3D点携带概念向量的开放词汇3D概念图；3) 在两幅（或两次扫描）之间，通过概念向量相似性与几何一致性建立稳健3D-3D对应；4) 使用RANSAC/ICP类姿态求解器估计6DoF相对位姿；全流程无对象或数据集特定训练。

Result: 在零样本相对位姿基准上达SOTA，ADD(-S)指标较现有方法提升超过62%，优于包含大量特定训练的数据驱动方法。

Conclusion: 语义引导的开放词汇3D概念映射可在无训练条件下实现精确6DoF相对位姿估计，显著超越已有方法，表明VLM语义与几何配准的结合对通用位姿估计有效。

Abstract: Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.

</details>


### [20] [SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding](https://arxiv.org/abs/2512.09062)
*Seongyong Kim,Yong Kwon Cho*

Main category: cs.CV

TL;DR: SIP（Site in Pieces）发布了面向施工现场的单站式地面激光雷达点云数据集，强调真实稀疏性与碎片化可见性，用于更鲁棒的3D感知基准与研究。


<details>
  <summary>Details</summary>
Motivation: 现有3D数据集多来自密集融合、多视角、均匀采样与完整可见性的条件，无法代表真实施工现场中因安全、通行与作业限制导致的单站扫描、径向密度衰减、遮挡严重与几何碎片化，影响进度监测、安全评估与数字孪生的可靠性与泛化。

Method: 构建SIP数据集：使用地面激光雷达在室内外施工场景以单站视角采集，保留真实感知特征（密度衰减、可见性依赖、碎片几何）；基于施工环境定制分类体系（A建筑环境、B施工操作、C场地周边），对点级进行标注；纳入结构构件与临时细长物（脚手架、机电管线、剪叉式升降机等）；制定扫描协议、标注流程与质控规范；提供开源仓库与可配置类别映射，便于接入现代3D深度学习框架。

Result: 得到覆盖室内外、包含结构与临时设施的点级标注数据集，保留现场稀疏与碎片化特性；数据与代码公开，支持灵活类别配置与基准搭建。

Conclusion: SIP弥补施工场景真实单站LiDAR数据的缺口，为分割等3D视觉任务提供更贴近实地的评测基准，促进面向施工的鲁棒3D感知研究与应用。

Abstract: Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.

</details>


### [21] [KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification](https://arxiv.org/abs/2512.09069)
*Erfan Nourbakhsh,Nasrin Sanjari,Ali Nourbakhsh*

Main category: cs.CV

TL;DR: 提出KD-OCT蒸馏框架，将重型ConvNeXtV2-Large教师压缩为轻量EfficientNet-B2学生，用于OCT上正常/玻璃膜疣/脉络膜新生血管分类；在NEH数据集病人级交叉验证中，接近教师精度但显著减少参数与推理时间，优于同类高效模型，适合边缘端AMD筛查。


<details>
  <summary>Details</summary>
Motivation: 临床部署高精度OCT分类模型受限于算力与时延；需要在保持诊断性能的同时大幅降低模型体量与推理开销，实现实时与边缘部署。

Method: 以增强+SWA+focal loss强化的ConvNeXtV2-Large为教师，EfficientNet-B2为学生；采用实时知识蒸馏，联合软目标（教师输出）与硬标签（GT）的组合损失进行平衡训练；在Noor Eye Hospital数据集上进行病人级交叉验证，与多尺度/特征融合方法对比。

Result: 学生模型在参数量与推理时间上大幅下降，同时准确性接近教师并超过多数现有高效OCT分类框架；在效率-准确性上取得更优权衡。

Conclusion: KD-OCT能够在不显著牺牲精度的前提下实现模型压缩，支持边缘端实时AMD/CNV筛查部署；提供代码以复现与推广。

Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.

</details>


### [22] [Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics](https://arxiv.org/abs/2512.09071)
*Nick Trinh,Damian Lyons*

Main category: cs.CV

TL;DR: 论文提出一种自动阈值选择方法，用“负样本”高斯混合统计来判定匹配是否足够好，从而在不同数据库与描述子上稳健地进行VPR匹配判决。


<details>
  <summary>Details</summary>
Motivation: VPR在不同季节、光照、结构、动态干扰下图像外观差异大，常用指标（recall@K、ROC）难以直接指导机器人在线决策；实际系统常靠手调阈值，难以跨场景泛化。

Method: 不直接从“正样本”匹配分布出发，而是建模“非此地点”的负样本图像统计，用高斯混合模型（GMM）表征负匹配分数分布；据此为每个地点/数据库自适应地选择区分阈值，以决定匹配是否接受。

Result: 所提基于负样本GMM的阈值选择在多种图像数据库与多种图像描述子上表现良好，能稳定挑选出有效阈值，提升匹配判决的可靠性。

Conclusion: 利用负样本统计自动设定VPR匹配阈值可替代手工阈值，具备跨场景与跨描述子的通用性，实用价值高。

Abstract: Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.

</details>


### [23] [AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models](https://arxiv.org/abs/2512.09081)
*Arman Zarei,Jiacheng Pan,Matthew Gwilliam,Soheil Feizi,Zhenheng Yang*

Main category: cs.CV

TL;DR: 提出AgentComp：用代理式偏好优化与LLM工具链自动构造组合性数据集，微调文生图模型以更好区分细粒度组合差异，显著提升组合性基准表现且不损画质，并对文本渲染等能力有泛化。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型虽然画质高，但难以正确处理组合性：对象关系、属性绑定、细粒度约束等。根因之一是训练阶段缺少对“组合上相近但细节不同”的样本进行显式区分与对比，导致生成结果“像但不对”。

Method: 构建AgentComp框架：利用具备推理与工具使用能力的大语言模型，调用图像生成、编辑与VQA等工具，自动合成多样化的组合性数据集（包含正负或近邻对，比对细粒度差异）。在此基础上进行代理式偏好优化（agentic preference optimization）来微调文生图模型，使其在相似组合样本间做出更精确的偏好判断与生成选择，从而提升组合性推理与绑定能力。

Result: 在T2I-CompBench等组合性评测上达成SOTA；在保持总体图像质量的同时提升组合性（避免以往方法常见的画质下降）；并在未显式训练的能力（如文本渲染）上表现出一定泛化提升。

Conclusion: 通过以代理驱动的数据构造与偏好优化，模型被显式训练去识别和偏好满足细粒度组合约束的输出，从而显著改善组合性生成，同时维持或提升画质，并带来跨能力的泛化收益。

Abstract: Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.

</details>


### [24] [Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters](https://arxiv.org/abs/2512.09092)
*Mizanur Rahman Jewel,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CV

TL;DR: 提出MDSE视觉-语言框架，针对地下矿难黑暗、粉尘、坍塌等退化环境生成细粒度文字描述；新建UMD数据集；在UMD与相关基准上显著优于现有图像描述模型。


<details>
  <summary>Details</summary>
Motivation: 矿难场景中视觉严重退化，传统感知与人类态势感知困难，需要能够在极端条件下自动给出可靠、可读、具关键语义的场景解释，辅助应急响应与决策。

Method: 提出MDSE框架，含三大创新：1) 语境感知交叉注意力，实现在严重退化下的稳健视觉-文本对齐；2) 分割感知的双通道视觉编码，融合全局与区域级嵌入；3) 资源高效的Transformer语言模型，在较低算力下生成富表达的描述。并构建真实矿难图像-字幕语料UMD数据集用于训练与评测。

Result: 在UMD及相关基准上，MDSE较SOTA图像描述模型显著提升准确性与情境相关性，能捕捉被遮挡与受限环境中的关键细节，从而提升地下应急态势感知。

Conclusion: MDSE与UMD数据集共同推动矿难场景解释研究；所提跨模态对齐、分割感知编码与高效语言建模有效，应在受限与退化环境的应急感知中具实用价值；代码已开源。

Abstract: Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.

</details>


### [25] [Food Image Generation on Multi-Noun Categories](https://arxiv.org/abs/2512.09095)
*Xinyue Pan,Yuhao Chen,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 论文指出多名词食物类别（如“egg noodle”）在生成图像时易被误解为多个独立物体，提出FoCULR通过引入食物领域知识与早期布局/语义引导改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 多名词复合食物名在真实数据集中普遍存在，但现有生成模型的文本编码器缺乏这类语义关系知识，常误解成多个独立实体并产生错误布局，导致生成结果不真实，影响如UEC-256等基准上的表现。

Method: 提出FoCULR框架：1) 融合食物领域知识库，明确复合名词的核心概念与成分关系；2) 在生成早期注入关键语义与布局提示，纠正文本-图像对齐与空间关系；3) 进行布局细化（layout refinement），避免把多个名词错当作独立对象。

Result: 在食物图像生成任务与包含多名词类别的数据集（如UEC-256）上，整合上述技术后，生成质量与语义一致性得到提升，错误成分与错误布局显著减少。

Conclusion: 通过将领域知识与早期语义/布局引导融入生成过程，FoCULR有效缓解多名词类别的语义误读与空间错配问题，提升了食物领域图像生成的真实性与一致性。

Abstract: Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.

</details>


### [26] [GimbalDiffusion: Gravity-Aware Camera Control for Video Generation](https://arxiv.org/abs/2512.09112)
*Frédéric Fortier-Chouinard,Yannick Hold-Geoffroy,Valentin Deschaintre,Matheus Gadelha,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 提出GimbalDiffusion：以物理世界坐标（以重力为全局参考）进行相机可控的文本生成视频框架，支持精确、可解释的相机姿态与运动控制，并构建数据与基准以提升与评测其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有文生视频方法对相机运动的表示多为相对或含糊，导致无法进行明确的几何级控制，且常受限于传统视频数据中单一的直行、前视轨迹；文本内容与相机规格冲突时也会误导生成。

Method: 1) 以绝对坐标系（以重力为参考）定义相机轨迹与姿态，而非相对前帧；2) 利用全景360°视频合成/采样多样化的相机轨迹，覆盖广泛俯仰角与运动模式；3) 提出“null-pitch conditioning”标注/条件化策略，弱化文本在与相机规格冲突时的影响；4) 重整SpatialVID-HQ，建立覆盖宽俯仰变化的相机感知生成基准。

Result: 实现精确、可解释、无需初始参考帧的相机控制；在构建的基准上表现出更强的可控性与鲁棒性，能在广泛俯仰角下遵循相机规范并减少由文本语义引起的偏差。

Conclusion: 通过绝对坐标系与重力对齐的相机建模、360°数据增强以及null-pitch条件化与新基准，显著提升文生视频的相机可控性与稳健性，为精细化、物理一致的相机操控提供可行路径。

Abstract: Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.

</details>


### [27] [SuperF: Neural Implicit Fields for Multi-Image Super-Resolution](https://arxiv.org/abs/2512.09115)
*Sander Riisøen Jyhne,Christian Igel,Morten Goodwin,Per-Arne Andersen,Serge Belongie,Nico Lang*

Main category: cs.CV

TL;DR: 提出SuperF：一种无需高分辨率训练数据的多图像超分辨率（MISR）测试时优化方法，使用隐式神经表示（INR）共享同一连续场并与亚像素配准参数联合优化，在卫星与手持图像上实现最高×8放大，减少“幻觉”并取得强竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 单幅超分辨率依赖强先验或外部引导，易产生与真实不符的“幻觉”；而多图像超分辨率可利用多视角的亚像素位移约束物理一致性。然而现有方法常需训练数据或未充分联合建模对齐与重建。

Method: 以坐标式神经网络（INR）表示目标高分辨率连续图像场，针对一组存在亚像素偏移的低分辨率帧共享同一个INR；在测试时同时优化：1）INR参数；2）每帧的可学习仿射（亚像素）对齐参数。采用与期望输出分辨率一致的超采样坐标网格进行渲染与误差回传，从而在无HR训练数据下完成重建。相较相关INR基线（源自突发融合/分层分离），该方法直接参数化并优化对齐。

Result: 在模拟的卫星影像突发序列与手持相机地面图像上，SuperF在最高×8放大下取得令人信服的定性与定量表现，优于相关INR基线，并降低伪细节/幻觉依赖。

Conclusion: SuperF证明了在MISR中利用共享INR并与对齐参数联合测试时优化的有效性，无需高分辨率训练数据即可获得高倍率放大与物理一致的细节恢复，为资源受限或跨域场景提供了实用方案。

Abstract: High-resolution imagery is often hindered by limitations in sensor technology, atmospheric conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution algorithmically. Since single-image super-resolution requires solving an inverse problem, such methods must exploit strong priors, e.g. learned from high-resolution training data, or be constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While qualitatively pleasing, such approaches often lead to "hallucinated" structures that do not match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) resolution by constraining the super-resolution process with multiple views taken with sub-pixel shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages coordinate-based neural networks, also called neural fields. Their ability to represent continuous signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task.
  The key characteristic of our approach is to share an INR for multiple shifted low-resolution frames and to jointly optimize the frame alignment with the INR. Our approach advances related INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield compelling results on simulated bursts of satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach does not rely on any high-resolution training data.

</details>


### [28] [Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation](https://arxiv.org/abs/2512.09134)
*Georgy Kopanitsa,Oleg Metsker,Alexey Yakovlev*

Main category: cs.CV

TL;DR: 提出AngioAI-QFR：一套纯冠脉造影的端到端AI流程，自动完成解剖分析、功能评估与虚拟支架，和入侵性FFR高度一致（r=0.89, MAE 0.045），诊断AUC 0.93，自动完成率93%，中位41秒。


<details>
  <summary>Details</summary>
Motivation: 现有造影对狭窄的视觉评估变异大、与缺血相关性有限；有导丝的FFR虽可改进决策但未被常规化；无导丝QFR工具往往工作流繁琐且与解剖分析、虚拟PCI割裂。需要一体化、自动化、接近实时的无导丝功能学解决方案。

Method: 开发AngioAI-QFR端到端管线：深度学习狭窄检测；腔道分割；中心线与直径提取；按毫米的相对流量能力（RFC）剖面；虚拟支架并自动重算基于造影的QFR。在100条连续血管（以入侵性FFR为金标准）上评估，主要终点为与FFR的一致性（相关、MAE）及对FFR≤0.80的诊断性能；同时记录自动化率与耗时。

Result: 在保留帧上，狭窄检测精度0.97，分割Dice 0.78。跨100血管，AngioAI-QFR与FFR强相关r=0.89，MAE 0.045；检测FFR≤0.80的AUC 0.93，敏感度0.88，特异度0.86。93%血管可全自动完成，结果中位时间41秒。RFC区分局灶与弥漫性能力损失；虚拟支架预测局灶病变QFR获益更大。

Conclusion: AngioAI-QFR将计算机视觉、功能学剖面与虚拟PCI一体化，实现近实时、实用的纯造影无导丝生理评估，具有较高准确性和自动化程度，并可辅助病变表型判定与干预获益预测。

Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.

</details>


### [29] [GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars](https://arxiv.org/abs/2512.09162)
*Kelian Baert,Mae Younes,Francois Bourel,Marc Christie,Adnane Boukhayma*

Main category: cs.CV

TL;DR: 将2D高斯喷溅与UV纹理映射融合：把每个规范高斯的局部坐标帧嵌入模板网格的UV贴图块，从单目视频重建可编辑的人头材质与几何，并用高效PBR反射模型实现重光照与外观编辑。


<details>
  <summary>Details</summary>
Motivation: 高斯喷溅近年在逼真头像重建上很准确，但缺乏像传统三角网格那样直观的可编辑性（尤其基于UV纹理的编辑）。需要一种既保持高斯喷溅精度与真实感，又获得直观UV域可编辑性的方案。

Method: 在模板网格的UV域中，为每个规范2D高斯基元分配并嵌入一个局部帧到对应的UV贴片，以计算高效的方式将高斯表征与标准UV纹理对齐；从单目视频重建连续的、可编辑的材质纹理（在常规UV域）；并引入高效的物理基反射（PBR）模型来估计/分解内在材质贴图，实现重光照与编辑。

Result: 与多种最先进方法对比，所提方法在重建准确性与重光照质量上更优或相当；同时能在无需额外优化的情况下，通过常规纹理映射对头像外观与几何进行直观修改。

Conclusion: 该方法把2D高斯喷溅的精度与UV纹理映射的可编辑性结合，能从单目视频得到可在PBR框架下重光照和编辑的连续材质贴图，实现高质量、可控的人头头像重建与编辑。

Abstract: Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.

</details>


### [30] [WonderZoom: Multi-Scale 3D World Generation](https://arxiv.org/abs/2512.09164)
*Jin Cao,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: WonderZoom 从单张图像生成可跨尺度浏览的3D场景，支持从宏观到微观逐级细化与实时渲染，显著优于现有视频/3D方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D世界生成多局限于单一尺度，无法在不同粒度下保持内容一致与结构连贯。缺乏能同时表达与渲染大范围与微小细节的尺度感知3D表示是核心瓶颈。

Method: 提出两项技术：1）尺度自适应的 Gaussian surfels（高斯表面元）作为3D表示，实现跨大幅度尺寸差异的生成与实时渲染；2）渐进式细节合成器，以自回归方式在“放大/下钻”时逐步生成更精细的局部3D内容，实现由景观到微观结构的细化。

Result: 在定量与定性上均优于最先进视频/3D模型，表现为更高质量与更好对齐；可从单张图像生成可交互、多尺度一致的3D世界，并提供视频与在线交互演示。

Conclusion: WonderZoom 通过尺度自适应表示与渐进细化策略，突破单尺度限制，实现单图像驱动的多尺度3D世界生成与实时浏览，为多尺度生成与编辑开辟新方向。

Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/

</details>


### [31] [Prompt-Based Continual Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.09172)
*Sauda Maryam,Sara Nadeem,Faisal Qureshi,Mohsen Ali*

Main category: cs.CV

TL;DR: 提出PromptCCZSL：在持续学习场景下，让冻结VLM通过提示与多教师蒸馏实现对新属性/对象/组合的自适应，同时减少遗忘；在UT-Zappos与C-GQA上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 组合零样本学习需要在看不见的属性-对象组合上泛化。持续组合零样本学习（CCZSL）更难：会不断遇到新组合且属性/对象会跨会话复现，既要学新组合又要避免灾难性遗忘。现有方法对VLM的持续自适应与保持可组合性不足。

Method: 在冻结VLM骨干上，提出PromptCCZSL：1) 近期加权的多教师蒸馏保留历史知识；2) 会话感知的组合prompt融合多模态以学习新组合；3) 属性/对象prompt用会话无关的融合方式学习，配合余弦锚点损失（CAL）稳定全局语义；4) 正交投影损失（OPL）使新属性/对象嵌入与历史正交，避免重叠；5) 会话内多样性损失（IDL）拉开当前会话嵌入以提升判别性；6) 提出同时评估遗忘与组合泛化的新协议。

Result: 在UT-Zappos与C-GQA的闭集设定下，较VLM与非VLM基线有大幅提升，树立新的CCZSL基准。

Conclusion: 通过提示驱动的持续适应与多重正则（蒸馏、锚点、正交、多样性），可在不微调主干的前提下兼顾学新与抗遗忘，显著提升CCZSL表现并提供统一评测协议。

Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.

</details>


### [32] [Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation](https://arxiv.org/abs/2512.09185)
*Hao Chen,Rui Yin,Yifan Chen,Qi Chen,Chao Li*

Main category: cs.CV

TL;DR: 将疾病进程建模为“速度场”，用Flow Matching对齐病程时间演化，并通过患者特异的潜变量对齐使潜在空间单轴单调，提升可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式进程模型存在三大不匹配：疾病动力学应是连续且单调，但（1）潜在表示分散、欠语义；（2）扩散模型的随机去噪破坏连续性；（3）自编码器潜空间跨患者不对齐且与临床严重度弱相关。需要一个既连续、单调又可解释的进程表示方法。

Method: 将疾病动力学视为速度场，并采用Flow Matching学习从健康到疾病的连续轨迹；同时提出患者特异的潜变量对齐策略，强制各患者轨迹沿潜空间某一特定轴方向单调递增（与严重度一致），构建一致且具语义的潜在空间；整体框架命名为Δ-LFM。

Result: 在三个纵向MRI基准上获得强劲的实证表现，同时显著提升进程解释与可视化能力。

Conclusion: Δ-LFM通过FM和潜变量对齐实现连续、单调、可解释的疾病进程建模，统一了跨患者的潜空间语义结构，并在多数据集上验证了有效性。

Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $Δ$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $Δ$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.

</details>


### [33] [View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs](https://arxiv.org/abs/2512.09215)
*Yuanyuan Liu,Haiyang Mei,Dongyang Zhan,Jiayue Zhao,Dongsheng Zhou,Bo Dong,Xin Yang*

Main category: cs.CV

TL;DR: 提出一种用于零样本3D视觉指代的View-on-Graph(VoG)方法，通过让VLM主动访问外化的3D空间信息的场景图，实现结构化探索与逐步推理，降低推理难度并提升可解释性，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本3DVG多把3D空间信息转成VLM可处理的复合2D输入（渲染视角/视频+标注），导致视觉表示缠结、线索杂乱，VLM难以有效利用空间-语义关系。需要一种能让VLM按需获取信息、降低推理负担并提升可解释性的范式。

Method: 提出VLM x SI范式：将3D空间信息外化为可按需检索的结构，并以多模态、多层次的场景图实现之。具体的VoG让VLM作为主动体在场景图上探索：选择性访问节点与边的必要线索，逐步推理。该结构将空间与语义关系显式编码，支持可追踪的步骤化推理。

Result: 在零样本3DVG任务上进行广泛实验，VoG取得了当前最优（SOTA）的无训练迁移表现。

Conclusion: 结构化的场景图探索与按需信息检索能显著降低VLM在3DVG中的推理难度，并同时带来透明、可解释的推理轨迹，是推进零样本3DVG的有效方向。

Abstract: 3D visual grounding (3DVG) identifies objects in 3D scenes from language descriptions. Existing zero-shot approaches leverage 2D vision-language models (VLMs) by converting 3D spatial information (SI) into forms amenable to VLM processing, typically as composite inputs such as specified view renderings or video sequences with overlaid object markers. However, this VLM + SI paradigm yields entangled visual representations that compel the VLM to process entire cluttered cues, making it hard to exploit spatial semantic relationships effectively. In this work, we propose a new VLM x SI paradigm that externalizes the 3D SI into a form enabling the VLM to incrementally retrieve only what it needs during reasoning. We instantiate this paradigm with a novel View-on-Graph (VoG) method, which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that selectively accesses necessary cues as it traverses the scene. This design offers two intrinsic advantages: (i) by structuring 3D context into a spatially and semantically coherent scene graph rather than confounding the VLM with densely entangled visual inputs, it lowers the VLM's reasoning difficulty; and (ii) by actively exploring and reasoning over the scene graph, it naturally produces transparent, step-by-step traces for interpretable 3DVG. Extensive experiments show that VoG achieves state-of-the-art zero-shot performance, establishing structured scene exploration as a promising strategy for advancing zero-shot 3DVG.

</details>


### [34] [Enabling Next-Generation Consumer Experience with Feature Coding for Machines](https://arxiv.org/abs/2512.09232)
*Md Eimran Hossain Eimon,Juan Merlos,Ashan Perera,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: 论文概述：MPEG-AI 中的机器特征编码（FCM）标准，通过对中间神经网络特征进行提取、压缩与传输，使低算力设备可借助远端高算力服务器完成推理，在与远程端到端推理保持相同精度的同时，将比特率开销降低约75.90%。


<details>
  <summary>Details</summary>
Motivation: 终端设备智能化与互联化加深，产生大量机器任务数据；直接上传原始数据或进行全模型远程推理带来带宽与延迟瓶颈；端侧算力受限难以运行大模型，亟需一种在不牺牲精度的前提下兼顾算力与通信效率的方案。

Method: 提出并标准化“为机器而编码”的管线：在端侧/边缘提取神经网络中间特征，对特征进行压缩编码（含特征量化、熵编、可能的剪枝/聚类等），传输到具有高算力的基站/服务器在后半段网络完成推理。标准定义了特征抽取点、比特流语法、解码器行为与互操作接口，确保不同模型与设备间的兼容与可复用。

Result: 实验表明，在与远程端到端推理保持同等精度的情况下，FCM相较“远程推理（上传更高负载数据）”的带宽需求下降约75.90%。

Conclusion: FCM作为MPEG-AI的一部分，为AI应用提供了高效的特征级传输标准，使低功耗设备可利用大型深度模型能力；在保证精度的同时显著降低通信成本，具备广泛落地潜力。

Abstract: As consumer devices become increasingly intelligent and interconnected, efficient data transfer solutions for machine tasks have become essential. This paper presents an overview of the latest Feature Coding for Machines (FCM) standard, part of MPEG-AI and developed by the Moving Picture Experts Group (MPEG). FCM supports AI-driven applications by enabling the efficient extraction, compression, and transmission of intermediate neural network features. By offloading computationally intensive operations to base servers with high computing resources, FCM allows low-powered devices to leverage large deep learning models. Experimental results indicate that the FCM standard maintains the same level of accuracy while reducing bitrate requirements by 75.90% compared to remote inference.

</details>


### [35] [Efficient Feature Compression for Machines with Global Statistics Preservation](https://arxiv.org/abs/2512.09235)
*Md Eimran Hossain Eimon,Hyomin Choi,Fabien Racapé,Mateen Ulhaq,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: 提出在分割推理中的特征压缩/解码侧引入Z-score归一化，集成到MPEG正在制定的FCM标准，降低比特率并提升任务准确性；平均码率降低17.09%，在目标跟踪上最高达65.69%，精度无损。


<details>
  <summary>Details</summary>
Motivation: 分割推理需要在边云两端传输中间特征，带宽/时延受限使高效特征压缩关键；现有FCM标准中的缩放方法存在开销大、复原不精确，影响下游任务精度与码率效率。

Method: 在解码端采用Z-score归一化（利用均值与标准差）来重构压缩后的特征分布，替换现行FCM中的缩放策略；并设计在特定场景下进一步减少开销的简化版本。方法无缝嵌入MPEG FCM工作流程，减少侧信息比特。

Result: 相较现有FCM方案，平均码率下降17.09%；在目标跟踪任务上码率最高下降65.69%；同时不牺牲下游任务精度（end-task accuracy 持平或更优）。

Conclusion: 在MPEG FCM框架下，用Z-score归一化替代现有缩放方案能更好地恢复特征分布，减少侧信息并保持/提升任务精度，是更高效的分割推理特征编码策略；简化版在特定场景还能进一步降开销。

Abstract: The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.

</details>


### [36] [A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI](https://arxiv.org/abs/2512.09244)
*Anas Bin Ayub,Nilima Sultana Niha,Md. Zahurul Haque*

Main category: cs.CV

TL;DR: 提出一种结合SMOTE与Grad-CAM的深度CNN，用于从CT肾脏图像早期检测CKD，在含12,446张图像的数据集上报告达到100%准确率。


<details>
  <summary>Details</summary>
Motivation: CKD全球负担重，早期诊断困难且关键；需要可靠高效的计算机辅助方法以促进早期筛查和干预。

Method: 构建深度卷积神经网络，对CT KIDNEY DATASET中的肾脏CT图像进行多类分类（囊肿、正常、结石、肿瘤），采用SMOTE进行类别不平衡处理，并用Grad-CAM提供可解释性；在该数据集上训练与评估。

Result: 在该数据集上宣称实现了CKD早期检测100%准确率的分类性能。

Conclusion: 该方法据称在数据集上表现极佳，显示出在CKD早期诊断与临床决策支持中的潜力，但需要进一步验证与泛化评估。

Abstract: Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.

</details>


### [37] [OmniPSD: Layered PSD Generation with Diffusion Transformer](https://arxiv.org/abs/2512.09247)
*Cheng Liu,Yiren Song,Haofan Wang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: OmniPSD提出一个统一扩散框架，既能从文本直接生成带透明通道的分层PSD，也能把扁平图像分解成可编辑PSD层；通过空间排布多层并利用空间注意力学习层间组合关系，结合迭代式上下文编辑与RGBA-VAE以保留透明度，实现高保真、结构一致、透明感知的结果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型擅长生成/编辑整图，但难以生成或重建带透明alpha的分层PSD，无法满足设计工作流对可编辑图层与层级结构的需求。

Method: 构建基于Flux生态的统一扩散框架OmniPSD：1) 文本到PSD：将多目标层在同一画布上进行空间排布，使用空间注意力学习层间的组合与层次关系，输出语义一致、结构化的图层。2) 图像到PSD分解：采用迭代式in-context编辑，逐步抽取并擦除文本与前景要素，重建可编辑图层。3) 引入RGBA-VAE作为辅助表示模块，专门保留透明度信息而不干扰结构学习。4) 在新构建的RGBA分层数据集上训练与评测。

Result: 在新RGBA分层数据集上的大量实验显示，OmniPSD在生成保真度、结构一致性与透明度感知方面表现优异，能够生成与分解具有可编辑性的PSD层。

Conclusion: OmniPSD为分层设计的生成与分解提供了新的统一范式，兼顾文本到PSD与图像到PSD任务，并有效处理透明通道与层级结构问题。

Abstract: Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.

</details>


### [38] [GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://arxiv.org/abs/2512.09251)
*Lalit Maurya,Saurabh Kaushik,Beth Tellman*

Main category: cs.CV

TL;DR: 提出GLACIA：首个将大语言模型与遥感分割结合，用于冰湖分割与空间推理，构建GLake-Pos数据集，mIoU达87.30，显著优于CNN/ViT/地学基础模型与推理分割方法，支持自然语言交互与可解释决策。


<details>
  <summary>Details</summary>
Motivation: 现有CNN/ViT分割多停留在像素级预测，缺乏全局场景语义与人类可解释的空间推理，难以满足冰湖溃决洪水风险监测中对可解释、可交互决策支持的需求。

Method: 提出GLACIA框架：将具备语言-视觉对齐与推理能力的大语言模型与分割模型整合，实现同时输出精确分割掩膜与空间位置推理文本。构建GLake-Pos数据集流水线，生成多样化、空间对齐的问答对，专注实例级位置推理以弥补遥感数据缺口。与多类基线（CNN、ViT、地学基础模型、推理分割）比较评测。

Result: 在冰湖分割上取得mIoU 87.30，超过CNN基线（78.55–79.01）、ViT基线（69.27–81.75）、地学基础模型（76.37–87.10）与推理分割（60.12–75.66）。同时提供可解释的空间推理输出。

Conclusion: GLACIA将语言推理与分割统一，提升精度与可解释性；GLake-Pos为实例级空间推理提供数据支持；该方法促进基于自然语言的人机交互式冰湖监测与灾害准备、政策制定。

Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA

</details>


### [39] [ROI-Packing: Efficient Region-Based Compression for Machine Vision](https://arxiv.org/abs/2512.09258)
*Md Eimran Hossain Eimon,Alena Krause,Ashan Perera,Juan Merlos,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: 提出ROI-Packing：面向机器视觉的高效图像压缩，通过优先保真并高效封装任务关键的兴趣区域、弱化或丢弃非关键区域，在无需重训下显著降码率且保持/提升下游精度。实验在目标检测与实例分割的5个数据集上，相比MPEG标准VVC，达到码率下降44.10%不降精度，或同码率下精度提升8.88%。


<details>
  <summary>Details</summary>
Motivation: 通用视频/图像编解码器优化主观感知或像素级失真，未直接面向机器视觉的任务精度；边缘/云端协同场景需要在有限带宽下最大化下游任务表现，且希望避免为每个任务重训模型。

Method: 1) 基于下游任务对ROI的重要性选择并优先编码这些区域；2) 通过“packing”将多个ROI紧凑排列到更小画布中，抑或保留低保真全局上下文，同时对非关键区域强压缩或丢弃；3) 作为编解码前后端的可插拔预/后处理，无需改动或重训下游模型；4) 与标准编解码器（如VVC）结合使用以实现端到端传输。

Result: 跨5个数据集、两类任务（检测与实例分割），在不牺牲任务精度的情况下码率最高降低44.10%；在相同码率下，相比VVC，任务精度最高提升8.88%。

Conclusion: 针对机器视觉的任务感知压缩可以在不重训模型的前提下显著提升带宽效率与任务精度；ROI-Packing作为可插拔模块，兼容既有编解码标准，适合部署于资源受限的边缘到云端流水线。

Abstract: This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).

</details>


### [40] [MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification](https://arxiv.org/abs/2512.09270)
*Sangwoon Kwak,Weeyoung Kwon,Jun Young Jeong,Geonho Kim,Won-Sik Cheong,Jihyong Oh*

Main category: cs.CV

TL;DR: MoRel 提出一种锚点接力的双向融合机制（ARBB）与层级加密策略（FHD），在长时序动态场景中实现时序一致、无闪烁、且内存受控的4D高斯重建，并发布长程动态数据集 SelfCap_LR 以验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现有4DGS在长距离时序与大幅运动下易导致：内存爆炸、时序不连贯/闪烁、以及遮挡的出现与消失处理失败。需要一种既可扩展又时序稳定的表示与优化框架。

Method: 1) 锚点接力的双向融合（ARBB）：在关键帧构建局部规范的锚点空间（KfA），学习关键帧间的双向形变，并通过可学习的不透明度进行自适应融合，缓解时序断裂与闪烁；2) 特征方差引导的层级加密（FHD）：依据特征方差分配加密等级，有效增密锚点而维持渲染质量与内存受控；3) 渐进式构建多关键帧锚点并在帧间进行锚点级形变建模，提升长时序一致性。

Result: 在新构建的长程动态数据集 SelfCap_LR（更大运动幅度、更宽场景范围）及现有数据上，MoRel 实现更好的时序一致、减少闪烁、并保持有界内存，占据速度/质量与可扩展性的优势。

Conclusion: MoRel 通过ARBB与FHD在高斯表征中实现对长时序大运动的稳健建模，兼顾质量、时序稳定与内存效率，并以新数据集验证其实用性与可扩展性。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.

</details>


### [41] [LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations](https://arxiv.org/abs/2512.09271)
*Zhichao Yang,Tianjiao Gu,Jianjie Wang,Feiyu Lin,Xiangfei Sheng,Pengfei Chen,Leida Li*

Main category: cs.CV

TL;DR: 提出LongT2IBench与LongT2IExpert，面向长文本到图像生成的对齐评估，提供图结构标注与层级化CoT，使MLLM能输出分数与可解释解释，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评测主要针对短提示，且多为MOS/李克特主观分，缺乏细粒度与可解释性，难以支持对长提示中复杂细节（实体、属性、关系）的对齐评估。

Method: 1) 构建LongT2IBench：14K长文本-图像对；提出Generate-Refine-Qualify流程，将长提示转为包含实体、属性、关系的文本图结构，并据此生成细粒度对齐标注；再将图标注转换为可用于模型训练与评测的对齐分数与解释。2) 基于该基准提出LongT2IExpert：对MLLM进行指令微调，引入分层对齐Chain-of-Thought，让模型输出定量分数与结构化解释。

Result: 在多项实验与对比中，LongT2IExpert在对齐评估准确性与解释质量方面优于现有评测器；基准与代码开源，验证了方法的有效性与通用性。

Conclusion: 图结构标注与层级化CoT能有效提升长提示T2I对齐评估的可解释性与可靠性；LongT2IBench为社区提供了标准数据与评测协议，LongT2IExpert是当前更强的长T2I评估器。

Abstract: The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.

</details>


### [42] [Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis](https://arxiv.org/abs/2512.09276)
*Xiaochen Huang,Xiaochen Bi,Cuihua Lv,Xin Wang,Haoyan Zhang,Wenjing Jiang,Xin Ma,Yibin Li*

Main category: cs.CV

TL;DR: 提出一种基于动态面部表情分析的帕金森病（PD）辅助诊断方法，利用CLIP融合视觉与文本并保留时序，提取表情强度特征，之后用LSTM分类，准确率93.1%。


<details>
  <summary>Details</summary>
Motivation: PD常见但诊断依赖专科与体征评估，门槛高、可及性差。PD的典型症状“面具脸/表情减少（hypomimia）”在日常视频中可观察，若能客观量化其“表情强度”和“面部僵硬”，可提供低成本、无创、远程的辅助筛查手段。

Method: 1) 设计多模态表情分析网络：在患者做多种表情任务的视频中，提取表情强度动态特征；2) 采用CLIP框架融合视觉与文本提示，引导模型关注“表情减弱、僵硬”等语义，同时保留表情的时间变化；3) 将得到的强度特征送入LSTM分类网络进行PD判别。

Result: 所提方法在数据集上获得93.1%的诊断准确率，优于其他体外（in-vitro）PD诊断方法的基线。

Conclusion: 基于动态面部表情与多模态融合的辅助诊断方案可有效识别PD相关的表情减弱与僵硬，具有便捷、可及性高的优势，为PD的早期筛查与远程评估提供新途径。

Abstract: Parkinson's disease (PD), a prevalent neurodegenerative disorder, significantly affects patients' daily functioning and social interactions. To facilitate a more efficient and accessible diagnostic approach for PD, we propose a dynamic facial expression analysis-based PD auxiliary diagnosis method. This method targets hypomimia, a characteristic clinical symptom of PD, by analyzing two manifestations: reduced facial expressivity and facial rigidity, thereby facilitating the diagnosis process. We develop a multimodal facial expression analysis network to extract expression intensity features during patients' performance of various facial expressions. This network leverages the CLIP architecture to integrate visual and textual features while preserving the temporal dynamics of facial expressions. Subsequently, the expression intensity features are processed and input into an LSTM-based classification network for PD diagnosis. Our method achieves an accuracy of 93.1%, outperforming other in-vitro PD diagnostic approaches. This technique offers a more convenient detection method for potential PD patients, improving their diagnostic experience.

</details>


### [43] [LoGoColor: Local-Global 3D Colorization for 360° Scenes](https://arxiv.org/abs/2512.09278)
*Yeonjin Chang,Juhwan Cho,Seunghyeon Seo,Wonsik Shin,Nojun Kwak*

Main category: cs.CV

TL;DR: LoGoColor提出一种面向单通道3D重建结果的上色管线，通过避免2D蒸馏过程中的颜色平均，利用“局部-全局”一致性策略与多视图扩散模型，生成既多样又跨视角一致的颜色，并在复杂360°场景中实现更好的定量与主观效果，辅以新提出的颜色多样性指标验证。


<details>
  <summary>Details</summary>
Motivation: 现有3D上色常由2D图像上色模型蒸馏而来，但2D模型在不同视角间存在固有不一致，训练时会发生颜色平均，导致颜色单调、细节被抹平，尤其在360°复杂场景中更明显。需要一种既能保留颜色多样性又能确保多视角严格一致的方案。

Method: 提出LoGoColor管线：1) 生成一组一致上色的训练视图，绕过传统的指导平均；2) 将场景划分为多个子场景（Local），用微调的多视图扩散模型分别处理，保证子场景内部一致；3) 设计“Local-Global”机制，显式建模并优化子场景间（Global）以及子场景内（Local）的颜色一致性；4) 基于这些视图对3D进行上色重建。

Result: 在复杂360°场景中，相比现有基于2D蒸馏的方法，LoGoColor在定量和主观评估上都更一致、更逼真；并通过新提出的Color Diversity Index（颜色多样性指数）验证其能保留更丰富的颜色。

Conclusion: 绕开2D蒸馏导致的颜色平均，结合局部-全局一致性约束与多视图扩散模型，LoGoColor实现了多样且跨视角一致的3D上色，对复杂360°场景尤为有效，并提供了评估颜色多样性的指标。

Abstract: Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360° scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360° scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.

</details>


### [44] [FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model](https://arxiv.org/abs/2512.09282)
*Xiang Chen,Jinshan Pan,Jiangxin Dong,Jian Yang,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出FoundIR-v2：一种大容量扩散式图像修复基础模型，通过“数据均衡调度”动态优化多任务训练数据配比，并引入MoE驱动调度在生成式预训练中为不同退化任务分配自适应扩散先验，实现跨50+子任务的稳健泛化与SOTA级表现。


<details>
  <summary>Details</summary>
Motivation: 现有一体化图像修复基础模型虽然受益于更大规模与更高质量的预训练数据，但忽视了多任务数据配比这一关键变量，导致模型在不同任务间性能不均衡、泛化欠佳。作者希望通过系统化的数据混合与调度策略，提升多任务一致性与综合性能。

Method: 1) 提出FoundIR-v2（扩散式大模型）。2) 数据均衡调度：依据“数据混合律”动态优化来自不同修复任务的数据比例，保证训练数据在任务维度上的平衡。3) MoE驱动调度：在生成式预训练阶段引入Mixture-of-Experts，为各修复任务分配任务自适应的扩散先验，以适应不同退化类型和强度。4) 统一多任务训练，覆盖50+子任务及更广泛真实场景。

Result: 在超过50个子任务与更广泛的真实世界场景中进行大量实验，取得相对于现有最先进方法的有利（SOTA级或接近SOTA）表现，并展现更一致的跨任务泛化能力。

Conclusion: 多任务数据配比是决定一体化图像修复模型性能的关键因素。通过数据均衡调度与MoE自适应扩散先验，FoundIR-v2在广泛任务上实现稳定、全面的性能提升，验证了所提数据混合与调度范式的有效性。

Abstract: Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.

</details>


### [45] [MelanomaNet: Explainable Deep Learning for Skin Lesion Classification](https://arxiv.org/abs/2512.09289)
*Sukhrobbek Ilyosbekov*

Main category: cs.CV

TL;DR: MelanomaNet是一套将高准确率与可解释性并重的皮肤病变多分类系统：基于EfficientNet V2，辅以Grad-CAM++可视化、ABCDE临床特征自动提取、FastCAV概念解释与MC Dropout不确定性分解，在ISIC 2019上达85.61%准确率/0.8564加权F1，并能标记不可靠预测以供临床复核。


<details>
  <summary>Details</summary>
Motivation: 深度学习在皮肤病变分类上表现突出，但因“黑箱”难以获得临床信任与采用。作者旨在在不牺牲性能的前提下，提供与皮肤科临床评估一致、可操作的不透明模型解释与不确定性评估。

Method: 采用EfficientNet V2作为主干进行九类病变分类；四种互补可解释机制：1) GradCAM++定位模型关注区域；2) 自动抽取ABCDE（Asymmetry、Border、Color、Diameter、Evolution）临床特征；3) FastCAV进行概念层面的可解释分析，将概念相关性与预测关联；4) 通过MC Dropout分解不确定性为认知不确定性（epistemic）与数据不确定性（aleatoric），并据此标记高风险样本。

Result: 在ISIC 2019（25,331张皮肤镜图、9类）上取得85.61%准确率与0.8564加权F1。可解释性模块显示模型关注与皮肤科评估准则一致；不确定性模块能有效识别低置信预测并进行自动标记。代码开源。

Conclusion: 该系统在保持较高分类性能的同时，提供多层次、与临床准则一致的解释与不确定性评估，有望提升临床信任与工作流程中的可采用性。

Abstract: Automated skin lesion classification using deep learning has shown remarkable accuracy, yet clinical adoption remains limited due to the "black box" nature of these models. We present MelanomaNet, an explainable deep learning system for multi-class skin lesion classification that addresses this gap through four complementary interpretability mechanisms. Our approach combines an EfficientNet V2 backbone with GradCAM++ attention visualization, automated ABCDE clinical criterion extraction, Fast Concept Activation Vectors (FastCAV) for concept-based explanations, and Monte Carlo Dropout uncertainty quantification. We evaluate our system on the ISIC 2019 dataset containing 25,331 dermoscopic images across 9 diagnostic categories. Our model achieves 85.61% accuracy with a weighted F1 score of 0.8564, while providing clinically meaningful explanations that align model attention with established dermatological assessment criteria. The uncertainty quantification module decomposes prediction confidence into epistemic and aleatoric components, enabling automatic flagging of unreliable predictions for clinical review. Our results demonstrate that high classification performance can be achieved alongside comprehensive interpretability, potentially facilitating greater trust and adoption in clinical dermatology workflows. The source code is available at https://github.com/suxrobgm/explainable-melanoma

</details>


### [46] [Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving](https://arxiv.org/abs/2512.09296)
*Songhan Wu*

Main category: cs.CV

TL;DR: 论文提出YOLOv8n-SPTS，针对自动驾驶小目标检测难题，通过改进特征提取、增强特征融合、增设小目标检测头并裁剪大目标头，在VisDrone2019-DET上取得SOTA级指标（P=61.9%、R=48.3%、mAP@0.5=52.6%、mAP@0.5:0.95=32.6%），显著降低遮挡与密集场景的小目标漏检。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中小尺度、密集、遮挡目标常被漏检，现有方法存在细粒度信息丢失、尺度不平衡、特征融合不足与检测头配置不匹配等问题，导致对低分辨率小目标的感知不足。

Method: 三项改进：1) 特征提取：在YOLOv8n Backbone 的Bottleneck中以SPD-Conv替换4个常规卷积，通过space-to-depth保留细粒度信息、提升小目标表征；2) 特征融合：以SPPFCSPC替换SPPF，结合SPP多尺度上下文与CSP的跨阶段部分连接，强化多尺度特征表达与上下文理解；3) 检测头：设计TSFP结构，新加160×160小目标检测头，移除冗余大目标检测头，充分利用浅层高分辨率特征并平衡算力。

Result: 在VisDrone2019-DET上，精度61.9%、召回48.3%、mAP@0.5为52.6%、mAP@0.5:0.95为32.6%，在对比实验中指标均列第一；可视化表明对遮挡/密集场景的行人、自行车等小目标漏检率明显降低。

Conclusion: 通过SPD-Conv保细粒度、SPPFCSPC强融合、TSFP增小目标头并裁剪大目标头的组合，显著提升YOLOv8n在小目标检测上的准确性与鲁棒性，同时兼顾计算效率，适用于复杂路况动态感知。

Abstract: This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.

</details>


### [47] [VABench: A Comprehensive Benchmark for Audio-Video Generation](https://arxiv.org/abs/2512.09299)
*Daili Hua,Xizhi Wang,Bohan Zeng,Xinyi Huang,Hao Liang,Junbo Niu,Xinlong Chen,Quanqing Xu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出VABench：一个用于评估同步音视频生成能力的多维度基准，覆盖T2AV/I2AV/双声道生成三类任务，提供15个维度的度量（包含跨模态相似度、同步性、唇音一致性与音视频QA等）与7类内容场景，旨在为具备同步音频能力的视频生成模型建立评测标准。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评测侧重视觉质量，缺乏对同步音视频生成（含音画对齐、语义一致性与交互复杂度）的系统化评价，难以比较模型在音频生成与多模态协同方面的真实能力。

Method: 构建多任务、多模块评测框架：三类任务（T2AV、I2AV、立体声）；两大评估模块下15个维度指标，覆盖文本-视频、文本-音频、视频-音频配对相似度，音画同步、唇-语一致性，以及精心设计的音频与视频QA；并定义七大内容类别用于全面覆盖场景分布。提供可视化与系统分析流程。

Result: 基准能够系统地刻画不同模型在同步音视频生成上的强弱差异与维度画像，产出可视化和定量结果，展示出在多类别、多任务下的对比评估效果。

Conclusion: VABench为同步音视频生成提供了统一、细粒度的评测标准与流程，有望成为新基准以推动模型在音画同步、跨模态一致性与多场景泛化方面的全面进步。

Abstract: Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.

</details>


### [48] [From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation](https://arxiv.org/abs/2512.09307)
*Shivanshu Agnihotri,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: 提出 Polyp-DiFoM：将大规模视觉基础模型的语义先验蒸馏到轻量分割网络，以在息肉分割中兼顾高精度与低计算。通过语义蒸馏与频域编码增强，在五个数据集上显著超越基线与SOTA，且计算开销降至约1/9。


<details>
  <summary>Details</summary>
Motivation: 内镜息肉分割因尺寸、形状、颜色变化大且伪装性强而困难。轻量模型易部署但泛化与鲁棒性不足；大规模基础模型（SAM、DINOv2、One/Mask2Former）泛化强，但医用数据稀缺和领域差异使其难以直接迁移。需要一种方法把基础模型的通用表示迁移到可部署的轻量网络中。

Method: 提出蒸馏框架 Polyp-DiFoM：以U-Net/U-Net++等为学生网络，引入基础模型提供的高层语义先验进行特征/输出层对齐；进一步采用频域编码进行增强蒸馏，提升对外观变化与纹理细节的建模与泛化。整体实现为在训练阶段融合基础模型提示与频域损失，推理仅保留轻量学生。

Result: 在Kvasir-SEG、CVC-ClinicDB、ETIS、ColonDB、CVC-300五个基准上广泛实验，相比对应基线与最新SOTA均有显著提升，同时计算开销约为对比方法的1/9（约9倍降低）。

Conclusion: 通过把基础模型的丰富表征蒸馏到标准轻量架构并结合频域编码，可在保持临床可部署性的同时显著提升息肉分割性能，验证了跨域基础模型知识迁移在医学影像中的有效性。

Abstract: Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.

</details>


### [49] [Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance](https://arxiv.org/abs/2512.09311)
*Kuldeep Singh Yadav,Lalan Kumar*

Main category: cs.CV

TL;DR: 提出USE50k大规模可疑性数据集与轻量级实时分析框架DeepUSEvision，集成改进YOLOv12、双DCNN表情/肢体识别与Transformer判别器，能在复杂场景下高效、可解释地给出可疑性评分，性能优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有安防场景中，对“可疑性”的实时、可解释评估数据稀缺，且多模态线索（武器、火焰、人群密度、表情、姿态）难以在统一、轻量的系统中融合；需要同时兼顾准确性、速度与可解释性以支撑公共安全的主动威胁检测。

Method: 1) 构建USE50k：来自机场、车站、餐馆、公园等非受控环境的65,500张图像，标注武器、火焰、人群密度、异常表情与异常姿态等线索。2) DeepUSEvision框架：- 改进YOLOv12作为可疑物体检测器；- 两个DCNN（DCNN-I/II）借助图像与关键点特征识别表情与肢体语言；- 基于Transformer的判别网络自适应融合多模态输出，给出可解释可疑性分数。

Result: 大规模实验显示相较SOTA具有更高的准确性、鲁棒性与可解释性；能在实时条件下运行。

Conclusion: USE50k与DeepUSEvision为安防与风险评估提供可扩展基线：轻量、模块化、多模态融合、可解释且优于现有方法，适用于安全关键实时监控。

Abstract: Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.

</details>


### [50] [Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook](https://arxiv.org/abs/2512.09315)
*Yuan Ma,Junlin Hou,Chao Zhang,Yukun Zhou,Zongyuan Ge,Haoran Xie,Lie Ju*

Main category: cs.CV

TL;DR: 提出LNMBench：系统评估医学成像中噪声标签鲁棒性的基准，覆盖10种方法、7个数据集、6种模态、3种噪声模式；发现现有LNL方法在高噪与真实噪下显著退化，并给出一个简单有效的改进方案与可复现代码库。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注依赖专家且存在较大主观差异，导致标签噪声普遍存在；虽有大量LNL研究，但其在医学图像场景的鲁棒性缺乏系统性评估与统一对比。

Method: 构建LNMBench：在统一、可复现实验框架下，选取10个代表性LNL方法，跨7个数据集、6种影像模态与3类噪声模式进行广泛评测；同时基于评测发现提出一个简单但有效的鲁棒性增强改进。

Result: 在高强度与真实世界噪声下，现有LNL方法性能显著下降；类不平衡与域间差异是关键难点。LNMBench提供客观对比与可复现实验结果。

Conclusion: 医学影像中的LNL仍面临严峻挑战；LNMBench为标准化评测与方法开发提供基础，所提改进在噪声与分布移位下提升鲁棒性；代码开源以促进可复现研究与实际落地。

Abstract: Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.

</details>


### [51] [UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking](https://arxiv.org/abs/2512.09327)
*Xuangeng Chu,Ruicong Liu,Yifei Huang,Yun Liu,Yichen Peng,Bo Zheng*

Main category: cs.CV

TL;DR: UniLS提出首个仅用双轨音频即可端到端生成“说-听”统一面部表情的框架，通过两阶段训练显著改善听者表情的自然性与多样性，同时保持说话精度SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多只关注“说话”生成，或依赖额外的说话者动作来驱动听者，导致非端到端且实时性差。根因在于监听表情主要受内在运动先验驱动，与外部语音的耦合弱，直接以音频监督训练会产生僵硬、静态的听者表情。

Method: 提出UniLS端到端框架，仅输入双轨音频（说话声与对端音轨），采用两阶段训练：阶段1不使用音频，训练自回归生成器学习自然面部运动的内在先验（自发动态）；阶段2引入双轨音频，微调生成器，使其在保持先验的基础上受外部语音线索调制，从而同时生成说者与听者的表情与动作。

Result: 在广泛评测中，说话相关指标达到SOTA；在监听相关指标上最高提升44.1%，生成的听者表情更加多样与自然，有效缓解“僵硬”问题。

Conclusion: 通过先学内在运动先验再以音频微调的两阶段策略，UniLS实现了仅用音频的统一“说-听”表情端到端生成，兼顾精度与实时性，为交互式数字人提供高保真、实用的解决方案。

Abstract: Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.

</details>


### [52] [Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video](https://arxiv.org/abs/2512.09335)
*Seonghwa Choi,Moonkyeong Choi,Mingyu Jang,Jaekyung Kim,Jianfei Cai,Wen-Huang Cheng,Sanghoon Lee*

Main category: cs.CV

TL;DR: 提出基于3D高斯溅射的可重光、可动画的人体头像建模框架RnD-Avatar，通过动态蒙皮权重和新正则化，在单目视频下重建细节丰富、可重光的人体，并在新视角、新姿态与重光任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF/3DGS人体重建在衣物褶皱等随姿态变化的几何细节上不足，导致真实感与重光表现欠佳；缺少适配重光评测的多视角多照明数据集。

Method: 1) 基于3DGS的人体表示；2) 动态蒙皮权重：随姿态自适应关节驱动并显式学习由运动引起的额外形变；3) 新的几何正则化，在稀疏视觉线索下捕获细粒度几何；4) 新建多视角、变光照数据集用于重光评测。

Result: 在新视角合成、新姿态渲染与重光任务上达到SOTA，呈现高保真几何（如衣褶）与真实感光照效果。

Conclusion: 通过将动态可形变的蒙皮权重与正则化融入3DGS，RnD-Avatar能从单目视频重建可动画、可重光且细节逼真的人体头像，并在多项基准上优于现有方法。

Abstract: Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.

</details>


### [53] [TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment](https://arxiv.org/abs/2512.09350)
*Kanghyun Baek,Sangyub Lee,Jin Young Choi,Jaewoo Song,Daemin Park,Jooyoung Choi,Chaehun Shin,Bohyung Han,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出TextGuider：一种无需训练的文本到图像模型推理期引导方法，通过对齐文本内容token与图像中文本区域，在扩散去噪早期施加潜空间引导，显著减少文本缺失并提升OCR、召回与CLIP分数。


<details>
  <summary>Details</summary>
Motivation: 扩散式文生图模型在文字渲染上仍常见错误，尤其被忽视的“文本遗漏”（想要的文字部分或全部缺失），现有微调或免训练后处理方法难以系统解决这一问题。

Method: 分析MM-DiT中的注意力模式，定位与应被渲染文字相关的文本token及其在图像中的关注区域；据此在去噪前期，对潜变量施加基于两种新损失函数的引导，使文本内容token与图像中文字区域对齐，实现训练期外的推理时优化。

Result: 在测试时文本渲染任务上达到了SOTA：召回显著提升，同时OCR准确率与CLIP分数也取得强劲表现。

Conclusion: 无需额外训练即可有效缓解与纠正文本遗漏问题，通过早期潜空间引导提升文本完整性与准确性，为文本到图像的文字渲染提供通用、可即插即用的改进方案。

Abstract: Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.

</details>


### [54] [Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding](https://arxiv.org/abs/2512.09354)
*Xinkui Zhao,Zuxin Wang,Yifan Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Chang Liu,Naibo Wang,Jianwei Yin*

Main category: cs.CV

TL;DR: 提出Video-QTR，通过查询驱动的时序推理在长视频理解中大幅减少帧输入（最高73%）却达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在长视频上需密集编码大量帧，产生冗余视觉token，带来高显存与计算成本，限制真实场景可扩展性；传统“先处理后推理”范式低效。

Method: 将视频理解重构为“查询引导的感知—推理闭环”：根据问题语义自适应选择关键时间片/帧分配感知资源，实现动态帧采样与时序关注，而非全量编码；形成推理与感知之间的反馈以迭代定位相关内容。

Result: 在MSVD-QA、ActivityNet-QA、MovieChat、Video-MME等基准上取得SOTA，同时将输入帧数量最多减少约73%，显著降低计算与内存。

Conclusion: 查询驱动的时序推理能高效且可扩展地理解长视频，在保证或提升性能的同时显著削减感知开销。

Abstract: The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.

</details>


### [55] [StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation](https://arxiv.org/abs/2512.09363)
*Ke Xing,Longfei Li,Yuyang Yin,Hanwen Liang,Guixun Luo,Chen Fang,Jue Wang,Konstantinos N. Plataniotis,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: StereoWorld提出将预训练视频生成器重用为单目转双目的视频生成系统，通过几何感知正则与时空拼贴实现高保真、几何一致的立体视频，并配套千万级高分辨率双目数据集，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: XR设备普及带来对高质量立体视频的强需求，但传统双目视频采集/生成成本高、伪影多、几何一致性差，需要一种高效、可扩展且具备3D结构保真的生成方法。

Method: 在预训练视频生成模型基础上进行端到端改造：1) 以单目视频作为条件输入；2) 引入几何感知的正则化，显式监督左右眼视图的3D结构一致性；3) 采用时空拼贴（spatio-temporal tiling）方案提升高分辨率生成的效率与可扩展性；4) 构建对齐自然人眼瞳距（IPD）的高分辨率双目视频数据集用于大规模训练与评估。

Result: 在超过1100万帧的双目数据上训练与评测，实验显示该方法在视觉保真度和几何一致性方面显著优于现有单目转双目方法，生成更高质量、更稳定的立体视频。

Conclusion: StereoWorld通过条件生成+几何正则+时空拼贴的组合，实现了高保真、结构一致的单目到立体视频生成，并以大规模数据集支撑其效果，适用于XR等对立体质量要求高的应用场景。

Abstract: The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.

</details>


### [56] [ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation](https://arxiv.org/abs/2512.09364)
*Shengchao Zhou,Jiehong Lin,Jiahui Liu,Shizhen Zhao,Chirui Chang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出ASSIST-3D，一条为类别无关3D实例分割生成训练数据的场景合成管线，通过异质对象选择、LLM引导的布局生成与多视角RGB-D融合构建真实感点云，显著提升在ScanNetV2/ScanNet++/S3DIS上的泛化与性能。


<details>
  <summary>Details</summary>
Motivation: 类无关3D实例分割需对已知与未知类别实例统一分割，但受限于稀缺/嘈杂标注数据与现有合成方法在几何多样性、上下文复杂度与布局合理性之间的折衷，模型泛化不足，亟需能全面满足三者并贴近传感器采集的合成数据。

Method: ASSIST-3D包含三步：1) 异质对象选择：从大规模3D CAD资产库随机采样并控制多样性，提升几何与语境分布覆盖；2) 场景布局生成：利用大语言模型进行空间/功能推理给出物体关系与初始摆放，再以深度优先搜索细化并约束碰撞、尺度与可达性，保证合理布局与复杂上下文；3) 真实点云构建：对合成场景进行多视角RGB-D渲染并融合，模拟真实传感器噪声与重建过程，得到训练用点云与实例掩码。

Result: 在ScanNetV2、ScanNet++、S3DIS基准上，以ASSIST-3D生成数据训练的模型优于现有方法；与其他3D场景合成方案对比亦表现更佳，证明该管线在提升类无关实例分割泛化上的有效性。

Conclusion: 综合几何多样性、语境复杂度与布局合理性的场景合成，并结合真实感点云构建，可显著增强类无关3D实例分割模型的泛化。ASSIST-3D为该方向提供了有效的数据生成范式，优于现有合成方法。

Abstract: Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.

</details>


### [57] [FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement](https://arxiv.org/abs/2512.09373)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: 提出FUSER与FUSER-DF，实现无需两两匹配的多视点云全局位姿直接预测，并通过SE(3)^N扩散精炼，达成更高精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统多视点云注册依赖大量成对匹配构建位姿图，计算代价高且缺乏全局几何约束，易陷入不适定。需要一种能在全局一致性下高效注册多视点云的新范式。

Method: 1) 端到端前馈Transformer：在统一紧凑潜空间联合处理所有扫描，直接输出全局位姿，无需成对估计。2) 稀疏3D CNN将点云压缩为保留绝对平移线索的低分辨率superpoint特征。3) 提出几何交替注意力模块（GAA），高效进行扫描内与扫描间的特征交互；并从2D基础模型迁移注意力先验以增强3D几何一致性。4) 基于FUSER构建FUSER-DF：在联合SE(3)^N空间进行扩散去噪精炼；以FUSER为代理模型定义去噪器，并推导带先验条件的SE(3)^N变分下界用于训练监督。

Result: 在3DMatch、ScanNet、ArkitScenes上取得更高的配准精度与显著的计算效率优势，优于现有方法。

Conclusion: 联合前馈建模+几何注意力+SE(3)^N扩散精炼可摆脱成对匹配与姿态图优化的瓶颈，实现高精度且高效的多视点云全局配准。

Abstract: Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.

</details>


### [58] [Log NeRF: Comparing Spaces for Learning Radiance Fields](https://arxiv.org/abs/2512.09375)
*Sihe Chen,Luv Verma,Bruce A. Maxwell*

Main category: cs.CV

TL;DR: 论文提出在log RGB色彩空间中训练NeRF，可更好分离照明与反射，从而显著提升新视角渲染质量，尤其在低光环境更稳健，且对不同模型规模与变体均泛化有效。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF多在sRGB或线性空间监督，少关注颜色空间对辐射场学习的影响。受BIDR模型启发，log变换有助于在图像域线性化地分离照明与反射，可能让网络以更紧凑的表示学习外观，从而提升鲁棒性与质量。

Method: 采集约30段GoPro视频并通过反向编码获取线性数据；在不同颜色空间（线性、sRGB、GPLog、log RGB）进行训练：通过将网络输出统一映射到公共色彩空间后再渲染与计算损失，从而强制网络在各自色彩空间进行表示学习；在相同比特深度下对比不同空间下的NeRF；并在不同网络规模与NeRF变体上进行泛化实验。

Result: 定量与定性实验均显示：log RGB训练持续优于其他空间，在多场景更稳健，低光条件优势明显；相同输入位深下渲染质量更高；在不同模型规模与NeRF变体上优势保持。

Conclusion: 在log RGB色彩空间中学习可让NeRF获得更紧凑有效的外观表示，带来更高质量与更强鲁棒性，尤其适合低光场景，并能稳定泛化到不同模型与变体。

Abstract: Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.

</details>


### [59] [Perception-Inspired Color Space Design for Photo White Balance Editing](https://arxiv.org/abs/2512.09383)
*Yang Cheng,Ziteng Cui,Lin Gu,Shenghan Su,Zenghui Zhang*

Main category: cs.CV

TL;DR: 提出一种面向后期白平衡校正的“可学习HSI(LHSI)”颜色空间与Mamba网络，解决sRGB后期WB在复杂光照下泛化差的问题；在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: sRGB 等加性颜色模型通道耦合、固定非线性，难以在复杂/混合光照下稳健进行后期WB编辑；真实RAW常不可得，需在sRGB域进行有效校正。

Method: 1) 设计感知启发的可学习HSI(LHSI)颜色空间：以圆柱模型分离亮度与色度，引入可学习参数进一步解耦，并通过可学习映射自适应细化灵活性；2) 针对LHSI特性定制Mamba-based网络结构，利用序列建模能力与高效长程依赖学习，实现更鲁棒的WB校正；3) 在sRGB域进行端到端训练与推理。

Result: 在多个WB基准数据集上取得优于现有方法的性能（文中称“superiority”），显示出在复杂光照/色偏条件下更好的泛化与还原真实色彩能力；代码已开源。

Conclusion: 面向后期WB校正，感知启发的颜色空间设计（LHSI）+与之匹配的Mamba网络能显著提升效果，表明从颜色空间层面重构比直接在sRGB上学习更具潜力。

Abstract: White balance (WB) is a key step in the image signal processor (ISP) pipeline that mitigates color casts caused by varying illumination and restores the scene's true colors. Currently, sRGB-based WB editing for post-ISP WB correction is widely used to address color constancy failures in the ISP pipeline when the original camera RAW is unavailable. However, additive color models (e.g., sRGB) are inherently limited by fixed nonlinear transformations and entangled color channels, which often impede their generalization to complex lighting conditions.
  To address these challenges, we propose a novel framework for WB correction that leverages a perception-inspired Learnable HSI (LHSI) color space. Built upon a cylindrical color model that naturally separates luminance from chromatic components, our framework further introduces dedicated parameters to enhance this disentanglement and learnable mapping to adaptively refine the flexibility. Moreover, a new Mamba-based network is introduced, which is tailored to the characteristics of the proposed LHSI color space.
  Experimental results on benchmark datasets demonstrate the superiority of our method, highlighting the potential of perception-inspired color space design in computational photography. The source code is available at https://github.com/YangCheng58/WB_Color_Space.

</details>


### [60] [Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography](https://arxiv.org/abs/2512.09393)
*Vasiliki Stoumpou,Rohan Kumar,Bernard Burman,Diego Ojeda,Tapan Mehta,Dimitris Bertsimas*

Main category: cs.CV

TL;DR: 作者提出一种多模态深度学习系统，将临床结构化数据、3D CT 卷积模型与带 Transformer 的2D分割模型融合，实现硬膜下血肿（SDH）的检测与定位；在2.5万余例头颅CT数据上，多模态集成达到AUC≈0.941，并生成解剖学一致的可解释定位图。


<details>
  <summary>Details</summary>
Motivation: 现有自动化工具多停留在“是否有SDH”的检测层面，解释性与空间定位不足；临床需要能在急诊环境中快速、透明、与影像-临床多源信息整合的工具，以指导及时干预和一致化管理。

Method: 构建多模态框架：1) 临床表格数据模型（人口学、合并症、用药、化验）；2) 3D CNN 基于CT体数据进行检测；3) 引入Transformer增强的2D分割模型输出体素级概率图；4) 采用贪婪集成策略融合各子模型；数据来自2015–2024年HHC的25,315例头颅CT（其中3,774例SDH）。

Result: 仅临床变量AUC=0.75；3D体数据CNN与分割派生特征AUC分别为0.922与0.926；全量多模态集成最佳，AUC=0.9407（95% CI 0.930–0.951），并生成与已知SDH分布一致的定位图。

Conclusion: 多模态、可解释框架可实现快速准确的SDH检测与定位，性能高且具解剖学可解释性；有望融入放射科流程，优化分诊、缩短干预时间并提升管理一致性。

Abstract: Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.
  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.
  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.
  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.

</details>


### [61] [Wasserstein-Aligned Hyperbolic Multi-View Clustering](https://arxiv.org/abs/2512.09402)
*Rui Wang,Yuting Jiang,Xiaoqing Luo,Xiao-Jun Wu,Nicu Sebe,Ziheng Chen*

Main category: cs.CV

TL;DR: 提出WAH多视图聚类：用洛伦兹超曲空间编码各视图，并以超曲切片Wasserstein距离做全局语义对齐，再结合软聚类提升跨视图一致性，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有超曲MVC多聚焦于实例级对齐，忽略全局语义一致性，易受视图特有噪声和跨视图差异影响，导致聚类不稳与性能受限。

Method: 为每个视图设计超曲（洛伦兹模型）编码器进行层次语义建模；基于超曲切片-Wasserstein距离的全局语义损失对齐各视图在流形上的分布；随后进行软聚类分配以强化跨视图的语义一致性。

Result: 在多项基准数据集上进行大量实验，所提方法在聚类指标上达到或超过当前最优（SOTA）。

Conclusion: 全局流形分布对齐结合超曲表示与软聚类能有效缓解视图特异噪声与差异，提升多视图聚类的鲁棒性与性能。

Abstract: Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.

</details>


### [62] [Generative Point Cloud Registration](https://arxiv.org/abs/2512.09407)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: 提出“生成式点云配准”范式：用可控2D生成模型生成与源/目标点云几何对齐、跨视一致的图像对，融合几何与纹理特征以提升3D配准鲁棒性；在3DMatch与ScanNet上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统点云配准在稀疏/噪声/遮挡场景下鲁棒性不足；单纯3D几何或单视信息难以形成稳定对应。先进2D生成模型具备强表观先验与可控条件，但尚未被充分用于3D配准。作者希望借助2D生成模型引入跨视纹理一致性与2D-3D几何一致性，增强匹配与配准精度。

Method: 提出生成式3D配准范式：从源/目标点云渲染深度图，利用匹配专用的可控生成模型Match-ControlNet生成一对与点云对齐的图像。1) 通过深度条件(ControlNet)保证生成图像与点云的2D-3D几何一致；2) 设计耦合的条件去噪与耦合prompt引导，使跨视图生成过程交互，从而促进纹理一致性；3) 将生成的跨视一致图像与点云进行几何-颜色特征融合，喂给现有配准流水线以提升匹配鲁棒性。该范式可无缝集成到多种配准方法中。

Result: 在3DMatch与ScanNet基准上进行大量实验，集成多种现有配准方法均获性能提升，验证了方法的有效性与泛化性（摘要未给出具体数值）。

Conclusion: 通过将可控2D生成模型与3D配准结合，既保证2D-3D几何一致，又促进跨视纹理一致，从而提升匹配与配准性能。该通用范式可作为增强模块接入不同配准方法，实验显示在标准数据集上有效。

Abstract: In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.

</details>


### [63] [DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping](https://arxiv.org/abs/2512.09417)
*Yanan Wang,Shengcai Liao,Panwen Hu,Xin Li,Fan Yang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出DirectSwap：一个无需掩膜的扩散式视频换头方法，并构建首个跨身份成对数据集HeadSwapBench，通过视频编辑模型生成“伪换头”真值以实现有监督训练；配合运动与表情感知重建损失（MEAR），在视觉质量、身份一致性、运动与表情连贯性上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频换头因缺少跨身份成对真值数据，只能用同人跨帧训练并依赖掩膜修复，易产生边界伪影且难恢复被掩蔽的姿态、表情与运动动态，导致身份泄漏与跨帧不一致。

Method: 1) 数据：用视频编辑模型在原视频上合成与参考身份匹配的新头部，保持逐帧同步的姿态与表情，构建HeadSwapBench成对数据（可训练与评测）。2) 模型：将图像U-Net扩展为带运动模块与条件输入的视频扩散模型，做直接换头（不使用掩膜）。3) 损失：提出MEAR损失，以帧间差分与面部关键点邻近度为权重，逐像素重加权扩散损失，强化运动与表情的跨帧一致性。

Result: 在多种野外视频上，DirectSwap在视觉质量、身份保真与运动/表情一致性方面优于现有方法；新数据集支持客观评测与训练。

Conclusion: 合成的跨身份成对数据能有效破解监督缺失难题；直接、无掩膜的视频扩散框架结合MEAR损失，实现更自然、连贯的换头效果。作者将开源代码与HeadSwapBench促进后续研究。

Abstract: Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\TrainNum{} videos) and benchmarking (\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.

</details>


### [64] [Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis](https://arxiv.org/abs/2512.09418)
*Zhe Li,Hadrien Reynaud,Johanna P Müller,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出MCDM：一种无需标签、以自监督运动特征为条件的潜空间扩散模型，用于生成逼真的超声心动图视频；配合MAFE模块分离运动与外观特征，并通过伪重识别与伪光流两项辅助损失提升特征质量；在EchoNet-Dynamic上生成具有时间一致性和临床可信度的视频，显示自监督条件在心超合成中的可扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 心超视频对临床评估心功能至关重要，但隐私与标注困难导致有标签数据稀缺，限制了深度学习尤其是生成建模的发展。需要一种无标签、可扩展、能保持时间连贯与临床真实性的生成方法。

Method: 1) 设计MAFE模块，从视频中自监督地解耦提取运动与外观特征；2) 在MAFE训练中加入两种辅助目标：基于伪外观特征的重识别损失与基于伪光流的光流一致性损失；3) 构建MCDM潜空间扩散模型，以运动特征为条件进行视频生成，从而无需人工标签即可合成心超序列。

Result: 在EchoNet-Dynamic数据集上，MCDM实现与现有方法相当的生成质量，生成的视频时间连贯、外观与运动逼真，具备临床可用的真实性，无需依赖人工标签。

Conclusion: 自监督得到的运动条件可以有效驱动无标签的心超视频生成；MCDM+MAFE在有限标注场景下具备可扩展性和实用性，为隐私与标注受限的医学影像合成提供了新途径。

Abstract: Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.

</details>


### [65] [InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography](https://arxiv.org/abs/2512.09422)
*Zhe Li,Hadrien Reynaud,Alberto Gomez,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出一种用于超声心动图视频数据集蒸馏的新方法：通过运动特征提取、按类别构图并用Infomap选择代表样本，仅用25个合成视频在EchoNet-Dynamic上达到69.38%测试准确率。


<details>
  <summary>Details</summary>
Motivation: 超声心动图数据量快速增长，带来存储、计算与训练效率的压力；需要将大规模医疗视频压缩为小而精的集合，同时保留关键临床动态信息，以便高效训练与部署。

Method: 先进行运动特征提取以捕获时序动态；对每个类别构建图，基于样本间相似性形成图结构；使用Infomap社区发现算法从图中选取多样且信息量大的代表性合成视频，得到紧凑的数据子集；在EchoNet-Dynamic上以该子集训练/评估。

Result: 在EchoNet-Dynamic数据集上，仅用25个合成视频即可获得69.38%的测试准确率，表明合成子集保留了关键判别信息。

Conclusion: 基于运动特征与图社区选择的医疗视频数据集蒸馏方法有效且可扩展，能以极小合成集保持较高性能，为超声心动图等时序医学影像的高效学习提供可行路径。

Abstract: Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \(69.38\%\) using only \(25\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.

</details>


### [66] [FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds](https://arxiv.org/abs/2512.09423)
*Marco Pegoraro,Evan Atherton,Bruno Roy,Aliasghar Khani,Arianna Rampini*

Main category: cs.CV

TL;DR: 提出FunPhase：一种函数式周期自编码器，通过相位流形建模人体运动，改用函数空间解码以获得可任意时间分辨率的平滑轨迹；在重建误差上显著优于周期自编码器基线，并统一预测与生成，支持超分辨率与部分身体补全等任务。


<details>
  <summary>Details</summary>
Motivation: 人体运动同时涉及空间几何与时间动力学的强耦合，难以学习与预测。现有基于相位流形的周期自编码器虽有效，但可扩展性差、适用范围窄、时间解码离散，限制了跨数据集/骨架泛化与高质量连续采样。

Method: 提出FunPhase：学习一个捕捉局部周期性的相位流形；将传统的离散时间步解码替换为函数空间的周期解码器，使轨迹在连续时间上可求值与可微；由此实现可任意时间分辨率采样，并将预测与生成统一到同一可解释的相位空间中。

Result: 与以往周期自编码器基线相比，FunPhase有显著更低的重建误差；在更广任务范围（超分辨率、部分身体运动补全、跨骨架/数据集）上表现稳健，并在运动生成上达到与SOTA相当的性能。

Conclusion: 函数式周期自编码器通过相位流形与连续时间解码统一了运动预测与生成，提升了可扩展性与泛化能力，能输出平滑、可任意采样的轨迹，兼顾精度与多样任务适配。

Abstract: Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.

</details>


### [67] [UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents](https://arxiv.org/abs/2512.09435)
*Xufan He,Yushuang Wu,Xiaoyang Guo,Chongjie Ye,Jiaqing Zhou,Tianlei Hu,Xiaoguang Han,Dong Du*

Main category: cs.CV

TL;DR: 提出UniPart：一种两阶段潜变量扩散框架，利用统一几何-分割表示Geom-Seg VecSet，在无强外部分割器下实现图像引导的可控部件级3D生成，兼顾全局与规范空间以提高几何与分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有部件级3D生成要么依赖隐式分割难控粒度，要么依赖大型标注数据训练的外部分割器，限制可控性与泛化。作者观察到在整物体几何学习中自然会出现“部件感知”，希望将几何与分割统一到一个潜表示中，以无强监督代价获得可控部件生成。

Method: 提出Geom-Seg VecSet作为统一潜变量，联合编码物体几何与部件结构。在此基础上构建UniPart：两阶段潜扩散。阶段一：联合生成几何并进行潜层部件分割；阶段二：在整物体与部件特定潜向量的双重条件下进行部件级扩散生成。引入双空间（全局与规范空间）预测部件潜向量的生成方案，提升几何保真。

Result: 在广泛实验中，UniPart在分割可控性和部件级几何质量上优于现有方法。

Conclusion: 统一几何-分割潜表示与两阶段扩散、双空间生成能在无需强外部分割器的情况下，实现更可控、更高质量的部件级3D生成。

Abstract: Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.

</details>


### [68] [Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model](https://arxiv.org/abs/2512.09441)
*Jiantao Tan,Peixian Ma,Tong Yu,Wentao Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出一种基于VLM的增量学习框架：冻结图像编码器、用任务特定适配器学习新类；通过轻量级投影器混合进行跨任务表示校准；并用不确定性引导的推理策略选择最合适的图像特征，显著减轻跨任务类混淆并在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的增量学习在不同任务间难以区分类别，导致跨任务类混淆与遗忘。需要一种既能持续学习新类、又能在统一特征空间区分所有已学类别的方法，同时提升推理阶段的稳健性。

Method: 1) 冻结预训练VLM的图像编码器，针对每个任务加入任务专属Adapter以学习新知识；2) 设计由多种轻量投影器组成的混合模块，对跨任务表示进行校准，使不同任务的特征在同一空间更可分；3) 提出不确定性引导的推理策略，根据预测不确定性选择最合适的图像特征进行分类。

Result: 在多数据集、不同设置下进行广泛实验，所提方法在准确率等指标上优于现有最先进方法，表现更稳定、跨任务区分更好。

Conclusion: 通过任务特定适配器、混合轻量投影器的跨任务表示校准与不确定性引导推理，方法有效缓解类混淆与遗忘，实现更强的类增量学习性能。

Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.

</details>


### [69] [Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation](https://arxiv.org/abs/2512.09446)
*Nadeem Nazer,Hongkuan Zhou,Lavdim Halilaj,Ylli Sadikaj,Steffen Staab*

Main category: cs.CV

TL;DR: 提出DAPO，一种缺陷感知的提示优化方法，通过混合提示和渐进式调优，在分布偏移下实现零样本多类型与二分类的异常检测与分割，显著优于现有VLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有VLM（如CLIP）在分布偏移下利用文本提示具备强大异常检测能力，但缺乏对细粒度缺陷类型（如hole、cut、scratch）的识别能力，导致难以理解异常本质与根因、难以指导制造业的针对性改进；手工为每类缺陷设计提示既耗时又易受偏见影响。

Method: 提出DAPO：通过“固定文本锚+可学习token嵌入”的混合缺陷感知提示，并采用渐进式调优，将异常相关图像特征与对应文本语义对齐，实现在零样本、多类型与二分类以及分割任务上的统一优化，特别面向分布偏移情形。

Result: 在MPDD、VisA、MVTec-AD、MAD、Real-IAD以及内部数据集上，相比基线在图像级AUROC与AP上平均提升3.7%，在零样本新型异常定位上平均提升6.5%。

Conclusion: 细粒度缺陷语义的引入与缺陷感知提示优化能显著缩小“异常”与具体缺陷类别间的语义鸿沟，提升分布偏移与零样本场景的检测与分割性能，为制造业提供更可操作的异常理解与处置。

Abstract: Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like "hole", "cut", "scratch" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of "abnormal" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.

</details>


### [70] [Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework](https://arxiv.org/abs/2512.09461)
*Anabia Sohail,Mohamad Alansari,Ahmed Abughali,Asmaa Chehab,Abdelfatah Ahmed,Divya Velayudhan,Sajid Javed,Hasan Al Marzouqi,Ameena Saad Al-Sumaiti,Junaid Kashir,Naoufel Werghi*

Main category: cs.CV

TL;DR: 提出首个用于人类IVF胚胎细胞质细丝（CS）自动分析的两阶段深度学习框架：帧级检测+区域定位，并引入NUCE损失解决极端类别不平衡与特征不确定性，取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 胚胎选择是IVF瓶颈。现有自动评估多依赖传统形态/时序特征，忽略新兴生物标志物。CS与更快囊胚形成、更高分级和更好生存力相关，但当前评估需人工目检，主观且耗时，且受细丝细弱、低对比度影响大，迫切需要自动化、客观的方法。

Method: 1) 构建“人类在环”标注流程，从TLI视频中策划生物学验证的CS数据集（13,568帧，正样本极稀疏）。2) 两阶段框架：先进行帧级CS存在性分类，再在阳性帧进行CS区域定位（采用RF-DETR变体）。3) 提出NUCE损失：结合置信度感知重加权与收缩式嵌入项，使类内更紧凑、类间更分离，以缓解极端不平衡与特征不确定性。4) 在五种Transformer骨干上验证。

Result: NUCE在五种Transformer骨干上均提升F1分数；RF-DETR在细、低对比度CS结构的定位上达到SOTA性能。

Conclusion: 通过专用数据集、两阶段检测与NUCE损失，可稳定、准确地检测与定位CS，为将CS纳入胚胎选择的自动化流程奠定基础；源码将开源，便于复现与扩展。

Abstract: Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.

</details>


### [71] [Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing](https://arxiv.org/abs/2512.09463)
*Sander De Coninck,Emilio Gamba,Bart Van Doninck,Abdellatif Bey-Temsamani,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 论文在真实工业环境数据上验证一种隐私保护的计算机视觉框架，展示在三类应用中通过任务特定的视觉变换实现隐私-效用平衡并具备落地可行性。


<details>
  <summary>Details</summary>
Motivation: 工业计算机视觉的落地受制于隐私与效用的权衡：企业需要监控、导航与人体工学评估等功能，但又必须保护工人隐私与建立信任。此前提出的框架缺乏在真实生产环境中的系统验证，因此需要在代表性场景中评估其实用性和可部署性。

Method: 基于已提出的隐私保护框架，使用学习到的视觉变换对图像中与任务无关或敏感的信息进行模糊/遮蔽，同时保留完成任务所需的特征。在三类工业用例（木工生产监控、人感知AGV导航、多摄像头人体工学风险评估）上进行评估，包含定量的隐私-效用权衡分析与来自工业伙伴的定性反馈，以考察有效性、部署可行性与信任影响。

Result: 实验结果表明，面向任务的特定模糊化在保持任务性能的同时显著降低隐私风险；在三个真实用例中均取得有效监控/导航/评估表现，并从合作方获得正面反馈，显示出部署可行性与可信度提升。

Conclusion: 该框架已具备真实世界应用准备度，能在多领域实现隐私-效用平衡；论文给出跨场景的实践建议，支持以人为本、负责任的工业AI部署。

Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.

</details>


### [72] [Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach](https://arxiv.org/abs/2512.09471)
*Yiqun Wang,Lujun Li,Meiru Yue,Radu State*

Main category: cs.CV

TL;DR: 提出基于ViViT的时空融合嵌入框架，利用t=2的3D卷积管块重建云遮挡MSI，较ViT基线显著降低误差，并在引入SAR融合时进一步提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于ViT的时序重建方法多采用粗粒度时间嵌入，整段序列聚合导致时间细节丢失，尤其在早季作物制图中云覆盖严重腐蚀光谱信息，亟需在局部时间尺度上更稳健地重建被云遮挡的MSI。

Method: 提出基于Video Vision Transformer (ViViT) 的重建框架，设计时空融合嵌入：用t=2的3D卷积提取非重叠tubelets，保持局部时间一致性并降低跨日信息退化；在Transformer中进行时空联合建模；同时设置MSI-only与SAR-MSI两种场景进行训练与评估。

Result: 在2020年Traill County数据上，MTS-ViViT较MTS-ViT基线MSE下降2.23%；在加入SAR的SMTS-ViViT相对SMTS-ViT进一步提升，MSE改善10.33%，表明方法在云区光谱重建上更优。

Conclusion: 时空融合的ViViT框架通过局部时间管块嵌入有效缓解云导致的时序与光谱退化，提升MSI重建精度；SAR融合可进一步增强鲁棒性，为农业监测与早季作物制图提供更可靠的光谱重建。

Abstract: Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.

</details>


### [73] [Color encoding in Latent Space of Stable Diffusion Models](https://arxiv.org/abs/2512.09477)
*Guillem Arias,Ariadna Solà,Martí Armengod,Maria Vanrell*

Main category: cs.CV

TL;DR: 作者系统研究了Stable Diffusion潜空间中色彩是如何编码的，发现颜色沿“对手色”圆形轴主要由c_3、c_4通道表示，强度与形状主要由c_1、c_2承载，说明其潜空间具有可解释、接近高效编码的结构。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在视觉质量上已很强，但对其内部如何表征感知属性（颜色、形状）缺乏细致理解；更可解释的表征有助于模型理解、编辑和可解耦生成。

Method: 构造受控合成数据集；对Stable Diffusion的潜表示做主成分分析（PCA）与相似度度量；逐通道分析潜变量，定位与颜色、强度、形状相关的通道与轴向结构。

Result: 发现颜色信息沿圆形、对手轴编码，主要集中在潜通道c_3、c_4；强度与形状信息主要在c_1、c_2；整体潜空间呈现可解释、近似高效编码的组织。

Conclusion: Stable Diffusion的潜空间对颜色与形状具有部分解耦且可解释的结构，为后续模型可解释性研究、编辑操作与更可解耦的生成框架设计提供基础。

Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.

</details>


### [74] [MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images](https://arxiv.org/abs/2512.09489)
*Shuaihao Han,Tingfa Xu,Peifu Liu,Jianan Li*

Main category: cs.CV

TL;DR: 提出MODA多光谱航拍检测大规模数据集与OSSDet框架，利用级联谱-空调制、光谱相似聚合、目标感知掩膜与跨谱注意力，有效提升小目标、复杂背景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: RGB航拍检测在小目标与复杂背景中判别信息不足，MSI虽含更多谱信息但缺乏大规模训练数据，限制方法发展与评测。

Method: 1) 数据集：构建MODA，包含14,041幅多光谱图像与330,191个标注，覆盖多种复杂场景。2) 模型：提出OSSDet——级联谱-空调制以优化目标感知；基于光谱相似性的特征聚合以强化目标内部相关性；目标感知掩膜抑制背景；在显式目标引导下进行跨谱注意力以进一步提炼目标相关表征。参数量与效率与现有方法可比。

Result: 在广泛实验中，OSSDet在各项指标上优于现有方法，同时保持相近的参数规模与推理效率。

Conclusion: MODA为多光谱航拍目标检测提供了关键数据基础；OSSDet有效融合谱与空信息并利用目标先验，显著提升检测效果，适用于小目标与强背景干扰场景。

Abstract: Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.

</details>


### [75] [StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio](https://arxiv.org/abs/2512.09492)
*Abdullah Al Mamun,Miaohua Zhang,David Ahmedt-Aristizabal,Zeeshan Hayder,Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: 提出StateSpace-SSL：基于Vision Mamba的线性时间自监督框架，通过方向扫描建模叶片上病斑的长程连续性，配合原型驱动的师生目标以获得稳定、病灶感知的表示；在三套公开植物病害数据集上优于CNN/Transformer式SSL，且特征更紧凑聚焦病灶。


<details>
  <summary>Details</summary>
Motivation: 农业图像具有沿叶脉与表面连续变化的病斑模式，CNN难以捕捉跨结构的长程依赖，ViT在高分辨率下注意力成本二次方膨胀；需要一种既能高效建模长程连续性又适配高分辨率叶片图像的SSL方法。

Method: 采用Vision Mamba（状态空间模型）作为编码器，以线性复杂度通过多方向扫描在叶面上建模长程依赖；设计原型驱动的师生一致性学习目标，在多视角（augments）间对齐表示，并利用有限标注促进稳定、病灶感知的特征学习。

Result: 在三个公开植物病害数据集上，相比CNN与Transformer为主的SSL基线，在多项评测指标上稳定取得更高性能；可视化显示其学习到更紧凑、聚焦病灶的特征图。

Conclusion: 线性状态空间建模结合原型式师生目标能更好地适配农业高分辨率图像与病斑的连续结构，实现更高效且更有效的植物病害自监督表征学习。

Abstract: Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.

</details>


### [76] [Gradient-Guided Learning Network for Infrared Small Target Detection](https://arxiv.org/abs/2512.09497)
*Jinmiao Zhao,Chuang Yu,Zelin Shi,Yunpeng Liu,Yingdi Zhang*

Main category: cs.CV

TL;DR: 提出GGL-Net，用梯度幅值图与双分支特征提取、双向引导融合，强化边缘与多尺度信息，在NUAA-SIRST与NUDT-SIRST上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 红外小目标体积小、纹理弱，易被复杂背景淹没，现有方法边缘定位不准、特征不显著，需要增强边缘与多尺度融合能力以提升检测精度与鲁棒性。

Method: 1) 首次将梯度幅值图引入深度学习小目标检测，突出边缘。2) 设计双分支特征提取网络：通过梯度补充模块(GSM)把原始梯度信息编码到深层，并结合注意力机制增强特征表达。3) 构建双向引导融合模块(TGFM)，针对不同层级特征，进行自上而下与自下而上的双向引导，促进多尺度特征的有效融合，兼顾语义与细节。

Result: 在NUAA-SIRST与NUDT-SIRST数据集上取得SOTA性能（文中称“广泛实验验证”，但摘要未列具体指标与提升幅度）。

Conclusion: 引入梯度信息与双向引导多尺度融合可显著改善红外小目标的边缘定位与检出性能；方法通用且代码已开源，具备应用潜力。

Abstract: Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net

</details>


### [77] [Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction](https://arxiv.org/abs/2512.09525)
*Hongyou Zhou,Cederic Aßmann,Alaa Bejaoui,Heiko Tzschätzsch,Mark Heyland,Julian Zierke,Niklas Tuttle,Sebastian Hölzl,Timo Auer,David A. Back,Marc Toussaint*

Main category: cs.CV

TL;DR: 提出一种结合3D空间变换网络与自编码器的框架，从骨折胫骨CT预测个体化健康胫骨重建目标，用于术前规划。


<details>
  <summary>Details</summary>
Motivation: 复杂胫骨骨折的术前规划困难，难以从碎裂CT想象理想三维复位形态；需要能从患者受损CT重建其健康骨形态的自动化方法，辅助个体化手术规划与内植物定位。

Method: 两阶段：1) 训练3D改造的STN，将原始CT配准到与联合训练的“胫骨原型”一致的标准坐标系，实现全局空间标准化；2) 训练多种AE架构以建模健康胫骨的正常变异。两者均设计为可处理带掩膜的输入（遮蔽骨折/缺损区域），从而对骨折CT进行编码并在标准坐标中解码生成患者特异的健康胫骨预测。包含i) 3D STN用于全局配准，ii) 对比多种AE以建模骨CT，iii) 扩展STN与AE以适配掩膜输入，实现预测性重建。

Result: 实现了从骨折胫骨CT到患者特异健康胫骨的预测重建，展示了3D配准与多种AE在骨形态建模上的有效性与鲁棒性（特别是对缺损/遮挡输入）。给出了比较分析与公开项目页面。

Conclusion: 面向复杂胫骨骨折术前规划的可行框架：标准化配准+掩膜鲁棒AE，可自动生成个体化健康胫骨目标，为复位与内固定设计提供依据；方法具有普适性，可扩展至其他骨骼重建任务。

Abstract: Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair

</details>


### [78] [A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution](https://arxiv.org/abs/2512.09546)
*Murat Karayaka,Usman Muhammad,Jorma Laaksonen,Md Ziaul Hoque,Tapio Seppänen*

Main category: cs.CV

TL;DR: 提出DDSRNet：结合空间域与小波域的轻量超分网络，利用Spatial-Net提取浅层特征，DWT分解低/高频，低频单独增强，高频三子带共享CNN细化，最后用IDWT重建，兼顾效果与计算效率，特别适用于高光谱图像超分。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱图像超分方法要么只在空间域学习，难以显式建模频带结构；要么在频域处理但参数/计算量大、泛化差。需要一种既能利用频域先验、又保持模型轻量且高效的方案，以在多数据集上获得具竞争力的重建质量。

Method: 构建双域网络DDSRNet：1) Spatial-Net进行残差学习与双线性上采样获取浅层/初步重建；2) 基于DWT的低频增强分支，针对LL子带优化整体结构；3) 共享权重的高频细化分支，用单个CNN同时处理LH/HL/HH子带，提升边缘/纹理；通过DWT实现子带分解，最后用IDWT将增强后的各子带与低频合成为高分辨率图像。

Result: 在三个高光谱数据集上，以更低的计算开销达到与先进方法相当或更优的超分性能（定量与视觉质量均有提升），证明共享高频分支与双域融合的有效性。

Conclusion: 空间-频域融合与小波子带共享细化策略能在保持轻量计算成本的同时提升高光谱超分效果；DWT/IDWT提供了有效的结构化分解与重建框架，适合资源受限场景下的高光谱超分。

Abstract: This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.

</details>


### [79] [Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment](https://arxiv.org/abs/2512.09555)
*Yuan Li,Zitang Sun,Yen-ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 论文指出当前基于VLM的无参考图像质量评价（BIQA）存在“描述-评分不一致”和“推断不稳定”的问题；通过分析原因并提出两阶段调优（先感知特征、后质量推断）来提升稳定性与一致性，在多数据集上显著提升SRCC/PLCC并降低不稳定率。


<details>
  <summary>Details</summary>
Motivation: VLM具有类人语义推理能力，被期待能像人类一样先感知图像质量要素再做判断。但实际出现文本描述与最终质量评分矛盾、推理过程中分数不稳定等与人类不符的现象。需要弄清矛盾与不稳定的成因，并提升VLM在BIQA场景中的可解释性、稳定性与可靠性。

Method: 1) 因果/相关性分析：估计最终质量预测与生成的视觉特征（或描述）的关系，发现预测并未充分落地到特征，二者逻辑联系弱。2) 中间层解码：解码VLM中间层，观察到模型常依赖少量候选token，导致输出易受采样或小扰动影响而不稳定。3) 两阶段调优：阶段一专注视觉感知学习，获取稳定且丰富的质量相关视觉特征；阶段二将质量推断仅基于这些特征进行训练，显式解耦“感知”与“推断”，减少语言生成对质量评分的干扰。

Result: 在SPAQ与KONIQ上验证：预测不稳定率从22.00%降至12.39%；在LIVE、CSIQ、SPAQ、KONIQ四个数据集上，相比基线平均SRCC/PLCC分别提升0.3124/0.3507；同时推理过程的可依赖性与一致性增强。

Conclusion: 问题根源在于VLM的质量评分未充分锚定视觉特征，且中间层对少量token的依赖导致不稳定。通过“感知—推断”解耦的两阶段调优，可提升BIQA的稳定性、可靠性与类人推理特性，并实现显著性能增益。

Abstract: Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.

</details>


### [80] [From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection](https://arxiv.org/abs/2512.09565)
*Faraz Ali,Muhammad Afaq,Mahmood Niazi,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出DNS-HyXNet：用轻量xLSTM替代图方法，实现单阶段多分类的DNS隧道实时检测；在公共数据集上达99.99%准确率与0.041毫秒/样本延迟，兼顾精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有图方法（如GraphTunnel）虽准确但因递归解析与图构建耗时、耗算力，不适合实时部署。需要一种低延迟、低资源、可部署且保持高精度的DNS隧道检测方法。

Method: 将域名token化生成嵌入，配合归一化的数值型DNS特征，输入两层扩展LSTM（xLSTM）序列模型，直接从包序列学习时序依赖；无需图重建，实现单阶段多类别分类；在两个公共基准数据集上用小内存与快速推理的超参进行训练与评估。

Result: 在DNS-Tunnel-Datasets各划分上准确率最高达99.99%，宏平均的Precision/Recall/F1均>99.96%；单样本检测延迟仅0.041毫秒，显示良好的可扩展性与实时性。

Conclusion: 基于xLSTM的序列建模可有效替代高开销的递归图生成，在通用硬件上实现可部署、节能、实时的DNS隧道检测。

Abstract: Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.

</details>


### [81] [Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment](https://arxiv.org/abs/2512.09573)
*Yuan Li,Zitang Sun,Yen-Ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 论文指出：现有基于多模态大语言模型（MLLM）的图像质量评价虽能生成解释，但对模糊、噪声、压缩等低层失真识别薄弱、评估不稳定。作者提出低层失真分类任务与组件级分析，发现问题源于对训练模板过拟合与视觉-语言对齐阶段弱化低层特征。通过加强视觉编码器对齐，失真识别准确率从14.92%提升到84.43%，表明需在视觉侧加入专门约束以获得更可解释、连贯的推理。


<details>
  <summary>Details</summary>
Motivation: MLLM在IQA中能给出自然语言解释，但对基本低层失真检测差、结果不一致，质疑其是否真正感知关键视觉特征。需要系统性任务与分析验证其视觉表征与对齐机制是否损伤低层信息。

Method: 1) 设定低层失真感知任务：要求模型分类具体失真类型。2) 组件级分析：拆分并评估视觉编码器、对齐/投影层及语言头，诊断过拟合模板与对齐阶段信息损失。3) 语义距离度量：计算视觉特征与对应语义token在对齐前后以及组件微调前后的语义距离变化。4) 对视觉编码器进行组件化微调/对齐强化，并比较前后性能。

Result: 基线MLLM对失真分类准确率低（约14.92%），且质量评分受训练模板偏置影响。对齐阶段导致低层特征弱化或丢失。在强化视觉编码器对齐后，失真识别准确率显著提升至84.43%，一致性与可解释性改善。

Conclusion: MLLM具备表征低层失真的潜力，但现有对齐与训练范式使其过拟合模板并削弱低层特征。应在视觉编码器加入专门约束与对齐策略，以获得文本可解释的视觉表示，提升IQA中对低层失真的识别、评分一致性与推理连贯性。

Abstract: Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.

</details>


### [82] [Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis](https://arxiv.org/abs/2512.09576)
*David Seu,Nicolas Longepe,Gabriel Cioltea,Erik Maidik,Calin Andrei*

Main category: cs.CV

TL;DR: 提出一个可扩展的混合建模系统，用遥感与环境协变量估计农田土壤性质（SOC、N、P、K、pH），兼用物理启发的RTM特征与基金模型嵌入，在欧洲农田数据上通过严格空间阻断与独立测试集验证，SOC与N精度最佳，并用保形校准量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 农业决策 increasingly 受环境变量影响，但易获取、可扩展的土壤评估工具匮乏。传统方法要么依赖昂贵采样，要么泛化差，因此需要一个既可扩展又能在未见区域保持精度、且具可解释性的土壤性质估计框架。

Method: 混合建模：1）间接法（利用土壤驱动因子与代理变量）+ 2）直接光谱建模。扩展点：从辐射传输模型（RTM）提取物理可解释协变量；结合大型基础模型产生的非线性嵌入。数据：覆盖欧洲多土候区的和谐化农田土壤数据。验证：严格空间阻断、分层划分、统计上可分离的训练/测试集；并进行保形校准以评估不确定性与覆盖率。

Result: 在未见地点与独立测试集上，SOC与N表现最佳：SOC MAE=5.12 g/kg、CCC=0.77；N MAE=0.44 g/kg、CCC=0.77；达到目标置信水平下约90%的覆盖率。

Conclusion: 该系统在真实、严格的空间泛化场景中可靠估计关键土壤性质，具备可扩展性与不确定性量化能力，可用于数字农业与碳市场等土壤定量评估场景，并可迁移到相关领域。

Abstract: Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.

</details>


### [83] [Hands-on Evaluation of Visual Transformers for Object Recognition and Detection](https://arxiv.org/abs/2512.09579)
*Dimitrios N. Vlachogiannis,Dimitrios A. Koutsomitropoulos*

Main category: cs.CV

TL;DR: 论文比较多种视觉Transformer（ViT）与CNN在分类、检测与医学影像任务上的表现；层级与混合式Transformer（如Swin、CvT）在精度-算力权衡上最佳，并在医学影像中借助数据增广取得显著提升，整体上常优于CNN，尤其需全局上下文的场景。


<details>
  <summary>Details</summary>
Motivation: CNN偏重局部感受野，难以建模全局关系；ViT通过自注意力具备全局建模能力。作者想系统评估不同ViT变体（纯、层级、混合）与传统CNN在通用与医学任务上的优劣与资源效率，并探究数据增广对医学影像的影响。

Method: 在ImageNet进行图像分类、在COCO进行目标检测、在ChestX-ray14进行医学图像分类；比较纯ViT、层级ViT（如Swin）、混合ViT（如CvT）与典型CNN；评估精度与计算成本；对医学影像引入多种数据增广并量化其效果。

Result: 层级与混合式Transformer（Swin、CvT）在准确率与计算开销间取得最佳平衡；多任务上常优于CNN；在ChestX-ray14上，结合数据增广的Swin Transformer表现提升显著。

Conclusion: ViT总体具有竞争力并在需全局上下文的任务（如医学影像）中往往优于CNN；层级/混合ViT是实践中的优选，数据增广进一步强化其医学影像性能。

Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.

</details>


### [84] [Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation](https://arxiv.org/abs/2512.09580)
*Hancheng Zhu,Xinyu Liu,Rui Yao,Kunyang Sun,Leida Li,Abdulmotaleb El Saddik*

Main category: cs.CV

TL;DR: 提出CA-ATP方法：用内容自适应曲线映射与属性文本引导，实现既考虑图像内容多样性又满足用户风格偏好的图像润色，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有润色方法多用全局统一的像素颜色映射，忽略由内容引起的颜色差异，难以适应多样的颜色分布与用户风格偏好，导致无法进行细粒度、上下文相关的调整。

Method: 1) 内容自适应曲线映射模块：以一组基曲线建立多重颜色映射关系，并学习空间权重图，使同色值在不同空间上下文中可被不同曲线变换，实现内容感知的颜色调整。2) 属性文本预测模块：从多种图像属性生成描述用户风格偏好的属性文本表示，并通过多模态模型将其与视觉特征融合，为润色提供可控、人性化的指导。

Result: 在多个公开数据集上进行大量实验，方法在定量指标上达到或超过现有最优水平，定性上呈现更符合内容与风格偏好的效果。

Conclusion: CA-ATP能通过内容自适应映射与属性文本引导实现更灵活、可控的图像润色，兼顾颜色多样性与用户偏好，达到SOTA表现。

Abstract: Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.

</details>


### [85] [UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision](https://arxiv.org/abs/2512.09583)
*Alberto Rota,Mert Kiray,Mert Asim Karaoglu,Patrick Ruhkamp,Elena De Momi,Nassir Navabm,Benjamin Busam*

Main category: cs.CV

TL;DR: UnReflectAnything 是一个仅用单张 RGB 图像的高光去除框架，通过预测高光图与无反射漫反射重建来还原物体本色。它用冻结的 ViT 编码器提取多尺度特征、轻量头定位高光，并用token级修复模块在特征层面补全被高光破坏的区域。为解决缺少成对监督，提出虚拟高光合成：基于单目几何、Fresnel 感知着色与随机光照渲染物理合理的高光，从任意RGB数据训练。方法跨自然与手术场景泛化，在多基准上达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 高光会改变外观、遮蔽纹理并干扰几何推理，尤其在手术等非朗伯表面与复杂光照场景中更严重。现有方法常需多模态/多视角或成对监督数据，限制了通用性与数据规模；因此需要一种在单张 RGB 场景下、无需配对数据也能鲁棒去高光的通用方法。

Method: 1) 架构：冻结的 ViT 编码器抽取多尺度特征；轻量化检测头定位镜面区域并输出高光图；token-level inpainting 在特征token层面修复被高光污染的patch；最后解码生成无反射的漫反射图。2) 训练数据：提出 Virtual Highlight Synthesis，用单目估计的几何（法线/深度）、Fresnel-aware着色与随机光照，物理可解释地渲染高光，将任意RGB变为“带高光/无高光”的伪配对，用于监督。3) 仅用RGB输入，无需额外传感或多视角。

Result: 在自然图像与手术图像多数据集上，与SOTA方法相比取得有竞争力甚至更优的定量/定性表现；同时展示跨域泛化能力，能在非朗伯、非均匀照明导致的强烈高光下恢复纹理与形状线索。

Conclusion: 通过冻结ViT特征、轻量定位头与token级修复，并辅以物理驱动的虚拟高光合成，UnReflectAnything 在无配对监督、单RGB输入条件下实现高质量高光去除，具有良好的跨域泛化与实用性。

Abstract: Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/

</details>


### [86] [CS3D: An Efficient Facial Expression Recognition via Event Vision](https://arxiv.org/abs/2512.09592)
*Zhe Wang,Qijin Song,Yucen Peng,Weibang Bai*

Main category: cs.CV

TL;DR: 提出CS3D：面向事件相机的人机交互表情识别，分解3D卷积降算力与能耗，引入软脉冲神经元与时空注意力，在多数据集上精度优于RNN/Transformer/C3D，仅用C3D能耗的约22%。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高时间分辨率、低延迟、低光鲁棒等方面优于RGB，相当适合捕捉表情动态；但主流深度模型能耗高、难上边缘设备，尤其事件数据高频导致成本飙升，需要兼顾准确与能效的新方法。

Method: 提出CS3D框架：将传统3D卷积分解以降低计算复杂度与能耗；结合软脉冲神经元（可微/低能耗地模拟脉冲神经元）以增强信息保持与高效推理；引入时空注意力机制以聚焦关键时空特征；整体面向事件相机表情识别任务设计。

Result: 在多个数据集上，CS3D的识别精度超过RNN、Transformer与C3D等基线；在同一设备上，能耗仅为原始C3D的21.97%。

Conclusion: CS3D在事件视觉表情识别中实现更高精度与显著更低能耗，适合边缘部署的人机交互场景；软脉冲神经元与时空注意力的结合有效提升信息保持与判别力。

Abstract: Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.

</details>


### [87] [Rethinking Chain-of-Thought Reasoning for Videos](https://arxiv.org/abs/2512.09616)
*Yiwu Zhong,Zi-Yuan Hu,Yin Li,Liwei Wang*

Main category: cs.CV

TL;DR: 作者提出：视频推理不必依赖冗长思维链与大量视觉token；通过压缩视觉token并引导模型生成简短推理即可在多基准上达到有竞争力表现，同时显著提升推理效率、无需人工CoT标注或监督微调。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大模型在推理时往往依赖长CoT与海量视觉token，带来高计算开销与低效率。作者在基准研究中观察到：更精炼的推理与更少的关键视觉信息可能已足够，因此希望验证“简洁推理+压缩视觉信息”是否能保持或提升性能并提高效率。

Method: 提出一个后训练与推理框架：1) 视觉侧对视频帧/patch进行token压缩，减少输入视觉token；2) 语言侧在回答前先生成短小的推理痕迹（brief reasoning traces），而非冗长CoT；3) 整体作为高效后处理与推理策略，无需人工CoT标注或监督微调即可增强视频推理能力。

Result: 在多种视频推理基准上，模型在维持或接近SOTA性能的同时大幅提升推理效率；简短推理+压缩token的组合在实践中运作良好，推理速度和计算成本显著下降。

Conclusion: 长而类似人类的思维链对通用视频推理并非必要；更简洁的推理与更少的视觉token能兼顾有效性与效率。作者计划开源代码，表明该策略具备可复现与推广价值。

Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.

</details>


### [88] [FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation](https://arxiv.org/abs/2512.09617)
*Hubert Kompanowski,Varun Jampani,Aaryaman Vasishta,Binh-Son Hua*

Main category: cs.CV

TL;DR: 提出一种轻量级外观迁移方法，将参考图像的材质/纹理/风格注入到多视角扩散模型的生成中，同时保持对象几何与多视角一致性。核心做法是在反向采样期间从对象与参考的少量自注意力层特征聚合并引导目标图像生成，仅需少量样本即可适配并获得多样外观。


<details>
  <summary>Details</summary>
Motivation: 现有多视角扩散模型虽能生成跨视角一致的真实感图像，但相比显式3D表示（网格/辐射场），在材质、纹理、风格等外观可控性方面不足，难以在保持几何与视角一致的前提下灵活编辑外观。

Method: 在一个预训练多视角扩散框架内引入三条去噪进程，分别对应原始对象、参考外观与目标图像；在反向扩散的阶段，从对象与参考分支选取少量层的自注意力特征，进行汇聚并注入以引导目标分支生成，从而在不改变几何的情况下转移参考的外观属性。方法为轻量级适配，仅需少量训练样本即可让模型具备外观感知能力。

Result: 在少量数据适配后，模型能稳定地将参考图像的材质/纹理/风格迁移到目标对象上，生成跨视角一致且几何保持的结果，展示出较强的外观多样性与可控性。

Conclusion: 通过在反向采样中聚合并注入自注意力特征，实现对预训练多视角扩散模型的低成本外观迁移，使隐式生成3D表示在实用中更具外观编辑能力与灵活性。

Abstract: Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.
  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.

</details>


### [89] [Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder](https://arxiv.org/abs/2512.09626)
*Yousef Azizi Movahed,Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: 论文研究用结构化运动特征对手-物交互中的原子状态（靠近、抓取、持握）进行细粒度分类，并意外发现将双向RNN的序列长度设为1可作为高容量静态编码器，显著提升精度至97.60%，对最难的过渡类“抓取”达到0.90的F1。


<details>
  <summary>Details</summary>
Motivation: 现有手-物交互意图预测困难，尤其是对细粒度、瞬态的原子交互状态识别（例如从靠近过渡到抓取）。作者希望用可解释、轻量的方法，验证序列建模是否必要，并为低层次交互识别建立基线。

Method: 从MANIAC原始视频中通过结构化数据工程提取并清洗统计-运动学特征，得到27,476个样本；每个样本是短时间窗内的关系与动态特征向量。比较静态分类器（MLP）与时序模型（RNN/双向RNN）。关键实验将BiRNN的序列长度设为1，使其退化为高容量的静态特征编码器；调参与训练以优化分类性能。

Result: 总体准确率97.60%；对最具挑战的过渡类“抓取”获得平衡F1=0.90；发现序列长度为1的BiRNN优于常规序列建模与MLP，显示模型作为静态编码器的优势。

Conclusion: 在结构化、可解释的运动学特征上，重时序建模并非必需；将BiRNN以seq_length=1用作静态高容量编码器可显著提升细粒度交互状态分类性能，为轻量架构在低层手-物交互识别设定了新基准。

Abstract: Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.

</details>


### [90] [Benchmarking SAM2-based Trackers on FMOX](https://arxiv.org/abs/2512.09633)
*Senem Aktas,Charles Markham,John McDonald,Rozenn Dahyot*

Main category: cs.CV

TL;DR: 论文对基于 SAM2 的目标跟踪方法在快速运动目标（FMO）数据集上的表现进行基准评估，发现 DAM4SAM 与 SAMURAI 在高难度序列上整体更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有许多以单帧示例模板初始化、随后进行跟随与分割的 SAM2 扩展式跟踪器在常规场景表现优异，但其在快速运动（模糊、形变、遮挡、出视野、尺度骤变）场景的鲁棒性缺乏系统性检验；作者旨在通过专门的 FMO 数据集基准，揭示这些最先进方法在极端运动条件下的局限与行为差异。

Method: 选取多种高性能基于 SAM2 的单模板分割跟踪器（SAM2、EfficientTAM、DAM4SAM、SAMURAI），在包含快速运动目标的专门数据集上统一评测；从更细粒度维度分析其跟踪行为与失败模式，对比不同序列难度下的性能。

Result: 整体而言，DAM4SAM 与 SAMURAI 在更具挑战性的序列上表现更好；相较之下，其他方法在高速运动导致的外观剧变与模糊场景中更易失效。

Conclusion: 基于 SAM2 的现有方法对快速运动的鲁棒性存在差异，DAM4SAM 与 SAMURAI 显示出更强适应性；对复杂动态场景仍需改进（如更强的运动建模、模糊鲁棒特征与再检测机制）。

Abstract: Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.

</details>


### [91] [Kaapana: A Comprehensive Open-Source Platform for Integrating AI in Medical Imaging Research Environments](https://arxiv.org/abs/2512.09644)
*Ünal Akünal,Markus Bujotzek,Stefan Denner,Benjamin Hamm,Klaus Kades,Philipp Schader,Jonas Scherer,Marco Nolden,Peter Neher,Ralf Floca,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: Kaapana 是一个面向医学影像研究的开源平台，提供模块化、可扩展的一体化工具链（数据摄取、队列构建、流程编排、结果查看），把算法“带到数据处”以满足隐私与监管要求，促进多中心、可复现、可协作的大规模研究。


<details>
  <summary>Details</summary>
Motivation: 当前多中心大规模医学影像研究受限于监管合规、软件碎片化和多机构协作难，往往依赖临时拼凑的不可复现工具链，难以扩展与协作。需要一种既能统一工具、又能在数据本地化前提下开展分布式实验的平台。

Method: 构建一个模块化、可扩展的统一平台：集成数据摄取、队列/队列管理（cohort curation）、可编排的处理工作流与前端结果检视；通过“算法到数据”的范式实现各机构本地数据可控并参与分布式实验；将工作流编排与研究者界面紧密集成以降低技术门槛。

Result: 平台实现并开源（GitHub: kaapana/kaapana），可支持从本地原型到全国性网络的多种用例，提升了可复现性、协作性与规模化能力，减少技术开销。

Conclusion: Kaapana 作为开源医学影像研究平台，解决了多中心研究中的合规与协作痛点，提供端到端、可复现的工作流与统一界面，促进在不转移敏感数据的前提下开展大规模分布式研究与模型开发。

Abstract: Developing generalizable AI for medical imaging requires both access to large, multi-center datasets and standardized, reproducible tooling within research environments. However, leveraging real-world imaging data in clinical research environments is still hampered by strict regulatory constraints, fragmented software infrastructure, and the challenges inherent in conducting large-cohort multicentre studies. This leads to projects that rely on ad-hoc toolchains that are hard to reproduce, difficult to scale beyond single institutions and poorly suited for collaboration between clinicians and data scientists. We present Kaapana, a comprehensive open-source platform for medical imaging research that is designed to bridge this gap. Rather than building single-use, site-specific tooling, Kaapana provides a modular, extensible framework that unifies data ingestion, cohort curation, processing workflows and result inspection under a common user interface. By bringing the algorithm to the data, it enables institutions to keep control over their sensitive data while still participating in distributed experimentation and model development. By integrating flexible workflow orchestration with user-facing applications for researchers, Kaapana reduces technical overhead, improves reproducibility and enables conducting large-scale, collaborative, multi-centre imaging studies. We describe the core concepts of the platform and illustrate how they can support diverse use cases, from local prototyping to nation-wide research networks. The open-source codebase is available at https://github.com/kaapana/kaapana

</details>


### [92] [VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification](https://arxiv.org/abs/2512.09646)
*Wanyue Zhang,Lin Geng Foo,Thabo Beeler,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出VHOI：将稀疏的人/物体关键点轨迹致密化为HOI掩码序列，并以此微调视频扩散模型，实现可控且真实的人-物交互视频生成。核心是颜色编码的HOI感知运动表示，可区分人/物及身体部位动态，达成SOTA并支持从导航到交互的端到端生成。


<details>
  <summary>Details</summary>
Motivation: 可控视频生成在HOI场景中很难：1) 人与物体的交互动力学复杂且实例相关；2) 现有控制信号存在易用性与信息量的权衡——关键点等稀疏信号易获取但缺乏实例感知，光流/深度/3D网格等致密信号信息丰富但成本高。需要一种既易获取又具备足够语义与动力学信息的控制条件。

Method: 两阶段框架VHOI：阶段一，将输入的稀疏轨迹（人、物关键点等）通过模型致密化为HOI掩码序列，并采用新颖的HOI感知运动表示，用颜色编码区分人/物及身体部位级的运动；阶段二，在这些致密掩码条件下，对视频扩散模型进行微调，使其学会理解并生成符合HOI动力学的可控视频。该表示将人体先验注入条件信号，提升模型对交互的理解与合成能力。

Result: 在可控HOI视频生成基准上达到SOTA；在实例感知与运动细节上优于仅用稀疏或昂贵致密信号的方法。能够从非交互阶段的导航动作自然过渡到交互动作，端到端生成更完整场景。

Conclusion: VHOI以低成本的致密掩码条件弥合稀疏与致密控制信号之间的鸿沟，颜色编码的HOI运动表示显著增强了模型对人-物交互动力学的理解，带来更真实、可控、可扩展的HOI视频生成，并可泛化到包含导航到交互的完整过程。

Abstract: Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.

</details>


### [93] [IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting](https://arxiv.org/abs/2512.09663)
*Tao Zhang,Yuyang Hong,Yang Xia,Kun Ding,Zeyu Zhang,Ying Wang,Shiming Xiang,Chunhong Pan*

Main category: cs.CV

TL;DR: 论文提出首个用于评估红外图像多模态理解能力的高质量基准IF-Bench，并给出一种免训练的生成式视觉提示方法GenViP，将红外图像翻译为语义与空间对齐的RGB图像以减少域差异。对40+开源/闭源MLLM进行系统评测，结果显示模型规模、架构与推理范式显著影响红外理解，GenViP在多模型上带来稳定且显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在可见光RGB领域表现突出，但对红外图像的理解能力尚未被系统评估与优化；缺乏专门基准与方法，且红外与RGB存在显著域偏移，限制模型泛化。

Method: 1) 构建IF-Bench：从23个红外数据集汇集499张图像和680个精心标注的VQA样例，覆盖10个理解维度；采用循环评测、双语评测与混合判定策略提高评测可靠性。2) 提出GenViP：一种免训练的生成式视觉提示，利用先进图像编辑/翻译模型将红外图像转换为与原图在语义与空间上对齐的RGB图像，再交由MLLM处理，从而缓解域偏移。

Result: 对40+模型的系统性评测揭示模型规模、架构与推理范式对红外理解的影响；GenViP在广泛的MLLM上带来显著且一致的性能提升。

Conclusion: IF-Bench为红外图像多模态理解提供了首个系统评测框架，并通过GenViP有效减轻红外与RGB域差异，显著提升现有MLLM在红外场景下的表现，为后续模型设计与训练策略提供实证依据。

Abstract: Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.

</details>


### [94] [OxEnsemble: Fair Ensembles for Low-Data Classification](https://arxiv.org/abs/2512.09665)
*Jonathan Rystrøm,Zihao Fu,Chris Russell*

Main category: cs.CV

TL;DR: 提出OxEnsemble，在小样本且群体不平衡条件下，通过约束各基学习器的公平性并在集成时聚合预测，实现数据与计算高效的公平分类；提供理论保证，并在多项医学影像数据上取得更稳健的公平-准确权衡。


<details>
  <summary>Details</summary>
Motivation: 小样本与跨人群分布不均在医学影像等高风险领域普遍存在，常见方法在数据稀缺时难以稳定地满足公平性要求，且假阴性代价极高，需要一种既能高效利用有限数据又能可靠保证公平性的训练策略。

Method: 提出OxEnsemble：训练多个满足公平性约束的模型（或子模型），并在推理时聚合其预测；在训练/校准中精心复用留出数据以可靠地施加公平性约束；整体计算量接近对单模型微调或评估的成本；并给出相应的理论保证。

Result: 在多个人工挑战的医学影像分类数据集上，相比现有方法，OxEnsemble产生更一致的结果，并在公平性与准确率之间实现更优权衡。

Conclusion: OxEnsemble在低数据、不平衡环境下实现了数据与计算高效的公平分类，兼具理论保证与实证优势，可作为医学影像等高风险领域的实用方案。

Abstract: We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.
  We propose a novel approach \emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.

</details>


### [95] [An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence](https://arxiv.org/abs/2512.09670)
*Gil Weissman,Amir Ivry,Israel Cohen*

Main category: cs.CV

TL;DR: 提出一个全自动“提示-执行”(Tip-and-Cue)卫星成像任务与调度框架：从外部/历史数据自动发现目标并优先级排序，生成满足多传感器约束的成像任务，跨星座基于连续效用函数优化调度，并用AI模型处理影像与生成结构化报告；以AIS辅助的海事船舶跟踪为验证，框架可扩展到智慧城市与灾害响应。


<details>
  <summary>Details</summary>
Motivation: 星座数量增多、任务延迟降低、传感器多样化使地球观测自动化机会大增，但如何从海量提示生成高价值任务、跨多星优化调度、并自动解析结果形成可执行洞见仍缺乏端到端方案。

Method: 提出Tip（从外部数据或先验影像分析提取时空目标与优先级）与Cue（根据传感器约束、时间窗口与效用函数形成的成像任务）的框架：自动生成候选任务；以连续效用函数衡量观测价值并进行多星调度优化；对获取影像用AI模型（目标检测、视觉-语言模型）自动处理；输出结构化可视化报告以反馈后续任务。

Result: 在海事船舶跟踪场景中，结合AIS进行轨迹预测与目标化观测，系统成功生成可行动输出，展示了端到端自动化调度与分析的有效性。

Conclusion: 该框架实现从提示生成到任务调度再到自动分析与报告的闭环，并可推广至智慧城市监测、灾害响应等对时效与自动化要求高的应用。

Abstract: The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.

</details>


### [96] [Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized](https://arxiv.org/abs/2512.09687)
*Er Jin,Yang Zhang,Yongli Mou,Yanfei Dong,Stefan Decker,Kenji Kawaguchi,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 提出UniForget：通过模型剪枝抑制扩散生成模型记忆的版权内容生成，避免高开销采样或特定概念定向卸学，同时保持生成质量，并可与现有方法互补。


<details>
  <summary>Details</summary>
Motivation: 生成模型易记忆训练数据，模型越大越严重，导致版权、肖像、商标风险。现有防记忆方法要么在采样时高开销地偏移表示，要么依赖对特定概念的卸学数据与再训练，扩展性差。需要一种通用、低开销、可扩展且不依赖目标概念列表的方法。

Method: 从模型内部归因出负责“版权化内容”的特定组件/通道，采用结构化或非结构化剪枝，移除或抑制这些部位；不针对具体概念进行数据驱动卸学。评估在不显著影响通用生成能力的前提下，降低生成受版权约束内容的概率；并与现有卸学方法组合验证正交性。

Result: 剪枝后，模型生成版权相关或记忆性样本的概率显著下降，通用图像质量和多样性基本保持；相比采样期方法开销更低；与现有卸学技术结合可进一步提升去记忆效果。

Conclusion: 记忆的根源可定位到模型局部结构，剪枝是一种高效、通用、可扩展的去记忆手段，既能独立使用也能与卸学方法互补，降低法律风险同时维持生成性能。

Abstract: Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.

</details>


### [97] [LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery](https://arxiv.org/abs/2512.09700)
*Seon-Hoon Kim,Hyeji Sim,Youeyun Jung,Ok-Chul Jung,Yerin Kim*

Main category: cs.CV

TL;DR: LiM‑YOLO通过将检测头下移到P2–P4并引入稳定小批量训练的GN‑CBLinear，在多遥感船舶数据集上实现更高精度与效率，专攻极小、细长船目标的检测。


<details>
  <summary>Details</summary>
Motivation: 通用检测器在卫星海事场景中遇到极端尺度不平衡与目标形态各向异性问题：P5/stride‑32特征对窄小船舶欠采样，导致空间信息被稀释，训练时高分辨率+小批量引发梯度不稳定。

Method: 1) 基于船尺度统计提出金字塔层级迁移（Pyramid Level Shift），将检测头由P3–P5改为P2–P4，满足小目标的奈奎斯特采样要求并去除深层冗余；2) 设计GN‑CBLinear（带组归一化的线性投影卷积块）以稳定微批量训练、缓解梯度波动；3) 在多遥感数据集上进行验证。

Result: 在SODA‑A、DOTA‑v1.5、FAIR1M‑v2.0、ShipRSImageNet‑V1上均优于SOTA，兼顾精度与效率（摘要未给具体数值）。

Conclusion: 针对遥感船舶检测的小目标与细长形态，降低金字塔层级并采用GN‑CBLinear可显著提升检测效果与训练稳定性，提供更高效的域特化YOLO方案。

Abstract: Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.

</details>


### [98] [Stylized Meta-Album: Group-bias injection with style transfer to study robustness against distribution shifts](https://arxiv.org/abs/2512.09773)
*Romain Mussard,Aurélien Gauffre,Ihsan Ullah,Thanh Gia Hieu Khuong,Massih-Reza Amini,Isabelle Guyon,Lisheng Sun-Hosoya*

Main category: cs.CV

TL;DR: 提出Stylized Meta-Album（SMA），通过对12个主体分类数据集进行风格迁移生成12个风格化数据集，共24个子数据集、4800组，面向OOD泛化、公平性与自适应评测；在更高组多样性下重新评估现有算法与调参指标，并构建更稳定的UDA基准。


<details>
  <summary>Details</summary>
Motivation: 现有OOD与公平性评测受限于数据收集成本与组（群体/风格/域）多样性不足，难以系统研究少数群体、组不平衡与复杂域转移；需要一个可控、可扩展的元数据集来模拟更广泛的现实场景。

Method: 从12个主体分类数据集出发，用风格迁移生成对应的12个风格化数据集；通过可配置的“主体类×风格×域”组合形成4800个组，支持灵活设定组数、类数与不平衡度；基于SMA构建两个基准：（1）OOD泛化与群体公平性基准，引入Top-M最差组准确率作为新调参指标；（2）无监督域适应（UDA）基准，覆盖closed-set与UniDA多情景。

Result: 在公平性基准上，简单重加权与利用组信息的方法在低多样性设置下仍具竞争力，但当组多样性增加时，各算法优劣与排名显著变化；以Top-M最差组准确率调参与优化，可在更大组多样性下获得更好的最终最差组性能。在UDA基准上，相比既有基准，误差条显著降低（closed-set降73%，UniDA降28%），评测更稳健、覆盖更全面。

Conclusion: SMA通过可控的风格与主体组合，构建大规模组结构，显著拓展OOD、公平性与UDA研究的评测空间；提升对少数群体、公平性与域迁移的检验力度，改变传统基准下算法结论，并提供更可靠的调参与比较框架。

Abstract: We introduce Stylized Meta-Album (SMA), a new image classification meta-dataset comprising 24 datasets (12 content datasets, and 12 stylized datasets), designed to advance studies on out-of-distribution (OOD) generalization and related topics. Created using style transfer techniques from 12 subject classification datasets, SMA provides a diverse and extensive set of 4800 groups, combining various subjects (objects, plants, animals, human actions, textures) with multiple styles. SMA enables flexible control over groups and classes, allowing us to configure datasets to reflect diverse benchmark scenarios. While ideally, data collection would capture extensive group diversity, practical constraints often make this infeasible. SMA addresses this by enabling large and configurable group structures through flexible control over styles, subject classes, and domains-allowing datasets to reflect a wide range of real-world benchmark scenarios. This design not only expands group and class diversity, but also opens new methodological directions for evaluating model performance across diverse group and domain configurations-including scenarios with many minority groups, varying group imbalance, and complex domain shifts-and for studying fairness, robustness, and adaptation under a broader range of realistic conditions. To demonstrate SMA's effectiveness, we implemented two benchmarks: (1) a novel OOD generalization and group fairness benchmark leveraging SMA's domain, class, and group diversity to evaluate existing benchmarks. Our findings reveal that while simple balancing and algorithms utilizing group information remain competitive as claimed in previous benchmarks, increasing group diversity significantly impacts fairness, altering the superiority and relative rankings of algorithms. We also propose to use \textit{Top-M worst group accuracy} as a new hyperparameter tuning metric, demonstrating broader fairness during optimization and delivering better final worst-group accuracy for larger group diversity. (2) An unsupervised domain adaptation (UDA) benchmark utilizing SMA's group diversity to evaluate UDA algorithms across more scenarios, offering a more comprehensive benchmark with lower error bars (reduced by 73\% and 28\% in closed-set setting and UniDA setting, respectively) compared to existing efforts. These use cases highlight SMA's potential to significantly impact the outcomes of conventional benchmarks.

</details>


### [99] [FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation](https://arxiv.org/abs/2512.09792)
*Pierre Ancey,Andrew Price,Saqib Javed,Mathieu Salzmann*

Main category: cs.CV

TL;DR: 提出FastPose-ViT，直接回归6DoF姿态并在裁剪框与全图间通过投影几何“表观旋转”形式化映射，速度快、精度优，适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有航天器单目6DoF姿态估计多依赖迭代PnP，计算开销大、不利于在资源受限的边缘设备上实时应用；需要一种无需PnP、端到端、可高效部署的方法。

Method: 采用ViT对目标检测得到的裁剪图像直接回归位姿（平移与“表观旋转”矩阵）。基于投影几何提出从局部裁剪到全图坐标的数学映射，并将网络预测的表观旋转纠正为真实姿态。使用SPEED数据训练与评测，并进行量化与在Jetson Orin Nano上的端到端部署。

Result: 在SPEED数据集上优于其他非PnP方法，并达到与先进PnP方法相当的性能。量化部署后在Jetson Orin Nano上顺序执行延迟约75 ms/帧；并行调度时非阻塞吞吐达33 FPS。

Conclusion: FastPose-ViT以端到端、非PnP方式实现快速准确的航天器单目6DoF姿态估计，并通过“表观旋转”与尺度映射理论闭环，兼具精度与实时性，适合上星与在轨任务的边缘硬件部署。

Abstract: Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of "apparent rotation", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.

</details>


### [100] [Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2512.09801)
*Tien-Dat Chung,Ba-Thinh Lam,Thanh-Huy Nguyen,Thien Nguyen,Nguyen Lan Vi Vu,Hoang-Loc Cao,Phat Kim Huynh,Min Xu*

Main category: cs.CV

TL;DR: 提出一种用于多模态医学图像分割的半监督框架，通过增强单模态表征与自适应跨模态融合，在极少标注（1%、5%、10%）下提升Dice与Sensitivity，并在BraTS2019 HGG上优于强基线。


<details>
  <summary>Details</summary>
Motivation: 多模态MRI序列间存在语义差异与配准不完美，现有半监督方法难以有效挖掘模态间互补信息，导致在标注稀缺条件下分割性能受限。

Method: 构建半监督多模态框架：1) 模态特异增强模块（MEM）使用通道注意力强化各模态独有语义线索；2) 互补信息融合模块（CIF）以可学习方式在模态间自适应交换互补知识；3) 以监督分割损失+跨模态一致性正则联合优化，并在未标注数据上施加一致性约束。

Result: 在BraTS 2019 HGG子集上，于1%、5%、10%标注比例下，较强半监督与多模态基线均取得显著提升，Dice与Sensitivity均明显提高；消融显示MEM与CIF均有效且具有互补性。

Conclusion: 加强模态内表征与自适应跨模态融合能缓解跨序列语义错配，在低标注场景显著提升多模态分割的鲁棒性与准确度。

Abstract: Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.

</details>


### [101] [CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing](https://arxiv.org/abs/2512.09806)
*Jianfei Li,Ines Rosellon-Inclan,Gitta Kutyniok,Jean-Luc Starck*

Main category: cs.CV

TL;DR: 提出CHEM指标，用小波/剪切波多尺度特征和保序化分位回归，无分布假设地检测与量化重建幻觉；并从逼近论解释U形网络易产幻觉，在CANDELS数据上验证（U-Net、SwinUNet、Learnlets）。


<details>
  <summary>Details</summary>
Motivation: U形重建网络虽强大，但在去卷积等任务中可能生成不真实的伪影/幻觉，影响安全关键场景的可靠性，需要一种模型无关、可量化、可解释地识别与评估幻觉的方法。

Method: 提出CHEM：1) 用小波与剪切波表示分解图像，多尺度、各向异性地抽取与真实结构不一致的特征作为“幻觉候选”；2) 采用保序化（共形化）分位回归在无分布假设下估计各位置/频带的幻觉水平与置信区间，从而给出校准的不确定性度量；3) 从逼近理论分析U形网络对频带耦合与上采样的偏置，解释其易幻觉的原因。

Result: 在CANDELS天文数据上，对U-Net、SwinUNet、Learnlets等重建模型，CHEM能有效定位并量化幻觉伪影，提供分布无关且可校准的评估；实验显示U形结构更易在高频细节与各向异性结构处产生幻觉。

Conclusion: CHEM为重建模型提供通用、可校准的幻觉检测与量化工具，并给出U形架构易幻觉的理论解释，可用于提升安全关键图像处理的可信度与模型选择/训练改进。

Abstract: U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.

</details>


### [102] [DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.09814)
*Zhizhong Wang,Tianyi Chu,Zeyi Huang,Nanyang Wang,Kehan Li*

Main category: cs.CV

TL;DR: 提出DynaIP：一个无需测试时微调的个性化图像提示适配器插件，通过动态解耦策略与层级MoE特征融合，提升个性保真、提示遵循和平衡、多主体可扩展性，刷新单/多主体PT2I任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有零样本个性化文生图在三方面受限：（1）难以在“概念保真（CP）”与“提示跟随（PF）”间取得平衡；（2）难以保留参考图像的细粒度概念细节；（3）多主体个性化可扩展性差。作者欲在无需测试时微调的框架下，系统性解决这三大痛点。

Method: 基于多模态扩散Transformer（MM-DiT），提出动态图像提示适配器DynaIP，包含两点核心：1）动态解耦策略：观察到在通过跨注意力向MM-DiT双分支注入参考特征时模型存在天然“解耦”学习行为，因而在推理时动态移除与概念无关的信息干扰，改善CP-PF平衡并提升多主体组合的可扩展性；2）层级专家混合（Hierarchical MoE）特征融合：识别视觉编码器是细粒度CP关键，利用CLIP的层级特征捕获不同粒度视觉信息，通过MoE融合实现对粒度的可控与细节的显著提升。

Result: 在单主体与多主体个性化文本到图像任务上进行广泛实验，DynaIP整体优于现有方法，在概念保真、提示遵循和平衡性，以及多主体组合质量与稳定性上取得显著提升。

Conclusion: DynaIP通过动态解耦和层级MoE特征融合，有效缓解CP与PF冲突、增强细粒度细节保留，并扩大多主体个性化生成的能力，为零样本文生图个性化提供更强泛化与控制力。

Abstract: Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.

</details>


### [103] [Composing Concepts from Images and Videos via Concept-prompt Binding](https://arxiv.org/abs/2512.09824)
*Xianghao Kong,Zeyu Zhang,Yuwei Guo,Zhuoran Zhao,Songchun Zhang,Anyi Rao*

Main category: cs.CV

TL;DR: 提出Bind & Compose，一种在Diffusion Transformer中通过将视觉概念与提示词绑定并可跨图像/视频来源组合的“一次性”视觉概念组合方法。其核心是分层绑定器进行交叉注意力调制、Diversify-and-Absorb机制提升概念-词元绑定精准度、以及视频的时间解耦双分支策略以兼容图像与视频概念。结果在概念一致性、提示忠实度和运动质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉概念组合无法：1) 从图像/视频中精确拆解复杂概念；2) 灵活跨模态（图像与视频）组合；3) 在组合时抑制与概念无关细节干扰；4) 有效处理视频的时间因素。

Method: 在Diffusion Transformer的交叉注意力中引入“绑定器”（binder），将视觉输入编码为与提示词元一一对应的条件信号：1) 分层绑定器结构，对复杂概念进行逐级分解并绑定至对应词元；2) Diversify-and-Absorb机制：用多样化文本提示训练，并加入“吸收”词元吸纳与概念无关特征，提升绑定纯度；3) 时间解耦策略：对视频概念采用两阶段、双分支绑定器（空间/时间分支）进行时序建模，以提升图像—视频概念兼容性。

Result: 在多项评测中，方法在概念一致性（概念可控）、提示忠实度（文本对齐）、以及视频的运动质量上优于现有方法，显示出更强的跨源（图像与视频）组合与创作能力。

Conclusion: 通过将视觉概念与提示词元牢靠绑定并可跨来源灵活组合，Bind & Compose解决了复杂概念分解、概念干扰与时序兼容三大难点，为图像/视频的创造式组合生成提供了统一、有效的范式。

Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.

</details>


### [104] [From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities](https://arxiv.org/abs/2512.09847)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 该论文将“挣扎（struggle）”从离线定位扩展为在线检测与预判任务：在实时流中检测即将或正在发生的困难片段，基线模型可在20FPS管线下运行并达到70–80%每帧mAP，预测提前至2秒时性能略降但可用。


<details>
  <summary>Details</summary>
Motivation: 智能辅助系统需要识别用户何时遇到困难以实时干预。既有工作多为离线分类/定位，无法满足在线场景的即时响应与提前预警需求，因此需要把挣扎识别转化为可在线检测和提前预判的设置，并评估其在任务/活动迁移和技能变化下的泛化能力。

Method: 将挣扎定位重构为在线检测任务，并进一步定义“提前预判”（在挣扎发生前t秒预测）。适配两种现成模型作为在线检测与预判的基线；使用特征化输入（feature-based）实现高帧率推理；评估包括：每帧mAP、不同提前量（至2秒）、跨任务与跨活动泛化、以及随技能演进的影响；报告端到端（含特征提取）与纯模型阶段的速度。

Result: 在线检测每帧mAP约70–80%；提前预判最多提前2秒时，性能略有下降但与检测接近；跨任务/活动的泛化存在域间差距，活动级更大，但仍比随机基线高4–20%；特征级模型推理速度最高可达143 FPS，端到端（含特征提取）约20 FPS，可满足实时应用。

Conclusion: 在线挣扎检测与预判是可行且高效的：在保持实时速度的同时提供较高准确度，并在跨域场景仍优于随机。该工作为面向实时人机协作/辅助系统的主动干预与提前提示提供了基础。

Abstract: Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.

</details>


### [105] [UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving](https://arxiv.org/abs/2512.09864)
*Hao Lu,Ziyang Liu,Guangfeng Jiang,Yuanfei Luo,Sheng Chen,Yangang Zhang,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出UniUGP统一框架，将场景推理、未来视频生成与轨迹规划耦合，利用预训练VLM与视频生成模型，多阶段训练，在长尾复杂场景中实现SOTA的感知、推理与决策泛化。


<details>
  <summary>Details</summary>
Motivation: AD系统在长尾场景表现不佳，主要因世界知识有限与视觉动态建模不足。现有VLA无法用无标注视频做因果视觉学习，世界模型法又缺乏大语言模型的推理能力。需要一个能结合视觉动力学与语义推理、并可利用多源数据的统一方法。

Method: 构建多种专用数据集，提供复杂场景的推理与规划标注；提出统一的理解-生成-规划（UniUGP）框架，采用混合专家架构，将场景理解/推理、未来视频生成与轨迹规划协同优化；集成预训练VLM与视频生成模型；输入为多帧观测与语言指令，输出可解释的CoT推理、物理一致的轨迹与连贯的未来视频；设计四阶段逐步训练策略，在多现有AD数据集与新数据集上建立能力。

Result: 在感知、推理与决策任务上达到SOTA，并在长尾复杂情形上表现出更强的泛化能力。

Conclusion: 统一融合推理、视频生成与规划的混合专家框架，结合预训练VLM与视频生成模型并配合分阶段训练，可显著提升自动驾驶在长尾复杂场景中的鲁棒性与可解释性。

Abstract: Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.

</details>


### [106] [MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI](https://arxiv.org/abs/2512.09867)
*Fengli Wu,Vaidehi Patil,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出MedForget：一个层级感知的多模态“遗忘”基准，用于评估MLLM在医疗场景中的选择性遗忘能力。结果表明现有方法难以在不损伤诊断性能的前提下实现完整、层级一致的遗忘；粗粒度遗忘更抗基于层级线索的重建攻击，细粒度遗忘更脆弱。


<details>
  <summary>Details</summary>
Motivation: 医疗MLLM涉及敏感患者数据，需满足HIPAA/GDPR“被遗忘权”。现有机器学习“遗忘”研究对复杂医疗、多模态与数据层级结构适配不足，缺少系统化评测与攻击视角。

Method: 构建MedForget测试床：将医院数据建模为层级（机构→患者→检查→章节等八级），在每一级提供显式retain/forget划分与带改写变体的评测集；覆盖三类任务（生成、分类、完形）。评测四种SOTA遗忘方法，并提出“层级重建攻击”，逐步向提示中添加层级上下文以检验遗忘路径是否被真正移除。

Result: 在3840个多模态样本上，现有方法无法实现完全、层级一致的遗忘，同时会牺牲诊断相关性能；对重建攻击的鲁棒性取决于遗忘粒度：粗粒度遗忘更稳健，细粒度遗忘容易被层级提示“召回”。

Conclusion: MedForget为HIPAA对齐的实用基准，揭示了医疗MLLM的层级遗忘难题与安全-性能权衡，为开发合规、可审计的医疗AI提供评测与攻击工具。

Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.

</details>


### [107] [Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling](https://arxiv.org/abs/2512.09871)
*Yimin Zhu,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出DPS4Un：把预训练条件扩散模型作为后验采样器，在超像素内构建图像驱动的端元先验并联合观测进行半盲光谱解混，迭代更新端元与丰度以刻画光谱可变性，三套实测数据上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: LMM解混面临两大难题：端元先验如何建模以及端元/光谱可变性如何表征。传统依赖光谱库易偏差、图像级一致性过粗，且后验推断难。贝叶斯框架能统一先验与观测，因此需要一种既能学习数据驱动先验，又能高效进行后验采样的解混方法。

Method: 在贝叶斯框架下，构建条件扩散模型作为后验采样器：以超像素内的图像端元束（而非外部库）训练端元先验；使用超像素级数据保真项替代图像级约束；将每个超像素的端元初始化为高斯噪声，并与丰度交替迭代更新，从而显式刻画光谱可变性。整体流程为DPS4Un的半盲解混。

Result: 在三组真实基准数据上，DPS4Un取得比现有最先进解混方法更优的定量指标与可视化效果，表明其对先验偏差与光谱可变性的鲁棒性更强。

Conclusion: 将条件扩散模型用于后验采样，结合超像素端元先验与超像素数据一致性，可有效解决先验偏差与光谱可变性问题，实现更优的半盲解混性能。

Abstract: Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.

</details>


### [108] [Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs](https://arxiv.org/abs/2512.09874)
*Pius Horn,Janis Keuper*

Main category: cs.CV

TL;DR: 提出一个用于评测PDF数学公式解析的基准：用可控合成PDF与精确LaTeX真值，结合LLM评审与两阶段匹配来做语义评估，并系统比较20多种解析器，证明LLM评估与人类判断更一致。


<details>
  <summary>Details</summary>
Motivation: 现有PDF解析基准要么排除公式、要么缺乏语义层面的评估指标，难以客观衡量解析器对数学公式的正确性，影响LLM训练与科学知识库构建。

Method: 1) 生成具有精确LaTeX真值的合成PDF，能控制版式、公式与内容；2) 提出“LLM作为评审”的语义公式评估，并设计稳健的两阶段匹配流程以对齐解析输出与真值；3) 通过人评对比验证评估指标；4) 在100份含2000+公式的文档上评测20+解析器（OCR、视觉语言模型、规则方法）。

Result: LLM评审与人类判断的相关性最高（Pearson r=0.78），显著优于CDM（0.34）与纯文本相似度（约0）。基于该基准的实验显示不同解析器在公式抽取质量上差异显著，并给出可复现的对比结果。

Conclusion: 该框架提供了可扩展、可复现且更贴近语义的公式解析评估方法，为实际应用中选择PDF解析器提供实证依据，并公开代码与数据以促进后续研究。

Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench

</details>


### [109] [VisualActBench: Can VLMs See and Act like a Human?](https://arxiv.org/abs/2512.09907)
*Daoan Zhang,Pai Liu,Xiaofei Zhou,Yuan Ge,Guangchen Lan,Jing Bi,Christopher Brinton,Ehsan Hoque,Jiebo Luo*

Main category: cs.CV

TL;DR: 提出VisualActBench基准以评估VLM在无文本提示下的主动视觉行动推理能力，涵盖四类真实场景的1074段视频与3733个动作标注，并发现现有29个VLM与人类在高优先级、主动行动生成上仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有VLM多擅长感知与描述，但缺乏在仅依赖视觉输入时进行前瞻性推理与行动规划的能力评估；现实世界代理需要能理解复杂情境、预判后果并做出价值敏感的决策，因此需要新的任务与标准化基准。

Method: 提出“Visual Action Reasoning”任务并构建VisualActBench：收集4类真实场景视频（1074个），由人工标注3733个候选行动，并为每个行动赋予行动优先级(APL)与主动/被动类型；以此衡量模型在人类一致性、优先级敏感性与主动性方面的推理；对29个VLM进行系统评测（含前沿模型如GPT-4o）。

Result: 总体上前沿模型表现较强，但与人类相比仍有明显差距，尤其在生成主动且高优先级的行动方面；模型在复杂上下文理解、结果预判与价值对齐方面存在不足。

Conclusion: VisualActBench为评估与提升面向现实、以视觉为中心的主动AI代理提供了系统框架；需要新的方法以增强VLM在主动推理、优先级判断与人类价值对齐上的能力。

Abstract: Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.

</details>


### [110] [NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway](https://arxiv.org/abs/2512.09913)
*Sander Riisøen Jyhne,Aditya Gupta,Ben Worsley,Marianne Andersen,Ivar Oveland,Alexander Salveson Nossum*

Main category: cs.CV

TL;DR: NordFKB 是基于挪威权威 FKB 的细粒度地理空间AI基准数据集，含高分辨率正射影像、36类精细标注（分割掩膜+COCO检测），覆盖七个多样区域并提供标准评测工具，旨在推动制图与国土管理AI研究。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间数据集在精度、类目细粒度、标注质量与评测一致性方面不足，尤其在高纬度、多样地貌与气候环境（如挪威）下的泛化性与可比性有限。需要一个权威、精细、可复现的基准来推进分割与检测方法在实际制图、国土与规划中的应用。

Method: 从国家级 FKB 数据库构建：挑选七个气候、地形、城镇化多样的区域；提供高分辨率正射影像与36类精细标注；同时给出每类二值分割掩膜（GeoTIFF）与COCO格式目标框；仅保留含有至少一个标注目标的瓦片；通过跨区域随机采样建立训练/验证划分，以保持类别与上下文分布代表性；由人工专家审核与质控；发布配套基准库与标准化评测协议/工具，覆盖语义分割与目标检测。

Result: 得到一个高精度、覆盖多样地理环境的公开基准集与工具链：高质量人工审核标注、36类、多模态标注形式、可复现训练/验证划分与统一评测流程，支持语义分割与目标检测任务。

Conclusion: NordFKB 为地理空间AI提供权威而细粒度的基准，能促进制图、土地管理与空间规划中的方法研究与公平比较，并为未来在覆盖范围、时间序列与数据模态上的扩展奠定基础。

Abstract: We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.

</details>


### [111] [Splatent: Splatting Diffusion Latents for Novel View Synthesis](https://arxiv.org/abs/2512.09923)
*Or Hirschorn,Omer Sela,Inbar Huberman-Spiegelglas,Netalee Efrat,Eli Alshan,Ianir Ideses,Frederic Devernay,Yochai Zvik,Lior Fritz*

Main category: cs.CV

TL;DR: Splatent在VAE潜空间上结合3D Gaussian Splatting与扩散式多视图注意，保留VAE重建稳定性的同时，从2D输入视角恢复细节，显著提升潜空间辐射场的清晰度与一致性，达到SOTA并增强稀视角重建。


<details>
  <summary>Details</summary>
Motivation: VAE潜空间便于与扩散模型集成并高效渲染，但多视图不一致导致3D重建纹理模糊、细节缺失。现有要么微调VAE牺牲重建质量，要么借助预训练扩散模型填细节但易幻觉。需要一种既保留VAE重建稳定性又能可靠恢复细节的方法。

Method: 提出Splatent：在VAE潜空间中，以3DGS为基础进行渲染表示，同时引入扩散式增强框架。核心思想是“2D优先”的细节恢复：通过来自多输入视图的多视图注意机制在2D域恢复细节，而非在3D域显式重建；保持预训练VAE不变，利用扩散过程对潜表示/渲染结果进行强化，并与3DGS耦合以实现高效渲染与跨视角一致性。

Result: 在多项基准上，Splatent对VAE潜空间辐射场的重建达到新的SOTA，显著改善纹理清晰度和细节保真；相较现有方法减少模糊与缺细节问题。与现有前馈式框架集成后，也能稳定提升细节保留。

Conclusion: 从2D多视角注意出发在VAE潜空间增强3DGS，可在不牺牲预训练VAE重建质量的前提下恢复高保真细节，实现更可靠的稀视角高质量3D重建，并为潜空间辐射场与扩散模型结合提供新范式。

Abstract: Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.

</details>


### [112] [ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning](https://arxiv.org/abs/2512.09924)
*Xinyu Liu,Hangjie Yuan,Yujie Wei,Jiazheng Xing,Yujin Han,Jiahao Pan,Yanbiao Ma,Chi-Min Chan,Kang Zhao,Shiwei Zhang,Wenhan Luo,Yike Guo*

Main category: cs.CV

TL;DR: 提出RVE任务与RVE-Bench基准，针对需要物理与因果推理的视频编辑；并提出自反式推理框架ReViSE，通过内置VLM反馈联合训练生成与评估，显著提升编辑准确性与保真度，在基准上整体提升32%。


<details>
  <summary>Details</summary>
Motivation: 现有视频统一模型虽能理解与生成，但在需要推理约束的编辑上表现不佳，原因在于缺乏相应数据集与评测，以及模型“会想但不会改”的断裂（推理与编辑能力未联通）。

Method: 1) 定义Reason-Informed Video Editing (RVE)任务，要求在编辑中考虑物理可行性与因果动态；2) 构建RVE-Bench，包含两个子集：推理感知视频编辑与上下文视频生成，覆盖多维推理与真实场景；3) 提出ReViSE：自反式推理（SRF）框架，内部VLM对编辑结果与指令的一致性打分，形成可差分/可学习的内在反馈，用于训练中校正生成器的推理与编辑行为，并统一生成与评估于同一架构。

Result: 在RVE-Bench上，ReViSE在推理感知视频编辑子集相对SOTA的Overall分数提升32%，同时提升编辑准确性和视觉保真度。

Conclusion: 通过将推理与编辑紧密耦合、并引入系统化评测基准，ReViSE证明了内置VLM自反馈能有效指导视频编辑中的因果与物理一致性，推进了推理驱动的视频编辑能力。

Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.

</details>


### [113] [GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures](https://arxiv.org/abs/2512.09925)
*Patrick Noras,Jun Myeong Choi,Didier Stricker,Pieter Peers,Roni Sengupta*

Main category: cs.CV

TL;DR: 提出GAINS：在稀疏多视角下稳健的高斯点云逆向渲染，两阶段用学习先验稳固几何与材质，显著提升材质恢复、重光照与新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯Splatting的逆渲染在稠密多视角时能恢复高质量材质，但在稀疏视角下几何、反射率与光照高度耦合导致不适定与退化，需引入强先验缓解歧义。

Method: 两阶段框架：1) 几何阶段：利用单目深度/法线和扩散模型先验精化高斯几何；2) 材质阶段：通过语义分割、固有图像分解（IID）以及扩散先验，对反照率、粗糙度等材质参数进行正则化与估计；在物理一致的光传输下进行优化。

Result: 在合成与真实数据上，相比现有高斯逆渲染方法，尤其在稀疏视角条件下，显著提高材质参数精度、重光照质量与新视角合成表现。

Conclusion: 学习先验与两阶段策略有效缓解稀疏视角下几何-材质-光照歧义，推动高斯Splatting逆渲染在低采样场景中的实用性。

Abstract: Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/

</details>
