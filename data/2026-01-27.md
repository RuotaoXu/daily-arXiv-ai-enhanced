<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: 本文系统研究科学图像合成：提出逻辑驱动的ImgCoder框架与评测基准SciGenBench，揭示像素生成的系统性失效与“表达力-精确性”权衡，并显示用高保真合成图像微调多模态大模型可显著提升科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在科学场景中常生成“看起来对、逻辑上错”的图像，导致多模态推理受限。需要可控且科学严谨的图像合成方法与客观评测，以复现文本域中合成数据带来的推理增益。

Method: 1) 对比两类生成范式：直接像素生成与程序化合成；2) 提出ImgCoder：遵循“理解-规划-编码”的显式工作流，以程序化/可执行表示提升结构精度；3) 引入SciGenBench：从信息效用与逻辑有效性评估科学正确性；4) 用经过严格验证的合成科学图像微调LMM并测量下游推理。

Result: 评测显示像素式模型存在系统性失败（逻辑错误、结构不一致等），并呈现表达力与精确性之间的基本权衡。用高保真合成图像微调LMM在多模态科学推理任务上获得一致增益，且呈现可扩展趋势。

Conclusion: 科学图像的高保真程序化合成结合严格评测可有效提升多模态推理；ImgCoder与SciGenBench为社区提供了可复现框架与基准，验证了“高质量合成→推理增强”的可行路径。

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [2] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: 提出一个用于医学图像分割的数据高效增广框架：用INR建模连续形变并在积分形变场上线性混合以扩展解剖多样性，同时通过Sim2Real病灶注入把真实病灶纹理移植到健康背景，显著提升nnU-Net、U-Mamba等模型在小标注数据下的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学分割越来越受限于标注稀缺而非原始数据量；复杂病变（如脑膜瘤）需要充分挖掘少量高质量标注中的潜在信息，现有增广方法难以同时保证解剖合理性与病灶外观真实性。

Method: 提出双重增广：1) 空间流形扩张：用隐式神经表示（INR）学习连续速度场，先积分成形变场，再在形变场空间做线性混合/插值，生成解剖合理的多样形变；2) 语义对象注入（Sim2Real）：将真实病灶纹理移植到健康解剖背景，构建高保真模拟域以缩小合成与真实分布差距。

Result: 在混合数据集上，对比最先进基线（nnU‑Net、U‑Mamba等），该框架在数据效率与鲁棒性上显著提升（定量细节未给出，但报告了全面优于基线的结果）。

Conclusion: 通过在形变流形上插值与真实纹理注入的双增广，可在有限标注预算下显著提升医学分割性能，为复杂病变场景提供高性价比的数据利用策略。

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [3] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: 提出一种自动分析外周血涂片图像的方法，通过分割与形状因子判别，区分正常与镰状/变形红细胞，在患者真实数据上优于多种现有方法（F1：正常0.97、延长0.95），可用于镰状细胞贫血的临床辅助诊断。


<details>
  <summary>Details</summary>
Motivation: 人工显微镜观察红细胞耗时、依赖专家且主观误差高，尤其在涉及红细胞变形（如镰状细胞贫血）时，客观自动化枚举与分类方法可提高效率与一致性，支撑临床诊断与监测。

Method: 对外周血涂片图像：1）采用Chan–Vese主动轮廓进行目标（红细胞）分割；2）用形状描述子——圆形形状因子（CSF）与椭圆形状因子（ESF）进行分类，将红细胞分为正常、延长（镰状/细长）及其他变形；3）针对簇拥/部分遮挡的细胞，进行椭圆拟合调整，以适配双凹圆盘与拉长形。

Result: 在古巴圣地亚哥某医院真实患者样本上验证，与若干SOTA方法对比，本文方法在镰状细胞识别与多分类整体指标上更优，F-measure：正常细胞0.97、延长细胞0.95，整体多类性能指标也表现领先。

Conclusion: 基于主动轮廓分割与形状因子的自动红细胞分类方法能可靠地区分正常与变形红细胞，对镰状细胞贫血的诊断与治疗监测提供有效的临床辅助支持。

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [4] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: 提出AMVICC基准，系统比较多模态大模型与图像生成模型在九类视觉推理中的失败模式，发现跨模态共享与特定失败并存，IGMs在显式提示下对细粒度属性控制尤弱，为跨模态对齐研究与统一视觉语言建模改进提供框架。


<details>
  <summary>Details</summary>
Motivation: 尽管ML/AI快速发展，现有VLM仍在基本视觉概念（朝向、数量、空间关系等）的理解与生成上反复失误；缺少能在图像到文本与文本到图像两端对照评估失败模式的统一基准。

Method: 将MMVP题目改写为显式与隐式提示，构建AMVICC基准；在九类视觉推理任务上，对11个MLLM与3个IGM进行测试，对比不同模型与模态的失败类型与频率。

Result: 发现许多失败模式跨模型与跨模态共享，但也存在模型/模态特异的错误；IGMs在根据提示操控具体视觉要素方面持续表现不佳，尤其在显式提示下，对细粒度视觉属性缺乏控制。

Conclusion: AMVICC为结构化视觉推理的跨模态评测提供工具，可用于探查生成与理解失败是否源于共同限制，指导未来在统一视觉—语言建模与跨模态对齐上的改进。

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [5] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: 提出一种混合视觉管线：用预训练Xception提取深度特征+传统ML分类器（SVM、kNN、Bagged Trees等）对C&D建筑拆除垃圾进行自动分类，在1800张四类实拍数据集上达到了最高99.5%准确率与macro-F1，优于端到端深度学习。


<details>
  <summary>Details</summary>
Motivation: 建筑行业产生大量废弃物，现场高效、可靠的分类有助于可持续废弃物管理与资源回收。现有端到端深度学习在小数据、部署成本与鲁棒性上可能受限，亟需一种兼顾精度、可解释性和落地性的方案。

Method: 采集来自阿联酋施工现场的1800张均衡高质量图像，涵盖陶瓷/瓷砖、混凝土、杂物/垃圾、木材四类与多样现场条件。使用预训练Xception网络提取深度特征；将这些特征输入多种经典分类器（线性/核SVM、kNN、Bagged Trees、LDA、Logistic Regression）系统评估；比较混合管线与端到端深度模型表现。

Result: 基于Xception特征的简单分类器（线性SVM、kNN、Bagged Trees）达到SOTA，最高约99.5%准确率与macro-F1，超过更复杂或端到端深度学习方法。

Conclusion: 混合深度特征+传统ML分类器在C&D垃圾识别上精度高、鲁棒、易部署，适合现场应用，并为与机器人和现场自动化系统的集成提供可行路径。

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [6] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: 提出MANGO：一个覆盖全球的红树林语义分割数据集（2020年S2单期图像-掩膜对，共42,703对，124国），并给出在国家互斥划分下的分割基线。


<details>
  <summary>Details</summary>
Motivation: 现有红树林监测数据集存在：仅有年度地图、缺少经过筛选的单期图像-掩膜配对；覆盖面局限于特定区域、非全球；或无法公开获取。这限制了深度学习在红树林检测中的发展与可推广性。

Method: 从全球红树林区域检索2020年所有Sentinel-2影像；利用目标检测驱动的挑选策略，结合像素级坐标参照，将年度红树林掩膜与最佳单期观测匹配，生成高质量图像-掩膜对；构建国家互斥（country-disjoint）的训练/验证/测试划分，并在多种语义分割架构上进行基准评测。

Result: 得到覆盖124个国家的42,703个标注图像-掩膜对的MANGO数据集；提供多个主流语义分割模型在国家互斥划分下的基准表现（具体数值未在摘要给出），验证了数据集用于可扩展、跨区域泛化监测的有效性。

Conclusion: MANGO填补了全球可获取、单期配对的红树林数据集空缺，并通过标准化基准促进深度学习方法在全球尺度上进行可靠的红树林监测与评估。

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [7] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: 提出一条“版面分析→文本行提取→OCR识别→整页拼装”的转写流水线，用于保留15-16世纪拉丁史料中的特殊字符与符号；在多数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 历史拉丁文文献包含特殊字符/记号，需在数字化转写中原样保留以维持史料风格与语义；现有OCR难以兼顾版面结构与此类符号的保持。

Method: 将布局分析模型与现有文本行识别方法结合：先以布局分析（含行检测/分割）抽取文本行，再用OCR模型识别并保留特殊字符，最后重建整页。采用（声明）Masked Autoencoder相关模型进行多类型文本的处理与特征学习。

Result: 流水线简化页面处理并取得高效结果；在多数据集上评测，表明所用的MAE能够有效处理手写、印刷及多语言文本。

Conclusion: 结合布局分析与行级OCR的流水线能在历史文献中较好保留特殊特征并实现有效转写；方法具有跨文本类型的适用性。

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [8] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: 提出融合Leap Motion与RGB相机的多模态阿拉伯手语识别框架，双支路分别处理传感器坐标与图像特征，特征级融合后SoftMax分类，在18词自建数据集上达78%准确率，显示多模态可行但仍需优化与扩充数据。


<details>
  <summary>Details</summary>
Motivation: 单一传感器（仅Leap或仅RGB）对复杂手部朝向与3D运动的表征不足，导致识别精度受限。为弥补各模态短板，探索多模态融合是否能提升ArSL识别的鲁棒性与准确性。

Method: - 数据来源：Leap Motion骨架/关键点数据 + RGB图像；自建包含18个阿语手语词的数据集。
- 模型架构：
  1) Leap分支：自定义致密神经网络（DNN），含dropout与L2正则化；
  2) 图像分支：微调VGG16，结合数据增强；
  融合：两分支特征拼接，经全连接层，SoftMax输出类别；用于同时捕捉空间与时间相关信息（摘要未详述序列建模细节）。

Result: 在18个词的测试上正确识别13个，整体准确率78%。

Conclusion: 多模态融合对ArSL识别具备可行性与初步效果，但受限于小规模数据集和可能的时序建模不足；需要进一步优化模型、改进时间信息处理并扩充与标准化数据集以提升泛化与精度。

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [9] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: 提出一种解耦MCR2目标得到的可解释稀疏线性注意力DMSA，用于视觉Transformer，提升ImageNet-1K精度1.08%-1.45%，并更高效可解释。


<details>
  <summary>Details</summary>
Motivation: 现有MCR2白盒Transformer中，成员矩阵与子空间矩阵U紧耦合，若token投影不正确会产生冗余编码，影响效率与可解释性。需要解耦以避免冗余并获得更稳健的表示与可解释注意力。

Method: 在MCR2目标中解耦“成员矩阵”和“子空间U”的关系：直接从输入学习成员矩阵，再从全空间S中派生稀疏子空间；对优化后的目标进行梯度展开（unrolled gradient descent），得到一个可解释的稀疏线性注意力算子DMSA；将其替换ToST中的注意力，得到DMST。

Result: 在ImageNet-1K上，DMST相较ToST提升Top-1准确率约1.08%-1.45%，并实现更快的编码率下降；相较标准Transformer，计算效率与可解释性显著提升。

Conclusion: 解耦MCR2中的成员-子空间关系并通过梯度展开导出DMSA，可在保持白盒可解释性的同时提升视觉模型的准确率与效率，成为一条可靠的可解释高效注意力设计路线。

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [10] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 用深度卷积神经网络把噪声较大的TEM图像的原子柱深度估计转化为语义分割任务，在合成噪声的模拟数据上训练，并在CeO2纳米颗粒的模拟与真实数据上验证，可得到准确、校准且抗噪的像素级深度分割结果。


<details>
  <summary>Details</summary>
Motivation: TEM图像常有强噪声与投影叠加效应，直接从单幅或少量视角中恢复原子级三维信息困难。传统反演或配准方法对噪声敏感、计算昂贵且对先验依赖强。作者希望以数据驱动方式鲁棒地从噪声TEM图像中恢复原子柱的深度信息。

Method: 将深度估计表述为语义分割：每个像素被分类到若干离散深度层。用模拟的TEM图像并叠加合成噪声生成训练数据，训练深度卷积神经网络输出像素级深度分割图。之后将模型应用于CeO2纳米颗粒的模拟与真实TEM图像，得到原子柱深度估计，并评估其校准性与鲁棒性。

Result: 在模拟与真实CeO2 TEM数据上，模型输出的深度分割与真实/期望深度一致，表现出良好的标定（预测概率与实际一致）与对强噪声的稳健性，达到准确的原子柱深度估计。

Conclusion: 把原子级三维重建转化为像素级深度分割并用深度CNN实现，可在高噪声TEM图像中获得准确、可校准且稳健的深度信息，证明了该数据驱动框架在原子尺度表征中的有效性。

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [11] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: 提出Noisomics与对比预训练CoP模型，将成像噪声从“应被抑制的干扰”转为“可被解码的信息”，在极小数据下实现跨设备零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有成像噪声建模高度依赖设备与大规模标注数据，难以分辨物理信号与算法伪影，且通常把噪声当作纯干扰，限制了诊断与跨域泛化能力。

Method: 提出Noisomics范式与对比预训练的CoP基础模型：基于流形假设与“合成噪声基因组”，用对比学习在语义信号与随机扰动之间实现解缠；以极小样本规模（100条）训练，强调解码噪声的多参数指纹；无需设备校准，支持零样本应用。

Result: 在12个分布外数据集上，CoP相较常规训练策略，估计误差降低63.8%，决定系数提升85.1%；以仅100样本超越用10万样本监督训练的基线，实现数据与算力依赖降三数量级；案例涵盖消费级摄影的非线性硬件-噪声耦合解析与深层组织显微的光子效率优化。

Conclusion: 将噪声由“退化”重新定义为“信息资源”，通过CoP在极低样本下实现稳健零样本泛化与设备无关的精确成像诊断，显著降低数据与计算开销，拓展从摄影到生物成像的应用边界。

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [12] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: 提出SiMiC：用带注意力的CNN从SEM图像自动表征硅场发射尖端微结构，实现几何特征提取与分类/尺寸预测，优于传统图像处理，提升一致性与效率，并与发射性能关联；数据集与代码已公开。


<details>
  <summary>Details</summary>
Motivation: 手工基于SEM的微结构测量低效、主观且难以规模化，需要自动化、可解释、与性能相关联的方法来提高表征速度、可重复性与研发迭代。

Method: 构建硅基场发射尖端SEM数据集；设计融合注意力机制的定制CNN，实现多类微结构分类与尺寸/曲率等连续参数的回归预测；与传统图像处理基线对比；强调可解释性与上下文感知。

Result: SiMiC在分类与几何参数预测上取得高准确度，同时保持一定可解释性；相较传统方法减少人工干预，测量一致性提升；提供开源数据集与算法仓库作为基线。

Conclusion: SiMiC为数据驱动的微结构表征提供了有效框架，可直接连接几何与场发射性能，促进优化冷阴极与SEM电子源设计，并为后续建立几何-发射行为关联奠定基础。

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [13] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: ISAS 2025“识别未见行为”挑战赛综述：基于视频骨架关键点区分正常/异常行为，采用LOSO评估与宏平均F1，40支队伍参赛。结果显示在噪声、低维、类别不平衡和突发稀有动作下，时序与上下文建模至关重要。


<details>
  <summary>Details</summary>
Motivation: 在发育障碍人群设施中，需以非接触方式自动识别异常行为以提升安全与护理质量；姿态估计可保护隐私、具可部署性，但如何在真实不平衡与时序不规则数据中鲁棒识别仍是难题。

Method: 构建基于姿态骨架关键点的模拟场景数据集，保留现实中的类别不平衡与时序不规则；采用留一受试者交叉验证（LOSO）确保跨主体泛化；开放挑战吸引传统机器学习与深度学习多样方法；以宏平均F1作为主要指标以缓解不平衡影响。

Result: 40支队伍提交的多种模型在宏F1上表现有限，表明对噪声与低维骨架数据中稀有且骤发的异常动作建模困难；能捕捉时间依赖与上下文信息的方法更具潜力。

Conclusion: 异常行为识别需更好地融合时序动态与上下文线索，并面向不平衡、噪声与跨主体泛化设计方法；挑战的经验可推动面向医疗与行为监测的负责任AI发展。

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [14] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 提出单像素视觉-语言模型（SP-VLM），在隐私敏感场景以超低维单像素信号获取行为语义，同时天然抑制身份可恢复性，实现“隐私优先”的安全监测。


<details>
  <summary>Details</summary>
Motivation: 传统摄像监控在更衣室、卫生间等场合被法规与伦理限制，但这些空间也容易发生霸凌、骚扰等危害事件，亟需既能感知异常又不侵犯隐私的新范式。

Method: 以单像素传感为输入（极低维、无法还原清晰图像），结合视觉-语言模型推理行为语义；系统性分析采样率对身份可识别性的影响，并在低采样率下用SP-VLM执行异常检测、人数统计与活动理解。

Result: 证明单像素采样在临界采样率以下会使最先进的人脸识别失效，显著降低身份可恢复性；同时SP-VLM仍能从严重退化的单像素观测中提取有用的行为语义，稳健完成异常检测与统计任务；据此给出一个兼顾隐私与行为智能的实用采样率区间。

Conclusion: 单像素+VLM可在隐私敏感空间实现“行为可监测、身份难恢复”的人权友好型安防路径，支持及时干预而不常态化侵入式监控。

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [15] [Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults](https://arxiv.org/abs/2601.17053)
*Shuhao Que,Dieuwke van Dartel,Ilse Heeringa,Han Hegeman,Miriam Vollenbroek-Hutten,Ying Wang*

Main category: cs.CV

TL;DR: 研究开发并验证一种适用于高龄老年人的稳健人体活动识别系统，用于髋部骨折康复中的持续体力活动监测；在模拟自由生活环境下，用腰部与大腿两枚加速度计采集数据，并结合合成数据引导的特征干预模型，显著提升姿势转换识别表现。


<details>
  <summary>Details</summary>
Motivation: 临床上需要连续量化髋部骨折康复期老年人的体力活动以减缓功能衰退，但现有可穿戴追踪器多基于中年人开发，难以适应高龄老年人步态缓慢且变异大的特点，尤其对临床关键但常被忽视的姿势转换识别不足。

Method: 招募24名80岁以上健康老年人在模拟自由生活条件下进行约75分钟的日常活动（步行、站立、坐、卧、姿势转换），佩戴下背部与大腿前侧两枚加速度计；采用留一被试交叉验证评估模型鲁棒性；引入合成数据以指导并训练特征干预模型（FIM），与不含合成数据的对照模型比较。

Result: FIM在各类活动上的平均F1分别为：步行0.896、站立0.927、坐0.997、卧0.937、姿势转换0.816；相较无合成数据的对照模型，FIM显著提高了姿势转换检测性能，并显示合成数据可提升跨受试者泛化能力。

Conclusion: 初步结果表明，在高龄老年人中实现稳健的活动识别是可行的，特别是对临床相关的姿势转换有改进；但需在髋部骨折患者群体中进一步验证以评估临床实用性。

Abstract: Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.

</details>


### [16] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: 提出Ego4OOD基准与一对多二分类训练策略，应对自中心视频在协变量移位下的泛化；轻量两层全连接网络在Argo1M与Ego4OOD上达SOTA级别表现，并量化协变量移位与性能的关联。


<details>
  <summary>Details</summary>
Motivation: 现有自中心视频领域泛化评测常把协变量移位与概念移位混杂，难以客观衡量模型在输入分布变化下的泛化能力；同时，动作与环境强相关、类内时空变化大、特征长尾，使跨域识别困难。

Method: 1) 构建Ego4OOD：从Ego4D提炼语义一致、片段级动作类别，覆盖8个地理域，突出协变量多样性并降低概念移位；并提出基于聚类的协变量移位度量，量化域难度。2) 训练策略：将多类动作识别拆为多个一对全(binary one-vs-all)任务，使用轻量两层全连接网络，降低在分布移位下相似类别间的干扰，无需额外模态与大量参数。

Result: 在Argo1M与Ego4OOD上，该轻量模型取得与现有SOTA可比的跨域表现；度量的协变量移位大小与识别性能显著相关。

Conclusion: 受控基准与可量化的域特征对于研究OOD泛化至关重要；在协变量移位情境下，将多类问题分解为独立二分类可有效提升鲁棒性与参数效率。

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [17] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: 提出一种端到端计算机视觉系统，在靶场前线拍照即可自动检测弹孔并按射击轮次追踪，结合YOLOv8小目标检测、IoU跨时序匹配、ORB透视校正与“移除式”数据增强，达到97.0% mAP与88.8%轮次分配准确度。


<details>
  <summary>Details</summary>
Motivation: 传统步枪归零需要频繁停火、近距离目测比对不同轮次的弹孔，易受安全流程与人为误差影响，效率低且难以稳定区分时序上的相似弹孔。因此需要一个从拍照到识别与时序归属的自动化方案，减少人工介入与误判。

Method: 1) 使用YOLOv8进行小目标（弹孔）检测；2) 用ORB提取特征并做透视校正/姿态标准化，统一目标纸方向；3) 设计基于IoU的跨帧匹配策略，将新帧弹孔与既有集合对齐，完成轮次区分；4) 提出“移除式”数据增强：从成对序列中系统性删除部分弹孔以模拟逐轮射击的真实序列，缓解标注序列稀缺；5) 构建端到端流水线从拍照输入到结果可视化输出。

Result: 在弹孔检测上达到97.0% mAP；在将弹孔正确归属到对应射击轮次上的准确率为88.8%；ORB驱动的透视校正提升了检测与匹配稳定性。

Conclusion: 该系统显著减少归零过程中人工查验与等待时间，在实际拍摄条件下实现高精度检测与较高的时序归属准确度。方法通用于需要区分时序上外观高度相似目标的场景，可扩展到其它工业/安防/计数类应用。

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [18] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 论文提出一个面向视频生成模型走向“世界模型”的新分类框架：以“状态构建”和“动力学建模”为两大支柱，并呼吁从视觉保真度转向功能性评测（物理持久性、因果推理）。指出两条前沿：通过数据驱动记忆与压缩保真增强持久性；通过潜因子解耦与引入推理先验提升因果性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型展现了物理一致性的“涌现”能力，但多为无状态架构，与传统强调显式状态的世界模型理论存在断层；缺乏适当的分类语言与评测标准来对齐二者。

Method: 提出以“状态构建（隐式：上下文管理；显式：潜表示压缩）”与“动力学建模（知识整合与架构重构）”为核心的分类法；并建议评测转向功能性基准，检验物理持久与因果推理。

Result: 形成一套系统化的分析框架，明确当前方法在状态与动力学维度上的位置；提出具体评测转向与研究方向，为社区提供路线图。

Conclusion: 要从“看起来像真的视频”迈向“通用世界模拟器”，需：1) 以数据驱动记忆与高效压缩提升跨时持久性；2) 以潜因子解耦与推理先验强化因果建模；并以功能性指标作为进展衡量。

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [19] [Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances](https://arxiv.org/abs/2601.17071)
*Jisui Huang,Andreas Alpers,Ke Chen,Na Lei*

Main category: cs.CV

TL;DR: 提出一种处理强非均匀性图像的高效分割方法：先用线性最小二乘的OT指派生成超像素，再以经验分布间的平方2-Wasserstein距离贪心合并为目标级别分割，准确率提升且高效。


<details>
  <summary>Details</summary>
Motivation: 传统基于均值颜色的超像素合并对强照度/颜色非均匀、复杂纹理不鲁棒，且两级聚类(像素→超像素→目标)缺乏统一理论度量。作者希望用分布层面的最优传输距离统一两级聚类并提升在困难图像上的分割精度，同时保持计算效率。

Method: 两级聚类框架：1) 像素→超像素：将指派建模为线性最小二乘问题，可视为离散OT的特例，得到超像素。2) 超像素→对象：使用超像素颜色/特征的经验分布，计算平方2-Wasserstein距离(分布型OT距离)作为相似度，采用贪心合并策略得到对象级分割。整个流程形成跨两级的一致OT度量。

Result: 数值实验显示，在具有强非均匀性和挑战性场景的图像上，所提方法分割准确率优于基于均值颜色的传统合并策略，同时保持较高的计算效率。

Conclusion: 以分布型OT距离统一两级聚类可在不牺牲效率的前提下显著提升困难图像的分割效果，提供了从像素到对象的一体化OT视角。

Abstract: We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.

</details>


### [20] [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088)
*Rui-Yang Ju,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: 提出GlassesGB：将2D个性化眼镜生成与3D头像渲染结合，实现VR场景中的可定制眼镜虚拟试戴。


<details>
  <summary>Details</summary>
Motivation: 现有VTON多基于预设镜框模板，缺乏用户细粒度定制；GlassesGAN虽支持个性化但仅限2D图像，难以用于3D头像与VR渲染。

Method: 受3D Gaussian Blendshapes在头部重建成功启发，将其与GlassesGAN结合：利用2D生成进行个性化眼镜设计，再通过3D高斯混合/形变表情(Blendshapes)将设计映射到3D头像，实现在VR中的几何与外观渲染。

Result: 实现了从2D个性化设计到3D头像眼镜的无缝桥接，支持对VR头模的定制化眼镜生成与渲染；开源实现已发布。

Conclusion: GlassesGB有效解决VTON中个性化与3D渲染的脱节问题，为VR应用提供可定制眼镜生成的新框架。

Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.

</details>


### [21] [GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing](https://arxiv.org/abs/2601.17089)
*Qigan Sun,Chaoning Zhang,Jianwei Zhang,Xudong Wang,Jiehui Xie,Pengcheng Zheng,Haoyu Wang,Sungyoung Lee,Chi-lok Andy Tai,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出GRASP，一种针对遥感图像的参数高效微调策略：用空间块相关的软提示并通过问题引导的稀疏融合，将任务相关上下文聚合为紧凑全局提示，提升VQA性能且抑制背景干扰。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在遥感VQA上易过拟合背景或忽视目标细节，原因在于遥感图像存在尺度变化大、目标稀疏、区域语义复杂，传统微调/提示策略难以有效聚焦关键区域。

Method: 提出GRASP（Guided Region-Aware Sparse Prompting）：在冻结的视觉token网格上划分空间块，为各块引入空间结构化软提示；采用由问题引导的稀疏融合机制，动态选择并聚合与问题相关的块级提示，形成紧凑的全局提示输入MLLM，从而突出相关区域并过滤噪声；整体为参数高效的PEFT流程。

Result: 在多个人遥感VQA（RSVQA）基准上，GRASP优于或可与现有微调与提示方法竞争，同时保持较高的参数效率。

Conclusion: 面向遥感VQA，GRASP通过区域感知的稀疏提示与问题引导融合，有效缓解背景噪声与尺度/稀疏性带来的挑战，在不增加大量可训练参数的前提下取得强竞争力。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

</details>


### [22] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: 提出一种自动从高细节建筑模型提取多LoD草图表示的框架，利用生成式AI与计算机视觉逐级简化，保证几何一致与层级连贯，并以SSIM与归一化Hausdorff距离验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多LoD建模高度依赖人工，耗时费力且容易出现几何不一致；生成式AI潜力大但缺乏高质量成对LoD训练数据，限制了从草图到多级模型的应用。

Method: 构建渐进式LoD草图提取管线：将高细节建筑模型通过计算机视觉与生成式AI相结合进行逐级简化，从细节表示过渡到体量抽象；保证层级连贯与几何一致性，自动生成LoD3→LoD2→LoD1的配对数据。

Result: 在LoD3→LoD2与LoD2→LoD1过渡中，分别获得SSIM=0.7319与0.7532；对应归一化Hausdorff距离为图像对角线的25.1%与61.0%，显示抽象过程中几何偏差受控并保持结构一致性。

Conclusion: 该框架能在保证全局结构的前提下实现逐级语义简化，生成几何一致、层级清晰的多LoD表示，为AI驱动的多层级建筑生成与分层建模提供可靠数据与技术支撑。

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [23] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: 本文通过大规模实证研究，系统评估医疗影像AI在不同任务、指标与聚合策略下多种置信区间（CI）方法的可靠性（覆盖率）与精度（区间宽度），总结出影响CI行为的五大关键因素，并为制定性能不确定性报告指南提供依据。


<details>
  <summary>Details</summary>
Motivation: 医疗影像AI临床转化需要对模型性能的不确定性进行可靠量化。置信区间是核心工具，但社区对现有CI方法的种类与在特定设置下的行为缺乏系统认识，可能导致误用或过度自信的结论。因此，需要全面比较不同CI方法在真实多任务多指标场景中的表现。

Method: 在24个分割与分类任务上，针对每个任务组训练19个模型；覆盖常用性能指标（如Dice/IoU、AUC、准确率等）、多种结果聚合策略（宏/微等）与若干常见CI构造方法。跨所有设定估计每种CI方法的覆盖率与区间宽度，并分析其与样本量、任务类型、指标与聚合策略的依赖关系。

Result: 五点主要发现：1）获得可靠CI所需样本量差异巨大，受设定影响可从几十到数千例不等；2）CI行为强依赖所选性能指标；3）聚合策略显著影响CI可靠性（宏平均通常需更多样本 than 微平均）；4）任务类型（分割vs分类）会调制上述效应；5）不同CI方法在不同用例下的可靠性与精度并不等同。

Conclusion: CI方法的选择、指标与聚合策略的设定及样本量规划需联合考虑，且应依据具体任务与用例定制。研究结果可作为制定医疗影像AI性能不确定性报告与样本量/评估指南的关键参考。

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [24] [StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors](https://arxiv.org/abs/2601.17107)
*Qinkai Yu,Chong Zhang,Gaojie Jin,Tianjin Huang,Wei Zhou,Wenhui Li,Xiaobo Jin,Bo Huang,Yitian Zhao,Guang Yang,Gregory Y. H. Lip,Yalin Zheng,Aline Villavicencio,Yanda Meng*

Main category: cs.CV

TL;DR: StealthMark在不改变分割结果的前提下，通过调制模型不确定性并结合可解释方法（如LIME）在黑盒条件下嵌入并验证二维码水印，以保护医疗分割模型的版权，实验在多数据集与多模型上显示高ASR且几乎不降性能。


<details>
  <summary>Details</summary>
Motivation: 医疗数据标注昂贵且受隐私伦理限制，导致高质量分割模型成为重要且需要保护的知识产权；现有模型水印/保护方法多聚焦分类与生成任务，医疗分割领域缺乏有效的黑盒版权验证手段。

Method: 提出StealthMark：在推理时（或参数层面）对模型的不确定性分布进行细微、隐蔽的调制，使最终分割掩码不变；利用模型无关解释方法（如LIME）从输出中提取特征归因，在特定触发条件下使解释图显现出预设水印图案；水印以二维码编码，增强鲁棒性与可读性。

Result: 在4个医疗影像数据集与5个主流分割模型（含SAM）上验证：在几乎不影响Dice与AUC（<1%下降）的前提下，所有数据集上ASR>95%，显著优于基于后门的水印方法；具备隐蔽性与对性能的无害性。

Conclusion: StealthMark为医疗分割模型提供一种黑盒且无损性能的水印验证机制，通过不确定性调制与可解释性触发使二维码水印可被可靠识别，适合实际部署并优于现有后门类方案。

Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.

</details>


### [25] [iFSQ: Improving FSQ for Image Generation with 1 Line of Code](https://arxiv.org/abs/2601.17124)
*Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan*

Main category: cs.CV

TL;DR: 提出iFSQ：以分布匹配映射替换FSQ的激活函数，使量化桶利用率与重建精度同时最优；用其统一评测离散AR与连续扩散模型，发现每维约4比特最佳；AR收敛快但上限低，扩散上限高；并将REPA对齐到AR提出LlamaGen-REPA。


<details>
  <summary>Details</summary>
Motivation: AR（离散token）与扩散（连续潜变量）两路并行，源于VQ-VAE与VAE差异，导致难以统一建模与公平对比。FSQ理论上可桥接，但等间隔量化导致激活塌缩，需在重建与信息效率间权衡，缺乏既统一又稳定准确的基准。

Method: 将FSQ中的等间隔激活函数替换为与潜变量分布匹配的映射，强制均匀先验，称为iFSQ；该改动极简（“一行代码”），保证量化桶均衡使用与重建精度的数学性质。基于iFSQ统一比较AR与扩散：控制相同重建约束，测量收敛速度与性能上限；并将表示对齐方法REPA适配到AR，形成LlamaGen-REPA。

Result: iFSQ避免激活塌缩，实现最优bin利用与重建；实证得出最佳离散-连续平衡点约为每维4 bit；在同等重建约束下，AR初期收敛更快，而扩散达到更高性能上限；REPA迁移到AR后得到LlamaGen-REPA。代码开源。

Conclusion: iFSQ以最小改动统一离散与连续表征的量化框架，既提供稳定高保真的重建，又作为公平基准揭示：约4 bit/维是最佳表征容量；AR受严格顺序限制，上限低于扩散。扩展的LlamaGen-REPA进一步验证对齐对AR的益处。

Abstract: The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ

</details>


### [26] [Scaling medical imaging report generation with multimodal reinforcement learning](https://arxiv.org/abs/2601.17151)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Yu Gu,Ying Jin,Sam Preston,Yanbo Xu,Sid Kiblawi,Wen-wai Yim,Tim Ossowski,Tristan Naumann,Mu Wei,Hoifung Poon*

Main category: cs.CV

TL;DR: 提出UniRG通用框架，用强化学习直接优化任务指标，提升医学影像（以胸片为例）报告生成的泛化与SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽擅长文本理解推理，但在医学等多模态高价值场景仍存在显著能力缺口；监督微调易过拟合模板化语言，导致跨机构与实践的泛化差。

Method: 提出Universal Report Generation（UniRG）：以强化学习为统一机制，直接对面向应用的评估指标进行优化；在公开胸片数据上训练实例UniRG-CXR；在多种严格情境下进行全面评测，并与权威基准（ReXrank）对比。

Result: UniRG-CXR在ReXrank基准上取得新的总体SOTA，显著超越此前最优方法；在多机构与临床实践设置下展现更稳健的泛化能力。

Conclusion: 强化学习驱动的通用报告生成框架能有效克服监督微调的模板化过拟合问题，在医学影像报告生成任务上实现显著、可泛化的性能提升。

Abstract: Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.

</details>


### [27] [LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction](https://arxiv.org/abs/2601.17185)
*Shima Salehi,Atharva Agashe,Andrew J. McFarland,Joshua Peeples*

Main category: cs.CV

TL;DR: 提出一种在少视角条件下结合全局与局部频率正则化的3D重建方法，并发布多光谱温室数据集与基准，实验显示方法在几张视角下更稳定、更清晰且光谱一致。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting在少视角时几何不稳、细节丢失且跨光谱一致性差，缺乏统一的少样本评测基准与多光谱数据。

Method: 在3DGS框架中引入全局与局部频率正则化：全局频域约束稳定几何与整体频谱分布，局部频域约束保留细节与纹理；并提供多光谱（4波段）温室数据集与标准化few-shot重建协议/开源基准。

Result: 在新多光谱数据集与标准基准上，相比现有基线获得更锐利、更稳定且光谱一致的重建结果。

Conclusion: 频率正则化能显著提升少视角3DGS重建质量；提供的数据集与基准为社区评测与复现提供了标准化平台，代码与数据已公开。

Abstract: We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available

</details>


### [28] [Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments](https://arxiv.org/abs/2601.17194)
*Cheyu Lin,Katherine A. Flanigan,Sirajum Munir*

Main category: cs.CV

TL;DR: 该论文提出DUET数据集与基于骨架运动的“嵌入式身势学识别”框架，用以在隐私保护前提下识别二人互动的交流功能（依托Ekman & Friesen身势学五大功能），并基准评测现有HAR模型与提出的迁移学习方法，展示其跨人群与场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前土木与建成环境研究缺乏一致、可比且保护隐私的方式来表征和测量具有社会意义的互动。不同研究对“互动”的操作化各异，难以评估设计干预是否改变了社会资本理论强调的重要互动形式。需要一个统一、与社会资本相关的互动词汇与可测量框架。

Method: 1) 构建DUET数据集：涵盖12种二人互动，覆盖身势学五大功能（象征、说明、情感展示、适应器、调节器），采集四种感知模态与三个建成环境场景，使用隐私友好的骨架表示。2) 基准评测：对6个开源SOTA人类活动识别模型在DUET上的表现进行量化，检验从单人动作识别扩展到二人、以交流功能为目标的难度与局限。3) 提出识别框架：直接从骨架时空序列中端到端推断交流功能，无需手工“动作-功能”字典；采用迁移学习架构，并分析表示质量与分类性能的关系及功能聚类结构。

Result: 基准结果显示：现有以单人动作为主的HAR模型在DUET上的交流功能识别效果受限，凸显任务难度与方法差距。所提出的框架能从骨架数据中学习到有结构的身势功能簇，表示质量与分类性能高度相关，并在跨被试与跨场景上具备较好的泛化。

Conclusion: DUET与身势功能识别框架为建成环境中的社会互动提供了统一、可比较且隐私保护的测量路径，弥合了动作级识别与社会资本相关的互动功能测量之间的鸿沟，为评估空间设计对社区韧性与社会福祉的影响提供了可复用基线与方法。

Abstract: Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.

</details>


### [29] [Structural Complexity of Brain MRI reveals age-associated patterns](https://arxiv.org/abs/2601.17211)
*Anzhe Cheng,Italo Ivo Lima Dias Pinto,Paul Bogdan*

Main category: cs.CV

TL;DR: 提出一种适用于三维（特别是脑MRI）信号的结构复杂度分析，通过滑动窗口粗粒化稳定估计多尺度信息损失；在大样本成人MRI中发现复杂度随年龄下降，且粗尺度效应最强，并可用于生物年龄预测。


<details>
  <summary>Details</summary>
Motivation: 二维/块式复杂度度量在3D体数据和粗尺度上不稳定，难以可靠表征多尺度组织；需要一种稳健的方法捕捉体素尺度到粗尺度的信息组织，用于神经影像表型与老化等生物学问题。

Method: 将结构复杂度框架扩展到三维体数据：对体素体积作逐级空间粗粒化，量化相邻分辨率间的信息损失。为避免传统块划分在大尺度采样不足导致的方差爆炸，提出滑动窗口粗粒化以获得平滑、稳健的复杂度估计。应用该方法于中晚年大型结构MRI队列，跨多尺度评估复杂度与年龄的关系，并用于生物年龄预测。

Result: 滑动窗口法在大尺度上较块式更平滑、鲁棒；在人群数据中，结构复杂度随年龄系统性下降，且在更粗的空间尺度上效应更强。基于复杂度特征可有效预测个体的生物年龄。

Conclusion: 滑动窗口多尺度结构复杂度为3D影像提供稳健的多尺度分析工具；在脑MRI中揭示随年龄增长的宏观尺度复杂度降低，并显示在年龄预测中的实用价值。

Abstract: We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.

</details>


### [30] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: 提出以语义通信为核心的V2X碰撞预测框架：RSU用V-JEPA生成未来帧语义嵌入，通过V2X传输给车端轻量解码器做即将碰撞预测；在数字孪生城市场景验证，通信量降4个数量级且F1提升10%。


<details>
  <summary>Details</summary>
Motivation: 传统方法需从RSU向车辆传输原始视频/高维传感数据，受车联网带宽与时延限制不可行；需要在保证实时性的同时保留与碰撞预测相关的关键动态信息，降低通信负载。

Method: 在RSU端部署V-JEPA，预测未来帧并提取时空语义嵌入；通过V2X仅发送嵌入至车端；车端使用轻量注意力probe与分类器对嵌入进行解码，预测即将发生的碰撞；构建城市交通数字孪生，生成包含安全与碰撞事件的多样场景以评估系统；比较原始视频传输与语义嵌入传输的准确性与通信开销。

Result: 在适当处理流程下，所提框架在碰撞预测任务的F1分数提升约10%，同时相较传输原始视频将通信需求降低约四个数量级。

Conclusion: 语义V2X通信结合V-JEPA生成的未来帧嵌入，能在大幅降低通信带宽的同时保持或提升碰撞预测性能，具备在智能交通系统中实现协同、实时碰撞预警的潜力。

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [31] [Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification](https://arxiv.org/abs/2601.17228)
*Tengyue Zhang,Ruiwen Ding,Luoting Zhuang,Yuxiao Wu,Erika F. Rodriguez,William Hsu*

Main category: cs.CV

TL;DR: 提出一种用于计算病理的半监督领域自适应框架：用跨源/目标无标注数据训练的潜在扩散模型生成保持形态、带有目标域外观的合成图像，与源域标注数据一起训练分类器，从而提升目标域泛化。


<details>
  <summary>Details</summary>
Motivation: 病理图像深度模型在跨机构/队列时易因域移位而失效。现有方法要么不能利用目标域无标注数据，要么依赖风格迁移/图像到图像翻译，可能扭曲组织形态，影响下游任务准确性。需要一种既利用目标域信息又保护组织结构的域适配增强方法。

Method: 训练一个潜在扩散模型，联合使用源域与目标域的无标注切片。扩散模型以基础模型特征、队列身份、组织制备方式为条件，生成“目标感知”的合成图像：保持源域形态结构，同时注入目标域外观。将这些合成图像与源域带标注的真实图像一起用于训练下游分类器，并在目标域测试。属于半监督领域自适应（SSDA）设定。

Result: 在肺腺癌预后任务上，所提增强在目标队列的保留测试集上显著优于基线：加权F1由0.611提升至0.706，宏平均F1由0.641提升至0.716；同时不降低源队列性能。

Conclusion: 目标感知、基于扩散的合成数据增强能在不破坏组织形态的前提下有效缓解域移位，提升计算病理的跨域泛化表现，是一种有前景的SSDA方案。

Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.

</details>


### [32] [C-RADIOv4 (Tech Report)](https://arxiv.org/abs/2601.17237)
*Mike Ranzinger,Greg Heinrich,Collin McCarthy,Jan Kautz,Andrew Tao,Bryan Catanzaro,Pavlo Molchanov*

Main category: cs.CV

TL;DR: C-RADIOv4 是一套通过多教师蒸馏聚合视觉骨干的模型家族，在相同计算复杂度下显著提升多项下游任务表现，并扩展任意分辨率与高分辨率高效推理选项。


<details>
  <summary>Details</summary>
Motivation: 在不增加计算成本的前提下，将多种强大但各具专长的视觉教师模型的能力统一到单一学生骨干中，既保留差异化优势，又提升通用性与下游任务表现。

Method: 采用多教师蒸馏框架（来自 SigLIP2、DINOv3、SAM3），以聚合式视觉骨干（延续 AM-RADIO/RADIOv2.5 设计）训练统一学生模型；发布两个规模（SO400M≈412M参数、H≈631M），并提供 ViTDet 选项以在高分辨率下大幅提升效率，同时增强任意分辨率支持。

Result: 在关键下游任务核心指标上取得显著提升；从 SAM3 继承了新的能力；在高分辨率场景下通过 ViTDet 选项获得大幅效率优势；总体计算复杂度与前代相当。

Conclusion: C-RADIOv4 将多教师的互补能力有效融合，提供更强的一体化视觉骨干，在不增算力的情况下提升性能与适用范围，并以宽松许可发布，便于实际应用。

Abstract: By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.

</details>


### [33] [Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization](https://arxiv.org/abs/2601.17254)
*Takato Yasuno*

Main category: cs.CV

TL;DR: 提出一个开源的桥梁病害检测系统：用SAM3定位钢筋外露腐蚀，DBSCAN补全漏检区域；对含地区信息的施工标志进行高斯模糊；多种预处理提升OCR；GPU加速将单图处理降至1.7秒，实现在保护区域隐私前提下的高效巡检。


<details>
  <summary>Details</summary>
Motivation: 日本法规要求每5年进行一次目视巡检，但现场图像常同时包含病害细节（裂缝、钢筋外露）与能暴露地区信息的施工标志。既要准确提取病害特征以支撑维修决策，又要避免因暴露区域信息引发公众焦虑与隐私风险，因此需要兼顾“病害检测精度”和“区域隐私保护”的自动化系统。

Method: 构建端到端的图像处理与检测管线：1) 使用Segment Anything Model (SAM3)对钢筋腐蚀/外露进行分割检测；2) 采用DBSCAN聚类对漏检的小片段进行自动补全；3) 通过检测施工标志区域并施加高斯模糊以隐去地区信息；4) 设计四种图像预处理策略提升OCR识别准确率；5) 结合GPU优化以缩短推理时延。技术栈涵盖SAM3、PyTorch、OpenCV、pytesseract、scikit-learn。

Result: 系统在单张图像上平均处理时延约1.7秒；在钢筋腐蚀检测上通过DBSCAN后提升了区域完整性；OCR在预处理后准确率提高；对施工标志的模糊处理有效屏蔽了地区信息，实现病害检测与隐私保护的兼顾（具体量化指标未在摘要中给出）。

Conclusion: 提出并实现了具区域隐私保护能力的开源桥梁病害检测系统，能在保证检测效率与效果的同时，隐去可能泄露地区信息的元素，为日本法规背景下的基础设施巡检与公众沟通提供实用方案。

Abstract: In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.

</details>


### [34] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: 提出FineVAU基准与FVScore指标和FineW3数据集，面向视频异常理解，强调对异常事件的细粒度“是什么-谁-哪里”评测，显著提升与人类感知的一致性，并揭示LVLM在时空细粒度异常感知上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VAU评测多依赖n-gram指标或LLM打分：前者无法反映自由形式、视觉落地的回答质量；后者偏重语言流畅性而非事实一致性，主观性强且与人类感知偏离。需要一个能细粒度、可解释、以视觉要素为核心且人类一致性高的评测框架。

Method: 将VAU表述为三要素理解：事件(What)、参与体(Who)、地点(Where)。提出FineVAU基准，包括：1) FVScore——对LVLM回答中关键视觉要素的存在性与正确性进行细粒度、可解释打分，强调与人类判断一致；2) FineW3数据集——通过结构化、全自动流程在现有人类标注上进行增强，加入高质量细粒度视觉信息，覆盖事件、实体与位置。

Result: 在人类评测中，FVScore与人类感知的一致性优于现有n-gram与LLM评测。基于FineVAU的大量实验显示：LVLM在粗粒度、静态信息与强视觉线索事件上表现较好，但在需要空间关系与细粒度时间建模的异常事件上显著不足。

Conclusion: FineVAU提供了更人类一致、可解释的VAU评测方案与数据资源，推动对异常视频的细粒度理解研究，同时揭示当前LVLM在时空细粒度异常感知方面的关键短板。

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [35] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: 提出一种在推理阶段、无需再训练的、区域约束颜色保持方法，结合ROI修补、背景潜空间再施加与基于Lab+线性RGB的梯度引导损失（含CVaR/soft-max、延迟启动与时间调度），显著提升文本到图像扩散在指定区域的目标颜色一致性并避免全局色漂，可直接集成到Stable Diffusion修补流程。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在需要精确遵循用户指定颜色时常失败，尤其在设计类工作流中；即便满足平均颜色约束，局部仍会出现显著偏差，影响可用性。

Method: 在推理时对预训练扩散模型进行无训练引导：1) 通过基于ROI的inpainting实现空间选择性；2) 对ROI外的背景潜变量进行再施加以抑制色漂；3) 在潜空间中施加梯度引导，损失在CIE Lab与线性RGB上联合定义，既约束ROI均值颜色，也通过CVaR式与soft-maximum惩罚控制像素误差尾部；并采用延迟启动门与随步数变化的时间调度以稳定去噪过程。

Result: 相较仅约束均值颜色的基线，所提方法在保持总体色彩目标的同时显著减少感知上突出的局部失败，实现更加严格、分布感知的颜色一致性。

Conclusion: 该方法为训练自由、可落地的定向颜色遵循机制，可无缝集成到标准Stable Diffusion修补管线中，解决设计场景下的精确颜色控制难题。

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [36] [Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales](https://arxiv.org/abs/2601.17271)
*Kun Huang,Fang-Lue Zhang,Neil Dodgson*

Main category: cs.CV

TL;DR: 提出Cross360：用跨注意力对齐切平面贴图与等距矩形特征，并逐级注意力聚合多尺度特征，实现更准确、全局一致的360°深度估计，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 360°图像深度估计难在既保持全局连续性又避免球面投影失真。多投影融合方法常在全局一致性与局部精度间失衡：局部补丁缺乏全局感知，跨补丁边界特征不一致。需要一种能同时利用低失真局部视图与全景全局上下文的表示与对齐机制。

Method: 提出Cross360架构：1) 使用切平面（tangent）局部补丁与等距矩形（ERP）全景特征并行编码；2) 设计Cross Projection Feature Alignment（CPFA）模块，以跨注意力将切平面补丁特征与ERP的360°全局视野对齐，使每个补丁获得全局语义；3) 设计Progressive Feature Aggregation with Attention（PFAA）模块，逐级、带注意力地融合多尺度特征，逐步细化深度；4) 端到端训练。

Result: 在多数基准数据集上显著优于现有方法，尤其在整幅360°图像可用的场景中表现突出。

Conclusion: 跨注意力对齐局部低失真补丁与全景全局特征，并进行逐级注意力聚合，可在360°深度估计中实现更高精度且全局一致的结果；方法通用、有效，代码已开源。

Abstract: 360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.

</details>


### [37] [Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing](https://arxiv.org/abs/2601.17288)
*Jin Bai,Huiyao Zhang,Qi Wen,Shengyang Li,Xiaolin Tian,Atta ur Rahman*

Main category: cs.CV

TL;DR: 提出Fluxamba，一种拓扑感知的轻量级地质线性要素分割网络，通过结构流块实现沿曲线几何的上下文聚合，在多数据集上达SOTA，以3.4M参数、6.3G FLOPs达>24 FPS与高精度（LROC F1 89.22%、mIoU 89.87%）。


<details>
  <summary>Details</summary>
Motivation: 现有SSM虽高效但沿固定轴向序列扫描，与曲线、各向异性地质线要素的拓扑不匹配，导致长程依赖断裂与特征侵蚀；需要一种能沿目标内在几何方向聚合上下文、兼顾低对比场景鲁棒性且适合实时/上机部署的方法。

Method: 提出Fluxamba拓扑感知特征矫正框架：核心结构流块(SFB)由各向异性结构门(ASG)与先验调制流(PMF)组成，实现将特征方向与空间位置解耦，按目标内在几何动态门控上下文聚合；并加入分层空间调节器(HSR)做多尺度语义对齐，及高保真聚焦单元(HFFU)提升低对比微弱特征的信噪比。网络轻量化，仅3.4M参数、6.3G FLOPs，支持实时推理。

Result: 在LROC-Lineament、LineaMapper、GeoCrack等地质基准上达SOTA；在LROC上取得F1=89.22%、mIoU=89.87%；推理速度>24 FPS，较重型基线在计算成本上可降至1/100量级。

Conclusion: 沿曲线拓扑进行各向异性信息流与多尺度对齐/增强，可在保持或提升精度的同时显著降低计算开销，刷新精度-部署可行性的Pareto前沿，适合资源受限场景的地质线性要素分割。

Abstract: The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.

</details>


### [38] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: 提出一个动态元集成框架（DMEF），在资源受限边缘设备上，通过自适应加权融合三种轻量CNN（MobileNetV2、NASNetMobile、InceptionV3），在精度与复杂度间权衡，实现马铃薯和玉米病害高精度识别（99.53%、96.61%），优于单模型与静态集成，同时保持<75ms延迟与<100万参数。


<details>
  <summary>Details</summary>
Motivation: 边缘设备（物联网传感器、手机、嵌入式系统）算力与能耗受限，传统高精度模型难以落地；需要在保证高准确率的同时兼顾模型体量与推理延迟，实现田间可用的病害诊断。

Method: 提出DMEF：以三种轻量CNN为基学习器，设计动态自适应加权机制。训练过程中迭代更新集成权重，以同时最大化精度增益（DeltaAcc）并最小化复杂度（以模型大小/参数量为代理）。在推理端使用学得的权重对三模型预测进行加权融合，达到精度-效率权衡。

Result: 在马铃薯、玉米基准数据集上达到SOTA分类准确率：99.53%与96.61%，分别较单模型与静态集成提升约2.1%与6.3%。推理延迟<75ms，参数量<100万，满足边缘部署需求。

Conclusion: DMEF在保证高准确率的同时显著降低计算与存储开销，优于单模型与静态集成，适合边缘农业监测场景，具备可扩展的田间病害管理应用潜力，弥合高精度AI与实际部署之间的鸿沟。

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [39] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: 提出ClinNet，用证据论序回归解决膝骨关节炎放射分级，融合双侧不对称编码、类原型记忆库与NIG证据头，实现连续等级与不确定度联合估计，性能与可靠性均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: KOA分级存在等级差异细微、标注不确定且病程具有序与连续性；传统多分类忽视连续退变与标注不确定，难以提供可信预测与不确定性评估。

Method: 将问题建模为证据论的序回归：1) 双侧不对称编码器显式捕获内外侧结构差异；2) 诊断记忆库维护各类原型以稳定表示并增强判别；3) 基于Normal-Inverse-Gamma的证据序头，联合回归连续KL等级并估计认识不确定性（epistemic），可做不确定性驱动的决策。

Result: 在实验中取得QWK=0.892、Accuracy=0.768，统计上显著优于现有方法（p<0.001）；不确定性可有效识别分布外样本与潜在误诊。

Conclusion: ClinNet在KOA放射分级中同时提升准确性与可信度，适合临床安全部署场景，提供连续预测与可操作的不确定性标记。

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [40] [SkyReels-V3 Technique Report](https://arxiv.org/abs/2601.17323)
*Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-V3 是一个基于扩散Transformer的统一多模态“上下文内学习”视频生成框架，单一架构支持参照图像生视频、视频续写/多镜头切换、以及音频驱动的说话人视频生成，并在视觉质量、指令跟随与专项指标上达到SOTA或接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 构建具备多模态上下文推理能力的世界模型需要强大的条件视频生成；现有方法在身份保持、时序一致、构图稳定、长时延音画同步与多镜头叙事等方面仍存在不足，且往往为不同任务采用割裂的架构。

Method: 提出统一的扩散Transformer框架，覆盖三种范式：
1) 参照图像→视频：通过跨帧配对、图像编辑与语义重写的数据管线降低拷贝粘贴伪影；训练时采用图像-视频混合与多分辨率联合优化以提升泛化与稳健性。
2) 视频续写/扩展：结合时空一致性建模与大规模视频理解，实现单镜头无缝续写与多镜头智能切换（符合专业电影摄影模式）。
3) 音频驱动说话人：通过首末帧插入训练策略与关键帧推理范式，实现分钟级音频条件视频生成并优化音画同步。

Result: 在视觉质量、指令遵循与专项指标上达到或接近SOTA水平，逼近领先的闭源系统；展示出强身份保持、时序一致与专业镜头语言能力。

Conclusion: 统一的多模态条件视频生成框架能在单一模型中兼顾参照生成、视频扩展与音频驱动任务，借助数据与训练策略优化实现高保真与强一致性，具备接近闭源前沿系统的综合性能。

Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.

</details>


### [41] [SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision](https://arxiv.org/abs/2601.17326)
*Jasmine Lesner,Michael Beyeler*

Main category: cs.CV

TL;DR: 提出SymbolSight框架：用SPV和神经代理估计字形混淆度，并结合语言双字统计优化“符号→字母”映射，以在视网膜假体的顺序呈现中减少余像干扰；在阿拉伯语、保加利亚语、英语上预测混淆中位数下降22倍。


<details>
  <summary>Details</summary>
Motivation: 视网膜假体能恢复有限视觉，但低空间分辨率与显著时间残留导致顺序字母阅读中前后干扰，传统字形为自然视觉设计，可能不适配串行、低带宽呈现。硬件短期难以显著提升，因此希望通过符号设计优化来缓解时间干扰并提升可读性。

Method: 构建SymbolSight：1) 在模拟假体视觉（SPV）下，用神经代理观察者估计任意两符号在顺序呈现下的成对混淆概率；2) 利用语言特定的双字（bigram）频率加权混淆成本；3) 通过组合优化选择异质符号集合并求解符号-字母映射，最小化整体混淆；4) 在多语言（阿拉伯语、保加利亚语、英语）上进行仿真评估。

Result: 与原生字母表相比，优化后的异质符号集在仿真中使预测混淆中位数降低约22倍；表明许多常见相邻字母对的识别干扰被显著缓解。

Conclusion: 标准排印体系不适合串行、低带宽的假体视觉；通过计算建模和语言统计可系统性缩小视觉编码设计空间，产出高潜力候选方案，值得后续心理物理与临床验证。

Abstract: Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.

</details>


### [42] [Learning with Geometric Priors in U-Net Variants for Polyp Segmentation](https://arxiv.org/abs/2601.17331)
*Fabian Vazquez,Jose A. Nuñez,Diego Adame,Alissen Moreno,Augustin Zhan,Huimin Li,Jinghao Yang,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: cs.CV

TL;DR: 提出一个可插拔的几何先验引导模块（GPM），用深度图为U-Net系方法注入几何与结构信息，显著提升息肉分割，尤其在低对比与杂乱场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN/Transformer/Mamba的U-Net变体虽性能强，但在低对比、遮挡与复杂背景下难以捕捉几何与结构线索，导致边界不准、漏检。需要显式几何先验来增强特征表达。

Method: 1) 在模拟的ColonDepth数据上微调VGGT，获取贴合内镜域的息肉图像深度图；2) 设计GPM，将深度图编码为几何先验并注入U-Net编码器特征；3) 通过空间与通道注意力联合细化，强化局部空间与全局通道信息；4) GPM为即插即用，可无缝集成到多种U-Net变体中。

Result: 在五个公开息肉分割数据集、三种强基线上，集成GPM均带来一致增益；作者开源代码与生成的深度图。

Conclusion: 显式几何先验（由域自适应深度估计提供）能稳定提升U-Net系息肉分割的鲁棒性与精度，特别适用于低对比和杂乱场景；模块通用、易集成，具有实践潜力。

Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg

</details>


### [43] [AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17336)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: 提出AGE-Net用于膝关节X线KL分级：在ConvNeXt上融合频谱-空间特征、解剖图推理与差分式精炼，并用NIG证据回归与序关系约束来处理不确定性与序性；在数据集上取得QWK≈0.902、MSE≈0.235，优于强CNN基线。


<details>
  <summary>Details</summary>
Motivation: KL分级存在细微结构变化难捕捉、需要长程解剖依赖、以及等级边界模糊带来的不确定性与序性问题；传统CNN和分类损失难以同时刻画不确定性、序关系与可解释性。

Method: 构建基于ConvNeXt的AGE-Net，包括：1) SSF：在频谱与空间域融合以增强细微结构敏感性；2) AGR：基于解剖关键区域的图结构建模长程依赖并进行图推理；3) DFR：对边界样本进行差分式精炼；4) 头部采用Normal-Inverse-Gamma证据回归输出均值与不确定性；5) 引入成对序排序约束保持等级有序性。

Result: 在膝关节KL数据集上，三种随机种子下QWK=0.9017±0.0045，MSE=0.2349±0.0028；相较强CNN基线显著提升，消融实验显示各组件均有贡献。

Conclusion: AGE-Net在KL分级上实现更高准确性并量化不确定性，兼顾序性与可解释性；后续将补充不确定性质量、鲁棒性与可解释性评估的更多图示与细节。

Abstract: Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.

</details>


### [44] [TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution](https://arxiv.org/abs/2601.17340)
*Haodong He,Xin Zhan,Yancheng Bai,Rui Lan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出Real-Texts数据集与TEXTS-Diff扩散模型，面向真实场景文本图像超分，兼顾背景与文字区域，显著提升可读性与视觉质量，达到SOTA并具更强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有数据集文本样本稀缺且多为割裂文本，导致对文字区域的恢复差、背景重建不足；真实世界复杂退化和文字变形下，现有方法易出现文字失真与幻觉。

Method: 1) 构建Real-Texts：大规模高质量、来自真实场景，涵盖中英文、丰富场景与退化，包含自然文本实例；2) 提出TEXTS-Aware Diffusion (TEXTS-Diff)：在扩散框架中引入“抽象概念”以提升对场景中文字语义/结构理解，并用“具体文本区域”监督强化细节；整体在背景与文字区域联合优化，抑制文字扭曲与幻觉，同时保持场景保真。

Result: 在多项评测指标上达到或超过现有方法，文字恢复准确度与复杂场景泛化能力显著提升；视觉质量与文本可读性均优。

Conclusion: 结合真实大规模数据与文本感知扩散建模，可同时提升背景保真与文字清晰度，缓解文本区域失真/幻觉问题，取得SOTA表现；代码、模型与数据集将开源。

Abstract: Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.

</details>


### [45] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: 提出STARS框架，针对多模态遥感中模态缺失问题，通过非对称对齐的双向翻译+停梯度与像素级语义采样对齐（PSA）缓解特征坍塌、类别不平衡，提升语义分割鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态遥感（光学、SAR、DSM）融合能提升地表语义理解，但实际常有光学或DSM缺失，导致传统融合模型显著退化。现有缺模态方法存在特征坍塌、恢复特征过于泛化、对类不平衡敏感等问题，亟需一种在不完整输入下稳健的分割方案。

Method: 提出STARS：1) 非对称对齐机制：在不同模态间进行双向特征翻译，并在关键路径施加stop-gradient以打破相互依赖，避免特征坍塌，同时降低对超参数敏感性；2) PSA像素级语义采样对齐：结合类别均衡的像素采样与跨模态语义对齐损失，重点对少数类与高难样本进行对齐，缓解类不平衡导致的对齐失败。整体作为端到端的语义分割框架，适配缺失模态场景。

Result: 在缺模态语义分割场景下，相比现有方法，STARS展现更强鲁棒性与更高精度，尤其提升少数类识别与对齐稳定性，并减少因超参数选择带来的性能波动。（具体数值未在摘要提供）

Conclusion: 通过共享-特定表示的翻译与非对称对齐，以及PSA的类均衡语义对齐，STARS有效避免特征坍塌并提升在模态缺失条件下的遥感语义分割表现，对实际不完整多模态应用具有实用价值。

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [46] [Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective](https://arxiv.org/abs/2601.17349)
*Hailong Yan,Shice Liu,Xiangtao Zhang,Lujian Yao,Fengxiang Yang,Jinwei Chen,Bo Li*

Main category: cs.CV

TL;DR: 提出一种基于YUV色彩空间的轻量化低照度图像增强框架，利用频域分析发现Y通道主要丢失低频、UV通道受高频噪声污染，据此设计Y/UV差异化注意力与引导交互模块，实现在更少参数下的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 移动设备上L3IE需要在视觉质量与模型轻量之间权衡。现有基于Retinex或YUV的解耦方法忽略了“按通道的退化模式”和“跨通道交互”，限制了性能。因此作者通过频域分析明确各通道的退化特性，作为改进设计依据。

Method: 1) 频域分析：验证YUV优于RGB并揭示Y通道低频缺失、UV通道高频受噪。2) 模型架构：- 对Y通道：Dual-Stream Global-Local Attention（全局-局部双流注意力）以恢复低频结构与局部细节；- 对UV通道：Y-guided Local-Aware Frequency Attention（由Y引导的本地频率注意力）以抑噪和保真；- Guided Interaction模块：融合跨通道特征以完成重建。3) 轻量化设计：在保证效果的同时显著减少参数量。

Result: 在多个标准基准上达到新的SOTA，视觉质量优于现有方法，同时参数量显著更低。

Conclusion: YUV色彩空间下的按通道频域感知与跨通道引导是实现轻量高效低照度增强的关键；所提架构在保持模型紧凑的同时显著提升了效果，可为移动端L3IE提供实用方案。

Abstract: In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.

</details>


### [47] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 提出NeRF-MIR，用NeRF解决被遮挡/掩膜图像的三维重建与修复，通过更聪明的射线采样、逐步自训练修复与动态损失加权，实现对掩膜区域更好的重建，实验与自建数据集上优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 现实拍摄中常有遮挡、损坏或掩膜导致图像缺失，直接用标准NeRF会因随机射线采样与监督不足而难以恢复细节与被遮挡区域；现有数据集也不足以评估NeRF在掩膜修复场景的表现。

Method: 1) PERE：基于补丁熵的射线发射策略，优先向纹理复杂/信息丰富补丁分配射线，以更好学习细节并融合多视角信息；2) PIRE：渐进式迭代自训练，逐步利用NeRF自身的预测去填补掩膜区域并再训练，提升缺失区域质量；3) 动态加权损失：自适应调整掩膜区域与非掩膜区域的损失权重，聚焦难点区域；4) 构建三套掩膜数据集，模拟不同破坏场景用于训练与评测。

Result: 在真实数据与自建三种掩膜数据集上，NeRF-MIR在掩膜图像修复/新视角合成指标上优于现有方法，显示更强的细节恢复与跨视角一致性。

Conclusion: 通过熵引导的射线调度、逐步自训练修复和动态损失权重，NeRF在掩膜图像修复场景中得到显著增强；自建数据集验证了其有效性，为受损图像的三维重建与新视角合成提供了新途径。

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [48] [HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data](https://arxiv.org/abs/2601.17352)
*M. L. Mamud,Piyoosh Jaysaval,Frederick D Day-Lewis,M. K. Mudunuru*

Main category: cs.CV

TL;DR: 提出HyDeMiC：基于CNN的高光谱矿物分类器，在合成噪声数据下仍保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法（判别分析、逻辑回归、SVM）在高维HSI中易受环境噪声、传感器限制与计算复杂度影响，分类鲁棒性不足；需要一种能在真实（含噪）场景中稳定识别矿物的模型。

Method: 构建CNN模型HyDeMiC。用USGS实验室实测的115种矿物光谱为参考，通过与特定HSI传感器响应函数卷积生成训练样本；包含含铜矿物（赤铜矿、孔雀石、黄铜矿）作为展示案例。在合成2D高光谱数据上注入1%、2%, 5%, 10%噪声以模拟实地条件；用MCC评估性能。

Result: 在无噪与低噪数据上MCC≈1.00，且在中等噪声下仍保持较强分类表现。

Conclusion: HyDeMiC对噪声具鲁棒性，适用于真实HSI矿物探测任务，尤其在中等噪声条件下仍能可靠分类。

Abstract: Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.

</details>


### [49] [PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling](https://arxiv.org/abs/2601.17354)
*Wenzhi Guo,Guangchi Fang,Shu Yang,Bing Wang*

Main category: cs.CV

TL;DR: PocketGS提出一种在移动设备上进行3D Gaussian Splatting训练的方案，通过三项协同设计（G/I/T）在极低训练时长与内存限制下仍保持高保真重建，实验显示其在移动端可达甚至超过常规工作站3DGS基线的质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS虽能实时建模，但默认充足资源与长时训练，不适用于受限的手机端：训练时间以分钟计、峰值显存/内存受限、反向传播易不稳定，导致质量和效率难两全。需要一种既高效又内存紧凑、且保持感知质量的移动端训练范式。

Method: 提出PocketGS，含三大算子协同设计：1) G：构建几何可信的点云先验，为后续高质量高斯初始化提供结构支撑；2) I：注入局部表面统计以初始化各向异性高斯，缩小早期条件差距、加速收敛；3) T：将alpha合成“展开”，并缓存中间量、采用索引映射的梯度散射，实现移动端稳定、低内存的反向传播。整体满足训练效率、内存紧凑与建模保真度三重目标。

Result: 在多组实验中，PocketGS在移动设备上实现高质量重建，且在感知质量上可超越主流工作站版3DGS基线；同时满足分钟级训练预算与设备峰值内存限制，训练过程稳定。

Conclusion: PocketGS证明了在移动端可实现实用的“采集到渲染”全流程3DGS训练。其G/I/T算子化设计化解标准3DGS在效率、内存与质量间的矛盾，为资源受限设备上的高保真3D建模提供了有效范式。

Abstract: Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.

</details>


### [50] [UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation](https://arxiv.org/abs/2601.17366)
*Chengbo Ding,Fenghe Tang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: UCAD是一种用于半监督医学图像分割的“基于不确定性引导、轮廓感知的位移一致性学习”框架：用超像素生成解剖一致区域、用不确定性挑选难例区域进行位移并以动态不确定性加权一致性损失稳定训练，从而在少标注下显著提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有位移/扰动一致性策略多在矩形块上操作，破坏解剖边界，导致边界形变与语义不一致，限制了一致性学习效果；需一种同时保留轮廓语义并聚焦难例区域的半监督方法。

Method: 1) 轮廓感知区域：利用超像素分割形成与解剖边界对齐的区域；2) 不确定性引导选择：依据模型不确定性挑选“困难”超像素区域进行位移/交换以加强一致性学习，同时避免易例噪声；3) 动态不确定性加权一致性损失：对无标注区域的监督按不确定性自适应加权，随训练进程动态调节，稳定训练并抑制伪标签噪声。

Result: 在多组半监督医学分割基准上，UCAD稳定优于当前SOTA，尤其在标注极少的设定下取得更高Dice/IoU等精度指标。

Conclusion: 面向半监督分割，结合超像素的结构一致区域与不确定性引导的难例位移，以及动态加权一致性损失，可在保持解剖与语义一致的同时提升泛化与精度；UCAD为半监督医学分割提供了有效且稳健的训练范式。

Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.

</details>


### [51] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: 提出首个物理层面的提示注入攻击(PPIA)，在完全黑盒、与查询无关的条件下，通过在现实物体上嵌入可被LVLM感知的文字指令来操控其行为，跨10个模型、仿真与真实环境中在多任务上达至最高98%攻击成功率，并对视距、视角、光照具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入多假设可访问输入通道或已知用户查询，这在真实部署中不成立；LVLM在开放物理环境中广泛应用但安全性薄弱，亟需研究无需接入权限、仅依赖视觉观测即可实施的现实世界攻击，以揭示系统风险。

Method: 提出PPIA：1) 离线筛选高可识别、语义有效的视觉提示(恶意文字/图形)；2) 基于时空注意指导在环境中策略性摆放(位置、时间、视角)，确保被LVLM感知并影响决策；3) 全程黑盒、与查询无关，不修改模型或输入管线，仅通过物理对象进行攻击。

Result: 在10个SOTA LVLM上、仿真与真实场景中，覆盖VQA、规划、导航等任务，PPIA实现最高98%攻击成功率；在不同距离、视角、光照等物理条件下表现出强鲁棒性。

Conclusion: LVLM在现实场景下对物理提示注入极为脆弱，黑盒、查询无关的攻击可稳定操控其行为；提示注入防护需从感知链路与多模态对齐到输入净化与鲁棒训练等层面系统性加强。

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [52] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的高质量、鲁棒数字水印框架：先进行空文本反演得到噪声，在潜空间优化后通过迭代去噪生成水印图，借助去噪“净化”提升视觉质量与抗畸变；并用自注意力约束与伪掩码保持语义。对COCO多种变换较Stable Signature平均提升约10%。


<details>
  <summary>Details</summary>
Motivation: 现有深度水印方法虽能隐匿水印且不显著降质，但在传输过程中的图像损坏（压缩、裁剪、噪声、模糊等）下鲁棒性不足，限制了实用性。需要能兼顾高画质与强鲁棒性的方案。

Method: 利用扩散模型的反演与去噪特性：1) 对输入清洁图像做null-text优化得到其反演噪声；2) 在潜空间对该反演噪声进行带约束优化以嵌入水印；3) 通过扩散模型的逐步去噪过程生成带水印图像。为防语义漂移，引入自注意力约束与伪掩码策略，限制优化对关键语义区域的影响；迭代去噪作为“净化器”，在恢复高质量外观的同时保留可解码水印并增强对多种破坏的鲁棒性。

Result: 在COCO数据集上，面对12种图像变换（如压缩、裁剪、噪声、模糊、颜色扰动等）整体表现优于Stable Signature，平均提升约10%，显示更强的可靠解码率与视觉质量。

Conclusion: 扩散模型的反演-去噪链路可同时实现水印高保真与高鲁棒；配合自注意力约束与伪掩码，能在不破坏原语义的前提下嵌入稳健水印。实验表明方法在多类畸变下显著优于现有强基线，并具有实际应用潜力。

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [53] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: 提出一种用于事件相机动作识别（EAR）的新型多视角时空表示与融合框架，在三数据集上显著提升Top-1精度（+7.0%/+10.7%/+10.2%），同时减少参数30.1%与计算35.7%。


<details>
  <summary>Details</summary>
Motivation: 现有针对事件数据的时空多视角表示（SMVRL）多从H、W轴投影获取视图，但存在空间分箱表示对平移敏感、以及简单早期拼接融合导致互补性利用不足的问题；动作识别对时间与运动动态尤为敏感，需更鲁棒与高效的表示与融合策略。

Method: (i) 采用稀疏事件到致密表示的平移不变转换，构建更稳健的多视角时空表征；(ii) 设计双分支、动态融合架构，按样本自适应建模不同视图（运动特征）的互补性；(iii) 提出受生物启发的时间扭曲增强，模拟真实动作速度变化以提升泛化。

Result: 在HARDVS、DailyDVS-200、THU-EACT-50-CHL三套EAR数据集上，相比现有SMVRL的EOR方法，Top-1准确率分别提升+7.0%、+10.7%、+10.2%，同时参数量减少30.1%，计算量降低35.7%。

Conclusion: 所提框架在精度与效率上均优于现有SMVRL基线，证明平移不变致密转换、双分支动态融合与时间扭曲增强对事件相机动作识别有效，提供了新的强大范式。

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [54] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: ReLE 提出一种可扩展、低成本、实时的评估系统，用于诊断大语言模型在不同领域与能力上的不均衡表现（能力各向异性），并通过新评分机制与方差感知调度器，在显著降算力成本的同时保持高排名一致性，揭示排行榜对加权方式高度敏感、模型更偏专长化而非全面领先。


<details>
  <summary>Details</summary>
Motivation: 现有中文LLM评测面临两大痛点：1）静态基准趋于饱和、难以细粒度反映模型结构性差异；2）全面评测计算成本高昂。传统排行榜只给出“快照式”总分，掩盖了不同领域×能力维度的权衡，难以及时监控模型生态的动态变化。

Method: - 构建Domain×Capability的正交评测矩阵（覆盖207,843样本），对304个模型进行大规模评测。
- 提出两项关键方法：
  1) 符号-语义混合评分：以符号对齐为主，抑制嵌入相似度在推理题上的“假阳性”。
  2) 动态方差感知调度：基于Neyman分配并进行噪声修正，按方差与难度分配采样预算，在保证排名相关性下显著降本。

Result: - 计算成本较全量评测降低约70%，整体排名相关性保持ρ=0.96。
- 发现排行榜对加权方案极为敏感；在ReLE中模型的排名稳定振幅（RSA）达11.4，高于传统基准（约5.0），显示模型更具“专科化”，而非单一维度“通用更强”。

Conclusion: ReLE不是静态基准的替代，而是面向高速演化模型生态的高频诊断监视器；其方法能更稳健地衡量跨域×能力的各向异性，同时以更低成本维持高可信度排名，为模型迭代与部署提供精细化洞察。

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [55] [HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection](https://arxiv.org/abs/2601.17405)
*Chunze Yang,Wenjie Zhao,Yue Tang,Junbo Lu,Jiusong Ge,Qidong Liu,Zeyu Gao,Chen Li*

Main category: cs.CV

TL;DR: 提出HAAF框架，通过跨层缩放对齐（CLSA）先用视觉上下文自适应生成文本描述，再反过来引导视觉编码聚焦ROI异常，并配合语义分数与几何原型的双分支推理，在多病理基准和低资源下显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 精密病理诊断依赖ROI内细粒度纹理异常，而通用V-L模型存在“粒度不匹配”，缺乏对ROI的语义落地。现有适配常把视觉与文本分离，难以让提示词与具体病理图像局部对齐，导致对微小缺陷识别不佳，尤其在小样本情形不稳定。

Method: 提出分层适配与对齐框架HAAF：核心为Cross-Level Scaled Alignment（CLSA）。流程为：1）先用视觉特征给文本提示注入上下文，生成内容自适应的描述；2）再用这些描述空间化地引导视觉编码器，突出异常区域（聚焦ROI）。此外提出双分支推理：一支输出语义分数，另一支基于几何原型（形状/空间原型）以增强小样本稳定性；框架可与领域骨干（如CONCH）结合并可扩展。

Result: 在四个病理基准上显著优于现有SOTA；在低资源/小样本条件下表现稳健，且与领域特定骨干（如CONCH）结合时可进一步提升与扩展。

Conclusion: 通过在视觉与文本间建立顺序化、跨层的对齐与反馈，HAAF有效弥合V-L模型的粒度不匹配问题，能在ROI级别捕捉细粒度异常，并在小样本与低资源情景保持稳定与高性能。

Abstract: Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

</details>


### [56] [Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity](https://arxiv.org/abs/2601.17408)
*Harsharaj Pathak,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 论文提出一种用于源数据不可用的领域自适应（SFDA）的新方法：通过“邻域签名”学习更稳健的目标域簇结构，并用一个单一损失在目标域上优化样本预测的相似/不相似关系，减少误导邻居带来的错误；在VisDA上优于现有方法，其它数据集也具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA多依赖邻域一致性来做伪标签与特征对齐，但目标域中邻域信息常含噪（误导邻居），导致错误传播与聚类不稳。需要一种既能学习更具判别性的簇，又能抑制噪声邻居影响的策略，同时尽量简化训练目标。

Method: 引入“邻域签名”描述每个样本在目标域中的局部结构信息，并据此构建更信息化的簇。以此为基础，设计单一损失函数，在目标域上同时促进同类样本预测相似、异类样本预测相斥，从而实现无源数据的自适应。核心在于用邻域签名过滤或减弱噪声邻居对一致性学习的负面影响。

Result: 在VisDA这一具有较大域差异的基准上，方法取得优于现有工作的性能；在其他常用基准上也达到了接近或具有竞争力的结果。

Conclusion: 通过邻域签名增强目标域聚类质量，并用单一相似-相斥损失即可实现有效SFDA，能缓解邻域噪声导致的误适应问题，在多数据集上验证了有效性与鲁棒性。

Abstract: Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.

</details>


### [57] [Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase](https://arxiv.org/abs/2601.17414)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 提出一套基于 Firebase Realtime Database 的云端物联网监测与控制系统：ESP32 采集 DHT22 温湿度与 HC-SR04 距离数据并上云，同步远程控制两路LED；实验显示99.2%数据成功率、<1.5s 控制时延、支持历史持久化，成本约$32.5，适用于从家居到工业的可扩展场景。


<details>
  <summary>Details</summary>
Motivation: 传统监测系统在实时数据可达性、远程可控性与云端集成上存在不足，搭建自有服务器复杂且成本高。作者希望用低门槛、低成本但可靠的云平台实现多端同步的环境监测与设备控制。

Method: 硬件：ESP32 + DHT22 + HC-SR04 + 两个LED。软件/云：Firebase Realtime Database 实时同步；ESP32 将传感器数据上报至云端，并监听云端控制指令以驱动LED；提供多端访问界面实现数据可视化与远程控制。评测：数据上报成功率、控制时延、数据持久化能力与总体成本。

Result: 实现多设备同步访问的实时监测与控制；数据传输成功率达99.2%，远程控制端到端时延<1.5秒；云端持久化支持历史分析；总体实现成本约$32.50。

Conclusion: 基于 Firebase 的云端 IoT 架构在无需自建复杂服务器的前提下，提供稳定、低延迟、可扩展且低成本的方案，适合资源有限的开发者与研究者，易推广至智能家居与工业监测等多种应用。

Abstract: The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.

</details>


### [58] [CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction](https://arxiv.org/abs/2601.17420)
*Shiu-hong Kao,Chak Ho Huang,Huaiqian Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: 提出CoT-Seg：一个免训练、将链式思考与自我纠错结合的推理分割框架，利用GPT-4o分解查询、抽取细粒度语义并迭代修正掩码，并可接入检索以补全外部知识；并发布困难数据集ReasonSeg-Hard，展示显著鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割在复杂查询与跨域图像上表现不稳，难以处理隐式目标或含糊指令。借鉴“难题需更长思考”的链式思考范式，作者期望构建能分步推理、必要时查找外部信息、自评并修正结果的系统，提高可靠性与泛化。

Method: 提出训练免调的CoT-Seg：1) 使用预训练多模态大模型（如GPT-4o）将用户查询分解为元指令，抽取图像细粒度语义并定位目标；2) 生成初始分割；3) 自我评估阶段依据原始查询与推理轨迹比对掩码，发现不一致并迭代优化；4) 可选检索增强，在信息不足时查询外部知识以辅助推理。

Result: 在新建的高难度数据集ReasonSeg-Hard及其他具有歧义与复杂指令场景中，CoT-Seg展现更高的可靠性与鲁棒性，尤其在隐式、跨域与易出错案例中显著优于传统方法。

Conclusion: 将链式思考与自我纠错紧密结合的免训练框架为视觉-语言驱动的分割提供了有效新范式，能在复杂与不确定情境中稳定提升表现，并具备通过检索增强的可扩展性。

Abstract: Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.

</details>


### [59] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: 提出并系统评估在X射线冠状动脉造影中进行鲁棒分割与血管类型标注的管线：经典滤波+按图像SVR调参，以及高分辨率FPN深度模型并采用“冠脉+导管”合并标注训练；在外部数据集上仍有域移，但通过轻量微调可恢复性能，并实现高准确的LAD/LCX/RCA标注。


<details>
  <summary>Details</summary>
Motivation: XCA是评估冠心病的金标准，但常规数据分割受低对比、运动、缩短、重叠及导管干扰等影响，造成跨中心域移，限制了定量分析和基于血管定位的下游测量。需要更稳健的分割与血管类型标注方法以支持临床量化。

Method: 1) 从670段电影序列选取峰值显影附近的最佳帧（低强度直方图准则），并进行联合超分辨与增强；2) 基准对比经典血管度滤波器（Meijering/Frangi/Sato），在三种设置下：逐图像“oracle”调参、全局均值参数、以及用SVR按图像预测参数；3) 深度学习基线：U-Net、FPN、Swin Transformer，分别以仅冠脉标签和“冠脉+导管”合并标签训练；4) 二阶段为主干血管分配类别（LAD/LCX/RCA）；5) 外部评估用公共DCA1，并测试轻量域内微调。

Result: - 经典方法：SVR按图像调参优于全局均值（如Frangi Dice 0.759 vs 0.741）。- 深度模型：FPN在院内Dice 0.914±0.007（仅冠脉），合并标签提升至0.931±0.006。- 外部DCA1：性能下降至0.798（仅冠脉）/0.814（合并），经轻量微调恢复至0.881±0.014/0.882±0.015。- 血管类型标注准确率：RCA 98.5%（Dice 0.844），LAD 95.4%（0.786），LCX 96.2%（0.794）。

Conclusion: 按图像学习化调参可增强经典分割管线；高分辨率FPN与“冠脉+导管”合并监督能提升稳健性与跨域迁移，在外部数据集上仍存域移但可通过少量微调显著恢复；管线还能高精度完成LAD/LCX/RCA标注，支持血管特异的后续分析。

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [60] [ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation](https://arxiv.org/abs/2601.17468)
*Chia-Ming Lee,Yu-Fan Lin,Jing-Hui Jung,Yu-Jou Hsiao,Chih-Chung Hsu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出ReflexSplit用于单幅图像反射分离，利用跨尺度门控融合、层级融合-分离块与课程式训练，缓解非线性混合下的透射-反射混淆，取得SOTA性能与更强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有SIRS方法在非线性混合与深层解码阶段易出现透射/反射混淆，原因在于隐式融合机制、跨尺度协调不足与梯度/特征不稳定，导致结构与纹理难以一致分离。

Method: 提出双流ReflexSplit框架：1) Cross-scale Gated Fusion（CrGF）在多层级间自适应聚合语义先验、纹理细节与解码上下文，稳定梯度并保持一致性；2) Layer Fusion-Separation Blocks（LFSB）交替执行共享结构的融合与特定层的差分分离，受Differential Transformer启发，通过跨流减法实现注意力抵消；3) 课程式训练，随深度初始化与按epoch暖启动，逐步强化差分分离能力。

Result: 在合成与真实数据集上实现SOTA，呈现更佳感知质量与稳健泛化；相较现有方法，在非线性混合与复杂场景下有效降低透射-反射混淆。

Conclusion: ReflexSplit通过跨尺度门控融合、差分式双流分离与课程训练，显著提升单幅反射分离的稳定性与准确性，提供更强的泛化与感知质量，并可作为后续SIRS研究的可扩展基线。

Abstract: Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.

</details>


### [61] [PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors](https://arxiv.org/abs/2601.17470)
*Chia-Ming Lee,Yu-Fan Lin,Yu-Jou Hsiao,Jing-Hui Jung,Yu-Lun Liu,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: PhaSR 通过物理对齐的归一化与跨模态注意力，对单光源到多源环境光下的阴影移除进行稳健处理：先用物理先验矫正光照与色偏，再以几何-语义对齐解决模态冲突，在更低复杂度下取得有竞争力的效果，并能泛化到多光源场景。


<details>
  <summary>Details</summary>
Motivation: 现有阴影移除方法在多源或复杂光照下常因物理先验失配与模态冲突（颜色偏置、几何与语义不一致）而失效；需要一种既能显式消除光照/反射率耦合又能统一几何与语义表征的方案。

Method: 提出 PhaSR，包含两大模块：1) PAN（Physically Aligned Normalization）：基于灰度世界归一化、对数域 Retinex 分解与动态范围重组的闭式光照校正，抑制色度偏差并分离照明与反射率；2) GSRA（Geometric-Semantic Rectification Attention）：将差分注意力扩展为跨模态对齐机制，融合深度估计得到的几何与 DINO-v2 语义嵌入，缓解不同照明下几何-语义冲突。整体以较低复杂度实现对不同光照的鲁棒影子移除。

Result: 在标准阴影移除基准上取得与 SOTA 可比的指标，同时显著降低复杂度；在传统方法失效的多源环境光场景中表现出良好泛化能力。

Conclusion: 双层先验对齐（物理归一+几何语义对齐）可有效解耦照明与反射率并缓解模态冲突，使阴影移除在从单光源到多源环境光的广泛条件下保持稳健，且以较低计算代价实现有竞争力的性能。

Abstract: Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.

</details>


### [62] [BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation](https://arxiv.org/abs/2601.17504)
*Yan Zhou,Zhen Huang,Yingqiu Li,Yue Ouyang,Suncheng Xiang,Zehua Wang*

Main category: cs.CV

TL;DR: 提出BMDS-Net：在多模态MRI脑肿瘤分割中强调临床稳健性与可信度，兼顾缺失模态鲁棒性与不确定性校准，保持精度同时显著提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有如Swin UNETR等Transformer分割模型在基准表现优但对缺失模态敏感、置信度校准不足，难以满足临床对安全与可靠的要求；仅追求Dice在理想数据上的提升无法应对真实场景的模态缺失与误差提示需求。

Method: 1) 构建鲁棒确定性主干：引入零初始化多模态上下文融合模块（MMCF）与残差门控的深层解码器监督（DDS），稳定特征学习、强化边界，降低Hausdorff距离，即便模态受损仍稳健。2) 记忆高效的贝叶斯微调：将网络转化为概率预测器，输出体素级不确定性图以提示潜在错误。3) 在BraTS 2021上进行系统实验与缺失模态评测。

Result: 在BraTS 2021上，BMDS-Net在常规设置下保持与SOTA相当的精度，同时在缺失模态/模态损坏场景显著更稳健，显著降低Hausdorff距离；基线模型在此情形下失败而BMDS-Net稳定工作；提供校准良好的体素级不确定性。

Conclusion: BMDS-Net兼顾精度、鲁棒性与可信不确定性，优于仅追求指标最大化的方案，更适合临床部署；代码已开源，便于复现与扩展。

Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.

</details>


### [63] [FMIR, a foundation model-based Image Registration Framework for Robust Image Registration](https://arxiv.org/abs/2601.17529)
*Fengting Zhang,Yue He,Qinghao Liu,Yaonan Wang,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: 提出FMIR：结合基础模型特征编码器与通用配准头，并以通道正则化在单一数据集训练，达成SOTA域内性能且对域外保持稳健。


<details>
  <summary>Details</summary>
Motivation: 深度学习配准虽快，但难以跨域泛化，尤其在小规模医疗数据场景中。作者希望用基础模型能力提升解剖结构表征，从而在有限数据下仍具泛化性。

Method: FMIR框架=基础模型驱动的特征编码器（聚焦解剖结构）+通用配准头；配合“通道正则化”训练策略，仅用一个数据集训练。编码器提供稳健的结构表示，配准头执行形变估计。

Result: 在域内数据上达到SOTA；在域外数据上仍保持稳健配准性能，优于现有方法。代码开源。

Conclusion: 以有限资源构建具备泛化性的医学影像配准基础模型是可行的；FMIR在域内与域外均表现优异，展示了基础模型+正则化策略的有效性。

Abstract: Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.

</details>


### [64] [Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries](https://arxiv.org/abs/2601.17535)
*Kevin Robbins,Xiaotong Liu,Yu Wu,Le Sun,Grady McPeak,Abby Stylianou,Robert Pless*

Main category: cs.CV

TL;DR: 他们利用文本比较与合成图像生成相结合，预测零样本视觉-语言模型（如CLIP）在特定任务/领域上的有效性，并向用户展示用于评估的代表性图像。


<details>
  <summary>Details</summary>
Motivation: CLIP等VLM能零样本构建分类器，但跨领域性能不稳定。非专家缺乏简单手段在无标注的情况下判断某VLM是否适合其任务。此前仅用文本相似度来预估模型表现，仍不足以可靠反映视觉层面的难点。

Method: 在文本仅比较的基线之上，针对目标自然语言任务生成与类描述相关的合成图像（例如通过文本到图像生成器），用这些图像与VLM进行零样本评估，得到更贴近视觉分布的分数；将图像驱动的分数与文本基线融合，形成对零样本准确率的预测，并把生成样例作为可解释反馈提供给用户。

Result: 在标准CLIP基准上，加入生成图像的评估显著提升了对真实零样本准确率的预测质量（相较文本-only基线）；同时生成的图像为用户提供了直观的领域适配反馈。

Conclusion: 通过结合文本信号与合成视觉证据，可在无标注数据下更可靠地预估VLM在新任务上的可用性，并帮助用户更好地做模型选择与任务适配。

Abstract: Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.

</details>


### [65] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: 提出一种无需模型、具可视化解释性的图像“可攻击性”度量OTI（Object Texture Intensity），用目标物体纹理强度衡量图像被对抗扰动欺骗的难易；理论与实验均显示OTI有效、廉价且具可视化直观性。


<details>
  <summary>Details</summary>
Motivation: 现有可攻击性度量依赖模型代理（梯度/最小扰动等）且缺乏可视化可解释性；现实中任务模型常不可得，且特征与图像关系不直观，因此需要一种模型无关且可解释的度量。

Method: 提出OTI：基于图像中语义对象的纹理强度来衡量可攻击性。通过对象分割/检测获取语义对象区域，计算其中的纹理强度（与中高频能量相关）作为分数。并从决策边界视角与对抗扰动中高频特性给出理论阐释。计算简单、无需访问目标模型的梯度或决策信息。

Result: 在多数据集与攻击设置上，OTI能有效区分“易攻”与“难攻”图像，预测攻击成功率/所需扰动量，与真实可攻击性高度相关，同时计算效率高于依赖模型的基线。

Conclusion: OTI提供了一个模型无关、直观可视化且高效的可攻击性度量，有助于主动学习、对抗训练与攻击策略优化，并为理解图像可攻击性提供了频域与决策边界层面的解释。

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [66] [Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper](https://arxiv.org/abs/2601.17555)
*Justin Downes,Sam Saltwick,Anthony Chen*

Main category: cs.CV

TL;DR: 利用显著性图指导的预处理，把不重要区域平滑、重要区域保真，再配合传统有损编码，实现单幅大尺度卫星图像的可变码率压缩，从而降低存储与带宽成本并兼顾下游任务需求。


<details>
  <summary>Details</summary>
Motivation: 卫星每天产出海量高分辨率影像，但多数下游任务只需关注小范围区域。现有编码方法通常全局同等对待像素，浪费比特；即便有遥感优化编码，仍缺乏对任务相关“兴趣区”的差异化分配。需要一种能根据显著性/关注度在同一图中不均匀分配码率的方案，以降低成本同时保持任务关键区域质量。

Method: 以显著性图为引导，将连续显著性分级量化为若干等级；为不同等级分配不同大小的平滑核，对低显著性区域施加强平滑以便后续更易压缩，对高显著性区域弱或不平滑以保持细节；随后使用传统有损压缩标准（未改动编码器本身）进行编码，从而在单幅大图内实现变码率。

Result: 该预处理使编码器在不重要区域更高压缩、重要区域更高保真，整体提高压缩效率、降低码率，同时为任务关键区域保留足够质量（尽管摘要未给出具体数值指标）。

Conclusion: 显著性驱动的可变核平滑预处理可与现有有损标准无缝配合，在不修改编码器的情况下实现单图内变码率压缩，既节省存储/带宽又更好服务下游任务；未来可在显著性估计、核设计与自适应量化映射上进一步优化。

Abstract: The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.

</details>


### [67] [Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning](https://arxiv.org/abs/2601.17566)
*Qi Li,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出“Sponge Tool Attack (STA)”，一种仅通过重写用户提示词、在不改动模型与工具的前提下让工具增强型LLM走冗长绕路推理，从而显著增加计算开销且保持任务语义与答案一致的隐蔽攻击。


<details>
  <summary>Details</summary>
Motivation: 工具增强型LLM的“代理式推理”正成为提升复杂推理能力的主流，但其调用工具的流程可能是新的攻击面。现有研究多关注越权、注入等安全性问题，较少探讨如何仅凭查询侧输入改写就能系统性拖慢或稀释推理效率，同时不破坏语义与最终答案。作者希望揭示并形式化这种“耗散型”对抗面。

Method: 界定“工具特定攻击面”，提出STA：在严格的query-only场景下，对原始提示进行多轮、协同式（多代理）改写，显式控制重写策略以保持高语义保真但诱导代理选择更冗长的工具序列与推理路径。框架迭代生成“看似良性”的提示重写，不修改底层模型或外部工具，也不改变用户意图，只改变推理轨迹长度与复杂度。

Result: 在6个模型（含开源与闭源API）、12个工具、4个代理框架、覆盖5个领域的13个数据集上进行评测，STA能把原本简洁高效的推理转为冗长复杂，显著增加计算与成本，同时仍能到达原答案，且保持较强的隐蔽性与语义一致性。

Conclusion: 工具增强型LLM存在可被提示重写利用的“耗散式”脆弱性。即便不改动模型与工具，也能通过良性外观的提示操纵显著拖慢代理推理。应对需要在框架与策略层面引入鲁棒性与调用开销防护机制，以抵御此类Stealthy效率攻击。

Abstract: Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.

</details>


### [68] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: 提出一种用于医学图像域泛化的Stylizing ViT：在同一注意力模块中共享权重，既做自注意力保持解剖结构，又做交叉注意力实现风格转移；用于数据增广与测试时增广，在皮肤科与病理三任务上显著提升鲁棒性与准确率（最高+13%），并在推理阶段带来+17%提升，生成图像无明显伪影。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据异质性强且稀缺，模型跨域与跨人群泛化差。传统几何/颜色增广在大域移位下失效；现有风格化增广要么风格多样性不足，要么引入伪影破坏解剖结构。需要一种同时保证解剖一致性与风格多样性的增广/表征方法。

Method: 设计Stylizing ViT编码器：将同一注意力块用于自注意力与交叉注意力并共享权重。自注意力通道负责保持解剖一致性；交叉注意力通道从风格源图提取风格特征并注入目标图，实现风格迁移。将该模型作为数据增广器用于训练，并作为测试时增广在推理阶段使用。评估于三项皮肤科与病理图像分类任务上。

Result: 与现有方法相比，在域泛化场景中准确率最高提升至+13%，生成图像主观上无明显伪影；在测试时增广设置下，推理性能可再提升约17%。代码已开源。

Conclusion: 共享权重的自/交叉注意力使得在不牺牲解剖结构的前提下实现高多样性的风格化增广，从而显著提升医学图像分类的域泛化与鲁棒性；该策略在训练与推理阶段均有效，具有实践价值。

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [69] [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657)
*Taewan Cho,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.CV

TL;DR: SPACE-CLIP提出一种双通路解码器，从冻结的CLIP视觉编码器直接解锁几何信息，绕过文本编码与提示，通过语义通路（FiLM调制）与结构通路（早期层细粒度空间特征）分层融合，在KITTI等上显著优于现有CLIP方法，证明双通路协同关键，作为通用空间感知模块可集成到VLA等具身AI系统。


<details>
  <summary>Details</summary>
Motivation: CLIP在语义理解上强，但对几何结构感知薄弱；现有通过文本提示间接引导，效率低且不稳定。需要一种无需文本提示、能直接从CLIP视觉表征中提取空间/几何信息的方法。

Method: 冻结CLIP视觉编码器；设计双路径解码器：1) 语义路径对高层特征进行FiLM调制以结合全局上下文；2) 结构路径从早期卷积/注意力层提取细粒度空间细节；通过分层融合（层级跨尺度融合）综合两者，输出深度/几何预测；不使用CLIP文本编码器或提示。进行消融实验验证各组件与融合策略贡献。

Result: 在KITTI基准上，相比所有基于CLIP的先前方法取得大幅性能提升；消融显示语义与结构双通路的协同融合对性能至关重要。

Conclusion: SPACE-CLIP提供了一种高效、优雅的方式重用大规模视觉模型以进行空间感知：无需文本提示即可从CLIP中挖掘几何知识；该方法不仅是强健的单目深度估计器，也可作为可插拔的空间感知模块集成到VLA等具身AI系统。

Abstract: Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip

</details>


### [70] [Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting](https://arxiv.org/abs/2601.17666)
*Xinyue Pan,Yuhao Chen,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出Prompt Grafting（PG），在扩散模型采样中分两阶段利用布局提示与目标提示，缓解多食物图像中的物体纠缠，显著提升目标食物出现率并可控制分离或混合。


<details>
  <summary>Details</summary>
Motivation: 实际餐食图像常含多种食物，需可靠的可组合生成用于饮食评估的数据增强与食谱可视化。现有文生图模型在多食物场景常出现相邻食物（如米饭与汤）融合的“物体纠缠”，因许多食物边界不清，导致生成不准确。

Method: 提出无需训练的Prompt Grafting框架：在扩散采样中先用“布局提示”建立清晰的空间区域与隐式布局引导，待布局稳定后再“嫁接”目标文本提示。用户可通过编辑布局指定哪些食物需保持分离或可被有意混合。

Result: 在两个食物数据集上，PG显著提升目标对象的出现率，并提供可控分离的定性证据。

Conclusion: PG以训练自由、两阶段提示控制方式有效缓解多食物图像生成中的纠缠问题，提升可控性与准确性，适用于多食物数据增强与可视化等应用。

Abstract: Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.

</details>


### [71] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: 提出Uni-RS统一遥感多模态模型，针对“空间反转诅咒”，通过显式空间布局规划、空间感知查询监督及图文布局变换，显著提升文本生成图像的空间忠实度，同时保持理解任务性能。


<details>
  <summary>Details</summary>
Motivation: 统一多模态遥感模型在理解时能正确识别/描述空间关系，但在文本到图像生成时却难以忠实执行同样的空间约束（空间反转诅咒），而空间关系在遥感语义中至关重要，亟需方法弥合理解与生成在空间建模上的不对称。

Method: 1) 空间布局规划：将文本指令显式解析为空间布局计划，将几何规划与视觉合成解耦；2) 空间感知查询监督：对可学习查询施加与指令中空间关系对应的监督，引导模型注意空间约束；3) 图像-字幕空间布局变换：对训练样本进行几何一致的空间变换，系统性暴露多样但一致的空间配置，提高泛化与鲁棒性。

Result: 在多项基准上，Uni-RS在文本到图像生成的空间忠实性显著优于现有方法；同时在图像描述、视觉定位和VQA等遥感理解任务上维持强劲或竞争力表现。

Conclusion: 通过显式化空间规划与监督以及布局变换训练，Uni-RS有效缓解理解-生成的空间不对称，成为首个面向遥感的统一多模态模型，在不牺牲理解能力的前提下大幅提升生成阶段的空间一致性。

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [72] [StyleDecoupler: Generalizable Artistic Style Disentanglement](https://arxiv.org/abs/2601.17697)
*Zexi Jia,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: StyleDecoupler 通过把单模态表征当作“内容参考”，从多模态表征中剥离风格信息，并在不微调VLM的前提下实现可插拔的风格表示学习；并发布包含28万艺术作品、覆盖152种风格与1556位艺术家的WeART基准，在风格检索等任务上达SOTA，并支持风格关系映射与生成模型评估。


<details>
  <summary>Details</summary>
Motivation: 艺术风格与语义内容高度纠缠，现有模型难以在不泄露内容的情况下提取纯风格特征；而多模态模型往往混合风格与内容，影响风格检索与分析。作者观察到单模态视觉模型为任务鲁棒性而压制风格，因而可作为内容不变参照。

Method: 提出StyleDecoupler：在冻结的视觉-语言模型(多模态)上，利用单模态视觉表征作为“内容仅”参考，通过互信息最小化使多模态嵌入去除与内容相关的成分，从而隔离纯风格特征。该模块可插拔、无需微调VLM。

Result: 构建WeART数据集（280K图像，152风格、1556艺术家）。在WeART与WikiART的风格检索任务上取得SOTA；还展示了风格关系映射与对生成模型风格一致性/多样性的评估能力。

Conclusion: 基于信息论的风格-内容解耦在冻结VLM上可行且有效；单模态表征可作为内容锚点来剥离风格，既提升风格检索表现，也拓展了风格分析与生成评测的应用场景。

Abstract: Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.

</details>


### [73] [An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays](https://arxiv.org/abs/2601.17703)
*Nikhil Kadivar,Guansheng Li,Jianlu Zheng,John M. Higgins,Ming Dao,George Em Karniadakis,Mengjia Xu*

Main category: cs.CV

TL;DR: 提出一个自动化深度学习框架，在高密度、重叠的红细胞显微视频中，实现RBC分割、分类与计数，量化镰状化动态并提升实验通量。


<details>
  <summary>Details</summary>
Motivation: 镰状红细胞病研究需要在多种生物物理条件下准确识别细胞形态转变；高密度且重叠的细胞群体使人工标注与传统分析困难且低效，限制对药物作用与力学生物学过程的定量研究。

Method: 构建端到端流水线：利用Roboflow进行AI辅助标注，训练nnU-Net用于语义/实例分割；结合分水岭算法分离重叠细胞；对时间序列显微图像进行细胞分类与实例计数，得到镰状细胞比例的时序演化；以少量标注数据训练以获得稳健泛化。

Result: 在标注数据有限的情况下仍取得高分割性能；能够准确预测随时间的镰状细胞比例，显著改善重叠细胞的量化精度；在高密度悬液中可将实验通量提高一倍以上，并能捕捉药物依赖的镰状化行为与不同的力学生物学形态演化特征。

Conclusion: 该AI框架提供了可扩展、可重复的计算平台，可在微生理系统中定量研究细胞生物力学并评估疗效，缓解了稀缺标注与细胞重叠带来的瓶颈。

Abstract: Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

</details>


### [74] [Advancing Structured Priors for Sparse-Voxel Surface Reconstruction](https://arxiv.org/abs/2601.17720)
*Ting-Hsun Chi,Chu-Rong Chen,Chi-Tun Hsu,Hsuan-Ting Lin,Sheng-Yu Huang,Cheng Sun,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 该论文将3D高斯泼洒与稀疏体素光栅化结合：用结构感知的体素初始化与改进的深度监督，实现更快收敛与更高几何精度、细节与完整性。


<details>
  <summary>Details</summary>
Motivation: 现有两类显式辐射场表示各有短板：3D高斯泼洒收敛快且具几何先验，但点状参数化限制表面保真；稀疏体素光栅化几何清晰且不透明度连续，但通常采用密集均匀网格初始化，收敛慢且未充分利用场景结构。需要兼具两者优势的方法。

Method: 1) 提出结构感知的体素初始化：在更可能存在表面的空间位置放置体素，并为不同区域分配合适细节层级，作为每个场景优化的强力起点；2) 提出精炼的深度几何监督：将多视角线索转化为逐射线的深度正则，增强深度一致性而不模糊边缘。

Result: 在标准基准上，相比以往方法获得更高的几何精度、更好的细节恢复和更完整的表面，同时保持快速收敛。

Conclusion: 通过结构化体素初始化与边缘友好的深度监督，融合了3D高斯泼洒与稀疏体素光栅化的优点，实现快速且高保真的表面重建。

Abstract: Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.

</details>


### [75] [Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study](https://arxiv.org/abs/2601.17723)
*Tayyab Nasir,Daochang Liu,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文对基于隐式神经表示（INR）的任意比例图像超分（ASSR）进行系统实证评测，统一框架复现实验，并提出一种改进损失以提升纹理保真。核心发现：复杂INR仅带来微小增益；训练配置对性能影响巨大；新损失可跨架构提升感知质量；ASSR遵循可预测的缩放律。


<details>
  <summary>Details</summary>
Motivation: 尽管INR已成为ASSR主流，但缺乏系统的、可复现的实证对比与训练配方影响分析，导致增益来源、性能上限与未来方向不清。作者旨在填补这一评测和方法学空白。

Method: - 构建统一评测框架与代码库，涵盖多种INR-ASSR方法与广泛的训练/测试设置。
- 聚合多个图像质量指标进行对比分析。
- 严格控制变量，系统考察训练配置（数据规模/多样性、优化策略、目标函数、模型大小等）。
- 设计并评估一种新损失：惩罚强度波动但保留边缘与纹理细节，以优化感知质量。

Result: - 跨数据与指标的综合结果显示：近期更复杂的INR方法对比早期方法仅有边际提升。
- 模型性能与训练配置高度相关，先前文献普遍低估该因素。
- 提出的损失在多种架构上提升纹理保真与细节保持。
- 观察到明确的缩放律：增大模型复杂度与数据多样性带来可预测的性能增长。

Conclusion: INR-ASSR领域的实际进展主要受训练配方与目标设计驱动而非架构复杂度；应重视可复现实证、合理的训练配置与面向感知质量的损失设计，同时依据缩放律进行资源配置与模型/数据扩展。

Abstract: Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.

</details>


### [76] [Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles](https://arxiv.org/abs/2601.17733)
*Junran Lu,Yuanqi Li,Hengji Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 提出将B-Rep重构为由可组合k-cell粒子组成的集合，并用多模态flow matching联合生成拓扑与几何，实现更强的上下文建模、纠错与可编辑性，在无条件与条件重建任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统B-Rep生成困难在于层级异质性：拓扑与几何跨不同阶元（点/边/面）强耦合，现有级联式序列方法难以利用相邻与共享等几何关系，导致上下文不足与误差难以恢复。

Method: 将每个拓扑实体编码为由粒子构成的组合体；相邻单元在共享边界处使用相同潜表示以实现几何耦合，打破刚性层级、统一点/边/面。采用多模态flow matching框架对粒子集合进行生成，支持无条件与条件（单视图、点云）任务；显式局部表示支持局部修补与直接合成非流形结构。

Result: 生成的CAD模型在有效性（拓扑合法）、保真度与可编辑性方面优于SOTA；可执行单视图/点云重建、局部in-painting与非流形结构合成等任务。

Conclusion: 基于可共享潜变量的k-Cell粒子集合表示与flow matching联合建模，实现了B-Rep拓扑与几何的全局一致生成，提升质量与可编辑性，并自然拓展到多种下游任务。

Abstract: Boundary Representation (B-Rep) is the widely adopted standard
  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.
  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.

</details>


### [77] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: 提出一个端到端“对话到电影视频”生成框架：用ScripterAgent把粗粒度对话转成可执行电影脚本，再由DirectorAgent跨场景连续生成视频，并用CriticAgent与VSA指标评估脚本-视频一致性，实现更长时程一致与更高脚本忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型虽能生成精彩画面，但难以从高层概念（如对话）生成长篇、叙事连贯且忠于脚本的影片，存在语义落差，需要一个能把创意想法系统化为可执行影视脚本并驱动长时程视频生成的方案。

Method: 1) 构建ScriptBench：含多模态上下文、经专家引导标注的大规模基准；2) 训练ScripterAgent：将粗对话翻译为细粒度、可执行的电影脚本（镜头、场景、动作等）；3) DirectorAgent：基于脚本调度SOTA视频模型，采用跨场景连续生成策略以保持长时程一致性；4) 评测：引入AI驱动的CriticAgent与新的Visual-Script Alignment (VSA) 指标，衡量脚本忠实度与时间一致性。

Result: 在多种现有视频模型上显著提升脚本忠实度与时间一致性；VSA与AI评审均显示该框架优于基线。

Conclusion: 该框架有效弥合对话与成片之间的语义鸿沟，推动长篇叙事视频生成；同时揭示当前SOTA模型在视觉震撼与脚本遵循之间存在关键权衡，为自动化电影制作的未来研究提供方向。

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [78] [Learning Sewing Patterns via Latent Flow Matching of Implicit Fields](https://arxiv.org/abs/2601.17740)
*Cong Cao,Ren Li,Corentin Dumery,Hao Li*

Main category: cs.CV

TL;DR: 提出一种基于隐式表示的缝纫纸样建模与生成方法：用SDF/UDF编码面片与缝线端点，学习潜空间分布并可微网格化，结合缝合关系预测，实现复杂纸样的准确建模、生成与从图像估计，支持补全与改码（refitting）。


<details>
  <summary>Details</summary>
Motivation: 传统自动纸样建模/生成受限于面片几何与缝合关系的高变性与离散性，难以统一表示与优化；现有方法在复杂结构、生成质量与从图像逆推的准确性方面不足，亟需一种可微、连续且能表达复杂拓扑的表示与生成框架。

Method: 以隐式场表征纸样：每个面片用签名距离场（SDF）描述边界，用无符号距离场（UDF）标识缝线端点；将两类场编码到连续潜空间，支持可微网格化。通过潜流匹配（latent flow matching）学习面片组合的分布；从提取的边缘段中用缝合预测模块恢复缝线关系。整体流水线支持建模、生成与从图像估计。

Result: 该方法能精确建模并生成具有复杂结构的纸样；在从图像估计纸样任务上优于现有方法；还能实现纸样补全与尺寸/版型调整（refitting）等应用示例。

Conclusion: 隐式场+潜空间流匹配的统一框架为复杂服装纸样提供了可微、表达力强的建模与生成能力，提升了从图像逆向估计精度，并在补全与改码等设计场景中具备实用价值。

Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.

</details>


### [79] [Frequency-aware Neural Representation for Videos](https://arxiv.org/abs/2601.17741)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: 提出FaNeRV：一种频率感知的隐式视频表示方法，通过显式解耦低/高频并多阶段监督与动态高频注入，显著提升重建细节与码率失真性能，超越现有INR并接近传统编解码器。


<details>
  <summary>Details</summary>
Motivation: 现有基于INR的视频压缩受谱偏置影响，偏好低频，导致高频细节丢失、重建过平滑、率失真不佳，需要一种能有效捕获高频纹理且保持压缩效率的方法。

Method: 1) 频率解耦表示：显式分离低频与高频分量分别建模；2) 多分辨率分阶段监督：从全局到细节逐级训练，引导网络先学结构后学纹理；3) 动态高频注入：自适应强调难区域的高频特征以提升细节重建；4) 频带分解网络模块：在不同频段上改进特征建模与融合。

Result: 在标准基准上显著优于最先进INR方法，并在率失真上与传统编解码器具有竞争力；表现为更清晰纹理、更少过平滑，码率相同下更高质量或同质量下更低码率。

Conclusion: 频率感知的设计能有效缓解INR的谱偏置问题；通过解耦、分阶段监督和自适应高频强化，FaNeRV实现高保真且高效的视频重建，在INR视频压缩方向提供了有效框架并具备实用潜力。

Abstract: Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.

</details>


### [80] [Video Compression with Hierarchical Temporal Neural Representation](https://arxiv.org/abs/2601.17743)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: 提出一种用于视频压缩的分层时域神经表示 TeNeRV，通过显式建模短期与长期时序依赖，相比现有 INR 方法在码率-失真上更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示（INR）的视频压缩将时间作为独立输入，难以捕获复杂时序关系，导致跨帧一致性与运动细节建模不足，需要更好地融合短程与长程时序信息以提升压缩性能。

Method: 构建分层时域表示框架 TeNeRV，包含两大组件：1）Inter-Frame Feature Fusion（IFF）从相邻帧聚合特征，强化局部时域一致性并捕获细粒度运动；2）GoP-Adaptive Modulation（GAM）按组（GoP）划分视频，学习组特定先验，通过调制网络参数以在不同 GoP 上实现自适应表示。

Result: 在广泛实验中，TeNeRV 在率失真（RD）性能上持续优于现有 INR 基线，表明其更有效地利用时序依赖进行压缩。

Conclusion: 通过 IFF 与 GAM 的分层时域建模，TeNeRV 能同时编码短期与长期依赖，实现更好的视频压缩效率与重建质量，验证了层次化时序神经表示的有效性。

Abstract: Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.

</details>


### [81] [Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.17747)
*Kaixuan Jiang,Chen Wu,Zhenghui Zhao,Chengxi Han*

Main category: cs.CV

TL;DR: 提出UniCD统一变更检测框架，单一编码器+多分支协同，兼容监督/弱监督/无监督三种设置，并在多数据集上达SOTA，弱监督与无监督在LEVIR-CD上分别提升约12.72%与12.37%。


<details>
  <summary>Details</summary>
Motivation: 现实中像素级变更标注昂贵且稀缺，现有CD模型难以在不同标注粒度（全监督、弱监督、无监督）间通用与迁移，造成应用受限与性能不稳定。需要一个能深度耦合异构监督信号、在多场景稳定表现的统一框架。

Method: 统一框架UniCD：共享编码器+三条任务专属分支的耦合架构。1) 监督分支：引入时空感知模块STAM，高效协同融合双时相特征；2) 弱监督分支：提出变化表征正则CRR，引导从粗粒度激活收敛到连贯且可分的变化建模；3) 无监督分支：提出语义先验驱动的变化推理SPCI，将无监督转化为可控的弱监督路径优化。三分支通过多分支协同学习实现异构监督的深度耦合与知识互促。

Result: 在主流数据集上三种任务均达最佳。尤其在LEVIR-CD上，弱监督与无监督相较现有SOTA分别提升12.72%与12.37%，表明对标注受限场景有显著优势。

Conclusion: UniCD打通监督、弱监督、无监督变更检测的架构鸿沟，通过共享编码器与任务特化分支实现统一建模与相互促进，在标注不足场景显著提升精度，具备通用性与实用价值。

Abstract: Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.

</details>


### [82] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: 论文提出多视角主体到视频生成（MV-S2V），从多张参考视图合成视频，显著提升3D层面的主体一致性，并在合成与小规模实拍数据及TS-RoPE机制支持下实现高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有S2V大多仅支持单视角参考，实质可退化为“图生图（S2I）+图生视频（I2V）”流水线，无法充分利用视频中对主体的三维一致性控制；训练数据稀缺且条件生成中易混淆跨主体与跨视角信息。

Method: 1) 定义并构建MV-S2V任务；2) 设计合成数据管线以生成高度可控的多视角数据，并辅以小规模真实采集数据；3) 在条件编码中引入Temporally Shifted RoPE（TS-RoPE），通过时间位移的旋转位置编码区分不同主体以及同一主体的不同视角，从而减少跨主体/视角混淆；4) 将多视角参考融入视频扩散生成框架以强化3D一致性。

Result: 在多视角参考条件下，实现更强的3D主体一致性与更高的视觉质量，优于现有单视角S2V方案；实验表明TS-RoPE有效区分主体与视角，配合合成+真实数据显著提升生成稳定性与逼真度。

Conclusion: MV-S2V为主体驱动视频生成提供了新的有效方向：通过多视角参考与TS-RoPE进行明确的主体/视角解耦，并借助定制合成数据与少量真实数据训练，实现高保真、三维一致的视频生成。

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [83] [Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation](https://arxiv.org/abs/2601.17791)
*Rabin Dulal,Wenfeng Jia,Lihong Zheng,Jane Quinn*

Main category: cs.CV

TL;DR: 研究提出基于多视角RGB图像与SAM-3D一致性引导融合的3D重建，再用集成回归估计牛只活重；在低数据条件下取得较高准确度（R^2≈0.69±0.10，MAPE≈2.22%±0.56%），适合农场部署。


<details>
  <summary>Details</summary>
Motivation: 传统称重（走动称/体况评分）需人工干预、影响生产与成本，且在真实农场环境下难以高频、无接触地获取活重。因此需要一种低成本、非接触、可扩展且对数据需求较低的方法。

Method: 构建从多视角RGB图像到单一动物3D点云的管线：利用SAM-3D并通过多视角一致性引导的融合生成高质量点云；在低数据场景下比较经典集成回归模型与深度学习模型的活重预测表现。

Result: SAM-3D的多视角一致性融合优于其他3D生成方法；在低数据条件下，经典集成模型较深度学习更稳定、更实用，达成R^2=0.69±0.10、MAPE=2.22%±0.56%。

Conclusion: 提升3D重建质量比增加模型复杂度更关键；所提非接触、低成本方案在难以大规模采集3D数据的农场中具备可行性与可扩展性。

Abstract: Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.

</details>


### [84] [ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning](https://arxiv.org/abs/2601.17818)
*Wen Luo,Peng Chen,Xiaotao Huang,LiQun Huang*

Main category: cs.CV

TL;DR: 提出ViTCoP，在视觉编码器先过滤冗余，再在LLM内按层级协同剪枝，并用K向量L2范数作为显著性，兼顾精度与加速；在多LVLM上达SOTA，尤其在高剪率下优势更大。


<details>
  <summary>Details</summary>
Motivation: 现有视觉token剪枝要么在视觉编码器过早丢失关键信息，要么在LLM端选择的token信息冗余，且与高效加速算子（如FlashAttention）兼容性不足，导致计算开销大与性能下降。

Method: 提出ViTCoP：1）视觉端冗余过滤，先在视觉编码器阶段保留关键信息；2）LLM端分层逐步协同剪枝（Visual-Textual semantic collaborative pruning），结合层级特性保留信息多样性；3）在LLM注意力中以K向量L2范数作为token显著性度量，便于与FlashAttention等加速技术兼容。

Result: 在多种LVLM上进行大量图像与视频理解实验，较现有方法取得SOTA，显著降低推理时延与显存占用；在极端高剪枝率下相对优势更明显。

Conclusion: ViTCoP通过视觉与文本语义协同、分层剪枝与可加速的显著性估计，有效减少视觉token冗余，同时保持甚至提升性能，实现更快且更省显存的LVLM推理，尤其在高剪率场景下更具优势。

Abstract: Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.

</details>


### [85] [VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training](https://arxiv.org/abs/2601.17830)
*Mengmeng Wang,Dengyang Jiang,Liuzhuozheng Li,Yucheng Lin,Guojiang Shen,Xiangjie Kong,Yong Liu,Guang Dai,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出一种轻量级内生引导框架（基于VAE特征对齐）来加速扩散Transformer训练，在几乎不增加计算与无需外部模型的前提下提升收敛速度与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有加速收敛方法（如REPA需外部表示编码器、SRA需双模型）带来显著训练开销与依赖，限制实用性。需要一种不依赖外部模型、额外成本低且能有效注入视觉先验的训练加速机制。

Method: 利用现成预训练VAE的中间特征作为内生指导信号：通过一个轻量投影层将扩散Transformer的中间潜变量对齐到VAE特征空间，并以特征对齐损失进行监督，从而让扩散模型在训练中更快习得纹理、结构与基本语义先验。无需额外表示编码器或双模型维护。

Result: 在广泛实验中，相比原生扩散Transformer，方法同时提升生成质量与收敛速度；与SOTA加速方法相当或更优，仅增加约4% GFLOPs，并且对外部指导模型零额外成本。

Conclusion: 通过将VAE重建先验内化为训练监督的轻量特征对齐，能在几乎不增加训练负担的情况下显著改善扩散Transformer的效率与效果，为高效扩散训练提供了简单有效的方案。

Abstract: Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.

</details>


### [86] [Geometry-Grounded Gaussian Splatting](https://arxiv.org/abs/2601.17835)
*Baowen Zhang,Chenxing Jiang,Heng Li,Shaojie Shen,Ping Tan*

Main category: cs.CV

TL;DR: 提出将高斯原语严格建模为“随机实体（stochastic solids）”，以几何为核心地改进GS的形状重建与深度渲染，从而在公共数据集上取得最优重建表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gaussian Splatting的形状重建缺乏合理的几何参数化与近似，导致多视一致性差、受浮点漂浮物（floaters）影响大，难以从高斯原语中稳定提取精确几何。

Method: 从理论上推导并证明：高斯原语可视为特定类型的随机实体，使其具备体渲染与显式几何表述的统一框架。基于该体积化表述，几何驱动地渲染高质量深度图，再进行细粒度几何提取；整体流程避免以往启发式近似，直接在显式几何域操作。

Result: 在公共数据集上，相比所有GS相关方法，本方法在形状重建指标上取得最优；深度质量更高、多视一致性更好，对浮点伪影更鲁棒。

Conclusion: 将高斯原语建模为随机实体为GS提供了严谨的几何基础，能高效渲染深度并稳健提取细致几何，显著提升多视一致性与抗浮点伪影能力，推动GS从影像渲染走向准确几何重建。

Abstract: Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

</details>


### [87] [SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction](https://arxiv.org/abs/2601.17857)
*Lan Yang,Minghan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 论文提出SynMind框架，通过将fMRI信号显式解析为分层文本语义并与视觉先验结合，引导扩散模型生成图像，从而缓解以往方法视觉逼真但语义错配的问题，显著提升语义一致性与多项指标，并在更小模型与单张消费级GPU上超越基于SDXL的方案。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI到图像重建虽具照片级逼真度，但常发生语义错配（对象被替换/幻觉），原因在于过度依赖纠缠的视觉嵌入，偏好纹理与全局外观而非明确语义身份。需要一种能显式建模语义、贴近人类分层与组合式视觉理解的解码方式。

Method: 将fMRI解析为句级、多粒度文本描述：利用具备落地能力的视觉-语言模型(VLM)生成涵盖对象身份与空间组织的合成“类人”文本表示；再将这些显式语义编码与视觉先验共同用于条件控制预训练扩散模型（采用Stable Diffusion 1.4），形成SynMind。并包含文本对齐模块，将语义推理从图像生成侧“外包”到文本侧。

Result: 在多数定量指标上优于SOTA；在人评中更符合人类感知；即便使用更小的SD1.4与单张消费级GPU，也超过使用SDXL的竞品；神经可视化显示其激活更广且更具语义相关的脑区，减少对高层视觉区的过度依赖。

Conclusion: 显式语义解析与文本对齐能有效缓解fMRI图像重建中的语义错配，SynMind在效率与效果上均具优势，并更贴近人类视觉语义表征，为神经解码提供可扩展且生物学启发的方向。

Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.

</details>


### [88] [Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment](https://arxiv.org/abs/2601.17862)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出一个轻量级、量子增强的跨域泛化框架，在无真实多中心标注数据的前提下提升医疗影像模型在未知域的鲁棒性。核心做法：模拟多域成像偏移、对抗式域不变训练、加入轻量量子特征增强层，并配合测试时自适应；在模拟多中心数据上提升AUC与敏感度，降低域间性能方差。


<details>
  <summary>Details</summary>
Motivation: 现实部署中，单中心/单设备训练的医疗影像AI在跨中心应用时因域偏移显著掉点，限制临床可用性；同时医疗数据标注稀缺、算力受限，亟需无需多中心标注、计算友好的泛化方法。量子特征映射或有助于在轻量配置下提升非线性表征与纠缠建模，从而增强跨域鲁棒性。

Method: 以MobileNetV2为骨干构建域不变编码器，包含三部分：1) 多域成像偏移模拟：通过亮度、对比度、锐化、噪声扰动生成异质采集条件；2) 域对抗训练：借助梯度反转抑制域可辨特征，促使域不变表示；3) 量子特征增强层：引入参数化量子电路实现非线性映射与纠缠建模，作为轻量附加模块。推理时加入测试时自适应以进一步缓解分布偏移。

Result: 在模拟的多中心医疗影像数据上，相比无域泛化或无量子增强的基线，方法在未知目标域取得更高AUC与敏感度，并显著降低各域性能方差。

Conclusion: 量子增强的轻量域泛化框架在计算受限场景下可提升医疗影像模型的跨中心稳健性，兼具实际部署潜力，展示混合量子-经典系统在医学影像中的可行路径。

Abstract: Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.

</details>


### [89] [MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance](https://arxiv.org/abs/2601.17866)
*Yoonwoo Jeong,Cheng Sun,Yu-Chiang Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: MV-SAM 利用从未配准多视图图像重建的 pointmap，将2D提示与图像特征“抬升”到3D空间，通过跨注意力在3D中交互，实现无需显式3D网络或3D标注的多视图一致分割；在多数据集上优于/可比现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可提示分割（如 SAM）扩展到视频/多视图时缺乏3D感知，跨视角/时间的一致性差，通常需要昂贵的逐场景优化来强制3D一致；需要一种无需显式3D监督与优化、能在多视图下自然保持一致性的方案。

Method: 引入 MV-SAM：利用视觉几何模型从无位姿图像重建 pointmap，建立像素-点的一一对应。将 SAM 编码器产生的2D图像嵌入“抬升”为3D点嵌入，并将用户提示（点击/框/文本）也映射为3D提示嵌入；在一个解码器（Transformer）中以跨注意力在3D中交互，并通过3D位置编码对齐2D交互与3D几何，从而在多视图中隐式学习一致掩码。无需显式3D网络或3D标注，端到端在 SA-1B 上训练。

Result: 在 NVOS、SPIn-NeRF、ScanNet++、uCo3D、DL3DV 等基准上，泛化良好；优于 SAM2-Video，并与需要逐场景优化的基线达到可比性能。代码将开源。

Conclusion: 通过 pointmap 将提示分割抬升到3D，使多视图分割在无需显式3D模块或3D标注的情况下实现跨视角一致；证明了以2D预训练（SAM）+3D几何对齐的范式可在多域数据上取得强竞争力，减少了昂贵的逐场景优化需求。

Abstract: Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.

</details>


### [90] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: VidLaDA提出一种基于扩散语言模型的双向注意力视频LLM，并用MARS-Cache加速推理，在保持推理准确度的同时实现>12倍加速，性能优于扩散基线、可与主流自回归视频LLM竞争。


<details>
  <summary>Details</summary>
Motivation: 自回归视频LLM受因果掩码限制，难以进行全局时空建模，导致理解效率和效果受限；扩散式解码虽具双向潜力，但在海量视频token上推理缓慢。需要一种既能全局双向建模又高效推理的方案。

Method: 1) 采用基于Diffusion Language Model的Video LLM（VidLaDA），使用双向注意力以捕获双向依赖，克服因果掩码偏置。2) 提出MARS-Cache推理框架：异步视觉缓存刷新+逐帧块级注意力，利用锚点token维持全局连通性，同时裁剪冗余，从而在扩散式解码中实现高效缓存与注意力。

Result: 在大量实验中，VidLaDA优于扩散类基线，并与领先的自回归模型（如Qwen2.5-VL、LLaVA-Video）表现相当；MARS-Cache在不牺牲推理/推断准确性的前提下带来超过12倍的加速。

Conclusion: 双向扩散语言模型能有效提升视频时空理解，MARS-Cache缓解扩散解码的效率瓶颈，实现高效且准确的视频理解；方法可作为自回归范式的有力替代并具工程实用性（代码开源）。

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [91] [Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran](https://arxiv.org/abs/2601.17880)
*Muhammad Umar Salman,Mohammad Areeb Qazi,Mohammed Talha Alam*

Main category: cs.CV

TL;DR: Quran MD 是一个多模态《古兰经》数据集，在经文与词级整合阿拉伯文文本、英文翻译、音译与多诵读者音频，并精细对齐到词层面，面向 NLP、语音与数字宗教学等任务。


<details>
  <summary>Details</summary>
Motivation: 现有《古兰经》资源多限于文本或单一朗读者，缺乏多诵读者、词级对齐的跨模态数据，难以支撑ASR、TTS、塔吉威德（tajweed）检测、风格迁移与语义检索等更高阶研究与应用。

Method: 构建 verse 与 word 双层标注：每节（ayah）含阿语原文、英文译文、音译与32名诵读者的语音；每词级包含阿语词形、英文释义、音译与对齐的词级音频。统一清洗与对齐流程，提供可下载数据集（Hugging Face 链接）。

Result: 得到覆盖全书、含32名诵读者的经文级与词级跨模态对齐数据，支持从ASR、TTS到语义检索、风格与口传规则分析等多种任务。

Conclusion: Quran MD 构筑文本-语音-语言学的细粒度桥梁，为古兰经诵读计算研究与应用提供基础设施，推动多模态嵌入、个性化辅学与社区应用的发展。

Abstract: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset

</details>


### [92] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: 提出PEAfowl：面向双臂操作的多视角VLA策略，使用可微3D提升与文本感知读出，实现更稳健的空间推理与细粒度指令对齐；并通过仅训练期的深度蒸馏提升感知，显著超越基线并成功实机迁移。


<details>
  <summary>Details</summary>
Motivation: 多视角视觉-语言-动作模型在杂乱场景、遮挡与视角变化下泛化差，原因在于：1）多视角简单拼接缺乏3D一致的空间理解；2）语言作为全局条件注入导致指令落地粗糙。需要一种能建立几何一致表示并更细粒度对齐语言的策略。

Method: - 空间推理：为每个视觉token预测深度分布，进行可微3D“抬升”，再聚合跨视角局部邻域，形成几何约束的一致表示。
- 指令对齐：用Perceiver风格的文本感知读出替代全局条件，将冻结的CLIP视觉特征与文本迭代交互，实现循证式检索与聚合。
- 感知增强：训练期从预训练深度教师进行深度蒸馏，监督深度分布头，以几何先验改进前端；推理时不增加开销。

Result: 在RoboTwin 2.0的域随机设定下，成功率较最强基线提升23.0个百分点；实机实验显示良好的从仿真到真实迁移，并验证深度蒸馏带来一致改进。

Conclusion: 几何一致的多视图表示与文本感知读出能显著提升双臂操作中的鲁棒空间推理与指令落地；仅训练期的深度蒸馏为感知注入几何先验、无推理开销，带来显著性能与实机可迁移性提升。

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [93] [Masked Depth Modeling for Spatial Perception](https://arxiv.org/abs/2601.17895)
*Bin Tan,Changjiang Sun,Xiage Qin,Hanat Adai,Zelin Fu,Tianxiang Zhou,Han Zhang,Yinghao Xu,Xing Zhu,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: 提出LingBot-Depth：通过“掩蔽深度建模”与自动数据筛选管线，在有噪/缺失的RGB-D输入上完成与校正深度，优于主流RGB-D相机并在多任务上表现出色；同时开源代码、模型与300万RGB-深度对。


<details>
  <summary>Details</summary>
Motivation: 真实世界场景（自动驾驶、机器人）需要可靠的空间视觉与公制深度，但RGB-D相机在镜面、无纹理等条件下常产生缺失与误差。作者将这些传感器误差视为“被掩蔽”的几何模糊信号，试图利用视觉上下文消解歧义、补全深度。

Method: (1) 掩蔽深度建模：将深度图中的不可靠/缺失区域视为mask，通过模型利用RGB上下文与可用深度完成与细化；(2) 设计可扩展的自动数据策划管线，汇集并清洗大规模RGB-深度对（含真实与仿真），以弱标注/自监督方式训练；(3) 学得跨RGB-Depth对齐的潜在表征，用于下游任务。

Result: 在深度精度与像素覆盖率上超越顶级RGB-D相机；在多种下游任务中展示更好的表现与跨模态对齐能力。

Conclusion: 将深度传感器误差视作可利用的掩蔽信号，并结合大规模自动化数据与掩蔽深度建模，可显著提升深度补全质量与跨模态表示；发布代码、模型与300万对RGB-深度数据促进空间感知研究。

Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.

</details>


### [94] [Revisiting 3D Reconstruction Kernels as Low-Pass Filters](https://arxiv.org/abs/2601.17900)
*Shengjun Zhang,Min Chen,Yibo Wei,Mingyu Dong,Yueqi Duan*

Main category: cs.CV

TL;DR: 论文将3D重建视为离散采样引起的频谱周期延拓问题，提出用理想低通对应的Jinc核及其调制变体，兼顾空间域效率与频域保真，显著提升渲染效果。


<details>
  <summary>Details</summary>
Motivation: 现有用于3D重建的核函数（高斯、指数、t分布）本质是低通滤波器，但其非理想低通特性导致离散信号频谱中高低频相互混叠，限制了重建质量与渲染性能。需要一种在截止频率处能够理想抑制频谱重叠、同时在空间域保持可计算性的核。

Method: 从信号处理视角分析离散采样的频谱周期延拓与混叠；引入与理想低通滤波器对应的Jinc核（空间域为Jinc，频域为理想“砖墙”低通），利用其在截止频率处瞬时为零的特性抑制高频泄漏；针对Jinc在空间域衰减慢的问题，设计调制核（对Jinc进行带窗或调制），在空间支持、衰减速度与频域保真之间取得平衡。

Result: 在渲染与重建实验中，采用Jinc核及其调制变体，相较于高斯、指数、t分布等传统核取得更优的重建质量与渲染表现（如更少的高频混叠伪影、更清晰细节）。

Conclusion: 将3D重建建模为频谱隔离问题，并以Jinc核作为理想低通的近似可行实现，结合调制核在空间/频域双重权衡下可显著提升渲染质量，验证了频域导向核设计的有效性。

Abstract: 3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.

</details>


### [95] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出Gen1S：在1-shot FSCIL中，通过将特征映射到“残差空间”并用生成模型学习其多模态结构先验，从而在无需增量训练的设置下提升新类识别，跨多数据集与骨干均优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 在FSCIL的极端1-shot且禁用增量训练/模型改动的设定下，模型对新类泛化极难。作者假设基类与新类的嵌入在结构上相似，若能捕捉这种结构先验，可在样本稀缺时提升新类识别。

Method: 1) 用基类数据训练特征提取与原型。2) 将样本嵌入减去其类别原型，得到“残差”表示，形成残差空间。3) 以VAE或扩散模型在基类上建模残差分布的多模态结构。4) 推理阶段，对新类1-shot样本同样计算残差，并利用已学得的残差先验进行匹配/校正，以改进新类分类，无需对主干再训练或更改结构。

Result: 在多个基准和不同骨干网络上，对新类识别准确率相较现有方法稳定提升，表现为持续SOTA或显著增益；同时保持对基类性能的兼顾（文摘强调跨基准一致改进）。

Conclusion: 基于“结构相似性”的残差空间建模提供了强有效的结构先验，使得在极端少样本、不可再训练的FSCIL场景中仍能提升新类识别。生成式建模（VAE/扩散）对嵌入残差的多模态分布学习是关键。

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [96] [Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models](https://arxiv.org/abs/2601.17918)
*Dain Kim,Jiwoo Lee,Jaehoon Yun,Yong Hoe Koo,Qingyu Chen,Hyunjae Kim,Jaewoo Kang*

Main category: cs.CV

TL;DR: 论文系统评测了DPO在医疗类大视觉语言模型中的效果，发现相较SFT提升不稳定且难以纠正视觉误判，并提出面向视觉误判的偏好构造策略，在VQA上较最强DPO基线提升3.6%，并开放数据与代码。


<details>
  <summary>Details</summary>
Motivation: LVLM在医疗领域前景广阔，但现有模型对齐不足、可靠性欠佳。DPO在通用领域有效，但在高风险医疗场景中的适用性和局限性缺乏系统性实证研究，难以指导方法改进。

Method: 选取两类医疗LVLM（LLaVA‑Med、HuatuoGPT‑Vision），覆盖九种DPO变体，跨多任务进行统一评测；分析DPO对比SFT的收益与失败模式（尤其是视觉误判）。据此设计一种面向视觉误判的定向偏好构造策略，用偏好数据显式惩罚/纠正视觉理解错误，并与现有DPO基线比较。

Result: 现有DPO在不同任务与骨干上的收益不一致，常难以优于SFT，且对基础视觉误判纠错乏力。提出的定向偏好构造作为验证性方案，在视觉问答任务上相对最强DPO基线提升3.6%。

Conclusion: 单纯套用通用DPO难以可靠提升医疗LVLM，需面向医疗特有错误（如视觉误判）定制偏好与训练流程。论文提供完整开源框架与资源，为后续针对性方法与更稳健对齐研究奠基。

Abstract: Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.

</details>


### [97] [RemEdit: Efficient Diffusion Editing with Riemannian Geometry](https://arxiv.org/abs/2601.17927)
*Eashan Adhikarla,Brian D. Davison*

Main category: cs.CV

TL;DR: RemEdit提出在扩散模型图像编辑中兼顾语义保真与推理速度：用流形几何与Mamba建模实现高保真编辑，同时通过任务感知注意力剪枝在不损语义的前提下显著加速，50%剪枝仍达实时并超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有可控图像生成/编辑在“编辑精确度（语义保真）”与“推理时延（速度）”之间两难：高保真通常慢、加速常损语义。需要一种既稳定可控又实时的编辑框架。

Method: 1) 编辑保真：将潜空间视为黎曼流形，用Mamba模块学习流形度量/结构，直接计算测地线以实现平滑且精确的语义路径；配合双重SLERP融合与VLM驱动的目标感知提示词增强，细化控制。2) 加速：提出任务特定的注意力剪枝，由轻量剪枝头学习保留与当前编辑相关的token，相比内容无关剪枝避免语义退化。

Result: 在50%注意力剪枝下仍保持实时；在编辑质量上超过先前SOTA框架；实现更平滑、准确的语义编辑与更快的推理。

Conclusion: 通过几何化的潜空间导航与任务感知剪枝，RemEdit在不牺牲语义保真的情况下实现显著加速，树立了实用高性能图像编辑的新基线；代码已开源。

Abstract: Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.

</details>


### [98] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出SC-SAM：用U-Net生成点提示与伪标签引导SAM适配，同时SAM反向监督U-Net，形成双向协同半监督训练，在前列腺MRI与肠息肉分割上达SOTA，优于其他半监督SAM与MedSAM。


<details>
  <summary>Details</summary>
Motivation: SAM具强泛化但在医疗图像适配受域移、标注稀缺与PEFT难以利用无标注数据限制；传统U-Net在半监督医疗分割强，但其与PEFT-SAM协同潜力被忽视。

Method: 构建“专家-通才”框架：U-Net（专家）在无/少标注数据上生成点级提示与伪标签，作为SAM（通才）的适配信号；SAM作为强通才监督，利用其预测对U-Net进行正则化。两者通过双向co-training循环，共同挖掘无标注数据，提高适配与泛化。

Result: 在前列腺MRI与息肉分割基准上取得SOTA，超过现有半监督SAM变体与医疗基础模型MedSAM。

Conclusion: 专家（U-Net）与通才（SAM）的互补协作能高效利用无标注数据，提升医疗分割的标签效率与性能；SC-SAM验证了协同训练在医疗领域适配基础模型的有效性。

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [99] [DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation](https://arxiv.org/abs/2601.17939)
*Chengkun Sun,Jinqian Pan,Renjie Liang,Zhengkang Fan,Xin Miao,Jiang Bian,Jie Xu*

Main category: cs.CV

TL;DR: 提出一种“可变形反卷积”(DTC)用于UNet类分割中的上采样，学习动态采样坐标以生成高分辨率特征，较固定位置的反卷积/插值更好恢复结构细节，并在多数据集2D/3D任务上带来稳定提升。


<details>
  <summary>Details</summary>
Motivation: 传统上采样（转置卷积、线性插值）在固定位置采样/写入，难以捕捉超出预设网格的结构信息，易产生棋盘格等伪影和细节流失。医学图像对边界与细粒度结构敏感，需要能自适应结构的上采样机制。

Method: 受可变形卷积启发，在解码器上采样阶段引入可学习的偏移，形成可变形转置卷积（DTC）：对每个上采样位置预测连续坐标偏移与权重，在2D/3D中对原特征进行动态位置采样与插值聚合，生成高分辨特征；可无缝替换UNet类结构中的转置卷积或插值模块。

Result: 在3D BTCV15及2D ISIC18、BUSI等数据集上，集成DTC的分割模型较基线稳定提升，表现为更好的解码器特征重建与细节恢复（文摘未给出具体数值）。

Conclusion: 学习式动态坐标的上采样（DTC）优于固定位置方法，能更好捕捉结构并减少伪影，可泛化于2D/3D医学分割并易于集成到现有UNet样式模型中。

Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.

</details>


### [100] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: 提出FlowMorph：物理一致的自监督框架，从短微流控明场视频中无标注学习RBC的标量力学代理k；在多数据集上实现高质量轮廓重建，并用少量RT‑DC标定将k映射为表观杨氏模量，具有跨设备与条件的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: RBC变形性是重要生物标志物，但现有高通量微流控分析流程依赖监督分割/手工特征，且很少嵌入层流斯托克斯流体力学。需要一种无需标签、物理一致、可泛化的方法直接从视频估计细胞力学指标。

Method: 提出FlowMorph：以低维参数化轮廓表示单细胞；通过可微“胶囊-在-流中”模型推进边界点，耦合层流对流与曲率正则的弹性松弛；以仅由视频自动得到的轮廓与光流为观测，优化包含轮廓重叠、胞内流一致性、面积守恒、壁面约束、时间平滑的损失；为每个细胞学习无标签的标量力学代理k，并用少量RT-DC样本学习单调映射E=g(k)以预测表观杨氏模量。

Result: 在4个公开RBC微流控数据集上，物理丰富视频的平均轮廓IoU达0.905，相比纯数据驱动基线显著改善面积守恒和壁面违规；在约1.5×10^5条居中序列上，单一标量k即可区分摆轮(tank‑treading)与翻转(flipping)动力学，AUC=0.863；用200个RT‑DC事件标定后，在600个保留细胞上预测杨氏模量的MAE为0.118 MPa，并在通道几何、光学、帧率变化下性能平稳退化。

Conclusion: 物理一致的自监督建模可在无标注条件下从微流控视频中可靠恢复RBC形变动力学与力学代理；标量k既能表征动力学相位，又可经少量标定映射为杨氏模量，具备跨实验条件的泛化与实用价值。

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [101] [UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders](https://arxiv.org/abs/2601.17950)
*Matthew Walmer,Saksham Suri,Anirud Aggarwal,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 提出UPLiFT，一种高效的通用像素致密特征上采样架构，结合局部注意力器(Local Attender)实现迭代式上采样，在准确性与推理成本上优于跨注意力方法，并在生成式任务中表现竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有从低分辨率 backbone 特征映射到高分辨率致密特征的“任务无关”上采样方法，早期迭代式方案效率高但性能受限；最新跨注意力方案性能强但计算/内存随分辨率与token数急剧增长，重蹈原始backbone扩展性的覆辙。需要一种既高效可扩展又能保持特征稳定与性能的上采样方法。

Method: 提出UPLiFT框架：采用迭代式特征上采样，并引入Local Attender算子。该算子使用完全局部的注意力式池化（替代全局跨注意力），在每个局部邻域内进行信息聚合/重分配，旨在在多步上采样过程中保持特征稳定。整体为任务无关的通用模块，可插拔到不同下游任务与骨干上。

Result: UPLiFT在像素致密特征上采样基准上达到SOTA，同时推理成本低于现有跨注意力上采样器。Local Attender使得在上采样各阶段特征更稳定，带来更好的精度-效率权衡。将UPLiFT用于生成式下游（如VAE特征上采样、与Coupled Flow Matching对比）亦获得有竞争力的结果。

Conclusion: 迭代式上采样在得当设计下可与甚至超越跨注意力方法。UPLiFT通过局部注意力的高效实现，提供通用、轻量且可扩展的致密特征上采样方案，在判别与生成任务中均表现优异。

Abstract: The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.

</details>


### [102] [Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors](https://arxiv.org/abs/2601.17977)
*Jinchen Gu,Nan Zhao,Lei Qiu,Lu Zhang*

Main category: cs.CV

TL;DR: 提出DKGH-MoE：将数据驱动MoE与临床专家知识（如医师凝视）引导的MoE融合，以在小数据医疗场景中提升性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据量有限使纯数据驱动MoE难以学到稳健且临床有意义的模式；而临床实践中蕴含的专家先验（凝视模式、诊断启发式）无法靠小数据可靠学习。需要一个框架把新颖数据模式与专家知识互补结合。

Method: 设计可插拔、可解释的混合MoE模块DKGH-MoE：一支数据驱动MoE从原始图像提取新特征；另一支由领域专家先验引导的MoE利用临床凝视线索强调高诊断相关区域；两支融合以统一学习。

Result: 融合后模型在性能与可解释性方面均优于仅数据驱动或仅先验引导的方案（摘要未给具体指标）。

Conclusion: 在小数据医疗场景，将领域专家先验与数据驱动特征在MoE框架中融合，可提升鲁棒性、性能与临床可解释性；DKGH-MoE为通用、可插拔的实现。

Abstract: Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.

</details>


### [103] [MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images](https://arxiv.org/abs/2601.18001)
*Aqsa Yousaf,Sint Sint Win,Megan Coffee,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: MorphXAI 是一个将寄生虫检测与细粒度形态学解释相结合的XAI框架，能在定位寄生虫的同时输出形状、曲率、斑点数、鞭毛、有丝阶段等可临床验证的属性；在三种寄生虫数据集上既提升检测性能，又提供结构化、生物学有意义的解释。


<details>
  <summary>Details</summary>
Motivation: 现有寄生虫诊断依赖人工血涂片判读且缺乏可解释自动化工具；深度学习虽有效但解释性弱，主流方法只给热力图/注意力图，无法反映临床真正使用的形态学特征。因此需要一个能把“检测+形态学属性”统一建模的可解释框架，弥合模型与临床判读之间的鸿沟。

Method: 提出 MorphXAI：在检测管线中显式引入形态学监督，实现多任务学习。模型一方面进行寄生虫定位/分类，另一方面同时预测多种临床相关的形态学属性（形状、曲率、可见斑点数、鞭毛存在、发育阶段等）。作者还构建并标注了包含三种寄生虫（利什曼原虫、布氏锥虫、克氏锥虫）的数据集，提供细粒度形态标签，作为可解释分析基准。

Result: 与基线相比，MorphXAI 在检测性能上取得提升，并能输出结构化、可生物学验证的解释信息（属性级别的可解释性），优于仅提供热力图的现有方法。

Conclusion: 将形态学监督融入检测模型可同时提升准确性与可解释性；新数据集为可解释寄生虫分析建立了基准，证明结构化形态属性能更贴近临床判读需求，具有潜在临床实用价值。

Abstract: Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.

</details>


### [104] [Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection](https://arxiv.org/abs/2601.18008)
*Asiegbu Miracle Kanu-Asiegbu,Nitin Jotwani,Xiaoxiao Du*

Main category: cs.CV

TL;DR: 提出Strip-Fusion：一种对多光谱（可见光+热成像）行人检测的时空融合网络，兼顾错位鲁棒性、光照变化与遮挡，在KAIST与CVC-14上表现竞争，并在重遮挡与错位情形显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱行人检测多做空间融合，忽视时间信息；RGB与热图像在基准中常存在配准误差；复杂光照、遮挡等导致检测困难，需要能兼顾时空、对齐偏差与困难场景的方案。

Method: 1) Strip-Fusion时空融合框架：引入“时间自适应卷积”动态加权空间-时间特征，捕捉运动与上下文；2) 设计KL散度损失，缓解可见/热成像模态不平衡，训练时引导特征向更信息量大的模态对齐；3) 新的后处理算法，降低误检；整体面向错位鲁棒与困难条件。

Result: 在KAIST与CVC-14基准上获得具有竞争力的总体性能；在重遮挡与模态错位等挑战场景相较以往SOTA有显著提升。

Conclusion: 时空融合+模态自适应对齐+针对性后处理可有效提升多光谱行人检测，尤其在错位、遮挡、复杂光照下更稳健。

Abstract: Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.

</details>


### [105] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: 提出PIs-Regressor与Topology SegNet，将持久同调的拓扑特征以可微“持久图像(PI)”形式直接融入分割网络，提升血管/神经等曲线结构分割的像素精度与拓扑一致性，且较基于手工拓扑损失的方法更稳健、灵活，三大基准达SOTA。


<details>
  <summary>Details</summary>
Motivation: 曲线结构分割需保持连通性等拓扑属性；现有通过PD/手工拓扑损失的做法不可微、代价高、泛化差，难以跨任务稳定提升。需要一种可学习、可微、低成本且可与主干网络紧密耦合的拓扑表征方式。

Method: 1) PIs-Regressor：从数据端到端学习PD的可微表示——持久图像(PI)，避免显式计算与手工设计；2) Topology SegNet：在下采样与上采样阶段融合由PIs-Regressor生成的拓扑特征，与语义特征联合编码；3) 架构级整合拓扑信息而非依赖辅助拓扑损失，且可与其他拓扑方法无缝组合。

Result: 在三个曲线结构医学影像基准上获得SOTA，兼顾像素级指标与拓扑一致性；对过曝、模糊等成像退化更稳健。

Conclusion: 学习型PI与架构级拓扑融合能稳定提升曲线结构分割的精度与拓扑保真，避免手工拓扑损失的不可微与泛化问题，并具备良好兼容性与扩展性。

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [106] [Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling](https://arxiv.org/abs/2601.18049)
*Yunfei Qiu,Qiqiong Ma,Tianhua Lv,Li Fang,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 提出一种将空间先验与动态学习结合的半监督HSI分类框架：EASLP抑制边界标签扩散，DHP+ATSC构成DREPL稳定伪标签并提升时空一致性，在四个基准上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 半监督HSI受限于标注稀缺与高成本，导致超像素边界处标签扩散、伪标签随时间波动不稳，影响分类鲁棒性与精度。需要一种既利用空间结构先验又能动态稳化伪标签的方案。

Method: (1) EASLP：在超像素标签传播中引入边缘强度惩罚并配合邻域校正，抑制跨边界扩散并提升边界鲁棒性。(2) DHP：维护历史预测并与当前预测动态加权融合，提升时间一致性与抗噪性。(3) ATSC：基于置信度与一致性将样本划分为易/歧义/困难分层利用，提升伪标签质量与学习效率。(4) DREPL= DHP+ATSC，从时间域与样本域增强伪标签可靠性，并与EASLP协同实现时空一致性优化。

Result: 在四个公开基准数据集上，所提框架取得持续领先的分类精度与鲁棒性（摘要未给出具体数值），表现优于现有方法。

Conclusion: 结合空间先验的边界感知传播与动态历史融合及自适应样本分级，可有效缓解半监督HSI中的边界扩散与伪标签不稳问题，实现时空一致性优化并带来更优分类性能。

Abstract: Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.

</details>


### [107] [Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification](https://arxiv.org/abs/2601.18088)
*Jianshu Chao,Tianhua Lv,Qiqiong Ma,Yunfei Qiu,Li Fang,Huifang Shen,Wei Yao*

Main category: cs.CV

TL;DR: 提出一种无需源域标签的自监督跨域迁移框架，用S2Former做光谱-空间联合建模、FDC保持频域一致性、DAFT蒸馏对齐语义演化，在少样本微调下实现稳健跨域分类。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱自监督方法在跨域迁移上依赖源域标注且易受分布偏移影响，导致目标域泛化差；需要一种在资源受限（少标签）条件下仍能学到可迁移表征并稳定适配的方案。

Method: 三阶段/两阶段流程：1) 自监督预训练：设计S2Former，采用空间/光谱双分支Transformer与双向交叉注意力，空间分支随机掩膜增强结构感知，光谱分支捕获细粒度差异，互相引导提升语义一致性；并加入频域约束FDC，通过实FFT与高频幅值损失，维持频域一致、强化边缘与细节识别。2) 微调阶段：提出DAFT扩散对齐蒸馏，基于教师-学生结构对齐语义演化轨迹，在少标签下实现稳健迁移。

Result: 在四个高光谱数据集上取得稳定分类性能和强跨域适应性，少样本条件下仍具优势。

Conclusion: 该框架在无源域标签的前提下学得可迁移的光谱-空间联合表征，并通过频域一致性与扩散对齐蒸馏实现高效少样本适配，验证了在资源受限情景下的有效性与鲁棒性。

Abstract: Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.

</details>


### [108] [Text-Pass Filter: An Efficient Scene Text Detector](https://arxiv.org/abs/2601.18098)
*Chuang Yang,Haozhao Ma,Xu Han,Yuan Yuan,Qi Wang*

Main category: cs.CV

TL;DR: 提出一种用于任意形状文本检测的Text-Pass Filter（TPF），直接分割整段文本并通过“特征-滤波器对”模拟带通滤波，实现自然分离粘连文本，结合REU与FPU提升整体检测质量与实时性。


<details>
  <summary>Details</summary>
Motivation: 现有基于收缩掩码的扩张策略会丢失文本边缘视觉特征并混淆前景/背景，导致文本特征识别受限；尤其对粘连与长条带状文本，解码后处理复杂且影响实时性。

Method: 以带通滤波思想为启发：为每个文本构建独特的“特征-滤波器”配对。推理时，滤波器仅通过其通带的匹配特征（pass-feature），阻断其他频段特征，从而分别提取整段文本。引入两模块：1) Reinforcement Ensemble Unit（REU）增强同一文本内的特征一致性并扩大滤波器的识别视野，便于覆盖大纵横比的带状文本；2) Foreground Prior Unit（FPU）注入前景先验，加强前景/背景判别，提升特征-滤波器对质量。

Result: 实验证明：TPF无需复杂解码或后处理即可自然分离粘连文本，具备实时潜力；REU与FPU在消融中均带来显著增益，整体性能优于现有方法。

Conclusion: TPF以带通滤波模拟的特征-滤波器机制直接分割整段文本，有效避免收缩掩码策略的固有限制；REU与FPU进一步提升对大纵横比与前景区分的鲁棒性，实现更准确且高效的任意形状文本检测。

Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.

</details>


### [109] [Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs](https://arxiv.org/abs/2601.18099)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 提出一个零训练的前向计算框架，利用高斯散焦成像的解析式，在实时条件下从较清晰图像预测较模糊图像或估计模糊强度；通过邻域相似性从多解中选唯一解；实验证明模糊估计MAE<1.7%，合成重建强度误差<2%。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习或迭代优化的散焦/模糊估计方法训练代价高、推理慢、泛化受限；而高斯散焦模型已有解析验证，但缺少可在实时场景直接应用、无需训练的稳定求解流程，尤其当两幅图像互为部分模糊版本时。

Method: - 以高斯散焦的解析成像公式为基础，离散化计算由清晰图像到离焦（更模糊）图像的强度映射。
- 针对给定的高斯核标准差搜索范围，对每个像素求解析式的候选解（可能多解）。
- 通过邻域相似性/一致性度量在空间上进行约束，从多解中过滤为单一解。
- 设计处理“两幅图像互为部分模糊”情形的框架，实现从较清晰到较模糊的前向映射与模糊量估计。
- 零训练、实时实现。

Result: 在真实图像实验中：
- 合成模糊值估计的平均绝对误差（MAE）<1.7%；
- 将提取的散焦滤波器应用于较少模糊图像后，与真实模糊图像的强度差异<2%。

Conclusion: 该零训练前向框架在高斯散焦模型下可实时估计与合成模糊，能在多解情形下通过邻域约束稳健选解，并在真实数据上取得低误差，适用于两幅图像部分互为模糊版本的应用场景。

Abstract: Following the earlier verification for Gaussian model in \cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\%$, obtained by applying the extracted defocus filters to less blurred images.

</details>


### [110] [Spatial-Conditioned Reasoning in Long-Egocentric Videos](https://arxiv.org/abs/2601.18100)
*James Tribble,Hao Wang,Si-En Hong,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 研究在长时程第一人称视频中，通过在输入层显式加入空间线索（如深度）来提升VLM的空间推理：构建Sanpo-D数据集，基准评测多种VLM，并验证RGB+深度融合对导航与安全相关任务的收益。


<details>
  <summary>Details</summary>
Motivation: 长时程自我视角视频存在视角漂移与缺乏持续几何上下文，导致当前擅长短视频/图像推理的VLM在空间推理上表现不足；需要探索无需改模型结构或推理流程的方式，引入更强的空间归纳偏置。

Method: 1) 对Google Sanpo进行细粒度重标注，形成面向导航空间查询的Sanpo-D基准；2) 在多个VLM上评测导航导向的空间问答；3) 在输入层将深度图与RGB帧融合（深度感知）以引入空间归纳偏置，比较对空间推理与通用性能的影响。

Result: 深度与空间显式线索带来“通用准确率 vs. 空间专长”的权衡：深度感知和空间落地的表征显著提升行人/障碍物检测等安全关键任务，但可能对通用任务准确率有一定影响。

Conclusion: 无需改动架构/推理流程，仅在输入层加入深度等空间信号，即可增强VLM在长时程自我视角视频的空间推理与安全关键任务表现；但需权衡通用能力与空间专长，并据任务需求选择表征。

Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.

</details>


### [111] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出LungCRCT：基于潜在因果表征学习的肺癌分析框架，结合图自编码因果发现、距离相关去耦与熵约束重建，实现因果干预分析，并在恶性肿瘤分类上以极轻量下游模型达到AUC 93.91%。


<details>
  <summary>Details</summary>
Motivation: 现有LDCT结合CNN/ViT虽在检测上表现强，但相关性依赖强、可解释性差，难以支撑治疗分析与干预模拟；需要能抓住疾病物理机制中因果因素的表示，以支持更可靠的决策与扩展到因果推断。

Method: 构建LungCRCT框架：利用图自编码器型因果发现算法获取潜在因果图；通过距离相关(HSIC/Distance Correlation)驱动的解耦以减少潜变量相关性；加入基于熵的图像重建精炼以稳定和可解释；从LDCT中学习可干预的潜在因子表示，并据此进行干预模拟与下游分类。

Result: 在恶性肿瘤分类任务中，使用由LungCRCT提取的因果表示驱动极轻量下游模型达到AUC 93.91%；框架还能进行治疗因果干预分析（定性/定量未详述）。

Conclusion: 因果表示学习能缓解深度模型的相关性与可解释性问题，使肺癌LDCT分析更适于干预与治疗评估，同时保持或提升分类性能并降低模型复杂度。

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [112] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: 提出FoGA：一个约2M参数、可达155 FPS的轻量级视频异常检测模型，通过前向一致性学习与门控上下文聚合，在精度与效率上兼顾并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VAD多依赖大模型以追求极致精度，难以在边缘设备部署；同时主流的预测式方法仅基于单帧未来预测误差判异常，忽略更长时域前向信息的约束，限制了检测鲁棒性与准确性。

Method: 设计轻量级U-Net骨干，对连续帧提取特征并生成即时预测与更前序列的前向预测；在跳连中加入门控上下文聚合模块，动态融合同尺度的编码器与解码器特征；以新颖的前向一致性损失进行联合优化，并采用融合即时与前向预测误差的混合异常度量策略。

Result: 在多组实验中显著优于多种SOTA方法，且推理速度最高达155 FPS，参数量约2M，显示出优异的实时性与资源效率。

Conclusion: FoGA在小模型、实时性的约束下，通过前向一致性学习与门控聚合实现更准确的异常检测，达到性能与效率的良好折中，适合边缘设备部署。

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [113] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: EGAgent提出基于实体场景图的代理式框架，支持在长时程自我视角视频中进行跨模态、可组合推理，实现对多天视频的结构化检索与记忆，SOTA于EgoLifeQA、在Video-MME(Long)具竞争力。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备推动“始终在线”的个人AI助理，需要对连续、长时程的自我视角视频进行语义理解与记忆。现有LLM与RAG受限于上下文窗口，难以在数天/数周视频上做多跳、组合推理与时序一致的回溯。

Method: 构建以实体场景图为核心的EGAgent：将人、地、物及其跨时间关系表示为图；规划型代理配备结构化检索与图推理工具，结合视觉与音频的混合检索；支持跨模态、跨时间、可组合的多跳推理，保证时序一致性。

Result: 在EgoLifeQA上达到SOTA 57.5%，在Video-MME(Long)上取得74.1%的竞争性成绩，适用于复杂的长时程视频理解任务。

Conclusion: 实体场景图+代理化规划与混合检索显著提升长时程自我视角视频的理解与问答能力，为可穿戴AI助理的持续记忆与推理奠定基础。

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [114] [TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration](https://arxiv.org/abs/2601.18168)
*Zehua Liu,Shihao Zou,Jincai Huang,Yanfang Zhang,Chao Tong,Weixin Si*

Main category: cs.CV

TL;DR: 论文提出一种用于TACE术中2D-3D血管配准的粗到细方法：先用结构感知PnP做全局对齐，再用时间扩散模型TempDiffReg迭代形变以适应解剖变化；在23例、626多帧样本上显著优于SOTA（MSE 0.63 mm、MAE 0.51 mm）。


<details>
  <summary>Details</summary>
Motivation: TACE对肝癌治疗有效，但术中微导管导航困难，个体血管解剖差异大，当前2D-3D配准在复杂形变与局部结构匹配上的鲁棒性与精度不足，限制了术者尤其是经验较少医生的操作效率与安全性。

Method: 提出粗到细两阶段：1) SA-PnP全局对齐——利用血管结构信息进行2D-3D对应与位姿估计，提升初始对齐稳健性；2) TempDiffReg——基于时间扩散模型的形变估计，利用多帧时间上下文，迭代优化血管局部形变与复杂解剖变化的配准。基于23名患者构建626对多帧样本进行评估。

Result: 相较SOTA，方法在准确性和解剖合理性上更优：注册MSE 0.63 mm、MAE 0.51 mm，分别较最强对比降低66.7%与17.7%。

Conclusion: 该方法在术中2D-3D血管配准上实现高精度且解剖一致的对齐，有望帮助经验不足的术者安全高效完成复杂TACE，改善疗效与患者护理；代码与数据已开源。

Abstract: Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}

</details>


### [115] [YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection](https://arxiv.org/abs/2601.18172)
*Lin Huang,Yujuan Tan,Weisheng Li,Shitai Shan,Liu Liu,Bo Liu,Linlin Shen,Jing Yu,Yue Niu*

Main category: cs.CV

TL;DR: 提出YOLO-DS，通过双统计协同算子（DSO）显式建模通道内异质目标响应，在COCO上较YOLOv8各尺度提升1.1–1.7 AP，延迟增量很小。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO一阶段检测虽高效，但共享通道内不同目标响应被混合，缺乏显式建模，限制性能上限。

Method: 核心是Dual-Statistic Synergy Operator（DSO），联合建模“通道均值”和“峰值-均值差”两种统计量以解耦目标特征；在此之上设计两种轻量门控：DSG用于自适应通道选择，MSG用于深度可分路径的分段加权，实现细粒度特征重标定。

Result: 在MS-COCO上，YOLO-DS在五个规模（N/S/M/L/X）均优于YOLOv8，AP提升约1.1%–1.7%，推理延迟仅有轻微增加；可视化、消融与对比实验均证明有效。

Conclusion: 显式利用双统计量解耦与门控能更好区分异质目标，在保持高效率的同时稳定提升检测精度，可作为YOLO系列的通用增强模块。

Abstract: One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.

</details>


### [116] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: NaVIDA将视觉-语言导航中的策略学习与基于动作的视觉动力学和自适应执行耦合，通过逆动力学监督和分层动作块提高可预见性、稳定性与泛化，参数更小却优于SOTA，并在真实机器人上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法多为反应式状态-动作映射，未显式建模“行动如何因果地改变后续视觉观测”。缺乏这种视觉-动作因果性导致无法预判自身动作引发的视觉变化，从而出现不稳定行为、弱泛化与轨迹上的误差累积。

Method: 提出NaVIDA框架：1) 逆动力学增强（chunk-based inverse-dynamics supervision），学习视觉变化与动作的因果对应；2) 层次化概率动作分块（HPAC），将轨迹组织为多步动作块，提供更具判别力的长程视觉变化线索并扩展规划范围；3) 基于熵的自适应执行，在推理时根据不确定性动态设定动作块执行长度，抑制误差累积并稳定行为。

Result: 在基准数据集上，以更少参数（3B vs 8B）取得优于当前SOTA的导航性能；并在真实机器人实验中验证了方法的可行性与有效性。

Conclusion: 显式建模视觉-动作因果关系并结合分层动作块与自适应执行，可显著提升VLN的稳定性、泛化与效率；NaVIDA在更小模型规模下实现SOTA，并具备现实部署潜力。

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [117] [Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2601.18190)
*Yifan Li,Shiying Wang,Jianqiang Huang*

Main category: cs.CV

TL;DR: 提出MPS-CLIP，通过关键词引导的多视角细粒度对齐与参数高效适配，显著提升遥感图文检索，超越全量微调与近期方法。


<details>
  <summary>Details</summary>
Motivation: 现有RSITR多依赖全局粗粒度对齐，难以捕获遥感影像密集、多尺度语义；同时对大型VLP模型进行全量微调计算开销大且易遗忘，需一种既细粒度又参数高效的方案。

Method: 1) 用LLM从文本提取核心语义关键词；2) 以关键词引导SamGeo生成语义相关的子视角/分割区域；3) 设计Gated Global Attention (G^2A) 适配器在冻结骨干上高效建模全局上下文与长程依赖；4) Multi-Perspective Representation (MPR) 将多局部线索聚合为多视角嵌入；5) 以多视角对比损失+加权三元组损失联合优化，动态选择最大响应视角以抑噪并强化精确匹配。

Result: 在RSICD与RSITMD上达到SOTA，mR分别为35.18%与48.40%，显著优于全量微调基线与近期竞争方法。

Conclusion: 通过关键词引导的多视角细粒度对齐与G^2A参数高效适配，MPS-CLIP在RSITR中兼顾性能与效率，缓解全局对齐不足与全量微调成本高的问题，并在两个基准上取得显著领先。

Abstract: Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.

</details>


### [118] [MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models](https://arxiv.org/abs/2601.18192)
*Tian-Yi Zhou,Xuan-Hao Liu,Bao-Liang Lu,Wei-Long Zheng*

Main category: cs.CV

TL;DR: MindCine提出一种多模态联合学习与大规模EEG预训练模型相结合的框架，在小样本条件下将EEG重建为高保真视频，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG具备无创和高时间分辨率，重建人类动态视觉感知具有重要意义。但现有EEG到视频重建面临两大难点：仅与文本对齐导致单一模态、易过拟合；数据稀缺导致训练难以收敛。

Method: 提出MindCine框架：1) 训练阶段采用多模态联合学习，将文本之外的多种模态共同对齐与融合；2) 利用预训练的大规模EEG模型解码语义信息，缓解数据不足；3) 设计带因果注意力的Seq2Seq模型解码感知（低层次时序与外观）信息。

Result: 在定性和定量评估上均优于SOTA，展示了多模态互补性的有效性；使用大规模EEG模型在小数据条件下进一步提升重建质量与稳定性。

Conclusion: 多模态联合学习与大规模EEG预训练模型的结合能有效缓解数据稀缺与单模态过拟合问题，实现更高保真的EEG到视频重建。

Abstract: Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.

</details>


### [119] [QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding](https://arxiv.org/abs/2601.18195)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Kaiwei Zhang,Jun Jia,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出QualiRAG：一种无需训练的检索增强生成框架，用于让多模态大模型进行可解释的视觉质量理解，通过动态构建和检索多源辅助知识，实现细粒度时空感知与有依据的推理，在多项任务上优于通用与微调模型。


<details>
  <summary>Details</summary>
Motivation: 现有VQA正从打分转向可解释的质量理解，需要细粒度时空感知与上下文信息；但依赖监督微调或强化学习，标注成本高且易受数据集偏置。需要一种无需额外训练、可系统利用LMM潜在感知知识的方法。

Method: 设计训练免RAG流程：将问题分解为结构化请求，动态生成四类互补知识源（视觉元数据、主体定位、全局质量摘要、局部质量描述），并进行相关性检索，将证据注入推理以提升质量感知与解释能力。

Result: 在视觉质量理解任务上，相比开源通用LMM与专门微调的VQA LMM显著提升；在质量比较任务上取得有竞争力表现，展现稳健性与泛化，无需任何任务特定训练。

Conclusion: QualiRAG通过动态知识构建与相关性检索，释放LMM潜在感知能力，实现可解释、稳健的视觉质量评估，避免昂贵标注与偏置问题；代码将开源以促进复现与扩展。

Abstract: Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.

</details>


### [120] [HomoFM: Deep Homography Estimation with Flow Matching](https://arxiv.org/abs/2601.18222)
*Mengfan He,Liangzheng Sun,Chunyu Li,Ziyang Meng*

Main category: cs.CV

TL;DR: 提出HomoFM：用生成建模中的flow matching把单应性估计从“回归/迭代优化”改为“速度场学习”，并结合GRL做域自适应，在多域/多模态下取得更高精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度单应性方法多直接回归或迭代细化，难以表达复杂几何、对跨域（多模态、光照变化等）泛化差；需要一种既能建模连续变换、又具备域不变特征学习的框架。

Method: 1) 将单应性估计表述为条件流匹配（flow matching）问题：学习一个连续、点对点的速度场，将带噪分布沿时间轨迹推向目标对齐坐标，从而恢复单应变换；2) 设计条件流网络，从图像对提取特征后生成速度场并积分得到变换；3) 引入GRL到特征编码器，联合域判别器进行对抗式训练，迫使编码器学习域不变表示，以缓解多模态/光照变化等域偏移。

Result: 在标准基准上，相比SOTA方法，HomoFM在估计精度与鲁棒性上均有提升；在多域设置（如多模态与光照变化）下优势更明显。

Conclusion: 基于flow matching的速度场学习为单应性估计提供了新的范式；结合GRL的域自适应提升跨域泛化，实验验证方法在精度与稳健性上优于现有方法。

Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.

</details>


### [121] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: 提出一个基于EfficientNetB2的轻量级表情识别管线，通过两阶段预训练+微调、AdamW、标签平滑、截断类权重与强数据增强，在FER-2013测试集达68.78%准确率，用参数量约为VGG16的十分之一，训练稳定且适合实时/边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现实场景表情识别受低清晰度、光照/姿态/背景变化、小类间差异、众包标注噪声与类别极不均衡影响。现有VGG/ResNet虽准确但代价高，难以实时应用，需要更高效且鲁棒的方案。

Method: 以EfficientNetB2为骨干，采用两阶段warm-up与fine-tune训练；优化器用AdamW并解耦权重衰减；标签平滑(eps=0.06)抑制噪声；对类别不平衡使用截断类权重；再配合dropout、混合精度与大规模在线数据增强。数据划分为分层的87.5%/12.5%训练/验证，保持官方测试集不变。

Result: 在FER-2013上获得68.78%的测试准确率；参数量约为VGG16基线的1/10；提供每类指标与学习曲线显示训练稳定、泛化良好。

Conclusion: 该轻量模型在保持较好精度的同时显著降低计算与内存开销，适合实时与边缘设备部署；训练策略与正则化共同提升了对噪声标注和类不平衡的鲁棒性。

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [122] [V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering](https://arxiv.org/abs/2601.18240)
*Mengyuan Jin,Zehui Liao,Yong Xia*

Main category: cs.CV

TL;DR: 提出V-Loop：一种无需训练、可即插即用的医学VQA幻觉检测框架，通过“答案→反向验证问题→再回答”的视觉逻辑闭环来核实事实；在多基准与多MLLM上优于不确定性等内省方法，并可与其结合增益。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在医学VQA中虽能辅助诊断但易产生与图像事实相矛盾的幻觉，风险高。主流内省检测（尤其基于不确定性）高效但间接，只估计样本层面的预测不确定性，不能直接验证某个具体答案是否与视觉证据一致。

Method: 提出Visual Logical Loop Verification（V-Loop）。流程：1）MLLM对原始图像-问题给出初始答案；2）从原始问答中抽取语义单元（问题单元、答案单元）；3）基于答案单元条件化，生成“验证问题”以反向查询问题单元（构造双向推理）；4）在回答原始问题与验证问题时施加视觉注意力一致性约束，确保依赖同一图像证据；5）若验证回答与期望语义一致，闭环成立，认定事实扎根；否则标记为幻觉。方法训练免疫、可插拔。

Result: 在多个医学VQA基准和多种MLLM上，V-Loop较现有内省方法（含不确定性法）取得更高的幻觉检测性能，同时效率保持较高；与不确定性方法联合使用可进一步提升表现。

Conclusion: 通过构建视觉扎根的逻辑闭环，V-Loop能直接核实答案的事实正确性，克服仅凭不确定性间接判断的局限；在医学高风险场景下更可靠，且实用性强（训练-free、即插即用、可与现有方法组合）。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.

</details>


### [123] [Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation](https://arxiv.org/abs/2601.18242)
*Zerui Kang,Yishen Lim,Zhouyou Gu,Seung-Woo Ko,Tony Q. S. Quek,Jihong Park*

Main category: cs.CV

TL;DR: 论文利用视觉-语言模型（VLM）为可微射线追踪（DRT）中的多材料电参数反演提供先验与测量布设指导，从而显著加速收敛并降低误差。


<details>
  <summary>Details</summary>
Motivation: 6G电磁数字孪生需要精确的材料射频参数，但基于梯度的反演对初始化敏感、测量有限时收敛慢且不稳定。需要一种能在有限测量下稳定、快速估计多材料参数的方法。

Method: 引入VLM解析场景图像，推断材料类别并通过ITU-R材料表映射为电导率等定量先验；同时由VLM选择信息量大的发射机/接收机位置以激发对材料区分度高的路径。在此先验初始化上，DRT用接收信号强度进行梯度优化精炼参数。并在NVIDIA Sionna中进行实验、复杂度与消融分析（RT深度、光线数）。

Result: 相较于均匀/随机初始化与随机布设基线，收敛加速2–4倍，最终参数误差降低10–100倍；仅用少量接收机即可达到<0.1%平均相对误差。每迭代时间近线性随材料数与测量配置增长；VLM引导的布设减少所需测量数。增加RT深度与射线数进一步提升精度且对单次迭代开销影响不大。

Conclusion: 将VLM的语义先验与物理可微优化结合，可在有限测量下实现快速、稳定、准确的RF材料参数估计，凸显了语义引导的物理反演在6G电磁孪生中的实用价值。

Abstract: Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.

</details>


### [124] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 提出OrthoFoundation：面向肌骨影像的多模态视觉基础模型，基于120万未标注膝关节X光与MRI进行自监督预训练，在14个下游任务达SOTA，标注效率高、跨关节泛化强，显著减轻标注负担并提升临床诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有骨科AI多为任务特定的监督学习：数据标注昂贵、碎片化强、跨模态与跨场景泛化差；同时缺乏大规模、可开放的肌骨数据限制了基础模型的发展。

Method: 构建包含约120万未标注膝关节X光与MRI图像的预训练语料；采用Dinov3骨干，基于自监督对比学习以获取稳健的放射学表示；在多种下游任务上微调与评估，并测试标签效率与跨解剖部位迁移能力。

Result: 在14项下游任务达成SOTA：X光骨关节炎诊断准确率领先，MRI结构性损伤检测排名第一；用仅50%标注数据即可匹配监督基线；尽管仅在膝关节预训练，仍对髋、肩、踝表现出优秀的跨解剖泛化。

Conclusion: OrthoFoundation通过从大规模多模态未标注数据中学习关节无关的放射学语义，克服传统模型对任务与标注的依赖，提供通用肌骨影像AI的坚实框架，有望降低标注成本并提升临床诊断表现。

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [125] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: 提出Co-PLNet：通过点-线协同与提示编码实现更稳健高效的线框解析，提升精度、鲁棒性与实时性。


<details>
  <summary>Details</summary>
Motivation: 线框解析需同时恢复线段与交点以服务SLAM等下游，但现有方法通常分别预测并事后匹配，导致点线不一致、错配与鲁棒性下降，亟需一个能在预测阶段显式耦合点线信息的框架。

Method: 提出点-线协同框架Co-PLNet：先将早期的点/线初检结果通过PLP-Encoder转为空间对齐、紧凑的“提示”图，编码其几何属性；随后CGL-Decoder利用稀疏注意力，以互补提示为条件对点、线进行交叉引导式精修，显式强化点线一致性并提升效率。

Result: 在Wireframe与YorkUrban数据集上，相比现有方法在精度与鲁棒性上取得一致提升，并保持实时推理效率。

Conclusion: 点-线协同与空间提示机制能有效消除事后对齐带来的不一致，提升线框解析的准确性、稳健性与效率，适用于结构化几何感知与SLAM等场景。

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [126] [Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images](https://arxiv.org/abs/2601.18260)
*Eytan Kats,Kai Geissler,Daniel Mensing,Jochen G. Hirsch,Stefan Heldman,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 提出一种从单幅人体表面深度图直接预测多器官三维位置与形状的学习框架，用合成深度图-解剖分割对训练统一CNN，实现无需显式表面重建的器官定位，助力放射流程自动化摆位。


<details>
  <summary>Details</summary>
Motivation: 手工摆位低效且影响扫描精度与通量；现有方法依赖复杂表面重建或额外成像，难以快速、准确估计体内器官位置。RGB-D深度信息可从体表推断内脏分布，为自动化摆位提供低成本途径。

Method: 利用大规模全身MRI数据生成配对的合成深度图与解剖分割，训练一个统一的卷积神经网络，输入单张2D深度图，直接输出多器官的3D位置与形状（包含骨骼与软组织），不需显式的表面重建；端到端的学习框架。

Result: 模型能准确定位多种解剖结构，涵盖骨与软组织；在实验中显示良好精度与泛化，证明深度传感器结合该模型可有效支持自动化摆位。

Conclusion: 将深度相机与学习模型整合到放射科工作流程，可减少人工、加速扫描、提升患者体验；该思路为基于体表深度信息的内脏三维估计提供可行路径。

Abstract: Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.

</details>


### [127] [Revisiting Aerial Scene Classification on the AID Benchmark](https://arxiv.org/abs/2601.18263)
*Subhajeet Das,Susmita Ghosh,Abhiroop Chatterjee*

Main category: cs.CV

TL;DR: 论文综述+新模型：回顾从手工特征到CNN与混合深度网络的航拍图像场景分类方法，并提出带空间注意力与多尺度特征融合的Aerial‑Y‑Net，在AID数据集达91.72%准确率，优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 航拍图像包含多样且异质的地物（建筑、森林、山地、空地），场景内在复杂度高，传统模型泛化与鲁棒性不足，亟需系统梳理现有方法并提出更能捕捉关键区域与多尺度信息的模型。

Method: 两部分：1) 文献综述——覆盖手工特征（SIFT、LBP）、传统CNN（VGG、GoogLeNet）到深度混合网络；2) 提出Aerial‑Y‑Net：在CNN框架中引入空间注意力模块与多尺度特征融合，作为注意力驱动的分类模型。

Result: 在AID数据集上，Aerial‑Y‑Net取得91.72%分类准确率，超过多种基线架构（如VGG、GoogLeNet等）。

Conclusion: 空间注意力与多尺度融合能够有效提升航拍场景分类，对复杂异质场景具有更好判别力；综述为方法选择提供参考，新模型显示出优于常见基线的性能。

Abstract: Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.

</details>


### [128] [Contextual Range-View Projection for 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.18301)
*Seyedali Mousavi,Seyedhamidreza Mousavi,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 提出两种改进投影策略（CAP与CWAP）以缓解LiDAR范围图像投影中的多对一冲突，在SemanticKITTI上带来最多+3.1% mIoU提升，并可按类别定制优先级。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度的选择仅保留最近点，忽视实例结构与语义重要性，导致上下文丢失与分割性能受限；需要在投影阶段更智能地保留关键点。

Method: 在范围图投影的冲突解决中引入两种机制：1) Centerness-Aware Projection（CAP）：依据点到实例中心的距离调整“有效深度”，中心更近的点获得更高优先级，从而优先保留实例核心点、抑制边界/背景噪声；2) Class-Weighted-Aware Projection（CWAP）：为各类别设置权重，在发生冲突时优先保留高权重类别点，实现任务/场景可定制的投影策略。

Result: 在SemanticKITTI上，CAP在不改动下游2D网络的情况下保留更多实例点，分割mIoU最高提升3.1%；CWAP可显著提升目标类别表现，对非目标类别影响极小。

Conclusion: 将实例中心性与类别权重纳入投影冲突决策，能在保持效率的同时减少关键信息丢失，提高分割性能，并为特定任务提供灵活可调的类别优先策略。

Abstract: Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \textit{Centerness-Aware Projection (CAP)} and \textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes

</details>


### [129] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: 提出SwipeGen自动合成“类人滑动”交互与GUISwiper代理，并发布首个评测滑动执行能力的基准；在滑动执行上达69.07%准确率，相比VLM基线提升214%。


<details>
  <summary>Details</summary>
Motivation: GUI代理在感知与指令对齐上进步显著，但在具体动作执行，尤其是“滑动”操作上表现薄弱，常用策略过于简化，无法复现人类细粒度滑动行为，成为任务完成瓶颈。

Method: 将人类滑动手势分解为可量化的多维参数（如方向、速度、加速度、轨迹曲率、时长、停顿等），提出自动化管线SwipeGen，通过GUI探索合成多样化、类人滑动数据；基于合成数据构建首个滑动执行评测基准；进一步训练/设计增强执行模块，形成GUISwiper以提升实际滑动操作能力。

Result: 在新基准上，GUISwiper达到69.07%滑动执行准确率，相比现有多模态大模型（VLM）基线提高214%。

Conclusion: 精细建模并数据合成“类人滑动”可显著提升GUI代理的动作执行能力；SwipeGen与基准为社区提供了可复现评测与数据来源，GUISwiper验证了该路线的有效性。

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [130] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出一种用于脑肿瘤MRI分析的高效Densely Swin Hybrid (EDSH)框架，结合DenseNet与Swin Transformer以同时获取细粒度纹理与长程依赖，两种肿瘤感知实验设置显著提升四类肿瘤分类，测试集达98.5%准确率与召回率。


<details>
  <summary>Details</summary>
Motivation: 现有CNN擅长局部纹理、ViT擅长全局依赖，但单一或简单混合难以同时捕获不规则肿瘤纹理与全局形态，易出现漏诊（尤其弥漫性胶质瘤）或误判（脑膜瘤、垂体瘤）。需要一种既保留MRI细节又高效建模全局结构的混合框架，并针对不同肿瘤表型的分类困难进行定制优化。

Method: 提出EDSH框架，包含两种设置：1）Boosted Feature Space（BFS）：独立定制DenseNet与Swin_t分支学习互补局部/全局表征，经维度对齐、特征融合与“boosting”增强以提高对弥漫性胶质瘤（形状不规则、边界不清、纹理异质）的敏感性。2）层次化DenseNet–Swin架构并引入深特征提取与双残差（DFE+DR）：DenseNet作为stem CNN学习结构化局部特征，Swin_t建模全局肿瘤形态，降低脑膜瘤与垂体瘤的漏检（利用位置、界清与肿瘤外生特征）。DenseNet在输入层按MRI空间特性定制，采用致密残差连接保纹理、抑梯度消失；Swin以任务对齐的patch embedding与移位窗口自注意力高效捕获分层全局依赖。

Result: 在包含四类肿瘤、共40,260张MRI的大规模数据上，相较独立CNN、ViT及其他混合方法表现更优，测试集准确率与召回率均达98.50%。

Conclusion: EDSH通过肿瘤感知的双路径/层次化混合与结构定制，实现局部纹理与全局形态的协同建模，显著提升多类脑肿瘤MRI分类与漏检抑制，在实际临床辅助诊断中具潜在价值。

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [131] [PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction](https://arxiv.org/abs/2601.18336)
*Isaac Deutsch,Nicolas Moënne-Loccoz,Gavriel State,Zan Gojcic*

Main category: cs.CV

TL;DR: 提出PPISP模块，基于物理可解释的ISP分解与控制，缓解多视图3D重建中由相机与ISP差异引发的光度不一致，支持新视角参数预测与元数据融合，在基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 多视图重建对跨视角/设备的色彩、曝光、白平衡等光度不一致极为敏感，现有用潜变量或仿射颜色校正的方法缺少物理依据、难以泛化至新视角并缺乏可解释控制。

Method: 设计“物理可行的ISP(PPISP)”校正模块：将相机内在因素与拍摄依赖因素解耦，并以物理可解释的变换建模；再训练一个PPISP控制器，仅基于输入视图学习，在推理时为新视角预测ISP参数（类比相机自动曝光/白平衡）。模块支持元数据融合与可控调参，用于在无GT图像下进行更真实公平的评估。

Result: 在标准基准上取得SOTA性能，同时提供直观控制能力，并能利用可用的拍摄元数据提升效果。

Conclusion: 物理驱动、可解释的ISP建模与控制能有效缓解光度不一致，提升新视角重建质量与泛化，且便于实际应用中的控制与评估。

Abstract: Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp

</details>


### [132] [Beyond Rigid: Benchmarking Non-Rigid Video Editing](https://arxiv.org/abs/2601.18340)
*Bingzheng Qu,Kehai Chen,Xuefeng Bai,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: 论文提出NRVBench用于评测非刚性视频编辑，含数据集、VLM驱动的新指标NRVE-Acc与零训练基线VM-Edit，能更好衡量与提升物理一致性和时序连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动视频编辑在非刚性形变上易出现物理失真与时间闪烁，通用指标难以捕捉复杂动力学，缺乏专门基准。

Method: (1) 构建NRVBench：180段非刚性物理视频、6类物理场景、2340条细粒度指令与360道多选题；(2) 设计VLM基础的新评测NRVE-Acc，评估物理合规、时序一致与指令对齐；(3) 提出零训练基线VM-Edit，采用双区域去噪实现结构感知控制，平衡结构保真与动态形变。

Result: 在标准与所提指标上，现有方法在物理可信度与时序方面表现不足；VM-Edit取得优异成绩。

Conclusion: NRVBench为非刚性视频编辑提供标准化评测平台；NRVE-Acc可更严谨衡量复杂动态；VM-Edit展示在无需训练下的强基线表现，有望推动物理感知的视频编辑发展。

Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.

</details>


### [133] [Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception](https://arxiv.org/abs/2601.18346)
*Sijing Wu,Yunhao Li,Zicheng Zhang,Qi Jia,Xinyue Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出Q-Bench-Portrait——首个面向人像图像质量感知的综合基准；收集多源人像、覆盖技术/AIGC失真与审美、多种题型；评价25个MLLM显示与人类有显著差距，旨在推动该方向研究。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在通用低层视觉基准上表现不错，但对结构与感知特性独特的人像图像感知与评估研究不足；缺少系统化、针对人像的质量基准来客观衡量与驱动模型能力提升。

Method: 构建Q-Bench-Portrait数据集与评测框架：包含2765个图-问-答三元组；数据来源涵盖自然、人为合成失真、AIGC、艺术、CG等多类人像；质量维度覆盖技术性失真、AIGC特有失真与审美；题型含单选、多选、判断与开放问答，兼顾全局与局部层级。基于该基准统一评测20个开源与5个闭源MLLM。

Result: 当前MLLM在人人像质量感知上仅具一定能力但整体有限且不稳定，与人类判断存在明显差距；不同模型在不同维度与题型上性能参差。

Conclusion: Q-Bench-Portrait为人像图像质量感知提供首个系统化评测基准，可揭示MLLM局限并为后续面向通用与垂直领域模型的改进提供方向，鼓励社区进一步研究与优化。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.

</details>


### [134] [OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI](https://arxiv.org/abs/2601.18368)
*Caterina Fuster-Barceló,Claudia Castrillón,Laura Rodrigo-Muñoz,Victor Manuel Vega-Suárez,Nicolás Pérez-Fernández,Gorka Bastarrika,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: OREHAS 是一个端到端全自动流程，用于从常规 3D-SPACE-MRC 与 3D-REAL-IR MRI 中定量内淋巴积水（EH），在仅需极少标注的情况下实现稳健三维分割和每耳 ELR 体积分数计算，准确性优于临床现有软件并具备良好外部泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前 EH 定量依赖手工或半自动操作，存在主观性强、流程碎片化、对序列和操作者敏感等问题；临床软件易高估内淋巴体积，影响诊断阈值与研究可复现性。需要一个对标准 MRI 友好、低标注成本、可全自动输出体积指标（如 ELR）的统一方法。

Method: 提出 OREHAS 三阶段流水线：1) 切片分类以筛选相关层面；2) 内耳定位以确定左右耳体素区域；3) 针对不同序列（SPACE-MRC、REAL-IR）的特异性分割网络。训练只用每位受试者 3–6 张标注切片，利用高效深度学习与体积化后处理，直接在整幅 MRI 上计算每耳 ELR。

Result: 在 3D 体数据上泛化良好：SPACE-MRC Dice=0.90、REAL-IR Dice=0.75。外部验证中，与专家全标注一致性 VSI=74.3%，显著优于临床 syngo.via 的 VSI=42.5%（后者倾向高估内淋巴）。在 19 例测试中，前庭体积与 syngo.via 一致，而内淋巴体积更小、更符合生理。

Conclusion: 标准 MRI 下可用有限监督实现可靠可复现的 EH 体积分割与定量。OREHAS 将深度学习分割与临床一致的体积流程结合，降低操作者依赖、提高方法一致性，兼容既有成像协议，并为大规模研究与基于准确体积测量的临床阈值重校准提供基础。

Abstract: We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.
  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.
  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.

</details>


### [135] [Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues](https://arxiv.org/abs/2601.18372)
*Christos Petrou,Harris Partaourides,Athanasios Balomenos,Yannis Kopsinis,Sotirios Chatzis*

Main category: cs.CV

TL;DR: 提出一种结合HMD运动信号与视频显著性线索的轻量级凝视预测框架，用TSMixer或LSTM进行时间序列预测，在EHTask与商用VR上优于中心/平均基线，降低感知时延、提升交互自然度。


<details>
  <summary>Details</summary>
Motivation: VR中凝视预测可用于降低传感器延迟并支撑注视渲染等重计算技术，但直接眼动追踪常因硬件或隐私受限；因此需要无需眼追的替代方案，通过可得的HMD运动与视觉线索来提前估计用户关注点。

Method: 采用轻量级显著性编码器UniSal从视频帧提取视觉显著性特征；与HMD运动数据进行特征融合后，输入时间序列预测模块；对比两种轻量级架构TSMixer与LSTM进行未来凝视方向预测；在EHTask数据集上训练/评估，并在商用VR硬件上部署验证。

Result: 在EHTask和实际硬件部署中，方法稳定优于Center-of-HMD与Mean Gaze等基线，能更准确地预测未来视线方向，体现出对降低感知时延的帮助。

Conclusion: 结合显著性与头显运动的预测式凝视建模在缺乏直接眼追场景下有效，可提升VR交互与注视渲染体验；轻量级设计适合在商业VR设备上实时部署。

Abstract: Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.

</details>


### [136] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: 提出一种利用网格状导频信号与Radon变换进行几何变换估计的鲁棒数字水印同步方法，针对裁剪、缩放、旋转、错切等攻击，能低误差还原变换矩阵并实现同步与水印提取。


<details>
  <summary>Details</summary>
Motivation: 现有图像水印在遭遇几何失真（尤其裁剪改变图像原点）时难以完成同步，导致水印提取失败；鲁棒抗裁剪同步方法稀缺，需能在复杂几何攻击后准确定位嵌入位置。

Method: 在宿主图像中嵌入具有水平/垂直不同编码的网格状导频信号。攻击后，网格随图像发生畸变；对畸变图像应用Radon变换估计网格角度与间隔，并利用水平/垂直差异判定网格朝向，从而估计整体几何变换矩阵（含各向异性缩放、旋转、错切与位移/裁剪引起的偏移），实现同步。

Result: 在仿真中面对各向异性缩放、旋转、错切和裁剪的单一与组合攻击，方法可低误差恢复变换矩阵，表现出良好的同步精度与鲁棒性。

Conclusion: 导频网格+Radon分析能有效解决裁剪引起的同步问题，显著提升几何失真下的水印鲁棒性；通过消除网格朝向歧义进一步提高估计稳定性。

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [137] [ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks](https://arxiv.org/abs/2601.18386)
*Gabriel Lee Jun Rong,Christos Korgialas,Dion Jia Xu Ho,Pai Chet Ng,Xiaoxiao Miao,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: ARMOR 用代理式推理统筹多种对抗攻击（CW/JSMA/STA），由VLM/LLM驱动自适应重参数与融合，在共享“混音台”上协同生成扰动，提升跨架构迁移与对白盒/黑盒目标的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化攻击流程是固定序列与静态组合，缺乏根据目标图像语义与环境反馈进行策略性调整与参数重配的能力，导致迁移性与鲁棒性有限。

Method: 提出 ARMOR：以VLM引导的多代理产生语义感知的扰动候选（涵盖CW、JSMA、STA三种原语），通过共享的“Mixing Desk”对多路扰动进行组合；LLM在闭环反馈中自适应调参与重参数化，实时协调并行代理；针对白盒以置信度+SSIM评分选择最佳或混合输出，对盲盒（黑盒）输出融合扰动以提高命中率与隐蔽性。

Result: 在标准基准上，相比固定流水线方法，ARMOR实现更好的跨架构迁移，能稳定欺骗黑盒与白盒设置；对白盒可选择最优或混合攻击，黑盒提供融合输出，整体成功率/转移性提升。

Conclusion: 通过代理式推理与语义感知的多原语协同，ARMOR克服静态攻击套件的局限，提供可适配、可融合、跨模型更强的对抗攻击框架。

Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.

</details>


### [138] [Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space](https://arxiv.org/abs/2601.18392)
*Moritz Rempe,Lukas T. Rotkopf,Marco Schlimbach,Helmut Becker,Fabian Hörst,Johannes Haubold,Philipp Dammann,Kevin Kröninger,Jens Kleesiek*

Main category: cs.CV

TL;DR: 提出一种直接在MRI k-space频域上进行分类的复杂值Vision Transformer（kViT），结合径向k-space切片策略，在fastMRI与自有数据上达到与图像域SOTA相当的性能，并在高加速和显存效率上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习多在重建后的幅度图像上训练，丢失相位信息且需昂贵的重建；同时常见网络的局部算子与k-space的全局、非局部特性不匹配。需要一种既符合MRI物理特性又能直接处理频域数据的模型，以提升鲁棒性与计算效率。

Method: 设计复杂值Vision Transformer（kViT），直接输入k-space数据；提出径向k-space分块策略，按频谱能量分布进行patch划分以缩小几何/物理鸿沟；在fastMRI与自有数据上进行分类实验，对比ResNet、EfficientNet、ViT等图像域基线，并评测不同加速因子与资源占用。

Result: kViT在分类准确度上与图像域SOTA相当；在高加速（欠采样更严重）场景下更稳健；训练期显存占用最高可比标准方法降低约68倍，展现显著的计算与内存效率优势。

Conclusion: 直接在k-space进行学习可避免重建开销、保留相位信息，并通过径向分块与复杂值Transformer实现对频域结构的有效建模；该范式在性能不降的前提下显著提升鲁棒性与资源效率，为从扫描仪直出的AI分析提供可行路径。

Abstract: Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.

</details>


### [139] [Larger than memory image processing](https://arxiv.org/abs/2601.18407)
*Jon Sporring,David Stansby*

Main category: cs.CV

TL;DR: 针对PB级超大图像数据（1.4 PB电镜、150 TB器官图谱），提出以“切片流式”方式最小化I/O、在有限内存下实现近线性磁盘扫描与可预测内存占用，并用DSL自动选择窗口、融合阶段与调度通道，显著提升吞吐。


<details>
  <summary>Details</summary>
Motivation: 超大体数据无法常驻内存，性能瓶颈主要在磁盘I/O；现有3D分块布局虽能减少部分算法的I/O，但在多次扫掠、邻域操作时会导致重复访问和复杂调度，亟需统一、I/O友好的执行模型。

Method: 提出以1D切片顺序的“扫掠执行”与窗口化操作、考虑重叠的分块（tiling）策略：- 将3D体以2D切片流式读写，限定两种扫掠方向，与磁盘顺序对齐；- 形式化自然2D/3D扫掠次序与窗口需求，减少重复读；- 在此之上开发DSL，进行编译期/运行期分析：自动选窗口大小、阶段融合、tee/zip流以及在有限RAM下的多通道调度，统一适配2D堆栈或3D分块（Zarr/HDF5）底层存储。

Result: 对需要邻域访问的常见预/后处理与分割形态学管线，实现近线性I/O扫描、显著降低重复磁盘访问，吞吐量大幅提升，同时内存占用可预测，且无需整卷驻留内存；对少数强依赖3D分块的算法亦可兼容。

Conclusion: 在PB级图像处理中，将分析重构为切片流式扫掠并配合DSL的自动管线优化，可在有限内存环境下接近I/O下界、获得稳定高吞吐；相较于纯3D分块式访问，切片流式在邻域密集型任务上更具I/O效率与实现简洁性。

Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

</details>


### [140] [Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings](https://arxiv.org/abs/2601.18414)
*Aura Loredana Dan*

Main category: cs.CV

TL;DR: 比较MobileNet、EfficientNet、VGG16在儿童绘画情绪分类上的性能、鲁棒性与效率，基于专家标注数据做迁移学习，讨论轻量与深层架构在移动/实时场景的权衡。


<details>
  <summary>Details</summary>
Motivation: ASD儿童早期情绪与沟通评估既重要又困难，传统方法侵入性强、主观或难以一致应用。借助儿童绘画进行情感识别提供非侵入、可扩展途径，需要系统评估适合的深度模型。

Method: 构建统一实验框架，使用心理专家标注的儿童绘画数据集，采用迁移学习分别微调MobileNet、EfficientNet、VGG16，比较分类准确性、鲁棒性和计算效率（含移动/实时相关指标）。

Result: 三种架构在准确率与资源消耗上呈现明显权衡：轻量模型在效率与实时性上更优，深层模型在某些情境下准确度更高；综合表现显示不同任务与部署需求下最佳选择不同。

Conclusion: 绘画情绪识别中不存在“一刀切”的最佳模型；在移动/实时应用应优先轻量网络，在追求极致精度可考虑更深网络。强调为具体应用进行权衡与调优的必要性。

Abstract: Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.

</details>


### [141] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: 论文指出：在将几何形态测量（GMM）用作机器学习（ML）输入时，常见做法是在训练/测试划分前用GPA对全体标本对齐，这会引入跨样本依赖并污染评估。作者用2D/3D仿真系统量化这种污染，提出“按训练集基准对测试集重对齐”的新流程，推导RMSE与样本量和标记点维度的缩放规律，展示地标空间相关性的关键作用，并给出实践指南。


<details>
  <summary>Details</summary>
Motivation: GMM在ML中的应用越来越多，但业内默认在数据拆分前进行GPA对齐，可能导致信息泄漏与依赖，从而高估模型性能。缺乏系统量化这一偏差的证据、明确的纠偏流程，以及对Procrustes形状空间统计限制的清晰阐释。

Method: 采用受控的2D与3D模拟，系统改变样本量、地标数量/密度、同速生长（allometry）模式与空间相关结构；比较传统“先全体GPA再拆分”与新提出的“先拆分、以训练集为参考对测试集重对齐”的流程；在Procrustes切空间中解析推导RMSE随自由度的缩放斜率；评测线性与卷积回归模型在不同空间自相关设定下的表现。

Result: 1) 证明全体GPA后再划分会产生跨集依赖与性能膨胀；2) 提出并验证“测试集对训练集对齐”的重对齐方案可消除污染；3) 发现样本量与地标空间维度之间存在稳健的“对角线”缩放规律，RMSE随各自自由度按解析斜率缩放；4) 显示忽视地标空间自相关会明显降级模型（线性与CNN均受影响）。

Conclusion: 在GMM+ML流程中，预处理必须避免GPA导致的信息泄漏；应先划分数据，再以训练集为参考对测试样本对齐。地标数量与样本量受Procrustes形状空间自由度的基本约束，性能与误差存在可预期的缩放规律；应在建模中显式利用地标间的空间相关。论文给出实际重对齐指南并澄清这些统计限制。

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [142] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: 提出3DGesPolicy：把整体共语手势生成视为连续轨迹控制，用扩散策略统一建模“帧间整体动作”，并结合GAP多模态融合，使肢体与面部在空间与语义上更连贯；在BEAT2上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为身体部件分解或逐帧回归，导致肢体与面部语义不一致、运动轨迹不稳定且含无意义抖动；需要一种能在整体层面建模、并与语音深度对齐的生成方法。

Method: 引入来自机器人学的扩散策略（diffusion policy），将帧到帧的变化视为统一的整体动作，作为连续轨迹控制来学习真实运动流形上的连贯轨迹；并提出GAP（Gesture-Audio-Phoneme）融合模块，深度整合语音、音频特征与音素级节拍信息，进行结构化、细粒度的跨模态对齐；在BEAT2上进行训练与评估。

Result: 在BEAT2数据集上进行定量（如语义对齐度、空间稳定性、自然度等指标）与定性评估，3DGesPolicy在自然性、表现力和与语音的对齐上均超过其他SOTA基线。

Conclusion: 将整体手势生成重构为连续轨迹控制并用扩散策略学习，可显著提升空间与语义连贯性；GAP融合进一步弥合语义-动作-表情的对齐差距；方法在标准数据集上验证有效，适合用于自然、表达性强的全身共语手势生成。

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [143] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: Fair-Eye Net 是一个面向青光眼从筛查到随访与早期风险预警的多模态、公平与可靠的AI系统，融合眼底照/OCT/VF与人口学信息，通过不确定性感知的分层门控与公平约束，实现高AUC、稳定跨域性能、显著降低种族漏诊差异，并可提前3–12个月高敏感度/特异度预警。


<details>
  <summary>Details</summary>
Motivation: 现实中青光眼筛查与进展评估常基于单一或松散耦合的测试，依赖主观判断，导致照护碎片化；高质量成像与专科资源不均衡，引发一致性与公平性问题。需要一个能端到端闭环、在多模态数据中稳健工作、并显著改善群体公平性的AI系统。

Method: 提出 Fair-Eye Net：双流异构融合架构整合眼底照片、OCT结构指标、视野（VF）功能指标及人口学因素；引入不确定性感知的分层门控（hierarchical gating）进行选择性预测与安全转诊；在多任务学习框架中将公平性作为主目标之一加入优化，使用公平约束减少弱势子群漏诊；并支持随访与风险预警。

Result: 实验显示：筛查AUC=0.912（特异性96.7%）；将种族假阴性差距从12.31%降至3.28%（下降73.4%）；跨域性能稳定；可提前3–12个月进行风险预警（敏感度92%，特异度88%）。

Conclusion: Fair-Eye Net 将公平性与临床可靠性内生化至模型训练与决策流程，较后验修正更具可迁移性与可复现性，为临床转化与大规模部署提供路径，有望提升全球眼健康公平。

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [144] [DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment](https://arxiv.org/abs/2601.18493)
*Sara Tehrani,Yonghao Xu,Leif Haglund,Amanda Berg,Michael Felsberg*

Main category: cs.CV

TL;DR: 提出DisasterInsight基准，围绕xBD重构约11.2万栋建筑实例，评测VLM在灾害场景下的多任务理解；并给出经LoRA微调的DI-Chat作为领域基线，显著提升损毁/灾种分类与报告生成，但建筑功能分类仍困难。


<details>
  <summary>Details</summary>
Motivation: 现有遥感VLM基准多为粗粒度、图像级标签，难以满足救灾流程需要的功能性理解、可指令鲁棒性与结构化输出能力，亟需面向真实任务的评测框架与领域适配模型。

Method: 将xBD重组为建筑为中心的实例级数据（约112K），设计多样指令评测任务：建筑功能分类、损毁等级、灾种分类、计数与按人道评估规范的结构化报告生成；并以参数高效的LoRA对现有VLM进行灾害指令微调得到DI-Chat，开展与通用/遥感VLM的大规模对比实验。

Result: 现有最先进通用与遥感VLM在多任务上存在显著性能差距，尤其在损毁理解与结构化报告生成上；DI-Chat在损毁等级、灾种分类及报告质量上取得显著提升，但在建筑功能分类上仍表现不足。

Conclusion: DisasterInsight为灾害图像的落地多模态推理提供统一评测平台；领域指令微调（DI-Chat）有效缩小关键任务差距，但功能分类仍是瓶颈，提示未来需更丰富的功能标注与先验建模。

Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.

</details>


### [145] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: 论文提出一种结合基础模型嵌入与聚类的冷启动采样，并在后续主动学习阶段融合空间多样性的基于不确定性选择，以减少医学分割标注成本并提升小样本性能；在X-ray与MRI三数据集上显著优于随机/基线策略（Dice与Hausdorff均有改进）。


<details>
  <summary>Details</summary>
Motivation: 医学分割需要精确标注，但人工标注昂贵且耗时。现有主动学习通常先用“多样性冷启动”再用“不确定性选择”，但如何在冷启动阶段更系统地覆盖数据分布、并在后续选择中兼顾空间多样性与可解释性仍不足。

Method: 1) 利用基础模型（foundation model）提取图像嵌入；2) 进行聚类并自动确定簇数，按簇比例抽样，构建代表性强的初始训练集；3) 在主动学习迭代中，采用不确定性度量（如熵）并融合空间多样性进行样本选择；4) 可视化候选样本在特征空间的分布，提升直观与可解释性。

Result: CheXmask：冷启动Dice由0.918→0.929，Hausdorff由32.41→27.66mm；AL阶段结合熵+多样性Dice由0.919→0.939，Hausdorff由30.10→19.16mm。Montgomery：冷启动Dice由0.928→0.950，Hausdorff由14.22→9.38mm。SynthStrip：冷启动对Dice影响小但Hausdorff由9.43→8.69mm；AL阶段Dice由0.816→0.826，Hausdorff由7.76→6.38mm。总体在低数据场景优于基线。

Conclusion: 基于基础模型嵌入的聚类式冷启动与融合空间多样性的主动学习策略，可在低标注预算下稳定提升医学分割精度与边界质量，方法直观、可解释且跨模态有效。

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [146] [GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning](https://arxiv.org/abs/2601.18543)
*Kaixun Jiang,Yuzheng Wang,Junjie Zhou,Pandeng Li,Zhihang Liu,Chen-Wei Xie,Zhaoyu Chen,Yun Zheng,Wenqiang Zhang*

Main category: cs.CV

TL;DR: GenAgent 通过“理解-生成解耦+代理式多轮交互”统一视觉理解与图像生成：模型自身负责理解，调用外部生成器作为工具完成生成；先监督冷启动，再用代理式强化学习端到端优化，显著提升多数据集表现，并展现跨工具泛化、测试时多轮可扩展与任务自适应推理。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在同时兼顾理解与生成时训练成本高且两者互相牵制；模块化系统又受限于固定流水线，难以进行自我反思与多轮优化。作者希望在不牺牲训练经济性的前提下，实现可自我迭代、可调用不同图像生成器的统一框架。

Method: 提出代理式框架GenAgent：多模态大模型负责理解与规划，将图像生成模型作为可调用工具；支持多轮链式思考，含推理、工具调用、判别与反思。训练分两阶段：1) 监督微调高质量数据，学习合适的工具调用与反思行为（冷启动）；2) 端到端代理式强化学习，将点式奖励（最终图像质量）与对式奖励（反思准确性）结合，并用轨迹重采样促进多轮探索。强调跨生成器调用与自适应推理策略。

Result: 在GenEval++上相对基座生成器(FLUX.1-dev)提升+23.6%，在WISE上提升+14%。实验还展示：可泛化到不同能力的生成器；多轮交互下测试时规模化带来持续增益；能根据任务自动调整推理深度与策略。

Conclusion: 通过理解-生成解耦和代理式多轮优化，GenAgent在无需昂贵统一训练的情况下实现了强大的图像生成与理解协同，具有跨工具泛化、可扩展性与任务自适应等关键优势，提供了实践可行的下一代多模态代理范式。

Abstract: We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.

</details>


### [147] [REMAC: Reference-Based Martian Asymmetrical Image Compression](https://arxiv.org/abs/2601.18547)
*Qing Ding,Mai Xu,Shengxi Li,Xin Deng,Xin Zou*

Main category: cs.CV

TL;DR: 提出REMAC：火星图像参考驱动的非对称压缩，编码端极简、解码端增强，利用跨图像与图内相似性与潜特征回收，在降43.51%编码复杂度下，BD-PSNR提升0.2664 dB。


<details>
  <summary>Details</summary>
Motivation: 火星到地球的通信带宽极低且火星端算力受限，地球自然图像的学习式压缩方法未考虑：1) 编码端算力极度受限；2) 火星图像间存在强烈的跨图像相似性可被利用，从而限制了在火星任务中的有效性。

Method: 提出REMAC（参考驱动的火星非对称压缩）：将复杂度从编码端转移到资源充足的解码端。为利用跨图像相似性，设计参考引导的熵模型与参考解码器（ref-decoder），从参考图像中提取信息以减少编码冗余；为利用图内相似性，ref-decoder 采用深度多尺度、扩大感受野以建模长程依赖；另提出潜表示循环（latent feature recycling）机制以进一步降低编码端计算。

Result: 在实验中，相比最新方法，编码端复杂度降低43.51%，同时实现BD-PSNR提升0.2664 dB。

Conclusion: REMAC通过参考驱动与解码端增强的非对称设计，在火星图像压缩中兼顾超低编码复杂度与压缩性能提升，适合受限火星端与资源充足地面端协同的传输场景。

Abstract: To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \textit{intra-} and \textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.

</details>


### [148] [Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray](https://arxiv.org/abs/2601.18555)
*Roberto Di Via,Vito Paolo Pastore,Francesca Odone,Siôn Glyn-Jones,Irina Voiculescu*

Main category: cs.CV

TL;DR: 研究比较X光与MRI在股骨髋臼撞击综合征（FAI）筛查中的等效性，发现在标准热力图回归框架下，MRI在冠状位也能达到与X光相当的标注定位与诊断性能，支持在常规MRI流程中集成自动FAI评估。


<details>
  <summary>Details</summary>
Motivation: 临床常以角度测量进行筛查，FAI传统依赖X光角度，但评估撞击区域的高度与范围需要3D视角（MRI）。需要验证是否可用MRI实现与X光相当的自动化标志点检测与诊断，从而统一与拓展工作流程至3D体积分析。

Method: 进行匹配队列验证研究（89例，配对MRI/X光），采用标准热力图回归的关键点检测网络，对X光与MRI的标志点定位与由此推导的诊断指标进行对比，重点评估cam型FAI在冠状位3D MRI切片中的性能与临床等效性。

Result: MRI在标志点定位精度与cam型FAI诊断准确率上与X光表现等效，展示了在3D MRI体数据的冠状位上实现可靠自动标志点定位的可行性。

Conclusion: MRI可作为自动化FAI评估的等效成像途径，并可通过在3D体数据上增加更多标志点以支持体积化分析；研究结果支持将自动FAI评估集成到常规MRI工作流程中，代码已开源。

Abstract: Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions

</details>


### [149] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出SDA-QEC框架：用简化扩散生成少数类数据并在MobileNetV2中加入量子特征层，缓解医学影像类别不平衡并提升分类性能；在冠脉造影任务上取得≈98.3%准确/敏感/特异与98.8% AUC。


<details>
  <summary>Details</summary>
Motivation: 医学影像分类常严重类别不平衡，导致对少数类召回低、临床误诊风险高。亟需既能重平衡数据又能提升特征判别力的方法，尤其在小样本、高风险场景。

Method: 1) 简化扩散数据增强：以轻量扩散模型为少数类生成高质量合成样本，重塑训练分布。2) 量子增强分类：在MobileNetV2中嵌入量子特征层，通过希尔伯特空间高维映射增强类间可分性；整体端到端训练。

Result: 在冠状动脉造影图像分类上，SDA-QEC达成Accuracy 98.33%、AUC 98.78%、F1 98.33%、Sensitivity 98.33%、Specificity 98.33%，显著优于ResNet18、MobileNetV2、DenseNet121、VGG16等基线。

Conclusion: 生成式扩增与量子增强建模的结合在真实医学影像不平衡任务中可行且有效，能实现敏感性与特异性的平衡，适合小样本、高不平衡、高风险诊断应用，并为可靠医疗AI提供新方向。

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [150] [AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging](https://arxiv.org/abs/2601.18560)
*Li Fang,Tianyu Li,Yanghong Lin,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 提出一种面向卫星边缘计算的轻量级高光谱图像分类方法，通过两阶段像素级标签传播与少样本学习，在资源受限和图像退化情况下实现快速、低通信负载的在轨自主判别。


<details>
  <summary>Details</summary>
Motivation: 高光谱卫星数据光谱信息丰富，但下行带宽成为灾害监测、应急制图等需要快速响应场景的瓶颈；同时在轨处理受资源限制，且传感器故障与扫描误差导致坏/错位像素与混合噪声，要求无需重依赖空间结构、鲁棒且高效的在轨分类方案。

Method: 构建AI使能的卫星边缘计算范式，采用非深度、轻量级框架结合少样本学习；提出两阶段像素级标签传播：阶段一通过锚点-像素亲和矩阵将选定锚点标签传播得到初始标签；随后基于像素相似度构建top-k裁剪稀疏图；阶段二利用该稀疏图的闭式解替代迭代优化完成精化；并提出基于秩约束的图聚类算法自动确定锚点标签。

Result: 实现无需空间结构信息、仅依赖单像素光谱特征的分类流程，适配资源受限与图像退化场景，降低在轨计算和通信负担，提升应急应用的响应速度与分类稳健性。

Conclusion: 轻量级、少样本、两阶段标签传播与秩约束锚点选择可在卫星端高效、稳健地完成高光谱分类，为在轨自主决策与快速下行减载提供可行路径。

Abstract: As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.

</details>


### [151] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出一种“自我精炼”的视频采样方法，在推理时对预训练视频生成器进行迭代内循环去噪与不确定性感知的局部精炼，无需外部验证器或额外训练，显著提升物理一致性与运动连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成器难以捕捉复杂物理动态，生成结果缺乏物理真实感。以往方法依赖外部物理验证器或在增强数据上再训练，代价高且对细粒度运动仍力有不逮，迫切需要一种无需额外训练、能在推理阶段提升物理与运动质量的方案。

Method: 将预训练视频生成器视为去噪自编码器，在推理期进行多步内循环迭代精炼（self-refining sampling）。提出基于自一致性的“不确定性感知精炼”：评估生成帧/区域的一致性与不确定性，只对高不确定/低一致性的局部进行选择性再精炼，从而避免过度精炼带来的伪影。无需外部验证器或额外微调训练。

Result: 在多种最先进视频生成器上验证，显著提升运动连贯性与物理对齐度。与默认采样器与基于引导（guidance）的采样器相比，获得超过70%的人工偏好率。

Conclusion: 自我精炼视频采样在不增加训练成本的情况下，提升了视频生成在物理与运动层面的质量；不确定性感知的选择性精炼有效抑制过精炼伪影，方法简单、通用，可作为现有视频生成器的即插即用推理增强。

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [152] [GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization](https://arxiv.org/abs/2601.18585)
*Chenxi Liu,Selena Ling,Alec Jacobson*

Main category: cs.CV

TL;DR: 提出GimmBO，通过偏好式贝叶斯优化高效探索扩散模型适配器合并的权重空间，较现有手动滑条与基线更快收敛且成功率更高。


<details>
  <summary>Details</summary>
Motivation: 社区中大量基于同一底模的风格/主题适配器可按权重合并，形成巨大连续设计空间；但现有依赖人工调参，维度高（20-30个适配器）、范围受限且稀疏，低效且难选权重。需要一种能利用人类偏好反馈、在高维受限空间高效搜索的算法。

Method: 提出GimmBO：基于Preferential Bayesian Optimization的人类偏好驱动交互式探索框架。结合真实使用观察（稀疏性、权重范围约束），设计两阶段BO后端：阶段一进行高维稀疏-aware筛选与范围定位，阶段二在精炼子空间内收敛优化。支持交互偏好查询与若干扩展（如约束、正则等）。

Result: 在模拟用户与用户研究中，相比标准BO与线性搜索基线，GimmBO表现出更快的收敛速度、更高的成功率与稳定的性能增益；在高维场景下采样更高效，并展示了框架的灵活扩展性。

Conclusion: GimmBO有效解决多适配器权重合并的高维探索难题，利用偏好反馈和两阶段BO实现高效、稳健的交互式图像生成适配器融合，优于常见基线并具推广潜力。

Abstract: Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.

</details>


### [153] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: 提出AGSP-DSA框架，用自适应图信号处理与动态语义对齐实现多模态融合（文本/音频/图像），通过双图关系、谱滤波与多尺度GCN嵌入，并用语义感知注意力动态分配模态权重；在CMU-MOSEI、AVE、MM-IMDB上达到或刷新SOTA且对缺失模态具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态任务常受异构来源（文本/音频/图像）间关系不齐、噪声与缺失模态影响；现有方法难以同时建模跨模态与模态内结构，并缺乏随上下文动态调整模态贡献的机制。作者希望通过图信号处理与语义对齐提升鲁棒融合与泛化。

Method: 1) 双图构建：学习模态内（intra-modal）与跨模态（inter-modal）关系图；2) 谱图滤波：在频域增强信息性信号、抑制噪声；3) 多尺度GCN嵌入：提取不同尺度的结构化表示；4) 语义感知注意力：根据上下文动态调节各模态权重，实现动态语义对齐；5) 端到端训练，实现自适应图结构与表示联合优化。

Result: 在三数据集达SOTA：CMU-MOSEI上Acc 95.3%、F1 0.936、mAP 0.924，较MM-GNN准确率提升2.6%；AVE上Acc 93.4%、F1 0.911；MM-IMDB上Acc 91.8%、F1 0.886；在缺失模态情形仍保持较好性能。

Conclusion: AGSP-DSA通过自适应图信号处理、双图关系与语义感知注意力实现稳健多模态融合，在情感分析、事件识别与多媒体分类上表现优异且对缺失模态鲁棒，验证其有效性与泛化潜力。

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


### [154] [EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery](https://arxiv.org/abs/2601.18597)
*Yu Xia,Chang Liu,Tianqi Xiang,Zhigang Tu*

Main category: cs.CV

TL;DR: EFSI-DETR通过频率-空间动态融合与高效语义聚合，提升UAV小目标实时检测，在VisDrone提升AP+1.6%、APs+5.8%，在RTX4090达188 FPS。


<details>
  <summary>Details</summary>
Motivation: UAV图像小目标检测受限于特征表达弱、多尺度融合不充分；现有方法很少利用频域信息且卷积操作静态，难以充分挖掘深层语义与细粒度细节。

Method: 提出EFSI-DETR，包括：1) DyFusNet：动态频率-空间统一协同网络，将频域与空间线索联合建模，进行鲁棒的多尺度特征融合；2) ESFC：高效语义特征浓缩模块，以极低计算开销提取深层语义；并加入FFR（细粒度特征保留）策略，在融合时引入空间信息丰富的浅层特征以保留细节。

Result: 在VisDrone与CODrone上取得SOTA：VisDrone上整体AP提升1.6%，小目标APs提升5.8%；在单卡RTX 4090上达到188 FPS实时推理速度。

Conclusion: 动态结合频域与空间信息并以轻量方式增强深层语义，同时保留浅层细节，可显著提升UAV小目标检测的精度与实时性。

Abstract: Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.

</details>


### [155] [Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures](https://arxiv.org/abs/2601.18619)
*Jorge Quesada,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 该论文提出一种面向尺度的自监督学习改造：在预训练增强流程中加入小窗口裁剪，以放大细粒度结构，提高在小、稀疏、局部不规则目标上的分割表现；在地震断层与神经影像细胞分割上分别最高提升约13%与5%。


<details>
  <summary>Details</summary>
Motivation: 现有SSL多为大且均匀区域分割而调参，对小目标、稀疏或不规则对象效果明显下降；需要让SSL更贴合目标任务的空间尺度特征。

Method: 在自监督预训练的数据增强管线上加入“小窗口裁剪/缩放”的尺度感知策略，使模型在预训练阶段更多接触细粒度结构；在两种差异化模态（地震成像与神经影像）上应用并评估。

Result: 在标注受限条件下，对地震稀疏断层分割准确率提升最高达13%，对神经影像小细胞结构分割提升约5%；而对大尺度特征（如地震相、组织大区）提升有限。

Conclusion: SSL的价值与目标对象的尺度和稀疏性强相关；将SSL设计与目标尺寸对齐（如小窗口裁剪）可构建更有效的表征学习流程，适用于多类科学成像任务。

Abstract: Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.

</details>


### [156] [Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation](https://arxiv.org/abs/2601.18623)
*Zihao Wang,Yuzhou Chen,Shaogang Ren*

Main category: cs.CV

TL;DR: 提出一种在扩散模型中显式建模域迁移动态的跨模态图像翻译方法：在每个反向步预测空间变化的混合场，并在漂移项中注入目标一致的恢复项，从而在更少步数下实现更高语义与结构一致性。


<details>
  <summary>Details</summary>
Motivation: 传统跨域扩散常用单一全局线性映射或固定时间表进行域迁移，导致采样轨迹偏离数据流形、校正负担加重与语义漂移，效率和鲁棒性差。作者希望把域迁移的动态直接融入生成过程，避免“固定日程域迁移”的共性失败模式。

Method: 在扩散反演的每一步：1) 预测空间可变的mixing field，控制源/目标成分的局部混合；2) 在漂移项中加入显式的、与目标域一致的restoration项，作为in-step guidance，使大更新保持在流形上，将模型职责由全局对齐转为局部残差校正。给出连续时间表述及精确解形式，并推导保持边缘一致性的一级采样器。

Result: 在医学影像、遥感、与电致发光语义映射等多个跨模态翻译任务上，相比标准扩散基线，提高了结构保真度与语义一致性，并以更少的去噪步数收敛。

Conclusion: 将域迁移动态内嵌到扩散生成过程、利用空间变化混合场与目标一致的恢复项进行逐步引导，可缓解固定日程域迁移的离流形与语义漂移问题，实现更稳健高效的跨模态图像翻译。

Abstract: Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.

</details>


### [157] [CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search](https://arxiv.org/abs/2601.18625)
*Zequn Xie*

Main category: cs.CV

TL;DR: 提出CONQUER：一个训练阶段增强跨模态对齐、推理阶段自适应优化文本查询的两阶段TBPS框架，显著提升跨域与不完整查询下的检索效果。


<details>
  <summary>Details</summary>
Motivation: TBPS需用自然语言在大规模图库中检索行人图像，但存在图文模态鸿沟与用户描述含糊/不完整，影响实际公共安全落地。

Method: 两阶段框架：训练期通过(1)多粒度编码获取细/粗层级特征；(2)互补样本对挖掘，提升判别性；(3)基于最优传输的上下文引导最优匹配，强化跨模态对齐。推理期引入即插即用的查询增强模块：通过锚点选择与属性驱动扩充对含糊/缺失描述进行细化补全，无需回训主干。

Result: 在CUHK-PEDES、ICFG-PEDES、RSTPReid上，相比强基线在Rank-1与mAP上均有一致提升；在跨域与不完整查询场景下仍保持显著增益。

Conclusion: CONQUER在训练与推理两端同时发力，有效缓解模态差异与查询模糊问题，具备现实TBPS部署可行性与稳健性；代码已开源。

Abstract: Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.

</details>


### [158] [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/abs/2601.18633)
*Tong Shi,Melonie de Almeida,Daniela Ivanova,Nicolas Pugeault,Paul Henderson*

Main category: cs.CV

TL;DR: Splat-Portrait 用高斯点云渲染，将单张人像与语音驱动生成自然口型的3D说话头像；仅用2D重建与SDXL式得分蒸馏训练，无3D/关键点监督，视觉质量与新视角效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D说话头像依赖基于形变/扭曲的先验（如光流/warp）与领域特定表征，导致3D重建不准、口型与视角真实性不足。需要一种无需3D监督、同时提升3D一致性与口型自然度的方法。

Method: 以Gaussian Splatting表示静态3D头部，并预测整幅2D背景实现前景-背景解耦；从音频条件生成口型与表情的动态变化，但不使用运动先验；训练使用纯2D重建损失与得分蒸馏损失（无3D监督、无关键点/landmarks），实现从单张人像的自动解耦与可听觉驱动的口型生成；支持新视角合成。

Result: 在说话头像生成与新视角合成上取得更好视觉质量；与以往方法相比，真实感与一致性更佳；实验表明其单图到3D重建与口型同步更稳定。

Conclusion: 基于高斯Splatting的Splat-Portrait可在无3D与关键点监督下，从单张人像与语音生成高真实感的说话头像，并兼顾新视角渲染；方法简化了先验依赖，提升了重建与口型质量，具有良好实用潜力。

Abstract: Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.

</details>


### [159] [Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge](https://arxiv.org/abs/2601.18698)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出GAP评测框架与GEOATTRACTION-500基准，系统评估文本到视频模型在全球景点上的地理公平与视觉知识；对Sora 2测试显示其区域间知识较均匀，受景点热度影响很小。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到视频生成效果显著提升，但是否存在对不同地区的偏见、以及模型是否真正掌握地理锚定的视觉知识仍不明确，需要一个可量化、可比较的评估体系。

Method: 构建GEOATTRACTION-500（500个全球分布的景点，覆盖不同地区与热度）；提出GAP框架，包含多重互补指标：整体结构对齐、关键点级精细对齐、以及基于视觉-语言模型的判断，并用人类评估验证这些指标的有效性；以Sora 2为被测模型进行吸引物中心的生成与对齐评估。

Result: 在Sora 2上，观察到跨地区、发展水平与文化群体的地理锚定视觉知识较为均匀，仅与景点热度呈弱相关；整体生成质量与景点特异性知识被指标区分开并与人评一致。

Conclusion: 当前顶尖文本到视频模型在全球视觉知识表达上比预期更均衡，适合面向全球的应用场景；但仍需持续监测与评估，以跟踪系统演进与潜在偏差变化。

Abstract: Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.

</details>


### [160] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: 提出MinkUNeXt-VINE：一种面向葡萄园环境的轻量级深度学习位姿/地点识别方法，利用稀疏低成本LiDAR与多损失Matryoshka表示学习，在实时性与准确性间取得优异权衡，并在两套长期数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 农业环境缺乏结构化与显著地标，导致移动机器人定位尤其是回环/地点识别困难；现有研究多聚焦于检测与分割，地点识别仍不成熟，且需要在低成本传感器与实时约束下取得可靠性能。

Method: 构建MinkUNeXt-VINE框架：1) 针对稀疏低成本LiDAR的预处理流程；2) 采用Matryoshka Representation Learning的多损失学习策略，生成低维嵌入用于高效地点匹配；3) 轻量化网络（基于MinkUNeXt稀疏卷积）以实现实时性；并进行多种评估设置与消融实验。

Result: 在两套长期葡萄园数据集（不同LiDAR配置）上，所提方法在准确率/召回率等指标上超过现有方法；在低分辨率、低成本输入下仍保持鲁棒，并以更低维输出达到更高效率，展现良好的性能-效率折中。

Conclusion: MinkUNeXt-VINE在农业（葡萄园）地点识别中实现SOTA级或更优表现，兼顾实时性与低成本传感器适配；其多损失Matryoshka表示学习与预处理是关键，代码已开源以便复现。

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [161] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: 提出SeNeDiF-OOD：用语义分层的嵌套二叉融合节点来做OOD检测，在MonuMAI真实场景上显著优于单阶段基线，同时保持ID性能。


<details>
  <summary>Details</summary>
Motivation: 单阶段OOD检测难以同时应对多样异质的分布外数据（从低级噪声/腐败到高级语义迁移、未知类别、对抗攻击）。需要一种能在不同语义抽象层面对边界进行建模与整合的方法，以提高在开放环境中的可靠性。

Method: 构建“语义嵌套二分融合”框架：将OOD检测分解成层级化的二元决策节点，每一层聚焦于特定语义抽象层次的判别（如低级外观扰动、类别级未知、非目标域）。各节点学习相应决策边界，并通过层级融合将多个边界集成为最终判定，从而兼顾多尺度语义信息。以MonuMAI建筑风格识别为案例进行实现与评测。

Result: 在MonuMAI开放环境下（含非纪念物图像、未知建筑风格、对抗攻击等多类OOD），该分层融合方法在过滤各类OOD方面显著优于传统基线（单阶段检测器），同时维持或不降低ID（分布内）识别性能。

Conclusion: 语义分层的嵌套二分融合可有效弥补单阶段OOD检测对异质OOD的处理不足，能在真实应用中实现更稳健的开放世界部署；该范式具有可扩展性，可适配不同语义层的检测需求。

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>
